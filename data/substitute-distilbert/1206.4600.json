{"id": "1206.4600", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes", "abstract": "we present a framework for concurrent inquiries in the presence against a nonexhaustively defined set of classes that incorporates supervised classification with data discovery and modeling. a dirichlet process prior ( dpp ) model defined over class distributions ensures that both known and unknown class distributions differentiate according just a common base distribution. in an industry to automatically discover potentially distinct class formations, the prior model is coupled with a suitably chosen data model, and sequential monte carlo sampling algorithms used to perform online inference. our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase could estimate number of samples originating thru this class indicates the onset of new outbreak.", "histories": [["v1", "Mon, 18 Jun 2012 14:40:38 GMT  (405kb)", "http://arxiv.org/abs/1206.4600v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["murat dundar", "ferit akova", "alan qi", "bartek rajwa"], "accepted": true, "id": "1206.4600"}, "pdf": {"name": "1206.4600.pdf", "metadata": {"source": "META", "title": "Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes", "authors": ["Murat Dundar", "Ferit Akova", "Bartek Rajwa"], "emails": ["dundar@cs.iupui.edu", "ferakova@cs.iupui.edu", "alanqi@cs.purdue.edu", "brajwa@purdue.edu"], "sections": [{"heading": "1. Introduction", "text": "A training set is considered exhaustive if it contains samples from all classes of informational value. When some classes are missing and hence not represented, the resulting training set is considered nonexhaustive. It is impractical, often impossible, to define a training set with a complete set of classes and then collect samples for each class, mainly because some of the classes\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmay not be in existence at the time of training, they may exist but are not known, or their existence may be known but samples are simply not obtainable. A traditional supervised classifier trained using a nonexhaustive training set misclassifies a sample of a missing class with a probability one, making the associated learning problem ill-defined."}, {"heading": "1.1. Motivation", "text": "The current research is driven mainly by a biosensing problem involving prediction of the presence of specific as well as unmatched/emerging pathogenic microorganisms in various biological samples. A global surge in the number of outbreaks together with elevated concerns about biosecurity has led to an enormous interest among scientific communities and government agencies in developing reagantless techniques for rapid identification of pathogens. Traditional recognition methods based on antibodies or genetic matching remain labor intensive and time consuming, and involve multiple steps. Recent studies based on quantitative phenotypic evaluation has shown great promise for distinguishing bacterial cultures at the genus, species, and strain level.\nThe core advantage of label-free methods is their ability to quantify phenotypes for which there are no available antibodies or genetic markers. This information can be used within a traditional supervised-learning framework in which knowledge discovered from independently tested and prelabeled samples is used for training. However, the quality of training libraries is potentially limited because the sheer number of bacterial classes would not allow for practical and manageable\ntraining in a traditional supervised setting; for instance Salmonella alone has over 2400 known serovars. Additionally, microorganisms are characterized by a high mutation rate, which indicates new classes of bacteria can emerge anytime. Nonexhaustive learning when implemented in this domain in an online fashion aims at rapid identification of new, emerging classes of microorganisms, which are not represented in the initial training library. Ability to detect the sudden presence of a new class (or classes) would be an important element of an automated outbreak-identification strategy."}, {"heading": "1.2. Proposed approach in a nutshell", "text": "Using a nonexhaustive training dataset, a Dirichlet process prior (DPP) is coupled with a Normal data model to deal with known as well as unknown classes. The parameters of the base distribution, which is chosen as a bivariate Normal \u00d7 Inverted Wishart distribution, are estimated using samples initially available for known classes. A sequential importance resampling (SIR) technique is proposed to perform online inference to efficiently evaluate the probability of a new sample belonging to an emerging class or one of the existing ones without the need for explicit knowledge of the class labels of previously observed samples. In this framework new classes characterized by a rapid increase in sample size is of significance for early identification of potentially interesting class formations."}, {"heading": "1.3. Related Work", "text": "Early work most similar to nonexhaustive learning includes the two studies reported in (Akova et al., 2010; Miller & Browning, 2003). In (Akova et al., 2010) a Bayesian approach based on the maximum likelihood detection of novelties using a dynamically updated class set was proposed. In (Miller & Browning, 2003) known and unknown classes were modeled by a mixture of experts model with learning performed by expectation-maximization. The former depends on the class conditional likelihoods for creating new classes and the latter uses minimum description length coupled with some heuristics to determine the optimal number of mixture components. Neither of the approaches consider a prior model for class distributions which results in the decision to create a new class to be mainly data driven and ad-hoc in both approaches. Also the lack of efficient online/incremental learning capabilities makes both approaches impractical for processing large sequential data.\nOther work related to nonexhaustive learning can be reviewed within the scope of offline anomaly/novelty detection, online/incremental learning, and online clus-\ntering with novelty detection. Most of the early work on offline anomaly/novelty detection is developed around one-class classification problems and uses either support estimation, one-class classifiers, or density-based models to identify novelties. These techniques provide an offline framework for detecting novelties but do not differentiate among them; thus, lack the capability to discover and model individual classes online.\nOnline/incremental learning develops efficient algorithms for sequential classification problems such that the classifier model can be updated using only the current sample without retraining with past samples. Many of these studies assume that the initial training set is exhaustive. The one (Kivinen et al., 2001) that does consider nonexhaustiveness studies novelty detection along with online learning, but its scope is limited to one class problems only.\nAnother line of work related to nonexhaustive learning has been developed in the area of online clustering with and without novelty detection. We will use DPP in this study for online class modeling the same way these techniques use it for online cluster modeling. However, unlike online cluster modeling, which depends on a vague prior, a more informative prior can be obtained in a nonexhaustive setting using samples of represented classes. To avoid the vague prior issue the work in (Zhang et al., 2005) proposes using a historical dataset to estimate the parameters of the DPP. Although the work is framed as a clustering problem with a multinomial data model, it is similar to the proposed study in using labeled data for estimating the prior model. However, the solution offered in this approach for the sequential update of the cluster models is suboptimal in that the posterior probability for each incoming sample was evaluated once and same values were used for all subsequent samples. As new samples emerge these previously evaluated probability values can increase/decrease resulting in suboptimal class assignments for samples observed earlier. We believe that the proposed sequential inference technique involving particle filters is an important step toward addressing this problem in a nonexhaustive setting."}, {"heading": "2. Problem formulation", "text": "In this section we present a general framework for learning with a nonexhaustively-defined training dataset, which allows for online discovery as well as modeling of new classes. To differentiate classes discovered online from those initially available in the training library we introduce the notion of labeled vs. unlabeled classes, where the terms labeled and unlabeled refer to verified and unverified classes, respectively. When referring to\nclasses discovered online the terms unlabeled class and cluster are used interchangeably throughout this text. In the proposed framework online class modeling is tackled by a DPP model (Ferguson, 1973)."}, {"heading": "2.1. Dirichlet process prior", "text": "Let xi, i = {1, . . . , n} be the feature vector characterizing a sample in the d-dimensional vector space < and yi be its corresponding class indicator variable. If xi is distributed according to an unknown distribution p(.|\u03b8i), then defining a DPP over class distributions is equivalent to modeling the prior distribution of \u03b8 by a Dirichlet process. More formally,\nxi|\u03b8i \u223c p(\u00b7|\u03b8i) \u03b8i \u223c G(\u00b7) G \u223c DP (\u00b7|G0, \u03b1)\n(1)\nwhere G is a random probability measure, which is distributed according to a Dirichlet process (DP) defined by a base distribution, G0, and the precision parameter, \u03b1. Given that G is distributed according to a DP, the stick-breaking construction due to (Ishwaran & James, 2001) suggests G = \u2211\u221e i=1 \u03b2i\u03b4\u03c6i where \u03b2i = \u03b2 \u2032\ni \u220fi\u22121 l=1(1 \u2212 \u03b2 \u2032 l ), \u03b2 \u2032\ni \u223c Beta(1, \u03b1), and \u03c6i \u223c G0. The points \u03c6i are called the atoms of G. Note that unlike continous distributions the probability of sampling the same \u03c6i twice is not zero and proportional to \u03b2i. Thus, G is considered a discrete distribution."}, {"heading": "2.2. DPP in a nonexhaustive framework", "text": "The suitability of the DPP model for nonexhaustive learning can be better conceived with the help of the conditional prior of \u03b8. Let\u2019s assume that at a certain time point the training set contains a sequence of n samples. The conditional prior of \u03b8n+1 conditioned on all past \u03b8i, i = {1, . . . , n} can be obtained by integrating out G in (1) which becomes,\n\u03b8n+1|\u03b81, . . . , \u03b8n \u223c \u03b1\n\u03b1+ n G0(\u00b7) +\n1\n\u03b1+ n n\u2211 i=1 \u03b4\u03b8i (2)\nThis conditional prior can be interpreted as a mixture of two distributions. Any sample that originates from this prior comes from the base distribution G0(\u00b7) with a probability of \u03b1\u03b1+n or uniformly generated from {\u03b81, . . . , \u03b8n} with a probability of n\u03b1+n . With a positive probability a sequence of n samples generated this way will not be all distinct. If we assume that there are k \u2264 n distinct values of \u03b8 in a sequence of size n, then (2) can be rewritten,\n\u03b8n+1|\u03b81, . . . , \u03b8n \u223c \u03b1\n\u03b1+ n G0(\u00b7) +\n1\n\u03b1+ n k\u2211 j=1 nj\u03b4\u03b8\u2217j (3)\nwhere \u03b8\u2217j , j = {1, . . . , k} are the distinct values of \u03b8i and nj are the number of occurrences of each \u03b8 \u2217 j in the sequence. Each \u03b8\u2217j defines a unique class with an indicator variable y\u2217j , whose samples are distributed according to the probability distribution p(\u00b7|\u03b8\u2217j ). Based on (3), after a sequence of n samples are generated, yn+1 = y \u2217 j with probability equal to nj \u03b1+n , and yn+1 = y\u2217k+1, with probability equal to \u03b1 \u03b1+n , where y \u2217 k+1 is the new class whose parameter is defined by \u03b8\u2217k+1 and sampled from G0(\u00b7).\nThis prior model can also be illustrated as a Chinese Restaurant process (CRP) (Aldous, 1985). The CRP uses a metaphor of a Chinese restaurant with infinitely many tables where the (n + 1)th customer sits at a previously occupied table j with a probability of\nnj \u03b1+n\nand at a new table k + 1 with a probability of \u03b1\u03b1+n . Here nj is the number of customers sitting at table j and n is the total number of customers.\nOur discussion so far has been limited to the prior model. Next, we will incorporate the data model and use the conditional posterior to determine whether a new sample xn+1 should be assigned to one of the existing classes or to a new class sampled from G0. More specifically, we are interested in the distribution p(\u03b8n+1|xn+1, \u03b81, . . . , \u03b8n), which is proportional to\np(\u03b8n+1|xn+1, \u03b81, . . . , \u03b8n) \u221d \u03b1\u03b1+np(xn+1)p(\u03b8n+1|xn+1) + 1\u03b1+n \u2211k j=1 njp(xn+1|\u03b8\u2217j )\u03b4\u03b8\u2217j\n(4)\nwhich indicates xn+1 either comes from a new class, y\u2217k+1, which inherits \u03b8 \u2217 k+1 sampled from p(\u03b8n+1|xn+1), with a probability proportional to \u03b1\u03b1+np(xn+1) or belongs to y\u2217j with a probability proportional to nj \u03b1+np(xn+1|\u03b8 \u2217 j ). Since \u03b8\u2217j are not known and has to be estimated using samples in the represented classes, p(xn+1|\u03b8\u2217j ) can be replaced with the class conditional predictive distribution p(xn+1|Dj) where Dj = {xi}i\u2208Cj denotes the subset of samples belonging to class y\u2217j defined by the index set Cj . Thus, provided that class membership information for all samples processed before xn+1 are known, the decision function to assign xn+1 to a new class or one of the existing ones can be expressed as,\nh (xn+1) =  yn+1 = y \u2217 j if nj\u2217 \u03b1+np (xn+1|Dj\u2217) > \u03b1 \u03b1+np (xn+1) yn+1 = y \u2217 k+1 if\nnj\u2217 \u03b1+np (xn+1|Dj\u2217) < \u03b1 \u03b1+np (xn+1)\n(5)\nwhere j\u2217 = argmaxj { nj \u03b1+np(xn+1|Dj) }k j=1 . However, in the nonexhaustive learning framework class mem-\nbership information is only available for samples initially present in the training dataset. For all samples processed online before xthn+1 sample the true class membership information is unknown."}, {"heading": "3. Inference with a nonexhaustive set of classes", "text": "Before we move on to discussing how inference can be performed in this framework, we introduce new notation to distinguish between the two types of samples available during online execution: samples initially available in the training dataset with known class membership information and samples observed online with no verified class membership information. Let X = {x1, . . . , x`} be the set of all training samples initially available, Y = {y1, . . . , y`} be the corresponding set of known class indicator variables with yi \u2208 {1, . . . , k}, k being the number of known classes, X\u0303n = {x\u03031, . . . , x\u0303n} be the set of n samples sequentially observed online, and Y\u0303 n = {y\u03031, . . . , y\u0303n} be the corresponding set of unknown class indicator variables with\ny\u0303i \u2208 { 1, . . . , k\u0303 + k } , k\u0303 being the number of unrepre-\nsented classes associated with these n samples."}, {"heading": "3.1. Inference by Gibbs Sampling", "text": "We are interested in predicting Y\u0303n+1, i.e., the class labels for all X\u0303n+1 at the time x\u0303n+1 is observed, which can be done by finding the mean of the posterior distribution p(Y\u0303 n+1|X\u0303n+1, X, Y ). Although this integral cannot be easily evaluated, the closed form solution for the conditional distributions of the latent variables y\u0303i can easily be obtained. Thus, Gibbs sampling with the sampler state consisting of variables y\u0303i, i = {1, . . . , n+ 1}, can be used to approximate p(Y\u0303 n+1|X\u0303n+1, X, Y ). One sweep of the Gibbs sampler will involve sampling from the following conditional distribution \u2200i.\np(y\u0303i|Y\u0303 (n+1)/i, X\u0303n+1, X, Y ) \u221d \u03b1\u03b1+n+`p(x\u0303i)\u03b4k\u0303+k+1 + 1\u03b1+n+` \u2211k+k\u0303 j=1 njp(x\u0303i|Dj)\u03b4j\n(6)\nwhere Y\u0303 (n+1)/i denotes Y\u0303 (n+1) without y\u0303i."}, {"heading": "3.2. Inference by Sequential Importance Resampling (SIR)", "text": "With the Gibbs sampler approach every time a new sample is observed the sampler has to run from start to predict whether the current sample belongs to one of the existing classes (labeled/unlabeled) or to a new class. This sampling scheme eventually becomes intractable as the number of unlabeled samples gradually\nincreases. We believe that this problem can be addressed to a greater extent by developing a sequential sampling approach based on Sequential Importance Resampling (SIR) (Doucet et al., 2000). In this approach, at any given time, the sampler only depends on a set of particles and their corresponding weights, which are efficiently updated in a sequential manner each time a new sample is observed without the need for the past samples.\nMore specifically, we are interested in evaluating the\nexpectation Ep(Y\u0303 n+1|Y\u0303 n,X\u0303n+1,Y,X)\n[ Y\u0303 n+1 ] . Using an\nimportance function q(Y\u0303 n+1|Y\u0303 n, X\u0303n+1, Y,X) the relevant integral can be approximated as follows.\nEp(Y\u0303 n+1|Y\u0303 n,X\u0303n+1,Y,X)\n[ Y\u0303 n+1 ] = \u222b Y\u0303 n+1p(Y\u0303 n+1|Y\u0303 n, X\u0303n+1, Y,X)\u2202Y\u0303 n+1\n= \u222b Y\u0303 n+1wn+1(Y\u0303\nn+1)q(Y\u0303 n+1|Y\u0303 n, X\u0303n+1, Y,X)\u2202Y\u0303 n+1 \u2248 \u2211M m=1 Y\u0303 n+1wn+1m (Y\u0303 n+1)\u03b4Y\u0303 n+1m\n(7) where M is the number of particles and wn+1m (Y\u0303\nn+1) = p(Y\u0303 n+1|Y\u0303 n,X\u0303n+1,Y,X) q(Y\u0303 n+1|Y\u0303 n,X\u0303n+1,Y,X) is the corresponding weight of the mth particle at the time (n+ 1)th sample is observed. Particles are sampled from the importance function q(Y\u0303 n+1|Y\u0303 n, X\u0303n+1, Y,X). The weights can be evaluated sequentially up to an unknown constant as outlined next.\nUsing the chain rule and after some manipulations a sequential update formula for the particle weights wmn+1(Y\u0303 n+1) can be derived as follows\nwn+1m (Y\u0303 n+1) = p(Y\u0303 n+1|Y\u0303 n,X\u0303n+1,Y,X)\nq(Y\u0303 n+1|Y\u0303 n,X\u0303n+1,Y,X)\n= wnm(Y\u0303 n) p(x\u0303n+1|Y\u0303 n+1,X\u0303n,Y,X)p(y\u0303n+1|Y\u0303 n,Y ) p(x\u0303n+1|Y\u0303 n,X\u0303n,Y,X)q(y\u0303n+1|Y\u0303 n,X\u0303n+1,Y,X)\n(8)\nAlthough, it is not optimal in terms of minimizing the variance, the common choice for q(y\u0303n+1|Y\u0303 n, X\u0303n+1, Y,X) = p(y\u0303n+1|Y\u0303 n, Y ) further simplifies the update formula by canceling out both terms in (8). After considering the fact that p(x\u0303n+1|Y\u0303 n, X\u0303n, Y,X) is constant with respect to Y\u0303 n+1, the sequential update formula for the particle weights become\nwn+1m (Y\u0303 n+1) = Cwnm(Y\u0303 n)p(x\u0303n+1|Y\u0303 n+1, X\u0303n, Y,X) (9)\nSince p(x\u0303n+1|Y\u0303 n+1, X\u0303n, Y,X) can be evaluated for x\u0303n+1 for any given particle, the weights at stage n+ 1 can be obtained up to an unknown constant C. Using normalized weights eliminates C and thus the discrete probability distribution in (7) can be fully evaluated and efficiently updated.\nEvery time a new sample is observed, first, a designated number of, i.e., R, new particles are resampled for each of the M particle using the importance function, then, weights are updated for the M \u2217 R particles, finally, downsampling, stratified on the particle weights, is performed to select M particles out of M \u2217 R ones. Resampling is critical to avoid the weight degeneracy problem mainly associated with Dirichlet process mixture models. To address the weight degeneracy problem a resampling strategy that ensures a well-distributed particle set was introduced in (Fearnhead & Clifford, 2003; Wood & Black, 2008). Using this strategy, instead of resampling R new particles for each existing particle from the importance function, k + k\u0303 + 1 particles are generated by considering all possible class labels an incoming sample can take for a given particle. Note that although k, i.e., number of labeled classes is constant across all particles, k\u0303, i.e., number of unlabeled classes, varies from one particle to other. Since all possible classes are considered in this approach, it is now essential to revise the weight update formula to include prior probability for each class."}, {"heading": "3.3. Estimating the precision parameter \u03b1", "text": "In the proposed framework \u03b1 is the parameter that controls the prior probability of assigning a new sample to a new cluster and thus, plays a critical role in the number of clusters generated. When the training samples are collected to reflect the true proportion of each class as well as the actual number of classes as in a training set with the same number of samples collected in real-time, the marginal distribution of the number of clusters p(k\u0303) can be maximized to obtain the maximum likelihood estimate of \u03b1. However, in many machine learning applications only the most prevalent classes are available in the training set and the training samples are almost never collected in real-time. Thus, p(k\u0303) may not model a training set collected offline very well.\nOne viable approach to predicting \u03b1 when training samples are not collected in real-time is to sample it from the distribution p(\u03b1|k\u0303, n) (Escobar & West, 1994). This approach although widely used in mixture density estimation involving batch data as part of a Gibbs sampler, it is not suitable for the proposed SIR algorithm, mainly because with SIR particles themselves are a function of \u03b1. Therefore, \u03b1 has to be fixed in order for the weight update formula to hold and thus, the SIR algorithm to work. In this study we encode our prior belief about the odds of encountering a new class by a prior probability value p that indicates the prior probability of a new sample coming from one of the labeled classes in the training set. Once a vague value for p is\nobtained for a given domain, \u03b1 can be estimated by empirical Bayes by sampling a large number of samples from a CRP for varying values of \u03b1 and then picking up the one that minimizes the difference between the empirical and true values of p."}, {"heading": "4. A Normally distributed data model", "text": "Both the Gibbs sampler and SIR requires the evaluation of the predictive p(x\u0303i|Dj) and the marginal p(x\u0303i) distributions. The predictive distribution for both labeled and unlabeled classes can be obtained by integrating out \u03b8. The marginal distribution can be obtained from p(x\u0303i|Dj) by setting Dj an empty set. In general the exact solution for the predictive and marginal distributions does not exist and approximations are needed. However, a closed-form solution does exist for a Normally distributed data model and a properly chosen base distribution as presented next. We give \u03c9j a Gaussian distribution with mean \u00b5j and covariance \u03a3j ; that is, \u03c9j \u223c N (\u00b5j ,\u03a3j). For the mean and covariance matrix, we use a joint conjugate prior G0:\nG0 = p (\u00b5,\u03a3) = N ( \u00b5|\u00b50, \u03a3\n\u03ba ) \ufe38 \ufe37\ufe37 \ufe38\np(\u00b5|\u03a3)\n\u00d7W\u22121 (\u03a3|\u03a30,m)\ufe38 \ufe37\ufe37 \ufe38 p(\u03a3)\n(10) where \u00b50 is the prior mean and \u03ba is a scaling constant that controls the deviation of the class conditional mean vectors from the prior mean. The smaller the \u03ba is, the larger the between class scattering will be. The parameter \u03a30 is a positive definite matrix that encodes our prior belief about the expected \u03a3. The parameter m is a scalar that is negatively correlated with the degrees of freedom. In other words the larger the m is the less \u03a3 will deviate from \u03a30 and vice versa.\nTo evaluate the update formula in (9) for SIR we need p(xn+1|Dj). To obtain p(xn+1|Dj) we need to integrate out \u03b8 = {\u00b5,\u03a3}. Since the sample mean x\u0304 and the sample covariance matrix S are sufficient statistics for the multivariate Normally distributed data, we can write p(\u00b5,\u03a3|Dj) = p(\u00b5,\u03a3|x\u0304j , Sj). The formula for this posterior and its derivation is widely available in books on multivariate statistics (Anderson, 2003). Once we integrate out p(xn+1, \u00b5,\u03a3|x\u0304j , Sj) first with respect to \u00b5 and then with respect to \u03a3 we obtain the predictive distribution in the form of a multivariate Student-t distribution.\nIn addition to p(xn+1|Dj) we also need p(xn+1) when evaluating the decision function in (5), which is also a multivariate Student-t distribution with Dj an empty set."}, {"heading": "4.1. Estimating the parameters of the prior model", "text": "The parameters (\u03a30,m, \u00b50, \u03ba) of the prior model can be estimated offline using samples from the well-defined classes. The maximum-likelihood estimates for \u03a30 and m do not exist. The study in (Greene & Rayens, 1989) suggests estimating \u03a30 by the unbiased and consistent estimate Sp, i.e., the pooled covariance, and maximizing the marginal likelihood of (nj \u2212 1)Sj for m > d + 1 numerically to estimate m. Here, Sp is the pooled covariance matrix defined by\nSp = (m\u2212 d\u2212 1)\n\u2211k j=1(nj \u2212 1)Sj\nn\u2212 k (11)\nwhere n is the total number of samples in the training set, i.e., n = \u2211k j=1 nj . The marginal distribution of (nj \u2212 1)Sj can be obtained by integrating out the joint distribution p((nj \u2212 1)Sj ,\u03a3j) = p((nj\u22121)Sj |\u03a3j)p(\u03a3j) with respect to \u03a3j . For a Normal data model p((nj \u2212 1)Sj |\u03a3j) is a Wishart distribution with a scale matrix \u03a3j and degrees of freedom nj \u2212 1, i.e., (nj \u2212 1)Sj |\u03a3j \u223c W (\u03a3j , nj \u2212 1) and p(\u03a3j) is an inverted Wishart distribution as defined in (10). The parameters \u03ba and \u00b50 can be estimated by maximizing the joint likelihood of x\u0304 and S, p(x\u0304, S), with respect to \u03ba and \u00b50, respectively."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. An illustrative example", "text": "We present an illustrative example demonstrating the proposed algorithm discovering and modeling classes with a 2-D simulated dataset. We generate twenty three classes where the class covariance matrix of each class is obtained from an inverted Wishart distribution with parameters \u03a8 = 10I and m = 20 and mean vectors are equidistantly placed alongside the peripheries of two circles with radius 4 and 8 creating a flower-shaped dataset. Here, I denotes the 2-D identity matrix. Three of the twenty three classes are randomly chosen as unrepresented. The nonexhaustive training data contains twenty classes with each class represented by 100 samples (a total of 2000 samples) whereas the exhaustive testing data contains twenty three classes with 100 samples from each (a total of 2300 samples). The objective here is to discover and model the three unrepresented classes while making sure samples of represented classes are classified as accurately as possible. Figure 1a shows true class distributions for all twenty three classes. The represented classes are shown by solid lines and unrepresented ones by dashed lines. The ellipses correspond to the distributions of the classes that are at most three standard deviations away from\nthe mean. The testing samples are classified sequentially using the SIR algorithm discussed in Section 3.2. The precision parameter \u03b1 and the number of particles M are chosen as 1 and 500, respectively. Figures 1b, 1c, and 1d demonstrate the online discovery and modeling of new classes when 100, 300, and all 2300 test samples are classified, respectively. The discovered classes are marked by solid blue lines. All three classes are discovered and their underlying distributions are successfully recovered by generating one cluster for each class."}, {"heading": "5.2. Bacteria detection", "text": "A total of 2054 samples from 28 classes each representing a different bacteria serovar were considered in this study. These are the type of serovars most commonly found in food samples. Each serovar is represented by between 40 to 100 samples where samples are the forward-scatter patterns characterizing the phenotype of a bacterial colony obtained by illuminating the colony surface by a laser light. Each scatter pattern is a gray level image characterized by a set of 50 features. More information about this dataset is available in (Akova et al., 2010). Samples are randomly splitted into two as train and test, with 70% of the samples going into the training set and the remaining 30% in the test. Stratified sampling is used to make sure each class is proportionately represented in both the training and the test sets. Four of the classes are considered unknown and all of their samples are moved from the training set to the test set. The nonexhaustive training set contains 24 classes whereas the exhaustive testing set contains 28 classes. Since the training samples are collected offline the number of samples initially available for labeled classes may not necessarily reflect true class proportions. To avoid introducing bias in favor of classes with larger numbers of samples we assumed that each labeled class is a priori likely by setting nj = 1.\nThe performance of the proposed SIR algorithm (NELSIR) discussed in Section 3.2 is evaluated on three fronts: classification accuracy for represented classes, classification accuracy for unrepresented classes, and the number of clusters discovered for each of the unrepresented classes. To compute the latter two values each unlabeled cluster is assigned to the unrepresented class having the majority of the samples in that class. Classification accuracy for each unrepresented class is computed by the ratio of the total number of samples recovered by the corresponding clusters to the total number of samples in that class. To see the effect of the execution order of the test samples on the overall results the experiment is repeated multiple times each time with a different ordering of test samples. Using the approach discussed in Section 3.3 the precision\nparameter \u03b1 is predicted as 10 for a p of 0.95, i.e., an incoming sample a priori belongs to one of the 24 labeled classes by a probability of 0.95. The number of particles M is chosen to be 2000.\nThe proposed NEL-SIR is compared against the BayesNoDe algorithm proposed in (Akova et al., 2010). Both the proposed NEL-SIR and Bayes-NoDe use the same data model, i.e., \u03c9j \u223c N (\u00b5j ,\u03a3j), (\u00b5,\u03a3) \u223c N ( \u00b5|\u00b50, \u03a3\u03ba ) \u00d7W\u22121 (\u03a3|\u03a30,m). However, as briefly mentioned in Section 1.3, there are significant differences between the two approaches in terms of prior modeling of class distributions and performing inference in a nonexhaustive setting. In addition to these two algorithms we also considered the exhaustive case, i.e., the setting under which all 28 classes are represented in the training set to serve as a benchmark for comparing our results. The results including the classification accuracies for represented as well as unrepresented classes and the number of clusters discovered for each unrepresented class are shown in Table 1.\nClassification accuracy achieved by the proposed NELSIR algorithm for represented classes is on par with that achieved by Bayes-NoDe. Although all four unrepresented classes are successfully discovered by both NEL-SIR and Bayes-Node, NEL-SIR tend to generate far less number of clusters in general and achieves significantly higher accuracy than BayesNoDe for each unrepresented class. The average number of clusters discovered for each unrepresented class is especially important for practical purposes because in a real-time biodetection system once new clusters are discovered as unknowns, their samples has to be analyzed offline to assess the pathogenic nature of these clusters. The less the number of clusters discovered for each unrepresented class the less time and resources offline analysis will require.\nThe perfect accuracies achieved for two of the unrepresented classes in the exhaustive setting indicate these classes are well separated from other classes. For the two well-separated classes NEL-SIR performs equally\nwell with the exhaustive case. These results indicate that if the unrepresented classes are well separated the proposed approach not only discover these classes and recover them by a reasonable number of clusters but also classify their samples at an accuracy comparable to an exhaustive classifier. If the unrepresented classes are not perfectly separated these classes can still be discovered but some loss in classifier accuracy as compared to an exhaustive classifier is inevitable."}, {"heading": "6. Conclusions", "text": "Online class discovery is an important problem that finds its place in many real-life applications involving evolving datasets. In this study, in an effort to discover emerging classes, a sequential inference algorithm based on particle filters was proposed for a Dirichlet process mixture model. In this approach the posterior distribution of the class indicator variables is approximated by a discrete distribution expressed by a set of particles and the corresponding weights. The particles and their weights are efficiently updated each time a new sample is observed. This way the posterior distribution is updated in a sequential manner without the need to have access to past samples enabling efficient online inference in a nonexhaustive setting. Our algorithm is validated using a 28-class bacteria with four of the classes considered unknown and promising results are obtained with respect to classification accuracy and class discovery.\nThe data model used in this study was limited with the Normal model. The proposed approach can be extended to problems involving more flexible class distributions by choosing a mixture model for each class data and a hierarchical DPP model over class distributions. Additionally, the model used in our studies does not explicitly model variation in class size as a function of time. Modeling time can be essential for modeling burstiness. We believe a time-dependent Dirichlet process can be useful toward achieving this end. Owing to the long tail behavior of DPP, with the current\napproach the probability of discovering a new class will converge to zero as n goes to infinity. Although we cannot verify whether Zipf\u2019s law holds for bacteria population we believe a Pitman-Yor process can offer more control over tail behavior of the prior model."}, {"heading": "Acknowledgments", "text": "The project was supported by Grant Number 5R21AI085531-02 from the National Institute of Allergy and Infectious Diseases (NIAID). The content is solely the responsibility of the authors and does not necessarily represent the official views of NIAID or the National Institutes of Health."}], "references": [{"title": "A machine-learning approach to detecting unknown bacterial serovars", "author": ["Akova", "Ferit", "Dundar", "Murat", "Davisson", "V. Jo", "Hirleman", "E. Daniel", "Bhunia", "Arun K", "Robinson", "J. Paul", "Rajwa", "Bartek"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "Akova et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Akova et al\\.", "year": 2010}, {"title": "Exchangeability and related topics", "author": ["Aldous", "David J"], "venue": "In E\u0301cole d\u2019E\u0301te\u0301 St Flour", "citeRegEx": "Aldous and J.,? \\Q1983\\E", "shortCiteRegEx": "Aldous and J.", "year": 1983}, {"title": "An Introduction to Multivariate Statistical Analysis, 3rd Edition", "author": ["Anderson", "Theodore W"], "venue": "WileyInterscience, 3rd edition,", "citeRegEx": "Anderson and W.,? \\Q2003\\E", "shortCiteRegEx": "Anderson and W.", "year": 2003}, {"title": "On sequential Monte Carlo sampling methods for Bayesian filtering", "author": ["Doucet", "Arnaud", "Godsill", "Simon", "Andrieu", "Christophe"], "venue": "Statistics and Computing,", "citeRegEx": "Doucet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2000}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["Escobar", "Michael D", "West", "Mike"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Escobar et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Escobar et al\\.", "year": 1994}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["Ferguson", "Thomas S"], "venue": "Annals of Statistics,", "citeRegEx": "Ferguson and S.,? \\Q1973\\E", "shortCiteRegEx": "Ferguson and S.", "year": 1973}, {"title": "Partially pooled covariance matrix estimation in discriminant analysis", "author": ["Greene", "Tom", "Rayens", "William"], "venue": "Commun. Statist. Theory Meth.,", "citeRegEx": "Greene et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Greene et al\\.", "year": 1989}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["Ishwaran", "Hemant", "James", "Lancelot F"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ishwaran et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ishwaran et al\\.", "year": 2001}, {"title": "A mixture model and embased algorithm for class discovery, robust classification, and outlier rejection in mixed labeled/unlabeled data sets", "author": ["D.J. Miller", "J. Browning"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Miller and Browning,? \\Q2003\\E", "shortCiteRegEx": "Miller and Browning", "year": 2003}, {"title": "A non-parametric Bayesian alternative to spike sorting", "author": ["F. Wood", "M.J. Black"], "venue": "Journal of Neuroscience Methods,", "citeRegEx": "Wood and Black,? \\Q2008\\E", "shortCiteRegEx": "Wood and Black", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Early work most similar to nonexhaustive learning includes the two studies reported in (Akova et al., 2010; Miller & Browning, 2003).", "startOffset": 87, "endOffset": 132}, {"referenceID": 0, "context": "In (Akova et al., 2010) a Bayesian approach based on the maximum likelihood detection of novelties using a dynamically updated class set was proposed.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "We believe that this problem can be addressed to a greater extent by developing a sequential sampling approach based on Sequential Importance Resampling (SIR) (Doucet et al., 2000).", "startOffset": 159, "endOffset": 180}, {"referenceID": 0, "context": "More information about this dataset is available in (Akova et al., 2010).", "startOffset": 52, "endOffset": 72}, {"referenceID": 0, "context": "The proposed NEL-SIR is compared against the BayesNoDe algorithm proposed in (Akova et al., 2010).", "startOffset": 77, "endOffset": 97}], "year": 2012, "abstractText": "We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.", "creator": "LaTeX with hyperref package"}}}