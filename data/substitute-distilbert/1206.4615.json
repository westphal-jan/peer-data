{"id": "1206.4615", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Levy Measure Decompositions for the Beta and Gamma Processes", "abstract": "we develop new representations for the levy measures involving the beta and gamma processes. these representations are manifested applying terms of an algebraic table of well - behaved ( proper ) beta matrix gamma distributions. further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize some errors. we also perform an analysis of the characteristics of posterior distributions, based on rigorous proposed decompositions. the decompositions provide deep insights when the beta and gamma processes ( and their generalizations ), and we demonstrate how the first representation unifies some equation containing the two. this paper is meant to provide a formal foundation for and new perspectives on levy processes, as these were of increasing importance in machine learning.", "histories": [["v1", "Mon, 18 Jun 2012 15:01:58 GMT  (238kb)", "http://arxiv.org/abs/1206.4615v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST stat.TH", "authors": ["yingjian wang", "lawrence carin"], "accepted": true, "id": "1206.4615"}, "pdf": {"name": "1206.4615.pdf", "metadata": {"source": "META", "title": "Le\u0301vy Measure Decompositions for the Beta and Gamma Processes", "authors": ["Yingjian Wang", "Lawrence Carin"], "emails": ["yw65@duke.edu", "lcarin@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "A prominent distinction of nonparametric methods relative to parametric approaches is the utilization of stochastic processes rather than probability distributions. For example, a Gaussian process (Rasmussen & Williams, 2006) may be employed to nonparametrically represent general smooth functions on a continuous space of covariates (e.g., time). Recently the idea of nonparametric methods has extended to feature learning and data clustering, with interest respectively in the beta-Bernoulli process (Thibaux & Jordan, 2007) and the Dirichlet process (Ferguson, 1973). In such processes the nonparametric aspect concerns the number of features/clusters, which are allowed to be unbounded (\u201cinfinite\u201d), permitting the model to adapt the number of these entities as the given and fu-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nture data indicate. The increasing importance of these models in machine learning warrants a detailed theoretical analysis of their properties, as well as simple constructions for their implementation. In this paper we focus on Le\u0301vy processes (Sato, 1999), which are of increasing interest in machine learning.\nA family of Le\u0301vy processes, the pure-jump nondecreasing Le\u0301vy processes, also fit into the category of the completely random measure proposed by Kingman (Kingman, 1967). The beta process (Hjort, 1990) is an example of such a process, which is applied in nonparametric feature learning. The gamma process falls in this family as well, with its normalization the well-known Dirichlet process. Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al., 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).\nAs a consequence of the important role these models are playing in machine learning, there is a need for the study of the properties of pure-jump nondecreasing Le\u0301vy processes. As examples of such work, (Thibaux & Jordan, 2007) and (Paisley et al., 2010) present explicit constructions for generating the beta process, (Teh et al., 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context. Apart from these specialized construction methods, in (Kingman, 1967) a general construction method for completely random measures is proposed, by first decomposing it into a sum of a countable number of \u03c3-finite measures, and then superposing the Poisson processes according to these sub-measures. By regarding the completely random measure as a Le\u0301vy process, this method corresponds to decomposing the Le\u0301vy measure, which provides clarity of theoretical properties and simplicity in practical implementation. However this Le\u0301vy measure\ndecomposition method has not yet come into wide use in machine learning and statistics, probably due to the nonexistence of a universal construction of the measure decomposition.\nIn this paper we develop explicit and simple decompositions by following the conjugacy principle for two widely used Le\u0301vy processes, the beta and gamma processes. The conjugacy means that the decompositions are manifested by leveraging the forms of conjugate likelihoods to the Le\u0301vy measures. The decompositions bring new perspectives on the beta and gamma processes, with associated properties analyzed here in detail. The decompositions are constituted in terms of an infinite set of sub-processes of form convenient for computation. Since the number of sub-processes is infinite, a truncation analysis is also presented, of interest for practical use. We show some posterior properties of such decompositions, with the beta process as an example. We also extend the decomposition to the symmetric gamma process (positive and negative jumps), suggesting that the Le\u0301vy measure decomposition is applicable for other pure-jump Le\u0301vy processes represented by their Le\u0301vy measures. Summarizing the main contributions of the paper:\n\u2022 We constitute Le\u0301vy measure decompositions for the beta, stable-beta, gamma, generalized gamma and symmetric gamma processes via the principle of conjugacy, providing new perspectives on these processes.\n\u2022 The decomposition of the beta process unifies the constructions in (Thibaux & Jordan, 2007), (Teh & Go\u0308ru\u0308r, 2009), and (with a different decomposing method) (Paisley et al., 2010), and a new generative construction for the gamma process and its variations is derived.\n\u2022 Truncation analyses and posterior properties for such decompositions are presented for practical use."}, {"heading": "2. Background", "text": "Le\u0301vy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts. Specifically, some Le\u0301vy processes can be regarded as completely random measures. In this section brief reviews and connections are presented for these two important concepts."}, {"heading": "2.1. Le\u0301vy process", "text": "A Le\u0301vy process X(\u03c9) is a stochastic process with independent increments on a measure space (\u2126,F). \u2126\nis usually taken to be one-dimensional, such as the real line, to represent a stochastic process with variation over time. By the Le\u0301vy-Ito\u0302 decomposition (Sato, 1999), a Le\u0301vy process can be decomposed into a continuous Brownian motion with drift, and a discrete part of a pure-jump process. When a Le\u0301vy process X(\u03c9) only has the discrete part and its jumps are positive, then for \u2200A \u2208 F the characteristic function of the random variable X(A) is given by:\nE{ejuX(A)} = exp{ \u222b\nR+\u00d7A (ejup \u2212 1)\u03bd(dp, d\u03c9)} (1)\nwith \u03bd satisfying the integrability condition (Sato, 1999). The expression in (1) defines a category of pure-jump nondecreasing Le\u0301vy processes, including most of the Le\u0301vy processes currently used in nonparametric Bayesian methods, such as the beta, gamma, Bernoulli, and negative binomial processes. With (1), such a Le\u0301vy process can be regarded as a Poisson point process on the product space R+ \u00d7 \u2126 with the mean measure \u03bd, called the Le\u0301vy measure. On the other hand, if the increments of X(\u03c9) on any measurable set A \u2208 F are regarded as a random measure assigned on the set, then X(\u03c9) is also a completely random measure. Due to this equivalence, in the following discussion we will not discriminate the pure-jump nondecreasing Le\u0301vy process X with its corresponding completely random measure \u03a6."}, {"heading": "2.2. Completely random measure", "text": "A random measure \u03a6 on a measure space (\u2126,F) is termed \u201ccompletely random\u201d if for any disjoint sets A1,A2 \u2208 F the random variables \u03a6(A1) and \u03a6(A2) are independent. A completely random measure \u03a6 can be split into three independent components:\n\u03a6 = \u03a6f +\u03a6d +\u03a6o (2) where \u03a6f = \u2211\n\u03c9\u2208I \u03c6(\u03c9)\u03b4\u03c9 is the fixed component, with the atoms in I fixed and the jump \u03c6(\u03c9) random; I is a countable set in F . The deterministic component \u03a6d is a deterministic measure on (\u2126,F). \u03a6f and \u03a6d are relatively less interesting compared to the third component \u03a6o, which is called the ordinary component of \u03a6. According to (Kingman, 1967), \u03a6o is discrete with both random atoms and jumps.\nIn (Kingman, 1967), it is noted that \u03a6o can be further split into a countable number of independent parts:\n\u03a6o = \u2211 k \u03a6k, \u03a6k = \u2211\n(\u03c6(\u03c9),\u03c9)\u2208\u03a0k\n\u03c6(\u03c9)\u03b4\u03c9 (3)\nDenote \u03bd as the Le\u0301vy measure of (the Le\u0301vy process corresponding to) \u03a6o, \u03bdk as the Le\u0301vy measure of \u03a6k,\n\u03a0 a Poisson process with \u03bd its mean measure, and \u03a0k a Poisson process with \u03bdk its mean measure; (3) further yields:\n\u03bd = \u2211 k \u03bdk, \u03a0 = \u22c3 k \u03a0k (4)\nwhich provides a constructive method for \u03a6o: first construct the Poisson process \u03a0k underlying \u03a6k, and then with the superposition theorem (Kingman, 1993) the union of \u03a0k will be a realization of \u03a6o. In the following sections we show how this general construction method of (4) can be applied on pure-jump nondecreasing Le\u0301vy processes of increasing interest in machine learning, with an emphasis on the beta and gamma processes, and their generalizations."}, {"heading": "3. Beta process", "text": "A beta process (Hjort, 1990) is a Le\u0301vy process with beta-distributed increments; B \u223c BP(c(\u03c9), \u00b5) is a beta process if\nB(d\u03c9) \u223c Beta(c(\u03c9)\u00b5(d\u03c9), c(\u03c9)(1\u2212 \u00b5(d\u03c9))) (5)\nwhere \u00b5 is the base measure on measure space (\u2126,F) and a positive function c(\u03c9) the concentration function. Expression (5) indicates that the increments of the beta process are independent, which makes it a special case of the Le\u0301vy process family. The Le\u0301vy measure of the beta process is\n\u03bd(d\u03c0, d\u03c9) = c(\u03c9)\u03c0\u22121(1\u2212 \u03c0)c(\u03c9)\u22121d\u03c0\u00b5(d\u03c9) (6)\nwhere Beta(0, c(\u03c9)) = c(\u03c9)\u03c0\u22121(1 \u2212 \u03c0)c(\u03c9)\u22121 is an improper beta distribution since its integral over (0, 1) is infinite. As a result, its underlying Poisson process, i.e., the Poisson process with \u03bd as its mean measure on the product space \u2126 \u00d7 (0, 1), denoted \u03a0, has an infinite number of points drawn from \u03bd, yielding\nB = \u221e\u2211 i=1 \u03c0i\u03b4\u03c9i (7)\nwhere \u03c0i is the jump (increment) which happens at the atom \u03c9i. Real variable \u03b3 = \u00b5(\u2126) is termed the mass parameter of B, and we assume \u03b3 <\u221e."}, {"heading": "3.1. Beta process Le\u0301vy measure decomposition", "text": "The infinite integral of the improper beta distribution inspires a decomposition of the improper distribution with an infinite number of proper distributions. The singularity in the improper beta distribution is manifested from \u03c0\u22121. Since \u03c0 \u2208 (0, 1), the geometric series expansion yields\n\u03c0\u22121 = \u221e\u2211 k=0 (1\u2212 \u03c0)k, \u03c0 \u2208 (0, 1) (8)\nand substituting (8) in (6), with manipulation detailed in the Supplementary Material, we have the Le\u0301vy measure decomposition theorem of the beta process:\nTheorem 1 For a beta process B \u223c BP(c(\u03c9), \u00b5) with base measure \u00b5 and concentration c(\u03c9), denote \u03a0 as its underlying Poisson process and \u03bd the Le\u0301vy measure, then B and \u03a0 can be expressed as\n\u03a0 = \u221e\u22c3 k=0 \u03a0k , B = \u221e\u2211 k=0 Bk (9)\nwhere Bk is a Le\u0301vy process with \u03a0k its underlying Poisson process. The Le\u0301vy measure \u03bdk of Bk is a decomposition of \u03bd:\n\u03bd = \u221e\u2211 k=0 \u03bdk\n\u03bdk(d\u03c0, d\u03c9) = Beta(1, c(\u03c9) + k)d\u03c0\u00b5k(d\u03c9)\n\u00b5k(d\u03c9) = c(\u03c9)\nc(\u03c9) + k \u00b5(d\u03c9)\n(10)\nwhere Beta(1, c(\u03c9)+k) is the PDF of beta distribution with parameters 1 and c(\u03c9) + k.\nTheorem 1 is the beta process instantiation of the completely random measure decomposing in (4), which indicates that the underlying Poisson process \u03a0 of the beta process B is the superposition of an infinite number of independent Poisson processes {\u03a0k}\u221ek=0, with \u03bdk the mean measure of \u03a0k and \u00b5k the mean measure of the restriction of \u03a0k on \u2126. As a result, the beta process B can be expressed as a sum of an infinite number of independent Le\u0301vy processes {Bk}\u221ek=0 with {\u03a0k}\u221ek=0 the underlying Poisson process. The independence of {\u03a0k}\u221ek=0 and {Bk}\u221ek=0 w.r.t. index k is justified by the fact that both \u00b5 and c(\u03c9) are fixed parameters."}, {"heading": "3.2. The Le\u0301vy process Bk", "text": "It is interesting to study the properties of Bk, such as the expectation and variance. Denoting Bk(d\u03c9) =\n1 c(\u03c9)+k+1\u00b5k(d\u03c9) as the base measure of Bk, for \u2200A \u2208 F :\nE(Bk(A)) = \u222b A Bk(d\u03c9) = Bk(A)\nVar(Bk(A)) = \u222b A 2 c(\u03c9) + k + 2 Bk(d\u03c9) (11)\nIt is noteworthy that the Le\u0301vy process Bk is no longer a beta process, since (5) is not satisfied. By Theorem 1, the jumps of Bk follow a proper beta distribution parameterized by the concentration function c(\u03c9) and the index k, and \u00b5k determines the locations where the jumps happen. Since {Bk}\u221ek=0 are independent w.r.t.\nthe index k, with Theorem 1: \u221e\u2211 k=0 E(Bk(A)) = E(B(A))\n\u221e\u2211 k=0 Var(Bk(A)) = Var(B(A)) (12)\nThe detailed procedure to derive (11) and (12) is given in the Supplementary Material."}, {"heading": "3.3. Simulating the beta process", "text": ""}, {"heading": "3.3.1. Poisson superposition simulation", "text": "Theorem 1 reveals that the underlying Poisson process of a beta process is a superposition of an infinite number of Poisson processes, each of which has a finite set of atoms. This perspective also provides a simulation procedure for the beta process: first, the Poisson process \u03a0k is sampled for all k = 0, 1, 2, \u00b7 \u00b7 \u00b7 , (here we term the index k as the \u201cround\u201d of the simulation); then take the union of the samples of each \u03a0k as a realization of the Poisson process \u03a0. With the marking theorem (Kingman, 1993) implicitly applied, the simulation procedure of the beta process is as follows:\nSimulation procedure: For round k:\n1: Sample the number of points for \u03a0k: nk \u223c Poisson( \u222b \u2126 \u00b5k(d\u03c9)); 2: Sample nk points from \u00b5k: \u03c9ki i.i.d.\u223c \u00b5k\u222b\n\u2126 \u00b5k(d\u03c9) , for\ni = 1, 2, \u00b7 \u00b7 \u00b7 , nk; 3: Sample Bk(\u03c9ki)\ni.i.d.\u223c Beta(1, c(\u03c9ki) + k), for i = 1, 2, \u00b7 \u00b7 \u00b7 , nk;\nThen the union \u22c3\u221e k=0{(\u03c9ki, Bk(\u03c9ki)} nk i=1 is a realization of \u03a0 (and equivalently of B).\nWe refer to the above simulation procedure as the Poisson superposition simulation, for the central role of the Poisson superposition. The especially convenient case is when the beta process is homogeneous, i.e., c(\u03c9) = c is a constant. In this case {\u03c9ki}nki=1 for all rounds k are drawn from the same distribution \u00b5/\u03b3; and nk is drawn from Poisson( c\u03b3c+k ). For round k, both the number of points and the jumps statistically diminish as k increases, suggesting that the infinite sum in (9) may be truncated as B = \u2211K k=0Bk for large K, with minimal impact. Such truncation effects are investigated in detail in Section 3.4."}, {"heading": "3.3.2. Related work", "text": "In (Thibaux & Jordan, 2007) the authors derived the above simulation procedure for the homogeneous case\nwithin the beta-Bernoulli process context, which is shown here a necessary result of the Le\u0301vy measure decomposition. The same decomposing manipulation of Theorem 1 can be also applied to the stable beta process (Teh & Go\u0308ru\u0308r, 2009) which yields:\n\u03bdk =Beta(1\u2212 \u03c3, c(\u03c9) + \u03c3 + k)d\u03c0\n\u00b7 \u0393(c(\u03c9) + \u03c3 + k)\u0393(c(\u03c9) + 1) \u0393(c(\u03c9) + k + 1)\u0393(c(\u03c9) + \u03c3) \u00b5(d\u03c9) (13)\nIt is noteworthy that the decomposition procedure described in Theorem 1 is not the only Le\u0301vy measure decomposing method for the beta process. The work of (Paisley & Jordan, 2012) and (Broderick et al., 2011) show that the stick-breaking construction of the beta process in (Paisley et al., 2010) is indeed a result of another way of decomposing the Le\u0301vy measure of the beta process. We next analyze the truncation property of the construction described in Section 3.3.1 and make comparison with the construction of beta process in (Paisley et al., 2010)."}, {"heading": "3.4. Truncation analysis", "text": "Since the Poisson superposition simulation operates in rounds, it is natural to analyze the distance between the true beta process B and its truncation \u2211K k=0Bk, with truncation at round K. A metric for such distance is the L1 norm:\n||B\u2212 K\u2211 k=0 Bk||1 = E|B\u2212 K\u2211 k=0 Bk| = \u222b \u2126 \u00b5K+1(d\u03c9) \u03b3 (14)\nThe expectation in (14) is w.r.t. the normalized measure \u03bd/\u03b3, which yields \u2016B\u20161 = 1. When B is homogeneous, (14) reduces to cc+K+1 , which indicates that the L1 distance decreases at a rate of O( 1K ). For the stick-breaking construction of beta process described in (Paisley et al., 2010), the L1 distance is: ( cc+1 ) K+1.\nAnother metric is the L1 distance between the marginal likelihood of a set of data b = b1:M , with m\u221e(b) denotes the marginal likelihood (here the likelihood is a Bernoulli process) with prior B, andmK(b) for \u2211K\nk=0Bk. This metric was applied on the truncated Indian buffet process (Doshi et al., 2009) and truncated stick-breaking construction of the beta process (Paisley & Jordan, 2012), which indicates\n1 4\n\u222b |m\u221e(b)\u2212mK(b)|db \u2264\nPr(\u2203k > K, 1 \u2264 i \u2264 nk, 1 \u2264 m \u2264M, s.t. bmki = 1) (15)\nwhere b1:M i.i.d.\u223c BeP(B) are drawn from a Bernoulli process with base measure B; bmki = bm(\u03c9ki) is the\nmth realization of the Bernoulli process at atom \u03c9ki. For the truncation \u2211K k=0Bk it can be shown that the RHS of (15) is bounded by:\nRHS of (15) \u2264 1\u2212 exp(\u2212M \u222b \u2126 \u00b5K+1(d\u03c9)) (16)\nFor the homogeneous case, the bound of (16) is 1 \u2212 exp(\u2212M\u03b3 cc+K+1 ). For the stick-breaking construction of beta process, the bound is given by: 1\u2212 exp(\u2212M\u03b3( cc+1 ) K+1) (Paisley & Jordan, 2012).\nIn order to analyze the bound w.r.t. the truncation level by number of atoms, denote IK = \u2211K k=0 nk\nas the total number of atoms in \u2211K\nk=0Bk. Since\nK \u223c O(e E(IK ) c\u03b3 ), it is proved that (14) and the bound in (16) decreases at a faster rate w.r.t. I than the stick-breaking construction of beta process. This indicates that the simulation procedure described in Section 3.3.1 follows a steeper statistically-decreasing order. The proof is presented in the Supplementary Material."}, {"heading": "3.5. Posterior estimation", "text": "The goal of the inference is to estimate the beta process B from a set of observed data b with prior BP(c, \u00b5). The data b = b1:M is the same as in Section 3.4, which can be expressed as:\nbm = \u221e\u2211 i=1 bi,m\u03b4\u03c9i , m = 1, 2, \u00b7 \u00b7 \u00b7 ,M (17)\nwhere each bi,m \u2208 {0, 1}."}, {"heading": "3.5.1. Posterior of Bk", "text": "Since B|b \u223c BP(c+M, c\u00b5c+M + \u2211M m=1 bm c+M ) (Thibaux & Jordan, 2007), the base measure of B|b is a measure with positive masses assigned on single atoms. Theorem 1 is still applicable to this beta process with mixed type of base measure, which yields\nB\u2032 = \u221e\u2211 k=0 B\u2032k \u03bd\u2032k = Beta(1, c+M + k)\u00b5 \u2032 k\n\u00b5\u2032k = c\u00b5 c+M + k + \u2211M m=1 bm c+M + k\n(18)\nwhere the B\u2032, B\u2032k, \u03bd \u2032 k, and \u00b5 \u2032 k are the posterior counterparts of B, Bk, \u03bdk, and \u00b5k."}, {"heading": "3.5.2. Posterior estimation of \u03c0i:", "text": "Since each \u00b5k has a mass \u2211M m=1 bi,m c+M+k at the atom \u03c9i,\neach Bk will contribute Poisson( \u2211M m=1 bi,m c+M+k ) draws with\nthe jumps following the distribution Beta(1, c +M + k) at the atom \u03c9i, whose sum is the \u03c0i. Thus the posterior estimation of \u03c0i is given by\n\u03c0i|b = \u221e\u2211 k=0 Hk\u2211 h=1 bkh\nHk \u223c Poisson( \u2211M\nm=1 bi,m c+M + k )\nbkh \u223c Beta(1, c+M + k)\n(19)\nfrom which it can be verified that E(\u03c0i|b) = \u2211M m=1 bi,m c+M , the same as the posterior of \u03c0i without decomposition: Beta( \u2211M m=1 bi,m, c+M \u2212 \u2211M m=1 bi,m).\nFor the \u03c0i with no observations, i.e., \u2211M\nm=1 bi,m = 0, only a particular Bk will contribute to \u03c0i. In this case, first the round k to which \u03c0i belongs is drawn, then \u03c0i is drawn from the beta distribution of that round:\n\u03c0i \u223c Beta(1, c+M + k)\nk \u223c MP(\u03b1), \u03b1 \u221d \u221e\u2211 k=0 1 c+M + k \u03b4k (20)\nwhere MP(\u03b1) is a multinomial process with probability vector \u03b1, and \u03b1 is proportional to the average number of points in each round. Since in practical processing \u03b1 is always to be truncated with a truncation level K, by the analysis in Section 3.4, (20) provides a way to estimate the \u03c0i within the first K rounds. And \u03c0i in each round are of statistically different importance, contrasted to the evenly assigned mass in the Indian buffet process."}, {"heading": "3.6. Relating the IBP and beta process", "text": "The study of the beta process through its Le\u0301vy measure, as discussed in this paper, also uncovers a connection between the Indian buffet process (IBP) (Griffiths & Ghahramani, 2005) and the beta process, by their Le\u0301vy measures. The IBP with prior \u03c0i \u223c Beta(c \u03b3N , c) can be regarded as a Le\u0301vy process with the Le\u0301vy measure given as:\n\u03bdIBP = N\n\u03b3 Beta(c\n\u03b3 N , c)d\u03c0\u00b5(d\u03c9) (21)\nhere N is the same as the K in (Griffiths & Ghahramani, 2005). It can be proved that:\n\u03bdIBP N\u2192\u221e= \u03bd (22)\nwhich indicates that the beta process is the limit of the IBP with N \u2192 \u221e. The detailed proof of (22) is presented in the Supplementary Material. Thus the IBP is like a \u201cmosaic\u201d approximation of beta process, which becomes finer with N increases."}, {"heading": "4. Gamma process", "text": "A gamma process (Applebaum, 2009) is a Le\u0301vy process with independent gamma increments. The gamma process is traditionally parameterized with a shape measure and a scale function: G \u223c \u0393P(\u03b1, \u03b8(\u03c9)) where \u03b1 is the shape measure on a measure space (\u2126,F), and the scale \u03b8(\u03c9) a positive function. A gamma process can be intuitively defined by its increments on infinitesimal sets:\nG(d\u03c9) \u223c Gamma(\u03b1(d\u03c9), \u03b8(\u03c9)) (23)\nWhen \u03b8(\u03c9) = \u03b8 is a scalar, the gamma process is called homogeneous. The gamma process can also be expressed in the form with a base measure G0 and a concentration c(\u03c9), with c = 1/\u03b8 and G0 = \u03b8\u03b1 (Jordan., 2009), to conform with other stochastic processes widely used in machine learning, such as the Dirichlet process. However, the discussion in this paper will stick to the traditional form given by (23).\nAs a pure-jump Le\u0301vy process, the gamma process can be regarded as a Poisson process on the product space \u2126\u00d7 R+ with mean measure \u03bd:\n\u03bd(dp, d\u03c9) = p\u22121e\u2212 p \u03b8(\u03c9) dp\u03b1(d\u03c9) (24)\nwhere Gamma(0, \u03b8(\u03c9)) = p\u22121e\u2212 p\n\u03b8(\u03c9) is an improper gamma distribution with an infinite integral on R+, which yields the expression of G:\nG = \u221e\u2211 i=1 pi\u03b4\u03c9i (25)"}, {"heading": "4.1. Le\u0301vy measure decomposition", "text": "Like the beta process, the Le\u0301vy measure of the gamma process is characterized by an improper distribution. However, unlike the beta process, the decomposition of the Le\u0301vy measure of the gamma process comes from the exponential part. With the details shown in the Supplementary Material, the gamma process G can be decomposed into two parts:\nG = \u03931 + \u0393P(\u03b1, \u03b8(\u03c9)/2) (26)\nThe second term in (26) is a gamma process with the same shape measure, and half the scale of the gamma process G; the first term \u03931 is a Le\u0301vy process with the Le\u0301vy measure \u2211\u221e h=1Gamma(h, \u03b8(\u03c9) 2 )dp \u03b1(d\u03c9) 2hh\n. Here Gamma(h, \u03b8(\u03c9)2 ) is the PDF of the gamma distribution, with shape parameter h and scale parameter \u03b8(\u03c9) 2 .\nFurther decomposing the exponential part of the gamma process \u0393P(\u03b1, \u03b8(\u03c9)/2) in (26) yields G =\n\u03931+\u03932+\u0393P(\u03b1, \u03b8(\u03c9)/3), bearing a gamma process with the same shape and with the scale parameter further decreased. Repeating this manipulation, we obtain the Theorem 2:\nTheorem 2 A gamma process G \u223c \u0393P(\u03b1, \u03b8(\u03c9)) with shape measure \u03b1 and scale \u03b8(\u03c9) can be decomposed as:\nG = \u221e\u2211 k=1 \u0393k, \u0393k = \u221e\u2211 h=1 \u0393kh, \u03bdk = \u221e\u2211 h=1 \u03bdkh \u03bdkh = Gamma(h, \u03b8(\u03c9) k + 1 )dp \u03b1(d\u03c9) (k + 1)hh\n(27)\nwith \u0393k, \u0393kh Le\u0301vy processes with \u03bdk, \u03bdkh their Le\u0301vy measures.\nTheorem 2 is the gamma process instantiation of (4), which indicates that G can be expressed as the sum of an infinite number of Le\u0301vy processes \u0393k, k = 1, 2, \u00b7 \u00b7 \u00b7 , where \u0393k is also the sum of an infinite number of Le\u0301vy processes \u0393kh, h = 1, 2, \u00b7 \u00b7 \u00b7 ."}, {"heading": "4.2. Le\u0301vy processes \u0393k and \u0393kh", "text": "In order to obtain further insights into the gamma process G in Theorem 2, the expectations and variances of \u0393k and \u0393kh on any measurable set A \u2208 F are given:\nE(\u0393kh(A)) = \u222b A \u03b8(\u03c9)\u03b1(d\u03c9) (k + 1)h+1\nE(\u0393k(A)) = \u222b A \u03b8(\u03c9)\u03b1(d\u03c9) k(k + 1)\n(28)\nFor the variances of \u0393k and \u0393kh:\nVar(\u0393kh(A)) = (h+ 1)\n(k + 1)h+2 \u222b A \u03b82(\u03c9)\u03b1(d\u03c9)\nVar(\u0393k(A)) = [ 1 k2 \u2212 1 (k + 1)2 ] \u222b A \u03b82(\u03c9)\u03b1(d\u03c9) (29)\nSince the Le\u0301vy processes \u0393k are independent w.r.t. k, with analogy to (12) it can be verified that the expectation and variance of \u0393k sum to the expectation of variance of G. The derivations in this section are presented in the Supplementary Material."}, {"heading": "4.3. Simulation of gamma process", "text": "Parallel to the simulation of beta process in Section 3.3.1, a simulation procedure of the gamma process is presented:\nSimulation procedure: Sample the Le\u0301vy process \u0393kh:\n1: Sample the number of points for \u0393kh: nkh \u223c Poisson(\u03b3/(k + 1)hh); 2: Sample nkh points from \u03b1: \u03c9khi i.i.d.\u223c \u03b1\u03b3 , for i =\n1, 2, \u00b7 \u00b7 \u00b7 , nkh; 3: Sample \u0393kh(\u03c9khi)\ni.i.d.\u223c Gamma(h, \u03b8(\u03c9khi)k+1 ), for i = 1, 2, \u00b7 \u00b7 \u00b7 , nkh;\nwhere \u03b3 = \u222b \u2126 \u03b1(d\u03c9) is the mass of the shape mea-\nsure. Then the union \u22c3\u221e k=1 \u22c3\u221e h=1(\u03c9khi,\u0393kh(\u03c9khi)) nkh i=1 is a realization of the gamma process G. An advantage of the above simulation procedure compared to the simulation procedure of the beta process in Section 3.3.1 is that independent of whether the gamma process is homogeneous or inhomogeneous, \u03c9khi is always drawn from a fixed distribution \u03b1/\u03b3. Like with the beta process construction in Section 3.3.1, for the gamma process simulation procedure, as k increases the expected number of new points and the expected jumps decrease, again suggesting accurate truncation."}, {"heading": "4.4. Truncation analysis", "text": "Since in the simulation procedure in Section 4.3 the index k and h both go to infinity, it is practical to analyze the distance between the true gamma process and the truncated one. To measure such a distance, we apply the L1 norm described in Section 3.4:\n||G\u2212 K\u2211 k=1 H\u2211 h=1 \u0393kh||1 = E|G\u2212 K\u2211 k=1 H\u2211 h=1 \u0393kh| (30)\nwhere the expectation in (30) is w.r.t. the normalized measure \u03bd/ \u222b \u2126 \u03b8(\u03c9)\u03b1(d\u03c9) with ||G||1 = 1; and K and H are the truncation level of k and h. Then for the situation with H =\u221e:\n\u2016G\u2212 K\u2211 k=1 \u221e\u2211 h=1 \u0393kh\u20161 = 1 K + 1 (31)\nwhich indicates a O( 1K ) decreasing rate as same as the truncated beta process shown in (14). It is noteworthy that \u03931 alone accounts for on average half the mass of G. When H is finite, a remaining distance\u2211K\nk=1 1 k(k+1)H+1 is added."}, {"heading": "4.5. Generalized gamma process and symmetric gamma process", "text": "Theorem 2 can be easily extended to some variations of the gamma process. Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (C\u0327inlar, 2010).\nThe generalized gamma process extends the ordinary gamma process by adding a parameter 0 < \u03c3 < 1,\nwhose Le\u0301vy measure is 1\u0393(1\u2212\u03c3)p \u2212\u03c3\u22121e\u2212 p \u03b8(\u03c9) dp\u03b1(d\u03c9). Then with the same decomposition procedure, it is straightforward that the Le\u0301vy measure for \u0393kh of the generalized gamma process will change to \u03bdkh = Gamma(h\u2212 \u03c3, \u03b8(\u03c9)k+1 )dp \u03b1(d\u03c9) \u0393(1\u2212\u03c3)(k+1)hh .\nThe symmetric gamma process is a Le\u0301vy process whose increments are the differences of two gammadistributed variables with the same law, whose Le\u0301vy measure is |p|\u22121e\u2212 |p| \u03b8(\u03c9) dp\u03b1(d\u03c9). Since there can be negative increments, the symmetric gamma process is not a completely random measure. However, the same decomposition procedure is still applicable, yielding \u03bdkh = Gamma(|p|\n\u2223\u2223h, \u03b8(\u03c9)k+1 )dp 2\u03b1(d\u03c9)(k+1)hh , where the distribution Gamma(|p|\n\u2223\u2223h, \u03b8(\u03c9)k+1 ) is to first draw |p| from Gamma(h, \u03b8(\u03c9)k+1 ), then decide the sign of p through a symmetric Bernoulli distribution."}, {"heading": "5. Conclusions", "text": "The Le\u0301vy measure decomposition of the beta and gamma processes provides new perspectives on the two widely used stochastic processes, by casting insights on the sub-processes constituting them, here the Bk and \u0393k. And the decomposition prescriptions described here are far from the only ways of such decomposition. Theoretically elegant construction methods are derived from the proposed decompositions, which are directly implementable in practice.\nWe have applied the proposed beta and gamma representations in numerical experiments, the details of which are omitted, as this paper focuses on foundational properties. However, to briefly summarize experience with such representations, consider for example the image inpainting problem considered in (Zhou et al., 2009), based upon a beta process factor analysis model (Paisley & Carin, 2009). In experiments we performed with such a model, using a Gibbs sampler, the beta process prior was implemented using the procedure discussed in Section 3.3.1, with the posterior estimation in Section 3.5 applied for inference. The proposed representation infers a dictionary with the \u201cimportant\u201d dictionary elements captured by the lowindex members (see the discussion in Section 3.3.1). The model prioritized the first three dictionary elements as being pure colors, specifically red, green, and blue, with the important structured dictionary elements following (and no other pure-color dictionary elements, while in (Zhou et al., 2009) many \u2013 seemingly redundant \u2013 pure-color dictionary elements are inferred). This \u201cclean\u201d inference of prioritized dictionary elements may be responsible for our also higher observed PSNR in signal recovery, compared to the re-\nsult given in (Zhou et al., 2009). The new gamma process construction in Section 4.3 may be implemented in a similar manner, and may be employed within recent models in machine learning in which the gamma process has been utilized (e.g., (Paisley et al., 2011))."}, {"heading": "Acknowledgements", "text": "The research reported here was supported by ARO, NGA, ONR and DARPA (MSEE program)."}], "references": [{"title": "Processes and Stochastic Calculus", "author": ["Applebaum", "D. Levy"], "venue": null, "citeRegEx": "Applebaum and Levy,? \\Q2009\\E", "shortCiteRegEx": "Applebaum and Levy", "year": 2009}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "J. ACM,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Generalized gamma measures and shot-noise Cox processes", "author": ["A. Brix"], "venue": "Advances in Applied Probability,", "citeRegEx": "Brix,? \\Q1999\\E", "shortCiteRegEx": "Brix", "year": 1999}, {"title": "Beta processes, stick-breaking, and power laws", "author": ["T. Broderick", "M. Jordan", "J. Pitman"], "venue": "Bayesian analysis,", "citeRegEx": "Broderick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2011}, {"title": "Probability and Stochastics", "author": ["E. \u00c7inlar"], "venue": "Graduate Texts in Mathematics. Springer,", "citeRegEx": "\u00c7inlar,? \\Q2010\\E", "shortCiteRegEx": "\u00c7inlar", "year": 2010}, {"title": "Variational inference for the Indian buffet process", "author": ["F. Doshi", "K.T. Miller", "J. Van Gael", "Y.W. Teh"], "venue": "In AISTATS,", "citeRegEx": "Doshi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi et al\\.", "year": 2009}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2005}, {"title": "Nonparametric Bayes estimators based on beta processes in models for life history data", "author": ["N.L. Hjort"], "venue": "Annals of Statistics,", "citeRegEx": "Hjort,? \\Q1990\\E", "shortCiteRegEx": "Hjort", "year": 1990}, {"title": "Hierarchical models, nested models and completely random measures. In Frontiers of Statistical Decision Making and Bayesian Analysis: In Honor of James", "author": ["M.I. Jordan"], "venue": null, "citeRegEx": "Jordan.,? \\Q2009\\E", "shortCiteRegEx": "Jordan.", "year": 2009}, {"title": "Completely random measure", "author": ["J.F.C. Kingman"], "venue": "In Pacific Journal of Mathematics,", "citeRegEx": "Kingman,? \\Q1967\\E", "shortCiteRegEx": "Kingman", "year": 1967}, {"title": "Construction of dependent dirichlet processes based on poisson processes", "author": ["D. Lin", "E. Grimson", "J. Fisher"], "venue": "In NIPS, pp", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Dependent Nonparametric Processes", "author": ["S.N. MacEachern"], "venue": "Proceedings of the Section on Bayesian Statistical Science,", "citeRegEx": "MacEachern,? \\Q1999\\E", "shortCiteRegEx": "MacEachern", "year": 1999}, {"title": "Stick-breaking beta processes and the poisson process", "author": ["J. Paisley", "Blei D.M", "M.I. Jordan"], "venue": null, "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Nonparametric factor analysis with beta process priors", "author": ["J. Paisley", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Paisley and Carin,? \\Q2009\\E", "shortCiteRegEx": "Paisley and Carin", "year": 2009}, {"title": "A stick-breaking construction of the beta process", "author": ["J. Paisley", "K. Zaas", "C. Woods", "G. Ginsburg", "L. Carin"], "venue": "In ICML, pp", "citeRegEx": "Paisley et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2010}, {"title": "The discrete infinite logistic normal distribution for mixed-membership modeling", "author": ["J. Paisley", "C. Wang", "D. Blei"], "venue": "In AISTATS,", "citeRegEx": "Paisley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["C. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "L\u00e9vy processes and infinitely divisible distributions", "author": ["K. Sato"], "venue": null, "citeRegEx": "Sato,? \\Q1999\\E", "shortCiteRegEx": "Sato", "year": 1999}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Y.W. Teh"], "venue": "In Coling/ACL,", "citeRegEx": "Teh,? \\Q2006\\E", "shortCiteRegEx": "Teh", "year": 2006}, {"title": "Indian buffet processes with power-law behavior", "author": ["Y.W. Teh", "D. G\u00f6r\u00fcr"], "venue": "In NIPS,", "citeRegEx": "Teh and G\u00f6r\u00fcr,? \\Q2009\\E", "shortCiteRegEx": "Teh and G\u00f6r\u00fcr", "year": 2009}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "JASA, pp. 101:1566\u20131581,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Stickbreaking construction for the Indian buffet process", "author": ["Y.W. Teh", "D. G\u00f6r\u00fcr", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Nonparametric Bayesian Models for Machine Learning", "author": ["R. Thibaux"], "venue": "PhD thesis, EECS Dept.,", "citeRegEx": "Thibaux,? \\Q2008\\E", "shortCiteRegEx": "Thibaux", "year": 2008}, {"title": "Hierarchical beta processes and the Indian buffet process", "author": ["R. Thibaux", "M.I. Jordan"], "venue": "In AISTATS,", "citeRegEx": "Thibaux and Jordan,? \\Q2007\\E", "shortCiteRegEx": "Thibaux and Jordan", "year": 2007}, {"title": "Dependent Indian buffet processes", "author": ["S. Williamson", "P. Orbanz", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Williamson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2010}, {"title": "Non-parametric Bayesian dictionary learning for sparse image representations", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "G. Sapiro", "L. Carin"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Recently the idea of nonparametric methods has extended to feature learning and data clustering, with interest respectively in the beta-Bernoulli process (Thibaux & Jordan, 2007) and the Dirichlet process (Ferguson, 1973).", "startOffset": 205, "endOffset": 221}, {"referenceID": 18, "context": "In this paper we focus on L\u00e9vy processes (Sato, 1999), which are of increasing interest in machine learning.", "startOffset": 41, "endOffset": 53}, {"referenceID": 10, "context": "A family of L\u00e9vy processes, the pure-jump nondecreasing L\u00e9vy processes, also fit into the category of the completely random measure proposed by Kingman (Kingman, 1967).", "startOffset": 152, "endOffset": 167}, {"referenceID": 8, "context": "The beta process (Hjort, 1990) is an example of such a process, which is applied in nonparametric feature learning.", "startOffset": 17, "endOffset": 30}, {"referenceID": 22, "context": "Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al.", "startOffset": 87, "endOffset": 140}, {"referenceID": 20, "context": "Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al.", "startOffset": 87, "endOffset": 140}, {"referenceID": 1, "context": ", 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al., 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 12, "context": ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).", "startOffset": 56, "endOffset": 117}, {"referenceID": 26, "context": ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).", "startOffset": 56, "endOffset": 117}, {"referenceID": 11, "context": ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).", "startOffset": 56, "endOffset": 117}, {"referenceID": 15, "context": "As examples of such work, (Thibaux & Jordan, 2007) and (Paisley et al., 2010) present explicit constructions for generating the beta process, (Teh et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 23, "context": ", 2010) present explicit constructions for generating the beta process, (Teh et al., 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.", "startOffset": 72, "endOffset": 90}, {"referenceID": 19, "context": ", 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.", "startOffset": 130, "endOffset": 148}, {"referenceID": 24, "context": ", 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.", "startOffset": 154, "endOffset": 169}, {"referenceID": 10, "context": "Apart from these specialized construction methods, in (Kingman, 1967) a general construction method for completely random measures is proposed, by first decomposing it into a sum of a countable number of \u03c3-finite measures, and then superposing the Poisson processes according to these sub-measures.", "startOffset": 54, "endOffset": 69}, {"referenceID": 15, "context": "\u2022 The decomposition of the beta process unifies the constructions in (Thibaux & Jordan, 2007), (Teh & G\u00f6r\u00fcr, 2009), and (with a different decomposing method) (Paisley et al., 2010), and a new generative construction for the gamma process and its variations is derived.", "startOffset": 158, "endOffset": 180}, {"referenceID": 18, "context": "L\u00e9vy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts.", "startOffset": 15, "endOffset": 27}, {"referenceID": 10, "context": "L\u00e9vy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts.", "startOffset": 59, "endOffset": 74}, {"referenceID": 18, "context": "By the L\u00e9vy-It\u00f4 decomposition (Sato, 1999), a L\u00e9vy process can be decomposed into a continuous Brownian motion with drift, and a discrete part of a pure-jump process.", "startOffset": 30, "endOffset": 42}, {"referenceID": 18, "context": "with \u03bd satisfying the integrability condition (Sato, 1999).", "startOffset": 46, "endOffset": 58}, {"referenceID": 10, "context": "According to (Kingman, 1967), \u03a6o is discrete with both random atoms and jumps.", "startOffset": 13, "endOffset": 28}, {"referenceID": 10, "context": "In (Kingman, 1967), it is noted that \u03a6o can be further split into a countable number of independent parts:", "startOffset": 3, "endOffset": 18}, {"referenceID": 8, "context": "A beta process (Hjort, 1990) is a L\u00e9vy process with beta-distributed increments; B \u223c BP(c(\u03c9), \u03bc) is a beta process if B(d\u03c9) \u223c Beta(c(\u03c9)\u03bc(d\u03c9), c(\u03c9)(1\u2212 \u03bc(d\u03c9))) (5) where \u03bc is the base measure on measure space (\u03a9,F) and a positive function c(\u03c9) the concentration function.", "startOffset": 15, "endOffset": 28}, {"referenceID": 3, "context": "The work of (Paisley & Jordan, 2012) and (Broderick et al., 2011) show that the stick-breaking construction of the beta process in (Paisley et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 15, "context": ", 2011) show that the stick-breaking construction of the beta process in (Paisley et al., 2010) is indeed a result of another way of decomposing the L\u00e9vy measure of the beta process.", "startOffset": 73, "endOffset": 95}, {"referenceID": 15, "context": "1 and make comparison with the construction of beta process in (Paisley et al., 2010).", "startOffset": 63, "endOffset": 85}, {"referenceID": 15, "context": "For the stick-breaking construction of beta process described in (Paisley et al., 2010), the L1 distance is: ( c c+1 ) .", "startOffset": 65, "endOffset": 87}, {"referenceID": 5, "context": "This metric was applied on the truncated Indian buffet process (Doshi et al., 2009) and truncated stick-breaking construction of the beta process (Paisley & Jordan, 2012), which indicates 1 4 \u222b |m\u221e(b)\u2212mK(b)|db \u2264 Pr(\u2203k > K, 1 \u2264 i \u2264 nk, 1 \u2264 m \u2264M, s.", "startOffset": 63, "endOffset": 83}, {"referenceID": 9, "context": "The gamma process can also be expressed in the form with a base measure G0 and a concentration c(\u03c9), with c = 1/\u03b8 and G0 = \u03b8\u03b1 (Jordan., 2009), to conform with other stochastic processes widely used in machine learning, such as the Dirichlet process.", "startOffset": 126, "endOffset": 141}, {"referenceID": 2, "context": "Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (\u00c7inlar, 2010).", "startOffset": 59, "endOffset": 71}, {"referenceID": 4, "context": "Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (\u00c7inlar, 2010).", "startOffset": 100, "endOffset": 114}, {"referenceID": 27, "context": "However, to briefly summarize experience with such representations, consider for example the image inpainting problem considered in (Zhou et al., 2009), based upon a beta process factor analysis model (Paisley & Carin, 2009).", "startOffset": 132, "endOffset": 151}, {"referenceID": 27, "context": "The model prioritized the first three dictionary elements as being pure colors, specifically red, green, and blue, with the important structured dictionary elements following (and no other pure-color dictionary elements, while in (Zhou et al., 2009) many \u2013 seemingly redundant \u2013 pure-color dictionary elements are inferred).", "startOffset": 230, "endOffset": 249}, {"referenceID": 27, "context": "sult given in (Zhou et al., 2009).", "startOffset": 14, "endOffset": 33}, {"referenceID": 16, "context": ", (Paisley et al., 2011)).", "startOffset": 2, "endOffset": 24}], "year": 2012, "abstractText": "We develop new representations for the L\u00e9vy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on L\u00e9vy processes, as these are of increasing importance in machine learning.", "creator": "LaTeX with hyperref package"}}}