{"id": "1206.4629", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Multiple Kernel Learning from Noisy Labels by Stochastic Programming", "abstract": "we study the problem of multiple kernel learning from noisy labels. this is in contrast to most of the earliest studies on multiple kernel learning that mainly goes on developing efficient algorithms and assume perfectly clean difficulty examples. directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. we address this challenge by casting multiple kernel learning from specific violations into a stochastic programming problem, and presenting inverse minimax formulation. we apply an efficient algorithm for solving noisy related convex - concave optimization problem - a fast convergence rate of $ o ( 1 / t ) $ where $ t $ is inverse sequence of iterations. empirical studies on uci generating sets verify both the effectiveness of the tool framework and the efficiency of the given optimization algorithm.", "histories": [["v1", "Mon, 18 Jun 2012 15:08:22 GMT  (216kb)", "http://arxiv.org/abs/1206.4629v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tianbao yang", "mehrdad mahdavi", "rong jin", "lijun zhang 0005", "yang zhou"], "accepted": true, "id": "1206.4629"}, "pdf": {"name": "1206.4629.pdf", "metadata": {"source": "META", "title": "Multiple Kernel Learning from Noisy Labelsby Stochastic Programming", "authors": ["Tianbao Yang", "Mehrdad Mahdavi", "Rong Jin", "Lijun Zhang", "Yang Zhou"], "emails": ["yangtia1@msu.edu", "mahdavim@msu.edu", "rongjin@msu.edu", "zljzju@zju.edu.cn", "young.zhou@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "Multiple Kernel Learning (MKL) (Lanckriet et al., 2004) has attracted a significant amount of interest in both machine learning and data mining communities due to the success of kernel methods (Scho\u0308lkopf & Smola, 2001). MKL aims to learn an optimal combination of multiple kernels and a nonlinear classifier from the Reproducing Kernel Hilbert\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nSpace (RKHS) endowed with the combined kernel.\nMost of the previous studies on MKL has focused on designing efficient algorithms for solving the related optimization problem. Research has also been done to study the effect of different regularizers on the combination of multiple kernels, including sparse regularizer \u21131 norm, non-sparse regularizer \u21132 norm, and in general \u2113p norm. One limitation of these studies is that they all assume perfectly labeled training examples, which significantly limits their application to problems where class assignments are often noisy and inaccurate. Noisy class assignments could arise either from the biases of human subjects or because the class labels are automatically derived from side information (e.g., hyperlink information (Yang et al., 2010)).\nIn this work, we address this limitation by casting MKL from noisy labels into a stochastic programming problem (Kall & Wallace, 1994). The key idea is to introduce a binary random variable for each training example to indicate if the class assignment of the example is correct. Using introduced binary random variables, we turn the deterministic constraint in MKL into a chance constraint (Ben-Tal et al., 2009), leading to a stochastic programming formulation1. By assuming that the percentage of incorrectly labeled training examples is given, we approximate\n1It is important to distinguish stochastic programming (Kall & Wallace, 1994) from stochastic optimization (Robbins & Monro, 1951). The former refers to the set of problems where the solution is affected by the uncertainty of the system to be optimized, while the later refers to the optimization algorithm (usually iterative) that depends on random variables. In particular, stochastic optimization can be applied to solving a deterministic optimization problem.\nthe stochastic programming problem into a convexconcave optimization problem. Unlike many previous studies (Lawrence & Scho\u0308lkopf, 2001; Pal et al., 2007; Yang et al., 2010) on learning with noisy labeled data that make strong assumptions about the noise model (e.g. conditional independence assumption between noisy label and the data given the true label), our framework depends only on a mild assumption about the noise (see section 3.2), making it practically more useful. Notably, we do NOT assume the label noise of different examples are independent, a common assumption shared by most existing studies on learning from noisy labels (Biggio et al., 2011; Yang et al., 2010). Based on the mirror prox method (Nemirovski, 2005), we develop a first order method for solving the related convex-concave optimization problem. Our analysis shows the convergence rate of O(1/T ) for the proposed algorithm, significantly faster than the classical O(1/ \u221a T ) rate. Empirical studies on five UCI data sets confirm the effectiveness and the efficiency of the proposed framework and the optimization algorithm."}, {"heading": "2. Related Work", "text": "Our work is closely related to MKL. Various criteria have been developed to find the optimal kernel combination, such as maximum classification margin (Lanckriet et al., 2004) and kernel alignment (Cristianini et al., 2002; Cortes et al., 2010a). Among them, maximum margin MKL receives most attention due to its empirical success and its close relationship with Support Vector Machines (SVMs). Many algorithms have been developed for maxmargin MKL by formulating the problem into SemiDefinite Programming (Lanckriet et al., 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including \u21131 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), \u21132 norm (Cortes et al., 2009), and \u2113p norm (Kloft et al., 2009).\nOur work is also related to learning with noisy labels. Lawrence & Scho\u0308lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels. Pal et al. (2007) present a probabilistic model for extracting location information for events with noisy training labels. Ramakrishnan et al. (2005)\npropose a Bayeisan model for learning with approximate, noisy or incomplete labels. Yang et al. (2010) propose a generalized maximum entropy model for learning from noisy side information. These probabilistic approaches have to make strong assumptions about label noise, which significantly limit their applications to real-world problems. In addition, it is difficult to adapt them to MKL. Several recent studies (Huang et al., 2010) address the limitation of probabilistic approaches by exploring the robust optimization (Ben-Tal et al., 2009). Our study is particularly related to the recent work on robust SVM (Xu et al., 2006; Yu et al., 2011) in which a SVM classifier is learned in the presence of outliers. Our work differs from these studies in two aspects. First, we address a different learning problem (i.e., MKL from noisy labels). Second, our work is based on stochastic programming that makes least possible assumption about the noise model compared to the other approaches."}, {"heading": "3. Multiple Kernel Learning From Noisy Labels", "text": "We first review a formulation of MKL based on the equivalence between MKL and group Lasso (Xu et al., 2010; Bach, 2008). We then describe the problem of MKL from noisy labels and present a stochastic programming framework to formulate it. Finally, we present an efficient algorithm for solving the related convex-concave optimization problem."}, {"heading": "3.1. Multiple Kernel Learning (MKL)", "text": "Let D = {(xi, yi), i = 1, \u00b7 \u00b7 \u00b7 , n} be the training data, where xi \u2208 Rd denotes the ith instance and yi \u2208 {\u22121, 1} denotes its binary label. We use y = (y1, \u00b7 \u00b7 \u00b7 , yn)\u22a4 to represent the class assignment of all training examples. We denote by {\u03baj(\u00b7, \u00b7) : Rd\u00d7Rd 7\u2192 R, j \u2208 [m]} the set of m kernels to be combined, and by H\u03baj the corresponding Reproducing Kernel Hilbert Space (RKHS) endowed by \u03baj . We use u = (u1, \u00b7 \u00b7 \u00b7 , um)\u22a4 for the combination weights of multiple kernels, \u03bau = \u2211 j uj\u03baj for the combined kernel, and H\u03bau for the RKHS endowed by \u03bau. In this work, we consider u \u2208 \u2206, where \u2206 = {u \u2208 Rm+ : \u2211 j uj = 1} is a simplex. MKL can be cast as the following problem:\nmin f\u2208H\u03bau ,u\u2208\u2206\n\u03bb 2 \u2016f\u20162H\u03bau + 1 n\nn\u2211\ni=1\n\u2113(yif(xi)), (1)\nwhere \u2113(z) = max(0, 1 \u2212 z) is the hinge loss function. It has been shown that the problem in (1) is equivalent\nto the following problem (Micchelli & Pontil, 2005),\nmin {fj}mj=1\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2 + 1\nn\nn\u2211\ni=1\n\u2113  yi m\u2211\nj=1\nfj(xi)\n  ,\n(2)\nor equivalently\nmin t,{fj}mj=1\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2\n+ t (3)\ns.t. 1\nn\nn\u2211\ni=1\n\u2113  yi m\u2211\nj=1\nfj(xi)\n  \u2264 t, (4)\nwhere fj belongs to H\u03baj and t is a slack variable to be optimized. Given the solutions fj , j \u2208 [m] to (3), the final classifier is defined as f(x) = \u2211m j=1 fj(x). In\nthe sequel, we use the notation f(x) = \u2211m\nj=1 fj(x) to simplify our presentation. Our formulation for MKL from noisy labels is based on the formulation in (3). In the next subsection, we present a stochastic programming framework for MKL from noisy labels. Due to the limit of space, we put the proofs of most analysis in the supplementary material."}, {"heading": "3.2. A Stochastic Programming Framework for MKL from Noisy Labels", "text": "In the case of noisy labels, we have some incorrect class assignments for the training examples in D. The key challenge is that we do not know which examples are incorrectly labeled. To facilitate learning from noisy labels, we assume the noise level of class assignments q \u2208 [0, 1/2), i.e., the expected probability for any randomly chosen example to be incorrectly labeled, is given.\nFor a given pair (x, y), let \u03be(x, y) \u2208 {0, 1} be a binary random variable indicating if y is a correct label of x (1) or not (0), and p(x, y) = E\u03be|(x,y)[\u03be(x, y)] be the probability for y to be a correct label of x.\nAssumption 1. The noise level q is given, i.e.,\nq = 1\u2212 E(x,y)[p(x, y)].\nWith Assumption 1, we present the following proposition to bound the empirical mean of p(x, y) on the training examples. Proposition 1. Let \u03bei, i \u2208 [n] denote the binary indicator variable of noise on the training examples, and pi = E[\u03bei]. Given the noise level q, with probability at least 1\u2212 \u01eb, we have\n1\nn\nn\u2211\ni=1\npi \u2264 1\u2212 q + \u03c4\u221a n ,\nwhere \u03c4 = \u221a (1/2) ln(1/\u01eb).\nThe proposition follows directly from the Hoeffding\u2019s inequality (Boucheron et al., 2004).\nTo handle the noisy labels, we consider a stochastic programming (Kall & Wallace, 1994) framework. More specifically, given the unknown random variables \u03be = (\u03be1, . . . , \u03ben), where \u03bei = \u03be(xi, yi), we relax the deterministic constraint in (4) into a chance constraint (Ben-Tal et al., 2009)\nPr\n( 1\nn\nn\u2211\ni=1\n\u03bei\u2113 (yif(xi)) > t\n) \u2264 \u01eb, (5)\nwhere Pr(\u00b7) takes over the unknown joint distribution of binary random variables \u03be, and \u01eb \u2208 (0, 1) bounds the probability for the constraint in (3) to be violated. The chance constraint in (5) requires that there is only a small chance for the constraint to be violated by the unknown correctly labeled examples. It has also been used for handling the uncertainty before. In (Bhadra et al., 2010), the authors introduce the chance constraints to handle the uncertainty in the elements of a kernel matrix, while the chance constraint in (5) is introduced to handle the uncertainty in the class labels.\nUsing the chance constraint in (5), we have the following stochastic programming problem for MKL from noisy labels:\nmin t,{fj}mj=1\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2\n+ t (6)\ns.t. Pr\n( 1\nn\nn\u2211\ni=1\n\u03bei\u2113 (yif(xi)) > t\n) \u2264 \u01eb.\nA major challenge of solving the stochastic programming problem in (6) is that the joint distribution for \u03be is unknown. The following lemma allows us to alleviate this challenge.\nLemma 1. If the following inequality holds,\n1\nn\nn\u2211\ni=1\nE[\u03bei]\u2113 (yif(xi)) \u2264 t\u2212 \u03c4 \u221a\u221a\u221a\u221a 1 n2 n\u2211\ni=1\n\u21132 (yif(xi)),\nwhere \u03c4 = \u221a (1/2) ln(1/\u01eb), then the chance constraint in (5) is satisfied.\nThe proof follows immediately from the McDiarmid inequality (Boucheron et al., 2004). It is important to note that Lemma 1 holds WITHOUT having to assume that the binary random variables {\u03bei}mi=1 are\nindependent, a common assumption that appears in almost all the studies on learning from noisy labels.\nUsing Lemma 1, we relax the problem in (6) into the following optimization problem\nmin t,{fj}mj=1\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2\n+ t (7)\ns.t. 1\nn\nn\u2211\ni=1\npi\u2113 (yif(xi)) + \u03c4\nn\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u21132 (yif(xi)) \u2264 t, (8)\nwhere pi denotes E[\u03bei]. We can turn the constrained problem into non-constrained problem by replacing t in (7) with the lower bound in (8). There are two problems with directly optimizing (7). First, the second term in the lower bound of t in (8) is square-root of a quadratic form on the training loss, making the optimization problem difficult to solve. Second, the variables {pi}ni=1 are unknown. Without knowing the value of {pi}ni=1, it is impossible to solve the optimization problem in (7).\nTo address the first problem, we use the inequality\u221a\u2211n i=1 \u2113 2 i \u2264 \u2211n i=1 |\u2113i| to relax the constraint in (8) into\n1\nn\nn\u2211\ni=1\npi\u2113 (yif(xi)) + \u03c4\nn\nn\u2211\ni=1\n\u2113 (yif(xi)) \u2264 t. (9)\nNote that inequality (9) indicates inequality (8), and therefore guarantees that the chance constraint in (5) holds. Then we turn problem (7) into\nmin t,{fj}mj=1\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2\n+ t\ns.t. 1\nn\nn\u2211\ni=1\n(pi + \u03c4)\u2113 (yif(xi)) \u2264 t,\nor equivalently\nmin {fj}mj=1\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2 + 1\nn\nn\u2211\ni=1\n(pi + \u03c4) \u2113 (yif(xi)) .\nTo address the second problem, we propose the following minimax formulation\nmin {fj}mj=1 max p\u2208Q\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2 + 1\nn\nn\u2211\ni=1\n(pi + \u03c4)\u2113 (yif(xi)) ,\n(10)\nwhere Q = {p \u2208 [0, 1]n,\u2211i pi \u2264 (1 \u2212 q)n + \u03c4 \u221a n} is a domain for p justified by Proposition 1.\nRemark: We choose to maximize over p \u2208 Q because it guarantees that the loss of any choice of correctly labeled examples is minimized. The idea of using the worst case analysis is closely related to robust optimization (Ben-Tal et al., 2009), which has been adopted by several recent studies, including budget SVM (Dekel & Singer, 2006), and robust metric learning (Huang et al., 2010). Note that an alternative approach is to consider the best case analysis (a strategy taken in robust SVM (Xu et al., 2006; Yu et al., 2011)) by minimizing the robust hinge loss, which can be defined as minp\u2208Q \u2211 i(pi\u2113i+1\u2212pi), where \u2113i denotes the loss on ith example, to address the uncertainty arising from noisy labels.\nThere are several problems with the alternative approach. First, minimization over p will lead to a nonconvex optimization problem, as shown in (Xu et al., 2006), making it difficult, if not impossible, to develop an efficient learning algorithm to find the global optimum. Second, unless a strong assumption is made about the examples with incorrect labels, minimization over p will not provide any guarantee about the generalized performance of the resulting classifier. Third, the formulation with minimization over p does not reduce to the normal case in (2) when there is no noise.\nIn contrast, our problem is a convex-concave problem, which allows us to derive an efficient optimization algorithm to solve it. Also, we do NOT make any assumption on the noisy labels except assuming that the noise level is given. More ever, the generalization error of the kernel classifier learned from (10) is given in Theorem 1, which also justifies the maximization over p. Finally, it is straightforward to show that our formulation in (10) reduces to (2) when there is no noise. This point is also demonstrated by our empirical studies.\nTheorem 1. Assume that the number of incorrectly labeled instances in D is no more than nq. Let fj , j \u2208 [m] be the final solutions to (10) and set f(x) = \u2211m j=1 fj(x). With probability at least 1 \u2212 \u03b4, we have\nE(x,y) [\u2113 (yf(x))] \u2264 max p\u2208Q\n1\nn(1\u2212 q)\nn\u2211\ni=1\npi\u2113 (yif(xi))\n+\n\u221a 2(1 + \u03c4)\nn(1 \u2212 q)\u03bb\n[ 4 + \u221a( 1 +\n\u03bb\n2(1 + \u03c4)\n) ln 1\n\u03b4\n] .\nRemark: The bound scales with 1/(1 \u2212 q), so when the noise level q is large, the generalization bound is also large. Additionally, we can see that with a probability (1\u2212m\u2212k), where k is an integer, the generaliza-\ntion error bound has an additional term of \u221a lnm/n, which is a tight bound for \u21131-regularized MKL in terms of the number of kernels (Ying & Campbell, 2009; Cortes et al., 2010b)."}, {"heading": "3.3. An Efficient Optimization Algorithm", "text": "In this section, we present an efficient algorithm for solving the minimax problem in (10). We first present an alternative formulation for (10), i.e.,\nmin {fj}mj=1 max \u03b1\u2208Q\u03b1\n\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016H\u03baj\n  2 + 1\nn\nn\u2211\ni=1\n\u03b1i (1\u2212 yif(xi)) ,\n(11)\nwhere Q\u03b1 = {\u03b1 : \u03b1 \u2208 [0, 1 + \u03c4 ]n, \u2016\u03b1\u20161 \u2264 \u03c1}, and \u03c1 = (1\u2212 q + \u03c4 + \u03c4/\u221an)n. This is obtained by writing \u2113(yif(xi)) = max(0, 1 \u2212 yif(xi)) = max\u03b2i\u2208[0,1] \u03b2i(1 \u2212 yif(xi)), and introducing {\u03b1i = \u03b2i(pi + \u03c4)}ni=1. Before presenting the optimization algorithm, we introduce a few notations that will be used throughout this section. We denote by f = (f1, \u00b7 \u00b7 \u00b7 , fm)\u22a4 \u2208 H where H = (H\u03ba1 , \u00b7 \u00b7 \u00b7 ,H\u03bam), and by R(f) and L(f, \u03b1) the first term and the second term in the objective function in (11), respectively. We write the problem in (11) as\nmin f\u2208H max \u03b1\u2208Q\u03b1 F (f, \u03b1) = R(f) + L(f, \u03b1).\nIn the following, we refer to f as the primal variable and \u03b1 as the dual variable. We denote by \u2207fL(f, \u03b1) = (\u2207f1L(f, \u03b1), \u00b7 \u00b7 \u00b7 ,\u2207fmL(f, \u03b1))\u22a4 and \u2207\u03b1L(f, \u03b1) the partial gradients of L(f, \u03b1) in terms of f and \u03b1, respectively. We use the notations \u2016fj\u2016 = \u2016fj\u2016H\u03baj , and \u2016f\u2016 2 = \u2211\nj \u2016fj\u20162H\u03baj for short. We denote by\n\u220f Q[v\u0302] the projection v\u0302 into domain Q, i.e.\u220f\nQ[v\u0302] = argminv\u2208Q 1 2\u2016v\u2212 v\u0302\u20162, where \u2016 \u00b7 \u2016 is \u21132 norm when it is applied to a vector, and a RKHS norm when applied to a function.\nNext, we present an accelerated mirror prox method that extends the mirror prox method (Nemirovski, 2005) to efficiently solve the convex-concave optimization problem in (11). The main limitation of the original mirror prox method is that it is only applicable to smooth objective functions whose gradients are Lipschitz continuous, which unfortunately is not the case for the problem in (11) because R(f) is not a smooth function in f . Algorithm 1 outlines the key steps of this method. In Algorithm 1, we maintain two copies for the dual variables (i.e., \u03b1 and \u03b2), but only one copy for the primal variable f . This is in contrast to the mirror prox method that introduces two copies for both primal and dual variables. Another key difference\nAlgorithm 1 An Accelerated Mirror Prox Method\n1: Input: step size \u03b3 = \u221a n/(2m) 2: Initialization: \u03b20 = 0, f 0 = 0 3: for t = 1, 2, . . . , T do 4: \u03b1t = \u220f\nQ\u03b1\n[\u03b2t\u22121 + \u03b3\u2207\u03b1L(f t\u22121, \u03b2t\u22121)]\n5: f t = argminf\u2208H 1 2\n\u2225\u2225\u2225f \u2212 f\u0302 t\u22121 \u2225\u2225\u2225 2 + \u03b3R(f)\nwhere f\u0302 t\u22121 = f t\u22121 \u2212 \u03b3\u2207fL(f t\u22121, \u03b1t) 6: \u03b2t = \u220f\nQ\u03b1\n[\u03b2t\u22121 + \u03b3\u2207\u03b1L(f t, \u03b1t)]\n7: end for 8: Output: f\u0302T = \u2211 t f t/T, \u03b1\u0302T = \u2211 t \u03b1t/T\nbetween Algorithm 1 and the mirror prox method is that in step 5, we update the primal variable f by a composite gradient mapping (Nesterov, 2007), instead of a gradient mapping. It is this step that allows us to solve the convex-concave optimization problems efficiently with a convergence rate of O(1/T ), without having to assume that the gradients of the objective function are Lipschitz continuous. It is important to point out that accelerated proximal gradient method by Tseng (Tseng, 2008) is not applicable to (11), since it requires the Lipschitz continuous gradients.\nIn order to efficiently implement Algorithm 1, we need to efficiently solve the optimization problems in steps 4, 5, and 6. The gradient mapping problems in steps 4 and 6 can be solved by utilizing the following lemma. Lemma 2. The optimal solution \u03b1\u2217 to \u220f\nQ\u03b1 [\u03b1\u0302] is\ngiven by \u03b1\u2217i = [\u03b1\u0302i\u2212\u03b7][0,1+\u03c4 ], i = 1, \u00b7 \u00b7 \u00b7 , n, where [s][a,b] is projection of the number s into the range [a, b], and \u03b7 = 0 if \u2211 i[\u03b1\u0302i][0,1+\u03c4 ] < \u03c1, otherwise \u03b7 is the solution to the following equation\n\u2211\ni\n[\u03b1\u0302i \u2212 \u03b7][0,1+\u03c4 ] \u2212 \u03c1 = 0. (12)\nSince \u2211\ni[\u03b1\u0302i\u2212 \u03b7][0,1+\u03c4 ]\u2212\u03c1 is monotonically decreasing function in \u03b7, we can efficiently compute \u03b7 in (12) by bisection search.\nThe composite gradient mapping in step 5 is\nmin {fj}mj=1\n1\n2\n\u2211\nj\n\u2016fj \u2212 f\u0302 t\u22121j \u20162 + \u03b3\u03bb\n2\n  m\u2211\nj=1\n\u2016fj\u2016\n  2\n, (13)\nwhere f\u0302 t\u22121j = f t\u22121 j \u2212 \u03b3\u2207fjL(f t\u22121, \u03b1t), and it can be solved by the following lemma.\nLemma 3. The optimal solution to (13) denoted by\nf\u2217j , j \u2208 [m] is given by\nf\u2217j =\n[ 1\u2212 \u03b3\u03bb\u00b5t\n2\u2016f\u0302 t\u22121j \u2016\n]\n+\nf\u0302 t\u22121j ,\nwhere [z]+ = z if z > 0 and 0 otherwise, and \u00b5t is the solution to the following equation,\n\u2211\nj\n[ 1\u2212 \u03b3\u03bb\u00b5t\n2\u2016f\u0302 t\u22121j \u2016\n]\n+\n\u2016f\u0302 t\u22121j \u2016 \u2212 \u00b5t = 0,\nwhere \u00b5t can be computed by efficient bisection search.\nRemark: Note that since the partial gradient \u2207fjL(f, \u03b1) = \u2212(1/n) \u2211n i=1 \u03b1iyi\u03baj(xi, \u00b7), hence, when updating the kernel classifier fj , we can write it in a parameterized form fj = \u2211n i=1 ziyi\u03baj(xi, \u00b7) and update the coefficients z = (z1, \u00b7 \u00b7 \u00b7 , zn) at each iteration. We conclude this subsection by presenting the convergence rate for Algorithm 1.\nTheorem 2. By running Algorithm 1 with T iterations, we have\nmax \u03b1\u2208Q\u03b1 F (f\u0302T , \u03b1)\u2212min f\nF (f, \u03b1\u0302T ) \u2264 ( \u2225\u2225f\u2217\u20162 + \u2016\u03b1\u2217\u201622 )\u221a m\u221a\n2nT ,\nwhere f\u2217 = (f\u22171 , \u00b7 \u00b7 \u00b7 , f\u2217m)\u22a4 = argminf F (f, \u03b1\u0302T ) and \u03b1\u2217 = argmax\u03b1\u2208Q\u03b1 F (f\u0302T , \u03b1).\nTheorem 2 indicates a O(1/T ) convergence rate for the accelerated mirror prox method presented in Algorithm 1 that is significantly faster than traditional O(1/ \u221a T ) convergence rate for non-smooth optimization problems. This is also demonstrated by our empirical studies."}, {"heading": "4. Experiments", "text": "In this section, we simulate our experiments on UCI data sets to verify the effectiveness and efficiency of the proposed algorithm for MKL from noisy labels. In a simulated environment, we can control the noise level to better understand the behavior of the proposed algorithm under different noise levels compared to baseline algorithms. We choose five data sets from UCI repository that have been used in MKL studies (Rakotomamonjy et al., 2008; Xu et al., 2010). The statistics of the data sets are summarized in Table 1. We normalize the data by scaling each attribute to [0, 1]. This is done by first subtracting each attribute from its minimal value and then dividing it by the difference between the maximal and the minimal value of the attribute. To generate label noise, we randomly flip the class label of each example with a probability\nof q. To create multiple kernels, we follow the setup in (Xu et al., 2010) to generate Gaussian kernels with 10 different width {2\u22123, 2\u22122, \u00b7 \u00b7 \u00b7 , 26} for all features as well as for individual features, leading to a total of m = 10(d + 1) kernels for each data set, where d is the number of features. We split the data into 80% for training and 20% for testing.\nIn the experiments, we focus on verifying the proposed stochastic programming framework for handling the noise in labels. We choose two baselines for comparison where one directly optimizes the objective in (2) assuming the labels are all correct, and the other one adopts a different strategy (i.e. minimization instead of maximization over p) to handle the noise. By comparing with the first baseline, we are able to verify that existence of noise in labels significantly deteriorates the performance and therefore handling the noise is important. By comparing with the second baseline, we are able to verify the proposed stochastic programming framework with maximization over p is a good choice for handling the noise. For the first baseline, we choose Simple MKL (SiPMKL) algorithm, a state-of-the-art algorithm for \u21131 regularized MKL\n2. For the second baseline, we extend the idea of robust SVM (Xu et al., 2006) to MKL from noisy labels by using the robust hinge loss and minimizing over p \u2208 Q. We refer to this baseline as MiPMKL. Finally, we refer to proposed algorithm as StPMKL.\nIn implementing the proposed algorithm, we project \u03b1i into [0, 1] by absorbing the upper bound 1 + \u03c4 into the regularization parameter \u03bb and the bound parameter \u03c1(\u03c4). The regularization parameter \u03bb in the proposed algorithm (and baselines as well) is chosen by cross validation on a validation data of 10% examples randomly selected from the training data. The parameter \u03c1(\u03c4) is also tuned among several values [1, 0.9, 0.8, 0.7, 0.6, 0.5]n on the validation data. To make fair comparison, we use the same stopping criterion for all algorithms, i.e., algorithms stop if the duality gap is less than threshold \u03b5 = 10\u22122 or the maximum number of iterations T = 1000 is reached. We repeat each experiment five times, and report the results by averaging over the 5 trials.\nThe left panels of Figure 1 show the classification accuracy of algorithms with noise level q varied from 0 to 0.4 on the five data sets. We observe that for almost\n2We do not compare many other MKL algorithms because (i) some algorithms (Xu et al., 2010) optimize the same objective as in (2), (ii) some algorithms (Cortes et al., 2009; Kloft et al., 2009) focus on different regularizations (e.g. \u21132 or \u2113p), and (iii) some algorithms (Cristianini et al., 2002; Cortes et al., 2010a) use different criteria (e.g. kernel alignment).\nall cases, (i) the proposed algorithm is significantly more resilient than SiPMKL algorithm to the label noise; (ii) the worst case analysis (StPMKL) is better than the best case analysis (MiPMKL) for noisy labels, particularly when the noise level is high; and (iii) when no noise is added (q=0), StPMKL achieves similar performance, if not the same, to SiPMKL, while MiPMKL could give different results (e.g. on ionosphere, heart, sonar). The reason is that the objective in StPMKL reduces to the objective in SiPMKL when q = 0, however, it is not the case for MiPMKL. This observation is consistent with our discussion in section 3.2 above Theorem 1.\nFinally, we verify the efficiency of the accelerated mirror prox method. We compare the proposed accel-\nerated mirror prox (AMP) method to the variational inequality method (VI) (Nemirovski, 1994) (i.e. gradient descent method for convex-concave problem). For fair comparison, we fix \u03bb = 0.01 and \u03c1 = 100. The step size in variational inequality method is set to \u03b30/ \u221a T where we tune \u03b30 in the range of [0.01, 0.1, 1, 10, 100] and the best convergence with the best \u03b30 is finally reported. We run both algorithms with 1000 iterations, and plot the duality gap versus the number of iterations. The results are shown in the right panels of Figure 1, which verify that the accelerated mirror prox method is significantly more efficient than the variational inequality method."}, {"heading": "5. Conclusions", "text": "In this paper, we present a stochastic programming framework for multiple kernel learning from noisy labels. We formulate the problem into a convex-concave optimization problem. We also present an efficient accelerated mirror prox method for solving the related convex-convex problem. Empirical studies in a simulated environment verify the effectiveness of the proposed framework and the efficiency of the developed optimization algorithm. For future work, we plan to apply the proposed approach to real-world problems with inherent noise in labels where the noise may not be synthetically generated by independent random flipping. An open problem associated with it would be how to obtain the knowledge of the noise level."}, {"heading": "Acknowledgement", "text": "This work is in part supported by National Science Foundation (IIS-0643494), Office of Navy Research (ONR Award N00014-09-1-0663 and N0001412-1-0431)."}], "references": [{"title": "Consistency of the group lasso and multiple kernel learning", "author": ["Bach", "Francis R"], "venue": "JMLR, 9:1179\u20131225,", "citeRegEx": "Bach and R.,? \\Q2008\\E", "shortCiteRegEx": "Bach and R.", "year": 2008}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["Bach", "Francis R", "Lanckriet", "Gert R. G", "Jordan", "Michael I"], "venue": "In ICML,", "citeRegEx": "Bach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2004}, {"title": "Robust Optimization", "author": ["Ben-Tal", "Aharon", "Ghaoui", "Laurent El", "Nemirovski", "Arkadi"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "Robust formulations for handling uncertainty in kernel matrices", "author": ["Bhadra", "Sahely", "Bhattacharya", "Sourangshu", "Bhattacharyya", "Chiranjib", "Ben-Tal", "Aharon"], "venue": "In ICML, pp", "citeRegEx": "Bhadra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bhadra et al\\.", "year": 2010}, {"title": "Support vector machines under adversarial label noise", "author": ["Biggio", "Battista", "Nelson", "Blaine", "Laskov", "Pavel"], "venue": "In ACML, pp", "citeRegEx": "Biggio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2011}, {"title": "Concentration Inequalities, pp. 208\u2013240", "author": ["S. Boucheron", "O. Bousquet", "G. Lugosi"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2004}, {"title": "L2 regularization for learning kernels", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "In UAI, pp", "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Two-stage learning kernel algorithms", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "In ICML, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Generalization bounds for learning kernels", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "In ICML, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "On kernel-target alignment", "author": ["Cristianini", "Nello", "Kandola", "Jaz", "Elisseeff", "Andre", "Shawe-Taylor", "John"], "venue": "In NIPS,", "citeRegEx": "Cristianini et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2002}, {"title": "Support vector machines on a budget", "author": ["Dekel", "Ofer", "Singer", "Yoram"], "venue": "In NIPS, pp", "citeRegEx": "Dekel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2006}, {"title": "Robust metric learning by smooth optimization", "author": ["Huang", "Kaizhu", "Jin", "Rong", "Xu", "Zenglin", "Liu", "ChengLin"], "venue": "In UAI,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Efficient and accurate lp-norm multiple kernel learning", "author": ["Kloft", "Marius", "Brefeld", "Ulf", "Sonnenburg", "Soeren", "Laskov", "Pavel", "M\u00fcller", "Klaus-Robert", "Zien", "Alexander"], "venue": "In NIPS,", "citeRegEx": "Kloft et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2009}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Lanckriet", "Gert", "Cristianini", "Nello", "Bartlett", "Peter", "Ghaoui", "Laurent E"], "venue": "JMLR, 5:27\u201372,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["Lawrence", "Neil D", "Sch\u00f6lkopf", "Bernhard"], "venue": "In ICML, pp", "citeRegEx": "Lawrence et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2001}, {"title": "Learning the kernel function via regularization", "author": ["Micchelli", "Charles A", "Pontil", "Massimiliano"], "venue": null, "citeRegEx": "Micchelli et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Micchelli et al\\.", "year": 2005}, {"title": "Efficient methods in convex programming", "author": ["A. Nemirovski"], "venue": null, "citeRegEx": "Nemirovski,? \\Q1994\\E", "shortCiteRegEx": "Nemirovski", "year": 1994}, {"title": "Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski and Arkadi.,? \\Q2005\\E", "shortCiteRegEx": "Nemirovski and Arkadi.", "year": 2005}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Nesterov", "Yu"], "venue": "Technical report,", "citeRegEx": "Nesterov and Yu.,? \\Q2007\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2007}, {"title": "Putting semantic information extraction on the map: Noisy label models for fact extraction", "author": ["Pal", "Chris", "Mann", "Gideon", "Minerich", "Richard"], "venue": "In AAAI Workshop on Information Integration on the Web,", "citeRegEx": "Pal et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pal et al\\.", "year": 2007}, {"title": "A model for handling approximate, noisy or incomplete labeling in text classification", "author": ["Ramakrishnan", "Ganesh", "Chitrapura", "Krishna Prasad", "Krishnapuram", "Raghu", "Bhattacharyya", "Pushpak"], "venue": "In ICML,", "citeRegEx": "Ramakrishnan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ramakrishnan et al\\.", "year": 2005}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": null, "citeRegEx": "Robbins and Monro,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "A general and efficient multiple kernel learning algorithm", "author": ["Sonnenburg", "S\u00f6ren", "R\u00e4tsch", "Gunnar", "Sch\u00e4fer", "Christin"], "venue": "In NIPS, pp", "citeRegEx": "Sonnenburg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["Tseng", "Paul"], "venue": "Technical report,", "citeRegEx": "Tseng and Paul.,? \\Q2008\\E", "shortCiteRegEx": "Tseng and Paul.", "year": 2008}, {"title": "Robust support vector machine training via convex outlier ablation", "author": ["Xu", "Linli", "Crammer", "Koby", "Schuurmans", "Dale"], "venue": "In AAAI,", "citeRegEx": "Xu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2006}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Xu", "Zenglin", "Jin", "Rong", "Yang", "Haiqin", "King", "Irwin", "Lyu", "Michael R"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Learning from noisy side information by generalized maximum entropy model", "author": ["Yang", "Tianbao", "Jin", "Rong", "Jain", "Anil K"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Generalization bounds for learning the kernel problem", "author": ["Ying", "Yiming", "Campbell", "Colin"], "venue": "In COLT,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}, {"title": "Relaxed clipping: A global training method for robust regression and classification", "author": ["Yu", "Yaoliang", "Yang", "Min", "Xu", "Linli", "White", "Martha", "Schuurmans", "Dale"], "venue": "In NIPS,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "Multiple Kernel Learning (MKL) (Lanckriet et al., 2004) has attracted a significant amount of interest in both machine learning and data mining communities due to the success of kernel methods (Sch\u00f6lkopf & Smola, 2001).", "startOffset": 31, "endOffset": 55}, {"referenceID": 27, "context": ", hyperlink information (Yang et al., 2010)).", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "Using introduced binary random variables, we turn the deterministic constraint in MKL into a chance constraint (Ben-Tal et al., 2009), leading to a stochastic programming formulation.", "startOffset": 111, "endOffset": 133}, {"referenceID": 19, "context": "Unlike many previous studies (Lawrence & Sch\u00f6lkopf, 2001; Pal et al., 2007; Yang et al., 2010) on learning with noisy labeled data that make strong assumptions about the noise model (e.", "startOffset": 29, "endOffset": 94}, {"referenceID": 27, "context": "Unlike many previous studies (Lawrence & Sch\u00f6lkopf, 2001; Pal et al., 2007; Yang et al., 2010) on learning with noisy labeled data that make strong assumptions about the noise model (e.", "startOffset": 29, "endOffset": 94}, {"referenceID": 4, "context": "Notably, we do NOT assume the label noise of different examples are independent, a common assumption shared by most existing studies on learning from noisy labels (Biggio et al., 2011; Yang et al., 2010).", "startOffset": 163, "endOffset": 203}, {"referenceID": 27, "context": "Notably, we do NOT assume the label noise of different examples are independent, a common assumption shared by most existing studies on learning from noisy labels (Biggio et al., 2011; Yang et al., 2010).", "startOffset": 163, "endOffset": 203}, {"referenceID": 13, "context": "Various criteria have been developed to find the optimal kernel combination, such as maximum classification margin (Lanckriet et al., 2004) and kernel alignment (Cristianini et al.", "startOffset": 115, "endOffset": 139}, {"referenceID": 9, "context": ", 2004) and kernel alignment (Cristianini et al., 2002; Cortes et al., 2010a).", "startOffset": 29, "endOffset": 77}, {"referenceID": 13, "context": "Many algorithms have been developed for maxmargin MKL by formulating the problem into SemiDefinite Programming (Lanckriet et al., 2004), Second Order Cone Programming (Bach et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 23, "context": ", 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006).", "startOffset": 46, "endOffset": 71}, {"referenceID": 26, "context": "A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010).", "startOffset": 96, "endOffset": 141}, {"referenceID": 26, "context": "Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al.", "startOffset": 105, "endOffset": 150}, {"referenceID": 6, "context": ", 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 12, "context": ", 2009), and lp norm (Kloft et al., 2009).", "startOffset": 21, "endOffset": 41}, {"referenceID": 11, "context": "Several recent studies (Huang et al., 2010) address the limitation of probabilistic approaches by exploring the robust optimization (Ben-Tal et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 2, "context": ", 2010) address the limitation of probabilistic approaches by exploring the robust optimization (Ben-Tal et al., 2009).", "startOffset": 96, "endOffset": 118}, {"referenceID": 25, "context": "Our study is particularly related to the recent work on robust SVM (Xu et al., 2006; Yu et al., 2011) in which a SVM classifier is learned in the presence of outliers.", "startOffset": 67, "endOffset": 101}, {"referenceID": 29, "context": "Our study is particularly related to the recent work on robust SVM (Xu et al., 2006; Yu et al., 2011) in which a SVM classifier is learned in the presence of outliers.", "startOffset": 67, "endOffset": 101}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels.", "startOffset": 40, "endOffset": 691}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels. Pal et al. (2007) present a probabilistic model for extracting location information for events with noisy training labels.", "startOffset": 40, "endOffset": 800}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels. Pal et al. (2007) present a probabilistic model for extracting location information for events with noisy training labels. Ramakrishnan et al. (2005) propose a Bayeisan model for learning with approximate, noisy or incomplete labels.", "startOffset": 40, "endOffset": 932}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels. Pal et al. (2007) present a probabilistic model for extracting location information for events with noisy training labels. Ramakrishnan et al. (2005) propose a Bayeisan model for learning with approximate, noisy or incomplete labels. Yang et al. (2010) propose a generalized maximum entropy model for learning from noisy side information.", "startOffset": 40, "endOffset": 1035}, {"referenceID": 26, "context": "We first review a formulation of MKL based on the equivalence between MKL and group Lasso (Xu et al., 2010; Bach, 2008).", "startOffset": 90, "endOffset": 119}, {"referenceID": 5, "context": "The proposition follows directly from the Hoeffding\u2019s inequality (Boucheron et al., 2004).", "startOffset": 65, "endOffset": 89}, {"referenceID": 2, "context": ", \u03ben), where \u03bei = \u03be(xi, yi), we relax the deterministic constraint in (4) into a chance constraint (Ben-Tal et al., 2009)", "startOffset": 99, "endOffset": 121}, {"referenceID": 3, "context": "In (Bhadra et al., 2010), the authors introduce the chance constraints to handle the uncertainty in the elements of a kernel matrix, while the chance constraint in (5) is introduced to handle the uncertainty in the class labels.", "startOffset": 3, "endOffset": 24}, {"referenceID": 5, "context": "The proof follows immediately from the McDiarmid inequality (Boucheron et al., 2004).", "startOffset": 60, "endOffset": 84}, {"referenceID": 2, "context": "The idea of using the worst case analysis is closely related to robust optimization (Ben-Tal et al., 2009), which has been adopted by several recent studies, including budget SVM (Dekel & Singer, 2006), and robust metric learning (Huang et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 11, "context": ", 2009), which has been adopted by several recent studies, including budget SVM (Dekel & Singer, 2006), and robust metric learning (Huang et al., 2010).", "startOffset": 131, "endOffset": 151}, {"referenceID": 25, "context": "Note that an alternative approach is to consider the best case analysis (a strategy taken in robust SVM (Xu et al., 2006; Yu et al., 2011)) by minimizing the robust hinge loss, which can be defined as minp\u2208Q \u2211 i(pili+1\u2212pi), where li denotes the loss on ith example, to address the uncertainty arising from noisy labels.", "startOffset": 104, "endOffset": 138}, {"referenceID": 29, "context": "Note that an alternative approach is to consider the best case analysis (a strategy taken in robust SVM (Xu et al., 2006; Yu et al., 2011)) by minimizing the robust hinge loss, which can be defined as minp\u2208Q \u2211 i(pili+1\u2212pi), where li denotes the loss on ith example, to address the uncertainty arising from noisy labels.", "startOffset": 104, "endOffset": 138}, {"referenceID": 25, "context": "First, minimization over p will lead to a nonconvex optimization problem, as shown in (Xu et al., 2006), making it difficult, if not impossible, to develop an efficient learning algorithm to find the global optimum.", "startOffset": 86, "endOffset": 103}, {"referenceID": 26, "context": "We choose five data sets from UCI repository that have been used in MKL studies (Rakotomamonjy et al., 2008; Xu et al., 2010).", "startOffset": 80, "endOffset": 125}, {"referenceID": 26, "context": "To create multiple kernels, we follow the setup in (Xu et al., 2010) to generate Gaussian kernels with 10 different width {2\u22123, 2, \u00b7 \u00b7 \u00b7 , 26} for all features as well as for individual features, leading to a total of m = 10(d + 1) kernels for each data set, where d is the number of features.", "startOffset": 51, "endOffset": 68}, {"referenceID": 25, "context": "For the second baseline, we extend the idea of robust SVM (Xu et al., 2006) to MKL from noisy labels by using the robust hinge loss and minimizing over p \u2208 Q.", "startOffset": 58, "endOffset": 75}, {"referenceID": 26, "context": "We observe that for almost We do not compare many other MKL algorithms because (i) some algorithms (Xu et al., 2010) optimize the same objective as in (2), (ii) some algorithms (Cortes et al.", "startOffset": 99, "endOffset": 116}, {"referenceID": 6, "context": ", 2010) optimize the same objective as in (2), (ii) some algorithms (Cortes et al., 2009; Kloft et al., 2009) focus on different regularizations (e.", "startOffset": 68, "endOffset": 109}, {"referenceID": 12, "context": ", 2010) optimize the same objective as in (2), (ii) some algorithms (Cortes et al., 2009; Kloft et al., 2009) focus on different regularizations (e.", "startOffset": 68, "endOffset": 109}, {"referenceID": 9, "context": "l2 or lp), and (iii) some algorithms (Cristianini et al., 2002; Cortes et al., 2010a) use different criteria (e.", "startOffset": 37, "endOffset": 85}, {"referenceID": 16, "context": "erated mirror prox (AMP) method to the variational inequality method (VI) (Nemirovski, 1994) (i.", "startOffset": 74, "endOffset": 92}], "year": 2012, "abstractText": "We study the problem of multiple kernel learning from noisy labels. This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples. Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem, and presenting a minimax formulation. We develop an efficient algorithm for solving the related convex-concave optimization problem with a fast convergence rate of O(1/T ) where T is the number of iterations. Empirical studies on UCI data sets verify both the effectiveness and the efficiency of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}