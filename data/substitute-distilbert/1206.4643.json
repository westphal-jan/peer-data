{"id": "1206.4643", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty", "abstract": "we consider markov decision processes under parameter uncertainty. previous studies all disagree about the case that uncertainties among different states are uncoupled, which leads to conservative solutions. in contrast, we introduce an intuitive concept, termed \" lightning does not cease twice, \" to model coupled uncertain parameters. specifically, we require that climate system can deviate from its nominal parameters after a bounded duration of times. we suggest probabilistic guarantees indicating that this model represents real life situations and devise tractable questions for computing adaptation control policies using warped concept.", "histories": [["v1", "Mon, 18 Jun 2012 15:19:07 GMT  (390kb)", "http://arxiv.org/abs/1206.4643v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.GT cs.SY", "authors": ["shie mannor", "ofir mebel", "huan xu"], "accepted": true, "id": "1206.4643"}, "pdf": {"name": "1206.4643.pdf", "metadata": {"source": "META", "title": "Lightning Does Not Strike Twice:  Robust MDPs with Coupled Uncertainty", "authors": ["Shie Mannor", "Ofir Mebel"], "emails": ["shie@ee.technion.ac.il", "ofirmebel@gmail.com", "mpexuh@nus.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "Markov decision processes (MDPs) are widely used tools to model sequential decision making in stochastic dynamic environments (e.g., Puterman, 1994; Sutton & Barto, 1998). Typically, the parameters of these models (reward and transition probability) are estimated from finite and sometimes noisy data, so they often deviate from their true value. Such deviation, termed \u201cparameter uncertainty,\u201d can cause the performance of the \u201coptimal\u201d policies to degrade significantly, as demonstrated in Mannor et al. (2007).\nMany efforts have been made to alleviate the effect of parameter uncertainty in MDPs (e.g., Nilim & El Ghaoui, 2005; Iyengar, 2005; Givan et al., 2000; Ep-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nstein & Schneider, 2007; Bagnell et al., 2001). Inspired by the so-called \u201crobust optimization\u201d framework (Ben-Tal & Nemirovski, 1998; Bertsimas & Sim, 2004; Ben-Tal et al., 2009), the common approach regards the MDP\u2019s uncertain parameters r and p as fixed but unknown elements of a known set U , often termed as the uncertainty set, and ranks solutions based on their performance under (respective) worst parameter realization.\nPrevious study in robust MDPs typically assumes that there exists no coupling between uncertain parameters of different states. That is, U = \u220f s\u2208S Us, where Us is the uncertainty set for (ps, rs), the transition probabilities and rewards for each state. Such an assumption is restrictive, and can often lead to conservative solutions as it essentially assumes that all parameters take worst possible realization simultaneously \u2013 an event that rarely happens in practice.\nIn this paper we propose and analyze a robust MDP approach that allows coupled uncertainty to mitigate conservativeness of the uncoupled uncertainty model. Specifically, we consider a setup which we term LDST after famous proverb \u201cLightning Does not Strike Twice:\u201d while each parameter may be any element of the uncertainty set, the total number of states whose parameters deviate from their nominal value is bounded. The motivation is intuitive: suppose the parameter of each state deviates with a small probability, and all states are independent, then the total number of states with deviated parameters will be small. Hence, protection against the scenario that only a limited number of states deviate from their nominal parameter is sufficient to ensure that the solution is very likely to be robust to the real parameters.\nThe LDST approach is conceptually intuitive and sim-\nple, yet with added modeling power. Indeed, it is backed by probabilistic guarantees that hold under reasonable conditions. While the LDST approach provides a flexible way to control the conservativeness of robust MDPs, it remains computationally friendly: it is tractable even when both reward parameters and transition probabilities are uncertain, for one of the two setups which we will consider. For the other one that is computationally more challenging, it is tractable in the special case where only the reward parameters are uncertain.\nOne advantage of the LDST approach is that it requires no distribution information. This is in sharp contrast to some other variants of MDP methods also aiming to mitigate conservativeness, including Bayesian reinforcement learning (Strens, 2000; Poupart, 2010), chance constrained MDP (Delage & Mannor, 2010), and distributionally robust MDP (Xu & Mannor, 2010), all assuming a-priori information on the distribution of the system parameters.\nIn Section 2 we give background on MDPs and define the LDST formulation. In Section 3 we define the non-adaptive model and prove that it is computationally hard in the general case, but tractable if only the rewards are subject to deviations. In Section 4 we define the adaptive model and give tractable algorithms for solving both finite and infinite-horizon problems. In Section 5 we define the concept of fractional deviation and prove that the adaptive model is solvable also when fractional deviations are considered. Simulation of the adaptive model is demonstrated in Section 6 and a conclusion is given in Section 7."}, {"heading": "2. Preliminaries", "text": "In this section we briefly explain our notations, some preliminaries about MDP, and the setups that we investigate. Following standard notations from Puterman (1994), we define a Markov decision process as a 6-tuple < T, \u03b3,S,A, p, r >: here T , possibly infinite, is the decision horizon; \u03b3 \u2208 (0, 1] is the discount factor, and can be 1 only when T <\u221e. The state space S and the action space A are both finite. In state s \u2208 S, the decision maker can pick an action a \u2208 A, and obtains a reward of r(s, a), and the next state will be s\u2032 \u2208 S with a probability p(s\u2032|s, a). We use a subscript s to denote the parameter for state s. For example, rs is the reward vector for different actions in state s. We assume that the initial state is distributed according to \u03b1(\u00b7), i.e., the initial state is s with probability \u03b1(s). The sets of all history-dependent randomized strategies, all Markovian randomized strategies, and all Markovian deterministic strategies are denoted by \u03a0HR, \u03a0MR and\n\u03a0MD, respectively. The decision goal is to maximize the performance X(\u03c0, p, r) , E\u03c0{ \u2211T t=1 \u03b3\nt\u22121r(st, at)} , i.e., the expected accumulative reward, under parameter p and r.\nAn uncertain MDP (uMDP) is defined as a tuple < T, \u03b3,S,A, p0, r0,U , D >. In contrast to standard MDP, the true parameters ptrue and rtrue are unknown. Instead, for each state s, we know p0s and r 0 s termed nominal parameters hereafter, which are essentially point estimations of the parameters ps and rs, as well as an uncertainty set Us such that (ps, rs) \u2208 Us. As standard in robust optimization and robust MDPs, we assume that the nominal parameters p0s and r 0 s belong to the uncertainty set Us, and the uncertainty sets {Us}s\u2208S are convex. As discussed above, previous work in robust MDP all focused on the case that parameter realization is uncoupled, i.e., (p, r) can take any value in U , \u220f s Us. In this paper we add the possibility of coupling between uncertainties to avoid being overly conservative. Following the proverb \u201clightning does not strike twice,\u201d we consider uncertainty models that do not allow disaster to happen too frequently. In particular, besides the constraint that the parameters belong to respective uncertainty sets, we further require that the number of states whose parameters deviate from their nominal values is bounded by the given threshold D. An illustration of different effective uncertainty sets the model can produce is given in Figure 1 for three states, where it can be seen that\nthe model spans both the model oblivious to uncertainty, and the conservative model of uncoupled uncertainty. We will later demonstrate that the values of D in-between these extremes are both advantageous, and tractable.\nIntuitively, LDST can be thought of as a two-player zero-sum game, where the decision maker chooses a policy, and an adversarial Nature picks an admissible parameter realization attempting to minimize the reward of the decision maker. Two models are possible, leading to two forms that we will investigate in the sequel. In the first model, termed non-adaptive model, depending on the control policy, Nature chooses the parameter realization of all states once and fixed thereafter, thus leading to a single stage game. In the second model, both players adapt their actions to the trajectory of the MDP, leading to a sequential game. To illustrate the difference, consider the following example: suppose at state s0, taking an action a will lead the system to either state s1 or state s2, both with positive probabilities, and that only the parameter of one state is allowed to deviate. In the adaptive uncertainty model the following deviation is allowed: deviate s1 if the system reaches s1, and deviate s2 if the system reaches s2. In contrast, in the non-adaptive model the state whose parameters deviate must be fixed, and can not depend on whether the system reaches s1 or s2."}, {"heading": "3. Non-Adaptive Uncertainty Model", "text": "In this section, we focus on the non-adaptive case. As mentioned above, the uncertain parameters, while unknown, are fixed a priori and do not depend on the trajectory of the MDP. Similarly, the decision maker has to fix a strategy and can not adapt to the realization of the parameters. Equivalently, this means that the decision maker does not observe whether or when the parameters deviate.\nOur goal is to find a strategy that performs best under the worst admissible parameter realization. That is, to solve the following problem:\nmax \u03c0\u2208\u03a0HR min (p,r)\u2208UD X(\u03c0, p, r) (1)\nwhere: UD , { (p, r) \u2208 U \u2223\u2223\u2223\u2223\u2223\u2211 s\u2208S 1(ps,rs)6=(p0s,r0s) \u2264 D } .\nTo put it in words, the set of admissible parameters UD is defined as follows: the parameters of no more than D states can deviate from their nominal values, and the deviated parameters belong to their respective uncertainty set Us.\nThe main motivation of this formulation is that often\nin practice, the correlation between the parameters of different states is weak, or even completely independent. In such a case, protection against the scenario that only a limited number of states deviate from their nominal parameter is sufficient to ensure that the solution is very likely to be robust to the real parameters. To illustrate this intuition, we have the following theorem which considers a generative deviation model where the parameters of different states are independent. The proof is deferred to Appendix A.\nTheorem 1. Suppose the parameters of different states deviate independently, with a probability \u03b1s for s \u2208 S. Then with probability at least 1 \u2212 \u03b4 we have (p, r) \u2208 UD\u2032 , where D\u2032 , \u2211 s\u2208S \u03b1s +\n1 3 log(1/\u03b4)\n( 1 + \u221a 1 + 18 \u2211 s\u2208S \u03b1s\nlog(1/\u03b4)\n) ."}, {"heading": "3.1. Computational Complexity", "text": "In general, the non-adaptive case \u2013 Problem 1 \u2013 is computationally hard. To make this statement formal, we consider the following \u201cyes/no\u201d decision problem.\nDecision Problem. Given a Markov decision process with |S| = n and |A| = m, D \u2208 [0 : n], Us for all s \u2208 S and \u03b2 \u2208 R. Does there exist \u03c0 \u2208 \u03a0HR such that\nmin (p,r)\u2208UD\nX(\u03c0, p, r) \u2265 \u03b2 ? (2)\nWe denote the decision problem by L(\u03a0HR). Similarly we can define L(\u03a0MR) and L(\u03a0MD). We next show that answering the decision problem is hard even for very simple uncertainty set U . This immediately implies that finding a strategy \u03c0 that satisfies (2) is computationally difficult. The proof, deferred to Appendix B, is based on reduction from Vertex Cover Problem.\nTheorem 2. Suppose for any s \u2208 S, Us is a line segment, i.e., the convex combination of two points. Then, the problems L(\u03a0HR), L(\u03a0MR) and L(\u03a0MD) are NP-hard."}, {"heading": "3.2. Reward-Uncertainty Case", "text": "While in general Problem 1 can be intractable, we show that if only the reward parameters are subject to uncertainty then the problem is solvable in polynomial time. We first define the concept of tractable uncertainty set:\nDefinition 1. A set U is called tractable, if in polynomial time, the following can be solved for any c:\nMinimize: c>x; s.t. x \u2208 U .\nUsing this formulation we state the main result of this section. The proof is given in Appendix C in the supplementary material.\nTheorem 3. Suppose that only the reward is subject to uncertainty. If for all s \u2208 S, Us is a tractable uncertainty set, then Problem 1 can be solved in polynomial time.\nWhen uncertainty sets are polytopes or ellipsoids as often the case in practice, we can convert Problem 1 into easier convex optimization problems such as LP or quadratic programming, for which large scale problems can be solved using standard solvers in reasonable time. See Appendix D and E for detail."}, {"heading": "4. Adaptive Uncertainty Model", "text": "This section is devoted to the adaptive case, i.e., the parameter realization \u2013 in particular the choice of deviating states \u2013 depends on the history trajectory of the MDP. Moreover, the decision maker observes parameter deviation retrospectively. That is, in each decision stage the decision maker chooses an action, after which the true parameters are realized and observed by the decision maker. Hence the decision maker is aware of the occurrence of parameter deviation, and his strategy is allowed to depend on such information.\nIn practice, there are two cases where the agent can confidently determines a deviation. The first case is where the parameter deviation is accompanied by external signals (e.g., lighting). For example, consider a problem of determining the route for a helicopter, the deviation is when there is a storm, and it is reasonable to assume that the agent can observe the storm if it really happens. A similar example is portfolio optimization where the parameter deviation is due to market crash or company bankrupcy, which certainly can be observed. The second case, where there is no external signal, is the case where the support of the nominal parameter and deviation barely intersects (in the probabilistic sense). Take the simulation in Section 6 as an example, although under the nominal parameter, it is possible that the number of customers come is large, the probability is very small. Recurrence of such events is more unlikely. Hence, the agent can count each of these events as one parameter deviation, and allow the Nature one or two extra chances of parameter deviation. Solving the resulting adaptive case would yield a solution that with overwhelming probabilities is robust to fixed number of parameter deviation, yet not overly conservative. Notice that a special example of the second case is when the supports are disjoint."}, {"heading": "4.1. Finite-horizon case", "text": "Mathematically, the finite-horizon adaptive case can be formulated as the following decision problem 1,\nmax a1\u2208A min (p1,r1)\u2208Us1 max a2\u2208A min (p2,r2)\u2208Us2\n\u00b7 \u00b7 \u00b7 (3)\nmax aT\u2208A min (pT ,rT )\u2208UsT\nE { T\u2211 t=1 rt(st, at) } ;\nSubject to: T\u2211 t=1 1(pt,rt)6=(p0t ,r0t ) \u2264 D.\nHere, st is the randomized state at decision epoch t. Hence, for all s, s\u2032 \u2208 S we have Pr(st+1 = s\u2032|st = s, at) = pt(s\n\u2032|s, at). We remark that in the adaptive case we require that the number of decision stages (as opposed to states) where the parameter deviates is bounded by D. Hence, the parameters of a state are allowed to take different values for multiple visits. Similarly to Theorem 1, Formulation (3) is justified by the following probabilistic guarantee. The proof is almost identical to that of Theorem 1, and omitted.\nTheorem 4. Suppose the parameters of different stages deviate independently, with a probability \u03b1t for t = 1, \u00b7 \u00b7 \u00b7 , T . Then with probability at least 1 \u2212 \u03b4 we have that the total number of deviations\u2211T t=1 1(pt,rt)6=(p0t ,r0t ) is upper bounded by \u2211T t=1 \u03b1t +\n1 3 log(1/\u03b4)\n( 1 + \u221a 1 + 18 \u2211T t=1 \u03b1t\nlog(1/\u03b4)\n) .\nThe next theorem, which is the main result of this subsection, asserts that in sharp contrast to the nonadaptive case, the adaptive-case can be solved via backward induction.\nTheorem 5. Let vT+1(s, d) = 0 for all s \u2208 S and d \u2208 [0 : D], and define for t = 1, \u00b7 \u00b7 \u00b7 , T ,\nvt(s, d) , max a\u2208A { min [ qt(s, d, a, p 0 s, r 0 s), min (p,r)\u2208Us qt(s, d\u2212 1, a, p, r) ]} , if d \u2265 1;\nmax a\u2208A\nqt(s, d, a, p 0 s, r 0 s), if d = 0;\na\u2217t (s, d) , arg max a\u2208A { min [ qt(s, d, a, p 0 s, r 0 s), min (p,r)\u2208Us qt(s, d\u2212 1, a, p, r) ]} , if d \u2265 1;\narg max a\u2208A\nqt(s, d, a, p 0 s, r 0 s), if d = 0;\n1In a slight abuse of notation we write (pt, rt) instead of (pst , rst).\nwhere qt(s, d, a, p, r) , \u2211 s\u2032 p(s\n\u2032|s, a)vt+1(s\u2032, d) + r(s, a). Then the optimal strategy to Problem 3 is to take a\u2217t (s, d) at stage t, state s, where stage-parameters are allowed to deviate at most d times from this stage on.\nBefore proving the theorem, we first explain our intuition: we can re-formulate Problem 3 as a sequential game of perfect information on an augmented statespace. More precisely, Problem 3 can be regarded as a sequential game, where there are two players, namely the decision maker who attempts to maximize the total expected reward by choosing actions sequentially, and Nature who aims to minimize the total expected reward by choosing parameter-realization. Thus, we can expand the decision horizon into 2T where the decision maker makes move in each odd decision stage, and Nature makes move in each even decision stage. Furthermore, both the decision maker and Nature are aware of the number of \u201cdeviated visits\u201d so far, which affect their optimal strategies. Hence, we need to augment the state-space with this information. Thus, we construct the following game.\nConsider the following augmented state space, where we have two types of states: the decision maker states and the Nature states. The set of decision maker states is given by\nSD = S \u00d7 [0 : D],\nand the set of Nature states is given by\nSN = S \u00d7 [0 : D]\u00d7A.\nThe entire (augmented) state space is thus S = SD \u22c3 SN .\nA zero-sum stochastic game with time horizon 2T between two players on S is constructed as follows. In each decision epoch 2t\u22121 for t = 1, \u00b7 \u00b7 \u00b7 , T , the system state belongs to SD and only the decision maker can choose an action a \u2208 A. Let the state be (s, d) where s \u2208 S and d \u2208 [0 : D], and suppose the decision maker takes action a \u2208 A, then the next state will be (s, d, a), with both players get zero-reward.\nIn each decision epoch 2t for t = 1, \u00b7 \u00b7 \u00b7 , T , the system state belongs to SN and only Nature can choose an action. The action set for Nature is the following: if d \u2265 1, Nature can either pick the nominal parameter (p0s, r 0 s), in which case the next state will be (s\n\u2032, d) where s\u2032 is a random variable following the probability distribution p0(s\u2032|s, a), and the decision maker gets a reward r0(s, a), which Nature simultaneously loses; alternatively, Nature can pick a parameter (p\u2032, r\u2032) \u2208 Us, in which case the next state will be (s\u2032, d\u22121) where s\u2032\nis a random variable following the probability distribution p\u2032(s\u2032|s, a), and the decision maker gets a reward r\u2032(s, a), which Nature, again, simultaneously loses. If d = 0, Nature can only pick the nominal parameter (p0s, r 0 s). An example of this construction is given graphically in Figure 2. It is easy to see that solving Problem 3 is equivalent to solving the aforementioned stochastic game. This leads to the proof of Theorem 5.\nProof of Theorem 5. We prove the theorem by investigating the aforementioned stochastic game, and showing that a Nash equilibrium exists for the stochastic game, where the decision maker\u2019s move is a\u2217t (s, d). Since the solution to the stochastic game and the solution to Problem 3 coincide (Filar & Vrieze, 1996), the theorem thus follows.\nWe now construct the Nash equilibrium. Let the decision maker take the strategy a\u2217t (s, d) at state (s, d) \u2208 SD in decision epoch 2t\u2212 1. Let Nature take the following action at state (s, d, a) \u2208 SN in decision epoch 2t: do not deviate the parameters (i.e, pick parameter (p0s, r 0 s)) if either d = 0, or qt(s, d, a, p 0 s, r 0 s) \u2264 min(p,r)\u2208Us qt(s, d\u2212 1, a, p, r); otherwise, pick parameters arg min(p,r)\u2208Us qt(s, d\u2212 1, a, p, r).\nObserve that if Nature fixes this strategy, the stochastic game reduces to a standard MDP for the decision maker to maximize his total reward, in which case his optimal strategy is take a\u2217t (s, d) at each (s, d) \u2208 SD at the decision epoch 2t \u2212 1. On the other hand, if the decision maker fixes his strategy, then for Nature to minimize the total-reward for the decision maker, it is easy to see that the aforementioned strategy is optimal. Thus, the aforementioned pair of strategies is a Nash equilibrium, which implies the theorem.\nAlgorithm 1 illustrates how Theorem 5 can be used to compute optimal policy for the non-adaptive model in the LDST setup. Its time-complexity is\nO ( TD|S||A| ( |S| + M )) where M is defined as the\nmaximal computational effort in performing a single minimization of qt(s, d, a, p, r) in (p, r) \u2208 Us for any s \u2208 S, t \u2208 [1 : T ]."}, {"heading": "4.2. Discounted-Reward Infinite Horizon Case", "text": "We extend the formulation of the adaptive case into the infinite horizon case, with discounted total reward criterion. Depending on whether the number of parameter deviations is also discounted, we formulate and investigate the following two setups:\nSetup A \u2013 Non-Discounted Deviates:\nmax a1\u2208A min (p1,r1)\u2208Us1 max a2\u2208A min (p2,r2)\u2208Us2\n\u00b7 \u00b7 \u00b7 (4)\nmax at\u2208A min (pt,rt)\u2208Ust\n\u00b7 \u00b7 \u00b7E { \u221e\u2211 t=1 \u03b3t\u22121rt(st, at) } ;\nSubject to: \u221e\u2211 t=1 1(pt,rt)6=(p0t ,r0t ) \u2264 D.\nSetup B \u2013 Discounted Deviates:\nmax a1\u2208A min (p1,r1)\u2208Us1 max a2\u2208A min (p2,r2)\u2208Us2\n\u00b7 \u00b7 \u00b7 (5)\nmax at\u2208A min (pt,rt)\u2208Ust\n\u00b7 \u00b7 \u00b7E { \u221e\u2211 t=1 \u03b3t\u22121rt(st, at) } ;\nSubject to: \u221e\u2211 t=1 \u03b2t\u221211(pt,rt) 6=(p0t ,r0t ) \u2264 D.\nAlgorithm 1 Backward Induction to solve the adaptive case\nInput: uMDP < T,S,A, p0, r0,U , D > Initialize VT+1(s, d) = 0 \u2200s \u2208 S, d \u2208 [0 : D]. for t = T downto 1 do\nfor d = 0 to D do for s \u2208 S do\nInitialize bestAction = \u03c6 Initialize bestActionV alue = \u2212\u221e for a \u2208 A do {Nominal value} Initialize actionV alue = qt(s, d, a, p 0 s, r 0 s)\nif d \u2265 1 then deviatedV alue =\nmin(p,r)\u2208Us qt(s, d\u2212 1, a, p, r) if actionV alue > deviatedV alue then actionV alue = deviatedV alue\nend if end if if bestActionV alue < actionV alue then bestActionV alue = actionV alue bestAction = a\nend if end for Vt(s, d) = bestActionV alue \u03c0t(s, d) = bestAction\nend for end for\nend for return \u03c0t(s, d) \u2200t \u2208 [1 : T ], s \u2208 S, d \u2208 [0 : D]\nIn Setup A the total number of stages where parameters deviate is bounded. In contrast, in Setup B, similarly to future reward, future parameter deviation is discounted. Setup A is easy to compute as we can apply the same technique to augment the statespace with the remaining budget of \u201cNature\u201d. Setup B further requires discretization of the remaining budget. Both setups\u2019 optimal value functions admit implicit equations similar to the Bellman Equations, as shown in Appendix F in the supplementary material, and thus are solvable in the same methods as classic infinite-horizon MDPs."}, {"heading": "5. Continuous Deviations", "text": "This section is motivated from the following setup: in each stage, the real parameter is likely to slightly deviate from the nominal one, whereas large deviation is rare. Notice that the previous approach that bounds the number that parameter deviates does not apply to this setup. Hence, we extend previous results to the continuous deviation case, i.e., Nature is allowed to\nperform \u201cfractional\u201d deviations.\nMore specifically, suppose the nominal parameters are p0, r0 and the uncertainty set is U , and that Us is star-shaped2 in respect to (p0s, r 0 s) for each s \u2208 S. If the realized parameter is (p, r) \u2208 U , then Nature is considered to have consumed a budget of deviation b(p, r, p0, r0,U) such that\nb(p, r, p0, r0,U) , min{\u03b1 \u2265 0 | \u2203(\u03b4p, \u03b4r) \u2208 \u2206U : (p, r) = (p0 + \u03b1\u03b4p, r0 + \u03b1\u03b4r)},\nwhere \u2206U , {(\u03b4p, \u03b4r)|(p0 + \u03b4p, r0 + \u03b4r) \u2208 U}.\nSince (p, r) \u2208 U , we get b(p, r, p0, r0,U) \u2264 1. Roughly speaking, a \u201csmall\u201d deviation is considered \u201cless expensive\u201d compared to a large one. Thus, we can formulate the continuous extension as follows, with \u03b3 \u2264 \u03b2 \u2264 1, and \u03b3 < 1.3\nContinuous Deviations:\nmax a1\u2208A min (p1,r1)\u2208Us1 max a2\u2208A min (p2,r2)\u2208Us2\n\u00b7 \u00b7 \u00b7 (6)\nmax at\u2208A min (pt,rt)\u2208Ust\n\u00b7 \u00b7 \u00b7E { \u221e\u2211 t=1 \u03b3t\u22121rt(st, at) } ;\nSubject to: \u221e\u2211 t=1 \u03b2t\u22121b(pt, rt, p 0 t , r 0 t ,Ust) \u2264 D.\nSimilarly to Problems 4 and 5 from the previous section, Problem 6 is solvable (approximately) using a discretization scheme. The statement and proof are given in Appendix G in the supplementary material.\nWe next provide a probabilistic guarantee of the continuous model, for the finite horizon case. The proof is similar to the previous probabilistic guarantees and is given in Appendix H in the supplementary material.\nTheorem 6. Suppose the amount of parameter deviation, b(pt, rt, p 0 t , r 0 t ,Ust) is independent with a mean \u03b1t. Then with probability at least 1 \u2212 \u03b4 we have the that total amount of deviation, \u2211T t=1 b(pt, rt, p 0 t , r\n0 t ,Ust), is upper bounded by\u2211T\nt=1 \u03b1t + 1 3 log(1/\u03b4)\n( 1 + \u221a 1 + 18 \u2211T t=1 \u03b1t\nlog(1/\u03b4)\n) ."}, {"heading": "6. Simulations", "text": "We present simulation results of the proposed method under a setup closely resembles our motivating example, i.e., parameters randomly deviate from their nominal values with a small probability. We use a scenario\n2A set U is star-shaped w.r.t. u0 if for any u \u2208 U , the line segment between u and u0 also belongs to U .\n3Notice that both the finite horizon case, and the two infinite horizon cases investigated, can be formulated in this way.\nbased on the Single-Product Stochastic Inventory Control problem described in Puterman (1994). Here the states represent the number of items in the inventory, with maximal capacity of MAXSTOCK items. Every day an order is made for new items at the cost of STOREPRICE each. A price is paid for holding the items in the inventory (both old and new), which is a function of the number of items HOLDINGCOST(n). A Poisson-distributed number of customers, with expected value of NUMCUSTOMERS place orders for the item and pay CUSTOMERPRICE for each product sold. If the demand cannot be met, the store is \u201cfined\u201d total of PENALTY(n) where n is the number of customers whose demand cannot be met. The simulation is run for T days, and any unused stock at time T + 1 is wasted.\nThe deviation we added to this scenario is Rush: the maximal number of customers, MAXSTOCK, arrive simultaneously on the same day. Thus the uncertainty set Ust consists of two points: regular Poisson arrival and Rush. Note that a Rush affects both the transition probabilities and the rewards.\nWe simulated the above scenario for different values of initially assumed number of deviations d0\n4, and applied random occurrences of Rush with probability prush for each day independently. We also added the expected value of the optimal MDP policy aware of the occurrence of Rush for comparison. The results 5 are given in Figure 3. It can be seen that with a suitable choice of d0 such as expected number of deviation, the performance of LDST\u2019s policy is comparable to the performance of the optimal policy for each scenario, while both nominal (d0 = 0) and conservative, uncoupled uncertainty (d0 = T ) polices give worse performance as expected."}, {"heading": "7. Conclusion", "text": "We proposed a new robust MDP framework, termed \u201cLightning Does not Strike Twice,\u201d to model the case that uncertain parameters among different states are coupled by sharing a common pool of \u201cbudget\u201d of deviation. This leads to a tractable formulation that provides a flexible tradeoff between risk and value, which can be adjusted by tuning the \u201cbudget.\u201d\n4Note that for d0 = 0 we get the nominal policy, and for d0 = T we get the conservative, uncoupled uncertainty policy.\n5The parameters chosen for simulation are T = 100, MAXSTOCK = 20, STOREPRICE = 5, CUSTOMERPRICE = 50, NUMCUSTOMERS = 10, HOLDINGCOST (n) = 2n2, PENALTY (n) = 7n2, initially the stock is empty.\nOne obstacle that prevents robust MDPs from being widely applied is that the solution tends to be conservative \u2013 an outcome of the traditional formulation that uncertainties are uncoupled. On the other hand, coupled uncertainty in MDPs is in general computationally difficult. Therefore, it is important to identify sub-classes of coupled uncertainty that are flexible enough to overcome conservativeness, yet remains computationally tractable. This paper is the first of such attempts, which we hope will facilitate the applicability of robust MDPs."}, {"heading": "Acknowledgments", "text": "S. Mannor has received funding from the Israel Science Foundation (contract 890015) and the European Unions Seventh Framework Programme (FP7/20072013) under grant agreement no. 249254. H. Xu is\npartially supported by the National University of Singapore under startup grant R-265-000-384-133."}], "references": [{"title": "Solving uncertain Markov decision problems", "author": ["A. Bagnell", "A. Ng", "J. Schneider"], "venue": "Technical Report CMU-RI-TR-01-25,", "citeRegEx": "Bagnell et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2001}, {"title": "Robust Optimization", "author": ["A. Ben-Tal", "L. El Ghaoui", "A. Nemirovski"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "Percentile optimization for Markov decision processes with parameter uncertainty", "author": ["E. Delage", "S. Mannor"], "venue": "Operations Research,", "citeRegEx": "Delage and Mannor,? \\Q2010\\E", "shortCiteRegEx": "Delage and Mannor", "year": 2010}, {"title": "Learning under ambiguity", "author": ["Epstein", "Larry G", "M. Schneider"], "venue": "Review of Economic Studies,", "citeRegEx": "Epstein et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Epstein et al\\.", "year": 2007}, {"title": "Competitive Markov Decision Process", "author": ["J. Filar", "K. Vrieze"], "venue": null, "citeRegEx": "Filar and Vrieze,? \\Q1996\\E", "shortCiteRegEx": "Filar and Vrieze", "year": 1996}, {"title": "Boundedparameter Markov decision processes", "author": ["R. Givan", "S. Leach", "T. Dean"], "venue": "Artificial Intelligence,", "citeRegEx": "Givan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Givan et al\\.", "year": 2000}, {"title": "Robust dynamic programming", "author": ["G. Iyengar"], "venue": "Mathematics of Operations Reseasrch,", "citeRegEx": "Iyengar,? \\Q2005\\E", "shortCiteRegEx": "Iyengar", "year": 2005}, {"title": "Bias and variance approximation in value function estimates", "author": ["S. Mannor", "D. Simeste", "P. Sun", "J. Tsitsiklis"], "venue": "Management Science,", "citeRegEx": "Mannor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2007}, {"title": "Robust control of Markov decision processes with uncertain transition matrices", "author": ["A. Nilim", "L. El Ghaoui"], "venue": "Operations Research,", "citeRegEx": "Nilim and Ghaoui,? \\Q2005\\E", "shortCiteRegEx": "Nilim and Ghaoui", "year": 2005}, {"title": "Markov Decision Processes", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q1994\\E", "shortCiteRegEx": "Puterman and L.", "year": 1994}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "In ICML, pp", "citeRegEx": "Strens,? \\Q2000\\E", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Distributionally robust Markov decision processes", "author": ["H. Xu", "S. Mannor"], "venue": "In NIPS, pp. 2505\u20132513,", "citeRegEx": "Xu and Mannor,? \\Q2010\\E", "shortCiteRegEx": "Xu and Mannor", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "Such deviation, termed \u201cparameter uncertainty,\u201d can cause the performance of the \u201coptimal\u201d policies to degrade significantly, as demonstrated in Mannor et al. (2007).", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": "Inspired by the so-called \u201crobust optimization\u201d framework (Ben-Tal & Nemirovski, 1998; Bertsimas & Sim, 2004; Ben-Tal et al., 2009), the common approach regards the MDP\u2019s uncertain parameters r and p as fixed but unknown elements of a known set U , often termed as the uncertainty set, and ranks solutions based on their performance under (respective) worst parameter realization.", "startOffset": 58, "endOffset": 131}, {"referenceID": 10, "context": "This is in sharp contrast to some other variants of MDP methods also aiming to mitigate conservativeness, including Bayesian reinforcement learning (Strens, 2000; Poupart, 2010), chance constrained MDP (Delage & Mannor, 2010), and distributionally robust MDP (Xu & Mannor, 2010), all assuming a-priori information on the distribution of the system parameters.", "startOffset": 148, "endOffset": 177}], "year": 2012, "abstractText": "We consider Markov decision processes under parameter uncertainty. Previous studies all restrict to the case that uncertainties among different states are uncoupled, which leads to conservative solutions. In contrast, we introduce an intuitive concept, termed \u201cLightning Does not Strike Twice,\u201d to model coupled uncertain parameters. Specifically, we require that the system can deviate from its nominal parameters only a bounded number of times. We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies.", "creator": "LaTeX with hyperref package"}}}