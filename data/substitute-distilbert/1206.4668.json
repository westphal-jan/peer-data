{"id": "1206.4668", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Approximate Principal Direction Trees", "abstract": "we introduce a new spatial node structure for high dimensional data called the \\ emph { approximate comparative direction tree } ( apd tree ) that adapts to the intrinsic weights of the lattice. atm algorithm ensures vector - quantization accuracy similar to that of computationally - expensive pca trees with similar time - complexity to simulations of lower - accuracy rp tables.", "histories": [["v1", "Mon, 18 Jun 2012 15:33:25 GMT  (880kb)", "http://arxiv.org/abs/1206.4668v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["mark mccartin-lim", "andrew mcgregor 0001", "rui wang 0003"], "accepted": true, "id": "1206.4668"}, "pdf": {"name": "1206.4668.pdf", "metadata": {"source": "META", "title": "Approximate Principal Direction Trees", "authors": ["Mark McCartin-Lim", "Andrew McGregor", "Rui Wang"], "emails": ["markml@cs.umass.edu", "mcgregor@cs.umass.edu", "ruiwang@cs.umass.edu"], "sections": [{"heading": null, "text": "APD trees use a small number of powermethod iterations to find splitting planes for recursively partitioning the data. As such they provide a natural trade-off between the running-time and accuracy achieved by RP and PCA trees. Our theoretical results establish a) strong performance guarantees regardless of the convergence rate of the powermethod and b) that O(log d) iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is d. We demonstrate this trade-off and the efficacy of our data structure on both the CPU and GPU."}, {"heading": "1. Introduction", "text": "Spatial partition trees are data structures that hierarchically subdivide a set of data points with the goal that resulting partitions contain \u201csimilar\u201d points. A ubiquitous example is the k-d tree, which is widely used in many unsupervised learning methods including classification, regression, nearest-neighbor finding, and vector quantization.\nAt level L of the tree, the data has been partitioned into 2L different classes. Every time we subdivide a set of points, we do so with the hope of reducing the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n000000\n0000000\n0000000\n0000000\n0000000\n0000000\n0000000\n0000000\n0 0 0 0 0 0 ) D r r r\neeeeeeee\neeeeeeeeeeeeee\neeeeeee\nFigure 1. Convergence of APD Trees to PCA Trees on the MNIST Dataset (depth 4 trees): The VQ error achieved by using APD trees with only a few powermethod iterations is close to that of PCA trees. This substantially improves upon the error for RP trees without significant computational overhead.\nthe average distance between points in the same subsets as much as possible. The goal is similar to that in k-means clustering, but unfortunately, k-means clustering is an NP-hard problem even when k = 2 (Dasgupta, 2008).\nWe know that k-d trees are vulnerable to the so-called curse of dimensionality \u2013 if the dimensionality of our data is D, then we may have to traverse O(D) levels to halve the average diameter. This is shown to be true even when the intrinsic dimensionality of the data is low (Verma et al., 2009). Thus, k-d tree are not well suited to applications that involve high-dimensional data, because to produce a good classifier, we would need to branch our tree O(2D) times. Similarly, it has been shown that using a k-d tree to do nearestneighbor queries in high-dimensional data is not much more efficient than scanning every element (Lee & Wong, 1977).\nRecently, random-projection (RP) trees (Dasgupta & Freund, 2008) and PCA trees (Verma et al., 2009) have been proposed as alternatives to k-d trees that adapt to intrinsic dimensionality. RP trees and PCA trees\ndiffer from k-d trees in that their splitting planes are not restricted to being axis-aligned. Thus, unlike k-d trees, they can adapt to the covariance of the data (see Figure 2).\nOf the two choices, RP trees are appealing because they can be computed very efficiently. Each subdivision in an RP tree is determined by a randomly-chosen hyperplane. PCA trees are significantly more expensive to compute, because the normal of each hyperplane is chosen by doing principal component analysis (PCA) on the data to find the principal direction. This requires one to compute the covariance matrix and perform eigendecomposition, or to do singular value decomposition (SVD). However, PCA trees perform significantly better than RP trees at reducing the average diameter, as demonstrated in (Verma et al., 2009).\nAPD Trees. We propose a new spatial partition tree, the approximate principal direction tree or APD trees, which generalize the idea behind RP trees, but perform almost as well as PCA trees when it comes to reducing average diameter with respect to intrinsic dimensionality.\nWhen choosing the normal of the hyperplane, APD trees start with random vectors like RP trees, but then apply a small number of power-method iterations (Burden & Faires, 2010) to these vectors. The power method is often used in data intensive applications to approximate principal eigenvectors (e.g., Google\u2019s PageRank (Wills, 2007)) and for spectral clustering (Lin & Cohen, 2010). However, it is important to note that in our application we are not simply concerned with getting a good approximation of the principal eigenvector. Rather, we are finding a hyperplane that will yield a good subdivision. This is important because the power method can be slow to converge when the first principal component has the same variance as the second principal component. However, this is not a concern for us since either component (or some linear combination thereof) would suffice for a good split.\nWe show strong performance guarantees with only one or two power-method iterations. See Figure 1 for an empirical illustration. Furthermore, when the intrinsic dimension of a dataset is d, we prove that O(log d) power-iterations are sufficient to produce trees that reduce average diameter at the same rate as PCA trees. Again, this is true even when the principal direction is not dominant in the data.\nOutline. In Section 2, we present the necessary definitions including that of local covariance dimension (Dasgupta & Freund, 2008) and what it means for a tree to adapt to intrinsic dimensionality. In Section 3, we presents the algorithm for building APD trees. Then, in Section 4, we prove that it adapts to intrinsic dimensionality with the average diameter converging at a similar rate to PCA trees. This is further demonstrated by our experimental results (Section 5), which also show that APD tree is much faster than the standard PCA tree algorithm. Finally, we also present a GPU-based implementation, which is even faster."}, {"heading": "2. Preliminary Definitions", "text": ""}, {"heading": "2.1. Average Diameter", "text": "For a given set of points S = {x1, . . . , xn} \u2282 RD we measure their similarity in terms of the average distance between points in the set. We will use the following notation:\nDefinition 2.1 (Average diameter of a set of points).\n\u2206a(S) = 1\n|S| \u221a \u2211 x,y\u2208S \u2016x\u2212 y\u20162\nor equivalently1\n\u2206a(S) =\n\u221a 2\n|S| \u2211 x\u2208S \u2016x\u2212mean(S)\u20162 ,\nwhere mean(S) = \u2211 x\u2208S x/|S|.\nWe are interested in partitioning S as {S1, S2} such that the average diameter of S1 and S2 is small.\nDefinition 2.2 (Average diameter over two sets).\n\u2206a(S1, S2) =\n\u221a \u22062a(S1)|S1|+ \u22062a(S2)|S2|\n|S1|+ |S2| ."}, {"heading": "2.2. Local Covariance Dimension", "text": "Several possible definitions of intrinsic dimension are discussed in (Verma et al., 2009). The one they use in\n1See, e.g., (Dasgupta & Freund, 2008).\ntheir analysis of RP trees and PCA trees, local covariance dimension is based upon a statistical representation of the data, and is thus well-suited to modeling data from machine learning problems. We will analysis APD trees using the same definition.\nRecall that the covariance matrix C \u2208 RD\u00d7D of a set of points S = {x1, . . . , xn} \u2282 RD with mean(S) = 0, can be written as C = XTX where X \u2208 Rn\u00d7D is the matrix whose i-th row equals xi. The idea behind local covariance dimension is as follows. The eigenvectors of the covariance matrix form an orthonormal basis for the data. If a small number of the eigenvectors describe almost all the variance in the data, then we know that the data is well represented by the subspace spanned by those eigenvectors. We can think of the dimension of this subspace as being the intrinsic dimension of the data.\nDefinition 2.3 (Local Covariance Dimension). Let \u03bb1 \u2265 \u03bb2 \u2265 . . . \u2265 \u03bbD be the eigenvalues of the covariance matrix of S = {x1, . . . , xn} \u2282 RD. We say S has local covariance dimension (d, \u03b5) for d \u2264 D and 0 < \u03b5 < 1 when\nd\u2211 i=1 \u03bbi \u2265 (1\u2212 \u03b5) D\u2211 i=1 \u03bbi .\nNotice that the local covariance dimension is also parametrized by \u03b5, which corresponds to how closely the subspace represents the data.\nWe say a method for partitioning S adapts to the intrinsic dimension of S if the resulting partition {S1, S2} satisfies \u2206a(S1, S2) < f(S) \u00b7 \u2206a(S) where f is independent of D, the extrinsic dimension."}, {"heading": "3. The APD Construction", "text": "A spatial partition tree is determined by the rule used to partition the points at each node of the tree. Before we discuss the partitioning rule used in APD trees, we review a general template for possible rules. This template is presented in Algorithm 1. If we assume S corresponds to the set of points assigned to a particular node in the tree, then it is obvious that this meta-algorithm can be applied recursively to produce a balanced binary tree.\nThe tree produced by this meta-algorithm is a hybrid of a BSP tree and a sphere tree (Devroye et al., 1996). If S is considered to contain \u201coutliers\u201d, we use a sphere to partition points that are close to the center of the data away from those that are not. We discuss this case further in Section 3.2. Otherwise, we use a hyperplane to partition the data. The choice of this hyper-\nAlgorithm 1 Tree construction meta-algorithm\nif S has outliers then D := {\u2016x\u2212mean(S)\u2016 | x \u2208 S} S1 := {x \u2208 S | \u2016x\u2212mean(S)\u2016 \u2264 median(D)} else Choose p \u2208 RD according to a splitting rule P := {x \u00b7 p | x \u2208 S} S1 := {x \u2208 S | x \u00b7 p \u2264 median(P )} end if S2 := S \\ S1 return {S1, S2}\nAlgorithm 2 APD splitting rule\np := a random vector s \u2208 RD for 1 . . . t do q := \u2211n h=1 (xh \u00b7 p)xTh\np := q/\u2016q\u2016 end for return p\nplane depends upon a splitting rule, which determines the normal for the hyperplane. Two existing splitting rules are the RP and PCA rules:\n1. RP rule: p is chosen uniformly at random from the unit sphere in RD.\n2. PCA rule: p is the principal eigenvector of C.\nAs we mentioned earlier, the downside of the PCA splitting rule is that computing p either requires one to compute the covariance matrix and perform eigendecomposition, or to do singular value decomposition on the data. Both are computationally intensive tasks. On the other hand, applying the RP splitting rule is computationally trivial. However, this splitting rule does not achieve as good accuracy as the PCA splitting rule."}, {"heading": "3.1. The New Splitting Rule", "text": "The new splitting rule we propose allows us to achieve similar accuracy to that achieved by the PCA rule without the computational overhead. The rule is based on the Power Method (Burden & Faires, 2010), a wellknown technique for approximating eigenvectors. See Algorithm 2. The technique translates nicely into a parallel algorithm we can implement on the GPU, as we will see in Section 5.\nTheorem 3.1. For i \u2208 [D], let pi \u2208 RD and \u03bbi > 0 be the (normalized) eigenvectors and eigenvalues of the\ncovariance matrix C. If s = \u2211D i=1 \u03b2ipi is the initial\nvector used in the APD splitting rule, then the vector returned by the splitting rule is\np = Cts\n\u2016Cts\u2016 =\n\u2211D i=1 \u03bb\nt i\u03b2ipi\u221a\u2211D\nj=1 \u03bb 2t j \u03b2 2 j\n.\nProof. The result follows from the observation that\u2211n h=1(xh \u00b7 p)xTh = (XTX)p = Cp.\nBelow are two properties of the APD splitting rule:\nGeneralization of RP and PCA: As t increases, p will converge to the principal eigenvector of the covariance matrix, i.e., the vector we would get if we were to use the PD splitting rule. On the other hand, when t = 0, p is a random unit vector in RD, and thus equivalent to if we were using the RP splitting rule. So intuitively, using the APD splitting rule gives us a trade-off between the RP splitting rule and the PCA splitting rule.\nDoesn\u2019t require convergence of power method: It is important to emphasize that the idea behind applying the power method in our setting is not to approximate the principal eigenvector per se. We need the splitting rule to be computed quickly and will primarily be interested in the case when t = 1 or 2. In this case, it is unlikely that the vector returned is similar to the principal eigenvector. For example, the power method converges slowly when the first few principal values are very close to each other. However, this is not an issue in our setting since a few iterations will still ensure that the direction of p has high variance."}, {"heading": "3.2. Fast outlier detection", "text": "The tree construction meta-algorithm has a special case for they S contains outliers, i.e., when there average distance between points is significantly less than the maximum distance between two points. This is important, because even if we were to find a good hyperplane to split our data, the average diameter of the resulting partitions will still be influenced by the outliers.\nFollowing (Dasgupta & Freund, 2008; Verma et al., 2009), we say that S has outliers if the maximum depends on the relative size of the average diameter and the maximum diameter:\nDefinition 3.1 (Outliers). For an user-definable parameter c > 0, we say S contains outliers if\n\u22062(S) > c\u22062a(S)\nwhere \u2206(S) = maxx,y\u2208S \u2016x\u2212 y\u2016.\nIt was shown in (Dasgupta & Freund, 2008) that in the case S has outliers according to this definition, the meta-algorithm (Algorithm 1) still guarantees a constant reduction in average diameter.\nProposition 3.1. Suppose \u22062(S) > c\u22062a (S), so that S is split into {S1, S2} as described in Algorithm 1. Then the following holds:\n\u22062a(S1, S2) \u2264 ( 1\n2 +\n2\nc\n) \u22062a(S)\nUnfortunately, it is computationally expensive to use Definition 3.1 to determine if there are outliers, because calculating \u22062(S) involves comparing O(|S|2) distances. Instead we proposed a simple variant. For an arbitrary point a \u2208 S, let D(S) = maxx\u2208S \u2016x\u2212 a\u2016. It can easily be shown that \u2206(S)/2 \u2264 D(S) \u2264 \u2206(S). Definition 3.2 (Outlier heuristic). S has outliers if\nD2(S) > c\u22062a(S) ."}, {"heading": "3.3. Comparison between splitting rules", "text": "We can now state our main theoretical result for the APD splitting rule and contrast it with the analogous results for RP and and PCA trees. Henceforth, we assume that there are no outliers, i.e., \u22062(S) < c\u22062a(S).\nThe following results were shown in (Dasgupta & Freund, 2008) and (Verma et al., 2009).\nProposition 3.2. There exist constants c1, c2 \u2208 (0, 1) such that if S has local covariance dimension (d, c1):\n1. RP rule: If p is chosen uniformly at random from the unit sphere in RD then,\nE [ \u22062a(S1, S2) ] < (1\u2212 c2/d) \u22062a(S) .\n2. PCA rule: p is the principal eigenvector then, \u22062a(S1, S2) < ( 1\u2212 c2/k2 ) \u22062a(S) ,\nwhere k = 1\u03bb1 \u2211d i=1 \u03bbi. Note that k \u2264 d.2\nThese results show that RP trees and PCA trees both adapt to intrinsic dimensionality, since the squared average diameter of nodes in the tree is decreasing as a function of d, the local covariance dimension of the data, rather than its extrinsic dimension D.\nIn the next section we will prove that a similar diameter reduction guarantee holds for APD trees:\n2k can be much less than d. For example, in the MNIST data set k2 < d for d larger than 80.\nTheorem 3.2 (Main Result). For c1, \u03b4 \u2208 (0, 1), there exists constant3 c2 \u2208 (0, 1) such that if\n1. S has local covariance dimension (d, c1) and\n2. p \u2208 RD is returned by the APD splitting rule with t iterations\nthen with probability 1\u2212 \u03b4,\n\u22062a(S1, S2) <\n( 1\u2212 c2\nk2(d\u2212 1) 2 2t+1\n) \u22062a(S)\nwhere k = 1\u03bb1 \u2211d i=1 \u03bbi.\nNote that for t = O(log d), this gives \u22062a(S1, S2) < ( 1\u2212 c2/k2 ) \u22062a(S) ,\ni.e., the same improvement as that achieved for PCA trees. However, even for smaller t, the bound only has a weak dependence on d."}, {"heading": "4. Theoretical Analysis of APD trees", "text": "To prove Theorem 3.2 we will need to analyze the quantity\nV (S, p) = 1\nn \u2211 x\u2208S (x \u00b7 p)2 .\nFor a fixed vector p, observe that V (S, p) corresponds to the variance of x \u00b7 p when x is drawn uniformly at random from S. Intuitively, it makes sense that a good splitting vector is one for which V (S, p) is large. Specifically, if we can prove a lower bound for V (S, p) when p is chosen according to the APD splitting rule, then we can appeal to the following variant of a proposition from (Verma et al., 2009).\nProposition 4.1. There exist constants 0 < c1, c2 < 1 with the following property. Suppose \u22062(S) \u2264 c\u22062a (S), so that S is split into {S1, S2} using the projection vector p. If S has local covariance dimension (d, c1), then:\n\u22062a(S1, S2) <\n( 1\u2212 c2\nk2\n( V (S, p)\n\u03bb1\n)2) \u22062a(S)\nwhere k = 1\u03bb1 \u2211d i=1 \u03bbi.\nTo lower bound V (S, p) we prove the following sequence of lemmas that relate V (S, p) to the eigenvectors p1, . . . , pD and corresponding eigenvalues\n3Where c2 \u221d c2\u03b4(1\u2212c1)4 and c\u03b4 is at worst polynomial in \u03b4 and the corresponding percentile of the \u03c72 distribution. Empirically, it suffices for c\u03b4 = 1/3 when \u03b4 = 0.01.\n\u03bb1, . . . , \u03bbD of the covariance matrix C. Recall that \u03bb1 \u2265 \u03bb2 \u2265 . . . \u2265 \u03bbD and that the eigenvalues are non-negative since C is positive semi-definite.\nLemma 4.1. For any q = \u2211D i=1 \u03b1ipi,\nV (S, q) = D\u2211 i=1 \u03bbi\u03b1 2 i .\nProof.\nV (S, q) = n\u2211 i=1 (xi \u00b7 q)2 n = 1 n (Xq) T (Xq) = qTCq\n= ( D\u2211 i=1 \u03b1ipi )T ( D\u2211 i=1 C\u03b1ipi )\n= ( D\u2211 i=1 \u03b1ipi )T ( D\u2211 i=1 \u03bbi\u03b1ipi ) = D\u2211 i=1 \u03bbi\u03b1 2 i ,\nwhere the last equality follows because the eigenvectors are orthonormal.\nLemma 4.1 and Theorem 3.1 imply the next lemma.\nLemma 4.2. Let p be the vector computed by the APD rule after t iterations where s = \u2211 i \u03b2ipi is the initial vector. Then\nV (S, p) =\n\u2211D i=1 \u03bb 2t+1 i \u03b2\n2 i\u2211D\nj=1 \u03bb 2t j \u03b2 2 j\n.\nWe now analyze the distribution of the \u03b2i coefficients to prove the next lemma.\nLemma 4.3. For any \u03b4 < 1 and \u03bb1, . . . , \u03bbD \u2265 0, there exists c\u03b4 > 0 with\nP\n[\u2211D i=1 \u03bb 2t+1 i \u03b2\n2 i\u2211D\nj=1 \u03bb 2t j \u03b2 2 j\n\u2265 c\u03b4 \u2211D i=1 \u03bb 2t+1 i\u2211D\nj=1 \u03bb 2t j\n] \u2265 1\u2212 \u03b4 , (1)\nif the direction of s = \u2211 i \u03b2ipi is chosen uniformly.\nProof. Let {\u03b3i}i\u2208[D] be independently distributed \u03c72random variables with one degree of freedom. Then, since the direction of the vector \u3008\u03b21, . . . , \u03b2D\u3009 is chosen uniformly at random in the APD rule, we know that\u2211D\ni=1 \u03bb 2t+1 i \u03b2 2 i\u2211D\ni=1 \u03bb 2t i \u03b2 2 i\n\u223c \u2211D i=1 \u03bb 2t+1 i \u03b3i\u2211D\ni=1 \u03bb 2t i \u03b3i\n=\n\u2211D i=1 \u00b5\n2t+1 i \u03b3i\u2211D\ni=1 \u00b5 2t i \u03b3i\n,\nwhere \u00b5i = \u03bbi/\u03bb1 so that 1 = \u00b51 \u2265 \u00b52 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u00b5D. This follows because a) a random point on the unit sphere can be sampled by choosing each coefficient according to the standard normal distribution and then\nrenormalizing (Muller, 1958); and b) the square of a variable with standard normal distribution has the \u03c72distribution with one degree of freedom.\nThe lemma follows by the union bound if we can show that there exist constants c1, c2 > 0 such that:\nP  D\u2211 j=1 \u00b52tj \u03b3j \u2265 c1 D\u2211 j=1 \u00b52tj  \u2264 \u03b4/2 (2)\nP [ D\u2211 i=1 \u00b52t+1i \u03b3i \u2264 c2 D\u2211 i=1 \u00b52t+1i ] \u2264 \u03b4/2 (3)\nsince then Eq. 1 holds with c\u03b4 = c2/c1.\nFor the first inequality, note that E [\u03b3i] = 1 (expectation of \u03c72 distribution) and hence\nE  D\u2211 j=1 \u00b52tj \u03b3j  = D\u2211 j=1 \u00b52tj .\nTherefore Eq. 2 follows from an application of the Markov inequality with c1 = 2/\u03b4.\nFor the second inequality there are two cases. First suppose that \u2211D i=1 \u00b5 2t+1 i \u2264 16/\u03b4. Using the inverse CDF of the \u03c72 distribution, we can compute \u03c3 where P [\u03b31 \u2264 \u03c3] = \u03b4/2. Then note that,\nD\u2211 i=1 \u00b52t+1i \u03b3i \u2265 \u03b31 \u2265 \u03c3 \u2265 \u03b4\u03c3 16 D\u2211 i=1 \u00b52t+1i\nwhere the second inequality (\u03b31 \u2265 \u03c3) holds with probability at least 1\u2212 \u03b4/2.\nAlternatively, suppose that \u2211D i=1 \u00b5 2t+1 i \u2265 16/\u03b4. Then, appealing to the Chebyshev inequality given that\nV [ D\u2211 i=1 \u00b52t+1i \u03b3i ] = 2 D\u2211 i=1 \u00b54t+2i \u2264 2 D\u2211 i=1 \u00b52t+1i ,\nwe conclude\nP [ D\u2211 i=1 \u00b52t+1i \u03b3i \u2264 1 2 D\u2211 i=1 \u00b52t+1i ] \u2264 8 \u2211D i=1 \u00b5 2t+1 i ( \u2211D i=1 \u00b5 2t+1 i ) 2 \u2264 \u03b4 2 .\nThis establishes Eq. 3 with c2 = min(\u03b4\u03c3/16, 1/2).\nLemma 4.4. For S = {x1, . . . , xn} \u2208 RD with local covariance dimension (d, \u03b5) with mean(S) = 0.\u2211D\ni=1 \u03bb 2t+1 i\u2211D\nj=1 \u03bb 2t j\n\u2265 (1\u2212 \u03b5) \u2211d i=1 \u03bb 2t+1 i\u2211d\nj=1 \u03bb 2t j\n.\nProof. Since \u2211D i=1 \u03bb 2t+1 i \u2265 \u2211d i=1 \u03bb 2t+1 i , it suffices to\nshow that \u2211d j=1 \u03bb 2t j \u2265 (1\u2212 \u03b5) \u2211D j=1 \u03bb 2t j .\nFrom Definition 2.3, it follows that \u2211d j=1 \u03bbj \u2265\n(1\u2212 \u03b5) \u2211D j=1 \u03bbj and therefore \u03b5 \u2211d j=1 \u03bbj \u2265\n(1\u2212 \u03b5) \u2211D j=d+1 \u03bbj . And hence, it follows that:\n\u03b5 d\u2211 j=1 \u03bb2tj \u2265 \u03b5 d\u2211 j=1 \u03bbj\u03bb 2t\u22121 d \u2265 (1\u2212 \u03b5) D\u2211 j=d+1 \u03bbj\u03bb 2t\u22121 d\n\u2265 (1\u2212 \u03b5) D\u2211\nj=d+1\n\u03bb2tj\nand therefore \u2211d j=1 \u03bb 2t j \u2265 (1\u2212 \u03b5) \u2211D j=1 \u03bb 2t j .\nTheorem 4.1. Let p be the vector computed by the APD rule after t \u2265 1 iterations. Then with probability 1\u2212 \u03b4, there exists a constant c\u03b4 > 0 such that\nV (S, p) \u2265 \u03bb1c\u03b4(1\u2212 \u03b5) 1 + 2t\n1 + 2t+ 2t 2t 2t+1 (d\u2212 1) 1 2t+1\n.\nNote this implies V (S, p) \u2265 \u03bb1c\u03b4(1\u2212 \u03b5)(d\u2212 1) \u22121 2t+1 /2.\nProof. Let \u03b1 = 1/(2t). By Lemmas 4.2, 4.3, and 4.4, if we set \u00b5i = \u03bb 2t i /\u03bb 2t 1 , we get\nV (S, p)\nc\u03b4 (1\u2212 \u03b5) \u2265\n\u2211d i=1 \u03bb\n2t+1 i\u2211d\nj=1 \u03bb 2t i\n=\n\u2211d i=1 \u00b5\n1+\u03b1 i\u2211d\nj=1 \u00b5i\n\u2265 min 1\u2265\u00b52,...,\u00b5d\n1 + \u2211d i=2 \u00b5 1+\u03b1 i\n1 + \u2211d j=2 \u00b5i\n\u2265 min 1\u2265\u00b52,...,\u00b5d\u22650\n1 + (d\u2212 1) \u2211d j=2( \u00b5i d\u22121 ) 1+\u03b1\n1 + \u2211d j=2 \u00b5i\n\u2265 min \u00b5\u22650\n1 + (d\u2212 1)\u2212\u03b1\u00b51+\u03b1\n1 + \u00b5\n\u2265 min \u00b5\u22650\n1 + (d\u2212 1)\u2212\u03b1\u00b51+\u03b1\n1 + (d\u2212 1)\u2212\u03b1\u00b51+\u03b1 + \u00b5 ,\nwhere the third last inequality follows from the convexity of the function x1+1/(2t) and the second last inequality follows by setting \u00b5 = \u2211d j=2 \u00b5i.\nBy analyzing the derivatives, it can be verified that\nf(u) = 1 + \u00b51+\u03b1\n/ (d\u2212 1)\u03b1\n1 + \u00b51+\u03b1 / (d\u2212 1)\u03b1 + \u00b5\nhas a unique minimum at u = ( (2t) 2t (d\u2212 1) )1/(2t+1) .\nSubstituting in this value establishes the theorem."}, {"heading": "5. Experimental Results", "text": "We compare the quality of APD trees to that of RP trees and PCA trees by measuring the vector quantization error (VQ-error). In vector quantization, the goal is to map all vectors (or points) in a given data set to a small number of representative vectors (or points). This can be done with a spatial partitioning tree \u2013 the points belonging to each partition are represented by the average of the points in that partition. Following this, the VQ-error is defined as the average squared representation error. Specifically, if S1, S2, . . . , S2` are the sets of points associated with the leaves of a tree T of depth `, then\nV QT (S) = 2`\u2211 i=1 \u2211 x\u2208Si \u2016x\u2212mean(Si)\u20162 |S| = 2`\u2211 i=1 |Si|\u22062a(Si) 2|S| .\nWe try to closely replicate the experiments done in (Freund et al., 2008), using the same kind of datasets and the same parameters. We ran our experiments on a synthetic dataset and the MNIST test dataset. Additionally we use a protein homology dataset from the KDD Cup 2004 data mining competition.\nAs in (Freund et al., 2008), the synthetic dataset consists of 10,000 points, each a 1,000-d vector and gener-\nated as follows: choose a peak value p uniformly randomly from [0, 1], and then generates the coordinates of the point from the normal distribution N(p, 1). The MNIST test dataset is a set of 10,000 images of handwritten digits, each of which has been normalized to 28 \u00d7 28 pixels, and is thus a 784-d vector. The protein homology dataset consists of pairs of proteins that have been tested for homology. It contains 285,409 data points, each of which is a 74-d vector.\nFor RP trees and APD trees, we perform 15 runs of each experiment, and calculated the average VQ-error. For PCA trees, we only needed to run each experiment once since there is no randomization in that algorithm. The results are shown in Figure 3. Each curve plots the decay of average VQ-errors as the depth of the tree increases. For APD trees, we show results of applying 1, 2, and 3 power iterations. RP tree is equivalent to applying no power iteration. The quality differences between APD trees and PCA trees are small, while the differences of RP trees with them are much more visible. Note that using 1 power iteration in APD trees already provides very good quality and gives the the biggest drop in VQ-error from RP trees.\nGPU Implementation of APD Trees. Our APD tree algorithm is well-suited for parallel computation\non modern GPUs. Since it only relies on basic matrix operations, most GPU-based linear algebra packages can be applied directly. Our implementation is written in MATLAB using the Jacket GPU library (AccelerEyes). The main tree building uses a standard recursive subdivision algorithm controlled by the host CPU, while the tree splitting algorithm (Algorithm 1) is accelerated on the GPU. Experimental results are presented in Figure 1 and 4. These results were obtained on a PC with NVIDIA 470 GTX graphics card (1.2GB GPU memory) and an Intel Core-i7 3.0GHz CPU with 8 hyperthreads.\nFigure 1 lists the computation time using the GPU and CPU implementations. The timing is averaged over 15 runs per test. The GPU implementation generally achieves 5 \u223c 12\u00d7 speedup compared to the CPU version, which is also written in MATLAB and multithreaded. This performance gain is mainly due to the acceleration of matrix-matrix and matrix-vector multiplications, which can easily exploit the GPU\u2019s parallel computation power. Note that the speedup for the KDDCup04 dataset is moderate, mainly because of the small vector size. We generally obtain higher performance gain on higher dimensional datasets, because they can better utilize available GPU resources.\nFor comparison we have also included a CPU PCA tree implementation using MATLAB\u2019s svds routine. As seen from the table, the CPU PCA trees are 4 \u223c 6 times slower than the CPU APD trees, which are in turn 5 \u223c 12 times slower than the GPU APD trees. We did not include a GPU PCA tree because the Jacket library only provides a full svd routine but not svds. As a result, the GPU PCA tree is only moderately faster than the CPU counterpart. This shows that our APD tree algorithm is well-suited for exploiting the GPU, due to its simplicity, at the same time providing comparable quality to PCA trees.\nFigure 4 plots the computation times for synthetic data sets containing different numbers of points. Each point is a 512-d vector generated using the same algorithm before. We plot the CPU and GPU timing for each data set and with 0 to 4 power iterations. From this plot we observe that the cost incurred by each additional APD iteration is generally quite small."}, {"heading": "6. Conclusion", "text": "We presented the APD-tree, a new spatial data structure for high-dimensional data. APD-trees use a small number of power iterations to achieve computational efficiency (comparable to RP trees) and high quality (comparable to PCA trees). The approach is insensi-\ntive to the convergence properties of the power method and is well-suited for GPU computation."}, {"heading": "Acknowledgments", "text": "This work is supported in part by NSF grants CCF0953754 and CCF-1025120."}], "references": [{"title": "The hardness of k-means clustering", "author": ["Dasgupta", "Sanjoy"], "venue": "Technical Report CS2008-0916,", "citeRegEx": "Dasgupta and Sanjoy.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Sanjoy.", "year": 2008}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Dasgupta", "Sanjoy", "Freund", "Yoav"], "venue": "In STOC,", "citeRegEx": "Dasgupta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2008}, {"title": "A probabilistic theory of pattern recognition", "author": ["L. Devroye", "L. Gyorfi", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "Learning the structure of manifolds using random projections", "author": ["Freund", "Yoav", "Dasgupta", "Sanjoy", "Kabra", "Mayank", "Verma", "Nakul"], "venue": "In NIPS, pp", "citeRegEx": "Freund et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2008}, {"title": "Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees", "author": ["D.T. Lee", "C.K. Wong"], "venue": "Acta Informatica,", "citeRegEx": "Lee and Wong,? \\Q1977\\E", "shortCiteRegEx": "Lee and Wong", "year": 1977}, {"title": "Power iteration clustering", "author": ["Lin", "Frank", "Cohen", "William W"], "venue": "In ICML, pp", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "A note on a method for generating points uniformly on n-dimensional spheres", "author": ["M.E. Muller"], "venue": "Comm. Assoc. Comput. Mach.,", "citeRegEx": "Muller,? \\Q1958\\E", "shortCiteRegEx": "Muller", "year": 1958}, {"title": "Which spatial partition trees are adaptive to intrinsic dimension", "author": ["Verma", "Nakul", "Kpotufe", "Samory", "Dasgupta", "Sanjoy"], "venue": "In UAI, pp", "citeRegEx": "Verma et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Verma et al\\.", "year": 2009}, {"title": "When rank trumps precision: Using the power method to compute Google\u2019s PageRank", "author": ["R.S. Wills"], "venue": "PhD thesis, North Carolina State University,", "citeRegEx": "Wills,? \\Q2007\\E", "shortCiteRegEx": "Wills", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "This is shown to be true even when the intrinsic dimensionality of the data is low (Verma et al., 2009).", "startOffset": 83, "endOffset": 103}, {"referenceID": 7, "context": "Recently, random-projection (RP) trees (Dasgupta & Freund, 2008) and PCA trees (Verma et al., 2009) have been proposed as alternatives to k-d trees that adapt to intrinsic dimensionality.", "startOffset": 79, "endOffset": 99}, {"referenceID": 7, "context": "However, PCA trees perform significantly better than RP trees at reducing the average diameter, as demonstrated in (Verma et al., 2009).", "startOffset": 115, "endOffset": 135}, {"referenceID": 8, "context": ", Google\u2019s PageRank (Wills, 2007)) and for spectral clustering (Lin & Cohen, 2010).", "startOffset": 20, "endOffset": 33}, {"referenceID": 7, "context": "Local Covariance Dimension Several possible definitions of intrinsic dimension are discussed in (Verma et al., 2009).", "startOffset": 96, "endOffset": 116}, {"referenceID": 2, "context": "The tree produced by this meta-algorithm is a hybrid of a BSP tree and a sphere tree (Devroye et al., 1996).", "startOffset": 85, "endOffset": 107}, {"referenceID": 7, "context": "Following (Dasgupta & Freund, 2008; Verma et al., 2009), we say that S has outliers if the maximum depends on the relative size of the average diameter and the maximum diameter: Definition 3.", "startOffset": 10, "endOffset": 55}, {"referenceID": 7, "context": "The following results were shown in (Dasgupta & Freund, 2008) and (Verma et al., 2009).", "startOffset": 66, "endOffset": 86}, {"referenceID": 7, "context": "Specifically, if we can prove a lower bound for V (S, p) when p is chosen according to the APD splitting rule, then we can appeal to the following variant of a proposition from (Verma et al., 2009).", "startOffset": 177, "endOffset": 197}, {"referenceID": 6, "context": "renormalizing (Muller, 1958); and b) the square of a variable with standard normal distribution has the \u03c7distribution with one degree of freedom.", "startOffset": 14, "endOffset": 28}, {"referenceID": 3, "context": "We try to closely replicate the experiments done in (Freund et al., 2008), using the same kind of datasets and the same parameters.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "As in (Freund et al., 2008), the synthetic dataset consists of 10,000 points, each a 1,000-d vector and generated as follows: choose a peak value p uniformly randomly from [0, 1], and then generates the coordinates of the point from the normal distribution N(p, 1).", "startOffset": 6, "endOffset": 27}], "year": 2012, "abstractText": "We introduce a new spatial data structure for high dimensional data called the approximate principal direction tree (APD tree) that adapts to the intrinsic dimension of the data. Our algorithm ensures vector-quantization accuracy similar to that of computationally-expensive PCA trees with similar time-complexity to that of loweraccuracy RP trees. APD trees use a small number of powermethod iterations to find splitting planes for recursively partitioning the data. As such they provide a natural trade-off between the running-time and accuracy achieved by RP and PCA trees. Our theoretical results establish a) strong performance guarantees regardless of the convergence rate of the powermethod and b) that O(log d) iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is d. We demonstrate this trade-off and the efficacy of our data structure on both the CPU and GPU.", "creator": "LaTeX with hyperref package"}}}