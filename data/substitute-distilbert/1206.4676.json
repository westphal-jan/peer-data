{"id": "1206.4676", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Clustering by Low-Rank Doubly Stochastic Matrix Decomposition", "abstract": "differential analysis by nonnegative low - rank approximations has achieved remarkable progress in the past decade. however, advanced empirical approaches in this direction are still basic to dynamic factorization. we propose a new low - rank mathematical method to improve the clustering performance, which is beyond generalized factorization. the approximation is based on a two - step bipartite random walk through virtual cluster databases, where the approximation, formed by only cluster assigning probabilities. minimizing the approximation error measured onto kullback - leibler divergence is equivalent to maximizing the likelihood of a discriminative decomposition, which endows your method with a solid generic interpretation. the optimization is implemented by a relaxed majorization - proof algorithm while is advantageous in finding good local minima. furthermore, sources point out that the regularized algorithm with dirichlet prior methods serves as initialization. experimental results show that the new method has strong property in clustering purity for optimization datasets, especially for large - region manifold data.", "histories": [["v1", "Mon, 18 Jun 2012 15:36:49 GMT  (339kb)", "http://arxiv.org/abs/1206.4676v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NA stat.ML", "authors": ["zhirong yang", "erkki oja"], "accepted": true, "id": "1206.4676"}, "pdf": {"name": "1206.4676.pdf", "metadata": {"source": "META", "title": "Clustering by Low-Rank Doubly Stochastic Matrix Decomposition", "authors": ["Zhirong Yang", "Erkki Oja"], "emails": ["zhirong.yang@aalto.fi", "erkki.oja@aalto.fi"], "sections": [{"heading": "1. Introduction", "text": "Cluster analysis assigns a set of objects into groups so that the objects in the same cluster are more similar to each other than to those in other clusters. Optimization of most clustering objectives is NP-hard and relaxation to \u201csoft\u201d clustering is often required. A non-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nnegativity constraint, together with various low-rank matrix approximation objectives, has widely been used for the relaxation purpose in the past decade.\nThe most popular nonnegative low-rank approximation method is Nonnegative Matrix Factorization (NMF). It finds a matrix that approximates the similarities and can be factorized into several nonnegative low-rank matrices. NMF was originally applied to vectorial data, where Ding et al. (2010) have shown that NMF is equivalent to the classical k-means method. Later NMF was applied to the (weighted) graph given by the pairwise similarities. For example, Ding et al. (2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al. (2011) proposed Left Stochastic Decomposition that approximates a similarity matrix based on Euclidean distance and a left-stochastic matrix. Another stream in the same direction is topic modeling. Hofmann (1999) gave a generative model in Probabilistic Latent Semantic Indexing (PLSI) for counting data, which is essentially equivalent to NMF using Kullback-Leibler (KL) divergence and Tri-factorizations. Bayesian treatment of PLSI by using Dirichlet prior was later introduced by Blei et al. (2001). Symmetric PLSI with the same Bayesian treatment is called Interaction Component Model (ICM) (Sinkkonen et al., 2008).\nDespite remarkable progress, the above relaxation approaches are still not fully satisfactory in all of the following requirements that affect the clustering performance using nonnegative low-rank approximation: (1) approximation error measure that takes into account sparse similarities, (2) decomposition form of the approximating matrix, where the decomposing matrices should contain just enough parameters for clustering but not more, and (3) normalization of the approximating matrix, which ensures relatively balanced clusters and equal contribution of each data sample. Lacking one or more of these dimensions can severely affect clustering performance.\nIn this paper we present a new nonnegative low-rank approximation method for clustering, which satisfies all of the above three requirements. First, because datasets often lie in curved manifolds such that only similarities in a small neighborhood are reliable, we adopt KL-divergence to handle the resulting sparsity. Second, different from PLSI, we enforce an equal contribution of every data sample and then directly construct the decomposition over the probabilities from samples to clusters. Third, these probabilities form the only decomposing matrix to be learned in our approach and directly gives the answer for probabilistic clustering. Furthermore, our decomposition method leads to a doubly-stochastic approximating matrix, which was shown to be desired for balanced graph cuts (Zass & Shashua, 2006). We name our new method DCD because it is based on Data-Cluster-Data random walks.\nIn order to solve the DCD learning objective, we propose a novel relaxed Majorization-Minimization algorithm to handle the new matrix decomposition type. Our relaxation strategy works robustly in finding sastisfactory local optimizers under the stochasticity constraint. Furthermore, we argue that complexity control such as Bayesian priors only provides initialization for the new algorithm. This eliminates the problem of hyperparameter selection in the prior.\nEmpirical comparison with NMF and other graphbased clustering approaches demonstrates that our method can achieve the best or nearly the best clustering purity in all tasks. For some datasets, the new method significantly improves the state-of-the-art.\nAfter this introductory part, we present the new method in Section 2, including its learning objective, probabilistic model, optimization and initialization techniques. In Section 3, we point out the connections and differences between our method and other recent related work. Experimental settings and results are given in Section 4. Finally we conclude the paper and discuss some future work in Section 5."}, {"heading": "2. Clustering by DCD", "text": "Suppose the similarities between n data samples are precomputed and given in a nonnegative symmetric matrix A. This matrix can be seen as (weighted) affinity of an undirected similarity graph where each node corresponds to a data sample (data node). A clustering analysis algorithm takes such input and divides the data nodes into r disjoint subsets. In probabilistic clustering analysis, we want to find P (k|i), the probability of assigning the ith sample to the kth cluster,\nwhere i = 1, . . . , n and k = 1, . . . , r. In the following, i, j and v stand for data sample (node) indices while k and l stand for cluster indices."}, {"heading": "2.1. Learning objective", "text": "Some of our work was inspired by the AnchorGraph (Liu et al., 2010) which was used in large approximative graph construction based on a two-step random walk between data nodes through a set of anchor nodes. Note that AnchorGraph is not a clustering method.\nIf we augment the input similarity graph by r cluster nodes, the cluster assigning probabilities can be seen as single-step random walk probabilities from data nodes to the augmented cluster nodes. Without preference to any particular samples, we impose uniform prior P (i) = 1/n over the data nodes. By this prior, the reversed random walk probabilities can then be calculated by the Bayes formula\nP (i|k) = P (k|i)P (i)\u2211 v P (k|v)P (v) = P (k|i)\u2211 v P (k|v) . (1)\nConsider next the probability of two-step random walks from ith data node to jth data node via all cluster nodes (DCD random walk):\nP (i|j) = \u2211 k P (i|k)P (k|j) = \u2211 k P (k|i)P (k|j)\u2211 v P (k|v) . (2)\nThis probability defines another similarity between two data nodes, A\u0302ij = P (i|j), with respect to cluster nodes. Note that this matrix has rank at most equal to r. The learning target is now to find a good approximation between the input similarities and the DCD random walk probabilities:\nA \u2248 A\u0302. (3)\nAnchorGraph does not provide any error measure for the above approximation. A conventional choice in NMF is the squared Euclidean distance, which employs the underlying assumption that the noise is additive and Gaussian.\nIn real-world clustering tasks for multivariate datasets, data points often lie in a curved manifold. Consequently, similarities based on Euclidean distances are reliable only in a small neighborhood. Such locality causes high sparsity in the input similarity matrix. Sparsity also commonly exists for real-world network data. Because of the sparsity, Euclidean distance is improper for the approximation in Eq. (3), because additive Gaussian noise should lead to a dense\nobserved graph. In contrast, (generalized) KullbackLeibler divergence is more suitable for the approximation. The underlying Poisson noise characterizes rare occurrences that are present in our sparse input. We can now formulate our learning objective as the following optimization problem:\nmin W\u22650 DKL(A||A\u0302) = \u2211 ij\n( Aij log Aij\nA\u0302ij \u2212Aij + A\u0302ij ) (4)\ns.t. \u2211 k Wik = 1, i = 1, . . . , n, (5)\nwhere we write Wik = P (k|i) for convenience and thus\nA\u0302ij = \u2211 k WikWjk\u2211 vWvk . (6)\nNote that A\u0302 is symmetric as it is easy to verify that P (i|j) = P (j|i). Therefore, A\u0302 is also doubly stochastic because it is left-stochastic by probability definition."}, {"heading": "2.2. Probabilistic model", "text": "The optimization objective has an analogous statistical model with the PLSI. Dropping the constant terms from DKL(A||A\u0302), the objective is equivalent to maximizing \u2211\nij\nAij log A\u0302ij . (7)\nThis can be identified as the log-likelihood of the following generative model if Aij are integers: for t = 1, . . . , T , add one to entry (i, j) \u223c Multinomial ( 1 n A\u0302, 1 ) , whose likelihood is given by\np(A) = T\u220f t=1 1 n A\u0302ij = \u220f ij ( 1 n A\u0302ij )Aij ,\nwhere T = \u2211 ij Aij .\nThe above model simply uses uniform prior on rows of W . It does not prevent from using informative priors or complexity control. A natural choice for probabilities is the Dirichlet distribution (\u03b1 > 0)\np(Wi1, . . . ,Wir|\u03b1) = \u0393(r\u03b1)\n[\u0393(\u03b1)] r r\u220f k=1 W\u03b1\u22121ik , (8)\nwhich is also the conjugate prior of multinomial distribution. The Dirichlet prior reduces to be uniform when \u03b1 = 1.\nAlthough it is possible to construct a multi-level graphical model similar to the Dirichlet process topic model, we emphasize that the smallest approximation error (or perplexity) is our final goal. Dirichlet prior is used only in order to ease the optimization. Therefore we do not employ more complex generative models; see Section 2.4 for more discussion."}, {"heading": "2.3. Optimization", "text": "The optimization problem with Dirichlet prior on W is equivalent to minimizing\nJ (W ) = \u2212 \u2211 ij Aij log A\u0302ij \u2212 (\u03b1\u2212 1) \u2211 ik logWik (9)\nThere are two ways to handle the constraint Eq. (5). First, one can develop the multiplicative algorithm by the procedure proposed by Yang & Oja (2011) by neglecting the stochasticity constraint, and then normalize the rows of W after each update. However, the optimization by this way easily gets stuck in poor local minima in practice.\nHere we employ a relaxing strategy to handle the constraint. We first introduce Lagrangian multipliers for the constraints:\nL(W,\u03bb) = J (W ) + \u2211 i \u03bbi (\u2211 k Wik \u2212 1 ) . (10)\nUnlike traditional constrained optimization that solves the fixed-point equations, we employ a heuristic to find the multipliers \u03bb. Denote \u2207 = \u2207+ \u2212\u2207\u2212 the gradient of J with respect to W , where \u2207+ and \u2207\u2212 are the positive and (unsigned) negative parts, respectively. This suggests a fixed-point update rule for W :\nW \u2032ik = Wik \u2207\u2212ik \u2212 \u03bbi \u2207+ik . (11)\nImposing \u2211 kW \u2032 ik = 1, we obtain\n\u03bbi = bi \u2212 1 ai , (12)\nwhere ai and bi are given in Algorithm 1. Next we show that the augmented objective Eq. (10) decreases after each iteration with the above \u03bb.\nTheorem 1. Denote Wnew the updated matrix after each iteration. It holds that L(Wnew, \u03bb) \u2264 L(W,\u03bb) with \u03bbi = (bi \u2212 1)/ai.\nProof. The algorithm construction mainly follows the Majorization-Minimization procedure (see e.g. Yang &\nOja, 2011). We use W and W\u0303 to distinguish the current estimate and the variable, respectively.\nAlgorithm 1 Relaxed MM Algorithm for DCD\nInput: similarity matrix A, number of clusters r, nonnegative initial guess of W . repeat\nZij = (\u2211 k WikWjk\u2211 vWvk )\u22121 Aij\nsk = \u2211 vWvk \u2207\u2212ik = 2 (ZW )ik s \u22121 k + \u03b1W \u22121 ik\n\u2207+ik = ( WTZW ) kk s\u22122k +W \u22121 ik\nai = \u2211 l Wil \u2207+il , bi = \u2211 l Wil \u2207\u2212il \u2207+il Wik \u2190Wik \u2207\u2212ikai + 1 \u2207+ikai + bi\nuntil W is unchanged Output: cluster assigning probabilities W .\n(Majorization)\nLet \u03c6ijk = WikWjk\u2211 vWvk (\u2211 l WilWjl\u2211 vWvl )\u22121 .\nL(W\u0303 ) \u2264\u2212 \u2211 ijk Aij\u03c6ijk [ log W\u0303ik + log W\u0303jk \u2212 log \u2211 v W\u0303vk ] \u2212 (\u03b1\u2212 1)\n\u2211 ik log W\u0303ik + \u2211 ik \u03bbiWik + C1\n\u2264\u2212 \u2211 ijk Aij\u03c6ijk\n[ log W\u0303ik + log W\u0303jk \u2212 \u2211 v W\u0303vk\u2211 vWvk ] \u2212 (\u03b1\u2212 1)\n\u2211 ik log W\u0303ik + \u2211 ik \u03bbiWik + C2\n\u2264\u2212 \u2211 ijk Aij\u03c6ijk\n[ log W\u0303ik + log W\u0303jk \u2212 \u2211 v W\u0303vk\u2211 vWvk ] \u2212 (\u03b1\u2212 1)\n\u2211 ik log W\u0303ik + \u2211 ik \u03bbiWik\n+ \u2211 ik ( 1 ai + 1 Wik ) Wik ( W\u0303ik Wik \u2212 log W\u0303ik Wik \u2212 1 ) + C2\n\u2261G(W\u0303 ,W ),\nwhere C1 and C2 are constants irrelevant to the variable W\u0303 . The first two inequalities follow the CCCP majorization (Yang & Oja, 2011) using the convexity and concavity of \u2212 log() and log(), respectively. The third inequality is called \u201cmoving term\u201d technique used in multiplicative updates (Yang & Oja, 2010).\nIt adds the same constant 1ai + 1 Wik\nto both numerator and denominator in order to guarantee that the updated matrix entries are positive, which is implemented by using a further upper-bound of zero. All the above upper bounds are tight at W\u0303 = W , i.e. G(W,W ) = J (W ).\n(Minimization)\n\u2202G\n\u2202W\u0303ik =\u2207+ik \u2212\n1 Wik \u2212 Wik W\u0303ik\n( \u2207\u2212ik \u2212 1\nWik ) + \u03bbi + ( 1\nai +\n1\nWik\n) Wik ( 1\nWik \u2212 1 W\u0303ik ) =\u2212 Wik\nW\u0303ik\n( \u2207\u2212ik + 1\nai\n) + ( \u2207+ik +\nbi ai\n) .\nSetting the gradient to zero gives\nW newik = Wik \u2207\u2212ik + 1 ai\n\u2207+ik + bi ai\n(13)\nMultiplying both numerator and denominator by ai gives the last update rule in Algorithm 1. Therefore, L(W new, \u03bb) \u2264 G(W new,W ) \u2264 L(W,\u03bb).\nAlgorithm 1 jointly minimizes the approximation error and drives the rows of W towards the probability simplex. The Lagrangian multipliers are automatically selected by the algorithm, without extra human tuning labor. The quantities bi are the row sums of the unconstrained multiplicative learning result, while the quantities ai balance between the gradient learning force and the probability simplex attraction. Besides convenience, we find that this relaxation strategy works more robustly than the brute-force normalization after each iteration."}, {"heading": "2.4. Initialization", "text": "The optimization problems of many clustering analysis methods, including ours, are non-convex. Usually finding the global optimum is very expensive or even NP-hard. When local optimizers are used, the optimization trajectory can easily get stuck in poor local optima if the algorithm starts from an arbitrary random guess. Proper initialization is thus needed to achieve satisfactory performance.\nThe cost of the initialization should be much cheaper than the main algorithm. There are two popular choices: k-means and Normalized Cut (Ncut). The first one can only be applied to vectorial data and could be slow for large-scale high-dimensional data. Here we employ the second initialization method. While the original Ncut is NP-hard, the relaxed Ncut\nproblem can be efficiently solved via spectral methods (Shi & Malik, 2000). Furthermore, it is particularly suitable for sparse graph input, which is our focus in this paper.\nBesides Ncut, we emphasize that the minimal approximation error is our sole learning objective and all regularized versions, e.g. with different Dirichlet priors, only serve as initialization. This is because clustering analysis, unlike supervised learning problems, does not need to provide inference for unseen data. That is, the complexity control such as Bayesian priors is not meant for better generalization performance, but for better-shaped space to facilitate optimization. In this sense, we can use the results of diverse regularized versions or even other clustering algorithms as starting guesses, and pick the one with the smallest approximation error among multiple runs.\nIn implementation, we first convert an initialization clustering result to an n \u00d7 r binary indicator matrix, and then add a small positive perturbation to all entries. Next, the perturbed matrix is fed to our optimization algorithm (with \u03b1 = 1 in Algorithm 1)."}, {"heading": "3. Related Work", "text": "Our method intersects with several other machine learning approaches. Here we discuss some of these directions, pinpointing the connections and our new contributions."}, {"heading": "3.1. Topic Model", "text": "A topic model is a type of statistical model for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. An early topic model was PLSI (Hofmann, 1999) which maximizes the following log-likelihood for symmetric input A:\u2211\nij Aij log \u2211 k P (k)P (i|k)P (j|k). (14)\nOne can see that PLSI has similar form as Eq. (7). Both objectives can be equivalently expressed by nonnegative low-rank approximation using KL-divergence.\nThe major difference is the decomposition form of the approximating matrix. There are two ways to model the hierarchy between latent variables and the observed ones. Topic model uses the pure generative way while our method employs the discriminative way. PLSI gives the clustering results indirectly. One should apply Bayes formula to evaluate P (k|i) using P (i|k) and P (k). There are n\u00d7 r \u2212 1 free parameters to be learned in the latter two quantities. In contrast, our method directly learns the cluster assigning prob-\nabilities P (k|i) which contains only n \u00d7 (r \u2212 1) free parameters. This difference can be large when there are only a few clusters (e.g. r = 2 or r = 3).\nIt is known that the performance of PLSI can be improved by using Bayesian non-parametric modeling. Bayesian treatment for the symmetric version of PLSI leads to Interaction Component Model (Sinkkonen et al., 2008). It associates Dirichlet priors to the PLSI factorizing matrices and then makes use of the conjugacy between Dirichlet and multinomial to derive collapsed Gibbs sampling or variational optimization methods.\nAn open problem of Bayesian methods is how to determine the hyperparameters that control the priors. Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance. The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al., 2009) or clustering purity in our experiments (see Section 4). Hofmann (1999); Asuncion et al. (2009) suggested to select the hyperparameters using the smallest approximation error for some heldout matrix entries, which is however more costly and might weaken or even break the cluster structure.\nBy contrast, there is no such prior hyperparameter selection problem in our method. The algorithms using various priors only play their role in the initialization. Among the runs with different starting points, we simply select the one with the smallest approximation error."}, {"heading": "3.2. Nonnegative Matrix Factorization", "text": "Nonnegative Matrix Factorization is one of the earliest methods for relaxing clustering problems by nonnegative low-rank approximation (see e.g. Xu et al., 2003). The research on NMF also opened the door for multiplicative majorization-minimization algorithms for optimization over nonnegative matrices. In the original NMF, an input nonnegative matrix X is approximated by a product of two low-rank matricesW andH. Later researchers found that more constraints or normalizations should be imposed on the factorizing matrices to achieve desired performance.\nOrthogonality is a popular choice (see e.g. Ding et al., 2006) for highly sparse factorizing matrices, especially the cluster indicator matrix. However, the orthogonality constraint seems exclusive of other constraints or priors. In practice, the orthogonality favors Euclidean distance as the approximation error measure for simple update rules, which is against our requirement for\nsparse graph input.\nStochasticity seems more natural for relaxing hard clustering to probabilities. Recently Arora et al. (2011) proposed a symmetric NMF using leftstochastic factorizing matrices called LSD. Their method also directly learns the cluster assigning probabilities. However, LSD is also restricted to Euclidean distance.\nOur method has two major differences from LSD. First, we use Kullback-Leibler divergence which is more suitable for sparse graph input or curved manifold data. This also enables us to make use of the Dirichlet and multinomial conjugacy pair. Second, our decomposition has good interpretation in terms of a random walk. Furthermore, imbalanced clustering is implicitly penalized because of the denominator in Eq. (6)."}, {"heading": "3.3. AnchorGraph", "text": "DCD uses the same matrix decomposition as AnchorGraph. However, there are several major differences between the two methods. First of all, AnchorGraph is not made for clustering, but for constructing the graph input. AnchorGraph has no learning objective that captures the global structure of data such as clusters. Each row of the decomposing matrix in AnchorGraph is learned individually and only encodes the local information. There is no learning over the decomposing matrix as a whole. Furthermore, anchors are either selected from data samples or pre-learned by e.g. kmeans. By contrast, cluster nodes in our formulation are virtual. They are not vectors and need no physical storage."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Compared methods", "text": "We have compared our method with eight other clustering algorithms that can take a symmetric nonnegative sparse matrix as input. The compared algorithms range from classical to state-of-the-art methods with various principles: graph cuts including Normalized Cut (Ncut) (Shi & Malik, 2000), Nonnegative Spectral Cut (NSC) (Ding et al., 2008), and 1-Spectral ratio Cheeger cut (1-Spec) (Hein & Bu\u0308hler, 2010); nonnegative matrix factorization including Projective NMF (PNMF) (Yang & Oja, 2010), Symmetric 3-Factor Orthogonal NMF (ONMF) (Ding et al., 2006), and LeftStochastic Decomposition (LSD) (Arora et al., 2011); topic models including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) and Interaction Component Model (ICM) (Sinkkonen et al., 2008).\nThe detailed settings of the compared methods are as follows. We implemented NSC, PNMF, ONMF, LSD, PLSI, and DCD using multiplicative updates. For these methods, we ran their update rules for 10,000 iterations to ensure that all algorithms have sufficiently converged. We used the default setting for 1-Spec. ICM uses collapsed Gibbs sampling, where each round of the sampling sweeps the graph once. We ran the ICM sampling for 100,000 rounds to ensure that the MCMC burn-in is converged (it took about one day for the largest dataset). The hyperparameters in ICM are automatically adjusted by using Minka\u2019s method (Minka, 2000).\nDespite mediocre results, Ncut runs very fast and gives pretty stable outputs. We thus used it for initialization. After getting the Ncut cluster indicator matrix, we add 0.2 to all entries and feed it as starting point for other methods, which is a common initialization setting for NMF methods. The other three initialization points for our method are provided by Ncut followed by DCD using three different Dirichlet priors (\u03b1 = 1.2, \u03b1 = 2, and \u03b1 = 5). The clustering result of our method is reported by the run with the smallest approximation error, see Eq. (4)."}, {"heading": "4.2. Datasets", "text": "The performance of clustering methods were evaluated using real-world datasets. In particular, we focus on data that lie in a curved manifold. We thus selected 15 such datasets which are publicly available from a variety of domains. The data sources are given in the supplemental document.\nThe statistics of the selected datasets are summarized\nin Table 1. In brief, Amazon are book similarities according to amazon.com buying records; Votes are voting records in US congress by two different parties; ORL, PIE, YaleB are face images collected under different conditions; COIL20 are small toy images; Isolet and LegReco are handwritten English letter images; Webkb4 and 7sectors are text document collections; Mfeat, USPS, PenDigits, MNIST are handwritten digit images.\nWe preprocessed the above datasets to produce similarity graph input except Amazon which is already in sparse graph format. We extracted the scattering features (Mallat, 2012) for image data except Isolet and Mfeat which have their own feature representation. We used Tf-Idf features for text documents.\nAfter feature extraction, we constructed a K-NearestNeighbor (KNN) graph for each dataset. We set K = 5 for the six smallest datasets (except Amazon) and K = 10 for the other datasets. We then symmetrized and binarized the KNN graph B to obtain the input similarities A (i.e. Aij = 1 if Bij = 1 or Bji = 1, and Aij = 0 otherwise)."}, {"heading": "4.3. Results", "text": "Clustering performance of the compared methods is evaluated by clustering purity\npurity = 1\nn r\u2211 k=1 max 1\u2264l\u2264r nlk (15)\nwhere nlk is the number of data samples in the cluster k that belong to ground-truth class l. A larger purity in general corresponds to better clustering result. The\nclustering purities for the compared methods are given in Table 2.\nOur method has strong performance in terms of clustering purity. DCD wins 12 out of 15 selected datasets. Even for the other three datasets, DCD is the first or second runner-up, with purities tied with or very close to the winner.\nThe new method is particularly more advantageous for large datasets. Note that the datasets in Table 2 are ordered by their sizes. We can see that there are some other winners or joint winners for smaller datasets, for example, LSD for the PIE faces or 1-Spec for the Mfeat digits. PLSI performs quite similarly with DCD for these small clustering tasks. However, DCD demonstrates clear win over the other methods for the five largest datasets.\nDCD has remarkable performance for the largest dataset MNIST. In this case, clustering as unsupervised learning by using our method has even achieved classification accuracy (i.e. purity) very close to many modern supervised approaches1, whereas we only need ten labeled samples to remove the cluster-class permutation ambiguity."}, {"heading": "5. Conclusions", "text": "We have presented a new clustering method based on nonnegative low-rank approximation with three major original contributions: (1) a novel decomposition approach for the approximating matrix derived from a two-step random walk; (2) a relaxed majorization-\n1see http://yann.lecun.com/exdb/mnist/\nminimization algorithm for finding better approximating matrices; (3) a strategy that uses regularization with the Dirichlet prior as initialization. Experimental results showed that our method works robustly for various selected datasets and can improve clustering purity for large manifold datasets.\nThere are some other dimensions that affect clustering performance. Our practice indicates that initialization could play an important role because most current algorithms are only local optimizers. Using Dirichlet prior is only one way to smooth the objective function space. One could use other priors or regularization techniques to achieve better initializations.\nAnother dimension is the input graph. We have focused on the grouping procedure given that the similarities are precomputed. One should notice that better features or a better similarity measure can significantly improve clustering purity. Though we did not use AnchorGraph for the sake of including topic models in our comparison, it could be more beneficial to construct both approximated and approximating matrices by the same principle. This also suggests that clustering analysis could be performed in a deeper way using hierarchical pre-training. Detailed implementation should be investigated in the future."}, {"heading": "Acknowledgment", "text": "This work was financially supported by the Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN, grant no. 251170; Zhirong Yang additionally by decision number 140398)."}], "references": [{"title": "Clustering by left-stochastic matrix factorization", "author": ["R. Arora", "M. Gupta", "A. Kapila", "M. Fazel"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Arora et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2011}, {"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Teh", "Y.-W"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Asuncion et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2001}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "In International conference on Knowledge discovery and data mining (SIGKDD),", "citeRegEx": "Ding et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2006}, {"title": "Convex and seminonnegative matrix factorizations", "author": ["C. Ding", "T. Li", "M.I. Jordan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ding et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2010}, {"title": "An inverse power method for nonlinear eigenproblems with applications in 1-spectral clustering and sparse pca", "author": ["M. Hein", "T. B\u00fchler"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hein and B\u00fchler,? \\Q2010\\E", "shortCiteRegEx": "Hein and B\u00fchler", "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In International Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Hofmann,? \\Q1999\\E", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "Chang", "S.-F"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Group invariant scattering", "author": ["S. Mallat"], "venue": "Communications in Pure and Applied Mathematics,", "citeRegEx": "Mallat,? \\Q2012\\E", "shortCiteRegEx": "Mallat", "year": 2012}, {"title": "Estimating a dirichlet distribution", "author": ["T. Minka"], "venue": null, "citeRegEx": "Minka,? \\Q2000\\E", "shortCiteRegEx": "Minka", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Component models for large networks", "author": ["J. Sinkkonen", "J. Aukia", "S. Kaski"], "venue": "ArXiv e-prints,", "citeRegEx": "Sinkkonen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sinkkonen et al\\.", "year": 2008}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "In International Conference on Research and Development in Informaion Retrieval (SIGIR),", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}, {"title": "Linear and nonlinear projective nonnegative matrix factorization", "author": ["Z. Yang", "E. Oja"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "Yang and Oja,? \\Q2010\\E", "shortCiteRegEx": "Yang and Oja", "year": 2010}, {"title": "Unified development of multiplicative algorithms for linear and quadratic nonnegative matrix factorization", "author": ["Z. Yang", "E. Oja"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Yang and Oja,? \\Q2011\\E", "shortCiteRegEx": "Yang and Oja", "year": 2011}, {"title": "Doubly stochastic normalization for spectral clustering", "author": ["R. Zass", "A. Shashua"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zass and Shashua,? \\Q2006\\E", "shortCiteRegEx": "Zass and Shashua", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Symmetric PLSI with the same Bayesian treatment is called Interaction Component Model (ICM) (Sinkkonen et al., 2008).", "startOffset": 92, "endOffset": 116}, {"referenceID": 1, "context": "NMF was originally applied to vectorial data, where Ding et al. (2010) have shown that NMF is equivalent to the classical k-means method.", "startOffset": 52, "endOffset": 71}, {"referenceID": 1, "context": "NMF was originally applied to vectorial data, where Ding et al. (2010) have shown that NMF is equivalent to the classical k-means method. Later NMF was applied to the (weighted) graph given by the pairwise similarities. For example, Ding et al. (2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al.", "startOffset": 52, "endOffset": 252}, {"referenceID": 0, "context": "(2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al. (2011) proposed Left Stochastic Decomposition that approximates a similarity matrix based on Euclidean distance and a left-stochastic matrix.", "startOffset": 80, "endOffset": 100}, {"referenceID": 0, "context": "(2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al. (2011) proposed Left Stochastic Decomposition that approximates a similarity matrix based on Euclidean distance and a left-stochastic matrix. Another stream in the same direction is topic modeling. Hofmann (1999) gave a generative model in Probabilistic Latent Semantic Indexing (PLSI) for counting data, which is essentially equivalent to NMF using Kullback-Leibler (KL) divergence and Tri-factorizations.", "startOffset": 80, "endOffset": 306}, {"referenceID": 0, "context": "(2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al. (2011) proposed Left Stochastic Decomposition that approximates a similarity matrix based on Euclidean distance and a left-stochastic matrix. Another stream in the same direction is topic modeling. Hofmann (1999) gave a generative model in Probabilistic Latent Semantic Indexing (PLSI) for counting data, which is essentially equivalent to NMF using Kullback-Leibler (KL) divergence and Tri-factorizations. Bayesian treatment of PLSI by using Dirichlet prior was later introduced by Blei et al. (2001). Symmetric PLSI with the same Bayesian treatment is called Interaction Component Model (ICM) (Sinkkonen et al.", "startOffset": 80, "endOffset": 595}, {"referenceID": 7, "context": "Some of our work was inspired by the AnchorGraph (Liu et al., 2010) which was used in large approximative graph construction based on a two-step random walk between data nodes through a set of anchor nodes.", "startOffset": 49, "endOffset": 67}, {"referenceID": 6, "context": "An early topic model was PLSI (Hofmann, 1999) which maximizes the following log-likelihood for symmetric input A: \u2211", "startOffset": 30, "endOffset": 45}, {"referenceID": 11, "context": "Bayesian treatment for the symmetric version of PLSI leads to Interaction Component Model (Sinkkonen et al., 2008).", "startOffset": 90, "endOffset": 114}, {"referenceID": 1, "context": "The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al., 2009) or clustering purity in our experiments (see Section 4).", "startOffset": 137, "endOffset": 160}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance. The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al.", "startOffset": 0, "endOffset": 183}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance. The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al., 2009) or clustering purity in our experiments (see Section 4). Hofmann (1999); Asuncion et al.", "startOffset": 0, "endOffset": 345}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance. The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al., 2009) or clustering purity in our experiments (see Section 4). Hofmann (1999); Asuncion et al. (2009) suggested to select the hyperparameters using the smallest approximation error for some heldout matrix entries, which is however more costly and might weaken or even break the cluster structure.", "startOffset": 0, "endOffset": 369}, {"referenceID": 0, "context": "Recently Arora et al. (2011) proposed a symmetric NMF using leftstochastic factorizing matrices called LSD.", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": ", 2008), and 1-Spectral ratio Cheeger cut (1-Spec) (Hein & B\u00fchler, 2010); nonnegative matrix factorization including Projective NMF (PNMF) (Yang & Oja, 2010), Symmetric 3-Factor Orthogonal NMF (ONMF) (Ding et al., 2006), and LeftStochastic Decomposition (LSD) (Arora et al.", "startOffset": 200, "endOffset": 219}, {"referenceID": 0, "context": ", 2006), and LeftStochastic Decomposition (LSD) (Arora et al., 2011); topic models including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) and Interaction Component Model (ICM) (Sinkkonen et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 6, "context": ", 2011); topic models including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) and Interaction Component Model (ICM) (Sinkkonen et al.", "startOffset": 78, "endOffset": 93}, {"referenceID": 11, "context": ", 2011); topic models including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) and Interaction Component Model (ICM) (Sinkkonen et al., 2008).", "startOffset": 132, "endOffset": 156}, {"referenceID": 9, "context": "The hyperparameters in ICM are automatically adjusted by using Minka\u2019s method (Minka, 2000).", "startOffset": 78, "endOffset": 91}, {"referenceID": 8, "context": "We extracted the scattering features (Mallat, 2012) for image data except Isolet and Mfeat which have their own feature representation.", "startOffset": 37, "endOffset": 51}], "year": 2012, "abstractText": "Clustering analysis by nonnegative lowrank approximations has achieved remarkable progress in the past decade. However, most approximation approaches in this direction are still restricted to matrix factorization. We propose a new low-rank learning method to improve the clustering performance, which is beyond matrix factorization. The approximation is based on a twostep bipartite random walk through virtual cluster nodes, where the approximation is formed by only cluster assigning probabilities. Minimizing the approximation error measured by Kullback-Leibler divergence is equivalent to maximizing the likelihood of a discriminative model, which endows our method with a solid probabilistic interpretation. The optimization is implemented by a relaxed Majorization-Minimization algorithm that is advantageous in finding good local minima. Furthermore, we point out that the regularized algorithm with Dirichlet prior only serves as initialization. Experimental results show that the new method has strong performance in clustering purity for various datasets, especially for large-scale manifold data.", "creator": "LaTeX with hyperref package"}}}