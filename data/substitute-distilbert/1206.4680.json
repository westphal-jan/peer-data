{"id": "1206.4680", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Fast Prediction of New Feature Utility", "abstract": "we study the new feature utility prediction problem : statistically testing whether attached a feedback curve to the data representation automatically improve predictive accuracy at a supervised learning pathway. in many applications, identifying new informative features is the primary platform for improving performance. however, evaluating every potential feature by re - training the predictor with it can be costly. the paper describes an efficient, learner - independent apparatus for estimating new feature utility without re - training based on the current predictor'll outputs. the method is obtained by deriving a matching between loss reduction potential causing the residual feature's correlation with the loss gradient of the current predictor. this leads to every simple yet powerful hypothesis testing procedure, assuming which we prove consistency. our theoretical analysis is accompanied by empirical evaluation on large benchmarks demonstrating a large - scale industrial dataset.", "histories": [["v1", "Mon, 18 Jun 2012 15:38:18 GMT  (875kb)", "http://arxiv.org/abs/1206.4680v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG math.ST stat.TH", "authors": ["hoyt a koepke", "mikhail bilenko"], "accepted": true, "id": "1206.4680"}, "pdf": {"name": "1206.4680.pdf", "metadata": {"source": "CRF", "title": "Fast Prediction of New Feature Utility", "authors": ["Hoyt Koepke", "Mikhail Bilenko"], "emails": ["hoytak@stat.washington.edu", "mbilenko@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "In many mature learning applications, training algorithms are advanced and well-tuned, making the discovery and addition of new, informative features the primary driver of error reduction. New feature design strategies include addition of previously unused descriptive signal sources, as well as various methods that derive new features from the existing representation. A newly proposed feature is typically evaluated by augmenting it to the data representation and re-running the training and validation procedures to observe the resulting difference in predictive accuracy. However, re-training carries significant costs in many real-world applications: \u2022 Computational costs: large-scale domains (e.g.,\nweb search and advertising) often employ com-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nputationally expensive learners and large training datasets. This imposes experimentation latency that is a barrier to rapid feature prototyping. \u2022 Logistical costs: training processes for industry tasks are often componentized across large infrastructure pipelines, running which requires domain expertise. Potential feature contributors lacking such expertise are hence deterred from evaluating their features by the training pipeline complexity. \u2022 Monetary costs: in the domains of medical and finance applications, new feature values may be unavailable for the entire training set or may carry non-negligible costs, calling for methods that predict their utility based on a sampled subset.\nThese costs call for feature utility prediction methods that do not rely on re-training, instead viewing the learner as a black box constructing a bestpossible predictor from a chosen model class. The black-box assumption implies that the only description of the learned predictor is provided via its evaluation on labeled data (e.g., on a hold-out set or via crossvalidation), on which its outputs are compared with true target values via a task-appropriate loss function. Thus, our objective is to design a computationally inexpensive algorithm for statistically determining whether adding a new feature can potentially reduce the expected loss, given the current predictor.\nTo derive a principled algorithm for the problem, we prove that under mild assumptions, testing whether a feature can yield predictive accuracy gains is equivalent to testing its correlation with the negative loss gradient, against which we train a squared-loss regressor. To construct a provably consistent hypothesis test, we form a null distribution by bootstrapping the marginal distributions. The overall algorithm is easily parallelizable, does not require re-training, and works on subsampled datasets, making it particularly appropriate for large data contexts. The method is applicable to a wide variety of learning tasks, requiring only estimates of the functional gradient of the loss, which can be approximated even for discontinuous losses that are common in structured tasks, i.e., ranking.\nThe rest of the paper is organized as follows: Section 2 describes related work, followed by Section 3 that formally defines the problem and motivates the approach. Section 4 describes our method and provides theoretical analysis. Section 5 summarizes empirical evaluation of the approach, followed by discussion of future work and conclusions in Sections 6 and 7."}, {"heading": "2. Related Work", "text": "The task addressed in this paper \u2013 efficient estimation of predictive utility for new features without re-training \u2013 is related yet distinct from three known problems: feature selection (Guyon & Elisseeff, 2003), active feature acquisition (Saar-Tsechansky et al., 2009), and feature extraction (Krupka et al., 2008). While these tasks also involve estimating measures of feature importance, they have different objectives. Critically, many techniques for these problems rely on re-training, while our motivation is avoiding it.\nIn contrast to our setting, where the objective is to efficiently triage new features for addition, feature selection aims to remove unnecessary existing features (Guyon & Elisseeff, 2003). Representatives include wrapper approaches that utilize multiple rounds of re-training with feature subsets, methods that use prediction results for instances with permuted or distorted feature values (Breiman, 2001; Kononenko, 1994), and filter techniques that rely on joint statistics of features and class labels (Song et al., 2007).\nFeature acquisition aims to incrementally select individual feature values for addition to the dataset via estimating their expected utility, and can be viewed as a feature-focused variant of active learning (Lizotte et al., 2003; Saar-Tsechansky et al., 2009). Proposed solutions rely on expensive value-of-information computation, making them prohibitive for our setting. Feature extraction methods attempt to construct new joint features that combine individual attributes by evaluating their dependency structure (Della Pietra et al., 1997; Krupka et al., 2008). In contrast, our approach seeks to directly evaluate the possibility of improvement in prediction accuracy for new features.\nOn the theoretical side, several approaches have used bootstrapping or permutation tests to assess predictive value of features (Fromont, 2007; Anderson & Robinson, 2001; Ojala & Garriga, 2010). These methods typically utilize the tests to assess the generalizability of results obtained on the finite sample case, a wellknown property (Van der Vaart & Wellner, 1996).\nAlso of note is recent work on testing for the statistical independence of features, a key component of our analysis (Gretton & Gyo\u0308rfi, 2010). In particular,\nthere has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperumbudur et al., 2010; Song et al., 2007; Gretton et al., 2008). While our approach also relies on functional analysis techniques, it provides an alternative that does not rely on kernels, instead using standard correlation methods, similarly in spirit to (Huang, 2010)."}, {"heading": "3. New Feature Utility & Independence", "text": "We consider the standard inductive learning setting, where training data is a set of samples of random variable pairs, (Xi, Yi) from an unknown joint distribution function PX,Y , corresponding to data instances described by feature values Xi and prediction targets Yi. Learning corresponds to finding a predictor function f0 from some function class FX that minimizes expected loss EL(f(X), Y ) for a given loss function L encoding the application-appropriate error measure:\nf0 = argmin f\u2208FX\nEL(f(X), Y ) (1)\nThe new feature utility prediction problem can be posited as determining whether adding an additional random variable, X \u2032, to the data representation can result in reduction of expected loss if the predictor was re-trained with it. We designate the function class for predictors on the resulting representation as FX,X\u2032 = F ; it subsumes function classes FX and FX\u2032 which are restricted to predictors that depend only on feature sets X or X \u2032 respectively. Formally,\nFX = { f \u2208 F : \u2203 g s.t. f(X,X \u2032) a.s.= g(X) } FX\u2032 = { f \u2208 F : \u2203 g s.t. f(X,X \u2032) a.s.= g(X \u2032)\n} Then, the new feature utility prediction problem can be formalized as the hypothesis test of:\n(H1) min f\u2208F\nEL(f(X,X \u2032), Y ) < EL(f0(X), Y )\nagainst the null hypothesis H0 in which they are equal. In other words, we define feature utility as the capability of the feature to lower the expected predictor loss in the infinite sample case (i.e., w.r.t. to the true distributions). Thus, we use the theoretical paradigm in which our \u201ctest set\u201d is the true distribution.\nTo motivate the approach, consider the ideal feature evaluation test: determining whether X \u2032 \u22a5 Y | X, i.e. if X \u2032 is independent of Y given X. If this is answered in the affirmative \u2013 the null hypothesis H0 is true \u2013 then X \u2032 contains no additional information about Y that is not already contained in X, and hence the loss cannot be reduced. Otherwise, knowing X \u2032 provides information that can be exploited to construct a better predictor as long as F is sufficiently rich. However, this ideal test is expensive to perform (Huang, 2010; Song, 2009; Su & White, 2008).\nInstead of the ideal conditional independence test, we consider a more restricted test that seeks to determine whether X \u2032 \u22a5 \u03bb(f0(X), Y ) for some function \u03bb capable of capturing the part of Y that could not be predicted by f0(X). If this is answered in the affirmative for an appropriate \u03bb, then X \u2032 contains no additional information about Y that is not already contained in X. Otherwise, knowing X \u2032 provides some new information that can be utilized to create a better predictor. In the next section, we show that for a broad class of loss functions and predictor classes, we can construct a \u03bb and test independence by maximizing the correlation between g(X \u2032) and \u03bb(f0(X), Y ) for g \u2208 FX\u2032 . We show this test to be equivalent to testing:\n(H \u20321) \u2203 g \u2208 FX\u2032 such that EL(f0(X) + g(X \u2032), Y ) < EL(f0(X), Y )\nagainst the null (H \u20320) in which such g does not exist. In the following sections, we develop a consistent test for (H \u20321) against (H \u2032 0) under mild regularity assumptions on the predictor class and the loss function L(\u00b7, Y )."}, {"heading": "4. A Consistent Feature Utility Test", "text": "In this section, we present a theoretical description of our approach and prove, under reasonable assumptions, that it provides an accurate test of whether a new feature X \u2032 can improve prediction performance. The key part of the proof, detailed in Section 4.2, is the use of the bootstrap to test for the statistical independence of the new feature to a residual function of the current predictions. To set up this test, we first list and discuss several assumptions on the predictor class and the loss function. Then, in Section 4.1, we show a sequence of equivalent formulations of our problem in the context of the true joint distribution of (X,X \u2032, Y ). The goal, reached in statement T4 of Theorem 4.2, is a formulation that can be accurately tested in the finite sample case using the bootstrap. This formulation, combined with a way to handle the optimization component of the bootstrap test, leads to the practical algorithm presented in Section 4.3.[Above summary\ncan be com-\npressed/culled\nas-neeeded.]\nLoss Function Assumptions. Our assumptions on the loss minimized when searching for a predictor from F are quite weak: finiteness, a type of strict monotonicity, and an available direction of descent: L1. Finiteness: \u2200f \u2208 F , E |L(f(X,X \u2032), Y )| <\u221e. L2. Weak augmenting functional convexity:\nFor all f \u2208 FX and g \u2208 FX\u2032 such that EL(f(X), Y ) \u2265 EL(f(X) + g(X \u2032), Y ) + \u03b7 for some \u03b7 > 0, there \u2203\u03b2 > 0 s.t. \u2200\u03b1 \u2208 [0, 1], EL(f(X), Y ) \u2265 EL(f(X) + \u03b1g(X \u2032), Y ) + \u03b1\u03b2\u03b7.\nL3. A functional descent direction: Let f0 be optimal in FX as given by (1). Then either f0 is also optimal in F , or there\nexists a random variable \u039bf0 dependent on (X,X \u2032, Y ), with std (\u039bf0) = 1, such that, \u2200h \u2208 F in a sufficiently small neighborhood of f0, E [\u039bf0(h(X,X \u2032)\u2212 f0(X))] > 0 implies that EL(h(X,X \u2032), Y ) < EL(f(X), Y ).\nCondition (L2) essentially imposes a type of strict monotonicity on the function class away from the minimum. It is much weaker than convexity; all it requires is that moving in the direction of a better optimizer gives some improvement, even if it is relatively small. Condition (L3) intuitively says there exists a direction along which improvement in the expected loss is guaranteed \u2013 provided improvement is possible.\nThese assumptions are quite weak and cover many non-convex, discontinuous loss functions. In the case of a convex, differentiable loss function, however, we show below that conditions (L2) and (L3) are satisfied and \u039bf0 has an easy and natural form.\nPrediction Functions Assumptions. The assumptions on the classes of prediction functions F , FX and FX\u2032 needed to prove consistency are the following: F1. Closure under scaling: cf \u2208 F \u2200 f \u2208 F , c \u2208 R+. F2. Closure under shifting: d+f \u2208 F \u2200 f \u2208 F , d \u2208 R. F3. min\nf\u2208F EL(f(X,X \u2032), Y )\u2264min g\u2208FX\u2032 EL(f0(X)+g(X \u2032), Y ).\nF4. \u2200f \u2208 F , f(X,X \u2032) is bounded, or, generally, F is P -Donsker (Van der Vaart & Wellner, 1996).\nThese conditions, while seemingly obscure, are generally satisfied by most modern predictor classes. Condition (F3) states that training on all features will result in a better predictor than one obtained by first training on a subset of features, then \u201cpatching\u201d it with the remaining features. This bound guarantees that the improvement in loss on the full predictor can only be greater than that obtained by a decomposed model.\nThe assumption (F4) \u2013 that F is P -Donsker \u2013 bounds the flexibility of the class of classifiers. Intuitively, it means that when working with an asymptotically large sample, the behavior of the classifier is not inordinately dominated by a few outlier values. This assumption ensures the behavior of the bootstrap is reasonable \u2013 two classifiers trained on different bootstrapped samples should not be wildly different. The boundedness of f \u2208 F , along with measurability assumptions, implies this (Van der Vaart & Wellner, 1996), but virtually all machine learning algorithms used in practice satisfy this assumption.\nCorrelated Features and the XOR Problem. An obvious question to ask is: what about new features that are only useful in conjunction with existing features (of which Y = X1 XOR X2, with X1, X2 iid\u223c Bernoulli ( 1 2 ) is the canonical example),\nwouldn\u2019t assumption (L3) be violated and the approach fail to predict their utility? This intuition is correct; however, when this situation occurs in practice, it is rarely in the absence of other modeling information indicating that it might be the case. As our method allows for multiple variables to be tested as a block, or existing features to be recycled by setting one or more dimension of X \u2032 to specific dimensions of X, such cases can easily be handled.\nConnection to Convex Loss Functions. In the case of a convex, differentiable loss functions, we show that (L2) and (L3) are satisfied, and direction of descent \u039bf0 can be defined by the distribution of the negative gradient of the loss function, making it easily computable in practice.\nTheorem 4.1: Suppose that for \u2200 y in the support of Y , L(u, y) is convex and differentiable in u and satisfies assumption (L1). Then (L2) and (L3) hold with \u039bf0 given by\n\u039bf0 = \u2212 1\n\u03c3 \u03bb(f0(X), Y ), \u03bb(u, y) =\n\u2202\n\u2202u L(u, y) (2)\nwhere \u03c3 is defined to produce std (\u039bf0) = 1. Proof. (L2)immediately follows;\u2200f \u2208FX and h\u2208FX\u2032,\nL(f(X), Y )\u2212 L(f(X) + \u03b1h(X \u2032), Y ) \u2264 \u03b1 [L(f(X), Y )\u2212 L(f(X) + h(X \u2032), Y )]\nby the definition of convexity; taking expectations yields the result. Now, using (L1) and the differentiability of L, it is easy to show that the Gateaux functional derivative d\u0393(f ; h) (Van der Vaart, 2000) of the functional \u0393(f) = EL(f(X,X \u2032), Y ) is given by d\u0393(f ; h) = E [\u03bb(f(X,X \u2032), Y )h(X,X \u2032, Y )] , with \u03bb given in (2). Now, d\u0393(f ; h) defines a linear operator in functional space in which d\u0393(f ; h) gives the change in \u0393(f) in the direction h. (L3) and (2) immediately follow from geometry."}, {"heading": "4.1. Equivalent Tests", "text": "In this section, we show that testing (H \u20321) is equivalent to testing for the existence of a feature transform positively correlated with the loss gradient, which allows designing a consistent bootstrap algorithm for it.\nWhen evaluating a new feature, we are interested in finding a function in FX\u2032 that improves the expected loss. In light of condition (L3), define\ng0 = argmax g\u2208FX\u2032 : std (g(X\u2032))=1\nE g(X \u2032)\u039bf0 . (3)\nas the function that most closely aligns with a direction of improvement. The following theorem connects improvement in expected loss to this function.\nTheorem 4.2: Suppose the loss function L and predictor class F satisfy conditions (L1)-(L3) and (F1)(F4). Let f0 = argmin f\u2208FX EL(f(X), Y ), and let\n\u039bf0 and g0 be as defined in (L3) and Eq. (3). Then the following are equivalent: T1. \u2203 g \u2208 FX\u2032 that improves the expected loss: EL(f0(X) + g(X \u2032), Y ) < EL(f0(X), Y ). T2. min \u03b2\u2208R+ EL(f0(X) + \u03b2g0(X \u2032), Y ) < EL(f0(X), Y ). T3. E [g0(X \u2032) \u00b7 \u039bf0 ] > 0. T4. E [g0(X \u2032) \u00b7 \u039bf0 ]\u2212 E g0(X \u2032)E\u039bf0 > 0. Proof. First, we show that T1 implies T3. Suppose that T1 is true. By (L2), EL(f0(X) + \u03b1g(X \u2032), Y ) < EL(f0(X), Y ) for all \u03b1 \u2208 (0, 1]. As \u03b1g(X \u2032) \u2208 FX\u2032 \u2200\u03b1, by (F1), E [(\u03b1g(X \u2032)) \u00b7 \u039bf0 ] > 0 for \u03b1 > 0 sufficiently small. Now, by (F1), g(X \u2032)/ std g(X \u2032) \u2208 FX\u2032 , thus E [g0(X \u2032) \u00b7 \u039bf0 ] \u2265 E {[g(X \u2032)/ std (g(X \u2032)) ] \u00b7 \u039bf0} > 0 as g(X \u2032)/ std (g(X \u2032)) is included in the optimization of equation (3). Now, by (L3), T3 implies that EL(f0(X) + \u03b20g0(X \u2032), Y ) < EL(f0(X), Y ) for some \u03b20 > 0 sufficiently small. T2 follows, as\nmin\u03b2\u2208R+ EL(f0(X) + \u03b2g0(X \u2032), Y ) \u2264 EL(f0(X) + \u03b20g0(X \u2032), Y ) < EL(f0(X), Y )\nT2 trivially implies T1, thus T1 - T3 are equivalent. For the equivalence of T3 and T4 we show that, given the closure of F under constant shifts, E\u039bf0 = 0.\nSuppose E\u039bf0 = c 6= 0. Let \u03b5 > 0, and consider the function g0(\u00b7) = \u03b5c. Now E \u03b5c\u039bf0 = \u03b5c2 > 0. As f0(\u00b7) + \u03b5c \u2208 FX \u2282 F by (F2), (L3) implies that, for \u03b5 sufficiently small, EL(f0(X) + \u03b5c, Y ) < EL(f0(X), Y ). This contradicts the optimality of f0 in FX ; thus E\u039bf0 = 0, making T3 equivalent to T4."}, {"heading": "4.2. A Consistent Hypothesis Test", "text": "The above proofs work for random variables with respect to their true distributions; the bridge between this and a practical algorithm is the bootstrap. As discussed earlier, we are interested in assessing the performance of our predictor on the true distribution, which requires a consistent test of whether\n(H\u22171 ) \u2203 g\u2208FX\u2032 s.t. E [g(X \u2032)\u2212 E g(X \u2032)] [\u039bf0\u2212 E\u039bf0 ]>0 against the null, where equality holds. By Theorem 4.2, we have that test (H\u22171 ) is equivalent to (H \u2032 1).\nIn the next theorem, we define an accurate hypothesis test of (H\u22171 ) and prove its consistency. The last step needed is a good estimator \u039b\u0302f0 of \u039bf0 generated by the minimum-risk predictor f0 \u2208 FX on the true distribution. This is because we show the bias in the bootstrap to be controlled by the standard deviation of \u221a n ( \u039bf0\u2212\u039b\u0302f0 ) ; in general, this is non-zero.\nThere are several ways to effectively control this bias. One can assume that the true \u039bf0 is known or comes from training on a much larger dataset, against which the performance is actually evaluated. This is a com-\nmon scenario for many large-data domains. Also, one can use methods known to asymptotically reduce the bias, such as k-fold cross validation (Cornec, 2010).\nTheorem 4.3: Let K(Un, Vn) = \u221a n { max g\u2208FX\u2032 ( E g(Un)Vn \u2212 [E g(Un)] [EVn] )} Let X\u0303 \u2032 D =X \u2032 be a bootstrap sample of X \u2032, and let \u039b\u0303f0 D = \u039b\u0302f0 be a bootstrap sample of \u039b\u0302f0, independent from X\u0303 \u2032. Let F be the c.d.f. of K(X\u0303 \u2032,\u039b\u0303f0), with quantile\nfunction F\u22121(t) = inf { u>0 : P ( K(X\u0303 \u2032, \u039b\u0303f0)>u ) \u2264 t } . Fix \u03b1 \u2208 (0, 1), and set the critical point c\u03b1 = F\u22121(\u03b1). The test that accepts the alternative hypothesis, (H\u22171 ), for values of K(X\n\u2032,\u039bf0) \u2265 c\u03b1, and rejects otherwise, has asymptotic level \u03b1 and bias at most max (F (c\u03b1 + \u03b7)\u2212 \u03b1, \u03b1\u2212 F (c\u03b1 \u2212 \u03b7)), where \u03b7 = \u221a\n8n \u221a\n1\u2212 E Pn\u039bf0\u039b\u0302f0 .\nProof. Drop the subscript f0 from \u039b and \u039b\u0302 for convenience. From Th.4.2, we need a consistent test of\n\u2016H \u2212 P \u00d7Q\u2016G = maxg\u2208FX\u2032 \u2223\u2223\u2223E g(X \u2032)\u039b\u0302\u2212 E g(X \u2032)E \u039b\u0302\u2223\u2223\u2223 > 0 where H is the joint measure of X \u2032 and \u039b\u0302, P , Q are the respective marginal distributions, and G = FX\u2032 \u00d7 {I}, where I is the identity function. However, only their empirical samples Hn, Pn, and Qn are available.\nLet P\u0302n and Q\u0302n be the measures formed from an independent bootstrap sample of X \u2032n = (x \u2032 1, ..., x \u2032 n) and \u039b\u0302 = (\u03bb\u03021, ..., \u03bb\u0302n), and let H\u0302n be the joint measure formed from P\u0302n and Q\u0302n. Then let\nZ\u0302n = \u221a n \u2225\u2225\u2225H\u0302n \u2212 P\u0302n \u00d7 Q\u0302n\u2225\u2225\u2225\nG , c\u03b1 = inf\n{ c : P (Z\u0302n > c) \u2264 \u03b1 } under these conditions, we reject the null if\n\u221a n \u2016Hn \u2212 Pn \u00d7Qn\u2016G > c\u03b1.\nFrom (Van der Vaart & Wellner, 1996), this test is consistent with asymptotic level \u03b1. To complete the proof, note that\nmax g\u2208FX\u2032 \u2223\u2223\u2223E g(X \u2032)\u039b\u0302\u2212 E g(X \u2032)E \u039b\u0302\u2223\u2223\u2223 = max g\u2208FX\u2032\n\u2223\u2223E g(X \u2032)\u039b\u2212 E g(X \u2032)E\u039b\u2223\u2223 \u2295 max g\u2208FX\u2032\n\u2223\u2223\u2223E g(X \u2032)(\u039b\u2212 \u039b\u0302)\u2212 E g(X \u2032)E(\u039b\u2212 \u039b\u0302)\u2223\u2223\u2223 where a = b\u2295 c denotes |a\u2212 b| \u2264 c. Now\nmax g\u2208FX\u2032 \u2223\u2223\u2223E g(X \u2032)(\u039b\u2212 \u039b\u0302)\u2212 E g(X \u2032)E(\u039b\u2212 \u039b\u0302)\u2223\u2223\u2223 \u2264 max g\u2208FX\u2032\n\u2223\u2223\u2223E g(X \u2032)(\u039b\u2212 \u039b\u0302)\u2223\u2223\u2223+ \u2223\u2223\u2223E g(X \u2032)E(\u039b\u2212 \u039b\u0302)\u2223\u2223\u2223 \u2264 max g\u2208FX\u2032 2 \u221a E g2(X \u2032)E ( \u039b\u2212 \u039b\u0302 )2 (4)\n\u2264 2 \u221a E ( \u039b2 \u2212 2\u039b\u039b\u0302 + \u039b\u03022 ) = 2 \u221a 2 \u221a 1\u2212 E\u039b\u039b\u0302 (5)\nwhere (4) follows from Liapunov\u2019s and Jensen\u2019s in-\nequalities, and (5) uses std(\u039b) = 1. Combing this result with the bootstrap completes the proof."}, {"heading": "4.3. A Feature Evaluation Algorithm", "text": "Performing the test requires obtaining a transform that maximizes the inner product between the function and the negative loss gradient. The following theorem allows doing this by via squared-error loss minimization for an appropriately weighted expectation. Theorem 4.4: Suppose F satisfies assumptions (F1) and (F2), and let \u039bf0 be defined as in (L3). Let\nf\u22171 = argmin g\u2208FX\u2032\nE [ g(X \u2032)\u2212 (\u039bf0 \u2212 E\u039bf0) ]2 f\u22172 = argmax\ng\u2208FX\u2032 : std(g(X\u2032))=1 E [ g(X \u2032)\u039bf0 ] \u2212 [ E g(X \u2032) ] [E\u039bf0 ]. (6)\nThen f\u22172 (\u00b7) a.s. = f\u22171 (\u00b7)/ std (f\u22171 (X \u2032)) .\nProof. Let Z = (\u039bf0 \u2212 E\u039bf0). Now, consider Eq.(6). We can enforce std(g(X \u2032)) = 1 as follows:\nf\u22172 = argmax g\u2208FX\u2032\nE {[ g(X \u2032)\nstd(g(X \u2032))\n] Z } = argmax\ng\u2208FX\u2032\nE (g(X \u2032)Z) std(g(X \u2032)) .\nThis is invariant to scaling of g, and FX\u2032 is closed under scaling, so g is scaled to make std(g(X \u2032)) = std(f\u22171 (X\n\u2032)). Furthermore, E {(g(X \u2032)+c)Z} = E g(X \u2032)Z+E cZ= E g(X \u2032)Z, so g is invariant to shifts. As FX\u2032 is closed under shifts, g is shifted so that E g(X \u2032) = 0. Thus g\u2217= argmax\ng\u2208G E g(X \u2032)Z= argmin g\u2208G E g2(X \u2032)\u22122E g(X \u2032)Z+ EZ2\nwhere G={g\u2208FX\u2032 : std(g(X \u2032))=std(f\u22171 (X \u2032)), E g(X \u2032)=0} . Thus g\u2217 is exactly f\u22171 , proving the theorem.\nThe practical implication of the theorem is that for new feature values X\u2032n = (X \u2032 1, ..., X \u2032 n) and standardized gradient samples \u039b\u0302n = (\u039b\u03021, ..., \u039b\u0302n), K in Theorem 4.3 becomes\ng\u2217= argmin g\u2208FX\u2032\n1 n n\u2211 i=1 [ g(X \u2032i)\u2212\u039b\u0302i ]2 = argmin g\u2208FX\u2032 E [ g(X\u2032n)\u2212\u039b\u0302n ]2 K(X\u2032n, \u039b\u0302n) = \u221a n [ E g\u2217(X\u2032n)\u039b\u0302n\u2212 E g\u2217(X\u2032n) \u00b7 E \u039b\u0302n ] (7)\nwhich corresponds to least-squares regression regardless of loss L. This surprising result allows reducing the new feature utility problem for a wide array of learning tasks and loss functions to a standard task for which powerful algorithms are readily accessible.\nUsing the bootstrap, this method is turned into a rigorous test for feature significance, summarized in Algorithm 1. The p-value score corresponds to rejecting or accepting the hypothesis that the new value will lead to loss reduction. The algorithm also outputs the number of null standard deviations by which the test statistic v is above the null mean (the z-score), here refered to as the utility score. As empirical evaluation demonstrates, this score provides an accurate measure\nAlgorithm 1: Feature Relevance Test\nInput: (X \u2032i, \u039b\u0302i), i = 1, ..., n. Output: Relevance Score of X \u2032 (p-value). v \u2190 K(X\u2032n, \u039b\u0302n), // K defined in (7). for i = 1, ..., Nbootstrap do\nX\u0303\u2032 \u2190 i.i.d. sample of n values from X\u2032n. \u039b\u0303 \u2020 \u2190 i.i.d. sample of n values from \u039b\u0302n.\n\u039b\u0303 \u2190 ( \u039b\u0303 \u2020 \u2212mean(\u039b\u0303\u2020) ) / std ( \u039b\u0303 \u2020)\nti \u2190 K(X\u0303\u2032, \u039b\u0303), return utility score as (v \u2212mean(t))/ std(t),\np-value as prop. of t1,...,tn greater than v.\nof relative feature utility, allowing to rank features for which no null statistic ti is greater than v.\nThe method scales well to large-data tasks as the Nbootstrap + 1 evaluations of K can be easily parallelized, X \u2032 is typically lower-dimensional than X, and efficient distributed algorithms for least-squares regression are well-studied (Bekkerman et al., 2012).\nIt is important to note that training a regressor g to maximize correlation with the loss function gradient is central to AnyBoost and MART views of boosting as gradient descent in function space (Mason et al., 1999; Friedman, 2001). Analogously, our approach can be viewed as coordinate descent in function space."}, {"heading": "5. Experimental Evaluation", "text": ""}, {"heading": "5.1. Datasets", "text": "We evaluate the proposed approach on three learning tasks: calibrated binary classification, regression and ranking. Standard loss functions are used for each task: cross-entropy (log-loss) for calibrated classification, squared loss for regression, and NDCG for ranking (Ja\u0308rvelin & Keka\u0308la\u0308inen, 2002). Despite the fact that NDCG is discontinuous, it satisfies assumptions (L1)-(L3), and its pointwise functional gradient estimates can be approximated by aggregating pairwise cost differentials as described in (Burges, 2010).\nFor classification and regression, we use standard realtask benchmarks from the UCI collection, Adult and Housing. For ranking, we employ a large-scale industrial search engine dataset, WebRanking. While it uses thousands of individual features, they are grouped\ninto several dozen distinct information sources. Each information source captures some document property and yields multiple numeric features derived from the property for a given query. For example, the DocumentBody source yields features based on the document\u2019s text contents (e.g., various similarity measures w.r.t. the query), while the DocumentAnchorText source yields analogous features based on the annotations of the document\u2019s incoming links. The operational setting for feature utility prediction in this domain is to triage potential new information sources considered for addition to the index, reducing the computational and logistical costs that full re-training would involve. Hence, we overload terminology and refer to each source as a multi-dimensional \u201cfeature\u201d.\nTable 1 summarizes the datasets and loss functions used in the experiments. We employ 10-fold crossvalidation for experiments on Adult and Housing, and hence the number of instances refers to the entire dataset size. For WebSearch, the number of instances refers to the size of the validation fold (the training set is much larger), and the number of features refers to the number of information sources evaluated."}, {"heading": "5.2. Methodology", "text": "Accuracy of feature utility prediction is evaluated w.r.t. actual error improvements obtained via retraining with the new feature included. Experimental procedure can be summarized as follows:\n1. Given dataset X comprised of d features, X(1)..X(d), perform evaluation with all features included to obtain complete data loss L(f\u2217). 2. For each feature X(i), perform evaluation on an ablated dataset which excludes the feature, ob-\ntaining d corresponding predictors f (\u00aci) 0 , i = 1..d. Difference in accuracy \u2206Li = L(f \u2217)\u2212L(f (\u00aci)0 ) for each predictor defines the actual utility of feature X(i). For each trained predictor, per-instance values of loss gradient \u039b\u0302\n\u00aci are obtained. 3. Using Algorithm 1, a p-value and utility score for\neach held-out feature is computed, where the utility score is the correlation K(X(i), \u039b\u0302\u00aci) normalized w.r.t. bootstrap-based null distribution. 4. Feature scores and p-values are compared to the actual utilities \u2206Li.\nIn the above procedure, \u201cevaluation\u201d in steps 1 and 2 refers to 10-fold cross-validation for UCI datasets, and training followed by testing on the validation fold for WebSearch. Gradient boosted trees were used for all tasks (Friedman, 2001), using training loss corresponding to each task. For ranking, the LambdaMART tree boosting algorithm that optimizes NDCG was used (Burges, 2010). Solving for optimal g(\u00b7) with maximal correlation to negative loss gradient and the corresponding bootstrap trials were performed using boosted regression trees, optimizing for squared-error loss as dictated by Theorem 4.4. Bootstrapping was performed for 100 rounds."}, {"heading": "5.3. Results and Discussion", "text": "Per-dataset plots in Figure 1 illustrate predicted vs. actual utilities for each feature, reported as percentages of the range obtained across all features, with actual utilities based on loss reduction due to feature being added, and predicted utilities based on scores produced by Algorithm 1. In other words, the feature with highest actual and predicted utility appears at 100% on horizontal and vertical axes, respectively. Features for which actual utility is significant at p < 0.05 (over validation folds) are demarkated.\nAs the results demonstrate, the proposed method identifies the features that produce actual accuracy gains with very high recall: all features that are determined to be insignificant indeed produce no meaningful ac-\ncuracy gains. While some of the features identified as relevant did not in fact produce sizable accuracy gains, this is expected: while a feature may have some predictive value, the predictor class or learning algorithm may be unable to realize it. The practical motivation for the problem is feature triage, where a feature engineer seeks to quickly prioritize features by their potential for improving prediction quality, and the results demonstrate that our approach indeed provides such prioritization accurately.\nComparison with Feature Selection Heuristics. We also evaluated several commonly used feature selection heuristics that do not rely on re-training. Figure 2 illustrates the performance of \u03c72 Statistic, Information Gain Ratio, and Correlation-based Feature Selection (CFS) (Guyon & Elisseeff, 2003; Hall, 1999) for the Adult dataset. These results demonstrate that methods that compute feature utility greedily (\u03c72 and Information Gain Ratio) can significantly overestimate the value of features that are not informative given others, as evidenced by the two top-scoring features that have near-zero actual utility (in top left corner of corresponding figures). CFS works better as it takes into account the new feature\u2019s correlation with other features as well as the label, yet it underestimates the utility of the best feature dramatically, demonstrating the shortcoming of label-based estimates vs. utilizing losses of the current predictor used by our approach."}, {"heading": "6. Future Work", "text": "While this paper demonstrated that the feature utility prediction problem can be solved by posing it as a hypothesis test in function space, it would be interesting to see alternative algorithms for the problem designed via information-theoretic formulations. Another potentially fruitful direction for future work is developing semi-supervised methods that can utilize unlabeled data for improving new feature utility estimates, given its abundance in large-scale domains. Additionally, designing modifications of the described approach for feature selection, extraction and active feature-value acquisition could yield new efficient methods for these tasks, as the overall idea of exploiting outputs of an existing predictor is clearly relevant for these problems. Finally, another attractive future work direction lies along creating new feature utility prediction algorithms that remove the \u201cblack-box\u201d assumption and utilize properties of a specific learning algorithm or predictor class, possibly yielding better performance."}, {"heading": "7. Conclusions", "text": "This paper considered the problem of predicting new feature utility without re-training the original learner. A solution was proposed based on a consistent testing procedure, derived by establishing a function-space relationship between loss gradient and a maximizing transform of the new features. The approach is general, supporting many common learning tasks and loss functions for which the problem is reduced to squarederror regression. This can be performed for just the new features in isolation or in conjuction with existing features. The resulting algorithm allows easy parallelization, making it appropriate for large-scale domains. Empirical evaluation demonstrated the accuracy of the approach on several learning tasks.\nAcknowledgements: The authors thank Tom Finley for help with ranking experiments and anonymous reviewers for helpful feedback. This work was done while the first author visited Microsoft Research."}], "references": [{"title": "Permutation tests for linear models", "author": ["M.J. Anderson", "J. Robinson"], "venue": "Australian & New Zealand Journal of Statistics,", "citeRegEx": "Anderson and Robinson,? \\Q2001\\E", "shortCiteRegEx": "Anderson and Robinson", "year": 2001}, {"title": "Scaling Up Machine Learning: Parallel and Distributed Approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "Bekkerman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bekkerman et al\\.", "year": 2012}, {"title": "From RankNet to LambdaRank to LambdaMART: An overview", "author": ["C.J.C. Burges"], "venue": "Technical Report MSR-TR2010-82, Microsoft Research,", "citeRegEx": "Burges,? \\Q2010\\E", "shortCiteRegEx": "Burges", "year": 2010}, {"title": "Concentration inequalities of the crossvalidation estimator for empirical risk minimiser", "author": ["M. Cornec"], "venue": "Arxiv preprint arXiv:1011.0096,", "citeRegEx": "Cornec,? \\Q2010\\E", "shortCiteRegEx": "Cornec", "year": 2010}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE PAMI,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Model selection by bootstrap penalization for classification", "author": ["M. Fromont"], "venue": "Machine Learning,", "citeRegEx": "Fromont,? \\Q2007\\E", "shortCiteRegEx": "Fromont", "year": 2007}, {"title": "Consistent nonparametric tests of independence", "author": ["A. Gretton", "L. Gy\u00f6rfi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton and Gy\u00f6rfi,? \\Q2010\\E", "shortCiteRegEx": "Gretton and Gy\u00f6rfi", "year": 2010}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "C.H. Teo", "L. Song", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "Correlation-based feature selection for machine learning", "author": ["M.A. Hall"], "venue": "PhD thesis, The University of Waikato,", "citeRegEx": "Hall,? \\Q1999\\E", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Testing conditional independence using maximal nonlinear conditional correlation", "author": ["T.M. Huang"], "venue": "Annals of Statistics,", "citeRegEx": "Huang,? \\Q2010\\E", "shortCiteRegEx": "Huang", "year": 2010}, {"title": "Cumulated Gain-based evaluation of IR techniques", "author": ["J\u00e4rvelin", "Kalervo", "Kek\u00e4l\u00e4inen", "Jaana"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Estimating attributes: analysis and extensions of RELIEF", "author": ["I. Kononenko"], "venue": "In Proceedings of ECML,", "citeRegEx": "Kononenko,? \\Q1994\\E", "shortCiteRegEx": "Kononenko", "year": 1994}, {"title": "Learning to select features using their properties", "author": ["E. Krupka", "A. Navot", "N. Tishby"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Krupka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krupka et al\\.", "year": 2008}, {"title": "Budgeted learning of N\u00e4\u0131ve-Bayes classifiers", "author": ["D.J. Lizotte", "O. Madani", "R. Greiner"], "venue": "In Proceedings of UAI,", "citeRegEx": "Lizotte et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lizotte et al\\.", "year": 2003}, {"title": "Boosting algorithms as gradient descent in function space", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "In NIPS,", "citeRegEx": "Mason et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mason et al\\.", "year": 1999}, {"title": "Permutation tests for studying classifier performance", "author": ["M. Ojala", "G.C. Garriga"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ojala and Garriga,? \\Q2010\\E", "shortCiteRegEx": "Ojala and Garriga", "year": 2010}, {"title": "Active feature-value acquisition", "author": ["M. Saar-Tsechansky", "P. Melville", "F. Provost"], "venue": "Management Science,", "citeRegEx": "Saar.Tsechansky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Saar.Tsechansky et al\\.", "year": 2009}, {"title": "Testing conditional independence via rosenblatt transforms", "author": ["K. Song"], "venue": "Annals of Statistics,", "citeRegEx": "Song,? \\Q2009\\E", "shortCiteRegEx": "Song", "year": 2009}, {"title": "Supervised feature selection via dependence estimation", "author": ["L. Song", "A. Smola", "A. Gretton", "K.M. Borgwardt", "J. Bedo"], "venue": "In Proceedings of ICML,", "citeRegEx": "Song et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Song et al\\.", "year": 2007}, {"title": "G.R.G. Hilbert space embeddings and metrics on probability measures", "author": ["B.K. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "Lanckriet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "A nonparametric hellinger metric test for conditional independence", "author": ["L. Su", "H. White"], "venue": "Econometric Theory,", "citeRegEx": "Su and White,? \\Q2008\\E", "shortCiteRegEx": "Su and White", "year": 2008}, {"title": "Asymptotic statistics", "author": ["A.W. Van der Vaart"], "venue": null, "citeRegEx": "Vaart,? \\Q2000\\E", "shortCiteRegEx": "Vaart", "year": 2000}, {"title": "Weak convergence and empirical processes", "author": ["A.W. Van der Vaart", "J.A. Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner,? \\Q1996\\E", "shortCiteRegEx": "Vaart and Wellner", "year": 1996}], "referenceMentions": [{"referenceID": 18, "context": "The task addressed in this paper \u2013 efficient estimation of predictive utility for new features without re-training \u2013 is related yet distinct from three known problems: feature selection (Guyon & Elisseeff, 2003), active feature acquisition (Saar-Tsechansky et al., 2009), and feature extraction (Krupka et al.", "startOffset": 240, "endOffset": 270}, {"referenceID": 14, "context": ", 2009), and feature extraction (Krupka et al., 2008).", "startOffset": 32, "endOffset": 53}, {"referenceID": 13, "context": "Representatives include wrapper approaches that utilize multiple rounds of re-training with feature subsets, methods that use prediction results for instances with permuted or distorted feature values (Breiman, 2001; Kononenko, 1994), and filter techniques that rely on joint statistics of features and class labels (Song et al.", "startOffset": 201, "endOffset": 233}, {"referenceID": 20, "context": "Representatives include wrapper approaches that utilize multiple rounds of re-training with feature subsets, methods that use prediction results for instances with permuted or distorted feature values (Breiman, 2001; Kononenko, 1994), and filter techniques that rely on joint statistics of features and class labels (Song et al., 2007).", "startOffset": 316, "endOffset": 335}, {"referenceID": 15, "context": "Feature acquisition aims to incrementally select individual feature values for addition to the dataset via estimating their expected utility, and can be viewed as a feature-focused variant of active learning (Lizotte et al., 2003; Saar-Tsechansky et al., 2009).", "startOffset": 208, "endOffset": 260}, {"referenceID": 18, "context": "Feature acquisition aims to incrementally select individual feature values for addition to the dataset via estimating their expected utility, and can be viewed as a feature-focused variant of active learning (Lizotte et al., 2003; Saar-Tsechansky et al., 2009).", "startOffset": 208, "endOffset": 260}, {"referenceID": 14, "context": "Feature extraction methods attempt to construct new joint features that combine individual attributes by evaluating their dependency structure (Della Pietra et al., 1997; Krupka et al., 2008).", "startOffset": 143, "endOffset": 191}, {"referenceID": 6, "context": "On the theoretical side, several approaches have used bootstrapping or permutation tests to assess predictive value of features (Fromont, 2007; Anderson & Robinson, 2001; Ojala & Garriga, 2010).", "startOffset": 128, "endOffset": 193}, {"referenceID": 21, "context": "In particular, there has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperumbudur et al., 2010; Song et al., 2007; Gretton et al., 2008).", "startOffset": 112, "endOffset": 181}, {"referenceID": 20, "context": "In particular, there has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperumbudur et al., 2010; Song et al., 2007; Gretton et al., 2008).", "startOffset": 112, "endOffset": 181}, {"referenceID": 8, "context": "In particular, there has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperumbudur et al., 2010; Song et al., 2007; Gretton et al., 2008).", "startOffset": 112, "endOffset": 181}, {"referenceID": 11, "context": "While our approach also relies on functional analysis techniques, it provides an alternative that does not rely on kernels, instead using standard correlation methods, similarly in spirit to (Huang, 2010).", "startOffset": 191, "endOffset": 204}, {"referenceID": 11, "context": "However, this ideal test is expensive to perform (Huang, 2010; Song, 2009; Su & White, 2008).", "startOffset": 49, "endOffset": 92}, {"referenceID": 19, "context": "However, this ideal test is expensive to perform (Huang, 2010; Song, 2009; Su & White, 2008).", "startOffset": 49, "endOffset": 92}, {"referenceID": 3, "context": "Also, one can use methods known to asymptotically reduce the bias, such as k-fold cross validation (Cornec, 2010).", "startOffset": 99, "endOffset": 113}, {"referenceID": 1, "context": "The method scales well to large-data tasks as the Nbootstrap + 1 evaluations of K can be easily parallelized, X \u2032 is typically lower-dimensional than X, and efficient distributed algorithms for least-squares regression are well-studied (Bekkerman et al., 2012).", "startOffset": 236, "endOffset": 260}, {"referenceID": 16, "context": "It is important to note that training a regressor g to maximize correlation with the loss function gradient is central to AnyBoost and MART views of boosting as gradient descent in function space (Mason et al., 1999; Friedman, 2001).", "startOffset": 196, "endOffset": 232}, {"referenceID": 5, "context": "It is important to note that training a regressor g to maximize correlation with the loss function gradient is central to AnyBoost and MART views of boosting as gradient descent in function space (Mason et al., 1999; Friedman, 2001).", "startOffset": 196, "endOffset": 232}, {"referenceID": 2, "context": "Despite the fact that NDCG is discontinuous, it satisfies assumptions (L1)-(L3), and its pointwise functional gradient estimates can be approximated by aggregating pairwise cost differentials as described in (Burges, 2010).", "startOffset": 208, "endOffset": 222}, {"referenceID": 2, "context": "WebSearch Ranking 26 741,325 NDCG \u03bb-estimates (Burges, 2010)", "startOffset": 46, "endOffset": 60}, {"referenceID": 5, "context": "Gradient boosted trees were used for all tasks (Friedman, 2001), using training loss corresponding to each task.", "startOffset": 47, "endOffset": 63}, {"referenceID": 2, "context": "For ranking, the LambdaMART tree boosting algorithm that optimizes NDCG was used (Burges, 2010).", "startOffset": 81, "endOffset": 95}, {"referenceID": 10, "context": "Figure 2 illustrates the performance of \u03c7 Statistic, Information Gain Ratio, and Correlation-based Feature Selection (CFS) (Guyon & Elisseeff, 2003; Hall, 1999) for the Adult dataset.", "startOffset": 123, "endOffset": 160}], "year": 2012, "abstractText": "We study the new feature utility prediction problem: statistically testing whether adding a feature to the data representation can improve the accuracy of a current predictor. In many applications, identifying new features is the main pathway for improving performance. However, evaluating every potential feature by re-training the predictor can be costly. The paper describes an efficient, learner-independent technique for estimating new feature utility without re-training based on the current predictor\u2019s outputs. The method is obtained by deriving a connection between loss reduction potential and the new feature\u2019s correlation with the loss gradient of the current predictor. This leads to a simple yet powerful hypothesis testing procedure, for which we prove consistency. Our theoretical analysis is accompanied by empirical evaluation on standard benchmarks and a large-scale industrial dataset.", "creator": "TeX"}}}