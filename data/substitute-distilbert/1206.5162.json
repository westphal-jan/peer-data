{"id": "1206.5162", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2012", "title": "Fast Variational Inference in the Conjugate Exponential Family", "abstract": "we present a general method for deriving collapsed variational inference algo - rithms for probabilistic models in exponential conjugate exponential family. our method unifies many existing mechanisms to collapse differential inference. our collapsed variational analysis leads relatively a new lower bound on the optimal likelihood. we exploit the information geometry of the bound to derive much faster optimization methods based on conjugate constructions for these models. weak approach is very good and is easily applied to any model below the mean field update equations have been derived. empirically we predict significant speed - ups for probabilistic models optimized using our bound.", "histories": [["v1", "Fri, 22 Jun 2012 14:36:15 GMT  (34kb,D)", "https://arxiv.org/abs/1206.5162v1", null], ["v2", "Tue, 4 Dec 2012 19:35:34 GMT  (54kb,D)", "http://arxiv.org/abs/1206.5162v2", "Accepted at NIPS 2012"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james hensman", "magnus rattray", "neil d lawrence"], "accepted": true, "id": "1206.5162"}, "pdf": {"name": "1206.5162.pdf", "metadata": {"source": "CRF", "title": "Fast Variational Inference in the Conjugate Exponential Family", "authors": ["James Hensman", "Magnus Rattray"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Variational bounds provide a convenient approach to approximate inference in a range of intractable models. Classical variational optimisation is achieved through coordinate ascent which can be slow to converge. A popular solution [King and Lawrence, 2006, Teh et al., 2007, Kurihara et al., 2007, Sung et al., 2008, La\u0301zaro-Gredilla and Titsias, 2011, La\u0301zaro-Gredilla et al., 2011] is to marginalize analytically a portion of the variational approximating distribution, removing this from the optimization. In this paper we provide a unifying framework for collapsed inference in the general class of models composed of conjugate-exponential graphs (CEGs).\nFirst we review the body of earlier work with a succinct and unifying derivation of the collapsed bounds. We describe how the applicability of the collapsed bound to any particular CEG can be determined with a simple d-separation test. Standard variational inference via coordinate ascent turns out to be steepest ascent with a unit step length on our unifying bound. This motivates us to consider natural gradients and conjugate gradients for fast optimization of these models. We apply our unifying approach to a range of models from the literature obtaining, often, an order of magnitude or more increase in convergence speed. Our unifying view allows collapsed variational methods to be integrated into general inference tools like infer.net [Minka et al., 2010].\nar X\niv :1\n20 6.\n51 62\nv2 [\ncs .L\nG ]\n4 D\nec 2"}, {"heading": "2 The Marginalised Variational Bound", "text": "The advantages to marginalising analytically a subset of variables in variational bounds seem to be well understood: several different approaches have been suggested in the context of specific models. In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach using both truncated stick-breaking and symmetric priors. Sung et al. [2008] proposed \u2018latent space variational Bayes\u2019 where both the clusterparameters and mixing weights were marginalised, again with some approximations. Teh et al. [2007] proposed a collapsed inference procedure for latent Dirichlet allocation (LDA). In this paper we unify all these results from the perspective of the \u2018KL corrected bound\u2019 [King and Lawrence, 2006]. This lower bound on the model evidence is also an upper bound on the original variational bound, the difference between the two bounds is given by a Kullback Leibler divergence. The approach has also been referred to as the marginalised variational bound by La\u0301zaro-Gredilla et al. [2011], La\u0301zaro-Gredilla and Titsias [2011]. The connection between the KL corrected bound and the collapsed bounds is not immediately obvious. The key difference between the frameworks is the order in which the marginalisation and variational approximation are applied. However, for CEGs this order turns out to be irrelevant. Our framework leads to a more succinct derivation of the collapsed approximations. The resulting bound can then be optimised without recourse to approximations in either the bound\u2019s evaluation or its optimization."}, {"heading": "2.1 Variational Inference", "text": "Assume we have a probabilistic model for data, D, given parameters (and/or latent variables), X, Z, of the form p(D,X,Z) = p(D |Z,X)p(Z |X)p(X). In variational Bayes (see e.g. Bishop [2006]) we approximate the posterior p(Z,X|D) by a distribution q(Z,X). We use Jensen\u2019s inequality to derive a lower bound on the model evidence L, which serves as an objective function in the variational optimisation:\np(D) \u2265 L = \u222b q(Z,X) ln\np(D,Z,X) q(Z,X) dZ dX. (1)\nFor tractability the mean field (MF) approach assumes q factorises across its variables, q(Z,X) = q(Z)q(X). It is then possible to implement an optimisation scheme which analytically optimises each factor alternately, with the optimal distribution given by\nq?(X) \u221d exp {\u222b q(Z) ln p(D,X|Z) dZ } , (2)\nand similarly for Z: these are often referred to as VBE and VBM steps. King and Lawrence [2006] substituted the expression for the optimal distribution (for example q?(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by La\u0301zaro-Gredilla et al. [2011], La\u0301zaro-Gredilla and Titsias [2011]. The resulting bound is not dependent on q(X). King and Lawrence [2006] referred to this new bound as \u2018the KL corrected bound\u2019. The difference between the bound, which we denote LKL, and a standard mean field approximation LMF, is the Kullback Leibler divergence between the optimal form of q\u2217(X) and the current q(X).\nWe rederive their bound by first using Jensen\u2019s inequality to construct the variational lower bound on the conditional distribution,\nln p(D|X) \u2265 \u222b q(Z) ln\np(D,Z|X) q(Z) dZ , L1. (3)\nThis object turns out to be of central importance in computing the final KL-corrected bound and also in computing gradients, curvatures and the distribution of the collapsed variables q?(X). It is easy to see that it is a function of X which lower-bounds the log likelihood p(D |X), and indeed our derivation treats it as such. We now marginalize the conditioned variable from this expression,\nln p(D) \u2265 ln \u222b p(X) exp{L1} dX , LKL, (4)\ngiving us the bound of King and Lawrence [2006] & La\u0301zaro-Gredilla et al. [2011]. Note that one set of parameters was marginalised after the variational approximation was made.\nUsing (2), this expression also provides the approximate posterior for the marginalised variables X:\nq?(X) = p(X)eL1\u2212LKL (5)\nand eLKL appears as the constant of proportionality in the mean-field update equation (2)."}, {"heading": "3 Partial Equivalence of the Bounds", "text": "We can recover LMF from LKL by again applying Jensen\u2019s inequality,\nLKL = ln \u222b q(X) p(X)\nq(X) exp{L1} dX \u2265\n\u222b q(X) ln { p(X)\nq(X) exp{L1}\n} dX, (6)\nwhich can be re-arranged to give the mean-field bound,\nLKL \u2265 \u222b q(X)q(Z) ln { p(D|Z,X)p(Z)p(X)\nq(Z)q(X)\n} dX dZ, (7)\nand it follows that LKL = LMF + KL(q\u2217(X)||q(X)) and1 LKL \u2265 LMF. For a given q(Z), the bounds are equal after q(X) is updated via the mean field method: the approximations are ultimately the same. The advantage of the new bound is to reduce the number of parameters in the optimisation. It is particularly useful when variational parameters are optimised by gradient methods. Since VBEM is equivalent to a steepest descent gradient method with a fixed step size, there appears to be a lot to gain by combining the KLC bound with more sophisticated optimization techniques.\n1We use KL(\u00b7||\u00b7) to denote the Kullback Leibler divergence between two distributions."}, {"heading": "3.1 Gradients", "text": "Consider the gradient of the KL corrected bound with respect to the parameters of q(Z):\n\u2202LKL \u2202\u03b8z = exp{\u2212LKL} \u2202 \u2202\u03b8z\n\u222b exp{L1}p(X) dX = Eq?(X) [\u2202L1 \u2202\u03b8z ] , (8)\nwhere we have used the relation (5). To find the gradient of the mean-field bound we note that it can be written in terms of our conditional bound (3) asLMF = Eq(X) [ L1 + ln p(X)\u2212 ln q(X) ] giving\n\u2202LMF \u2202\u03b8z = Eq(X) [\u2202L1 \u2202\u03b8z ] (9)\nthus setting q(X) = q?(X) not only makes the bounds equal, LMF = LKL, but also their gradients with respect to \u03b8Z .\nSato [2001] has shown that the variational update equation can be interpreted as a gradient method, where each update is also a step in the steepest direction in the canonical parameters of q(Z). We can combine this important insight with the above result to realize that we have a simple method for computing the gradients of the KL corrected bound: we only need to look at the update expressions for the mean-field method. This result also reveals the weakness of standard variational Bayesian expectation maximization (VBEM): it is a steepest ascent algorithm. Honkela et al. [2010] looked to rectify this weakness by applying a conjugate gradient algorithm to the mean field bound. However, they didn\u2019t obtain a significant improvement in convergence speed. Our suggestion is to apply conjugate gradients to the KLC bound. Whilst the value and gradient of the MF bound matches that of the KLC bound after an update of the collapsed variables, the curvature is always greater. In practise this means that much larger steps (which we compute using conjugate gradient methods) can be taken when optimizing the KLC bound than for the MF bound leading to more rapid convergence."}, {"heading": "3.2 Curvature of the Bounds", "text": "King and Lawrence [2006] showed empirically that the KLC bound could lead to faster convergence because the bounds differ in their curvature: the curvature of the KLC bound enables larger steps to be taken by an optimizer. We now derive analytical expressions for the curvature of both bounds. For the mean field bound we have\n\u22022LMF \u2202\u03b82z = Eq(X) [\u22022L1 \u2202\u03b82z ] , (10)\nand for the KLC bound, with some manipulation of (4) and using (5):\n\u22022LKL \u2202\u03b8 [i] z \u2202\u03b8 [j] z = e\u2212LKL \u22022eLKL \u2202\u03b8 [i] z \u2202\u03b8 [j] z \u2212 e\u22122LKL {\u2202eLKL \u2202\u03b8 [i] z }{\u2202eLKL \u2202\u03b8 [j] z } = Eq?(X) [ \u22022L1 \u2202\u03b8 [i] z \u2202\u03b8 [j] z ] + Eq?(X) [ \u2202L1 \u2202\u03b8 [i] z \u2202L1 \u2202\u03b8 [j] z ] \u2212 { Eq?(X) [ \u2202L1 \u2202\u03b8 [i] z ]}{ Eq?(X) [ \u2202L1 \u2202\u03b8 [j] z ]} .\n(11)\nIn this result the first term is equal to (10), and the second two terms combine to be always positive semi-definite, proving King and Lawrence [2006]\u2019s intuition about the curvature of the bound. When curvature is negative definite (e.g. near a maximum), the KLC bound\u2019s curvature is less negative definite, enabling larger steps to be taken in optimization. Figure 1(b) illustrates the effect of this as well as the bound\u2019s similarities."}, {"heading": "3.3 Relationship to Collapsed VB", "text": "In collapsed inference some parameters are marginalized before applying the variational bound. For example, Sung et al. [2008] proposed a latent variable model where the model parameters were marginalised, and Teh et al. [2007] proposed a nonparametric topic model where the document proportions were collapsed. These procedures lead to improved inference, or faster convergence.\nThe KLC bound derivation we have provided also marginalises parameters, but after a variational approximation is made. The difference between the two approaches is distilled in these expressions:\nlnEp(X) [ exp { Eq(Z) [ ln p(D|X,Z) ]}] Eq(Z) [ ln { Ep(X) [ p(D|X,Z) ]}] (12)\nwhere the left expression appears in the KLC bound, and the right expression appears in the bound for collapsed variational Bayes, with the remainder of the bounds being equal. Whilst appropriately conjugate formulation of the model will always ensure that the KLC expression is analytically tractable, the expectation in the collapsed VB expression is not. Sung et al. [2008] propose a first order approximation to the expectation of the form Eq(Z) [ f(Z) ] \u2248 f(Eq(Z) [ Z ] ), which reduces the right expression to the that on the left. Under this approximation2 the KL corrected approach is equivalent to the collapsed variational approach."}, {"heading": "3.4 Applicability", "text": "To apply the KLC bound we need to specify a subset, X, of variables to marginalize. We select the variables that break the dependency structure of the graph to enable the analytic computation of the integral in (4). Assuming the appropriate conjugate exponential structure for the model we are left with the requirement to select a sub-set that induces the appropriate factorisation. These induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in the approximate posterior which arise from the form of the variational approximation and from the structure of the model. These factorisations allow application of KLC bound, and can be identified using a simple d-separation test as Bishop discusses.\nThe d-separation test involves checking for independence amongst the marginalised variables (X in the above) conditioned on the observed data D and the approximated\n2Kurihara et al. [2007] and Teh et al. [2007] suggest a further second order correction and assume that that q(Z) is Gaussian to obtain tractability. This leads to additional correction terms that augment KLC bound. The form of these corrections would need to be determined on a case by case basis, and has in fact been shown to be less effective than those methods unified here [Asuncion et al., 2012].\nvariables (Z in the above). The requirement is to select a sufficient set of variables, Z, such that the effective likelihood for X, given by (3) becomes conjugate to the prior. Figure 1(a) illustrates the d-separation test with application to the KLC bound.\nFor latent variable models, it is often sufficient to select the latent variables for X whilst collapsing the model variables. For example, in the specific case of mixture models and topic models, approximating the component labels allows for the marginalisation of the cluster parameters (topics allocations) and mixing proportions. This allowed Sung et al. [2008] to derive a general form for latent variable models, though our formulation is general to any conjugate exponential graph."}, {"heading": "4 Riemannian Gradient Based Optimisation", "text": "Sato [2001] and Hoffman et al. [2012] showed that the VBEM procedure performs gradient ascent in the space of the natural parameters. Using the KLC bound to collapse the problem, gradient methods seem a natural choice for optimisation, since there are fewer parameters to deal with, and we have shown that computation of the gradients is straightforward (the variational update equations contain the model gradients). It turns out that the KLC bound is particularly amenable to Riemannian or natural gradient methods, because the information geometry of the exponential family distrubution(s), over which we are optimising, leads to a simple expression for the natural gradient. Previous investigations of natural gradients for variational Bayes [Honkela et al., 2010, Kuusela et al., 2009] required the inversion of the Fisher information at every step (ours does not), and also used VBEM steps for some parameters and Riemannian optimisation for other variables. The collapsed nature of the KLC bound means that these VBEM steps are unnecessary: the bound can be computed by parameterizing the dis-\ntribution of only one set of variables (q(Z)) whilst the implicit distribution of the other variables is given in terms of the first distribution and the data by equation (5).\nWe optimize the lower bound LKL with respect to the parameters of the approximating distribution of the non-collapsed variables. We showed in section 2 that the gradient of the KLC bound is given by the gradient of the standard MF variational bound, after an update of the collapsed variables. It is clear from their definition that the same is true of the natural gradients."}, {"heading": "4.1 Variable Transformations", "text": "We can compute the natural gradient of our collapsed bound by considering the update equations of the non-collapsed problem as described above. However, if we wish to make use of more powerful optimisation methods like conjugate gradient ascent, it is helpful to re-parameterize the natural parameters in an unconstrained fashion. The natural gradient is given by [Amari and Nagaoka, 2007]:\ng\u0303(\u03b8) = G(\u03b8)\u22121 \u2202LKL \u2202\u03b8\n(13)\nwhere G(\u03b8) is the Fisher information matrix whose i,jth element is given by\nG(\u03b8)[i,j] = \u2212Eq(X | \u03b8) [\u22022 ln q(X |\u03b8)\n\u2202\u03b8[i]\u2202\u03b8[j]\n] . (14)\nFor exponential family distributions, this reduces to \u22072\u03b8\u03c8(\u03b8), where \u03c8 is the lognormaliser. Further, for exponential family distributions, the Fisher information in the canonical parameters (\u03b8) and that in the expectation parameters (\u03b7) are reciprocal, and we also have G(\u03b8) = \u2202\u03b7/\u2202\u03b8. This means that the natural gradient in \u03b8 is given by\ng\u0303(\u03b8) = G(\u03b8)\u22121 \u2202\u03b7\n\u2202\u03b8 \u2202LKL \u2202\u03b7 = \u2202LKL \u2202\u03b7 and g\u0303(\u03b7) = \u2202LKL \u2202\u03b8 . (15)\nThe gradient in one set of parameters provides the natural gradient in the other. Thus when our approximating distribution q is exponential family, we can compute the natural gradient without the expensive matrix inverse."}, {"heading": "4.2 Steepest Ascent is Coordinate Ascent", "text": "Sato [2001] showed that the VBEM algorithm was a gradient based algorithm. In fact, VBEM consists of taking unit steps in the direction of the natural gradient of the canonical parameters. From equation (9) and the work of Sato [2001], we see that the gradient of the KLC bound can be obtained by considering the standard meanfield update for the non-collapsed parameter Z. We confirm these relationships for the models studied in the next section in the supplementary material.\nHaving confirmed that the VB-E step is equivalent to steepest-gradient ascent we now explore whether the procedure could be improved by the use of conjugate gradients."}, {"heading": "4.3 Conjugate Gradient Optimization", "text": "One idea for solving some of the problems associated with steepest ascent is to ensure each gradient step is conjugate (geometrically) to the previous. Honkela et al. [2010] applied conjugate gradients to the standard mean field bound, we expect much faster convergence for the KLC bound due to its differing curvature. Since VBEM uses a step length of 1 to optimize,3 we also used this step length in conjugate gradients. In the natural conjugate gradient method, the search direction at the ith iteration is given by si = \u2212g\u0303i + \u03b2si\u22121. Empirically the Fletcher-Reeves method for estimating \u03b2 worked well for us:\n\u03b2FR = \u3008g\u0303i, g\u0303i\u3009i\n\u3008g\u0303i\u22121, g\u0303i\u22121\u3009i\u22121 (16)\nwhere \u3008\u00b7, \u00b7\u3009i denotes the inner product in Riemannian geometry, which is given by g\u0303>G(\u03c1)g\u0303. We note from Kuusela et al. [2009] that this can be simplified since g\u0303>Gg\u0303 = g\u0303>GG\u22121g = g\u0303>g, and other conjugate methods, defined in the supplementary material, can be applied similarly."}, {"heading": "5 Experiments", "text": "For empirical investigation of the potential speed ups we selected a range of probabilistic models. We provide derivations of the bound and fuller explanations of the models in the supplementary material. In each experiment, the algorithm was considered to have converged when the change in the bound or the Riemannian gradient reached below 10\u22126. Comparisons between optimisation procedures always used the same initial conditions (or set of initial conditions) for each method. First we recreate the mixture of Gaussians example described by Honkela et al. [2010]."}, {"heading": "5.1 Mixtures of Gaussians", "text": "For a mixture of Gaussians, using the d-separation rule, we select for X the cluster allocation (latent) variables. These are parameterised through the softmax function for unconstrained optimisation. Our model includes a fully Bayesian treatment of the cluster parameters and the mixing proportions, whose approximate posterior distributions appear as (5). Full details of the algorithm derivation are given in the supplementary material. A neat feature is that we can make use of the discussion above to derive an expression for the natural gradient without a matrix inverse.\nIn Honkela et al. [2010] data are drawn from a mixture of five two-dimensional Gaussians with equal weights, each with unit spherical covariance. The centers of the components are at (0, 0) and (\u00b1R,\u00b1R). R is varied from 1 (almost completely overlapping) to 5 (completely separate). The model is initialised with eight components with an uninformative prior over the mixing proportions: the optimisation procedure is left to select an appropriate number of components.\n3We empirically evaluated a line-search procedure, but found that in most cases that Wolfe-Powell conditions were met after a single step of unit length.\nSung et al. [2008] reported that their collapsed method led to improved convergence over VBEM. Since our objective is identical, though our optimisation procedure different, we devised a metric for measuring the efficacy of our algorithms which also accounts for their propensity to fall into local minima. Using many randomised restarts, we measured the average number of iterations taken to reach the best-known optimum. If the algorithm converged at a lesser optimum, those iterations were included in the denomiator, but we didn\u2019t increment the numerator when computing the average. We compared three different conjugate gradient approaches and standard VBEM (which is also steepest ascent on the KLC bound) using 500 restarts.\nTable 1 shows the number of iterations required (on average) to come within 10 nats of the best known solution for three different conjugate-gradient methods and VBEM. VBEM sometimes failed to find the optimum in any of the 500 restarts. Even relaxing the stringency of our selection to 100 nats, the VBEM method was always at least twice as slow as the best conjugate method."}, {"heading": "5.2 Topic Models", "text": "Latent Dirichlet allocation (LDA) [Blei et al., 2003] is a popular approach for extracting topics from documents. To demonstrate the KLC bound we applied it to 200 papers from the 2011 NIPS conference. The PDFs were preprocessed with pdftotext, removing non-alphabetical characters and coarsely filtering words by popularity to form a vocabulary size of 2000.4 We selected the latent topic-assignment variables for parameterisation, collapsing the topics and the document proportions. Conjugate gradient optimization was compared to the standard VBEM approach.\nWe used twelve random initializations, starting each algorithm from each initial condition. Topic and document distributions where treated with fixed, uninformative priors. On average, the Hestenes-Steifel algorithm was almost ten times as fast as standard VB, as shown in Table 2, whilst the final bound varied little between approaches.\n4Some extracted topics are presented in the supplementary material."}, {"heading": "5.3 RNA-seq alignment", "text": "An emerging problem in computational biology is inference of transcript structure and expression levels using next-generation sequencing technology (RNA-Seq). Several models have been proposed. The BitSeq method [Glaus et al., 2012] is based on a probabilistic model and uses Gibbs sampling for approximate inference. The sampler can suffer from particularly slow convergence due to the large size of the problem, which has six million latent variables for the data considered here. We implemented a variational version of their model and optimised it using VBEM and our collapsed Riemannian method. We applied the model to data described in Xu et al. [2010], a study of human microRNA. The model was initialised using four random initial conditions, and optimised using standard VBEM and the conjugate gradient versions of the algorithm. The Polack-Ribie\u0301re conjugate method performed very poorly for this problem, often giving negative conjugation: we omit it here. The solutions found for the other algorithms were all fairly close, with bounds coming within 60 nats. The VBEM method was dramatically outperformed by the Fletcher-Reeves and HestenesSteifel methods: it took 4600\u00b120 iterations to converge, whilst the conjugate methods took only 268\u00b1 4 and 265\u00b1 1 iterations to converge. At about 8 seconds per iteration, our collapsed Riemannian method requires around forty minutes, whilst VBEM takes almost eleven hours. All the variational approaches represent an improvement over a Gibbs sampler, which takes approximately one week to run for this data [Glaus et al., 2012]."}, {"heading": "6 Discussion", "text": "Under very general conditions (conjugate exponential family) we have shown the equivalence of collapsed variational bounds and marginalized variational bounds using the KL corrected perspective of King and Lawrence [2006]. We have provided a succinct derivation of these bounds, unifying several strands of work and laying the foundations for much wider application of this approach.\nWhen the collapsed variables are updated in the standard MF bound the KLC bound is identical to the MF bound in value and gradient. Sato [2001] has shown that coordinate ascent of the MF bound (as proscribed by VBEM updates) is equivalent to steepest\nascent of the MF bound using natural gradients. This implies that standard variational inference is also performing steepest ascent on the KLC bound. This equivalence between natural gradients and the VBEM update equations means our method is quickly implementable for any model where the mean field update equations have been computed. It is only necessary to determine which variables to collapse using a d-separation test. Importantly this implies our approach can readily be incorporated in automated inference engines such as that provided by infer.net [Minka et al., 2010]. We\u2019d like to emphasise the ease with which the method can be applied: we have provided derivations of equivalencies of the bounds and gradients which should enable collapsed conjugate optimisation of any existing mean field algorithm, with minimal changes to the software. Indeed our own implementations (see supplementary material) use just a few lines of code to switch between the VBEM and conjugate methods.\nThe improved performance arises from the curvature of the KLC bound. We have shown that it is always less negative than that of the original variational bound allowing much larger steps in the variational parameters as King and Lawrence [2006] suggested. This also provides a gateway to second-order optimisation, which could prove even faster.\nWe provided empirical evidence of the performance increases that are possible using our method in three models. In a thorough exploration of the convergence properties of a mixture of Gaussians model, we concluded that a conjugate Riemannian algorithm can find solutions that are not found with standard VBEM. In a large LDA model, we found that performance can be improved many times over that of the VBEM method. In the BitSeq model for differential expression of genes transcripts we showed that very large improvements in performance are possible for models with huge numbers of latent variables."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Michalis Titsias for helpful commentary on a previous draft and Peter Glaus for help with a C++ implementation of the RNAseq alignment algorithm. This work was funded by EU FP7-KBBE Project Ref 289434 and BBSRC grant number BB/1004769/1."}, {"heading": "7 Conjugate gradient algorithms", "text": "There are several different methods for approximating the parameter \u03b2 in the conjugate gradient algorithm. We used the Polack-Ribie\u0300re, Fletch-Reeves or Hestenes-Stiefel methods:\n\u03b2PR = \u3008g\u0303i, g\u0303i \u2212 g\u0303i\u22121\u3009i \u3008g\u0303i\u22121, g\u0303i\u22121\u3009i\u22121 \u03b2FR = \u3008g\u0303i, g\u0303i\u3009i\n\u3008g\u0303i\u22121, g\u0303i\u22121\u3009i\u22121\n\u03b2HS = \u3008g\u0303i, g\u0303i \u2212 g\u0303i\u22121\u3009i\n\u3008g\u0303i\u22121, g\u0303i \u2212 g\u0303i\u22121\u3009i\u22121\n(17)\nwhere \u3008\u00b7, \u00b7\u3009i denotes the inner product in Riemannian geometry, which is given by g\u0303>G(\u03c1)g\u0303"}, {"heading": "8 Mixture of Gaussians", "text": "A MoG model is defined as follows. We have a set of N D-dimensional vectors Y = {yn}Nn=1. The likelihood is\np(Y|\u03b7,L) = K\u220f k=1 N\u220f n=1 N (yn|\u00b5k,\u039b \u22121 k ) `nk (18)\nwhere L is a collection of binary latent variables indicating cluster membership, L = {{`nk}Nn=1}Kk=1 and \u03b7 is a collection of cluster parameters, \u03b7 = {\u00b5k,\u039bk}Kk=1\nThe prior over L is given by a multinomial distribution with components \u03c0, which in turn have a Dirichlet prior with uniform concentrations for simplicity:\np(L|\u03c0) = K\u220f k=1 N\u220f n=1 \u03c0`nkk , p(\u03c0) = RD(\u03b1) K\u220f k=1 \u03c0\u03b1\u22121k (19)\nwith \u03b1 representing a K dimensional vector with elements \u03b1, and RD being the normalising constant for the Dirichlet distribution, RD(\u03b1) = \u0393(K\u03b1)\u0393(\u03b1)\u2212K .\nFinally we choose a conjugate Gaussian-Wishart prior for the cluster parameters which can be written\nln p(\u00b5k,\u039bk) = lnRGW (S0, \u03bd0, \u03ba0) + \u03bd0 \u2212D\n2 ln |\u039bk|\n\u22121 2\ntr ( \u039bk ( \u03ba0\u00b5k\u00b5 > k + \u03ba0m0m > 0 \u2212 2\u03ba0m0\u00b5>k + S0 )) (20) where RGW is the normalising constant, and is given by\nRGW (S, \u03bd, \u03ba) = |S| \u03bd 2 2\u2212 (\u03bd+1)D 2 \u03c0\u2212 D(D+1) 4 \u03ba D 2 ( D\u220f d=1 \u0393((\u03bd + 1\u2212 d)/2))\u22121."}, {"heading": "8.1 Applying the KLC bound", "text": "The first task in applying the KLC bound is to select which variables to parameterise and which to marginalise. From the graphical model representation of the MoG problem in Figure 2, we can see that we can select the latent variables Z = {L} for parameterisation, whilst marginalising the mixing proportions and cluster parameters (X = {\u03c0,\u03b7}). We note that it is possible to select the variables the other way around: parameterising \u03c0 and \u03b7 and marginalising L, but parameterisation of the latent variables makes implementation a little simpler.\nWe use a factorised multinomial distribution q(L) to approximate the posterior for p(L|Y), parameterised using the softmax functions so\nq(L) = N\u220f n=1 K\u220f k=1 r`nknk , rnk = e\u03c1nk\u2211K i=1 e \u03c1ni . (21)\nWe are now ready to apply the procedure described above to derive the KLC bound. First, ln p(Y|\u03b7,\u03c0) \u2265 \u222b q(L){ln p(Y|\u03b7,L) + ln p(L|\u03c0)} dL +HL, (22)\nwhere HL is the entropy of the distribution q(L). We expand to give\nL1 = 12 K\u2211 k=1 { \u2212 tr ( \u039bk ( r\u0302k\u00b5k\u00b5 > k + Ck \u2212 2\u00b5ky\u0304k )) +r\u0302k ln\u03c0k + r\u0302k ln |\u039bk| } +HL \u2212 ND2 ln(2\u03c0)\n(23)\nwhere r\u0302k = \u2211N n=1 rnk, Ck = \u2211N n=1 rnkyny > n , and y\u0304k = \u2211N n=1 rnkyn. The conjugacy between the intermediate bound L1 and the prior now emerges, making the second integral in the KLC bound tractable.\nAfter exponentiating this expression and multiplying by the prior, p(\u03b7)p(\u03c0), we find that the integrals with respect to both \u03b7 and \u03c0 are tractable. This result means that the only variational parameters needed are those of q(L). The integrals result in\nLKL = \u2212 ND\n2 ln(2\u03c0) + lnRDi(\u03b1)\u2212 lnRDi(\u03b1\u2032)\n+K lnRGW (S0, \u03bd0, \u03ba0)\u2212 K\u2211 k=1 lnRGW (Sk, \u03bdk, \u03bak) +HL\n(24)\nwhere we have defined\n\u03b1k = \u03b1+ r\u0302k \u03bak = \u03ba0 + r\u0302k\nmk = (\u03ba0m0 + y\u0304k)/\u03bak \u03bdk = \u03bd0 + r\u0302k\nSk = S0 + Ck + \u03ba0m0m > 0 \u2212 \u03bakmkm>k\n(25)\nand \u03b1\u2032 represents a vector containing each \u03b1k. Some simplification of (24) leads to\nLKL = K\u2211 k=1 { ln \u0393(\u03b1k)\u2212 D2 ln\u03bak \u2212 \u03bdk 2 ln |Sk|\n+ D\u2211 d=1 ln \u0393((\u03bdk + 1\u2212 d)/2) } +HL + const.\n(26)\nwhere const. contains terms independent of r. Equations (25) are similar to the update equations for the approximating distributions in the VBEM methodology [see e.g. Bishop, 2006]. However, for our model they are simply intermediate variables, representing combinations of the true variational parameters r, the data, and the model prior parameters. When optimizing the model with respect to the variational parameters, the dependency of these intermediate variables on r is not ignored as it would be in MF variational approach.\nThe gradient of the MV bound (26) with respect to the parameters r is given by\n\u2202LKL \u2202rnk =\u2212 D2 \u03ba \u22121 k \u2212 1 2 ln |Sk|+ \u03c8(\u03b1k)\u2212 ln rnk\n\u2212 \u03bdk2 (yn \u2212mk) >S\u22121k (yn \u2212mk))\n+ 12 D\u2211 d=1 \u03c8((\u03bdk + 1\u2212 d)/2)\u2212 1.\n(27)\nTaking a step in this direction (in the valiables \u03b3) yields exactly the VB-E step associated with the mean-field bound. the gradient in r is the natural gradient in \u03b3 (see paper section 4.1)."}, {"heading": "9 Latent Dirichlet Allocation", "text": "Latent Dirichlet allocation is a popular topic model. See Blei et al. [2003] for a thorough introduction.\nSuppose we have D documents, K topics and a vocabulary of size V . The dth document contains Nd words Wd = {wdn}Ndn=1, and each word is represented as a binary vector wdn \u2208 {0, 1}V . Each word is associated with a latent variable `dn, which assigns the word to a topic, thus `dn \u2208 {0, 1}K . We\u2019ll use W to represent the colletion of all words, W = {Wd}Dd=1, and L to represent the collection of all latent variables L = {{`dn}Ndn=1}Dd=1.\nEach document has an associated vector of topic proportions, \u03b8d \u2208 [0, 1]K , and each topic is represented by a vector of word proportions \u03c6k \u2208 [0, 1]V . We assume a symmetrical prior distribution over topics in each document p(\u03b8d) = Dir(\u03b8d|\u03b1), and similarly for words within topics. p(\u03c6k) = Dir(\u03c6k|\u03b2).\nThe LDA generative model states that for each word, first the associated topic is drawn from the topic proportions for the document, and then the word is drawn from the selected topic.\np(`dn|\u03b8d) = K\u220f k=1 \u03b8`dnkdk\np(wdn|`dn, \u03c6) = K\u220f k=1 V\u220f v=1 \u03c6wdnv`dnkkv\n(28)"}, {"heading": "9.1 The collapsed bound", "text": "To derive the colapsed bound, we use a similar d-separation test as for the mixture model to select the latent variables as the parameteriser (non-collapsed) nodes. See Figure 3.\nTo proceed we assume a factorising multinomial posterior for L:\nq(L) = D\u220f d=1 Nd\u220f n=1 K\u220f k=1 r`dnkdnk (29)\nsubject to the constraint \u2211K k=1 `dnk = 1, which we enforce through a softmax reparameterisation rdnk =\ne\u03c1dnk\u2211K k\u2032=1 e \u03c1dnk\u2032 . (30)\nWe proceed by deriving the conditional bound ln p(W | \u03b8, \u03c6) \u2265 L1 = D\u2211 d=1 Nd\u2211 n=1 K\u2211 k=1 V\u2211 v=1 (wdnvrdnk) ln\u03c6kv+ D\u2211 d=1 Nd\u2211 n=1 K\u2211 k=1 (rdnk ln \u03b8dk)+H[q(L)].\n(31) To marginalise the variables \u03b8, \u03c6, we exponentiate this bound and take the expecta-\ntion under the priors. This results in p(W ) \u2265 \u222b exp{L1}p(\u03b8)p(\u03c6) d\u03b8 d\u03c6 = \u222b K\u220f k=1 V\u220f v=1 \u03c6 \u2211d d=1 \u2211nd n=1(wdnvrdnk) kv D\u220f d=1 K\u220f k=1 \u03b8 ( \u2211Nd n=1 rdnk) dk\nK\u220f k=1 RDi(\u03b2) V\u220f v=1 \u03c6\u03b2\u22121kv\nD\u220f d=1 RDi(\u03b1) K\u220f k=1 \u03b8\u03b1\u22121dk d\u03b8 d\u03c6\nexp{H[q(L)]}. (32)\nCareful inspection of the above reveals that the two integrals separate as expected, and result in the normalizers for each of the independent Dirichlet approximations. Taking the logarithm reults in LKL = D lnRDi(\u03b1)\u2212 D\u2211 d=1 lnRDi(\u03b1 \u2032 d) +K lnRDi(\u03b2)\u2212 K\u2211 k=1 lnRDi(\u03b2 \u2032 k) +H[q(L)] (33) where we have defined \u03b1\u2032dk = \u03b1+ \u2211Nd n=1 rdnk and \u03b2 \u2032 kv = \u03b2+ \u2211d d=1 \u2211nd n=1 wdnvrdnk."}, {"heading": "9.2 Topics found by LDA", "text": "For completeness we show here some topics found by LDA on the NIPS conference data."}, {"heading": "10 BitSeq Model", "text": "The generative model for an RNA-seq assay is as follows. We assume that the experiment consists of a pile of RNA fragments, where the abundance of fragments from transcript Tm in the assay is \u03b8m. The sequencer then selects a fragment at random from the pile, such that the probability of picking a fragment corresponding to transcript Tm is \u03b8m. Introducing a convenient membership vector `n for each read, we can write\np(L|\u03b8) = N\u220f n=1 M\u220f m=1 \u03b8`nmm (34)\nwhere `nm \u2208 {0, 1} is a binary variable which indicates whether the nth fragment came from the mth transcript (`nm = 1) and is subject to \u2211M m=1 `nm = 1. We use L to represent the collection of all alignment variables. Both \u03b8 and L are variables to be inferred, with \u03b8 the main object of interest. Writing the collection of all reads as R = {rn}Nn=1, the likelihood of a set of alignments L is\np(R|T,L) = N\u220f n=1 p(rn|Tm)`nm (35)\nwhere Tm represents the mth transcript, T represents the transcriptome. The values of p(rn|Tm) can be computed before performing inference in \u03b8 since we are assuming a known transcriptome. We compute these values based on the quality of alignment of the read rn to the transcript Tm, using a model which can correct for sequence specific or fragmentation biases. The method is described in detatil in Glaus et al. [2012].\nWe specify a conjugate Dirichlet prior over the vector \u03b8.\np(\u03b8) = \u0393(\u03b1\u0302o)\u220fM\nm=1 \u0393(\u03b1 o m) M\u220f m=1 \u03b8 \u03b1om\u22121 m (36)\nwith \u03b1\u0302o = \u2211M m=1 \u03b1 o m. \u03b1 o m represents our prior belief in the values of \u03b8m, and we use a relatively uninformative but proper prior \u03b1om = 1 \u2200m = 1 . . .M . A priori, we assume that the concentrations are all equal, but with large uncertainty."}, {"heading": "10.1 The collapsed bound", "text": "Figure 4 shows a graphical representation of the BitSeq model. It\u2019s clear that parameterisation of the latent variables will allow us to collapse \u03b8, or vica-versa. Selecting again the latent variables for parameterisation, X = {L}, Z = {\u03b8}, we first find the conditional bound as usual by:\nln p(R |T,\u03b8) = ln \u222b p(R |L,T)p(L |\u03b8) dL\n\u2265 Eq(L) [ ln p(R |L,T) + ln p(L |\u03b8)\u2212 ln q(L) ] \u2265\nN\u2211 n=1 M\u2211 m=1 `nm ( ln p(rn |Tm) + ln \u03b8m \u2212 ln `nm )\n\u2265 L1\n(37)\nIt\u2019s clear that this bound is conjugate to the prior for \u03b8, so we can marginalise:\nln p(R |T) \u2265 LKL = N\u2211 n=1 M\u2211 m=1 `nm ( ln p(rn |Tm)\u2212 ln `nm ) + ln \u0393(\u03b1\u0302o)\u2212 ln \u0393(\u03b1\u0302o +N)\n\u2212 M\u2211 m=1 ( ln \u0393(\u03b1om)\u2212 ln \u0393(\u03b1om + \u02c6\u0300m) ) (38)\nwhere \u02c6\u0300m = \u2211N n=1 `n and we also have that the approximate posterior distribution for \u03b8 is a Dirichlet distribution with parameters \u03b1om + \u02c6\u0300m."}], "references": [{"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Y. Teh"], "venue": "arXiv preprint arXiv:1205.2662,", "citeRegEx": "Asuncion et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2012}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Identifying differentially expressed transcripts from RNA-seq data with biological variation", "author": ["P. Glaus", "A. Honkela", "M. Rattray"], "venue": null, "citeRegEx": "Glaus et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Glaus et al\\.", "year": 2012}, {"title": "Stochastic variational inference", "author": ["M. Hoffman", "D. Blei", "C. Wang", "J. Paisley"], "venue": "arXiv preprint arXiv:1206.7051,", "citeRegEx": "Hoffman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2012}, {"title": "Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes", "author": ["A. Honkela", "T. Raiko", "M. Kuusela", "M. Tornio", "J. Karhunen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Honkela et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Honkela et al\\.", "year": 2010}, {"title": "Fast variational inference for Gaussian process models through KL-correction", "author": ["N. King", "N.D. Lawrence"], "venue": "Machine Learning: ECML", "citeRegEx": "King and Lawrence.,? \\Q2006\\E", "shortCiteRegEx": "King and Lawrence.", "year": 2006}, {"title": "Collapsed variational Dirichlet process mixture models", "author": ["K. Kurihara", "M. Welling", "Y.W. Teh"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Kurihara et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kurihara et al\\.", "year": 2007}, {"title": "A gradient-based algorithm competitive with variational Bayesian EM for mixture of Gaussians", "author": ["M. Kuusela", "T. Raiko", "A. Honkela", "J. Karhunen"], "venue": "In Neural Networks,", "citeRegEx": "Kuusela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kuusela et al\\.", "year": 2009}, {"title": "Variational heteroscedastic Gaussian process regression", "author": ["M. L\u00e1zaro-Gredilla", "M.K. Titsias"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "L\u00e1zaro.Gredilla and Titsias.,? \\Q2011\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla and Titsias.", "year": 2011}, {"title": "Overlapping mixtures of Gaussian processes for the data association problem", "author": ["M. L\u00e1zaro-Gredilla", "S. Van Vaerenbergh", "N. Lawrence"], "venue": "Pattern Recognition,", "citeRegEx": "L\u00e1zaro.Gredilla et al\\.,? \\Q2011\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla et al\\.", "year": 2011}, {"title": "Online model selection based on the variational Bayes", "author": ["M.A. Sato"], "venue": "Neural Computation,", "citeRegEx": "Sato.,? \\Q2001\\E", "shortCiteRegEx": "Sato.", "year": 2001}, {"title": "Latent-space variational Bayes", "author": ["J. Sung", "Z. Ghahramani", "S. Bang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Sung et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2008}, {"title": "A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation", "author": ["Y.W. Teh", "D. Newman", "M. Welling"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Transcriptome and targetome analysis in MIR155 expressing cells using RNA-seq", "author": ["G. Xu"], "venue": "RNA, pages 1610\u20131622,", "citeRegEx": "Xu,? \\Q2010\\E", "shortCiteRegEx": "Xu", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "In this paper we unify all these results from the perspective of the \u2018KL corrected bound\u2019 [King and Lawrence, 2006].", "startOffset": 90, "endOffset": 115}, {"referenceID": 6, "context": "In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach using both truncated stick-breaking and symmetric priors.", "startOffset": 36, "endOffset": 59}, {"referenceID": 6, "context": "In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach using both truncated stick-breaking and symmetric priors. Sung et al. [2008] proposed \u2018latent space variational Bayes\u2019 where both the clusterparameters and mixing weights were marginalised, again with some approximations.", "startOffset": 36, "endOffset": 166}, {"referenceID": 6, "context": "In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach using both truncated stick-breaking and symmetric priors. Sung et al. [2008] proposed \u2018latent space variational Bayes\u2019 where both the clusterparameters and mixing weights were marginalised, again with some approximations. Teh et al. [2007] proposed a collapsed inference procedure for latent Dirichlet allocation (LDA).", "startOffset": 36, "endOffset": 329}, {"referenceID": 6, "context": "In this paper we unify all these results from the perspective of the \u2018KL corrected bound\u2019 [King and Lawrence, 2006]. This lower bound on the model evidence is also an upper bound on the original variational bound, the difference between the two bounds is given by a Kullback Leibler divergence. The approach has also been referred to as the marginalised variational bound by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011].", "startOffset": 91, "endOffset": 405}, {"referenceID": 6, "context": "In this paper we unify all these results from the perspective of the \u2018KL corrected bound\u2019 [King and Lawrence, 2006]. This lower bound on the model evidence is also an upper bound on the original variational bound, the difference between the two bounds is given by a Kullback Leibler divergence. The approach has also been referred to as the marginalised variational bound by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011]. The connection between the KL corrected bound and the collapsed bounds is not immediately obvious.", "startOffset": 91, "endOffset": 441}, {"referenceID": 1, "context": "Bishop [2006]) we approximate the posterior p(Z,X|D) by a distribution q(Z,X).", "startOffset": 0, "endOffset": 14}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011].", "startOffset": 0, "endOffset": 248}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011]. The resulting bound is not dependent on q(X).", "startOffset": 0, "endOffset": 284}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011]. The resulting bound is not dependent on q(X). King and Lawrence [2006] referred to this new bound as \u2018the KL corrected bound\u2019.", "startOffset": 0, "endOffset": 356}, {"referenceID": 6, "context": "giving us the bound of King and Lawrence [2006] & L\u00e1zaro-Gredilla et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 6, "context": "giving us the bound of King and Lawrence [2006] & L\u00e1zaro-Gredilla et al. [2011]. Note that one set of parameters was marginalised after the variational approximation was made.", "startOffset": 23, "endOffset": 80}, {"referenceID": 10, "context": "Sato [2001] has shown that the variational update equation can be interpreted as a gradient method, where each update is also a step in the steepest direction in the canonical parameters of q(Z).", "startOffset": 0, "endOffset": 12}, {"referenceID": 5, "context": "Honkela et al. [2010] looked to rectify this weakness by applying a conjugate gradient algorithm to the mean field bound.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "2 Curvature of the Bounds King and Lawrence [2006] showed empirically that the KLC bound could lead to faster convergence because the bounds differ in their curvature: the curvature of the KLC bound enables larger steps to be taken by an optimizer.", "startOffset": 26, "endOffset": 51}, {"referenceID": 6, "context": "In this result the first term is equal to (10), and the second two terms combine to be always positive semi-definite, proving King and Lawrence [2006]\u2019s intuition about the curvature of the bound.", "startOffset": 126, "endOffset": 151}, {"referenceID": 12, "context": "For example, Sung et al. [2008] proposed a latent variable model where the model parameters were marginalised, and Teh et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 12, "context": "For example, Sung et al. [2008] proposed a latent variable model where the model parameters were marginalised, and Teh et al. [2007] proposed a nonparametric topic model where the document proportions were collapsed.", "startOffset": 13, "endOffset": 133}, {"referenceID": 12, "context": "Sung et al. [2008] propose a first order approximation to the expectation of the form Eq(Z) [ f(Z) ] \u2248 f(Eq(Z) [ Z ] ), which reduces the right expression to the that on the left.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "The form of these corrections would need to be determined on a case by case basis, and has in fact been shown to be less effective than those methods unified here [Asuncion et al., 2012].", "startOffset": 163, "endOffset": 186}, {"referenceID": 0, "context": "These induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in the approximate posterior which arise from the form of the variational approximation and from the structure of the model.", "startOffset": 61, "endOffset": 75}, {"referenceID": 0, "context": "These induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in the approximate posterior which arise from the form of the variational approximation and from the structure of the model. These factorisations allow application of KLC bound, and can be identified using a simple d-separation test as Bishop discusses. The d-separation test involves checking for independence amongst the marginalised variables (X in the above) conditioned on the observed data D and the approximated 2Kurihara et al. [2007] and Teh et al.", "startOffset": 61, "endOffset": 543}, {"referenceID": 0, "context": "These induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in the approximate posterior which arise from the form of the variational approximation and from the structure of the model. These factorisations allow application of KLC bound, and can be identified using a simple d-separation test as Bishop discusses. The d-separation test involves checking for independence amongst the marginalised variables (X in the above) conditioned on the observed data D and the approximated 2Kurihara et al. [2007] and Teh et al. [2007] suggest a further second order correction and assume that that q(Z) is Gaussian to obtain tractability.", "startOffset": 61, "endOffset": 565}, {"referenceID": 12, "context": "This allowed Sung et al. [2008] to derive a general form for latent variable models, though our formulation is general to any conjugate exponential graph.", "startOffset": 13, "endOffset": 32}, {"referenceID": 4, "context": "Sato [2001] and Hoffman et al. [2012] showed that the VBEM procedure performs gradient ascent in the space of the natural parameters.", "startOffset": 16, "endOffset": 38}, {"referenceID": 11, "context": "2 Steepest Ascent is Coordinate Ascent Sato [2001] showed that the VBEM algorithm was a gradient based algorithm.", "startOffset": 39, "endOffset": 51}, {"referenceID": 11, "context": "2 Steepest Ascent is Coordinate Ascent Sato [2001] showed that the VBEM algorithm was a gradient based algorithm. In fact, VBEM consists of taking unit steps in the direction of the natural gradient of the canonical parameters. From equation (9) and the work of Sato [2001], we see that the gradient of the KLC bound can be obtained by considering the standard meanfield update for the non-collapsed parameter Z.", "startOffset": 39, "endOffset": 274}, {"referenceID": 5, "context": "Honkela et al. [2010] applied conjugate gradients to the standard mean field bound, we expect much faster convergence for the KLC bound due to its differing curvature.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "We note from Kuusela et al. [2009] that this can be simplified since g\u0303>Gg\u0303 = g\u0303>GG\u22121g = g\u0303>g, and other conjugate methods, defined in the supplementary material, can be applied similarly.", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": "First we recreate the mixture of Gaussians example described by Honkela et al. [2010].", "startOffset": 64, "endOffset": 86}, {"referenceID": 5, "context": "In Honkela et al. [2010] data are drawn from a mixture of five two-dimensional Gaussians with equal weights, each with unit spherical covariance.", "startOffset": 3, "endOffset": 25}, {"referenceID": 2, "context": "2 Topic Models Latent Dirichlet allocation (LDA) [Blei et al., 2003] is a popular approach for extracting topics from documents.", "startOffset": 49, "endOffset": 68}, {"referenceID": 3, "context": "The BitSeq method [Glaus et al., 2012] is based on a probabilistic model and uses Gibbs sampling for approximate inference.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "All the variational approaches represent an improvement over a Gibbs sampler, which takes approximately one week to run for this data [Glaus et al., 2012].", "startOffset": 134, "endOffset": 154}, {"referenceID": 3, "context": "The BitSeq method [Glaus et al., 2012] is based on a probabilistic model and uses Gibbs sampling for approximate inference. The sampler can suffer from particularly slow convergence due to the large size of the problem, which has six million latent variables for the data considered here. We implemented a variational version of their model and optimised it using VBEM and our collapsed Riemannian method. We applied the model to data described in Xu et al. [2010], a study of human microRNA.", "startOffset": 19, "endOffset": 465}, {"referenceID": 6, "context": "Under very general conditions (conjugate exponential family) we have shown the equivalence of collapsed variational bounds and marginalized variational bounds using the KL corrected perspective of King and Lawrence [2006]. We have provided a succinct derivation of these bounds, unifying several strands of work and laying the foundations for much wider application of this approach.", "startOffset": 197, "endOffset": 222}, {"referenceID": 6, "context": "Under very general conditions (conjugate exponential family) we have shown the equivalence of collapsed variational bounds and marginalized variational bounds using the KL corrected perspective of King and Lawrence [2006]. We have provided a succinct derivation of these bounds, unifying several strands of work and laying the foundations for much wider application of this approach. When the collapsed variables are updated in the standard MF bound the KLC bound is identical to the MF bound in value and gradient. Sato [2001] has shown that coordinate ascent of the MF bound (as proscribed by VBEM updates) is equivalent to steepest", "startOffset": 197, "endOffset": 528}, {"referenceID": 6, "context": "We have shown that it is always less negative than that of the original variational bound allowing much larger steps in the variational parameters as King and Lawrence [2006] suggested.", "startOffset": 150, "endOffset": 175}], "year": 2012, "abstractText": "We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic inference using our bound.", "creator": "TeX"}}}