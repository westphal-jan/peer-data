{"id": "1206.6389", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Poisoning Attacks against Support Vector Machines", "abstract": "we investigate a family of poisoning attacks targeting swarm vector machines ( ks ). such attacks inject specially crafted training schedules ultimately increases the model's test power. central to the motivation for these attacks extends the fact your good learning algorithms assume that their training data comes from a natural or well - behaved distribution. however, this assumption does not generally hold in security - sensitive settings. as we demonstrate, an intelligent adversary can, to some extent, predict the change of the svm's decision function due to dummy input and use this ability to construct malicious data.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (504kb)", "http://arxiv.org/abs/1206.6389v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Fri, 20 Jul 2012 12:33:21 GMT  (184kb,D)", "http://arxiv.org/abs/1206.6389v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v3", "Mon, 25 Mar 2013 10:16:36 GMT  (184kb,D)", "http://arxiv.org/abs/1206.6389v3", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CR stat.ML", "authors": ["battista biggio", "blaine nelson", "pavel laskov"], "accepted": true, "id": "1206.6389"}, "pdf": {"name": "1206.6389.pdf", "metadata": {"source": "META", "title": "Poisoning Attacks against Support Vector Machines", "authors": ["Battista Biggio", "Blaine Nelson", "Pavel Laskov"], "emails": ["battista.biggio@diee.unica.it", "blaine.nelson@wsii.uni-tuebingen.de", "pavel.laskov@uni-tuebingen.de"], "sections": [{"heading": null, "text": "The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM\u2019s optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier\u2019s test error."}, {"heading": "1. Introduction", "text": "Machine learning techniques are rapidly emerging as a vital tool in a variety of networking and large-scale system applications because they can infer hidden patterns in large complicated datasets, adapt to new behaviors, and provide statistical soundness to decisionmaking processes. Application developers thus can\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nemploy learning to help solve so-called big-data problems and these include a number of security-related problems particularly focusing on identifying malicious or irregular behavior. In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer & Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton & Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov & S\u030crndic\u0301, 2011). Unfortunately, in these domains, data is generally not only non-stationary but may also have an adversarial component, and the flexibility afforded by learning techniques can be exploited by an adversary to achieve his goals. For instance, in spam-detection, adversaries regularly adapt their approaches based on the popular spam detectors, and generally a clever adversary will change his behavior either to evade or mislead learning.\nIn response to the threat of adversarial data manipulation, several proposed learning methods explicitly account for certain types of corrupted data (Globerson & Roweis, 2006; Teo et al., 2008; Bru\u0308ckner & Scheffer, 2009; Dekel et al., 2010). Attacks against learning algorithms can be classified, among other categories (c.f. Barreno et al., 2010), into causative (manipulation of training data) and exploratory (exploitation of the classifier). Poisoning refers to a causative attack in which specially crafted attack points are injected into the training data. This attack is especially important from the practical point of view, as an attacker usually cannot directly access an existing training database but may provide new training data; e.g., web-based repositories and honeypots often collect malware examples for training, which provides an opportunity for the adversary to poison the training data. Poisoning attacks have been previously studied only for simple anomaly detection methods (Barreno et al., 2006; Rubinstein et al., 2009; Kloft & Laskov, 2010).\nIn this paper, we examine a family of poisoning attacks against Support Vector Machines (SVM). Following the general security analysis methodology for machine learning, we assume that the attacker knows the learning algorithm and can draw data from the underlying data distribution. Further, we assume that our attacker knows the training data used by the learner; generally, an unrealistic assumption, but in real-world settings, an attacker could instead use a surrogate training set drawn from the same distribution (e.g., Nelson et al., 2008) and our approach yields a worstcase analysis of the attacker\u2019s capabilities. Under these assumptions, we present a method that an attacker can use to construct a data point that significantly decreases the SVM\u2019s classification accuracy.\nThe proposed method is based on the properties of the optimal solution of the SVM training problem. As was first shown in an incremental learning technique (Cauwenberghs & Poggio, 2001), this solution depends smoothly on the parameters of the respective quadratic programming problem and on the geometry of the data points. Hence, an attacker can manipulate the optimal SVM solution by inserting specially crafted attack points. We demonstrate that finding such an attack point can be formulated as optimization with respect to a performance measure, subject to the condition that an optimal solution of the SVM training problem is retained. Although the test error surface is generally nonconvex, the gradient ascent procedure used in our method reliably identifies good local maxima of the test error surface.\nThe proposed method only depends on the gradients of the dot products between points in the input space, and hence can be kernelized. This contrasts previous work involving construction of special attack points (e.g., Bru\u0308ckner & Scheffer, 2009; Kloft & Laskov, 2010) in which attacks could only be constructed in the feature space for the nonlinear case. The latter is a strong disadvantage for the attacker, since he must construct data in the input space and has no practical means to access the feature space. Hence, the proposed method breaks new ground in optimizing the impact of data-driven attacks against kernel-based learning algorithms and emphasizes the need to consider resistance against adversarial training data as an important factor in the design of learning algorithms."}, {"heading": "2. Poisoning attack on SVM", "text": "We assume the SVM has been trained on a data set Dtr = {xi, yi}ni=1, xi \u2208 Rd. Following the standard notation, K denotes the matrix of kernel values between two sets of points, Q = yy> K denotes the label-\nannotated version of K, and \u03b1 denotes the SVM\u2019s dual variables corresponding to each training point. Depending on the value of \u03b1i, the training points are referred to as margin support vectors (0 < \u03b1i < C, set S), error support vectors (\u03b1i = C, set E) and reserve points (\u03b1i = 0, set R). In the sequel, the lower-case letters s, e, r are used to index the corresponding parts of vectors or matrices; e.g., Qss denotes the margin support vector submatrix of Q."}, {"heading": "2.1. Main derivation", "text": "For a poisoning attack, the attacker\u2019s goal is to find a point (xc, yc), whose addition to Dtr maximally decreases the SVM\u2019s classification accuracy. The choice of the attack point\u2019s label, yc, is arbitrary but fixed. We refer to the class of this chosen label as attacking class and the other as the attacked class.\nThe attacker proceeds by drawing a validation data set Dval = {xk, yk}mk=1 and maximizing the hinge loss incurred on Dval by the SVM trained on Dtr\u222a (xc, yc):\nmax xc L(xc) = m\u2211 k=1 (1\u2212 ykfxc(xk))+ = m\u2211 k=1 (\u2212gk)+ (1)\nIn this section, we assume the role of the attacker and develop a method for optimizing xc with this objective.\nFirst, we explicitly account for all terms in the margin conditions gk that are affected by xc:\ngk = \u2211 j Qkj\u03b1j + ykb\u2212 1 (2)\n= \u2211 j 6=c Qkj\u03b1j(xc) +Qkc(xc)\u03b1c(xc) + ykb(xc)\u2212 1 .\nIt is not difficult to see from the above equations that L(xc) is a non-convex objective function. Thus, we exploit a gradient ascent technique to iteratively optimize it. We assume that an initial location of the attack point x (0) c has been chosen. Our goal is to update the attack point as x (p) c = x (p\u22121) c + tu where p is the current iteration, u is a norm-1 vector representing the attack direction, and t is the step size. Clearly, to maximize our objective, the attack direction u aligns to the gradient of L with respect to u, which has to be computed at each iteration.\nAlthough the hinge loss is not everywhere differentiable, this can be overcome by only considering point indices k with non-zero contributions to L; i.e., those for which \u2212gk > 0. Contributions of such points to the gradient of L can be computed by differentiating\nEq. (2) with respect to u using the product rule:\n\u2202gk \u2202u = Qks \u2202\u03b1 \u2202u + \u2202Qkc \u2202u \u03b1c + yk \u2202b \u2202u , (3)\nwhere\n\u2202\u03b1 \u2202u =\n \u2202\u03b11 \u2202u1 \u00b7 \u00b7 \u00b7 \u2202\u03b11\u2202ud ... . . . ...\n\u2202\u03b1s \u2202u1 \u00b7 \u00b7 \u00b7 \u2202\u03b1s\u2202ud\n , simil. \u2202Qkc \u2202u , \u2202b \u2202u .\nThe expressions for the gradient can be further refined using the fact that the step taken in direction u should maintain the optimal SVM solution. This can expressed as an adiabatic update condition using the technique introduced in (Cauwenberghs & Poggio, 2001). Observe that for the i-th point in the training set, the KKT conditions for the optimal solution of the SVM training problem can be expressed as:\ngi = \u2211 j\u2208Dtr Qij\u03b1j + yib\u2212 1  > 0; i \u2208 R = 0; i \u2208 S < 0; i \u2208 E\n(4)\nh = \u2211 j\u2208Dtr yj\u03b1j = 0 . (5)\nThe equality in condition (4) and (5) implies that an infinitesimal change in the attack point xc causes a smooth change in the optimal solution of the SVM, under the restriction that the composition of the sets S, E and R remain intact. This equilibrium allows us to predict the response of the SVM solution to the variation of xc, as shown below.\nBy differentiation of the xc-dependent terms in Eqs. (4)\u2013(5) with respect to each component ul (1 \u2264 l \u2264 d), we obtain, for any i \u2208 S,\n\u2202g \u2202ul = Qss \u2202\u03b1 \u2202ul + \u2202Qsc \u2202ul \u03b1c + ys \u2202b \u2202ul = 0 \u2202h \u2202ul = y>s \u2202\u03b1 \u2202ul = 0 ,\n(6)\nwhich can be rewritten as[ \u2202b \u2202ul \u2202\u03b1 \u2202ul ] = \u2212 [ 0 y>S ys Qss ]\u22121 [ 0 \u2202Qsc \u2202ul ] \u03b1c . (7)\nThe first matrix can be inverted using the ShermanMorrison-Woodbury formula (Lu\u0308tkepohl, 1996):[\n0 y>s ys Qss\n]\u22121 = 1\n\u03b6 [ \u22121 \u03c5> \u03c5 Q\u22121ss \u2212 \u03c5\u03c5> ] (8)\nwhere \u03c5 = Q\u22121ss ys and \u03b6 = y > s Q \u22121 ss ys. Substituting (8) into (7) and observing that all components of the\ninverted matrix are independent of xc, we obtain:\n\u2202\u03b1 \u2202u = \u22121 \u03b6 \u03b1c(Q \u22121 ss \u2212 \u03c5\u03c5>) \u00b7 \u2202Qsc \u2202u\n\u2202b \u2202u = \u22121 \u03b6 \u03b1c\u03c5 > \u00b7 \u2202Qsc \u2202u .\n(9)\nSubstituting (9) into (3) and further into (1), we obtain the desired gradient used for optimizing our attack:\n\u2202L \u2202u = m\u2211 k=1 { Mk \u2202Qsc \u2202u + \u2202Qkc \u2202u } \u03b1c, (10)\nwhere\nMk = \u2212 1\n\u03b6 (Qks(Q\n\u22121 ss \u2212 \u03c5\u03c5T ) + yk\u03c5T )."}, {"heading": "2.2. Kernelization", "text": "From Eq. (10), we see that the gradient of the objective function at iteration k may depend on the attack point x (p) c = x (p\u22121) c + tu only through the gradients of the matrix Q. In particular, this depends on the chosen kernel. We report below the expressions of these gradients for three common kernels.\n\u2022 Linear kernel:\n\u2202Kic \u2202u\n= \u2202(xi \u00b7 x(p)c )\n\u2202u = txi\n\u2022 Polynomial kernel:\n\u2202Kic \u2202u\n= \u2202(xi \u00b7 x(p)c +R)d\n\u2202u = d(xi \u00b7x(p)c +R)d\u22121txi\n\u2022 RBF kernel:\n\u2202Kic \u2202u\n= \u2202e\u2212 \u03b3 2 ||xi\u2212xc|| 2\n\u2202u = K(xi, x\n(p) c )\u03b3t(xi \u2212 x(p)c )\nThe dependence on x (p) c (and, thus, on u) in the gradients of non-linear kernels can be avoided by substituting x (p) c with x (p\u22121) c , provided that t is sufficiently small. This approximation enables a straightforward extension of our method to arbitrary kernels."}, {"heading": "2.3. Poisoning Attack Algorithm", "text": "The algorithmic details of the method described in Section 2.1 are presented in Algorithm 1.\nIn this algorithm, the attack vector x (0) c is initialized by cloning an arbitrary point from the attacked class and flipping its label. In principle, any point sufficiently deep within the attacking class\u2019s margin can\nAlgorithm 1 Poisoning attack against SVM Input: Dtr, the training data; Dval, the validation data; yc, the class label of the attack point; x (0) c , the initial attack point; t, the step size. Output: xc, the final attack point.\n1: {\u03b1i, b} \u2190 learn an SVM on Dtr. 2: k \u2190 0. 3: repeat 4: Re-compute the SVM solution on Dtr\u222a{x(p)c , yc} using incremental SVM (e.g., Cauwenberghs & Poggio, 2001). This step requires {\u03b1i, b}. 5: Compute \u2202L\u2202u on Dval according to Eq. (10). 6: Set u to a unit vector aligned with \u2202L\u2202u . 7: k \u2190 k + 1 and x(p)c \u2190 x(p\u22121)c + tu 8: until L ( x (p) c ) \u2212 L ( x (p\u22121) c ) <\n9: return: xc = x (p) c\nbe used as a starting point. However, if this point is too close to the boundary of the attacking class, the iteratively adjusted attack point may become a reserve point, which halts further progress.\nThe computation of the gradient of the validation error crucially depends on the assumption that the structure of the sets S, E and R does not change during the update. In general, it is difficult to determine the largest step t along an arbitrary direction u, which preserves this structure. The classical line search strategy used in gradient ascent methods is not suitable for our case, since the update to the optimal solution for large steps may be prohibitively expensive. Hence, the step t is fixed to a small constant value in our algorithm. After each update of the attack point x (p) c , the optimal solution is efficiently recomputed from the solution on Dtr, using the incremental SVM machinery (e.g., Cauwenberghs & Poggio, 2001).\nThe algorithm terminates when the change in the validation error is smaller than a predefined threshold. For kernels including the linear kernel, the surface of the validation error is unbounded, hence the algorithm is halted when the attack vector deviates too much from the training data; i.e., we bound the size of our attack points."}, {"heading": "3. Experiments", "text": "The experimental evaluation presented in the following sections demonstrates the behavior of our proposed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST handwritten digit recognition dataset."}, {"heading": "3.1. Artificial data", "text": "We first consider a two-dimensional data generation model in which each class follows a Gaussian distribution with mean and covariance matrices given by \u00b5\u2212 = [\u22121.5, 0], \u00b5+ = [1.5, 0], \u03a3\u2212 = \u03a3+ = 0.6I. The points from the negative distribution are assigned the label \u22121 (shown as red in the subsequent figures) and otherwise +1 (shown as blue). The training and the validation sets, Dtr and Dval (consisting of 25 and 500 points per class, respectively) are randomly drawn from this distribution.\nIn the experiment presented below, the red class is the attacking class. To this end, a random point of the blue class is selected and its label is flipped to serve as the starting point for our method. Our gradient ascent method is then used to refine this attack until its termination condition is satisfied. The attack\u2019s trajectory is traced as the black line in Fig. 1 for both the linear kernel (upper two plots) and the RBF kernel (lower two plots). The background in each plot represents the error surface explicitly computed for all points within the box x \u2208 [\u22125, 5]2. The leftmost plots in each pair show the hinge loss computed on a validation set while the rightmost plots in each pair show the classification error for the area of interest. For the linear kernel, the range of attack points is limited to the box x \u2208 [\u22124, 4]2 shown as a dashed line.\nFor both kernels, these plots show that our gradient ascent algorithm finds a reasonably good local maximum of the non-convex error surface. For the linear kernel, it terminates at the corner of the bounded region, since the error surface is unbounded. For the RBF kernel, it also finds a good local maximum of the hinge loss which, incidentally, is the maximum classification error within this area of interest."}, {"heading": "3.2. Real data", "text": "We now quantitatively validate the effectiveness of the proposed attack strategy on a well-known MNIST handwritten digit classification task (LeCun et al., 1995). Similarly to Globerson & Roweis (2006), we focus on two-class sub-problems of discriminating between two distinct digits.1 In particular, we consider the following two-class problems: 7 vs. 1; 9 vs. 8; 4 vs. 0. The visual nature of the handwritten digit data provides us with a semantic meaning for an attack.\nEach digit in the MNIST data set is properly normalized and represented as a grayscale image of 28 \u00d7 28 pixels. In particular, each pixel is ordered in a raster-\n1The data set is also publicly available in Matlab format at http://cs.nyu.edu/~roweis/data.html.\nscan and its value is directly considered as a feature. The overall number of features is d = 28 \u00d7 28 = 768. We normalized each feature (pixel value) x \u2208 [0, 1]d by dividing its value by 255.\nIn this experiment only the linear kernel is considered, and the regularization parameter of the SVM is fixed to C = 1. We randomly sample a training and a validation data of 100 and 500 samples, respectively, and retain the complete testing data given by MNIST for Dts. Although it varies for each digit, the size of the testing data is about 2000 samples per class (digit).\nThe results of the experiment are presented in Fig. 2. The leftmost plots of each row show the example of the attacked class taken as starting points in our algorithm. The middle plots show the final attack point. The rightmost plots displays the increase in the validation and testing errors as the attack progresses.\nThe visual appearance of the attack point reveals that the attack blurs the initial prototype toward the appearance of examples of the attacking class. Comparing the initial and final attack points, we see this effect:\nthe bottom segment of the 7 straightens to resemble a 1, the lower segment of the 9 becomes more round thus mimicking an 8, and round noise is added to the outer boundary of the 4 to make it similar to a 0.\nThe increase in error over the course of attack is especially striking, as shown in the rightmost plots. In general, the validation error overestimates the classification error due to a smaller sample size. Nonetheless, in the exemplary runs reported in this experiment, a single attack data point caused the classification error to rise from the initial error rates of 2\u20135% to 15\u201320%. Since our initial attack point is obtained by flipping the label of a point in the attacked class, the errors in the first iteration of the rightmost plots of Fig. 2 are caused by single random label flips. This confirms that our attack can achieve significantly higher error rates than random label flips, and underscores the vulnerability of the SVM to poisoning attacks.\nThe latter point is further illustrated in a multiple point, multiple run experiment presented in Fig. 3. For this experiment, the attack was extended by in-\njecting additional points into the same class and averaging results over multiple runs on randomly chosen training and validation sets of the same size (100 and 500 samples, respectively). One can clearly see a steady growth of the attack effectiveness with the increasing percentage of the attack points in the training set. The variance of the error is quite high, which can be explained by relatively small sizes of the training and validation data sets."}, {"heading": "4. Conclusions and Future Work", "text": "The poisoning attack presented in this paper is the first step toward the security analysis of SVM against training data attacks. Although our gradient ascent method is arguably a crude algorithmic procedure,\nit attains a surprisingly large impact on the SVM\u2019s empirical classification accuracy. The presented attack method also reveals the possibility for assessing the impact of transformations carried out in the input space on the functions defined in the Reproducing Kernel Hilbert Spaces by means of differential operators. Compared to previous work on evasion of learning algorithms (e.g., Bru\u0308ckner & Scheffer, 2009; Kloft & Laskov, 2010), such influence may facilitate the practical realization of various evasion strategies. These implications need to be further investigated.\nSeveral potential improvements to the presented method remain to be explored in future work. The first would be to address our optimization method\u2019s restriction to small changes in order to maintain the SVM\u2019s structural constraints. We solved this by tak-\ning many tiny gradient steps. It would be interesting to investigate a more accurate and efficient computation of the largest possible step that does not alter the structure of the optimal solution.\nAnother direction for research is the simultaneous optimization of multi-point attacks, which we successfully approached with sequential single-point attacks. The first question is how to optimally perturb a subset of the training data; that is, instead of individually optimizing each attack point, one could derive simultaneous steps for every attack point to better optimize their overall effect. The second question is how to choose the best subset of points to use as a starting point for the attack. Generally, the latter is a subset selection problem but heuristics may allow for improved approximations. Regardless, we demonstrate that even non-optimal multi-point attack strategies significantly degrade the SVM\u2019s performance.\nAn important practical limitation of the proposed method is the assumption that the attacker controls the labels of the injected points. Such assumptions may not hold when the labels are only assigned by trusted sources such as humans. For instance, a spam filter uses its users\u2019 labeling of messages as its ground truth. Thus, although an attacker can send arbitrary messages, he cannot guarantee that they will have the labels necessary for his attack. This imposes an additional requirement that the attack data must satisfy certain side constraints to fool the labeling oracle. Further work is needed to understand these potential side constraints and to incorporate them into attacks.\nThe final extension would be to incorporate the realworld inverse feature-mapping problem; that is, the problem of finding real-world attack data that can achieve the desired result in the learner\u2019s input space. For data like handwritten digits, there is a direct mapping between the real-world image data and the input features used for learning. In many other problems (e.g., spam filtering) the mapping is more complex and may involve various non-smooth operations and normalizations. Solving these inverse mapping problems for attacks against learning remains open."}, {"heading": "Acknowledgments", "text": "This work was supported by a grant awarded to B. Biggio by Regione Autonoma della Sardegna, PO Sardegna FSE 2007-2013, L.R. 7/2007 \u201cPromotion of the scientific research and technological innovation in Sardinia\u201d. The authors also wish to acknowledge the Alexander von Humboldt Foundation and the Heisenberg Fellowship of the Deutsche Forschungsgemein-\nschaft (DFG) for providing financial support to carry out this research. The opinions expressed in this paper are solely those of the authors and do not necessarily reflect the opinions of any sponsor."}], "references": [{"title": "Can machine learning be secure", "author": ["Barreno", "Marco", "Nelson", "Blaine", "Sears", "Russell", "Joseph", "Anthony D", "J.D. Tygar"], "venue": "In Proceedings of the ACM Symposium on Information, Computer and Communications Security (ASIACCS),", "citeRegEx": "Barreno et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2006}, {"title": "The security of machine learning", "author": ["Barreno", "Marco", "Nelson", "Blaine", "Joseph", "Anthony D", "J.D. Tygar"], "venue": "Machine Learning,", "citeRegEx": "Barreno et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2010}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["Biggio", "Battista", "Fumera", "Giorgio", "Roli", "Fabio"], "venue": "International Journal of Machine Learning and Cybernetics,", "citeRegEx": "Biggio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2010}, {"title": "Statistical fraud detection: A review", "author": ["Bolton", "Richard J", "Hand", "David J"], "venue": "Journal of Statistical Science,", "citeRegEx": "Bolton et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bolton et al\\.", "year": 2002}, {"title": "Nash equilibria of static prediction games", "author": ["Br\u00fcckner", "Michael", "Scheffer", "Tobias"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Br\u00fcckner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Br\u00fcckner et al\\.", "year": 2009}, {"title": "Incremental and decremental support vector machine learning", "author": ["Cauwenberghs", "Gert", "Poggio", "Tomaso"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Cauwenberghs et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cauwenberghs et al\\.", "year": 2001}, {"title": "Detection and analysis of drive-by-download attacks and malicious JavaScript code", "author": ["M. Cova", "C. Kruegel", "G. Vigna"], "venue": "In International Conference on World Wide Web (WWW), pp", "citeRegEx": "Cova et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cova et al\\.", "year": 2010}, {"title": "ZOZZLE: Fast and precise in-browser JavaScript malware detection", "author": ["C. Curtsinger", "B. Livshits", "B. Zorn", "C. Seifert"], "venue": "In USENIX Security Symposium,", "citeRegEx": "Curtsinger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Curtsinger et al\\.", "year": 2011}, {"title": "Learning to classify with missing and corrupted features", "author": ["O. Dekel", "O. Shamir", "L. Xiao"], "venue": "Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "A sense of self for unix processes", "author": ["Forrest", "Stephanie", "Hofmeyr", "Steven A", "Somayaji", "Anil", "Longstaff", "Thomas A"], "venue": "In Proceedings of the IEEE Symposium on Security and Privacy,", "citeRegEx": "Forrest et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Forrest et al\\.", "year": 1996}, {"title": "Nightmare at test time: Robust learning by feature deletion", "author": ["A. Globerson", "S. Roweis"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Globerson and Roweis,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis", "year": 2006}, {"title": "Online anomaly detection under adversarial impact", "author": ["Kloft", "Marius", "Laskov", "Pavel"], "venue": "In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Kloft et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2010}, {"title": "Static detection of malicious JavaScript-bearing PDF documents", "author": ["Laskov", "Pavel", "\u0160rndi\u0107", "Nedim"], "venue": "In Proceedings of the Annual Computer Security Applications Conference (ACSAC),", "citeRegEx": "Laskov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laskov et al\\.", "year": 2011}, {"title": "Handbook of matrices", "author": ["L\u00fctkepohl", "Helmut"], "venue": null, "citeRegEx": "L\u00fctkepohl and Helmut.,? \\Q1996\\E", "shortCiteRegEx": "L\u00fctkepohl and Helmut.", "year": 1996}, {"title": "SpamBayes: Effective open-source, Bayesian based, email classification system", "author": ["Meyer", "Tony A", "Whateley", "Brendon"], "venue": "In Proceedings of the Conference on Email and Anti-Spam (CEAS),", "citeRegEx": "Meyer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Meyer et al\\.", "year": 2004}, {"title": "Cujo: Efficient detection and prevention of drive-by-download attacks", "author": ["K. Rieck", "T. Kr\u00fcger", "A. Dewald"], "venue": "In Proceedings of the Annual Computer Security Applications Conference (ACSAC),", "citeRegEx": "Rieck et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rieck et al\\.", "year": 2010}, {"title": "A behaviorbased approach to securing email systems. In Mathematical Methods, Models and Architectures for Computer Networks", "author": ["Stolfo", "Salvatore J", "Hershkop", "Shlomo", "Wang", "Ke", "Nimeskern", "Olivier", "Hu", "Chia-Wei"], "venue": "Security. Springer-Verlag,", "citeRegEx": "Stolfo et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Stolfo et al\\.", "year": 2003}, {"title": "Convex learning with invariances", "author": ["C.H. Teo", "A. Globerson", "S. Roweis", "A. Smola"], "venue": "In Advances in Neural Information Proccessing Systems (NIPS),", "citeRegEx": "Teo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teo et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer & Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton & Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov & \u0160rndi\u0107, 2011).", "startOffset": 174, "endOffset": 370}, {"referenceID": 16, "context": "In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer & Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton & Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov & \u0160rndi\u0107, 2011).", "startOffset": 174, "endOffset": 370}, {"referenceID": 9, "context": "In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer & Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton & Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov & \u0160rndi\u0107, 2011).", "startOffset": 174, "endOffset": 370}, {"referenceID": 6, "context": "In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer & Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton & Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov & \u0160rndi\u0107, 2011).", "startOffset": 174, "endOffset": 370}, {"referenceID": 15, "context": "In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer & Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton & Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov & \u0160rndi\u0107, 2011).", "startOffset": 174, "endOffset": 370}, {"referenceID": 7, "context": "In fact, learning approaches have already been used or proposed as solutions to a number of such security-sensitive tasks including spam, worm, intrusion and fraud detection (Meyer & Whateley, 2004; Biggio et al., 2010; Stolfo et al., 2003; Forrest et al., 1996; Bolton & Hand, 2002; Cova et al., 2010; Rieck et al., 2010; Curtsinger et al., 2011; Laskov & \u0160rndi\u0107, 2011).", "startOffset": 174, "endOffset": 370}, {"referenceID": 17, "context": "In response to the threat of adversarial data manipulation, several proposed learning methods explicitly account for certain types of corrupted data (Globerson & Roweis, 2006; Teo et al., 2008; Br\u00fcckner & Scheffer, 2009; Dekel et al., 2010).", "startOffset": 149, "endOffset": 240}, {"referenceID": 8, "context": "In response to the threat of adversarial data manipulation, several proposed learning methods explicitly account for certain types of corrupted data (Globerson & Roweis, 2006; Teo et al., 2008; Br\u00fcckner & Scheffer, 2009; Dekel et al., 2010).", "startOffset": 149, "endOffset": 240}, {"referenceID": 0, "context": "Poisoning attacks have been previously studied only for simple anomaly detection methods (Barreno et al., 2006; Rubinstein et al., 2009; Kloft & Laskov, 2010).", "startOffset": 89, "endOffset": 158}], "year": 2012, "abstractText": "We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM\u2019s test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM\u2019s decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM\u2019s optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier\u2019s test error.", "creator": "LaTeX with hyperref package"}}}