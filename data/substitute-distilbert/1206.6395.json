{"id": "1206.6395", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Convergence Rates for Differentially Private Statistical Estimation", "abstract": "differential privacy is a cryptographically - motivated definition of privacy which has gained significant attention over successive past few years. differentially private solutions enforce privacy involving adding random noise to nonlinear function computed under the data, and fourth principle in designing filtering algorithms is to control the added noise in order to optimize the privacy - accuracy - sample size tradeoff.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (346kb)", "http://arxiv.org/abs/1206.6395v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CR stat.ML", "authors": ["kamalika chaudhuri", "daniel j hsu"], "accepted": true, "id": "1206.6395"}, "pdf": {"name": "1206.6395.pdf", "metadata": {"source": "CRF", "title": "Convergence Rates for Differentially Private Statistical Estimation", "authors": ["Kamalika Chaudhuri", "Daniel Hsu"], "emails": ["kamalika@cs.ucsd.edu", "dahsu@microsoft.com"], "sections": [{"heading": null, "text": "This work studies differentially-private statistical estimation, and shows upper and lower bounds on the convergence rates of differentially private approximations to statistical estimators. Our results reveal a formal connection between differential privacy and the notion of Gross Error Sensitivity (GES) in robust statistics, by showing that the convergence rate of any differentially private approximation to an estimator that is accurate over a large class of distributions has to grow with the GES of the estimator. We then provide an upper bound on the convergence rate of a differentially private approximation to an estimator with bounded range and bounded GES. We show that the bounded range condition is necessary if we wish to ensure a strict form of differential privacy."}, {"heading": "1. Introduction", "text": "Differential privacy (Dwork et al., 2006b) is a strong, cryptographically-motivated definition of privacy which has gained significant attention in the machine-learning and data-mining communities over the past few years (McSherry & Mironov, 2009;\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nChaudhuri et al., 2011; Friedman & Schuster, 2010; Mohammed et al., 2011). In differentially private solutions, privacy is guaranteed by ensuring that the participation of a single individual in a database does not change the outcome of a private algorithm by much. This is typically achieved by adding some random noise, either to the sensitive input data, or to the output of some function, such as a classifier, computed on the sensitive data. While this guarantees privacy, for most statistical and machine learning tasks, there is a subsequent loss in statistical efficiency, in terms of the number of samples required to estimate a function to a given degree of accuracy. Thus the main challenge in designing differentially private algorithms is to optimize the privacy-accuracy-sample size trade-off, and a body of literature has been devoted to this goal.\nIn this paper, we focus on differentially-private statistical estimation. We ask: what properties should a statistical estimator have, so that it can be approximated accurately with differential privacy? Privately approximating an estimator based on a functional T that performs well when data is drawn from a specific distribution F is easy: ignore the sensitive data, and output T (F ). Thus the challenge is to design differentially private approximations to estimators that are accurate over a wide range of distributions.\nPrevious work (Smith, 2011) on differentially private statistical estimation shows how to construct differentially private approximations to estimators which have asymptotic normality guarantees under fairly mild conditions. In practical situations, however, we must take into account the effect of a finite number of samples. Moreover, it has been empirically observed (e.g., Chaudhuri et al., 2011; Vu & Slavkovic, 2009) that there is often a significant gap in statistical efficiency between a differentially private estimator and its nonprivate counterpart. Thus there is a need to study finite sample convergence rates for differentially private statistical estimators, in order to characterize the\nproperties that make a statistical estimator amenable to differentially-private approximations.\nIn this paper, we provide upper and lower bounds on the finite sample convergence rates of such estimators. Our first finite sample result draws a connection between differentially private statistical estimators and Gross Error Sensitivity, a measure commonly used in the robust statistics literature (Huber, 1981). The Gross Error Sensitivity (GES) of a statistical functional T at a distribution F is the maximum change in the value of T (F ) by an arbitrarily small perturbation of F by any point mass x in the domain. We provide a lower bound on the convergence rate of any differentially private statistical estimator, showing that an estimator that approximates T (Fn) well with differential privacy over a large class of distributions must have its convergence rate grow with the GES of T .\nA natural question to ask next is whether bounded GES is sufficient for the existence of differentially private estimators that are accurate for large classes of distributions. We next show that at least for \u03b1differential privacy, this is not the case. Any estimator based on a functional T that takes values in a range of length R and guarantees \u03b1-differential privacy for a wide class of distributions, has to have a finite sample convergence rate that grows with increasing R.\nWe then show that bounded range and GES are indeed sufficient for differentially private estimation. In particular, given an estimator based on a functional T which takes values in a bounded range, and has bounded GES for all distributions close to the underlying data distribution F , we show how to compute a differentially private approximation to T (F ) based on sensitive data drawn from F . Our approximation preserves (\u03b1, \u03b4)-differential privacy, a relaxation of \u03b1differential privacy, and is based on the smoothed sensitivity method (Nissim et al., 2007). We provide a finite sample upper bound on the convergence rate of this estimator.\nThe statistical estimators in our upper bounds are computationally inefficient in general. We conclude by providing a separate explicit method for privately approximating M-estimators with certain properties. We prove that these differentially-private estimators enjoy similar privacy and statistical guarantees as those based on the smooth-sensitivity method, while being more efficiently computable.\nRelated Work\nDifferential privacy was proposed by (Dwork et al., 2006b), and has been used since in many works on pri-\nvacy (e.g., Blum et al., 2005; Barak et al., 2007; Nissim et al., 2007; McSherry & Mironov, 2009; Chaudhuri et al., 2011). It has been shown to have strong semantic guarantees (Dwork et al., 2006b) and is resistant to many attacks (Ganta et al., 2008) that succeed against some other definitions of privacy.\nDwork & Lei (2009) is the first work to identify a connection between differential privacy and robust statistics; based on robust statistical estimators as a starting point, they provide differentially private algorithms for several common estimation tasks, including interquartile range, trimmed mean and median, and regression.\nIn further work, Smith (2011) shows how to construct a differentially private approximationAT to certain types of statistical estimators T , and establishes asymptotic normality of his estimator provided certain conditions on T hold. We in contrast focus on finite sample bounds, with an aim towards characterizing the statistical properties of estimators that determine how closely they can be approximated with differential privacy. Lei (2011) considers M-estimation, and provides a simple and elegant differentially-private Mestimator which is statistically consistent.\nFinally, work on the sample requirement of differentially private algorithms include bounds on the accuracy of differentially private data release (Hardt & Talwar, 2010), and the sample complexity of differentially private classification (Chaudhuri & Hsu, 2011)."}, {"heading": "2. Preliminaries", "text": "The goal of this paper is to examine the conditions under which we can find private approximations to estimators. The notion of privacy we use is differential privacy (Dwork et al., 2006b;a).\nDefinition 1. A (randomized) algorithm A taking values in a range S is (\u03b1, \u03b4)-differentially private if for all S \u2286 S, and all data sets D and D\u2032 differing in a single entry,\nPrA[A(D) \u2208 S] \u2264 e\u03b1PrA[A(D\u2032) \u2208 S] + \u03b4,\nwhere PrA[\u00b7] is the distribution on S induced by the output of A given a data set.\nA (randomized) algorithm A is \u03b1-differentially private if it is (\u03b1, 0)-differentially private.\nHere \u03b1 > 0 and \u03b4 \u2208 [0, 1] are privacy parameters, where smaller \u03b1 and \u03b4 imply stricter privacy.\nA general approach to developing differentially private approximations to functions is to add noise, either to the sensitive data, or to the output of a non-private\nfunction computed on the data. This work explores what properties statistical functionals need to have so that they can be accurately approximated with differential privacy.\nLet F denote the space of probability distributions on a domain X . A statistical functional T : F \u2192 R is a real-valued function of a distribution F . The plug-in estimator of \u03b8 = T (F ) is given by \u03b8n := T (Fn), where Fn is the empirical distribution corresponding to an i.i.d. sample of size n drawn from F .\nA common measure of the robustness of a statistical functional is the influence function, which measures how a functional T (F ) responds to small changes to the input F .\nDefinition 2. The influence function IF(x, T, F ) for a functional T and distribution F at x \u2208 X is:\nIF(x, T, F ) = lim \u03c1\u21920 T ((1\u2212 \u03c1)F + \u03c1\u03b4x)\u2212 T (F ) \u03c1\nwhere \u03b4x denotes the point mass distribution at x.\nIt is a well-established result in theoretical statistics (see, e.g, Wasserman, 2006) that if T is Hadamarddifferentiable, and if Ex\u223cF [IF(x, T, F )2] is bounded, then T (Fn) converges to T (F ) as n\u2192\u221e.\nA related notion is that of gross error sensitivity, which measures the worst-case value of the influence function for any x \u2208 X . Definition 3. The gross error sensitivity GES(T, F ) for a functional T and distribution F is:\nGES(T, F ) = sup x\u2208X |IF(x, T, F )|.\nWe also define the notions of influence function and gross error sensitivity at a fixed scale \u03c1 > 0:\nIF\u03c1(x, T, F ) := T ((1\u2212 \u03c1)F + \u03c1\u03b4x)\u2212 T (F )\n\u03c1\nGES\u03c1(T, F ) := sup x\u2208X |IF\u03c1(x, T, F )|.\nIn this work, the data domain X will be a subset of R. We overload notation and use F to denote a distribution as well as its cumulative distribution function. For two distributions F and G, we use dGC(F,G) := supx\u2208R |F (x) \u2212 G(x)| to denote the Glivenko-Cantelli distance between F and G. For a distribution F from a family F and a radius r > 0, let BGC(F, r) denote the set of distributions G \u2208 F such that dGC(F,G) \u2264 r. Finally,we use dTV(F,G) to denote the total variantion distance between F and G.\nA statistical functional T is B-robust at F if GES(T, F ) is finite. B-robustness has been studied in the robust statistics literature (Hampel et al., 1986; Huber, 1981), and plug-in estimators for B-robust functionals are considered to be resistant to outliers and changes in the input."}, {"heading": "3. Lower Bounds", "text": "We begin by establishing lower bounds on the convergence rate of any differentially private approximation to a statistical functional T (F )."}, {"heading": "3.1. Lower Bounds based on Gross Error Sensitivity", "text": "We first show a lower bound on the error of any (\u03b1, \u03b4)differentially private approximation to T in terms of the gross error sensitivity of T at a distribution F . Theorem 1. Pick any \u03b1 \u2208 (0, ln 22 ) and \u03b4 \u2208 (0, \u03b1 23 ). Let F be the family of all distributions over X , and let A be any (\u03b1, \u03b4)-differentially private algorithm. For all n \u2208 N and all F \u2208 F , there exists a radius \u03c1 = \u03c1(n) = 1 n \u00b7d ln 2 2\u03b1 e and a distribution G \u2208 F with dTV(F,G) \u2264 \u03c1, such that either\nEFn\u223cFEA[|A(Fn)\u2212 T (F )|] \u2265 \u03c1\n16 GES\u03c1(T, F ), or\nEGn\u223cGEA[|A(Gn)\u2212 T (G)|] \u2265 \u03c1\n16 GES\u03c1(T, F ).\nSeveral remarks are in order. First of all, the form of Theorem 1 is slightly unconventional in the sense that applies not to particular distributions, but to a set of distributions. In particular, the bound states that either the convergence rate of F is high, or the convergence rate of some G close to F is high. Observe that for a fixed distribution F , it is trivial to construct a differentially private approximation to T (F ) that is accurate for F \u2013 ignore any sensitive input data, and simply output T (F ). This algorithm provides a perfectly accurate estimate when the input is drawn from F , but performs poorly otherwise; thus any lower bound that applies to all differentially private algorithms will have a similar form. On the other hand, the differentially private estimators in Theorem 1 have few restrictions: they are only expected to be accurate for distributions lying in a small neighborhood of F , and may be extremely inaccurate in general.\nSecond, for fixed n, \u03c1 is a function \u03c1(n) = 1n \u00b7 d ln 22\u03b1 e, which decreases to zero as n \u2192 \u221e; provided GES\u03c1(T, F ) remains the same as \u03c1 diminishes, the lower bound grows weaker with increasing n. The lower bound thus does not rule out the existence of consistent private estimators.\nFinally, we observe from the proof of Theorem 1 that F need not be the family of all distributions over X ; the theorem will still apply if for every F \u2208 F , and for all x \u2208 X , (1 \u2212 \u03c1)F + \u03c1\u03b4x also lies in the family F ; for example if F is the set of all discrete distributions over X .\nWhile Theorem 1 is very general, we present below an example that illustrates an implication of the theorem.\nExample 1. Let X = [0, a], and let F be the set of all discrete distributions over X . Let T (F ) be the mean of F .\nCosnider a fixed F \u2208 F , and a fixed n. Let \u03c1 = \u03c1(n) as in Theorem 1. For any F , GES\u03c1(T, F ) \u2265 a2 . It can be shown that for any G \u2208 BTV(F, \u03c1(n)), Var [G] \u2264 Var [F ] + \u03c1(1 \u2212 \u03c1)a2. Thus, the expected errors of the (non-private) plug-in estimators are bounded as E[|T (Fn)\u2212 T (F )|] \u2264 O( \u221a Var [F ] /n) and E[|T (Gn)\u2212\nT (G)|] \u2264 O( \u221a Var [F ] /n+ \u221a \u03c1(1\u2212 \u03c1)a2/n) for all G \u2208 BTV(F, \u03c1(n)). On the other hand, Theorem 1 shows that for every differentially private estimator A, at least one of E[|A(Fn)\u2212T (F )|] and E[|A(Gn)\u2212T (G)|] is \u2126(\u03c1a); this quantity is higher than the corresponding quantity for the non-private estimator so long as n \u2264 O( a 2\nVar[F ]\u03b12 ).\nProof of Theorem 1. Let x\u2217 be the x \u2208 X that maximizes |IF\u03c1(x, T, F )|. Let \u03b3 > 0, and let \u03c1 := 1 nd ln 2 2\u03b1 e, and let G := (1 \u2212 \u03c1)F + \u03c1\u03b4x\u2217 . Observe that dTV(F,G) \u2264 \u03c1 and IF\u03c1(x\u2217, T, F ) = (T (G)\u2212 T (F ))/\u03c1.\nConsider the following procedure for drawing n samples from G. First, draw a random sample Fn of size n from F (we overload the notation Fn to refer to both a random sample and its empirical distribution). Next, for each i = 1, 2, . . . , n, independently toss a biased coin with heads probability \u03c1; if the coin turns up heads, replace the i-th element of Fn by x\n\u2217; otherwise, do nothing. This procedure constructs a random sample Gn of size n from G, and in the process constructs a coupling between samples of size n from F and G. In what follows, we will use this coupling to calculate the quantity\nEFn\u223cFEA[|A(Fn)\u2212T (F )|]+EGn\u223cGEA[|A(Gn)\u2212T (G)|].\nLet Fn be any randomly drawn sample of size n from F , and let Gn be a corresponding sample from G as drawn from the coupling procedure. Call a pair (Fn, Gn) \u03c1-close if they differ in at most \u03c1n entries. As the median of Binomial(n, \u03c1) is \u2264 d\u03c1ne = \u03c1n, the probability that at most \u03c1n of the elements of Fn are converted to x\u2217 by the coupling process is at least 1/2.\nIn other words,\nPrGn [(Fn, Gn) is \u03c1-close] \u2265 1/2. (1)\nFor any \u03c1-close pair (Fn, Gn), we can apply Lemma 3 1 with the parameters t := T (F ), t\u2032 := T (G), \u03b3 := 1/4, and\n\u2206 := \u03c1n \u2264 ( 1 + ln 2\n2\u03b1\n) \u2264 ln 2\n\u03b1 = ln 12\u03b3 \u03b1 ;\nthe lemma implies, for any \u03c1-close pair (Fn, Gn),\nEA[|A(Fn)\u2212 T (F )|] + EA[|A(Gn)\u2212 T (G)|]\n\u2265 1 4 |T (F )\u2212 T (G)|.\nTherefore, conditioned on Fn, we have\nEA[|A(Fn)\u2212 T (F )|] + EGnEA[|A(Gn)\u2212 T (G)||Fn]\n\u2265 1 8 |T (F )\u2212 T (G)|\nby (1). Taking a final expectation over Fn \u223c F ,\nEFn\u223cFEA[|A(Fn)\u2212T (F )|]+EGn\u223cGEA[|A(Gn)\u2212T (G)|]\n\u2265 1 8 |T (F )\u2212 T (G)| = \u03c1\n8 |IF\u03c1(x\u2217, T, F )| =\n\u03c1 8 GES\u03c1(T, F ).\nThe theorem follows."}, {"heading": "3.2. Lower Bounds as a Function of Range", "text": "Is the bound in Theorem 1 tight? In other words, if T has bounded GES, can we compute accurate differentially private approximations to T (F ) for all distributions F over a domain? We next show that at least for (\u03b1, 0)-differential privacy, Theorem 1 is not tight; if we wish to compute differentially private and accurate estimates of T (F ) for all distributons F in a family, where T (F ) can take any value in a range [\u03bb, \u03bb\u2032], then the sample size must grow as a function of \u03bb\u2032 \u2212 \u03bb. Theorem 2. Let F be a family of distributions over X , and let A be any (\u03b1, 0)-differentially private algorithm. Suppose for all \u03c4 \u2208 [\u03bb, \u03bb\u2032], there exists some F \u03c4 \u2208 F such that T (F \u03c4 ) = \u03c4 . Then there exists some F \u2208 F such that\nEFn\u223cF,A[|A(Fn)\u2212 T (F )|] \u2265 1 4 \u00b7 \u03bb \u2032 \u2212 \u03bb 2 + e\u03b1n .\nExample 2. For any \u03b3 \u2208 R, let U\u03b3 be the uniform distribution on [\u03b3 \u2212 1, \u03b3 + 1], and let F be the family\n1See Appendix A for omitted lemmas.\nF = {U\u03b3 : \u03b3 \u2208 [\u2212R,R]}. Let T (F ) be the median of F . For every F \u2208 F , the non-private estimator T (Fn) converges to T (F ) at a rate proportional to O( 1\u221a\nn ),\nindependent of R. However, Theorem 2 shows that for every differentially private estimator A, there is some F \u2208 F such that |A(Fn)\u2212T (F )| grows with R.\nProof of Theorem 2. Let r := \u03bb \u2032\u2212\u03bb 2+e\u03b1n and \u0393 := b \u03bb\u2032\u2212\u03bb r c. For each i = 1, 2, . . . ,\u0393, let F i be a distribution in F such that T (F i) = \u03bb + (i \u2212 12 )r; such distributions are guaranteed to exist by assumption. Also, for each i = 1, 2, . . . ,\u0393, let F in be an iid sample of size n from F i, and define the half-open interval Ii := [\u03bb + (i \u2212 1)r, \u03bb+ ir). Observe that the intervals Ii are disjoint. To prove the theorem, let us assume the contrary:\nEF in,A[|A(F i n)\u2212 T (F i)|] \u2264 r/4 for all i. (2)\nThis, along with a Markov\u2019s inequality on |A(F in) \u2212 T (F i)|, implies that PrF in,A[A(F i n) \u2208 Ii] \u2265 1/2. Therefore, for any i,\n1 2 \u2265 PrF in,A[A(F i n) /\u2208 Ii] \u2265 \u2211 j 6=i PrF in,A[A(F i n) \u2208 Ij ]\n\u2265 e\u2212\u03b1n \u2211 j 6=i PrF jn,A[A(F j n) \u2208 Ij ] \u2265 1 2 (\u0393\u2212 1)e\u2212\u03b1n\nwhere the first step follows by assumption, the second step follows because the intervals {Ij} are disjoint, and the third step from Lemma 2 and the fact that for any i and j, any F in and F j n differ in at most n entries. Rearranging, the inequality becomes \u0393 \u2264 1 + e\u03b1n, which is a contradiction since \u0393 = b(\u03bb\u2032 \u2212 \u03bb)/rc > 1 + e\u03b1n. Therefore (2) cannot hold, so the theorem follows."}, {"heading": "4. Upper Bounds", "text": "In this section, we show that bounded GES and bounded range are sufficient conditions for the existence of an (\u03b1, \u03b4)-differentially private approximation to T . Our approximation uses the smooth-sensitivity method of Nissim et al. (2007), for which we provide a new statistical analysis in Section 4.1 (Theorem 3). We also provide a specific analysis for the case of linear functionals in Appendix B.\nLet dH(D,D \u2032) denote the Hamming distance between D and D\u2032 (the number of entries in which D and D\u2032 differ), and recall the following definitions from Nissim et al. (2007).\nDefinition 4. The local sensitivity of a function \u03d5 : Rn \u2192 R at a data set D \u2208 Rn, denoted by LS(\u03d5,D), is\nLS(\u03d5,D) := sup{|\u03d5(D)\u2212 \u03d5(D\u2032)| : dH(D,D\u2032) = 1}.\nFor \u03b2 > 0, the \u03b2-smooth sensitivity of \u03d5 at D, denoted by SS\u03b2(\u03d5,D), is\nSS\u03b2(\u03d5,D) := sup{e\u2212\u03b2dH(D,D \u2032) \u00b7 LS(\u03d5,D\u2032) : D\u2032 \u2208 Rn}.\nThroughout, we assume D \u2208 Rn is an i.i.d. sample of size n drawn from a fixed distribution F , and Fn is the empirical CDF corresponding to this sample. For a statistical functional T , we use the overloaded notation SS\u03b2(T, Fn) to denote the \u03b2-smooth sensitivity of T (Fn) at the data set Fn = D."}, {"heading": "4.1. Estimator Based on Smooth Sensitivity", "text": "For a statistical functional T , let AT be the randomized estimator given by\nAT (Fn) := T (Fn) + SS\u03b2(\u03b1,\u03b4)(T, Fn) \u00b7 2\n\u03b1 \u00b7 Z (3)\nwhere \u03b2(\u03b1, \u03b4) := \u03b12 ln(1/\u03b4) and Z is an independent random variable drawn from the standard Laplace density pZ(z) = 0.5e\n\u2212|z|. AT essentially computes T (Fn) and adds zero-mean noise, with the scale determined by the privacy parameters and the smooth sensitivity. Computing SS\u03b2(\u03b1,\u03b4)(T, Fn) in general can be computationally challenging \u2013see Nissim et al. (2007); our result thus demonstrates an upper bound.\nThe following guarantee is due to Nissim et al. (2007).\nProposition 1. AT is (\u03b1, \u03b4)-differentially private.\nTo give a statistical guarantee for AT , we begin with a standard tail bound based on the simple fact that PrZ [|Z| > t] \u2264 e\u2212t. Proposition 2. For any t > 0,\nPrZ [ |AT (Fn)\u2212T (Fn)| > SS\u03b2(\u03b1,\u03b4)(T, Fn) \u00b7 2 \u03b1 \u00b7 t ] \u2264 e\u2212t.\nIt follows that the convergence rate of AT depends on the \u03b2-smooth sensitivity of T at Fn, which can be bounded under the following conditions on T .\nCondition 1 (Bounded range). There exists a finite R > 0 such that the range of T is contained in an interval of length R.\nCondition 2 (Bounded gross error sensitivity). The sequence (\u0393n) given by\n\u0393n := sup { GES1/n(T,G) : G \u2208 BGC ( F, \u221a 2 ln(2/\u03b7) n )} is bounded.\nEven for non-private estimation, the robustness of an estimator depends not just on the influence functions\nat the target distribution F , but also on these quantities in a local neighborhood around F (Huber, 1981, p. 72). For convenience, Condition 2 is stated in terms of Glivenko-Cantelli distance, but can be easily changed to any distance under which Fn converges to F as n \u2192 \u221e with suitable modifications in the analysis.\nWe now state our main statistical guarantee for AT . Theorem 3. Assume Condition 1 and Condition 2 hold. Pick any \u03b7 \u2208 (0, 1/4). With probability \u2265 1\u22122\u03b7, the estimator AT from (3) satisfies\n|AT (Fn)\u2212 T (F )| \u2264 |T (Fn)\u2212 T (F )|+\n2 ln(1/\u03b7)\n\u03b1 max { 2\u0393n n ,R \u00b7 exp ( \u2212 \u03b1 \u221a n ln(2/\u03b7) 74 ln(1/\u03b4) )} where R is the quantity in Condition 1, and \u0393n is the quantity in Condition 2.\nProof. Follows from Proposition 2, Lemma 1, a union bound, and the triangle inequality.\nThe first term in the bound, |T (Fn) \u2212 T (F )|, is the error of the non-private plug-in estimate T (Fn). If T is Hadamard-differentiable, then T (Fn) \u2212 T (F ) converges in distribution to a zero-mean normal random variable with variance n\u22121 \u222b IF(x, T, F )2dF (x); in this case, T (Fn) converges to T (F ) at an asymptotic n \u22121/2 rate (Wasserman, 2006). Non-asymptotic rates can also be established in terms of other specific properties of T and F (see Appendix B for an example).\nThe second term in the bound from Theorem 3 is roughly the larger of\nA1 := O ( \u0393n \u03b1n ) and A2 := R \u03b1 \u00b7 exp ( \u2212\u2126 ( \u03b1 \u221a n ln(1/\u03b4) )) (for constant \u03b7), can be compared to the lower bounds from Section 3. The lower bound from Theorem 1 is close to A1 as long as GES\u03c1(T, F ) \u2248 \u0393n for \u03c1 = ln 22\u03b1n . This hold for sufficiently large n when limn\u2192\u221e \u0393n = GES(T, F ). The lower bound from Theorem 2 decreases as R\u00b7exp(\u2212\u2126(\u03b1n)), which is a little better than A2, but is otherwise qualitatively similar in terms of its dependence on the range R2.\nExample 3. If T (F ) is the median of F , and F := {U\u03b3 : \u03b3 \u2208 [\u2212R,R]} is the family of uniform distributions on unit length intervals [\u03b3 \u2212 1, \u03b3 + 1] from Example 2, then \u0393n = 1/2, and the bound in Theorem 3 reduces to\n|T (Fn)\u2212 T (F )|+O ( 1\n\u03b1n\n) + R\n\u03b1 \u00b7 e\u2212\u2126(\u03b1\n\u221a n/ ln(1/\u03b4)).\n2Appendix E shows how this discrepancy can be reduced with a stronger condition."}, {"heading": "4.2. Bounding the Smooth Sensitivity", "text": "The proof of Theorem 3 (see Appendix C) is based on the following lemma, which establishes a highprobability bound on SS\u03b2(T, Fn) under Conditions 1 and 2.\nLemma 1. Assume Condition 1 and Condition 2 hold. With probability \u2265 1\u2212 \u03b7,\nSS\u03b2(T, Fn)\u2264max {\n2\u0393n n ,R exp\n( \u2212\u03b2 (\u221a n ln(2/\u03b7) 2 \u22121 ))}\nwhere R is the quantity in Condition 1, and \u0393n is the quantity in Condition 2.\n5. Differentially-Private M-Estimation\nWe now provide a procedure for constructing differentially private approximations to M -estimators that satisfy certain conditions. Unlike our estimators in Section 4.1, these estimators are computationally efficient; however they only apply to a more restricted class of estimators.\n5.1. M-Estimators\nAnM -estimator T\u03c8(Fn) is given as the solution \u03b8n \u2208 R to the equation \u222b\n\u03c8(x, \u03b8n)dFn(x) = 0\nfor some function \u03c8 : R \u00d7 R \u2192 R. For a CDF G and \u03b8 \u2208 R, define\n\u03a8(G, \u03b8) := \u222b \u03c8(x, \u03b8)dG(x)\nso \u03a8(Fn, T\u03c8(Fn)) = 0. The derivative of \u03a8 with respect to its second argument, which is assumed to exist, is denoted by \u03a8\u2032. Throughout, we will assume \u03c8 satisfies the following condition.\nCondition 3 (Bounded \u03c8-range and monotonicity). There exists a finite K > 0 such that the range of \u03c8 is contained in [\u2212K,K], and \u03c8 is non-decreasing in its second argument.\nUnder this condition, the gross error sensitivity of T\u03c8 at F can be bounded as\nGES(T\u03c8, F )= supx\u2208R |\u03c8(x, T\u03c8(F ))| |\u03a8\u2032(F, T\u03c8(F ))| \u2264 K\u2223\u2223\u03a8\u2032(F, T\u03c8(F ))\u2223\u2223 . (4)\nPrevious works (Chaudhuri et al., 2011) and (Rubinstein et al., 2009) have provided differentially private and computationally efficient algorithms for M - estimation under assumptions that are very similar\nto Condition 3. The algorithm in Rubinstein et al. (2009), and one of the algorithms in Chaudhuri et al. (2011) are based on the sensitivity method, while the main algorithm in Chaudhuri et al. (2011) is based on an objective perturbation method. While both algorithms are computationally efficient, both require explicit regularization. This is problematic in practice because determining the regularization parameter privately through differentially-private parameter-tuning requires extra data \u2013 for a more detailed discussion of this issue, see Chaudhuri et al. (2011). In contrast, our algorithm is based on the Exponential Mechanism, and does not have an explicit regularization parameter; instead we assume that \u03a8\u2032 is smooth, and our guarantees depend on the value of the derivative \u03a8\u2032(F, T\u03c8(F )).\n5.2. Exponential Mechanism for M-Estimation\nFix a density \u00b5 on R, and let A\u03c8,\u00b5 be the randomized estimator whose output has probability density\npA\u03c8,\u00b5(Fn)(\u03b8) \u221d \u00b5(\u03b8) exp ( \u2212 n\u03b1\n2K \u2223\u2223\u03a8(Fn, \u03b8)\u2223\u2223) . This estimator is derived from the exponential mechanism of McSherry & Talwar (2007), where the \u201ccost\u201d function is taken to be |\u03a8(Fn, \u00b7)|/K. In many M - estimators of interest, particularly those involving data lying in a bounded range, a prior knowledge of K is reasonable.\nIf it is known that T\u03c8(F ) is contained in some interval, then one can take the prior density \u00b5 to be uniform over this interval. If no such prior knowledge is available, then \u00b5 can be taken to be a density with full support on R such as the standard Cauchy density.\nThe privacy guarantee for A\u03c8,\u00b5 follows easily from known properties of the exponential mechanism (McSherry & Talwar, 2007).\nProposition 3. A\u03c8,\u00b5 is (\u03b1, 0)-differentially private.\nThe accuracy guarantee forA\u03c8,\u00b5 relies on the following smoothness condition on \u03a8 at F .\nCondition 4 (Smoothness). There exist r1 > 0, r2 > 0, \u039b1 > 0, and \u039b2 > 0 such that\n|\u03a8\u2032(G, \u03b8)\u2212\u03a8\u2032(F, \u03b8)| \u2264 \u039b1 \u00b7 dGC(G,F ) and |\u03a8\u2032(F, \u03b8)\u2212\u03a8\u2032(F, T\u03c8(F ))| \u2264 \u039b2 \u00b7 |\u03b8 \u2212 T\u03c8(F )|\nwhenever dGC(G,F ) \u2264 r1 and |\u03b8 \u2212 T\u03c8(F )| \u2264 r2. Also, for \u03b5 > 0 and \u03b7 \u2208 (0, 1), define N\u03b5,\u03b7 :=min { n \u2208\nN : PrFn\u223cF [|T\u03c8(Fn) \u2212 T\u03c8(F )| > \u03b5] \u2264 \u03b7 }\nto be the minimum sample size such that, with probability \u2265 1 \u2212 \u03b7, the non-private estimator T\u03c8(Fn) lies within distance \u03b5 of T\u03c8(F ).\nTheorem 4. Assume Condition 3 and Condition 4 hold. Let \u03b51 := min{r1, |\u03a8\u2032(F, T\u03c8(F ))|/(6\u039b1)}, \u03b52 := min{r2/2, |\u03a8\u2032(F, T\u03c8(F ))|/(6\u039b2)}, and \u0393 := K/|\u03a8\u2032(F, T\u03c8(F ))|. Pick any \u03b7 \u2208 (0, 1) and \u03b5 \u2208 (0, \u03b52). Suppose\nn \u2265 max { ln(2/\u03b7)\n2\u03b521 , N\u03b52,\u03b7\n} , (5)\nand one of the following holds:\n1. the range of T\u03c8 is contained in an interval I of length R, \u00b5 is the uniform density on I, and\nn \u2265 8 ln(6R/(\u03b5\u03b7)) \u03b1\u03b5 \u00b7 \u0393;\n2. \u00b5(\u03b8) = 1\u03c0 (1 + \u03b8 2)\u22121 is the standard Cauchy den-\nsity, and n \u2265 8 \u03b1\u03b5 \u00b7 ln ( \u03c0 \u03b7 ( 2(|T\u03c8(F )|+ \u03b52)2 + 1 \u03b5/3 + \u03b5 6 )) \u00b7 \u0393.\nWith probability at least 1 \u2212 3\u03b7, the estimator A\u03c8,\u00b5 satisfies\n|A\u03c8,\u00b5(Fn)\u2212 T\u03c8(F )| \u2264 |T\u03c8(Fn)\u2212 T\u03c8(F )|+ \u03b5.\nThe proof of Theorem 4 is in Appendix D. The condition in (5) required by Theorem 4 essentially states that the sample size n should be large enough for Fn and T\u03c8(Fn) to be in the neighborhoods of F and T\u03c8(F ), respectively, where \u03a8\n\u2032 is locally Lipschitzsmooth.\nIt is straightforward to generalize the results to other prior densities \u00b5. Observe that in the case the range of T\u03c8 is [\u2212R,R] for some unknown R, using the standard Cauchy density as \u00b5 yields a similar dependence on R (via log |T\u03c8(F )| \u2264 logR) as what is obtained when \u00b5 is uniform over [\u2212R,R]. The more probability mass \u00b5 assigns around T\u03c8(F ), the better the bounds are.\nAlso note that the main scaling factor of \u0393 = K/|\u03a8\u2032(F, T\u03c8(F ))| in the sample size bound is precisely the bound on GES(T\u03c8, F ) from (4). A dependence on GES(T\u03c8, F ) is to be expected as per Theorem 1."}, {"heading": "6. Conclusions", "text": "The finite sample analysis reveals a concrete connection between differential privacy and robust statistics, The main results shown here suggest using Brobustness as a criterion for designing differentiallyprivate statistical estimators, and also highlight the obstacles that even robust estimators face when the parameter space is very large or unbounded.\nWhile our lower bounds may seem pessimistic, they apply to estimators that succeed for a wide class of distributions. One way of avoiding our lower bounds would be by using priors that allow an estimator to perform well on some input distributions but not-sowell on others; a future research direction is to investigate how this can help design better differentially private estimators.\nAcknowledgements. KC would like to thank NIH U54 HL108460 for research support."}], "references": [{"title": "Privacy, accuracy, and consistency too: a holistic solution to contingency table release", "author": ["B. Barak", "K. Chaudhuri", "C. Dwork", "S. Kale", "F. McSherry", "K. Talwar"], "venue": "In PODS,", "citeRegEx": "Barak et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2007}, {"title": "Practical privacy: the SuLQ framework", "author": ["A. Blum", "C. Dwork", "F. McSherry", "K. Nissim"], "venue": "In PODS,", "citeRegEx": "Blum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2005}, {"title": "Sample complexity bounds for differentially private learning", "author": ["K. Chaudhuri", "D. Hsu"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Hsu,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri and Hsu", "year": 2011}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Dvoretzky et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Dvoretzky et al\\.", "year": 1956}, {"title": "Differential privacy and robust statistics", "author": ["C. Dwork", "J. Lei"], "venue": "In STOC,", "citeRegEx": "Dwork and Lei,? \\Q2009\\E", "shortCiteRegEx": "Dwork and Lei", "year": 2009}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["C. Dwork", "K. Kenthapadi", "F. McSherry", "I. Mironov", "M. Naor"], "venue": "In EUROCRYPT,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Data mining with differential privacy", "author": ["A. Friedman", "A. Schuster"], "venue": "In KDD,", "citeRegEx": "Friedman and Schuster,? \\Q2010\\E", "shortCiteRegEx": "Friedman and Schuster", "year": 2010}, {"title": "Composition attacks and auxiliary information in data privacy", "author": ["S.R. Ganta", "S.P. Kasiviswanathan", "A. Smith"], "venue": "In KDD,", "citeRegEx": "Ganta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganta et al\\.", "year": 2008}, {"title": "Robust Statistics - The Approach", "author": ["F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel"], "venue": "Based on Influence Functions. Wiley,", "citeRegEx": "Hampel et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hampel et al\\.", "year": 1986}, {"title": "On the geometry of differential privacy", "author": ["M. Hardt", "K. Talwar"], "venue": "In STOC,", "citeRegEx": "Hardt and Talwar,? \\Q2010\\E", "shortCiteRegEx": "Hardt and Talwar", "year": 2010}, {"title": "The tight constant in the DvoretzkyKiefer-Wolfowitz inequality", "author": ["P. Massart"], "venue": "Annals of Probability,", "citeRegEx": "Massart,? \\Q1990\\E", "shortCiteRegEx": "Massart", "year": 1990}, {"title": "Differentially private recommender systems: building privacy into the net", "author": ["F. McSherry", "I. Mironov"], "venue": "In KDD,", "citeRegEx": "McSherry and Mironov,? \\Q2009\\E", "shortCiteRegEx": "McSherry and Mironov", "year": 2009}, {"title": "Mechanism design via differential privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "In FOCS,", "citeRegEx": "McSherry and Talwar,? \\Q2007\\E", "shortCiteRegEx": "McSherry and Talwar", "year": 2007}, {"title": "Differentially private data release for data mining", "author": ["N. Mohammed", "R. Chen", "B.C.M. Fung", "P.S. Yu"], "venue": "In KDD,", "citeRegEx": "Mohammed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohammed et al\\.", "year": 2011}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "In STOC,", "citeRegEx": "Nissim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2007}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for svm learning", "author": ["Rubinstein", "Benjamin I. P", "Bartlett", "Peter L", "Huang", "Ling", "Taft", "Nina"], "venue": "CoRR, abs/0911.5708,", "citeRegEx": "Rubinstein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2009}, {"title": "Privacy-preserving statistical estimation with optimal convergence rates", "author": ["A. Smith"], "venue": "In STOC,", "citeRegEx": "Smith,? \\Q2011\\E", "shortCiteRegEx": "Smith", "year": 2011}, {"title": "Differential privacy for clinical trial data: Preliminary evaluations", "author": ["D. Vu", "A. Slavkovic"], "venue": "In Data Mining Workshops,", "citeRegEx": "Vu and Slavkovic,? \\Q2009\\E", "shortCiteRegEx": "Vu and Slavkovic", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": "Previous work (Smith, 2011) on differentially private statistical estimation shows how to construct differentially private approximations to estimators which have asymptotic normality guarantees under fairly mild conditions.", "startOffset": 14, "endOffset": 27}, {"referenceID": 16, "context": "Our approximation preserves (\u03b1, \u03b4)-differential privacy, a relaxation of \u03b1differential privacy, and is based on the smoothed sensitivity method (Nissim et al., 2007).", "startOffset": 144, "endOffset": 165}, {"referenceID": 0, "context": ", 2006b), and has been used since in many works on privacy (e.g., Blum et al., 2005; Barak et al., 2007; Nissim et al., 2007; McSherry & Mironov, 2009; Chaudhuri et al., 2011).", "startOffset": 59, "endOffset": 175}, {"referenceID": 16, "context": ", 2006b), and has been used since in many works on privacy (e.g., Blum et al., 2005; Barak et al., 2007; Nissim et al., 2007; McSherry & Mironov, 2009; Chaudhuri et al., 2011).", "startOffset": 59, "endOffset": 175}, {"referenceID": 3, "context": ", 2006b), and has been used since in many works on privacy (e.g., Blum et al., 2005; Barak et al., 2007; Nissim et al., 2007; McSherry & Mironov, 2009; Chaudhuri et al., 2011).", "startOffset": 59, "endOffset": 175}, {"referenceID": 9, "context": ", 2006b) and is resistant to many attacks (Ganta et al., 2008) that succeed against some other definitions of privacy.", "startOffset": 42, "endOffset": 62}, {"referenceID": 18, "context": "In further work, Smith (2011) shows how to construct a differentially private approximationAT to certain types of statistical estimators T , and establishes asymptotic normality of his estimator provided certain conditions on T hold.", "startOffset": 17, "endOffset": 30}, {"referenceID": 18, "context": "In further work, Smith (2011) shows how to construct a differentially private approximationAT to certain types of statistical estimators T , and establishes asymptotic normality of his estimator provided certain conditions on T hold. We in contrast focus on finite sample bounds, with an aim towards characterizing the statistical properties of estimators that determine how closely they can be approximated with differential privacy. Lei (2011) considers M-estimation, and provides a simple and elegant differentially-private Mestimator which is statistically consistent.", "startOffset": 17, "endOffset": 446}, {"referenceID": 10, "context": "B-robustness has been studied in the robust statistics literature (Hampel et al., 1986; Huber, 1981), and plug-in estimators for B-robust functionals are considered to be resistant to outliers and changes in the input.", "startOffset": 66, "endOffset": 100}, {"referenceID": 16, "context": "Our approximation uses the smooth-sensitivity method of Nissim et al. (2007), for which we provide a new statistical analysis in Section 4.", "startOffset": 56, "endOffset": 77}, {"referenceID": 16, "context": "Our approximation uses the smooth-sensitivity method of Nissim et al. (2007), for which we provide a new statistical analysis in Section 4.1 (Theorem 3). We also provide a specific analysis for the case of linear functionals in Appendix B. Let dH(D,D \u2032) denote the Hamming distance between D and D\u2032 (the number of entries in which D and D\u2032 differ), and recall the following definitions from Nissim et al. (2007).", "startOffset": 56, "endOffset": 412}, {"referenceID": 16, "context": "Computing SS\u03b2(\u03b1,\u03b4)(T, Fn) in general can be computationally challenging \u2013see Nissim et al. (2007); our result thus demonstrates an upper bound.", "startOffset": 77, "endOffset": 98}, {"referenceID": 16, "context": "The following guarantee is due to Nissim et al. (2007).", "startOffset": 34, "endOffset": 55}, {"referenceID": 3, "context": "Previous works (Chaudhuri et al., 2011) and (Rubinstein et al.", "startOffset": 15, "endOffset": 39}, {"referenceID": 17, "context": ", 2011) and (Rubinstein et al., 2009) have provided differentially private and computationally efficient algorithms for M estimation under assumptions that are very similar", "startOffset": 12, "endOffset": 37}, {"referenceID": 16, "context": "The algorithm in Rubinstein et al. (2009), and one of the algorithms in Chaudhuri et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 3, "context": "(2009), and one of the algorithms in Chaudhuri et al. (2011) are based on the sensitivity method, while the main algorithm in Chaudhuri et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 3, "context": "(2009), and one of the algorithms in Chaudhuri et al. (2011) are based on the sensitivity method, while the main algorithm in Chaudhuri et al. (2011) is based on an objective perturbation method.", "startOffset": 37, "endOffset": 150}, {"referenceID": 3, "context": "(2009), and one of the algorithms in Chaudhuri et al. (2011) are based on the sensitivity method, while the main algorithm in Chaudhuri et al. (2011) is based on an objective perturbation method. While both algorithms are computationally efficient, both require explicit regularization. This is problematic in practice because determining the regularization parameter privately through differentially-private parameter-tuning requires extra data \u2013 for a more detailed discussion of this issue, see Chaudhuri et al. (2011). In contrast, our algorithm is based on the Exponential Mechanism, and does not have an explicit regularization parameter; instead we assume that \u03a8\u2032 is smooth, and our guarantees depend on the value of the derivative \u03a8\u2032(F, T\u03c8(F )).", "startOffset": 37, "endOffset": 522}], "year": 2012, "abstractText": "Differential privacy is a cryptographicallymotivated definition of privacy which has gained significant attention over the past few years. Differentially private solutions enforce privacy by adding random noise to a function computed over the data, and the challenge in designing such algorithms is to control the added noise in order to optimize the privacyaccuracy-sample size tradeoff. This work studies differentially-private statistical estimation, and shows upper and lower bounds on the convergence rates of differentially private approximations to statistical estimators. Our results reveal a formal connection between differential privacy and the notion of Gross Error Sensitivity (GES) in robust statistics, by showing that the convergence rate of any differentially private approximation to an estimator that is accurate over a large class of distributions has to grow with the GES of the estimator. We then provide an upper bound on the convergence rate of a differentially private approximation to an estimator with bounded range and bounded GES. We show that the bounded range condition is necessary if we wish to ensure a strict form of differential privacy.", "creator": "pdftk 1.44 - www.pdftk.com"}}}