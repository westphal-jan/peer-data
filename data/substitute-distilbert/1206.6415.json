{"id": "1206.6415", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "The Big Data Bootstrap", "abstract": "the bootstrap provides a simple and powerful means of assessing decision quality of estimators. however, in settings involving large datasets, the computation onto bootstrap - based quantities can be prohibitively demanding. as an alternative, now present the bag of little bootstraps ( blb ), a computational improvement which incorporates features of overcoming the bootstrap and subsampling to obtain a robust, potentially efficient means of assessing estimator returns. blb is well matched to modern parallel and distributed measurement architectures and retains the generic applicability, statistical value, and favorable memory properties of ordinary bootstrap. we provide the results of an extensive empirical and theoretical investigation of blb's behavior, including a study challenging its statistical correctness, its variable - scale implementation and description, selection of hyperparameters, and performance on real data.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (385kb)", "http://arxiv.org/abs/1206.6415v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: text overlap witharXiv:1112.5016"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: text overlap witharXiv:1112.5016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ariel kleiner", "ameet talwalkar", "purnamrita sarkar", "michael i jordan"], "accepted": true, "id": "1206.6415"}, "pdf": {"name": "1206.6415.pdf", "metadata": {"source": "META", "title": "The Big Data Bootstrap", "authors": ["Ariel Kleiner", "Ameet Talwalkar", "Purnamrita Sarkar"], "emails": ["akleiner@cs.berkeley.edu", "ameet@cs.berkeley.edu", "psarkar@cs.berkeley.edu", "jordan@cs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "Assessing the quality of estimates based upon finite data is a task of fundamental importance in data analysis. For example, when estimating a vector of model parameters given a training dataset, it is useful to be able to quantify the uncertainty in that estimate (e.g., via a confidence region), its bias, and its risk. Such quality assessments provide far more information than a simple point estimate itself and can be used to improve human interpretation of inferential outputs, per-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nform hypothesis testing, do bias correction, make more efficient use of available resources (e.g., by ceasing to process data when the confidence region is sufficiently small), perform active learning, and do feature selection, among many more potential uses.\nAccurate assessment of estimate quality has been a longstanding concern in statistics. A great deal of classical work in this vein has proceeded via asymptotic analysis, which relies on deep study of particular classes of estimators in particular settings (Politis et al., 1999). While this approach ensures asymptotic correctness and allows analytic computation, its applicability is limited to cases in which the relevant asymptotic analysis is tractable and has actually been performed. In contrast, recent decades have seen greater focus on more automatic methods, which generally require significantly less analysis, at the expense of doing more computation. The bootstrap (Efron, 1979; Efron & Tibshirani, 1993) is perhaps the best known and most widely used among these methods, due to its simplicity, generic applicability, and automatic nature. Efforts to ameliorate statistical shortcomings of the bootstrap in turn led to the development of related methods such as the m out of n bootstrap and subsampling (Bickel et al., 1997; Politis et al., 1999).\nDespite the computational demands of this more automatic methodology, advancements have been driven primarily by a desire to remedy shortcomings in statistical correctness. However, with the advent of increasingly large datasets and diverse sets of often complex and exploratory queries, computational considerations and automation (i.e., lack of reliance on deep analysis of the specific estimator and setting of interest) are increasingly important. Even as the amount of available data grows, the number of parameters to be estimated and the number of potential sources of bias often also grow, leading to a need to be able to tractably assess\nestimator quality in the setting of large data.\nThus, unlike previous work on estimator quality assessment, here we directly address computational costs and scalability in addition to statistical correctness and automation. Indeed, existing methods have significant drawbacks with respect to one or more of these desiderata. The bootstrap, despite its strong statistical properties, has high\u2014even prohibitive\u2014 computational costs; thus, its usefulness is severely blunted by the large datasets increasingly encountered in practice. We also find that its relatives, such as the m out of n bootstrap and subsampling, can have lesser computational costs, as expected, but are generally not robust to specification of hyperparameters (such as the number of subsampled data points) and are also somewhat less automatic due to their need to explicitly utilize theoretically derived estimator convergence rates.\nMotivated by the need for an automatic, accurate means of assessing estimator quality that is scalable to large datasets, we present a new procedure, the Bag of Little Bootstraps (BLB), which functions by combining the results of bootstrapping multiple small subsets of a larger original dataset. BLB has a significantly more favorable computational profile than the bootstrap, as it only requires repeated computation of the estimator under consideration on quantities of data that can be much smaller than the original dataset. Hence, BLB is well suited to implementation on modern distributed and parallel computing architectures. Our procedure maintains the bootstrap\u2019s automatic and generic applicability, favorable statistical properties, and simplicity of implementation. Finally, as we show empirically, BLB is consistently more robust than alternatives such as the m out of n bootstrap and subsampling.\nWe next formalize our statistical setting and notation in Section 2, discuss relevant prior work in Section 3, and present BLB in full detail in Section 4. Subsequently, we present an empirical and theoretical study of statistical correctness in Section 5, an exploration of scalability including large-scale experiments on a distributed computing platform in Section 6, practical methods for automatically selecting hyperparameters in Section 7, and assessments on real data in Section 8."}, {"heading": "2. Setting and Notation", "text": "We assume that we observe a sample X1, . . . , Xn \u2208 X drawn i.i.d. from some true (unknown) underlying distribution P \u2208 P. Based only on this observed data, we obtain an estimate \u03b8\u0302n = \u03b8(Pn) \u2208 \u0398, where\nPn = n\u22121 \u2211n i=1 \u03b4Xi is the empirical distribution of X1, . . . , Xn. The true (unknown) population value to be estimated is \u03b8(P ). For example, \u03b8\u0302n might estimate a measure of correlation, the parameters of a regressor, or the prediction accuracy of a trained classification model. Noting that \u03b8\u0302n is a random quantity because it is based on n random observations, we define Qn(P ) \u2208 Q as the true underlying distribution of \u03b8\u0302n, which is determined by both P and the form of the mapping \u03b8. Our end goal is the computation of some metric \u03be(Qn(P )) : Q \u2192 \u039e, for \u039e a vector space, which informatively summarizes Qn(P ). For instance, \u03be might compute a confidence region, a standard error, or a bias. In practice, we do not have direct knowledge of P or Qn(P ), and so we must estimate \u03be(Qn(P )) itself based only on the observed data and knowledge of the form of the mapping \u03b8."}, {"heading": "3. Related Work", "text": "The bootstrap (Efron, 1979; Efron & Tibshirani, 1993) provides an automatic and widely applicable means of quantifying estimator quality: it simply uses the datadriven plugin approximation \u03be(Qn(P )) \u2248 \u03be(Qn(Pn)). While \u03be(Qn(Pn)) cannot be computed exactly in most cases, it is generally amenable to straightforward Monte Carlo approximation as follows: repeatedly resample n points i.i.d. from Pn, compute the estimate on each resample, form the empirical distribution Qn of the computed estimates, and approximate \u03be(Qn(P )) \u2248 \u03be(Qn). Though conceptually simple and powerful, this procedure requires repeated estimator computation on resamples having size comparable to that of the original dataset. Therefore, if the original dataset is large, then this computation can be costly.\nWhile the literature does contain some discussion of techniques for improving the computational efficiency of the bootstrap, that work is largely devoted to reducing the number of Monte Carlo resamples required (Efron, 1988; Efron & Tibshirani, 1993). These techniques in general introduce significant additional complexity of implementation and do not obviate the need for repeated estimator computation on resamples having size comparable to that of the original dataset.\nBootstrap variants such as the m out of n bootstrap (Bickel et al., 1997) and subsampling (Politis et al., 1999) were introduced to achieve statistical consistency in edge cases in which the bootstrap fails, though they also have the potential to yield computational benefits. The m out of n bootstrap (and subsampling) functions as follows, for m < n: repeatedly resample m points i.i.d. from Pn (subsample m points without replacement from X1, . . . , Xn), compute the\nestimate on each resample (subsample), form the empirical distribution Qn,m of the computed estimates, approximate \u03be(Qm(P )) \u2248 \u03be(Qn,m), and apply an analytical correction to in turn approximate \u03be(Qn(P )). This final analytical correction uses prior knowledge of the convergence rate of \u03b8\u0302n as n increases and is necessary because each estimate is computed based on only m rather than n points.\nThese procedures have a more favorable computational profile than the bootstrap, as they only require repeated estimator computation on smaller sets of data. However, they require knowledge and explicit use of the convergence rate of \u03b8\u0302n, and, as we show in our empirical investigation below, their success is sensitive to the choice of m. While schemes have been proposed for automatic selection of an optimal value of m (Bickel & Sakov, 2008), they require significantly greater computation which would eliminate any computational gains. It is also worth noting that some work on the m out of n bootstrap has explicitly sought to reduce computational costs using two different values of m in conjunction with extrapolation (Bickel & Yahav, 1988; Bickel & Sakov, 2002). However, these approaches explicitly use series expansions of the cdf values of the estimator\u2019s sampling distribution and hence are less automatically usable; they also require execution of the m out of n bootstrap for multiple different values of m."}, {"heading": "4. Bag of Little Bootstraps (BLB)", "text": "The Bag of Little Bootstraps (Algorithm 1) functions by averaging the results of bootstrapping multiple small subsets of X1, . . . , Xn. More formally, given a subset size b < n, BLB samples s subsets of size b from the original n data points, uniformly at random (one can also impose the constraint that the subsets be disjoint). Let I1, . . . , Is \u2282 {1, . . . , n} be the corresponding index multisets (note that |Ij | = b,\u2200j), and let P(j)n,b = b\u22121 \u2211 i\u2208Ij \u03b4Xi be the empirical distribution corresponding to subset j. BLB\u2019s estimate of\n\u03be(Qn(P )) is then given by s \u22121\u2211s j=1 \u03be(Qn(P (j) n,b)). Although the terms \u03be(Qn(P(j)n,b)) cannot be computed analytically in general, they are computed numerically by the inner loop in Algorithm 1 via Monte Carlo approximation in the manner of the bootstrap: for each term j, we repeatedly resample n points i.i.d. from P(j)n,b, compute the estimate on each resample, form the empirical distribution Q\u2217n,j of the computed estimates, and approximate \u03be(Qn(P(j)n,b)) \u2248 \u03be(Q\u2217n,j).\nTo realize the substantial computational benefits afforded by BLB, we utilize the following crucial fact: each BLB resample, despite having nominal size n,\nAlgorithm 1 Bag of Little Bootstraps (BLB)\nInput: Data X1, . . . , Xn \u03b8: estimator of interest \u03be: estimator quality assessment b: subset size s: number of sampled subsets r: number of Monte Carlo iterations Output: An estimate of \u03be(Qn(P )) for j \u2190 1 to s do //Subsample the data\nRandomly sample a set I = {i1, . . . , ib} of b indices from {1, . . . , n} without replacement [or, choose I to be a disjoint subset of size b from a predefined random partition of {1, . . . , n}] //Approximate \u03be(Qn(P(j)n,b)) for k \u2190 1 to r do\nSample (n1, . . . , nb) \u223c Multinomial(n,1b/b) P\u2217n,k \u2190 n\u22121 \u2211b a=1 na\u03b4Xia\n\u03b8\u0302\u2217n,k \u2190 \u03b8(P\u2217n,k) end for Q\u2217n,j \u2190 r\u22121 \u2211r k=1 \u03b4\u03b8\u0302\u2217n,k\n\u03be\u2217n,j \u2190 \u03be(Q\u2217n,j) end for //Average values of \u03be(Qn(P(j)n,b)) computed //for different data subsets\nreturn s\u22121 \u2211s j=1 \u03be \u2217 n,j\ncontains at most b distinct data points. In particular, to generate each resample, it suffices to draw a vector of counts from an n-trial uniform multinomial distribution over b objects. We can then represent each resample by simply maintaining the at most b distinct points present within it, accompanied by corresponding sampled counts (i.e., each resample requires only storage space in O(b)). In turn, if the estimator can work directly with this weighted data representation, then its computational requirements\u2014with respect to both time and storage space\u2014scale only in b, rather than n. Fortunately, this property does indeed hold for many if not most commonly used estimators, including M-estimators such as linear and kernel regression, logistic regression, and Support Vector Machines, among many others.\nAs a result, BLB only requires repeated computation on small subsets of the original dataset and avoids the bootstrap\u2019s problematic need for repeated computation of estimates on resamples having size comparable to that of the original dataset. A simple and standard calculation (Efron & Tibshirani, 1993) shows that each bootstrap resample contains approximately 0.632n distinct points, which is large if n is large. In contrast, as\ndiscussed above, each BLB resample contains at most b distinct points, and b can be chosen to be much smaller than n or 0.632n. For example, we might take b = n\u03b3 where \u03b3 \u2208 [0.5, 1]. More concretely, if n = 1, 000, 000, then each bootstrap resample would contain approximately 632, 000 distinct points, whereas with b = n0.6 each BLB subsample and resample would contain at most 3, 981 distinct points. If each data point occupies 1 MB of storage space, then the original dataset would occupy 1 TB, a bootstrap resample would occupy approximately 632 GB, and each BLB subsample or resample would occupy at most 4 GB.\nBLB thus has a significantly more favorable computational profile than the bootstrap. As seen in subsequent sections, our procedure typically requires less total computation to reach comparably high accuracy (fairly modest values of s and r suffice); is significantly more amenable to implementation on distributed and parallel computing architectures which are often used to process large datasets; maintains the favorable statistical properties of the bootstrap; and is more robust than the m out of n bootstrap and subsampling to the choice of subset size."}, {"heading": "5. Statistical Correctness", "text": "We show via both a simulation study and theoretical analysis that BLB shares the favorable statistical performance of the bootstrap while also being consistently more robust than the m out of n bootstrap and subsampling to the choice of b. Here we present a representative summary of our investigation of statistical correctness; see Kleiner et al. (2012) for more detail.\nWe investigate the empirical performance characteristics of BLB and compare to existing methods via experiments on different simulated datasets and estimation tasks. Use of simulated data is necessary here because it allows knowledge of Qn(P ) and hence \u03be(Qn(P )) (i.e., ground truth). We consider two different settings: regression and classification. For both settings, the data have the form Xi = (X\u0303i, Yi) \u223c P , i.i.d. for i = 1, . . . , n, where X\u0303i \u2208 Rd; Yi \u2208 R for regression, whereas Yi \u2208 {0, 1} for classification. We use n = 20, 000 for the plots shown, and d is set to 100 for regression and 10 for classification. In each case, \u03b8\u0302n estimates a parameter vector in Rd (via either least squares or logistic regression with Newton\u2019s method, all implemented in MATLAB) for a linear or generalized linear model of the mapping between X\u0303i and Yi. We define \u03be as computing a set of marginal 95% confidence intervals, one for each element of the estimated parameter vector (averaging across \u03be\u2019s consists of averaging element-wise interval boundaries).\nTo evaluate the various quality assessment procedures on a given estimation task and true underlying data distribution P , we first compute the ground truth \u03be(Qn(P )) based on 2, 000 realizations of datasets of size n from P . Then, for an independent dataset realization of size n from the true underlying distribution, we run each quality assessment procedure and record the estimate of \u03be(Qn(P )) produced after each iteration (e.g., after each bootstrap resample or BLB subsample is processed), as well as the cumulative time required to produce that estimate. Every such estimate is evaluated based on the average (across dimensions) relative absolute deviation of its component-wise confidence intervals\u2019 widths from the corresponding true widths: given an estimated confidence interval width c and a true width co, the relative deviation of c from co is defined as |c\u2212 co|/co. We repeat this process on five independent dataset realizations of size n and average the resulting relative errors and corresponding times across these five datasets to obtain a trajectory of relative error versus time for each quality assessment procedure (the trajectories\u2019 variances are small relative to the relevant differences between their means, so the variances are not shown in our plots). To maintain consistency of notation, we henceforth refer to the m out of n bootstrap as the b out of n bootstrap. For BLB, the b out of n bootstrap, and subsampling, we consider b = n\u03b3 where \u03b3 \u2208 {0.5, 0.6, 0.7, 0.8, 0.9}; we use r = 100 in all runs of BLB.\nFigure 1 shows a set of representative results for the classification setting, where P generates the components of X\u0303i i.i.d. from StudentT(3) and Yi \u223c Bernoulli((1 + exp(\u2212X\u0303Ti 1))\u22121); we use this representative empirical setting in subsequent sections as well. As seen in the figure, BLB (left plot) succeeds in converging to low relative error more quickly than the bootstrap for b > n0.5, while converging to somewhat higher relative error for b = n0.5. We are more robust than the b out of n bootstrap (middle plot), which fails to converge to low relative error for b \u2264 n0.6. In fact, even for b = n0.5, BLB\u2019s performance is substantially superior to that of the b out of n bootstrap. For the aforementioned case in which BLB does not match the relative error of the bootstrap, additional empirical results (right plot) and our theoretical analysis indicate that this discrepancy in relative error diminishes as n increases. Identical evaluation of subsampling (plots not shown) shows that it performs strictly worse than the b out of n bootstrap. Qualitatively similar results also hold in both the classification and regression settings (with the latter generally showing better performance) when P generates X\u0303i from Normal, Gamma, or StudentT distributions, and when P uses a non-\nlinear noisy mapping between X\u0303i and Yi (so that we estimate a misspecified model).\nThese experiments illustrate the statistical correctness of BLB, as well as its improved robustness to the choice of b. The results are borne out by our theoretical analysis, which shows that BLB has statistical properties that are identical to those of the bootstrap, under the same conditions that have been used in prior analysis of the bootstrap. In particular, BLB is asymptotically consistent for a broad class of estimators and quality measures (see Kleiner et al. (2012) for proof):\nTheorem 1. Suppose that \u03b8 is Hadamard differentiable at P tangentially to some subspace, with P and P(j)n,b viewed as maps from some Donsker class F to R such that F\u03b4 is measurable for every \u03b4 > 0, where F\u03b4 = {f \u2212 g : f, g \u2208 F , \u03c1P (f \u2212 g) < \u03b4} and \u03c1P (f) = ( P (f \u2212 Pf)2 )1/2 . Additionally, assume that \u03be is continuous in the space of distributions Q with respect to a metric that metrizes weak convergence. Then, up to centering and rescaling of \u03b8\u0302n,\ns\u22121 \u2211s j=1 \u03be(Qn(P (j) n,b)) \u2212 \u03be(Qn(P ))\nP\u2192 0 as n \u2192 \u221e, for any sequence b \u2192 \u221e and for any fixed s. Furthermore, the result holds for sequences s \u2192 \u221e if E|\u03be(Qn(P(j)n,b))| <\u221e.\nBLB is also higher-order correct: despite the fact that the procedure only applies the estimator in question to subsets of the full observed dataset, it shares the fast convergence rates of the bootstrap, which permit convergence of the procedure\u2019s output at rate O(1/n) rather than the rate of O(1/ \u221a n) achieved by asymptotic approximations. To achieve these fast convergence rates, some natural conditions on BLB\u2019s hyperparameters are required, for example that b = \u2126( \u221a n) and that s be sufficiently large with respect to the variability in the observed data. Quite interestingly, these conditions permit b to be significantly smaller than n, with b/n\u2192 0 as n\u2192\u221e. See Kleiner et al. (2012) for our theoretical results on higher-order correctness."}, {"heading": "6. Scalability", "text": "The experiments of the preceding section, though primarily intended to investigate statistical performance, also provide some insight into computational performance: as seen in Figure 1, when computing serially, BLB generally requires less time, and hence less total computation, than the bootstrap to attain comparably high accuracy. Those results only hint at BLB\u2019s superior ability to scale computationally to large datasets, which we now demonstrate in full in the following discussion and via large-scale experiments.\nModern massive datasets often exceed both the processing and storage capabilities of individual processors or compute nodes, thus necessitating the use of parallel and distributed computing architectures. Indeed, in the large data setting, computing a single fulldata point estimate often requires simultaneous distributed computation across multiple compute nodes, among which the observed dataset is partitioned. As a result, the scalability of a quality assessment method is closely tied to its ability to effectively utilize such computing resources.\nDue to the large size of bootstrap resamples (recall that approximately 63% of data points appear at least once in each resample), the following is the most natural avenue for applying the bootstrap to large-scale data using distributed computing: given data on a cluster of compute nodes, parallelize the estimate computation on each resample across the cluster, and compute on one resample at a time. This approach, while at least potentially feasible, remains quite problematic. Each estimate computation requires the use of an entire cluster of compute nodes, and the bootstrap repeatedly incurs the associated overhead, such as the cost of repeatedly communicating intermediate data among nodes. Additionally, many cluster computing systems in widespread use (e.g., Hadoop MapReduce) store data only on disk, rather than in memory, due\nto physical size constraints (if the data size exceeds the amount of available memory) or architectural constraints (e.g., the need for fault tolerance). In that case, the bootstrap incurs the extreme costs associated with repeatedly reading a very large dataset from disk; though disk read costs may be acceptable when (slowly) computing only a single point estimate, they easily become prohibitive when computing many estimates on one hundred or more resamples.\nIn contrast, BLB permits computation on multiple (or even all) subsamples and resamples simultaneously in parallel, allowing for straightforward and effective distributed and parallel implementations which enable effective scalability and large computational gains. Because BLB subsamples and resamples can be significantly smaller than the original dataset, they can be transferred to, stored by, and processed on individual (or very small sets of) compute nodes. For example, we can naturally leverage modern hierarchical distributed architectures by distributing subsamples to different compute nodes and subsequently using intranode parallelism to compute across different resamples generated from the same subsample. Note that generation and distribution of the subsamples requires only a single pass over the full dataset (i.e., only a single read of the full dataset from disk, if it is stored only on disk), after which all required data (i.e., the subsamples) can be stored in memory. Beyond this significant architectural benefit, we also achieve implementation and algorithmic benefits: we do not need to parallelize the estimator computation internally, as BLB uses the available parallelism to compute on multiple resamples simultaneously, and exposing the estimator to only b rather than n distinct points significantly reduces the computational cost of estimation, particularly if the estimator computation scales super-linearly.\nWe now empirically substantiate the preceding discussion via large-scale experiments performed on Amazon EC2. We use the representative experimental setup of Figure 1, but now with d = 3, 000, n = 6, 000, 000, Yi \u223c Bernoulli((1 + exp(\u2212X\u0303Ti 1/ \u221a d))\u22121), and the logistic regression implemented using L-BFGS. The size of a full observed dataset in this setting is thus approximately 150 GB. We compare the performance of BLB and the bootstrap (now omitting the m out of n bootstrap and subsampling due to the shortcomings illustrated in Section 5), both implemented as described above (we parallelize the estimator computation for the bootstrap by simply distributing gradient computations via MapReduce) using the Spark cluster computing framework (Spark, 2012), which provides the ability to either read data from disk or cache it in memory (provided that sufficient memory is available)\nfor faster repeated access. For BLB, we use r = 50, s = 5, and b = n0.7. Due to the larger data size and use of a distributed architecture, we now implement the bootstrap using Poisson resampling, and we compute ground truth using 200 independent dataset realizations.\nIn the left plot of Figure 2, we show results obtained using a cluster of 10 worker nodes, each having 6 GB of memory and 8 compute cores; thus, the total memory of the cluster is 60 GB, and the full dataset (150 GB) can only be stored on disk. As expected, the time required by the bootstrap to produce even a low-accuracy output is prohibitively high, while BLB provides a high-accuracy output quite quickly, in less than the time required to process even a single bootstrap resample. In the right plot of Figure 2, we show results obtained using a cluster of 20 worker nodes, each having 12 GB of memory and 4 compute cores; thus, the total memory of the cluster is 240 GB, and we cache the full dataset in memory for fast repeated access. Unsurprisingly, the bootstrap\u2019s performance improves significantly with respect to the previous experiment. However, the performance of BLB (which also improves), remains substantially better than that of the bootstrap.\nThus, relative to the bootstrap, BLB both allows more natural and effective use of parallel and distributed computational resources and decreases the total computational cost of assessing estimator quality. Finally, it is worth noting that even if only a single compute node is available, BLB allows the following somewhat counterintuitive possibility: even if it is prohibitive to actually compute a point estimate for the full observed data using a single compute node (because the full dataset is large), it may still be possible to efficiently assess such a point estimate\u2019s quality using only a single compute node by processing one subsample (and the associated resamples) at a time."}, {"heading": "7. Hyperparameter Selection", "text": "Like existing resampling-based procedures such as the bootstrap, BLB requires the specification of hyperparameters controlling the number of subsamples and resamples processed. Setting such hyperparameters to be sufficiently large is necessary to ensure good statistical performance; however, setting them to be unnecessarily large results in wasted computation. Prior work on the bootstrap and related procedures generally assumes that a procedure\u2019s user will simply select a priori a large, constant number of resamples to be processed (with the exception of Tibshirani (1985), who does not provide a general solution to this issue). However, this approach reduces the level of automation of these methods and can be quite inefficient in the large data setting.\nThus, we now examine the dependence of BLB\u2019s performance on the choice of r and s, with the goal of better understanding their influence and providing guidance and adaptive (i.e., more automatic) methods for their selection. The left plot of Figure 3 illustrates the influence of r and s, giving the relative errors achieved by BLB with b = n0.7 for different r, s pairs in the representative empirical setting described in Section 5. In particular, note that for all but the smallest values of r and s, it is possible to choose these values independently such that BLB achieves low relative error; in this case, choosing s \u2265 3, r \u2265 50 is sufficient.\nWhile these results are useful and provide some guidance, we expect the minimal sufficient values of r and s to change based on the identity of \u03be (e.g., we expect a confidence interval to be harder to compute and hence to require larger r than a standard error) and the properties of the underlying data. Thus, to help avoid the need to choose r and s conservatively, we now provide a means for adaptive hyperparameter selection, which we validate empirically.\nConcretely, to select r adaptively in the inner loop of Algorithm 1, we propose, for each subsample j, to continue to process resamples and update \u03be\u2217n,j until it has ceased to change significantly. Noting that the values \u03b8\u0302\u2217n,k used to compute \u03be \u2217 n,j are conditionally i.i.d. given a subsample, for most forms of \u03be the series of computed \u03be\u2217n,j values will be well behaved and will converge (in many cases at rate O(1/ \u221a r), though with unknown constant) to a constant target value as more resamples are processed. Therefore, it suffices to process resamples (i.e., to increase r) until we are satisfied that \u03be\u2217n,j has ceased to fluctuate significantly; we propose using Algorithm 2 to assess this convergence. The same scheme can be used to select s adaptively by processing more subsamples (i.e., increasing s) until BLB\u2019s\nAlgorithm 2 Convergence Assessment\nInput: A series z(1), z(2), . . . , z(t) \u2208 Rd w \u2208 N: window size (< t) \u2208 R: target relative error (> 0)\nOutput: true iff the input series is deemed to have ceased to fluctuate beyond the target relative error\nif \u2200j \u2208 [1, w], 1d \u2211d i=1 |z(t\u2212j)i \u2212z (t) i |\n|z(t)i | \u2264 then true\nelse false\noutput value s\u22121 \u2211s j=1 \u03be \u2217 n,j has stabilized; in this case, one can simultaneously also choose r adaptively and independently for each subsample.\nThe middle plot of Figure 3 shows the results of applying such adaptive hyperparameter selection. For selection of r we use = 0.05 and w = 20, and for selection of s we use = 0.05 and w = 3. As seen in the plot, the adaptive hyperparameter selection allows BLB to cease computing shortly after it has converged (to low relative error), limiting the amount of unnecessary computation that is performed. Though selected a priori, and w are more intuitively interpretable and less dependent on the details of \u03be and the underlying data generating distribution than r and s. Indeed, the aforementioned specific values of and w yield results of comparably good quality when also used for a variety of other synthetic and real data generation settings (see Section 8 below), as well as for different forms of \u03be (see the righthand table in Figure 3, which shows that smaller values of r are selected when \u03be is easier to compute). Thus, our scheme significantly helps to alleviate the burden of hyperparameter selection.\nAutomatic selection of a value of b in a computationally efficient manner is more difficult due to the inability to reuse computations performed for different values of b. One could consider similarly increasing b from some small value until the output of BLB stabilizes; devising a means of doing so efficiently is the subject of future work. Nonetheless, based on our fairly extensive empirical investigation, it seems that b = n0.7 is a reasonable and effective choice in many situations."}, {"heading": "8. Real Data", "text": "We now present the results of applying BLB to several different real datasets. In this setting, given the absence of ground truth, it is not possible to objectively evaluate estimator quality assessment methods\u2019 statistical correctness. As a result, we are reduced to comparing the outputs of different methods to each other; we also now report the average (across dimensions) absolute confidence interval width produced by\neach procedure, rather than relative error.\nFigure 4 shows results for BLB, the bootstrap, and the b out of n bootstrap on the UCI connect4 dataset (logistic regression, d = 42, n = 67, 557). We select the BLB hyperparameters r and s using the adaptive method described in the previous section. Notably, the outputs of BLB for all values of b considered, and the output of the bootstrap, are tightly clustered around the same value; additionally, as expected, BLB converges more quickly than the bootstrap. However, the values produced by the b out of n bootstrap vary significantly as b changes, thus further highlighting this procedure\u2019s lack of robustness. We have obtained qualitatively similar results on six additional UCI datasets (ct-slice, magic, millionsong, parkinsons, poker, shuttle) with different estimators (linear regression and logistic regression) and a range of different values of n and d; see Kleiner et al. (2012) for additional plots."}, {"heading": "9. Conclusion", "text": "We have presented a new procedure, BLB, which provides a powerful new alternative for automatic, accurate assessment of estimator quality that is well suited to large-scale data and modern parallel and distributed computing architectures."}, {"heading": "Acknowledgments", "text": "This research is supported in part by an NSF CISE Expeditions award, gifts from Google, SAP, Amazon Web Services, Blue Goji, Cisco, Cloudera, Ericsson, General Electric, Hewlett Packard, Huawei, Intel, MarkLogic, Microsoft, NetApp, Oracle, Quanta, Splunk, VMware and by DARPA (contract #FA8650-11-C-7136). Ameet Talwalkar was supported by NSF award No. 1122732."}], "references": [{"title": "Extrapolation and the bootstrap", "author": ["P.J. Bickel", "A. Sakov"], "venue": "Sankhya: The Indian Journal of Statistics,", "citeRegEx": "Bickel and Sakov,? \\Q2002\\E", "shortCiteRegEx": "Bickel and Sakov", "year": 2002}, {"title": "On the choice of m in the m out of n bootstrap and confidence bounds for extrema", "author": ["P.J. Bickel", "A. Sakov"], "venue": "Statistica Sinica,", "citeRegEx": "Bickel and Sakov,? \\Q2008\\E", "shortCiteRegEx": "Bickel and Sakov", "year": 2008}, {"title": "Richardson extrapolation and the bootstrap", "author": ["P.J. Bickel", "J.A. Yahav"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bickel and Yahav,? \\Q1988\\E", "shortCiteRegEx": "Bickel and Yahav", "year": 1988}, {"title": "Resampling fewer than n observations: Gains, losses, and remedies for losses", "author": ["P.J. Bickel", "F. Gotze", "W. van Zwet"], "venue": "Statistica Sinica,", "citeRegEx": "Bickel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 1997}, {"title": "Bootstrap methods: Another look at the jackknife", "author": ["B. Efron"], "venue": "Annals of Statistics,", "citeRegEx": "Efron,? \\Q1979\\E", "shortCiteRegEx": "Efron", "year": 1979}, {"title": "More efficient bootstrap computations", "author": ["B. Efron"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Efron,? \\Q1988\\E", "shortCiteRegEx": "Efron", "year": 1988}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1993}, {"title": "A scalable bootstrap for massive data", "author": ["A. Kleiner", "A. Talwalkar", "P. Sarkar", "M.I. Jordan"], "venue": "URL http://arxiv.org/abs/1112.5016", "citeRegEx": "Kleiner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kleiner et al\\.", "year": 2012}, {"title": "How many bootstraps", "author": ["R. Tibshirani"], "venue": "Technical report,", "citeRegEx": "Tibshirani,? \\Q1985\\E", "shortCiteRegEx": "Tibshirani", "year": 1985}], "referenceMentions": [{"referenceID": 4, "context": "The bootstrap (Efron, 1979; Efron & Tibshirani, 1993) is perhaps the best known and most widely used among these methods, due to its simplicity, generic applicability, and automatic nature.", "startOffset": 14, "endOffset": 53}, {"referenceID": 3, "context": "Efforts to ameliorate statistical shortcomings of the bootstrap in turn led to the development of related methods such as the m out of n bootstrap and subsampling (Bickel et al., 1997; Politis et al., 1999).", "startOffset": 163, "endOffset": 206}, {"referenceID": 4, "context": "The bootstrap (Efron, 1979; Efron & Tibshirani, 1993) provides an automatic and widely applicable means of quantifying estimator quality: it simply uses the datadriven plugin approximation \u03be(Qn(P )) \u2248 \u03be(Qn(Pn)).", "startOffset": 14, "endOffset": 53}, {"referenceID": 5, "context": "While the literature does contain some discussion of techniques for improving the computational efficiency of the bootstrap, that work is largely devoted to reducing the number of Monte Carlo resamples required (Efron, 1988; Efron & Tibshirani, 1993).", "startOffset": 211, "endOffset": 250}, {"referenceID": 3, "context": "Bootstrap variants such as the m out of n bootstrap (Bickel et al., 1997) and subsampling (Politis et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 7, "context": "Here we present a representative summary of our investigation of statistical correctness; see Kleiner et al. (2012) for more detail.", "startOffset": 94, "endOffset": 116}, {"referenceID": 7, "context": "In particular, BLB is asymptotically consistent for a broad class of estimators and quality measures (see Kleiner et al. (2012) for proof):", "startOffset": 106, "endOffset": 128}, {"referenceID": 7, "context": "See Kleiner et al. (2012) for our theoretical results on higher-order correctness.", "startOffset": 4, "endOffset": 26}, {"referenceID": 8, "context": "Prior work on the bootstrap and related procedures generally assumes that a procedure\u2019s user will simply select a priori a large, constant number of resamples to be processed (with the exception of Tibshirani (1985), who does not provide a general solution to this issue).", "startOffset": 198, "endOffset": 216}, {"referenceID": 7, "context": "We have obtained qualitatively similar results on six additional UCI datasets (ct-slice, magic, millionsong, parkinsons, poker, shuttle) with different estimators (linear regression and logistic regression) and a range of different values of n and d; see Kleiner et al. (2012) for additional plots.", "startOffset": 255, "endOffset": 277}], "year": 2012, "abstractText": "The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB\u2019s behavior, including a study of its statistical correctness, its largescale implementation and performance, selection of hyperparameters, and performance on real data.", "creator": "LaTeX with hyperref package"}}}