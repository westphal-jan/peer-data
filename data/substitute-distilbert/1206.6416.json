{"id": "1206.6416", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "An Infinite Latent Attribute Model for Network Data", "abstract": "latent theoretical models for network data extract a summary of the relational structure underlying an acquired data. the simplest possible models subdivide vertices of the network into clusters ; the probability between a link between any phylogenetic levels then depends only then their cluster hierarchy. relatively available models can be classified by whether clusters are disjoint or are allowed to overlap. these models can explain a \" flat \" clustering structure. hierarchical bayesian models provide a natural analogy to capture more complex dependencies. we propose conceptual model in which objects interact characterised by a latent object vector. each feature is itself partitioned into disjoint groups ( hierarchy ), corresponding to a second layer of hierarchy. in experimental comparisons, the model achieves promising improved predictive performance on social and biological link prediction parameters. the results indicate that models with a single layer hierarchy over - simplify real networks.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (412kb)", "http://arxiv.org/abs/1206.6416v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["konstantina palla", "david a knowles", "zoubin ghahramani"], "accepted": true, "id": "1206.6416"}, "pdf": {"name": "1206.6416.pdf", "metadata": {"source": "META", "title": "An Infinite Latent Attribute Model for Network Data", "authors": ["Konstantina Palla", "David A. Knowles", "Zoubin Ghahramani"], "emails": ["kp376@cam.ac.uk", "dak33@cam.ac.uk", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Network data encoding pairwise relations between objects appears in many fields. For instance, in biology, a protein network connects interacting partners, while in a social network, links among people indicate relations. We focus on the most common type of network data \u2014sets of observations represented as an unweighted, undirected graph\u2014in the ensuing discussion. The motivation behind the analysis of these networks is two\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nfold. Firstly, there is a desire to understand the latent structure of the network; what are the features of the proteins that account for the observed interactions and what is the mechanism behind the links or non-links among groups of people. Second, the prediction of \u201cmissing\u201d links in the network arises as an important challenge; how likely is it that a pair of proteins interact or that two social network members are friends. A prominent theme in machine learning is the use of latent variable methods, which approach this problem by extracting a simplified summary of the graph and predicting the presence or absence of links based on this latent representation. Latent class and latent feature models are the two most common categories found in the literature.\nLatent class models assume that there are a number of clusters (classes) and that each object belongs to a single cluster. Under these models, the link probability between two objects depends only on their cluster assignments. Early work in this category includes the stochastic block model (SB) proposed in Nowicki and Snijders (2001). Instead of assuming a fixed number of clusters, the Infinite Relational Model (IRM) and the Infinite Hidden Relational Model (Kemp and Tenenbaum, 2006; Xu et al., 2006) use the Chinese restaurant process (Pitman, 2002) to allow a potentially infinite number of clusters. The Mixed Membership Stochastic Block Model (Airoldi et al., 2009) (MMSB) increases the expressiveness of the latent class models by allowing mixed membership, associating each object with a distribution over clusters.\nLatent feature models increase the flexibility of the generative process by letting each object possess a vector of features and determine the link probabilities based on interactions among the features. In Hoff et al. (2001) the link probability between two objects is determined by the similarity of their real-valued fea-\nture vectors. Miller et al. (2009) uses a vector of binary features which can be interpreted as allowing objects to belong to multiple clusters at the same time. Their model, the Latent Feature Infinite Relational Model (LFRM), assumes that the number of clusters is not known a priori and uses the Indian Buffet Process (IBP) (Griffiths and Ghahramani, 2005) to determine the number of latent clusters.\nThe limitation of a single cluster membership makes the latent class models less flexible than the latent feature models. As an intuitive example, consider a network of individuals at a collegiate University, in which a link denotes friendship or acquaintance. Here there will be multiple types of cluster, for instance colleges, departments and sports teams. A person might be a member of more than one cluster and his clustermemberships determine his interaction with others. To capture this structure a single membership model, such as the IRM, must introduce a cluster for each possible combination of the types of cluster, which in our example would be to introduce clusters such as \u2018Gryffindor college, Department of Mathematics, Football\u2019. This results in an exponential explosion of clusters, making learning, inference and generalisation difficult. Latent feature models, e.g. the LFRM, can instead use the feature vector representation to implicitly account for the possible combination of clusters. Though powerful, these models only account for a flat clustering of the objects. In the context of the University social network, the \u2018college\u2019 feature might be divided into many different subclusters, such as \u2018Slytherin college\u2019, \u2019Gryffindor college\u2019 etc. The same for \u2018sport\u2019, with subclusters like \u2018basketball\u2019, \u2018tennis\u2019, etc. The LFRM must represent each cluster with a new feature, which will result in feature vectors of greater size with a cost in interpretability. Allowing an explicit representation of the partitioning of each general class into subclasses would provide a more structured representation of the data.\nTowards this end, we develop a new nonparametric latent feature model. We use a binary feature vector to indicate the features that an object has. If an object has a particular feature, then the object belongs to a particular subcluster of this feature. Equivalently, we can think of objects having several attributes (features) which have discrete values (the subcluster assignments). Following our university example, a person might have the \u2018college\u2019 attribute and belong to the \u2018Gryffindor college\u2019 subcluster, but cannot simultaneously be a member of another college. We denote our model by ILA for Infinite Latent Attribute model. We use a nonparametric Bayesian approach to simultaneously infer the number of features and number of sub-\nclusters inside each feature, while at the same time inferring what features are active for each object, which subcluster it belongs to and how subcluster membership influences the observed interactions.\nThe paper is arranged as follows. In Section 2 we describe the generative process for our nonparametric model. Section 3 explains the relationship of our model to several recently proposed models. In Section 4 we derive the algorithm for performing approximate posterior inference, parameter estimation and link prediction. Section 5 gives some observations about the computational cost of our proposed model relative to others. Finally, in Section 6 we study our model\u2019s performance on one synthetic and two real datasets."}, {"heading": "2. Model Description", "text": "Let R be the N \u00d7 N binary matrix that contains the links among the objects. In ILA, each object i = 1, . . . , N , is represented by a binary vector of latent feature values, zi. If there are M features, then Z is a N \u00d7M binary matrix indicating which features each object has active, with zim \u2261 Z(i,m) = 1 if the ith object has feature m and zim = 0 otherwise. Let C be a set of vectors, that is C = {c(1), . . . , c(M)}, that describe the subcluster assignments within each feature, such that c(m) is a vector of length N where c (m) i denotes the subcluster the ith object belongs to in the mth feature (c (m) i is set to 0 if object i does not have feature m). The number of subclusters present in the mth feature, which is also not known a priori, is denoted as K(m), so that c (m) i \u2208 {0, 1, ...,K(m)}. Finally, let W be a set of M real-valued weight matrices of size K(m) \u00d7 K(m) each, where w(m)kk\u2032 \u2261 W (m)(k, k\u2032) is the weight that affects the probability of there being a link from object i to object j, given that object i belongs to subcluster k and object j belongs to subcluster k\u2032 of the mth feature.\nGiven the feature matrix Z, the set of the subcluster assignments C, and the set of the weight matrices W, the probability that there is a link from object i to object j is given by\nPr(rij = 1|zi, zj ,C,W) = \u03c3 (\u2211\nm\nzimzjmw (m) cmi c m j\n+ s ) ,\n(1) where the sum ranges over all M features, s is a bias term, and \u03c3(x) = (1 + e\u2212x)\u22121 is the sigmoid (logistic) function that maps the input arguments from (\u2212\u221e,+\u221e) to (0, 1), ensuring that the result is a valid probability. Under this model, only features that are on for both objects influence the probability of a link between them. For these common features, the ap-\npropriate weight values are summed up, depending on the subcluster assignments of i and j. The weight values are continuous variables which can be positive or negative allowing pairs of subclusters to encourage or discourage links between them correspondingly. We assume that given the Z, C and W, the probability of each link is independent and the likelihood is therefore as follows\nPr(R|Z,C,W) = \u220f\ni,j\nPr(rij |zi, zj ,C,W). (2)\nIn order to allow flexible inference of the latent structure from data, we set the number of possible features M and the number of subclusters in each feature K(m) to infinity by using an IBP prior on Z and CRP priors on the c\u2019s. The hierarchical generative model is then:\nZ|\u03b1 \u223c IBP(\u03b1) c(m)|\u03b3 \u223c CRP(\u03b3)\nw (m) kk\u2032 |\u03c3w \u223c N(0, \u03c32w)\nrij |Z,C,W \u223c Bernoulli ( \u03c3 (\u2211\nm\nzimzjmw (m) cmi c m j + s\n)) .\nThe ILA model is illustrated in Figure 1.\nThe IBP parameter, \u03b1, affects the number of represented features, whereas the CRP parameter, \u03b3, controls the number of subclusters inside each feature. To improve the flexibility of our model, we put Gamma priors on \u03b1 and \u03b3, and a Gaussian prior on the bias term s as follows\n\u03b1 \u223c G(1, 1), \u03b3 \u223c G(1, 1), s \u223c N (\u00b5s, \u03c32s),\nwhere \u00b5s and \u03c3s are the mean and standard deviation hyperparameters for the bias (we use \u00b5s = \u22121, \u03c3s = 4 unless otherwise stated)."}, {"heading": "3. Related work", "text": "Here we examine three models that are closely related to ILA. The IRM model of Kemp and Tenenbaum (2006) and the LFRM of Miller et al. (2009) both use nonparametric Bayesian approaches to account for potentially infinite number of clusters in the data. In the IRM, the link probability between two objects depends only on the clusters they are assigned to:\nPr(rij = 1|ci, cj , \u03b7) = \u03b7cicj , (3)\nwhere the link probabilities for each pair of clusters, {\u03b7kk\u2032 : k, k\u2032 = 1, ...,K} are given independent Beta priors, and the cluster assignments, c are given a CRP prior. The ILA and LFRM on the other hand, put a logistic-normal prior on the between feature and subcluster link probabilities. More specifically, the LFRM defines the link probability as\nPr(rij = 1|Z,W) = \u03c3 (\u2211\nkl\nzikzjlwkl + s\n) , (4)\nwhere W is a K \u00d7K real valued weight matrix (with K being the number of features), given an elementwise Gaussian prior, and Z is an N \u00d7 K matrix of binary feature vectors drawn from an IBP. Comparing Equations 4 and 1 for the ILA, we see how the two models differ. The LFRM defines a weight value for each possible pair of features, while ILA defines a weight matrix for each feature, whose elements correspond to every pair of subclusters in that feature. The link probability in LFRM depends on all the possible pairs of features that are on for both objects, while in the ILA model, the link probability is contributed to only by features that are simultaneously on for both objects. While subclusters within a feature can interact in ILA, subclusters from different features do not interact.\nUnlike the IRM, the ILA model does not partition the objects into a set of non-overlapping clusters; although it specifies non-overlapping subclusters for each feature, it also allows each object to have multiple features, thus accounting for multiple membership. ILA more expressive than LFRM because it associates each feature with a set of subclusters.\nInterestingly, both the IRM and LFRM can be thought of as special cases of our model. If only one column of Z is switched on in ILA (i.e. there is only one feature\nwhich is on for every object) then this is equivalent to the IRM. In this case the ILA likelihood becomes\nPr(rij = 1|Z = 1,C,W) = \u03c3 ( w (1)\nc1i c 1 j\n+ s ) . (5)\nContrasting this to Equation 3 the ILA has a logisticnormal prior on the between subcluster link probabilities rather than a Beta prior, but this is a relatively minor difference.\nIf the LFRM is constrained to have a weight matrix W with only diagonal non-zero elements, then its link probability becomes\nPr(rij = 1|Z,W) = \u03c3 (\u2211\nk\nzikzjkwkk + s\n) .\nThis is then equivalent to ILA in the case when there is only one subcluster in each feature, since the ILA link probability is then Pr(rij = 1|zi, zj ,C = 1,W) = \u03c3 (\u2211\nm\nzimzjmw (m) 11 +s\n) .\nThe ILA model can also be seen as a extension of the Multiplicative Attribute Graph (MAG) model proposed in Kim and Leskovec (2011), where the link probability is\nPr(rij = 1|C, \u03b7) = \u220f\nm\n\u03b7 (m) cmi c m j ,\nwhere \u03b7 is a set of M two by two matrices of probabilities with elementwise independent Beta priors, and the c\u2019s are equivalent to our subcluster assignment variables but constrained to takes values in {1, 2}. We extend this model in three ways: 1) we learn the number of subclusters in each feature, rather than fixing it to two, 2) we learn the number of features M , and 3) we incorporate additional sparsity, in that an object need not have a particular feature active at all. We parameterise our model in terms of real valued weights which contribute to the log odds of a link being on, rather than with probabilities that are multiplied together, but this entails no loss of flexibility. In fact this may be advantageous to ILA since the MAG suffers from each new feature decreasing all link probabilities.\nThere are several models that have been proposed for discovering hierarchical structure in relational data (Girvan and Newman, 2002; Roy et al., 2007). In these models, each object is still a member of one out of many non-overlapping clusters. Our model is distinct in allowing each object to be a member of many subclusters as long as these subclusters are in different features."}, {"heading": "4. Inference", "text": "In the following, we present a method for inferring the latent variables of the model: the infinite binary feature matrix Z, the subcluster assignments, c(m) for each feature m, and the weight matrices, W(m). Simultaneously we recover the number of features and the number of subclusters inside each feature. As with many other Bayesian models, exact inference is intractable so we employ Markov Chain Monte Carlo (MCMC), and follow an iterative procedure that achieves posterior inference over the latent variables. The sampler iterates as follows:\nSampling the feature matrix, Z. We Gibbs sample each element of Z in succession. For each object i, the sampler makes the following decisions: which of the current M available features should be turned on/off, and how many new features should be turned on. However, when turning on a feature the sampler must also sample a new subcluster assignment and, in case of adding a new subcluster, the related new weights.\nWe use exchangeability of the rows of Z and assume that the ith object is the last to be added to Z after N \u2212 1 rows have already been added. For all the M features currently present in Z, the conditional posterior probability of an entry zim, m = 1, . . . ,M follows a Bernoulli distribution:\nPr(zim = 1|Z\u2212im,C\u2212im,W,R) \u221d n\u2212im N Pr(R|zim = 1,Z\u2212im,C\u2212im,W), (6)\nwhere Z\u2212im is the Z matrix excluding the Z(i,m) element, n\u2212im is the number of times feature m is present in Z\u2212im and C\u2212im excludes the subcluster assignment c (m) i . To compute the probability in Equation 6, we need to sum over c (m) i , the space of the possible subclusters that the ith object may be assigned to if zim is to be turned on. This also includes integration over a possible new subcluster. However, the prior over the parameters W(m) related to a new subcluster is not conjugate because of the logistic link function, and thus the likelihood term cannot be computed exactly. To overcome this problem, we use the auxiliary variable approach proposed in Neal (2000) (Algorithm 8), both to facilitate the integration required in Equation 6, and to decide which subcluster to assign the ith object to in the mth feature if zim is turned on.\nWe must also sample the number of new features unique to the ith row, M (i) new. Instead of considering these features separately, we calculate the conditional posterior over M (i) new, using the fact that under\nthe IBP the prior distribution over Mnew for the last row is Poisson(\u03b1/N). Combining the Poisson prior with the likelihood, we obtain the conditional posterior over M (i) new. However, to obtain the required likelihood term we need values for C(m) and W(m) for the proposed new features. Clearly c\n(m) i = 1 for any new\nfeatures, since a feature active for only one object can only have one subcluster. Integrating over the weights is not straightforward because the prior over W(m) is not conjugate to the logistic likelihood. We therefore employ a Metropolis Hastings step, proposing values for w (m) 11 from the prior so that the acceptance ratio becomes simply the likelihood ratio for including the new features and associated C(m) and W(m) values in the model versus not including them.\nSampling the subcluster assignments, C. We may choose to resample each C(m) in succession as a second step, again using Algorithm 8 of Neal (2000). In practice we found this unnecessary since C is sampled in the process of sampling Z.\nSampling the weights, W. Given Z and C, the sampler successively resamples each of the weights {w(m)kk\u2032 : k, k\u2032 = 1, . . . ,K(m),m = 1, . . . ,M}. Since we do not have conjugacy (due to the logistic link function), we cannot sample directly from the posterior over w (m) kk\u2032 . To overcome this problem we used both Metropolis Hastings and slice sampling (Neal, 2003) but found the later resulted in faster mixing.\nHyperparameters. We use slice sampling for both the IBP hyperparameter, \u03b1, the CRP concentration parameter, \u03b3 and the bias, s.\nIRM implementation. Our implementation of the IRM model of Kemp and Tenenbaum (2006) uses standard single site Gibbs sampling along with the restricted Gibbs sampling split merge method of Jain and Neal (2000). In the IRM we are able to integrate out the parameters \u03b7 analytically due to conjugacy, so we need only sample the cluster assignments and the CRP concentration parameter, for which we use slice sampling.\nLFRM implementation. For the LFRM of Miller et al. (2009), we Gibbs sample the IBP matrix Z and slice sample each element of the weight matrix W sequentially, followed by the IBP concentration parameter."}, {"heading": "4.1. Sequential initialisation", "text": "The Gibbs updates described above are the simplest moves we could make in a MCMC inference procedure for the ILA model. However, these updates are quite incremental, since only a single variable is updated at a time. Due to the extremely large number of possible configuration states, \u220fM m=1(K\n(m)+1)N , the sampler can suffer from local modes and have somewhat slow mixing. Non-incremental moves, like splitting and merging features in the Z matrix or subcluster assignments in C can produce major changes in the configuration state in a single iteration and can help the sampler explore more efficiently. Split-merge sampling in the IBP has been previously described in Meeds et al. (2006). However, we found that a sequential initialization of the sampler improved the performance, guiding the sampler closer to neighborhoods of higher probability.\nTo sequentially initialise all parameters the objects are first randomly permuted and then added to the model as follows. Initially two objects are added to the model with no features active. Then a few (typically three) iterations of the MCMC sampler are run. Then the next object is added, with no features turned on, and another three iterations of the sampler are run. This procedure is iterated until all objects have been added. The sampler will naturally grow the number of features and subclusters within each feature as more data is added. The advantage of this method is that the initialisation is appropriate for the model, the sampler is very fast initially due to the small number of objects, and the search space is small initially so it is easier for the Markov chain to find a relatively high probability region of parameter space. We also used sequential initialisation for our implementation of LFRM, but not for IRM where we find split-merge sampling is able to better overcome local optima."}, {"heading": "4.2. Prediction", "text": "A principled way to evaluated a generative model is by its ability to predict missing data values given some observations. In our model, we collect T samples {{Z(1),C(1),W(1)}, . . . , {Z(T ),C(T ),W(T )}} and estimate the predictive distribution of a missing link as the average of the predictive distributions for each of the collected samples. Assuming that we want to predict the missing link rij between objects i and j, the approximate predictive distribution will be as follows\nPr(rij = 1|Rtrain) \u2248 1\nT\nT\u2211\nt=1\nPr(rij = 1|Z(t),C(t),W(t))."}, {"heading": "5. Computational complexity", "text": "In general, the computational cost of latent feature models scales quadratically in the number of objects. In the LFRM, computing the likelihood has a complexity of O(M2N2), where M and N is the number of represented features1 and the number of objects correspondingly. For ILA, the link probability between two objects given by Equation 1, results in computational cost O(MN2) when calculating the likelihood across all pairs. The computational cost of the IRM scales linearly in the number of links in the network, L = \u2211 ij rij , because the likelihood, with the link probabilities \u03b7 integrated out, can be written as\nPr(R|c) = \u220f\na,b\nBeta(n(a, b) + \u03b2, n\u0304(a, b) + \u03b2)\nBeta(\u03b2, \u03b2) ,\nwhere n(a, b) is the number of pairs of objects (i, j) where i \u2208 a and j \u2208 b and R(i, j) = 1, n\u0304(a, b) is the number of such pairs where R(i, j) = 0, and Beta(\u00b7, \u00b7) is the Beta function. The computational cost of computing the likelihood in the IRM is therefore O(K2L). In Morup et al. (2011), it is observed that if a noisyor likelihood model is used in the LFRM rather than the logistic Gaussian model, then the likelihood can be calculated in O(K2L) as for the IRM. This allows excellent scalability on typical sparse real world networks where the number of links is much smaller than the number of non-links. This scalable variant is applicable to our model, but comes with the significant restriction of only being able to have positive weights between clusters (homophily). As a result we leave this development to future work."}, {"heading": "6. Results", "text": "We present results on a toy synthetic data set and on two real world datasets: the NIPS coauthorship network and a novel gene interaction network."}, {"heading": "6.1. Synthetic data", "text": "We first explored the ability of our model to recover the underlying structure of a network using synthetic data. We considered one simple synthetic dataset (Figure 2a) hand-constructed to have an unambiguous most parsimonious solution under each model. Under ILA this is the feature matrix shown in Figure 2(b) with two features. The first feature has three homophilic subclusters (i.e. individuals tend to have links if they\n1M is potentially unbounded, but in practice the model will use some finite number of features to model any finite dataset.\nare in the same cluster), whereas the second feature has two heterophilic subclusters (i.e. individuals tend to link if they are in different clusters). We ran ILA for 200 MCMC iterations following sequential initialisation. The sample with the lowest energy (highest log probability under the posterior) corresponds exactly to the expected \u201ctrue\u201d structure, as shown in Figure 2b. The MAP sample found using LFRM is shown in Figure 2c. Again this is a passable explanation of the data but it is considerably more convoluted than the simple, interpretable but rich solution found using ILA. Note that running 2000 iterations (following sequential initialisation) of LFRM no better solution was found. In contrast the IRM finds the flat clustering of six clusters shown in Figure 2d, which is an acceptable solution but does not capture the rich structure that ILA is able to."}, {"heading": "6.2. NIPS coauthorship network", "text": "We compare the performance of the IRM, LFRM and ILA on the NIPS coauthorship dataset (Globerson et al., 2007), where a link corresponds to two individuals being coauthors of a paper at one of the first 17 NIPS conferences (see Figure 3(a)). Following Miller et al. (2009) we use only the 234 most connected authors. We run 10 repeats, each time holding out a different 20% of the data (links and non-links) and using a different random initialisation. We run two versions of ILA: the first with a fixed number of features M = 6, and the second which learns M , denoted M = \u221e. Note that even with M = 6 ILA is still extremely flexible since it can learn the number of subclusters in each feature. We run 500 iterations for ILA and 1000 iterations for IRM and LFRM, and calculate\nevaluation metrics averaged over the last 300 samples. The results are shown in Table 1. We confirm the finding in Miller et al. (2009) that LFRM outperforms the IRM on this dataset. However, across all three evaluation metrics both ILA versions significantly outperform LFRM (for example, the t-test between the test log likelihoods for LFRM and ILA (M = 6) shows the means to be significantly different with a p-value of 10\u22127). The fully infinite version of ILA performs slightly, but still statistically significantly, better than when we constrain M = 6 for training error and test error. For test log likelihood ILA (M = \u221e) still appears to perform slightly better than ILA (M = 6) but the difference is not statistically significant based on a t-test. Under the ILA posterior M is concentrated around 7 or 8 features, with typically 2 to 4 subclusters per feature.\nIn Figure 3 the link predictions for each of the three models are presented. Figures 3(b)-(d) visualize the belief of each model that there should be a link between each pair of authors. The link matrices were constructed after running the three models on the NIPS 1-17 dataset for 500 iterations, using the same random seed and averaging over the last 160 samples. To facilitate interpretability, we ordered the authors by the clusters found by the IRM. It can be clearly seen that both the LFRM and ILA models outperform the IRM model by appearing more confident and reproducing the corresponding network more faithfully.\nConsidering Figures 3(c)-(d), LFRM and ILA appear comparable, with ILA being slightly more confident. Quantitatively however, ILA gives a test log likelihood of \u22120.0295 as opposed to \u22120.0386 for the LFRM model. We also report the AUC metric, the area under the ROC (Receiver Operating Characteristic) curve, for the held-out data."}, {"heading": "6.3. Gene interaction network", "text": "Finally we present results on a subset of the interaction data presented in Jonikas et al. (2009)2. This is an example of a new class of high throughput gene interaction assays, in this case using the yeast S. cerevisiae. A range of \u201cdeletion\u201d strains are created, each of which has a single gene deleted. Some phenotypic response is measured during the growth of each strain, in this case unfolded protein response (UPR), a measure of how badly the cell is doing at correctly folding its membrane proteins. \u201cDouble mutants\u201d with two distinct genes deleted are then screened. Based on the single deletion strains, the expected UPR response for these double mutants can be predicted (see Jonikas et al. (2009) for details) assuming no interaction between the two deleted genes. If the observed UPR response is significantly different from this predicted value then the genes must interact in some way, so we consider this as an edge in the network. We use\n2See http://weissmanlab.ucsf.edu/upremap/\nthe 156 genes with the least missing data. We run 10 repeats with a different 10% of the observed data heldout each time, and perform 500 MCMC iterations for ILA and 1000 for the IRM and LFRM. Again we find the ILA model significantly outperforms LFRM, which in turn outperforms the simple IRM (see Table 2). In this case the infinite version M = \u221e has considerably better predictive performance than with M = 6, suggesting there is considerably more structure in this data so that allowing more features is beneficial. In fact ILA typically finds around M = 30 features with 3 to 5 subclusters per feature. We find significantly more features are associated with particular properties of the genes as defined by Gene Ontology classes3 than would be expected by chance (p < 10\u22123 calculated by permutation testing), for example the three subclusters of one particular feature have very different proportions of ligand binding genes (10/41, 21/27 and 2/20 respectively)."}, {"heading": "7. Conclusion", "text": "Our experimental results on two very different datasets suggest that the network models proposed to date fail to capture the complex nature of real world networks. We have introduced a hierarchical nonparametric Bayesian model, ILA, which is able to naturally represent this complexity, with corresponding gains in empirical performance. In principle ILA could be made even more flexible by allowing multiple membership of subclusters within a feature, corresponding to a hierarchical IBP. We leave investigating whether this is beneficial to future work."}], "references": [{"title": "Mixed membership stochastic block models", "author": ["E.M. Airoldi", "D.M. Blei", "E.P. Xing", "S.E. Fienberg"], "venue": "NIPS.", "citeRegEx": "Airoldi et al\\.,? 2009", "shortCiteRegEx": "Airoldi et al\\.", "year": 2009}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "PNAS.", "citeRegEx": "Girvan and Newman,? 2002", "shortCiteRegEx": "Girvan and Newman", "year": 2002}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "NIPS.", "citeRegEx": "Griffiths and Ghahramani,? 2005", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2005}, {"title": "Latent space approaches to social network analysis. JASA", "author": ["P.D. Hoff", "A.E. Raftery", "M.S. Handcock", "M. S"], "venue": null, "citeRegEx": "Hoff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2001}, {"title": "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model", "author": ["S. Jain", "R. Neal"], "venue": "JCGS.", "citeRegEx": "Jain and Neal,? 2000", "shortCiteRegEx": "Jain and Neal", "year": 2000}, {"title": "Comprehensive characterization of genes required for protein folding in the endoplasmic reticulum", "author": ["M.C. Jonikas", "S.R. Collins", "V. Denic", "E. Oh", "E.M. Quan", "V. Schmid", "J. Weibezahn", "B. Schwappach", "P. Walter", "J.S. Weissman", "M. Schuldiner"], "venue": "Science.", "citeRegEx": "Jonikas et al\\.,? 2009", "shortCiteRegEx": "Jonikas et al\\.", "year": 2009}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J.B. Tenenbaum"], "venue": "21st National Conference on Artificial Intelligence.", "citeRegEx": "Kemp and Tenenbaum,? 2006", "shortCiteRegEx": "Kemp and Tenenbaum", "year": 2006}, {"title": "Modeling social networks with node attributes using the multiplicative attribute graph model", "author": ["M. Kim", "J. Leskovec"], "venue": "UAI.", "citeRegEx": "Kim and Leskovec,? 2011", "shortCiteRegEx": "Kim and Leskovec", "year": 2011}, {"title": "Modeling dyadic data with binary latent factors", "author": ["E. Meeds", "Z. Ghahramani", "R.M. Neal", "S.T. Roweis"], "venue": "NIPS.", "citeRegEx": "Meeds et al\\.,? 2006", "shortCiteRegEx": "Meeds et al\\.", "year": 2006}, {"title": "Nonparametric latent feature models for link prediction", "author": ["K. Miller", "T. Griffiths", "M. Jordan"], "venue": "NIPS.", "citeRegEx": "Miller et al\\.,? 2009", "shortCiteRegEx": "Miller et al\\.", "year": 2009}, {"title": "Infinite multiple membership relational modeling for complex networks", "author": ["M. Morup", "M. Schmidt", "L. Hansen"], "venue": "MLSP.", "citeRegEx": "Morup et al\\.,? 2011", "shortCiteRegEx": "Morup et al\\.", "year": 2011}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "JCGS.", "citeRegEx": "Neal,? 2000", "shortCiteRegEx": "Neal", "year": 2000}, {"title": "Slice Sampling", "author": ["R.M. Neal"], "venue": "The Annals of Statistics.", "citeRegEx": "Neal,? 2003", "shortCiteRegEx": "Neal", "year": 2003}, {"title": "Estimation and prediction for stochastic blockstructures", "author": ["K. Nowicki", "T.A.B. Snijders"], "venue": "JASA.", "citeRegEx": "Nowicki and Snijders,? 2001", "shortCiteRegEx": "Nowicki and Snijders", "year": 2001}, {"title": "Combinatorial stochastic processes", "author": ["J. Pitman"], "venue": "Technical report, Department of Statistics, University of California at Berkeley.", "citeRegEx": "Pitman,? 2002", "shortCiteRegEx": "Pitman", "year": 2002}, {"title": "Learning annotated hierarchies from relational data", "author": ["D.M. Roy", "C. Kemp", "V. Mansinghka", "J.B. Tenenbaum"], "venue": "NIPS.", "citeRegEx": "Roy et al\\.,? 2007", "shortCiteRegEx": "Roy et al\\.", "year": 2007}, {"title": "Infinite hidden relational models", "author": ["Z. Xu", "V. Tresp", "K. Yu", "H.P. Kriegel"], "venue": "UAI.", "citeRegEx": "Xu et al\\.,? 2006", "shortCiteRegEx": "Xu et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "Instead of assuming a fixed number of clusters, the Infinite Relational Model (IRM) and the Infinite Hidden Relational Model (Kemp and Tenenbaum, 2006; Xu et al., 2006) use the Chinese restaurant process (Pitman, 2002) to allow a potentially infinite number of clusters.", "startOffset": 125, "endOffset": 168}, {"referenceID": 16, "context": "Instead of assuming a fixed number of clusters, the Infinite Relational Model (IRM) and the Infinite Hidden Relational Model (Kemp and Tenenbaum, 2006; Xu et al., 2006) use the Chinese restaurant process (Pitman, 2002) to allow a potentially infinite number of clusters.", "startOffset": 125, "endOffset": 168}, {"referenceID": 14, "context": ", 2006) use the Chinese restaurant process (Pitman, 2002) to allow a potentially infinite number of clusters.", "startOffset": 43, "endOffset": 57}, {"referenceID": 0, "context": "The Mixed Membership Stochastic Block Model (Airoldi et al., 2009) (MMSB) increases the expressiveness of the latent class models by allowing mixed membership, associating each object with a distribution over clusters.", "startOffset": 44, "endOffset": 66}, {"referenceID": 11, "context": "Early work in this category includes the stochastic block model (SB) proposed in Nowicki and Snijders (2001). Instead of assuming a fixed number of clusters, the Infinite Relational Model (IRM) and the Infinite Hidden Relational Model (Kemp and Tenenbaum, 2006; Xu et al.", "startOffset": 81, "endOffset": 109}, {"referenceID": 3, "context": "In Hoff et al. (2001) the link probability between two objects is determined by the similarity of their real-valued fea-", "startOffset": 3, "endOffset": 22}, {"referenceID": 2, "context": "Their model, the Latent Feature Infinite Relational Model (LFRM), assumes that the number of clusters is not known a priori and uses the Indian Buffet Process (IBP) (Griffiths and Ghahramani, 2005) to determine the number of latent clusters.", "startOffset": 165, "endOffset": 197}, {"referenceID": 8, "context": "Miller et al. (2009) uses a vector of binary features which can be interpreted as allowing objects to belong to multiple clusters at the same time.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "The IRM model of Kemp and Tenenbaum (2006) and the LFRM of Miller et al.", "startOffset": 17, "endOffset": 43}, {"referenceID": 6, "context": "The IRM model of Kemp and Tenenbaum (2006) and the LFRM of Miller et al. (2009) both use nonparametric Bayesian approaches to account for potentially infinite number of clusters in the data.", "startOffset": 17, "endOffset": 80}, {"referenceID": 7, "context": "The ILA model can also be seen as a extension of the Multiplicative Attribute Graph (MAG) model proposed in Kim and Leskovec (2011), where the link probability is", "startOffset": 108, "endOffset": 132}, {"referenceID": 1, "context": "There are several models that have been proposed for discovering hierarchical structure in relational data (Girvan and Newman, 2002; Roy et al., 2007).", "startOffset": 107, "endOffset": 150}, {"referenceID": 15, "context": "There are several models that have been proposed for discovering hierarchical structure in relational data (Girvan and Newman, 2002; Roy et al., 2007).", "startOffset": 107, "endOffset": 150}, {"referenceID": 11, "context": "To overcome this problem, we use the auxiliary variable approach proposed in Neal (2000) (Algorithm 8), both to facilitate the integration required in Equation 6, and to decide which subcluster to assign the i object to in the m feature if zim is turned on.", "startOffset": 77, "endOffset": 89}, {"referenceID": 11, "context": "We may choose to resample each C in succession as a second step, again using Algorithm 8 of Neal (2000). In practice we found this unnecessary since C is sampled in the process of sampling Z.", "startOffset": 92, "endOffset": 104}, {"referenceID": 12, "context": "To overcome this problem we used both Metropolis Hastings and slice sampling (Neal, 2003) but found the later resulted in faster mixing.", "startOffset": 77, "endOffset": 89}, {"referenceID": 5, "context": "Our implementation of the IRM model of Kemp and Tenenbaum (2006) uses standard single site Gibbs sampling along with the restricted Gibbs sampling split merge method of Jain and Neal (2000).", "startOffset": 39, "endOffset": 65}, {"referenceID": 4, "context": "Our implementation of the IRM model of Kemp and Tenenbaum (2006) uses standard single site Gibbs sampling along with the restricted Gibbs sampling split merge method of Jain and Neal (2000). In the IRM we are able to integrate out the parameters \u03b7 analytically due to conjugacy, so we need only sample the cluster assignments and the CRP concentration parameter, for which we use slice sampling.", "startOffset": 169, "endOffset": 190}, {"referenceID": 9, "context": "For the LFRM of Miller et al. (2009), we Gibbs sample the IBP matrix Z and slice sample each element of the weight matrix W sequentially, followed by the IBP concentration parameter.", "startOffset": 16, "endOffset": 37}, {"referenceID": 8, "context": "Split-merge sampling in the IBP has been previously described in Meeds et al. (2006). However, we found that a sequential initialization of the sampler improved the performance, guiding the sampler closer to neighborhoods of higher probability.", "startOffset": 65, "endOffset": 85}, {"referenceID": 10, "context": "In Morup et al. (2011), it is observed that if a noisyor likelihood model is used in the LFRM rather than the logistic Gaussian model, then the likelihood can be calculated in O(K2L) as for the IRM.", "startOffset": 3, "endOffset": 23}, {"referenceID": 9, "context": "Following Miller et al. (2009) we use only the 234 most connected authors.", "startOffset": 10, "endOffset": 31}, {"referenceID": 9, "context": "We confirm the finding in Miller et al. (2009) that LFRM outperforms the IRM on this dataset.", "startOffset": 26, "endOffset": 47}, {"referenceID": 5, "context": "Finally we present results on a subset of the interaction data presented in Jonikas et al. (2009). This is an example of a new class of high throughput gene interaction assays, in this case using the yeast S.", "startOffset": 76, "endOffset": 98}, {"referenceID": 5, "context": "Finally we present results on a subset of the interaction data presented in Jonikas et al. (2009). This is an example of a new class of high throughput gene interaction assays, in this case using the yeast S. cerevisiae. A range of \u201cdeletion\u201d strains are created, each of which has a single gene deleted. Some phenotypic response is measured during the growth of each strain, in this case unfolded protein response (UPR), a measure of how badly the cell is doing at correctly folding its membrane proteins. \u201cDouble mutants\u201d with two distinct genes deleted are then screened. Based on the single deletion strains, the expected UPR response for these double mutants can be predicted (see Jonikas et al. (2009) for details) assuming no interaction between the two deleted genes.", "startOffset": 76, "endOffset": 708}], "year": 2012, "abstractText": "Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a \u201cflat\u201d clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.", "creator": "LaTeX with hyperref package"}}}