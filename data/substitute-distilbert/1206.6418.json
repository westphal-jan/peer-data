{"id": "1206.6418", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Learning Invariant Representations with Local Transformations", "abstract": "learning invariant model is an important problem in machine image and pattern recognition. in this paper, scholars present a novel framework of transformation - invariant integrated learning involve crossing linear transformations into the feature learning function. for example, with present sophisticated transformation - invariant restricted boltzmann machine that compactly folds data by its weights and their transformations,, achieves invariance of the component representation via probabilistic max pooling. taking addition, we show that our transformation - invariant feature learning framework wil also be extended to other unsupervised learning activities, such as autoencoders or sparse coding. we evaluate our method on several image classification benchmark datasets, such as mnist variations, cifar - 10, and stl - 10, and show competitive user error filtering performance when compared to the state - of - the - art. furthermore, our method achieves state - of - the - art performance on phone classification tasks with the timit dataset, which demonstrates consistent applicability using our robust algorithms to other domains.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (826kb)", "http://arxiv.org/abs/1206.6418v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["kihyuk sohn", "honglak lee"], "accepted": true, "id": "1206.6418"}, "pdf": {"name": "1206.6418.pdf", "metadata": {"source": "META", "title": "Learning Invariant Representations with Local Transformations", "authors": ["Kihyuk Sohn", "Honglak Lee"], "emails": ["kihyuks@umich.edu", "honglak@eecs.umich.edu"], "sections": [{"heading": "1. Introduction", "text": "In recent years, unsupervised feature learning algorithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006). In particular, it is an important problem to learn invariant representations that are robust to variability in high-dimensional data (e.g., images, speech, etc.) since they will en-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nable machine learning systems to achieve good generalization performance while using a small number of labeled training examples. In this context, several feature learning algorithms have been proposed to learn invariant representations for specific transformations by using customized approaches. For example, convolutional feature learning methods (Lee et al., 2011; Kavukcuoglu et al., 2010; Zeiler et al., 2010) can achieve shift-invariance by exploiting convolution operators. As another example, the denoising autoencoder (Vincent et al., 2008) can learn features that are robust to the input noise by trying to reconstruct the original data from the hidden representation of the perturbed data. However, learning invariant representations with respect to general types of transformations is still a challenging problem.\nIn this paper, we present a novel framework of transformation-invariant feature learning. We focus on local transformations (e.g., small amounts of translation, rotation, and scaling in images), which can be approximated as linear transformations, and incorporate linear transformation operators into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine, which is a generative model that represents input data as a combination of transformed weights. In this case, a transformation-invariant feature representation is obtained via probabilistic max pooling of the hidden units over the set of transformations. In addition, we show extensions of our transformationinvariant feature learning framework to other unsupervised feature learning algorithms, such as autoencoders or sparse coding.\nIn our experiments, we evaluate our method on the variations of the MNIST dataset and show that our algorithm can significantly outperform the baseline restricted Boltzmann machine when underlying transformations in the data are well-matched to those considered in the model. Furthermore, our method can learn features that are much more robust to the wide range of local transformations, which results in highly\ncompetitive performance in visual recognition tasks on CIFAR-10 (Krizhevsky, 2009) and STL-10 (Coates et al., 2011) datasets. In addition, our method also achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.\nThe rest of the paper is organized as follows. We provide the preliminaries in Section 2, and in Section 3, we introduce our proposed transformation-invariant feature learning algorithms. In Section 4, we review the previous work on invariant feature learning. Then, in Section 5, we report the experimental results on several datasets. Section 6 concludes the paper."}, {"heading": "2. Preliminaries", "text": "In this paper, we present a general framework for learning locally-invariant features using transformations. For presentation, we will use the restricted Boltzmann machine (RBM) as the main example.1 We first describe the RBM below, followed by its novel extension (Section 3).\nThe restricted Boltzmann machine is a bipartite undirected graphical model that is composed of visible and hidden layers. Assuming binary-valued visible and hidden units, the energy function and the joint probability distribution are given as follows:2\nE(v,h) = \u2212vTWh\u2212 bTh\u2212 cTv, (1)\nP (v,h) = 1\nZ exp (\u2212E(v,h)) (2)\nwhere v \u2208 {0, 1}D are binary visible units, h \u2208 {0, 1}K are binary hidden units, and W \u2208 RD\u00d7K , b \u2208 RK , and c \u2208 RD are weights, hidden biases, and visible biases, respectively. Z is a normalization factor that depends on the parameters {W,b, c}. Since RBMs have no intra-layer connectivity, exact inference is tractable and block Gibbs sampling can be done efficiently using the following conditional probabilities:\np(hj = 1|v) = sigmoid( \u2211 i viWij + bj) (3)\np(vi = 1|h) = sigmoid( \u2211 j hjWij + ci) (4)\nwhere sigmoid(x) = 11+exp (\u2212x) . We train the RBM parameters by minimizing the negative log-likelihood\n1We will describe extensions to other feature learning algorithms in Section 3.4.\n2Due to space constraints, we present only the case of binary-valued input variables; however, the RBM with real-valued input variables can be formulated straightforwardly (Hinton & Salakhutdinov, 2006; Lee et al., 2008).\nvia stochastic gradient descent. Although computing the exact gradient is intractable, we can approximate it using contrastive divergence (Hinton, 2002)."}, {"heading": "3. Learning Transformation-Invariant Feature Representations", "text": ""}, {"heading": "3.1. Transformation-invariant RBM", "text": "In this section, we formulate a novel feature learning framework that can learn invariance to a set of linear transformations based on the RBM. We begin the section with describing the transformation operator.\nThe transformation operator is defined as a mapping T : RD1 \u2192 RD2 that maps D1-dimensional input vectors into D2-dimensional output vectors (D1 \u2265 D2). In our case, we assume a linear transformation matrix T \u2208 RD2\u00d7D1 , i.e., each coordinate of the output vector is represented as a linear combination of the input coordinates.\nWith this notation, we formulate the transformationinvariant restricted Boltzmann machine (TIRBM) that can learn invariance to a set of transformations. Specifically, for a given set of transformation matrices Ts (s = 1, \u00b7 \u00b7 \u00b7 , S), the energy function of TIRBM is defined as follows:\nE(v,H) = (5)\n\u2212 K\u2211 j=1 S\u2211 s=1 (Tsv) Twjhj,s \u2212 K\u2211 j=1 S\u2211 s=1 bj,shj,s \u2212 cTv\ns.t. S\u2211 s=1 hj,s \u2264 1, hj,s \u2208 {0, 1}, j = 1, \u00b7 \u00b7 \u00b7 ,K, (6)\nwhere v are D1-dimensional visible units, and wj are D2-dimensional (filter) weights corresponding to the j-th hidden unit. The hidden units are represented as a matrix H \u2208 {0, 1}K\u00d7S with hj,s as its (j, s)-th entry. In addition, we denote zj = \u2211S s=1 hj,s, zj \u2208 {0, 1} as a pooled hidden unit over the transformations.\nIn Equation (6), we impose a softmax constraint on hidden units so that at most one unit is activated at each row of H. This probabilistic max pooling3 allows us to obtain a feature representation invariant to linear transformations. More precisely, suppose that the input v1 matches the filter wj . Given another input v2 that is a transformed version of v1, the TIRBM will try to find a transformation matrix Tsj so that the v2 matches the transformed filter T T sjwj , i.e., vT1 wj \u2248 vT2 TTsjwj . 4 Therefore, v1 and v2 will both\n3A similar technique is used in convolutional deep belief networks (Lee et al., 2011), in which spatial probabilistic max pooling is applied over a small spatial region.\n4Note that the transpose TTs of a transformation ma-\nactivate zj after probabilistic max pooling. Figure 1 illustrates this idea.\nCompared to the regular RBM, the TIRBM can learn more diverse patterns, while keeping the number of parameters small. Specifically, multiplying transformation matrix (e.g., TTs wj) can be viewed as increasing the number of filters by the factor of S, but without significantly increasing the number of parameters due to parameter sharing. In addition, by pooling over local transformations, the filters can learn invariant representations (i.e., zj \u2019s) to these transformations.\nThe conditional probabilities are computed as follows:\np(hj,s = 1|v) = exp (wTj Tsv + bj,s) 1 + \u2211 s\u2032 exp (w T j Ts\u2032v + bj,s\u2032)\n(7)\np(vi = 1|h) = sigmoid( \u2211 j,s (TTs wj)ihj,s + ci) (8)\nSimilar to RBM training, we use stochastic gradient descent to train TIRBM. The gradient of the loglikelihood is approximated via contrastive divergence by taking the gradient of the energy function (Equation (5)) with respect to the model parameters."}, {"heading": "3.2. Sparse TIRBM", "text": "The sparseness of the feature representation is often a desirable property. By following Lee et al.\u2019s (2008) approach, we can extend our model to sparse TIRBM by adding the following regularizer for a given set of data {v(1), \u00b7 \u00b7 \u00b7 ,v(N)} to the negative log-likelihood:\nLsp = K\u2211 j=1 D\n( p, 1\nN N\u2211 n=1 E[z(n)j |v (n)]\n) (9)\nwhere D is a distance function; p is the target sparsity.\ntrix Ts also induces a linear transformation. In the paper, we will occasionally abuse the term \u201ctransformation\u201d to denote these two cases, as long as the context is clear.\nThe expectation of pooled activation is written as E[zj |v] = \u2211 s exp (w T j Tsv + bj,s)\n1 + \u2211 s exp (w T j Tsv + bj,s) . (10)\nNote that we regularize over the pooled hidden units zj rather than individual hidden units hj,s. In our experiments, we used L2 distance for D(\u00b7, \u00b7), but one can also use KL divergence for the sparsity penalty."}, {"heading": "3.3. Generating transformation matrices", "text": "In this section, we discuss how to design the transformation matrix T . For the ease of presentation, we assume 1-d transformations, but it can be extended to 2-d cases (e.g., image transformations) straightforwardly. Further, we assume the case of D1 = D2 = D here; but, we will discuss general cases later.\nAs mentioned previously, T \u2208 RD\u00d7D is a linear transformation matrix from x \u2208 RD to y \u2208 RD; i.e., each coordinate of y is constructed via linear combination of the coordinates in x with weight matrix T as follows:\nyi = D\u2211 j=1 Tijxj ,\u2200i = 1, \u00b7 \u00b7 \u00b7 , D. (11)\nFor example, shifting by s can be defined as\nTij(s) =\n{ 1 if i = j + s,\n0 otherwise.\nFor 2-d image transformations such as rotation and scaling, the contribution of input coordinates to each output coordinate is computed with bilinear interpolation. Since T \u2019s are pre-computed once and usually sparse, Equation (11) can be computed efficiently."}, {"heading": "3.4. Extensions to other methods", "text": "We emphasize that our transformation-invariant feature learning framework is not limited to the energybased probabilistic models, but can be extended to other unsupervised learning methods as well.\nFirst, it can be readily adapted to autoencoders by defining the following softmax encoding and sigmoid decoding functions:\nfj,s(v) = exp (wTj Tsv + bj,s) 1 + \u2211 s\u2032 exp (w T j Ts\u2032v + bj,s\u2032)\n(12)\nv\u0302i = sigmoid( \u2211 j,s (TTs wj)ifj,s + ci) (13)\nFollowing the idea of TIRBM, we can also formulate the transformation-invariant sparse coding as follows:\nmin W,H \u2211 n \u2016 K\u2211 j=1 S\u2211 s=1 TTs wjh (n) j,s \u2212 v (n)\u20162, (14)\ns.t. \u2016H\u20160 \u2264 \u03b3, \u2016H(j, :)\u20160 \u2264 1, \u2016wj\u20162 \u2264 1, (15)\nwhere \u03b3 is a constant. The second constraint in (15) can be understood as an analogy to the softmax constraint in Equation (6) of TIRBMs.\nSimilar to standard sparse coding, we can optimize the parameters by alternately optimizing W and H while fixing the other. Specifically, H can be (approximately) solved using Orthogonal Matching Pursuit, and therefore we refer this algorithm a transformationinvariant Orthogonal Matching Pursuit (TIOMP)."}, {"heading": "4. Related Work", "text": "Researchers have made significant efforts to develop invariant feature representations. For example, the rotation- or scale-invariant descriptors, such as SIFT (Lowe, 1999), have shown a great success in many computer vision applications. However, these image descriptors usually demand a domain-specific knowledge with a significant amount of hand-crafting.\nAs an alternative approach, several unsupervised learning algorithms have been proposed to learn robust feature representations automatically from the sensory data. As an example, the denoising autoencoder (Vincent et al., 2008) can learn robust features by trying to reconstruct the original data from the hidden representations of randomly perturbed data generated from the distortion processes, such as adding noise or multiplying zeros for randomly selected coordinates.\nAmong types of transformations relating to temporally or spatially correlated data, translation has been extensively studied in the context of unsupervised learning. Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning. For example, the convolutional deep belief network (CDBN) (Lee et al., 2011),\nwhich is composed of multiple layers of convolutional restricted Boltzmann machines and probabilistic max pooling, can learn a representation invariant to local translation.\nBesides translation, however, learning invariant features for other types of image transformations have not been extensively studied. In contemporary work of ours, Kivinen and Williams (2011) proposed the transformation equivariant Boltzmann machine, which shares a similar mathematical formulation to our models in that both try to infer the best matching filters by transforming them using linear transformation matrices. However, while their model was motivated from the \u201cglobal equivariance\u201d, the main purpose of our work is to learn locally-invariant features that can be useful in classification tasks. Thus, rather than considering an algebraic group of transformation matrices (e.g., \u201cfull rotations\u201d), we focus on the variety of local transformations that include rotation, translation as well as scale variations. Furthermore, we effectively address the boundary effects that can be highly problematic in scaling and translation operators by forming a non-square T matrix, rather than zeropadding.5 In addition, we present a general framework of transformation-invariant feature learning and show extensions based on the autoencoder and sparse coding. Overall, our argument is strongly supported by the state-of-the-art performance in image and audio classification tasks.\nAs another related work, feature learning methods with topographic maps can also learn invariant representations (Hyva\u0308rinen et al., 2001; Kavukcuoglu et al., 2009). Compared to these methods, our model is more compact and has fewer parameters to train since it factors out the (filter) weights and their transformations. In addition, given the same number of parameters, our model can represent more diverse and variable input patterns than topographic filter maps."}, {"heading": "5. Experiments", "text": "We begin by describing the notation. For images, we assume a receptive field size of r \u00d7 r pixels (for input image patches) and a filter size of w \u00d7 w pixels. We define gs to denote the number of pixels corresponding to the transformation (e.g., translation or scaling). For example, we translate the w\u00d7w filter across the r\u00d7 r receptive field with a stride of gs pixels (Figure 2(a)), or scale down from (r\u2212l\u00b7gs)\u00d7(r\u2212l\u00b7gs) to w\u00d7w (where 0 \u2264 l \u2264 b(r \u2212 w)/gsc) by sharing the same center for\n5We observed that learning with zero-padded squared transformation matrices showed significant boundary effect in its visualization of filters, and this often resulted in significantly worse classification performance.\nthe filter and the receptive field (Figure 2(b)).\nFor classification tasks, we used the posterior probability of the pooled hidden unit (Equation (10)) as a feature. Note that the dimension of the extracted feature vector for each image patch is K, not K \u00d7 S. Thus, we argue that the performance gain of the TIRBM over the regular RBM comes from the better representation (i.e., transformation-invariant features), rather than from the classifier\u2019s use of higher-dimensional features."}, {"heading": "5.1. Handwritten digit recognition with prior transformation information", "text": "First, we verified the performance of our algorithm on the variations of a handwritten digit dataset, assuming that the transformation information is given. From the MNIST variation datasets (Larochelle et al., 2007), we tested on \u201cmnist-rot\u201d (rotated digits, referred to as rot) and \u201cmnist-rot-back-image\u201d (rotated digits with background images, referred to as rot-bgimg). To further evaluate with different types of transformations, we created four additional datasets that contain scale and translation variations with and without random background (referred to as scale, scale-bgrand, trans, and trans-bgrand, respectively).6 Some examples are shown in Figure 3.\nFor these datasets, we trained sparse TIRBMs on the image patches of size 28 \u00d7 28 pixels with dataspecific transformations. For example, we considered 16 equally-spaced rotations (i.e., the step size of \u03c08 ) for the rot and rot-bgimg datasets. Similarly, for the scale and scale-bgrand datasets, we generated scaletransformation matrices with w = 20 and gs = 2, which can map from (28 \u2212 2l) \u00d7 (28 \u2212 2l) pixels to 20 \u00d7 20 pixels with l \u2208 {0, ..., 4}. For the trans and trans-bgrand datasets, we set w = 24 and gs = 2 to have total nine translation matrices that cover the\n6We followed the generation process described in http://www.iro.umontreal.ca/~lisa/twiki/bin/view. cgi/Public/MnistVariations to create the customized scaled and translated digits. For example, we randomly selected the scale-level uniformly from 0.3 to 1 or the number of pixel shifts in horizontal and vertical directions without making any truncation of the foreground pixels. For the datasets with random background, we randomly added the uniform noise in [0, 1] to the background pixels.\n28\u00d7 28 regions using 24\u00d7 24 pixels with a horizontal and vertical stride of 2 pixels. For classification, we trained 1, 000 filters for both sparse RBMs and sparse TIRBMs and used a softmax classifier. We used 10,000 examples for the training set, 2,000 examples for the validation set, and 50,000 examples for the test set.\nAs reported in Table 1, our method (sparse TIRBMs) consistently outperformed the baseline method (sparse RBMs) for all datasets. These results suggest that the TIRBMs can learn better representations for the foreground objects by transforming the filters. It is worth noting that our error rates for the mnist-rot and mnistrot-back-image datasets are also significantly lower than the best published results obtained with stacked denoising autoencoders (Vincent et al., 2010) (9.53% and 43.75%, respectively).\nFor qualitative evaluation, we visualize the learned filters on the mnist-rot dataset trained with the sparse TIRBM (Figure 3(e)) and the sparse RBM (Figure 3(f)), respectively. The filters learned from sparse TIRBMs show much clearer pen-strokes than those learned from sparse RBMs, which partially explains\nthe impressive classification performance."}, {"heading": "5.2. Learning invariant features from natural images", "text": "For handwritten digit recognition in the previous section, we assumed prior information on global transformations on the image (e.g., translation, rotation, and scale variations) for each dataset. This assumption enabled the proposed TIRBMs to achieve significantly better classification performance than the baseline method, since the data-specific transformation information was encoded in the TIRBM.\nHowever, for natural images, it is not reasonable to assume such global transformations due to the complex image structures. In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.g., few pixel translation or coordinate-wise noise) leads to improved performance in classification. From this viewpoint, it makes more sense to learn representations with local receptive fields that are invariant to generic image transformations (e.g., small amounts of translation, rotation, and scaling), which does not require data-specific prior information.\nWe visualize the learned TIRBM filters in Figure 4, where we used the 14\u00d714 natural image patches taken from the van Hateren dataset (van Hateren & van der Schaaf, 1998). The baseline model (sparse RBM) learns many similar vertical edges (Figure 4(a)) that are shifted by a few pixels, whereas our methods can learn diverse patterns, including diagonal and horizontal edges, as shown in Figure 4(b), 4(c), and 4(d).\nThese results suggest that TIRBMs can learn diverse sets of filters, which is reminiscent of the effects of convolutional training (Kavukcuoglu et al., 2010). However, our model is much easier to train than convolutional models, and it can further handle generic transformations beyond translations."}, {"heading": "5.3. Object recognition", "text": "We evaluated on image classification tasks using two datasets. First, we tested on the widely used CIFAR10 dataset (Krizhevsky, 2009), which is composed of 50,000 training and 10,000 testing examples with 10 categories. Rather than learning features from the whole image (32 \u00d7 32 pixels), we trained TIRBMs on local image patches while keeping the RGB channels. As suggested by Coates et al.(2011), we used the fixed filter size w = 6 and determined the receptive field size depending on the types of transformations.7 Then, after unsupervised training with TIRBM, we used the convolutional feature extraction scheme, also following the Coates et al.(2011). Specifically, we computed the TIRBM pooling-unit activations for each local r\u00d7r\n7For example, we used r = 6 for rotations. For both scale variations or translations, we used r = 8 and gs = 2.\npixel patch that was densely extracted with a stride of 1 pixel, and averaged the patch-level activations over each of the 4 quadrants in the image. Eventually, this procedure yielded 4K-dimensional feature vectors for each image, which were fed into an L2-regularized linear SVM. We performed 5-fold cross validation to determine the hyperparameter C.\nFor comparison to the baseline model, we separately evaluated the sparse TIRBMs with a single type of transformation (translation, rotation, or scaling) using K = 1, 600. As shown in Table 2, each single type of transformation in TIRBMs brought a significant performance gain over the baseline sparse RBMs. The classification performance was further improved by combining different types of transformations into a single model.\nIn addition, we also report the classification results obtained using TIOMP-1 (see Section 3.4) for unsupervised training. In this experiment, we used the following two-sided soft thresholding encoding function:\nfj = max s\n{ max(wTj Tsv \u2212 \u03b1, 0) } fj+K = max\ns\n{ max(\u2212wTj Tsv \u2212 \u03b1, 0) } ,\nwhere \u03b1 is a constant threshold that was crossvalidated. As a result, we observed about 1% improvement over the baseline method (OMP-1/T) using 1,600 filters, which supports the argument that our transformation-invariant feature learning framework can be effectively transferred to other unsupervised learning methods. Finally, by increasing the number of filters (K= 4,000), we obtained better results (82.2%) than the previously published results using single-layer models, as well as those using deep networks.\nWe also performed the object classification task on STL-10 dataset (Coates et al., 2011), which is more challenging due to the smaller number of labeled training examples (100 per class for each training fold). Since the original images are 96\u00d796 pixels, we downsampled the images into 32\u00d732 pixels, while keeping the RGB channels. We followed the same unsupervised training and classification pipeline as we did for CIFAR-10. As reported in Table 3, there were consistent improvements in classification accuracy by incorporating the various transformations in learning algorithms. Finally, we achieved 58.7% accuracy using 1,600 filters, which is competitive to the best published single layer result (59.0%)."}, {"heading": "5.4. Phone classification", "text": "To show the broad applicability of our method to other data types, we report the 39-way phone classification\naccuracy on the TIMIT dataset. By following (Ngiam et al., 2011), we generated 39-dimensional MFCC features and used 11 contiguous frames of them as an input patch. For TIRBMs, we applied three temporal translations with the stride of 1 frame.\nFirst, we compared the classification accuracy using the linear SVM to evaluate the performance gain coming from the unsupervised learning algorithms, by following the experimental setup in (Ngiam et al., 2011).8 As reported in Table 4, the TIRBM showed an improvement over the sparse RBM, as well as the sparse coding and sparse filtering.\nIn the next setting, we used the RBF-kernel SVM (Chang & Lin, 2011) on the extracted features that are concatenated with MFCC features. We used default RBF kernel width for all experiments and performed cross-validation on the C values. As shown in Table 5, combining MFCC with TIRBM features was the most effective and resulted in 1% improvement in classification accuracy over the baseline MFCC features. By increasing the number of TIRBM features to 512, we were able to beat the best published results on the TIMIT phone classification tasks using hierarchical LM-GMM classifier (Chang & Glass, 2007)."}, {"heading": "6. Conclusion and Future Work", "text": "In this work, we proposed novel feature learning algorithms that can achieve invariance to the set of predefined transformations. Our method can handle general transformations (e.g., translation, rotation, and\n8We used K = 256 with a two-sided encoding function by using the positive and negative weight matrices [W,\u2212W], as suggested by Ngiam et al. (2011).\nscaling), and we experimentally showed that learning invariant features for such transformations leads to strong classification performance. In future work, we plan to work on learning transformations from the data and combine it with our algorithm. By automatically learning transformation matrices from the data, we will be able to learn more robust features, which will potentially lead to significantly better feature representations."}, {"heading": "Acknowledgments", "text": "This work was supported in part by a Google Faculty Research Award."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Hierarchical large-margin gaussian mixture models for phonetic classication", "author": ["H. Chang", "J.R. Glass"], "venue": "In ASRU,", "citeRegEx": "Chang and Glass,? \\Q2007\\E", "shortCiteRegEx": "Chang and Glass", "year": 2007}, {"title": "High-performance neural networks for visual object classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Technical report,", "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Coates and Ng,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Coates and Ng,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Reducing the dimensionality of data with neural", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "networks. Science,", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P.O. Hoyer", "M.O. Inki"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2001}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Learning convolutional feature hierachies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "In NIPS,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Transformation equivariant boltzmann machines", "author": ["J. Kivinen", "C. Williams"], "venue": "In ICANN,", "citeRegEx": "Kivinen and Williams,? \\Q2011\\E", "shortCiteRegEx": "Kivinen and Williams", "year": 2011}, {"title": "Learning multiple layers of features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Sparse deep belief network model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Unsupervised learning of hierarchical representations with convolutional deep belief networks", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Communications of the ACM,", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Object recognition from local scale-invariant features", "author": ["D. Lowe"], "venue": "In ICCV,", "citeRegEx": "Lowe,? \\Q1999\\E", "shortCiteRegEx": "Lowe", "year": 1999}, {"title": "Efficient learning of sparse representations with an energybased model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "In NIPS,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "Efficient learning of sparse, distributed, convolutional feature representations for object recognition", "author": ["K. Sohn", "D.Y. Jung", "H. Lee", "A. Hero III"], "venue": "In ICCV,", "citeRegEx": "Sohn et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2011}, {"title": "Independent component filters of natural images compared with simple cells in primary visual cortex", "author": ["J.H. van Hateren", "A. van der Schaaf"], "venue": "Proceedings of the Royal Society B,", "citeRegEx": "Hateren and Schaaf,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Schaaf", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In ICML,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "In CVPR,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "In recent years, unsupervised feature learning algorithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006).", "startOffset": 129, "endOffset": 193}, {"referenceID": 0, "context": "In recent years, unsupervised feature learning algorithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006).", "startOffset": 129, "endOffset": 193}, {"referenceID": 21, "context": "In recent years, unsupervised feature learning algorithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006).", "startOffset": 129, "endOffset": 193}, {"referenceID": 19, "context": "For example, convolutional feature learning methods (Lee et al., 2011; Kavukcuoglu et al., 2010; Zeiler et al., 2010) can achieve shift-invariance by exploiting convolution operators.", "startOffset": 52, "endOffset": 117}, {"referenceID": 12, "context": "For example, convolutional feature learning methods (Lee et al., 2011; Kavukcuoglu et al., 2010; Zeiler et al., 2010) can achieve shift-invariance by exploiting convolution operators.", "startOffset": 52, "endOffset": 117}, {"referenceID": 24, "context": "As another example, the denoising autoencoder (Vincent et al., 2008) can learn features that are robust to the input noise by trying to reconstruct the original data from the hidden representation of the perturbed data.", "startOffset": 46, "endOffset": 68}, {"referenceID": 14, "context": "competitive performance in visual recognition tasks on CIFAR-10 (Krizhevsky, 2009) and STL-10 (Coates et al.", "startOffset": 64, "endOffset": 82}, {"referenceID": 6, "context": "competitive performance in visual recognition tasks on CIFAR-10 (Krizhevsky, 2009) and STL-10 (Coates et al., 2011) datasets.", "startOffset": 94, "endOffset": 115}, {"referenceID": 17, "context": "Due to space constraints, we present only the case of binary-valued input variables; however, the RBM with real-valued input variables can be formulated straightforwardly (Hinton & Salakhutdinov, 2006; Lee et al., 2008).", "startOffset": 171, "endOffset": 219}, {"referenceID": 7, "context": "Although computing the exact gradient is intractable, we can approximate it using contrastive divergence (Hinton, 2002).", "startOffset": 105, "endOffset": 119}, {"referenceID": 19, "context": "4 Therefore, v1 and v2 will both A similar technique is used in convolutional deep belief networks (Lee et al., 2011), in which spatial probabilistic max pooling is applied over a small spatial region.", "startOffset": 99, "endOffset": 117}, {"referenceID": 17, "context": "By following Lee et al.\u2019s (2008) approach, we can extend our model to sparse TIRBM by adding the following regularizer for a given set of data {v, \u00b7 \u00b7 \u00b7 ,v} to the negative log-likelihood:", "startOffset": 13, "endOffset": 33}, {"referenceID": 20, "context": "For example, the rotation- or scale-invariant descriptors, such as SIFT (Lowe, 1999), have shown a great success in many computer vision applications.", "startOffset": 72, "endOffset": 84}, {"referenceID": 24, "context": "As an example, the denoising autoencoder (Vincent et al., 2008) can learn robust features by trying to reconstruct the original data from the hidden representations of randomly perturbed data generated from the distortion processes, such as adding noise or multiplying zeros for randomly selected coordinates.", "startOffset": 41, "endOffset": 63}, {"referenceID": 16, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 12, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 19, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 22, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 19, "context": "For example, the convolutional deep belief network (CDBN) (Lee et al., 2011), which is composed of multiple layers of convolutional restricted Boltzmann machines and probabilistic max pooling, can learn a representation invariant to local translation.", "startOffset": 58, "endOffset": 76}, {"referenceID": 13, "context": "In contemporary work of ours, Kivinen and Williams (2011) proposed the transformation equivariant Boltzmann machine, which shares a similar mathematical formulation to our models in that both try to infer the best matching filters by transforming them using linear transformation matrices.", "startOffset": 30, "endOffset": 58}, {"referenceID": 10, "context": "As another related work, feature learning methods with topographic maps can also learn invariant representations (Hyv\u00e4rinen et al., 2001; Kavukcuoglu et al., 2009).", "startOffset": 113, "endOffset": 163}, {"referenceID": 11, "context": "As another related work, feature learning methods with topographic maps can also learn invariant representations (Hyv\u00e4rinen et al., 2001; Kavukcuoglu et al., 2009).", "startOffset": 113, "endOffset": 163}, {"referenceID": 15, "context": "From the MNIST variation datasets (Larochelle et al., 2007), we tested on \u201cmnist-rot\u201d (rotated digits, referred to as rot) and \u201cmnist-rot-back-image\u201d (rotated digits with background images, referred to as rot-bgimg).", "startOffset": 34, "endOffset": 59}, {"referenceID": 25, "context": "It is worth noting that our error rates for the mnist-rot and mnistrot-back-image datasets are also significantly lower than the best published results obtained with stacked denoising autoencoders (Vincent et al., 2010) (9.", "startOffset": 197, "endOffset": 219}, {"referenceID": 26, "context": "In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.", "startOffset": 27, "endOffset": 84}, {"referenceID": 19, "context": "In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.", "startOffset": 27, "endOffset": 84}, {"referenceID": 24, "context": "In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.", "startOffset": 27, "endOffset": 84}, {"referenceID": 6, "context": "The numbers with \u2020 and \u2021 are from (Coates et al., 2011) and (Coates & Ng, 2011a), respectively.", "startOffset": 34, "endOffset": 55}, {"referenceID": 3, "context": "9% deep NN (Ciresan et al., 2011) 80.", "startOffset": 11, "endOffset": 33}, {"referenceID": 12, "context": "These results suggest that TIRBMs can learn diverse sets of filters, which is reminiscent of the effects of convolutional training (Kavukcuoglu et al., 2010).", "startOffset": 131, "endOffset": 157}, {"referenceID": 14, "context": "First, we tested on the widely used CIFAR10 dataset (Krizhevsky, 2009), which is composed of 50,000 training and 10,000 testing examples with 10 categories.", "startOffset": 52, "endOffset": 70}, {"referenceID": 6, "context": "As suggested by Coates et al.(2011), we used the fixed filter size w = 6 and determined the receptive field size depending on the types of transformations.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "As suggested by Coates et al.(2011), we used the fixed filter size w = 6 and determined the receptive field size depending on the types of transformations. Then, after unsupervised training with TIRBM, we used the convolutional feature extraction scheme, also following the Coates et al.(2011). Specifically, we computed the TIRBM pooling-unit activations for each local r\u00d7r For example, we used r = 6 for rotations.", "startOffset": 16, "endOffset": 294}, {"referenceID": 6, "context": "We also performed the object classification task on STL-10 dataset (Coates et al., 2011), which is more challenging due to the smaller number of labeled training examples (100 per class for each training fold).", "startOffset": 67, "endOffset": 88}, {"referenceID": 18, "context": "5% MFCC + CDBN (Lee et al., 2009) 80.", "startOffset": 15, "endOffset": 33}], "year": 2012, "abstractText": "Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformationinvariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-theart performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.", "creator": "LaTeX with hyperref package"}}}