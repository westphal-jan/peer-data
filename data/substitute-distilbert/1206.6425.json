{"id": "1206.6425", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Sparse stochastic inference for latent Dirichlet allocation", "abstract": "finally present a hybrid algorithm for bayesian topic models that combines data efficiency of sparse gibbs sampling improving the scalability of online stochastic inference. we used our algorithm ; analyze a corpus of 1. 2 million books ( 33 billion samples ) with thousands of topics. our approach reduces the bias of variational inference and generalizes similarly many bayesian time - variable models.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (686kb)", "http://arxiv.org/abs/1206.6425v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["david m mimno", "matthew d hoffman", "david m blei"], "accepted": true, "id": "1206.6425"}, "pdf": {"name": "1206.6425.pdf", "metadata": {"source": "META", "title": "Sparse stochastic inference for latent Dirichlet allocation", "authors": ["David Mimno", "Matthew D. Hoffman", "David M. Blei"], "emails": ["mimno@cs.princeton.edu", "mdhoffma@cs.princeton.edu", "blei@cs.princeton.edu"], "sections": [{"heading": "1. Introduction", "text": "Topic models are hierarchical Bayesian models of document collections (Blei et al., 2003). They can uncover the main themes that pervade a corpus and then use those themes to help organize, search, and explore the documents. In topic modeling, a \u201ctopic\u201d is a distribution over a fixed vocabulary and each document exhibits the topics with different proportions. Both the topics and the topic proportions of documents are hidden variables. Inferring the conditional distribution of these variables given an observed set of documents is the central computational problem.\nIn this paper, we develop a posterior inference method for topic modeling that can find large numbers of topics in massive collections of documents. We demonstrate our approach by analyzing a collection of 1.2 million out-of-copyright books, comprising 33 billion observed words. Using our algorithm, we fit a topic model to this corpus with thousands of topics. We illustrate the most frequent words from several of these topics in Table 1.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nOur algorithm builds on variational inference (Jordan et al., 1999). In variational inference, we define a parameterized family of distributions over the hidden structure\u2014in this case topics and document-topic proportions\u2014and then optimize the parameters to find a member of the family that is close to the posterior. Traditional variational inference for topic modeling uses coordinate ascent. The algorithm alternates between estimating document-topic proportions under the current settings of the topics and re-estimating the topics based on the estimated document proportions. This requires multiple passes through an entire collection, which is not practical when working with very large corpora.\nRecently, Hoffman et al. (2010) introduced Online LDA, a stochastic gradient optimization algorithm for topic modeling. The algorithm repeatedly subsamples a small set of documents from the collection and then updates the topics from an analysis of the subsample. This method uses less memory than the standard approach because we do not need to store topic proportions for the full corpus. It also converges faster because we update topics more frequently. However, while it handles large corpora it does not scale to large\nnumbers of topics.\nOur algorithm builds on this method by using sampling to introduce a second source of stochasticity into the gradient. This approach lets us take advantage of sparse computation, scaling sublinearly with the number of topics. Using this algorithm, we can fit topic models to large collections with many topics."}, {"heading": "2. Hybrid stochastic-MCMC inference", "text": "We model each of the D documents in a corpus as a mixture of K topics. This topic model can be divided into corpus-level global variables and document-level local variables. The global variables are K topic-word distributions \u03b21, ...,\u03b2K over the V -dimensional vocabulary, each drawn from a Dirichlet prior with parameter \u03b7. For a document d of length Nd, the local variables are (a) a distribution over topics \u03b8d drawn from a Dirichlet prior with parameter \u03b1 and (b) Nd tokentopic indicator variables zd1, ..., zdNd drawn from \u03b8d.\nOur goal is to estimate the posterior distribution of the hidden variables given an observed corpus. We will use variational inference. Unlike standard mean-field variational inference, but similar to Griffiths & Steyvers (2004) and Teh et al. (2006), we will marginalize out the topic proportions \u03b8d. Thus we need to approximate the posterior over the topic assignments zd and the topics \u03b21:K .\nWe will use a variational distribution of the form q(z1, ...,zD,\u03b21, ...,\u03b2K) = \u220f d q(zd) \u220f k q(\u03b2k). (1)\nThis factorization differs from the usual mean-field family for topic models. Rather than defining a distribution that factorizes over individual tokens, we treat each document\u2019s sequence of topic indicator variables zd as a unit. As a result q(zd) will be a single distribution over the KNd possible topic configurations, rather than a product of Nd distributions, each over K possible values.\nWe now derive an algorithm that uses Gibbs sampling to estimate variational expectations of the local variables and a stochastic natural gradient step to update the variational distribution of global variables. A lower bound on the marginal log probability of the observed words given the hyperparameters is\nlog p(w|\u03b1, \u03b7) \u2265 \u2211 d Eq log\n[ p(zd|\u03b1)\n\u220f i \u03b2zdiwdi\n] (2)\n+ \u2211 k Eq log p(\u03b2k|\u03b7) +H(q),\nwhere H(q) denotes the entropy of q.\nFollowing Bishop (2006), the optimal variational distribution over topic configurations for a document, holding all other variational distributions fixed, is\nq?(zd) \u221d exp{Eq(\u00aczd) [log p(zd|\u03b1)p(wd|zd,\u03b2)]} (3)\n= \u0393(K\u03b1)\n\u0393(K\u03b1+Nd) \u220f k \u0393(\u03b1+ \u2211 i Izdi=k) \u0393(\u03b1) (4)\n\u00d7 \u220f i expEq[log \u03b2zdiwdi ]\nwhere Ia=b is 1 if a = b and 0 otherwise, and \u00aczd denotes the set of all unobserved variables besides zd. We can compute Eq. 4 for a specific topic configuration zd, but we cannot tractably normalize it to get the distribution q?(zd) over all K Nd configurations.\nThe optimal variational distribution over topic-word distributions, holding the other distributions fixed, is the kernel of a Dirichlet distribution with parameters\n\u03bbkw = \u03b7 + \u2211 d \u2211 i Eq[Izdi=kIwdi=w]. (5)\nThis expression includes the expectation under q of the number of tokens of type w assigned to topic k. Computing this expectation would require evaluating the intractable distribution q?(zd)."}, {"heading": "2.1. Online stochastic inference", "text": "We optimize the variational topic-word parameters \u03bbkw using stochastic gradient ascent. Stochastic gradient ascent iteratively updates parameters with noisy estimates of the gradient. We obtain these noisy estimates by subsampling the data (Sato, 2001; Hoffman et al., 2010).\nWe first recast the variational objective in Eq. 2 as a summation over per-document terms `d, so that the full gradient with respect to \u03bbk is the sum \u2211 d \u2202 \u2202\u03bbk\n`d. We can then generate a noisy approximation to this full gradient by sampling a minibatch of documents B and then scaling the sum of the document-specific gradients to match the total size of the corpus,\u2211\nd\n\u2202\n\u2202\u03bbk `d = E\n[ D\n|B| \u2211 d\u2208B \u2202 \u2202\u03bbk `d\n] . (6)\n(The expectation is with respect to the random sample B.) Pushing the per-topic terms in Eq. 2 inside the summation over documents and removing terms not involving \u03bbkw we obtain\n`d = \u2211 w ( Eq[Ndkw] + 1 D (\u03b7 \u2212 \u03bbkw) ) Eq[log \u03b2kw] (7)\n+ 1\nD\n( log \u0393( \u2211 w \u03bbkw)\u2212 \u2211 w log \u0393(\u03bbkw) )\nAlgorithm 1 Algorithm for hybrid stochastic variational-Gibbs inference.\nfor t \u2208 1, ...,\u221e do \u03c1t \u2190\n( 1\nt0+t )\u03ba sample minibatch B for d \u2208 B do\ninitialize z0d discard B burn-in sweeps for sample s \u2208 1, ..., S do\nfor token i \u2208 1, ..., Nd do sample zsdi \u221d (\u03b1+Ndk)eEq [log \u03b2kw]\nend for end for\nend for \u03bbtkw \u2190 (1\u2212 \u03c1t)\u03bb t\u22121 kw + \u03c1t ( \u03b7 + D|B|N\u0302kw ) end for\nwhere Eq[Ndkw] = \u2211 i Eq[Izdi=kIwdi=w]. The gradient of Eq. 7 with respect to the parameters \u03bbk1, ..., \u03bbkV can be factored into the product of a matrix and a vector. The matrix, which contains derivatives of the digamma function, is the Fisher information matrix for the topic parameters. Element w of the vector is\nEq[Ndkw] + 1\nD (\u03b7 \u2212 \u03bbkw). (8)\nPremultiplying the gradient of an objective function by the inverse Fisher information matrix of the distribution being optimized (in our case the variational distribution q) results in the natural gradient (Sato, 2001). Since our gradient is the product of the Fisher information matrix and a vector, the natural gradient is therefore simply Eq. 8 (Hoffman et al., 2010). Compared to the standard Euclidean gradient, the natural gradient offers both faster convergence (because it takes into account the information geometry of the variational distribution) and cheaper computation (because the vector in Eq. 8 is a simple linear function)."}, {"heading": "2.2. MCMC within stochastic inference", "text": "We cannot evaluate the expectation in Eq. 8 because we would have to consider a combinatorial number of topic configurations zd. To use stochastic gradient ascent, however, we only need an approximation to this expectation. We use Markov chain Monte Carlo to sample topic configurations from q?(zd). We then use the empirical average of these samples to estimate the expectations needed for Eq. 8.\nGibbs sampling for a document starts with a random initialization of the topic indicator variables zd. We then iteratively resample the topic indicator at each position from the conditional distribution over that\nposition given the remaining topic indicator variables: q?(zdi = k|z\\i) \u221d (\u03b1+ \u2211 j 6=i Izj=k) exp{Eq[log \u03b2kwdi ]},\n(9)\nwhere the expectation of the log probability of word w given a topic k is \u03a8(\u03bbkw) \u2212 \u03a8( \u2211 w\u2032 \u03bbkw\u2032). After B burn-in sweeps, we begin saving sampled topic configurations. Once we have saved S samples {z}1,...,S , we can define approximate sufficient statistics\nEq[Ndkw] \u2248 N\u0302kw = 1\nS \u2211 s \u2211 d\u2208B \u2211 i Izsdi=kIwdi=w. (10)\nUsing MCMC estimates adds noise to our gradient, but allows us to use a collapsed objective function that does not represent document-topic proportions \u03b8d. In addition, an average over a finite set of samples provides a sparse estimate of the gradient: for many words and topics, our estimate of Eq[Ndkw] will be zero."}, {"heading": "2.3. Algorithm", "text": "We have defined a natural gradient and a method for approximating the sufficient statistics of that gradient. For a sequence of learning rates \u03c1t = (t0 + t)\n\u2212\u03ba, the following update will lead to a stationary point:\n\u03bbtkw \u2190 \u03bbt\u22121kw + \u03c1t D |B| \u2211 d\u2208B ( N\u0302dkw + 1 D (\u03b7 \u2212 \u03bbkw) )\n= (1\u2212 \u03c1t)\u03bbt\u22121kw + \u03c1t\n( \u03b7 + D\n|B| \u2211 d\u2208B N\u0302dkw\n) . (11)\nThis update results in Algorithm 1. Two implementation details that result in sparse computations can be found in Appendix A. This online algorithm has the important advantage over Online LDA of preserving sparsity in the topic-word parameters, so that \u03bbkw = \u03b7 for most values of k and w. Sparsity increases the efficiency of updates to \u03bbk and of Gibbs sampling for zd. Previous variational methods lead to dense updates to KV topic parameters, making them expensive to apply to large vocabularies and large numbers of topics. Our method, in contrast, is able to exploit the sparsity exhibited by samples from the variational distribution q?, resulting in much more efficient updates."}, {"heading": "3. Related Work", "text": "This paper combines two sources of zero-mean noise in constructing an approximate gradient for a variational inference algorithm: subsampling of data, and Monte Carlo inference. These sources of variance have been\nused individually in previous work. Stochastic approximation EM (SAEM, Delyon et al., 1999) combines an EM algorithm with a stochastic online inference procedure. SAEM does not subsample data, but rather interpolates between Monte Carlo estimates of the complete data. Kuhn & Lavielle (2004) extend SAEM to use MCMC estimates. Similarly, online EM (Cappe\u0301 & Moulines, 2009) sub-samples data but preserves standard inference procedures for local variables.\nStandard collapsed Gibbs sampling uses multiple sweeps over the entire corpus, representing topic-word distributions using the topic-word assignment variables of the entire corpus except for the current token. As a result, topic assignment variables must in theory be sampled sequentially, although parallel approximations work well empirically (Asuncion et al., 2008). In contrast, Algorithm 1 treats topic-word distributions as a global variable distinct from the local token-topic assignment variables, and so can parallelize trivially.\nIn this work we integrate over document-topic proportions \u03b8d within a variational algorithm. Collapsed variational inference (Teh et al., 2006) also analytically marginalizes over the topic proportions, but still maintains a fully factorized distribution over topic assignments at each position. The method described here does not restrict itself to such factored distributions, and therefore reduces bias, but this reduction may be offset by the bias we introduce when we initialize the Gibbs chain."}, {"heading": "4. Empirical Results", "text": "In this section we compare the sampled online algorithm to two related online methods and measure the effect of model parameters. We use a selection of metrics to evaluate models."}, {"heading": "4.1. Evaluation", "text": "Held-out probability. A model that characterizes the semantic structure of a corpus should place more of its probability mass on sensible documents than on random sequences of words. We can use this assumption to compare different models by asking each model to estimate the probability of a previously unseen document. A better model should, on average, assign higher probability to real documents than a lower-quality model. We evaluate held-out probability using the left-to-right sequential sampling method (Wallach et al., 2009; Buntine, 2009). For each trained model we generate point estimates of the topic-word probabilities p\u0303(w|k). We then process each document by iterating through the tokens w1, ..., wNd . At each\nposition i we calculate the marginal probability p\u0303(wi|w<i) = \u2211 k p(zi = k|w<i, z<i, \u03b1)p\u0303(wi|k). (12)\nWe then sample zi proportional to the terms of that summation and continue to the next token.1 In order to normalize for document lengths, we divide the sum of the logs of the marginal probabilities by Nd.\nCoherence. This metric measures the semantic quality of a topic by approximating the experience of a user viewing the W most probable words for the topic (Mimno et al., 2011). It is related to point-wise mutual information (Newman et al., 2010). Let D(w) be the document frequencies for each word w, that is, the number of documents containing one or more tokens of type w, and let D(w1, w2) be the number of documents containing at least one token of w1 and of w2. For each pair of words w1, w2 in the top W list, we calculate the number of documents that contain at least one token of the higher ranked word w1 that also contain at least one token of the lower ranked word w2:\nC(W ) = \u2211 i \u2211 j<i log D(wi, wj) + D(wj) (13)\nwhere is a small constant used to avoid log zero. Values closer to zero indicate greater co-occurrence. Unlike held-out probability, which reports scores for held-out documents, coherence reports scores for individual topics.\nWallclock time. Our goal is to train useful models as efficiently as possible. In addition to model quality metrics, we are therefore also interested in total computation time."}, {"heading": "4.2. Comparison to Online VB", "text": "Our first corpus consists of 350,000 research articles from three major journals: Science, Nature, and the Proceedings of the National Academy of Sciences of the USA. We use a vocabulary with 19,000 distinct words, including selected multi-word terms. We train models on 90% of the Science/Nature/PNAS corpus, holding out the remaining documents for testing purposes. We save topic-word parameters N\u0302kw after epochs consisting of 500,000 documents.\nSampled online variational Bayesian inference compares well in terms of wallclock time to standard online VB inference, particularly with respect to the number of topics K. Figure 1 shows results comparing standard online VB inference to sampled online inference\n1We set the document-topic hyperparameter \u03b1 = 0.1.\nfor K up to 1000. Each iteration consists of a minibatch of 100 documents. Standard online inference takes time linear in K, while wallclock time for sampled online inference grows more slowly.\nWe would like to know if there is a difference in the quality of models trained through the hybrid sampled variational algorithm and the online LDA algorithm. We compare an implementation of Online LDA that tries to be as close as possible to the sampled online implementation, but using a dense VB update instead of a sparse sampled update for the local variables. In particular, the number of coordinate ascent steps in standard VB is equal to the number of Gibbs sweeps in the sampled algorithm.\nPer-topic coherence for K = 200 is shown in Figure 2. Sampled online inference produces fewer very poor topics. This difference is significant under a twosample t-test (p < 0.001) and does not decrease with additional training epochs. Sampled online inference also assigns greater held-out probability than Online LDA for every test document, by a wide margin. We evaluated several possible reasons for this difference in performance. Held-out probability estimation can be affected by evaluation-time smoothing parameter settings, but we found both models were affected equally. The log probability of a document is the sum of the log probabilities of its words. It is possible that if one model assigned very small probability to a handful of tokens, those words could significantly affect the overall score, but the difference in log probability was consistent across many tokens. The scale of parameters might not be comparable, but as both methods use the same learning schedule, the sum of the trained parameters \u03bbkw is nearly identical.\nThe main difference appears to be the entropy of the\nCoherence\nA lg\nor ith\nm\nSampOnline\nVB\n\u25cf\u25cf \u25cf\n\u25cf\u25cf \u25cf\u25cf\u25cf \u25cf\u25cf\u25cf \u25cf \u25cf\u25cf \u25cf \u25cf\u25cf\n\u22121200 \u22121000 \u2212800 \u2212600 \u2212400 \u2212200\nCoherence\nHeldOut\nA lg\nor ith m SampOnline\nSMC\n\u25cf \u25cf\u25cf \u25cf\u25cf\u25cf\u25cf \u25cf \u25cf\u25cf\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\u25cf\u25cf \u25cf \u25cf\u25cf \u25cf \u25cf \u25cf\u25cf\u25cf\u25cf\u25cf \u25cf \u25cf \u25cf \u25cf\u25cf \u25cf\u25cf \u25cf\u25cf\u25cf \u25cf \u25cf\u25cf\u25cf\u25cf \u25cf\u25cf\u25cf\u25cf \u25cf\u25cf \u25cf\u25cf\u25cf\u25cf\n\u25cf \u25cf\u25cf\u25cf \u25cf \u25cf\u25cf\u25cf\u25cf \u25cf\u25cf\u25cf\u25cf \u25cf \u25cf\u25cf \u25cf\u25cf\u25cf\u25cf \u25cf \u25cf\u25cf \u25cf \u25cf \u25cf\u25cf \u25cf\u25cf\u25cf\u25cf \u25cf\u25cf \u25cf \u25cf\u25cf \u25cf \u25cf\u25cf \u25cf\u25cf\u25cf\u25cf \u25cf\u25cf \u25cf\u25cf\u25cf \u25cf \u25cf\u25cf \u25cf\u25cf \u25cf\u25cf\u25cf \u25cf \u25cf\u25cf\n\u22129.5 \u22129.0 \u22128.5 \u22128.0 \u22127.5 \u22127.0\nFigure 3. Sampled online inference performs better than one pass of sequential Monte Carlo, after processing a comparable number of documents with K = 200.\ntopic distributions: the sampled-online algorithm produces less concentrated distributions (mean entropy 6.8 \u00b1 0.46) than standard online LDA (mean entropy 6.0\u00b1 0.58). This result could indicate that coordinate ascent over the local variables for Online LDA is not converging."}, {"heading": "4.3. Comparison to Sequential Monte Carlo", "text": "Sequential Monte Carlo is an online algorithm similar to Gibbs sampling in that it represents topics using sums over assignment variables (Ahmed et al., 2012). A Gibbs sampler starts with a random initialization for all hidden variables and sweeps repeatedly over the entire data set, updating each variable given the current value of all other variables. SMC samples values for hidden variables in sequential order, conditioning only on previously-seen variables. It is common to keep multiple sampling states or \u201cparticles\u201d, but this process adds both computation and significant bookkeeping complexity. Ahmed et al. (2012) use a single SMC state.\nIn order to compare SMC to the sampled online algorithm, we ran 10 independent SMC samplers over the Science/Nature/PNAS dataset, with documents\nordered randomly. We also ran 10 independent sampled trainers, stopping after a number of documents had been sampled equivalent to the size of the corpus. In order to make the comparison more fair, we allowed the SMC sampler to sweep through each document the same number of times as the sampled online algorithm, but only the final topic configuration of a document was available to the subsequent documents.2 Results for K = 200 are shown in Figure 3. SMC has consistently worse per-topic coherence and per-document held-out log probability. The sampled online algorithm in this paper differs from SMC in that the contribution of local token-topic assignment variables decays according to the learning rate schedule, so that more recently sampled documents can have greater weight than earlier documents. This decay allows sampled online inference to \u201cforget\u201d its initial topics, unlike SMC, which weights all documents equally."}, {"heading": "4.4. Effect of parameter settings", "text": "Number of samples. In the inner loop of our algorithm we initialize3 the topic indicator variables z for a document and then perform several Gibbs sweeps. In each sweep we resample the value of each topic indicator variable in turn. We introduce bias when we initialize, so we discard B \u201cburn-in\u201d sweeps and use values of z saved after S additional sweeps to calculate the gradient. Since performance is linear in the total number of sweeps B + S, we want to find the smallest number of sweeps that does not sacrifice performance.\nWe consider nine settings of the pair (B,S). Under the first three settings we save one sweep and vary the number of burn-in sweeps: (1,1), (2,1), (3,1). For the second three settings we perform five sweeps, varying how many we discard: (2,3), (3,2), (4,1). The final three settings fix B = S and consider larger total numbers of sweeps: (5,5), (10,10), (20,20). We evaluate each setting after processing 500,000 documents.\nPerformance was similar across settings with the following exceptions, which were significant at p < 0.001 under a two-sample t-test. The two-sweep setting (1,1) had better topic coherence but worse held-out probability than the all other settings. The (5,5) setting had the best mean held-out probability, but it was not significantly better than (10,10) and (20,20). The\n2Note that SMC has an advantage. In the sampled online algorithm we Gibbs sample each document within a mini-batch independently, while in SMC, documents \u201csee\u201d results from all previous documents.\n3We initialize by sampling each token conditioned on the topics of the previous tokens in the document: p(zdi = k) \u221d (\u03b1+ \u2211 j<i Izdj=k)p(wdi|k).\nmany-sweep settings (5,5), (10,10), (20,20) had worse topic coherence than the other settings, with many visibly low-quality topics. These results suggest that 3\u20135 sweeps is sufficient.\nTopic-word smoothing. Eq. 9 involves the function e\u03a8(x). This function approaches x \u2212 12 as x gets large, but for values of x near 0, it is non-linear. For example, e\u03a8(0.05) is 1034 times greater than e\u03a8(0.01). If the values of topic parameters are in this range, a minuscule increase in the parameter for word w in topic k can cause a profound change in the sampling distribution for that word: all subsequent tokens of type w will be assigned to topic k with probability near 1.0.\nIn general, the randomness introduced by sampling topic assignments helps to avoid becoming trapped in local maxima. When parameters are near zero, however, random decisions early in the inference process risk becoming permanent. The topic-word smoothing parameter \u03b7 can push parameter values away from this explosive region. We measured coherence for six settings of the topic-word hyperparameter \u03b7, {0.1, 0.2, 0.3, 0.4, 0.5, 0.6}. At \u03b7 = 0.1, a common value for batch variational inference, many topics are visibly nonsensical. Average coherence improves significantly for each increasing value of \u03b7 \u2208 {0.2, 0.3, 0.4} (p < 0.001). There is no significant difference in average coherence for \u03b7 \u2208 {0.4, 0.5, 0.6}.\nForgetting factors. We now consider the learning rate \u03c1t = (t0 + t)\n\u2212\u03ba and its relation to the corpus size D. We fix \u03ba = 0.6 and vary the offset parameter t0 \u2208 {3000, 15000, 30000, 150000, 300000}, saving topic parameters after five training epochs of 500,000 documents each. There was no significant difference in average topic coherence.\nThe learning rate, however, is not the only factor that determines the magnitude of parameter updates. Eq. 11 also includes the size of the corpus D. If the corpus is larger, we will take larger steps, regardless of the contents of the mini-batch. The offset parameter t0 had no significant effect on coherence for the full corpus, but it may have an effect if we also vary the corpus size.\nWe simulate different size corpora by subsampling the full data set. Results are shown in Figure 4 for models trained on one half, one quarter, and one eighth of the corpus. Each corpus is a subset of the next larger corpus. In the smallest corpus (12.5%), the model with t0 = 300000 is significantly worse than other settings (p < 0.001). Otherwise, there is no significant difference in average topic coherence."}, {"heading": "4.5. Scalability", "text": "Pre-1922 books. To demonstrate the scalability of the method, we modeled a collection of 1.2 million outof-copyright books. Topic models are useful in characterizing the contents of the corpus and supporting browsing applications: even scanning titles for a collection of this size is impossible for one person. Previous approaches to million-book digital libraries have focused on keyword search and word frequency histograms (Michel et al., 2011). Such methods do not account for variability in meaning or context. There is no guarantee that the words being counted match the meaning assumed by the user. In contrast, an interface based on a topic model could, for example, distinguish uses of the word \u201cstrain\u201d in immunology, mechanical engineering, and cookery.\nWe divide each book into 10-page sections, resulting in 44 million \u201cdocuments\u201d with a vocabulary size of 216. We trained models with K \u2208 {100, 500, 1000, 2000}. Randomly selected example topics are shown in Table 1, illustrating the average level of topic quality. Models are sparse: at K = 2000, less than 1% of the 2000 \u00b7216 possible topic-word parameters are non-zero. The algorithm scales well as K increases. The number of milliseconds taken to process a sequence of 10,000 documents was similar for K = 1000 and 2000, despite doubling the number of topics."}, {"heading": "5. Conclusions", "text": "Stochastic online inference allows us to scale topic modeling to large document sets. Sparse Gibbs sampling allows us to scale to large numbers of topics. The algorithm presented in this paper combines the advantages of these two methods. As a result, models can be trained on vast, open-ended corpora without requiring access to vast computer clusters. If parallel architectures are available, we can trivially parallelize computation within each mini-batch. As this work is related to the Online LDA algorithm of Hoffman et al.\n(2010), extensions to that model are also applicable, such as adaptive scheduling algorithms (Wahabzada & Kersting, 2011). The use of MCMC within stochastic variational inference reduces one source of bias in estimating local variables. Although we have focused on text analysis applications, this hybrid method generalizes to a broad class of Bayesian models."}, {"heading": "Acknowledgments", "text": "John Langford, Iain Murray, Charles Sutton provided helpful comments. Yahoo! and PICSciE provided computational resources. DM is supported by a CRA CI fellowship. MDH is supported by NSF ATM0934516, DOE DE-SC0002099, and IES R305D100017. DMB is supported by ONR N00014-11-1-0651, NSF CAREER 0745520, AFOSR FA9550-09-1-0668, the Alfred P. Sloan foundation, and a grant from Google."}, {"heading": "A. Sparse computation", "text": "Sparse sampling over topics. Sampling zsdi \u221d (\u03b1 + Ndk)e\nEq [log \u03b2kw] requires calculating the normalizing constant Z = \u2211 k(\u03b1+Ndk)e\nEq [log \u03b2kw]. This calculation can be accomplished in time much less than O(k) if we can represent the topic-word parameters \u03bbkw sparsely. The smoothing parameter \u03b7 can be factored out of Equation 11 as long as we assume that all initial values \u03bb0kw \u2265 \u03b7. Rearranging this equation to separate the Dirichlet hyperparameter \u03b7\n\u03bbtkw \u2190 \u03b7 + (1\u2212 \u03c1t) ( \u03bbt\u22121kw \u2212 \u03b7 ) + \u03c1t D\n|B| NSkw (14)\nshows that we can define an alternative parameter N\u0303 tkw = \u03bb t kw \u2212 \u03b7 that represents the \u201cnon-smoothing\u201d portion of the variational Dirichlet parameter, and ignore the contribution of the smoothing parameter until it is time to calculate expectations.\nFor any given w, it is likely that most values of N\u0303kw will be zero. We can therefore rewrite the normalizing constant as\nZ = \u2211 k \u03b1+Ndk e\u03a8(V \u03b7+N\u0303k\u25e6) ( e\u03a8(\u03b7+N\u0303kw) \u2212 e\u03a8(\u03b7) ) +\n\u2211 k \u03b1+Ndk e\u03a8(V \u03b7+N\u0303k\u25e6) e\u03a8(\u03b7). (15)\nThe second summation does not depend on any wordspecific variables, and can therefore be calculated and then updated incrementally as Ndk changes. The first summation is non-zero only for k such that N\u0303kw > 0.\nSparse updates in the vocabulary. We expect that a typical mini-batch will contain a small fraction of the words in the vocabulary. Eq. 11, however, updates N\u0303kw for all words, even words that do not occur in the current mini-batch. Expanding the recursive definition of N\u0303 tkw, and letting N\u0302 t kw = D |B|N S kw,\nN\u0303 tkw = \u03c1tN\u0302 t kw + (1\u2212 \u03c1t) ( \u03c1t\u22121N\u0302 t\u22121 kw + (1\u2212 \u03c1t\u22121)(...) ) (16)\n= \u03c1tN\u0302 t kw + (1\u2212 \u03c1t)\u03c1t\u22121N\u0302 t\u22121kw + (1\u2212 \u03c1t)(1\u2212 \u03c1t\u22121)...\n(17) Dividing both sides by \u220ft i=1(1\u2212 \u03c1i),\nN\u0303 tkw\u220ft i=1(1\u2212 \u03c1i) = \u03c1tN\u0302 t kw\u220ft i=1(1\u2212 \u03c1i) + \u03c1t\u22121N\u0302 t\u22121 kw\u220ft\u22121 i=1(1\u2212 \u03c1i) (18)\n+ \u03c1t\u22122N\u0302 t\u22122 kw\u220ft\u22122\ni=1(1\u2212 \u03c1i) + ....\nDefining a variable \u03c0t = \u220ft i=1(1\u2212 \u03c1i), the update becomes\nN\u0303 tkw \u03c0t = N\u0303 t\u22121kw \u03c0t\u22121 + \u03c1tN\u0302 t kw \u03c0t . (19)\nThis update is sparse: only elements with nonzero ndw will be modified. To calculate the expectation of p(w|k), we compute \u03a8 ( \u03b7 + \u03c0t Ntkw \u03c0t ) \u2212\n\u03a8 ( W\u03b7 + \u03c0t \u2211 w Ntkw \u03c0t ) .\nThe scale factor \u03c0t can become small after several hundred mini-batches. We periodically \u201creset\u201d this parameter by setting all stored values to N\u0303 tkw = \u03c0t N\u0303tkw \u03c0t\n, avoiding the possibility of numerical instability."}], "references": [{"title": "Scalable inference in latent variable models", "author": ["Ahmed", "Amr", "Aly", "Mohamed", "Gonzalez", "Joseph", "Narayanamurthy", "Shravan", "Smola", "Alexander"], "venue": "In WSDM,", "citeRegEx": "Ahmed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2012}, {"title": "Asynchronous distributed learning of topic models", "author": ["Asuncion", "Arthur", "Smyth", "Padhraic", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Asuncion et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2008}, {"title": "Pattern Recognition and Machine Learning", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["Blei", "David", "Ng", "Andrew", "Jordan", "Michael"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Estimating likelihoods for topic models", "author": ["Buntine", "Wray L"], "venue": "In Asian Conference on Machine Learning,", "citeRegEx": "Buntine and L.,? \\Q2009\\E", "shortCiteRegEx": "Buntine and L.", "year": 2009}, {"title": "Online EM algorithm for latent data models", "author": ["Capp\u00e9", "Olivier", "Moulines", "Eric"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2009}, {"title": "Convergence of a stochastic approximation version of the EM algorithm", "author": ["Delyon", "Bernard", "Lavielle", "Marc", "Moulines", "Eric"], "venue": "Annals of Statistics,", "citeRegEx": "Delyon et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Delyon et al\\.", "year": 1999}, {"title": "Finding scientific topics", "author": ["Griffiths", "Thomas L", "Steyvers", "Mark"], "venue": "PNAS, 101(suppl", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Online learning for latent dirichlet allocation", "author": ["Hoffman", "Matthew", "Blei", "David", "Bach", "Francis"], "venue": "In NIPS,", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Introduction to variational methods for graphical models", "author": ["Jordan", "Michael", "Ghahramani", "Zoubin", "Jaakkola", "Tommi", "Saul", "Laurence"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Coupling a stochastic approximation version of EM with an MCMC procedure", "author": ["Kuhn", "Estelle", "Lavielle", "Marc"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Kuhn et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 2004}, {"title": "Optimizing semantic coherence in topic models", "author": ["Mimno", "David", "Wallach", "Hanna", "Talley", "Edmund", "Leenders", "Miriam", "McCallum", "Andrew"], "venue": "In EMNLP,", "citeRegEx": "Mimno et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "Automatic evaluation of topic coherence", "author": ["Newman", "David", "Lau", "Jey Han", "Grieser", "Karl", "Baldwin", "Timothy"], "venue": "In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Newman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Online model selection based on the variational Bayes", "author": ["M.A. Sato"], "venue": "Neural Computation,", "citeRegEx": "Sato,? \\Q2001\\E", "shortCiteRegEx": "Sato", "year": 2001}, {"title": "A collapsed variational bayesian inference algorithm for latent dirichlet allocation", "author": ["Teh", "Yee-Whye", "Newman", "David", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Larger residuals, less work: Active document scheduling for latent Dirichlet allocation", "author": ["Wahabzada", "Mirwaes", "Kersting", "Kristian"], "venue": "In ECML/PKDD,", "citeRegEx": "Wahabzada et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wahabzada et al\\.", "year": 2011}, {"title": "Evaluation methods for topic models", "author": ["Wallach", "Hanna", "Murray", "Iain", "Salakhutdinov", "Ruslan", "Mimno", "David"], "venue": "In ICML,", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Topic models are hierarchical Bayesian models of document collections (Blei et al., 2003).", "startOffset": 70, "endOffset": 89}, {"referenceID": 9, "context": "Our algorithm builds on variational inference (Jordan et al., 1999).", "startOffset": 46, "endOffset": 67}, {"referenceID": 8, "context": "Recently, Hoffman et al. (2010) introduced Online LDA, a stochastic gradient optimization algorithm for topic modeling.", "startOffset": 10, "endOffset": 32}, {"referenceID": 14, "context": "Unlike standard mean-field variational inference, but similar to Griffiths & Steyvers (2004) and Teh et al. (2006), we will marginalize out the topic proportions \u03b8d.", "startOffset": 97, "endOffset": 115}, {"referenceID": 13, "context": "We obtain these noisy estimates by subsampling the data (Sato, 2001; Hoffman et al., 2010).", "startOffset": 56, "endOffset": 90}, {"referenceID": 8, "context": "We obtain these noisy estimates by subsampling the data (Sato, 2001; Hoffman et al., 2010).", "startOffset": 56, "endOffset": 90}, {"referenceID": 13, "context": "Premultiplying the gradient of an objective function by the inverse Fisher information matrix of the distribution being optimized (in our case the variational distribution q) results in the natural gradient (Sato, 2001).", "startOffset": 207, "endOffset": 219}, {"referenceID": 8, "context": "8 (Hoffman et al., 2010).", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": "Stochastic approximation EM (SAEM, Delyon et al., 1999) combines an EM algorithm with a stochastic online inference procedure. SAEM does not subsample data, but rather interpolates between Monte Carlo estimates of the complete data. Kuhn & Lavielle (2004) extend SAEM to use MCMC estimates.", "startOffset": 35, "endOffset": 256}, {"referenceID": 1, "context": "As a result, topic assignment variables must in theory be sampled sequentially, although parallel approximations work well empirically (Asuncion et al., 2008).", "startOffset": 135, "endOffset": 158}, {"referenceID": 14, "context": "Collapsed variational inference (Teh et al., 2006) also analytically marginalizes over the topic proportions, but still maintains a fully factorized distribution over topic assignments at each position.", "startOffset": 32, "endOffset": 50}, {"referenceID": 16, "context": "We evaluate held-out probability using the left-to-right sequential sampling method (Wallach et al., 2009; Buntine, 2009).", "startOffset": 84, "endOffset": 121}, {"referenceID": 11, "context": "This metric measures the semantic quality of a topic by approximating the experience of a user viewing the W most probable words for the topic (Mimno et al., 2011).", "startOffset": 143, "endOffset": 163}, {"referenceID": 12, "context": "It is related to point-wise mutual information (Newman et al., 2010).", "startOffset": 47, "endOffset": 68}, {"referenceID": 8, "context": "Comparison of seconds per mini-batch between online variational Bayes (Hoffman et al., 2010) and sampled online inference (this paper).", "startOffset": 70, "endOffset": 92}, {"referenceID": 0, "context": "Sequential Monte Carlo is an online algorithm similar to Gibbs sampling in that it represents topics using sums over assignment variables (Ahmed et al., 2012).", "startOffset": 138, "endOffset": 158}, {"referenceID": 0, "context": "Sequential Monte Carlo is an online algorithm similar to Gibbs sampling in that it represents topics using sums over assignment variables (Ahmed et al., 2012). A Gibbs sampler starts with a random initialization for all hidden variables and sweeps repeatedly over the entire data set, updating each variable given the current value of all other variables. SMC samples values for hidden variables in sequential order, conditioning only on previously-seen variables. It is common to keep multiple sampling states or \u201cparticles\u201d, but this process adds both computation and significant bookkeeping complexity. Ahmed et al. (2012) use a single SMC state.", "startOffset": 139, "endOffset": 626}, {"referenceID": 8, "context": "As this work is related to the Online LDA algorithm of Hoffman et al. (2010), extensions to that model are also applicable, such as adaptive scheduling algorithms (Wahabzada & Kersting, 2011).", "startOffset": 55, "endOffset": 77}], "year": 2012, "abstractText": "We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.", "creator": "LaTeX with hyperref package"}}}