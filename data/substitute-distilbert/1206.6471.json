{"id": "1206.6471", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On causal and anticausal learning", "abstract": "mathematicians consider nonlinear problem of function estimation in the case where an underlying causal model can be inferred. reasoning has implications for popular scenarios, as covariate shift, concept drift, reciprocal learning and semi - supervised learning. we argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. making particular, we formulate a hypothesis for when partial - supervised learning activities help, and corroborate it with biased results.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (476kb)", "http://arxiv.org/abs/1206.6471v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1112.2738"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1112.2738", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["bernhard sch\u00f6lkopf", "dominik janzing", "jonas peters", "eleni sgouritsa", "kun zhang", "joris m mooij"], "accepted": true, "id": "1206.6471"}, "pdf": {"name": "1206.6471.pdf", "metadata": {"source": "META", "title": "On Causal and Anticausal Learning", "authors": ["Bernhard Sch\u00f6lkopf", "Dominik Janzing", "Jonas Peters", "Eleni Sgouritsa", "Kun Zhang", "Joris Mooij"], "emails": ["FIRST.LAST@TUE.MPG.DE", "J.MOOIJ@CS.RU.NL"], "sections": [{"heading": "1. Introduction", "text": "A large part of machine learning research aims at exploiting statistical associations or dependences between variables to make predictions about certain variables. This is useful especially in situations where we have sizable training sets, but no detailed model of the underlying data generating process. It has been argued that statistical associations are always due to underlying causal structures (Reichenbach, 1956). This suggests the question how machine learning could benefit from knowledge of these structures. The present paper addresses this question in the simplest possible setting, where the causal structure only consists of cause and effect, and there are no unobserved confounders. We argue that under certain assumptions, detailed below, there are asymmetries in joint distributions that have implications for statistical machine learning. We try to give a systematic outline of these implications. This paper does not prove theorems; rather, it aims at providing insight and drawing connections. It does not contain new experimental data, but a meta-analysis of performances reported by three other studies, focusing on an implication of causal structure for semi-supervised learning. We believe that the implications of causal structure for machine learning are conceptually intriguing, and we hope that they will raise interest\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nfor causal inference in the machine learning community.\nAn example illustrating the difference between the statistical and the causal point of view is the correlation between the frequency of storks and the human birth rate (Matthews, 2000). We may be able to train a good predictor of the birth rate which uses the frequency of storks (along with other features) as an input. However, if politicians asked us whether one could boost the birth rate by increasing the number of storks, we would have to tell them that this kind of intervention is not covered by the standard i.i.d. assumption of statistical learning. In practice, however, interventions can be relevant, distributions may shift over time, and we might want to combine data recorded under different conditions or from different but related regularities.\nWe briefly summarize some aspects of causal graphical models as pioneered by Pearl (2000); Spirtes et al. (1993). These are usually thought of as joint probability distributions over a set of variables X1, . . . , Xn, along with a directed acyclic graph with vertices Xi and arrows indicating direct causal influences. The causal Markov assumption states that each vertex Xi is independent of its nondescendants in the graph, given its parents. Crucially, this links causal semantics (which is important for predicting how a system reacts to interventions) to something that has empirically measurable consequences. Given observations from a joint distribution, it allows us to test conditional independence statements and thus infer (subject to a genericity assumption referred to as faithfulness) which causal models are consistent with an observed distribution. This will typically not lead us to a unique causal model though.\nAn alternative approach, referred to as a functional causal model (a.k.a. structural causal model or nonlinear structural equation model), starts with a set of jointly independent noise variables, one per vertex, and each vertex computes a deterministic function of its noise variable and its parents. These functions do not describe relations between observations only, but also how the system behaves under interventions: by changing the input of some of the functions, one can compute the effect of setting some variables to specific values. A functional model entails a joint distribution\nwhich along with the graph satisfies the causal Markov assumption (Pearl, 2000). Vice versa, each causal graphical model can be expressed as a functional causal model.\nThe functional point of view allows us to come up with assumptions on causal models that would be harder to conceive in a pure probabilistic view. Such assumptions (see below) allow us to distinguish between X \u2192 Y and X \u2190 Y . This cannot be achieved by conditional independence testing, since no nontrivial conditional independences exist if we only have two variables.\nThe functional point of view thus opens up the possibility to infer the causal direction for input-output learning problems. Perhaps somewhat surprisingly, learning problems need not always predict effect from cause, and we will argue that the direction of the prediction has consequences for which tasks are easy and which tasks are hard.\nNotation. We consider the causal structure shown in Fig. 1, with two observables, modeled by random variables. The variableC stands for the cause andE for the effect. We denote their distributions by P (C) and P (E) (overloading the notation P ), and the domains by calligraphic symbols C and E. The variable X will always be the input and Y the output (or prediction), but input and output can either be cause or effect \u2014 more below. For simplicity, we assume that their distributions have a joint density with respect to some product measure. We write the values of this density as P (c, e) and the values of the marginal densities as P (c) and P (e), again keeping in mind that these three P are different functions \u2014 we can always tell from the argument which function is meant. In some places, we will use conditional densities, always implicitly assuming that they exist.\nThe following assumptions are used throughout the paper.\nCausal sufficiency. We assume that there are two independent noise variables NC and NE , modeled as random variables with distributions P (NC) and P (NE).\nThe function \u03d5 and the noise NE jointly determine P (E|C) via E = \u03d5(C,NE) . We think of P (E|C) as the mechanism transforming cause C into effect E.1\n1Note that we will use the term \u201cmechanism\u201d both for the\nIndependence of mechanism and input. We finally assume that the mechanism is \u201cindependent\u201d of the distribution of the cause (i.e., independent of P (C) = P (NC), cf. Fig. 1), in the sense that P (E|C) contains no information about P (C) and vice versa; in particular, if P (E|C) changes at some point in time, there is no reason to believe that P (C) changes at the same time.2\nThis assumption has been used by Janzing & Scho\u0308lkopf (2010), inspired by Lemeire & Dirkx (2007). It is plausible if we are dealing with a mechanism of nature that does not care what we feed into it. For instance, in the problem of predicting splicing patterns from genomic sequences, the basic splicing mechanism (driven by the ribosome) may be assumed evolutionarily stable and thus independent of the species (Schweikert et al., 2009), even though the genomic sequences and their statistical properties differ. Intuitively, if we learn a causal model of splicing, we could hope to be more robust with respect to changes of the input statistics.\nThe independence assumption introduces an asymmetry between cause and effect, since it will usually be violated in the backward direction, i.e., P (E) and P (C|E) are dependent because both inherit properties from P (E|C) and P (C) (Janzing & Scho\u0308lkopf, 2010; Danius\u030cis et al., 2010).\nRichness of functional causal models It turns out that the two-variable functional causal model is so rich that the causal direction cannot be inferred. To understand the richness of the class intuitively, consider the simple case where the noise NE can take only a finite number of values, say {1, . . . , v}. This noise could affect \u03d5 for instance as follows: there is a set of functions {\u03d5n:n = 1, . . . , v}, and the noise randomly switches one of them on at any point, i.e., \u03d5(c, n) = \u03d5n(c). The functions \u03d5n could implement arbitrarily different mechanisms, and it would thus be hard to identify\u03d5 from empirical data sampled from such a complex model. In view of this, it is surprising that conditional independence alone does allow us to do causal inference of practical significance, as implemented by the PC and FCI algorithms (Spirtes et al., 1993; Pearl, 2000). However, additional assumptions that prevent the noise switching construction can significantly facilitate the task of inferring causal graphs from data. Intuitively, such assumptions need to control the sensitivity of the mechanism \u03d5 to the change in the noise NE , and thus the complexity of P (E|C). Additive noise models. One such assumption is referred to as ANM, standing for additive noise model (Hoyer et al., 2009). This model assumes \u03d5(C,NE) = \u03c6(C) + NE for some function \u03c6:\nE = \u03c6(C) +NE , (1)\nfunction \u03d5 and for the conditional P (E|C), but not for P (C|E). 2This \u201cindependence\u201d condition is closely related to the concept of exogeneity in economics (Pearl, 2000). Given two variables C and E, we say C is exogenous if P (E|C) remains invariant to changes in the process that generates C.\nand it has been shown that \u03c6 and NE can be inferred in the generic case, provided that NE has zero mean. This means that apart from some exceptions, such as the case where \u03c6 is linear and NE is Gaussian, a given joint distribution of two real-valued random variables X and Y can be fit by an ANM model in at most one direction (which we then consider the causal one). A similar statement has been shown for the postnonlinear ANM model (Zhang & Hyva\u0308rinen, 2009) E = \u03c8(\u03c6(C) +NE) , where \u03c8 is an invertible function. In practice, an ANM model can be fit by regressing the effect on the cause while enforcing that the residual noise variable is independent of the cause (Mooij et al., 2009). If this is impossible, the model is incorrect (e.g., cause and effect are interchanged, the noise is not additive, or there are confounders; in the latter two cases the method cannot find the causal direction).\nANMs play an important role in this paper; first, the methods below will presuppose that we know what is cause and what is effect, and second, we will generalize ANM to handle the case where we have several models of the form (1) that share the same \u03c6. The following sections provide an overview of how causal direction affects various learning scenarios, partly relying on assumptions such as ANMs.\nThe comprehensive work of Storkey (2009) already describes the cases discussed in Sections 2.1.1 and 3.2.1, but not the other cases where further assumptions are needed. He also describes several scenarios where both P (C) and P (E|C) change, for instance if the data set is obtained by sample selection according to the value of a common effect of C and E, or an effect of E, and the case where the data sets correspond to different values of a common cause of C and E. Pearl & Bareinboim (2011) introduce a variable S that labels different domains or data sets and explain how the way in which S is causally linked to variables of interest is relevant for transferring causal or statistical statements across domains. Their notion of transportability employs conditional independences to express invariance of mechanisms, which is not general enough to include all the types of invariances we have in mind. For instance, the functions representing a causal mechanism could remain the same, while the unobserved noise terms may differ across data sets. Finally, we point out that an earlier version of the present work appeared as (Scho\u0308lkopf et al., 2011)."}, {"heading": "2. Predicting Effect from Cause", "text": "Let us consider the case where we are trying to estimate a function f : X \u2192 Y or a conditional distribution P (Y |X) in the causal direction, i.e., thatX is the cause and Y the effect. Intuitively, this situation of causal prediction should be the \u2018easy\u2019 case since there exists a functional mechanism \u03d5 which f should try to mimic. We are interested in the question how robust the estimation is with respect to changes in the noise variables of the underlying model."}, {"heading": "2.1. Additional information about the input", "text": ""}, {"heading": "2.1.1. ROBUSTNESS W.R.T. INPUT CHANGES", "text": "Given: training points from P (X,Y ) and an additional set of inputs sampled from P \u2032(X), with P (X) 6= P \u2032(X).\nGoal: estimate P \u2032(Y |X).\nSolution: by independence of mechanism and input, there is no reason to assume that the observed change in P (X) (i.e., in P (NX)) entails a change in P (Y |X), and we thus conclude P \u2032(Y |X) = P (Y |X). This scenario is referred to as covariate shift (Sugiyama & Kawanabe, 2012). The equation P \u2032(Y |X) = P (Y |X) should, however, not be mistaken as saying that the rule for predicting Y from X need not be adapted to the new input distribution P (X). This is because prediction from finite data may favor simple functions that fit the data well in the region where P (X) has high probability, but not where P \u2032(X) is high."}, {"heading": "2.1.2. SEMI-SUPERVISED LEARNING (SSL)", "text": "Given: training points sampled from P (X,Y ) and an additional set of inputs sampled from P (X).\nGoal: estimate P (Y |X).\nNote: by independence of the mechanism, P (X) contains no information about P (Y |X). A more accurate estimate of P (X), as may be possible by the addition of the test inputs P (X), does thus not influence an estimate of P (Y |X), and SSL is pointless for the scenario in Figure 2."}, {"heading": "2.2. Additional information about the output", "text": ""}, {"heading": "2.2.1. ROBUSTNESS W.R.T. OUTPUT CHANGES", "text": "Given: training points from P (X,Y ) and an additional set of outputs sampled from P \u2032(Y ), with P \u2032(Y ) 6= P (Y ).\nGoal: estimate P \u2032(Y |X).\nAssumption: not clear\nSolution: first we need to decide whether P (X) or P (Y |X) has changed, for some rough ideas see Localizing distribution change (Section 4). If P (X) has changed, proceed as in Section 2.1.1. If P (Y |X) has changed, we can estimate P \u2032(Y |X) via Estimating causal conditionals (Section 4). Here, additive noise is a sufficient assumption."}, {"heading": "2.2.2. ADDITIONAL OUTPUTS", "text": "Given: training points sampled from P (X,Y ) and an additional set of outputs sampled from P (Y ).\nGoal: estimate P (Y |X).\nAssumption: P (X,Y ) has an additive noise model from X to Y and P (Y ) has a unique decomposition as convolution of two distributions, say P (Y ) = Q \u2217 R. This is, for instance, satisfied if the noise is Gaussian and P (\u03c6(C)) is indecomposable (i.e., it cannot be written as a non-trivial convolution of two distributions).\nSolution: The additional outputs help because the decomposition tells us that either P (NY ) = Q or P (NY ) = R. The additive noise model learned from the x, y-pairs will probably tell us which of the alternatives is true. Knowing P (Y ), learning P (Y |X) reduces to learning \u03c6 from the x, y-pairs, which is a weaker problem than learning P (Y |X) would be in general."}, {"heading": "2.3. Additional information about input and output", "text": ""}, {"heading": "2.3.1. TRANSFER LEARNING (ONLY NOISE CHANGES)", "text": "Given: training points sampled from P (X,Y ) and an additional set of points sampled from P \u2032(X,Y ), with P \u2032(X,Y ) 6= P (X,Y ).\nGoal: estimate P \u2032(Y |X).\nAssumption: Additive noise, where \u03c6 is invariant but the noises can change.\nSolution: run Conditional ANM to output a single function, only enforcing independence of residuals separately for the two data sets (Section 4).\nThere is also a SSL variant of this scenario: Given a training set plus two unpaired sets from the two original marginals, the extra sets help to better estimate P (X,Y ) because we have argued in Section 2.2.2 that additional yvalues sampled from P (Y ) already help.\nThe fact that causal directions matter for transferring knowledge from one data set to another one has previously been pointed out by Storkey (2009)."}, {"heading": "2.3.2. CONCEPT DRIFT (ONLY FUNCTION CHANGES)", "text": "Given: training points sampled from P (X,Y ) and an additional set of points sampled from P \u2032(X,Y ), with P \u2032(X,Y ) 6= P (X,Y ).\nGoal: estimate P \u2032(Y |X).\nAssumption: ANM with NX , NY invariant, but \u03c6 has changed.\nSolution: Apply ANM to points sampled from P \u2032(X,Y ) to obtain \u03c6. Then P \u2032(Y |X) is given by P \u2032(Y |X) =\nPNY (Y \u2212 \u03c6(X)) , where the index NY indicates the variable this distribution refers to."}, {"heading": "3. Predicting Cause from Effect", "text": "We now turn to the opposite direction, where we consider the effect as input and we try to predict the value of the cause variable that led to it. This situation, that we refer to as anticausal prediction, may seem unnatural, but it is actually ubiquitous in machine learning. Consider, for instance, the task of predicting the class label of a handwritten digit from its image. The causal structure is as follows: a person intends to write the digit 7, say, and this intention causes a motor pattern producing an image of the digit 7 \u2014 in that sense the class label Y causes the image X .\nP (X|Y ) represents the causal mechanism that generatesX from Y , and it is independent from the distribution of the cause, P (Y ). On the other hand, P (Y |X) is sensitive to the change of the distribution of P (Y ). Therefore, generally speaking, when estimating P (Y |X), it would be better to model P (X|Y ) first and then construct P (Y |X) using Bayes\u2019 rule P (Y |X) = P (X|Y )P (Y )/P (X).\nFor a simple illustration, suppose X = Y + NX , where both Y and NX are independent from each other and uniformly distributed. Fig. 4 shows the scatter plot of Y and X . The expectation E(X|Y ) is linear in Y , and P (X|Y ) can be easily described. However, one can see that E(Y |X) is rather complex; it is nonlinear inX and its shape depends heavily on the distribution of P (Y )."}, {"heading": "3.1. Additional information about the input", "text": ""}, {"heading": "3.1.1. ROBUSTNESS W.R.T. INPUT CHANGES", "text": "Given: training points sampled from P (X,Y ) and an additional set of inputs from P \u2032(X), with P \u2032(X) 6= P (X).3\nGoal: estimate P \u2032(Y |X).\nAssumption: additive Gaussian noise with invertible function \u03c6 and indecomposable P (\u03c6(Y )) is sufficient. Other assumptions are also possible, but invertibility of the causal conditional P (X|Y ) is necessary in any case.\n3A related scenario is that we do not have additional data from P \u2032(X), but we want to still use our knowledge of the causal direction to learn a model that is somewhat robust w.r.t. changes of P (X) due to changes in either P (Y ) or P (X|Y ).\nSolution: We apply Localizing distribution change (Section 4) to decide if P (Y ) or P (X|Y ) has changed. In the first case, we can estimate P \u2032(Y ) via Inverting conditionals (Section 4) if we assume that P (X|Y ) is an injective conditional.4 From this we get P \u2032(X,Y ), and then P \u2032(Y |X) = P\n\u2032(X,Y )\u222b P \u2032(X,Y )dY .\nIf, of the other hand, P (X|Y ) has changed, we can estimate P \u2032(X|Y ) via Estimating causal conditionals (Section 4)."}, {"heading": "3.1.2. SEMI-SUPERVISED LEARNING", "text": "Given: training points sampled from P (X,Y ) and an additional set of inputs sampled from P (X).\nGoal: estimate P (Y |X).\nAssumption: various options, see below\nNote: P (X) and P (Y |X) are not independent and thus contain information about each other. The additional inputs hence may allow a more accurate estimate of P (X).\nKnown assumptions for SSL, as discussed by Chapelle et al. (2006), can indeed be viewed as linking properties of P (X) to properties of P (Y |X): for instance, the cluster assumption stipulates that points lying in the same cluster of P (X) have the same Y ; and the low density separation assumption states that the decision boundary of a classifier (i.e., the point where P (Y |X) crosses 0.5) should lie in a region where P (X) is small. The semi-supervised smoothness assumption says that the estimated function (which we may think of as the expectation of P (Y |X)) should be smooth in regions where P (X) is large.\n4Injectivity means that the input distribution can uniquely be computed from the output distribution, see Section 4."}, {"heading": "3.2. Additional information about the output", "text": ""}, {"heading": "3.2.1. ROBUSTNESS W.R.T. OUTPUT CHANGES", "text": "Given: training points sampled from P (X,Y ) and an additional set of outputs sampled from P \u2032(Y ), with P \u2032(Y ) 6= P (Y ). This scenario is also refered to as prior probability shift (Storkey, 2009).\nGoal: estimate P \u2032(Y |X).\nSolution: independence of mechanism implies P \u2032(X|Y ) = P (X|Y ), thus P \u2032(X,Y ) = P (X|Y )P \u2032(Y ). From this, we compute P \u2032(Y |X) = P\n\u2032(X|Y )P \u2032(Y )\u222b P \u2032(X,Y )dY ."}, {"heading": "3.3. Additional information about input and output", "text": ""}, {"heading": "3.3.1. ROBUSTNESS W.R.T. CHANGES OF INPUT AND OUTPUT NOISE (TRANSFER LEARNING)", "text": "Given: training points sampled from P (X,Y ) and an additional set of points sampled from P \u2032(X,Y ), with P \u2032(X,Y ) 6= P (X,Y ).\nGoal: estimate P \u2032(Y |X).\nAssumption: additive noise where \u03c6 is invariant, but the noises can change.\nSolution: analogous to Section 2.3.1, but use the model backwards in the end."}, {"heading": "3.3.2. CONCEPT DRIFT (CHANGE OF FUNCTION)", "text": "Given: training points sampled from P (X,Y ) and an additional set of points sampled from P \u2032(X,Y ), with P \u2032(X,Y ) 6= P (X,Y ).\nGoal: estimate P \u2032(Y |X).\nAssumption: NX , NY invariant, but \u03c6 has changed to \u03c6\u2032.\nSolution: We can learn \u03c6\u2032 from P \u2032(X,Y ) and then estimate the entire distribution P \u2032(X,Y ) using the estimates of P (NX) and P (NY ) obtained from observing those x, y pairs that were sampled from P (X,Y )."}, {"heading": "4. Modules", "text": "Inverting conditionals We can think of a condi-\ntional P (Y |X) as a mechanism that transforms P (X) into P (Y ). In some cases, we do not lose any information by this mechanism:\nDefinition 1 (injective conditionals) a conditional distribution P (Y |X) is called injective if there are no two distributions P (X) 6= P \u2032(X) such that\u222b\nP (y|x)P (x)dx = \u222b P (y|x)P \u2032(x)dx .\nExample 1 (full rank stochastic matrix) Let X,Y have\nfinite range. Then P (Y |X) is given by a stochastic matrix M and is injective if and only if M has full rank. Note that this is only possible if |X| \u2264 |Y|.\nExample 2 (post-nonlinear model) Let X,Y be realvalued and let Y = \u03c8(\u03c6(X) + NY ) with NY \u22a5 X be a post-nonlinear model where \u03c6 and\u03c8 are injective. Then the distribution of Y uniquely determines the distribution of \u03c6(X) +NY because \u03c8 is invertible. This in turn, uniquely determines the distribution of \u03c6(X) provided that the convolution with P (NY ) is invertible. Since \u03c8 is invertible, this determines the distribution of X uniquely.\nLocalizing distribution change Given data points sampled from P (C,E) and additional points from P \u2032(E) 6= P (E), we wish to decide whether P (C) or P (E|C) has changed. To show that appropriate assumptions render this problem solvable, we sketch some rough ideas. Let E = \u03c6(C) + NE , with the same \u03c6 for both distributions P (E,C) and P \u2032(E,C), but the distribution of the noise NE or the distribution of C changes. Let P (\u03c6(C)) denote the distribution of \u03c6(C).5 Then the distributions of the effect are given by\nP (E) = P (\u03c6(C)) \u2217 P (NE), P \u2032(E) = P \u2032(\u03c6(C)) \u2217 P \u2032(NE) ,\nwhere either P \u2032(\u03c6(C)) = P (\u03c6(C)) or P \u2032(NE) = P (NE). In the following situations, for instance, we can decide which of the cases is true:\n1) If the Fourier transform of P (E) contains zeros, then some of them correspond to zeros in the spectrum of P (\u03c6(C)), the others to zeros of the spectrum of P (NE). Then we may check which zeros still appear in P \u2032(E).\n2) Suppose P (\u03c6(C)) and P \u2032(\u03c6(C)) are indecomposable and P (NE) and P \u2032(NE) are zero mean Gaussian; then the distribution P (E) = P (\u03c6(C)) \u2217 P (NE) uniquely determines P (\u03c6(C)) by deconvolving P (E) with the Gaussian of maximal possible width that still yields a density.\nEstimating causal conditionals Given P \u2032(E), estimate P \u2032(E|C) under the assumption that P (C) remains constant. Assume that P (E,C) and P \u2032(E,C) have been generated by the additive noise model E = \u03c6(C) + NE , with the same P (C) and \u03c6, while the distribution of NE has changed. We have\nP (E) = P (\u03c6(C)) \u2217 P (NE) , P \u2032(E) = P (\u03c6(C)) \u2217 P \u2032(NE) .\nHence, P \u2032(NE) can be obtained by the deconvolution P \u2032(NE) = P (\u03c6(C)) \u2217\u22121 P \u2032(E) . This way, we can compute the new conditional P \u2032(E|C).\n5Explicitly, it is derived from the distribution of C by P (\u03c6(C) \u2208 A) = P (C \u2208 \u03c6\u22121(A)).\nConditional ANM Given two data sets generated by E = \u03c6(C) +NE and E\u2032 = \u03c6(C \u2032) +N \u2032E , respectively. We modify the algorithm of Mooij et al. (2009) to obtain the shared function \u03c6, enforcing separate independence C \u22a5 NE and C \u2032 \u22a5 N \u2032E .\nThis can be interpreted as a generalized ANM model, enforcing conditional independence inE|i = \u03c6(C|i)+NE |i, where i \u2208 {1, 2} is an index, and C \u22a5 NE | i."}, {"heading": "5. Empirical Results", "text": "An evaluation of all methods described is beyond the scope of this paper. We focus on assaying our main prediction regarding the difficulty of SSL; for a toy example applying Conditional ANM in transfer learning, see [1].\nSemi-supervised classification We compare the performance of SSL algorithms with that of base classifiers using only labeled data. For many examples X is vector-valued. We assign each dataset to one of three categories: 1. Anticausal/Confounded: (a) datasets in which at least one feature Xi is an effect of the class Y to be predicted (Anticausal) (includes also cyclic causal relations between Xi and Y ) and (b) datasets in which at least one feature Xi has an unobserved common cause with the class Y to be predicted (Confounded). In both (a) and (b) the mechanism P (Y |Xi) can be dependent on P (Xi). For these datasets, additional data from P (X) may thus improve prediction. 2. Causal: datasets in which some features are causes of the class, and there is no feature which (a) is an effect of the class or (b) has a common cause with the class. If our assumption on independence of cause and mechanism holds, then SSL should be futile on these datasets. 3. Unclear: datasets which were difficult to be categorized to one of the aforementioned categories. Some of the reasons for that are incomplete documentation or lack of domain knowledge.\nIn practice, we count a dataset already as causal when we believe that the dependence between X and Y is mainly due to X causing Y , although additional confounding effects may be possible.\nWe first analyze the results in the benchmark chapter of a book on SSL (Tables 21.11 and 21.13 of Chapelle et al. (2006)), for the case of 100 labeled training points. The chapter compares 11 SSL methods to the base classifiers 1-NN and SVM. In [1], we give details on our subjective categorization of the eight datasets used in the chapter.\nIn view of our hypothesis, it is encouraging to see (Figure 5) that SSL does not significantly improve the accuracy in the one causal dataset, but it helps in most of the anticausal/confounded datasets. However, it is difficult to draw conclusions from this small collection of datasets; moreover, two additional issues may confound things: (1) the experiments were carried out in a transductive setting. In-\nductive methods use unlabeled data to arrive at a classifier which is subsequently applied to an unknown test set; in contrast, transductive methods use the test inputs to make predictions. This could potentially allow performance improvements independent of whether a dataset is causal or anticausal; (2) the SSL methods used cover a broad range, and were not extensions of the base classifiers; moreover, the results for the SecStr dataset are based on a different set of methods than the rest of the benchmarks.\nWe next consider 26 UCI datasets and six different base classifiers. The original results are from Tables III and IV in (Guo et al., 2010), and are presently re-analyzed in terms of the above dataset categories. The comprehensive results of Guo et al. (2010) allow us the luxury of (1) considering only self-training, which is an extension of supervised\nlearning to unlabeled data in the sense that if the set of unlabeled data is empty, we recover the results of the base method (in this case, self-training would stop at the first iteration). This lets us compare an SSL method to its corresponding base algorithm. Moreover, (2) we included only the inductive methods considered by Guo et al. (2010), and not the transductive ones (cf. our discussion above).\nThe web page [1] describes our subjective categorization of the 26 UCI datasets into Anticausal/Confounded, Causal, or Unclear. e\nIn Figure 6, we observe that SSL does not significantly decrease the error rate in the three causal datasets, but it does increase the performance in several of the anticausal/confounded datasets. This is again consistent with our hypothesis that if mechanism and input are independent, SSL will not help for causal datasets.\nSemi-supervised regression (SSR) Classification problems are often inherently asymmetric in that the inputs are continuous and the outputs categorical. It is worth reassuring that we obtain similar results in the case of regression. To this end, we consider the co-regularized least squares regression (co-RLSR) algorithm, compared to regular RLSR on 32 real-world data sets by Brefeld et al. (2006) (two of which are identical, so 31 data sets were considered). We categorized them into causal/anticausal/unclear, prior to the subsequent analysis.\nWe deemed seven of the data sets anticausal, i.e., the target variable can be considered as the cause of (some of) the predictors; Fig. 7 shows that SSR reduces the root mean square errors (RMSE) in all these cases. Nine of the remaining datasets can be considered causal, and Fig. 8 shows that there is usually little performance improvement for those. As Brefeld et al. (2006), we used the Wilcoxon signed rank test to assess whether SSR outperforms supervised regression, in the anticausal and causal cases. The null hypothesis is that the distribution of the difference between the RMSE produced by SSR and that by supervised regression is symmetric around 0 (i.e., that SSR does not help). On the anticausal datasets, the p-value is 0.0156, while it is 0.6523 on the causal datasets. Therefore, we reject the null hypothesis in the anticausal case at a 5% significance level, but not in the causal case."}, {"heading": "6. Conclusion", "text": "If one is interested in predicting one variable from another one, it helps to know the causal structure underlying the variables. We give an overview of the implication of prediction in causal and anticausal directions, in particular formulating the hypothesis that under an independence assumption for causal mechanism and input, semi-supervised learning works better in anticausal or confounded problems than in causal problems. Our preliminary meta-analysis of\nresults from the literature seems to support this claim.\nAcknowledgement We thank Ulf Brefeld and Stefan Wrobel who kindly shared their detailed experimental results with us, allowing for our meta-analysis. We thank Bob Williamson, Vladimir Vapnik, and Jakob Zscheischler for helpful discussions.\nReferences 1. http://pl.is.tue.mpg.de/p/causal-anticausal.\nBrefeld, U., Ga\u0308rtner, T., Scheffer, T., and Wrobel, S. Efficient co-regularised least squares regression. In ICML, 2006.\nChapelle, O., Scho\u0308lkopf, B., and Zien, A. Semi-Supervised Learning. MIT Press, Cambridge, MA, USA, 2006.\nDanius\u030cis, P., Janzing, D., Mooij, J., Zscheischler, J., Steudel, B., Zhang, K., and Scho\u0308lkopf, B. Inferring deterministic causal relations. In UAI, 2010.\nGuo, Y., Niu, X., and Zhang, H. An extensive empirical study on semi-supervised learning. In ICDM, 2010.\nHoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., and Scho\u0308lkopf, B. Nonlinear causal discovery with additive noise models. In NIPS, 2009.\nJanzing, D. and Scho\u0308lkopf, B. Causal inference using the algorithmic Markov condition. IEEE Transactions on Information Theory, 56(10):5168\u20135194, 2010.\nLemeire, J. and Dirkx, E. Causal models as minimal descriptions of multivariate systems. http://parallel.vub.ac.be/\u223cjan/, 2007.\nMatthews, R. Storks deliver babies (p= 0.008). Teaching Statistics, 22(2):36\u201338, 2000.\nMooij, J., Janzing, D., Peters, J., and Scho\u0308lkopf, B. Regression by dependence minimization and its application to causal inference in additive noise models. In ICML, 2009.\nPearl, E. and Bareinboim, E. Controlling selection bias in causal inference. In Proceedings of the 25th AAAI Conference on Artificial Intelligence, San Francisco, pp. 247\u2013254, 2011.\nPearl, J. Causality. Cambridge University Press, 2000.\nReichenbach, H. The direction of time. University of California Press, Berkeley, 1956.\nScho\u0308lkopf, B., Janzing, D., Peters, J., and Zhang, K. Robust learning via cause-effect models. arXiv:1112.2738v1 [stat.ML], 2011.\nSchweikert, G., Widmer, C., Scho\u0308lkopf, B., and Ra\u0308tsch, G. An empirical analysis of domain adaptation algorithms for genomic sequence analysis. In NIPS, 2009.\nSpirtes, P., Glymour, C., and Scheines, R. Causation, prediction, and search. Springer-Verlag. (2nd edition MIT Press 2000), 1993.\nStorkey, A. When training and test sets are different: characterizing learning transfer. In Dataset Shift in Machine Learning. MIT Press, 2009.\nSugiyama, M. and Kawanabe, M. Machine Learning in Non-Stationary Environment. MIT Press, Cambridge, MA, 2012.\nZhang, K. and Hyva\u0308rinen, A. On the identifiability of the post-nonlinear causal model. In UAI, 2009."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.", "creator": "LaTeX with hyperref package"}}}