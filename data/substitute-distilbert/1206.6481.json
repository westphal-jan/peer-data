{"id": "1206.6481", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Cross Language Text Classification via Subspace Co-regularized Multi-view Learning", "abstract": "in many multilingual text classification ways, the documents containing different languages always share the same set definition classes. to reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. in this paper we develop multiple comprehensive subspace co - product multi - view learning method for cross language text classification. this product is centered on parallel corpora produced by machine translation. it jointly minimizes the training dimension of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. our empirical study on a large set of binary language document classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi - panel learning devices.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (154kb)", "http://arxiv.org/abs/1206.6481v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["yuhong guo", "min xiao"], "accepted": true, "id": "1206.6481"}, "pdf": {"name": "1206.6481.pdf", "metadata": {"source": "CRF", "title": "Cross Language Text Classification via Subspace Co-Regularized Multi-View Learning", "authors": ["Yuhong Guo", "Min Xiao"], "emails": ["yuhong@temple.edu", "minxiao@temple.edu"], "sections": [{"heading": "1. Introduction", "text": "With the rapid growth of multilingual data in all aspects of human society, it is very common that documents in different languages share the same set of categories. In such multilingual learning scenarios, applying standard monolingual classification methods directly requires costly and time-consuming document annotation in each language. Thus developing effective cross language text classification methods, which transfer the categorization knowledge in a label-rich language, source language, to assist classifications in\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\na label-scarce language, target language, is becoming increasingly important.\nPrevious work on cross language text classification mainly focuses on the use of automatic machine translation technology. Most of these methods translate documents from the source language to the target language or vice versa, and then apply standard monolingual classification methods. However, due to the difference in language and culture, there exists a word drift problem. That is, while a word frequently appears in one language, its translated version may rarely appear in the other language. This creates a data distribution discrepancy between the translated training documents from the source language and the original testing documents in the target language, which poses a standard domain adaptation problem. Although many domain adaptation methods can be used in cross language text classification on the top of machine translation, e.g., the work in (Shi et al., 2010; Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011), they nevertheless suffer from the information loss and translation error introduced in machine translation process without direct access to the original documents. Multi-view learning methods on the other hand treat each language as one independent view of the data and use both the translated documents and the original documents in each language for text classification (Wan, 2009; Amini et al., 2009; Amini & Goutte, 2010).\nIn this paper, we propose a novel subspace coregularized multi-view learning method to address cross language text classification based on machine translation. Our assumption is that a document and its translated version describe the same data object in two different views. The underlying discriminative subspace representations of the same data object in the two views thus should be very similar regarding the same classification task. We then simultaneously train two different classifiers, one for each language, by formulating a semi-supervised optimization prob-\nlem that minimizes the training losses on the labeled data in both views and penalizes the distance between the two projected subspace representations of all data objects. We develop a gradient descent optimization algorithm with curvilinear search to solve the proposed optimization problem for a local optimal solution. Our extensive empirical study on a large number of cross language text classification tasks suggests the proposed approach consistently outperforms a number of comparison inductive methods, domain adaptation methods, and multi-view learning methods."}, {"heading": "2. Related Work", "text": "Previous work on cross language text classification mostly relied on machine translation methods, by translating the test data into the language of the training data or vice versa, so that classification algorithms for monolingual texts can be applied (Bel et al., 2003; Shanahan et al., 2004). Although simple and intuitive, these methods suffer from the error and noise introduced in machine translation and the discrepancy of data distribution across languages (Shi et al., 2010). Various methods have been proposed to tackle these issues on translated data to increase cross language text classification accuracy, including an information bottleneck method (Ling et al., 2008), EM-based model translation techniques (Shi et al., 2010; Rigutini & Maggini, 2005), and cross language domain adaptation methods (Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011).\nDomain adaptation refers to the problem of adapting a prediction model trained on data from a source domain to a different target domain, where the data distributions in the two domains are different. Effective domain adaptation techniques are essential when labeled data are scarce or barely available in the target domain while there are plenty labeled instances in the source domain. A major challenge of domain adaptation is the data distribution divergence between the source and target domains. Some domain adaptation methods attempt to bridge the distribution gap between the two domains by conducting instance weighting (Bickel et al., 2007) or co-training (Chen et al., 2011). Many others propose to reduce the domain divergence by learning generalizable features from the two domains, including structural correspondence learning (Blitzer et al., 2006), coupled subspace learning (Blitzer et al., 2011), and feature augmentation methods, easyadapt (EA) (Daume\u0301 III, 2007) and its co-regularization based semi-supervised extension (EA++) (Daume\u0301 III et al., 2010). These methods however are unsuitable for domain adaptation tasks where the feature spaces of the\ntwo domains are different. Nevertheless, with machine translation, the cross language text classification problem naturally forms a domain adaptation problem, where the source domain includes the documents translated from the source language and the target domain includes the original documents in the target language. The domain divergence in this case mainly comes from the word drift due to the differences of culture and linguistic expression in different language regions. Many existing domain adaptation methods can be used for cross-lingual text classification. (Wei & Pal, 2010; Prettenhofer & Stein, 2010) use structural correspondence learning for cross language text classification. (Wan et al., 2011) presents a feature and instance bi-weighting adaptation method for cross language text classification. These domain adaptation methods nevertheless can not directly exploit the original documents existing in the source language and thus suffer from the information loss introduced in machine translation process.\nRecently, multi-view learning methods in combination with machine translation have been applied on multilingual learning scenarios, including cross language text classification tasks. Using machine translation, documents in each language can be translated into parallel documents in the other language to create two independent views of the text objects in different feature spaces. A few multi-view learning methods then have been applied on such multi-view data, including the co-training method (Wan, 2009) which is an instance of the standard co-training algorithm of (Blum & Mitchell, 1998), the multi-view majority voting method (Amini et al., 2009), and the multi-view coclassification method (Amini & Goutte, 2010) which is an instance of the co-regularized multi-view classification (Sindhwani et al., 2002; Sindhwani & Rosenberg, 2008). These multi-view learning methods can exploit the original documents in both languages without translation information loss."}, {"heading": "3. Cross Language Text Classification", "text": "In this work, we combine the domain adaptation intuition of learning generalizable feature representations with the co-regularization principle of multi-view learning, and develop a subspace co-regularized multiview method for cross language text classification. For simplicity, we consider binary classification tasks. We assume there are documents in two languages, the source language and the target language, for the same classification task. We exploit the data in the labelrich source language to assist training classifiers for the data in the label-scarce target language on top of ma-\nchine translation. In this section, we first introduce the basic notations, and then present the proposed multiview learning method."}, {"heading": "3.1. Notations", "text": "Assume there are ns documents in the source language where ls of them are labeled and the remaining us documents are unlabeled. Similarly, assume there are nt documents in the target language where lt (lt < ls) of them are labeled and the remaining ut documents are unlabeled. Using machine translation, we can translate each document in the source language into a parallel document in the target language, and vice versa. Combing the original and translated data together in each language, we obtain two parallel matrices, X1 \u2208 IR n\u00d7d1 in the source view and X2 \u2208 IR n\u00d7d2 in the target view, where n = ns + nt. The first l = ls + lt rows of X1 and X2 form the labeled submatrices, X\u21131 and X \u2113 2, respectively. Their corresponding labels are given as a column vector y \u2208 {\u22121,+1}l."}, {"heading": "3.2. Multi-View Training via Subspace Co-Regularization", "text": "We assume there is a low-dimensional subspace representation of the data in each view. The linear predictive function in the ith view is derived from the subspace as follows\nfi(X \u2113 i ) = X \u2113 i\u0398iwi + bi (1)\nwhere wi \u2208 IR m is the linear weight vector, bi \u2208 IR is the bias parameter, \u0398i \u2208 IR di\u00d7m is the linear transformation matrix that projects the input data into the low-dimensional subspace, and m is the dimensionality of the subspace. The transformation matrix \u0398i has orthogonal columns such that \u0398\u22a4i \u0398i = I where I is an identity matrix. Since the same classification task is shared between the two views, the underlying predictive subspace representations of the parallel documents in the two views should be very similar. We thus formulate the cross language text classification as a semi-supervised multi-view optimization problem that minimizes the training losses on the labeled data in each view while penalizing the distance between the two view subspace representations of both labeled and unlabeled data. Specifically, we conduct training by minimizing the following regularized loss over the model parameters {\u0398i,wi, bi} 2 i=1,\n2\u2211\ni=1\nV ( fi(X \u2113 i ),y ) + \u03b1i\u2016wi\u2016 2 + \u03b3d(X1\u03981, X2\u03982),\nsubject to the constraints \u0398\u22a41 \u03981 = I and \u0398 \u22a4 2 \u03982 = I. Here V(\u00b7, \u00b7) is a general loss function, d(\u00b7, \u00b7) is a dis-\ntance function that measures the distance between the two projected low-dimensional matrices, {\u03b1i} 2 i=1 and \u03b3 are tradeoff parameters. By conducting two-view semi-supervised training, we expect the subspace representations can capture both the task specific discriminative information of the labeled data and the underlying intrinsic information of the unlabeled data.\nIn this work, we consider a least square loss function and a squared Euclidean distance function, i.e.,\nV ( fi(X \u2113 i ),y ) = \u2016X\u2113i\u0398iwi + bi \u2212 y\u2016 2 (2)\nd(X1\u03981, X2\u03982) = \u2016X1\u03981 \u2212X2\u03982\u2016 2 F (3)\nwhere \u2016 \u00b7 \u20162F denotes the Frobenius norm of matrix. Hence we get the following optimization problem\nmin {\u0398i,wi,bi}\n2\u2211\ni=1\n\u2016X\u2113i\u0398iwi + bi \u2212 y\u2016 2 + \u03b1i\u2016wi\u2016 2\n+\u03b3\u2016X1\u03981 \u2212X2\u03982\u2016 2 F (4)\ns. t. \u0398\u22a41 \u03981 = I, \u0398 \u22a4 2 \u03982 = I.\nBelow we show that the optimal {wi, bi} can be solved in terms of \u03981 and \u03982 from the optimization problem.\nLemma 1 The optimal {w\u2217i , b \u2217 i } 2 i=1 that solve the optimization problem in Eq. (4) is given by\nw\u2217i = (\u0398 \u22a4 i X \u2113\u22a4 i HX \u2113 i\u0398i + \u03b1iI) \u22121\u0398\u22a4i X \u2113\u22a4 i Hy (5)\nb\u2217i = 1\nl 1\u22a4(y \u2212X\u2113i\u0398iw \u2217 i ) (6)\nfor i = 1, 2, where H = I \u2212 1 l 11\u22a4 and 1 denotes a column vector of length l with all 1 entries.\nProof: Taking the derivatives of the objective function in Eq. (4) with respect to b1 and b2 respectively, and setting them to zeros, we obtain\nbi = 1\nl 1\u22a4(y \u2212X\u2113i\u0398iwi)\nfor i = 1, 2. Substituting them back into Eq. (4), we have a new objective function as below\n2\u2211\ni=1\n\u2225 \u2225 \u2225 H(X\u2113i\u0398iwi \u2212 y) \u2225 \u2225 \u2225 2 +\u03b1i\u2016wi\u2016 2+\u03b3 \u2225 \u2225 \u2225 X1\u03981\u2212X2\u03982\u2016 2 F\nThen taking derivatives of this new objective function with respect to w1 and w2, and setting them to zeros, we obtain\nwi = (\u0398 \u22a4 i X \u2113\u22a4 i HX \u2113 i\u0398i + \u03b1iI) \u22121\u0398\u22a4i X \u2113\u22a4 i Hy\nfor i = 1, 2.\nFollowing Lemma 1, the objective function in Eq. (4) can be rewritten as below by replacing {wi, bi}\nL(\u03981,\u03982) = \u03b3 \u2225\u2225X1\u03981 \u2212X2\u03982\u20162F + 2y\u22a4Hy (7)\n\u2212\n2\u2211\ni=1\nz\u22a4i \u0398i(\u0398 \u22a4 i Mi\u0398i + \u03b1iI) \u22121\u0398\u22a4i zi\nwhere Mi and zi are defined as\nMi = X \u2113\u22a4 i HX \u2113 i and zi = X \u2113\u22a4 i Hy.\nHence the optimization problem in Eq. (4) can be equivalently re-expressed as\nmin \u03981,\u03982\nL(\u03981,\u03982) s. t. \u0398 \u22a4 1 \u03981 = I, \u0398 \u22a4 2 \u03982 = I. (8)\nThe problem above is a non-convex optimization problem. Nevertheless, the gradient of the objective function with respect to {\u03981,\u03982} can be easily computed, and its part corresponding to each \u0398i is given as\n\u2207\u0398iL(\u03981,\u03982) = 2\u03b3X \u22a4 i (Xi\u0398i \u2212Xi\u0304\u0398i\u0304) (9)\n\u2212 2ziz \u22a4 i \u0398i(\u0398 \u22a4 i Mi\u0398i + \u03b1iI) \u22121 + 2Mi\u0398i(\u0398 \u22a4 i Mi\u0398i + \u03b1iI) \u22121\n\u0398\u22a4i ziz \u22a4 i \u0398i (\u0398 \u22a4 i Mi\u0398i + \u03b1iI) \u22121\nfor {i = 1, i\u0304 = 2} or {i = 2, i\u0304 = 1}."}, {"heading": "3.3. Optimization Algorithm", "text": "The non-convex optimization problem (8) is generally difficult to optimize due to the orthogonal constraints. In this work, we use a gradient descent optimization procedure with curvilinear search (Wen & Yin, 2010) to solve it for a local optimal solution.\nIn each iteration of the gradient descent procedure, given the current feasible point (\u03981,\u03982), the gradients can be computed using (9), such that\nG1 = \u2207\u03981L(\u03981,\u03982), G2 = \u2207\u03982L(\u03981,\u03982). (10)\nWe then compute two skew-symmetric matrices\nF1 = G1\u0398 \u22a4 1 \u2212\u03981G \u22a4 1 , F2 = G2\u0398 \u22a4 2 \u2212\u03982G \u22a4 2 . (11)\nIt is easy to see F\u22a41 = \u2212F1 and F \u22a4 2 = \u2212F2. The next new point can be searched as a curvilinear function of a step size variable \u03c4 , such that\nQ1(\u03c4) = ( I + \u03c4\n2 F1\n)\u22121( I \u2212 \u03c4\n2 F1\n) \u03981 (12)\nQ2(\u03c4) = ( I + \u03c4\n2 F2\n)\u22121( I \u2212 \u03c4\n2 F2\n) \u03982 (13)\nIt is easy to verify that Q1(\u03c4) \u22a4Q1(\u03c4) = I and Q2(\u03c4) \u22a4Q2(\u03c4) = I for all \u03c4 \u2208 IR. Thus we can stay\nin the feasible region along the curve defined by \u03c4 . Moreover, d\nd\u03c4 Q1(0) and d d\u03c4 Q2(0) are equal to the pro-\njections of (\u2212G1) and (\u2212G2) onto the tangent space Q = {(\u03981,\u03982) : \u0398 \u22a4 1 \u03981 = I,\u0398 \u22a4 2 \u03982 = I} at the current point (\u03981, \u03982). Hence {Q1(\u03c4), Q2(\u03c4)}\u03c4\u22650 is a descent path in the close neighborhood of the current point. We thus apply a similar strategy as the standard backtracking line search to find a proper step size \u03c4 using curvilinear search, while guaranteeing the iterations to converge to a stationary point. We determine a proper step size \u03c4 as one satisfying the following Armijo-Wolfe conditions\nL(Q1(\u03c4), Q2(\u03c4)) \u2264 L(Q1(0), Q2(0)) (14)\n+\u03c11\u03c4L \u2032 \u03c4 (Q1(0), Q2(0)),\nL\u2032\u03c4 (Q1(\u03c4), Q2(\u03c4)) \u2265 \u03c12L \u2032 \u03c4 (Q1(0), Q2(0)) (15)\nHere L\u2032\u03c4 (Q1(\u03c4), Q2(\u03c4)) is the derivative of L with respect to \u03c4 ,\nL\u2032\u03c4 (Q1(\u03c4), Q2(\u03c4)) (16)\n= \u2212 2\u2211\ni=1\ntr ( Ri(\u03c4) \u22a4 ( I + \u03c4\n2 Fi\n)\u22121 Fi (\u0398i +Qi(\u03c4) 2 ))\nwhere Ri(\u03c4) = \u2207\u0398iL(Q1(\u03c4), Q2(\u03c4)). Therefore\nL\u2032\u03c4 (Q1(0), Q2(0)) = \u2212 2\u2211\ni=1\ntr ( G\u22a4i (Gi\u0398 \u22a4 i \u2212\u0398iG \u22a4 i )\u0398i )\n= \u2212 1\n2 \u2016F1\u2016\n2 F \u2212 1\n2 \u2016F2\u2016\n2 F (17)\nThe overall algorithm is given in Algorithm 1."}, {"heading": "3.4. Multi-View Testing", "text": "After the semi-supervised multi-view training, we obtain two prediction models defined in Eq. (1) with model parameters {\u0398i,wi, bi} 2 i=1. We then conduct multi-view testing on new documents. Specifically, given a test document, x \u2208 IRd2 , in the target language, we first translate it into the source language to obtain x\u0302 \u2208 IRd1 . Then we compute the prediction values using the two prediction models\nf1(x\u0302) = x\u0302 \u22a4\u03981w1 + b1, (18) f2(x) = x \u22a4\u03982w2 + b2. (19)\nThe prediction confidence of each predictor can be calculated as |f1(x\u0302)| and |f2(x)| respectively. We finally set the prediction label for x as the one predicted from the most confident predictor, i.e.,\ny =\n{ sign(f1(x\u0302)) if |f1(x\u0302)| > |f2(x)|\nsign(f2(x)) otherwise (20)\nAlgorithm 1 Optimization procedure\nInput: \u01eb \u2265 0 , 0 < \u00b5 < 1, 0 < \u03c11 < \u03c12 < 1, and initial feasible \u03981, \u03982. Procedure\nfor iter = 1 to maxiters 1. compute gradients G1, G2 using Eq. (9),(10). 2. if \u2016G1\u2016 2 F + \u2016G2\u2016 2\nF \u2264 \u01eb then stop and exit. 3. compute F1, and F2 according to Eq. (11). 4. compute L\u2032\u03c4 (Q1(0), Q2(0)) via Eq. (17) 5. set \u03c4 = 1 6. for s = 1 to maxsteps\n-compute Q1(\u03c4), Q2(\u03c4) via Eq. (12)(13) -compute L\u2032\u03c4 (Q1(\u03c4), Q2(\u03c4)) via Eq. (16) -if Armijo-Wolfe conditions in Eq. (14) (15)\nare satisfied then break-out -set \u03c4 = \u00b5\u03c4\nend for\n7. if s > maxsteps then stop and exit. 7. update \u03981 = Q1(\u03c4), \u03982 = Q2(\u03c4).\nend for"}, {"heading": "4. Experiments", "text": "In this section, we report our empirical results on a set of cross language text classification tasks."}, {"heading": "4.1. Experimental Setting", "text": "The experiments were conducted on cross language text classification (CLTC) tasks constructed from a comparable multilingual corpus used in (Amini et al., 2009), which contains newswire articles written in 5 languages (English(E), French(F), German(G), Italian(I), Spanish(S)), distributed over 6 classes (C15, CCAT, E21, ECAT, GCAT, M11). In this multilingual corpus, each original document was translated into the other 4 languages using a statistical machine translation system. Our first set of experiments aim to evaluate CLTC tasks with different languages. We constructed a set of 20 binary cross language classification tasks over all possible source-target pairs of 5 languages, using two large classes, CCAT and ECAT, as shown in Table 1. For example, E2F denotes the task that uses English as the source language and uses French as the target language. For each language, we randomly selected 2000 original documents for each class to form the datasets. Thus in each task, we used 4000 original documents and 4000 translated documents in each language. In each language, we used the top 400 features according to the sums of their TFIDF weights over all documents.\nNext, we constructed datasets to evaluate CLTC tasks on different classes. We selected 3 languages,\nFrench(F), German(G) and Italian(I), that have sufficient number of documents in all 6 classes to use. We then constructed 36 1-vs-all binary classification problems over all 6 classes using 6 source-target pairs of languages, as shown in Table 2. For example, F2G denotes the tasks that use French as the source language and use German as the target language; C15 denotes the 1-vs-all binary classification task on class C15. For each task, we randomly selected 2000 original documents from both the target class and the remaining classes in each language. Thus same as above, we used 4000 original documents and 4000 translated documents for each task in each language.\nIn the experiments, we compared the proposed Subspace Co-regularized Multi-View learning method (SCMV) method with five other methods: (1) TB, a baseline method that trains a classifier using only the labeled original documents in the target language; (2) TSB, a baseline method that trains a classifier on both the labeled original documents in the target language and the labeled documents translated from the source language; (3) EA++, the co-regularization based semi-supervised domain adaptation method developed in (Daume\u0301 III et al., 2010), which uses a synthetic source domain formed by translating all documents in the source language into the target language; (4) MVMV, the multi-view majority voting method developed in (Amini et al., 2009); and (5) MVCC, the semi-supervised version of the multiview co-classification method (Amini & Goutte, 2010), which penalizes the disagreement of the two view predictions on unlabeled data. Among these methods, only the MVCC uses a logistic regression predictor as base classifier, and all other methods use least squares predictors as base classifiers."}, {"heading": "4.2. Experiment I", "text": "The first set of experiments are conducted on the 20 CLTC tasks constructed above. For each task, we randomly chose 900 labeled and 2100 unlabeled original documents from the source language domain, and chose 100 labeled and 2900 unlabeled original documents from the target language domain for classification model training. Thus in total we had 1000 labeled documents and 5000 unlabeled documents in each language view for training. We used the remaining 1000 original documents in the target language as testing data. Based on this random data partition procedure, we repeated the E2F experiment 3 times to conduct model parameter selection for MVCC and the proposed SCMV. The trade-off parameter for MVCC is selected from {1/10, 1/2, 1}. For the proposed SCMV, we used \u03b11 = 0.1, \u03b12 = 0.1 and selected\n\u03b3 from {1/600, 1/60, 1/30, 1/12, 1/6}. Finally we selected 1/2 as the trade-off parameter for MVCC and selected 1/6 as the \u03b3 parameter for SCMV.\nUsing the selected parameters and the stated random data partition procedure, we then set the subspace dimension for SCMV as 10, and repeated each experiment 10 times for the six methods in consideration. The average classification results on the test data in term of accuracy are reported in Table 1. We can see that between the two baseline methods, by exploiting the translated labeled documents from the source language, TSB has slight advantages over TB on many tasks. The domain adaptation method, EA++, however, produced similar performance as the baseline TSB. By exploiting both original data and translated data in the two languages, even the simple multiview method, MVMV, works well on most tasks except the G2I and G2S. The semi-supervised multi-view co-classification method, MVCC, consistently outperforms the four methods mentioned above, although the improvements are not significant on a few tasks including E2G, I2G and I2S. The proposed SCMV on the other hand consistently outperforms the other five methods on all tasks. The improvements over the first four methods are significant across all tasks. Even comparing to MVCC, the improvements are significant over most tasks.\nSubspace dimensions. Next, we empirically studied the influence of subspace dimension size on the proposed SCMV method. We repeated the experiments above for SCMV with different subspace dimension sizes, m={10,20,30,40,50}. The average test accuracy results on tasks E2F,E2G,E2I and E2S are reported in Figure 1. We can see the performance of SCMV is not very sensitive to the subspace dimension size within the considered dimension range."}, {"heading": "4.3. Experiment II", "text": "The second set of experiments are conducted on the 36 CLTC tasks constructed with 1-vs-all classification problems. We used the same random data partition procedure and model parameters stated in experiment\nI. For the proposed method SCMV, we used 10 as the subspace dimension size. We repeated each experiment 10 times and the average test accuracy results are reported in Table 2. Similar to the first set of experiments, the proposed approach consistently outperforms all the other five methods on all 36 tasks, and the improvements are significant on 22 tasks comparing to the best comparison results.\nAll these results suggest that identifying two-view consistent subspace representations based on both the original and translated data in two languages can effectively overcome the cross language divergence and achieve good prediction models."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a novel subspace coregularized multi-view learning method to address cross language text classification. By training two subspace based prediction models in two language views together while penalizing the distance between the two projected subspace representations of both labeled and unlabeled instances, the underlying discriminative subspace representations can be identified to produce prediction models with better generalization performance. We developed a gradient descent algorithm with curvilinear search to solve the proposed joint optimization problem for a local optimal solution. Our extensive empirical results on a large number of cross language text classification tasks demonstrated the superior performance of the proposed method comparing to a few inductive methods, domain adaptation methods, and multi-view learning methods."}], "references": [{"title": "A co-classification approach to learning from multilingual corpora", "author": ["M. Amini", "C. Goutte"], "venue": "Machine Learning,", "citeRegEx": "Amini and Goutte,? \\Q2010\\E", "shortCiteRegEx": "Amini and Goutte", "year": 2010}, {"title": "Cross-lingual text", "author": ["N. Bel", "C. Koster", "M. Villegas"], "venue": "Neural Information Process. Systems", "citeRegEx": "Bel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bel et al\\.", "year": 2009}, {"title": "Coregularization based semi-supervised domain adaptation", "author": ["H. Daum\u00e9 III", "A. Kumar", "A. Saha"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "III et al\\.,? \\Q2010\\E", "shortCiteRegEx": "III et al\\.", "year": 2010}, {"title": "Can chinese web pages be classified with english data source", "author": ["X. Ling", "G. Xue", "W. Dai", "Y. Jiang", "Q. Yang", "Y. Yu"], "venue": "In Proc. of the international conference on World Wide Web,", "citeRegEx": "Ling et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2008}, {"title": "Cross-language text classification using structural correspondence learning", "author": ["P. Prettenhofer", "B. Stein"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Prettenhofer and Stein,? \\Q2010\\E", "shortCiteRegEx": "Prettenhofer and Stein", "year": 2010}, {"title": "An EM based training algorithm for cross-language text categorization", "author": ["L. Rigutini", "M. Maggini"], "venue": "In Proc. of the Web Intelligence Conference,", "citeRegEx": "Rigutini and Maggini,? \\Q2005\\E", "shortCiteRegEx": "Rigutini and Maggini", "year": 2005}, {"title": "Mining multilingual opinions through classification and translation", "author": ["J.G. Shanahan", "G. Grefenstette", "Y. Qu", "D.A. Evans"], "venue": "In Proc. of AAAI\u201904 Spring Symp. on Explor. Attitude and Affect in Text,", "citeRegEx": "Shanahan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shanahan et al\\.", "year": 2004}, {"title": "Cross language text classification by model translation and semisupervised learning", "author": ["L. Shi", "R. Mihalcea", "M. Tian"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "An RKHS for multiview learning and manifold co-regularization", "author": ["V. Sindhwani", "D. Rosenberg"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "Sindhwani and Rosenberg,? \\Q2008\\E", "shortCiteRegEx": "Sindhwani and Rosenberg", "year": 2008}, {"title": "A coregularization approach to semi-supervised learning with multiple views", "author": ["V. Sindhwani", "P. Niyogi", "M. Belkin"], "venue": "In Proc. of the Workshop on Learning with Multiple Views,", "citeRegEx": "Sindhwani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2002}, {"title": "Bi-weighting domain adaptation for cross-language text classification", "author": ["C. Wan", "R. Pan", "J. Li"], "venue": "In Proc. of the Interational Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Wan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2011}, {"title": "Cross lingual adaptation: an experiment on sentiment classifications", "author": ["B. Wei", "C. Pal"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Wei and Pal,? \\Q2010\\E", "shortCiteRegEx": "Wei and Pal", "year": 2010}, {"title": "A feasible method for optimization with orthogonality constraints", "author": ["Z. Wen", "W. Yin"], "venue": "Technical report, Rice University,", "citeRegEx": "Wen and Yin,? \\Q2010\\E", "shortCiteRegEx": "Wen and Yin", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": ", the work in (Shi et al., 2010; Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011), they nevertheless suffer from the information loss and translation error introduced in machine translation process without direct access to the original documents.", "startOffset": 14, "endOffset": 95}, {"referenceID": 10, "context": ", the work in (Shi et al., 2010; Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011), they nevertheless suffer from the information loss and translation error introduced in machine translation process without direct access to the original documents.", "startOffset": 14, "endOffset": 95}, {"referenceID": 6, "context": "Previous work on cross language text classification mostly relied on machine translation methods, by translating the test data into the language of the training data or vice versa, so that classification algorithms for monolingual texts can be applied (Bel et al., 2003; Shanahan et al., 2004).", "startOffset": 252, "endOffset": 293}, {"referenceID": 7, "context": "Although simple and intuitive, these methods suffer from the error and noise introduced in machine translation and the discrepancy of data distribution across languages (Shi et al., 2010).", "startOffset": 169, "endOffset": 187}, {"referenceID": 3, "context": "Various methods have been proposed to tackle these issues on translated data to increase cross language text classification accuracy, including an information bottleneck method (Ling et al., 2008), EM-based model translation techniques (Shi et al.", "startOffset": 177, "endOffset": 196}, {"referenceID": 7, "context": ", 2008), EM-based model translation techniques (Shi et al., 2010; Rigutini & Maggini, 2005), and cross language domain adaptation methods (Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al.", "startOffset": 47, "endOffset": 91}, {"referenceID": 10, "context": ", 2010; Rigutini & Maggini, 2005), and cross language domain adaptation methods (Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011).", "startOffset": 80, "endOffset": 143}, {"referenceID": 10, "context": "(Wan et al., 2011) presents a feature and instance bi-weighting adaptation method for cross language text classification.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": ", 2009), and the multi-view coclassification method (Amini & Goutte, 2010) which is an instance of the co-regularized multi-view classification (Sindhwani et al., 2002; Sindhwani & Rosenberg, 2008).", "startOffset": 144, "endOffset": 197}], "year": 2012, "abstractText": "In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}