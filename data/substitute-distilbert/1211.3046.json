{"id": "1211.3046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2012", "title": "Recovering the Optimal Solution by Dual Random Projection", "abstract": "in this work, candidates address inverse problem of how to recover the optimal solution to the specific problem related to high dimensional data classification titled random projection, to which we refer as recovery of optimal solution. this is noticeable contrast to the previous studies that were focused on assess the classification performance using random projection. we reveal the relationship between compressive sensing and the problem of recovering optimal solution using random convergence. we also present a simple algorithm, termed as dual random projection, that captures the optimal solution with a small measure by computing dual solution provided providing the gamma matrix is of low rank.", "histories": [["v1", "Tue, 13 Nov 2012 16:39:45 GMT  (26kb)", "http://arxiv.org/abs/1211.3046v1", null], ["v2", "Thu, 15 Nov 2012 22:09:06 GMT  (26kb)", "http://arxiv.org/abs/1211.3046v2", null], ["v3", "Tue, 2 Apr 2013 19:05:01 GMT  (25kb)", "http://arxiv.org/abs/1211.3046v3", null], ["v4", "Fri, 21 Feb 2014 20:57:42 GMT  (28kb)", "http://arxiv.org/abs/1211.3046v4", "The 26th Annual Conference on Learning Theory (COLT 2013)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang", "mehrdad mahdavi", "rong jin", "tianbao yang", "shenghuo zhu"], "accepted": false, "id": "1211.3046"}, "pdf": {"name": "1211.3046.pdf", "metadata": {"source": "CRF", "title": "Recovering Optimal Solution by Dual Random Projection", "authors": ["Lijun Zhang", "Rong Jin", "Tianbao Yang", "Zhang Jin Yang"], "emails": ["zhanglij@msu.edu", "rongjin@cse.msu.edu", "tyang@ge.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n21 1.\n30 46\nv1 [\ncs .L\nG ]\n1 3\nKeywords: Random projection, Primal solution, Dual solution, Low rank"}, {"heading": "1. Introduction", "text": "Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al., 2005). In this work, we focus on random projection for classification.\nMany studies were devoted to analyzing the classification performance using random projection. In this paper, we examine the effect of random projection for data classification from a very different aspect. In particular, we are interested in accurately recovering the optimal solution to the original optimization problem related to data classification using random projection. This is particularly useful for feature selection (Guyon and Elisseeff, 2003), where important features are often selected based on their weights in the linear prediction model learned from the training data. In this case, it is insufficient to simply guarantee a low classification error for the learned prediction model based on random projection. In order to ensure that similar features are selected by the prediction model based on random projection, it is important to guarantee that the recovered solution based on random projection is close to the one obtained by solving the original optimization problem without random projection.\nc\u00a9 2012 L. Zhang, R. Jin & T. Yang.\nThe rest of the draft is arranged as follows: Section 2 describes the problem of recovering optimal solution by random projection, the center of this work. Section 3 describes the dual random projection approach for reconstructing optimal solutions. Section 4 presents the main theoretical results for the proposed algorithm. Section 5 presents the proof for the theorems stated in Section 4. Section 6 concludes this work with open questions."}, {"heading": "2. The Problem of Recovering Optimal Solutions from Random Projection", "text": "Let (xi, yi), i = 1, . . . , n be a set of training examples, where xi \u2208 Rd is a vector of d dimension and yi \u2208 {\u22121,+1} is the binary class assignment for xi. Let X = (x1, . . . ,xn) and y = (y1, . . . , yn)\n\u22a4 include input patterns and the class assignments of all training examples. A classifier w \u2208 Rd is learned from the training examples by solving the following optimization problem:\nmin w\u2208Rd\n\u03bb 2 \u2016w\u20162 +\nn\u2211\ni=1\n\u2113(yix \u22a4 i w) (1)\nwhere \u2113(z) is a convex loss function that is differentiable 1. By writing \u2113(z) in its convex conjugate form, i.e.\n\u2113(z) = min \u03b1\u2208\u2126\n\u03b1z \u2212 \u2113\u2217(\u03b1),\nwhere \u2113\u2217(\u03b1) is the convex conjugate of \u2113(z) and \u2126 is a domain for dual variable \u03b1, we have the dual optimization problem\nmax \u03b1\u2208\u2126n\n\u2212 n\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bb \u03b1\n\u22a4G\u03b1 (2)\nwhere \u03b1 = (\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1n)\u22a4 and D(y) = diag(y) and G is the Gram matrix given by\nG = D(y)X\u22a4XD(y) (3)\nIn the following, we denote by w\u2217 the optimal primal solution to (1), and by \u03b1\u2217 the optimal dual solution to (2). The following proposition connects w\u2217 and \u03b1\u2217.\nProposition 1 Let w\u2217 be the optimal primal solution to (1), and \u03b1\u2217 be the optimal dual solution to (2), we have\nw\u2217 = \u2212 1\n\u03bb XD(y)\u03b1\u2217, and [\u03b1\u2217]i = \u2207\u2113\n( yix \u22a4 i w\u2217 ) , i = 1, . . . , n (4)\nThe proof of Proposition 1 and other omitted proofs are deferred to the Appendix. When dimension d is high and the number of training examples n is large, solving either the primal problem in (1) or the dual problem in (2) can be computationally expensive. To reduce the computational cost, one common approach is to significantly reduce the dimensionality by\n1. For non differentiable loss functions such as hinge loss, we could apply the smoothing technique (Nesterov, 2005) to make it differentiable.\nrandom projection. Let S \u2208 Rd\u00d7m be a Gaussian random matrix, where each entry Si,j is independently drawn from a Gaussian distribution N (0, 1) and m is significantly smaller than d. Using random matrix S, we generate a new data representation for input data points by\nx\u0302i = 1\u221a m S\u22a4xi, (5)\nand we solve the following problem in the projected space:\nmin z\u2208Rm\n\u03bb 2 \u2016z\u20162 +\nn\u2211\ni=1\n\u2113(yiz \u22a4x\u0302i) (6)\nThe corresponding dual problem is written as\nmin \u03b1\u2208\u2126n\n\u2212 n\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bb \u03b1\n\u22a4G\u0302\u03b1 (7)\nwhere\nG\u0302 = D(y)X\u22a4 SS\u22a4\nm XD(y) (8)\nRemark 2 Initially, the choice of Guassian random matrix S is justified by that the expectation of dot-product of any two examples in the projected space is equal to the dot-product in the original space, i.e.,\nE[x\u0302\u22a4i x\u0302j] = x \u22a4 i E\n[ 1\nm SS\u22a4\n] xj = x \u22a4 i xj\nwhere the last equality follows that E [ 1 m SS\u22a4 ] = I.\nLet z\u2217 denote the optimal solution to the primal problem (6) in the projected space, and \u03b1\u0302 denote the optimal dual solution to (7). Similar to Proposition 1, the following proposition, connects z\u2217 and \u03b1\u0302.\nProposition 3 We have\nz\u2217 = \u2212 1\n\u03bb 1\u221a m S\u22a4XD(y)\u03b1\u0302, and [\u03b1\u0302\u2217]i = \u2207\u2113 ( yi\u221a m x\u22a4i Sz\u2217 ) , i = 1, . . . , n (9)\nGiven the optimal solution z\u2217 \u2208 Rm, the data point x \u2208 Rd is classified by x\u22a4Sz\u2217/ \u221a m, which is equivalent to defining a new solution w\u0302 \u2208 Rd given below, to which we refer as the naive solution,\nw\u0302 = 1\u221a m Sz\u2217 (10)\nThe classification performance of w\u0302 has been examined by many studies (e.g. (Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)). The general conclusion is that when the original data is linearly separable with a large margin, the classification error for the solution based on random projection is usually small.\nAlthough these studies show that w\u0302 can achieve a small classification error under appropriate assumption, it is unclear if solution w\u0302 is a good approximation of the true optimal solution w\u2217. In fact, as we will see the result in Section 4, w\u0302 is almost guaranteed to be a BAD approximation of w\u2217 (i.e., \u2016w\u0302 \u2212w\u2217\u20162 = \u2126(\u2016w\u2217\u20162)). This observation leads to an interesting question, Is it possible to accurately recover the optimal solution w\u2217 based on z\u2217, the random projection based solution. We refer to this problem as Recovery of Optimal Solution.\nRelationship to Compression Sensing The proposed problem is closely related to compressive sensing (Cande\u0300s and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections. The key difference between our work and compressive sensing is that we don\u2019t have the direct access to the random measurement of the target vector (which in our case is w\u2217). Instead, z\u2217 is the optimal solution to (6), the primal problem using random projection. However, the following Theorem shows that z\u2217 is a good approximation of S \u22a4w\u2217/ \u221a m, which includes m random measurements of w\u2217, if the data matrix X is of low rank and the number of random measurements m is sufficiently large.\nTheorem 1 With a probability at least 1\u2212 \u03b4 \u2212 exp(\u2212m/32), we have\n\u2016\u221amz\u2217 \u2212 S\u22a4w\u2217\u20162 \u2264 2 \u221a 2\u03b5\u221a\n1\u2212 \u03b5\u2016S \u22a4w\u2217\u20162\nprovided\nm \u2265 r(log(r 2 + r) + log(1/\u03b4))\nc\u03b52\nwhere constant c is at least 1/32, and r is the rank of X.\nGiven the approximation bound in Theorem 1, it is appealing to reconstruct w\u2217 using the compressive sensing algorithm provided that w\u2217 is sparse to certain bases. We note that the low rank assumption for data matrix X implies that w\u2217 is sparse with respect to the singular vector system of X. However, since z\u2217 only provides an approximation to the random measurements of w\u2217, running the compressive sensing algorithm will not be able to perfectly recover w\u2217 from z\u2217. In Section 3, we present an algorithm, that recovers w\u2217 with a small error provided that the data matrix X is of low rank. Compared to the compressive sensing algorithm, the main advantage of the proposed algorithm is its computational simplicity because it does not need to compute the eigenvectors of X and solve an optimization problem that minimizes the \u21131 norm."}, {"heading": "3. Algorithm", "text": "To motivate our algorithm, let us revisit the optimal primal solution w\u2217 to (1), which is given in Proposition 1, i.e.,\nw\u2217 = \u2212 1\n\u03bb XD(y)\u03b1\u2217, (11)\nAlgorithm 1 A Dual Random Projection Approach for Recovering Optimal Solution\n1: Input: input patterns X \u2208 Rd\u00d7n, binary class assignment y \u2208 {\u22121,+1}n, and sample size m 2: Sample a Gaussian random matrix S \u2208 Rd\u00d7m 3: Compute the projected data matrix as X\u0302 = S\u22a4X/ \u221a m. 4: Compute \u03b1\u0302 by solving the primal problem (6) and constructing \u03b1\u0302 by Proposition 3. 5: Output: the recovered solution w\u0303 = \u2212XD(y)\u03b1\u0302/\u03bb\nwhere \u03b1\u2217 is the optimal solution to the dual problem (2). Given the projected data x\u0302 = S\u22a4x/ \u221a m, we have reached an approximate dual problem in (7). Comparing it with the dual problem in (2), and noticing that E[SS\u22a4/m] = I. As a result, when the number of random projections m is sufficiently large, we would expect \u03b1\u0302 to be close to \u03b1\u2217. As a result, we can use \u03b1\u0302 as an approximate of \u03b1\u2217 in (11), which yields a recovered prediction model, denoted by w\u0303:\nw\u0303 = \u2212 1 \u03bb XD(y)\u03b1\u0302 = \u2212\nn\u2211\ni=1\n1 \u03bb yi[\u03b1\u0302]ixi (12)\nRemark 4 Note that the key difference between the recovered solution w\u0303 and the naive solution w\u0302 is that w\u0302 is computed by projecting the optimal primal solution z\u2217 in the projected space back to the original space via S, while w\u0303 is computed directly in the original space using the approximate dual solution \u03b1\u0302. As a result, the naive solution w\u0302 lies in the subspace spanned by the column vectors in random matrix S (denoted by AS), while the recovered solution w\u0303 lies in the subspace that also contains the optimal solution w\u2217, i,e., the subspace spanned by columns of X (denoted by A). The mismatch between spaces AS and A leads to the large approximation error for w\u0302.\nAlgorithm 1 shows the steps of the proposed method. We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.\nTo further reduce the recovery error, we develop an iterative method shown in Algorithm 2. The intuition comes from that if \u2016w\u2217 \u2212 w\u0303\u20162 \u2264 \u01eb\u2016w\u2217\u20162 with a small \u01eb, we can apply the same dual random projection algorithm again to recover \u2206w = w\u2217 \u2212 w\u0303, which should result in a recovery error of \u2016\u2206w\u20162 \u2264 \u01eb2\u2016w\u2217\u20162. This simple intuition leads to an iterative method shown in Algorithm 2. If we repeat the process with T iterations, we should be able to obtain a solution with a recovery error of \u01ebT . In Algorithm 2, at iteration t, given the recovered solution w\u0303t\u22121 obtained from the previous iteration, we then solve the optimization problem in (13) that is designed to recover w\u2217 \u2212 w\u0303t\u22121.\nRemark 5 It is important to note that although Algorithm 2 is consisted of multiple iterations, the random projection of the data matrix is only computed once before the start of the iterations. This important feature makes the iterative algorithm computationally attractive\nAlgorithm 2 An Iterative Dual Random Projection Approach for Recovering Optimal Solution\n1: Input: input patterns X \u2208 Rd\u00d7n, binary class assignment y \u2208 {\u22121,+1}n, sample size m, and number of iterations T 2: Sample a Gaussian random matrix S \u2208 Rd\u00d7m 3: Compute the projected data matrix as X\u0302 = (x\u03021, . . . , x\u0302n) = S \u22a4X/ \u221a m. 4: Initialize w\u03030 = 0 5: for t = 1, . . . , T do 6: Obtain zt\u2217 \u2208 Rm by solving the following optimization problem\nzt\u2217 = argmin z\u2208Rm\n\u03bb\n2\n\u2225\u2225\u2225z+ S\u22a4w\u0303t\u22121/ \u221a m \u2225\u2225\u2225 2\n2 +\nn\u2211\ni=1\n\u2113 ( yiz \u22a4x\u0302i + yiw\u0303 \u22a4 t\u22121xi ) (13)\n7: Compute the dual solution a\u0303t using\n[a\u0303t]i = \u2207\u2113 ( yix\u0302 \u22a4 i z t \u2217 + yiw\u0303 \u22a4 t\u22121xi )\n8: Update the solution by w\u0303t = \u2212 \u2211n i=1 yi[a\u0303 t]ixi/\u03bb\n9: end for 10: Output the recovered solution w\u0303T\nas calculating random projections of large data matrix is computationally expensive and has been the subject of many studies, e.g., (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010). However, it is worth noting that in Algorithm 2 at each iteration, we need to compute the dot-product w\u0303\u22a4t xi for all training data in the original space. We also note that Algorithm 2 is related to the Epoch gradient descent algorithm (Hazan and Kale, 2011) for stochastic optimization in that the solution obtained in the previous iteration is served as the center to the optimization problem of the current iteration. Unlike the algorithm in (Hazan and Kale, 2011), we do not shrink the domain size over the iterations in Algorithm 2."}, {"heading": "4. Main Results", "text": "In this section, we will present a bound of the recovery error \u2016w\u2217\u2212w\u0303\u20162 for the dual random projection algorithm. We will then extend the result to the iterative algorithm. Similar to compressive sensing, we need to assume certain sparse structure for the recovery problem. In our case, we assume that the data matrix X is of low rank. We note that the low rank assumption is closely related to the sparsity assumption made in compressive sensing. This is because w\u2217 lies in the subspace spanned by the column vectors of X and the low rank assumption of X directly implies that w\u2217 is sparse with respect to the eigen system of X.\nWe denote by r the rank of matrix X. The following theorem shows that the recovery error of Algorithm 1 is small provided that (1) X is of low rank (i.e., r \u226a min(d, n)), and (2) sufficiently large number of random projections.\nTheorem 2 Let w\u2217 be the optimal solution to (1) and let w\u0303 be the solution recovered by Algorithm 1. Then, with a probability at least 1\u2212 \u03b4, we have\n\u2016w\u2217 \u2212 w\u0303\u20162 \u2264 2\u03b5\n1\u2212 \u03b5\u2016w\u2217\u20162\nprovided\nm \u2265 r(log(r 2 + r) + log(1/\u03b4))\nc\u03b52\nwhere constant c is at least 1/32.\nRemark 6 According to Theorem 2, the number of required random projections is \u2126(r log r). This is similar to compressive sensing result if we view rank r as the sparsity measure used in compressive sensing. Following the same arguments as compressive sensing, it may be possible to argue that \u2126(r log r) is optimal due to the result of coupon collector\u2019s problem (Mowani and Raghavan, 1995), although the rigorous analysis remains to be developed.\nAs a comparison, the following theorem shows that with a high probability, the naive solution w\u0302 given in (10) (i.e., the solution based on random projection without exploiting the dual variables) does not accurately recover the true optimal solution w\u2217.\nTheorem 3 With a probability 1\u2212 exp(\u2212(d\u2212 r)/32) \u2212 exp(\u2212m/32) \u2212 \u03b4, we have\n\u2016w\u0302 \u2212w\u2217\u20162 \u2265 \u221a\nd\u2212 r m\n( 1 2 \u2212 \u03b5 \u221a 2(1 + \u03b5) 1\u2212 \u03b5 ) \u2016w\u2217\u20162\nprovided\nm \u2265 r(log(r 2 + r) + log(1/\u03b4))\nc\u03b52\nRemark 7 As indicated by Theorem 3, when m is sufficiently larger than r but significantly smaller than d, we have \u2016w\u0302 \u2212w\u2217\u20162 = \u2126( \u221a d/m\u2016w\u2217\u20162), indicating that w\u0302 does not approximate w\u2217 well.\nIt is important to note that Theorem 3 does not contradict with the previous results showing that the random projection method could result in a small classification error if the data set is almost linearly separable with a large margin. This is because, to decide if w\u0302 carries similar classification performance as w\u2217, we need to measure the following term\nmax x\u2208span(X),\u2016x\u20162\u22641\n|x\u22a4(w\u0302 \u2212w\u2217)| (14)\nSince \u2016w\u0302 \u2212w\u2217\u20162 can also be written as\n\u2016w\u0302 \u2212w\u2217\u20162 = max \u2016x\u20162\u22641 x\u22a4(w\u0302 \u2212w\u2217)\nthe quantity defined in (14) could be significantly smaller than \u2016w\u0302 \u2212w\u2217\u20162 if data matrix X is of low rank. The following theorem quantifies this statement.\nTheorem 4 With a probability at least 1\u2212 \u03b4, we have\nmax x\u2208span(X),\u2016x\u20162\u22641\nx\u22a4(w\u2217 \u2212 w\u0302) \u2264 \u03b5 ( 1 + 2\n1\u2212 \u03b5\n) \u2016w\u2217\u20162\nprovided\nm \u2265 r(log(r 2 + r) + log(1/\u03b4))\nc\u03b52\nwhere constant c is at least 1/32.\nThe proof of Theorem 4 can be found in Appendix. We note that Theorem 4 directly implies the result of margin classification error for random projection (Blum, 2006). This is because when a data point (xi, yi) can be separated by w\u2217 with a margin \u03b3, i.e. yiw \u22a4 \u2217 xi \u2265 \u03b3|w\u2217|, it\nwill be classified by w\u0302 with a margin at least \u03b3\u2212 ( 1 + 2(1+\u03b5)1\u2212\u03b5 ) \u03b5 provided \u03b3 > ( 1 + 2(1+\u03b5)1\u2212\u03b5 ) \u03b5.\nUsing Theorem 2, we now state the recovery result for the iterative method in Algorithm 2.\nTheorem 5 Let w\u2217 be the optimal solution to (1) and let w\u0303T be the solution recovered by Algorithm 2. Then, with a probability at least 1\u2212 \u03b4, we have\n\u2016w\u2217 \u2212 w\u0303T \u20162 \u2264 ( 2\u03b5\n1\u2212 \u03b5\n)T \u2016w\u2217\u20162\nprovided\nm \u2265 r(log(r 2 + r) + log(T/\u03b4))\nc\u03b52\nwhere constant c is at least 1/32."}, {"heading": "5. Analysis", "text": "Before presenting the analysis, we first establish some notations and facts. Let the SVD of X be\nX = U\u03a3V \u22a4 =\nr\u2211\ni=1\n\u03bbiuiv \u22a4 i\nwhere \u03bbi is the ith singular value of X, ui and vi are the corresponding left and right singular vectors of X. Let U = (u1, . . . ,ur), V = (v1, . . . ,vr). Using the singular value decomposition of X, we define\n\u03b3\u2217 = \u2212\u03a3V \u22a4D(y)\u03b1\u2217, \u03b3\u0302 = \u2212\u03a3V \u22a4D(y)\u03b1\u0302 It is straightforward to show that\nw\u2217 = 1\n\u03bb U\u03b3\u2217, w\u0303 =\n1 \u03bb U \u03b3\u0302\nSince U is an othorgonal matrix, we have\n\u2016w\u2217\u20162 = 1\n\u03bb \u2016\u03b3\u2217\u20162, \u2016w\u0303\u20162 =\n1 \u03bb \u2016\u03b3\u0302\u20162\nLet us define A = U\u22a4S \u2208 Rr\u00d7m. It is easy to verify that A is an Gaussian matrix of size r \u00d7m."}, {"heading": "5.1. Proof of Theorem 2", "text": "The key to our analysis is to show that G\u0302 in (7) is close to G in (2) when the number of random projections is sufficiently large. To this end, we need the following concentration inequality for Gaussian random matrix.\nCorollary 6 Let M \u2208 Rr\u00d7m be a standard Gaussian random matrix. Then, with a probability at least 1\u2212 \u03b4, we have \u2225\u2225\u2225\u2225 1\nm MM\u22a4 \u2212 I \u2225\u2225\u2225\u2225 2 \u2264 \u03b5\nprovided that\nm \u2265 r ( log(r2 + r) + log(1/\u03b4) )\nc\u03b52\nwhere \u2016M\u20162 is the spectral norm of matrix M and c is a constant whose value is at least 1/32.\nRemark 8 The above corollary serves the key to our analysis, which enable us to bound G\u2212 G\u0302 and furthermore to bound \u03b1\u2217 \u2212 \u03b1\u0302.\nUsing Corollary 6, we have the following theorem that bounds the difference between G\u0302 and G.\nTheorem 7 With a probability 1\u2212 \u03b4, we have\n(1 + \u03b5)G G\u0302 (1\u2212 \u03b5)G\nprovided\nm \u2265 r ( log(r2 + r) + log(1/\u03b4) )\nc\u03b52\nProof We rewrite G and G\u0302 as\nG = D(y)V \u03a3U\u22a4U\u03a3V \u22a4D(y) G\u0302 = D(y)V \u03a3U\u22a4 SS\u22a4\nm U\u03a3V \u22a4D(y) = D(y)V \u03a3\nAA\u22a4\nm \u03a3V \u22a4D(y)\nThen with a probability 1\u2212 \u03b4 under the given condition on m, we can show that\nG\u0302\u2212 (1 + \u03b5)G = D(y)V \u03a3 ( AA\u22a4\nm \u2212 (1 + \u03b5)I\n) \u03a3V \u22a4D(y) 0\nand\nG\u0302\u2212 (1\u2212 \u03b5)G = D(y)V \u03a3 ( AA\u22a4\nm \u2212 (1\u2212 \u03b5)I\n) \u03a3V \u22a4D(y) 0\nusing the result in Corollary 6 since A is a Guassian matrix of size r \u00d7m.\nWe now give the proof for Theorem 2. The basic logic is straightforward. Since G\u0302 is close to G, we would expect \u03b1\u0302, the optimal solution to (7), to be close to \u03b1\u2217, the optimal\nsolution to (2). Since w\u2217 = XD(y)\u03b1\u2217/\u03bb and w\u0303 = XD(y)\u03b1\u0302/\u03bb, we would then expect w\u0303 to be close to w\u2217. Proof [Theorem 2] Define L(\u03b1) and L\u0302(\u03b1) as\nL(\u03b1) = \u2212 n\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bb \u03b1\n\u22a4G\u03b1, L\u0302(\u03b1) = \u2212 n\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bb \u03b1\n\u22a4G\u0302\u03b1\nSince \u03b1\u0302 maximizes L\u0302(\u03b1) over the domain \u2126n, we have\nL\u0302(\u03b1\u0302) \u2265 L\u0302(\u03b1\u2217) + 1\n2\u03bb (\u03b1\u0302\u2212\u03b1\u2217)\u22a4G\u0302(\u03b1\u0302\u2212\u03b1\u2217) (15)\nUsing the concaveness of L\u0302(\u03b1), we have L\u0302(\u03b1\u0302) \u2264 L\u0302(\u03b1\u2217) + (\u03b1\u0302\u2212\u03b1\u2217)\u22a4\u2207L\u0302(\u03b1\u2217) = L\u0302(\u03b1\u2217) + (\u03b1\u0302\u2212\u03b1\u2217)\u22a4 ( \u2207L\u0302(\u03b1\u2217)\u2212\u2207L(\u03b1\u2217) +\u2207L(\u03b1\u2217) )\n\u2264 L\u0302(\u03b1\u2217) + 1\n\u03bb (\u03b1\u0302\u2212\u03b1\u2217)\u22a4(G\u2212 G\u0302)\u03b1\u2217 (16)\nwhere the last inequality follows from the fact that (\u03b1\u0302 \u2212 \u03b1\u2217)\u22a4\u2207L(\u03b1\u2217) \u2264 0 since \u03b1\u2217 maximizes L(\u03b1) over the domain \u2126n. Combining the inequalities in (15) and (16), we have\n1 \u03bb (\u03b1\u0302\u2212\u03b1\u2217)\u22a4(G \u2212 G\u0302)\u03b1\u2217 \u2265 1 2\u03bb (\u03b1\u0302\u2212\u03b1\u2217)\u22a4G\u0302(\u03b1\u0302\u2212\u03b1\u2217)\nTherefore\n(\u03b3\u0302 \u2212 \u03b3\u2217)\u22a4 ( I \u2212 AA \u22a4\nm\n) \u03b3\u2217 \u2265 1\n2 (\u03b3\u0302 \u2212 \u03b3\u2217)\u22a4\nAA\u22a4\nm (\u03b3\u0302 \u2212 \u03b3\u2217) (17)\nUsing Corollary 6, with a probability 1\u2212 \u03b4, we have \u2016I \u2212AA\u22a4/m\u20162 \u2264 \u03b5 and therefore\n(1\u2212 \u03b5)\u2016\u03b3\u0302 \u2212 \u03b3\u2217\u20162 \u2264 2\u03b5\u2016\u03b3\u2217\u20162\nWe complete the proof using the fact that\nw\u2217 = 1\n\u03bb U\u03b3\u2217, w\u0303 =\n1 \u03bb U \u03b3\u0302."}, {"heading": "5.2. Proof of Theorem 3", "text": "As indicated before, the key reason for the large difference between w\u0302 and w\u2217 is because they do not lie in the same subspace: w\u2217 lies in the subspace spanned by the columns in U while w\u0302 lies in the subspace spanned by the column vectors in a random matrix. Before presenting our analysis, we first state a version of John Linderstrauss theorem that is useful to our analysis.\nTheorem 8 (Theorem 2 (Blum, 2006)) Let x \u2208 Rd, and x\u0302 = S\u22a4x/\u221am, where S \u2208 Rd\u00d7m is a random matrix whose entries are chosen independently from N (0, 1). Then\nPr { (1\u2212 \u01eb)\u2016x\u201622 \u2264 \u2016x\u0302\u201622 \u2264 (1 + \u01eb)\u2016x\u201622 } \u2265 1\u2212 2 exp ( \u2212m\n4 (\u01eb2 \u2212 \u01eb3)\n)\nIn the subspace orthogonal to u1, . . . ,ur, we randomly choose a subset of d\u2212r orthogonal bases, denoted by ur+1, . . . ,ud. Let U\u22a5 = (ur+1, . . . ,ud). Since\n\u2016w\u2217 \u2212 w\u0302\u20162 = max \u2016x\u20162\u22641 x\u22a4(w\u2217 \u2212 w\u0302),\nto facilitate our analysis, we restrict the choice of x to the subspace span(ur+1, . . . ,ud) and have\n\u2016w\u2217 \u2212 w\u0302\u20162 \u2265 max x\u2208span(ur+1,...,ud),\u2016x\u20162\u22641 x\u22a4w\u0302\nwhere we use the fact w\u2217 \u22a5 span(ur+1, . . . ,ud). Write x as x = U\u22a5a, where a \u2208 Rd\u2212r. Define\n\u039b = U\u22a4\u22a5S \u2208 R(d\u2212r)\u00d7m\nAs a result, we bound \u2016w\u2217 \u2212 w\u0302\u20162 by\nmax x\u2208span(ur+1,...,um),\u2016x\u20162\u22641 x\u22a4w\u0302 = max \u2016a\u20162\u22641\n1\nm\u03bb a\u22a4U\u22a4\u22a5SS\n\u22a4U \u03b3\u0302 = 1\nm\u03bb \u2016\u039bA\u22a4\u03b3\u0302\u20162 (18)\nwhere \u03b3\u0302 is given by \u03b3\u0302 = \u03a3V \u22a4D(y)\u03b1\u0302\nIt is easy to verify that A and \u039b are two independent Gaussian random matrices. Therefore, we can fix the vector A\u22a4\u03b3\u0302 and estimate how the random matrix \u039b affect the norm of vector A\u22a4\u03b3\u0302. According to the John Linderstrauss Theorem (i.e. Theorem 8), for a fixed vector A\u22a4\u03b3\u0302, with a probability 1\u2212 exp(\u2212(\u01eb2 \u2212 \u01eb3)(d\u2212 r)/4), we have\n1\u221a d\u2212 r\n\u2016\u039bA\u22a4\u03b3\u0302\u20162 \u2265 \u221a 1\u2212 \u01eb\u2016A\u22a4\u03b3\u0302\u20162\nBy choosing \u03b5 = 1/2, we have, with a probability 1\u2212 exp(\u2212(d\u2212 r)/32),\n1\u221a d\u2212 r \u2016\u039bA\u22a4\u03b3\u0302\u20162 \u2265 1\u221a 2 \u2016A\u22a4\u03b3\u0302\u20162 (19)\nWe now bound \u2016A\u22a4\u03b3\u0302\u20162. Note that we cannot directly apply the John Linderstrauss Theorem to bound the length of A\u22a4\u03b3\u0302 because \u03b3\u0302 is a random variable depending on the random matrix A. To decouple the dependence between A and \u03b3\u0302, we expand \u2016A\u22a4\u03b3\u0302\u20162 as\n\u2016A\u22a4\u03b3\u0302\u20162 \u2265 \u2016A\u22a4\u03b3\u2217\u20162 \u2212 \u2016A\u22a4(\u03b3\u2217 \u2212 \u03b3\u0302)\u20162 (20)\nwhere \u03b3\u2217 = \u03a3V \u22a4D(y)\u03b1\u2217\nWe bound the two terms on the right side of the inequality in (20) separately. Using the John Linderstrauss Theorem, with a probability 1\u2212 exp(\u2212m/32), we bound \u2016A\u22a4\u03b3\u2217\u2016 by\n1\u221a m \u2016A\u22a4\u03b3\u2217\u20162 \u2265 1\u221a 2 \u2016\u03b3\u2217\u20162 = \u03bb\u221a 2 \u2016w\u2217\u20162 (21)\nTo bound the second term \u2016A\u22a4(\u03b3\u2217 \u2212 \u03b3\u0302)\u2016, with a probability 1\u2212 \u03b4, we have\n1\u221a m \u2016A\u22a4(\u03b3\u2217 \u2212 \u03b3\u0302)\u20162 \u2264\n\u221a \u03bbmax(AA\u22a4/m)\u2016\u03b3\u2217 \u2212 \u03b3\u0302\u20162 \u2264 \u221a 1 + \u03b5\u03bb\u2016w\u2217 \u2212 w\u0303\u20162\nwhere we use the result in Corollary 6. According to Theorem 2, with a probability 1\u2212 \u03b4, we have\n\u2016w\u2217 \u2212 w\u0303\u20162 \u2264 2\u03b5\n1\u2212 \u03b5\u2016w\u2217\u20162\nAs a result, with probability 1\u2212 \u03b4, we have 1\u221a m \u2016A\u22a4(\u03b3\u2217 \u2212 \u03b3\u0302)\u20162 \u2264 \u03bb \u221a 1 + \u03b5 2\u03b5 1\u2212 \u03b5\u2016w\u2217\u20162 (22)\nWe complete the proof by putting together (18), (19), (20), (21), and (22)."}, {"heading": "5.3. Proof of Theorem 5", "text": "Given a solution w\u0303t obtained at iteration t, we consider the following optimization problem\nmin w\u2208Rd\nLt(w;X,y) = \u03bb\n2 \u2016w + w\u0303t\u201622 +\nn\u2211\ni=1\n\u2113(yi(w + w\u0303t) \u22a4xi) (23)\nIt is straightforward to show that \u2206t+1\u2217 = w\u2217\u2212 w\u0303t is the optimal solution to (23). Then we can use the dual random projection approach to recover \u2206t+1\u2217 by \u2206\u0303t+1. If we can similarly show that\n\u2016\u2206\u0303t+1 \u2212\u2206t+1\u2217 \u20162 \u2264 2\u03b5\n1\u2212 \u03b5\u2016\u2206 t+1 \u2217 \u20162\nthen we define the updated recovered solution by w\u0303t+1 = w\u0303t + \u2206\u0303t+1 and have\n\u2016w\u0303t+1 \u2212w\u2217\u20162 \u2264 2\u03b5\n1\u2212 \u03b5\u2016\u2206 t+1 \u2217 \u20162 =\n2\u03b5\n1\u2212 \u03b5\u2016w\u0303t \u2212w\u2217\u20162\nContinously, if we repeat the above process for t = 1, . . . , T , the recovery error of w\u0303T is given by\n\u2016w\u0303T \u2212w\u2217\u20162 \u2264 ( 2\u03b5\n1\u2212 \u03b5\n)T\u22121 \u2016w\u03031 \u2212w\u2217\u20162 \u2264 ( 2\u03b5\n1\u2212 \u03b5\n)T \u2016w\u2217\u20162\nThe remaining question is how to compute the \u2206\u0303t+1 using the dual random projection approach. In order to make the previous analysis remain valid for the recovered solution \u2206\u0303t+1 to the problem (23), we need to write the primal optimization problem in the same\nform as in (1). To this end, we first note that w\u0303t lies in the subspace spanned by x1, . . . ,xn, thus we write w\u0303t as\nw\u0303t = \u2212 1\n\u03bb\nn\u2211\ni=1\n[a\u0303t]iyixi\nThus, Lt(w;X,y) can be written as\nLt(w;X,y) = \u03bb\n2 \u2016w\u0303t\u201622 +\n\u03bb 2 \u2016w\u201622 + \u03bbw\u22a4w\u0303t +\nn\u2211\ni=1\n\u2113(yiw \u22a4xi + yiw\u0303 \u22a4 t xi)\n= \u03bb\n2 \u2016w\u0303t\u201622 +\n\u03bb 2 \u2016w\u201622 +\nn\u2211\ni=1\n\u2113(yiw \u22a4xi + yiw\u0303 \u22a4 t xi)\u2212 [a\u0303t]iyiw\u22a4xi\n= \u03bb\n2 \u2016w\u0303t\u201622 +\n\u03bb 2 \u2016w\u201622 +\nn\u2211\ni=1\n\u2113it(yiw \u22a4xi)\nwhere the new loss function \u2113it(z), i = 1, . . . , n is defined as\n\u2113it(z) = \u2113(z + yiw\u0303 \u22a4 t xi)\u2212 [a\u0303t]iz (24)\nTherefore \u2206t+1\u2217 is the solution to the following problem\n\u2206t+1\u2217 = arg min w\u2208Rd\n\u03bb 2 \u2016w\u201622 +\nn\u2211\ni=1\n\u2113it(yiw \u22a4xi)\nTo apply the dual random projection approach to recover \u2206t+1\u2217 , we solve the following optimization problem in the projected space:\nmin z\u2208Rm\n\u03bb 2 \u2016z\u201622 +\nn\u2211\ni=1\n\u2113it(yiz \u22a4x\u0302i)\nThe following derivation signifies that the above problem is equivalent to the problem in (13).\nminz\u2208Rm \u03bb\n2 \u2016z\u201622 +\nn\u2211\ni=1\n\u2113it(yiz \u22a4x\u0302i)\n= \u03bb\n2 \u2016z\u201622 +\nn\u2211\ni=1\n\u2113(yiz \u22a4x\u0302i + yiw\u0303 \u22a4 t xi)\u2212 [a\u0303t]iyiz\u22a4x\u0302i\n= \u03bb\n2 \u2016z\u201622 + \u03bb\u221a m z\u22a4(S\u22a4w\u0303t) +\nn\u2211\ni=1\n\u2113(yiz \u22a4x\u0302i + yiw\u0303 \u22a4 t xi)\n= \u03bb\n2 \u2016z+ S\u22a4w\u0303t/\n\u221a m\u201622 +\nn\u2211\ni=1\n\u2113(yiz \u22a4x\u0302i + yiw\u0303 \u22a4 t xi)\u2212\n\u03bb 2 \u2016S\u22a4w\u0303t/\n\u221a m\u20162\nwhere we use w\u0303t = \u2212 \u2211 i[a\u0303 t]iyixi. Given the optimal solution z t+1 \u2217 to the above problem, we can recover \u2206t+1\u2217 by\n\u2206\u0303t+1 = \u2212 1\n\u03bb XD(y)\u03b1\u0302t+1\nwhere \u03b1\u0302t+1 is computed by\n[\u03b1\u0302t+1]i = \u2207\u2113i(yix\u0302\u22a4zt\u2217) = \u2207\u2113(yix\u0302\u22a4zt\u2217 + yiw\u0303\u22a4t xi)\u2212 [a\u0303t]i The updated solution w\u0303t+1 is computed by\nw\u0303t+1 = w\u0303t + \u2206\u0303t = \u2212 1\n\u03bb\nT\u2211\ni=1\n[a\u0303t+1]ixiyi\nwhere [a\u0303t+1]i = [\u03b1\u0302t+1]i + [a\u0303 t]i = \u2207\u2113(yix\u0302\u22a4zt\u2217 + yiw\u0303\u22a4t xi)."}, {"heading": "6. Conclusion", "text": "In this paper, we discuss the problem of recovering optimal solutions through random projection. Our goal is to first efficiently obtain an approximate solution z\u2217 for a given optimization problem by using random projection and then reconstruct the true optimal solution w\u2217 from the random projection based solution z\u2217. We developed a dual random projection approach and show that under the assumption that the data matrix X is of low rank, the proposed approach is able to accurately recover the true optimal solution w\u2217 with a small error.\nThere are several open questions that need to be addressed in the future. The first open question is to analyze the behavior of the proposed algorithm when X can be well approximated by a low rank matrix, an assumption that is significantly weaker than the low rank assumption. The second open question is to develop a parallel version of the proposed algorithm by running it independently over multiple machines. The challenge is to design an effective approach for combining multiple sets of random projection based solutions into one solution with significantly smaller recovering error. This is an important question when we need to adapt the proposed algorithm to a distributed computing environment."}, {"heading": "Acknowledgments", "text": "We thank a bunch of people."}, {"heading": "Appendix A. Proof of Proposition 1 and Proposition 3", "text": "Since the two propositions can be proved similarly, we only present the proof of Proposition 1. First if \u03b1\u2217 is the optimal dual solution, the optimal primal solution can be solved by\nw\u2217 = arg min w\u2208Rd\n\u03bb 2 \u2016w\u201622 +\nn\u2211\ni=1\n[\u03b1\u2217]iyix \u22a4 i\nBy setting the gradient with respect to w to zero, we obtain w\u2217 = \u2212 \u2211n\ni=1[\u03b1]iyixi/\u03bb = \u2212XD(y)\u03b1/\u03bb\nSecond, to prove the dual solution \u03b1\u2217 given the primal solution w\u2217, we note that\n\u2113(yix \u22a4 i w\u2217) = [\u03b1\u2217]i(yix \u22a4 i w\u2217)\u2212 \u2113\u2217([\u03b1]i\nBy the Fenchel conjugate theory (e.g., Theorem 11.4 in(Cesa-Bianchi and Lugosi, 2006)) we have \u03b1 satisfying\n[\u03b1\u2217]i = \u2207\u2113(yixiw\u2217)."}, {"heading": "Appendix B. Proof of Corollary 6", "text": "In the proof, we make use of the following concentration inequality regarding the eigenvalues of Gaussian random matrix.\nTheorem 9 (Corollary 7.2 (Gittens and Tropp, 2011)) Let C \u2208 Rp\u00d7p be a positive definite matrix. Let \u03b7j \u2208 Rp, j = 1, . . . , n be i.i.d. samples drawn from a N (0, C) distribution. Define\nC\u0302n = 1\nn\nn\u2211\nj=1\n\u03b7j\u03b7 \u22a4 j .\nWrite \u03bbk for the kth eigenvalue of C, and write \u03bb\u0302k for the kth eigenvalue of C\u0302n. Then, for k = 1, . . . , p,\nPr { \u03bb\u0302k \u2265 (1 + \u03b5)\u03bbk } \u2264 (p\u2212 k + 1) exp ( \u2212 cn\u03b5 2\n\u2211p i=k \u03bbi/\u03bbk\n) , for \u03b5 \u2264 4n\nand\nPr { \u03bb\u0302k \u2264 (1\u2212 \u03b5)\u03bbk } \u2264 k exp ( \u2212 cn\u03b5 2\n\u2211k i=1 \u03bb1\u03bbi/\u03bb 2 k\n) , for \u03b5 \u2208 (0, 1]\nwhere constant c is at least 1/32.\nWe write M = (\u03b71, . . . ,\u03b7m), where \u03b7i \u2208 Rr is i.i.d sample from a Gaussian distribution N (0, I) and write MM\u22a4/m as\nCm = 1\nm MM\u22a4 =\n1\nm\nm\u2211\ni=1\n\u03b7i\u03b7 \u22a4 i\nUsing Theorem 9, we have,\n1\u2212 \u03b5 \u2264 \u03bbk(Cm) \u2264 1 + \u03b5, k = 1, . . . , r\nwith the failure probability at most r\u2211\nk=1\n(r \u2212 k + 1) exp ( \u2212cm\u03b5 2\nr\n) + k exp ( \u2212cm\u03b5 2\nr\n) = (r2 + r) exp ( \u2212cm\u03b5 2\nr\n)\nWe complete proof by setting the above failure probability to be less than \u03b4."}, {"heading": "Appendix C. Proof of Theorem 4", "text": "We write w\u0302 in terms of w\u0303 as w\u0302 = SS\u22a4w\u0303/m and therefore\nmax \u2016x\u20162\u22641,x\u2208span(X) x\u22a4(w\u2217 \u2212 w\u0302) \u2264 \u2016w\u2217 \u2212 w\u0303\u20162 + max \u2016x\u20162\u22641,x\u2208span(X) x\u22a4(w\u0303 \u2212 w\u0302)\n= \u2016w\u2217 \u2212 w\u0303\u20162 + max \u2016a\u20162\u22641\na\u22a4 ( I \u2212 1\nm U\u22a4SS\u22a4U\n) \u03b3\u0302/\u03bb\n\u2264 \u2016w\u2217 \u2212 w\u0303\u20162 + \u03bbmax ( I \u2212 1\nm U\u22a4SS\u22a4U\n) \u2016w\u0303\u20162\n\u2264 \u2016w\u2217 \u2212 w\u0303\u20162 + \u03bbmax ( I \u2212 1\nm AA\u22a4\n) \u2016w\u2217\u2016\nThe last but one inequality uses the fact \u2016w\u0303\u20162 = \u2016\u03b3\u0302\u20162/\u03bb. Using Corollary 6, we have, with a probability 1\u2212 \u03b4,\n\u03bbmax\n( I \u2212 1\nm AA\u22a4\n) \u2264 \u03b5\nWe complete the proof by using the bound for \u2016w\u2217 \u2212 w\u0303\u20162 stated in Theorem 2."}, {"heading": "Appendix D. Proof of Theorem 1", "text": "According to (17) in the proof of Theorem 2, we have\n2(\u03b3\u0302 \u2212 \u03b3\u2217)\u22a4 ( I \u2212 AA \u22a4\nm\n) \u03b3\u2217 \u2265 (\u03b3\u0302 \u2212 \u03b3\u2217)\u22a4 AA\u22a4\nm (\u03b3\u0302 \u2212 \u03b3\u2217)\nUsing the fact \u221a mz\u2217 = A \u22a4 \u03b3\u0302/\u03bb and S\u22a4w\u2217 = A \u22a4 \u03b3\u2217/\u03bb, we have\n\u03bb2 m \u2016\u221amw\u0302\u2032 \u2212 S\u22a4w\u2217\u201622 \u2264 2(\u03b3\u0302 \u2212 \u03b3\u2217)\u22a4\n( I \u2212 AA \u22a4\nm\n) \u03b3\u2217\nUsing Corollary 6, with a probability 1\u2212 \u03b4, we have 1\nm \u2016\u221amz\u2217 \u2212 S\u22a4w\u2217\u201622 \u2264 2\u03b5\u2016w\u2217\u20162\u2016w\u0303 \u2212w\u2217\u20162\nUsing Theorem 2, with a probability 1\u2212 \u03b4, we have 1\nm \u2016\u221amz\u2217 \u2212 S\u22a4w\u2217\u201622 \u2264 4\u03b52 1\u2212 \u03b5\u2016w\u2217\u2016 2 2 (25)\nTo replace w\u2217 on R. H. S. of the above inequality with S \u22a4w\u2217, we make use of Theorem 8. As a result, with a probability 1\u2212 exp(\u2212m/32), we have 1\nm \u2016S\u22a4w\u2217\u201622 \u2265\n1 2 \u2016w\u2217\u201622 (26)\nWe complete the proof by combining the two inequalities in (25) and (26)."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "An algorithmic theory of learning: robust concepts and random projection", "author": ["Rosa I. Arriaga", "Santosh Vempala"], "venue": "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Arriaga and Vempala.,? \\Q1999\\E", "shortCiteRegEx": "Arriaga and Vempala.", "year": 1999}, {"title": "A pac-style model for learning from labeled and unlabeled data", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Balcan and Blum.,? \\Q2005\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2005}, {"title": "Kernels as features: On kernels, margins, and low-dimensional mappings", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the 7th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Bingham and Mannila.,? \\Q2001\\E", "shortCiteRegEx": "Bingham and Mannila.", "year": 2001}, {"title": "Random projection, margins, kernels, and feature-selection", "author": ["Avrim Blum"], "venue": "In Proceedings of the 2005 international conference on Subspace, Latent Structure and Feature Selection,", "citeRegEx": "Blum.,? \\Q2006\\E", "shortCiteRegEx": "Blum.", "year": 2006}, {"title": "Random projections for kmeans clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Petros Drineas"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Boutsidis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2010}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Rademacher chaos, random eulerian graphs and the sparse johnson-lindenstrauss transform", "author": ["V. Braverman", "R. Ostrovsky", "Y. Rabani"], "venue": "ArXiv e-prints,", "citeRegEx": "Braverman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2010}, {"title": "An introduction to compressive sampling", "author": ["Emmanuel J. Cand\u00e8s", "Michael B. Wakin"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Cand\u00e8s and Wakin.,? \\Q2008\\E", "shortCiteRegEx": "Cand\u00e8s and Wakin.", "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Proceedings of the 40th annual ACM symposium on Theory of computing,", "citeRegEx": "Dasgupta and Freund.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Freund.", "year": 2008}, {"title": "Compressed sensing", "author": ["David L. Donoho"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Relative-error cur matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Random projection for high dimensional data clustering: a cluster ensemble approach", "author": ["Xiaoli Zhang Fern", "Carla E. Brodley"], "venue": "In Proceedings of the 20th International Conference on Machine Learning,", "citeRegEx": "Fern and Brodley.,? \\Q2003\\E", "shortCiteRegEx": "Fern and Brodley.", "year": 2003}, {"title": "Experiments with random projections for machine learning", "author": ["Dmitriy Fradkin", "David Madigan"], "venue": "In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Fradkin and Madigan.,? \\Q2003\\E", "shortCiteRegEx": "Fradkin and Madigan.", "year": 2003}, {"title": "Learning the structure of manifolds using random projections", "author": ["Yoav Freund", "Sanjoy Dasgupta", "Mayank Kabra", "Nakul Verma"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Freund et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2008}, {"title": "Tail bounds for all eigenvalues of a sum of random matrices", "author": ["Alex Gittens", "Joel A. Tropp"], "venue": "CoRR, abs/1104.4513v2,", "citeRegEx": "Gittens and Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Gittens and Tropp.", "year": 2011}, {"title": "Face recognition experiments with random projection", "author": ["Navin Goel", "George Bebis", "Ara Nefian"], "venue": "In Proceedings of SPIE,", "citeRegEx": "Goel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2005}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff.,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Beating sgd: Learning svms in sublinear time", "author": ["Elad Hazan", "Tomer Koren", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hazan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2011}, {"title": "Dimensionality reduction by random mapping: fast similarity computation for clustering", "author": ["Samuel Kaski"], "venue": "In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Kaski.,? \\Q1998\\E", "shortCiteRegEx": "Kaski.", "year": 1998}, {"title": "Dense fast random projections and lean walsh transforms", "author": ["Edo Liberty", "Nir Ailon", "Amit Singer"], "venue": "Proceedings of the 12th International Workshop on Randomization and Computation (RANDOM),", "citeRegEx": "Liberty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liberty et al\\.", "year": 2008}, {"title": "Linear regression with random projections", "author": ["Oldalric-Ambrym Maillard", "Remi Munos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maillard and Munos.,? \\Q2012\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2012}, {"title": "Randomized Algorithms", "author": ["Rajeev Mowani", "Prabhakar Raghavan"], "venue": null, "citeRegEx": "Mowani and Raghavan.,? \\Q1995\\E", "shortCiteRegEx": "Mowani and Raghavan.", "year": 1995}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Rahimi and Recht.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2008}, {"title": "Online learning meets optimization in the dual", "author": ["Shai Shalev-Shwartz", "Yoram Singer"], "venue": "In Proceedings of 19th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Shalev.Shwartz and Singer.,? \\Q2006\\E", "shortCiteRegEx": "Shalev.Shwartz and Singer.", "year": 2006}, {"title": "Is margin preserved after random projection", "author": ["Qinfeng Shi", "Chunhua Shen", "Rhys Hill", "Anton van den Hengel"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "The Random Projection Method", "author": ["Santosh S. Vempala"], "venue": "American Mathematical Society,", "citeRegEx": "Vempala.,? \\Q2004\\E", "shortCiteRegEx": "Vempala.", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 30, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 15, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 2, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 5, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 27, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 24, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al.", "startOffset": 250, "endOffset": 298}, {"referenceID": 13, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al.", "startOffset": 250, "endOffset": 298}, {"referenceID": 22, "context": ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 14, "context": ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 6, "context": ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 22, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 34, "endOffset": 74}, {"referenceID": 4, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 34, "endOffset": 74}, {"referenceID": 11, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 16, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 18, "context": ", 2008), and information retrieval (Goel et al., 2005).", "startOffset": 35, "endOffset": 54}, {"referenceID": 19, "context": "This is particularly useful for feature selection (Guyon and Elisseeff, 2003), where important features are often selected based on their weights in the linear prediction model learned from the training data.", "startOffset": 50, "endOffset": 77}, {"referenceID": 26, "context": "For non differentiable loss functions such as hinge loss, we could apply the smoothing technique (Nesterov, 2005) to make it differentiable.", "startOffset": 97, "endOffset": 113}, {"referenceID": 1, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 29, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 3, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 24, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 9, "context": "Relationship to Compression Sensing The proposed problem is closely related to compressive sensing (Cand\u00e8s and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections.", "startOffset": 99, "endOffset": 137}, {"referenceID": 12, "context": "Relationship to Compression Sensing The proposed problem is closely related to compressive sensing (Cand\u00e8s and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections.", "startOffset": 99, "endOffset": 137}, {"referenceID": 7, "context": "We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.", "startOffset": 98, "endOffset": 147}, {"referenceID": 21, "context": "We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.", "startOffset": 98, "endOffset": 147}, {"referenceID": 28, "context": ", 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.", "startOffset": 28, "endOffset": 61}, {"referenceID": 0, "context": ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).", "startOffset": 2, "endOffset": 66}, {"referenceID": 23, "context": ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).", "startOffset": 2, "endOffset": 66}, {"referenceID": 8, "context": ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).", "startOffset": 2, "endOffset": 66}, {"referenceID": 20, "context": "We also note that Algorithm 2 is related to the Epoch gradient descent algorithm (Hazan and Kale, 2011) for stochastic optimization in that the solution obtained in the previous iteration is served as the center to the optimization problem of the current iteration.", "startOffset": 81, "endOffset": 103}, {"referenceID": 20, "context": "Unlike the algorithm in (Hazan and Kale, 2011), we do not shrink the domain size over the iterations in Algorithm 2.", "startOffset": 24, "endOffset": 46}, {"referenceID": 25, "context": "Following the same arguments as compressive sensing, it may be possible to argue that \u03a9(r log r) is optimal due to the result of coupon collector\u2019s problem (Mowani and Raghavan, 1995), although the rigorous analysis remains to be developed.", "startOffset": 156, "endOffset": 183}, {"referenceID": 5, "context": "We note that Theorem 4 directly implies the result of margin classification error for random projection (Blum, 2006).", "startOffset": 104, "endOffset": 116}, {"referenceID": 5, "context": "Theorem 8 (Theorem 2 (Blum, 2006)) Let x \u2208 R, and x\u0302 = S\u22a4x/\u221am, where S \u2208 Rd\u00d7m is a random matrix whose entries are chosen independently from N (0, 1).", "startOffset": 21, "endOffset": 33}], "year": 2017, "abstractText": "In this work, we address the problem of how to recover the optimal solution to the optimization problem related to high dimensional data classification using random projection, to which we refer as Recovery of Optimal Solution. This is in contrast to the previous studies that were focused on analyzing the classification performance using random projection. We reveal the relationship between compressive sensing and the problem of recovering optimal solution using random projection. We also present a simple algorithm, termed as Dual Random Projection, that recovers the optimal solution with a small error by computing dual solution provided that the data matrix is of low rank.", "creator": "LaTeX with hyperref package"}}}