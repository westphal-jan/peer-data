{"id": "1211.3212", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2012", "title": "Distributed Non-Stochastic Experts", "abstract": "we consider the online distributed non - stochastic experts problem, where the distributed architecture consists of one coordinator node that is connected to $ k $ sites, occupying the sites previously required to communicate throughout each other via the coordinator. at each time - step $ t $, assignment of a $ k $ site nodes has to designate an expert from the set $ { 1,..., } } $, and the same site receives information about payoffs of all experts for that technique. the challenge of the distributed system is to minimize variability at time horizon $ t $, while significantly keeping communication to a minimum.", "histories": [["v1", "Wed, 14 Nov 2012 06:45:38 GMT  (1706kb,D)", "http://arxiv.org/abs/1211.3212v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["varun kanade", "zhenming liu", "bozidar radunovic"], "accepted": true, "id": "1211.3212"}, "pdf": {"name": "1211.3212.pdf", "metadata": {"source": "CRF", "title": "Distributed Non-Stochastic Experts", "authors": ["Varun Kanade", "Zhenming Liu"], "emails": ["vkanade@eecs.berkeley.edu", "zhenming@cs.princeton.edu", "bozidar@microsoft.com"], "sections": [{"heading": null, "text": "\u221a log(n)T ) regret bound at the cost of\nT communication. (ii) No communication: Each site runs an independent copy \u2013 the regret is O( \u221a log(n)kT ) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \u221a kT and communication better than T . We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O( \u221a k5(1+ )/6T ) and communication O(T/k ), for any value of \u2208 (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off."}, {"heading": "1 Introduction", "text": "In this paper, we consider the well-studied non-stochastic expert problem in a distributed setting. In the standard (non-distributed) setting, there are a total of n experts available for the decisionmaker to consult, and at each round t = 1, . . . , T , she must choose to follow the advice of one of the experts, say at, from the set [n] = {1, . . . , n}. At the end of the round, she observes a payoff vector pt \u2208 [0, 1]n, where pt[a] denotes the payoff that would have been received by following the advice of expert a. The payoff received by the decision-maker is pt[at]. In the non-stochastic setting, an adversary decides the payoff vectors at any time step. At the end of the T rounds, the regret of the decision maker is the difference in the payoff that she would have received using\n\u2217This research was carried out while the author was at Harvard University supported in part by grant NSF-CCF09-64401 \u2020This research was carried out while the author was at Harvard University supported in part by grants NSF-IIS0964473 and NSF-CCF-0915922\nar X\niv :1\n21 1.\n32 12\nv1 [\ncs .L\nG ]\n1 4\nthe single best expert at all times in hindsight, and the payoff that she actually received, i.e. R = maxa\u2208[n] \u2211T t=1 p t[a] \u2212 \u2211T t=1 p t[at]. The goal here is to minimize her regret; this general problem in the non-stochastic setting captures several applications of interest, such as experiment design, online ad-selection, portfolio optimization, etc. (See [1, 2, 3, 4, 5] and references therein.)\nTight bounds on regret for the non-stochastic expert problem are obtained by the so-called follow the regularized leader approaches; at time t, the decision-maker chooses a distribution, xt, over the n experts. Here xt minimizes the quantity \u2211t\u22121 s=1 p\nt \u00b7 x + r(x), where r is a regularizer. Common regularizers are the entropy function, which results in Hedge [1] or the exponentially weighted forecaster (see chap. 2 in [2]), or as we consider in this paper r(x) = \u03b7\u0304 \u00b7 x, where \u03b7\u0304 \u2208R [0, \u03b7]n is a random vector, which gives the follow the perturbed leader (FPL) algorithm [6].\nWe consider the setting when the decision maker is a distributed system, where several different nodes may select experts and/or observe payoffs at different time-steps. Such settings are common, e.g. internet search companies, such as Google or Bing, may use several nodes to answer search queries and the performance is revealed by user clicks. From the point of view of making better predictions, it is useful to pool all available data. However, this may involve significant communication which may be quite costly. Thus, there is an obvious trade-off between cost of communication and cost of inaccuracy (because of not pooling together all data), which leads to the question:\nWhat is the explicit trade-off between the total amount of communication needed and the regret of the expert problem under worst case input?"}, {"heading": "2 Models and Summary of Results", "text": "We consider a distributed computation model consisting of one central coordinator node connected to k site nodes. The site nodes must communicate with each other using the coordinator node. At each time step, the distributed system receives a query1, which indicates that it must choose an expert to follow. At the end of the round, the distributed system observes the payoff vector. We consider two different models described in detail below: the site prediction model where one of the k sites receives a query at any given time-step, and the coordinator prediction model where the query is always received at the coordinator node. In both these models, the payoff vector, pt, is always observed at one of the k site nodes. Thus, some communication is required to share the information about the payoff vectors among nodes. As we shall see, these two models yield different algorithms and performance bounds.\nGoal: The algorithm implemented on the distributed system may use randomness, both to decide which expert to pick and to decide when to communicate with other nodes. We focus on simultaneously minimizing the expected regret and the expected communication used by the (distributed) algorithm. Recall, that the expected regret is:\nE[R] = E [ max a\u2208[n] T\u2211 t=1 pt[a]\u2212 T\u2211 t=1 pt[at], ] (1)\nwhere the expectation is over the random choices made by the algorithm. The expected communication is simply the expected number (over the random choices) of messages sent in the system.\n1We do not use the word query in the sense of explicitly giving some information or context, but merely as indication of occurrence of an event that forces some site or coordinator to choose an expert. In particular, if any context is provided in the query the algorithms considered in this paper ignore all context \u2013 thus we are in the non-contextual expert setting.\nAs we show in this paper, this is a challenging problem and to keep the analysis simple we focus on bounds in terms of the number of sites k and the time horizon T , which are often the most important scaling parameters. In particular, our algorithms are variants of follow the perturbed leader (FPL) and hence our bounds are not optimal in terms of the number of experts n. We believe that the dependence on the number of experts in our algorithms (upper bounds) can be strengthened using a different regularizer. Also, all our lower bounds are shown in terms of T and k, for n = 2. For larger n, using techniques similar to Theorem 3.6 in [2] should give the appropriate dependence on n.\nAdversaries: In the non-stochastic setting, we assume that an adversary may decide the payoff vectors, pt, at each time-step and also the site, st, that receives the payoff vector (and also the query in the site-prediction model). An oblivious adversary cannot see any of the actions of the distributed system, i.e. selection of expert, communication patterns or any random bits used. However, the oblivious adversary may know the description of the algorithm. In addition to knowing the description of the algorithm, an adaptive adversary is stronger and can record all of the past actions of the algorithm, and use these arbitrarily to decide the future payoff vectors and site allocations.\nCommunication: We do not explicitly account for message sizes. However, since we are interested in scaling with T and k, we do require that message size should not depend on the number of sites k or the number of time-steps T , but only on the number of experts n. In other words, we assume that n is substantially smaller than T and k. All the messages used in our algorithms contain at most n real numbers. As is standard in the distributed systems literature, we assume that communication delay is 0, i.e. the updates sent by any node are received by the recipients before any future query arrives. All our results still hold under the weaker assumption that the number of queries received by the distributed system in the duration required to complete a broadcast is negligible compared to k. 2\nWe now describe the two models in greater detail, state our main results and discuss related work:\n1. Site Prediction Model: At each time step t = 1, . . . , T , one of the k sites, say st, receives a query and has to pick an expert, at, from the set, [n] = {1, . . . , n}. The payoff vector pt \u2208 [0, 1]n, where pt[i] is the payoff of the ith expert is revealed only to the site st and the decision-maker (distributed system) receives payoff pt[at], corresponding to the expert actually chosen. The site prediction model is commonly studied in distributed machine learning settings (see [7, 8, 9]). The payoff vectors, p1, . . . ,pT , and also the choice of sites that receive the query, s1, . . . , sT , are decided by an adversary. There are two very simple algorithms in this model: (i) Full communication: The coordinator always maintains the current cumulative payoff vector,\u2211t\u22121 \u03c4=1 p \u03c4 . At time step t, st receives the current cumulative payoff vector \u2211t\u22121 \u03c4=1 p \u03c4 from the coordinator, chooses an expert at \u2208 [n] using FPL, receives payoff vector pt and sends pt to the coordinator, which updates its cumulative payoff vector. Note that the total communication is 2T and the system simulates (non-distributed) FPL to achieve (optimal) regret guarantee O( \u221a nT ).\n(ii) No communication: Each site maintains cumulative payoff vectors corresponding to the queries received by them, thus implementing k independent versions of FPL. Suppose that the ith site\n2This is because in regularized leader like approaches, if the cumulative payoff vector changes by a small amount the distribution over experts does not change much because of the regularization effect.\nreceives a total of Ti queries ( \u2211k i=1 Ti = T ), the regret is bounded by \u2211k i=1O( \u221a nTi) = O( \u221a nkT ) and the total communication is 0. This upper bound is actually tight, as shown in Lemma 3 (Appendix C.2.1), in the event that there is 0 communication.\nSimultaneously achieving regret that is asymptotically lower than \u221a knT using communication asymptotically lower than T turns out to be a significantly challenging question. Our main positive result is the first distributed expert algorithm in the oblivious adversarial (non-stochastic) setting, using sub-linear communication. Finding such an algorithm in the case of an adaptive adversary is an interesting open problem.\nTheorem 1. When T \u2265 2k2.3, there exists an algorithm for the distributed experts problem that against an oblivious adversary achieves regret O(log(n) \u221a k5(1+ )/6T ) and uses communication O(T/k ), giving non-trivial guarantees in the range \u2208 (0, 1/5).\n2. Coordinator Prediction Model: At every time step, the query is received by the coordinator node, which chooses an expert at \u2208 [n]. However, at the end of the round, one of the site nodes, say st, observes the payoff vector pt. The payoff vectors pt and choice of sites st are decided by an adversary. This model is also a natural one and is explored in the distributed systems and streaming literature (see [10, 11, 12] and references therein).\nThe full communication protocol is equally applicable here getting optimal regret bound, O( \u221a nT ) at the cost of substantial (essentially T ) communication. But here, we do not have any straightforward algorithms that achieve non-trivial regret without using any communication. This model is closely related to the label-efficient prediction problem (see Chapter 6.1-3 in [2]), where the decision-maker has a limited budget and has to spend part of its budget to observe any payoff information. The optimal strategy is to request payoff information randomly with probability C/T at each time-step, if C is the communication budget. We refer to this algorithm as LEF (label-efficient forecaster) [13].\nTheorem 2. [13] (Informal) The LEF algorithms using FPL with communication budget C achieves regret O(T \u221a n/C) against both an adaptive and an oblivious adversary.\nOne of the crucial differences between this model and that of the label-efficient setting is that when communication does occur, the site can send cumulative payoff vectors comprising all previous updates to the coordinator rather than just the latest one. The other difference is that, unlike in the label-efficient case, the sites have the knowledge of their local regrets and can use it to decide when to communicate. However, our lower bounds for natural types of algorithms show that these advantages probably do not help to get better guarantees.\nLower Bound Results: In the case of an adaptive adversary, we have an unconditional (for any type of algorithm) lower bound in both the models:\nTheorem 3. Let n = 2 be the number of experts. Then any (distributed) algorithm that achieves expected regret o( \u221a kT ) must use communication (T/k)(1\u2212 o(1)).\nThe proof appears in Appendix A. Notice that in the coordinator prediction model, when C = T/k, this lower bound is matched by the upper bound of LEF.\nIn the case of an oblivious adversary, our results are weaker, but we can show that certain natural types of algorithms are not applicable directly in this setting. The so called regularized leader algorithms, maintain a cumulative payoff vector, Pt, and use only this and a regularizer to select an expert at time t. We consider two variants in the distributed setting: (i) Distributed Counter Algorithms: Here the forecaster only uses P\u0303t, which is an (approximate)\nversion of the cumulative payoff vector Pt. But we make no assumptions on how the forecaster will use P\u0303t. P\u0303t can be maintained while using sub-linear communication by applying techniques from distributed systems literature [11].\n(ii) Delayed Regularized Leader: Here the regularized leaders don\u2019t try to explicitly maintain an approximate version of the cumulative payoff vector. Instead, they may use an arbitrary communication protocol, but make prediction using the cumulative payoff vector (using any past payoff vectors that they could have received) and some regularizer.\nWe show in Section 3.2 that the distributed counter approach does not yield any non-trivial guarantee in the site-prediction model even against an oblivious adversary. It is possible to show a similar lower bound the in the coordinator prediction model, but is omitted since it follows easily from the idea in the site-prediction model combined with an explicit communication lower bound given in [11].\nSection 4 shows that the delayed regularized leader approach does not yield non-trivial guarantees even against an oblivious adversary in the coordinator prediction model, suggesting LEF algorithm is near optimal.\nRelated Work: Recently there has been significant interest in distributed online learning questions (see for example [7, 8, 9]). However, these works have focused mainly on stochastic optimization problems. Thus, the techniques used, such as reducing variance through mini-batching, are not applicable to our setting. Questions such as network structure [8] and network delays [9] are interesting in our setting as well, however, at present our work focuses on establishing some non-trivial regret guarantees in the distributed online non-stochastic experts setting. Study of communication as a resource in distributed learning is also considered in [14, 15, 16]; however, this body of work seems only applicable to offline learning.\nThe other related work is that of distributed functional monitoring [10] and in particular distributed counting[11, 12], and sketching [17]. Some of these techniques have been successfully applied in offline machine learning problems [18]. However, we are the first to analyze the performance-communication trade-off of an online learning algorithm in the standard distributed functional monitoring framework [10]. An application of a distributed counter to an online Bayesian regression was proposed in Liu et al. [12]. Our lower bounds discussed below, show that approximate distributed counter techniques do not directly yield non-trivial algorithms."}, {"heading": "3 Site-prediction model", "text": ""}, {"heading": "3.1 Upper Bounds", "text": "We describe our algorithm that simultaneously achieves non-trivial bounds on expected regret and expected communication. We begin by making two assumptions that simplify the exposition. First, we assume that there are only 2 experts. The generalization from 2 experts to n is easy, as discussed in the Remark 1 at the end of this section. Second, we assume that there exists a global query counter, that is available to all sites and the co-ordinator, which keeps track of the total number of queries received across the k sites. We discuss this assumption in Remark 2 at the end of the section. As is often the case in online algorithms, we assume that the time horizon T is known. Otherwise, the standard doubling trick may be employed. The notation used in this Section is defined in Table 1. Algorithm Description: Our algorithm DFPL is described in Figure 1(a). We make use of FPL algorithm, described in Figure 1(b), which takes as a parameter the amount of added noise \u03b7.\nDFPL algorithm treats the T time steps as b(= T/`) blocks, each of length `. At a high level, with probability q on any given block the algorithm is in the step phase, running a copy of FPL (with noise parameter \u03b7\u2032) across all time steps of the block, synchronizing after each time step. Otherwise it is in a block phase, running a copy of FPL (with noise parameter \u03b7) across blocks with the same expert being followed for the entire block and synchronizing after each block. This effectively makes Pi, the cumulative payoff over block i, the payoff vector for the block FPL. The block FPL has on average (1\u2212 q)T/` total time steps. We begin by stating a (slightly stronger) guarantee for FPL.\nLemma 1. Consider the case n = 2. Let p1, . . . ,pT \u2208 [0, 1]2 be a sequence of payoff vectors such that maxt |pt|\u221e \u2264 B and let the number of experts be 2. Then FPL(\u03b7) has the following guarantee on expected regret, E[R] \u2264 B\u03b7 \u2211T t=1 |pt[1]\u2212 pt[2]|+ \u03b7.\nThe proof is a simple modification to the proof of the standard analysis [6] and is given in Appendix B for completeness. The rest of this section is devoted to the proof of Lemma 2\nLemma 2. Consider the case n = 2. If T > 2k2.3, Algorithm DFPL (Fig. 1) when run with parameters `, T , \u03b7 = `5/12T 1/2 and b, \u03b7\u2032, q as defined in Fig 1, has expected regret O( \u221a `5/6T )\nand expected communication O(Tk/`). In particular for ` = k1+ for 0 < < 1/5, the algorithm simultaneously achieves regret that is asymptotically lower than \u221a kT and communication that is asymptotically lower3 than T .\nSince we are in the case of an oblivious adversary, we may assume that the payoff vectors p1, . . . ,pT are fixed ahead of time. Without loss of generality let expert 1 (out of {1, 2}) be the one that has greater payoff in hindsight. Recall that FRi1(\u03b7\n\u2032) denotes the random variable that is the regret of playing FPL(\u03b7\u2032) in a step phase on block i with respect to the first expert. In particular, this will be negative if expert 2 is the best expert on block i, even though globally expert 1 is better. In fact, this is exactly what our algorithm exploits: it gains on regret in the communication-expensive, step phase while saving on communication in the block phase.\nThe regret can be written as\nR = b\u2211 i=1 ( Yi \u00b7 FRi1(\u03b7\u2032) + (1\u2212 Yi)(Pi[1]\u2212Pi[ai] ) .\nNote that the random variables Yi are independent of the random variables FR i 1(\u03b7 \u2032) and the random variables ai. As E[Yi] = q, we can bound the expression for expected regret as follows:\nE[R] \u2264 q b\u2211 i=1 E[FRi1(\u03b7\u2032)] + (1\u2212 q) b\u2211 i=1 E[Pi[1]\u2212Pi[ai]] (2)\nWe first analyze the second term of the above equation. This is just the regret corresponding to running FPL(\u03b7) at the block level, with T/` time steps. Using the fact that maxi |Pi|\u221e \u2264 `maxt |pt|\u221e \u2264 `, Lemma 1 allows us to conclude that:\nb\u2211 i=1 E[Pi[1]\u2212Pi[ai]] \u2264 ` \u03b7 b\u2211 i=1 |Pi[1]\u2212Pi[2]|+ \u03b7 (3)\nNext, we also analyse the first term of the inequality (2). We chose \u03b7\u2032 = \u221a ` (see Fig. 1) and the analysis of FPL guarantees that E[FRi(\u03b7\u2032)] \u2264 2 \u221a `, where FRi(\u03b7\u2032) denotes the random variable that is the actual regret of FPL(\u03b7\u2032), not the regret with respect to expert 1 (which is FRi1(\u03b7 \u2032)). Now either FRi(\u03b7\u2032) = FRi1(\u03b7 \u2032) (i.e. expert 1 was the better one on block i), in which case\nE[FRi1(\u03b7\u2032)] \u2264 2 \u221a `; otherwise FRi(\u03b7\u2032) = FRi2(\u03b7\n\u2032) (i.e. expert 2 was the better one on block i), in which case E[FRi1(\u03b7\u2032)] \u2264 2 \u221a `+ Pi[1]\u2212Pi[2]. Note that in this expression Pi[1]\u2212Pi[2] is negative.\nPutting everything together we can write that E[FRi1(\u03b7\u2032)] \u2264 2 \u221a `\u2212(Pi[2]\u2212Pi[1])+, where (x)+ = x if x \u2265 0 and 0 otherwise. Thus, we get the main equation for regret.\nE[R] \u2264 2qb \u221a `\u2212 q b\u2211 i=1\n(Pi[2]\u2212Pi[1])+\ufe38 \ufe37\ufe37 \ufe38 term 1\n+ `\n\u03b7 b\u2211 i=1\n|Pi[1]\u2212Pi[2]|\ufe38 \ufe37\ufe37 \ufe38 term 2 +\u03b7 (4)\nNote that the first (i.e. 2qb \u221a `) and last (i.e. \u03b7) terms of inequality (4) are O( \u221a `5/6T ) for the setting of the parameters as in Lemma 2. The strategy is to show that when \u201cterm 2\u201d becomes large, then \u201cterm 1\u201d is also large in magnitude, but negative, compensating the effect of \u201cterm 1\u201d.\n3Note that here asymptotics is in terms of both parameters k and T . Getting communication of the form T 1\u2212\u03b4f(k) for regret bound better than \u221a kT , seems to be a fairly difficult and interesting problem\nWe consider a few cases:\nCase 1: When the best expert is identified quickly and not changed thereafter. Let \u03b6 denote the maximum index, i, such that Qi[1] \u2212 Qi[2] \u2264 \u03b7. Note that after the block \u03b6 is processed, the algorithm in the block phase will never follow expert 2.\nSuppose that \u03b6 \u2264 (\u03b7/`)2. We note that the correct bound for \u201cterm 2\u201d is now actually (`/\u03b7) \u2211\u03b6 i=1 |Pi[1]\u2212Pi[2]| \u2264 (`2\u03b6/\u03b7) \u2264 \u03b7 since |Pi[1]\u2212Pi[2]| \u2264 ` for all i. Case 2 The best expert may not be identified quickly, furthermore |Pi[1]\u2212Pi[2]| is large often. In this case, although \u201cterm 2\u201d may be large (when (Pi[1] \u2212 Pi[2]) is large), this is compensated by the negative regret in \u201cterm 1\u201d in expression (4). This is because if |Pi[1] \u2212 Pi[2]| is large often, but the best expert is not identified quickly, there must be enough blocks on which (Pi[2]\u2212Pi[1]) is positive and large.\nNotice that \u03b6 \u2265 (\u03b7/`)2. Define \u03bb = \u03b72/T and let S = {i \u2264 \u03b6 | |Pi[1] \u2212 Pi[2]| \u2265 \u03bb}. Let \u03b1 = |S|/\u03b6. We show that \u2211\u03b6 i=1(P\ni[2] \u2212 Pi[1])+ \u2265 (\u03b1\u03b6\u03bb)/2 \u2212 \u03b7. To see this consider S1 = {i \u2208 S | Pi[1] > Pi[2]} and S2 = S \\S1. First, observe that \u2211 i\u2208S |Pi[1]\u2212Pi[2]| \u2265 \u03b1\u03b6\u03bb. Then,\nif \u2211\ni\u2208S2(P i[2]\u2212Pi[1]) \u2265 (\u03b1\u03b6\u03bb)/2, we are done. If not \u2211 i\u2208S1(P\ni[1]\u2212Pi[2]) \u2265 (\u03b1\u03b6\u03bb)/2. Now notice that \u2211\u03b6 i=1 P i[1] \u2212Pi[2] \u2264 \u03b7, hence it must be the case that \u2211\u03b6 i=1(P i[2] \u2212Pi[1])+ \u2265 (\u03b1\u03b6\u03bb)/2 \u2212 \u03b7. Now for the value of q = 2`3T 2/\u03b75 and if \u03b1 \u2265 \u03b72/(T`), the negative contribution of \u201cterm 1\u201d is at least q\u03b1\u03b6\u03bb/2 which greater than the maximum possible positive contribution of \u201cterm 2\u201d which is `2\u03b6/\u03b7. It is easy to see that these quantities are equal and hence the total contribution of \u201cterm 1\u201d and \u201cterm 2\u201d together is at most \u03b7. Case 3 When |Pi[1]\u2212Pi[2]| is \u201csmall\u201d most of the time. In this case the parameter \u03b7 is actually well-tuned (which was not the case when |Pi[1] \u2212 Pi[2]| \u2248 `) and gives us a small overall regret. (See Lemma 1.) We have \u03b1 < \u03b72/(T`). Note that \u03b1` \u2264 \u03bb = \u03b72/T and that \u03b6 \u2264 T/`. In this case \u201cterm 2\u201d can be bounded easily as follows: `\u03b7 \u2211\u03b6 i=1 |Pi[1]\u2212Pi[2]| \u2264 ` \u03b7 (\u03b1\u03b6`+ (1\u2212 \u03b1)\u03b6\u03bb) \u2264 2\u03b7 The above three cases exhaust all possibilities and hence no matter what the nature of the payoff sequence, the expected regret of DFPL is bounded by O(\u03b7) as required. The expected total communication is easily seen to be O(qT +Tk/`) \u2013 the q(T/`) blocks on which step FPL is used contribute O(`) communication each, and the (1 \u2212 q)(T/`) blocks where block FPL is used contributed O(k) communication each.\nRemark 1. Our algorithm can be generalized to n experts by recursively dividing the set of experts in two and applying our algorithm to two meta-experts, as shown in Section C.1 in the Appendix. However, the bound obtained in Section C.1 is not optimal in terms of the number of experts, n. This observation and Lemma 2 imply Theorem 1.\nRemark 2. The assumption that there is a global counter is necessary because our algorithm divides the input into blocks of size `. However, it is not an impediment because it is sufficient that the block sizes are in the range [0.99`, 1.01`]. Assuming that the coordinator always signals the beginning and end of the block (by a broadcast which only adds 2k messages to any block), we can use a distributed counter that guarantees a very tight approximation to the number of queries received in each block with at most O(k log(`)) messages communicated (see [11])."}, {"heading": "3.2 Lower Bounds", "text": "In this section we give a lower bound on distributed counter algorithms in the site prediction model. Distributed counters allow tight approximation guarantees, i.e. for factor \u03b2 additive approximation, the communication required is only O(T log(T ) \u221a k/\u03b2) [11]. We observe that the noise used by FPL is quite large, O( \u221a T ), and so it is tempting to find a suitable \u03b2 and run FPL using approximate\ncumulative payoffs. We consider the class of algorithms such that:\n(i) Whenever each site receives a query, it has an (approximate) cumulative payoff of each expert to additive accuracy \u03b2. Furthermore, any communication is only used to maintain such a counter. (ii) Any site only uses the (approximate) cumulative payoffs and any local information it may have to choose an expert when queried.\nHowever, our negative result shows that even with a highly accurate counter \u03b2 = O(k), the nonstochasticity of the payoff sequence may cause any such algorithm to have \u2126( \u221a kT ) regret. Furthermore, we show that any distributed algorithm that implements (approximate) counters to additive error k/10 on all sites4 is at least \u2126(T ).\nTheorem 4. At any time step t, suppose each site has an (approximate) cumulative payoff count, P\u0303t[a], for every expert such that |Pt[a]\u2212 P\u0303t[a]| \u2264 \u03b2. Then we have the following: 1. If \u03b2 \u2264 k, any algorithm that uses the approximate counts P\u0303t[a] and any local information at the site making the decision, cannot achieve expected regret asymptotically better than \u221a \u03b2T . 2. Any protocol on the distributed system that guarantees that at each time step, each site has a \u03b2 = k/10 approximate cumulative payoff with probability \u2265 1/2, uses \u2126(T ) communication."}, {"heading": "4 Coordinator-prediction model", "text": "In the co-ordinator prediction model, as mentioned earlier it is possible to use the label-efficient forecaster, LEF (Chap. 6 [2, 13]). Let C be an upper bound on the total amount of communication we are allowed to use. The label-efficient predictor translates into the following simple protocol: Whenever a site receives a payoff vector, it will forward that particular payoff to the coordinator with probability p \u2248 C/T . The coordinator will always execute the exponentially weighted forecaster over the sampled subset of payoffs to make new decisions. Here, the expected regret is O(T \u221a log(n)/C). In other words, if our regret needs to be O( \u221a T ), the communication needs to be linear in T . We observe that in principle there is a possibility of better algorithms in this setting for mainly two reasons: (i) when the sites send payoff vectors to the co-ordinator, they can send cumulative payoffs rather than the latest ones, thus giving more information, and (ii) the sites may decided when to communicate as a function of the payoff vectors instead of just randomly. However, we present a lower-bound that shows that for a natural family of algorithms achieving regret O( \u221a T ) requires at least \u2126(T 1\u2212 ) for every > 0, even when k = 1. The type of algorithms we consider may have an arbitrary communication protocol, but it satisfies the following: (i) Whenever a site communicates with the coordinator, the site will report its local cumulative payoff vector. (ii) When the coordinator makes a decision, it will execute, FPL(\n\u221a T ), (follow the perturbed leader with noise\u221a\nT ) using the latest cumulative payoff vector. The proof of Theorem 5 appears in Appendix D and the results could be generalized to other regularizers.\nTheorem 5. Consider the distributed non-stochastic expert problem in coordinator prediction model. Any algorithm of the kind described above that achieves regret O( \u221a T ) must use \u2126(T 1\u2212 ) communication against an oblivious adversary for every constant ."}, {"heading": "5 Simulations", "text": "In this section, we describe some simulation results comparing the efficacy of our algorithm DFPL with some other techniques. We compare DFPL against simple algorithms \u2013 full communication and no communication, and two other algorithms which we refer to as mini-batch and HYZ. In the mini-batch algorithm, the coordinator requests randomly, with some probability p at any time step, all cumulative payoff vectors at all sites. It then broadcasts the sum (across all of the sites) back to the sites, so that all sites have the latest cumulative payoff vector. Whenever such a communication does occur, the cost is 2k. We refer to this as mini-batch because it is similar in spirit to the minibatch algorithms used in the stochastic optimization problems. In the HYZ algorithm, we use the distributed counter technique of Huang et al. [11] to maintain the (approximate) cumulative payoff for each expert. Whenever a counter update occurs, the coordinator must broadcast to all nodes to make sure they have the most current update.\nWe consider two types of synthetic sequences. The first is a zig-zag sequence, with \u00b5 being the length of one increase/decrease. For the first \u00b5 time steps the payoff vector is always (1, 0) (expert 1 being better), then for the next 2\u00b5 time steps, the payoff vector is (0, 1) (expert 2 is better), and then again for the next 2\u00b5 time-steps, payoff vector is (1, 0) and so on. The zig-zag sequence is also the sequence used in the proof of the lower bound in Theorem 5. The second is a two-state Markov chain (MC) with states 1, 2 and Pr[1\u2192 2] = Pr[2\u2192 1] = 12\u03bb . While in state 1, the payoff vector is (1, 0) and when in state 2 it is (0, 1).\nIn our simulations we use T = 20000 predictions, and k = 20 sites. Fig. 2 (a) shows the performance of the above algorithms for the MC sequences, the results are averaged across 100 runs, over both the randomness of the MC and the algorithms. Fig. 2 (b) shows the worstcase cumulative communication vs the worst-case cumulative regret trade-off for three algorithms: DFPL, mini-batch and HYZ, over all the described sequences. While in general it is hard to compare algorithms on non-stochastic inputs, our results confirm that for non-stochastic sequences inspired by the lower-bounds in the paper, our algorithm DFPL outperforms other related techniques."}, {"heading": "A Adaptive Adversary", "text": "This section contains a proof of Theorem 3. The proof makes use of Khinchine\u2019s inequality (see Appendix A.1.14 in [2]).\nKhinchine\u2019s Inequality. Let \u03c31, . . . , \u03c3n be Rademacher random variables, i.e. Pr[\u03c3i = 1] = Pr[\u03c3i = \u22121] = 1/2. Then for any real numbers a1, . . . , an,\nE [ | n\u2211 i=1 ai\u03c3i| ] \u2265 1\u221a 2 \u221a\u221a\u221a\u221a n\u2211 i=1 a2i = 1\u221a 2 \u221a\u221a\u221a\u221a\u221aE ( n\u2211 i=1 ai\u03c3i )2 Proof of Theorem 3. The adaptive adversary divides the total T time steps into T/k time blocks, each consisting of k time-steps. During each block of k time-steps, each of the k sites receives exactly 1 query. At time t = 1, k + 1, 2k + 1, . . ., the adversary tosses an unbiased coin. Let pH denote the payoff vector corresponding to heads, where pH [1] = 1 and pH [2] = 0. Similarly let pT (corresponding to tails) be such that pT [1] = 0 and pT [2] = 1. For i = 1, . . . , T/k and j = 1, . . . , k, the adaptive adversary does the following: At time (i\u2212 1)k + j, if there was no communication on part of the decision maker (distributed system) between time steps (i\u2212 1)k+ 1, . . . , (i\u2212 1)k+ j\u2212 1 \u2013 then if the coin toss at time (i \u2212 1)k + 1 was heads the payoff vector is pH , otherwise it is pT . On the other hand if there was any communication, then the adaptive adversary tosses a random coin and sets the payoff vector accordingly.\nConsider the expected payoff of the algorithm: At time t = (i \u2212 1)k + j, if there was communication between time steps (i \u2212 1)k + 1 to (i \u2212 1)k + j \u2212 1, then the adversary has chosen the payoff vector uniformly at random between pH and pT and hence the expected reward at time step t is exactly 1/2. On the other hand if there was no communication between these time steps, then the site j making the decision has no information about the coin toss of the adversary at time (i \u2212 1)j + 1, and hence the expected reward is still 1/2. Thus, the total expected reward of the algorithm (by linearity of expectation) is T/2.\nNote that,\nE [ max i=1,2 T\u2211 t=1 pt[i] ] = 1 2 ( E [ T\u2211 t=1 pt[1] + pt[2] ] + E [ | T\u2211 t=1 (pt[1]\u2212 pt[2])| ])\n= T\n2 +\n1 2 E [ | T\u2211 t=1 (pt[1]\u2212 pt[2])| ] (5)\nLet I \u2286 [T/k] be the indices of the blocks for which there was some communication. Consider blocks in I and those outside of I. Suppose the block (i \u2212 1)k + 1, . . . , ik is such that i 6\u2208 I, then | \u2211t=ik\nt=(i\u22121)k+1 p t[1]\u2212pt[2]| = k. Note that all such block sums (as random variables) are independent of all other block sums. For some block (i\u2212 1)k+ 1, . . . , ik such that i \u2208 I, let c(i) be such the first such that communication occurs at block (i\u2212 1)k+ c(i). Then | \u2211t=(i\u22121)k+c(i) t=(i\u22121)k+1 p\nt[1]\u2212pt[2]| = c(i), also note that pt for t = (i\u2212 1)k + c(i) + 1, . . . , ik are all based on independent coin tosses. Then note that,\nT\u2211 t=1 pt[1]\u2212 pt[2] = \u2211 i 6\u2208I k\u03c3i, 1 + \u2211 i\u2208I (c(i)\u03c3i, 1 + k\u2211 j=c(i)+1 \u03c3i,j), (6)\nwhere \u03c3i, j are the Rademacher variables corresponding to the coin tosses of the adversary at time step (i\u2212 1)k + j. Also note that,\nE ( T\u2211 t=1 pt[1]\u2212 pt[2] )2 \u2265 (T k \u2212 |I| ) k2\nThen, Khinchine\u2019s inequality and (5) gives us that\nE[max i=1,2 T\u2211 t=1 pt[i]] \u2265 T 2 + 1 2 \u221a 2\n\u221a\u221a\u221a\u221a\u221aE ( T\u2211\nt=1\npt[1]\u2212 pt[2] )2 \u2265 T\n2 +\n1 2 \u221a 2\n\u221a( T\nk \u2212 |I|\n) k2\nNow, unless |I| = (T/k)(1\u2212o(1)), it must be the case that E[maxi=1,2 \u2211T t=1 p t[i]] \u2265 T/2 + \u2126( \u221a kT )\nleading to total expected regret \u2126( \u221a kT ). Hence, any algorithm that achieves regret o( \u221a kT ) must have communication (1\u2212 o(1))T/k."}, {"heading": "B Follow the Perturbed Leader", "text": "Proof of Lemma 1. We first note that using the given notation, the regret guarantee of FPL(\u03b7) (see Fig. 1(b)) is\nE[R] \u2264 B \u03b7 T\u2211 t=1 |pt|1 + \u03b7\nThe above appears in the analysis of Kalai and Vempala [6]. Note that although |pt|1 = pt[1]+pt[2] (pt[a] \u2265 0 in our setting), we can use the following trick. We first observe that since FPL(\u03b7) only depends on the difference between the cumulative payoffs of the two experts, we may replace the payoff vectors pt by p\u0303t, where (i) if pt[1] \u2265 pt[2], p\u0303t[1] = pt[1] \u2212 pt[2] and p\u0303t[2] = 0 (ii) if pt[1] < pt[2], p\u0303t[1] = 0 and p\u0303t[2] = pt[2]\u2212 pt[1]\nNext, we observe that the regret of FPL(\u03b7) with payoff sequence pt and p\u0303t is identically distributed, since the random choices only depend on the difference between the cumulative payoffs at any time. Lastly, we note that |p\u0303t|1 = |pt[1]\u2212 pt[2]|, which completes the proof."}, {"heading": "C Site Prediction : Missing Proofs", "text": "C.1 Generalizing DFPL to n experts\nIn this section, we generalize our DFPL algorithm for two experts to handle n experts. Lemma 2 showed that algorithm DFPL, in the setting of two experts, guarantees that the expected regret is at most c0 \u221a `5/6T , where c0 is a universal constant.\nOur generalization follows a recursive approach. Suppose that some algorithm A can achieve expected regret, c0 log(n) \u221a `5/6T with n experts, we show that we can construct algorithm A\u2032 that\nachieves expected regret, c0(log(n) + 1) with 2n experts as follows: We run 2 independent copies of A (say A1 and A2) such that A1 only deals with the first n experts a1, a2, ..., an and A2 with the rest of the experts an+1, ..., a2n. Then our algorithm A\n\u2032 treats A1 and A2 as 2 experts and runs the DFPL algorithm (Section 3.1) over these two experts. The analysis for regret is straightforward:\nLet the regret for A1 be R1 and the regret for A2 be R2. We have\nE[Payoff(A1)] \u2265 max i\u2208[n] \u2211 t\u2264T pt[i]\u2212 E[R1] and E[Payoff(A2)] \u2265 max i\u2208{n+1,...,2n} \u2211 t\u2264T pt[i]\u2212 E[R2].\nWe know that E[R1] \u2264 c0 log(n) \u221a `5/6T and E[R2] \u2264 c0 log(n) \u221a `5/6T .\nNext, we can see that E[Payoff(A\u2032) | Payoff(A1),Payoff(A2)] \u2265 max{Payoff(A1),Payoff(A2)} \u2212 c0 \u221a `5/6T\nWe can use the above expression to conclude (taking expectations) that\nE[Payoff(A\u2032)] \u2265 E[Payoff(A1)]\u2212 c0 \u221a `5/6T\nE[Payoff(A\u2032)] \u2265 E[Payoff(A2)]\u2212 c0 \u221a `5/6T\nBut using the above two inequalities we can conclude that\nE[Payoff(A\u2032)] \u2264 max i\u2208[2n] \u2211 t\u2264T pt[i]\u2212 c0(log(n) + 1) \u221a l5/6T\nThis immediately implies that for n experts (starting from base case of n = 2 where DFPL\nworks), this recursive approach results in an algorithm for n experts achieves regretO(log(n) \u221a `5/6T ). In order to analyze the communication, we observe that in order to implement the algorithm correctly, when algorithm (which is DFPL at some depth in the recursion) decides to communicate at each time step on a block, the communication on that block is `. There are at most n copies of DFPL running (depth of the recursion is log(n) \u2212 1). However, the corresponding term in the communication bound O(nqT`) is lower than the term arising from blocks where communication occurs only at the beginning and end of block, O((1\u2212qn)Tk/`). Thus, the expected communication (in terms of number of messages) is asymptotically the same as in the case of 2 experts. If we count communication complexity as the cost of sending 1 real number, instead of one message, then the total communication cost is O(nTk/`).\nC.2 Lower Bounds\nC.2.1 No Communication Protocol\nIn the site-prediction setting, we show that any algorithm that uses no communication must achieve regret \u2126( \u221a kT ) on some sequence. The proof is quite simple, but does not follow directly from the \u2126( \u221a T ) lower-bound of the non-distributed case, because although the k sites each run a copy of some FPL-like algorithm, the best expert might be different across the sites. We only consider the case when n = 2, since we are more interested in dependence on T and k.\nLemma 3. If no-communication protocol is used in the site-prediction model expected regret achieved by any algorithm is at least \u2126( \u221a kT ).\nProof. The oblivious adversary does the following: Divide T time steps into T/k blocks of size k. For each block, toss a random coin and set the payoff vector to be pH = (1, 0) for heads or pT = (0, 1) for tails. And each query in a block is assigned to one site (say in a cyclic fashion). Note that the expected reward of any algorithm that does not use any communication is T/2. Because, no site at any time can perform better than random guessing. But the standard analysis shows that for the sequence as constructed above E[maxa=1,2 \u2211T t=1 p t[a]] \u2265 T/2 + \u2126(k \u221a T/k) = T/2 + \u2126( \u221a kT ).\nC.2.2 Lower Bound using Distributed Counter\nThis section contains proof of Theorem 4.\nProof of Theorem 4.\nPart 1: The oblivious adversary decides to only use \u03b2 out of the k sites. The adversary divides the input sequence into T/\u03b2 blocks, each block of size \u03b2. For each block, the adversary tosses an unbiased coin and sets the payoff vector pH = (1, 0) or pT = (0, 1) according to whether the coin toss resulted in heads or tails. Let P\u0303t[a] = Pt \u2217 [a], where t\u2217 is largest such that t\u2217 < t and t\u2217 = \u03b2i for some integer i (i.e. t\u2217 is the time at the end of the block). Note that |P\u0303t[a] \u2212 Pt[a]| \u2264 \u03b2, so P\u0303t[a] is a valid (approximate) value of the cumulative payoff of action a. However, since the payoff vectors across the blocks are completely uncorrelated and each site makes a decision only once in each block, the expected reward at any time step t is 1/2, and overall expected reward is T/2.\nNote, that it is easy to show that E[maxi=1,2 \u2211T t=1 p t[i]] \u2265 T/2 + \u2126( \u221a \u03b2T ) using standard tech-\nniques. Thus the expected regret is at least \u2126( \u221a \u03b2T ).\nPart 2: Let \u03b2 = k/10. Now consider the input sequence that is all 1. But that this is divided into T/k blocks of size k. For each block, the oblivious adversary chooses a random permutation of {1, . . . , k} and allocates the 1 to the site in that order. Note that when the site receives a 1, it is required to have an \u03b2-approximate value to the current count. Suppose there was no communication since this site last received a query, then at that time the estimate at this site was at most ik + \u03b2. Now, depending on where in the permutation the site is it may be required to have a value in any of the intervals [ik\u2212 \u03b2, ik+ \u03b2], [ik, ik+ 2\u03b2], [ik+ \u03b2, ik+ 3\u03b2], . . . , [(i+ 1)k\u2212 \u03b2, (i+ 1)k+ 2\u03b2]. There are at least 5 disjoint intervals in this state and each of them are equally probable. Thus with probability at least 4/5, in the absence of any communication, this site fails to have the correct approximate estimate.\nIf on the other hand, every site does communicate at least once every time it receives a query. The total communication is at least T ."}, {"heading": "D Proof of Theorem 5", "text": "Proof of Theorem 5. To prove Theorem 5, we construct a set of reward sequences pt0,p t 1, ...,, and\nshow that any FPL-like algorithm (as described in Section 4), will have regret \u2126( \u221a T ) on least one of these sequences unless the communication is essentially linear in T . Before we start the actual analysis, we need to introduce some more notation. First, recall that C is an upper bound on the amount of communication allowed in the protocol. We shall focus reward sequences where at any time-step exactly one of the experts receives payoff 1 and the other expert receives payoff 0, i.e. pt \u2208 {(0, 1), (1, 0)} for any t. Let gp(t) = pt[1] \u2212 pt[2], and let\nGp(t) = \u2211t\ni=1 g p(t). Thus, we note that the payoff vectors p, the function gp, and the function\nGp all encode equivalent information regarding payoffs as a function of time. Suppose, A is an algorithm that achieves optimal regret under the communication bound C. Let r denote the random coin tosses used by, A. Thus we may think of r as being a string of length poly(n, k)T fixed ahead of time. Let p1, ..., pT be a specific input sequence. Let T1, T2, . . . , TC denote the time-steps when communication occurs. We note that Ti may depend on ri which is a prefix of the (random) string r, which the algorithm observes until time-step Ti and may also depend on the payoff vectors p1, . . . ,pTi .\nNext, we describe the set of reward sequences to \u201cfool\u201d the algorithm. Let \u03bb be a parameter that will be fixed later. We construct up to (T/(2\u03bb)) + 1 possible payoff sequences. We denote this payoff sequences as p(0),p(1), . . . ,p(T/(2\u03bb))+1. These sequences are constructed as follows:\n\u2022 p(0): Let g+ denote a sequence of \u03bb consecutive 1\u2019s and g\u2212 denote a sequence of \u03bb consecutive \u22121\u2019s. Then the sequence \u3008gp(0)(t)\u3009t\u2264T is defined to be the sequence g\u2212, g+, g+, g\u2212, g\u2212, ..., i.e. gp(0)(t) = \u22121 if d(t \u2212 1)/\u03bbe is even and gp(0)(t) = 1 if d(t \u2212 1)/\u03bbe is odd. Furthermore, we assume that T = (4m1 + 3)\u03bb for some integer m1. This means that G\np(0)(T ) = \u03bb, i.e. eventually expert 1 will be the better expert.\n\u2022 p(i) for i > 0 and i even: In this payoff sequence, the payoff vectors for the first (2i \u2212 1)\u03bb time-steps will be identical to those in p0. For the rest of the time-steps the payoff vector will always be {(1, 0)}, i.e. the first expert always receives a unit payoff for t > (2i \u2212 1)\u03bb. Thus, for sequences of this form, where i is even, expert 1 will be the better expert.\n\u2022 p(i) for i > 0 and i odd: In this payoff sequence, the payoff vectors for the first (2i \u2212 1)\u03bb time-steps will be identical to p(0). For the rest of the time-steps, the payoff vector will always be {(0, 1)}, i.e. the second expert always receives a unit payoff after t > (2i\u2212 1)\u03bb. Thus, for sequences of this form, where i is odd, expert 2 will be the better expert.\nFurthermore, in what follows, we assume that there is only one site node. (This is not a problem, since worst adversary could send all the payoff vectors to just one of the site nodes.) We shall refer to the i-th cycle of the input in the above sequences as the input between time steps (4i + 2)\u03bb \u2212 ( \u221a T/2) + 1 and (4i + 4)\u03bb + ( \u221a T/2). Let F i be an indicator random variable (depending on the randomness r of the algorithm), such that F i = 0, if there is some communication between the time steps 2i\u03bb+ \u221a T/2 and (2i+ 2)\u03bb\u2212 \u221a t/2. If there is no communication, we will set F i = 1.\nNow, we prove the main result using a series of claims. First, we show add a few extra communication points, showing that this only increases the payoff of the algorithm (hence decreases regret). Let I = {i | F 2i = F 2i+1 = F 2i+2 = 0}. Note that I itself is a random variable. For every i \u2208 I, we allow extra communication to the algorithm (for free) at the end of the following time-steps: (4i+ 2)\u03bb\u2212 \u221a T/2 (4i+ 2)\u03bb+ \u221a T/2, (4i+ 4)\u03bb\u2212 \u221a T/2, and (4i+ 4) \u221a T/2. Note, that this extra communication can only increase the payoff, precisely because F 2i = F 2i+1 = F 2i+2 = 0. This extra communication is given for free, thus this is favorable to the trade-off of the algorithm. Despite this we will show that even the regret of this algorithm has to be large. This is done by a series of claims. Each of which are proved as lemmas subsequently.\nClaim A Let R p(i) A (1, T ) denote the (random variable) regret of playing according to algorithm, A, against payoff sequence, p(i) using randomness r, between time-steps 1 and T . Then, if E[Rp(i)A (1, T )] = O( \u221a T ) for all 1 \u2264 i \u2264 T/(2\u03bb), then E[|I|] \u2265 T4\u03bb . This fact is proved in Lemma 4.\nClaim B Suppose, i \u2208 I, and let C(i) be the communication during the ith cycle. Then we can state the following regarding the payoff on the rounds with respect to sequence p(0) within\nthe ith cycle. Here c0 is some absolute constant.\nPayoff p(0) A ((4i+ 2)\u03bb\u2212\n\u221a T/2 + 1, (4i+ 4)\u03bb+ \u221a T/2) \u2264 \u03bb+ \u221a T/2\u2212 c0\n\u221a T\nC(i)\nThis fact is proved in Lemma 5.\nClaim C Let t be a point such that communication happened just after time step t. Let \u03c4 > t be a point such that G(\u03c4) = G(t). Then Payoff\np(0) A (t + 1, \u03c4) \u2264 (\u03c4 \u2212 t)/2. This fact is proved\nin Lemma 6.\nNow, let us calculate the regret of the algorithm. If the expected regret of the algorithm with respect to sequence p(i) for i > 0, is at most O( \u221a T ), then it must be the case that E[|I|] \u2265 T/(4\u03bb) (using Claim A above). Now, we assumed that in the sequence p(0), expert 1 eventually wins. Let I = {i1, . . . , ik}, where i1 < i2 < \u00b7 \u00b7 \u00b7 < ik and E[k] \u2265 T/(4\u03bb). Then, we add up the payoff of the algorithm as follows. First, (using Claim B above) notice that:\nE[Payoffp(0)A ((4ij + 2)\u03bb\u2212 \u221a T/2 + 1, (4ij + 4)\u03bb+ \u221a T/2)] \u2264 \u03bb+ \u221a T 2 \u2212\nc0 \u221a T\nC(i) (7)\nThen let Bj denote the interval, ((4ij + 4)\u03bb+ \u221a T/2 + 1, (4ij+1 + 2)\u03bb\u2212 \u221a T/2), i.e. between the ith\nand the jth cycle. Also, let B0 denote ( \u221a T/2 + 1, (4i1 + 2)\u03bb\u2212 \u221a T/2) be the interval before the first\ncycle in I, and let Bk = ((4ik + 4)\u03bb+ \u221a T/2 + 1, T \u2212 \u03bb\u2212 \u221a T/2) denote the interval after the last cycle. Now, using Claim C above, we get that the payoff received by algorithms in any interval Bj is half the length of the interval. Thus, the only time-steps that we have not accounted for is (1, \u221a T/2) and (T \u2212 \u03bb\u2212 \u221a T/2 + 1, T ). The total number of time-steps in these two intervals is \u03bb. Let us give the algorithm payoff \u03bb for free on these time steps. Then, adding up everything and the payoff of the algorithm, Payoff\np(0) A is a random variable defined over the space measurable by\n{F i}i\u22650 and C\nPayoff p(0) A (1, T ) \u2264\nT 2 + \u03bb 2 \u2212 k\u2211 j=1 c0 \u221a T C(ij)\nThus, we get\nE[Rp(0)A | {F i}i\u22650, C] \u2265 E [\u2211 i\u2208I \u221a T C(i) | {F i}i\u22650, C ] \u2212 \u03bb 2 (I is measurable by {F i}i\u22651)\n\u2265 E\n[ |I|2 \u221a T\nC | {F i}i\u22650, C\n] \u2212 \u03bb\n2\n\u2265 c0 |I|2 \u221a T\nC \u2212 \u03bb 2 (I is measurable by {F i}i\u22650)\nWe use Jensen\u2019s inequality and the fact that C \u2265 \u2211\ni\u2208I C(i) to get the last inequality. Finally, using Claim A and by setting \u03bb appropriately, we get\nE[Rp(0)A (1, T )] \u2265 c0T 1.5\u22122 116C\nWe now prove the Lemmas mentioned in the above proof.\nLemma 4. If E[Rp(i)A (1, T )] = O( \u221a T ) for all 1 \u2264 i \u2264 T2\u03bb , then E[|I|] \u2265 T 4\u03bb .\nProof. Our crucial observation here is that when the random tosses of the algorithm is fixed, the algorithm will have identical behavior against the reward sequences p(0) and p(m) for any 1 \u2264 m \u2264 T2\u03bb up to time 2m\u03bb\u2212 \u03bb. Thus, if we couple the process for executing A against p(0) with the one for executing A against p(m) with the same random tosses in the algorithm, we are able to relate the random variables {F i}i\u22650 with the regrets for other reward sequences. Specifically, it is not difficult to see that\nE[Rp(m)A (1, 2m\u03bb+ 1) | {F i}i\u22650] \u2265 c0 max\ni odd (1\u2212 F i)Fm\u22121  m\u22122\u220f j=i+1 F j  \u00b7 \u03bb (8) when m is odd and\nE[Rp(m)A (1, 2m\u03bb+ 1) | {F i}i\u22650] \u2265 c0 max\ni even (1\u2212 F i)Fm\u22121  m\u22122\u220f j=i+1 F j  \u00b7 \u03bb (9) when m is even.\nWe may then use this observation to prove Lemma 5. Let m be an arbitrary number. We shall show that Pr[m \u2208 I] \u2265 12 .\nLet us define the event E(s) be the event so that the suffix of {F i}1\u2264i\u2264m is s. For example, E(000) represents the event that Fm\u22122 = Fm\u22121 = Fm = 0. Let partition the probability space into the following events:\nE(000), E(001), E(010), E(011), E(0100), E(01100), E(11100), E(101), E(0110), E(1110), and E(111).\nFurthermore, we let E0(01100) be the subset of E(01100) such that the last zero in the sequence F 0, ..., Fm\u22125 has an even index. And let E1(01100) = E(01100)\u2212 E0(01100). Similarly, we let \u2022 E0(1110) be the subset of E(1110) such that the last zero in the sequence F 0, ..., Fm\u22124 has an\neven index; let E1(1110) = E(1110)\u2212 E0(1110) \u2022 E0(111) be the subset of E(111) such that the last zero in the sequence F 0, ..., Fm\u22123 has an\neven index; let E1(111) = E(111)\u2212 E0(111) Now the whole probability space can be partitioned into the following events: E(000), E(001), E(010), E(011), E(0100), E(01100), E0(11100), E1(11100) E(101), E(0110), E0(1110), E1(1110) E0(111), E1(111).\nLet 2 be an arbitrary constant such that 0 < 2 < 1. It is not difficult to see that if any of the events above, except for E(000), happens with probability at least T\u2212 2 , then one of pi will have \u03c9( \u221a T ) regret. We will just examine one event to illustrate the idea. The rest of them can be verified in a similar way. Suppose Pr[E(001)] \u2265 T\u2212 2 , we have\nE[Rpm\u22121A (1, T )] \u2265 E[R pm\u22121 A (1, T ) | E(001)] Pr[E(001)]\n\u2265 E[Rpm\u22121A (1, T ) | E(001)] Pr[E(001)] = \u03c9( \u221a T ) (By (8) and (9)).\nThus, we can conclude that Pr[E(000)] \u2265 1\u2212 13T\u2212 2 \u2265 12 for sufficiently large T , which concludes our proof.\nLemma 5. Let i \u2208 I, and let C(i) denote the communication in the ith cycle. Then,\nE[Payoffp(0)A ((4i+ 2)\u03bb\u2212 \u221a T/2 + 1, (4i+ 4)\u03bb+ \u221a T/2)] \u2264 \u03bb+ \u221a T/2\u2212 c0\n\u221a T\nC(i)\nProof. Actually, using Lemma 6 it is easy to see that E[Payoffp(0)A ((4i+ 2)\u03bb+ \u221a T/2 + 1, (4i+ 4)\u03bb\u2212\u221a\nT/2)] \u2264 \u03bb\u2212 \u221a T/2. Now, let us consider the interval, ((4i+ 2)\u03bb\u2212 \u221a T/2 + 1, (4i+ 2)\u03bb+ \u221a T/2).\nLet T0 = (4i+ 2)\u03bb\u2212 \u221a T/2, T1, . . . , Tc = (4i+ 2)\u03bb+ \u221a T/2, be the time-steps when communication occurs. Note that the communication at time-steps T0 and Tc is for free, and that c \u2264 C(i). Let w(x) denote the probability of picking the first expert according to follow the perturbed leader (FPL), if the x is the difference between the cumulative payoff of the first and second expert so far. Thus, if x = \u2212 \u221a T , w(x) = 0 and if x = \u221a T , w(x) = 1. We have,\nw(x) =  1 x > \u221a T 1\u2212 12 ( 1\u2212 x\u221a T )2 0 \u2264 x \u2264 \u221a T 1 2 ( 1 + x\u221a T )2 \u2212 \u221a T \u2264 x \u2264 0\n0 x < \u2212 \u221a T\nThen, we have\nE[Payoffp(0)A ((4i+ 2)\u03bb\u2212 \u221a T/2 + 1, (4i+ 2)\u03bb+ \u221a T/2)] = c\u22121\u2211 j=0 w(Gp(0)(Tj))(Tj+1 \u2212 Tj)\nWe use the following claim (which is an exercise in simple calculus) to complete the proof.\nClaim 1. Let f : [a, b]\u2192 R+ be an increasing function such that f \u2032(x) \u2265 L on [a, b]. Let x0 = a < x1 < \u00b7 \u00b7 \u00b7xc = b, then\nc\u22121\u2211 j=0 f(xj)(xj+1 \u2212 xj) \u2264 \u222b b a f(x)dx\u2212 L(b\u2212 a) 2 c\nNow, notice that Gp(0)(T0) = \u2212 \u221a T/2, Gp (0) (Tc) = \u221a T/2, and \u222b \u221aT/2 \u2212 \u221a T/2 w(x)dx = \u221a T/2. Also,\nw\u2032(x) \u2265 1/(2 \u221a T ). Thus, applying the above claim, we get\nE[Payoffp(0)A ((4i+ 2)\u03bb\u2212 \u221a T/2 + 1, (4i+ 2)\u03bb+ \u221a T/2)] = c\u22121\u2211 j=0 w(Gp(0)(Tj))(Tj+1 \u2212 Tj) \u2264 \u221a T/2\u2212 c0 \u221a T C(i)\nSimilarly, we can prove that.\nE[Payoffp(0)A ((4i+ 4)\u03bb\u2212 \u221a T/2 + 1, (4i+ 4)\u03bb+ \u221a T/2)] = c\u22121\u2211 j=0 w(Gp(0)(Tj))(Tj+1 \u2212 Tj) \u2264 \u221a T/2\u2212 c0 \u221a T C(i)\nAdding up across the three intervals, we can complete the proof the lemma.\nFinally, we prove the following:\nLemma 6. Let {Ti}i\u22651 be point where communication occurs in the algorithm A. Pick some Ti and let \u03c4 > Ti, be such that G p(0)(\u03c4) = Gp(0)(Ti). Then, Payoff p(0) A (Ti + 1, \u03c4) \u2264 (Ti \u2212 t)/2.\nProof. We will instead show that E[Rp(0)A (Ti + 1, \u03c4)] \u2265 0 and observe that both experts have equal payoffs in the time-steps (Ti + 1, \u03c4) since, G p(0 A (Ti) = G p(0 A (\u03c4).\nWe shall construct a new reward sequence p\u2032 such that\n\u2022 p\u2032t = pt0 for all t \u2264 Ti. \u2022 There exists a \u03c4 \u2032 > Ti such that\np\u2032\u03c4 \u2032\n= p\u03c40 = p Ti 0 and ER\np\u2032\nFull(Ti + 1, \u03c4 \u2032) \u2264 ERpA(Ti + 1, \u03c4).\nIn other words, we first construct a new sequence. Then we argue that the local regret by using Full over the new sequence is better than the original regret. Here, Full is an implementation of FPL that communicates at every time step (essentially a non-distributed version). Finally, it is not difficult to see that ERp \u2032\nFull(Ti + 1, \u03c4 \u2032) \u2265 0 because Gp\u2032(Ti + 1) = Gp \u2032 (\u03c4 \u2032), which would complete\nthe proof of the Lemma. Let T` be the largest communicated time step that is no larger than \u03c4 . We use the algorithmic procedure described in Figure 3 to construct the new sequence. Notice that our construction gives the function Gp \u2032 , which indirectly gives p\u2032.\nRoughly speaking, our new p\u2032 uses the \u201cshortest path\u201d to connect between G(Tj) and G(Tj+1) for all Tj between Ti and T`. Then p\n\u2032 is concatenated with another \u201cshortest path\u201d from T` to \u03c4 . For the purpose of our analysis, we also let t(j) be the new time step in p\u2032 that corresponds with the old Tj in p0. We shall prove the following two statements,\n\u2022 For any i \u2264 j \u2264 `\u2212 1,\nE[Rp0A (Tj + 1, Tj+1) | {Ti}i\u22651] \u2265 ER p\u2032 Full(t(j) + 1, t(j + 1)). (10)\n\u2022 Also, E[Rp0A (T` + 1, \u03c4) | {Ti}i\u22651] \u2265 ER p\u2032 Full(t(`) + 1, \u03c4 \u2032). (11)\nOne can see that these two statements are sufficient to prove our claim:\nE[RpA(Ti + 1, \u03c4) | {Ti}i\u22651] \u2265 `\u22121\u2211 j=1 E[Rp \u2032 Full(t(j) + 1, t(j + 1))] + E[R p\u2032 Full(t(`) + 1, \u03c4 \u2032)]\n= E[Rp \u2032 Full(Ti + 1, \u03c4 \u2032)] \u2265 0.\nWe now move to prove (10) and(11). Specifically, we only demonstrate the proof of (10) and the proof for (11) would be similar.\nWithout loss of generality, we may assume that Tj+1 \u2212 Tj \u2264 4\u03bb for any i \u2264 j \u2264 ` \u2212 1 since if within one whole cycle there is no communication, the expected regret for this cycle is 0.\nWe consider the following three cases. Case 1. Tj and Tj+1 are on the same slope of a cycle (i.e. G(t) is monotonic between Tj and Tj+1). In this case, t(j + 1) \u2212 t(j) = Tj\u22121 \u2212 Tj . With straightforward calculation, we can see that Full is always better on p\u2032.\nCase 2. There is only one zig-turn (namely, at time Tz) between Tj and Tj+1. Furthermore, we may assume |Tz\u2212Tj | \u2265 |Tz\u2212Tj+1|. The other case can be proved similarly. Let T \u2032j+1 = Tz\u2212|Tz\u2212Tj+1|. The crucial observation here is that Gp(T \u2032j+1) = G\np(Tj+1). Since there is no communication between time Tj+1 + 1 and T \u2032 j+1, the expected regret in this region is 0, i.e.\nE[RpA(T \u2032 j+1, Tj+1) | {Ti}i\u22651] = 0.\nOn the other hand, since T \u2032j+1 and Tj are on the same slope, running a full communication algorithm is strictly better between Tj and T \u2032 j+1 Finally, notice that the sub-interval G p\u2032(t(j)+1), ...Gp \u2032 (t(j+ 1)) is identical to Gp(Tj + 1), ..., G p(T \u2032j+1) by construction, we have\nE[Rp \u2032\nFull(t(j) + 1, t(j + 1))] \u2265 E[R p A(Tj + 1, T \u2032 j+1)] = E[R p A(Tj + 1, Tj+1)].\nCase 3. There are two zig-turns (namely Tz and Tz\u2032) between Tj and Tj+1. Let T \u2032 j = 2Tz \u2212 Tj and T \u2032j+1 = 2Tz\u2032 \u2212Tj+1. Without loss of generality, let us assume that T \u2032j < T \u2032j+1. Our observation here is that the expected regret between Tj + 1 and T \u2032 j for A is 0. Furthermore, the expected regret between T \u2032j+1 + 1 and Tj+1 is also 0. Then we can apply the arguments appeared in Case 2 again here to show that running Full for the intervals T \u2032j + 1 and T \u2032 j+1 is strictly better than running A. Then we can conclude that E[Rp \u2032\nFull(t(j) + 1, t(j + 1))] \u2265 E[R p A(Tj + 1, Tj+1)] for this case as well."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "We consider the online distributed non-stochastic experts problem, where the distributed<lb>system consists of one coordinator node that is connected to k sites, and the sites are required<lb>to communicate with each other via the coordinator. At each time-step t, one of the k site<lb>nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information<lb>about payoffs of all experts for that round. The goal of the distributed system is to minimize<lb>regret at time horizon T , while simultaneously keeping communication to a minimum. The<lb>two extreme solutions to this problem are: (i) Full communication: This essentially simulates<lb>the non-distributed setting to obtain the optimal O(<lb>\u221a<lb>log(n)T ) regret bound at the cost of<lb>T communication. (ii) No communication: Each site runs an independent copy \u2013 the regret<lb>is O(<lb>\u221a<lb>log(n)kT ) and the communication is 0. This paper shows the difficulty of simultane-<lb>ously achieving regret asymptotically better than<lb>\u221a<lb>kT and communication better than T . We<lb>give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret<lb>O(<lb>\u221a<lb>k5(1+ )/6T ) and communication O(T/k ), for any value of \u2208 (0, 1/5). We also consider<lb>a variant of the model, where the coordinator picks the expert. In this model, we show that<lb>the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near<lb>optimal in regret vs communication trade-off.", "creator": "LaTeX with hyperref package"}}}