{"id": "1211.3402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2012", "title": "Genetic Optimization of Keywords Subset in the Classification Analysis of Texts Authorship", "abstract": "the genetic selection of keywords set, the text frequencies of which are considered as attributes in logical classification analysis, had been analyzed. the sample optimization was performed on a set of words, which is this fraction of the classical dictionary with given frequency limits. the frequency dictionary was formed on the basis of analyzed text array of texts of english fiction. as the fitness function which is minimized by the genetic algorithm, numerical error of nearest k neighbors classifier was used. different obtained readings show true precision and recall of texts classification by authorship categories in the basis of attributes of keywords set which were selected by comb mutation algorithm from the frequency dictionary.", "histories": [["v1", "Wed, 14 Nov 2012 20:04:51 GMT  (779kb)", "http://arxiv.org/abs/1211.3402v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["bohdan pavlyshenko"], "accepted": false, "id": "1211.3402"}, "pdf": {"name": "1211.3402.pdf", "metadata": {"source": "CRF", "title": "Genetic Optimization of Keywords Subset in the Classification Analysis of Texts Authorship", "authors": ["Bohdan Pavlyshenko", "Ivan Franko"], "emails": ["pavlsh@yahoo.com"], "sections": [{"heading": null, "text": " \u00a0 1 \u00a0\nThe analysis of texts authorship takes an important place in data mining. In classification and cluster analysis of textual data the vector model of text documents is widely used, where those documents are represented as vectors in some phase space [Pantel and Turney, 2010]. Text frequencies of words form the basis of this space. One of important tasks is to find the optimal vector subspaces of documents for classification and cluster analysis of text documents [Sebastiani, 2002; Forman, 2003]. In particular the problem lies in the selection of keywords, the text frequencies of which can be used as input parameters for text classifiers with satisfactory precision. The solution of this problem optimizes the required number of calculations and the precision of a classifier in data mining. Different parameters are used in the classification analysis for determining the classification potential of each word. However, a set of words may have general synergistic potential which is not visible in every single word. Genetic algorithms are widely used in the artificial intelligence [Booker et al., 1989; Whitley, 1994]. Genetic optimization can be used for attribute selection in the classification analysis [Vafaie and De Jong, 1992; Raymer et al. , 2000; Tan et al., 2008]. Genetic algorithms are also used for text mining [Atkinson-Abutridy, 2004]. In this paper we use the genetic algorithm to select a subset of keywords from chosen range in the frequency dictionary. This keywords subset forms bases of documents vector space in the authorship classification analysis. For our analysis we have chosen texts of English fiction categorized by authors. As the fitness function for genetic optimization we chose the precision of a classifier by k nearest neighbors."}, {"heading": "Genetic Optimization of the Basis of Documents Vector Space.", "text": "Genetic algorithms are used in a wide class of optimization problems which consist in finding a set of input parameters that minimize some fitness function. As the fitness function we can consider the classifier\u2019s error or some quantitative characteristics of cluster structure that text documents possess. As input parameters of optimization problem we consider a set of keywords that form the basis of text documents vector space. The concept of genetic algorithms consists in using the main principles of Darwin's evolutionary theory, in particular the law of natural selection and genetic variability in optimization problems solving. Let us consider the basic theses of genetic algorithms in the context of the problem of finding an optimal basis for data mining of text documents, in particular on the basis of classification algorithms. In considering the genetic algorithm for finding the optimal keywords basis we use the classical\n \u00a0 2 \u00a0\nscheme [Booker et al., 1989; Whitley, 1994]. A set of input parameters is called a chromosome or individual. In simple case, an individual is formed on the basis of a single chromosome. A set of chromosomes forms a population. A set of vector basis keywords in the context of genetic algorithms we called a keywords chromosome. A classical genetic algorithm includes the following steps:\n1. The initial population of n chromosomes is formed. 2. For each chromosome the fitness function is defined. 3. Based on the specified selection rules, two parent chromosomes are selected on the\nbasis of which a new child chromosome for the next population will be formed. 4. For selected parent pairs a crossover operator is applied, by means of which a new child\nchromosome is formed. 5. The mutation of chromosomes with the given probability is effected. 6. The steps 3-5 are being repeated until a new population of n chromosomes is generated. 7. The steps 2-6 are being repeated until they meet conditions of algorithm stop. Such a\ncondition can be, for example, an assigned set of the fitness function value, or the maximum number of iterations. In discrete optimization with the use of genetic algorithms, the number of steps required to find the optimal set of input parameters is polynomially less as compared to the enumeration of possibilities. This is due to the presence of some sections of chromosomes which are somewhat similiar to genes by their behavior and which collectively make optimization contribution to the fitness function. That means that input parameters are considered as some groups (genes) that chromosomes are being exchanged by, using a crossover operator, which reduces significantly the number of parameters combinations in the optimization analysis. Let us consider a set-theoretical model of the genetic algorithm of optimization selection semantic fields for forming the semantic space of text documents. We consider the evolution of genetic optimization as an ordered set of populations\n{ }||,...2,1| EvkPopEv k == . (1) We assume that one generation of chromosomes is formed by one population. The population consists of a set of chromosomes\n{ } ||,...2,1| ; |,...2,1 | EvkPopjPop kpjkk === \u03c7 . (2) Generally different populations may contain a different number of chromosomes. In simplified case, we suppose that the number of chromosomes is the same in all populations, i. e.\n\u03c7 popk NPopPop == | || | . (3)\nWe consider each chromosome as a set of keywords\n{ }| |,...2,1| ; |,...2,1| ; |...2,1 | EvkPopjiw pijkjk ==== \u03c7\u03c7 \u03c7 , (4) Where i is the index of the keyword position in the chromosome jk\u03c7 of the population kPop . Text documents are represented as vectors of keywords text frequencies wdkjp that mean the frequency of the keyword wj in the text document jd . The set of values wd kjp form the feature-\ndocument matrix where the features are the keywords frequencies in the documents:\n( ) ds NN jk wd kjwd pM , 1,1 == = . (5)\nThe vector\n( )Twd jNwdjwdjwj wpppV ,...,, 21= (6)\n \u00a0 3 \u00a0\ndisplays the document dj in Nw\u2013dimensional space of text documents with the basis formed by keywords. Now we consider the use of the genetic algorithm for the optimization of keywords set in the task of text documents classification. The words with the largest text frequencies carry the minimal semantic information, so it is important to choose such set of words for genetic selection of keywords, which will consist of the words that carry the semantics of a text. Such words in the structure of the frequency dictionary are those of medium and minimum frequencies. As an initial set of attributes we consider some fraction of a frequency dictionary with given frequencies limits\n{ }fiffiig pwppWwwW maxmin )(,| <\u2264\u2208= , (7) where fW is a set of words of the frequency dictionary; fpmin , fpmax are the minimum and\nmaximum limits of the frequency dictionary fraction. As a fitness function for evolutionary optimization of the keywords set of the vector space basis we examine the precision of the classifier. Suppose there are some categories of text documents. These categories may have different nature, for example, they can identify author\u2019s idiolect, discourse, or characterize different objects, phenomena, events, etc. We denote the set of these categories as\n},...,2,1 |{ ctgm NmCtgCategories == , (8)\nwhere |Categories|Nctg = defines the size of categories set. According to given categories the text documents of the document set D are distributed. The task is to find the fitness function that is described by the mapping\n{ }1,0: \u2192\u00d7\u2192 DCategoriesF ctgd . (9)\nThe precision characteristic is widely used to characterize classifiers. The precision of the classifier for the category jCtg is defined as the ratio of the number of items, correctly classified as belonging to the category jCtg , to the total number of items, classified as belonging to the category jCtg\n{ } { }jii jijii j CtgdClassd CtgdCtgdClassd = \u2208\u2227= = )( | )( | Pr , (10)\nwhere )( idClass is the category defined by the classifier. Let us define the fitness function of genetic optimization as follows:\navg ga sF Pr1\u2212= , (11)\nwhere avgPr is the classifier\u2019s precision averaged by all categories. The target of genetic optimization is to minimize the fitness function gasF . As the classification method in the study of genetic optimization we consider the classification by the nearest k neighbors that is called the kNN classification [Sebastiani, 2002; Manning et al., 2008]. This method is referred to as vector classifiers. The basis of vector classification methods is the hypothesis of compactness. According to this hypothesis, the documents belonging to one and the same class form a compact domain, and the domains that belong to different classes do not intersect. As a similarity measure between the documents we chose Euclidean distance. In kNN classification the boundaries of categories are defined locally. Some documents are referred to a category which is dominant for its k neighbors. In the case k = 1 the document obtains the category of its nearest neighbor. Due to the compactness hypothesis a test document d has the same category as most of the documents in the training\n \u00a0 4 \u00a0\nsample in some local spatial neighborhood of a document d. In the genetic selection of semantic fields we use the indices of keywords as input parameters of the optimization problem. The result of genetic optimization will be a set of indices which determines the optimal set of keywords. Experimental part\nThe experimental array of text documents consisted of 503 English fiction texts that were classified by the categories of 17 authors. The study sample consisted of 300 randomly selected documents, and the test sample consisted of 153 documents. The set of keywords for genetic optimization is formed by the first 1000 words of the frequency dictionary which have the text frequency less than 0.001. These words form a frequency interval ]10,1070.7[ 35 \u2212\u2212\u22c5 . The populations of 50 chromosomes size were under analysis. The operator of scattered crossover with the size fraction 0.8 was applied. In each population five elite chromosomes were selected. The chromosomes with the size of 30 and 10 keywords were under analysis. Fig. 1 shows the dynamics of minimum fitness function value and the one averaged over the populations at the chromosome size of 30. The classifier by the k nearest neighbors was chosen for calculating the fitness function which is based on the classification precision. The resulting minimum fitness function value is 0.0858. This fitness function value corresponds to the following set of keywords:\n{name, seven, together, mind, meeting, north, threw, laid, fifty, rate, cast, move, blow, took, showed, opinion, make, shook, leave, feel, times, address, around, chief, next, hall, half, tea,\nworth, started}\nObtained keywords set was used for the classification texts by authorship categories. As an additional classifier characteristics the recall Rcj is used, which is defined as the ratio of the number of items, correctly classified as belonging to the category jCtg , to the total number of items, which belong to the category jCtg .\n \u00a0 5 \u00a0\n0.8220=Rc . In case of random formation of training and test samples, the distribution precision and recall by authors\u2019 categories will vary with each application of the classifier to the test sample. However, the main parameters such as the average values of precision and recall will be similar.\nFig.2 Classifier\u2019s precision Pr and recall Rc for the authorship categories\nat optimized set of 30 words.\nWe also used the genetic optimization of keywords set, when the size of the chromosome was 10 keywords. In this case the dimension of documents vector space was 10. Fig. 3 shows the dynamics of minimum and averaged over the population fitness function value at the chromosome size of 10. The resulting minimum fitness function value is equal to 0.1923.\n \u00a0 6 \u00a0\nFig.4. Classifier\u2019s precision Pr and recall Rc for the authorship categories\nat optimized set of 10 words."}, {"heading": "Summary and Conclusions", "text": "The paper described the genetic optimization of keywords, the frequencies of which are the components of documents vectors and they act as attributes in text classification analysis. The genetic optimization was performed on the set of words, which is the fraction of the frequency dictionary with given frequency limits. The frequency dictionary was formed on the basis of analyzed text array of texts of English fiction. As the fitness function which is minimized by the genetic algorithm, the error of nearest k neighbors classifier was used. The obtained results show high precision and recall of texts classification by authorship categories on the basis of multiple attributes of keywords that were selected by the genetic algorithm from the frequency dictionary."}], "references": [{"title": "Combining Information Extraction with Genetic Algorithms for Text Mining", "author": ["J. Atkinson-Abutridy", "C. Mellish", "S. Aitken"], "venue": "IEEE Intelligent Systems", "citeRegEx": "Atkinson.Abutridy et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Atkinson.Abutridy et al\\.", "year": 2004}, {"title": "Classifier Systems and Genetic Algorithms", "author": ["L.B. Booker", "D.E. Goldberg", "J.H. Holland"], "venue": "Artificial Intelligence", "citeRegEx": "Booker et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Booker et al\\.", "year": 1989}, {"title": "An extensive empirical study of feature selection metrics for text classification", "author": ["G. Forman"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Forman,? \\Q2003\\E", "shortCiteRegEx": "Forman", "year": 2003}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "Raghavan P", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P. Pantel", "P.D. Turney"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Pantel and Turney,? \\Q2010\\E", "shortCiteRegEx": "Pantel and Turney", "year": 2010}, {"title": "Dimensionality Reduction Using Genetic Algorithms", "author": ["M.L. Raymer", "W.F. Punch", "E.D. Goodman", "L.A. Kuhn", "A.K. Jain"], "venue": "IEEE Transactions on Evolutionary Computation. 4, 164\u2013171.", "citeRegEx": "Raymer et al\\.,? 2000", "shortCiteRegEx": "Raymer et al\\.", "year": 2000}, {"title": "Machine Learning in Automated Text Categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys. 34, 1\u201347.", "citeRegEx": "Sebastiani,? 2002", "shortCiteRegEx": "Sebastiani", "year": 2002}, {"title": "A genetic algorithm-based method for feature subset selection", "author": ["F. Tan", "X. Fu", "Zhang Y", "A.G. Bourgeois"], "venue": "Soft Computing", "citeRegEx": "Tan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2008}, {"title": "Genetic algorithm as a tool for feature selection in machine learning", "author": ["Vafaie", "K.H. De Jong"], "venue": "Fourth International Conference on Tools with Artificial Intelligence. TAI '92,", "citeRegEx": "Vafaie and Jong,? \\Q1992\\E", "shortCiteRegEx": "Vafaie and Jong", "year": 1992}, {"title": "A genetic algorithm tutorial", "author": ["D. Whitley"], "venue": "Statistics and Computing", "citeRegEx": "Whitley,? \\Q1994\\E", "shortCiteRegEx": "Whitley", "year": 1994}], "referenceMentions": [{"referenceID": 4, "context": "In classification and cluster analysis of textual data the vector model of text documents is widely used, where those documents are represented as vectors in some phase space [Pantel and Turney, 2010].", "startOffset": 175, "endOffset": 200}, {"referenceID": 6, "context": "One of important tasks is to find the optimal vector subspaces of documents for classification and cluster analysis of text documents [Sebastiani, 2002; Forman, 2003].", "startOffset": 134, "endOffset": 166}, {"referenceID": 2, "context": "One of important tasks is to find the optimal vector subspaces of documents for classification and cluster analysis of text documents [Sebastiani, 2002; Forman, 2003].", "startOffset": 134, "endOffset": 166}, {"referenceID": 1, "context": "Genetic algorithms are widely used in the artificial intelligence [Booker et al., 1989; Whitley, 1994].", "startOffset": 66, "endOffset": 102}, {"referenceID": 9, "context": "Genetic algorithms are widely used in the artificial intelligence [Booker et al., 1989; Whitley, 1994].", "startOffset": 66, "endOffset": 102}, {"referenceID": 7, "context": "Genetic optimization can be used for attribute selection in the classification analysis [Vafaie and De Jong, 1992; Raymer et al. , 2000; Tan et al., 2008].", "startOffset": 88, "endOffset": 154}, {"referenceID": 1, "context": "2 scheme [Booker et al., 1989; Whitley, 1994].", "startOffset": 9, "endOffset": 45}, {"referenceID": 9, "context": "2 scheme [Booker et al., 1989; Whitley, 1994].", "startOffset": 9, "endOffset": 45}, {"referenceID": 6, "context": "As the classification method in the study of genetic optimization we consider the classification by the nearest k neighbors that is called the kNN classification [Sebastiani, 2002; Manning et al., 2008].", "startOffset": 162, "endOffset": 202}, {"referenceID": 3, "context": "As the classification method in the study of genetic optimization we consider the classification by the nearest k neighbors that is called the kNN classification [Sebastiani, 2002; Manning et al., 2008].", "startOffset": 162, "endOffset": 202}], "year": 2012, "abstractText": "The genetic selection of keywords set, the text frequencies of which are considered as attributes in text classification analysis, has been analyzed. The genetic optimization was performed on a set of words, which is the fraction of the frequency dictionary with given frequency limits. The frequency dictionary was formed on the basis of analyzed text array of texts of English fiction. As the fitness function which is minimized by the genetic algorithm, the error of nearest k neighbors classifier was used. The obtained results show high precision and recall of texts classification by authorship categories on the basis of attributes of keywords set which were selected by the genetic algorithm from the frequency dictionary.", "creator": "Word"}}}