{"id": "1212.5524", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2012", "title": "Reinforcement learning for port-Hamiltonian systems", "abstract": "passivity - based control for human - hamiltonian systems provides an intuitive way of achieving stabilization between rendering a buffer fluid with sensitivity to a desired storage function. actually, in most instances domain control law has to be maintained by solving a complex simple differential equation ( pde ). this paper considers energy - balancing passivity - based control ( eb - pbc ), which is a form of pbc assuming which the closed - loop energy is reactive to the difference between the stored and supplied energies. practitioners propose a method to parameterize eb - pbc that preserves the systems's pde matching conditions, does not require the specification of a truly desired hamiltonian, utilizes performance criteria, and is robust without extra non - linearities underlying one control input saturation. the parameters of specific control processes are found for actor - critic reinforcement learning, enabling learning near - optimal control policies satisfying a desired closed - loop energy landscape. the advantages are that near - optimal controllers can be generated using standard energy shaping techniques then that the feedback learned can be interpreted in terms of energy shaping during damping injection, another makes it accessible to numerically assess stability using passivity theory. from the reinforcement learning perspective, our proposal allows evolving the class of port - balancing systems to be incorporated in the actor - critic framework, speeding up the learning factor to the resulting parameterization of the policy. the method has been successfully applied to the pendulum swing - up problem in quantitative and real - life experiments.", "histories": [["v1", "Fri, 21 Dec 2012 16:57:28 GMT  (213kb,D)", "https://arxiv.org/abs/1212.5524v1", "submitted"], ["v2", "Thu, 22 Aug 2013 16:16:31 GMT  (263kb,D)", "http://arxiv.org/abs/1212.5524v2", "submitted"]], "COMMENTS": "submitted", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["olivier sprangers", "gabriel a d lopes", "robert babuska"], "accepted": false, "id": "1212.5524"}, "pdf": {"name": "1212.5524.pdf", "metadata": {"source": "CRF", "title": "Reinforcement learning for port-Hamiltonian systems", "authors": ["Olivier Sprangers", "Gabriel A. D. Lopes"], "emails": ["osprangers@gmail.com;", "r.babuska}@tudelft.nl"], "sections": [{"heading": null, "text": "Reinforcement learning for port-Hamiltonian systems\nOlivier Sprangers, Gabriel A. D. Lopes, and Robert Babus\u030cka\nAbstract\u2014Passivity-based control (PBC) for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law is obtained without any performance considerations and it has to be calculated by solving a complex partial differential equation (PDE). In order to address these issues we introduce a reinforcement learning approach into the energy-balancing passivity-based control (EBPBC) method, which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a technique to parameterize EB-PBC that preserves the systems\u2019s PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust to extra non-linearities such as control input saturation. The parameters of the control law are found using actor-critic reinforcement learning, enabling learning near-optimal control policies satisfying a desired closedloop energy landscape. The advantages are that near-optimal controllers can be generated using standard energy shaping techniques and that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the reinforcement learning perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the actor-critic framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments.\nIndex Terms\u2014Reinforcement learning, port-Hamiltonian systems, passivity-based control, energy balancing, actor-critic\nI. INTRODUCTION\nPASSIVITY-based control (PBC) [1] is a methodologythat achieves the control objective by rendering a system passive with respect to a desired storage function [2]. Different forms of PBC have been successfully applied to design robust controllers [3] for mechanical systems and electrical circuits [2], [4]. A key feature of PBC is that it exploits structural properties of the system. In this paper, we are interested in the passivity-based control of systems endowed with a special structure, called port-Hamiltonian (PH) systems. PH systems have been widely used in PBC applications [5], [6]. Their geometric structure allows reformulating a PBC problem in terms of solving a set of partial differential equations (PDE\u2019s). Much research in the literature concerns solving or simplifying such generally complex PDE\u2019s [2].\nSprangers, Lopes, and Babus\u030cka are with the Delft Center for Systems & Control, Delft University of Technology, Mekelweg 2, 2628 CD Delft, The Netherlands. Email: osprangers@gmail.com; {g.a.delgadolopes, r.babuska}@tudelft.nl\nManuscript received XXXX, 2013; revised XXXX, 2013.\nThe drive for passivity-based control of port-Hamiltonian systems is grounded in the search for global stability, thus strongly relying on models. Other control techniques have been developed when no models are known and performance is important. One such example is reinforcement learning (RL) [7]. RL is a semi-supervised learning control method that can solve optimal (stochastic) control problems for nonlinear systems, without the need for a process model or for explicitly solving complex equations. In RL the controller receives an immediate numerical reward as a function of the process state and possibly control action. The goal is to find an optimal control policy that maximizes the cumulative long-term rewards, which corresponds to maximizing a value function [7]. In this paper, we use actor-critic techniques [8], which are a class of RL methods in which a separate actor and critic are learned. The critic approximates the value function and the actor the policy (control law). Actor-critic reinforcement learning is suitable for problems with continuous state and action spaces. A general disadvantage of RL is that the progress of learning can be very slow and non-monotonic. However, by incorporating (partial) model knowledge, learning can be sped up [9].\nIn this paper we address two important issues: First, we propose a learning control structure within the PH framework that retains important properties of the PBC. To this end, first a parameterization of a particular type of PBC, called energybalancing passivity-based control (EB-PBC), is proposed such that the PDE arising in EB-PBC can be split into a nonassignable part satisfying a matching condition following from the EB-PBC framework and an assignable part that can be parameterized. Then, by applying actor-critic reinforcement learning the parameterized part can be learned while automatically verifying the matching PDE. This can be seen as a paradigm shift from the traditional model-based control synthesis for PH systems: we do not seek to synthesize a controller in closed-form, but we aim instead to learn one online with proper structural constraints. This brings a number of advantages: I) It allows to specify the control goal in a \u201clocal\u201d fashion through a reward function, without having to consider the entire global behavior of the system. The simplest example to illustrate this idea is by considering a reward function to be 1 when the system is in a small neighborhood of the desired goal and 0 everywhere else [7]. The learning algorithm will eventually find a global control policy. In the model-based PBC synthesis counterpart one needs to specify a desired global Hamiltonian. II) Learning brings performance in addition to the intrinsic stability properties of PBC. The structure of RL is such that the rewards are maximized, ar X iv :1 21 2.\n55 24\nv2 [\ncs .S\nY ]\n2 2\nA ug\n2 01\n3\nand these can include performance criteria, such as minimal time, energy consumption, etc. III) Learning offers additional robustness and adaptability since it tolerates model uncertainty in the PH framework.\nFrom a learning control point of view, we present a systematic way of incorporating a priori knowledge into the RL problem. The approach proposed in this paper yields, after learning, a controller that can be interpreted in terms of energy shaping control strategies. The same interpretability is typically not found in the traditional RL solutions.\nThus, this work combines the advantages of both aforementioned control techniques, PBC and RL, and mitigates some of their respective disadvantages. Historically, the trends in control synthesis have oscillated between performance and stabilization. PBC of PH systems is rooted in the stability of multi-domain nonlinear systems. By including learning we aim to address performance in the PH framework. In the experimental section of the paper, we show that our method is also robust to unmodeled nonlinearities, such as control input saturation. Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16]. We show that our approach solves the problem of control input saturation on the learning side without the need of augmenting the model-based PBC.\nThe work presented in this paper draws an interesting parallel with the application of iterative feedback tuning (IFT) [17] in the PH framework [18]. Both techniques optimize the parameters of the controller online, with the difference that in IFT the objective is to minimize the error between the desired output and the measured output of the system, while our approach aims at maximizing a reward function, that can be very general. The choice of RL is warranted by its semi-supervised paradigm, as opposed to other traditional fully-supervised learning techniques, such as artificial neural networks or fuzzy approximators where the control specification (function approximation information) is input/output data instead of reward functions. Such fully-supervised techniques can be used within RL as function approximators to represent the value functions and control policies. Genetic algorithms can also be considered as an alternative to RL since they rely on fitness functions that are analogous to the rewards functions of RL. We aim to explore such classes of algorithms in our future work.\nThe theoretical background on PH systems and actorcritic reinforcement learning is described in Section II and Section III, respectively. In Section IV, our proposal for a parameterization of input-saturated EB-PBC control, compatible with actor-critic reinforcement learning, is introduced. We then specialize this result to mechanical systems in Section V. Section VI provides simulation and experimental results for the problem of swinging up an input-saturated inverted pendulum and Section VII concludes the paper."}, {"heading": "II. PORT-HAMILTONIAN SYSTEMS", "text": "Port-Hamiltonian (PH) systems are a natural way of representing a physical system in terms of its energy exchange with the environment through ports [4]. The general framework\nof PH systems was introduced in [19] and was formalized in [20], [3]. In this paper, we consider the input-state-output representation of the PH system which is of the form1:\n\u03a3 : { x\u0307 = [J(x)\u2212R(x)]\u2207xH(x) + g(x)u y = gT (x)\u2207xH(x)\n(1)\nwhere x \u2208 Rn is the state vector, u \u2208 Rm, m \u2264 n is the control input, J,R : Rn \u2192 Rn\u00d7n with J(x) = \u2212J(x)T and R(x) = R(x)T \u2265 0 are the interconnection and damping matrix, respectively, H : Rn \u2192 R the Hamiltonian which is the stored energy in the system, u, y \u2208 Rm are conjugated variables whose product has the unit of power and g : Rn \u2192 Rn\u00d7m is the input matrix assumed to be full rank. For the remainder of this paper, we denote:\nF (x) := J(x)\u2212R(x) (2)\nThis matrix satisfies F (x) + FT (x) = \u22122R(x) \u2264 0. System (1) satisfies the power-balance equation [2]:\nH\u0307(x) = (\u2207xH(x))T x\u0307 = \u2212 (\u2207xH(x))T R(x)\u2207xH(x) + uT y (3)\nSince R(x) \u2265 0, we obtain:\nH\u0307(x) \u2264 uT y (4)\nwhich is called the passivity inequality, if H(x) is positive semi-definite, and cyclo-passivity inequality, if H(x) is not positive semi-definite nor bounded from below [2]. Hence, systems satisfying (4) are called (cyclo-)passive systems. The goal is to obtain the target closed-loop system:\n\u03a3cl : x\u0307 = [J(x)\u2212Rd(x)]\u2207xHd(x) (5)\nthrough energy shaping using EB-PBC [2] and damping injection, such that Hd(x) is the desired closed-loop energy which has a minimum at the desired equilibrium x\u2217 and satisfies:\nH\u0307d(x) = \u2212 (\u2207xHd(x))T Rd(x)\u2207xHd(x) (6)\nwhich implies (cyclo-)passivity according to (3)-(4) if the desired damping Rd(x) \u2265 0. Hence, the control objective is achieved by rendering the closed-loop system passive with respect to the desired storage function Hd(x)."}, {"heading": "A. Energy Shaping", "text": "Define the added energy function:\nHa(x) := Hd(x)\u2212H(x) (7)\nA state-feedback law ues(x) is said to satisfy the energybalancing property if it satisfies:\nH\u0307a(x) = \u2212uTes(x)y (8)\nIf (8) holds, the desired energy Hd(x) is the difference between the stored and supplied energy. Assuming g(x) \u2208 Rn\u00d7m, m < n, rank {g(x)} = m, the control law:\nues(x) = g \u2020(x)F (x)\u2207xHa(x) (9)\n1We use the notation \u2207x := \u2202/\u2202x. Furthermore, all (gradient) vectors are column vectors.\nwith g\u2020(x) = (gT (x)g(x))\u22121gT (x) solves the EB-PBC problem with Ha(x) a solution of the following set of PDE\u2019s [2]:[\ng\u22a5(x)FT (x) gT (x)\n] \u2207xHa(x) = 0 (10)\nwith g\u22a5(x) \u2208 R(n\u2212m)\u00d7n the full rank left-annihilator of g(x), i.e. g\u22a5(x)g(x) = 0."}, {"heading": "B. Damping Injection", "text": "Damping is injected by feeding back the (new) passive output gT (x)\u2207xHd(x),\nudi(x) = \u2212K(x)gT (x)\u2207xHd(x) (11)\nwith K(x) \u2208 Rm\u00d7m,K(x) = KT (x) \u2265 0 such that:\nRd(x) = R(x) + g(x)K(x)g T (x) (12)\nHence, the full control law consists of an energy shaping part and a damping injection part:\nu(x) = ues(x) + udi(x)\n= g\u2020(x)F (x)\u2207xHa(x) \u2212K(x)gT (x)\u2207xHd(x) (13)"}, {"heading": "III. ACTOR-CRITIC REINFORCEMENT LEARNING", "text": "In reinforcement learning, the system to be controlled (called \u2018environment\u2019 in the RL literature) is modeled as a Markov decision process (MDP). In a deterministic setting, this MDP is defined by the tuple M(X,U, f, \u03c1), where X is the state space, U the action space and f : X \u00d7 U \u2192 X the state transition function that describes the process to be controlled that returns the state xk+1 after applying action uk in state xk. The vector xk is obtained by applying a zero-order hold discretization xk = x(kTs) with Ts the sampling time. The reward function is defined by \u03c1 : X\u00d7U \u2192 R and returns a scalar reward rk+1 = \u03c1(xk+1, uk) after each transition. The goal of RL is to find an optimal control policy \u03c0 : X \u2192 U by maximizing an expected cumulative or total reward described as some function of the immediate expected rewards. In this paper, we consider a discounted sum of rewards. The value function V \u03c0 : X \u2192 R,\nV \u03c0(x) = \u221e\u2211 i=0 \u03b3ir\u03c0k+i+1\n= \u221e\u2211 i=0 \u03b3i\u03c1(xk+i+1, \u03c0(xk+i)), x = xk (14)\napproximates this discounted sum during learning while following policy \u03c0 where \u03b3 \u2208 [0, 1) is the discount factor.\nWhen dealing with large and/or continuous state and action spaces, it is necessary to approximate the value function and policy. Actor-critic (AC) algorithms [21], [8] learn a separate actor (policy \u03c0) and critic (value function V \u03c0). The critic approximates and updates (improves) the value function. Then, the actor\u2019s parameters are updated in the direction of that improvement. The actor and critic are usually defined by a differentiable parameterization such that gradient ascent can be used to update the parameters. This is beneficial when dealing\nwith continuous action spaces [22]. In this paper, the temporaldifference based Standard Actor-Critic (S-AC) algorithm from [9] is used. Define the approximated policy \u03c0\u0302 : Rn \u2192 Rm and the approximated value function as V\u0302 : Rn \u2192 R. Denote the parameterization of the actor by \u03d1 \u2208 Rp and of the critic by \u03b8 \u2208 Rq . The temporal difference [7]:\n\u03b4k+1 := rk+1 + \u03b3V\u0302 (xk+1, \u03b8k)\u2212 V\u0302 (xk, \u03b8k) (15)\nis used to update the critic parameters using the following gradient ascent update rule:\n\u03b8k+1 = \u03b8k + \u03b1c\u03b4k+1\u2207\u03b8V\u0302 (xk, \u03b8k) (16)\nin which \u03b1c > 0 is the learning rate. Eligibility traces ek \u2208 Rq [7] can be used to speed up learning by including reward information about previously visited states. The update for the critic parameters becomes:\nek+1 = \u03b3\u03bbek +\u2207\u03b8V\u0302 (xk, \u03b8k) (17) \u03b8k+1 = \u03b8k + \u03b1c\u03b4k+1ek+1 (18)\nwith \u03bb \u2208 [0, 1) the trace-decay rate. The policy approximation can be updated in a similar fashion, as described below. RL needs exploration in order to visit new, unseen parts of the state-action space so as to possibly find better policies. This is achieved by perturbing the policy with a exploration term \u2206uk. Many techniques have been developed for choosing the type of exploration term (see e.g. [23]). In this paper we consider \u2206uk to be random with zero-mean. In the experimental section we choose \u2206uk to be have a normal distribution. The overall control action now becomes:\nuk = \u03c0\u0302(xk, \u03d1k) + \u2206uk (19)\nThe policy update is such that the policy parameters are updated towards (away from) \u2206uk if the temporal difference (15) is positive (negative). This leads to the following policy update rule:\n\u03d1k+1 = \u03d1k + \u03b1a\u03b4k+1\u2206uk\u2207\u03d1\u03c0\u0302(xk, \u03d1k) (20)\nwith \u03b1a > 0 the actor learning rate."}, {"heading": "IV. ENERGY-BALANCING ACTOR-CRITIC", "text": "In this section we present our main results. Our approach is that we will use the PDE (10) and split it into an assignable, parameterizable part and an unassignable part that satisfies the matching condition. In this way, it is possible to parameterize the desired closed-loop Hamiltonian Hd(x) and simultaneously satisfy (10). After that, we parameterize the damping matrix K(x). The two parameterized variables \u2014 the desired closed-loop energy Hd(x) and damping K(x) \u2014 are then suitable for Actor-Critic RL by defining two actors for these variables. First, we reformulate the PDE (10) in terms of the desired closed-loop energy Hd(x) by applying (7):[\ng\u22a5(x)FT (x) gT (x) ] \ufe38 \ufe37\ufe37 \ufe38\nA(x)\n(\u2207xHd(x)\u2212\u2207xH(x)) = 0 (21)\nand we denote the kernel of A(x) as:\nker(A(x)) = {N(x) \u2208 Rn\u00d7b : A(x)N(x) = 0} (22)\nsuch that (21) reduces to:\n\u2207xHd(x)\u2212\u2207xH(x) = N(x)a (23)\nwith a \u2208 Rb. Suppose that (an example is given further on) the state vector x can be split, such that x = [wT zT ]T , where z \u2208 Rc and w \u2208 Rd, c+d = n corresponding to the zero and non-zero elements of N(x) such that:[\n\u2207wHd(x) \u2207zHd(x) ] \u2212 [ \u2207wH(x) \u2207zH(x) ] = [ Nw(x) 0 ] a (24)\nWe assume that the matrix Nw(x) is rank d, which is always true for fully actuated mechanical systems (see Section V). It is clear that\u2207zHd(x) = \u2207zH(x), which we call the matching condition, and hence \u2207zHd(x) cannot be chosen freely. Thus, only the desired closed-loop energy gradient vector \u2207wHd(x) is free for assignment. We consider a \u03be-parameterized total desired energy with the following form:\nH\u0302d(x, \u03be) := H(x) + \u03be T\u03c6H(w) + H\u0304d(w) + C (25)\nwhere \u03beT\u03c6H(w) represents a linear-in-parameters basis function approximator (\u03be \u2208 Re a parameter vector and \u03c6H(w) \u2208 Re the basis function, with e chosen sufficiently large to represent the assignable desired closed-loop energy), H\u0304d(w) is an arbitrary function of w, and C chosen to render H\u0302d(x, \u03be) non-negative. The function H\u0302d(x, \u03be) automatically verifies (24). To guarantee that the system is passive in relation to the storage function H\u0302d(x, \u03be) the basis functions \u03c6H(w) should be chosen to be bounded such that \u03beT\u03c6H(w) does not grow unbounded towards \u2212\u221e when ||w|| \u2192 \u221e. Moreover, it is important to constrain the minima of H\u0302d(x, \u03be) to be the desired equilibrium x\u2217, via the choice of the basis functions. The elements of the desired damping matrix K(x) of (13), denoted K\u0302(x,\u03a8), can be parameterized in a similar way:\n[K\u0302(x,\u03a8)]ij = f\u2211 l=1 [\u03a8]ijl[\u03c6K(x)]l (26)\nwith \u03a8 \u2208 Rm\u00d7m\u00d7f and\n[\u03a8]ijl = [\u03a8]jil (27)\na parameter vector such that K\u0302(x,\u03a8) = K\u0302T (x,\u03a8), (i, j) = 1, . . . ,m and \u03c6K(x) \u2208 Rf basis functions. We purposefully do not impose K\u0302(x,\u03a8) \u2265 0 to allow the injection of energy in the system via the damping term. This idea has been used in [24] to overcame the dissipation obstacle when synthesizing controllers by interconnection. Although this breaches the passivity criterion of (4) in our particular case we show that local stability can still be numerically demonstrated using passivity analysis in Section VI-C. This choice is made based on the knowledge that the standard Energy Balancing PBC method (without any extra machinery to accommodate saturation) cannot stabilize in the up position a saturated-input pendulum system starting from the down position. As such, this choice illustrates the power of RL in finding alternative routes to obtain control policies, such as\ninjecting energy though the damping term. In other settings enforcing that K\u0302(x,\u03a8) \u2265 0 benefits the stability analysis. The control law (13) now becomes (when no ambiguity is present, the function arguments are dropped to improve readability):\nu(x, \u03be,\u03a8) = g\u2020(x)F (x)\n[ \u2207wH\u0302d(x, \u03be)\u2212\u2207wH(x)\n0 ] \u2212 K\u0302(x,\u03a8)gT (x)\u2207xH\u0302d(x, \u03be)\n= g\u2020F\n[ DTw\u03c6H\u03be +\u2207wH\u0304d\n0 ] \u2212 K\u0302gT [ DTw\u03c6H\u03be +\u2207wH\u0304d +\u2207wH\n\u2207zH\n] (28)\nNow, we are ready to introduce the update equations for the parameter vectors \u03be, [\u03a8]ij . Denote by \u03bek, [\u03a8k]ij the value of the parameters at the discrete time step k. The policy \u03c0\u0302 of the actor-critic reinforcement learning algorithm is chosen equal to the control law parameterized by (28):\n\u03c0\u0302(xk, \u03bek,\u03a8k) := u(xk, \u03bek,\u03a8k) (29)\nIn this paper we take the control input saturation problem into account by considering a generic saturation function \u03c2 : Rm \u2192 S, S \u2282 Rm, such that:\n\u03c2(u(x)) \u2208 S \u2200u (30)\nwhere S is the set of valid control inputs. The control action with exploration (19) becomes:\nuk = \u03c2 (\u03c0\u0302(xk, \u03bek, \u03c8k) + \u2206uk) (31)\nwhere \u2206uk is drawn from a desired distribution. The exploration term to be used in the actor update (20) must be adjusted to respect the saturation:\n\u2206u\u0304k = uk \u2212 \u03c0\u0302(xk, \u03bek, \u03c8k) (32)\nNote that due to this step, the exploration term \u2206u\u0304k used in the learning algorithm is no longer drawn from the chosen distribution present in \u2206uk. Furthermore, we obtain the following gradients of the saturated policy:\n\u2207\u03be\u03c2(\u03c0\u0302) = \u2207\u03c0\u0302\u03c2(\u03c0\u0302)\u2207\u03be\u03c0\u0302 (33) \u2207[\u03a8]ij \u03c2(\u03c0\u0302) = \u2207\u03c0\u0302\u03c2(\u03c0\u0302)\u2207[\u03a8]ij \u03c0\u0302 (34)\nAlthough not explicitly indicated in the previous equations, the (lack of) differentiability of the saturation function \u03c2 has to be considered for the problem at hand such that the computation of the gradient of \u03c2 can be made. For a traditional saturation in ui \u2208 R of the form max(umin,min(umax, ui)), i.e. assuming each input ui is bounded by umin and umax, then the gradient of \u03c2 is the zero matrix outside the unsaturated set S (i.e. when ui < umin or ui > umax). For other types of saturation the function \u2207\u03c2 must be computed. Finally, the actor parameters \u03bek, [\u03a8k]ij are updated according to (20), respecting the saturated policy gradients. For the parameters of the desired Hamiltonian we obtain:\n\u03bek+1 = \u03bek + \u03b1a,\u03be\u03b4k+1\u2206u\u0304k\u2207\u03be\u03c2 (\u03c0\u0302(xk, \u03bek,\u03a8k)) (35)\nAlgorithm 1 Energy-Balancing Actor-Critic Input: System (1), \u03bb, \u03b3, \u03b1a for each actor, \u03b1c.\n1: e0(x) = 0 \u2200x 2: Initialize x0, \u03b80, \u03be0, \u03a80 3: k \u2190 1 4: loop 5: Execute: 6: Draw \u2206uk \u223c N (0, \u03c32), calculate action uk = \u03c2 (\u03c0\u0302(xk, \u03bek, \u03c8k) + \u2206uk), \u2206u\u0304k = uk \u2212 \u03c0\u0302(xk, \u03bek, \u03c8k) 7: Observe next state xk+1 and calculate reward rk+1 = \u03c1(xk+1, uk) 8: Critic: 9: Temporal difference: \u03b4k+1 = rk+1 + \u03b3V\u0302 (xk+1, \u03b8k)\u2212 V\u0302 (xk, \u03b8k)\n10: Eligibility trace: ek+1 = \u03b3\u03bbek +\u2207\u03b8V\u0302 (xk, \u03b8k) 11: Critic update: \u03b8k+1 = \u03b8k + \u03b1c\u03b4k+1ek+1 12: Actors: 13: Actor 1 (H\u0302d(x, \u03be)): \u03bek+1 = \u03bek + \u03b1a,\u03be\u03b4k+1\u2206u\u0304k\u2207\u03be\u03c2 (\u03c0\u0302(xk, \u03bek,\u03a8k)) 14: Actor 2 (K\u0302(x,\u03a8)): 15: for i, j = 1, . . . ,m do 16: [\u03a8k+1]ij = [\u03a8k]ij + \u03b1a,[\u03a8]ij\u03b4k+1\u2206u\u0304k\u2207[\u03a8k]ij \u03c2 (\u03c0\u0302(xk, \u03bek,\u03a8k)) 17: end for 18: end loop\nand for the parameters of the desired damping we have:\n[\u03a8k+1]ij = [\u03a8k]ij+\n\u03b1a,[\u03a8]ij\u03b4k+1\u2206u\u0304k\u2207[\u03a8k]ij \u03c2 (\u03c0\u0302(xk, \u03bek,\u03a8k)) (36)\nwhere (i, j) = 1, . . . ,m, while observing (27). Algorithm 1 gives the entire Energy-Balancing Actor-Critic algorithm with input saturation.\nThe dynamics of the Energy-Balancing Actor Critic Algorithm 1 raises a number of questions regarding stability and convergence: are the good stability properties of the traditional energy-balancing PBC lost? In effect this is not the case, as if the parameter \u03be is fixed then stability is preserved, in the sense that the system is passive to the storage function H\u0302d(x, \u03be), assuming bounded basis functions describing H\u0302d(x, \u03be) as discussed after equation (25) and that the dissipation matrix K\u0302 is semi-positive definite. A related question is if during learning (while the parameter \u03be is evolving) will the Hamiltonian H\u0302d(x, \u03be) capture the desired control specification. One cannot assume that the desired Hamiltonian will immediately fulfil the control specification, since if that was the case then no learning is needed. In the RL community it is generally accepted that during learning no stability and convergence guarantees can be given [7], as exploration is a necessary component of the framework. In our framework, we cannot guaranteed convergence during learning, but by constraining the desired Hamiltonian we can prevent the total energy to grow unbounded, avoiding possible instabilities. Another relevant question is: will RL converge in this setting? The control law in (28) can be rewritten as\nu = \u03c6\u03041(x) + \u2211 ij \u03bei\u03c6\u03042,i(x) + \u03a8\u0304j \u03c6\u03043,j(x) + \u03bei\u03a8\u0304j \u03c6\u03044,ij(x)\nwith \u03a8\u0304 representing the stacked version of \u03a8, meaning that the policy is parameterised in an affine bilinear way, as opposed to the standard linear in parameter representations found in\nthe standard actor-critic literature. As such, we cannot at this moment take advantage of the existing actor-critic RL convergence proofs since we violate this condition. Given this hurdle we observe, however, that in practice not only will the RL component converge very fast (faster then traditional model-free RL) but throughout learning the system never gets unstable, as presented next in Section VI. Additionally, the resulting policy performs as well as standard model-free approximated RL algorithms. The last point to consider is that since the Energy Balancing PBC suffers from the dissipation obstacle [2], limiting its applicability to special classes of systems such as mechanical systems, the algorithm we present contains the same limitation. Eliminating such limitation is ongoing work (see also the results presented in [24])"}, {"heading": "V. MECHANICAL SYSTEMS", "text": "To illustrate an application of the method, consider a fully\nactuated mechanical system of the form:\n\u03a3m :  [ q\u0307 p\u0307 ] = [ 0 I \u2212I \u2212R\u0304 ] [ \u2207qH(q, p) \u2207pH(q, p) ] + [ 0 I ] u y = [ 0 I ] [\u2207qH(q, p) \u2207pH(q, p)\n] (37) with q \u2208 Rn\u0304, p \u2208 Rn\u0304 (n\u0304 = n2 , n even) the generalized positions and momenta, respectively, and R\u0304 \u2208 Rn\u0304\u00d7n\u0304 the damping matrix. The system admits (1) with R\u0304 > 0 and the Hamiltonian:\nH(q, p) = 1\n2 pTM\u22121(q)p+ P (q) (38)\nwith M(q) = MT (q) > 0 the inertia matrix and P (q) the potential energy. For the system (37) it holds that rank {g(x)} = n\u0304 and the state vector can be split into part w = [q1, q2, . . . , qn\u0304]\nT and part z = [p1, p2, . . . , pn\u0304]T . Since g(x) = [0 I]T its annihilator can be written as g\u22a5(x) = [g\u0304(x) 0], for an arbitrary matrix g\u0304(x). This means that only\nthe potential energy can be shaped, which is widely known in EB-PBC for mechanical systems. The approximated desired closed-loop energy (25) reads:\nH\u0302d(x, \u03be) = 1\n2 pTM\u22121(q)p+ \u03beT\u03c6H(q) (39)\nwhere the first term represents the unassignable part, i.e. the kinetic energy of the system Hamiltonian (38), and the second term \u03beT\u03c6H(q) the assignable desired potential energy. The actor updates can be defined for each parameter according to (35)\u2013(36). For underactuated mechanical systems, e.g. G = [0 I]\nT , the split state vector z is enlarged with those q-coordinates that cannot be actuated directly because these coordinates correspond to the zero elements of N(x), (i.e. the matrix Nq(x) is no longer rank n\u0304)."}, {"heading": "VI. EXAMPLE: PENDULUM SWING-UP", "text": "To validate our method, the problem of swinging up an inverted pendulum subject to control saturation is studied in simulation and using the actual physical setup depicted in Fig. 1.\nThe pendulum swing-up is a low-dimensional, but highly nonlinear control problem commonly used as a benchmark in the RL literature [9] and it has also been studied in PBC [11]. The equations of motion admit (37) and read:\n\u03a3p :  [ q\u0307 p\u0307 ] = [ 0 1 \u22121 \u2212R\u0304(q\u0307) ] [ \u2207qH(q, p) \u2207pH(q, p) ] + [ 0 Kp Rp ] u y = [ 0\nKp Rp ] [\u2207qH(q, p) \u2207pH(q, p) ] (40) with q the angle of the pendulum and p the angular momentum, thus we denote the full measurable state x = [q, p]T . The damping term is:\nR\u0304(q\u0307) = bp + K2p Rp + \u03c3p |q\u0307|\n(41)\nfor which it holds that R\u0304(q\u0307) > 0, \u2200q\u0307. Note that the fraction \u03c3p/|q\u0307| arrises from the modelled Coulomb friction fc = \u03c3p sign(q\u0307) = \u03c3p/|q\u0307|q\u0307 = \u03c3p/|q\u0307| p/Jp. Furthermore, we denote the Hamiltonian:\nH(q, p) = p2\n2Jp + P (q) (42)\nwith: P (q) = Mpgplp(1 + cos q) (43)\nThe model parameters are given in Table I. The desired Hamiltonian (25) reads:\nH\u0302d(x, \u03be) = p2\n2Jp + \u03beT\u03c6H(q) (44)\nOnly the potential energy can be shaped that we denote by P\u0302d(q, \u03be) = \u03be\nT\u03c6H(q). Furthermore, as there is only one input, K\u0302(x,\u03a8) becomes a scalar:\nK\u0302(x, \u03c8) = \u03c8T\u03c6K(x) (45)\nThus, control law (28) results in:\nu(x, \u03be, \u03c8) = g\u2020F\n[ \u03beT\u2207q\u03c6H \u2212\u2207qP\n0\n] \u2212 K\u0302gT [ \u03beT\u2207q\u03c6H J\u22121p p ] = \u2212Rp\nKp\n( \u03beT\u2207q\u03c6H +Mpgplp sin(q) ) (46)\n\u2212 Kp Rp \u03c8T\u03c6K q\u0307 (47)\nwhich we define as the policy \u03c0\u0302(x, \u03be, \u03c8). Hence, we have two actor updates:\n\u03bek+1 = \u03bek + \u03b1a,\u03be\u03b4k+1\u2206u\u0304k\u2207\u03be\u03c2 (\u03c0\u0302(xk, \u03bek, \u03c8k)) (48) \u03c8k+1 = \u03c8k + \u03b1a,\u03c8\u03b4k+1\u2206u\u0304k\u2207\u03c8\u03c2 (\u03c0\u0302(xk, \u03bek, \u03c8k)) (49)\nfor the desired potential energy P\u0302d(q, \u03be) and the desired damping K\u0302(x, \u03c8), respectively."}, {"heading": "A. Function Approximation", "text": "To approximate the critic and the two actors, function approximators are necessary. In this paper we use the Fourier basis [25] because of its ease of use, the possibility to incorporate information about the symmetry in the system and the ability to ascertain properties useful for stability analysis of this specific problem. The periodicity of the function approximators obtained via a Fourier basis is compatible with the topology of the configuration space of the pendulum, defined to be S1 \u00d7 R. We define a multivariate N th-order2 Fourier basis for n dimensions as:\n\u03c6i(x\u0304) = cos(\u03c0c T i x\u0304), i \u2208 {1, . . . , (N + 1)n} (50)\nwith ci \u2208 Zn, which means that all possible N + 1 integer values, or frequencies, are combined in a vector in Zn to create\n2\u2018Order\u2019 refers to the order of approximation; \u2018dimensions\u2019 to the number of states in the system.\na matrix c \u2208 Zn\u00d7(N+1)n containing all possible frequency combinations. For example,\nc1 = [0 0] T , c2 = [1 0] T , . . . , c(3+1)2 = [4 4] T (51)\nfor a 3rd-order Fourier basis in 2 dimensions. The state x is scaled according to:\nx\u0304i = xi \u2212 xi,min\nxi,max \u2212 xi,min (x\u0304i,max \u2212 x\u0304i,min) + x\u0304i,min (52)\nfor i = 1, . . . , n with (x\u0304i,min, x\u0304i,max) = (\u22121, 1). Projecting the state variables onto this symmetrical range has several advantages. First, this means that the policy will be periodic with period T = 2, such that it wraps around (i.e., modulo 2\u03c0) and prevents discontinuities at the boundary values of the angle (x = [\u03c0 \u00b1 , p], very small). This way we are taking into consideration the topology of the system: for the inverted pendulum we have that q \u2208 S1 and p \u2208 R. Second, learning will be faster because updating the value function and policy for some x also applies to the sign-opposite value of x. Third, \u02d9\u0302Pd(0, \u03be) = 0 by the choice of parameterization, which is beneficial for stability analysis. Although the momentum will now also be periodic in the value function and policy, this is not a problem because the value function and policy approximation are restricted to a domain and the momentum itself is also restricted to the same domain using saturation. We adopt the adjusted learning rate from [25] such that:\n\u03b1ai,\u03be = \u03b1ab,\u03be \u2016ci\u20162 , \u03b1ai,\u03c8 = \u03b1ab,\u03c8 \u2016ci\u20162\n(53)\nfor i = 1, . . . , (N+1)n with \u03b1ab,\u03be, \u03b1ab,\u03c8 the base learning rate for the two actors (Table II) and \u03b1a1,\u03be = \u03b1ab,\u03be, \u03b1a1,\u03c8 = \u03b1ab,\u03c8 to avoid division by zero for c1 = [0 0]T . Equation (53) implies that parameters corresponding to basis functions with higher (lower) frequencies are learned slower (faster). The parameterizations described above result in \u02d9\u0302Hd(x\u2217, \u03be) = 0 for all \u03be, where x\u2217 = [0, 0]T is the goal state. This entails that the goal state is a critical point of the Hamiltonian throughout the learning process, in effect speeding up the RL convergence."}, {"heading": "B. Simulation", "text": "The task is to learn to swing up and stabilize the pendulum from the initial position pointing down x0 = [\u03c0, 0]T to the desired equilibrium position at the top x\u2217 = [0, 0]T . Since the control action is saturated, the system is not able to swing up the pendulum directly, but rather it must swing back and forth to build up momentum to eventually reach the equilibrium. The reward function \u03c1 is defined such that it has its maximum in the desired unstable equilibrium and penalizes other states via:\n\u03c1(x, u) = Qr (cos(q)\u2212 1)\u2212Rrp2 (54)\nwith: Qr = 25 , Rr = 0.1\nJ2p (55)\nThis reward function is consistent with the mapping S1 \u2192 R for the angle and proved to improve performance over a purely\nquadratic reward, such as the one used in e.g. [9]. For the critic, we define the basis function approximation as:\nV\u0302 (x, \u03b8) = \u03b8T\u03c6c(x) (56)\nwith \u03c6c(x) a 3rd-order Fourier basis resulting in 16 learnable parameters \u03b8 in the domain [qmin, qmax] \u00d7 [pmin, pmax] = [\u2212\u03c0, \u03c0] \u00d7 [\u22128\u03c0Jp, 8\u03c0Jp]. Actor 1 (P\u0302d(q, \u03be)) is parameterized using a 3rd-order Fourier basis in the range [\u2212\u03c0, \u03c0] resulting in 4 learnable parameters. Actor 2 (K\u0302(x, \u03c8)) is also parameterized using a 3rd-order Fourier basis for the full state space, in the same domain as the critic. Exploration is done at every time step by randomly perturbing the action with a normally distributed zero-mean white noise with standard deviation \u03c3 = 1, i.e.:\n\u2206u \u223c N (0, 1) (57)\nWe incorporate saturation by defining the saturation function (30) as:\n\u03c2(uk) = { uk if |uk| \u2264 umax sgn(uk)umax otherwise\n(58)\nRecall that the saturation must be taken into account in the policy gradients by applying (33)-(34). The parameters were all initialized with zero vectors of appropriate dimensions, i.e. (\u03b80, \u03be0, \u03c80) = 0. The algorithm was first run with the system simulated in Matlab for 200 trials of three seconds each (with a near-optimal policy, the pendulum needs approximately one second to swing up). Each trial begins in the initial position x0. This simulation was repeated 50 times to get an estimate of the average, minimum, maximum and confidence regions for the learning curve. The simulation parameters are given in Table II. Fig. 2 shows the average learning curve obtained\nafter 50 simulations. The algorithm shows good convergence and on average needs about 2 minutes (40 trials) to reach a near-optimal policy. The initial drop in performance is caused by the zero-initialization of the value function (critic), which is too optimistic compared to the true value function. Therefore, the controller explores a large part of the state space and receives a lot of negative rewards before it learns the true value of the states. A simulation using the policy learned in a typical experiment is given in Fig. 3a. As can be seen, the pendulum swings back once to build up momentum to eventually get to the equilibrium. The desired Hamiltonian H\u0302d(x, \u03be) (44), acquired through learning, is given in Fig. 3b. There are three minima, of which one corresponds to the\ndesired equilibrium. The other two equilibria are undesirable wells that come from the shaped potential energy P\u0302d(q, \u03be) (Fig. 4a). These minima are the result of the algorithm trying to swing up the pendulum in a single swing, which is not possible due to the saturation. Hence, a swing-up strategy is necessary to avoid staying in these wells. The number of these undesirable wells is a function of the control saturation and of the number of basis functions chosen to approximate P\u0302d(q, \u03be). The learned damping K\u0302(x, \u03c8) (Fig. 4b) is positive (white) towards the equilibrium thus extracting energy from the system, while it is negative (gray) in the region of the initial state. The latter corresponds to pumping energy into the system, which is necessary to build up momentum for the swing-up and to escape the undesirable wells of P\u0302d(q, \u03be) (see discussion of expression (26)). A disadvantage is that control law (47), with the suggested basis functions, is always zero for the set \u2126 = {x | x = (0+j\u03c0, 0), j = 1, 2, . . . } which implies that it is zero not only at the desired equilibrium, but also at the initial state x0. During learning this is not a problem because there is constant exploration, but after learning the system should not be initialized in exactly x0 otherwise it\nwill stay in this set. It can be overcome by initializing with a small perturbation around x0. In real-life systems it will also be less a problem because there is generally noise present on the sensors."}, {"heading": "C. Stability of the Learned Controller", "text": "Since control saturation is present, the target dynamics do not satisfy (5). Hence, to conclude local stability of x\u2217 based on (6), we calculate \u02d9\u0302Hd(x, \u03be) for the unsaturated case (Fig. 5a)3 and the saturated case ( \u02d9\u0302Hd,sat(x, \u03be)) and compute the sign of the difference (Fig. 5b). By looking at Fig. 5b, it appears that \u2203\u03b4 \u2282 Rn : |x\u2212 x\u2217| < \u03b4 such that \u02d9\u0302Hd,sat(x, \u03be) = \u02d9\u0302 Hd(x, \u03be). It can be seen from Fig. 5b that such a \u03b4 exists, i.e., a small gray region around the equilibrium x\u2217 exists. Hence, we can use \u02d9\u0302Hd(x, \u03be) around x\u2217 and assess stability using (6). From Fig. 3b it follows that H\u0302d(x, \u03be) > 0 for all states in Fig. 3b. From Fig. 4a we infer that locally,\narg min P\u0302d(q, \u03be) = x \u2217; \u02d9\u0302 Pd(x \u2217, \u03be) = 0; \u00a8\u0302 Pd(x \u2217, \u03be) > 0 (59)\nthe latter two of which naturally result from the basis function definition. Furthermore, from Fig. 4b it can be seen that around x\u2217, K\u0302(x, \u03c8) > 0. Hence, in a region \u03b4 around x\u2217,\nH\u0302d(x, \u03be) > 0; \u02d9\u0302 Hd(x, \u03be) \u2264 0; \u02d9\u0302Hd(x\u2217, \u03be) = 0 (60)\nwhich implies local asymptotic stability of x\u2217. Extensive simulations show that similar behaviour is always achieved."}, {"heading": "D. Real-time Experiments", "text": "Using the physical setup shown in Fig. 1, 20 learning experiments were run using identical settings as in the simulations. The result is given in Fig. 6. The algorithm shows slightly slower convergence - about 3 minutes of learning (60 trials) to reach a near-optimal policy instead of 40 - and a\n3Fig. 5a is sign-opposite to Fig. 4b, which is logical, because the negative (positive) regions of K\u0302(x, \u03c8) correspond to negative (positive) damping which corresponds to a positive (negative) value of \u02d9\u0302Hd(x, \u03be) based on (6).\nsgn\n(\n\u02d9\u0302 Hd(x, \u03c8)\u2212 \u02d9\u0302Hd,sat(x, \u03c8)\n)\nindicating regions where\n\u02d9\u0302 Hd(x, \u03c8) = \u02d9\u0302 Hd,sat(x, \u03c8) (gray) and \u02d9\u0302 Hd(x, \u03c8) 6= \u02d9\u0302Hd,sat(x, \u03c8) (white). Black dots indicate the simulated trajectory.\nless consistent average when compared to Fig. 2. This can be attributed to a combination of model mismatch and the symmetrical basis functions (through which it is not possible to incorporate non-symmetrical friction that is present in the real system). Overall though, the performance can be considered good when compared to the simulation results. Also, the same performance dip is present which can again be attributed to the optimistic value function initialization."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we have presented a method to systematically parameterize EB-PBC control laws that is robust to extra nonlinearities such as input control saturation. The parameters are then found by making use of actor-critic reinforcement learning. In this way, we are able to learn a closed-loop energy landscape for PH systems. The advantages are that optimal controllers can be generated using energybased control techniques, there is no need to specify a global\nsystem Hamiltonian, and the solutions acquired by means of reinforcement learning can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. By making use of the model knowledge the actor-critic method is able to quickly learn near-optimal policies. A drawback is that for multiple input systems, generating many actor updates for the desired damping matrix can be computationally expensive. We have found that the proposed Energy Balancing Actor Critic algorithm performs very well in a physical mechanical setup. Due to the intrinsic energy boundedness of the learned desired Hamiltonian, we have observed that the system never gets unstable during learning. We are currently active on the extension of the algorithms presented to an IDA-PBC setting, such that more classes of systems can be addressed and more freedom is given in shaping the desired Hamiltonian (e.g. Kinetic energy shaping)."}], "references": [{"title": "Adaptive motion control of rigid robots: a tutorial", "author": ["R. Ortega", "M.W. Spong"], "venue": "Automatica, vol. 25, pp. 877\u2013888, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Control by interconnection and standard passivity-based control of port-Hamiltonian systems", "author": ["R. Ortega", "A. van der Schaft", "F. Castanos", "A. Astolfi"], "venue": "IEEE Transactions on Automatic Control, vol. 53, no. 11, pp. 2527\u20132542, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Putting energy back in control", "author": ["R. Ortega", "A. van der Schaft", "I. Mareels", "B. Maschke"], "venue": "IEEE Control Systems Magazine, vol. 21, no. 2, pp. 18\u201333, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling and Control for Efficient Bipedal Walking Robots: A Port-Based", "author": ["V. Duindam", "S. Stramigioli"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Control of Interactive Robotic Interfaces: a port-Hamiltonian approach", "author": ["C. Secchi", "S. Stramigioli", "C. Fantuzzi"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "On actor-critic algorithms", "author": ["V. Konda", "J. Tsitsiklis"], "venue": "SIAM Journal on Control and Optimization, vol. 42, no. 4, pp. 1143\u20131166, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient model learning methods for actor-critic control", "author": ["I. Grondman", "M. Vaandrager", "L. Bu\u015foniu", "R. Babu\u0161ka", "E. Schuitema"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, In press.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 0}, {"title": "Stabilization and H\u221e control of nonlinear portcontrolled Hamiltonian systems subject to actuator saturation", "author": ["Y.W.A. Wei"], "venue": "Automatica, vol. 46, no. 12, pp. 2008\u20132013, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A family of smooth controllers for swinging up a pendulum", "author": ["K. \u00c5strom", "J. Aracil", "F. Gordillo"], "venue": "Automatica, vol. 44, pp. 1841\u20131848, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1841}, {"title": "Output-feedback global stabilization of a nonlinear benchmark system using a saturated passivity-based controller", "author": ["G. Escobar", "R. Ortega", "H. Sira-Ramirez"], "venue": "IEEE Transactions on Control Systems Technology, vol. 7, no. 2, pp. 289\u2013293, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Iterative learning control of Hamiltonian systems: I/O based optimal control approach", "author": ["K. Fujimoto", "T. Sugie"], "venue": "IEEE Transactions on Automatic Control, vol. 48, no. 10, pp. 1756\u20131761, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Port Hamiltonian systems: A unified approach for modeling and control finite and infinite dimensional physical systems", "author": ["A. Macchelli"], "venue": "Ph.D. dissertation, University of Bologna, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "A variable structure approach to energy shaping", "author": ["A. Macchelli", "C. Melchiorri", "C. Secchi", "C. Fantuzzi"], "venue": "Proceedings of the European Control Conference, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Global asymptotic and finite-gain L2 stabilization of port-controlled Hamiltonian systems subject to actuator saturation", "author": ["W. Sun", "Z. Lin", "Y. Wang"], "venue": "Proceedings of the American Control Conference, St. Louis, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Iterative feedback tuning: an overview", "author": ["H. Hjalmarsson"], "venue": "International Journal Adaptive Control and Signal Processing, vol. 16, pp. 373\u2013395, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Iterative feedback tuning for Hamiltonian systems", "author": ["K. Fujimoto", "I. Koyama"], "venue": "Proceedings of the 17th IFAC World Congress, Seoul, Korea, 2008, pp. 15 678\u201315 683.  XXXXXXXXX, VOL. XX, NO. XX, XXXXX 2013  10", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Port-controlled Hamiltonian systems: modelling origins and system theoretic properties", "author": ["B. Maschke", "A. van der Schaft"], "venue": "Proceedings of the third Conference on nonlinear control systems (NOLCOS), 1992, pp. 282\u2013288.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1992}, {"title": "On the Hamiltonian formulation of nonholonomic mechanical systems", "author": ["A. van der Schaft", "B. Maschke"], "venue": "Reports on Mathematical Physics, vol. 34, pp. 225\u2013233, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A. Barto", "R. Sutton", "C. Anderson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 13, pp. 835\u2013846, 1983.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1983}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "Advances in Neural Information Processing Systems, vol. 12, pp. 1057\u2013 1063, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M. Littman", "A. Nouri", "D. Wingate"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009, pp. 19\u201326.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Casimir-Based Control Beyond the Dissipation Obstacle", "author": ["J. Koopman", "D. Jeltsema"], "venue": "arXiv.org, Jun. 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Value function approximation in reinforcement learning using the Fourier basis", "author": ["G. Konidaris", "S. Osentoski"], "venue": "Autonomous Learning Laboratory, Computer Science Department, University of Massachusetts Amherst, Tech. Rep. 101, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "PASSIVITY-based control (PBC) [1] is a methodology that achieves the control objective by rendering a system passive with respect to a desired storage function [2].", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "PASSIVITY-based control (PBC) [1] is a methodology that achieves the control objective by rendering a system passive with respect to a desired storage function [2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Different forms of PBC have been successfully applied to design robust controllers [3] for mechanical systems and electrical circuits [2], [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "Different forms of PBC have been successfully applied to design robust controllers [3] for mechanical systems and electrical circuits [2], [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "PH systems have been widely used in PBC applications [5], [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "PH systems have been widely used in PBC applications [5], [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "Much research in the literature concerns solving or simplifying such generally complex PDE\u2019s [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "One such example is reinforcement learning (RL) [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "The goal is to find an optimal control policy that maximizes the cumulative long-term rewards, which corresponds to maximizing a value function [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 6, "context": "In this paper, we use actor-critic techniques [8], which are a class of RL methods in which a separate actor and critic are learned.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "However, by incorporating (partial) model knowledge, learning can be sped up [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "The simplest example to illustrate this idea is by considering a reward function to be 1 when the system is in a small neighborhood of the desired goal and 0 everywhere else [7].", "startOffset": 174, "endOffset": 177}, {"referenceID": 8, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "The work presented in this paper draws an interesting parallel with the application of iterative feedback tuning (IFT) [17] in the PH framework [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "The work presented in this paper draws an interesting parallel with the application of iterative feedback tuning (IFT) [17] in the PH framework [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "Port-Hamiltonian (PH) systems are a natural way of representing a physical system in terms of its energy exchange with the environment through ports [4].", "startOffset": 149, "endOffset": 152}, {"referenceID": 17, "context": "The general framework of PH systems was introduced in [19] and was formalized in [20], [3].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "The general framework of PH systems was introduced in [19] and was formalized in [20], [3].", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "System (1) satisfies the power-balance equation [2]:", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "which is called the passivity inequality, if H(x) is positive semi-definite, and cyclo-passivity inequality, if H(x) is not positive semi-definite nor bounded from below [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "through energy shaping using EB-PBC [2] and damping injection, such that Hd(x) is the desired closed-loop energy which has a minimum at the desired equilibrium x\u2217 and satisfies: \u1e22d(x) = \u2212 (\u2207xHd(x)) Rd(x)\u2207xHd(x) (6)", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "with g\u2020(x) = (g (x)g(x))\u22121gT (x) solves the EB-PBC problem with Ha(x) a solution of the following set of PDE\u2019s [2]: [ g\u22a5(x)FT (x) g (x) ] \u2207xHa(x) = 0 (10)", "startOffset": 111, "endOffset": 114}, {"referenceID": 19, "context": "Actor-critic (AC) algorithms [21], [8] learn a separate actor (policy \u03c0) and critic (value function V ).", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "Actor-critic (AC) algorithms [21], [8] learn a separate actor (policy \u03c0) and critic (value function V ).", "startOffset": 35, "endOffset": 38}, {"referenceID": 20, "context": "This is beneficial when dealing with continuous action spaces [22].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "In this paper, the temporaldifference based Standard Actor-Critic (S-AC) algorithm from [9] is used.", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "The temporal difference [7]:", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "Eligibility traces ek \u2208 R [7] can be used to speed up learning by including reward information about previously visited states.", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": "[23]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This idea has been used in [24] to overcame the dissipation obstacle when synthesizing controllers by interconnection.", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "In the RL community it is generally accepted that during learning no stability and convergence guarantees can be given [7], as exploration is a necessary component of the framework.", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "The last point to consider is that since the Energy Balancing PBC suffers from the dissipation obstacle [2], limiting its applicability to special classes of systems such as mechanical systems, the algorithm we present contains the same limitation.", "startOffset": 104, "endOffset": 107}, {"referenceID": 22, "context": "Eliminating such limitation is ongoing work (see also the results presented in [24])", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "The pendulum swing-up is a low-dimensional, but highly nonlinear control problem commonly used as a benchmark in the RL literature [9] and it has also been studied in PBC [11].", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "The pendulum swing-up is a low-dimensional, but highly nonlinear control problem commonly used as a benchmark in the RL literature [9] and it has also been studied in PBC [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 23, "context": "In this paper we use the Fourier basis [25] because of its ease of use, the possibility to incorporate information about the symmetry in the system and the ability to ascertain properties useful for stability analysis of this specific problem.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "c1 = [0 0] T , c2 = [1 0] T , .", "startOffset": 20, "endOffset": 25}, {"referenceID": 2, "context": ", c(3+1)2 = [4 4] T (51)", "startOffset": 12, "endOffset": 17}, {"referenceID": 2, "context": ", c(3+1)2 = [4 4] T (51)", "startOffset": 12, "endOffset": 17}, {"referenceID": 23, "context": "We adopt the adjusted learning rate from [25] such that:", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "[9].", "startOffset": 0, "endOffset": 3}], "year": 2013, "abstractText": "Passivity-based control (PBC) for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law is obtained without any performance considerations and it has to be calculated by solving a complex partial differential equation (PDE). In order to address these issues we introduce a reinforcement learning approach into the energy-balancing passivity-based control (EBPBC) method, which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a technique to parameterize EB-PBC that preserves the systems\u2019s PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust to extra non-linearities such as control input saturation. The parameters of the control law are found using actor-critic reinforcement learning, enabling learning near-optimal control policies satisfying a desired closedloop energy landscape. The advantages are that near-optimal controllers can be generated using standard energy shaping techniques and that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the reinforcement learning perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the actor-critic framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments.", "creator": "LaTeX with hyperref package"}}}