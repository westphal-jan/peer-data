{"id": "1301.3889", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Pivotal Pruning of Trade-offs in QPNs", "abstract": "qualitative probabilistic had - been designed for probabilistic reasoning completely a qualitative way. due to their coarse level of representation detail, qualitative probabilistic networks don't provide for particular trade - offs and typically yield compelling results upon inference. we present research algorithm for computing more insightful results for resolution trade - offs. the algorithm builds upon the idea of probability pivots only zoom in on the trade - offs and identifying the information that also serve to resolve them.", "histories": [["v1", "Wed, 16 Jan 2013 15:52:25 GMT  (277kb)", "http://arxiv.org/abs/1301.3889v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["silja renooij", "linda c van der gaag", "simon parsons", "shaw green"], "accepted": false, "id": "1301.3889"}, "pdf": {"name": "1301.3889.pdf", "metadata": {"source": "CRF", "title": "Pivotal Pruning of Trade-offs in QPNs", "authors": ["Silja Renooij", "Linda C. van der Gaag", "Simon Parsons"], "emails": [], "sections": [{"heading": null, "text": "Qualitative probabilistic networks have been de signed for probabilistic reasoning in a qualita tive way. Due to their coarse level of represen tation detail, qualitative probabilistic networks do not provide for resolving trade-offs and typ ically yield ambiguous results upon inference. We present an algorithm for computing more in sightful results for unresolved trade-offs. The al gorithm builds upon the idea of using pivots to zoom in on the trade-offs and identifying the in formation that would serve to resolve them.\n1 INTRODUCTION\nQualitative probabilistic networks were introduced in the early 1990s for probabilistic reasoning with uncertainty in a qualitative way [Wellman, 1990]. A qualitative prob abilistic network encodes variables and the probabilistic relationships between them in a directed acyclic graph. The encoded relationships basically represent influences on probability distributions. Each of these influences is sum marised by a qualitative sign indicating the direction of shift in one variable's distribution occasioned by a shift in another variable's distribution. For probabilistic inference with qualitative networks, an elegant algorithm based upon the idea of propagating and combining signs is available [Druzdzel & Henrion, 1993a] .\nQualitative probabilistic networks capture the relationships between their variables at a coarse level of representation detail. As a consequence, these networks do not provide for resolving trade-offs, that is, for establishing the net re sult of two or more conflicting influences on a variable's probability distribution. If trade-offs are represented in a qualitative network, then probabilistic inference will typi cally yield ambiguous results. Once an ambiguity arises, it will spread throughout most of the network upon infer ence, even if only a very small part of the network is truly ambiguous.\nThe issue of dealing with trade-offs in qualitative prob abilistic networks has been addressed before by several researchers. S. Parsons has introduced, for example, the concept of categorical influences. A categorical influence is either an influence that serves to increase a probability to 1 or an influence that decreases a probability to 0, re gardless of any other influences, and thereby resolves any trade-off in which it is involved [Parsons, 1995] . C.-L. Liu and M.P. Wellman have designed a method for resolving trade-offs based upon the idea of reverting to numerical probabilities whenever necessary [Liu & Wellman, 1998]. S. Renooij and L.C. van der Gaag have enhanced the ba sic formalism of qualitative probabilistic networks by dis tinguishing between strong and weak influences. Trade off resolution during inference is then based on the idea that strong influences dominate over conflicting weak ones [Renooij & Van der Gaag, 1999] . These approaches to trade-off resolution are all based on a refinement of the rep resentation used in the basic formalism.\nIn this paper, we present a new algorithm for dealing with trade-offs in qualitative probabilistic networks. Rather than resolving trade-offs by providing for a finer level of rep resentation detail, our algorithm identifies the information that would serve to resolve the trade-offs present in a quali tative probabilistic network. From this information, a more insightful result than ambiguity is constructed.\nOur algorithm for dealing with trade-offs builds upon the idea of zooming in on the part of a qualitative probabilis tic network where the actual trade-offs reside. After a new observation has been entered into the network, probabilis tic inference will provide the sign of the influence of this observation on the variable of interest, given previously en tered observations. If this sign is ambiguous, then there are trade-offs present in the network. In fact, a trade-off must reside along the reasoning chains between the observation and the variable of interest. Our algorithm isolates these reasoning chains to constitute the part of the network that is relevant for addressing the trade-offs present. From this relevant part, an informative result is constructed for the variable of interest in terms of values for the variables in-\n516 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nvolved and the relative strengths of the influences between them.\nWe believe that qualitative probabilistic networks can play an important role in the construction of quantitative proba bilistic networks for real-life application domains, as well as for explanation of their reasoning processes. The con struction of a probabilistic network typically sets out with the construction of the network's digraph. As the assess ment of the various probabilities required is a far harder task, it is performed only when the network's digraph is considered robust. Now, by assessing signs for the influ ences modelled in the digraph, a qualitative network is ob tained that can be exploited for studying the projected prob abilistic network's reasoning behaviour prior to the assess ment of its probabilities. For this purpose, algorithms are required that serve to derive as much information as possi ble from a qualitative probabilistic network. We look upon our algorithm as a first step to this end.\nThe paper is organised as follows. In Section 2, we pro vide some preliminaries concerning qualitative probabilis tic networks. In Section 3, we introduce our algorithm for zooming in on trade-offs informally, by means of an ex ample. The algorithm is discussed in further detail in Sec tion 4. The paper ends with some concluding observations in Section 5.\n2 PRELIMINARIES\nA qualitative probabilistic network encodes the statistical variables from a domain of application and the probabilis tic relationships between them in a directed acyclic graph G = (V(G) , A( G) ) . Each node in the set V(G) repre sents a statistical variable. Each arc in the set A( G) can be looked upon as expressing a causal influence from the node at the tail of the arc on the node at the arc's head. More for mally, the set of arcs captures probabilistic independence among the represented variables. We say that a chain be tween two nodes is blocked if it includes either an observed node with at least one outgoing arc or an unobserved node with two incoming arcs and no observed descendants. If all chains between two nodes are blocked, then these nodes are said to be d-separated and the corresponding variables are considered conditionally independent given the entered observations [Pearl, 1988] .\nA qualitative probabilistic network associates with its di graph G a set \ufffd of qualitative influences and synergies [Wellman, 1990] . A qualitative influence between two nodes expresses how the values of one node influence the probabilities of the values of the other node. A positive qualitative influence of node A on its successor B expresses that observing higher values for A makes higher values for B more likely, regardless of any other direct influences on B; the influence is denoted S6(A, B) , where'+' is the in fluence's sign. A negative qualitative influence, denoted\nS(j, and a zero qualitative influence, denoted S2,, are de fined analogously. If the influence of node A on node B is not monotonic or unknown, we say that it is ambiguous, denoted Sb(A, B) .\nThe set of influences of a qualitative probabilistic net work exhibits various properties [Wellman, 1990] . The property of symmetry states that, if the network includes the influence Sb(A, B) , then it also includes Sb(B, A) , J E { +, -, 0, ?}. The property of transitivity asserts that qualitative influences along a simple chain that specifies at most one incoming arc for each node, combine into a single influence with the \u00ae-operator from Table 1. The property of composition asserts that multiple influences between two nodes along parallel chains combine into a single influence with the EB-operator.\nIn addition to influences, a qualitative probabilistic net work includes synergies that express how the value of one node influences the probabilities of the values of another node in view of a value for a third node [Druzdzel & Henri on, 1993b] . A negative product synergy of node A on node B (and vice versa) given the value c for their common successor C, denoted X 0 ( {A, B}, c) , ex presses that, given c, higher values for A render higher val ues for B less likely. Positive, zero, and ambiguous prod uct synergies are defined analogously. A product synergy induces a qualitative influence between the predecessors of a node upon observation of that node; the induced influence is coined an intercausal influence. In this paper, we assume that induced intercausal influences are added to a qualita tive probabilistic network's graph as undirected edges.\nFor probabilistic inference with a qualitative probabilis tic network, an elegant algorithm is available from M.J. Druzdzel and M. Henrion (1993a) ; this algorithm is summarised in pseudocode in Figure 1. The basic idea of the algorithm is to trace the effect of observing a node's value on the other nodes in a network by message-passing\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 517\nbetween neighbouring nodes. For each node, a node sign is determined, indicating the direction of change in the node's probability distribution occasioned by the new ob servation given all previously observed node values. Ini tially, all node signs equal ' 0'. For the newly observed node, an appropriate sign is entered, that is, either a '+' for the observed value true or a '-' for the value false, by calling PropagateSign(observed node, observed node, sign). Each node receiving a message updates its sign and sub sequently sends a message to each neighbour that is not d-separated from the observed node and to every node on which it exerts an induced intercausal influence. The sign of this message is the 0-product of the node's (new) sign and the sign of the influence it traverses. This process is re peated throughout the network, building on the properties of symmetry, transitivity, and composition of influences. The process repeatedly visits each node that needs a change of sign. Since a node can change sign at most twice, once from 0 to +or-, and then only to ?, each node is visited at most twice. The process is therefore guaranteed to halt.\n3 OUTLINE OF THE ALGORITHM\nIf a qualitative probabilistic network models trade-offs, it will typically yield ambiguous results upon inference with the sign-propagation algorithm. From Table 1, we have that whenever two conflicting influences on a node are combined with the EEl-operator, an ambiguous sign will re sult. Once an ambiguous sign is introduced, it wil.l spread throughout most of the network and an ambiguous sign is likely to result for the node of interest. By zooming in on the part of the network where the actual trade-offs reside and identifying the information that would serve to resolve these trade-offs, a more insightful result can be constructed. We illustrate the basic idea of our algorithm to this end.\nFigure 3: The Result of Propagating '+' for Node H.\nhas been observed for the node H and that we are inter ested in its influence on the probability distribution of node A. Tracing the influence of the node sign '+' for node H, indicating its observed value, on every node's distribution by means of the sign-propagation algorithm, results in the node signs shown in Figure 3. These signs reveal that at least one trade-off must reside along the reasoning chains between the observed node H and the node of interest A. These chains together constitute the part of the network that is relevant for addressing the trade-offs that have given rise . to the ambiguous result for node A; we term this part the relevant network. For the example, the relevant network is shown in Figure 4 below the dashed line. Our algorithm now isolates this relevant network for further investigation. To this end, it deletes from the network all nodes and arcs that are connected to, but no part of the reasoning chains fromH to A.\nA relevant network for addressing trade-offs typically in cludes many nodes with ambiguous node signs. Often, however, only a small number of these nodes are actually involved in the trade-offs that have given rise to the am biguous result for the node of interest. Figures 3 and 4, for example, reveal that, while the nodes A, B, and C have ambiguous node signs, the influences between them are not conflicting. In fact, every possible unambiguous node sign sign[C] for node C would result in the unambiguous sign sign[C]0 ( ( + 0-) EB-) = sign[C]0- for node A. For addressing the trade-offs involved, therefore, the part of the relevant network between node C and node A can be dis regarded. Node C in fact separates the part of the relevant network that contains trade-offs from the part that does not. We call node C the pivot node for the node of interest.\nIn general, the pivot node in a relevant network is a node with an ambiguous sign for which every possible unam biguous sign would uniquely determine an unambiguous sign for the node of interest; in addition, no other node hav ing this property resides on an unblocked chain from the\n518 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nFigure 4: The Relevant Network, below the Dashed Line.\nobserved node to the pivot node, that is, the pivot node is the node with this property \"closest\" to the observed node. Note that every network includes such a node. Our algo rithm now selects from the relevant network the pivot node for the node of interest.\nFrom the definition of pivot node, it can be shown that there must be two or more different reasoning chains in the rel evant network from the observed node to the pivot node; the net influences along these reasoning chains, moreover, must be conflicting or ambiguous. To resolve the ambiguity at the pivot node, the relative strengths of the various influ ences as well as the signs of some of the nodes involved need be known. From Figures 3 and 4, for example, we have that node I lies at the basis of the ambiguous sign for the pivot node C. Note that it receives an ambiguous node sign itself as a result of two conflicting (non-ambiguous) influences. An unambiguous node sign for node I would not suffice to fix an unambiguous sign for node C. Even knowledge of the relative strengths of the two conflicting influences from node I on the pivot node would not suf fice for this purpose, however: a positive node sign for node I, for example, would still cause node G, residing on one of the reasoning chains from I to C, to receive an ambiguous node sign, which in tum gives rise to an am biguous influence on C. Node G therefore also lies at the basis of the ambiguity at the pivot node. Now, every com bination of unambiguous node signs for the nodes G and I would render the separate influences on the pivot node unambiguous. Knowledge of the relative strengths of these influences would suffice to determine an unambiguous sign for the pivot node. We call a minimal set of nodes having this property the resolution frontier for the pivot node.\nIn terms of signs for the nodes from the resolution frontier, our algorithm now constructs a (conditional) sign for the pivot node by comparing the relative strengths of the vari ous influences exerted on it upon inference. In the example\nFigure 5: The Construction of a Sign for Node C.\nnetwork, the nodes from the resolution frontier exert two separate influences on the pivot node C: the influence from node I via node Don C and the influence from G on C. For the sign 8 of the influence of node I via node D on C and for the sign o' of the influence of G on C, we find that\n8 = sign[I] 0 81 0 83 = sign[I]0 +\n8' = sign[G] 0 84 = sign[G]0-\nwhere Di, i = 1, 3, 4, are as in Figure 5. For the node sign sign [ C] of the pivot node, the algorithm now constructs the following result:\nif 181 2: 10'1, then sign[C] = 8, else sign[C] = 8';\nwhere 181 denotes the strength of the sign 8. So, if the two influences on node C have opposite signs, then their rela tive strengths will determine the sign for node C. The sign of the node of interest A then follows directly from the node sign of C."}, {"heading": "4 SPLITTING UP AND CONSTRUCTING SIGNS", "text": "In this section we detail some of the issues involved in our algorithm for pivotal pruning of trade-offs. In doing so, we assume that a qualitative probabilistic network does not in clude any ambiguous influences, that is, ambiguous node signs upon inference result from unresolved trade-offs. We further assume that observations are entered into the net work one at a time. We also assume that sign propagation resulted in an ambiguous sign for the network's node of in terest. For ease of reference, Figure 6 summarises the zoom algorithm in pseudocode.\nIn detailing the algorithm, we focus attention on identify ing the relevant part of a qualitative probabilistic network along with its pivot node and on constructing from these an informative result for the node of interest.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 519\n4.1 IDENTIFYING THE RELEVANT NETWORK\nOur algorithm identifies from a qualitative probabilistic network the relevant part for addressing the trade-offs that have resulted in an ambiguous sign for the node of inter est. We begin by formally defining the concept of relevant network.\nDefinition 1 Let Q = ( G, \ufffd) be a qualitative probabilistic network as defined in Section 2. Let 0 be the set of previ ously observed nodes in Q, let E be the node for which new evidence has become available, and let I be the network's node of interest. The relevant network for E and I given 0 is the qualitative probabilistic network Qrel = (G', \ufffd') such that\n\u2022 V(G') consists of all nodes that occur on a chain from E to I that is not blocked by 0;\n\u2022 A(G') = (V(G') x V(G') ) n A( G) ; and\n\u2022 \ufffd' consists of all qualitative influences and synergies from \ufffd that involve nodes from G' only.\nThe concept of relevance has been introduced before, most notably for quantitative probabilistic networks (see for ex ample [Druzdzel & Suermondt, 1994, Shachter, 1998] ) . In fact, for quantitative and qualitative probabilistic networks various different concepts of relevance have been distin guished. For a node of interest I, previously observed nodes 0, and a newly observed node E, we say that a node N is\n\u2022 structurally relevant to I, if N is not d-separated from I given 0 U {E};\n\u2022 computationally relevant to I, if the (conditional) probabilities for N are required for computing the posterior probability distribution for I given the ob servations for 0 U { E}; and\n\u2022 dynamically relevant to I and E, if N partakes in the impact of E on I in the presence of the observations forO.\nIn our example qualitative network, node D is structurally relevant, computationally relevant, and dynamically rele vant to the node of interest A. Node E is structurally rele vant to node A yet neither computationally nor dynamically relevant. Node J is structurally irrelevant to the observed node H, as is also evidenced by its node sign '0' upon in ference; it is both structurally and computationally relevant to the node of interest A, yet dynamically irrelevant. The newly observed node H is d-separated from A by its be ing observed. It therefore is not structurally relevant to A; it is computationally as well as dynamically relevant to A, however. Node M, to conclude, is neither structurally nor\ncomputationally nor dynamically relevant to the node of interest A.\nThe concept of dynamic relevance was introduced to de note the nodes constituting the reasoning chains between a newly observed node and a node of interest in a probabilis tic network [Druzdzel & Suermondt, 1994] . The set of all nodes that are dynamically relevant to the node of interest I and the newly observed node E, given the previously ob served nodes 0, can in fact be shown to induce the relevant network forE and I given 0, as defined in Definition 1.\nFrom a qualitative probabilistic network, the set of dy namically relevant nodes can be established by first deter mining all nodes that are computationally relevant to the node of interest I and then removing the nodes that are not on any reasoning chain from the newly observed node E to I. For computing the set of all computationally rele vant nodes, the efficient Bayes-Ball algorithm is available from R.D. Shachter (1998). The algorithm takes for its in put a probabilistic network, the set of all observed nodes 0 U {E}, and the node of interest I; it returns the sets of nodes that are computationally relevant, or requisite, to I. From the set of computationally relevant nodes, all nodes that are not on any reasoning chain from the newly observed node E to the node of interest I need be iden tified; these nodes are termed nuisance nodes for E and I. An efficient algorithm is available for identifying these nodes [Lin & Druzdzel, 1997] . The algorithm takes for its input a computationally relevant network, the set of previ ously observed nodes 0, the newly observed node E, and the node of interest I; it returns the set of nuisance nodes for E and I. The algorithm for computing the relevant part of a qualitative probabilistic network is summarised in pseudocode in Figure 7.\nfunction ComputeRelevantNetwork(Q): Qrel\nrequisites+--- BayesBaii(G, 0 U {E}, I); V(G) +--- (V(G) \\requisites) U {E}; A( G) +--- (V(G) x V(G)) n A( G); nuisances+--- ComputeNuisanceNodes(G); V (G) +--- V (G) \\ nuisances; A( G)+--- (V(G) x V(G)) n A( G); L1 +--- {all influences and synergies from L1 in G}; return Qrel = (G, L1)\nFigure 7: The Algorithm for Computing the Relevant Net work.\n4.2 IDENTIFYING THE PIVOT NODE\nAfter establishing the relevant part of a qualitative proba bilistic network for addressing the trade-offs present, our algorithm identifies the pivot node. The pivot node serves to separate the part of the relevant network that contains the trade-offs that have given rise to the ambiguous sign for the node of interest, from the part that does not con-\n520 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\ntain these trade-offs. The pivot node will allow for further focusing. We recall that the pivot node is a node with an ambiguous node sign, for which every possible unambigu ous sign would uniquely determine an unambiguous sign for the node of interest. We define the concept of pivot node more formally.\nDefinition 2 Let Q = ( G, \ufffd ) be a relevant qualitative probabilistic network; let 0 be the set of previously ob served nodes, let E be the newly observed node, and let I be the network's node of interest, as before. The pivot node for I and E is a node P E V (G) such that\n\u2022 Sb(E, P) E \ufffd with o = '?';\n\u2022 Sb (P, I) E \ufffd with <5' =/:- '?';and\n\u2022 there does not exist a node P' with the above prop erties that resides on a chain from E to P that is not blocked by 0.\nThe pivot node in a relevant qualitative probabilistic net work has various convenient properties. Before discussing these properties, we briefly review the concept of an ar ticulation node from graph theory. In a digraph, an ar ticulation node is a node that upon removal along with its incident arcs, makes the digraph fall apart into vari ous separate components. In the digraph of our example network, as shown in Figure 2, the articulation nodes are the nodes C, D, H, I, and L; for the relevant network, de picted in Figure 4, node C is the only articulation node, however. Articulation nodes are identified using a depth first search algorithm; for details, we refer the reader to [Cormen et al., 1990] . Theorems 1 and 2 now state impor tant properties of a pivot node that allow for its identifica tion.\nTheorem 1 Let Q = (G, \ufffd ) be a relevant qualitative probabilistic network; let E be the newly observed node and let I be the node of interest. The pivot node for I and E is either the node of interest I or an articulation node in G.\nProof (sketch). By definition we have that every possible unambiguous node sign for the pivot node determines an unambiguous sign for the node of interest I. It will be ev ident that node I itself satisfies this property. Either the node of interest I or another node on an unblocked chain from E to I, therefore, is the pivot node. Now, suppose that node I is not the pivot node. As a sign for the pivot node uniquely determines the sign for I, we conclude that all influences exerted upon I must traverse the pivot node. Every unblocked chain from E to I, therefore, must include the pivot node. As a consequence, removing the pivot node along with its incident arcs from the relevant network will cause the network to fall apart into separate components. We conclude that the pivot node is an articulation node. 0\nTheorem 2 Let Q = ( G, \ufffd ) be a relevant qualitative probabilistic network; let E and I be as before. The pivot node for I and E is unique.\nProof (sketch). From Definition 1 we have that the rele vant network consists of only nodes that reside on an un blocked chain from the newly observed node E to the node of interest I. From the definition of articulation node, we further have that every such chain must include all articula tion nodes in the relevant network. In fact, every reasoning chain from E to I visits the articulation nodes in the same order. From Definition 2 we have that no two pivot nodes can reside on the same unblocked chain to the node of in terest. We conclude that the pivot node is unique. 0\nFrom the proof of Theorem 2 we have that the articula tion nodes in a relevant network allow a total ordering. We number the articulation nodes, together with the node of in terest I, from I, for the node closest to the newly observed node, to m, for the node of interest. The pivot node now is the node with the lowest ordering number for which an unambiguous sign would uniquely determine an unambigu ous sign for the node of interest. To identify the pivot node, our algorithm starts with investigating the articulation node closest to the node of interest; this node is numbered m - 1. The algorithm investigates whether an unambiguous sign for this candidate pivot node would result in an unambigu ous sign for the node of interest upon sign propagation. By propagating a '+' from the candidate pivot node to the node of interest I, the node sign resulting for I is the sign of the net influence of the candidate pivot node on I. If this sign is ambiguous, then the node of interest itself is the pivot node. Otherwise, the algorithm proceeds by investigating the ar ticulation node numbered m - 2, and so on. The algorithm is summarised in pseudocode in Figure 8.\nfunction ComputePivot(Q): pivot candidates+- {I} U FindArticulationNodes(G); order the nodes from candidates from 1 to m; return FindPivot(m- 1);\n4.3 CONSTRUCTING RESULTS\nFrom its definition, we have that there must be two or more different reasoning chains in the relevant network from the newly observed node to the pivot node; the net influences along these reasoning chains are conflicting or ambiguous. Our algorithm focuses on the ambiguity at the pivot node and identifies the information that would serve to resolve it. For this purpose, the algorithm zooms in on the part\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 521\nof the relevant network between the newly observed node and the pivot node; we call this part the pruned relevant network. Note that the pruned relevant network is readily computed by exploiting the property that the pivot node is an articulation node. From the pruned relevant network, the algorithm first selects the so-called candidate resolvers.\nDefinition 3 Let Q = ( G, \ufffd) be a relevant qualitative probabilistic network; let E be the newly observed node and let I be the network's node of interest. Let P be the pivot node for I and E. Now, let Q pru = ( G', \ufffd1) be the pruned relevant network for P. A candidate resolver for P is a node Ri E V(G') , Ri =f. P, such that\n\u2022 Ri = E, or\n\u2022 sign[Ri] ='?'and in-degree[Ri J 2: 2.\nThe candidate resolvers for the pivot node are easily iden tified from the pruned relevant network.\nFrom among the candidate resolvers in the pruned relevant network, our algorithm now constructs the resolution fron tier. We recall that the resolution frontier is a minimal set of nodes for which unambiguous node signs would uniquely determine the signs of the separate influences on the pivot node.\nDefinition 4 Let Q = (G, \ufffd) be a pruned relevant quali tative probabilistic network; let E and I be as before. Let P be the pivot node for I and E, and let R be the set of candidate resolvers for P, as defined in Definition 3. The resolution frontier F for P is the maximal subset of R such that for each candidate resolver Ri E F there exists at least one unblocked chain from E via Ri to P such that no node Rj E R resides on the subchain from Ri to P.\nThe resolution frontier can be constructed by recursively traversing the various reasoning chains from the pivot node back to the observed node E and checking whether the nodes visited are candidate resolvers.\nOnce the resolution frontier has been identified from the pruned relevant network, the algorithm constructs a (con ditional) sign for the pivot node in terms of signs for the nodes from the frontier. Let F be the resolution frontier for the pivot node P. For each resolver Ri E F, let s;, j 2: 1, denote the signs of the various different reasoning chains from Ri to the pivot node. For each combination of node signs sign[Ri], Ri E F, the sign of the pivot node is computed to be\nif I EB(sign[R;]\u00aes; )=+ ( sign[Ri] l8l s}) 12:\nI EB(sign[R;]\u00aes; )=- ( sign[Ri] l8l s}) I (I) then sign[PJ = +, else sign[P] = -\nwhere j8j once again is used to denote the strength of the sign 8. We would like to note that as, in general, the resolu tion frontier includes a small number of nodes, the number of signs to be computed for the pivot node is limited. In ad dition, we note that the process of constructing informative results can be repeated recursively for the nodes in the pivot node's resolution frontier, until the newly observed node is reached. The basic algorithm is summarised in pseudocode in Figure 9.\nTo conclude, we would like to note that for computing in formative results for a relevant network's pivot node, the pruned network can be even further restricted. To this end, a so-called boundary node can be identified for the newly observed node. The boundary node is the articulation node closest to the node of interest that has an unambiguous node sign after propagation of the observation entered. Con structing results can then focus on the part of the relevant network between the pivot node and the boundary node. Moreover, if the thus pruned network includes many artic ulation nodes, it may very well be that trade-offs exist be tween the articulation nodes numbered k - 1 and k, but not between k and k + 1. Distinguishing between these com ponents is straightforward and allows for further focusing on the actual trade-offs involved in inference.\n5 CONCLUSIONS\nWe have presented a new algorithm for dealing with trade offs in qualitative probabilistic networks. Rather than re solve trade-offs by providing for a finer level of representa tion detail, our algorithm identifies from a qualitative prob abilistic network the information that would serve to re solve the trade-offs present. For this purpose, the algorithm zooms in on the part of the network where the actual trade offs reside and identifies the pivot node for the node of in-\n522 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nterest. The sign of the pivot node uniquely determines the sign of the node of interest. For the pivot node, a more informative result than ambiguity is constructed in terms of values for the node's resolvers and the relative strengths of the influences upon it. This process of constructing in formative results can be repeated recursively for the pivot node's resolvers.\nAs we have already mentioned in our introduction, we be lieve that qualitative probabilistic networks can play an im portant role in the construction of quantitative networks for real-life application domains, as well as for explanation of their reasoning processes. For the purpose of explanation, qualitative probabilistic networks have been proposed be fore. The concept of pivot node for zooming in on trade offs and constructing insightful results for a network's node of interest is a very powerful concept to enable explanation of complex reasoning processes in quantitative probabilis tic networks.\nAcknowledgments\nThis work was partially supported by the EPSRC under grant GR/L84117 and a Ph.D. studentship.\nReferences\n[Cormen et al., 1990] T.H. Cormen, C.E. Leiserson, R.L. Rivest (1990). Introduction to Algorithms. MIT Press, Cambridge, Massachusetts.\n[Druzdzel & Henrion, 1993a] M.J. Druzdzel and M. Hen rion (1993). Efficient reasoning in qualitative prob abilistic networks. Proceedings of the Eleventh Na tional Conference on Artificial Intelligence, pp. 548- 553.\n[Druzdzel & Henrion, 1993b] M.J. Druzdzel and M. Hen rion ( 1993 ). lntercausal reasoning with uninstantiated ancestor nodes. Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, pp. 317 - 325.\n[Druzdzel & Suermondt, 1994] M.J. Druzdzel and H.J. Suermondt ( 1994 ). Relevance in probabilistic models: \"Backyards\" in a \"small world\". Working Notes of the AAAI 1994 Fall Symposium Series: Relevance, pp. 60 -63.\n[Lin & Druzdzel, 1997] Y. Lin and M.J. Druzdzel (1997). Computational advantages of relevance reasoning in Bayesian belief networks. Proceedings of the Thir teenth Conference on Uncertainty in Artificial Intel ligence, pp. 342-350.\n[Liu & Wellman, 1998] C.-L. Liu and M.P. Wellman (1998). Incremental tradeoff resolution in qualitative probabilistic networks. Proceedings of the Fourteenth\nConference on Uncertainty in Artificial Intelligence, pp. 338- 345.\n[Parsons, 1995] S. Parsons (1995). Refining reasoning in qualitative probabilistic networks. Proceedings of the Eleventh Conference on Uncertainty in Artificial In telligence, pp. 427 -434.\n[Pearl, 1988] J. Pearl ( 1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, Palo Alto.\n[Renooij & Van der Gaag, 1999] S. Renooij and L.C. van der Gaag (1999). Enhancing QPNs for trade-off res olution. Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, pp. 559-566.\n[Shachter, 1998] R.D. Shachter (1998). Bayes-Ball: the rational pastime (for determining irrelevance and req uisite information in belief networks and influence di agrams). Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pp. 480 - 487.\n[Wellman, 1990] M.P. Wellman ( 1990). Fundamental con cepts of qualitative probabilistic networks. Artificial Intelligence, vol. 44, pp. 257-303."}], "references": [{"title": "Efficient reasoning in qualitative prob\u00ad abilistic networks", "author": ["Druzdzel", "Henrion", "1993a] M.J. Druzdzel", "M. Hen"], "venue": "Proceedings of the Eleventh Na\u00ad tional Conference on Artificial Intelligence,", "citeRegEx": "Druzdzel et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Druzdzel et al\\.", "year": 1993}, {"title": "lntercausal reasoning with uninstantiated ancestor nodes", "author": ["Druzdzel", "Henrion", "1993b] M.J. Druzdzel", "M. Hen"], "venue": "Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Druzdzel et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Druzdzel et al\\.", "year": 1993}, {"title": "Enhancing QPNs for trade-off res\u00ad olution", "author": ["Renooij", "Van der Gaag", "1999] S. Renooij", "L.C. van der Gaag"], "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Renooij et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Renooij et al\\.", "year": 1999}], "referenceMentions": [], "year": 2011, "abstractText": "Qualitative probabilistic networks have been de\u00ad signed for probabilistic reasoning in a qualita\u00ad tive way. Due to their coarse level of represen\u00ad tation detail, qualitative probabilistic networks do not provide for resolving trade-offs and typ\u00ad ically yield ambiguous results upon inference. We present an algorithm for computing more in\u00ad sightful results for unresolved trade-offs. The al\u00ad gorithm builds upon the idea of using pivots to zoom in on the trade-offs and identifying the in\u00ad formation that would serve to resolve them.", "creator": "pdftk 1.41 - www.pdftk.com"}}}