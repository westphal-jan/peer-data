{"id": "1301.5348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Why Size Matters: Feature Coding as Nystrom Sampling", "abstract": "recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their expressive simplicity, well preserved properties of linear classifiers, and their computational efficiency. in this paper we propose a novel usage of spectral pipeline based on kernel methods and nystrom representation. in particular, 3 project on the coding of a data point with a local representation based on a binary with fewer elements than the number of binary points, secondly view it as an approximation to the actual function that would compute pair - wise similarity containing all data points ( often too many to solve in practice ), followed by a nystrom sampling step to isolate a subset of comparable data points.", "histories": [["v1", "Tue, 15 Jan 2013 21:36:06 GMT  (25kb,D)", "https://arxiv.org/abs/1301.5348v1", null], ["v2", "Tue, 16 Apr 2013 00:17:54 GMT  (25kb,D)", "http://arxiv.org/abs/1301.5348v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["oriol vinyals", "yangqing jia", "trevor darrell"], "accepted": false, "id": "1301.5348"}, "pdf": {"name": "1301.5348.pdf", "metadata": {"source": "CRF", "title": "Why Size Matters: Feature Coding as Nystro\u0308m Sampling", "authors": ["Oriol Vinyals", "Yangqing Jia"], "emails": [], "sections": [{"heading": null, "text": "Why Size Matters: Feature Coding as Nystro\u0308m Sampling\nOriol Vinyals UC Berkeley Berkeley, CA\nYangqing Jia UC Berkeley Berkeley, CA\nTrevor Darrell UC Berkeley Berkeley, CA"}, {"heading": "1 Introduction", "text": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystro\u0308m sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystro\u0308m sampling step to select a subset of all data points.\nFurthermore, since bounds are known on the approximation power of Nystro\u0308m sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity."}, {"heading": "2 The Nystro\u0308m View", "text": "We specifically consider forming a dictionary by sampling our training set. To encode a new sample x \u2208 Rd, we apply a (generally non-linear) coding function c so that c(x) \u2208 Rc. Note that d is the dimensionality of the original feature space, while c is the dictionary size. The standard classification pipeline considers c(x) as the new feature space, and typically uses a linear classifier on this space. For example, one may use the threshold encoding function [2] as an example: c(x) = max(0,x>D \u2212 \u03b1) where D \u2208 Rd\u00d7c is the dictionary. Note that our discussion on coding is valid for many different feed-forward coding schemes.\nIn the ideal case (infinite computation and memory), we encode each sample x using the whole training set X \u2208 Rd\u00d7N , which can be seen as the best local coding of the training set X (as long as over-fitting is handled by the classification algorithm). In general, larger dictionary sizes yield better performance assuming the linear classifier is well regularized, as it can be seen as a way to do manifold learning [6]. We define the new coded feature space as C = max(0,X>X \u2212 \u03b1), where the i-th row of C corresponds to coding the i-th sample c(xi). The linear kernel function between samples i and j is k(xi,xj) = c(xi)>c(xj). The kernel matrix is then K = CC>. Naively applying Nystro\u0308m sampling to the matrix K does not save any computation, as every column of K requires computing an inner product with N samples. However, if we decompose the matrix C with Nystro\u0308m sampling (i.e., with a subsampled dictionary) we obtain C\u2032 \u2248 C, and as a consequence K\u2032 \u2248 K:\nC\u2032 = EW\u22121E>, K\u2032 = C\u2032C\u2032> = EW\u22121E>EW\u22121E> = E\u039bE>\nar X\niv :1\n30 1.\n53 48\nv2 [\ncs .L\nG ]\n1 6\nA pr\n2 01\n3\nwhere the first equation comes from applying Nystro\u0308m sampling to C, E is a random subsample of the columns of C, and W the corresponding square matrix with the same random subsample of both columns and rows of C."}, {"heading": "3 Main Results on Approximation Bounds", "text": "More interestingly, many bounds on the error made in estimating C by C\u2032 exist, and finding better sampling schemes that improve such bounds is an active topic in the machine learning community (see e.g. [4]). The bound we start with is [4]:\n||C\u2212C\u2032||F \u2264 ||C\u2212Ck||F + max(nCii) (1) valid if c \u2265 64k/ 4 (c is the number of columns that we sample from C to form E, i.e. the codebook size), where k is the sufficient rank to estimate the structure of C, and Ck is the optimal rank k approximation (given by Singular Value Decomposition (SVD), which we cannot compute in practice). Note that, if we assume that our training set can be explained by a manifold of dimension k (i.e. the first term in the right hand side of eq. 1 vanishes), then the error is proportional to times a constant (that is dataset dependent).\nThus, if we fix k to the value that retains enough energy from C, we get a bound that for every c (dimension of code), gives a minimum to plug in equation 1. This gives us a useful bound of the form \u2265Mc\u2212 14 for some constant M (that depends on k). Putting it all together, we get:\n||C\u2212C\u2032||F \u2264 O +Mc\u2212 1 4\nwith O and M constants that are dataset specific.\nHaving bounded the error C is not sufficient to establish how the code size will affect the classifier performance. In particular, it is not clear how the error on C affect the error on the kernel matrix K. However, we are able to prove that the error bound on K\u2032 is in the same format as that on C:\n||K\u2212K\u2032||F \u2264 O +Mc\u2212 1 4 (2)\nEven though we are not aware of an easy way to formally link degradation in Frobenius norm of our approximation K\u2032 to K to classification accuracy, the bound above is informative as one may reasonably expect kernel matrices of different quality to have classification performances in the same trend."}, {"heading": "4 Experiments", "text": "We empirically evaluate the bound on the kernel matrix, used as a proxy to model classification accuracy, which is the measure of interest. To estimate the constants in the bound, we do interpolation of the observed accuracy in the first two samples of accuracy versus codebook size, which is of practical interest: one may want to quickly run a new dataset through the pipeline with small codebook sizes, and then quickly estimate what the accuracy would be when running a full experiment with a much larger dictionary size.\nFigure 1 shows the results on on the CIFAR-10 image classification and TIMIT speech recognition datasets respectively. It is observed that the derived model closely follow our own empirical observations, with red dashed line serving as a lower bound of the actual accuracy and following the\nshape of the empirical accuracy, predicting its saturation. The model is never too tight though, due to various factors of our approximation, e.g., the analytical relationship between the approximation of K and the classification accuracy is not clear.\nThe Nystro\u0308m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystro\u0308m sampling context [4].\nIn addition, in many image classification tasks the feature extraction pipeline is composed of more than feature encoding. For example, recent state-of-the-art methods pool locally encoded features spatially to form the final feature vector. The Nystro\u0308m view presented in the paper inspires us to employ findings in the machine learning field to learn better, pooling-aware dictionaries. In one of our related work [3], we form a dictionary by first \u201covershooting\u201d the coding stage with a larger dictionary, and then pruning it using K-centers with pooled features. Figure 2 shows an increase in the final classification accuracy compared with the baseline that only learns the dictionary on the patch-level, with no additional computation cost for either feature extraction or classification."}], "references": [{"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A Coates", "H Lee", "AY Ng"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A Coates", "AY Ng"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Pooling-invariant image feature learning", "author": ["Y Jia", "O Vinyals", "T Darrell"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["S Kumar", "M Mohri", "A Talwalkar"], "venue": "JMLR, 13(4):981\u20131006,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On random weights and unsupervised feature learning", "author": ["A Saxe", "PW Koh", "Z Chen", "M Bhand", "B Suresh", "AY Ng"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Locality-constrained linear coding for image classification", "author": ["J Wang", "J Yang", "K Yu", "F Lv", "T Huang", "Y Gong"], "venue": "In CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Efficient highly over-complete sparse coding using a mixture model", "author": ["J Yang", "K Yu", "T Huang"], "venue": "In ECCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "startOffset": 72, "endOffset": 78}, {"referenceID": 6, "context": "This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "startOffset": 72, "endOffset": 78}, {"referenceID": 1, "context": "For example, one may use the threshold encoding function [2] as an example: c(x) = max(0,x>D \u2212 \u03b1) where D \u2208 Rd\u00d7c is the dictionary.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "In general, larger dictionary sizes yield better performance assuming the linear classifier is well regularized, as it can be seen as a way to do manifold learning [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The bound we start with is [4]:", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "We refer to our tech report [3] for more details.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 284, "endOffset": 287}, {"referenceID": 6, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 315, "endOffset": 318}, {"referenceID": 0, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 425, "endOffset": 431}, {"referenceID": 4, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 425, "endOffset": 431}, {"referenceID": 3, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 587, "endOffset": 590}, {"referenceID": 2, "context": "In one of our related work [3], we form a dictionary by first \u201covershooting\u201d the coding stage with a larger dictionary, and then pruning it using K-centers with pooled features.", "startOffset": 27, "endOffset": 30}], "year": 2013, "abstractText": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystr\u00f6m sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystr\u00f6m sampling step to select a subset of all data points.", "creator": "TeX"}}}