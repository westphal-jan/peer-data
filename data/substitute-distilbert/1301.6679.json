{"id": "1301.6679", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Possibilistic logic bases and possibilistic graphs", "abstract": "possibilistic logic bases and possibilistic graphs are two different frameworks of conditioned for representing knowledge. the former stratifies the pieces of knowledge ( expressed by logical formulas ) according representing their level of certainty, while the latter exhibits relationships between variables. the necessary types of representations are merely equivalent while they lead to the same possibility distribution ( which rank - orders grant possible interpretations ). a possibility distribution can be decomposed using linear chain rule derivation may be based on two different kinds ; conditioning which exist in possibility theory ( one based on product in a numerical setting, one based on minimum operation upon a qualitative setting ). these two types implementing conditioning induce simultaneous disciplines of possibilistic graphs. in both cases, a translation of these graphs into possibilistic bases is provided. the converse translation from a possibilistic intuition base into a min - l presentation is also documented.", "histories": [["v1", "Wed, 23 Jan 2013 15:56:55 GMT  (429kb)", "http://arxiv.org/abs/1301.6679v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salem benferhat", "didier dubois", "laurent garcia", "henri prade"], "accepted": false, "id": "1301.6679"}, "pdf": {"name": "1301.6679.pdf", "metadata": {"source": "CRF", "title": "Possibilistic logic bases and possibilistic graphs", "authors": ["Salem BENFERHAT", "Didier DUBOIS", "Laurent GARCIA", "Henri PRADE", "Paul Sabatier"], "emails": ["@irit.fr"], "sections": [{"heading": null, "text": "1 Introduction\nPossibilistic logic is issued from Zadeh's possibility theory [16], which offers a framework for the representation of states of partial ignorance owing to the use of a dual pair of possibility and necessity measures. Possibility theory may be quantitative or qualitative according to the range of these measures which may be the real interval [0, I] or a finite linearly ordered scale as well [8]. Possibilistic logic (e.g. , [7]) has been developed for more than ten years. It provides a sound and complete machinery for handling qualitative uncertainty with r\ufffdsp\ufffdt to a semantics expressed by means of possibility distributions which rank-orders the possible interpretations. At the syntactic level, possibili\ufffdtic log!c handles pairs of the form (p a) where p IS a classical logic formula and a is an element of a totally ordered set. The pair (p a) expresses that the formula p is certain at lea\ufffdt to the level a or more formally by N(p)<::a, where N IS the necessit; measure associated to the possibility distribution expressing the underlying semantics. Possibilistic logic is essentially qualitative since only the preordering induced on the formulas\n. !s impo\ufffd\ufffdt \ufffd(p) ?'\"\nN(q) means \"p is more certain than q ). Posstbthstic logic\nis as tractable as classical logic since its complexity is about log2n*SAT where n is the number of certainty levels used in the knowledge base and SAT is the complexity of satisfaction problem\n. in classica\ufffd \ufffdo.gi\ufffd.\nBesides there exist few works on directed posstbthsttc graphs (\ufffdhich are the counterpart of Bayesi\ufffd p\ufffdobabilistic networks [13, 14] in the framework of posstbJltty theory). Existing works are either a direct adaptation of a probabilistic logic approach without caring for \ufffdowledge representation [10], or a way to do learnmg from imprecise data [ 11 ]. Because of the .existence of the possibilistic logic machinery, there Is n?t the . s\ufffd\ufffde necessity to introduce graphical structures m possibility theory, as in probability theory (where probabilistic logic is more complex to handle). Yet Bayesian-like networks have a clear appeal for knowledge acquisition and directed graphs could be used to help in the specificati?\ufffd \ufffdf possibilistic knowledge as much as probabilistic knowledge. Next section gives a background on possibilistic logic, on conditioning in possibility theory and on the directed possibilistic graphs. Section 3 studies their encoding in possibilistic lo\ufffdi\ufffd .. Section_ \ufffd discuss\ufffds . t\ufffde p:ob\ufffdem. of recovering the Initial conditiOnal possibility dtstn\ufffdutwn from the joint possibility distribution computed With the chain rule, and the discussion briefly refers to the idea of possibilistic independence. Section 5 proposes an encoding of a set of possibilistic logic fo.rmulas into directed possibilistic graphs. Proofs are omitted for the sake of brevity, but can be found in [2].\n2 Background\n2. 1. Possibilistic knowledge bases\nA possibilistic (knowledge or belief) base is a set. of possibilistic logic formulas of the form (p,a) where pIS a classical propositional logic formula; a an element of the semi-open real interval (0, 1] in a numerical setting, or of a finite linearly ordered scale in a qualitative setting. It estimates to what extent it is certain that p is true considering the available, possibly incomplete information about the world. Given a possibilistic base I;, we can generate a possibility distribution from 1: by associating to each classical interpretation a degree in [0, I] expressing the level of compatibility with the available information. When a\n58 Benferhat, Dubois, Garcia, and Prade\npossibilistic base is made of one formula { (p a)}, then each interpretation ro which satisfies p gets the degree 1t( ro) = 1 since it is completely consistent with p, and each interpretation ro which falsifies p gets a degree 1t( ro) such that the highest is a (i.e., the more certain is p ), the lowest is 1t(ro). In particular, if a= I (i.e., p is completely certain), then 1t(ro) = 0, i.e., ro is impossible. One way to realize this constraint is to assign to 1t( ro) the degree I - a (on an ordered scale, we use a reversing map of the scale). Then, the possibility distribution associated to {(p a)} is:\nl;lroen, lt{(p a)}(ro) = I if rol=p = 1 - a otherwise.\nWhen L = {(pi, ai), i=l,n} is a general possibilistic base then all the interpretations satisfying all the beliefs in L will have the highest possibility degree, namely I, and the other interpretations are ranked w.r.t. the highest belief that they falsified, namely we get l;lroeQ: 1tL( ro) = I if ro1= L\n= !-max { ai : (pi ai) E L and O>i=\"\"'Pil otherwise.\nThus, ltL can be viewed as the result of the combination of the lt{(p; a;)} 's using the min operator, i.e.:\nltL(ro) = min {lt{(pi ai)}(ro): (pi ai) E L }. A possibility distribution 1tL is said to be normal if there exists an interpretation ro which is totally possible, namely ltL( ro) = I. However, in general there may exist several distinct interpretations which are totally possible. This normalization condition reflects the consistency of the available knowledge L represented by this possibility distribution (i.e., 3 0) En, s.t., O>l= L). A possibility distribution 1t induces two mappings grading respectively the possibility and the certainty of a formula p:\n-the possibility degree II(p) = max { 1t( ro) : ro 1= p} which evaluates to what extent p is consistent with the available knowledge expressed by 1t. Note that we have:\n\\;lp l;lq II(p v q) = rnax(Il(p), II(q));\n-the necessity (or certainty) degree N(p) = rnin{l-lt(ro): ro 1= \"\"'P} which evaluates to what extent p is entailed by the available knowledge. We have:\nl;lp l;lq N(p A q) = rnin(N(p), N(q)).\nIt can be checked that if (p,a)EL, the semantic constraint N(p) \ufffd a holds, where N is a necessity measure based on 1t\ufffd. Note the duality equation: N(p) = 1-II(\"\"'p). Moreover, note that, contrasting with probability theory N(p) and N(\"\"'p) (resp. II(p) and II (\"\"'p)) are not functionally related: we only have (for normal possibility distributions) min(N(p), N(\"\"'p)) = 0 (resp. max(II(p), II(\"\"'P)) = !). It leaves room for representing complete ignorance in an unbiased way: p is ignored whenever ll(p) = II(\"\"'P)) = I. Lastly, several syntactically different possibilistic belief bases may have the same possibility distribution as a semantic counterpart. In such a case, it can be shown that these bases are equivalent in the following sense: their a cuts, which are classical bases, are logically equivalent in the usual sense, where the a-cut of a possibilistic base L\nis the set of classical formulas whose level of certainty is greater than or equal to a.\n2.2. Possibillstic graph Let V = {AJ, ... ,A0} be a set of variables. These variables will identify the nodes of a network. In possibilistic logic, these variables are binary. The domains associated with the variables Ai are denoted by Dv={DJ, . . . ,Dnl\u00b7 A possible assignment of a value of Di to every variable Ai will be called an elementary event. When each variable is binary (i.e., Di={ai,\"\"'ai}), the elementary events are also called interpretations and are denoted by ro. The set of all the elementary events is simply the Cartesian product D1 x ... x Dn of the domains. Formulas, represented as sets of interpretations, are also called events. Possibilistic graphs, denoted by II G, as probabilistic networks, are based on directed acyclic graph [10, II]. The nodes represent variables (for example, the temperature of a patient, the colour of a car, ... ) and the edges encode the causal link (or influence) between these variables. Uncertainty is represented on each node. When there is an edge from the node Ai to the node Aj, the node Ai is said to be \"parent\" of Aj (parents of A are denoted by Par( A)). A possibility measure II is associated with a graph G in the following way:\n\u2022 For the nodes Ai which are roots of the graph (i.e., Par(Ai)=0), we specify the prior possibility degrees associated with each instance of Aj, namely we give every II(a) where ae D i. Possibilities must satisfy the normalization condition: rnaxaeD; II( a)=!.\n\u2022 To each other node Aj are attached conditional possibilities II(a I O>Par(Aj)) where aEDj et O>Par(Aj) is an element of the Cartesian product XiDi of the DiEPar{Aj), domains associated with the variables of Par(Aj) . Conditional possibilities should satisfy the following normalization condition:\n\\;IO>Par(A)EXiDi, maxaeDj II(a I O>Par(A)) = I,\nII( .lb) is a conditional possibility measure, which can be defined in two different ways (as explained after the following Definition). Then:\nDefinition 1: The joint possibility distribution associated with a possibilistic graph IIG is computed with the following equation:\n1t0na(XJ ... x0) = D i=l ,n II(xi I O>xi) ( !), where Xi e { ai, \"\"'ai} are the two possible instances of the variable Ai, roxi,;;; {XJ . . . x0} and o represents either the minimum or the product. We shall use 1t\u2022 and ltm for short, in a case o =product oro =min respectively.\nIndeed, in possibility theory, there exist two definitions for the conditioning: \u2022 In a qualitative setting the conditioned possibility measure is defined by:\nII(q!p) = I ifii(pAq) = II(p) = II(pAq) otherwise (i.e., II(pAq) < II(p)),\nand obeys the following equation [ 12] : II(pAq) = rnin(II(qlp), II(p)); (2)\nThis definition of conditioning only requires the ordering between interpretations, and can be defined on any finite ordeed scale. \u2022 In a numerical setting we use the following definition of conditioning based on the product :\nTI(p\"q) = TI(qlp) * TI(p) ifTI(p)..o = 0 otherwise. (3)\nIn both cases we have N(qlp) = 1 - TI(...,qlp). Up to a rescaling, (3) is also the conditioning rule of kappa functions [15]:\nK(rojp) = K(ro)- K(p) if (J}I= p = oo otherwise,\nwith n ( cl>) = 2-K( \u00ab<>). Conditioning a possibility distribution with p then with r gives the same result as conditioning with r and then with p. When the joint possibility distribution (I) is computed with the minimum (resp. product), the TI G will be denoted by TIGmin (resp.TIG\u2022).\n2.3. Decomposition The decomposition of a possibility distribution consists in expressing a joint possibility distribution as a combination of conditional possibility distributions. For this decomposition, we can follow the same way as in probability theory. Let {A 1, ... ,A0} be the set of variables which is ordered arbitrarily. From the definition of conditioning, we have :\n1t(A 1\u00b7 .. A0)=min [1t(A0IA 1\u00b7 .. An-!),1t(A 1\u00b7 .. An-!)]. Applying repeatedly this definition to 1t(A1 ... An-!), then to Jt(AJ ... An-2) ... , the joint possibility distribution is decomposed into 1t(A, ... A0) = min [1t(A0IA!\u00b7\u00b7\u00b7An-!) , ... , 1t(A1)]. (4)\nThe decomposition given by equation (4) can be simplified by assuming conditional independence between variables. When conditioning with the product, the decomposition follows the way used in probability: 1t(A, ... A0) = 1t(A0IA!\u00b7\u00b7\u00b7An-!)* ... *1t(A!). (5)\nThere is no unique decomposition of a possibility distribution since it depends on the initial ordering between variables as it is shown in the following example:\nExample 1: Let 1t be defined on {a,...,a} x {b, ...,b}: 1t(...,a..., b)=l; 1t(a b) = 1t(a ..,b) =.8; 1t(...,a b) = .7. There are two ways for decomposing this possibility distribution,\n\u2022 either 1t(A, B) = min (1t(BIA), 1t(A)), and hence IS given by the matrix: 1t(BIA) .\nBIA a ..,a b 1 .7\n...,b 1 1 and 1t( A) by the values 1t( a)=. 8 and 1t( ..,a )-I,\n\u2022 or 1t(A, B) = min (1t(AIB), 1t(B)), and hence 1t(AIB) is given b h y t e matrix:\nAlB b ...,b a I .8\n...,a .7 I and 1t(B) by the values 1t(b)=.8 and 1t(...,b)=l.\nPossibilistic logic bases and possibilistic graphs 59\nIt can be verified that the two possible decompositions of the possibility distribution 1t correspond to the two following possibilistic knowledge bases:\nL! = {(...,bva . 3), (...,a .2)}, L2 = {(b\ufffda .2), (...,bva . 3), (...,b .2)}.\nThese two bases (which can be computed as explained in next sections) are equivalent since they are associated with the same possibility distribution (i.e., 1tLi = 1t1;2).\n3. From the graph to the logical base\nThe goal of this section is to translate a directed possibilistic graph into a possibilistic base. The translation should be such that the possibility distribution associated to the graph using the chain rule is the same as the one associated to the possibilistic knowledge base. The restriction to binary variables is made for the sake of simplicity. See [I] for an extension to the non-binary case. The directed possibilistic graph can be seen as the result of the fusion of one-formula knowledge bases, each one containing a single possibilistic formula. Each formula corresponds to a conditional possibility of the directed possibilistic graph. A possibilistic causal network is viewed as a set of triples: TIG = {(a, Pa, a) : TI(aiPa)=a;<l is an element of the\ndirected graph} where 'a' is an instance of the variable A and P a is an element of the Cartesian product XiDi of the Di. It can be restricted to the conditional possibilities different from I since the ones which are equal to I are not used in the computation of joint possibility distributions.\n3. 1 Encoding TIGmin Let us start with min-based conditioning. With each triple (a, P a. a) of the directed possibilistic graph is associated the single formula (...,av...,Pa 1-a.). The conditional symbol ...,aiPa is encoded as the material implication ...,av...,Pa, using the duality TI(aiPa)=l-N(...,aiPa). The joint possibility distribution obtained from a directed possibilistic graph using (I) is equivalent to the one obtained by min-combination of the possibility distributions associated with the formulas encoding the different triples of the directed graph:\nProposition 1: Let Jtai be the possibility distribution associated with the formula corresponding to the triple (aj, Pai\u2022 aj). Then: 1tm = mini Jtai .\nAs already suggested in the background section, the union of two possibilistic bases I: 1 and L2 corresponds to the min-combination of the two possibility distributions 1t! and 1t2 associated with Ll and L2 respectively. Therefore, the following Proposition states the knowledge base associated with a directed possibilistic graph:\nProposition 2: The possibilistic knowledge base associated with TIGmin is:\nI:= {(...,YiV...,Pyj 1-a.j)) I (yj, PYi\u2022 llj)ETIG}.\n60 Benferhat, Dubois, Garcia, and Prade\nThis result is important since. it implies that every result known for possibilistic logic can be applied to directed possibilistic graphs.\nExample 2 : Consider the following DAG :\n\\lc\\ B D\n\\I E\nAssume that the conditional possibility degrees are given by the following tables :\nllA\nll(BIAC BIAC a c ...,a c elsewhere\nb 112 I I ...,b I 114 I\nll(D IC) D I C c -.c\nd I 114 -.d I I\nll(E IBD) E I B D b -.d ...,b d elsewhere\ne I 112 I \ufffd 3/4 I I\nWe have: Rll = {(..,a, 0, 3/4), (b, a c, 112), (...,b, ..,a c, 114), (d, -.c, 114), (-.e, b -.d, 3/4), (e, ...,b d, I/2)}. Then : :t = {(a 114), (...,bv...,av...,c 112), (bvav...,c 3/4), (...,dvc 3/4), (ev...,bvd 114), (-.evbv-.d 1/2)}.\n3.2. Encoding llG* Let us now tum towards product-based conditioning. Following the same approach as above, to each triple (a, P a. a) of a possibilistic graph, is associated the single possibilistic formula (...,av...,Pa 1--a). Let us notice that: lla(a I Pa)=lla(aAPa} I na(Pa) =a, where na is the possibility measure obtained from the possibility distribution associated with (...,av...,Pa 1--a). Indeed, lla(aAPa) =a since each interpretation satisfying a A P a falsifies (...,av ..., P a I -a) . Moreover , ll a(Pa) =m a x(lla(aAP a), ll a(..., aAPa)). As every interpretation satisfying ...,aAPa satisfies (...,av...,Pa I-a), we get na< Pa)=J. The counterpart of Proposition I holds:\nProposition 3: Let 1tai be the possibility distribution associated with (ai, Pai\u2022 Clj). Then 1t* = *i 1tai , where \u2022 is the pointwise product of possibility distributions.\nWe need to find what is the possibilistic base associated with the combination of two possibility distributions by the product operator. This is summarized in the following proposition [3]:\nProposition 4: Let :ti={(pi ai) : iel} and t2={(qj /Jj) : j e J}. Let 1t 1 and 1t2 be the two possibilistic distributions associated with Ll and L2\u00b7 Let 1t* be the product-based combination of 1t! and 1t2. The resulting base associated with 1t* is: C \\t1, :t2) = :t, u t2 u\n{(pivqj (ai+\ufffdj-ai*\ufffdj)) : (pi ai)ELJ and ('lj \ufffdj)EL2}\u00b7\nSince the operator C \u2022 is commutative and associative, this straithforwardly extends to the case of more than two bases. Hence, the possibilistic base which is associated to no\u2022 can be obtained by combining the different triples of the graph with the syntactic operator C \u2022.\nExample 2 (continued) : Recall that : Rll={(...,a, 0, 3/4), (b, a c, 1/2), (...,b, ..,a c, 114), (d, -.c, 114), (...,e, b -.d, 3/4), (e,-.b d,l /2)}. The six elementary knowledge bases associated to the triples ofiiG* are:\nt1={(a 114)} ; :t3={(avbv-.c 3/4)} ; :t5={(...,bvdve 114)} ;\nt2={ (...,a\ufffdbv-.c 112)} ; u={(c\ufffd 3/4)} ; L6={(b\ufffd\ufffd 1/2)}.\nNow let us combine them with C \u2022 : - the combination of :t1 et t2Ieads to:\nL12=C*(L!. L2) = {(a 1/4), (...,av...,bv-.c 112)};\n- combining the result with L3 : :t123=C\u2022(:t12, l:3) = {(a 114), (...,a\ufffdbv-.c 112), (avbv-.c 13/16)} ;\n- combining the result with 4 : L1234=C\u2022(:tl23, U) = {(a 1/4), (...,av-.bv-.c 112), (avbv-.c 13/16),\n(c\ufffdd 3/4), (ave\ufffd 13/16)} ;\n- combining the result with L5 : L12345=C\u2022(:t1234. :t5)\n= {(a 114), (...,av...,bv-.c 112), (avbv-.c 13/16), (cv-.d 3/4), (avcv...,d 13116), (...,bvdve 114), (av...,bvdve 7/16), (...,av...,bv-.cvdve 5/8)};\n- combining the result with L6 leads to the final base : :t*=C\u2022(:t12345. L6) = {(a 114), (...,av...,bv...,c 1/2), (avbv...,c 13/16), (cv...,d 3/4), (avcv-.d 13/16), (...,bvdve 114), (av...,bvdve 7/16), (...,a v ...,b v...,c v d v e 5/8)} u {(bv...,d v...,e 1/2)} u {(avbv...,dv-.e 5/8), (avbv...,cv...,dv\ufffd 29/32), (bvc\ufffdv\"\"\"e 7/8), (avbvc\ufffdv-.e 29/32)}.\nThis knowledge base contains 13 clauses while in the case of min we only have 6. This clearly illustrates that\n--;\nthe combination with the product leads to a larger knowledge base than if we combined with the minimum, due to Proposition 4. This comes with the fact that the product is not compatible with a finite scale.\n4. Recovering initial data A natural question when we compute the joint possibility distribution using the chain rule is to see if we recover the a priori and conditional probabilities given by the expert. In the probability theory the answer is always yes. The following proposition shows that this is also the case if the chaining rule is based on the product:\nProposition 5: Let IT (aiPa) be the conditional possibility distributions over the variables A in the ITG*. Let Jt\u2022 be the joint possibility distribution obtained using the chain rule with the product (I) , and its associated possibility measure IT \u2022. Then, for any conditional possibility distribution we have:\nIT\u2022(aiPa) = IT(aiPa) .\nConcerning the minimum-based conditioning the answer is no, as it is illustrated by the following small example:\nEumplo 3 U. m -'''I tire fullowiog gnph' with IT(a)=l; IT(..,a)=ll4; IT(bla) = 1/3; IT(..,bla) = I and IT(bl..,a) = 1/3; IT(..,bl..,a) =I. The joint possibility distribution is:\n1tm(ab) = 113; 1tm(a..,b) = I; 1tm(..,ab) = 114; 1tm(..,a..,b) = 1/4;\nClearly, we have ITm(bl..,a) = I 'I' I since 1tm(..,ab) = 1tm(..,a) = 114.\nThe reason for not recovering the original values is that the conditional possibility distributions specified by the users are not coherent with the axioms of possibility distributions. Indeed, using the definition of conditional possibility measure, we always have: lfiT(plq) '\ufffd' I then IT(plq) = IT(pAq) < IT( q) . We see clearly, from the previous example that this constraint is violated since IT(bl..,a) = 113'1'1 and IT(bl..,a) > IT(..,a)=ll4. Therefore, it is not surprizing if we do not recover the above value. This behaviour also exists in possibilistic logic, namely a possibility distribution associated a possibilistic base do not guarantee to recover the exact value of the knowledge base. To be convinced, just consider a small example where L = {(a .8), (avb . 4) }. We can easily check that NJtr(avb) = .8. This is due to the fact that (avb . 4) is strictly subsumed by (a . 4) . Hence, we have:\nProposition 6: Let L be such that it does not contain any strictly subsumed formulas, namely there is no (p, a) such that {q : (q, \ufffd)eL and \ufffd> a} 1- p. Then V'(p, a) eL we have: NJtr(p) =a.\nPossibilistic logic bases and possibilistic graphs 61\nLet us go back to the causal network, the following proposition characterizes to what the computed joint possibility corresponds w. r.t. the conditional possibility distributions specified by the user.\nProposition 7: Let IT (aiPa ) be the conditional possibility distributions over the variables A in the DAG. Let Jtm be the joint possibility distribution obtained using the chain rule with the minimum-based conditioning. Then: either 1tm(a1Pa) = IT(aiPa) or 1tm(a1Pa) = I .\nThis means that the computed joint possibility distribution either preserves the initial values or push them up to I (this is observed in E xample I ) . We now focus on the DAG, from where we recover the original values. In the possibility theory, the definition of independence is not unique [4][6]. The following is a weaker one called \"non-interactivity\":\nDefinition 2: Two variables A and B are independent in the context C, if for each instance (a,b,c) of (A, B, C) we have: IT(a,b lc) = min (IT(alc) , IT(blc) ) .\nThe following proposition shows that the joint possibility distribution guarantees the independence relations from the structure of the D AG, as in probabilistic network:\nProposition 8: Let X be a given variable, and Y be a variable which is neither a parent of X nor any of its descendant. Let ltm be the joint possibility distribution computed from a D AG ITG using the min-based chain rule. Then X and Y are independent in the context of Par(X) in the sense of Definition 2.\nThe question of whether a joint possibility distribution can be decomposed using a stronger definition of independence is left for further research.\nS. Encoding bases into min-based graphs In this section, we present the transformation of possibilistic knowledge bases into directed possibilistic graphs ITGmi n\u00b7 One way to do it is to use possibility distributions as intermediary. Indeed, a knowledge base leads to a possibility distribution, from which it is possible to build a ITGmin (see Section 2. 3) . This would apply as well to IT G \u2022. However, this way is computationally expensive. Moreover, we want to find the ITGmi n directly from the knowledge base. The encoding of a possibilistic knowledge into a ITGmin is less straightforward than the previous transformations. Indeed, we cannot directly view each formula as a triple and then build the graph, but we need some pre-processing steps. The constructed possibilistic graph ITG should be such that:\n\u2022 the joint possibility distribution computed from the ITG using the minimum operator should be the same as the one computed from the knowledge base;\n62 Benferhat, Dubois, Garcia, and Prade\n\u2022 the joint possibility distribution allows us to recover all the conditional possibilities of the DAG, and\n\u2022 the joint possibility distribution satisfies the independence relation (in the sense of Definition 2) induced by the structure of the DAG, namely each variable should be independent of any, non-descendent, variable in the context of its parents.\nThe construction of a causal network associated to a possibilistic knowledge is obtained in three steps: the first step simply consists in putting the knowledge base into a clausal form and in removing tautologies. The second step consists in constructing the graph, and the last step computes the conditional possibilities associated to the constructed graph.\nS.l Putting bases in a clausal form In this first step, a base I is rewritten into an equivalent base :E'. The equivalence means here: Jtr=Jtr'\u00b7 Getting I ' consists in putting the knowledge base into a clausal form and in removing tautologies. The following proposition shows how to put the base in a clausal form first:\nProposition 9 : Let (p a) e :E. Let { CJ, ... , en} be the set of clauses encoding p in classical logic. Let :E* be a new knowledge base obtained from I by replacing (p a) by {(cl a), ... , (en a)}. Then the two knowledge bases I and I* are equivalent in the sense that Jtr=Jtr*\u00b7\nThen removing tautologies leads still to an equivalent possibilistic base. Indeed, tautologies are always satisfied, and Jtr( co) is only defined with respect to that formulas of I falsified by co. The removing of tautologies is an important point since this will avoid having links which do not make sense. For example, the tautological formula (\"\"X v \"\"'Y v x I) could induce a link between X and Y.\n5.2 Constructing the graph The second step consists in constructing the graph, namely the determination of the vertices (variables) of the graph and the parents of each vertex. The set of variables is simply the set of propositional symbols which appear in the knowledge base. Moreover, since possibilistic logic is based on propositional logic, then all the used variables are binary variables. By X we denote the variable which can be either x or -.x. To construct the graph, we first rank the variables, according to an arbitrary numbering {X1, X2, ... , Xn} of the variables. This ranking intends to mean that parents of each variable X 1 can only be in {Xi+ 1, ... , Xn} (but they may not exist). We first give several intuitive examples before presenting the technical construction of the graph.\nExample 4: Let: I = {(t 0.6), (tvv 0.4)}. From this knowledge base one may think that the variable T depends on the variable V. However, we can easily check that I is equivalent to the following one: I ' = {(t 0.6))}, where clearly, V has no influence on T. The formula (tvv 0.4) is said to be subsumed by :E-{(tvv 0.4)}.\nNext definition formally introduces the notion of\nsubsumed beliefs, which can be removed as stated by Proposition I 0:\nDefinition 3: A formula (p a ) of I is said to be subsumed if :E;e:al= p where I :e:a={q : (q 13)e:E, and l3:e:a}- {(p a)} (namely p can be recovered from clauses of the base having weights greater or equal than a).\nProposition 10: Let I ' be a new base obtained from I by removing subsumed formulas. Then Jtr=Jtr'.\nTherefore, we can remove, or add, subsumed beliefs without any damage. Subsumed beliefs are not the only ones which may induce fictitious dependencies:\nExampleS: Let: I = {(a .5), (..,avb .5)}. In this base, neither (a .5) nor (..,avb . 5) is subsumed, and one can think that there\u00b7 is a relationship between the variables A and B. However, we can easily check that this base is equivalent to I '= {(a .5), (b .5)}, where clearly A and B are unrelated. So we are let to state:\nProposition 11: Let X be a variable, and (xvp a) be a clause of I containing the instance x of X s.t. I 1- (p a)1\u2022 Then the base I and the knowledge base I ' obtained from I by replacing (xvp a) by (p a) are equivalent.\nIntuitively, one can say that two variables are related if there is a clause containing an instance of these two variables, and they are unrelated otherwise. Example 6 shows that two variables can be related even if there is no clause in the base containing an instance of variables:\nExample 6: Let: I = {(xva .5), (..,xvb .5)}. In this base, if we would define the relationship between variables only if there exists a clause containing an instance of each of them, then clearly A and B would be unrelated. However, we can check that: I 1- (avb .5).\nThe following example shows that in order to compute the conditional possibility distribution of a variable given its parents, it is not enough to look only to clauses of the base containing instances of the variable X:\nExample 7: Let: I = {(avb .6), (xvb .5), (xva .5)}. We assume that parents of X are A and B. Clearly, if we compute the conditional possibilities Il(XIAB) only from clauses containing X, namely Ix={(xvb . 5) , (xva .5)} then it is not guaranteed to recover all original values. Indeed, in this example, if the computation ofil(\"\"Xi..,a-.b) is simply based on :Ex we get: IJ(-.xl..,a-.b) = .5\n(since {(xvb .5), (xva .5)} 1- (xvavb .5)) but we can check that after computing the joint possibility distribution Jtr: Ilr(..,xl-.a-.b) = I, this is due to the fact that we have both: I 1- (xvavb .6) and I 1- (avb .6), hence: Ilr(..,xA -.a-.b) = Ilr(_,a.,b ). Indeed, (xvb .5) and (xva .5) are not subsumed but (xvbvb\nI This is the syntactic possibilistic entailment. We have: L>- (p, o.) iff {q: (q, [3)EL and 13\ufffd o.} u {-,} is inconsistent.\nPossibilistic logic bases and possibilistic graphs 63\n.5) is because of the clause ( avb .6) in r.\nThe following definition enables a direct computation of conditional possibility distributions:\nDefinition 4: Let X be a variable and Par (X) be the set of its parents. Let K be a set of clauses _ of the form. x_vp such that P\ufffdPar (X) (P is the set of vanables contammg an instance in xvp). We call complete extension of K, denoted by E(K), the set of all clauses of the form\n(xvVyePx y, a) where x is an instance of X, Px is an instance of Par (X), and\na = max {ai : (xvpi ai) e K and Pii==VyePx y}, with max {0}=0.\nExample 8: Let K = {(xvb .5), (xva .5)}. Assume that the parents of X are A and B. Then we can check that: E(K) = {(xvbv..,a .5), (xvbva .5), (xvav..,b .5)}.\nProposition 12: The two bases K and E(K) are equivalent.\nWith the help of the previous propositions we are now ready to determine the exact parents of each variable. Let {X1, ... , Xn} be a numbering of the variables. Then the following algorithm determines the set of parents of each variable:\nFor i =l , ... , n do Begin I* Determination of Parents of Xi *I I. Let (XiVP, a) be a clause or s. t. :\nXi is an instance of Xi, and p is only built from {Xi+J, ... ,X0}.\n\u2022 If (xivp, a) is subsumed, then remove it from L (due to Prop. I 0). \u2022 lf L f- (p, a) then replace (xivp, a) by (p, a) (due to Prop. 1 1)\n2. Let Ki be the set of clauses (xivp, a) in L s. t. p is only built from {Xi+ 1, ... ,Xn}\n3. The parents of the variable Xi are : Par(Xi)= {Xj : 3 c e Ki such that c contains\nan instance of Xj} /* Rewriting L: to facilitate the computation of conditional possibilities , and for looking for hidden dependencies *I\n4. Replace in L, Ki by its complete extensions E(Ki) (due to Prop. 12).\n5. For each (XjVp a) or (where p is built from {Xi+J, ... ,X0}) such that L f- (p, a) replace (xivp, a) by (p, a). 6. Let LXi be the set of clauses (xivp, a) in L s. t.\np is only built from {Xi+J, ... ,Xn} End.\nThe algorithm is composed of two parts: the first part (Steps 1- 3) consists in the construction of the DAG by determining the parents of each variable, and the second part (Steps 4-6) consists in rewriting the knowledge base\nsuch that : i) it gives immediately the conditional possibility distributions, and ii) . ensures to reco\ufffder original values when using the cham rule for computu\ufffdg the joint possibility distributions. Indeed, once E(Ki) 1s computed, in order to evaluate Il(xlpx) then either (\"\"'XiV\"\"'P, a)\ufffdE(KXi) then Il(XIPx) = 1, or (\"\"'XiV\"\"'p, a) E E(Ki), then if L f- (\"\"'p, a) then Il(xlpx) = 1 (since ll(xApx) = Il(x)), otherwise Il(xlpx) = 1 - a.\nThe result of the algorithm is a partition {LX 1, . .. , \ufffdXn} such that LXIV ... v LXn is equivalent tor. Clearly, for i>l, LXi does not contain any variable of {XJ, ... , Xi-d\u00b7 Moreover, LXi can be empty. In this case Xi has no parents : it corresponds to the root of the graph, and the priori possibility degrees associated to Xi are equal to I. The subbases \ufffdX i's give a direct computation of conditional possibility degrees as explained later.\nA graph associated to L is such that its vertices are the variables in L, and a link is drawn from Xj to Xi iffXj e Par(Xj), where Par(Xi) is given by step 3 in the algorithm. This graph is indeed a DAG.\nExample 9 : Let us consider the following base: L = {(avb 0.7), (\"\"'avcv..,d 0.7), (avcvd 0.9), (bvc 0.8), (\"\"'bve 0.2), (\"\"'dvf 0.5)} L contains six variables numbered in the following way: X1=A, X2=B, X3=C, \"X4=D, Xs=E et Xt;=F. From this partition we can check that we get: Par(A)={B, C, D}, Par(B)={C, E}, Par(C)=0, Par(D)={F}, Par(E)= PAR(F)=0. The final graph is:\nE F\nt l B+-C D\n\\!/ A\n5.3 Determining the conditional possibilities Once the graph is constructed we need to compute the\nconditional possibilities. The computation of Il(Xil Par(Xi)) is immediately obtained from LXj, as it is given by the following way: Let {\ufffdXJ, ... , LXn} be the result of Step 6 of the previous algorithm. Let Xi be a variable and Par (X)={YJ, ... , Ym} be the set of its parents. Let x be an instance of Xi and Px=Yl\"\u00b7\u00b7\u00b7\"Ym be an instance of Par (Xi)- Let:\nIl(xiPx) = 1-ai if (\"\"'X v \"\"'Px, ai) e \ufffdXi = I otherwise.\nThen we have:\nProposition 13: The possibility distribution associated to L and the possibility distribution obtained from the graph using the minimum operator are equal, namely: 1t\ufffd=1tm.\n64 Benferhat, Dubois, Garcia, and Prade\nExample 10 (continued) : Let us show how compute 7t (..,al..,b..,cd) and 7t(..,al..,b..,c..,d), the others are obtained in the same way. Recall that\nLA = {(avbvcvd 0.7), (avbvcv..,d 0.7), (avbv..,cvd 0.7), (avbv-.cv..,d 0.7), (\"\"'\ufffdavbvcv..,d 0.7), (\"\"'\ufffdaV\"\"'IbvcV\"\"'Id 0.7), (avbvcvd 0.9), (av..,bvcvd 0.9)}. Then by definition:\n7t(..,al..,b\"\"'1Cd) =.3 7t(..,al..,b\"\"'1Cd) = .I\nThe conditional possibility distributions are given by the following tables :\n1t AI BCD AI BCD \"\"'\ufffdb\"\"'ICd -.bc-.J ..,bed -.b-.oo\na I 1 1 0.3 ..,a 0.3 0.3 0.3 1\nk\\IBCD b\"\"'ICd \"\"'\ufffdb\"\"'IC-.d b\"\"'IC-.d pthetwise a 0.3 I 1 I\n..,a 1 0. 1 0.1 I\n1t(B ICE): BICE \"\"'IC\ufffd \"\"'ICe \"\"'IC\ufffd c\ufffd\nb 1 1 0.8 0.8 ..,b 0.2 0.2 I 1\n7t(D I -.f): D I F f -.f\nd -.d\n1 0.5 1 I\nffii3E : E?[fBF : e 1 f 1\n\ufffd 1 -.f 1\nLet us notice that it is possible to express ignorance, i.e., it is not necessary to give the a priori possibilities on X if they are unknown.\nProposition 14: Let Il(X IPar(X)) be the conditional possibility distributions over the variables X in the DAG associated to :E. Let 7tm be the joint possibility distribution obtained using the chain rule with minimum from a causal network. Then: 7tm(a1Pa) =IT( alP a)\u00b7\nThe previous proposition shows that the constructed DAG from the possibilistic logic base guarantees the recovering of initial values, when using the min-based chain rule, and hence it satisfies the independence relations in the sense of Definition 2 encoded by the structure of the graph.\n6. Conclusion This paper has established the links between possibilistic logic and directed possibilistic graphs. We have shown that directed possibilistic graphs can be encoded into possibilistic logic, for the two possible definitions of conditioning based on the minimum and the product operator. When it is based on product, it also provides a\nmean for turning a Bayesian-net equipped with a kappa function into a possibilistic logic base (using the transformation possibility measures-kappa functions recalled at the end of Section 2.2. ). The inverse passage from a possibilistic logic base to a network has also been provided. This allows the expert to express his knowledge using \"causality\" relations between variables, and then the possibilistic logic machinery can be applied after the computation of the corresponding possibilistic logic base. A future work would be the study of the complexity of the conversion of a directed causal network based on the product into possibilistic logic (with the min, the conversion is linear) and the comparison of the cost of the inference using the network directly with the one using the corresponding possibilistic knowledge base, which may be also used for explanation purposes.\nReferences [I] S. Benferhat, D. Dubois, L. Garcia and H. Prade (1998) Directed possibilistic graphs and possibilistic logic. Proceedings of IPMU-98, pp. 1 470- 1 477 . [2] S. Benferhat, D. Dubois, L. Garcia and H. Prade ( 1999) Possibilistic logic bases and possibilistic graphs. Technical Report, IRIT, Univ. P. Sabatier, Toulouse. [3] S. Benferhat, D. Dubois and H. Prade ( 1 997). Syntactic combination of uncertain information: a possibilistic approach. Proc. of Qua/it. and Quantit. Practical Reasoning (ECSQARU-FAPR'97). LNAI 1244,pp. 30-42. [4] L. M. de Campos et J. F. Huete ( 1994). Independence in possibility theory. Department of Artificial Intelligence. University of Granada, Spain. [5] A. P. Dawid ( 1 979). Conditional independence in statistical theory. J. Royal Statistical Society A, 4 1 . 1 -31. [6] D. Dubois, L. Farinas del Cerro, A. Herzig and H. Prade (1994). An ordinal view of independence with application to plausible reasoning. Proc. of UAI-94 , 195-203. [7] D. Dubois, J. Lang and H. Prade (1994). Possibilistic logic. In: Handbook of Logic in Artificial Intelligence and Logic Programming, Vol. 3 (D.M. Gabbay et al., eds.). Oxford University Press. 439-51 3. [8] Dubois D., Prade H. (1998) Possibility theory: qaulitative and quantitative aspects. In Handbook of Defeasible Reasoning and Uncertainty Management Systems. Vol. I, pp. 1 69-226, Kluwer Academic Press. [ 1 0] P. Fonck (1993). Reseaux d'inference pour le trailement possibiliste. These de doctorat. Univ. de Liege. [II] J. Gebhardt and R. Kruse ( 1997). Background and perspectives of possibilistic graphical models. Proc. of Qua/it. and Quantit. Practical Reasoning (ECSQARU FAPR'97). LNAI 1 244. Springer. 1 08-121. [ 1 2 ] E. Risdal ( 1978). Conditional possibilities, independence and noninteraction. Fuzzy Sets and Systems, I . 283-297. [13] F. V. Jensen (1996). An Introduction to Bayesian Networks. University College London: UCL Press. [ 1 4] J. Pearl (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann. [ 1 5] W. Spohn ( 1 988) Ordinal conditional functions: A dynamic theory of epistemic states. In: Causation in Decision, Belief Change, and Statistics, Vol. 2 (W.L. Harper, B. Skyrrns, eds.), D. Reidel, Dordrecht, 105-1 34. [ 16) L.A. Zadeh ( 1978). Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems, I. 3-28."}], "references": [{"title": "Possibility theory: qaulitative", "author": ["D. Dubois", "H. Prade"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Reseaux d'inference pour le trailement", "author": ["P. Fonck"], "venue": null, "citeRegEx": "Fonck,? \\Q1993\\E", "shortCiteRegEx": "Fonck", "year": 1993}, {"title": "Conditional possibilities, independence and noninteraction", "author": ["E. Risdal"], "venue": "Fuzzy Sets and Systems, I ", "citeRegEx": "Risdal,? \\Q1978\\E", "shortCiteRegEx": "Risdal", "year": 1978}, {"title": "An Introduction to Bayesian", "author": ["F.V. Jensen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Fuzzy sets as a basis for a theory", "author": ["L.A. Zadeh"], "venue": null, "citeRegEx": "Zadeh,? \\Q1978\\E", "shortCiteRegEx": "Zadeh", "year": 1978}], "referenceMentions": [], "year": 2011, "abstractText": "Possibilistic logic bases and possibilistic graphs are two different frameworks of interest for representing knowledge. The former stratifies the pieces of knowledge (expressed by logical formulas) accor?i\ufffdg to their level of certainty, while the latter exhibits relationships between variables. The two types of representations are semantically equivalent when they lead to the same possibility distribution (which rank\u00ad orders the possible interpretations). A possibility distribution can be decomposed using a chain rule which may be based on two different kinds of conditioning which exist in possibility theory (one based on product in a numerical setting, one based on minimum operation in a qualitative setting). These two types of conditioning induce two kin_ds of possibilistic graphs. In both cases, a translatiOn of these graphs into possibilistic bases is provided. The converse translation from a possibilistic knowledge base into a min-based graph is also described.", "creator": "pdftk 1.41 - www.pdftk.com"}}}