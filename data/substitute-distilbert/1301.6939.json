{"id": "1301.6939", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2013", "title": "Multi-Step Regression Learning for Compositional Distributional Semantics", "abstract": "we present a model for compositional vector semantics related to the framework of coecke et al. ( 2010 ), and emulating formal semantics by representing functions as tensors between arguments as vectors. we introduce a new learning method for tensors, generalising the approach of baroni zhu zhang ( 1994 ). we evaluate it on two benchmark data sets, and find it to outperform existing leading methods. we argue in our analysis that the disadvantage of this error path renders it viable also on solving more subtle problems compositional estimation models might face.", "histories": [["v1", "Tue, 29 Jan 2013 14:59:34 GMT  (58kb,D)", "https://arxiv.org/abs/1301.6939v1", "10 pages + 1 page references, to be presented at the 10th International Conference on Computational Semantics (IWCS 2013)"], ["v2", "Wed, 30 Jan 2013 12:01:23 GMT  (58kb,D)", "http://arxiv.org/abs/1301.6939v2", "10 pages + 1 page references, to be presented at the 10th International Conference on Computational Semantics (IWCS 2013)"]], "COMMENTS": "10 pages + 1 page references, to be presented at the 10th International Conference on Computational Semantics (IWCS 2013)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["edward grefenstette", "georgiana dinu", "yao-zhong zhang", "mehrnoosh sadrzadeh", "marco baroni"], "accepted": false, "id": "1301.6939"}, "pdf": {"name": "1301.6939.pdf", "metadata": {"source": "CRF", "title": "Multi-Step Regression Learning for Compositional Distributional Semantics", "authors": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni"], "emails": [], "sections": [{"heading": null, "text": "ecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face."}, {"heading": "1 Introduction", "text": "The staggering amount of machine readable text available on today\u2019s Internet calls for increasingly powerful text and language processing methods. This need has fuelled the search for more subtle and sophisticated representations of language meaning, and methods for learning such models. Two well-researched but prima-facie orthogonal approaches to this problem are formal semantic models and distributional semantic models, each complementary to the other in its strengths and weaknesses.\nFormal semantic models generally implement the view of Frege (1892)\u2014that the semantic content of an expression is its logical form\u2014by defining a systematic passage from syntactic rules to the composition of parts of logical expressions. This allows us to derive the logical form a of sentence from its syntactic structure (Montague, 1970). These models are fully compositional, whereby the meaning of a phrase is a function of the meaning of its parts; however, as they reduce meaning to logical form, they are not necessarily adapted to all language processing applications such as paraphrase detection, classification, or search, where topical and pragmatic relations may be more relevant to the task than equivalence of logical form or truth value. Furthermore, reducing meaning to logical form presupposes the provision of a logical model and domain in order for the semantic value of expressions to be determined, rendering such models essentially a priori.\nIn contrast, distributional semantic models, suggested by Firth (1957), implement the linguistic philosophy of Wittgenstein (1953) stating that meaning is associated with use, and therefore meaning can be learned through the observation of linguistic practises. In practical terms, such models learn the meaning of words by examining the contexts of their occurrences in a corpus, where \u2018context\u2019 is generally taken to mean the tokens with which words co-occur within a sentence or frame of n tokens. Such models have been successfully applied to various tasks such as thesaurus extraction (Grefenstette, 1994) and essay grading (Landauer and Dumais, 1997; Dumais, 2003). However, unlike their formal semantics counterparts, distributional models have no explicit canonical composition operation, and provide no way to integrate syntactic information into word meaning combination to produce sentence meanings.\nIn this paper, we present a new approach to the development of compositional distributional semantic models, based on earlier work by Baroni and Zamparelli (2010), Coecke et al. (2010) and Grefenstette et al. (2011), combining features from the compositional distributional framework of the latter two with the learning methods of the former. In Section 2 we outline a brief history of approaches to compositional distributional semantics. In Section 3 we overview a tensor-based compositional distributional model resembling traditional formal semantic models. In Section 4 we present a new multi-step regres-\nar X\niv :1\n30 1.\n69 39\nv2 [\ncs .C\nL ]\n3 0\nJa n\n20 13\nsion algorithm for learning the tensors in this model. Sections 5\u20137 present the experimental setup and results of two experiments evaluating our model against other known approaches to compositionality in distributional semantics, followed by an analysis of these results in Section 8. We conclude in Section 9 by suggesting future work building on the success of the model presented in this paper."}, {"heading": "2 Related work", "text": "Although researchers tried to derive sentence meanings by composing vectors since the very inception of distributional semantics, this challenge has attracted special attention in recent years. Mitchell and Lapata (2008, 2010) proposed two broad classes of composition models (additive and multiplicative) that encompass most earlier and related proposals as special cases. The simple additive method (summing the vectors of the words in the sentence or phrase) and simple multiplicative method (component-wise multiplication of the vectors) are straightforward and empirically effective instantiations of the general models. We re-implemented them here as our Add and Multiply methods (see Section 5.2 below).\nIn formal semantics, composition has always been modeled in terms of function application, treating certain words as functions that operate on other words to construct meaning incrementally according to a calculus of composition that reflects the syntactic structure of sentences (Frege, 1892; Montague, 1970; Partee, 2004). Coecke et al. (2010) have proposed a general formalism for composition in distributional semantics that captures the same notion of function application. Empirical implementations of Coecke\u2019s et al.\u2019s formalism have been developed by Grefenstette et al. (2011) and tested by Grefenstette and Sadrzadeh (2011a,b). In the methods they derive, a verb with r arguments is a rank r tensor to be combined via component-wise multiplication with the Kronecker product of the vectors representing its arguments, to obtain another rank r tensor representing the sentence:\nS = V (a1 \u2297 a2 \u2297 ...\u2297 ar)\nGrefenstette and Sadrzadeh (2011b) propose various ways to estimate the components of verb tensors in the two-argument (transitive) case, with the simple method of constructing the rank 2 tensor (matrix) by the Kronecker product of a corpus-based verb vector with itself giving the best results. The Kronecker method outperformed the best method of Grefenstette and Sadrzadeh (2011a), referred to as the Categorical model. We re-implement the Kronecker method for our experiments below. It was not possible to efficiently implement the Categorial method across our large corpus, but we still provide a meaningful indirect comparison with this method.\nBaroni and Zamparelli (2010) propose a different approach to function application in distributional space, that they apply to adjective-noun composition (see also Guevara (2010) for similar ideas). Adjectives are functions, encoded as linear maps, that take a noun vector as input and return another nominal vector representing the composite meaning as output. In linear algebraic terms, adjectives are matrices, and composition is matrix-by-vector multiplication:\nc = A\u00d7 n\nBaroni and Zamparelli (2010) estimate the adjective matrices by linear regressions on corpus-extracted examples of their input and output vectors. In this paper, we derive their approach as a special case of a more general framework and extend it, both theoretically and empirically, to two-argument functions (transitive verbs), as well as testing the original single argument variant in the verbal domain. Our generalisation of their approach is called Regression in the experiments below.\nIn the MV-RNN model of Socher et al. (2012), all words and phrases are represented by both a vector and a matrix, and composition also involves a non-linear transformation. When two expressions are combined, the resulting composed vector is a non-linear function of the concatenation of two linear transformations (multiplying the first element matrix by the second element vector, and vice versa). In parallel, the components of the matrices associated with the resulting phrase are linear combinations of the components of the input matrices. Socher and colleagues show that MV-RNN reaches state-of-the-art performance on a variety of empirical tasks.\nWhile the proposal of Socher et al. is similar to our approach in many respects, including syntaxsensitivity and the use of matrices in the calculus of composition, there are three key differences. The\nfirst is that MV-RNN requires task-specific labeled examples to be trained for each target semantic task, which our framework does not, attempting to achieve greater generality while relying less on manual annotation. The second difference, more theoretical in nature, is that all composition in MV-RNN is pairwise, whereas we will present a model of composition permitting functions of larger arity, allowing the semantic representation of functions that take two or more arguments simultaneously. Finally, we follow formal semantics in treating certain words as functions and other as arguments (and can thus directly import intuitions about the calculus of composition from formal semantics into our framework), whereas Socher and colleagues treat each word equally (as both a vector and a matrix). However, we make no claim at this stage as to whether or not these differences can lead to richer semantic models, leaving a direct comparison to future work.\nSeveral studies tackle word meaning in context, that is, how to adapt the distributional representation of a word to the specific context in which it appears (e.g., Dinu and Lapata, 2010; Erk and Pado\u0301, 2008; Thater et al., 2011). We see this as complementary rather than alternative to composition: Distributional representations of single words should first be adapted to context with these methods, and then composed to represent the meaning of phrases and sentences."}, {"heading": "3 A general framework for distributional function application", "text": "A popular approach to compositionality in formal semantics is to derive a formal representation of a phrase from its grammatical structure by representing the semantics of words as functions and arguments, and using the grammatical structure to dictate the order and scope of function application. For example, formal semantic models in the style of Montague (1970) will associate a semantic rule to each syntactic rule in a context-free grammar. A sample formal semantic model is shown here:\nSyntax Semantics S \u21d2 NP VP [[S]] \u21d2 [[V P ]] ([[NP ]]) NP \u21d2 N [[NP ]] \u21d2 [[N ]] N \u21d2 ADJ N [[N ]] \u21d2 [[ADJ ]] ([[N ]]) VP \u21d2 Vt NP [[V P ]] \u21d2 [[V t]] ([[NP ]]) VP \u21d2 Vi [[V P ]] \u21d2 [[V i]]\nSyntax (cont\u2019d) Semantics (cont\u2019d) Vt \u21d2 {verbst} [[V t]] \u21d2 [[verbt]] Vi \u21d2 {verbsi} [[V i]] \u21d2 [[verbi]] ADJ \u21d2 {adjs} [[ADJ ]] \u21d2 [[adj]] N \u21d2 {nouns} [[N ]] \u21d2 [[noun]]\nFollowing these rules, the parse of a simple sentence like \u2018angry dogs chase furry cats\u2019 yields the following interpretation: [[chase]]([[furry]]([[cats]]))([[angry]]([[dogs]])). This is a simple model, where typically lambda abstraction will be liberally used to support quantifiers and argument inversion, but the key point remains that the grammar dictates the translation from natural language to the functional form, e.g. predicates and logical relations. Whereas in formal semantics these functions have a set theoretic form, we present here a way of defining them as multilinear maps over geometric objects. This geometric framework is also applicable to other formal semantic models than that presented here. This is particularly important, as the version of the model presented here is overly simple compared to modern work in formal semantics (which, for example, apply NPs to VPs instead of VPs to NPs, to model quantification), and only serves as a model frame within which we illustrate how our approach functions.\nThe bijective correspondence between linear maps and matrices is a well known property in linear algebra: Every linear map f : A\u2192 B can be encoded as a dim(B) by dim(A) matrixM , and conversely every such matrix encodes a class of linear maps determined by the dimensionality of the domain and co-domain. The application of a linear map f to a vector v \u2208 A producing a vector w \u2208 B is equivalent to the matrix multiplication: f(v) = M \u00d7 v = w In the case of multilinear maps, this correspondence generalises to a correlation between n-ary maps and rank n + 1 tensors (Bourbaki, 1989; Lee, 1997). Tensors are generalisations of vectors and matrices; they have larger degrees of freedom referred to as tensor ranks, which is one for vectors and two for matrices. To illustrate this generalisation, consider how a row/column vector may be written as the weighted superposition (summation) of its basis elements: any vector v in a vector space V with a fixed basis {bi}i, can be written\nv = \u2211 i cvi bi = [ cv1 , . . . , c v i , . . . , c v dim(V ) ]>\nHere, the weights cvi are elements of the underlying field (e.g. R), and thus vectors can be fully described by such a one-index summation. Likewise, matrices, which are rank 2 tensors, can be seen as a collection of row vectors from some space Vr with basis {ai}i, or of column vectors from some space Vc with basis {dj}j . Such a matrix M is an element of the space Vr \u2297 Vc, and can be fully described by the two index summation:\nM = \u2211 ij cMij ai \u2297 dj\nwhere, once again, cMij is an element of the underlying field which in this case is simply the element from the ith row and jth column of the matrix M , and the basis element ai\u2297dj of Vr\u2297Vc is formed by a pair of basis elements from Vr and Vc. The number of indices (or degrees of freedom) used to fully describe a tensor in this superposition notation is its rank, e.g., a rank 3 tensor T \u2208 A\u2297B \u2297C would be described by the superposition of weights cTijk associated with basis elements ei \u2297 fj \u2297 gk.\nThe notion of matrix multiplication and inner product both generalise to tensors as the non-commutative tensor contraction operation (\u00d7). For tensors T \u2208 A\u2297 . . .\u2297B\u2297C and U \u2208 C\u2297D\u2297 . . .\u2297E, with bases {ai \u2297 . . .\u2297 bj \u2297 ck}i...jk and {ck \u2297 dl \u2297 . . .\u2297 em}kl...m, the tensor contraction of T \u00d7U is calculated: \u2211\ni...jkl...m\ncTi...jkc U kl...mai \u2297 . . .\u2297 bj \u2297 dl \u2297 . . .\u2297 em\nwhere the resulting tensor is of rank equal to two less than the sum of the ranks of the input tensors; the subtraction reflects the elimination of matching basis elements through summation during contraction.\nFor every curried multilinear map g : A\u2192 . . .\u2192 Y \u2192 Z, there is a tensor T g \u2208 Z \u2297 Y \u2297 . . .\u2297 A encoding it (Bourbaki, 1989; Lee, 1997). The application of a curried n-ary map h : V1 \u2192 . . .\u2192 Vn \u2192 W to input vectors v1 \u2208 V1, . . . , vn \u2208 Vn to produce output vector w \u2208 W corresponds to the tensor contraction of the tensor T h \u2208W \u2297 Vn \u2297 . . .\u2297 V1 with the argument vectors:\nh(v1) . . . (vn) = T h \u00d7 v1 \u00d7 . . .\u00d7 vn\nUsing this correspondence between n-ary maps and tensors of rank n+1 we can turn any formal semantic model into a compositional distributional model. This is done by first running a type inference algorithm on the generative rules and obtaining types, then assigning to each basic type a vector space and to each function type a tensor space, and representing arguments by vectors and functions by tensors, finally, model function application by tensor contraction.\nTo give an example, in the simple formal semantic model given above, a type inference algorithm would provide us with basic types [[N ]] and [[S]]; we assign vector spaces N and S to these respectively. Nouns and noun phrases are vectors in N , whereas sentences are vectors in S. Verb phrases map noun phrase interpretations to sentence interpretations, hence they are of type [[V P ]] : type([[NP ]]) \u2192 type([[S]]), in vector space terms we have [[V P ]] : N \u2192 S. Intransitive verbs map noun phrases to verb phrases, therefore have the tensor form T vi \u2208 S\u2297N . Transitive verbs have type [[V t]] : [[NP ]]\u2192 [[V P ]], expanded to [[V t]] : N \u2192 N \u2192 S, giving us the tensor form T vt \u2208 S \u2297N \u2297N . Finally, adjectives are of type [[ADJ ]] : [[N ]] \u2192 [[N ]], and hence have the tensor form T adj \u2208 N \u2297N . Putting all this together with tensor contraction (\u00d7) as function application, the vector meaning of our sample sentence \u201cangry dogs chase furry cats\u201d is obtained by calculating the following operations, for lexical semantic vectors T cats and T dogs, square matrices T furry and T angry , and a rank 3 tensor T chase:(\nT chase \u00d7 ( T furry \u00d7 T cats )) \u00d7 ( T angry \u00d7 T dogs ) An important feature of the proposed approach is that elements with the same syntactic category will always be represented by tensors of the same rank and dimensionality. For examples, all phrases of type S (namely sentences) will be represented by vectors with the same number of dimensions, making a direct comparison of sentences with arbitrary syntactic structures possible."}, {"heading": "4 Learning functions by multi-step regression", "text": "The framework described above grants us the ability to determine the rank of the tensors needed to encode functions, as well as their dimensions relative to those of the vectors used to represent arguments.\nIt leaves open the question of how to learn tensors of specific ranks. This, very much like in the case of the DisCoCat framework of Coecke et al. (2010) from which it originated, is intentional: There may be more than one suitable semantic representation for arguments, functions, and sentences, and it is a desirable feature that we may alternate between such representations or combine them while leaving the mechanics of function composition intact. Furthermore, there may be more than one way of learning the tensors and vectors of a particular representation. Previous work on learning tensors has been described independently by Grefenstette and Sadrzadeh (2011a,b) for transitive verbs, and by Baroni and Zamparelli (2010) for adjective-noun constructions. In this section, we describe a new way to learn such tensors, based on ideas from both aforementioned approaches, namely that of multi-step regression.\nMulti-step regression learning is a generalisation of linear regression learning for tensors of rank 3 or higher, as procedures already exist for tensors of rank 1 (lexical semantic vectors) and rank 2 (Baroni and Zamparelli, 2010). For rank 1 tensors, we suggest learning vectors using any standard lexical semantic vector learning model, and present sample parameters in Section 5.1 below. Learning rank 2 tensors (matrices) can be treated as a multivariate multiple regression problem, where the matrix components are chosen to optimise (in a least square error sense) the mapping from training instances of input (argument) to output (composed expression) vectors. Consider for example the task of estimating the components of the matrix representing an intransitive verb, that maps subject vectors to (subject-verb) sentence vectors (Baroni and Zamparelli discuss the analogous adjective-noun composition case):\ns = V \u00d7 subj\nThe weights of the matrix are estimated by least-squares regression from example pairs of input subject and output sentence vectors directly extracted from the corpus. For example, the matrix for sing is estimated from corpus-extracted vectors representing pairs such as <mom, mom sings>, <child, child sings>, etc. Note that if the input and output vectors are n dimensional, we must estimate an n \u00d7 n matrix, each row corresponding to a separate regression problem (the i-th row vector of the estimated matrix will provide the weights to linearly combine the input vector components to predict the i-th output vector component). Regression is a supervised technique requiring training data. However, we can extract the training data automatically from the corpus and so this approach does not incur an extra knowledge cost with respect to unsupervised methods.\nLearning tensors of higher rank by linear regression involves iterative application of the linear regression learning method described above. The idea is to progressively learn the functions of arity two or higher encoded by such tensors by recursively learning the partial application of these functions, thereby reducing the problem to the same matrix-learning problem as addressed by Baroni and Zamparelli. To start with an example: the matrix-by-vector operation of Baroni and Zamparelli (2010) is a special case of the general tensor-based function application model we are proposing, where a \u2018mono-argumental\u2019 function (intransitive verbs) corresponds to a rank 2 tensor (a matrix). The approach is naturally extended to bi-argumental functions, such as transitive verbs, where the verb will be a rank 3 tensor to be multiplied first by the object vector and then by the subject, to return a sentence-representing vector:\ns = V \u00d7 obj\u00d7 subj\nThe first multiplication of a n \u00d7 n \u00d7 n tensor by a n-dimensional vector will return a n-by-n matrix (equivalent to an intransitive verb, as it should be: both sings and eats meat are VPs requiring a subject to be saturated). Note that given n-dimensional input vectors, the ij-th n-dimensional vector in the estimated tensor provides the weights to linearly combine the input object vector components to predict the ij-th output component of the unsaturated verb-object matrix. The matrix is then multiplied by the subject vector to obtain a n-dimensional vector representing the sentence. Again, we estimate the tensor components by linear regression on input-output examples. In the first stage, we apply linear regression to obtain examples of semi-saturated matrices representing verb-object constructions with a specific verb. These matrices are estimated, like in the intransitive case, from corpus-extracted examples of <subject, subject-verb-object> pairs. After estimating a suitable number of such matrices for a variety of objects of the same verb, we use pairs of corpus-derived object vectors and the corresponding estimated verb-object matrices as input-output pairs for another regression, where we estimate the verb tensor components. The estimation procedure is schematically illustrated for eat in Fig. 1.\nSTEP 1: ESTIMATE VP MATRICES\nWe can generalise this learning procedure to functions of arbitrary arity. Consider an n-ary function f : X1 \u2192 . . . \u2192 Xn \u2192 Y . Let Li be the set of i-tuples {wj1, . . . , w j i }i\u2208[1,k], where k = |Li|, corresponding to the words which saturate the first i arguments of f in a corpus. For each tuple in some set Li, let f w j 1 . . . w j i = f j i : Xi+1 \u2192 . . . \u2192 Xn \u2192 Y . Trivially, there is only one such f j 0\u2014namely f itself\u2014since L0 = \u2205 (as there are no arguments of f to saturate for i = 0). The idea behind multi-step regression is to learn, at each step, the tensors for functions f ji by linear regression over the set of pairs (wj \u2032\ni+1, f j\u2032 i+1), where the tensors f j\u2032 i+1 are the expected outcomes of applying f j i to w j\u2032\ni+1 and are learned during the previous step. We bootstrap this algorithm by learning the vectors in Y of the set {f jn}j by treating the word which each f jn models combined with the words of its associated tuple in Ln as a single token. We then learn the vector for this token from the corpus using our preferred distributional semantics method. By recursively learning the sets of functions from i = n down to 0, we obtain smaller and smaller sets of increasingly de-saturated versions of f , which finally allow us to learn f0 = f .\nTo specify how the set of pairs used for recursion is determined, let there exist a function super which takes the index of a tuple from Li and returns the set of indices from Li+1 which denote tuples identical to the first tuple, excluding the last element:\nsuper : N\u00d7 N\u2192 P(N) :: (i, j) 7\u2192 {j\u2032|\u2200j\u2032 \u2208 [1, k\u2032].[wj1 = w j\u2032 1 \u2227 . . . \u2227 w j i = w j\u2032 i ]} where k \u2032 = |Li+1|\nUsing this function, the regression set for some f ji can be defined as {(w j\u2032 i+1, f j\u2032 i+1)|j\u2032 = super(i, j)}. While we just demonstrated how our model generalises to functions of arbitrary arity, it remains to be seen if in actual linguistic modeling there is an effective need for anything beyond tri-argumental functions (ditransitive verbs)."}, {"heading": "5 Experimental procedure", "text": ""}, {"heading": "5.1 Construction of distributional semantic vectors", "text": "We extract co-occurrence data from the concatenation of the Web-derived ukWaC corpus (http:// wacky.sslmit.unibo.it/), a mid-2009 dump of the English Wikipedia (http://en.wikipedia. org) and the British National Corpus (http://www.natcorp.ox.ac.uk/). The corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (http://www.ims.uni-stuttgart. de/projekte/corplex/TreeTagger/) and dependency-parsed with MaltParser (http://www. maltparser.org/). It contains about 2.8 billion tokens.\nWe collect vector representations for the top 8K most frequent nouns and 4K verbs in the corpus, as well as for the subject-verb (320K) and subject-verb-object (1.36M) phrases containing one of the verbs to be used in one of the experiments below and subjects and objects from the list of top 8K nouns. For all target items, we collect within-sentence co-occurrences with the top 10K most frequent content words (nouns, verbs, adjectives and adverbs), save for a stop list of the 300 most frequent words. We extract co-occurrence statistics at the lemma level, ignoring inflectional information. Following standard practice, raw co-occurrence counts are transformed into statistically weighted scores. We tested various weighting schemes of the semantic space on a word similarity task, observing that non-negative pointwise mutual information (PMI) and local mutual information (raw frequency count multiplied by PMI score) generally outperform other weighting schemes by a large margin, and that PMI in particular works best when combined with dimensionality reduction by non-negative matrix factorization (described below). Consequently, we pick PMI weighting for our experiments.\nReducing co-occurrence vectors to lower dimensionality is a common step in the construction of distributional semantic models. Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Schu\u0308tze, 1997). In our setting, dimensionality reduction is virtually necessary, since working with 10K-dimensional vectors is problematic for the Regression approach (see Section 5.2 below), that requires learning matrices and tensors with dimensionalities which are quadratic and cubic in the dimensionality of the input vectors, respectively. We consider two dimensionality reduction methods, the Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF). SVD is the most common technique in distributional semantics, and it was used by Baroni and Zamparelli (2010). NMF is a less commonly adopted method, but it has also been shown to be an effective dimensionality reduction technique for distributional semantics (Dinu and Lapata, 2010). It has a fundamental advantage from our point of view: The Multiply and Kronecker composition approaches (see Section 5.2 below), because of their multiplicative nature, cannot be meaningfully applied to vectors containing negative values. NMF, unlike SVD, produces non-negative vectors, and thus allows a fair comparison of all composition methods in the same reduced space.1\nWe perform the Singular Value Decomposition of the input matrix X: X = U\u03a3V T and, like Baroni and Zamparelli and many others, pick the first k = 300 columns ofU\u03a3 to obtain reduced representations. Non-negative Matrix Factorization factorizes a (m \u00d7 n) non-negative matrix X into two (m \u00d7 k) and (k \u00d7 n) non-negative matrices: X \u2248 WH (we normalize the input matrix to \u2211 i,j Xij = 1 before applying NMF). We use the Matlab implementation2 of the projected gradient algorithm proposed in Lin (2007), which minimizes the squared error of Frobenius norm F (W,H) = \u2016X \u2212WH\u20162F . We set k = 300 and we use W as reduced representation of input matrix X .3"}, {"heading": "5.2 Composition methods", "text": "Verb is a baseline measuring the cosine between the verbs in two sentences as a proxy for sentence similarity (e.g., similarity of mom sings and boy dances is approximated by the cosine of sing and dance).\nWe adopt the widely used and generally successful multiplicative and additive models of Mitchell and Lapata (2010) and others. Composition with the Multiply and Add methods is achieved by, respectively, component-wise multiplying and adding the vectors of the constituents of the sentence we want to represent. Vectors are normalised before addition, as this has consistently shown to improve Add performance in our earlier experiments.\nGrefenstette and Sadrzadeh (2011b) proposed a specific implementation of the general DisCoCat approach to compositional distributional semantics (Coecke et al., 2010) that we call Kronecker here.\n1We ran the experiments reported below in full space for those models for which it was possible, finding that Multiply obtained better results there (approaching those of reduced-spaced Regression). This suggests that, although in our preliminary word similarity tests the original 10K-dimensional space and the two reduced spaces produced very similar results, it is still necessary to look for better low-dimensionality approximations of the full space.\n2Available at http://www.csie.ntu.edu.tw/\u02dccjlin/nmf/. 3For both SVD and NMF, the latent dimensions are computed using a \u201ccore\u201d matrix containing nouns and verbs only, subsequently projecting phrase vectors onto the same space. In this way, the dimensions of the reduced space do not depend on the ad-hoc choice of phrases required by our experiments.\nUnder this approach, a transitive sentence is a matrix S derived from:\nS = (v \u2297 v) (subj\u2297 obj)\nThat is, if nouns and verbs live in a n-dimensional space, a transitive sentence is a n \u00d7 n matrix given by the component-wise multiplication of two Kronecker products: that of the verb vector with itself and that of the subject and object vectors. Grefenstette and Sadrzadeh show that this method outperforms other implementations of the same formalism and is the current state of the art on the transitive sentence task of Grefenstette and Sadrzadeh (2011a) we also tackle below. For intransitive sentences, the same approach reduces to component-wise multiplication of verb and subject vectors, that is, to the Multiply method.\nComposition of nouns and verbs under the proposed (multi-step) Regression model is implemented using Ridge Regression (RR) (Hastie et al., 2009). RR, also known as L2 regularized regression, is a different approach from the Partial Least Square Regression (PLSR) method that was used in previous related work (Baroni and Zamparelli, 2010; Guevara, 2010) to deal with the multicollinearity problem. When multicollinearity exists, the matrix XTX (X here is the input matrix after dimensionality reduction) becomes nearly singular and the diagonal elements of (XTX)\u22121 become quite large, which makes the variance of weights too large. In RR, a positive constant \u03bb is added to the diagonal elements ofXTX to strengthen its non-singularity. Compared with PLSR, RR has a simpler solution for the learned weight matrix B = (XTX + \u03bbI)\u22121XTY and produces competitive results at a faster speed. For each verb matrix or tensor to be learned, we tuned the parameter \u03bb by generalized cross-validation (Golub et al., 1979). The objective function used for tuning minimizes least square error when predicting corpusobserved sentence vectors or intermediate VP matrices (the data sets we evaluate the models on are not touched during tuning!). Training examples are found by combining the 8K nouns we have vectors for (see Section 5.1 above) with any verb in the evaluation sets (see Sections 6 and 7 below) into subjectverb-(object) constructions, and extracting the corresponding vectors from the corpus, where attested (vectors are normalised before feeding them to the regression routine). We use only example vectors with at least 10 non-0 dimensions before dimensionality reduction, and we require at least 3 training examples per regression. For the first experiment (intransitives), these (untuned) constraints result in an average of 281 training examples per verb. In the second experiment, in the verb-object matrix estimation phase, we estimate on average 324 distinct matrices per verb, with an average of 15 training examples per matrix. In the verb tensor estimation phase we use all relevant verb-object matrices as training examples.4"}, {"heading": "6 Experiment 1: Predicting similarity judgments on intransitive sentences", "text": "We use the test set of Mitchell and Lapata (2008), consisting of 180 pairs of simple sentences made of a subject and an intransitive verb. The stimuli were constructed so as to ensure that there would be pairs where the sentences have high similarity (the fire glowed vs. the fire burned) and cases where the sentences are dissimilar while having a comparable degree of lexical overlap (the face glowed vs. the face burned). The sentence pairs were rated for similarity by 49 subjects on a 1-7 scale. Following Mitchell and Lapata, we evaluate each composition method by the Spearman correlation of the cosines of the sentence pair vectors, as predicted by the method, with the individual ratings produced by the subjects for the corresponding sentence pairs.\nThe results in table 1(a) show that the Regression-based model achieves the best correlation when applied to SVD space, confirming that the approach proposed by Baroni and Zamparelli for adjectivenoun constructions can be successfully extended to subject-verb composition. The Regression model also achieves good performance in NMF space, where it is comparable to Multiply. Multiply was found to be the best model by Mitchell and Lapata, and we confirm their results here (recall that Multiply can also be seen as the natural extension of Kronecker to the intransitive setting). The correlations attained by Add and Verb are considerably lower than those of the other methods.\n4All materials and code used in these experiments that are not already publicly available can be requested to the first author."}, {"heading": "7 Experiment 2: Predicting similarity judgments on transitive sentences", "text": "We use the test set of Grefenstette and Sadrzadeh (2011a), which was constructed with the same criteria that Mitchell and Lapata applied, but here the sentences have a simple transitive structure. An example of a high-similarity pair is table shows result vs. table expresses result; whereas map shows location vs. map expresses location is a low-similarity pair. Grefenstette and Sadrzadeh had 25 subjects rating each sentence. Model evaluation proceeds like in the intransitive case.5\nAs the results in table 1(b) show, the Regression model performs very well again, better than any other methods in NMF space, and with a further improvement when SVD is used, similarly to the first experiment. The Kronecker model is also competitive, confirming the results of Grefenstette and Sadrzadeh\u2019s experiments. Neither Add nor Verb achieve very good results, although even for them the correlation with human ratings is significant."}, {"heading": "8 General discussion of the results", "text": "The results presented here show that our iterative linear regression algorithm outperforms the leading multiplicative method on intransitive sentence similarity when using SVD (and it is on par with it when using NMF), and outperforms both the multiplicative method and the leading Kronecker model in predicting transitive sentence similarity. Additionally, the multiplicative model, while commendable for its extreme simplicity, is of limited general interest, since it cannot take word order into account. We can trivially make this model fail by testing it on transitive sentences with subject and object inverted: For Multiply, pandas eat bamboo and bamboo eats pandas are identical statements, whereas for humans they are obviously very different.\nConfirming what Grefenstette and Sadrzadeh found, we saw that Kronecker performs very well also in our experimental setup (although not as well as Regression). The main advantage of Kronecker over Regression lies in its simplicity: there is no training involved, all it takes is two outer vector products and a component-wise multiplication. However, as pointed out by Grefenstette and Sadrzadeh (2011b), this method is ad hoc compared to the linguistically motivated Categorical method they initially presented in Grefenstette and Sadrzadeh (2011a). It is conceivable that the Kronecker model\u2019s good performance is primarily tied to the nature of the evaluation data-set, where only verbs change while subject and object stay the same in sentence pairs.\nWhile our regression-based model\u2019s estimation procedure is considerably more involved than for Kronecker, the model has much to recommend it, both from a statistical and from a linguistic point of view. On the statistical side, there are many aspects of the estimation routine that could be tuned on\n5Kronecker produces matrix representations of transitive sentences, so technically the similarity measure used for this method is the Frobenius inner product of the normalised matrices, equivalent to unfolding the matrices into vectors and computing cosine similarity.\nautomatically collected training data, thus bringing up the Regression model performance. We could for example harvest a larger number of training phrases (not limiting them to those that contain nouns from the 8K most frequent in the corpus, as we did), or vice versa limit training to more frequent phrases, whose vectors are presumably of better quality. Moreover, Ridge Regression is only one of of many estimation techniques that could be tried to come up with better matrix and tensor weights. On the linguistic side, the model is clearly motivated as an instantiation of the vector-space \u201cdual\u201d of classic composition by function application via the tensor contraction operation, as discussed in Section 3 above. Moreover, Regression produces vectors of the same dimensionality for sentences formed with intransitive and transitive verbs, whereas for Kronecker, if the former are n-dimensional vectors, the second are n\u00d7n matrices. Thus, under Kronecker composition, sentences with intransitive and transitive verbs are not directly comparable, which is counter-intuitive (being able to measure the similarity of, say, kids sing and kids sing songs is both natural and practically useful).\nFinally, we remark that in both experiments SVD-reduced vectors lead to Regression models outperforming their NMF counterparts. Regression, unlike the multiplication-based models, is not limited to non-negative vectors, and it can thus harness the benefits of SVD reduction (although of course it is precisely because of the large regression problems we must solve that we need to perform dimensionality reduction at all!)."}, {"heading": "9 Conclusion", "text": "The main advances introduced in this paper are as follows. First, we discussed a tensor-based compositional distributional semantic framework in the vein of that of Coecke et al. (2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework. Second, we presented a generalisation of Baroni and Zamparelli\u2019s matrix learning method to higher rank tensors, allowing us to induce the semantic representation of functions modelled in this framework. Finally, we evaluated this new semantic tensor learning model against existing benchmark data-sets provided by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011a), and showed it to outperform other models. We furthermore claim that the generality of our extended regression method allows it to capture more information than the multiplicative and Kronecker models, and will allow us to canonically model more complex and subtle relations where argument order and semantic roles matter more, such as quantification, logical operations, and ditransitive verbs.\nAmong the plans for future work, we intend to improve regression-based tensor estimation, focusing in particular on automated ways to choose informative training examples. On the evaluation side, we want to construct a larger test set to directly compare sentences with different argument counts (e.g., transitive vs. intransitive constructions) and word orders (e.g., sentences with subject and object inverted), as well as extending modeling and evaluation to other syntactic structures and types of function application (including the challenging cases we listed in the previous paragraph). We want moreover to test the Regression model against the Categorical model of Grefenstette and Sadrzadeh (2011a) and to design evaluation scenarios allowing a direct comparison with the MV-RNN model of Socher et al. (2012)."}, {"heading": "Acknowledgments", "text": "Edward Grefenstette is supported by EPSRC Project A Unified Model of Compositional and Distributional Semantics: Theory and Applications (EP/I03808X/1). Georgiana Dinu and Marco Baroni are partially supported by the ERC 2011 Starting Independent Research Grant to the COMPOSES project (n. 283554). Mehrnoosh Sadrzadeh is supported by an EPSRC Career Acceleration Fellowship (EP/J002607/1)."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "Proceedings of EMNLP, Boston, MA, pp. 1183\u20131193.", "citeRegEx": "Baroni and Zamparelli,? 2010", "shortCiteRegEx": "Baroni and Zamparelli", "year": 2010}, {"title": "Commutative Algebra: Chapters 1-7", "author": ["N. Bourbaki"], "venue": "Springer-Verlag (Berlin and New York).", "citeRegEx": "Bourbaki,? 1989", "shortCiteRegEx": "Bourbaki", "year": 1989}, {"title": "Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming and SVD", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods 44, 890\u2013907.", "citeRegEx": "Bullinaria and Levy,? 2012", "shortCiteRegEx": "Bullinaria and Levy", "year": 2012}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark"], "venue": "Linguistic Analysis 36, 345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Measuring distributional similarity in context", "author": ["G. Dinu", "M. Lapata"], "venue": "Proceedings of EMNLP, Cambridge, MA, pp. 1162\u20131172.", "citeRegEx": "Dinu and Lapata,? 2010", "shortCiteRegEx": "Dinu and Lapata", "year": 2010}, {"title": "Data-driven approaches to information access", "author": ["S. Dumais"], "venue": "Cognitive Science 27, 491\u2013524.", "citeRegEx": "Dumais,? 2003", "shortCiteRegEx": "Dumais", "year": 2003}, {"title": "A structured vector space model for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3"], "venue": "Proceedings of EMNLP, Honolulu, HI, USA, pp. 897\u2013906.", "citeRegEx": "Erk and Pad\u00f3,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3", "year": 2008}, {"title": "Papers in linguistics, 1934-1951", "author": ["J. Firth"], "venue": "Oxford University Press.", "citeRegEx": "Firth,? 1957", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "\u00dcber Sinn und Bedeutung", "author": ["G. Frege"], "venue": "Zeitschrift fuer Philosophie un philosophische Kritik 100, 25\u201350.", "citeRegEx": "Frege,? 1892", "shortCiteRegEx": "Frege", "year": 1892}, {"title": "Generalized cross-validation as a method for choosing a good Ridge parameter", "author": ["G. Golub", "M. Heath", "G. Wahba"], "venue": "Technometrics 21, 215\u2013223.", "citeRegEx": "Golub et al\\.,? 1979", "shortCiteRegEx": "Golub et al\\.", "year": 1979}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "Proceedings of EMNLP, Edinburgh, UK, pp. 1394\u20131404.", "citeRegEx": "Grefenstette and Sadrzadeh,? 2011a", "shortCiteRegEx": "Grefenstette and Sadrzadeh", "year": 2011}, {"title": "Experimenting with transitive verbs in a DisCoCat", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "Proceedings of GEMS, Edinburgh, UK, pp. 62\u201366.", "citeRegEx": "Grefenstette and Sadrzadeh,? 2011b", "shortCiteRegEx": "Grefenstette and Sadrzadeh", "year": 2011}, {"title": "Concrete sentence spaces for compositional distributional models of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh", "S. Clark", "B. Coecke", "S. Pulman"], "venue": "Proceedings of IWCS, pp. 125\u2013134.", "citeRegEx": "Grefenstette et al\\.,? 2011", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["G. Grefenstette"], "venue": "Boston, MA: Kluwer.", "citeRegEx": "Grefenstette,? 1994", "shortCiteRegEx": "Grefenstette", "year": 1994}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["E. Guevara"], "venue": "Proceedings of the ACL GEMS Workshop, Uppsala, Sweden, pp. 33\u201337.", "citeRegEx": "Guevara,? 2010", "shortCiteRegEx": "Guevara", "year": 2010}, {"title": "The Elements of Statistical Learning, 2nd ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "New York: Springer.", "citeRegEx": "Hastie et al\\.,? 2009", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T. Landauer", "S. Dumais"], "venue": "Psychological Review 104(2), 211\u2013240.", "citeRegEx": "Landauer and Dumais,? 1997", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Riemannian manifolds: An introduction to curvature, Volume 176", "author": ["J. Lee"], "venue": "Springer Verlag.", "citeRegEx": "Lee,? 1997", "shortCiteRegEx": "Lee", "year": 1997}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Lin", "C.-J."], "venue": "Neural Computation 19(10), 2756\u20132779.", "citeRegEx": "Lin and C..J.,? 2007", "shortCiteRegEx": "Lin and C..J.", "year": 2007}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "Proceedings of ACL, Columbus, OH, pp. 236\u2013244.", "citeRegEx": "Mitchell and Lapata,? 2008", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science 34(8), 1388\u20131429.", "citeRegEx": "Mitchell and Lapata,? 2010", "shortCiteRegEx": "Mitchell and Lapata", "year": 2010}, {"title": "English as a formal language", "author": ["R. Montague"], "venue": "Linguaggi nella societ\u00e0 e nella tecnica, 189\u2013224.", "citeRegEx": "Montague,? 1970", "shortCiteRegEx": "Montague", "year": 1970}, {"title": "Compositionality in Formal Semantics", "author": ["B. Partee"], "venue": "Malden, MA: Blackwell.", "citeRegEx": "Partee,? 2004", "shortCiteRegEx": "Partee", "year": 2004}, {"title": "The Word-Space Model", "author": ["M. Sahlgren"], "venue": "Dissertation, Stockholm University.", "citeRegEx": "Sahlgren,? 2006", "shortCiteRegEx": "Sahlgren", "year": 2006}, {"title": "Ambiguity Resolution in Natural Language Learning", "author": ["H. Sch\u00fctze"], "venue": "Stanford, CA: CSLI.", "citeRegEx": "Sch\u00fctze,? 1997", "shortCiteRegEx": "Sch\u00fctze", "year": 1997}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "Proceedings of EMNLP, Jeju Island, Korea, pp. 1201\u20131211.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Word meaning in context: A simple and effective vector model", "author": ["S. Thater", "H. F\u00fcrstenau", "M. Pinkal"], "venue": "Proceedings of IJCNLP, Chiang Mai, Thailand, pp. 1134\u20131143.", "citeRegEx": "Thater et al\\.,? 2011", "shortCiteRegEx": "Thater et al\\.", "year": 2011}, {"title": "Philosophical Investigations", "author": ["L. Wittgenstein"], "venue": "Oxford: Blackwell. Translated by G.E.M. Anscombe.", "citeRegEx": "Wittgenstein,? 1953", "shortCiteRegEx": "Wittgenstein", "year": 1953}], "referenceMentions": [{"referenceID": 2, "context": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors.", "startOffset": 90, "endOffset": 111}, {"referenceID": 0, "context": "We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods.", "startOffset": 77, "endOffset": 106}, {"referenceID": 21, "context": "This allows us to derive the logical form a of sentence from its syntactic structure (Montague, 1970).", "startOffset": 85, "endOffset": 101}, {"referenceID": 8, "context": "Formal semantic models generally implement the view of Frege (1892)\u2014that the semantic content of an expression is its logical form\u2014by defining a systematic passage from syntactic rules to the composition of parts of logical expressions.", "startOffset": 55, "endOffset": 68}, {"referenceID": 13, "context": "Such models have been successfully applied to various tasks such as thesaurus extraction (Grefenstette, 1994) and essay grading (Landauer and Dumais, 1997; Dumais, 2003).", "startOffset": 89, "endOffset": 109}, {"referenceID": 16, "context": "Such models have been successfully applied to various tasks such as thesaurus extraction (Grefenstette, 1994) and essay grading (Landauer and Dumais, 1997; Dumais, 2003).", "startOffset": 128, "endOffset": 169}, {"referenceID": 5, "context": "Such models have been successfully applied to various tasks such as thesaurus extraction (Grefenstette, 1994) and essay grading (Landauer and Dumais, 1997; Dumais, 2003).", "startOffset": 128, "endOffset": 169}, {"referenceID": 4, "context": "In contrast, distributional semantic models, suggested by Firth (1957), implement the linguistic philosophy of Wittgenstein (1953) stating that meaning is associated with use, and therefore meaning can be learned through the observation of linguistic practises.", "startOffset": 58, "endOffset": 71}, {"referenceID": 4, "context": "In contrast, distributional semantic models, suggested by Firth (1957), implement the linguistic philosophy of Wittgenstein (1953) stating that meaning is associated with use, and therefore meaning can be learned through the observation of linguistic practises.", "startOffset": 58, "endOffset": 131}, {"referenceID": 0, "context": "In this paper, we present a new approach to the development of compositional distributional semantic models, based on earlier work by Baroni and Zamparelli (2010), Coecke et al.", "startOffset": 134, "endOffset": 163}, {"referenceID": 0, "context": "In this paper, we present a new approach to the development of compositional distributional semantic models, based on earlier work by Baroni and Zamparelli (2010), Coecke et al. (2010) and Grefenstette et al.", "startOffset": 134, "endOffset": 185}, {"referenceID": 0, "context": "In this paper, we present a new approach to the development of compositional distributional semantic models, based on earlier work by Baroni and Zamparelli (2010), Coecke et al. (2010) and Grefenstette et al. (2011), combining features from the compositional distributional framework of the latter two with the learning methods of the former.", "startOffset": 134, "endOffset": 216}, {"referenceID": 8, "context": "In formal semantics, composition has always been modeled in terms of function application, treating certain words as functions that operate on other words to construct meaning incrementally according to a calculus of composition that reflects the syntactic structure of sentences (Frege, 1892; Montague, 1970; Partee, 2004).", "startOffset": 280, "endOffset": 323}, {"referenceID": 21, "context": "In formal semantics, composition has always been modeled in terms of function application, treating certain words as functions that operate on other words to construct meaning incrementally according to a calculus of composition that reflects the syntactic structure of sentences (Frege, 1892; Montague, 1970; Partee, 2004).", "startOffset": 280, "endOffset": 323}, {"referenceID": 22, "context": "In formal semantics, composition has always been modeled in terms of function application, treating certain words as functions that operate on other words to construct meaning incrementally according to a calculus of composition that reflects the syntactic structure of sentences (Frege, 1892; Montague, 1970; Partee, 2004).", "startOffset": 280, "endOffset": 323}, {"referenceID": 3, "context": "Coecke et al. (2010) have proposed a general formalism for composition in distributional semantics that captures the same notion of function application.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Coecke et al. (2010) have proposed a general formalism for composition in distributional semantics that captures the same notion of function application. Empirical implementations of Coecke\u2019s et al.\u2019s formalism have been developed by Grefenstette et al. (2011) and tested by Grefenstette and Sadrzadeh (2011a,b).", "startOffset": 0, "endOffset": 261}, {"referenceID": 0, "context": "Baroni and Zamparelli (2010) propose a different approach to function application in distributional space, that they apply to adjective-noun composition (see also Guevara (2010) for similar ideas).", "startOffset": 0, "endOffset": 29}, {"referenceID": 0, "context": "Baroni and Zamparelli (2010) propose a different approach to function application in distributional space, that they apply to adjective-noun composition (see also Guevara (2010) for similar ideas).", "startOffset": 0, "endOffset": 178}, {"referenceID": 6, "context": "Several studies tackle word meaning in context, that is, how to adapt the distributional representation of a word to the specific context in which it appears (e.g., Dinu and Lapata, 2010; Erk and Pad\u00f3, 2008; Thater et al., 2011).", "startOffset": 158, "endOffset": 228}, {"referenceID": 26, "context": "Several studies tackle word meaning in context, that is, how to adapt the distributional representation of a word to the specific context in which it appears (e.g., Dinu and Lapata, 2010; Erk and Pad\u00f3, 2008; Thater et al., 2011).", "startOffset": 158, "endOffset": 228}, {"referenceID": 21, "context": "For example, formal semantic models in the style of Montague (1970) will associate a semantic rule to each syntactic rule in a context-free grammar.", "startOffset": 52, "endOffset": 68}, {"referenceID": 1, "context": "In the case of multilinear maps, this correspondence generalises to a correlation between n-ary maps and rank n + 1 tensors (Bourbaki, 1989; Lee, 1997).", "startOffset": 124, "endOffset": 151}, {"referenceID": 17, "context": "In the case of multilinear maps, this correspondence generalises to a correlation between n-ary maps and rank n + 1 tensors (Bourbaki, 1989; Lee, 1997).", "startOffset": 124, "endOffset": 151}, {"referenceID": 1, "context": "\u2297 A encoding it (Bourbaki, 1989; Lee, 1997).", "startOffset": 16, "endOffset": 43}, {"referenceID": 17, "context": "\u2297 A encoding it (Bourbaki, 1989; Lee, 1997).", "startOffset": 16, "endOffset": 43}, {"referenceID": 0, "context": "Multi-step regression learning is a generalisation of linear regression learning for tensors of rank 3 or higher, as procedures already exist for tensors of rank 1 (lexical semantic vectors) and rank 2 (Baroni and Zamparelli, 2010).", "startOffset": 202, "endOffset": 231}, {"referenceID": 2, "context": "This, very much like in the case of the DisCoCat framework of Coecke et al. (2010) from which it originated, is intentional: There may be more than one suitable semantic representation for arguments, functions, and sentences, and it is a desirable feature that we may alternate between such representations or combine them while leaving the mechanics of function composition intact.", "startOffset": 62, "endOffset": 83}, {"referenceID": 0, "context": "Previous work on learning tensors has been described independently by Grefenstette and Sadrzadeh (2011a,b) for transitive verbs, and by Baroni and Zamparelli (2010) for adjective-noun constructions.", "startOffset": 136, "endOffset": 165}, {"referenceID": 0, "context": "The idea is to progressively learn the functions of arity two or higher encoded by such tensors by recursively learning the partial application of these functions, thereby reducing the problem to the same matrix-learning problem as addressed by Baroni and Zamparelli. To start with an example: the matrix-by-vector operation of Baroni and Zamparelli (2010) is a special case of the general tensor-based function application model we are proposing, where a \u2018mono-argumental\u2019 function (intransitive verbs) corresponds to a rank 2 tensor (a matrix).", "startOffset": 245, "endOffset": 357}, {"referenceID": 2, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 16, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 23, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 24, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 4, "context": "NMF is a less commonly adopted method, but it has also been shown to be an effective dimensionality reduction technique for distributional semantics (Dinu and Lapata, 2010).", "startOffset": 149, "endOffset": 172}, {"referenceID": 0, "context": "SVD is the most common technique in distributional semantics, and it was used by Baroni and Zamparelli (2010). NMF is a less commonly adopted method, but it has also been shown to be an effective dimensionality reduction technique for distributional semantics (Dinu and Lapata, 2010).", "startOffset": 81, "endOffset": 110}, {"referenceID": 0, "context": "SVD is the most common technique in distributional semantics, and it was used by Baroni and Zamparelli (2010). NMF is a less commonly adopted method, but it has also been shown to be an effective dimensionality reduction technique for distributional semantics (Dinu and Lapata, 2010). It has a fundamental advantage from our point of view: The Multiply and Kronecker composition approaches (see Section 5.2 below), because of their multiplicative nature, cannot be meaningfully applied to vectors containing negative values. NMF, unlike SVD, produces non-negative vectors, and thus allows a fair comparison of all composition methods in the same reduced space.1 We perform the Singular Value Decomposition of the input matrix X: X = U\u03a3V T and, like Baroni and Zamparelli and many others, pick the first k = 300 columns ofU\u03a3 to obtain reduced representations. Non-negative Matrix Factorization factorizes a (m \u00d7 n) non-negative matrix X into two (m \u00d7 k) and (k \u00d7 n) non-negative matrices: X \u2248 WH (we normalize the input matrix to \u2211 i,j Xij = 1 before applying NMF). We use the Matlab implementation2 of the projected gradient algorithm proposed in Lin (2007), which minimizes the squared error of Frobenius norm F (W,H) = \u2016X \u2212WH\u2016F .", "startOffset": 81, "endOffset": 1158}, {"referenceID": 3, "context": "Grefenstette and Sadrzadeh (2011b) proposed a specific implementation of the general DisCoCat approach to compositional distributional semantics (Coecke et al., 2010) that we call Kronecker here.", "startOffset": 145, "endOffset": 166}, {"referenceID": 15, "context": "We adopt the widely used and generally successful multiplicative and additive models of Mitchell and Lapata (2010) and others.", "startOffset": 88, "endOffset": 115}, {"referenceID": 9, "context": "Grefenstette and Sadrzadeh (2011b) proposed a specific implementation of the general DisCoCat approach to compositional distributional semantics (Coecke et al.", "startOffset": 0, "endOffset": 35}, {"referenceID": 15, "context": "Composition of nouns and verbs under the proposed (multi-step) Regression model is implemented using Ridge Regression (RR) (Hastie et al., 2009).", "startOffset": 123, "endOffset": 144}, {"referenceID": 0, "context": "RR, also known as L2 regularized regression, is a different approach from the Partial Least Square Regression (PLSR) method that was used in previous related work (Baroni and Zamparelli, 2010; Guevara, 2010) to deal with the multicollinearity problem.", "startOffset": 163, "endOffset": 207}, {"referenceID": 14, "context": "RR, also known as L2 regularized regression, is a different approach from the Partial Least Square Regression (PLSR) method that was used in previous related work (Baroni and Zamparelli, 2010; Guevara, 2010) to deal with the multicollinearity problem.", "startOffset": 163, "endOffset": 207}, {"referenceID": 9, "context": "For each verb matrix or tensor to be learned, we tuned the parameter \u03bb by generalized cross-validation (Golub et al., 1979).", "startOffset": 103, "endOffset": 123}, {"referenceID": 8, "context": "Grefenstette and Sadrzadeh show that this method outperforms other implementations of the same formalism and is the current state of the art on the transitive sentence task of Grefenstette and Sadrzadeh (2011a) we also tackle below.", "startOffset": 0, "endOffset": 211}, {"referenceID": 19, "context": "We use the test set of Mitchell and Lapata (2008), consisting of 180 pairs of simple sentences made", "startOffset": 23, "endOffset": 50}, {"referenceID": 10, "context": "We use the test set of Grefenstette and Sadrzadeh (2011a), which was constructed with the same criteria that Mitchell and Lapata applied, but here the sentences have a simple transitive structure.", "startOffset": 23, "endOffset": 58}, {"referenceID": 10, "context": "Confirming what Grefenstette and Sadrzadeh found, we saw that Kronecker performs very well also in our experimental setup (although not as well as Regression). The main advantage of Kronecker over Regression lies in its simplicity: there is no training involved, all it takes is two outer vector products and a component-wise multiplication. However, as pointed out by Grefenstette and Sadrzadeh (2011b), this method is ad hoc compared to the linguistically motivated Categorical method they initially presented in Grefenstette and Sadrzadeh (2011a).", "startOffset": 16, "endOffset": 404}, {"referenceID": 10, "context": "Confirming what Grefenstette and Sadrzadeh found, we saw that Kronecker performs very well also in our experimental setup (although not as well as Regression). The main advantage of Kronecker over Regression lies in its simplicity: there is no training involved, all it takes is two outer vector products and a component-wise multiplication. However, as pointed out by Grefenstette and Sadrzadeh (2011b), this method is ad hoc compared to the linguistically motivated Categorical method they initially presented in Grefenstette and Sadrzadeh (2011a). It is conceivable that the Kronecker model\u2019s good performance is primarily tied to the nature of the evaluation data-set, where only verbs change while subject and object stay the same in sentence pairs.", "startOffset": 16, "endOffset": 550}, {"referenceID": 2, "context": "First, we discussed a tensor-based compositional distributional semantic framework in the vein of that of Coecke et al. (2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework.", "startOffset": 106, "endOffset": 127}, {"referenceID": 0, "context": "(2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework.", "startOffset": 48, "endOffset": 77}, {"referenceID": 0, "context": "(2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework. Second, we presented a generalisation of Baroni and Zamparelli\u2019s matrix learning method to higher rank tensors, allowing us to induce the semantic representation of functions modelled in this framework. Finally, we evaluated this new semantic tensor learning model against existing benchmark data-sets provided by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011a), and showed it to outperform other models.", "startOffset": 48, "endOffset": 500}, {"referenceID": 0, "context": "(2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework. Second, we presented a generalisation of Baroni and Zamparelli\u2019s matrix learning method to higher rank tensors, allowing us to induce the semantic representation of functions modelled in this framework. Finally, we evaluated this new semantic tensor learning model against existing benchmark data-sets provided by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011a), and showed it to outperform other models.", "startOffset": 48, "endOffset": 539}, {"referenceID": 10, "context": "We want moreover to test the Regression model against the Categorical model of Grefenstette and Sadrzadeh (2011a) and to design", "startOffset": 79, "endOffset": 114}, {"referenceID": 25, "context": "evaluation scenarios allowing a direct comparison with the MV-RNN model of Socher et al. (2012).", "startOffset": 75, "endOffset": 96}], "year": 2013, "abstractText": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face.", "creator": "LaTeX with hyperref package"}}}