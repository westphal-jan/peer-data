{"id": "1301.7367", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Utility Elicitation as a Classification Problem", "abstract": "we investigate the application of classification techniques describing utility elicitation. modeling a decision problem, two sets within parameters must generally be elicited : the probabilities yield the utilities. while the prior versus response probabilities in the model don't change from user knowledge domain, the utility models do. thus it is necessary to elicit a utility model correctly for each new user. assessment is long and easy, particularly if the outcome space spans large and not decomposable. there are two common approaches to utility function elicitation. the first is to base the determination of the marginal utility function solely on elicitation of qualitative preferences. the second generates assumptions about the form and decomposability of the utility function. here we take a different approach : we attempt to identify the new users utility function based on classification unique to a database of previously collected utility functions. manufacturers do assist by identifying clusters of expenditure functions that minimize an appropriate distance allocated. having identified the keys, we develop a classification scheme that requires many fewer slightly simpler assessments than full utility elicitation providing nothing more robust than utility elicitation based solely on preferences. we have tested our work on a hidden database of utility objects in a prenatal diagnosis domain and the results are quite promising.", "histories": [["v1", "Wed, 30 Jan 2013 15:03:05 GMT  (326kb)", "http://arxiv.org/abs/1301.7367v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["urszula chajewska", "lise getoor", "joseph norman", "yuval shahar"], "accepted": false, "id": "1301.7367"}, "pdf": {"name": "1301.7367.pdf", "metadata": {"source": "CRF", "title": "Utility Elicitation as a Classification Problem", "authors": ["Urszula Chajewska", "Lise Getoor"], "emails": ["getoor@cs.stanford.", "norman@smi.stanford."], "sections": [{"heading": null, "text": "We investigate the application of classification tech niques to utility elicitation. In a decision problem, two sets of parameters must generally be elicited: the prob abilities and the utilities. While the prior and condi tional probabilities in the model do not change from user to user, the utility models do. Thus it is necessary to elicit a utility model separately for each new user. Elicitation is long and tedious, particularly if the out come space is large and not decomposable. There are two common approaches to utility function elicitation. The first is to base the determination of the user's util ity function solely on elicitation of qualitative prefer ences. The second makes assumptions about the form and decomposability of the utility function. Here we take a different approach: we attempt to identify the new user's utility function based on classification rel ative to a database of previously collected utility func tions. We do this by identifying clusters of utility func tions that minimize an appropriate distance measure. Having identified the clusters, we develop a classifi cation scheme that requires many fewer and simpler assessments than full utility elicitation and is more ro bust than utility elicitation based solely on preferences. We have tested our algorithm on a small database of utility functions in a prenatal diagnosis domain and the results are quite promising.\n1 INTRODUCTION\nProbabilistic systems, such as Bayesian networks (Pearl 1988) and influence diagrams (Howard and Matheson 1984) are a major research focus and have been gaining popularity in a number of application domains. Many sys tems based on them are now in use. Some of these systems, especially those used in medical domains, are designed to give advice to large numbers of users. Often these users do not agree on their preferences in a given decision context. Therefore, in order to recommend appropriate actions, we need to elicit a utility function not once, as a part of cre ating a model, but many times-once for each user. This can be an extremely long and tedious process, particularly if the outcome space is large and the utility function is not easily decomposable into independent components.\nUtility elicitation has been studied extensively in the area\nof decision analysis (DA) (Luce and Raiffa 1957; Keeney and Raiffa 1976; Howard 1984a; Howard 1984b ). Recently it has started to receive attention in medical informatics (Heckerman et al. 1992; Farr and Shachter 1992; Horn berger et al. 1995) and artificial intelligence (AI) (Ha and Haddawy 1997; Linden et al. 1997; Boutilier et al. 1997). Most of the research concentrates on decomposing utility functions, taking advantage of various assumptions of in dependence between the attributes. Decomposed functions are easier to elicit and can allow more efficient reasoning procedures. Common approaches restrict attention to addi tive models (Hornberger et al. 1995; Linden et al. 1997), and partial elicitation of models (Ha and Haddawy 1997).\nIn many cases, however, attributes are preferentially depen dent and thus assumptions of decomposability are suspect. An incorrect assumption of decomposability can adversely affect our choice of actions. This problem suggests that we should perform full utility elicitation. However, outcome spaces can be very large and eliciting full utility functions from every user may be infeasible.\nYet, there is still hope for effective elicitation of users' util ity functions. Quite often, there are only a few qualita tively different utility functions and we can partition most users' utility functions into classes with very similar func tions within each group. Having these clusters of utility functions defined and knowing their prevalence in the pop ulation can guide the process of utility elicitation from a new user.\nIn our approach, we begin with a database of fully-specified utility functions 1. From this database, we cluster the util ity functions in such a way that in a given context, there is some strategy that is close to optimal according to all the functions in that cluster. This strategy is the optimal strat egy for some function in the cluster and we identify that function with the cluster. Next, we build a decision tree for classifying the utility functions according to their clusters.\n1The appreciation of the importance of individual preferences is growing in the medical community. Some medical informat ics centers are collecting users' preferences for medical decision analysis. We have reasons to hope that such studies will become common and thus databases of patient utility functions will soon be available for many domains.\nWhen we are presented with a new user, we use this deci sion tree to find a suitable cluster and the utility function that is associated with it. The types of questions posed to the user during this elicitation will be easier for them to answer than the questions required during full elicitation. In addition, the number of questions required to classify a new utility function should be significantly fewer than the full utility function elicitation would require.\nThe domain we focus on is prenatal testing. We are us ing a simplified version of the model developed by the PANDA project at Stanford Medical Informatics2. PANDA is a loose acronym for \"prenatal testing decision analy sis.\" PANDA uses knowledge gained from many studies and from practicing clinicians to advise patients on which prenatal diagnostic tests they should choose during their pregnancies. The full model includes data about six major diseases which can be diagnosed before birth, along with their prevalence and severities. It considers four tests used to diagnose these diseases. These tests have different sen sitivities, specificities, costs, and health risks. In this paper, we use a simpler model, shown in Figure I. Our simplified version considers only one disease-Down's syndrome and two tests which can diagnose it--chorionic villus sam pling (CVS) and amniocentesis (AMNIO).\nIn the real world, the decision about the choice of tests is rarely easy. The patient's risk for having a child with a serious disease depends on the mother's age, child's sex and race, and the family history. Some tests are not very accurate; others carry a significant risk of inducing miscar riages. Both miscarriage (SAB) and elective termination of the pregnancy (TAB) can affect the woman's chances of conceiving again.\nThe outcomes in our models have many attributes: the in convenience and expense of fairly invasive testing, the pos sibility of test-induced miscarriage, knowledge about the\n2See http://smi-web.stanford.edu/projects/panda!.\nhealth of the child early in the pregnancy, the possibility of future conception, and the actual health of the child. A recent study (Kuppermann et al. I997) showed that these attributes are highly correlated and the utility of an outcome cannot be predicted from the utilities of the individual at tributes. For example, consider the attributes \"future preg nancy\" and \"miscarriage\". While we can generally assume that a woman would like to conceive again following a mis carriage, and thus the attribute \"future pregnancy\" will be preferred to its negation, we can make no such assumption when a miscarriage has not occurred. Our initial analysis of the model revealed the considerable influence of the utility function (especially the patient's attitude towards the risk of having a child with a serious disease and toward a mis carriage) on the optimal choice of actions.\nIn the following section, we review some concepts from utility theory in the context of this example. In Section 3, we describe our approach to identifying clusters of util ity functions. In Section 4, we show how these clusters are used to classify a new user's utility function. In Sec tion 5, we present the results of our experiments on a small database of utility functions. In Section 6, we review some of the related work. Finally, in Section 7, we discuss poten tial benefits of our approach and future directions for this work.\n2 REVIEW: UTILITY THEORY\nThe principle of maximizing expected utility has long been established as the guide to making rational decisions. The axioms of utility theory, which are stated in terms of con straints on preferences, imply the existence of utility func tions (von Neumann and Morgenstern 1947; Savage 1954; Luce and Raiffa 1957). Let 0 be a set of possible outcomes {at, . . . ,on}\u00b7 The outcomes are also called situations (in AI literature) or consequences (in DA literature). For our model, the possible outcomes include sequences of events;\nthere are 22 outcomes in the simplified version. The set of possible strategies, S = { St, . .. , sm}, contains all possi ble decision sequences (conditional plans), such as: \"take CVS; if the result is negative, do not take any more tests; otherwise, take amniocentesis; if the result is negative, con tinue; if not, terminate the pregnancy.\" The patient's his tory, hb is an instantiation of the observable variables in the model representing information specific to the patient and her pregnancy: patient's age, child's sex and race, fam ily history of diabetes, etc. (Since we are only considering one disease in our simplified model, namely Down's syn drome, we are able to reduce the number of history vari ables to only one relevant to this disease: mother's age.)\nThe given decision strategy together with the patient's his tory induces a probability distribution over the set of out comes P(O JH,S) . Given a probability distribution P and the user's utility function defined over the outcomes, U(O), we can compute the expected utility for the given patient and the chosen strategy:\nEU(sJh) = L,P(oJh,s)U(o). 0\nVon Neumann and Morgenstern (1947) developed an ap proach to utility elicitation based on measuring the strength of a person's preference for an outcome by the risks he or she is willing to take to obtain it. Consider three out comes o 1, 02, and 03 and a user with the preference order ing 01 >- 02 >- 03. If he or she is offered a choice between 02 for sure and a gamble in which 01 will be received with probability 1t and 03 with probability ( 1 - rt), then, accord ing to the theory, there exists a value of 1t for which the user will be indifferent. The outcome 02 can then be assigned the utility value rtU(ot) + (1 -n)U(o3). The utility function was shown by von Neumann and Mor genstern (1947) to be uniquely determined up to an increas ing linear transformation, i.e., for any utility function U ( 0) and constants a and b, such that a > 0, aU ( 0) + b is also a utility function encoding the same preferences. The con stant a changes the scale of the utility function. The con stant b changes its zero point.\nIn order to compare two utility functions, we have to make sure that they are normalized, i.e., their zero points and scales are the same. Usually this is done by finding two endpoints of the scale-the best and worst possible out comes, OT and o 1_ -and assigning them the values of 1 and 0 respectively. However, the worst outcome in any given set does not necessarily have the same value for every per son. Thus it is common practice to include the death of the decision maker in the set of outcomes, on the presumption that its value is equally abysmal for everyone.\nIn this paper, we will assume that the utility functions in our database have been normalized. In the PANDA domain, the two outcomes chosen for the endpoints are OT, the birth of a healthy baby following a healthy pregnancy with full knowledge throughout that the baby is not affected by any disease, and o 1_, the death of the pregnant woman herself; the utility functions take the values in the interval [0, 1].\nUtility Elicitation as a Classification Problem 81\n3 IDENTIFYING CLUSTERS OF UTILITY MODELS\nWe assume that we have a database of N normalized util ity functions over our outcome space 0, where JOJ = D. Our data points-the utility functions-are repre sented as vectors of values, one value for each outcome, {u(ot), ... ,u(oo)}. We also assume that the utility func tions of our new users will be drawn from the same distri bution as those in our database.\nIn order to create the utility clusters, we can use any of the popular clustering algorithms. The goal is to divide a set of data points into non-overlapping groups, or clusters, of points, where points in a cluster are \"more similar\" to one another than to points in other clusters. When a dataset is clustered, every point is assigned to some cluster, and ev ery cluster can be characterized by a single reference point which we will call its prototype.\n3.1 CHARACTERIZING SIMILARITY BETWEEN\nUTILITY FUNCTIONS\nA key component of any clustering algorithm is a notion of distance between points. The simplest approach would be to treat all utility functions as vectors of values, one for each outcome, and use the Euclidean distance between them as the distance measure. In essence this approach gives each outcome equal weight. It would be a mistake, however, to do this. Not all outcomes are equally prob able, thus the values attached to different outcomes by a utility function contribute differently to the expected utility value. For example, the probability of having a miscarriage is much smaller than the probability of having a healthy child and thus the changes in utilities for these outcomes do not affect the value of the expected utility equally.\nHow can we resolve this problem? Since we are cluster ing the utility functions in the context of a specific decision problem, we can take advantage of the information con tained in our model. The quantity we want to minimize is the difference between the expected utility of a strategy we would choose for our user if we elicited her full utility function and the expected utility of the strategy we would choose for her based on our algorithm.\nTo specify it more formally, we begin by defining the ex pected utility of a particular strategy s with respect to a par ticular utility function u; and a particular history hk:"}, {"heading": "EUui(sJhk) = L,P(otJs,hk)u;(ot)", "text": "Of\nwhere 01 ranges over possible outcomes. We can then look at the best strategy for a particular utility function and his tory,\nIn the following, let Up be the utility function for some pro totype p and lets* lh be the best strategy for this prototype.\nUp k\n82 Chajewska, Getoor, Norman, and Shahar\nEventually, we will be giving advice to new users based some cluster's prototype. We would not like the result to differ significantly from what users could expect based on the full utility elicitation. In order to compute this differ ence, we need to consider two possibly different strategies: the strategy that we will pick for up, s* lh , and the strategy Up k that we would pick for the true utility function, ii, s\ufffdlhk. We will evaluate both of these strategies for a particular history hk using ii.\nDefinition 3.1: The Utility Loss (UL) for a utility function ii with respect to a utility function up and a history hk in the context of a given decision model M is"}, {"heading": "UL(ii, upihk) = EUa(s\ufffdlhk)- EUa(s:plhk)", "text": "where the expected utility is defined with respect toM. I\nWe will use this as the score we wish to minimize. Note that this measure is not symmetric: the UL of a utility func tion u; with respect to another utility function u j may be very different than the UL of Uj with respect to u; . This asymmetry matches the intended use of the measure. We will use one prototype function to advise many users.\nBased on our measure of the UL, we define the distance between two utility functions as follows:\nDefinition 3.2: The distance between two utility functions u; and u i with respect to a history hk is defined as\nNote that our distance measure is not a metric; while it is symmetric, it does not satisfy the triangle inequality.\nWe could create clusters based on averaging over histories, by minimizing the following distance:\nd(u;,uo) = 'LP(hk) \u00b7 d(u;,uoihk) hk\nThis approach may be satisfactory when using the strategy that is very good in most cases and does not have serious consequences even when we are faced with an unlikely sce nario. However, in a medical setting, where it is imperative to follow the correct strategy in the case of unlikely situa tion, this approach is not appropriate.\nInstead, in our algorithm, we are creating clusters for a par ticular history. We can do this in two ways: online and offline. In the online version, we start the process when we are presented with a new user whose utility function we want to classify. At that point, we know the particular history hk that we are interested in, and we create the clus ters relative to that history. Alternatively, we may do this offline, and create clusters for each potential history. The choice will depend on the size of the problem, the response requirements for the online utility elicitation and the stor age availability. Note that in either case, we are able to use the entire database to build our clusters because we assume\nthat the utility function is independent of the patient his tory. This lack of data fragmentation is one of the benefits of this approach.\n3.2 CLUSTERING THE UTILITY FUNCTIONS\nClustering algorithms come in two general flavors, parti tioning methods (such ask-means) and hierarchical meth ods (Willett 1988). In general, hierarchical methods are faster than partitioning methods. We are concerned about the efficiency of our algorithm, since it may be done in an online setting. Therefore, we chose a hierarchical agglom erative clustering algorithm. Another benefit of a hierarchi cal method is that it allows us to tradeoff explicitly between the number of clusters and the similarity score.\nThe algorithm starts by putting every data point in a sepa rate cluster. Then, it computes the distances between every two clusters. It finds the two closest clusters and combines them into one. It continues to merge clusters until some stopping criterion is met.\nThere are several ways to define the distances between clus ters containing more than one element. We use one of the standard definitions, the group average link method, which uses the average values of the pairwise links within the po tential new cluster to determine similarity.\nAfter the clustering, we choose a prototype (cluster repre sentative) for every cluster by picking the utility function with the lowest score\nScore(u,) = L UL(u;, uj ihk) ujEcluster\n4 CLASSIFYING UTILITY MODELS\nGiven that we have found our k clusters, we would like to find the cluster to which a new user's utility is most likely to belong, and use the prototype utility function for that cluster to determine the recommended strategy for our new user. Once we have clustered the data, we can label each utility function in the database with the cluster to which it was assigned in the clustering phase. Thus the task of identifying a cluster prototype for the new user is a classi fication problem. At this point, we could apply a nearest neighbor or case-based approach to find a cluster label for the new user. The problem is that we would still need to elicit the user's full utility function in order to do the near est neighbor calculation, which would defeat the purpose of our endeavor. Instead, we consider building a decision tree for the labeled database. We construct the tree by choosing tests and recursively splitting the database into partitions based on the outcomes of the tests. We keep splitting the partitions until the labels of the utility functions in each partition are largely from the same cluster. These will be the leaves of the decision tree. In order to classify a new user, we begin at the root of the decision tree, and ask the user to answer the test at each node in the decision tree. We traverse a path down the tree until we reach a leaf node. We then classify the new user's utility function according to the labels of the database utility functions that are at that leaf. Decision trees are particularly appropriate in this context because of the ease of human interpretation. More impor tantly, we will see that the questions required of users are of a form that are easier to answer than standard gamble questions.\n4.1 BUILDING THE DECISION TREE\nThe key components of decision tree induction algorithms are: the types of splits considered, the splitting criteria, or how the best split is chosen, and stopping or pruning rules. We describe each of these in turn.\nThere are N utility functions in our database and D fea tures in each utility vector. Each of the features is a real value between 0.0 and 1.0 that is the normalized utility for a particular outcome in the outcome space. There are two types of splits we allow in the tree: preference splits and feature splits. Feature splits are of the form \"Is outcome Oi preferred to the standard lottery, [c,oT; (1- c),oj_]?''. In decision tree algorithms that allow real valued features, this is the standard type of split considered. Preference splits are of the form \"Is outcome Oi preferred to outcome oj?\" Preference splits are a special type of linear combi nation splits. These linear combination splits are supported in some of the more sophisticated decision tree packages, such as CART (Breiman et al. 1984); here, by using the domain knowledge and looking only at preference splits, we need to consider only a small subset of all the possible linear combination splits.\nIf we were doing full model elicitation, users would need\nUtility Elicitation as a Classification Problem 83\nto answer questions of the form: \"For what value of c is the user indifferent between outcome Oi and the standard lot tery, [c,oT; (1- c),oj_]?\". The questions required for the feature splits and preference splits are each easier to an swer. Rather than having to specify a value for c, the user just has a yes or no question to answer.\nHow do we compute the best split? There are many split ting rules considered in the literature. Each in some sense measures the impurity of a child node in the tree relative to its parent. The purity of a node n is a measure of the con centration of labels at that node in the tree. A node is pure if the labels of all of the examples at that node are the same. The impurity reaches its maximum if the distribution of la bels is uniform. The most common measure of impurity is entropy: k\nI(n) =- L Pilogpi , i=i\nwhere Pi is an estimate of the probability of having cluster label i given that we are at node n in the decision tree. Pi is simply the number of examples at node n having cluster label i divided by the total number of examples at node n.\nOnce we have chosen our measure, we can compute the gain in purity for a particular split s. It is the impurity of the node n minus the impurity of each of its children, n1 and nr, weighted by the estimated probability of being at each of the children given the split s:\nGain= I(n)- P(niJs)I(ni)- P(nrls)I(nr).\nOur tree building algorithm is a greedy algorithm that chooses the split that has the greatest Gain at each step. Thus we compute, for each preference split and each fea ture split, the gain with that split. There are D(D- 1) /2 preference splits to consider, each of the form Oi >-OJ, for each i, 1 :S i < D, and each j > i. For each feature, there are at most N feature splits to consider, each of the form u(oi) :S c, where i is the feature we are splitting on and c is the split point. We need only consider as split points ob served values of that feature for some utility function in our database.\nWe disallow a feature split if the values immediately to the left and right of the split are very close. We prefer to find \"gaps\" in the values for the feature. We want the questions to be easy for the user to answer. If the split value is too close to the user's utility value, the question may be very difficult for her to answer and her answer would not be re liable. For example, if we split on the utility of the outcome \"No tests, child with Down's syndrome\" at the value 0.975, and the user's utility is quite close to that value, she will have a hard time answering the question. The appropri ate threshold can be determined experimentally. Note that such a threshold will introduce a slight bias towards pref erence questions. This is quite fortunate, since preference questions are much easier for the user to answer.\nThere are also many stopping criteria to consider. Here we stop when all the utility functions have the same cluster label.\n4.2 USING THE TREE TO CLASSIFY NEW USERS\nNote that in general, for full utility elicitation, there are 101 lottery questions that must be asked of the form \"For what value of c is the user indifferent between outcome o; and the standard lottery, [ c, OT; ( 1 - c), o l.] ?\". In our decision tree classification, there will be at most d questions asked, where d is the maximum depth of the tree, and each of the questions will be a preference question of the form, \"Is out come o; preferred to outcome Oj?\" or, for a feature split a question of the form, \"Is outcome o; preferred to the stan dard lottery, [c, OT; (1 - c),o _1_]?\", for fixed c.\nHow is this procedure helping us to advise new users? First, we collect the history data hk from the user. We then find the appropriate clustering for that history, and classify the new user's utility function. When a user's utility function is determined to belong to a given cluster characterized by the prototype p;, we find the best strategy for the patient's history hk and the prototype's utility function Up;\u00b7 Thus we find a nearly-optimal strategy for the user with only a small number of question.\nDecision trees have many well known advantages. In our context, an important advantage is the ease of handling both preference splits and feature splits in a single frame work. This allows us to seamlessly integrate the two types of questions in a well-founded manner based on informa tion theoretic principles. Another important advantage is the human interpretability of a decision tree; practitioners can look at the tree and see if the classifications it provides make sense. Finally, as mentioned earlier, the key advan tage that we are exploiting for elicitation is that the ques tions required of the users are significantly easier for them to answer.\nDecision trees also have many disadvantages. All tech niques for building decision trees rely on greedy strategies, because finding the optimal decision tree is intractable. This leads to instability and high variance in the algorithms. A small perturbation in the input data can lead to quite dif ferent decision trees. Our approach of first clustering the data, before building the decision tree for the data, should help us overcome this problem. We have not explored this issue in depth, but see it an interesting area for further work.\nThere are many related data structures such as regression trees and KD-trees. It is possible to reformulate our ap proach of clustering based on UL and building a classifica tion tree from the clusters, into a modification of the appro priate regression tree or KD-tree algorithms, where we use UL to compute the distances and we consider splits that are equivalent to our preference and feature splits. The result ing algorithms are equivalent in terms of time and space complexity.\n5 EMPIRICAL RESULTS\nWe ran our algorithm with the simplified PANDA model and a database of utility functions that were collected by Miriam Kuppermann of UCSF/Mount Zion Medical Cen ter (1997). There were 70 utility functions in the database. Of these, many had missing values. In the experiments re ported, we ran on the 55 utility functions with no missing values.\nWe considered four different patient histories, correspond ing to the age of the woman: TEEN, 25YO, 35YO, and 45YO. We also considered an average history, AVE, as a baseline.\nUtility Elicitation as a Classification Problem 85\nFigure 4: Decision tree for the TEEN history. The numbers at the leaves represent cluster labels and corresponding strategies (see Figure 3).\nFigure 5: Decision tree for the 45YO history. The numbers at the leaves represent cluster labels and corresponding strategies (see Figure 3).\n86 Chajewska, Getoor, Norman, and Shahar\n5.1 PROPERTIES OF THE DOMAIN\nWe began by computing for every utility function u j in our database the optimal strategy for the current history hk: s: \u00b7lhk. We also computed the expected utility of each of th\ufffdse strategies: EUu.(s* \u00b7lh ) . Now, for each utility func-\nJ U) k tion we compared the expected utility of its best strategy to the expected utility of the best strategy for each of the other utility functions. This allowed us to compute the UL for all pairs of utility functions.\nWe next clustered the data using our hierarchical agglom erative clustering algorithm. We found that for every age group we considered, we could identify four clusters with intra-cluster distances of 0. Thus, out of 18 strategies al lowed in our model3 only four strategies were found to be optimal for some utility function in the database. These four strategies are shown in Figure 3. Interestingly, the strategies didn't differ across the age groups. There were, however, significant differences in the sizes of clusters cor responding to these strategies. For some utility functions, the same strategy was optimal regardless of history. For others, it was different in every age group. These results correspond to the intuitions of many practitioners in the field of prenatal diagnosis. Since the probability of hav ing a child with Down's syndrome increases dramatically with the age of the mother, for some women the optimal strategy should change with age. On the other hand, if the woman's attitude towards the possibility of having a dis abled child is extreme (very negative or very positive), one strategy might be optimal for her throughout her life.\nIt is hard to characterize precisely the group of women for which a given strategy is optimal. The utility functions are sometimes very diverse even within the same cluster. Intu itively, we can think of Strategy 1 as appropriate for women with relatively low risk aversion towards the possibility of giving birth to a child with Down's syndrome. Strategy 4 should be used for women very risk averse in this respect. Strategy 2 is best for women who are equally afraid of hav ing a child with Down's syndrome and aborting a healthy fetus. Strategy 3 was found to be appropriate for a woman who was also very afraid of aborting a healthy fetus and in addition, placed a very high value on knowing early in the pregnancy whether the disease was present. These charac terizations, however, are very crude approximations.\nAfter the data were clustered, we ran our decision tree al gorithm. The trees generated for different histories were very different. Figures 4 and 5 show examples of decision trees for two extreme age groups: TEEN and 45YO. As we expected, the most important split for the TEEN tree was the feature split on deciding not to undergo any diagnos tic procedure and having a Down's syndrome child. The probability of having a child with this disease is so low for this age group that only extreme values for this feature can cause the strategy to differ from the one the doctors\n3We restricted the space of possible strategies to eliminate nonsensical ones, e.g., involving prenatal tests after termination.\nusually advise their teen patients to pursue: \"do nothing,\" i.e., strategy 1. Note that there are three feature splits for this feature in the tree, with different split points. On the other hand, the decision tree for 45YO demonstrates sen sitivity to women's attitudes towards abortion and miscar riage. This phenomenon is again understandable, since the risk of Down's is the highest for this age group and all of the diagnostic tests carry a significant risk of inducing mis carriages.\nNote that we need at most 6 (in the TEEN tree) or 5 (in the 45YO tree) assessments to completely identify the user's utility function. The numbers in the leaves of the decision tree represent different clusters and corresponding optimal strategies (see Figure 3). Recall that a full utility assess ment for our model would require the user to assign values to 22 outcomes.\n5.2 PERFORMANCE RESULTS\nWe were interested to see how well our algorithm would perform at classifying a new user. We measured the error of our algorithm by creating clusters and decision trees for a subset of the database, the training set, and then evaluated the performance of the decision tree on a small test set. We measure the error as the UL from using the utility function assigned by the decision tree as compared to the true utility function.\nFigure 6 shows a sample learning curve for this domain. It shows the error as a function of the dataset size. We see that surprisingly, even for this small dataset, the results are quite promising. We are able to notice a steady decrease in error as the training set size increases. The results are particularly promising for the TEEN, 25YO and 35YO cat egories. The errors for AVE and 45YO are higher. A plau sible explanation for the higher error for the 45YO users is that the choice of strategy is more sensitive for this age group to small changes in the utility function. This sensi tivity is caused by the fact that very unlikely outcomes such as Down's or miscarriage are more probable for 45 year old women than for other age groups.\nFigure 7 shows the error as a function of the number of clusters. We see that initially, as cluster size increases, we have an improvement in error. Then, as cluster size increases further, the error increases. The interesting phe nomenon that we notice, is that despite the fact that there are only four strategies, it is actually better to create more than four clusters. This is because our decision trees are built over the space of the utility functions, while our clus tering algorithm is geared toward identifying utility func tions with similar optimal strategies. This curve also il lustrates the classic phenomenon of overfitting-with too weak a bias (by allowing a large number of clusters), we overfit the data and observe poor performance on the test set. Using our algorithm, we can find the appropriate num ber of clusters.\nThere are several directions in which we would like expand\nour experiments. The most important of these is to test on a larger database for a richer model. Data for a larger study is currently being collected, and we plan to apply our algorithm to this data when it becomes available.\n6 RELATED WORK\nWith recent advances in the power and scope of decision theoretic systems, utility elicitation has become a lively and expanding area of research. A few projects are particularly relevant to this work. Hornberger (1995) applied CART classification to utilities for the purpose of simplifying util ity assessment. His work assumed linear additive indepen dence between utility attributes, and used simulated rather than observed utilities as a basis for classification. Horvitz and Klein ( 1993) used utility as a basis for categorization in the context of decision analysis. They abstract policy and outcome spaces using clustering approaches that are sim ilar to ours. Farr and Shachter (1992) designed a system to assess utilities parsimoniously using simulated decision scenarios rather than complete standard-gamble utility as sessment. Their approach was geared toward eliciting util ities from a single expert user rather than many individual end-users. Finally, Jimison (1992) addressed the broader task of tailoring general decision models to individual users by explicitly representing uncertainties about key utility and probability parameters. Her work was geared toward explaining clinical decision models and refining them in a principled way using expected value of information.\n7 CONCLUSIONS A ND FUTURE WORK\nWe have presented a new approach to utility function elici tation based on machine learning techniques. This method provides an effective alternative to the current approaches to utility elicitation. We neither have to make assumptions about the decomposability of the utility function nor are we limited to eliciting only preferences; thus our approach should suit a wide variety of applications. We have applied\nUtility Elicitation as a Classification Problem 87\nthis framework to a prenatal testing decision model, and found the results quite promising. We found that we could in fact identify a small number of prototype utility func tions. We also found that we could classify a new user's utility function with only a small number of easy-to-answer questions, with the introduction of only minimal error due to using the prototype utility function rather than the user's true utility function.\nThe output of our approach, a decision tree, has the advan tage of being easily interpretable by practitioners. More importantly, it makes the elicitation of user preferences much easier. The types of questions the users must answer are simpler than standard gamble questions. We hope that this will enable individualized utility elicitation for users in real clinical settings.\nWe plan to continue our research in several directions:\n\u2022 We would like to bound the UL for a particular pa tient given the cluster we have identified. Every leaf in our decision tree corresponds to a convex region in the utility space. By looking at the optimal policies at the vertices of this region, we can come up with a bound on the UL over the entire region. In cases where this bound is not tight enough, we may ask additional questions until we achieve satisfactory bounds.\n\u2022 We would like to explore the possibility of including history information in the clustering procedure. Cur rently we create separate clusterings for each history, using the entire database under the assumption that the utility function is independent of the history. This as sumption allows us to avoid data segmentation in our small database. We would like to explore the conse quences of relaxing this assumption.\n\u2022 Currently, we do not consider clustering based on the similarity of the actions in the policies, we only cluster on the basis of UL. We also do not consider abstrac tions of the outcome space. It would be quite inter esting to consider flexible methods of clustering along all three of these dimensions.\n88 Chajewska, Getoor, Norman, and Shahar\n\u2022 While many utility functions are not decomposable in general, we may find a decomposition applicable to particular clusters. We could also use conditional independence between the attributes to speed up the classification process.\n\u2022 In many cases, there are some obvious constraints that all utility functions should obey. For example, in the PANDA model, we can assume that a woman seeking advice on the choice of diagnostic tests would con sider having a healthy baby more desirable than hav ing a baby with a severe disability. We would like to explore the possibility of using such constraints background knowledge-to increase the effectiveness of our procedure.\nAcknowledgments\nWe are very grateful to Miriam Kuppermann of UCSF/Mount Zion Medical Center for sharing her database of utility functions. We would like to thank Mehran Sa hami for useful suggestions and use of his document clus tering code that we adapted to our framework. We also used the inference engine made available to us by Cecil Huang. We discussed the work presented in this paper with many people. We'd like to acknowledge helpful com ments from Xavier Boyen, Denise Draper, Jerome Fried man, Eric Horvitz, Daphne Koller, Mark Peot and Yoav Shoham. Samuel Posner helped us with the transmission and format of the database. Urszula Chajewska was sup ported by the ARPI grant F30602-95-C-0251. Lise Getoor was supported by a National Physical Sciences Consortium fellowship. This work was also supported through the gen erosity of the Powell Foundation, by ONR grant N0001496-1-0718, and by DARPA contract DACA76-93-C-0025, under subcontract to Information Extraction and Transport, Inc. Joseph Norman was supported by the Medical Scien tist Training Program, and Yuval Shahar by National Li brary of Medicine grant LM06245 and National Science Foundation grant IRI-9528444.\nReferences\nBoutilier, C., R. Brafman, C. Geib, and D. Poole (1997). A constraint-based approach to preference elicitation and de cision making. In AAAI Spring Symposium on Qualitative Preferences in Deliberation and Practical Reasoning.\nBreiman, L., J. H. Friedman, R. A. Olshen, et al. (1984). Classification and regression trees. In P. J. Bickel, W. S. Cleveland, and R. M. Dudley (Eds.), The Wadsworth Statis tics/Probability Series, pp. 5-20. Wadsworth International Group.\nFarr, B. R. and R. D. Shachter (1992). Representation of preferences in decision-support systems. Computers and Biomedical Research 25, 324-335.\nHa, V. and P. Haddawy (1997). Problem-focused incre mental elicitation of multi-attribute utility models. In Proc. UAI-97, pp. 215-222.\nBeckerman, D., E. Horvitz, and B. Nathwani (1992). To ward normative expert systems: Part I. The Pathfinder project. Methods of Information in Medicine 3/, 90-105.\nHornberger, J. C., H. Habraken, and D. A. Bloch (1995). Minimum data needed on patient preferences for accurate, efficient medical decision making. Medical Care 33(3), 297-310.\nHorvitz, E. and A. Klein (1993). Utility-based abstraction and categorization. In Proc. UA/-93, pp. 128-135.\nHoward, R. A. (1984a). The foundations of decision anal ysis. In R. A. Howard and J. E. Matheson (Eds.), The Prin ciples and Applications of Decision Analysis. Menlo Park, CA: Strategic Decisions Group.\nHoward, R. A. (1984b). Risk preference. In R. A. Howard and J. E. Matheson (Eds.), The Principles and Applications of Decision Analysis. Menlo Park, CA: Strategic Decisions Group.\nHoward, R. A. and J. E. Matheson (1984). Influence dia grams. In R. A. Howard and J. E. Matheson (Eds.), The Principles and Applications of Decision Analysis. Menlo Park, CA: Strategic Decisions Group.\nJimison, H. B., L. M. Fagan, R. D. Shachter, and E. H. Shortliffe (1992). Patient-specific explanation in models of chronic disease. AI in Medicine 4, 191-205.\nKeeney, R. L. and H. Raiffa (1976). Decisions with Mul tiple Objectives: Preferences and Value Tradeoffs. New York: John Wiley & Sons, Inc. Kuppermann, M., S. Shiboski, D. Feeny, E. P. Elkin, and A. E. Washington (1997, Jan-Mar). Can preference scores for discrete states be used to derive preference scores for an entire path of events? Medical Decision Making 17(1).\nLinden, G., S. Hanks, and N. Lesh (1997). Interactive as sessment of user preference models: The automated travel assistant. In Proc. User Modelling '97.\nLuce, R. D. and H. Raiffa (1957). Games and Decisions. New York: John Wiley & Sons. Pearl, J. (1988). Probabilistic Reasoning in Intelligent Sys tems. San Francisco, CA: Morgan Kaufmann.\nSavage, L. J. (1954). Foundations of Statistics. New York: John Wiley & Sons. Shahar, Y., J. W. Egar, and R. Pichumani (1992). Decision analysis of amniocentesis for prenatal diagnosis. In Proc. of Medical Decision Analysis.\nvon Neumann, J. and 0. Morgenstern (1947). Theory of Games and Economic Behavior (2nd ed.). Princeton, N.J.: Princeton University Press.\nWillett, P. (1988). Recent trends in hierarchic document clustering: A critical review. Information Processing & Management 24(5), 577-597."}], "references": [{"title": "A constraint-based approach to preference elicitation and de\u00ad cision making", "author": ["C. Boutilier", "R. Brafman", "C. Geib", "D. Poole"], "venue": "AAAI Spring Symposium on Qualitative Preferences in Deliberation and Practical Reasoning.", "citeRegEx": "Boutilier et al\\.,? 1997", "shortCiteRegEx": "Boutilier et al\\.", "year": 1997}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J.H. Friedman", "R.A. Olshen"], "venue": "The Wadsworth Statis\u00ad tics/Probability Series,", "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Representation of preferences in decision-support systems", "author": ["B.R. Farr", "R.D. Shachter"], "venue": "Computers and Biomedical Research 25, 324-335.", "citeRegEx": "Farr and Shachter,? 1992", "shortCiteRegEx": "Farr and Shachter", "year": 1992}, {"title": "Problem-focused incre\u00ad mental elicitation of multi-attribute utility models", "author": ["V. Ha", "P. Haddawy"], "venue": "Proc. UAI-97, pp. 215-222.", "citeRegEx": "Ha and Haddawy,? 1997", "shortCiteRegEx": "Ha and Haddawy", "year": 1997}, {"title": "To\u00ad ward normative expert systems: Part I", "author": ["D. Beckerman", "E. Horvitz", "B. Nathwani"], "venue": "The Pathfinder project. Methods of Information in Medicine 3/, 90-105.", "citeRegEx": "Beckerman et al\\.,? 1992", "shortCiteRegEx": "Beckerman et al\\.", "year": 1992}, {"title": "Minimum data needed on patient preferences for accurate, efficient medical decision making", "author": ["J.C. Hornberger", "H. Habraken", "D.A. Bloch"], "venue": "Medical Care 33(3), 297-310.", "citeRegEx": "Hornberger et al\\.,? 1995", "shortCiteRegEx": "Hornberger et al\\.", "year": 1995}, {"title": "Utility-based abstraction and categorization", "author": ["E. Horvitz", "A. Klein"], "venue": "Proc. UA/-93, pp. 128-135.", "citeRegEx": "Horvitz and Klein,? 1993", "shortCiteRegEx": "Horvitz and Klein", "year": 1993}, {"title": "The foundations of decision anal\u00ad ysis", "author": ["R.A. Howard"], "venue": "R. A. Howard and J. E. Matheson (Eds.), The Prin\u00ad ciples and Applications of Decision Analysis. Menlo Park, CA: Strategic Decisions Group.", "citeRegEx": "Howard,? 1984a", "shortCiteRegEx": "Howard", "year": 1984}, {"title": "Risk preference", "author": ["R.A. Howard"], "venue": "R. A. Howard and J. E. Matheson (Eds.), The Principles and Applications of Decision Analysis. Menlo Park, CA: Strategic Decisions Group.", "citeRegEx": "Howard,? 1984b", "shortCiteRegEx": "Howard", "year": 1984}, {"title": "Influence dia\u00ad grams", "author": ["R.A. Howard", "J.E. Matheson"], "venue": "R. A. Howard and J. E. Matheson (Eds.), The Principles and Applications of Decision Analysis. Menlo Park, CA: Strategic Decisions Group.", "citeRegEx": "Howard and Matheson,? 1984", "shortCiteRegEx": "Howard and Matheson", "year": 1984}, {"title": "Patient-specific explanation in models of chronic disease", "author": ["H.B. Jimison", "L.M. Fagan", "R.D. Shachter", "E.H. Shortliffe"], "venue": "AI in Medicine 4, 191-205.", "citeRegEx": "Jimison et al\\.,? 1992", "shortCiteRegEx": "Jimison et al\\.", "year": 1992}, {"title": "Decisions with Mul\u00ad tiple Objectives: Preferences and Value Tradeoffs", "author": ["R.L. Keeney", "H. Raiffa"], "venue": "New York: John Wiley & Sons, Inc.", "citeRegEx": "Keeney and Raiffa,? 1976", "shortCiteRegEx": "Keeney and Raiffa", "year": 1976}, {"title": "Jan-Mar). Can preference scores for discrete states be used to derive preference scores for an entire path of events? Medical Decision Making 17(1)", "author": ["M. Kuppermann", "S. Shiboski", "D. Feeny", "E.P. Elkin", "A.E. Washington"], "venue": null, "citeRegEx": "Kuppermann et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kuppermann et al\\.", "year": 1997}, {"title": "Interactive as\u00ad sessment of user preference models: The automated travel assistant", "author": ["G. Linden", "S. Hanks", "N. Lesh"], "venue": "Proc. User Modelling '97.", "citeRegEx": "Linden et al\\.,? 1997", "shortCiteRegEx": "Linden et al\\.", "year": 1997}, {"title": "Games and Decisions", "author": ["R.D. Luce", "H. Raiffa"], "venue": "New York: John Wiley & Sons.", "citeRegEx": "Luce and Raiffa,? 1957", "shortCiteRegEx": "Luce and Raiffa", "year": 1957}, {"title": "Probabilistic Reasoning in Intelligent Sys\u00ad tems", "author": ["J. Pearl"], "venue": "San Francisco, CA: Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Foundations of Statistics", "author": ["L.J. Savage"], "venue": "New York: John Wiley & Sons.", "citeRegEx": "Savage,? 1954", "shortCiteRegEx": "Savage", "year": 1954}, {"title": "Decision analysis of amniocentesis for prenatal diagnosis", "author": ["Y. Shahar", "J.W. Egar", "R. Pichumani"], "venue": "Proc. of Medical Decision Analysis.", "citeRegEx": "Shahar et al\\.,? 1992", "shortCiteRegEx": "Shahar et al\\.", "year": 1992}, {"title": "Theory of Games and Economic Behavior (2nd ed.)", "author": ["J. von Neumann"], "venue": null, "citeRegEx": "Neumann,? \\Q1947\\E", "shortCiteRegEx": "Neumann", "year": 1947}, {"title": "Recent trends in hierarchic document clustering: A critical review", "author": ["P. Willett"], "venue": "Information Processing & Management 24(5), 577-597.", "citeRegEx": "Willett,? 1988", "shortCiteRegEx": "Willett", "year": 1988}], "referenceMentions": [{"referenceID": 15, "context": "Probabilistic systems, such as Bayesian networks (Pearl 1988) and influence diagrams (Howard and Matheson 1984) are a major research focus and have been gaining popularity in a number of application domains.", "startOffset": 49, "endOffset": 61}, {"referenceID": 9, "context": "Probabilistic systems, such as Bayesian networks (Pearl 1988) and influence diagrams (Howard and Matheson 1984) are a major research focus and have been gaining popularity in a number of application domains.", "startOffset": 85, "endOffset": 111}, {"referenceID": 2, "context": "Recently it has started to receive attention in medical informatics (Heckerman et al. 1992; Farr and Shachter 1992; Horn\u00ad berger et al. 1995) and artificial intelligence (AI) (Ha and Haddawy 1997; Linden et al.", "startOffset": 68, "endOffset": 141}, {"referenceID": 3, "context": "1995) and artificial intelligence (AI) (Ha and Haddawy 1997; Linden et al. 1997; Boutilier et al. 1997).", "startOffset": 39, "endOffset": 103}, {"referenceID": 13, "context": "1995) and artificial intelligence (AI) (Ha and Haddawy 1997; Linden et al. 1997; Boutilier et al. 1997).", "startOffset": 39, "endOffset": 103}, {"referenceID": 0, "context": "1995) and artificial intelligence (AI) (Ha and Haddawy 1997; Linden et al. 1997; Boutilier et al. 1997).", "startOffset": 39, "endOffset": 103}, {"referenceID": 5, "context": "Common approaches restrict attention to addi\u00ad tive models (Hornberger et al. 1995; Linden et al. 1997), and partial elicitation of models (Ha and Haddawy 1997).", "startOffset": 58, "endOffset": 102}, {"referenceID": 13, "context": "Common approaches restrict attention to addi\u00ad tive models (Hornberger et al. 1995; Linden et al. 1997), and partial elicitation of models (Ha and Haddawy 1997).", "startOffset": 58, "endOffset": 102}, {"referenceID": 3, "context": "1997), and partial elicitation of models (Ha and Haddawy 1997).", "startOffset": 41, "endOffset": 62}, {"referenceID": 16, "context": "The axioms of utility theory, which are stated in terms of con\u00ad straints on preferences, imply the existence of utility func\u00ad tions (von Neumann and Morgenstern 1947; Savage 1954; Luce and Raiffa 1957).", "startOffset": 132, "endOffset": 201}, {"referenceID": 14, "context": "The axioms of utility theory, which are stated in terms of con\u00ad straints on preferences, imply the existence of utility func\u00ad tions (von Neumann and Morgenstern 1947; Savage 1954; Luce and Raiffa 1957).", "startOffset": 132, "endOffset": 201}, {"referenceID": 18, "context": "Von Neumann and Morgenstern (1947) developed an ap\u00ad proach to utility elicitation based on measuring the strength of a person's preference for an outcome by the risks he or she is willing to take to obtain it.", "startOffset": 4, "endOffset": 35}, {"referenceID": 18, "context": "The utility function was shown by von Neumann and Mor\u00ad genstern (1947) to be uniquely determined up to an increas\u00ad ing linear transformation, i.", "startOffset": 38, "endOffset": 71}, {"referenceID": 19, "context": "Clustering algorithms come in two general flavors, parti\u00ad tioning methods (such ask-means) and hierarchical meth\u00ad ods (Willett 1988).", "startOffset": 118, "endOffset": 132}, {"referenceID": 1, "context": "These linear combination splits are supported in some of the more sophisticated decision tree packages, such as CART (Breiman et al. 1984); here, by using the domain knowledge and looking only at preference splits, we need to consider only a small subset of all the possible linear combination splits.", "startOffset": 117, "endOffset": 138}, {"referenceID": 2, "context": "Farr and Shachter (1992) designed a system to assess utilities parsimoniously using simulated decision scenarios rather than complete standard-gamble utility as\u00ad sessment.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Farr and Shachter (1992) designed a system to assess utilities parsimoniously using simulated decision scenarios rather than complete standard-gamble utility as\u00ad sessment. Their approach was geared toward eliciting util\u00ad ities from a single expert user rather than many individual end-users. Finally, Jimison (1992) addressed the broader task of tailoring general decision models to individual users by explicitly representing uncertainties about key utility and probability parameters.", "startOffset": 0, "endOffset": 316}], "year": 2011, "abstractText": "We investigate the application of classification tech\u00ad niques to utility elicitation. In a decision problem, two sets of parameters must generally be elicited: the prob\u00ad abilities and the utilities. While the prior and condi\u00ad tional probabilities in the model do not change from user to user, the utility models do. Thus it is necessary to elicit a utility model separately for each new user. Elicitation is long and tedious, particularly if the out\u00ad come space is large and not decomposable. There are two common approaches to utility function elicitation. The first is to base the determination of the user's util\u00ad ity function solely on elicitation of qualitative prefer\u00ad ences. The second makes assumptions about the form and decomposability of the utility function. Here we take a different approach: we attempt to identify the new user's utility function based on classification rel\u00ad ative to a database of previously collected utility func\u00ad tions. We do this by identifying clusters of utility func\u00ad tions that minimize an appropriate distance measure. Having identified the clusters, we develop a classifi\u00ad cation scheme that requires many fewer and simpler assessments than full utility elicitation and is more ro\u00ad bust than utility elicitation based solely on preferences. We have tested our algorithm on a small database of utility functions in a prenatal diagnosis domain and the results are quite promising.", "creator": "pdftk 1.41 - www.pdftk.com"}}}