{"id": "1302.4549", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2013", "title": "Breaking the Small Cluster Barrier of Graph Clustering", "abstract": "this paper investigates graph clustering in the planted cluster model in the presence of { \\ em small clusters }. traditional results dictate that for an algorithm to fail correctly recover the clusters, { \\ - all } clusters must are sufficiently large ( in particular, $ \\ tilde { \\ omega } ( \\ sqrt { n } ) $ ne $ n $ plus lowest number of nodes of the kernel ). we show plainly this is not really a restriction : by presenting more refined analysis of the trace - norm based recovery structures proposed in jalali et al. ( 2011 ) and chen et al. ( 2012 ), we prove that small plots, under certain mild restriction, don't hinder suppression of clusters ones.", "histories": [["v1", "Tue, 19 Feb 2013 09:21:09 GMT  (57kb,D)", "https://arxiv.org/abs/1302.4549v1", null], ["v2", "Wed, 20 Feb 2013 08:35:39 GMT  (56kb,D)", "http://arxiv.org/abs/1302.4549v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nir ailon", "yudong chen", "huan xu"], "accepted": true, "id": "1302.4549"}, "pdf": {"name": "1302.4549.pdf", "metadata": {"source": "CRF", "title": "Breaking the Small Cluster Barrier of Graph Clustering", "authors": ["Nir Ailon", "Yudong Chen", "Xu Huan"], "emails": ["nailon@cs.technion.ac.il", "ydchen@utexas.edu", "mpexuh@nus.edu.sg"], "sections": [{"heading": null, "text": "\u221a n) where n is the number of nodes of the graph). We show\nthat this is not really a restriction: by a more refined analysis of the trace-norm based matrix recovery approach proposed in Jalali et al. [2011] and Chen et al. [2012], we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to recover almost all clusters via a \u201cpeeling strategy\u201d, i.e., recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often as large clusters are learned (and removed).\nFrom a high level, this paper sheds novel insights on high-dimensional statistics and learning structured data, by presenting a structured matrix learning problem for which a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations does the job."}, {"heading": "1 Introduction", "text": "This paper considers a classic problem in machine learning and theoretical computer science, namely graph clustering, i.e., given an undirected unweighted graph, partition the nodes into disjoint clusters, so that the density of edges within one cluster is higher than those across clusters. Graph clustering arises naturally in many application across science and engineering. Some prominent examples include community detection in social network Mishra et al. [2007], submarket identification in E-commerce and sponsored search Yahoo!-Inc [2009], and co-authorship analysis in analyzing document database Ester et al. [1995], among others. From a purely binary classification theoretical point of view, the edges of the graph are (noisy) labels of similarity or affinity between pairs of objects, and the concept class consists of clusterings of the objects (encoded graphically by identifying clusters with cliques).\nMany theoretical results in graph clustering [e.g., Boppana, 1987, Chen et al., 2012, McSherry, 2001] consider the planted partition model, in which the edges are generated randomly; see Section 1.1 for more details. While numerous different methods have been proposed, their performance guarantees all share the following manner \u2013 under certain condition of the density of edges (within clusters and across clusters), the proposed method succeeds to recover the correct clusters exactly if all clusters are larger than a threshold size, typically \u2126\u0303( \u221a n).\n\u2217Department of Computer Science, Technion Israel Institute of Technology. nailon@cs.technion.ac.il \u2020Department of Electrical and Computer Engineering, The University of Texas at Austin. ydchen@utexas.edu \u2021Department of Mechanical Engineering, National University of Singapore. mpexuh@nus.edu.sg\nar X\niv :1\n30 2.\n45 49\nv2 [\ncs .L\nG ]\n2 0\nFe b\nIn this paper, we aim to break this small cluster barrier of graph clustering. Correctly identifying extremely small clusters is inherently hard as they are easily confused with \u201cfake\u201d clusters generated by noisy edges1, and is not the focus of this paper. Instead, in this paper we investigate a question that has not been addressed before: Can we still recover large clusters in the presence of small clusters? Intuitively, this should be doable. To illustrate, consider an extreme example where the given graph G consists two disjoint subgraphs G1 and G2, where G1 is a graph that can be correctly clustered using some existing method, and G2 is a small-size clique. G certainly violates the minimum cluster size requirement of previous results, but why should G2 spoil our ability to cluster G1?\nOur main result confirms this intuition. We show that the cluster size barrier arising in previous work [e.g., Chaudhuri et al., 2012, Bolloba\u0301s and Scott, 2004, Chen et al., 2012, McSherry, 2001] is not really a restriction, but rather an artifact of the attempt to solve the problem in a single shot using convex relaxation techniques. Using a more careful analysis, we prove that the mixed trace-norm and `1 based convex formulation, initially proposed in Jalali et al. [2011], can recover clusters of size \u2126\u0303( \u221a n) even in the presence of smaller clusters. That is, small clusters do not interfere with recovery of the big clusters.\nThe main implication of this result is that one can apply an iterative \u201cpeeling\u201d strategy, recovering smaller and smaller clusters. The intuition is simple \u2013 suppose the number of clusters is limited, then either all clusters are large, or the sizes of the clusters vary significantly. The first case is obviously easy. The second one is equally easy: use the aforementioned convex formulation, the larger clusters can be correctly identified. If we remove all nodes from these larger clusters, the remaining subgraph contains significantly fewer nodes than the original graph, which leads to a much lower threshold on the size of the cluster for correct recovery, making it possible for correctly clustering some smaller clusters. By repeating this procedure, indeed, we can recover the cluster structure for almost all nodes with no lower bound on the minimal cluster size. We summarize our main contributions and techniques:\n(1) We provide a refined analysis of the mixed trace norm and `1 convex relaxation approach for exact recovery of clusters proposed in Jalali et al. [2011] and Chen et al. [2012], focusing on the case where small clusters exist. We show that in the classical planted partition settings Boppana [1987], if each cluster is either large (more precisely, of size at least x \u2248 \u221a n log2 n) or small (of size at most x/ log2 n), then with high probability, this convex relaxation approach correctly identifies all big clusters while \u201cignoring\u201d the small ones. Notice that the multiplicative gap between the two thresholds is logarithmic w.r.t. n. In addition, it is possible to arbitrarily increase x, thus turning a \u201cknob\u201d in quest of an interval (x/ log2 n, x) that is disjoint from the set of cluster sizes. The analysis is done by identifying a certain feasible solution to the convex program and proving its almost sure optimality using a careful construction of a dual certificate. This feasible solution easily identifies the big clusters. This method has been performed before only in the case where all clusters are of size \u2265 x.\n(2) We provide a converse of the result just described. More precisely, we show that if for some value of the knob x an optimal solution appears to look as if the interval (x/ log2 n, x) were indeed free of cluster sizes, then the solution is useful (in the sense that it correctly identifies big clusters) even if this weren\u2019t the case.\n(3) The last two points imply that if some interval of the form (x/ log2 n, x) is free of cluster sizes, then an exhaustive search of this interval will constructively find big clusters (though not necessarily for that particular interval). This gives rise to an iterative algorithm, using a \u201cpeeling strategy\u201d, to recover smaller and smaller clusters that are otherwise impossible to recover. Using the \u201cknob\u201d, we prove that as long as the number of clusters is bounded by \u2126(log n/ log log n), regardless of the cluster sizes, we can correctly recover the cluster structure for an overwhelming fraction of nodes. To the best of our knowledge, this is the first result of provably correct graph clustering without any assumptions on the cluster sizes.\n1Indeed, even in a more lenient setup where one clique (i.e., a perfect cluster) of size K is embedded in an Erdos-Renyi graph of n nodes and 0.5 probability of forming an edge, to recover this clique, the best known polynomial method requires K = \u2126( \u221a n) and it has been a long standing open problem to relax this requirement.\n(4) We extend the result to the partial observation case, where only a faction of similarity labels (i.e., edge/no edge) is known. As expected, smaller observation rates allow identification of larger clusters. Hence, the observation rate serves as the \u201cknob\u201d. This gives rise to an active learning algorithm for graph clustering based on adaptively increasing the rate of sampling in order to hit a \u201cforbidden interval\u201d free of cluster sizes, and concentrating on smaller inputs as we identify big clusters and peel them off.\nBeside these technical contributions, this paper provides novel insights into low-rank matrix recovery and more generally high-dimensional statistics, where data are typically assumed to obey certain low-dimensional structure. Numerous methods have been developed to exploit this a priori information so that a consistent estimator is possible even when the dimensionality of data is larger than the number of samples. Our result shows that one may combine these methods with a \u201cpeeling strategy\u201d to further push the envelope of learning structured data \u2013 By iteratively recovering the easier structure and then reducing the problem size, it is possible to learn structures that are otherwise difficult using previous approaches."}, {"heading": "1.1 Previous work", "text": "The literature of graph clustering is too vast for a detailed survey here; we concentrate on the most related work, and in specific those provide theoretical guarantees on cluster recovery.\nPlanted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al. [2011]. Here, n nodes are partitioned into subsets, referred as the \u201ctrue clusters\u201d, and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probability p or q respectively. The goal is to correctly recover the clusters given the random graph. The planted partition model has been studied as early as 1980\u2019s Boppana [1987]. Earlier work focused on the 2-partition or more generally l-partition case, i.e., the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bolloba\u0301s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al. [2012], Ames and Vavasis [2011], Oymak and Hassibi [2011]. While these work differs in the methodology, they all impose constraints on the size of the minimum true cluster \u2013 the best result up-to-date requires it to be \u2126\u0303( \u221a n).\nCorrelation Clustering This problem, originally defined by Bansal, Blum and Chawla Bansal et al. [2004], also considers graph clustering but in an adversarial noise setting. The goal there is to find the clustering minimizing the total disagreement (intercluster edges plus intracluster nonedges), without there being necessarily a notion of true clustering (and hence no \u201cexact recovery\u201d). This problem is usually studied in the combinatorial optimization framework and is known to be NP-Hard to approximate to within some constant factor. Prominent work includes Demaine et al. [2006], Ailon et al. [2008], Charikar et al. [2005]. A PTAS is known in case the number of clusters is fixed Giotis and Guruswami [2006].\nLow rank matrix decomposition via trace norm: Motivated from robust PCA, it has recently been shown Chandrasekaran et al. [2011], Cande\u0300s et al. [2011], that it is possible to recover a low-rank matrix from sparse errors of arbitrary magnitude, where the key ingredient is using trace norm (aka nuclear norm) as a convex surrogate of the rank. A similar result is also obtained when the low rank matrix is corrupted by other types of noise Xu et al. [2012].\nOf particular relevance to this paper is Jalali et al. [2011] and Chen et al. [2012], where the authors apply this approach to graph clustering, and specifically to the planted partition model. Indeed, Chen et al. [2012] achieve state-of-art performance guarantees for the planted partition problem. However, they don\u2019t overcome the \u2126\u0303( \u221a n) minimal cluster size lower bound.\nActive learning/Active clustering Another line of work that motivates this paper is study of active learning algorithms (a settings in which labeled instances are chosen by the learner, rather than by nature), and in particular active learning for clustering. The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al. [2011], Krishnamurthy et al. [2012]. These setups differ from ours and cannot be easily compared."}, {"heading": "2 Notation and Setup", "text": "Throughout, V denotes a ground set of elements, which we identify with the set [n] = {1, . . . , n}. We assume a true ground truth clustering of V given by a pairwise disjoint covering V1, . . . , Vk, where k is the number of clusters. We say i \u223c j if i, j \u2208 Va for some a \u2208 [k], otherwise i 6\u223c j. We let ni = |Vi| for all i \u2208 [k]. For any i \u2208 [n], \u3008i\u3009 is the unique index satisfying i \u2208 V\u3008i\u3009.\nFor a matrix X \u2208 Rn\u00d7n and a subset S \u2286 [n] of size m, the matrix X[S] \u2208 Rm\u00d7m is the principal minor of X corresponding to the set of indexes S. For a matrix M , \u0393(M) denotes the support of M , namely, the set of index pairs (i, j) such that M(i, j) 6= 0.\nThe ground truth clustering matrix, denoted K\u2217, is defined so that K\u2217(i, j) = 1 is i \u223c j, otherwise 0. This is a block diagonal matrix, each block consisting of 1\u2019s only. Its rank is k. The input is a symmetric matrix A, a noisy version of K\u2217. It is generated using the well known planted clustering model, as follows. There are two fixed edge probabilities, p > q. We think of A as the adjacency matrix of an undirected random graph, where edge (i, j) is in the graph for i > j with probability p if i \u223c j, otherwise with probability q, independent of other choices. The error matrix is denoted by B\u2217 := A\u2212K\u2217. We let \u2126 := \u0393(B\u2217) denote the noise locations.\nNote that our results apply to the more practical case in which the edge probability of (i, j) is pij for each i \u223c j and qij for i 6\u223c j, as long as (min pij) =: p > q := (max qij)."}, {"heading": "3 Results", "text": "We remind the reader that the trace norm of a matrix is the sum of its singular values, and we define the `1 norm of a matrix M to be \u2016M\u20161 = \u2211 ij |M(ij)|. Consider the following convex program, combining the trace norm of a matrix variable K with the `1 norm of another matrix variable B using two parameters c1, c2 that will be determined later:\n(CP1) min \u2016K\u2016\u2217 + c1 \u2225\u2225P\u0393(A)B\u2225\u22251 + c2 \u2225\u2225P\u0393(A)cB\u2225\u22251\ns.t. K +B = A\n0 \u2264 Kij \u2264 1,\u2200(i, j).\nTheorem 1. There exist constants b1, b3, b4 > 0 such that the following holds with probability at least 1\u2212n\u22123.\nFor any parameter \u03ba \u2265 1 and t \u2208 [ 14p+ 3 4q, 3 4p+ 1 4q], define\n`] = b3 \u03ba \u221a p(1\u2212 q)n p\u2212 q log2 n `[ = b4 \u03ba \u221a p(1\u2212 q)n p\u2212 q . (1)\nIf for all i \u2208 [k], either ni \u2265 `] or ni \u2264 `[ and if (K\u0302, B\u0302) is an optimal solution to (CP1), with\nc1 = b1\n\u03ba \u221a n log n \u221a 1\u2212 t t c2 = b1 \u03ba \u221a n log n \u221a t 1\u2212 t , (2)\nthen (K\u0302, B\u0302) = (P]K\u2217, A\u2212 K\u0302), where for a matrix M , P]M is the matrix defined by\n(P]M)(i, j) = { M(i, j) max{n\u3008i\u3009, n\u3008j\u3009} \u2265 `] 0 otherwise .\n(Note that by the theorem\u2019s premise, K\u0302 is the matrix obtained from K\u2217 after zeroing out blocks corresponding to clusters of size at most `[.) The proof is based on Chen et al. [2012] and is deferred to the supplemental material due to lack of space. The main novelty in this work compared to previous work is the treatment of small clusters of size at most `[, whereas in previous work only large clusters were treated, and the existence of small clusters did not allow recovery of the big clusters.\nDefinition 2. An n\u00d7n matrix K is a partial clustering matrix if there exists a collection of pairwise disjoint sets U1, . . . , Ur \u2286 [n] (the induced clusters) such that K(i, j) = 1 if and only if i, j \u2208 Us for some s \u2208 [r], otherwise 0. If K is a partial clustering matrix then \u03c3min(K) is defined as min r i=1 |Ui|.\nThe definition is depicted in Figure 1. Theorem 1 tells us that by choosing \u03ba (and hence c1, c2) properly such that no cluster size falls in the range (`[, `]), the unique optimal solution (K\u0302, B\u0302) to convex program (CP1) is such that K\u0302 is a partial clustering induced by big ground truth clusters.\nIn order for this fact to be useful algorithmically, we also need a type of converse: there exists an event with high probability (in the random process generating the input), such that for all values of \u03ba, if an optimal solution to the corresponding (CP1) looks like the solution (K\u0302, B\u0302) defined in Theorem 1, then the blocks of K\u0302 correspond to actual clusters.\nTheorem 3. There exists constants C1, C2 > 0 such that with probability at least 1 \u2212 n\u22122, the following holds. For all \u03ba \u2265 1 and t \u2208 [ 34q + 1 4p, 1 4q + 3 4p], if (K,B) is an optimal solution to (CP1) with c1, c2 as defined in Theorem 1, and additionally K is a partial clustering induced by U1, . . . , Ur \u2286 V , and also\n\u03c3min(K) \u2265 max\n{ C1k log n\n(p\u2212 q)2 , C2\u03ba \u221a p(1\u2212 q)n log n p\u2212 q } , (3)\nthen U1, . . . , Ur are actual ground truth clusters, namely, there exists an injection \u03c6 : [r] 7\u2192 [k] such that Ui = V\u03c6(i) for all i \u2208 [r].\n(Note: Our proof of Theorem 3 uses Hoeffding tail bounds for simplicity, which are tight for p, q bounded away from 0 and 1. Bernstein tail bounds can be used to strengthen the result for other classes of p, q. We elaborate on this in Section 3.1.)\nThe combination of Theorems 1 and 3 implies that, as long as there exists a relatively small interval which is disjoint from the set of cluster sizes, and such that at least one cluster size is larger than this interval (and large enough), we can recover at least one (large) cluster using (CP1). This is made clear in the following. Corollary 4. Assume we have a guarantee that there exists a number \u03b1 \u2265 b4 \u221a p(1\u2212q)n p\u2212q , such that no cluster size falls in the interval (\u03b1, b3b4\u03b1 log 2 n) and at least one cluster size is of size at least s := max{ b3b4\u03b1 log\n2 n, (C1k log n)/(p\u2212 q)2, C2 \u221a p(1\u2212 q)n log n/(p\u2212 q)}. Then with probability at least 1\u2212 n\u22122, we can recover at least one cluster\nof size at least s efficiently by solving (CP1) with \u03ba = \u03b1/ ( b4 \u221a p(1\u2212q)n p\u2212q ) .\nOf course we do not know what \u03b1 (and hence \u03ba) is. We could exhaustively search for a \u03ba \u2265 1 and hope to recover at least one large cluster. A more interesting question is, when is such a \u03ba guaranteed to exist? Let g = b3b b4 log\n2 n. The number g is the (multiplicative) gap size, equaling the ratio between `] and `[ (for any \u03ba). If the number of clusters k is a priori bounded by some k0, we both ensure that there is at least one cluster of size n/k0, and by the pigeonhole principle, that one of the intervals in the sequence (n/gk0, n/k0), (n/g 2k0, n/gk0), . . . , (n/g k0+1k0, n/g\nk0k0). is disjoint of cluster sizes. If, in addition, the smallest interval in the sequence is not too small and n/k0 is not too small so that Corollary 4 holds, then we are guaranteed to recover at least one cluster using Algorithm 1. We find this condition difficult to work with. An elegant, useful version of the idea is obtained if we assume p, q are some fixed constants.2 As the following lemma shows, it turns our that in this regime, k0 can be assumed to be almost logarithmic in n to ensure recovery of at least one cluster. 3 In what follows, notation such as C(p, q), C3(p, q), . . . denotes universal positive functions that depend on p, q only.\nLemma 5. There exists C3(p, q), C4(p, q), C5 > 0 such that the following holds. Assume that n > C4(p, q), and that we are guaranteed that k \u2264 k0, where k0 = C3(p,q) lognlog logn . Then with probability at least 1 \u2212 n \u22122 Algorithm 1 will recover at least one cluster in at most C5k0 iterations.\nThe proof is deferred to the supplemental material section. Lemma 5 ensures that by trying at most a logarithmic number of values of \u03ba, we can recover at least one large cluster, assuming the number of clusters is roughly logarithmic in n. The next proposition tells us that as long as this step recovers the clusters covering at most all but a vanishing fraction of elements, the step can be repeated.\nProposition 6. A pair of numbers (n\u2032, k\u2032) is called good if n\u2032 \u2264 n, k\u2032 \u2264 k and k\u2032 \u2264 C3(p,q) logn \u2032\nlog logn . If (n \u2032, k\u2032)\nis good, then (n\u2032\u2032, k\u2032\u2032) is good for all n\u2032\u2032, k\u2032\u2032 satisfying n\u2032 \u2265 n\u2032\u2032 \u2265 n\u2032/(log n)1/C3(p,q) and k\u2032 \u2212 1 \u2265 k\u2032\u2032 \u2265 1.\nThe proof is trivial. The proposition implies an inductive process in which at least one big (with respect to the current unrecovered size) cluster can be efficiently removed as long as the previous step recovered at most a (1\u2212 (log n)\u22121/C3(p,q))-fraction of its input. Combining, we proved the following:\nTheorem 7. Assume n, k satisfy the requirements of Lemma 5. Then with probability at least 1 \u2212 2n\u22122 Algorithm 2 recovers clusters covering all but at most a ((log n)\u22121/C3(p\u2212q)) fraction of the input in the full observation case, without any restriction of the minimal cluster size. Moreover, if we assume that k is bounded by a constant k0, then the algorithm will recover clusters covering all but a constant number of input elements.\n2In fact, we need only fix (p\u2212 q), but we wish to keep this exposition simple. 3In comparison, Ailon et al. [2012] require k0 to be constant for their guarantees, as do the Correlation Clustering PTAS\nGiotis and Guruswami [2006]."}, {"heading": "3.1 Partial Observations", "text": "We now consider the case where the input matrix A is not given to us in entirety, but rather that we have oracle access to A(i, j) for (i, j) of our choice. Unobserved values are formally marked with A(i, j) =?.\nConsider a more particular setting in which the edge probabilities defining A are p\u2032 (for i \u223c j) and q\u2032 (for i 6\u223c j), and we observe A(i, j) with probability \u03c1, for each i, j, independently. More precisely: For i \u223c j we have A(i, j) = 1 with probability \u03c1p\u2032, 0 with probability \u03c1(1\u2212 p\u2032) and ? with remaining probability. For i 6\u223c j we have A(i, j) = 1 with probability \u03c1q\u2032, 0 with probability \u03c1(1\u2212 q\u2032) and ? with remaining probability. Clearly, by pretending that the values ? in A are 0, we emulate the full observation case with p = \u03c1p\u2032, q = \u03c1q\u2032.\nOf particular interest is the case in which p\u2032, q\u2032 are held fixed and \u03c1 tends to zero as n grows. In this regime, by varying \u03c1 and fixing \u03ba = 1, Theorem 1 implies the following:\nCorollary 8. There exist constants b1(p \u2032, q\u2032), b3(p \u2032, q\u2032), b4(p \u2032, q\u2032), b5(p \u2032, q\u2032) > 0 such that for any sampling rate parameter \u03c1 the following holds with probability at least 1\u2212 n\u22123. define\n`] = b3(p \u2032, q\u2032) \u221a n \u221a \u03c1 log2 n `[ = b4(p \u2032, q\u2032) \u221a n \u221a \u03c1 .\nIf for all i \u2208 [k], either ni \u2265 `] or ni \u2264 `[ and if (K\u0302, B\u0302) is an optimal solution to (CP1), with\nc1 = b1(p \u2032, q\u2032)\u221a n log n \u221a 1\u2212 b5(p\u2032, q\u2032)\u03c1 b5(p\u2032, q\u2032)\u03c1\nc2 = b1(p \u2032, q\u2032)\u221a n log n\n\u221a b5(p\u2032, q\u2032)\n1\u2212 b5(p\u2032, q\u2032)\u03c1 ,\nthen (K\u0302, B\u0302) = (P]K\u2217, A\u2212 K\u0302), where P] is as defined in Theorem 1.\n(Note: We\u2019ve abused notation by reusing previously defined global constants (e.g. b1) with global functions of p\u2032, q\u2032 (e.g. b1(p\n\u2032, q\u2032)).) Notice now that the observation probability \u03c1 can be used as a knob for controlling the cluster sizes we are trying to recover, instead of \u03ba. We would also like to obtain a version of Theorem 3. In particular, we would like to understand its asymptotics as \u03c1 tends to 0.\nAlgorithm 1 RecoverBigFullObs(V,A, p, q)\nrequire: ground set V , A \u2208 RV\u00d7V , probs p, q n\u2190 |V | t\u2190 14p+ 3 4q (or anything in [ 1 4p+ 3 4q, 3 4p+ 1 4q]) `] \u2190 n, g \u2190 b3b4 log 2 n // (If have prior bound k0 on num clusters, // take `] \u2190 n/k0)\nwhile `] \u2265 max { C1k logn (p\u2212q)2 , C2 \u221a p(1\u2212q)n logn p\u2212q } do\nsolve for \u03ba using (1), set c1, c2 as in (2) (K\u0302, B\u0302)\u2190 optimal solution to (CP1) with c1, c2 if K\u0302 partial clustering matrix with \u03c3min(K\u0302) \u2265 `] then\nreturn induced clusters {U1, . . . , Ur} of K\u0302 end if `] \u2190 `]/g\nend while return \u2205\nAlgorithm 2 RecoverFullObs(V,A, p, q)\nrequire: ground set V , matrix A \u2208 RV\u00d7V , probs p, q {U1, . . . , Ur} \u2190 RecoverBigFullObs(V,A, p, q) V \u2032 \u2190 [n] \\ (U1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ur) if r = 0 then\nreturn \u2205 else\nreturn RecoverFullObs(V \u2032, A[V \u2032], p, q) \u222a {U1, . . . , Ur} end if\nTheorem 9. There exist constants C1(p \u2032, q\u2032), C2(p \u2032, q\u2032) > 0 such that for all observation rate parameters \u03c1 \u2264 1, the following holds with probability at least 1 \u2212 n\u22122. If (K,B) is an optimal solution to (CP1) with c1, c2 as defined in Theorem 8, and additionally K is a partial clustering induced by U1, . . . , Ur \u2286 V , and also\n\u03c3min(K) \u2265 max { C1(p \u2032, q\u2032)k log n\n\u03c1 , C2(p\n\u2032, q\u2032) \u221a n log n\n\u221a \u03c1\n} , (4)\nthen U1, . . . , Ur are actual ground truth clusters, namely, there exists an injection \u03c6 : [r] 7\u2192 [k] such that Ui = V\u03c6(i) for all i \u2208 [r].\nThe proof can be found in the supplemental material. Using the same reasoning as before, we derive the following:\nTheorem 10. Let g = b3(p \u2032, q\u2032)/b4(p \u2032, q\u2032) log2 n (with b3(p \u2032, q\u2032), b4(p \u2032, q\u2032) defined in Corollary 8). There exists a constant C4(p \u2032, q\u2032) such that the following holds. Assume the number of clusters k is bounded by some known number k0 \u2264 C4(p\u2032, q\u2032)(log n)/(log log n). Let \u03c10 = b3(p \u2032,q\u2032)2k20 log 4 n n . Then there exists \u03c1 in the set {\u03c10, \u03c1g, . . . , \u03c1gk0} for which, if A is obtained with observation rate \u03c1 (zeroing ?\u2019s), then with probability at least 1\u2212 n\u22122, any optimal solution (K,B) to (CP1) with c1, c2 from Corollary 8 satisfies (4).\n(Note that the upper bound on k0 ensures that \u03c1g k0 is a probability.) The theorem is proven using a\nsimple pigeonhole principle, noting that one of the intervals (`[(\u03c1), `](\u03c1)) must be disjoint from the set of cluster sizes, and there is at least one cluster of size at least n/k0. The theorem, together with Corollary 8 and Theorem 9 ensures the following. On one end of the spectrum, if k0 is a constant (and n is large enough), then with high probability we can recover at least one large cluster (of size at least n/k0) after querying no more than\nO ( nk20 ( b3(p \u2032, q\u2032)\nb4(p\u2032, q\u2032) log2 n\n)2k0 log4 n ) (5)\nvalues of A(i, j). On the other end of the spectrum, if k0 \u2264 \u03b4(log n)/(log log n) and n is large enough (exponential in 1/\u03b4), then we can recover at least one large cluster after querying no more than n1+O(\u03b4) values of A(i, j). (We omit the details of the last fact from this version.) This is summarized in the following:\nTheorem 11. Assume an upper bound k0 on the number of clusters k. As long as n is larger than some function of k0, p\n\u2032, q\u2032, Algorithm 4 will recover, with probability at least 1 \u2212 n\u22121, at least one cluster of size at least n/k0, regardless of the size of other (small) clusters. Moreover, if k0 is a constant, then clusters covering all but a constant number of elements will be recovered with probability at least 1 \u2212 n\u22121, and the total number of observation queries is (5), hence almost linear.\nNote that unlike previous results for this problem, the recovery guarantee does not impose any lower bound on the size of the smallest cluster. Also note that the underlying algorithm is an active learning one, because more observations fall in smaller clusters which survive deeper in the recursion of Algorithm 4."}, {"heading": "4 Experiments", "text": "We experimented with simplified versions of our algorithms. Here we did not make an effort to compute the various constants defining the algorithms in this work, creating a difficulty in exact implementation. Instead, for Algorithm 1, we increase \u03ba by a multiplicative factor of 1.1 in each iteration until a partial clustering matrix is found. Similarly, in Algorithm 3, \u03c1 is increased by an additive factor of 0.025. Still, it is obvious that our experiments support our theoretical findings. A more practical \u201cuser\u2019s guide\u201d for this method with actual constants is subject to future work.\nIn all experiment reports below, we use a variant of the Augmented Lagrangian Multiplier (ALM) method Lin et al. [2009] to solve the semi-definite program (CP1). Whenever we say that \u201cclusters {Vi1 , Vi2 , . . . } were recovered\u201d, we mean that a corresponding instantiation of (CP1) resulted in an optimal solution (K,B) for which K was a partial clustering matrix induced by {Vi1 , Vi2 , . . . }.\nExperiment 1 (Full Observation) Consider n = 1100 nodes partitioned into 4 clusters V1, . . . , V4, of sizes 800, 200, 80, 20, respectively. The graph is generated according to the planted partition model with p = 0.5, q = 0.2, and we assume the full observation setting. We apply a simplified version of Algorithm 2, which terminates in 4 steps. The recovered clusters at each step are detailed in Table 1.\nExperiment 2 (Partial Observation - Fixed Sample Rate) We have n = 1100 with clusters V1, . . . , V4 of sizes 800, 200, 50, 50. The observed graph is generated with p\u2032 = 0.7, q\u2032 = 0.1, and observation rate\nAlgorithm 3 RecoverBigPartialObs(V, k0) (Assume p \u2032, q\u2032 known, fixed)\nrequire: ground set V , oracle access to A \u2208 RV\u00d7V , upper bound k0 on number of clusters n\u2190 |V | \u03c10 \u2190 b3(p \u2032,q\u2032)2k20 log 4 n\nn\ng \u2190 b3(p\u2032, q\u2032)/b4(p\u2032, q\u2032) log2 n for s \u2208 {0, . . . , k0} do \u03c1\u2190 \u03c10gs obtain matrix A \u2208 {0, 1, ?}V\u00d7V by sampling oracle at rate \u03c1, then zero ? values in A // (can reuse observations from prev. iterations) c1(p \u2032, q\u2032), c2(p \u2032, q\u2032)\u2190 as in Corollary 8\n(K,B)\u2190 an optimal solution to (CP1) if K partial clustering matrix satisfying (4) then\nreturn induced clusters {U1, . . . , Ur} end if\nend for return \u2205\nAlgorithm 4 RecoverPartialObs(V, k0) (Assume p \u2032, q\u2032 known, fixed)\nrequire: ground set V , oracle access to A \u2208 RV\u00d7V , upper bound k0 on number of clusters {U1, . . . , Ur} \u2190 RecoverBigFullObs(V, k0) V \u2032 \u2190 [n] \\ (U1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ur) if r = 0 then\nreturn \u2205 else\nreturn RecoverFullObs(V \u2032, k0 \u2212 r) \u222a {U1, . . . , Ur} end if\n\u03c1 = 0.3. We repeatedly solved (CP1) with c1, c2 as in Corollary 8. At each iteration, at least one large cluster (compared to the input size at that iteration) was recovered exactly and removed. This terminated in exactly 3 iterations. Results are shown in Table 1.\nExperiment 3 (Partial Observation - Incremental Sampling Rate) We tried a simplified version of Algorithm 4. We have n = 1100 with clusters V1, . . . , V4 of sizes 800, 200, 50, 50. The observed graph is generated with p\u2032 = 0.7, q\u2032 = 0.3, and an observation rate \u03c1 which we now specify. We start with \u03c1 = 0 and increase it by 0.025 incrementally until we recover (and then remove) at least one cluster, then repeat. The algorithm terminates in 3 steps. Results are shown in Table 1.\nExperiment 3A We repeat the experiment with a larger instance: n = 4500 with clusters V1, . . . , V6 of sizes 3200, 800, 200, 200, 50, 50, and p\u2032 = 0.8, q\u2032 = 0.2. Results are shown in Table 1. Note that we recover the smallest clusters, whose size is below \u221a n.\nExperiment 4 (Mid-Size Clusters) Our current theoretical results do not say anything about the midsize clusters \u2013 those with sizes between `[ and `]. It is interesting to study the behavior of (CP1) in the presence of mid-size clusters. We generated an instance with n = 750, {|V1|, |V2|, |V3|, |V4|} = {500, 150, 70, 30}, p = 0.8, q = 0.2, and \u03c1 = 0.12. We then solved (CP1) with a fixed \u03ba = 1. The low-rank part K\u0302 of the solution is shown in Fig. 2. The large cluster V1 is completely recovered in K\u0302, while the small clusters V3 and V4 are entirely ignored. The mid-size cluster V2, however, exhibits a pattern we find difficult to characterize. This shows that the polylog gap in our theorems is a real phenomenon and not an artifact of our proof technique. Nevertheless, the large cluster appears clean, and might allow recovery using a simple combinatorial procedure. If this is true in general, it might not be necessary to search for a gap free of cluster sizes. Perhaps for any \u03ba, (CP1) identifies all large clusters above `] after a possible simple mid-size cleanup procedure. Understanding this phenomenon and its algorithmic implications is of much interest."}, {"heading": "5 Discussion", "text": "An immediate future research is to better understand the \u201cmid-size crisis\u201d. Our current results say nothing about clusters that are neither big nor small, falling in the interval (`[, `]). Our numerical experiments confirm that the mid-size phenomenon is real: they are neither completely recovered nor entirely ignored by the optimal K\u0302. The part of K\u0302 restricted to these clusters does not seem to have an obvious pattern.Proving whether we can still efficiently recover large clusters in the presence of mid-size clusters is an interesting open problem.\nOur study was mainly theoretical, focusing on the planted partition model. As such, our experiments focused on confirming the theoretical findings with data generated exactly according to the distribution we could provide provable guarantees for. It would be interesting to apply the presented methodology to real applications, particularly big data sets merged from web application and social networks.\nAnother interesting direction is extending the \u201cpeeling strategy\u201d to other high-dimensional learning problems. This requires understanding when such a strategy may work. One intuitive explanation of the small cluster barrier encountered in previous work is ambiguity \u2013 when viewing the input at the \u201cbig cluster resolution\u201d, a small cluster is both a low-rank matrix and a sparse matrix. Only when \u201czooming in\u201d (after recovering big clusters), small clusters patterns emerge. There are other formulations with similar property. For example, in Xu et al. [2012], the authors propose to decompose a matrix into the sum of a low rank one and a column sparse one to solve an outlier-resistant PCA task. Notice that a column sparse matrix is also a low rank matrix. We hope the \u201cpeeling strategy\u201d may also help with that problem."}, {"heading": "A Notation and Conventions", "text": "We use the following notation and conventions throughout the supplement. For a real n\u00d7 n matrix M , we use the unadorned norm \u2016M\u2016 to denote its spectral norm. The notation \u2016M\u2016F refers to the Frobenius norm, \u2016M\u20161 is \u2211 i,j |M(i, j)| and \u2016M\u2016\u221e is maxij |M(i, j)|.\nWe will also study operators on the space of matrices. To distinguish them from the matrices studied in this work, we will simply call these objects \u201coperators\u201d, and will denote them using a calligraphic font, e.g. P. The norm \u2016P\u2016 of an operator is defined as\n\u2016P\u2016 = sup M :\u2016M\u2016F =1 \u2016PM\u2016F ,\nwhere the supremum is over matrices M .\nFor a fixed, real n\u00d7 n matrix M , we define the matrix linear subspace T (M) as follows:\nT (M) := {YM +MX : X,Y \u2208 Rn\u00d7n} .\nIn words, this subspace is the set of matrices spanned by matrices each row of which is in the row space of M , and matrices each column of which is in the column space of M .\nFor any given subspace of matrices S \u2286 Rn\u00d7n, we let PS denote the orthogonal projection onto S with respect to the the inner product \u3008X,Y \u3009 = \u2211n i,j=1X(i, j)Y (i, j) = trX\ntY . This means that for any matrix M ,\nPSM = argminX\u2208S \u2016M \u2212X\u2016F .\nFor a matrix M , we let \u0393(M) denote the set of matrices supported on a subset of the support of M . Note that for any matrix X,\n(P\u0393(X)M)(i, j) = { M(i, j) X(i, j) 6= 0 0 otherwise .\nIt is a well known fact that PT (X) is given as follows:\nPT (X)M = PC(X)M +MPR(X) \u2212 PC(X)MPR(X) ,\nwhere PC(X) is projection (of a vector) onto the column space of X, and PR(X) is projection onto the row space of X.\nFor a subspace S \u2286 Rn\u00d7n we let S\u22a5 denote the orthogonal subspace with respect to \u3008\u00b7, \u00b7\u3009:\nS\u22a5 = {X \u2208 Rn\u00d7n : \u3008X,Y \u3009 = 0 \u2200Y \u2208 S} .\nSlightly abusing notation, we will use the set complement operator (\u00b7)c to formally define \u0393(M)c to be \u0393(M)\u22a5 (by this we are stressing that the space \u0393(M)\u22a5 is given as \u0393(M \u2032) where M \u2032 is any matrix such that M and M \u2032 have complementary supports). Note that PT (X)\u22a5M = M \u2212 PT (X)M = (I \u2212 PC(X))M(I \u2212 PR(X)) .\nFor a matrix M , sgnM is defined as the matrix satisfying:\n(sgnM)(i, j) =  1 M(i, j) > 0\n\u22121 M(i, j) < 0 0 otherwise ."}, {"heading": "B Proof of Theorem 1", "text": "The proof is based on Chen et al. [2012]. We prove it for \u03ba = 1. The adjustment for \u03ba > 1 is done using a padding argument, presented at the end of the proof.\nAdditional notation:\n1. We let V[ \u2286 V denote the set of of elements i such that n\u3008i\u3009 \u2264 `[. (We remind the reader that n\u3008i\u3009 = |V\u3008i\u3009|.)\n2. We remind the reader that the projection P] is defined as follows:\n(P]M)(i, j) = { M(i, j) max{n\u3008i\u3009, n\u3008j\u3009} \u2265 `] 0 otherwise .\n3. The projection P[ is defined as follows:\n(P[M)(i, j) = { M(i, j) max{n\u3008i\u3009, n\u3008j\u3009} \u2264 `[ 0 otherwise .\nIn words, P[ projects onto the set of matrices supported on V[ \u00d7 V[. Note that by the theorem assumption, P] + P[ = Id (equivalently, P] projects onto the set of matrices supported on (V \u00d7 V ) \\ (V[ \u00d7 V[)).\n4. Define the set D = { \u2206 \u2208 Rn\u00d7n|\u2206ij \u2264 0,\u2200i \u223c j, (i, j) /\u2208 V[ \u00d7 V[; 0 \u2264 \u2206ij ,\u2200i 6\u223c j, (i, j) /\u2208 V[ \u00d7 V[ } ,\nwhich contains all feasible deviation from K\u0302.\n5. For simplicity we write T := T (K\u0302) and \u0393 := \u0393(B\u0302),\u0393c := \u0393(B\u0302)c = \u0393\u22a5.\nWe will make use of the following:\n1. sgn(B\u0302) = B\u0302.\n2. Id = P\u0393 + P\u0393c = P\u0393(A) + P\u0393(A)c .\n3. P],P[,P\u0393,P\u0393c ,P\u0393(A), and P\u0393(A)c commute with each other.\nB.1 Approximate Dual Certificate Condition\nProposition 12. (K\u0302, B\u0302) is the unique optimal solution to (CP) if there exists a matrix Q \u2208 Rn\u00d7n and a positive number satisfying:\n1. \u2016Q\u2016 < 1\n2. \u2016PT (Q)\u2016\u221e \u2264 2 min {c1, c2}\n3. \u2200\u2206 \u2208 D:\n(a) \u2329 UU> +Q,P\u0393(A)P\u0393P]\u2206 \u232a = (1 + )c1 \u2225\u2225P\u0393(A)P\u0393P]\u2206\u2225\u22251 (b) \u2329 UU> +Q,P\u0393(A)cP\u0393P]\u2206 \u232a = (1 + )c2\n\u2225\u2225P\u0393(A)cP\u0393P]\u2206\u2225\u22251 4. \u2200\u2206 \u2208 D:\n(a) \u2329 UU> +Q,P\u0393(A)P\u0393cP]\u2206 \u232a \u2265 \u2212(1\u2212 )c1 \u2225\u2225P\u0393(A)P\u0393cP]\u2206\u2225\u22251 (b) \u2329 UU> +Q,P\u0393(A)cP\u0393cP]\u2206 \u232a \u2265 \u2212(1\u2212 )c2\n\u2225\u2225P\u0393(A)cP\u0393cP]\u2206\u2225\u22251 5. P\u0393P[(UU> +Q) = c1P[B\u0302\n6. \u2225\u2225P\u0393cP[(UU> +Q)\u2225\u2225\u221e \u2264 c2\nProof. Consider any feasible solution to (CP1) (K\u0302 + \u2206, B\u0302 \u2212 \u2206); we know \u2206 \u2208 D due to the inequality constraints in (CP1). We will show that this solution will have strictly higher objective value than \u2329 K\u0302, B\u0302 \u232a if \u2206 6= 0.\nFor this \u2206, let G\u2206 be a matrix in T \u22a5 \u2229 Range(P[) satisfying \u2016G\u2016 = 1 and \u3008G\u2206,\u2206\u3009 = \u2016PT\u22a5P[\u2206\u2016\u2217 ;\nsuch a matrix always exists because RangeP[ \u2286 T\u22a5. Suppose \u2016Q\u2016 = b. Clearly, PT\u22a5Q+ (1\u2212 b)G \u2208 T\u22a5 and, due to desideratum 1, we have \u2016PT\u22a5Q+ (1\u2212 b)G\u2206\u2016 \u2264 \u2016Q\u2016 + (1 \u2212 b) \u2016G\u2206\u2016 = b + (1 \u2212 b) = 1. Therefore, UU> + PT\u22a5Q + (1 \u2212 b)G\u2206 is a subgradient of f(K) = \u2016K\u2016\u2217 at K = K\u0302. On the other hand, define the matrix F\u2206 = \u2212P\u0393csgn(\u2206). We have F\u2206 \u2208 \u0393c and \u2016F\u2206\u2016\u221e \u2264 1. Therefore, P\u0393(A)(B\u0302 + F\u2206) is a subgradient of g1(B) =\n\u2225\u2225P\u0393(A)B\u2225\u22251 at B = B\u0302, and P\u0393(A)c(B\u0302 + F\u2206) is a subgradient of g2(B) = \u2225\u2225P\u0393(A)cB\u2225\u22251 at B = B\u0302. Using these three subgradients, the difference in the objective value can be bounded as follows:\nd(\u2206) , \u2225\u2225\u2225K\u0302 + \u2206\u2225\u2225\u2225\n\u2217 + c1 \u2225\u2225\u2225P\u0393(A)(B\u0302 \u2212\u2206)\u2225\u2225\u2225 1 + c2 \u2225\u2225\u2225P\u0393(A)c(B\u0302 \u2212\u2206)\u2225\u2225\u2225 1 \u2212 \u2225\u2225\u2225K\u0302\u2225\u2225\u2225 \u2217 \u2212 c1 \u2225\u2225\u2225P\u0393(A)B\u0302\u2225\u2225\u2225 1 \u2212 c2 \u2225\u2225\u2225P\u0393(A)cB\u0302\u2225\u2225\u2225 1\n\u2265 \u2329 UU> + PT\u22a5Q+ (1\u2212 b)G\u2206,\u2206 \u232a + c1 \u2329 P\u0393(A)(B\u0302 + F\u2206),\u2212\u2206 \u232a + c2 \u2329 P\u0393(A)c(B\u0302 + F\u2206),\u2212\u2206 \u232a = (1\u2212 b) \u2016PT\u22a5P[\u2206\u2016\u2217 + \u2329 UU> + PT\u22a5Q,\u2206 \u232a + c1 \u2329 P\u0393(A)B\u0302,\u2212\u2206 \u232a + c2 \u2329 P\u0393(A)cB\u0302,\u2212\u2206\n\u232a +c1 \u2329 P\u0393(A)F\u2206,\u2212\u2206 \u232a + c2 \u2329 P\u0393(A)cF\u2206,\u2212\u2206\n\u232a = (1\u2212 b) \u2016PT\u22a5P[\u2206\u2016\u2217 + \u2329 UU> + PT\u22a5Q,\u2206 \u232a + c1 \u2329 P[P\u0393(A)B\u0302,\u2212\u2206 \u232a + c2 \u2329 P[P\u0393(A)cB\u0302,\u2212\u2206\n\u232a +c1 \u2329 P]P\u0393(A)B\u0302,\u2212\u2206 \u232a + c2 \u2329 P]P\u0393(A)cB\u0302,\u2212\u2206 \u232a + c1 \u2329 P\u0393(A)F\u2206,\u2212\u2206 \u232a + c2 \u2329 P\u0393(A)cF\u2206,\u2212\u2206 \u232a .\nThe last six terms of the last RHS satisfy:\n1. c1 \u2329 P[P\u0393(A)B\u0302,\u2212\u2206 \u232a + c2 \u2329 P[P\u0393(A)cB\u0302,\u2212\u2206 \u232a = c1 \u2329 P[B\u0302,\u2212\u2206 \u232a , because P[B\u0302 \u2208 \u0393(A).\n2. \u2329 P]P\u0393(A)B\u0302,\u2212\u2206 \u232a \u2265 \u2212 \u2225\u2225P]P\u0393(A)P\u0393\u2206\u2225\u22251 and \u2329P]P\u0393(A)cB\u0302,\u2206\u232a \u2265 \u2212\u2225\u2225P]P\u0393(A)cP\u0393\u2206\u2225\u22251, because B\u0302 \u2208 \u0393 and \u2225\u2225\u2225B\u0302\u2225\u2225\u2225 \u221e \u2264 1.\n3. \u2329 P\u0393(A)F\u2206,\u2212\u2206 \u232a = \u2225\u2225P\u0393(A)P\u0393c\u2206\u2225\u22251 and \u2329P\u0393(A)cF\u2206,\u2212\u2206\u232a = \u2225\u2225P\u0393(A)cP\u0393c\u2206\u2225\u22251, due to the definition of F .\nIt follows that d(\u2206) \u2265 (1\u2212 b) \u2016PT\u22a5P[\u2206\u2016\u2217 + \u2329 UU> + PT\u22a5Q,\u2206 \u232a + c1 \u2329 P[B\u0302,\u2212\u2206 \u232a \u2212 c1 \u2225\u2225P]P\u0393(A)P\u0393\u2206\u2225\u22251 \u2212c2\n\u2225\u2225P]P\u0393(A)cP\u0393\u2206\u2225\u22251 + c1 \u2225\u2225P\u0393(A)P\u0393c\u2206\u2225\u22251 + c2 \u2225\u2225P\u0393(A)cP\u0393c\u2206\u2225\u22251 . (6) Consider the second term in the last RHS, which equals \u2329 UU> + PT\u22a5Q,\u2206 \u232a = \u2329 UU> +Q,P]\u2206 \u232a + \u2329 UU> +Q,P[\u2206 \u232a \u2212 \u3008PTQ,\u2206\u3009. We bound these three separately.\nFirst term:\u2329 UU> +Q,P]\u2206 \u232a = \u2329 UU> +Q, ( P\u0393(A)P\u0393P] + P\u0393(A)cP\u0393P] + P\u0393(A)P\u0393cP] + P\u0393(A)cP\u0393cP] ) \u2206 \u232a\n\u2265 (1 + )c1 \u2225\u2225P\u0393(A)P\u0393P]\u2206\u2225\u22251 + (1 + )c2 \u2225\u2225P\u0393(A)cP\u0393P]\u2206\u2225\u22251 \u2212 (1\u2212 )c1 \u2225\u2225P\u0393(A)P\u0393cP]\u2206\u2225\u22251\n\u2212(1\u2212 )c2 \u2225\u2225P\u0393(A)cP\u0393cP]\u2206\u2225\u22251 (Using properties 3 and 4)\nSecond term: \u2329 UU> +Q,P[\u2206 \u232a = \u2329 P\u0393P[(UU> +Q),\u2206 \u232a + \u2329 P\u0393cP[(UU> +Q),\u2206\n\u232a \u2265 c1 \u2329 P[B\u0302,\u2206 \u232a \u2212 c2 \u2016P\u0393cP[\u2206\u20161 (using properties 5 and 6)\n= c1 \u2329 P[B\u0302,\u2206 \u232a \u2212 c2 \u2225\u2225P\u0393(A)cP\u0393cP[\u2206\u2225\u22251 (because P\u0393(A)cP\u0393cP[ = P\u0393cP[) Third term: Due to the block diagonal structure of the elements of T , we have PT = P]PT\n\u3008\u2212PTQ,\u2206\u3009 = \u2212\u3008PTQ,P]\u2206\u3009 \u2265 \u2212\u2016PTQ\u2016\u221e \u2016P]\u2206\u20161 \u2265 \u2212\n2 min {c1, c2} \u2016P]\u2206\u20161 .\nCombining the above three bounds with Eq. (6), we obtain\nd(\u2206) \u2265 (1\u2212 b) \u2016PT\u22a5P[\u2206\u2016\u2217 + c1 \u2225\u2225P]P\u0393(A)P\u0393\u2206\u2225\u22251 + c2 \u2225\u2225P]P\u0393(A)cP\u0393\u2206\u2225\u22251 + c1 \u2225\u2225P\u0393(A)P\u0393cP]\u2206\u2225\u22251\n+ c2 \u2225\u2225P\u0393(A)cP\u0393cP]\u2206\u2225\u22251 + c1 \u2225\u2225P\u0393(A)P\u0393cP[\u2206\u2225\u22251 \u2212 2 min {c1, c2} \u2016P]\u2206\u20161\n= (1\u2212 b) \u2016PT\u22a5P[\u2206\u2016\u2217 + c1 \u2225\u2225P]P\u0393(A)\u2206\u2225\u22251 + c2 \u2225\u2225P]P\u0393(A)c\u2206\u2225\u22251 \u2212 2 min {c1, c2} \u2016P]\u2206\u20161\n(note that P\u0393(A)P\u0393cP[\u2206=0)\n\u2265 (1\u2212 b) \u2016P[\u2206\u2016\u2217 + 2 min {c1, c2} \u2016P]\u2206\u20161 ,\nwhich is strictly greater than zero for \u2206 6= 0.\nB.2 Constructing Q.\nWe construct a matrix Q with the properties required by Proposition 12. Suppose we take = 2 log 2 n\n`]\n\u221a n\nt(1\u2212t)\nand use the weights c1 and c2 given in Theorem 1. We specify P]Q and P[Q separately.\nP]Q is given by P]Q = P]Q1 + P]Q2 + P]Q3, where for (i, j) /\u2208 V[ \u00d7 V[,\nP]Q1(i, j) =  \u2212 1nc(i) i \u223c j, (i, j) \u2208 \u0393 1 nc(i) \u00b7 1\u2212pijpij i \u223c j, (i, j) \u2208 \u0393 c\n0 i 6\u223c j\nP]Q2(i, j) =  \u2212(1 + )c2 i \u223c j, (i, j) \u2208 \u0393 (1 + )c2 1\u2212pij pij i \u223c j, (i, j) \u2208 \u0393c\n0 i 6\u223c j\nP]Q3(i, j) =  (1 + )c1 i 6\u223c j, (i, j) \u2208 \u0393 \u2212(1 + )c1 qij1\u2212qij i 6\u223c j, (i, j) \u2208 \u0393 c\n0 i \u223c, j\nNote that these matrices have zero-mean entries.\nP[Q as follows. For (i, j) \u2208 V[ \u00d7 V[,\nP[Q =  c1 i \u223c j, (i, j) \u2208 \u0393(A) \u2212c2 i \u223c j, (i, j) \u2208 \u0393(A)c\nc1 i 6\u223c j, (i, j) \u2208 \u0393(A) c2W (i, j) i 6\u223c j, (i, j) \u2208 \u0393(A)c\n,\nwhere\nW (i, j) = { +1 with probability t\u2212q2t(1\u2212q) \u22121 with remaining probability .\nB.3 Validating Q\nUnder the choice of t in Theorem 1, we have 14p \u2264 t \u2264 p and 1 4 (1\u2212 q) \u2264 1\u2212 t \u2264 1\u2212 q. Also under the second assumption of the theorem and p \u2212 q \u2264 p(1 \u2212 q), we have p(1 \u2212 q) & n log 4 n\n`2] \u2265 log 4 n `] . We will make use of\nthese facts frequently in the proof.\nIt is easy to check that := 2 log 2 n\n`]\n\u221a n\nt(1\u2212t) < 1 2 under the assumption of Theorem 1.\nProperty 1):\nNote that \u2016Q\u2016 \u2264 \u2016P]Q\u223c\u2016 + \u2016P]Q6\u223c\u2016 + \u2016P[Q\u223c\u2016 + \u2016P[Q6\u223c\u2016. We show that all four terms are upperbounded by 14 .\n(a) P[Q\u223c is a block diagonal matrix with each block having size at most `[. Moreover, P[Q\u223c is the sum of a deterministic matrix Q\u223c,d with all non-zero entries equal to\nb1\u221a n logn p\u2212t\u221a t(1\u2212t) and a random matrix\nQ\u223c,r whose entries are i.i.d., bounded almost surely by max{c1, c2} and have zero mean with variance\nb21 n logn \u00b7 p(1\u2212p) t(1\u2212t) . Therefore, we have \u2016Q\u223c\u2016 \u2264 \u2016Q\u223c,r\u2016+ \u2016Q\u223c,d\u2016, where w.h.p.\n\u2016P[Q\u223c,d\u2016 \u2264 `[ b1\u221a n log n p\u2212 t\u221a t(1\u2212 t) ,\n\u2016P[Q\u223c,r\u2016 \u2264 6 max {\u221a `[ log n\nb1\u221a n log n \u221a p(1\u2212 p) t(1\u2212 t) + max{c1, c2} log2 n } ;\nhere in the second inequality we use Lemma 16. We conclude that \u2016P[Q\u223c\u2016 is bounded by 14 as long as\n`[ \u2264 \u221a t(1\u2212t)n logn 8b1(p\u2212t) and max{c1, c2} \u2264 1 48 log2 n , which holds under the assumption of Theorem 1.\n(b) P[Q6\u223c is a random matrix supported on V[\u00d7V[, whose entries are i.i.d., zero mean, bounded almost surely by max{c1, c2}, and have variance b 2 1 n logn \u00b7 t2+q\u22122tq (1\u2212t)t . It follows from Lemma 16 that\n\u2016P[Q6\u223c\u2016 \u2264 6 max { \u221a n[ \u00b7\nb1\u221a n log n\n\u221a t2 + q \u2212 2tq\n(1\u2212 t)t ,max{c1, c2} log2 n\n} \u2264 1\n4\nbecause n[ \u2264 n and max{c1, c2} \u2264 148 log2 n , which holds under the assumption of the theorem.\n(c) Note that P]Q\u223c = P]Q1 + P]Q2. By construction these two matrices are both block-diagonal, have i.i.d zero-mean entries which are bounded almost surely by B\u223c := max { 1 `]p , 2c2p } and have variance\nbounded by \u03c32\u223c := max {\n1\u2212p p`2] , 4(1\u2212p)p c 2 2\n} . Lemma 16 gives \u2016P]Q\u223c\u2016 \u2264 6 max {\u221a `] \u00b7 \u03c3\u223c, B\u223c log2 n } \u2264 14 under\nthe assumption of Theorem 1.\n(d) Note that P]Q6\u223c = P]Q3 is a random matrix with i.i.d. zero-mean entries which are bounded almost surely by B 6\u223c := 2c1 1\u2212q and have variance bounded by \u03c3 2 6\u223c := 4q 1\u2212q c 2 1. Lemma 16 gives \u2016P]Q6\u223c\u2016 \u2264\n6 max {\u221a n \u00b7 \u03c36\u223c, B6\u223c log2 n } \u2264 14 .\nProperty 2):\nDue to the structure of T , we have \u2016PTQ\u2016\u221e = \u2016PTP]Q\u2016\u221e = \u2225\u2225UU>(P]Q) + (P]Q)UU> + UU>(P]Q)UU>\u2225\u2225\u221e\n\u2264 3 \u2225\u2225UU>P]Q\u2225\u2225\u221e \u2264 3 3\u2211\nm=1 \u2225\u2225UU>P]Qm\u2225\u2225\u221e . Now observe that (UU>P]Qm)(i, j) = \u2211 l\u2208Vc(i) 1 nc(i) P]Qm(l, j) is the sum of i.i.d. zero-mean random variables with bounded magnitude and variance. Using Lemma 18, we obtain that for i \u2208 V],\n\u2223\u2223(UU>P]Q1)(i, j)\u2223\u2223 . 1 nc(i) (\u221a 1\u2212 p p`2] \u00b7 \u221a nc(i) log n+ log n `]p )\n\u2264 1 `]\n\u221a log n\np`] \u2264 log n 242`]\n\u221a t\np .\nwhere in the last inequality we use t \u2265 p4 & logn `] . For i \u2208 V[, clearly (UU>P]Q1)(i, j) = 0. By union bound we conclude that \u2225\u2225UU>P]Q1\u2225\u2225\u221e \u2264 logn242`]\u221a tp . Similarly, we can bound \u2225\u2225UU>P]Q2\u2225\u2225\u221e and \u2225\u2225UU>P]Q3\u2225\u2225\u221e with the same quantity (cf. Chen et al. [2012]).\nOn the other hand, under the definition of c1, c2 and , we have\nc1 = b1 \u221a 1\u2212 t tn log n \u00b7 2 log 2 n\n`]\n\u221a n\nt(1\u2212 t) = b1\n\u221a p log n\nt \u221a t \u00b7 log n 24`]\n\u221a t\np \u2265 log n\n24`]\n\u221a t\np and similarly c2 \u2265 logn24`] \u221a t p . It follows that \u2016PTQ\u2016\u221e \u2264 9 \u00b7 1 24 min {c1, c2}, proving property 2).\nProperties 3a) and 3b)\nFor 3a), by construction of Q we have\u2329 UU> +Q,P\u0393(A)P\u0393P]\u2206 \u232a = \u2329 P\u0393(A)P\u0393P]Q3,P\u0393(A)P\u0393P]\u2206 \u232a = (1 + )c1\n\u2211 (i,j)\u2208\u0393\u2229\u0393(A) P]\u2206(i, j)\n= (1 + )c1 \u2225\u2225P\u0393(A)P\u0393P]\u2206\u2225\u22251 (because \u2206 \u2208 D)\nProperty 3b) can be verified similarly.\nProperties 4a) and 4b):\nFor 4a), we have\u2329 UU> +Q,P\u0393(A)P\u0393cP]\u2206 \u232a = \u2329 P\u0393(A)P\u0393cP] ( UU> + P]Q1 + P]Q2 ) ,P\u0393(A)P\u0393cP]\u2206 \u232a =\n\u2211 (i,j)\u2208\u0393c\u2229\u0393(A) ( 1 nc(i) + 1 nc(i) 1\u2212 pij pij + (1 + )c2 1\u2212 pij pij ) P]\u2206(i, j)\n\u2265 \u2212 ( 1\np`] + (1 + )c2 1\u2212 p p )\u2225\u2225P\u0393(A)P\u0393cP]\u2206\u2225\u22251 , (here we use \u2206 \u2208 D, pij \u2265 p, and nc(i) \u2265 `]for i \u2208 V]).\nConsider the two terms in the parenthesis in the last RHS. For the first term, we have\n1\np`] =\n2 log2 n\n`]\n\u221a n\nt(1\u2212 t) \u00b7\n\u221a t(1\u2212 t)\n4p2n log4 n \u2264 2 log\n2 n\n`]\n\u221a n t(1\u2212 t) \u00b7 b1 \u221a 1\u2212 t tn log n = c1.\nFor the second term, we have the following\np\u2212 t \u2265 p\u2212 q 4 \u2265 b3 4 log2 n \u221a p(1\u2212 q)n `]\n= b3 4 \u00b7 \u221a t(1\u2212 q)\u221a p(1\u2212 t) \u00b7 p(1\u2212 t) \u00b7 2 log 2 n \u221a n `] \u221a t(1\u2212 t) \u2265 8 \u00b7 p(1\u2212 t) \u00b7 2 log 2 n \u221a n\n`] \u221a t(1\u2212 t) = 8p(1\u2212 t) ,\nwhich implies (1 + )c2 1\u2212p p \u2264 (1\u2212 2 )c1. We conclude that\u2329 UU> +Q,P\u0393(A)P\u0393cP]\u2206 \u232a \u2265 \u2212 ( c1 + (1\u2212 2 )c1) \u2225\u2225P\u0393(A)P\u0393cP]\u2206\u2225\u22251 , proving property 4a).\nFor 4b), we have\u2329 UU> +Q,P\u0393(A)cP\u0393cP]\u2206 \u232a = \u2329 P\u0393(A)cP\u0393cP]Q3,P\u0393(A)cP\u0393cP]\u2206 \u232a =\n\u2211 (i,j)\u2208\u0393(A)c\u2229\u0393c\u2229RangeP] \u2212(1 + ) c1qij 1\u2212 qij P]\u2206(i, j)\n\u2265 \u2212(1 + ) c1q 1\u2212 q \u2225\u2225P\u0393(A)cP\u0393cP]\u2206\u2225\u22251 . (here we use qij \u2264 q)\nConsider the factor before the norm in the last RHS. Similarly as before, we have\nt\u2212 q \u2265 p\u2212 q 4 \u2265 b3 4 log2 n \u221a p(1\u2212 q)n `]\n\u2265 2 \u00b7 t(1\u2212 q) \u00b7 2 log 2 n \u221a n `] \u221a t(1\u2212 t) = 2t(1\u2212 q) .\nThis implies (1 + )c1 q 1\u2212q \u2264 (1\u2212 )c2. We conclude that\u2329 UU> +Q,P\u0393(A)cP\u0393cP]\u2206 \u232a \u2265 \u2212(1\u2212 )c2 \u2225\u2225P\u0393(A)cP\u0393cP]\u2206\u2225\u22251 , proving property 4b).\nProperties 5) and 6): It is obvious that these two properties hold by construction of Q.\nNote that properties 3)-6) hold deterministically.\nB.4 The \u03ba > 1 case\nLet n\u2032 = \u03ba2n and assume n\u2032 is an integer. Let A\u2032 \u2208 Rn\u2032\u00d7n\u2032 be such a matrix that\nA\u2032 = [ A 0 0 I ] .\nConsider the following padded program\n(CP1\u2019) min K\u2032,B\u2032\u2208Rn\u2032\u00d7n\u2032\n\u2016K \u2032\u2016\u2217 + c1 \u2225\u2225P\u0393(A\u2032)B\u2032\u2225\u22251 + c2 \u2225\u2225P\u0393(A\u2032)cB\u2032\u2225\u22251\ns.t. K \u2032 +B\u2032 = A\u2032\n0 \u2264 K \u2032ij \u2264 1,\u2200(i, j).\nApplying Theorem 1 with \u03ba = 1 (which we have proved) to A\u2032 and the padded program (CP1\u2019), we conclude that the unique optimal solution (K\u0302 \u2032, B\u0302\u2032 = A\u2032 \u2212 K\u0302 \u2032) to (CP1\u2019) has the form\nK\u0302 \u2032 =\n[ P]K\u2217 0\n0 0\n] .\nWe claim that K\u0302 = P]K\u2217 is the unique optimal solution to (CP1).\nProof by contradiction: suppose an optimal solution to (CP1) is K\u0302 = K0 6= P]K\u2217. By optimality we have\n\u2016K0\u2016\u2217+c1 \u2225\u2225P\u0393(A)(A\u2212K0)\u2225\u22251+c2 \u2225\u2225P\u0393(A)c(A\u2212K0)\u2225\u22251 \u2264 \u2016P]K\u2217\u2016\u2217+c1 \u2225\u2225P\u0393(A)(A\u2212 P]K\u2217)\u2225\u22251+c2 \u2225\u2225P\u0393(A)c(A\u2212 P]K\u2217\u2225\u22251 .\nDefine K \u20320 = [ K0 0 0 0 ] \u2208 Rn\u2032\u00d7n\u2032 . It follows that\n\u2016K \u20320\u2016\u2217 + c1 \u2225\u2225P\u0393(A\u2032)(A\u2032 \u2212K \u20320)\u2225\u22251 + c2 \u2225\u2225P\u0393(A\u2032)c(A\u2032 \u2212K \u20320)\u2225\u22251\n= \u2016K0\u2016\u2217 + c1 \u2225\u2225P\u0393(A)(A\u2212K0)\u2225\u22251 + c1(n\u2032 \u2212 n) + c2 \u2225\u2225P\u0393(A)c(A\u2212K0)\u2225\u22251\n\u2264 \u2016P]K\u2217\u2016\u2217 + c1 \u2225\u2225P\u0393(A)(A\u2212 P]K\u2217)\u2225\u22251 + c1(n\u2032 \u2212 n) + c2 \u2225\u2225P\u0393(A)c(A\u2212 P]K\u2217)\u2225\u22251\n= \u2225\u2225\u2225K\u0302 \u2032\u2225\u2225\u2225\n\u2217 + c1 \u2225\u2225\u2225P\u0393(A\u2032)(A\u2032 \u2212 K\u0302 \u2032)\u2225\u2225\u2225 1 + c2 \u2225\u2225\u2225P\u0393(A\u2032)c(A\u2032 \u2212 K\u0302 \u2032)\u2225\u2225\u2225 1 ,\ncontradicting the fact that (K\u0302 \u2032, B\u0302\u2032 = A\u2032 \u2212 K\u0302 \u2032) is the unique optimal to (CP1\u2019)."}, {"heading": "C Proof of Theorem 3", "text": "Fix \u03ba \u2265 1 and t in the allowed range, let (K,B) be an optimal solution to (CP1) , and assume K is a partial clustering induced by U1, . . . , Ur for some integer r, and also assume \u03c3min(K) = mini\u2208[r] |Ui| satisfies (3). Let M = \u03c3min(K).\nWe need a few helpful facts. First, note that any value of t in the allowed range [ 14p + 3 4q, 3 4p + 1 4q]\nsatisfies q + 14 (p\u2212 q) \u2264 t \u2264 p\u2212 1 4 (p\u2212 q). Also note that from the definition of t, c1, c2,\nq + 1 4 (p\u2212 q) \u2264 c2 c1 + c2 = t \u2264 p\u2212 1 4 (p\u2212 q) . (7)\nWe say that a pair of sets Y \u2286 V,Z \u2286 V is cluster separated if there is no pair (y, z) \u2208 Y \u00d7Z satisfying y \u223c z.\nAssumption 13. There exists a constant C \u2032 > 0 such that for all pairs of cluster-separated sets Y,Z of size at least m := C \u2032 logn\n(p\u2212q)2 each,\n|d\u0302Y,Z \u2212 q| < 1\n4 (p\u2212 q) , (8)\nwhere d\u0302Y,Z := |(Y\u00d7Z)\u2229\u2126| |Y |\u00b7|Z| .\nThis is proven by a Hoeffding tail bound and a union bound to hold with probability at least 1\u2212 n\u22124. To see why, fix the sizes mY ,mZ of |Y |, |Z|, assume mY \u2264 mZ w.l.o.g. For each such choice, there are at most exp{C(mY + mZ) log n} \u2264 exp{2CmZ log n} possibilities for the choice of sets Y,Z, for some C > 0. For each such choice, the probability that (8) does not hold is\nexp{\u2212C \u2032\u2032mYmZ(p\u2212 q)2} (9)\nusing Hoeffding inequality, for some C \u2032\u2032 > 0. Hence, as long as mY \u2265 m as defined above, for properly chosen C \u2032, using union bound (over all possibilities of mY ,mZ and of Y,Z) we obtain (8) uniformly.\nIf we assume also , say, that M \u2265 3m , (10)\n(which can be done by setting C1 \u2265 3C \u2032) the implication of the assumption is that it cannot be the case that some Ui contains a subset U \u2032 i of size in the range [m, |Ui| \u2212m] such that U \u2032i = Vg \u2229Ui for some g. Indeed, if such a set existed, then we would find a strictly better solution to (CP1), call it (K \u2032, B\u2032), which is defined so that K \u2032 is obtained from K by splitting the block corresponding to Ui into two blocks, one corresponding to U \u2032i and the other to Ui \\U \u2032i . The difference \u2206 between the cost of (K,B) and (K \u2032, B\u2032) is (renaming Y := U \u2032i and Z := U \\U \u2032i) \u2206 = c1|(Y \u00d7Z) \u2229\u2126| \u2212 c2|(Y \u00d7Z) \u2229\u2126c| = (c1 + c2)d\u0302Y,Z |Y | |Z| \u2212 c2|Y | |Z|. But the sign of \u2206 is exactly the sign of d\u0302Y,Z \u2212 c2c1+c2 which is strictly negative by (8) and (7). (We also used the fact that the trace norm part of the utility function is equal for both solutions: \u2016K \u2032\u2016\u2217 = \u2016K\u2016\u2217).\nThe conclusion is that for each i, the sets (Ui \u2229 V1), . . . , (Ui \u2229 Vk) must all be of size at most m, except maybe for at most one set of size at least |Ui| \u2212m. If we now also assume that\nM > km = (kC \u2032 log n)/(p\u2212 q)2 , (11)\nthen we conclude that not all these sets can be of size at most m. Hence exactly one of these sets must have size at least |Ui| \u2212m. From this we conclude that there is a function \u03c6 : [r] 7\u2192 [k] such that for all i \u2208 [r],\n|Ui \u2229 V\u03c6(i)| \u2265 |Ui| \u2212m .\nWe now claim that this function is an injection. We will need the following assumption:\nAssumption 14. For any 4 pairwise disjoint subsets (Y, Y \u2032, Z, Z \u2032) such that (Y \u222a Y \u2032) \u2286 Vi for some i, (Z \u222a Z \u2032) \u2286 [n] \\ Vi, max{|Z|, |Z \u2032|} \u2264 m, min{|Y |, |Y \u2032|} \u2265M \u2212m:\n|Y | \u00b7 |Y \u2032| d\u0302Y,Y \u2032 \u2212 |Y | \u00b7 |Z| d\u0302Y,Z \u2212 |Y \u2032| \u00b7 |Z \u2032| d\u0302Y \u2032,Z\u2032 > c2\nc1 + c2 (|Y | \u00b7 |Y \u2032| \u2212 |Y | \u00b7 |Z| \u2212 |Y \u2032| \u00b7 |Z \u2032|) (12)\nThe assumption holds with probability at least 1\u2212 n\u22124 by using Hoeffding inequality, union bounding over all possible sets Y, Y \u2032, Z, Z \u2032 as above. Indeed, notice that for fixed mY ,mY \u2032 ,mZ ,mZ\u2032 (with, say, mY \u2265 mY \u2032), and for each tuple Y, Y \u2032, Z, Z \u2032 such that |Y | = mY , |Y \u2032| = mY \u2032 , |Z| = mZ , |Z \u2032| = mZ\u2032 , the probability that (12) is violated is at most\nexp{\u2212C(p\u2212 q)2(mYmY \u2032 +mYmZ +mY \u2032mZ\u2032)} (13)\nfor some C > 0. Using (10), this is at most\nexp{\u2212C \u2032\u2032(p\u2212 q)2(mYmY \u2032)} , (14)\nfor some global C \u2032\u2032 > 0. Now notice that the number of possibilities to choose such a 4 tuple of sets is bounded above by exp{C \u2032\u2032\u2032mY log n}, for some global C \u2032\u2032\u2032 > 0. Assuming\nM \u2265 C\u0302 log n (p\u2212 q)2\n(15)\nfor some C\u0302, and applying a union bound over all possible combinations Y, Y \u2032, Z, Z \u2032 of sizes mY ,mY \u2032 ,mZ ,mZ\u2032 respectively, of which there are at most exp{C\u25e6mY log n} for some C\u25e6 > 0, we conclude that (12) is violated for some combination with probability at most\nexp{\u2212C \u2032\u2032(p\u2212 q)2mYmY \u2032/2} (16)\nwhich is at most exp{\u221220 log n} if\nM \u2265 C\u0302 \u2032 log n\n(p\u2212 q)2 . (17)\nfor some C\u0302 \u2032 > 0. Apply a union bound now over the possible combinations of the tuple (mY ,mY \u2032 ,mZ ,mZ\u2032) , of which there are at most exp{4 log n} to conclude that (12) holds uniformly for all possibilities of Y, Y \u2032, Z, Z \u2032 with probability at least 1\u2212 n\u22124.\nNow assume by contradiction that \u03c6 is not an injection, so \u03c6(i) = \u03c6(i\u2032) =: j for some distinct i, i\u2032 \u2208 [r]. Set Y = Ui\u2229Vj , Y \u2032 = Ui\u2032 \u2229Vj , Z = Ui \\Y, Z \u2032 = Ui\u2032 \\Y \u2032. Note that max{|Z|, |Z \u2032|} \u2264 m and min{|Y |, |Y \u2032|} \u2265 M\u2212m. Consider the solution (K \u2032, B\u2032) whereK \u2032 is obtained fromK by replacing the two blocks corresponding to Ui, Ui\u2032 with four blocks: Y, Y\n\u2032, Z, Z \u2032. Inequality (12) guarantees that the cost of (K \u2032, B\u2032) is strictly lower than that of (K,B), contradicting optimality of the latter. (Note that \u2016K\u2016\u2217 = \u2016K \u2032\u2016\u2217.)\nWe can now also conclude that r \u2264 k. Fix i \u2208 [r]. We show that not too many elements of V\u03c6(i) can be contained in V \\ {U1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ur}. We need the following assumption.\nAssumption 15. For all pairwise disjoint sets Y,X,Z \u2286 V such that |Y | \u2265M \u2212m, |X| \u2265 m, (Y \u222aX) \u2286 Vj for some j \u2208 [k], |Z| \u2264 m, Z \u2229 Vj = \u2205:\n|X| \u00b7 |Y |d\u0302X,Y + ( |X| 2 ) d\u0302x,x \u2212 |Y | \u00b7 |Z|d\u0302Y,Z >\nc2 c1 + c2 (|X| \u00b7 |Y |+ ( |X| 2 ) \u2212 |Y | \u00b7 |Z|) + |X| c1 + c2 . (18)\nThe assumption holds with probability at least 1 \u2212 n\u22124. To see why, first notice that |X|/(c1 + c2) \u2264 1 8 (p\u2212 q)|X| \u00b7 |Y | by (3), as long as C2 is large enough. This implies that the RHS of (18) is upper bounded by (\np\u2212 1 8\n(p\u2212 q) ) |X| \u00b7 |Y |+ c2\nc1 + c ( ( |X| 2 ) \u2212 |Y | \u00b7 |Z|) (19)\nProving that the LHS of (18) (denoted f(X,Y, Z)) is larger than (19) (denoted g(X,Y, Z)) uniformly w.h.p. can now be easily done as follows. By fixing mY = |Y |,mX = |X|, the number of combinations for Y,X,Z is at most exp{C(mY + mX) log n} for some global C > 0. On the other hand, the probability that f(X,Y, Z) \u2264 g(X,Y, Z) for any such option is at most\nexp{\u2212C \u2032\u2032(p\u2212 q)2mYmX} (20)\nfor some C \u2032 > 0. Hence, by union bounding, the probability that some tuple Y,X,Z of sizes mY ,mX ,mZ respectively satisfies f(X,Y, Z) \u2264 g(X,Y, Z) is at most\nexp{\u2212C \u2032\u2032(p\u2212 q)2mY /2} , (21)\nwhich is at most exp{\u221210 log n} assuming\nM \u2265 C\u0304(log n)/(p\u2212 q)2 , (22)\nfor some C\u0304 > 0. Another union bound over the possible choices of mY ,mX ,mZ proves that (18) holds uniformly with probability at least 1\u2212 n\u22124.\nNow for some i \u2208 [r] set X := V\u03c6(i) \u2229 (V \\ {U1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ur}) and assume by contradiction that |X| > m. Set Y := V\u03c6(i)\u2229Ui and Z = Ui \\V\u03c6(i). Define the solution (K \u2032, B\u2032) where K \u2032 is obtained from K by replacing the block corresponding to Ui in K with two blocks: V\u03c6(i) and Ui \\ V\u03c6(i). Assumption 15 tells us that the cost of (K \u2032, B\u2032) is strictly lower than that of (K,B). Note that the expression |X|c1+c2 in the RHS of (18) accounts for the trace norm difference \u2016K \u2032\u2016\u2217 \u2212 \u2016K\u2016\u2217 = |X|.\nWe are prepared to perform the final \u201ccleanup\u201d step. At this point we know that for each i \u2208 [r], the set Ti = Ui \u2229 V\u03c6(i) satisfies\n|Ti| \u2265 |Ui| \u2212m |Ti| \u2265 |Vj | \u2212 rm .\n(The second inequality is implied by the fact that at most m elements of V\u03c6(i) may be contained in Ui\u2032 for i\u2032 6= i, and another at most m elements in V \\ (U1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ur). We are now going to conclude from this that Ui = V\u03c6(i) for all i. To that end, let (K\n\u2032, B\u2032) be the feasible solution to (CP1) defined so that K \u2032 is a partial clustering induced by V\u03c6(1), . . . , V\u03c6(r). We would like to argue that if K 6= K \u2032 then the cost of (K \u2032, B\u2032) is strictly smaller than that of (K,B). Fix the value of the collection\nY := ((r, \u03c6(1), . . . , \u03c6(r),( mij := |V\u03c6(i) \u2229 Uj |) ) i,j\u2208[r],i6=j ,(\nm\u2032i := |V\u03c6(i) \u2229 (V \\ (U1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ur)) ) i\u2208[r])\nLet \u03b2(Y) denote the number of i 6= j such that mij > 0 plus the number of i \u2208 [r] such that mi > 0. We can assume \u03b2(Y) > 0, otherwise Ui = V\u03c6(i) for all i \u2208 [r] as required. The number of possibilities for K and K \u2032 giving rise to Y is exp{C( \u2211 i6=jmij + \u2211 imi) log n} for some C > 0. (Note that K \u2032 depends on r, \u03c6(1), . . . , \u03c6(r) only, while K depends on all elements of Y). For each such possibility, the probability that the cost of (K,B) is lower than that of (K \u2032, B\u2032) is at most\nexp{\u2212C \u2032\u2032(p\u2212 q)2M( \u2211 ij mij + \u2211 i mi)} (23)\nusing Hoeffding inequalities, for some C \u2032\u2032 > 0. (Note that special care needs to be made to account for the difference \u2016K\u2016\u2217 \u2212 \u2016K \u2032\u2016\u2217 = \u2211r i=1mi - this is similar to what we did above .) As long as\nM \u2265 C\u0302\u2020k(log n)/(p\u2212 q)2 (24)\nfor some C\u0302\u2020 > 0, we conclude that the cost of (K \u2032, B\u2032) is at least that of (K,B) for some K giving rise to Y with probability at most exp{\u221210(k log n)\u03b2(Y)}. The number of combinations of Y for a fixed value of \u03b2(Y) is at most exp{5(k+ \u03b2(Y) log n}. By union bounding, we conclude that for fixed \u03b2(Y), the probability that some (K,B) has cost at most that of (K \u2032, B\u2032) is at most exp{\u221210(k log n)\u03b2(Y)}. Finally union bound over all possibilities for \u03b2(Y), of which there are at most n2.\nTaking C1, C2 large enough to satisfy the requirements above concludes the proof."}, {"heading": "D Proof of Theorem 9", "text": "The proof of Theorem 3 in the previous section made repeated use of Hoeffding tail inequalities, for bounding the size of the intersection of the noise support \u2126 with various submatrices. This is tight for p, q which are bounded away from 0 and 1. However, if p = \u03c1p\u2032, q = \u03c1q\u2032, the noise probabilities p\u2032, q\u2032 are fixed and \u03c1 tends to 0, a sharper bound is obtained using Bernstein tail bound (see Appendix F.2,Lemma 17). Using Bernstein inequality instead of Chernoff inequality, the expression (p\u2212q)2 in (9),(11),(13),(14),(15),(16),(17),(20),(21), (22),(23) can be replaced with \u03c1. This clearly gives the required result."}, {"heading": "E Proof of Lemma 5", "text": "Proof. We remind the user that g = b3b4 log 2 n, the multiplicative size of the interval `[, `]. Consider the set of intervals (n/gk0, n/k0), (n/g 2k0, n/gk0), . . . , (n/g k0+1k0, n/g k0k0). By the pigeonhole principle, one of these intervals must not intersect the set of cluster sizes. Assume this interval is (n/gi0+1k0, n/g i0k0), for some 0 \u2264 i0 \u2264 k0. Let \u03b1 = n/gi+1k0. By setting C3(p, q) small enough and C4(p, q) large enough, one easily checks that the requirements of Corollary 4 hold with this value of \u03b1 and s = n/k0. This concludes the proof."}, {"heading": "F Technical Lemmas", "text": "F.1 The spectral norm of random matrices\nIt is well-known that the spectral norm \u03bb1(A) of a zero-mean random matrix A is bounded above w.h.p. by C \u221a n, where C is a constant that might depend on the variance and magnitude of the entries of A. Here we state and (re-)prove an upper bound of \u03bb1(A) with an explicit estimate of the constant C, which is needed in the proof of the main theorem.\nLemma 16. Let Aij, 1 \u2264 i, j \u2264 n be independent random variables, each of which has mean 0 and variance at most \u03c32 and is bounded in absolute value by B. Then with probability at least 1\u2212 2n\u22122\n\u03bb1(A) \u2264 6 max { \u03c3 \u221a n log n,B log2 n } Proof. Let ei be the i-th standard basis in Rn. Let Zij = Aijeie>j . Then Zij \u2019s are zero-mean random matrices independent of each other, and A = \u2211 i,j Zij . We have \u2016Zij\u2016 \u2264 B almost surely. We also have\n\u2016 \u2211 i,j E(ZijZ>ij )\u2016 = \u2016 \u2211 i eie > i \u2211 j E(A2ij)\u2016 \u2264 n\u03c32. Similarly \u2016 \u2211 i,j E(Z>ijZij)\u2016 \u2264 n\u03c32. Applying the Non-\ncommutative Bernstein Inequality (Theorem 1.6 in ?) with t = 6 max { \u03c3 \u221a n log n,B log2 n } yields the desired bound.\nF.2 Standard Bernstein Inequality for Sum of Independent Variables\nLemma 17. ( Bernstein inequality) Let Y1, . . . , YN be independent random variables, each of which has variance bounded by \u03c32 and is bounded in absolute value by B a.s.. Then we have that\nPr [\u2223\u2223\u2223\u2223\u2223 N\u2211 i=1 Yi \u2212 E [ N\u2211 i=1 Yi ]\u2223\u2223\u2223\u2223\u2223 > t ] \u2264 2 exp { t2/2 N\u03c32 +Bt/3 } .\nThe following well known consequence of the theorem will also be of use.\nLemma 18. (?, Proposition 5.16) Let Y1, . . . , YN be independent random variables, each of which has variance bounded by \u03c32 and is bounded in absolute value by B a.s. Then we have\u2223\u2223\u2223\u2223\u2223 N\u2211 i=1 Yi \u2212 E [ N\u2211 i=1 Yi\n]\u2223\u2223\u2223\u2223\u2223 \u2264 C0 max{\u03c3\u221aN log n,B log n} with probability at least 1\u2212C1n\u2212C2 where the positive constants C0, C1, C2 are independent of \u03c3, B, N and n."}], "references": [{"title": "Active learning using smooth relative regret approximations with", "author": ["N. Ailon", "R. Begleiter", "E. Ezra"], "venue": null, "citeRegEx": "Ailon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2008}, {"title": "Correlation clustering", "author": ["N. Bansal", "A. Blum", "S. Chawla"], "venue": "Machine Learning,", "citeRegEx": "Bansal et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2004}, {"title": "Robust principal component analysis", "author": ["E. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Hill-climbing finds random planted bisections", "author": ["T. Carson", "R. Impagliazzo"], "venue": "In SODA,", "citeRegEx": "Carson and Impagliazzo.,? \\Q2001\\E", "shortCiteRegEx": "Carson and Impagliazzo.", "year": 2001}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "S. Parrilo", "A. Willsky"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Clustering with qualitative information", "author": ["Moses Charikar", "Venkatesan Guruswami", "Anthony Wirth"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Charikar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2005}, {"title": "Spectral clustering of graphs with general degrees in the extended planted partition model", "author": ["K. Chaudhuri", "F. Chung", "A. Tsiatas"], "venue": null, "citeRegEx": "Chaudhuri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "Clustering sparse graphs", "author": ["Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "In NIPS. Available on arXiv:1210.3335,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Random Structures and Algorithms,", "citeRegEx": "Condon and Karp.,? \\Q2001\\E", "shortCiteRegEx": "Condon and Karp.", "year": 2001}, {"title": "Correlation clustering in general weighted graphs", "author": ["E. Demaine", "D. Emanuel", "A. Fiat", "N. Immorlica"], "venue": "Theoretical Comp. Sci.,", "citeRegEx": "Demaine et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Demaine et al\\.", "year": 2006}, {"title": "Active clustering: Robust and efficient hierarchical clustering using adaptively selected similarities", "author": ["B. Eriksson", "G. Dasarathy", "A. Singh", "R. Nowak"], "venue": null, "citeRegEx": "Eriksson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 2011}, {"title": "A database interface for clustering in large spatial databases", "author": ["M. Ester", "H. Kriegel", "X. Xu"], "venue": "In KDD,", "citeRegEx": "Ester et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ester et al\\.", "year": 1995}, {"title": "Reconstructing many partitions using spectral techniques", "author": ["J. Giesen", "D. Mitsche"], "venue": "In Fundamentals of Computation Theory, pages 433\u2013444,", "citeRegEx": "Giesen and Mitsche.,? \\Q2005\\E", "shortCiteRegEx": "Giesen and Mitsche.", "year": 2005}, {"title": "Correlation clustering with a fixed number of clusters", "author": ["Ioannis Giotis", "Venkatesan Guruswami"], "venue": "Theory of Computing,", "citeRegEx": "Giotis and Guruswami.,? \\Q2006\\E", "shortCiteRegEx": "Giotis and Guruswami.", "year": 2006}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "In ICML. Available on arXiv:1104.4803,", "citeRegEx": "Jalali et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2011}, {"title": "Efficient active algorithms for hierarchical clustering", "author": ["A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh"], "venue": null, "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG-09-2215,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": "In FOCS, pages 529\u2013537,", "citeRegEx": "McSherry.,? \\Q2001\\E", "shortCiteRegEx": "McSherry.", "year": 2001}, {"title": "Clustering social networks. Algorithms and Models for Web-Graph", "author": ["N. Mishra", "I. Stanton R. Schreiber", "R.E. Tarjan"], "venue": null, "citeRegEx": "Mishra et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mishra et al\\.", "year": 2007}, {"title": "Finding dense clusters via low rank + sparse decomposition", "author": ["S. Oymak", "B. Hassibi"], "venue": null, "citeRegEx": "Oymak and Hassibi.,? \\Q2011\\E", "shortCiteRegEx": "Oymak and Hassibi.", "year": 2011}, {"title": "Spectral clustering and the high-dimensional stochastic block model", "author": ["K. Rohe", "S. Chatterjee", "B. Yu"], "venue": "Ann. of Stat.,", "citeRegEx": "Rohe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohe et al\\.", "year": 2011}, {"title": "Spectral Clustering on a Budget", "author": ["O. Shamir", "N. Tishby"], "venue": "In AISTATS,", "citeRegEx": "Shamir and Tishby.,? \\Q2011\\E", "shortCiteRegEx": "Shamir and Tishby.", "year": 2011}, {"title": "Improved algorithms for the random cluster graph model", "author": ["R. Shamir", "D. Tsur"], "venue": "Random Struct. & Alg.,", "citeRegEx": "Shamir and Tsur.,? \\Q2007\\E", "shortCiteRegEx": "Shamir and Tsur.", "year": 2007}, {"title": "Active clustering of biological sequences", "author": ["K. Voevodski", "M. Balcan", "H. R\u00f6glin", "S. Teng", "Y. Xia"], "venue": "JMLR, 13:203\u2013225,", "citeRegEx": "Voevodski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2012}, {"title": "Proof of Theorem 1 The proof is based on Chen et al. [2012]. We prove it for \u03ba = 1. The adjustment for \u03ba > 1 is done using a padding", "author": [], "venue": null, "citeRegEx": "B,? \\Q2012\\E", "shortCiteRegEx": "B", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "We show that this is not really a restriction: by a more refined analysis of the trace-norm based matrix recovery approach proposed in Jalali et al. [2011] and Chen et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones.", "startOffset": 11, "endOffset": 30}, {"referenceID": 17, "context": "Some prominent examples include community detection in social network Mishra et al. [2007], submarket identification in E-commerce and sponsored search Yahoo!-Inc [2009], and co-authorship analysis in analyzing document database Ester et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 17, "context": "Some prominent examples include community detection in social network Mishra et al. [2007], submarket identification in E-commerce and sponsored search Yahoo!-Inc [2009], and co-authorship analysis in analyzing document database Ester et al.", "startOffset": 70, "endOffset": 170}, {"referenceID": 11, "context": "[2007], submarket identification in E-commerce and sponsored search Yahoo!-Inc [2009], and co-authorship analysis in analyzing document database Ester et al. [1995], among others.", "startOffset": 145, "endOffset": 165}, {"referenceID": 6, "context": ", Chaudhuri et al., 2012, Bollob\u00e1s and Scott, 2004, Chen et al., 2012, McSherry, 2001] is not really a restriction, but rather an artifact of the attempt to solve the problem in a single shot using convex relaxation techniques. Using a more careful analysis, we prove that the mixed trace-norm and `1 based convex formulation, initially proposed in Jalali et al. [2011], can recover clusters of size \u03a9\u0303( \u221a n) even in the presence of smaller clusters.", "startOffset": 2, "endOffset": 370}, {"referenceID": 13, "context": "(1) We provide a refined analysis of the mixed trace norm and `1 convex relaxation approach for exact recovery of clusters proposed in Jalali et al. [2011] and Chen et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], focusing on the case where small clusters exist.", "startOffset": 11, "endOffset": 30}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], focusing on the case where small clusters exist. We show that in the classical planted partition settings Boppana [1987], if each cluster is either large (more precisely, of size at least x \u2248 \u221a n log n) or small (of size at most x/ log n), then with high probability, this convex relaxation approach correctly identifies all big clusters while \u201cignoring\u201d the small ones.", "startOffset": 11, "endOffset": 152}, {"referenceID": 14, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al.", "startOffset": 85, "endOffset": 100}, {"referenceID": 12, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al. [2011]. Here, n nodes are partitioned into subsets, referred as the \u201ctrue clusters\u201d, and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probability p or q respectively.", "startOffset": 142, "endOffset": 161}, {"referenceID": 12, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al. [2011]. Here, n nodes are partitioned into subsets, referred as the \u201ctrue clusters\u201d, and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probability p or q respectively. The goal is to correctly recover the clusters given the random graph. The planted partition model has been studied as early as 1980\u2019s Boppana [1987]. Earlier work focused on the 2-partition or more generally l-partition case, i.", "startOffset": 142, "endOffset": 586}, {"referenceID": 12, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al. [2011]. Here, n nodes are partitioned into subsets, referred as the \u201ctrue clusters\u201d, and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probability p or q respectively. The goal is to correctly recover the clusters given the random graph. The planted partition model has been studied as early as 1980\u2019s Boppana [1987]. Earlier work focused on the 2-partition or more generally l-partition case, i.e., the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004].", "startOffset": 142, "endOffset": 717}, {"referenceID": 5, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004].", "startOffset": 51, "endOffset": 74}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004].", "startOffset": 75, "endOffset": 105}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes.", "startOffset": 75, "endOffset": 132}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al.", "startOffset": 75, "endOffset": 497}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al. [2012], Ames and Vavasis [2011], Oymak and Hassibi [2011].", "startOffset": 75, "endOffset": 517}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al. [2012], Ames and Vavasis [2011], Oymak and Hassibi [2011].", "startOffset": 75, "endOffset": 542}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al. [2012], Ames and Vavasis [2011], Oymak and Hassibi [2011]. While these work differs in the methodology, they all impose constraints on the size of the minimum true cluster \u2013 the best result up-to-date requires it to be \u03a9\u0303( \u221a n).", "startOffset": 75, "endOffset": 568}, {"referenceID": 0, "context": "Correlation Clustering This problem, originally defined by Bansal, Blum and Chawla Bansal et al. [2004], also considers graph clustering but in an adversarial noise setting.", "startOffset": 83, "endOffset": 104}, {"referenceID": 0, "context": "Correlation Clustering This problem, originally defined by Bansal, Blum and Chawla Bansal et al. [2004], also considers graph clustering but in an adversarial noise setting. The goal there is to find the clustering minimizing the total disagreement (intercluster edges plus intracluster nonedges), without there being necessarily a notion of true clustering (and hence no \u201cexact recovery\u201d). This problem is usually studied in the combinatorial optimization framework and is known to be NP-Hard to approximate to within some constant factor. Prominent work includes Demaine et al. [2006], Ailon et al.", "startOffset": 83, "endOffset": 587}, {"referenceID": 0, "context": "[2006], Ailon et al. [2008], Charikar et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "[2006], Ailon et al. [2008], Charikar et al. [2005]. A PTAS is known in case the number of clusters is fixed Giotis and Guruswami [2006].", "startOffset": 8, "endOffset": 52}, {"referenceID": 0, "context": "[2006], Ailon et al. [2008], Charikar et al. [2005]. A PTAS is known in case the number of clusters is fixed Giotis and Guruswami [2006].", "startOffset": 8, "endOffset": 137}, {"referenceID": 3, "context": "Low rank matrix decomposition via trace norm: Motivated from robust PCA, it has recently been shown Chandrasekaran et al. [2011], Cand\u00e8s et al.", "startOffset": 100, "endOffset": 129}, {"referenceID": 2, "context": "[2011], Cand\u00e8s et al. [2011], that it is possible to recover a low-rank matrix from sparse errors of arbitrary magnitude, where the key ingredient is using trace norm (aka nuclear norm) as a convex surrogate of the rank.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "[2011], Cand\u00e8s et al. [2011], that it is possible to recover a low-rank matrix from sparse errors of arbitrary magnitude, where the key ingredient is using trace norm (aka nuclear norm) as a convex surrogate of the rank. A similar result is also obtained when the low rank matrix is corrupted by other types of noise Xu et al. [2012].", "startOffset": 8, "endOffset": 334}, {"referenceID": 13, "context": "Of particular relevance to this paper is Jalali et al. [2011] and Chen et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], where the authors apply this approach to graph clustering, and specifically to the planted partition model.", "startOffset": 11, "endOffset": 30}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], where the authors apply this approach to graph clustering, and specifically to the planted partition model. Indeed, Chen et al. [2012] achieve state-of-art performance guarantees for the planted partition problem.", "startOffset": 11, "endOffset": 166}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering.", "startOffset": 25, "endOffset": 45}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do.", "startOffset": 25, "endOffset": 404}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al.", "startOffset": 25, "endOffset": 832}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al.", "startOffset": 25, "endOffset": 858}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al. [2011], Krishnamurthy et al.", "startOffset": 25, "endOffset": 910}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al. [2011], Krishnamurthy et al. [2012]. These setups differ from ours and cannot be easily compared.", "startOffset": 25, "endOffset": 939}, {"referenceID": 7, "context": ") The proof is based on Chen et al. [2012] and is deferred to the supplemental material due to lack of space.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": "3In comparison, Ailon et al. [2012] require k0 to be constant for their guarantees, as do the Correlation Clustering PTAS Giotis and Guruswami [2006].", "startOffset": 16, "endOffset": 36}, {"referenceID": 0, "context": "3In comparison, Ailon et al. [2012] require k0 to be constant for their guarantees, as do the Correlation Clustering PTAS Giotis and Guruswami [2006].", "startOffset": 16, "endOffset": 150}, {"referenceID": 16, "context": "In all experiment reports below, we use a variant of the Augmented Lagrangian Multiplier (ALM) method Lin et al. [2009] to solve the semi-definite program (CP1).", "startOffset": 102, "endOffset": 120}], "year": 2013, "abstractText": "This paper investigates graph clustering in the planted cluster model in the presence of small clusters. Traditional results dictate that for an algorithm to provably correctly recover the clusters, all clusters must be sufficiently large (in particular, \u03a9\u0303( \u221a n) where n is the number of nodes of the graph). We show that this is not really a restriction: by a more refined analysis of the trace-norm based matrix recovery approach proposed in Jalali et al. [2011] and Chen et al. [2012], we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to recover almost all clusters via a \u201cpeeling strategy\u201d, i.e., recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often as large clusters are learned (and removed). From a high level, this paper sheds novel insights on high-dimensional statistics and learning structured data, by presenting a structured matrix learning problem for which a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations does the job.", "creator": "LaTeX with hyperref package"}}}