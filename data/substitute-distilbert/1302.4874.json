{"id": "1302.4874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "A Labeled Graph Kernel for Relationship Extraction", "abstract": "in first paper, we propose an approach for relationship data ( recall ) based on labeled graph kernels. the kernel we propose is a particularization of a random walk kernel that exploits two properties previously studied in the re metadata : ( i ) the constraints between the candidate entities or connecting terms on a syntactic representation are particularly likely to carry information within the relationship ; and ( ii ) combining information from distinct sources in a kernel may help the re data make better decisions. we performed experiments on a dataset of protein - nucleus interactions and the results show that direct approach enables such values here are comparable with the state - of - the art kernel methods. moreover, our approach is able to outperform the state - of - the - art kernels when combined with other kernel strategies.", "histories": [["v1", "Wed, 20 Feb 2013 11:06:25 GMT  (232kb,D)", "http://arxiv.org/abs/1302.4874v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["gon\\c{c}alo sim\\~oes", "helena galhardas", "david matos"], "accepted": false, "id": "1302.4874"}, "pdf": {"name": "1302.4874.pdf", "metadata": {"source": "CRF", "title": "A Labeled Graph Kernel for Relationship Extraction", "authors": ["Gon\u00e7alo Sim\u00f5es", "David Matos", "Helena Galhardas"], "emails": ["goncalo.simoes@ist.utl.pt", "david.matos@inesc-id.pt", "helena.galhardas@ist.utl.pt"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.2 [Information Storage and Retrieval]: Content Analysis and Indexing - Linguistic Processing\nGeneral Terms Algorithms\nKeywords Information Extraction, Machine Learning, Graph Kernels"}, {"heading": "1. INTRODUCTION", "text": "With the increasing use of Information Technologies, the amount of unstructured text available in digital data sources (e.g., email communications, blogs, reports) has grown at an impressive rate. These texts may contain vital knowledge to Human decision making processes. However, it is unfeasible for a human to analyze big amounts of unstructured information in a short time. In order to solve this problem, a typical approach is to transform unstructured information in digital sources into a previously defined structured format.\nInformation Extraction (IE) is the scientific area that studies techniques to extract semantically relevant segments from unstructured text and represent them in a structured format that can be understood/used by humans or programs (e.g., decision support systems, interfaces for digital libraries). In the past few years, there has been an increasing interest in IE, from industry and scientific communities. In fact, this interest led to huge advances in this area and several solutions were proposed in applications such as Semantic Web [4] and Bioinformatics [14, 2].\nRegardless of the application domain, an IE activity can be modeled as a composition of the following high-level tasks [18]:\n\u2022 Segmentation: divides the text into atomic segments (e.g., words).\n\u2022 Entity recognition: assigns a class (e.g., organization, person) to each segment of the text. Each pair (segment, class) is called an entity.\n\u2022 Relationship extraction: determines relationships (e.g., born in, works for) between entities.\n\u2022 Entity normalization: converts entities into a standard format (e.g., convert all dates to a pre-defined format).\n\u2022 Co-reference resolution: determines which entities represent the same object/individual in the real world (e.g., IBM is the same as \u201cBig Blue\u201d).\nIn the last decade, several techniques to increase the accuracy of these tasks were proposed. In this paper, we focus only on the Relationship Extraction (RE) task. The approaches that are typically used for RE can be divided into two major groups: (i) handcrafted solutions, in which the programs are manually specified by the user through a set of rules; and (ii) Machine Learning solutions, in which the programs are automatically generated by a machine either by explicitly producing rules or by generating a statistical model that is able to produce extraction results with regard to a set of characteristics of the input text.\nMost of the first approaches for RE were based on handcrafted rules [3, 15]. Typically, they exploited common patterns and heuristics to extract the desired relationships from\nar X\niv :1\n30 2.\n48 74\nv1 [\ncs .C\nL ]\n2 0\nFe b\n20 13\nthe results of complex Natural Language Processing chains. These solutions were able to produce good results in several specific domains. However, they need a lot of human effort to produce rules for distinct domains.\nTo overcome this problem of handcrafted solutions, the application of Machine Learning to RE started to receive a lot of attention. Typically, machine learning techniques used for RE are supervised. However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques. Supervised approaches to RE are typically based on classifiers that are responsible for determining whether there is a relationship or not between a set of entities.\nThere are two major lines of works in supervised approaches to RE: (i) feature-based methods, which try to find a good set of features to use in the classification process; and (ii) kernel methods, which try to avoid the explicit computation of features by developing methods that are able to compare structured data (e.g., sequences, graphs, trees). Even though feature-based methods for RE work well [16], there has been an increasing interest in exploiting kernel-based methods, due to the fact that sentences are better described as structures (e.g., sequences of words, parsing trees, dependency graphs).\nIn this paper, we describe a new supervised approach to RE that is based on labeled dependency graph representations of the sentences. The advantage is that a representation of a sentence as a labeled dependency graph contains rich semantic information that, typically, contains useful hints when discriminating whether a set of entities in a sentence are related. The solution we propose uses kernels to deal with these structures. We propose the application of a marginalized kernel to compare labeled graphs [17]. This kernel is based on walks on random graphs and is able to exploit an infinite dimensional feature space by reducing its computation to the problem of solving a system of linear equations. In order to make this graph kernel suitable for RE, we modified the kernel to exploit the following properties that were previously introduced proposals of kernels for RE: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship [7]; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions [14].\nIn order to evaluate the model we propose, we performed some experiments with a biomedical dataset called AImed [8]. This dataset is composed of several abstracts from Biology papers. The documents are annotated with interaction relationships between proteins. The results show that the performance of our approach is comparable to the state-ofthe-art. Morever, when combining our kernel with other kernel methods, we were able to outperfom other state-ofthe-art kernel methods.\nThe rest of the paper is organized as follows. In Section 2, we present the related work. Section 3 defines the problem that we are trying to solve. In Section 4, we describe our method for relationship extraction. In Section 5, we report on the experiments performed. Finally, Section 6 presents the conclusions and some topics for future work."}, {"heading": "2. RELATED WORK", "text": "The most relevant works in the topic of this paper are the ones that propose kernel methods for RE. In the past ten years, several autors proposed kernels for different syntactic and semantic structures of a sentence. One of the first approaches, presented in 2003 by Zelenko et al. [20], is a kernel based on shallow parse tree representation of sentences. This approach had some problems in what concerns the vulnerability to parsing errors. In order to overcome these problems, Culotta and Sorensen [9] proposed a generalization of this kernel that, when combined with a bag-of-words kernel, is able to compensate the parsing errors.\nIn 2005, Bunescu and Mooney [7] proposed a kernel based on the shortest path between entities in a dependency graph. The kernel was based on the hypothesis that the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship. The problem of this kernel is the fact that it is not very flexible when comparing candidates, which leads to very low values of recall when the training data is too small. The same authors proposed a different kernel based on subsequences [8]. The subsequences used in this approach could be combinations of words and other tags (e.g., POS tags, Wordnet Synsets). The results of this kernel are very interesting and even today it is still pointed out as a kernel with a very good performance in RE tasks.\nGiuliano et al. [14] proposed in 2006 a kernel based only on shallow linguistic information of the sentences. The idea was to exploit two simple kernels that, when combined, were able to obtain very interesting results. The global context kernel compares the whole sentence using a bag-of-n-grams approach. The frequencies of the n-grams are computed in three different locations of the sentence: (i) before the first entity; (ii) between the two entities; and (iii) after the second entity. The local context kernel evaluates the similarity between the entities of the sentences as well as the words in a window of limited size around them. The advantage of this kernel is its simplicity since it does not need deep Natural Language Processing tools to preprocess the sentences in order to compute the kernel. However, its major advantage may very well be a big disadvantage since it is not able to exploit rich syntactic/semantic information like a parsing tree or a dependency graph representation of a sentence (which are structures that can be useful for determining whether a set of entities are related).\nIn 2008, Airola et al. [2] presented a kernel that combines two graph representations of a sentence: (i) a labeled dependency graph; and (ii) a linear order representation of the sentence. The kernel considers all possible paths connecting any two vertices in the graph. The results obtained are comparable with the state-of-the-art results. However, this kernel is very demanding in terms of computational resources.\nIn 2010, Tikk et al. [19] performed a study to analyze how a very comprehensive set of kernels for relationship extraction performs when dealing the task of extracting protein-protein interactions. Even though they were not able to determine a clear winner in their comparison, they were still able to outline some very interesting conclusions. First, they notice\nthat kernels based on dependency parsing tend to obtain better results than kernels based on tree parses. Moreover, they show that a simple kernel, like [14], can still obtain results that are at the level of the best kernels based on dependency parsing."}, {"heading": "3. PROBLEM DEFINITION", "text": "In general, the problem of finding an n-ary relationship between entities can be seen as a classification problem for which the input is a set of n entities and the output is the type of relationship between them or an indication that they are not related at all.\nWith this definition, given a text document with all the entities identified, the candidate results are all the sets of n entities that exist in the text. This approach would generate a huge set of candidates among which very few correspond to actually related entities. For this reason, this configuration would potentially lead to some performance issues (due to the huge amount of candidates) and to some problems in terms of accuracy (due to the unbalancement of the data). To avoid these issues, we exploit an heuristic that is typically used in related works, which consists in limiting the candidates to sets of entities that can be found in the same sentence.\nThis way, for one sentence with k entities, the number of candidates generated for a n-ary relationship is given by the number of combinations of the k entities, selected n at a time, i.e. ( k n ) . For instance, consider the sentence in Figure 1, in which we present an example of a sentence from a biomedical text. Suppose that we aim at finding interaction relationships between proteins. This sentence contains three identified proteins: TRADD, RIP and Fas. Moreover, there are two interaction relationships between these entities: TRADD interacts with RIP and RIP interacts with Fas.\nGiven the fact that a protein interaction is a binary relationship, we have a total of ( 3 2 ) = 3 candidates, which are presented in Figure 2.\nNote that it is also possible to use other heuristics to reduce the number of candidates. For instance, in some cases, we may have knowledge about the types of entities that can fulfill a given role in a relationship (e.g. in a relationship between a company and its CEO, it is known that one of the entities must be a a company and the other, a person). Even though these heuristics typically involve some type of prior knowledge about the application domain, they tend to\ndrastically reduce the space of candidates. This fact makes the relationship extraction process a lot easier and helps it produce better results since some of the candidates involving entities that are never related are not used.\nAssuming a set of candidate results, Figure 3 describes how the RE extraction task can be represented as a classification problem. The problem can be divided into two main phases: training and execution. In the training phase, the objective is to automatically generate a statistical model that is able to determine whether a given candidate corresponds to a relationship. In order to produce this model, some training examples must be provided to a learning algorithm (e.g., solving a quadratic optimization problem in the case of a SVM classifier). These examples are generated in the same fashion as the candidates, however, they include an additional label that indicates whether they correspond to a relationship.\nThe execution phase aims at classifying each unlabeled candidate from new untagged documents as containing a relationship or not. This decision is made using the statistical model created in the training phase and a classification algorithm. In the end of the process, the sets of entities in the candidates that are classified as containing a relationship are returned."}, {"heading": "4. METHOD", "text": "In this Section, we present the proposed kernel method. We start by describing the basic idea behind kernel methods for RE in Section 4.1. Then, in Section 4.2, we propose a representation of the candidate sentences as labeled graphs. In Section 4.3, we explain the random walk kernel that was used as the basis for our RE kernel. In Section 4.4, we present the parameters used to modify the random walk kernel for our problem. Finally, in Section 4.5, we propose our kernel for RE."}, {"heading": "4.1 Kernel Methods for Relationship Extraction", "text": "In some cases, input objects of a classifier may not be easily expressed via feature vectors (e.g., if the range of possible\nfeatures is too wide or if the nature of the object does not make it clear how to choose the features). Therefore, the feature engineering process may become painfully hard and lead to high-dimensional feature spaces and consequently to computational problems. Kernel methods are an alternative to feature-based methods that can be used to classify objects while keeping their original representation.\nIn kernel methods, the idea is to exploit a similarity function (kernel) between input objects. This function, with the help of a discriminative machine learning technique, is used to classify new examples. In order for a similarity function to be an acceptable kernel function, K(x, y), it must respect the following properties: (i) it must be a bidimentional function over the object space X to a number in [0,+\u221e[ (K : X \u00d7 X \u2212\u2192 [0,+\u221e[); (ii) it must be symmetric (\u2200x,y X ,K(x, y) = K(y, x)); and (iii) it must be positivesemidefinite (\u2200x1,x2,...xn X , the n\u00d7n matrix (K(xi, xj))ij is positive-semidefinite).\nRE is an example of a problem for which the inputs may not be easily expressed via feature vectors. As described in Section 3, the inputs of the learning and classification algorithms in supervised RE tasks are sentences. Typically, sentences are better described as structures (e.g., sequences of words, parsing trees, dependency graphs) and it is interesting to use these representations directly."}, {"heading": "4.2 Labeled Graph Representation of the Sentences", "text": "In our approach, we assume that the inputs of the learning and classification algorithms are labeled graph representations of the candidate sentences (see Figure 4). In this graph, each vertex is associated with a word in the sentence and is enriched with additional features of the word. In our representation, the additional features include POS tags, generic\nPOS tags, the lemma of the word and capitalization patterns (however, due to simplicity, we represent only one additional feature in the graph of Figure 4 which is the POS tag). We could use other potentially useful features like hypernyms or synsets extracted from the WordNet. The edges represent semantic relationships between the words. The type of the semantic relationship is represented by the edge label.\nRecall that, for a given sentence with k entities, when searching for a n-ary relationship, the number of candidates that are generated is ( k n ) . In terms of structure (vertexes and edges), the corresponding dependency graph for each of these candidates is always the same. If we used only structural information to compare candidates we could have a problem because we would not be able to distinguish between different candidates generated from the same sentence that are expected to produce different classification results.\nFor this reason, we used heuristics to enrich our graph representation. First, the entities that are candidate to be related can provide very important clues for detecting if there is a relationship [14]. We define a predicate isEntity(v), which receives a vertex of the graph and determines whether it is an entity. With this, it is possible for a kernel to use this information in the computation of the similarity between graphs. Second, the shortest path hypothesis, formalized in [7], states that the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding their relationship. Analogously to [7] and [2], we exploited this hypothesis by defining a predicate called inSP (x) that receives as input a node or an edge of the graph and returns true if they belong to the shortest path between the two entities of the graph. Like in the case of the entities, this allows the kernel to treat these vertexes and edges in a special fashion way."}, {"heading": "4.3 Random Walks Kernel", "text": "The random walk kernel used as a basis of our RE kernel was defined in [17] as a marginalized kernel between labeled graphs. The basic idea behind this kernel is the following one: given a pair of graphs, perform simultaneous random walks between the vertexes of the graphs and count the number of matching paths. In a more formal way, the objective of the kernel is to compute the expected number of matching paths between the two graphs.\nIn order to explain this kernel, we start by defining the graph that is expected as input. Let G be a labeled directed graph and |G| be the number of vertexes in the graph. All vertexes in the graph are labeled and vi denotes the label of vertex i. The edges of the graph are also labeled and eij denotes the label of the edge that connects vertex i and vertex j. Moreover, we assume two kernel functions, Kv(v, v\n\u2032) and Ke(e, e\n\u2032) that are kernel functions between vertexes and edges respectively. Figure 5 presents an example of a graph that can be used as input of the random walk kernel.\nAdditionally to the graph, this kernel also assumes the existence of three probability distributions: (i) the initial probability distribution, ps(h), that corresponds to the probability that a path starts in the vertex h; (ii) the ending probability, pq(h), that corresponds to the probability that a path ends in the vertex h; and (iii) the transition proba-\nbility, pt(hi|hi\u22121), that corresponds to the probability that we walk from vertex hi\u22121 to vertex hi. With all these probabilities defined, it is possible to compute the probability of a path h = [h1, h2, ..., hl] in the graph G with Equation 1.\np(h|G) = ps(h1) l\u220f\ni=2\npt(hi|hi\u22121)pq(hl) (1)\nAs we stated before, the objective of the kernel is to compute the expected number of matching paths between two input graphs. Let us define a kernel to compute the number of matching subpaths between two paths of different graphs. We assume that if the paths have different lenghts, then there is no match between them. If the paths have the same length, the matching between them is given by the product of the vertex and edge kernels. Assuming we have two paths h and h\u2019 from two different graphs G and G\u2032, then the kernel between z = (h, G) and z\u2032 = (h\u2019, G\u2032) is given by Equation 2.\nKz(z, z \u2032 ) =  0 if l 6= l\u2032 Kv(vh1 , v \u2032 h\u20321 ) \u220fl i=2Kv(vhi , v \u2032 h\u2032 i )\u00d7 if l = l\u2032\nK(ehi\u22121hi , e \u2032 h\u2032 i\u22121h \u2032 i )\n(2)\nGiven Kz(z, z \u2032) and p(h|G), we can compute the expected number of matching paths between the two graphs with Equation 3.\nK(G,G \u2032 ) = E[Kz(z, z\u2032)] = \u2211 h \u2211 h\u2019 Kz(z, z \u2032 )p(h|G)p(h\u2019|G\u2032) (3)\nComputing this kernel using a naive approach (i.e., going\nthrough all the possible pairs of paths in the kernels), would be computational expensive for acyclic graphs and impossible for graphs containing cycles. However, [17] demonstrated that this kernel can be efficiently computed by solving a system of linear equations. In order to define this system of linear equations, let us first define the following matrices:\nS =  s(1, 1\u2032) s(1, 2\u2032) . . . s(1, |G\u2032|\u2032) s(2, 1\u2032) .\n.\n. s(|G|, |G\u2032|\u2032)\n Q =  q(1, 1\u2032) q(1, 2\u2032) . . . q(1, |G\u2032|\u2032) q(2, 1\u2032) .\n.\n. q(|G|, |G\u2032|\u2032)\n\nT=  t(1, 1\u2032, 1, 1\u2032) t(1, 1\u2032, 1, 2\u2032) \u00b7 \u00b7 \u00b7 t(1, 1\u2032, |G|, |G\u2032|\u2032) t(1, 2\u2032, 1, 1\u2032) t(1, 2\u2032, 1, 2\u2032) \u00b7 \u00b7 \u00b7 t(1, 2\u2032, |G|, |G\u2032|\u2032) .\n. . . . .\n. . . . . .\nt(|G|, |G\u2032|\u2032, 1, 1\u2032) t(|G|, |G\u2032|\u2032, 1, 2\u2032)\u00b7 \u00b7 \u00b7 t(|G|, |G\u2032|\u2032, |G|, |G\u2032|\u2032)  Where\ns(h1, h \u2032 1) = ps(h1)ps\u2032 (h \u2032 1)Kv(vh1 , v \u2032 h\u20321 ) (4)\nq(hl, h \u2032 l) = pq(hl)pq\u2032 (h \u2032 l) (5)\nt(hi\u22121, h \u2032 i\u22121, hi, h \u2032 i) = pt(hi|hi\u22121)pt(h \u2032 i|h \u2032 i\u22121)\u00d7\nKv(vhi , v \u2032 h\u2032 i )K(ehi\u22121hi , e \u2032 h\u2032 i\u22121h \u2032 i ) (6)\nThe system of linear equations that we need to solve is presented in Equation 7\n(I \u2212 T )X = Q (7)\nwhere X is the solution of the system and I is the identity matrix. [17] demonstrated that the random walk kernel between graphs, K(G,G\u2032), can be given by Equation 8.\nK(G,G \u2032 ) =< S,X > (8)\nwhere < S,X > is the inner product between two vectors."}, {"heading": "4.4 Parameters of the Random Walks Kernel for Relationship Extraction", "text": "In Section 4.3, we described a kernel for generic labeled graphs. The kernel we propose is a particularization of this one applied to RE.\nRecall that our representation of a sentence, presented in Section 4.2 corresponds to a labeled graph where the labels of the vertexes are vectors of tags (containing the word itself, its lemma, POS tags, and ortographic patterns) and the labels of the edges contain simply the type of the semantic relationship between the two entities. Moreover, each vertex and edge contains information about whether it is in the shortest path between the two entities. The vertexes also contain information about whether they are entities.\nIn order to use the random walk kernel described in Section 4.3, we had to define the kernels between the vertex labels and the kernels between the edge labels. Given the fact that the labels of the vertexes are simply vectors of attributes of the word associated with the vertex, we can use the normalized linear kernel presented in Equation 9.\nKv(v, v \u2032 ) = c(v, v\u2032)\u221a c(v, v)c(v\u2032, v\u2032)\n(9)\nwhere c(v, v\u2032) counts the number of common features between the labels of v and v\u2032.\nIn order to guarantee that entities can only match in a random walk with other entities and that vertexes contained in a shortest path can only match with vertexes contained in a shortest path, we actually used a slightly modified version of the kernel presented in Equation 9. The modified version is presented in Equation 10.\nKv(v, v \u2032 ) =  c(v,v\u2032)\u221a c(v,v)c(v\u2032,v\u2032) if inSP (v) = inSP (v\u2032) \u2227 isEntity(v) = isEntity(v\u2032)\n0 otherwise (10)\nThe kernel between the edges is very simple. Since the label for the edges is only a string indicating the type of semantic relationship between the two words. We define this kernel in Equation 11.\nKe(e, e \u2032 ) = \u03b4(e = e \u2032 ) (11)\nwhere, \u03b4 is a function that returns 1 if its argument holds and 0 otherwise.\nOnce again, since we want to differenciate edges in the shortest path from edges outside the shortest path, we added a simple modification to the kernel that is presented in Equation 12.\nKe(e, e \u2032 ) = { \u03b4(e = e\u2032) if inSP (e) = inSP (e\u2032) 0 otherwise\n(12)\nFinally, we still need to define the probability distributions necessary to compute the random walk kernel in our problem. Due to the fact that we have no prior knowledge about the probability distributions, we follow the solution proposed in [17] and consider that all the distributions are uniform."}, {"heading": "4.5 Random Walks Kernel for Relationship Extraction", "text": "Using the random walk kernel presented in Section 4.3 and the parameterization for the RE problem proposed in Section 4.4, we produced three variations of the kernel: (i) Full Graph Kernel; (ii) Shortest Path Kernel; and (iii) No Shortest Path Kernel.\nThe Full Graph Kernel (FGK) corresponds to the application of the random walk kernel to the whole structure described in Section 4.2. The idea of this kernel is to capture the whole view of the graph structure (which is the same for all the candidates generated from a given sentence) but still be able to capture the similarity between interesting properties that are specific to the candidates (i.e., shortest path and entities information).\nThe Shortest Path Kernel (SPK) aims at exploiting the shortest path hypothesis presented in [7]. The idea is to apply the random walk kernel to the subgraph that corresponds to the shortest path between the entities.\nThe No Shortest Path Kernel (NSPK) is a variation of FGK where the nodes and edges that belong to the shortest path are not marked as such. For this reason, the only thing that distinguishes the graph structures for candidates generated from a given sentence are the entities.\nThe kernel we propose is actually based on a very interesting property of kernels: the linear combination of several kernels is itself a kernel. We used this approach because several works empirically demonstrated that combining kernels using this approach typically improves the performance of individual kernels [9, 14]."}, {"heading": "5. EXPERIMENTS", "text": "In this Section, we present the experiments performed in order to evaluate our solution for RE and report on the results obtained. First, we present the relationship extraction task. Then, in Section 5.2, we describe the dataset. Section 5.3 presents the metrics used to evaluate our kernel and Section 5.4 presents the method used to support our claims in what concerns the comparison of the kernels. In Section 5.5, we point out some implementation details of our experiments. In Section 5.6 we report on the performance of the individual kernels presented in Section 4.5 and in Section 5.7 we report on the combination of these kernels. In Section 5.8, we perform a comparison between our solution and other methods. Finally, in Section 5.9 we report on some experiments when combining our kernel with other methods."}, {"heading": "5.1 Relationship Extraction Task", "text": "In our evaluation, we focused exclusively on the extraction of relationships that correspond to protein-protein interactions. The idea is that, given pairs of entities there is a\nrelationship between them if the text indicates that the proteins have some kind of biological interaction."}, {"heading": "5.2 Dataset", "text": "We performed our experiments over a protein-protein interaction dataset called AImed1. This dataset has been used in previous works to evaluate the performance of relationship extraction systems in the task of extracting protein-protein interactions [8, 14, 2]. AImed is composed by 225 Medline abstracts from which 200 describe interactions between proteins and the other 25 do not refer to any interaction. The total number of interacting pairs is 974 and the total number of non-interacting pairs is 4072.\nDuring the evaluation of our model we used a cross-validation strategy that is based on splits of the AImed dataset at the level of document [8, 2]. Table 1 presents the number of positive and negative candidates that can be found in the training and testing data of each split."}, {"heading": "5.3 Evaluation Metrics", "text": "Our experiments are focused on measuring the quality of the results produced when using our kernel. In Information Extraction (and particularly in Relationship Extraction), the quality of the results produced is based on two metrics: recall and precision.\nRecall gives the ratio between the amount of information correctly extracted from the texts and the information available in texts. Thus, recall measures the amount of relevant information extracted and is given by Equation 13:\nrecall = C\nP (13)\nwhere C represents the number of correctly extracted relationships while P represents the total number of relationships that should be extracted. The disadvantage of this measure is the fact that it returns high values when we extract all possible pairs of entities as a relationship regardless of them being related or not.\nPrecision is the ratio between the amount of information correctly extracted from the texts and all the information extracted. The precision is then a measure of confidence on the information extracted and is given by Equation 14:\n1ftp://ftp.cs.utexas.edu/pub/mooney/biodata/interactions.tar.gz\nprecision = C\nC + I (14)\nwhere C represents the number of relationships correctly extracted, I represents the number of relationships incorrectly extracted.\nThe disadvantage of precision is that we can get high results extracting only information that we are sure to be right and ignoring information that are in the text and may be relevant.\nThe values of recall and precision may enter in conflict. When we try to increase the recall, the value of precision may decrease and vice versa. The F-measure was adopted to measure the general performance of a system, balancing the values of recall and precision. It is given by Equation 15:\nF -measure = (\u03b22 + 1)\u00d7 P \u00d7 R \u03b22 \u00d7 P + R\n(15)\nwhere R represents the recall, P represents the precision, \u03b2 is an adaptation value of the equation that allows to define the relative weight of recall and precision. The value \u03b2 can be interpreted as the number of times that the recall is more important than accuracy. A value for \u03b2 that is often used is 1, in order to give the same weight to recall and precision. In this case, the F-measure value is obtained through Equation 16:\nF1 = 2\u00d7 P \u00d7 R P + R\n(16)"}, {"heading": "5.4 Significance Tests", "text": "In order to support our claims during the comparison of each pair of kernels, we relied significance tests. We used a the paired t-test between each pair of kernels that we wanted to compare directly. Details about this significance test can be found on most statistics text books [5].\nFor a given metric presented in Section 5.3, we give as input to the test the result obtained for each split of the dataset. Our claims are based on a significance level of 5%."}, {"heading": "5.5 Implementation Details", "text": "Our experiments used the SVM package jLIBSVM2, a Java port of LIBSVM that allows for easy customization when using different kernels. During the experiments, we used most of the default parameters of jLIBSVM. The only exception was the parameter C of the SVM (which controls the trade-off between the errors of the SVM and the size of the margin). For this parameter, after some empirical experimentation we fixed its value in 50 for all the experiments.\nWe used the OpenNLP3 module for sentence detection and\n2http://dev.davidsoergel.com/trac/jlibsvm/ 3http://incubator.apache.org/opennlp/\nthe Stanford parser4 for the word segmentation, POS tagging and generation of the labeled dependency graph.\nFinally, we used Parallel Colt5 to perform the matrix operations necessary for our kernel."}, {"heading": "5.6 Performance of the Individual Kernels", "text": "Our first experiment aimed at understanding how each of the individual kernels that we proposed (i.e., FGK, SPK and NSPK introduced in Section 4.5) performs. Table 2 shows the results of this experiment.\nThe results obtained are according to what was expected. First, the individual kernel that obtains the highest value of F1 is SPK. Knowing how the shortest path hypothesis has been exploited with success in several other works, this comes with no surprise. Even though the average value of F1 for SPK is higher than that for FGK, the difference is not statistically significant according to the significance tests.\nIf we look only at the average values of recall and precision presented in Table 2, it seems that SPK is the best kernel in terms of recall and FGK is the best in terms of precision. However, by comparing the results obtained by these two kernels using the significance tests the differences are not significant for both these metrics.\nAnother result that is not surprising is the fact that the performance of NSPK is very poor. As discussed before, this kernel does not distinguish very well candidates that are generated from the same sentence but are associated with different pairs of entities. This reflects in a drastic drop of the recall value."}, {"heading": "5.7 Performance of the Combination of Kernels", "text": "After analyzing the performance of the individual kernels, we evaluated the performance of the kernels that result from their combination. We considered the following four combinations: (i) FGK+SPK; (ii) FGK+NSPK; (iv) SPK+ NSPK; and (iii) ALL = FGK + SPK +NSPK.\nTable 3 shows the results of this experiment. Given the performance of the individual kernels reported before, it was expected that the best combination of kernels would be either the one that combines all the individual kernels (ALL) or the one that combines the two best individual kernels (FGK+SPK). In fact, the results show that regarding the average values of recall, precision and F1, the best combination is actually SPK +NSPK.\n4http://nlp.stanford.edu/software/lex-parser.shtml 5http://sites.google.com/site/piotrwendykier/software/\nThe explanation for this surprising result has to do with the definition of these kernels. On one side, SPK was designed as a good solution to distinguish between candidates generated from the same sentence and associated with different pairs of entities. On the other side, NSPK is good to analyze the whole structure of the dependency graph but it does not distinguish very well the candidates generated from the same sentence. Thus, these two kernels are good at distinguishing very different contexts of the candidates. For this reason, they end up being a good complement to each other.\nEven though SPK +NSPK obtained the best average values of recall, precision and F1, it is important to note that according to the significance tests, it is not fair to claim that it is a superior solution in comparison to FGK+SPK and ALL since the differences for all the metrics were not statistically significant.\nAnother interesting observation has to do with the terrible results obtained by FGK +NSPK. It is the kernel combination with worst results in all the metrics. Moreover, the significance tests indicated that in terms of recall and F1 measure, the differences in comparison to the other combinations were significant. These results are also related with the type of information that the two individual kernels try to analyze. Recall that FGK is actually a modified and more refined version of NSPK in which vertexes and edges of the shortest path between the candidate entities are treated differently. For this reason, most of the information exploited by both kernels is the same, which makes their combination a little bit redundant.\nFinally, we wanted to compare the combination kernels with the individual kernels to understand whether it pays off to use the combinations. For each metric, we compared the combination kernels with the individual kernel with the highest value of the metric as presented in Table 2. First, in what concerns recall, we observe that the differences between SPK and most of the combinations is not significant. The only exception is SPK + NSPK. In what concerns precision, we compared with FGK and we observed that the gains from using the combinations in this case are not significant. For, the comparison regarding F1, most of the combination kernels significantly outperform SPK. The only exception is FGK + NSPK. In fact, if we compare FGK + NSPK with both kernels that originate it, we notice that the differences in terms of F1 between them are not statistically significant. This is interesting because it illustrates how combining two kernels does not necessarily mean that the results will improve."}, {"heading": "5.8 Comparison with Other Methods", "text": "In order to compare the performance of our solution with other methods, we implemented two additional kernels described in the literature: (i) a kernel based on shallow linguistic information of the sentences, [14]; and (ii) a kernel based on subsequences, [8]. During these experiments we always compared these kernels with our combination of kernels that showed better performance on the average values of the recall, precision and F1: SPK + NSPK. Table 4 shows the results of this experiment.\nThe most evident conclusion obtained by observing the results is that our solution is still outperformed by the shallow linguistic information kernel in terms of average values of the metrics. However, the significance tests for all the metrics indicate that the differences between SPK + NSPK and [14] are not significative.\nIf we compare SPK +NSPK with [8], the results are very different. In fact, the results of the significance tests show that there are significant differences between these two kernels in terms of recall and precision (SPK +NSPK is better in terms of recall and [8] is better in terms of precision). However, in terms of F1, the differences are not significative (even though the SPK +NSPK obtains an higher average value of F1).\nThe differences of the results of precision and recall of SPK+ NSPK and [14] in comparison to [8] are something worth mentioning: the precision values are not as high as in the subsequences kernel but the values of recall are significantly higher. This is interesting because it goes against a typical trend in works on supervised RE in which the values of precision tend to be very high but the values of recall tend to be very low."}, {"heading": "5.9 Combination with Other Kernel Methods", "text": "Finally, we performed some experiments to evaluate how combining SPK + NSPK with other methods influences the results. Once again, we used the two kernels that we compared our solution to in Section 5.8. Table 5 presents the results of this experiment.\nBy analyzing the results obtained in this experiment, we observe that the best combination is the one that joins SPK+\nNSPK with [14]. Moreover, even the combination of SPK+ NSPK with [8] is able to outperform the combination of [14] and [8].\nIn order to understand these results, recall that [14] is based on several kernels including information of n-grams in three different locations of the sentence: before the first entity, between the entities and after the second entity. Knowing that n-grams are among the subsequences of the sentence, it is easy to undestand that there is some overlapped information when combining these two kernel.\nWhen these kernels are combined with SPK +NSPK, we are joining information from completely different sources: sequences and dependency graph. For this reason, the kernel we propose is very interesting when used in combinations with kernels from different sources.\nWe also wanted to determine whether the difference of the results of these combinations in comparison to the individual kernels was significative. Thus, we performed significance tests between SPK + NSPK, [14], [8] and all their combinations presented in Table 5.\nIn what concerns recall, the differences between the combinations, SPK+NSPK and [14] are not significative. However, the tests indicate that all the combinations are able to outperform [8]. This comes with no surprise knowing that the differences in terms of average value of recall were very high.\nRegarding precision, the significance tests show that combining SPK +NSPK with all the other kernels have a significant impact. The tests also obtain the same result for [14]. With [8] the results are different: none of the combinations is able to significantly outperform [8].\nWhen comparing the results of the significance tests for F1, there is only one combination that is able to clearly outperforms SPK+NSPK and [14]. This combination is actually the one that combines both these kernels. In all the other cases, the differences are not significative. Regarding [8], all the combinations are able to significantly outperform it in terms of F1."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "This paper proposes a solution for Relationship Extraction (RE) based on labeled graphs kernels. The proposed kernel is a particularization of the Random Walk Kernel for generic labeled graphs presented in [17]. In order to make the kernel suitable for RE tasks, we exploited two properties typically used in this line of work: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship; and (ii) combining information from distinct sources in a kernel may help the RE system to make better decisions. Our experiments show that the performance of our solution is comparable with the state-of-the-art on RE. Moreover, we showed that combining our solution with other methods for RE leads to significant gains in terms of performance.\nInteresting topics for future work include the study of differ-\nent parameterizations of the Random Walk Kernel for RE. Namely, we want to try different kernels for vertex and edge labels as well as different probability distributions associated to the vertexes and the transitions. Moreover, it would be interesting to compare this kernel directly with other methods and test the combination of other kernels with ours. Finally, we would also like to test our solution with other datasets, namely the ACE dataset, which is composed by documents containing a wide variety of relationships (e.g., CEO OF , Located In) involving several types of entities (e.g., person, organization, location)."}, {"heading": "7. REFERENCES", "text": "[1] E. Agichtein, L. Gravano, J. Pavel, V. Sokolova, and\nA. Voskoboynik. Snowball: a prototype system for extracting relations from large text collections. In SIGMOD \u201901: Proceedings of the 2001 ACM SIGMOD international conference on Management of data, 2001.\n[2] A. Airola, S. Pyysalo, J. Bjo\u0308rne, T. Pahikkala, F. Ginter, and T. Salakoski. A Graph Kernel for Protein-Protein Interaction Extraction. In BioNLP 2008: Current Trends in Biomedical Natural Language Processing, 2008.\n[3] C. Aone, L. Halverson, T. Hampton, and M. Ramos-Santacruz. SRA: Description Of The Ie2 System Used for MUC-7. In Proceedings of the Seventh Message Understanding Conferences (MUC-7), 1998.\n[4] R. Baumgartner, T. Eiter, G. Gottlob, M. Herzog, and C. Koch. Information extraction for the semantic web. In Reasoning Web, volume 3564 of Lecture Notes in Computer Science, pages 95\u201396. Springer Berlin / Heidelberg, 2005.\n[5] G. E. P. Box, W. G. Hunter, J. S. Hunter, and W. G. Hunter. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons, 1978.\n[6] S. Brin. Extracting patterns and relations from the World Wide Web. In EDBT\u201998: WebDB Workshop at 6th International Conference on Extending Database Technology, 1998.\n[7] R. Bunescu and R. Mooney. A shortest path dependency kernel for relation extraction. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-05), 2005.\n[8] R. Bunescu and R. Mooney. Subsequence Kernels for Relation Extraction. In Advances in Neural Information Processing Systems 18. 2006.\n[9] A. Culotta and J. Sorensen. Dependency tree kernels for relation extraction. In ACL \u201904: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, 2004.\n[10] K. Eichler, H. Hemsen, and G. Neumann. Unsupervised relation extraction from web documents. In LREC 2008: Procedings of the 6th edition of the International Conference on Language Ressources and Evaluation, 2008.\n[11] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A.-M. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. Web-Scale Information Extraction in KnowItAll. In Proceedings of the 13th international\nconference on World Wide Web, 2004.\n[12] A. Fader, S. Soderland, and O. Etzioni. Identifying Relations for Open Information Extraction. In EMNLP 2011: Procedings of the Conference on Empirical Methods in Natural Language Processing, 2011.\n[13] Y. Fang and K. C.-C. Chang. Searching patterns for relation extraction over the web: rediscovering the pattern-relation duality. In WSDM\u201911: Proceedings of the fourth ACM international conference on Web search and data mining, 2011.\n[14] C. Giuliano, A. Lavelli, and L. Romano. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Procedings of EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, 2006.\n[15] K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. Cunningham, and Y. Wilks. University Of Sheffield: Description Of The Lasie-Ii System As Used For MUC-7. In Proceedings of the Seventh Message Understanding Conferences (MUC-7), 1998.\n[16] J. Jiang and C. Zhai. A systematic exploration of the feature space for relation extraction. In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics, 2007.\n[17] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized kernels between labeled graphs. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.\n[18] A. McCallum. Information Extraction: Distilling Structured Data from Unstructured Text. ACM Queue, 3(9):48\u201357, 2005.\n[19] D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and U. Leser. A comprehensive benchmark of kernel methods to extract protein\u2013protein interactions from literature. PLoS Computational Biololgy, 6(7):e1000837, 2010.\n[20] D. Zelenko, C. Aone, A. Richardella, J. K, T. Hofmann, T. Poggio, and J. Shawe-taylor. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:2003, 2003."}], "references": [{"title": "Snowball: a prototype system for extracting relations from large text collections", "author": ["E. Agichtein", "L. Gravano", "J. Pavel", "V. Sokolova", "A. Voskoboynik"], "venue": "Proceedings of the 2001 ACM SIGMOD international conference on Management of data,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "A Graph Kernel for Protein-Protein Interaction Extraction", "author": ["A. Airola", "S. Pyysalo", "J. Bj\u00f6rne", "T. Pahikkala", "F. Ginter", "T. Salakoski"], "venue": "In BioNLP 2008: Current Trends in Biomedical Natural Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "SRA: Description Of The Ie2 System Used for MUC-7", "author": ["C. Aone", "L. Halverson", "T. Hampton", "M. Ramos-Santacruz"], "venue": "In Proceedings of the Seventh Message Understanding Conferences (MUC-7),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Information extraction for the semantic web. In Reasoning Web, volume 3564 of Lecture Notes in Computer Science, pages 95\u201396", "author": ["R. Baumgartner", "T. Eiter", "G. Gottlob", "M. Herzog", "C. Koch"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building", "author": ["G.E.P. Box", "W.G. Hunter", "J.S. Hunter"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1978}, {"title": "Extracting patterns and relations from the World Wide Web", "author": ["S. Brin"], "venue": "In EDBT\u201998: WebDB Workshop at 6th International Conference on Extending Database Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["R. Bunescu", "R. Mooney"], "venue": "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-05),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Subsequence Kernels for Relation Extraction", "author": ["R. Bunescu", "R. Mooney"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Dependency tree kernels for relation extraction", "author": ["A. Culotta", "J. Sorensen"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Unsupervised relation extraction from web documents", "author": ["K. Eichler", "H. Hemsen", "G. Neumann"], "venue": "In LREC 2008: Procedings of the 6th edition of the International Conference on Language Ressources and Evaluation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Web-Scale Information Extraction in KnowItAll", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A.-M. Popescu", "T. Shaked", "S. Soderland", "D.S. Weld", "A. Yates"], "venue": "In Proceedings of the 13th international  conference on World Wide Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Identifying Relations for Open Information Extraction", "author": ["A. Fader", "S. Soderland", "O. Etzioni"], "venue": "In EMNLP 2011: Procedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Searching patterns for relation extraction over the web: rediscovering the pattern-relation duality", "author": ["Y. Fang", "K.C.-C. Chang"], "venue": "In WSDM\u201911: Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Exploiting shallow linguistic information for relation extraction from biomedical literature", "author": ["C. Giuliano", "A. Lavelli", "L. Romano"], "venue": "In Procedings of EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "University Of Sheffield: Description Of The Lasie-Ii System As Used For MUC-7", "author": ["K. Humphreys", "R. Gaizauskas", "S. Azzam", "C. Huyck", "B. Mitchell", "H. Cunningham", "Y. Wilks"], "venue": "In Proceedings of the Seventh Message Understanding Conferences (MUC-7),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "A systematic exploration of the feature space for relation extraction", "author": ["J. Jiang", "C. Zhai"], "venue": "In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Marginalized kernels between labeled graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Information Extraction: Distilling Structured Data from Unstructured Text", "author": ["A. McCallum"], "venue": "ACM Queue,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "A comprehensive benchmark of kernel methods to extract protein\u2013protein interactions from literature", "author": ["D. Tikk", "P. Thomas", "P. Palaga", "J. Hakenberg", "U. Leser"], "venue": "PLoS Computational Biololgy,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Kernel methods for relation extraction", "author": ["D. Zelenko", "C. Aone", "A. Richardella", "J. K", "T. Hofmann", "T. Poggio", "J. Shawe-taylor"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "In fact, this interest led to huge advances in this area and several solutions were proposed in applications such as Semantic Web [4] and Bioinformatics [14, 2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "In fact, this interest led to huge advances in this area and several solutions were proposed in applications such as Semantic Web [4] and Bioinformatics [14, 2].", "startOffset": 153, "endOffset": 160}, {"referenceID": 1, "context": "In fact, this interest led to huge advances in this area and several solutions were proposed in applications such as Semantic Web [4] and Bioinformatics [14, 2].", "startOffset": 153, "endOffset": 160}, {"referenceID": 17, "context": "Regardless of the application domain, an IE activity can be modeled as a composition of the following high-level tasks [18]:", "startOffset": 119, "endOffset": 123}, {"referenceID": 2, "context": "Most of the first approaches for RE were based on handcrafted rules [3, 15].", "startOffset": 68, "endOffset": 75}, {"referenceID": 14, "context": "Most of the first approaches for RE were based on handcrafted rules [3, 15].", "startOffset": 68, "endOffset": 75}, {"referenceID": 5, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 0, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 10, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 12, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 9, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 11, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 15, "context": "Even though feature-based methods for RE work well [16], there has been an increasing interest in exploiting kernel-based methods, due to the fact that sentences are better described as structures (e.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "We propose the application of a marginalized kernel to compare labeled graphs [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "In order to make this graph kernel suitable for RE, we modified the kernel to exploit the following properties that were previously introduced proposals of kernels for RE: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship [7]; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions [14].", "startOffset": 338, "endOffset": 341}, {"referenceID": 13, "context": "In order to make this graph kernel suitable for RE, we modified the kernel to exploit the following properties that were previously introduced proposals of kernels for RE: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship [7]; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions [14].", "startOffset": 453, "endOffset": 457}, {"referenceID": 7, "context": "In order to evaluate the model we propose, we performed some experiments with a biomedical dataset called AImed [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 19, "context": "[20], is a kernel based on shallow parse tree representation of sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In order to overcome these problems, Culotta and Sorensen [9] proposed a generalization of this kernel that, when combined with a bag-of-words kernel, is able to compensate the parsing errors.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "In 2005, Bunescu and Mooney [7] proposed a kernel based on the shortest path between entities in a dependency graph.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "The same authors proposed a different kernel based on subsequences [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 13, "context": "[14] proposed in 2006 a kernel based only on shallow linguistic information of the sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] presented a kernel that combines two graph representations of a sentence: (i) a labeled dependency graph; and (ii) a linear order representation of the sentence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] performed a study to analyze how a very comprehensive set of kernels for relationship extraction performs when dealing the task of extracting protein-protein interactions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Moreover, they show that a simple kernel, like [14], can still obtain results that are at the level of the best kernels based on dependency parsing.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "First, the entities that are candidate to be related can provide very important clues for detecting if there is a relationship [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Second, the shortest path hypothesis, formalized in [7], states that the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding their relationship.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "Analogously to [7] and [2], we exploited this hypothesis by defining a predicate called inSP (x) that receives as input a node or an edge of the graph and returns true if they belong to the shortest path between the two entities of the graph.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Analogously to [7] and [2], we exploited this hypothesis by defining a predicate called inSP (x) that receives as input a node or an edge of the graph and returns true if they belong to the shortest path between the two entities of the graph.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "The random walk kernel used as a basis of our RE kernel was defined in [17] as a marginalized kernel between labeled graphs.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "However, [17] demonstrated that this kernel can be efficiently computed by solving a system of linear equations.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "[17] demonstrated that the random walk kernel between graphs, K(G,G\u2032), can be given by Equation 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Due to the fact that we have no prior knowledge about the probability distributions, we follow the solution proposed in [17] and consider that all the distributions are uniform.", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "The Shortest Path Kernel (SPK) aims at exploiting the shortest path hypothesis presented in [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "We used this approach because several works empirically demonstrated that combining kernels using this approach typically improves the performance of individual kernels [9, 14].", "startOffset": 169, "endOffset": 176}, {"referenceID": 13, "context": "We used this approach because several works empirically demonstrated that combining kernels using this approach typically improves the performance of individual kernels [9, 14].", "startOffset": 169, "endOffset": 176}, {"referenceID": 7, "context": "This dataset has been used in previous works to evaluate the performance of relationship extraction systems in the task of extracting protein-protein interactions [8, 14, 2].", "startOffset": 163, "endOffset": 173}, {"referenceID": 13, "context": "This dataset has been used in previous works to evaluate the performance of relationship extraction systems in the task of extracting protein-protein interactions [8, 14, 2].", "startOffset": 163, "endOffset": 173}, {"referenceID": 1, "context": "This dataset has been used in previous works to evaluate the performance of relationship extraction systems in the task of extracting protein-protein interactions [8, 14, 2].", "startOffset": 163, "endOffset": 173}, {"referenceID": 7, "context": "During the evaluation of our model we used a cross-validation strategy that is based on splits of the AImed dataset at the level of document [8, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 1, "context": "During the evaluation of our model we used a cross-validation strategy that is based on splits of the AImed dataset at the level of document [8, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 4, "context": "Details about this significance test can be found on most statistics text books [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "[14] 47.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "49% [8] 41.", "startOffset": 4, "endOffset": 7}, {"referenceID": 13, "context": "SPK +NSPK + [14] 49.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "43% SPK +NSPK + [8] 45.", "startOffset": 16, "endOffset": 19}, {"referenceID": 13, "context": "[14] + [8] 45.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[14] + [8] 45.", "startOffset": 7, "endOffset": 10}, {"referenceID": 13, "context": "12% SPK +NSPK + [14] + [8] 46.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "12% SPK +NSPK + [14] + [8] 46.", "startOffset": 23, "endOffset": 26}, {"referenceID": 13, "context": "In order to compare the performance of our solution with other methods, we implemented two additional kernels described in the literature: (i) a kernel based on shallow linguistic information of the sentences, [14]; and (ii) a kernel based on subsequences, [8].", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "In order to compare the performance of our solution with other methods, we implemented two additional kernels described in the literature: (i) a kernel based on shallow linguistic information of the sentences, [14]; and (ii) a kernel based on subsequences, [8].", "startOffset": 257, "endOffset": 260}, {"referenceID": 13, "context": "However, the significance tests for all the metrics indicate that the differences between SPK + NSPK and [14] are not significative.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "If we compare SPK +NSPK with [8], the results are very different.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "In fact, the results of the significance tests show that there are significant differences between these two kernels in terms of recall and precision (SPK +NSPK is better in terms of recall and [8] is better in terms of precision).", "startOffset": 194, "endOffset": 197}, {"referenceID": 13, "context": "The differences of the results of precision and recall of SPK+ NSPK and [14] in comparison to [8] are something worth mentioning: the precision values are not as high as in the subsequences kernel but the values of recall are significantly higher.", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "The differences of the results of precision and recall of SPK+ NSPK and [14] in comparison to [8] are something worth mentioning: the precision values are not as high as in the subsequences kernel but the values of recall are significantly higher.", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "By analyzing the results obtained in this experiment, we observe that the best combination is the one that joins SPK+ NSPK with [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "Moreover, even the combination of SPK+ NSPK with [8] is able to outperform the combination of [14] and [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "Moreover, even the combination of SPK+ NSPK with [8] is able to outperform the combination of [14] and [8].", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "Moreover, even the combination of SPK+ NSPK with [8] is able to outperform the combination of [14] and [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "In order to understand these results, recall that [14] is based on several kernels including information of n-grams in three different locations of the sentence: before the first entity, between the entities and after the second entity.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "Thus, we performed significance tests between SPK + NSPK, [14], [8] and all their combinations presented in Table 5.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "Thus, we performed significance tests between SPK + NSPK, [14], [8] and all their combinations presented in Table 5.", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "In what concerns recall, the differences between the combinations, SPK+NSPK and [14] are not significative.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "However, the tests indicate that all the combinations are able to outperform [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "The tests also obtain the same result for [14].", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "With [8] the results are different: none of the combinations is able to significantly outperform [8].", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "With [8] the results are different: none of the combinations is able to significantly outperform [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 13, "context": "When comparing the results of the significance tests for F1, there is only one combination that is able to clearly outperforms SPK+NSPK and [14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Regarding [8], all the combinations are able to significantly outperform it in terms of F1.", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": "The proposed kernel is a particularization of the Random Walk Kernel for generic labeled graphs presented in [17].", "startOffset": 109, "endOffset": 113}], "year": 2013, "abstractText": "In this paper, we propose an approach for Relationship Extraction (RE) based on labeled graph kernels. The kernel we propose is a particularization of a random walk kernel that exploits two properties previously studied in the RE literature: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions. We performed experiments on a dataset of protein-protein interactions and the results show that our approach obtains effectiveness values that are comparable with the state-ofthe art kernel methods. Moreover, our approach is able to outperform the state-of-the-art kernels when combined with other kernel methods.", "creator": "LaTeX with hyperref package"}}}