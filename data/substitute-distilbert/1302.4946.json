{"id": "1302.4946", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "A Constraint Satisfaction Approach to Decision under Uncertainty", "abstract": "the constraint satisfaction problem ( csp ) framework offers an simple and sound basis for representing and solving simple cognitive problems, without uncertainty. original paper is dedicated to an extension of the csp framework enabling us to deal with some decisions problems under uncertainty. this extension relies on allowing differentiation between known agent - controllable decision variables and the uncontrollable parameters whose energies depend on the occurrence of uncertain events. the expectation on the values of the parameters is assumed to be given under the form of a probability distribution. some interpretations are given, for computing respectively decisions answering the problem on a maximal probability, and conditional decisions mapping the largest possible amount of possible cases confronting actual decisions.", "histories": [["v1", "Wed, 20 Feb 2013 15:20:24 GMT  (538kb)", "http://arxiv.org/abs/1302.4946v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["helene fargier", "jerome lang", "roger martin-clouaire", "thomas schiex"], "accepted": false, "id": "1302.4946"}, "pdf": {"name": "1302.4946.pdf", "metadata": {"source": "CRF", "title": "A constraint satisfaction framework for decision under uncertainty", "authors": ["Helene Fargier", "Jerome Lang", "Paul Sabatier", "Roger Martin-Ciouaire", "Thomas Schiex"], "emails": ["lang}@irit.fr", "rmc@toulouse.inra.fr", "tschiex@toulouse.inra.fr"], "sections": [{"heading": null, "text": "The Constraint Satisfaction Problem (CSP) framework offers a simple and sound basis for representing and solving simple decision problems, without uncertainty. This paper is devoted to an extension of the CSP frame work enabling us to deal with some decisions problems under uncertainty. This extension relies on a differen tiation between the agent-controllable decision vari ables and the uncontrollable parameters whose values depend on the occurrence of uncertain events. The un certainty on the values of the parameters is assumed to be given under the form of a probability distribution. Two algorithms are given, for computing respectively decisions solving the problem with a maximal proba bility, and conditional decisions mapping the largest possible amount of possible cases to actual decisions.\n1 Introduction\nDecision making is primarily a matter of choosing between alternatives that most commonly are expressed implicitly. Thus solving a decision problem amounts to generate the option(s) that is(are) most appropriate with respect to the specification of the decision problem at hand. Different decision problems can be characterized along two discrim inating features:\n- decisions are made at a single time point (although they may have a temporal structure i.e., form a plan) or are elaborated in sequence of steps, each being enriched by information resulting from the previous ones;\n- the requirements (constraints, criteria) that implicitly define the alternatives are uncertainty-free or not. Re quirements are uncertain if they vary depending on the circumstances, the occurrence of which is incom pletely known (uncertain).\nSequential decisions and uncertain requirements increase the complexity of the decision problem, the most complex case being the joint situation. This paper is devoted to the case of single-instant decisions and uncertain requirements and proposes an approach of the problem in the framework of constraint satisfaction.\nSo far the only decision problem tackled within CSP (Con straint Satisfaction Problem) approaches concern the sim plest case: single-instant decision problem with no uncer tainty at all. In order to cast the addressed decision problem in a CSP framework it is essential to distinguish between two types of unknowns that are called parameters and deci sion variables respectively. Parameters are uncontrollable unknowns, i.e. , the value taken by a parameter is a matter of occurrence of an event that cannot be controlled (not even influenced) by the decision maker (also referred to as the agent). The value of a parameter is imposed by the exter nal world and the agent may have only partial knowledge about what this value might be. By contrast, the assignment of the decision variables is what the agent wants to decide upon. Failing to differentiate between parameters and de cision variables may yield nonsensical results in a classical CSP approach because a particular value of a parameter can restrict the range of allowed (satisfactory) values for the de cision variables whereas the converse is physically impossi ble. The key issue is therefore to develop a CSP framework and resolution algorithms that provide for the uncontrol lable/controllable dichotomy in the set of unknowns.\nThis paper addresses an extension of the CSP framework, namely probabilistic CSP, involving both parameters and decision variables, the uncertainty on the values of the pa rameters being represented by a probability distribution. A formal representation framework is defined and two algo rithms relying on an assumption of independence of the parameters are described. In this paper we consider succes sively two assumptions concerning the agent's awareness of the paremeter values (the state of the world) at the time the decision must imperatively be made (deadline for acting):\n- (NK) (\"no more knowledge\"): the agent will never learn anything new about the actual state of the world before the deadline; all it will ever know is already encoded by the probability distribution. For this case we propose an algorithm which gives an actual, un conditional decision, that is most likely to be suitable.\n- (CK) (\"complete knowledge\"): the actual world will be completely revealed before the deadline is reached (possibly, just before), so that it it useful to the agent to compute \"off-line\" a ready-to-use conditional deci sion, that the agent will be able to instantiate \"on-line\",\n168 Fargier, Lang, Martin-Clouaire, and Schiex\nas soon as it knows what the actual world is. For this purpose we have developed an \"anytime\" resolution algorithm that provides a set of decisions with their conditions of applicability, together with the likeli hood of occurrence of these conditions.\nThe intermediate case where some partial knowledge about the state of the world may be learned is not considered in this paper. See [5) for further discussion on this point.\nIn Section 2 we define probabilistic CSP s and several types of solutions (corresponding to decisions); then we propose algorithms dealing with the problem of computing optimal decisions under the assumptions (NK) (Section 3) and (CK) (Section 4).\n2 Probabilistic constraint satisfaction problems\n2.1 Preliminary definitions and notations\nA classical constraint satisfaction problem is a triple P = (X, D, C), where X is a set of variables, each of which has its possible values in a domain (supposed here finite) D; (with D = x;D;), and Cis a set of constraints. Each constraint C; E C involves a set of variables noted V ( C;) and is defined by a subset of the Cartesian product of the domains of the variables in V(C;). This subset, noted"}, {"heading": "C; as the constraint itself, gives the set of all the possible assignments of the variables in V ( C; ): the constraint C; is", "text": "satisfied by an assignment of the variables in V ( C;) iff this assignment belongs to the set of admissible tuples of C;."}, {"heading": "A solution of the CSP is an assignment of values to all the variables such that all the constraints are satisfied. The set", "text": "of all the solutions of a CSP Pis noted Sol(P).\nIn the rest of the paper, assignments of values to a set of variables Y are also considered as tuples of values of the variables of Y, i.e., elements of the Cartesian product of the domains of the variables in Y. The concatenation of two assignments al and a2 of variables in vl and v2 such that vl n v2 = 0 is noted (al, a2); it is an assignment of vl uv2. The projection of an assignment a of a set of variables Y on a set of variables Z \ufffd Y, noted t !Z is simply the tuples of the values of the variables of Z in t. This notion (and notation) is extended to a set of tuples: the projection R!z on Z of a subset R of the Cartesian product of the domains of the variables of Y is the set of the projections on Z of all the assignments in R.\n2.2 Probabilistic CSPs: definitions\nRoughly speaking, a probabilistic CSP is a CSP equipped with a partition between (controllable) decision variables and (uncontrollable) parameters, and a probability distribu tion over the possible values of the parameters.\nDefinition 1 A probabilistic CSP is a 6-uple P = (A, W, X, D, C, pr) where:\n- A = { .X1, ... , Ap} is a set of parameters;\n- w = wl X . . . X Wp. where W; is the domain of >..;; - X = { x1, ... , Xn} is a set of decision variables; - D = D1 x \u00b7 \u00b7 \u00b7 x Dn, where D; is the domain of x;; - C is a set of constraints, each of them involving at least one decision variable.\n- pr : W - [0, 1] is a probability distribution over the parameter assignments.\nConstraints are defined in the same way as in classical CSP. We note X(C;) (resp. A(C;)) the set of variables (resp. parameters) involved in a constraint C;."}, {"heading": "A complete assignment of the parameters (resp. of the", "text": "decision variables) will be called a world (resp. a decision) and will be generally denoted by w (resp. by d). A partial world (resp. a partial decision) will be an assignment of a subset of the parameters (resp. of the decision variables).\nThe subset of C containing the constraints involving no parameters will be denoted by C* ; the other constraints in C (involving at least a parameter) restrict the allowed values of some decision variables, dependently of the values of some parameters: they will be called parameterized constraints. Obviously, if A = 0, P is a classical CSP. We assume that the constraints of C involve at least a de cision variable since the available information about the actual values of parameters is completely encoded by pr if a tuple of parameter assignments is impossible then the probability of any world extending this tuple is 0.\nWe have not y et discussed how the probability distribution on worlds is specified. Clearly, it is not reasonable to assume that the input contains the explicit specification of pr(w) for each w. Hence, generally pr will be given in a much more concise way. The simplest case occurs when parameters are mutually independent (pr is then specified only by an individual probability distributions p1\u00b7,x, for each parameter .X;). A more complex case consists in structuring the parameters in a Bayesian network: the computation of pr requires then the propagation of probabilities through the network. Thus, pr will generally be given implicitly rather than explicitly, and will generally require some computation in order to be available. From now on we choose to ignore this step: the computation of pr is taken for granted.\nDefinition 2 (possible worlds) A world w of W such that pr( w) > 0 is a possible world. The set of all possible worlds is denoted by Poss(P).\nLet a be an assignment of a subset of the variables (where variables is here the general terminology for both parame ters and decision variables). We define the reduction of a constraint C by assignment a as the set of assignments of the unassigned variables compatible with a according to C:\nDefinition 3 Let C be a constraint of C involving a set V (C) = A( C) U X (C) of variables and parameters. The reduction ofC by anassignment a of V' \ufffd V(C) is the con straint Reduce(C, a) on V(C)- V' defined by the tuples {bE DJY(C)-V' I (b, a)!V(C) E C}.\nA constraint satisfaction framework for decision under uncertainty 169\nWe note C[a] = {Reduce(C, a), C E C} the reduction ofC by a.\nThis notion of reduction is especially meaningful when a is either a world w or a decision d. The reduction of C by a world w is a set of classical decision constraints (involving decision variables only); it defines the decisions which are suitable if the world is w. Namely, to each world w we as sociate the uncertainty-freedecision problem (X, D, C[w]). Each of these classical CSPs will be called a candidate problem. One of them (and only one) is the actual problem (X, D, C[w*]), corresponding to the actual world w* (but since there is generally more than one possible world, the agent does not know which one is the actual one).\nDefinition 4 The set of candidate problems induced by P is defined by Candidates(P) = {(X,D,C[w]) lw E W )} .\nNote that among the CSPs of Candidates(P), some may be inconsistent, which means that the actual problem may be inconsistent.\nDually, the reduction of C by a decision d yields a CSP (A, W, C[d]) involving parameters only, whose solutions are the worlds for which d is a suitable decision. These worlds are said to be covered by d. Note that, obviously, w is a solution of (A, W, C[d]) iff d is a solution of (X, D, C[w]) iff(w, d) is a solution of (AU X, W x D, C).\nAmong the possible worlds, those which are covered by at least a decision are called good worlds- and the others are called bad worlds. Equivalently, w is good iff (X, D, C[w ]) is consistent.\nDefinition S Good(P) = {w E Poss(P) I 3d ED, (w, d) satisfies C)} Bad(P) = Poss(P) \\ Good(P)\nNow, for each possible decision d we can compute the probability that it is a suitable decision, i.e. that it covers the actual world.\nDefinition 6 (probability that a decision is a solution) The probability that a given decision d is a solution of the actual problem is the probability of the set of worlds it covers, i.e. PS(d) = Pr(Sol((A, W,C[d]))).\nNotice that P S is not a probability distribution 1.\nNow, we can compute the probability that the actual world can be covered: this is the probability of consistency of the actual problem.\nDefinition 7 (probability of consistency) To a proba bilistic CSP P we associate the probability that the actual problem is consistent, i.e.\nPcon\u2022(P) = Pr(Good(P))\nIf Pcons(P) = 1 then P will be said consistent.\n1It is actually a contour function in the sense of Dempster Shafer theory. We omit details due to lack of space (see [5]).\nExample: (strongly modified from [14]). Cons ider a dinner to be organized, to which each of the three guests (Grandgousier, Gargantua and Pantagruel) is not sure to come. The problem is to choose a wine and a meal according to four cons traints, among which three depends on the presence of each guest. The probabilistic CSP P corresponding to this problem is defined by:\n- Decision variables: X = {x1, x2 }; D1 = {White, Red}; D2 = {Turkey, Beef, Fish} - Parameters: A = {.XI, A2, A3}; WI = w2 = w3 = {comes, -.comes} (these two values are abbreviated c and -.c in the rest of the paper) . -Cons traints: C = {C1,C2,C3,C4};\nCuisine rules Grandgousier (bee[)\nXl X2 w F R B w T R T\n.X1 X2 c T c F : T -.c\n-.c F -.c B\nGargantua (white wine) Pantagruel (vegetarian)\n.X2 X! c R -.c R -.c w\n.X3 X2 c F -.c T -.c F -.c B\nThe participation of the different guests are mutually independent. The probability that Grandgousier comes is 0.6 and respectively 0. 9 and 0.5 for Gargantua and Pantagruel.\nIn this example, all worlds are possible ( Poss(P) = W). If we cons ider world w1 = ( c, -.c, c) whose probability is 0.03, the reduction of the cons traints are: Reduce( C1, w!) = C1, Reduce(C2,w!) = {T, F}, Reduce(C3,w!) = {R, W } and Reduce(C3,wl) = {F}. TheCSP (X, D,C[w1]) that corre sponds to w1 has a single solution (W, F) and thus w1 is a good world. The decision (W, F) covers 3 more worlds (( -.c, -.c, c), ( c, -.c, -.c) and ( -.c, -.c, -.c)) which finally give a probability PS((W, F)) = 0.1 (since the worlds covered by (W, f) are exactly those where Gargantua does not come) .\nIn the world w2 = ( c, c, c), whose probability is 0.27, the reduction of the cons traints are: Reduce( cl' W2) = cl' Reduce(C2,w2) = {T, F}, Reduce(C3,w2) = {R} and Reduce(C3, w2 ) = {F} which defines an inconsistentCSP. Thus, w2 is a bad world. The only other bad worldisw3 = (-.c, c, c) with probability 0.18. Therefore, Bad(P) = {(c,c,c),(-.c,c,c)}, Pr(Bad(P) = 0.27 + 0.18 = 0.45 and Pcons(P) = 0.55.\nThere is a number of interesting particular cases of proba bilistic CSPs obtained by some simplifying assumptions. A first assumption, already evoked in Section 2.1, is the mu tual independence of parameters: in this case pr is specified only by individual probability distributions2\u2022\nAnother particular case is illustrated by our previous exam ple: we have a problem where the relevance of some con straints is uncertain, and every uncertain constraint c; is en coded by a parameterized constraint C; linking the decision\n2Hence, the problem can be stated in such a way that Poss(P) = W, simply by removing impossible values from the domain.\n170 Fargier, Lang, Martin-Clouaire, and Schiex\nvariables of c; to one parameter A;, whose domain has two values (in our example, comes and -.comes), corresponding respectively to the relevance and the irrelevance of c; to the actual problem. This simplifying assumption is denoted by ( U). If { c1, ... , cq} denotes the set of these uncertain constraints, then it can be shown that C andidates(P) is a relaxation lattice in the sense ofFreuder [6], equipped with the probability distribution obviously induced by the distri bution on worlds: namely, the set of candidates problems are obtained by taking the union of C* and of any subset of { c;, i = 1 . . . q }. Moreover when the c; 's are independently relevant, P is strongly consistent (see Definition 13) iff the top of the lattice, namely { c;, i = 1 . . . q} u C* is consistent in the classical sense. A more general class of probabilistic CSPs occurs when each parameterized constraint involves exactly one parameter. Each parameterized constraint cor responds thus to an disjunctive family of classical decision constraints (one for each possible value of the involved pa rameter). We denote by (F) this simplifying assumption.\n2.3 Conditional decisions as solutions\nA conditional decision will ideally associate to each possi ble world a decision satisfying the corresponding decision problem. Since some possible worlds may induce an in consistent CSP, a weaker request consists in giving a partial conditional decision, i.e., defined on a subset of W (ideally on Good(P)).\nDefinition 8 (conditional decisions) A complete condi tional decision s is a map from Poss(P) to D. A par tial conditional decision s is a map from a subset W ( s) of Poss(P) to D. A (partial or complete) conditional decision is soundiffVw E W(s),s(w )coversw.\nIn the rest of the paper, we refer to \"conditional decisions\" as to partial or complete conditional decisions. Clearly, complete conditional decisions generalize decisions as de fined in Section 2.2 (the latter are called pure decisions as opposed to conditional decisions). Namely, a conditional decision which is constant over Poss(P) corresponds to a (pure) decision. Conditional decisions will play the role of solutions for probabilistic CSPs (ideally, a solution is a complete sound conditional decision). As for (pure) deci sions, a conditional decision has a probability to cover the actual world:\nDefinition 9 The probability that a conditional decision s will yield a solution to the actual problem is defined by:\nPS(s) = L pr\u00b7(w) wEIV(s)\ns(w) covers w\nThe following simple results are easy to prove:\nProposition 1 (i) for any conditional decisions, PS(s) :S Pcons(P). (ii) there exists as such that PS(s) = Pcons(P). (iii) P is consistent iff there exists a s such that P S( s) = 1.\nDefinition 10 (optimal conditional decisions) A condi tional decision (partial or complete) s is optimal for P iff PS(s) is maximum (or equivalently iff PS(s) = Pcons(P)).\nThus, one may compute an optimal conditional decision off line and use it later on, in real time, when the actual world will be known. This assumes that this actual world will be known before the decision has to be taken ((CK)). Clearly, under the other extreme assumption ((NK)), it is useless to look for a conditional decision: consequently, in this case one has to look for a pure decision rather than for a conditional one. A reasonable strategy is then to maximize the probability that this pure decision will work:\nDefinition 11 (optimal pure decisions) A pure decision d is optimal iff PS(d) is maximum. We let Pspv(P) = P S( d) where dis an optimal pure decision.\nPspv(P) is the maximum probability of success of a Pure Decision. Obviously, Pspv(P) :S Pcons(P). The ideal case is when there is a pure decision covering all good worlds:\nDefinition 12 (universal decisions) A pure decision dis a universal decision iff PS(d) = Pcons(P).\nThus, a universal decision will work for any possible world whose associated candidate problem is consistent and thus it is an optimal decision. Clearly, when there exists a universal decision, it is useless to look for a conditional decision.\nDefinition 13 (strong universal decisions) A pure deci sion dis a strong universal decision ofP iff P S( d) = 1. P is strongly consistent iff it has a strong universal decision.\nA strong universal decision is a universal decision covering all possible worlds. The case where P is strongly consistent is the ideal one. Obviously, strong consistency implies consistency .\nExample: using the same probabilistic CSP, we may consider the following conditional decisions, which is optimal since P S(s) = Pcons(\"P) = 0.55."}, {"heading": "AI .X2 AJ Decision", "text": "c c c Bad c c ..,c (R,T) c ..,c ..,c (R, T) c -,c c (W,F)\n..,c c c Bad ..,c c ..,c (R,T) ..,c -,c -,c (R, T) -,c ..,c c (W,F) . . The dectswn ( R, T) LS the only optimal pure decision( PS((R, T)) = 0.5) but is not univer sal. However, if the probability that Pantagruel ( .X3) comes were 0, ( R, T) would be a strong universal decision.\n3 Searching for an optimal pure decision\nIn both Sections 3 and 4 we assume mutual independence of parameters, for the sake of simplicity; however this as-\nA constraint satisfaction framework for decision under uncertainty 171\nsumption could be relaxed provided that the computation of the probability of a set of worlds is taken for granted.\nFurthermore, in this Section (only) we make the assumption (NK) that the agent will never have any further knowledge about the actual world before acting (all it knows is already encoded in pr) and consequently, all it can execute is a pure decision. Now, the best the agent can require from the solver is an optimal pure decision. As it may be computa tionally costly to compute an optimal one, especially if the agent has a deadline, we propose an \"anytime\" algorithm which computes an approximately optimal pure decision if stopped before its natural stop (the longer the algorithm runs, the better the pure decision), and eventually gives an optimal one if it runs until its natural stop.\nOur approach to the problem of maximizing P S( d) consists in using a Depth First Branch and Bound algorithm. For the sake of simplicity, consider that variables are instanti ated in a prescribed order, say ( x 1 , .. . , x\") and that all the constraint are binary3. The root of the tree is the empty instantiation. Intermediate nodes denote partial decisions ( d1, . . , d;). Leaves represent instantiations of X,i.e., pure decisions d = ( d1, ... , dn)\u00b7 In a depth first exploration of the tree, we keep track of the leaves d maximizing P S( d).\nEach time a new variable decision d; is assigned to variable x;, all the constraints C E C connecting this variable x; to an unassigned decision variable xi or a parameter >.A, will be used as in the classical Forward-checking algorithm [8] to remove all the values incompatible with the assignment x; = d;from the domains of xi (resp. >.k): each domain Di (resp. Wk) is simply replaced by (Dj n Reduce(C, (d;)) (resp (Wk n Reduce(C, (d;) ) if C involves a parameter). Updating the domain of a variable v using constraint C after the assignment of decision d; to x; is performed by the procedure FORWARDCHECK(v, C, d;) in Algorithm 1, line 2.\nAfter forward-checking, the Cartesian product of the re maining values in the domains of all the parameters define the worlds which are compatible with the current partial de cision: all the worlds that are obviously incompatible have been removed. The cumulated probabilities of the remaining worlds give an upper bound Ps( d) on the probability of the best complete decision among the descendants of the current node since only incompatible worlds have been re moved. This bound is easily computed if independence is assumed: the bound is simply the product on all domains of the sums of the probabilities of the remaining values of each domain. When a complete decision dis reached, since all the constraint have been propagated, the worlds remaining are the worlds covered by the decision: Ps(d) = P S( d).\nWhen does backtrack occur ? First, if the domain of a decision variable or a parameter becomes empty, then we know that no world is covered by our current partial deci sion and we may therefore backtrack, reconsider the value of the last decision variable assigned, restore the domains\n3This implies that we are in case (F) since a constraint involves at least one decision variable. The algorithm is easily extended to handle larger arities as long as assumption (F) holds.\nmodified due to the last assignment to their previous values and go on. Further cutoffs can be obtained using our upper bound and a very simple lower bound on the best proba bility. This lower bound is simply the probability P S( d) associated to the best decision found so far and is embod ied in the algorithm in a threshold a, initialized to 0 and updated each time a decision d such that P S( d) > a is reached. Whenever the two bounds meet, we know that the best probability of any complete decision that can be reached from this node is worse than the best probability found up to now and backtrack occurs, as previously.\nThe search will stop when no more node can be created. It is successful if a leaf has been reached, and the best decision among those which have been reached is optimal.\nFunction SEARCH(d1, ... , d;, p;)\n1 2\nif i = n then a := p; {Update upper bound} else\nChoose a variable x;+1 for d;+l E Di+l do\nSAVEDOMAINS(A U X) forC E C s.t. Xi+l E V(C)\nand V(C) C\u00a3 {x1, . . . , x;+d do\nL Let {vj} = V(C)- {x;+d FORWARDCHECK(vj, C, di+l)\n3 Pi+l := flw. LvEW\u2022 [pr.x.(v)]\nif (Pi+l > a) and ('t:/i, D; =f:. 0) then L SEARCH(dl' . .. 'di+l' Pi+l)\nRESTOREDOMAINS(A U X)\nAlgorithm 1: A Forward-checking-based Depth First Branch and Bound\nThe algorithm is sketched as Algorithm 1. The function SEARCH should initially be called with an empty partial decision and a probability p0 equal to 1, the initial triv ial upper bound on the probability of an optimal decision. The functions SAVEDOMAINS and RESTOREDOMAINS save and restore the domains of the the variables (actually, only modified domains have to be saved). One should note that on line 1 of the algorithm vi can be a decision variable or a parameter. On line 3, the current upper bound Pi could easily be incrementally updated by only taking into account the parameters whose domains have been modified: the previous probability p; is simply multiplied by the rela tive decrease of the probability associated to each modified domain. The algorithm is related to existing extensions of Forward checking which have been considered in the context of valued CSP [15] and partial CSP [6].\n4 Searching for a conditional decision\nClearly, searching for an optimal conditional decision is computationally costly (since in the worst case, its size equals the number of possible worlds); this leads us to look for an algorithm which gives a conditional decision which tends eventually to an optimal one if we let the algorithm run until it stops naturally (the longer we let it run, the higher the\n172 Fargier, Lang, Martin-Clouaire, and Schiex\nprobability associated to the current conditional decision). Intuitively, the algorithm incrementally builds a conditional decision which eventually covers a superset of all good worlds. Repeatedly, (i) we pick a new pure decision d (to be added to the current partial conditional decision) that covers at least one possible world among the worlds which are not covered yet, (ii) we compute the set R of worlds that this decision covers and (iii) we subtract this set from the set of worlds which haven't yet been covered (initially, this set has the value W, i.e. it contains all worlds). In order to easily compute the worlds covered by d and their probabilities, we make the following two assumptions: first, parameters are mutually independent (again), and second, any constraint of C involves at most one parameter (F). Due to assumption {F), the worlds covered by a decision d (i.e., C[d)) form a Cartesian product of subsets of the parameter domains. The successive subtractions of these Cartesian products is performed using a technique recently proposed by Freuder and Hubbe [7], called subdomain subproblem extraction. Before we describe our algorithm, we first recall this technique of subdomain extraction.\n4.1 Subdomain extraction\nWe define an environment E as a set of worlds of the form /1 x \u00b7 \u00b7 \u00b7 x lp. with 'Vk, lk \ufffd Wk. An example of envi ronment is (A1 E {c},A2 E {c,-,c},A3 E {..,c}), also c written { c} x {c, -,c} x { -,c} or [ c,\ufffdc) when there is no ambiguity on the order of parameters. The set of all possi ble environments is obviously a lattice (equipped with the inclusion order), whose top is the set of all possible parameter assignments (in the example, [ \ufffd: :::\ufffd ] ) and whose c, -.c bottom is the empty set. The probability of an environment E = /1 x \u00b7 \u00b7 \u00b7 x lp, under the independence assumption, is Pr(E) = n\ufffd=1 LvEik pr(v). Given two environments E and R, sub-domain subprob lem extraction technique [7], decomposes E into a set of disjoint sub-environments Dec(E, R) such that all worlds of E belong either to R or to one of the sub-environments of the decomposition. The decomposition is unique if an ordering on the variables is fixed. We give here a modi fication of Freuder and Robbe's decomposition algorithm, where we also compute incrementally the probability of each environment generated by the decomposition. This function returns the decomposition of E by F and the prob ability of the set of worlds of E that are already in F (used in Algorithm 3).\nWhen actually used, the probability p E of the environment E needed on entry has already been computed since if the procedure DEC is called, either E = W and PE = 1, orE comes from a previous decomposition.\nNote that when E n F = 0 we get DEc( E, F) { (E, pE) }, and when E \ufffd F we get DEC(E, F)= 0.\nExample: E = W = [ \ufffd::::\ufffd) , F = [ c,\ufffdc). c,-,c c\n1. i = 1; Rest = [ c,':,c); PRest = 0.6, E' = [ c\ufffdc); C1oC C,\u2022C\nFunction DEC( ( E, p E), F)\nList := 0; E' := E; PE\u2022 = PE; i := 1 repeat\nRest:= E' Rest!{A;} := E!{A;}- F!{A;}\n\u00b7- p(Rest I{ >j})\nPReot \u00b7- PE'\u00b7 p(E' ) II>;} {independence} if (Rest !{A;} # 0) then l Add (\n. Rest, PRest) to List\nE!{A,} := E!{A;} n F!{A;} PE' := PE' -PRest\ni := i + 1 until ((i > p) or (E!{A;} = 0)) return (List, PE')\nAlgorithm 2: The decomposition algorithm\nPE' = 0.4; List= { ( [ c,':,c), 0.6) }; c,-,c -.c\n2. i = 2; Rest = [ c,-.c); PRest = 0; c,-,c 3. i = 3; Rest = [ c\ufffdc); PRest = 0.2, E' =\np(E' ) = 0.2; List = { ([ \ufffd:\ufffd\ufffd), 0.6), ([ c\ufffdc), 0.2) }. END\n[ c\ufffdc]\u00b7 -.c '\n4.2 An algorithm for searching a conditional decision\nThe algorithm is sketched as Algorithm 3. Two steps of the algorithm deserve some comments: first (line 1), the se quences ofCSP (AUX, W x D, CUE), repeatedly solved by the algorithm, define dynamic constraint satisfaction prob lems [ 4] and their resolution may be improved by any tech niques developed to solve such problems. Second (line 2), once a new W C is built, it is subtracted from all the await ing environments to avoid some redundant computations. This furthermore guarantees that the computation will stop when the current list Dec is ions defines a solution.\nExample ( continued) :\n1. Env = {(W, 1)}; p900d = 0; Pbad = 0; Decisions= 0\n2. (E,pE) = (W, 1); d1 = (R, B); WC = [ c2c ); Env = { ( [ \ufffd\u00b7\ufffd\ufffd), 0.6), ( [ c?c), 0.2) }; ' -.c Decisions = ( {[ c:..,-;,c), ( R, B))); Pgood = 0.2.\n3. (E,pE)={[\ufffd:\ufffd\ufffd),0.6);d2=(R,T);WC= [\ufffd\ufffd\ufffd\ufffd); Env = { ( [ c,;c), 0.3), ( [ c?c), 0.2) }; Decisions = (( [ \ufffd\ufffd\ufffd\ufffd), (R, T)), ( [ c2c), (R, B))); Pgood = 0.5. c [\ufffd-.c l 4. (E,pE)=([c.;c),o.3;d3=(W,F);WC= c:-:;c J; Env = { {[g), 0.27), {[ g\nc), 0.18) }; (( [ :\ufffd:), (W, F)) is added to Decisions; Pgood = 0.55; c [ c 5. (E,pE) = ( [g), 0.27); inconsistent; Bad = { g)};\nA constraint satisfaction framework for decision under uncertainty 173\nDecisions := 0 Env := {(W, 1)} Bad:= 0\n{decisions & env. covered} {uncovered environment}\n{ uncoverable envitonment} {lower bound of Peons (P)} {lower bound of 1- Pcon$(P)}\n1\nPgood := 0 Pbad := 0 repeat\nPick a pair (E, PE) from Env {possible heuristics} if (A U X, W x D, C U E) is inconsistent then\nI %E contains only non-coverable worlds Bad:= Bad UE; Pbad := Pbad + PE\nelse (d,w) :=a solution of (AUX, W x D,C U E) %d covers at least one possible world of E WC := C[d] {Worlds covered by D} Add (WC, d) to Decisions; Env' = 0 forGE Env do\n2 l (H,p) = DEC(G, WC) Env':= Env' U H Pgood := Pgood + P\nEnv :=Env'\nuntil (Env = 0) (or interruption by the user)\nAlgorithm 3: Computing an optimal conditional decision\nPbad = 0.27. Env = { ( [ g c], 0.18) }.\n6. (E, PE) = ( [ gc], 0.18); inconsistent; Bad = { [ g], [ gc] }; Pbad = 0.45; Env = 0. END\n( ( [ :\ufffd:), (W, F)}, ( ( \ufffd\ufffd\ufffd\ufffd), (R, T)}, ( ( c2c), (R, B))) is the final value of Decisions; from it we may built a conditional\ndecision; moreover Bad is in Bad. Pgood tends to Pcons(P) and reaches a eventually.\n4.3 Correctness of the algorithm\nProposition 2 At any point of the algorithm, the following properties hold:\n\u2022 V(E, PE ) of Envwe have PE = P1\u00b7(E);\n\u2022 VE EBadwe have E \ufffd Bad(P);\n\u2022 V(E and d) E Decisions, Vw E E, d covers w;\n\u2022 Pgood \ufffd Pr( Good(P)) \ufffd 1 - Pbad\nProposition 3 (Correctness of the algorithm) If run qui etly, the algorithm stops and :\n\u2022 the final value of Decisions defines an optimal con ditional decision;\n\u2022 the final values of Pgood and Pbad verify Pgood + Pbad = 1. Pgood = Pr(Good(C)) and Pbad = P1\u00b7(Bad(P)).\nThus, the set of worlds covered by Decisions grows monotonically (and so does Bad), and the\ninterval [p9ood, 1 - Pbad] shrinks monotonically from its initial value [0, 1] to its final singleton value [Pr(Good(C)), Pr(Good(C))]. Therefore, it is possible to use the algorithm as an \"anytime\" algorithm: Dec is ions tends to a cover of Good(P) and Bad tends to Bad(P).\nSo far we have assumed that the applicability of a con ditional decision is taken for granted (since, according to assumption (CK), the actual world will be revealed before the deadline for acting). We could consider more com plex situations where further knowledge about the world can be learned by means of knowledge-gathering actions; another problem consists then in finding a relevant set of knowledge-gathering actions, sufficient to make the agent able to act properly. This is left for further research (see (5] for further considerations on this point).\n5 Related work\nIn the field of constraint satisfaction and automated reason ing: Partial and valued CSP [6, 15] define general frame works that extends the traditional CSP framework in a sim ilar direction, but without any distinction between decision variables and parameters. Another related area of research is dynamic constraint satisfaction [4], where several tech niques are proposed in order to solve a sequence of CSP that differs only in some constraints more efficiently than by naively solving each CSP one after the other. Such improve ments could immediately be incorporated in the algorithm proposed . The computation of a compact representation of (partial) solutions could probably be performed using Fi nite Automata or Ordered Binary Decision Diagrams [2]. OBDD are used to solve problems issued from propositional logic, closely related to CSP. In fact, an OBDD could rep resent - using exponential memory and time in the worst case - all the partial solutions of a probabilistic CSP by shifting to propositional logic.\nIn the field of decision analysis: Deciding under uncertainty has been studied for long in decision theory and has been applied recently to planning. There is a number of computa tional approaches to decision analysis, which all distinguish in a way or another parameters (subject to probability distri butions) and decision variables, the most common of them include influence diagrams [9], Markov decision processes and also valuation-based systems [16]. On the one hand, these computational frameworks tackle much more general decision problems since they consider sequences of deci sions (and also utility values), while ours tackles only one step decisions. On the other hand, our framework describes the constraints between dependent variables by means of an elaborate representation language equipped with elaborate computational tools (namely constraint satisfaction prob lems) while influence diagrams and other approaches do not focus on this representation issue and represent these relationships more explicitly. Both kind of approaches are however somewhat complementary, and one could think of extending our framework so as to integrate for instance influence diagrams (for the nice representation of the de pendency structure of the decision problem) and constraint satisfaction (for the representation of the constraints be-\n174 Fargier, Lang, Martin-Clouaire, and Schiex\ntween dependent variables). An interesting particular problem related to our framework is in Qi and Poole [13] who discuss a navigation problem in U-graphs, i.e., graphs where some of the edges are un certain (the probability associated to an uncertain edge is the probability that the connection between two vertices is traversable); the static version of this problem may be encoded by a probabilistic CSP where a parameter is asso ciated to each uncertain edge4\u2022 In the field of knowledge representation: Recently, Boutilier [ 1] has proposed a logical, qualitative basis for decision the ory, distinguishing as we did between controllable and un controllable propositions; however, while we look for con ditional decisions, Boutilier's very cautious strategy looks only for a pure decision (the best one maximizing the worst possible outcome). [12] also discusses controllability issues in an abduction-based framework for planning.\n6 Concluding remarks\nOur contribution was mainly in extending the CSP frame work in order to deal with decision problems under uncer tainty, by distinguishing between controllable variables and uncontrollable parameters and by representing the knowl edge of the world by a probability distribution on the pa rameters. This framework can be considered as a first step to embed decision theory into constraint satisfaction; two other important steps in this direction will consists in con sidering utilities and sequences of decisions. Utility func tions would enable us to represent flexibility and it should not be significantly harder to embed them in our frame work: in the constraints of C, an extra field is added to each tuple, namely the utility of the corresponding variable assignment. Forbidden tuples have a utility of -oo. Non flexible constraints allow only two utility degrees for the tuples, namely 0 and -oo. Extending our framework to sequences of decisions is significantly harder. The partition of the variables is not sufficient: not only decisions are in fluenced by parameters, but the value of some parameters may also be influenced by some earlier decisions; for this we think of reusing ideas from influence diagrams [9] or causal networks (e.g [3]) by structuring decision variables and parameters in a directed acyclic network (not to be con fused with the undirected constraint graph of the CSP), a link from A; to xi meaning that the allowed values of the decision variable xi depend on ..\\;, and a link from x; to ..\\ i meaning that the decision of assigning a value to xi has or may have some effects on ..\\ i.\nAn interesting potential application of probabilistic CSP (and its possible extensions) is planning under uncertainty. A preliminary version of probabilistic CSP has been ap plied to an agricultural planning problem [ 11]. Clearly, this problematic needs the notion of controllability, and would certainly gain a lot in being encoded in an extension of the\n4However the dynamic version of the problem, where the ac tual presence of an edge can be learned by the agent only when it is at one on its extremities cannot be easily modeled in our framework yet.\nCSP framework -since it could benefit from the numer ous advances on the resolution of CSPs. The similarity between our conditional decisions and conditional plans is clear, at least for a single action. For handling sequences of actions, probabilistic constraint satisfaction has to be extended as evoked in the previous paragraph. Actions with probabilistic effects may be encoded by using extra parameters and probability distributions. Lastly, a lot of recent approaches to planning make use of decision the ory; clearly, extending probabilistic CSP with utility values and sequences of decisions goes in this direction; our long term goal is thus to provide a constraint satisfaction based framework to decision-theoretic planning.\nReferences\n[1] Craig Boutilier, Toward a logic for qualitative decision the ory, Proc. of KR'94, 75-86.\n[2] Randal E. Bryant, Symbolic Boolean Manipulation with Or dered Binary-Decision Diagrams,ACM Computing Surveys, 24,3,1992,293-318.\n[3] Adnan Darwiche and Judea Pearl, Symbolic Causal Net works, Proc. AAA/'94, 238-244.\n[4] Rina Dechter and Avi Dechter, Belief Maintenance inDy namic Constraint Networks, Proceedings of AAAI' 88, 37-42\n[5) Hellme Fargier, Jerome Lang, Roger Martin-Clouaire and T homas Schiex, A constraint satisfaction framework for de cision under uncertainty, Tech. Report IRIT, University of Toulouse, France, June 1995.\n[6] Eugene Freuder, Partial constraint satisfaction, Proceedings of /JCAI' 89, 278-283.\n[7] Eugene Freuder and Paul Hubbe, Extracting constraint sat isfaction subproblems, Proceedings of /JCA/'95.\n[8] R. M. Haralick and G. L. Elliot. Increasing tree search efficiency for constraint satisfaction problems. Artificial In telligence, 14, 263-313, 1980.\n[9) R.A. Howard and J.E. Matheson, Influence Diagrams, In fluence Diagrams, in R.A. Howard and J.E. Matheson, eds., The Principles and Applications of Decision Analysis, vol.2 (1984), 720-761.\n[10] Alan K. Mackworth, Consistency in networks of relations, Artificial Intelligence, 8, 1977,99-118.\n[11) R. Martin-Clouaire and J.P. Rellier, Crop management plan ning as a fuzzy and uncertain constraint satisfaction problem, Artificial Intelligence Applications in Natural Resources, Agriculture and Environmental Science, 9, (1 ), 1995.\n[12) David Poole and Keiji Kanazawa, An abductive framework for decision-theoretic planning, AAAI Spring Symposium on Decision Theoretic Planning, Stanford, March 1994.\n[13) Runping Qi and David Poole, High level path planning with uncertainty, Proceedings of Uncertainty in AI' 91, 287-294.\n[14) Franyois Rabelais, <Euvres completes, Collection \"La plei ade\", 1965, Gallirnard. Original ed.1535 (Pantagruel).\n[15] Thomas Schiex, Helene Fargier and Gerard Verfaillie, Val ued constraint satisfaction problems: hard and easy prob lems, Proc. of /JCA/'95.\n[16] Prakash P. Shenoy, Valuation-based systems for Bayesian decision analysis, Operations Research 40 (92), 3, 463-484."}], "references": [{"title": "Symbolic Boolean Manipulation with Or\u00ad dered Binary-Decision  Diagrams,ACM Computing Surveys,  24,3,1992,293-318", "author": ["Randal E. Bryant"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "A constraint satisfaction framework", "author": ["Hellme Fargier", "Jerome Lang", "Roger Martin-Clouaire", "T homas Schiex"], "venue": "Belief Maintenance inDy\u00ad namic Constraint Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "Two steps of the algorithm deserve some comments: first (line 1), the se\u00ad quences ofCSP (AUX, W x D, CUE), repeatedly solved by the algorithm, define dynamic constraint satisfaction prob\u00ad lems [ 4] and their resolution may be improved by any tech\u00ad niques developed to solve such problems.", "startOffset": 193, "endOffset": 197}, {"referenceID": 1, "context": "Another related area of research is dynamic constraint satisfaction [4], where several tech\u00ad niques are proposed in order to solve a sequence of CSP that differs only in some constraints more efficiently than by naively solving each CSP one after the other.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "The computation of a compact representation of (partial) solutions could probably be performed using Fi\u00ad nite Automata or Ordered Binary Decision Diagrams [2].", "startOffset": 155, "endOffset": 158}], "year": 2011, "abstractText": "The Constraint Satisfaction Problem (CSP) framework offers a simple and sound basis for representing and solving simple decision problems, without uncertainty. This paper is devoted to an extension of the CSP frame\u00ad work enabling us to deal with some decisions problems under uncertainty. This extension relies on a differen\u00ad tiation between the agent-controllable decision vari\u00ad ables and the uncontrollable parameters whose values depend on the occurrence of uncertain events. The un\u00ad certainty on the values of the parameters is assumed to be given under the form of a probability distribution. Two algorithms are given, for computing respectively decisions solving the problem with a maximal proba\u00ad bility, and conditional decisions mapping the largest possible amount of possible cases to actual decisions.", "creator": "pdftk 1.41 - www.pdftk.com"}}}