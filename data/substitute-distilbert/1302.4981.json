{"id": "1302.4981", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "A New Pruning Method for Solving Decision Trees and Game Trees", "abstract": ", main goal of this argument is to describe a new pruning method for solving decision trees and game data. the pruning method for decision logic suggests a slight variant of decision trees that we call scenario trees. in scenario trees, we don't need a conditional probability for each edge emanating from a chance node. instead, we require hierarchical joint table guiding each path from the root node to a leaf node. we compare the pruning method to the traditional rollback method for decision analysis within game trees. for problems that ignore bayesian revision graph probabilities, a sparse tree representation with the pruning method is farther efficient than a log tree representation with the rollback method. for game trees, the pruning method is more versatile than the rollback method.", "histories": [["v1", "Wed, 20 Feb 2013 15:23:29 GMT  (351kb)", "http://arxiv.org/abs/1302.4981v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prakash p shenoy"], "accepted": false, "id": "1302.4981"}, "pdf": {"name": "1302.4981.pdf", "metadata": {"source": "CRF", "title": "A New Pruning Method for Solving Decision Trees and Game Trees", "authors": ["Prakash P. Sbenoy"], "emails": ["p-shenoy@ukans.edu"], "sections": [{"heading": null, "text": "1 INTRODUCTION The main goal of this paper is to describe a new pruning method for solving decision trees and game trees. Decision trees and game trees are graphical methods for representing Bayesian decision problems.\nThe decision tree representation method was formulated by Raiffa and Schlaifer [ 1961] based on von Neumann and Morgenstern's [1944] extensive form game representation of n-person games. In extensive form games, information available to the decision makers is encoded using information sets. In decision trees, however, information about uncertainties is encoded by sequencing of the chance and decision variables. One consequence of this encoding is that Bayesian revision of probability models may be required before decision problems can be represented as decision trees. Decision trees are solved using the rollback method in which decision and chance nodes in the tree are pruned recursively starting from nodes adjacent to the leaves. Chance nodes are pruned by averaging the utilities at the ends of its edges, and decision nodes are pruned by maximizing the utilities at the ends of its edges. Recently, Shenoy [1993b] has suggested using information sets to encode information constraints in decision problems. The resulting representation is called a\ngame tree. Game trees generalize decision trees in the sense that a decision tree is a game tree in which all information sets are singletons. The rollback method of decision trees generalizes to game trees [Shenoy 1993b]. An advantage of game trees over decision trees is that if we have a Bayesian network model [Pearll988] for the uncertainties in a problem, then no preprocessing is ever required before a problem can be represented as a game tree. This is because information sets allow us to sequence the chance variables in the game tree as in the Bayesian network model. In this paper, we propose a new pruning method to solve decision trees and game trees. The pruning method for decision trees is as follows. First we compute the path probabilities for each path from the root node to a leaf node by simply multiplying all the probabilities on the path. Next, we compute the weighted utility for eac\ufffd \ufffdeaf node by multiplying the utility and the path probabthty. Next we prune all nodes starting from the nodes adjacent to leaf nodes and proceeding toward the root node as in the rollback method. Chance nodes are pruned by adding the weighted utilities at the ends of its edges, and decision nodes are pruned by maximizing the weighted utilities at the ends of its edges. This pruning method generalizes to game trees. Notice that in the pruning method for decision trees, the probabilities on the chance edges are used only to compute the path probabilities (they are not used to prune chance nodes). The path probabilities are in fact joint probabilities. In decision problems that require a Bayesian revision of probabilities, we can avoid computation of the conditionals by simply computing the joint probability distribution and using the joint probabilities as path probabilities. Since conditionals are not necessary, we do not have to include them in the representation. We call this slight variant of decision trees scenario trees. For decision problems that involves Bayesian revision of probabilities, a scenario tree representation solved using the pruning method requires less computation than a decision tree representation solved using the rollback method. For solving game trees, the pruning method requires less computation than the rollback method. An outline of the remainder of the paper is as follows.\nA New Pruning Method for Solving Decision Trees and Game Trees 483\nTable I. The Physician's Utility Function For All Act-State Pairs symptom s. And for patients\nknown not to suffer from p, 20% have symptom s. Let variable D denotes the presence or absence of disease d, let variable P denotes the presence or absence of pathological state p, and let variable S denote the presence or absence of symptom s. We assume D and S are conditionally independent given P. Table I shows the physician's utility function. Physician's Events Utilities Has pathological state (p) No pathological state (-p) (u) Has disease No disease Has disease No disease (d) (-d) Treat (t) 10 6 Acts Not treat ( -t) 0 2 Section 2 describes a medical diagnosis (MD) problem, a small symmetric decision problem involving Bayesian revision of probabilities. Section 3 describes a strategy matrix representation and solution of the MD problem. Section 4 describes a decision tree representation and solution of the MD problem. Section 5 describes the scenario tree representation and the new pruning method. Section 6 describes a game tree representation of the MD problem and its solution using the rollback method and the pruning method. Section 7 describes a comparison of the different techniques. Finally, in section 8, we conclude.\n2 A MEDICAL DIAGNOSIS PROBLEM In this section, we give a statement of a medical diagnosis (MD) problem [Shenoy 1994]. The MD problem is a simple symmetric decision problem that involves Bayesian revision of probabilities. We will use this problem to illustrate the different representation and solution techniques.\nA physician is trying to devise a policy for treating patients suspected of suffering from a disease d. d causes a pathological state p that in turn causes symptom s. The physician first observes whether or not a patient has symptom s. Based on this observation, she either treats the patient (for d and p) or not. The physician's utility function depends on her decision to treat or not, the presence or absence of disease d, and the presence or absence of pathological state p. The prior probability of disease d is I 0%. For patients known to suffer from d, 80% suffer from pathological state p. On the other hand, for patients known not to suffer from d, 15% suffer from p. For patients known to suffer from p, 70% exhibit\n(d) 8\n1\n(-d) 4\n10\n3 STRATEGY MATRICES\nIn this section, we describe the strategy matrix representation and solution technique for decision problems. A strategy matrix representation of a decision problem is derived from von Neumann and Morgenstern's [1944] normal form representation of ann-person game.\nThe main task in the MD problem is to determine an optimal strategy. Since the physician observes the symptom before making a diagnosis, she has four distinct strategies as follows: o1 = (t, t) meaning choose T = t if S = s, and choose T = t if S = -s (i.e., choose t regardless of the observed value of S), 02 = (t, -t), 03 = ( -t, t), and o4= (-t, -t). There are eight possible events in the joint space of D, P, and S: (d, p, s), (d, p, -s), (d, -p, s), (d, -p, -s), ( -d, p, s), ( -d, p, -s), ( -d, -p, s), and ( -d, -p, -s). In a strategy matrix representation, each strategy constitutes a row of the matrix, each event constitutes a column, and for each strategy-event pair, there is a unique utility value determined by the utility function. For example, consider the pair ((t, -t), ( -s, p, d)). Since S = -s, the strategy (t, -t) specifies T = -t. Accordingly, the utility associated with this pair is the utility value u( -t, p, d) = 0. Table IT shows a strategy matrix representation of the MD problem. (In Table IT, the expected utilities shown in bold are computed during the solution phase and are not part of the strategy matrix representation.)\nThe probabilities of all event are obtained by computing the joint probability distribution of all random variables in the problem (see left-hand side of Figure I). Solving a strategy matrix representation is straightforward. For each strategy, we compute its\n484 Shenoy\nexpected utility. Next, we identify an optimal strategy by identifying the maximum expected utility. In the MD problem, the expected utility of each strategy is shown in the last column in Table IT. The maximum expected utility is 7.998. Thus the optimal strategy is (t, -t), i.e., treat if S = s, and not treat if S = -s.\n4 DECISION TREES In this section, we describe a decision tree representation and solution of the MD problem. See Raiffa [1968] for a primer on the decision tree representation and solution method. Decision Tree Representation. Figure 1 shows the preprocessing of probabilities that has to be done before we can complete a decision tree representation of the MD problem. In the probability tree on the left, we compute the joint probability distribution by multiplying the conditionals. For example, Pr(d, p, s) = Pr(d) Pr(p I d) Pr(s lp) = (.10)(.80)(.70) = .0560. In the probability tree on the right, we compute the desired conditionals by additions and divisions.\nFigure 2 shows a complete decision tree representation of the Medical Diagnosis problem. (In Figure 2, the utility values shown in bold are computed during the solution phase and are not part of the representation.) Each path from the root node to a leaf node represents a scenario. This tree has 16 scenarios. The tree is symmetric, i.e., each scenario includes the same four variables S, T, P, and D, in the same sequence STPD.\nRollback Method. Starting from the leaves, we recursively delete all random and decision variable nodes in the tree. We delete each random variable node by averaging the utilities at the end of its edges with the probability distribution at that node (\"averaging out\"). We delete each decision variable node by maximizing the utilities at the end of its edges (\"folding back\"). This method is called rollback. The results of the rollback method for the MD problem are shown in Figure 2.\nFigure 1. The Preprocessing of Probabilities in the MD Problem\n\u00b7- d\n.ow\n.0040\n.0160\n.6945\n.0405\n.1530\n.6120\n. 0560\n.0!145\n.0040\n.1530\n.0240\n.0405\n.0160\n.6120\n5 SCENARIO TREES In this section, we describe the scenario tree representation and solution technique .\nA scenario tree representation of the MD problem is shown in Figure 3. (In Figure 3, the utility values shown in bold are computed during the solution phase and are not part of the representation.) A scenario tree is slightly different from a decision tree. In a scenario tree, we do not have to specify probabilities for each edge emanating from a chance node. Instead we have to specify probabilities for each scenario, i.e., for each path from the root node to a leaf node. We call these probabilities path probabilities. For a leaf node, the path probability represents the conditional probability of reaching the leaf node conditional on a DM' s\nA New Pruning Method for Solving Decision Trees and Game Trees 485\nFigure 3. A Scenario Tree Representation and Solution of the MD Problem a strategy, then 1t\u00ae\ufffdY is a m-obability distribution function, i.e., 2,;{ (1t\u00ae\ufffdy)(x) I XE\ufffd}=l. Path Weighted Utility Probability Utility Let u: \ufffd \ufffd lR denote the utility function,\ni.e., u(x) is the DM's utility under scenario x. Consider the product 1t\u00aeu: \ufffd \ufffd lR defined as follows: (1t\u00aeu)(x) = 1t(x)u(x) for all xe \ufffd-We call1t\u00aeU the weighted utility function . 10 .0560 6 .0945\n8 . 0040\n4 .1530\n0 .0560\n2 .0945\n.0040\n10 .1530\n10 .0240\n6 . 0405\n8 .0160\n4 .6120\n0 .0240\n2 .0405\n.0160\n10 .6120\nstrategy that makes the leaf node reachable. This probability is simply the conditional probability of all events on the path conditional on all acts on the path. For example, the probability of the top-most path in the scenario tree of Figure 3 is Pr(S = s, P = p, D = d I T= t). Let \ufffd denote the set of all scenarios. Consider the probability function 1t: \ufffd \ufffd [0, 1] that assigns the path probability 1t(x) to each scenario x e \ufffd. We call the function 1t the path probability function. Consider a strategy y available to the DM. We can encode y as a function \ufffdY: \ufffd \ufffd {0, 1 }such that \ufffdy(x) = 0 if scenario x has a zero probability of occurring, and \ufffdy(x) = 1 otherwise. We call \ufffdY a strategy function corresponding to strategy y. Given a strategy function \ufffd,Y\u00b7 we can define the product 1t\u00ae\ufffdy as the function 1t\u00ae<;y: \ufffd \ufffd [0, 1] such that (1t\u00ae\ufffdyJ(x) = 1t(x) \ufffdy(x) for each x e \ufffd. A path probability function 1t has the following property. If y is\nB.560\nB.567\nB.03l\nB.61l\n0\nB-189\nB.004\n1.530\nB.l40\nB.l43\nB.1l8\n1.448\n0\n.081\n.016\n6.1l\nIf the DM chooses strategy y, then the DM's expected utility is 1'.{ ((1t\u00ae\ufffdy)\u00aeu)(x) I x e \ufffd}. Notice that (1t\u00ae\ufffdy)\u00aeu = (1t\u00aeU)\u00ae\ufffdy\u00b7 Thus the problem can be stated as follows: Find strategy y so as to maximize L{ ((1t\u00aeu)\u00ae\ufffdy)(x) I x e \ufffd}. The pruning method that we will now describe solves precisely this problem using deterministic dynamic programming.\nThe Pruning Method. First, for each leaf node, we compute the weighted utility by multiplying the utility and the path probability. Hence forth, we use the weighted utilities for pruning chance and decision nodes .\nPruning Chance Nodes. If we have a chance node all of whose edges end in leaf nodes, first we compute the sum of the weighted utilities at the end of its edges, and next we replace the subtree associated with the chance node by a payoff node whose value is set equal to the computed sum of weighted utilities. Pruning Decision Nodes. If we have a decision node all of whose edges end in payoff nodes, first we compute the maximum of the weighted utilities, and next we replace the subtree associated with the decision node by a payoff node whose value is set at the computed maximum\nvalue of weighted utilities. Each time we prune a decision node, we keep track of a value of the decision node where the maximum is achieved.\nWhen we have pruned all decision nodes, we have an optimal strategy. When we have pruned all decision and chance nodes, we end with one payoff node whose utility value is the expected utility of an optimal strategy. This pruning method is illustrated in Figure 3.\n6 GAME TREES The game tree representation technique for decision problems is described in [Shenoy 1993b].lt is based on von Neumann and Morgenstern's [1944] extensive form representation of n-person games. Figure 4 shows a game tree representation of the MD problem. (In Figure 4, the utilities shown in bold are computed during the solution phase and are not part of the game tree representation.)\n486 Shenoy\nGame trees are similar in many ways to decision trees. The main difference between the two representation techniques is the encoding of information constraints. In decision tree, information constraints are encoded in the sequencing of chance and decision nodes in each scenario. Thus if the value of a chance variable R is known to the decision maker when she chooses a value of a decision variable N, then R must precede Non the paths from the root node to leaf nodes. In game trees, information constraints are encoded by information sets. Information sets are a partition of the set of decision nodes in a game tree. When faced with a decision, a decision maker knows which information set she is in, but does not know which node in an information set she is at. In Figure 4, there are two information sets I 1 and l2 each containing 4 instances of node T. In information set I 1, the physician knows that S = s, but not the values of D and P. In information set l2, the physician knows that S = -s, but not the values of\nD and P. In game trees, the sequence of variables on a path from the root node to a leaf node need not represent information constraints. Instead, the sequence may represent time, causation, etc. If we have a causal probability model (as we do in the MD problem), and we use the sequence to represent causation, then no preprocessing is required to represent a decision problem as a game tree. In Figure 4, for example, all the probabilities on edges are specified in the statement of the MD problem. The Rollback Method for Game Trees. The decision tree rollback method generalizes to game trees [Shenoy 1993b]. We start from the neighbors of payoff nodes and go toward the root until the entire tree has been pruned. !he rule for pruning chance nodes is exactly the same as m the rollback procedure of decision trees. The rule for pruning decision nodes is slightly different from the rollback procedure of decision trees.\nFigure 4. A Game Tree Representation and Solution of the MD Problem Suppose we have an information set such that the edges le\ufffdding out of the decision nodes end at payoff nodes. Ftrst, we compute the conditional probability distribution on the decision nodes of the information set conditioned on the event that we have reached the information set (the details of this step are explained Utility\n\ufffd-\ufffd--10\n10\n0 8\n8\n6\n2\n6\n2\n4\n10\n4\n10\nin the following paragraph). Second, for each value of the decision variable associated with the in\ufffdormation set, we compute the expected payoff usmg the payoffs at the end of corresponding edges and using the conditional distribution computed in the first step. Third, we identify a value of the decision variable (and the corresponding edges of each decision node) associated with the maximum expected payoff. Fourth, we prune each decision node by replacing the corresponding subtree by a payoff node whose payoff is equal to the payoff at the end of its edge identified in step 3. We call this technique (for pruning decision nodes in an information set) pruning by maximization of conditional expectation. Pruning by maximization of conditional expectation generalizes the method of pruning a singleton information set by maximization.\nComputing the conditional distribution for an information set is easy. For each decision node in the information set, we simply multiply all probabilities on the chance edges in the path from the root to the decision node. This gives an unconditional distribution on the nodes in the i\ufffdformation set. The sum of these probabilities gtves us the probability of reaching the information set assuming prior decisions that allow us to get there. To compute the conditional distribution, we n?\ufffd\ufffdlize this unconditional distribution by dlVldmg by the sum of the probabilities. From a computational perspective, the normalization step is unnecessary and can be dispensed with. We will illustrate pruning decision nodes by conditional expectation for the game tree representation of the MD problem. Consider\nA New Pruning Method for Solving Decision Trees and Game Trees 487\ninformation set 11 in Figure 4. Notice that all four decision nodes in I 1 have edges leading to payoff nodes.\nThe unnormalized distribution for the three nodes in I 1 (from top to bottom) is given by the probability vector (0.1*0.8*0.7, 0.1*0.2*0.2, 0.9*0.15*0.7, 0.9*0.85*0.2) = (0.0560, 0.0040, 0.0945, 0.1530). Thus for T = t, the (unnormalized) expected payoff is (0.0560 *10) + (0.0040*8) + (0.0945*6) + (0.1530*4) = 1.771, and forT = -t, the expected payoff is (0.0560*0) + (0.0040*1) + (0.0945*2) + (0.1530*10) = 1.723. Since 1.771 > 1.723, we identify edge T = t with each node in I 1 . The rollback method is illustrated in Figure 4. The Pruning Method for Game Trees. Next, we describe a generalization of the pruning method described in the previous section so that it applies to game trees. First, we compute path probabilities and weighted utilities for all payoff nodes. The rule for pruning chance nodes is exactly the same as in decision trees. The rule for pruning decision nodes is as follows.\nscenario trees. The computational efficiencies of the rollback method and the pruning method are compared in the next section.\n7 COMPARISON In this section, we compare scenario trees and game trees to strategy matrices and decision trees. Also we compare the pruning method of scenario trees and game trees to the rollback method of decision trees and game trees.\nStrategy Matrices. The strategy matrix representation involves considerable preprocessing. Not only do we have to compute the joint probability distribution of all chance variables, we also have to identify all possible strategies.\nThe number of configurations in the frame of all chance variables is an exponential function of the number of chance variables, and the number of strategies is an exponential function of the number of decision nodes in a\nPruning Decision Nodes. Suppose we have an information set such that the edges leading out of the decision nodes end at payoff nodes. First, for each value of the decision variable, we compute the sum of the weighted utilities at the end of the respective edges of the decision nodes in the information set. Second, we identify a value of the decision variable that has the maximum sum of weighted utilities computed in the first step.\nThird, we prune each decision node by replacing the corresponding subtree by a payoff node whose value is equal to the weighted utility at the end of its edge corresponding to the value of the decision variable identified in the second step.\nWe will illustrate the method for pruning decision nodes in game trees using the MD problem. Consider decision nodes in information set I 1, For T = t, the sum of the weighted utilities is 0.560 + 0.032 + 0.567 + 0.612 = 1.771, and forT= -t, the sum of the weighted utilities is 0 + .004 + 0.189 + 1.530 = 1.723. Since 1.771 > 1.723, we identify edge T = t with each node in I 1. The results of executing the pruning method for the MD game tree are shown in Figure 5.\nThe correctness of this method follows from exactly the same arguments as in the case of\n488 Shenoy\ndecision tree. Thus, representation and solution of strategy matrices involve global computation (on the space of configurations of all chance variables) to compute the joint probability distribution, and global computation (on the space of all strategies) to compute an optimal strategy. Strategy matrices have some advantages. They can be used to define optimal strategies. Also, the solution method of strategy matrices yields not only the utility of an optimal strategy but also the utilities of all strategies. For example, in the MD problem, the strategy ( -t, -t) has a utility of 7.940 that is only marginally lower than the utility of the optimal strategy.\nIn the MD problem, representation and solution of the strategy matrix (shown in Table II) involves a total of 75 operations (where we count each addition, multiplication, division, and comparison as an operation). A breakdown is as\nfollows. Computing the joint probability distribution involves 12 operations, and computing the maximum expected utility involves 63 operations (15 operations to compute the expected utility of each strategy and 3 operations to identify the maximum).\nIn general, suppose we have a symmetric decision problem with m chance variables each of which has 2 values, with n decision variables each of which has 2 values, and with 2k distinct strategies 1. Then representing and solving the problem using the strateyr matrix method requires a total of approximatelf 2m+k+ + 2m+ 1 operations (approximately 2m+ to compute the joint probability distribution, 2m+ 1 - 1 to compute the expected utility of each strategy, and approximately 2k to identify an optimal strategy).\nDecision Trees. The decision tree representation requires conditional probability distributions for each chance node in a decision tree. If we compute the conditional probability distributions from the joint probability distribution (as described in Figure I), then this involves global computation on the space of all configurations. The rollback method however computes an optimal strategy using local computation. In the MD problem, representation and solution of the decision tree (shown in Figure 2) involve a total of 71 operations. A breakdown of the total is as follows. Computing the conditional probability distributions as\n1 k denotes. for example, the number of decision nodes in a decision tree representation, or the number of information sets in a game tree representation. The value of k depends on the information constraints. It can be easily shown that zn - I ::;; k ::;; zm+n - zm.\nshown in Figure I involves 30 operations, and executing the rollback method as shown in Figure 2 involves 4I operations (3 operations for each of the chance nodes and I operation for each of the decision nodes). The decision tree representation and solution technique can be made more efficient as follows. First, we can use the arc-reversal method [Olmsted I983, Shachter 1986] to compute the conditional probability distributions. The arc reversal method uses local computation. This is illustrated in Figure 6. Only 20 operations are needed to compute the requisite conditionals using the arc-reversal method. Second, the decision tree solution can be made more efficient by identifying coalescence [Olmsted I983]. In the MD problem, by using coalescence, we can simplify the decision tree as shown in Figure 7. The rollback method can be executed for the coalesced decision tree using only 23 operations (3 operation for each chance node and I operation for each decision node). Thus by using the arc reversal method and coalescence, we can represent and solve the MD problem using only 43 operations. We are assuming here that identifying coalescence is done during the representation phase by the modeler. In general, automating coalescence in decision trees is not easy. However, coalescence is automatically achieved in influence diagrams and valuation networks [Shenoy I994]. In influence diagrams and valuation networks, coalescence is achieved by local computation. The arc-reversal method of influence diagrams solves the MD problem using 43 operations, and the fusion algorithm of valuation networks [Shenoy I992, 1993a] solves the MD problem using 3I operations [Shenoy 1994].\nScenario Trees. Scenario trees do not require conditional probability distributions. However, we do\nA New Pruning Method for Solving Decision Trees and Game Trees 489\nrequire path probabilities. In the MD problem, computation of the path probabilities is achieved by computing the joint distribution of all chance variables. This involves 12 operations. Executing the pruning method for the scenario tree representation shown in Figure 3 involves a total of 31 operations (16 operations to compute the weighted utilities, and I operation per decision and chance node). Thus a total of 43 operations are required to represent and solve the MD problem using the scenario tree method.\nIn general, the scenario tree method involves working on the global space of all configurations. Unlike decision trees, we cannot use either arc-reversal or coalescence to make the representation and solution more efficient. The pruning method of scenario trees does use local computation to identify an optimal strategy and compute its utility. Suppose that we have a symmetric decision problem with m chance variables and n decision variables, suppose that each variable has two values, and suppose that all conditional probabilities required in the decision tree representation are specified in the problem. A decision tree representation would have a total of 2m+n -1 decision and chance nodes, and 2m+n leaf nodes. Suppose that the decision tree representation has approximately the same number of decision and chance nodes, say 2m+n-1 chance nodes and 2m+n-1 decision nodes. Then the rollback method would require a total of approximately 2m+n+ 1 operations (3 operations to prune each chance node and I\noperation to prune each decision node), and the pruniny method would require a total of approximately 2m+n+ + 2m+ 1 operations (2m+ 1 to compute the joint probability distribution, 2m+n to compute the weighted utilities, 1 operation to prune each chance node and 1 operation to prune each decision node). In this case, the rollback method is more efficient than the pruning method. Now suppose that we have the same decision problem as above except that Bayesian revision of probabilities is required before the problem can be represented as a decision tree. In this case, if we compute the required conditionals by computing the joint, this would require a total of approximately 2m+n+l + 2m+2 +2m operations (2m+ 1 operations to compute the joint, 2m+ 1 + 2m operations to compute the required conditionals, and 2m+n+1 operations for rollback). If we use a scenario tree representation with the pruning method, then we would need a total of approximately 2m+n+ 1 + 2m+ 1 operations (2m+ 1 to compute the joint probability distribution, 2m+n to compute the weighted utilities, and 2m+n operations to prune the nodes).ln this case, the scenario tree representation with the pruning method is more efficient than the decision tree with the rollback method.\nGame Trees. If we assume that we have a causal probability model for all chance variables, then unlike strategy matrices, decision trees, and scenario trees, game trees do not require any preprocessing. Executing the rollback method for the MD game tree (see Figure 4) requires 63 operations (12 to compute conditional probabilities of reaching decision nodes in information sets, I5 operation to prune each information set, and 3 operations to prune each chance node).ln comparison, executing the pruning method for the MD game tree (see Figure 5) requires only 49 operations (12 operations to compute the path probabilities, I6 operations to compute the weighted utilities, 7 operations to prune the decision nodes in each of 2 information sets, and I operation to prune each of 7 chance nodes). In general, suppose we have a game tree with m chance variables each of which has 2 values, with n decision variables each of which has 2 values, with 2m+n-1 chance nodes, and with 2m+n-1 decision nodes in k information sets. If we use the rollback method to solve the game tree, we would need a total of approximately 2m+n+ r + 2m+n + zm+n-1 + 2m+1 operations (2m+1 operations to compute the unnormalized conditional probability distributions at each information set, 2m+n+l to prune the k information sets, and 3(2m+n-1) operations to prune the 2m+n-! chance nodes). If we use the pruning method described in this paper, then we need a total of approximately 2m+n+l + zm+n-1 +2m+ I operations (zm+l operations to compute the path probabilities, 2m+n operations to compute the weighted utilities, 2m+n to prune the k information sets, and 2m+n-I operations to prune the 2m+n-I chance nodes). Thus the pruning method is more efficient than the rollback method for game trees.\n490 Shenoy\nTable III. A Comparison of Computational Efficiency of Different Techniques for the MD Problem Acknowledgments\n#Operations ( +, x; +, \ufffd) Representation in MD Problem (Preprocessing)\nStrategy Matrix with Expected Values 12\nDecision Tree with Rollback 30\nScenario Tree with Pruning 12\nGame Tree with Rollback 0\nGame Tree with Pruning 0\nInfluence Diagram with Arc Reversal 0\nSolution TOTAL\n63 75\n41 71\n31 43\n63 63\n49 49\n49 49\nI am grateful to Rui Guo for comments and discussions.\nReferences\nLuce, R. D. and H. Raiffa (1957), Games and Decisions: Introduction and Critical Survey, John Wiley & Sons, New York, NY.\nOlmsted, S. M. (1983), \"On representing and solving decision problems, \" Ph.D. thesis, Department of Engineering Economic Systems, Stanford University, Stanford, CA.\nPearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\nInference, Morgan Kaufmann,\nTable ill summarizes the computational efficiencies of the different techniques for the MD problem, and Table IV summarizes the computational efficiencies of the different techniques in general\n8 CONCLUSION The main objective of this paper was to introduce a new pruning method to solve decision trees and game trees. In decision problems requiring Bayesian revision of probabilities for a decision tree representation, the new pruning method suggests a slight variant of decision trees that we call scenario trees. For such problems, the scenario tree representation with the pruning method is more efficient than the decision tree representation with the rollback method (assuming we compute the conditionals from the joint). The pruning method generalizes to game trees and it is more efficient than the rollback method for game trees.\nsolving Bayesian decision problems, \" in D. J. Hand (ed.), Artificial Intelligence Frontiers in Statistics: AI and Statistics III, 119-138, Chapman & Hall, London. Shenoy, P. P. (1993b), \"Game trees for decision analysis, \" Working Paper No. 239, School of Business, University of Kansas, Lawrence, KS. An 8-page summary of this paper appeared as \"Information sets in decision theory, \" in M. Clarke, R. Kruse and S. Moral (eds.), Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Lecture Notes in Computer Science No. 747, 1993, 318-325, Springer-Verlag, Berlin.\nShenoy, P. P. (1994), \"A comparison of graphical techniques for decision analysis, \" European Journal of Operational Research, 78(1), 1-21.\nvon Neumann, J. and 0. Morgenstern (1944), Theory of Games and Economic Behavior, 1st edition, Princeton University Press, Princeton, NJ."}], "references": [{"title": "Games and Decisions: Introduction and Critical Survey", "author": ["R.D. Luce"], "venue": "Raiffa", "citeRegEx": "Luce and H.,? \\Q1957\\E", "shortCiteRegEx": "Luce and H.", "year": 1957}, {"title": "On representing and solving decision problems, ", "author": ["S.M. Olmsted"], "venue": "Ph.D. thesis,", "citeRegEx": "Olmsted,? \\Q1983\\E", "shortCiteRegEx": "Olmsted", "year": 1983}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, Table IV. A Comparison of Computational Efficiencies of Different Techniques", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Evaluating influence diagrams, ", "author": ["R.D. Shachter"], "venue": "Operations Research,", "citeRegEx": "Shachter,? \\Q1986\\E", "shortCiteRegEx": "Shachter", "year": 1986}, {"title": "Valuation-based systems for Bayesian decision analysis, ", "author": ["P.P. Shenoy"], "venue": "Operations Research,", "citeRegEx": "Shenoy,? \\Q1992\\E", "shortCiteRegEx": "Shenoy", "year": 1992}, {"title": "A comparison of graphical techniques for decision analysis, ", "author": ["P.P. Berlin. Shenoy"], "venue": "European Journal of Operational Research,", "citeRegEx": "Shenoy,? \\Q1994\\E", "shortCiteRegEx": "Shenoy", "year": 1994}], "referenceMentions": [{"referenceID": 3, "context": "Recently, Shenoy [1993b] has suggested using information sets to encode information constraints in decision problems.", "startOffset": 10, "endOffset": 25}, {"referenceID": 5, "context": "In this section, we give a statement of a medical diagnosis (MD) problem [Shenoy 1994].", "startOffset": 73, "endOffset": 86}, {"referenceID": 4, "context": "The game tree representation technique for decision problems is described in [Shenoy 1993b].lt is based on von Neumann and Morgenstern's [1944] extensive form representation of n-person games.", "startOffset": 78, "endOffset": 144}, {"referenceID": 5, "context": "The arc-reversal method of influence diagrams solves the MD problem using 43 operations, and the fusion algorithm of valuation networks [Shenoy I992, 1993a] solves the MD problem using 3I operations [Shenoy 1994].", "startOffset": 199, "endOffset": 212}], "year": 2011, "abstractText": "The main goal of this paper is to describe a new pruning method for solving decision trees and game trees. The pruning method for decision trees suggests a slight variant of decision trees that we call scenario trees. In scenario trees, we do not need a conditional probability for each edge emanating from a chance node. Instead, we require a joint probability for each path from the root node to a leaf node. We compare the pruning method to the traditional rollback method for decision trees and game trees. For problems that require Bayesian revision of probabilities, a scenario tree representation with the pruning method is more efficient than a decision tree representation with the rollback method. For game trees, the pruning method is more efficient than the rollback method.", "creator": "pdftk 1.41 - www.pdftk.com"}}}