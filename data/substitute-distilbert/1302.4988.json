{"id": "1302.4988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Defaults and Infinitesimals: Defeasible Inference by Nonarchimedean Entropy-Maximization", "abstract": "we develop a new semantics encompassing defeasible inference based on particular probability measures allowed to take conditional values, on the interpretation of defaults as generalized conditional regression constraints and on a preferred - model implementation of entropy maximization.", "histories": [["v1", "Wed, 20 Feb 2013 15:24:05 GMT  (597kb)", "http://arxiv.org/abs/1302.4988v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["emil weydert"], "accepted": false, "id": "1302.4988"}, "pdf": {"name": "1302.4988.pdf", "metadata": {"source": "CRF", "title": "Defaults and lnfinitesimals Defeasible Inference by Nonarchimedean Entropy Maximization", "authors": ["Emil Weydert"], "emails": ["@mpi-sb.mpg.de"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nProbabilistic and default inference, emphasizing either fme- or coarse-grained uncertainty management, scienti fic or commonsense analysis, constitute the cornerstones of plausible reasoning. Probabilistic approaches are based on a well-established, largely canonical mathematical framework and have a long tradition of successful ap plications in numerous areas. In addition, they have been recently strengthened by more qualitative accounts allow ing an efficient encoding and exploitation of structural in formation, e.g. about independencies and causal relation ships. Default formalisms, on the other hand, have a less impressive record. In fact, after fifteen years of intensive research, the discipline is still characterized by a confusin gly large number of competing, partly quite isolated pro posals, which are often ad hoc, poorly motivated and fre quently unable to produce either answers we would expect or at least explanations why our expectations would be misleading. Nevertheless, there are lots of inferential tasks involving uncertainty - more or less efficiently executed by humans - where standard numerical representations and strategies turn out to be computationally expensive, cumbersome, non-intuitive or even meaningless. So, there is still a strong feeling that we need the concept of a default for practical reasoning and, hopefully, that we should also be able to make explicit our corresponding intuitions.\nThe most promising approaches - at least as far as foun dational coherence and arguably correct behaviour are concerned- appear to be those derived from the probabi listic paradigm, e.g. ranking measures for interpreting default conditionals [Wey 91, 94] and revisable belief strengths [Spo 90], and the limiting-probability accounts\nfor defeasible inference based on minimal information and indifference principles [GMP 90, BGHK 93]. There are several reasons why the quasi-probabilistic perspec tive is particularly attractive. First, it helps us to avoid serious conflicts between default conclusions and probabi listic judgments. Here, we should keep in mind that pro bability theory is strongly backed not only by practical considerations, but also by foundational reflections about rational behaviour. Next, it makes it easier to evaluate how reasonable or reliable our defeasible reasoning pat terns are, because we obtain a reference point allowing us to identify and clarify the assumptions, simplifications and compromises behind our inference philosophies. A further advantage is the possibility to exploit some power ful classical probabilistic tools. Last but not least, this ap proach facilitates decision-theoretic investigations, which are the main purpose of plausible reasoning in the real world.\nIn this paper, we are going to continue this research program and present an integrated tt:amework for e\ufffd tended probabilistic and default reasonmg, whose tas\ufffd ts to offer a powerful normative background for practical realizations. More precisely, we are going to interpret defaults by generalized probability constraints, which opens the way to the implementation of promising new plausible inference strategies. We begin with a critical look at the ranking measure paradigm for default know ledge and propose to replace it by a more powerful sem antic perspective using nonarchimedean probability mea sures, i.e. distributions allowed to take infinitely small (\ufffd 0) values. A central idea is to develop defeasible entail ment strategies based on extended information-theoretic notions. But to achieve this, we need valuation algebras offering more sophisticated algebraic tools, like exponen tiation or logarithmization, as well as some sophisticated results from model-theoretic algebra. On top of these non standard measures, we investigate and discuss several possible interpretation policies for defaults. Based on this, we then propose a general four-step-methodology for de feasible inference. In particular, we exploit nonarchi medean entropy maximization to implement the minimal information philosophy for nonmonotonic inference. This amounts to define a suitable preference relation on non-\narchimedean probability distributions, which allows us to validate Lehmann's postulates for preferential inference. To conclude, we provide a comparison with other popular approaches and illustrate our entailment relations and their competitors by the way they handle relevant bench mark problems.\n2 RANKING MEASURE SEMANTICS\nWhat is a default ? Generally speaking, it is a binary relationship between propositions, written cp -\u00bb 'Jf, whose role is to guide plausible inference processes. In practice, accepting a default cp -\u00bb 'I' means that if cp were the only fact we knew, then we would be willing to assume or ex pect that 'Jf is true as well. In what follows, we are going to adopt the descriptive paradigm, which sees defaults as representing strong conditional expectations effectively anchored in some objective or subjective reality. That is, they are supposed to have global truth conditions and to tell us - in a very simplified manner - something about common or normal relationships in the real world resp. in our epistemic model of it. This reading immediately sug gests a quasi-probabilistic semantics for default know ledge, which may be informally described by\n\u2022 cp -)) 'Jf holds iff P(-ij!l cp) is extremely small.\nHow should we interpret P and \"extreme smallness\"? A first possibility would be to use a coarse-grained form of quasi-probabilistic valuations called ranking measures. This approach was originally advocated in [Wey 91] and further developed in [W ey 94] 1.\nDefmition 2.1 A function R : B -> V is called a ranking measure iff\n1. \ufffd = (B, ('), u, -, 0, 1) is a boolean algebra (e.g. of pro positions), 2. V = (V, *\u2022 \u00ab) is a ranking algebra : (V\\{ -oo }, *\u2022 \u00ab) is the ne\ufffdative half of a nontrivial ordered commutative group with identity o and, for all ve V, -oo i! v ( \u00ab-mi nimum) and -oo * v = v * -oo = -oo (absorptive for * ), 3. For A, A' e B, R(A u A') = max\u00ab { R(A), :R(A')} and R(O) = -oo, R(1) = o, 4. R(A) = -oo if A= u\ufffd { Ai I ie I} and, for all ie I, :R(Aj) = -oo (coherence).\nThe conditional ranking measure corresponding to R is defined by the equations R(A (')B)= R(B I A) * :R(A) for R(A) ::!- -oo, and R(B I A) = -oo for R(A) = -oo.\nRanking measure values can be seen either - on the objec tive side - as coarse, simplifying representations of ex treme probabilities in the real world or - on the subjective side - as rough degrees of disbelief or potential surprise, i.e. the smaller R( -,A), the stronger our belief in the pro-\n1 It turns out to generalize Spohn's natural conditional functions [Spo 90], i.e. Pearl's x:-rankings , and Dubois and Prade's [DP 88] possibility measures. 2 (G, *\u2022 \u00ab)is an ordered commutative group iff* is associative, commutative, has a neutral element, admits inverses and \u00ab is a strict total order satisfying x \u00ab y -> X*V \u00ab Y*V.\nDefaults 541\nposition A. So, R (A) \u00ab R (B) basically means that in some sense, A is negligible w.r.t. B. For us, the main role of ranking measures is to provide a transparent descrip tive monotonic semantics for default implication. In fact, extreme smallness can be expressed in a natural way by \"\u00ab o\". LetL be a first-order language and \ufffdL = (BL, &, v, , F, T) be the corresponding (compact) boolean Linden baumalgebra3 induced by first-order predicate logic. The standard ranking measure semantics for defaults is now easily given (for cp, 'I' E L) by\n\u2022 R satisfies cp -)) 'Jf iff R( -lJf I cp) \u00ab o iffR(cp&-,'IJO \u00abR(cp&'Jf) orR(cp&-,'IJO = -oo.\nFor this interpretation, a sound and complete axiomatiza tion of default conditionals is offered by (an object-level, boolean version of) Lehmann's rational conditional logic [Wey 91]. We could now strengthen our default concept by imposing more restrictive bounds on the correspon ding conditional ranking measures, e.g. R(-lJf I cp) \u00ab e or R(-lJf I cp) i! e fore\u00ab o. It is not difficult to see that under this reading, the rules of preferential conditional logic [KLM 90] are still guaranteed. However, rational mono tony (a. -)) \ufffd and not a. -)) -,ri. implies a.&a.' -)) \ufffd) is no longer valid, as the following example shows. Suppose, :R is a Spohn-type K-ranking, i.e. a ranking measure whose ranking algebra is based on the additive structure of nega tive integers ({ -oo, ... -2, -1, 0}, +,<) and which verifies R(-<p) = -2, R(-<p&'Jf) = -2 and R(cp&'Jf) = -1. Interpret ing a. -)) \ufffd by R( -f3 I a.) \u00ab -1 ( \u00ab 0), we then get T -)) cp, but neither T -)) -ijl nor 'I'-)) cp, because R(-<p IT)= -2, R('Jf 11) = -1 and R(-<p I 'Jf) = -1. But, there are also more basic problems affecting the ranking measure approach.\n1. It is too coarse-grained for most decision-theoretic purposes. For instance, we cannot make a difference bet ween a situation where we have gotten 100 positive and 1 negative equally plausible outcomes and one where the the odds are inversed. The main reason is that equal ran king measure values don't add up.\n2. It is somewhat isolated from standard probabilistic approaches, because its valuation structures are too rudi mentary. This means that there are only indirect possibili ties to exchange informations and powerful methods. Fur thermore, it is difficult to give a precise, objective mean ing to ranking measure values. On the other hand, a direct connection to or a common ground with the fine-grained world of real probability could well provide a better theo retical understanding making it easier to motivate, develop and investigate more reasonable quasi-probabilis tic plausible consequence relations for default knowledge.\nHowever, we should not forget that the classical probabi listic approach itself has notorious defects. For instance, it may be inappropriate when precise numbers are not re quired, unavailable or meaningless and the computational costs become disproportionate w.r.t. to the inferential\n3 For the sake of notational economy, we sloppily denote the elements of BL (the sets of classically equivalent L-formulas) by their representatives from L.\n542 Weydert\nneeds. This is and was a major justification for the switch to ranking measures. But probability measures are also haunted by technical and foundational weaknesses. A very prominent one is the non-existence of uniform distribu tions on infinite sets as long as we want to ban non-empty sets with vanishing probability, reserving measure zero for real impossibility. The difficulties to represent re visable - i.e. belief negations with non-zero probability - plain belief - i.e. closed under conjunction and logical consequence - in a natural way by classical subjective probabilities, interfer with the desire for dynamic logical accounts [Spo 90]. Standard probabilistic threshold inter pretations for defaults are easily seen to be absolutely un suitable. Translating <p -\u00bb 'I' by P(-lJI I <p) \ufffda, for non zero c:x, would be completely ad hoc and in flagrant con flict with our intuitions. But P(-lJI I <p) \ufffd 0 is inacceptable as well because it would necessarily induce the trivializa tion of defaults with an exceptional antecedent, e.g. auto matically validating <p&-lJI -\u00bb -,cj> and cp&-,'1' -\u00bb cj>. So, there are several reasons to look for an extended frame work.\n3 NONARCHIMEDEAN PROBABILITIES\nWe have seen that the traditional probabilistic as well as related coarse-grained approaches both fail - for different reasons - to meet our expectations. The major remaining alternative then is to consider generalized probability measures using more fine-grained valuation algebras ex tending ([0, 1], +, x, <). Because we want to conserve as much of the classical probabilistic context as possible, the most interesting candidates are finitely additive measures takinj their values from the unit interval of some ordered field IR1 = (R 1, 0, 1, +1, x', <') properly extending the standard ordered real number field IR = (R, 0, 1, +, x, <), ie. R1 :2 R s.t. the restrictions of+', x', <' to R are just+, x, <5. First, note that such an IR 1 is necessarily a non archimedean extension of IR, i.e. it includes e ::J:. 0 with -r < e < r for every strictly positive standard real r. These e are called infinitesimals. To deal with them, we define for 0 \ufffda, b e R1 the much-smaller-than-ordering <<. a << b iff a < r x b for all standard reals r > 0. This makes << a strict total pre-order on the positive half of R1\u2022 Because we are going to need an extended logarithm function for defining information measures aimed at nonarchimedean probability distributions, in fact, we have to look for pro per extensions IR 1 of the real ordered exponential field IRe= (R, 0, 1, +, x, exp2, <),where exp2(r) stands for 2r. Let LEF be the first-order language of ordered exponential fields and LEp(R) be the expansion of LEF by constants r for each real number r. The existence of such IR1 is now guaranteed by the compactness theorem ( { <p I I Re I= <p, <pe LEp(R)} u { 0 < x, x < r I re R} is finitely satisfiable over Re, hence satisfiable). There exists a correct, pre sumably imcomplete axiomatization RCE of the LEF-\n4 An axiom set for ordered fields can be found in [Bac 90]. 5 To ease notation, from now on, we are going to use the same relational/functional notation for structures and their extensions.\ntheory Th(IRe) of the authentic real ordered exponential field.\nDefinition 3.1 Let R C F be the theory of real closed ordered fields, i.e. ordered fields where every polynomial of odd degree has a root(= theory oflR). The structure IF\n= (F, 0, 1, +, x, exp, <) is called an ordered real closed exponential field iff IF verifies RCE = RCF + E1 - E4,\nE1 exp( l) = 1+1, exp(v + w) = exp(v) x exp(w), E2 v < w \ufffd exp(v) < exp(w), E3 O< v \ufffd 3w exp(w)= v, E4 n2 < v \ufffd vn < exp(v), for neNat (vn = v x ... x v).\nThe corresponding logarithmic function log is defined by log(v) = w iff (v \ufffd 0 and w = 0) or (exp(w) = v).\nOf course, for practical and foundational reasons, IR1 should be as close to IRe as possible. A straightforward way to achieve this is to require that IR1 satisfies Th(JRe). Among others, we may then freely use classically defin able notions. But we can go even one step further. First, some definitions. An L-theory T is called model-complete iff for any pair ofT -models M and N, if N is an extension of M, then N is an elementary extension of M, i.e. every L-sentence with parameters from M is true in N iff it holds in M. In other words, as far as first-order logic is concerned, elementary extensions don't bring anything new. Another equivalent way to put it is to say that T u {<pI M I= <p, <p quantifier-free L(M)-formula} is complete in L(M), i.e. L extended by constants for the elements of M. Now Wilkie has shown the following [Dri 94].\nProposition 3.1 Th(IRe) is model-complete.\nThat is, by assuming that our IRe-extension IR1 validates Th(IRe), we can ensure that every classical result about IRe, obtained by any means but expressible in LEp(R), also holds in IR1\u2022 In other words, that IR1 is an elementary extension of Re. But Wilkie was also able to prove that Th(IRe) is o-minimal, which means that the only para meter-definable sets {aeF I I F l=<p(ll), <p(x)ELEF} in its models IF are finite unions of open intervals and points. From this it follows that all the strictly positive infini tesimals in IR1 satisfy the same LEp(R)-formulas with a single free variable. In fact, unary properties in the realm of smallness are completely fixed by the limit-behaviour of the standard reals.\nProposition 3.2 Let IR1= (R1, 0, 1, +, x, exp, <) be a proper extension oflRe satisfying Th(JRe) and e, -r > 0 be infinitesimals in IR1\u2022 Then, for all <p(x)e LEp(R), IR1 I= <p(e) iff there is 0 < re R' s.t. for all se R1, 0 < s < r im plies IR1 I= <p(s) iff there is 0 < re R s.t. for all se R, 0 < s < r implies IR I= <p(s) iff IR1 I= <p(I).\nThis means in particular that there are no privilegued non zero infinitesimal values, which corresponds well to our intuitions about the indiscernibility of infinitesimals. We cannot differentiate infinitely small numbers independent ly from other nonstandard reference points. Hence, each infinitesimal might be called prototypical or generic in the\ncontext of Th(IRe). Observe that all this would not be true if we could refer to an integer concept allowing the dis tinction of odd- and evenness. We are now ready to pro vide a more formal defmition of our extended probability measures.\nDefinition 3.2 Let IR' be a Th(IRe)-model extending IRe and :B = (B, n, u, -, 0, 1) be a boolean algebra. The function P : B -> R' is called an IR'-valued nonarchi medean probability measure iff for all A, Be B, P(A) \ufffd 0, P(l) = 1 and P(AuB) = P(A) + P(B)for disjoint A, B. It is called coherent i/P(A) = 0 only holds for A= 0.\nSuppose, P is an IR'-valued probability measure on B, and {Ai I i \ufffdNat} a subset ofB s.t. Ai n A j = 0 for i,;:. j, P(Ai) = 1/21+1 and A= U:BAi (unique least upper bound w.r.t.lJ). But then, because the infmite sum of the P(Ai) is just defmed \ufffdo be the supremum of the partial smns Si = 1/2 + ... + 1121+ 1, which doesn't exist in IR' (for any potential limit s, we would get Si < s - E < s for all i), we cannot set P (A) = EP(Ai), i.e. we cannot assume a additivity. Coherence, i.e. respecting impossibility, can only be a facultative requirement given that important classical distributions like the Lebesgue measure on [0, 1] violate this principle. In practice, however, we are mostly concerned with restrictions of IR '-valued measures to boolean algebras of definable sets or Lindenbaum-alge bras :BL. Note that we may construct from a given coher ent nonarchimedean probability measure P: B -> [0, l]R\u2022 a corresponding canonical ranking measure :R. P by identi fying those values a, b in [0, l]R\u00b7 which are <<-incom parable (not a << b or b << a) and taking the quotient structure. The ranking algebra resulting from this cons truction will be dense, contrasting with the discrete !C-ran king structures. We get the following extendibility result\nProposition 3.3 Let .I be a boolean subalgebra of the power-set-algebra 1>ow(S), IR' be as above and P be an IR'-valued probability measure on B . Then there is a Th(IRe)-model m\" extending IR' and an lR\"-valued pro bability measure P' on Pow(S) extending P.\nWhat is the real meaning of infinitesimal probabilities ? Certainly, it seems hard to imagine any objective, statisti cal interpretation, in particular because of our indiscer nibility result. That is, we have to adopt a subjectivist perspective. Then, their role is three-fold. To begin with, they provide a rough classification of propositions accor ding to their respective relevance or order of magnitude. For an arbitrary infinitesimal e, P(A I B) = e practically means that within the context B - at least for finite boolean algebras - we should neglect the alternative A for utility considerations and decision-taking. Similar to ran king measure values, they allow us to express absolute and relative ne\ufffdigibility. Of course, this could be done as well through :R. . The fine-grained distinctions offered by the extended probabilistic scale matter when extreme con ditioning on neglected evidence has to occur and we have to revise probabilities (e.g. to prepare a decision). So, their second task is to supply a coherent framework for borderline conditionalizations and more subtle compa risons within the same order of magnitude. Last but not\nDefaults 543\nleast, we are going to nse them in the following for more fine-grained interpretations of default knowledge and re asoning, trying to accommodate established probabilistic inference mechanisms to our extended framework and thereby to defeasible inference.\nTo conclude, some words about previous uses of non standard probabilities in the context of default reasoning and belief revision. Spohn [Spo 90] exploited this notion to interpret and justify his discrete -.c-ranking revision qamework. He did so by associating K(A) = i with P(A) = e1 for an arbitrary but fixed infinitesimal E. Nonstandard valuations admitting infinitesimals were also used in [LM 92] for a probabilistic reinterpretation of ranked models. A difference with our approach is that they worked within the traditional formal context of nonstandard analysis [Sto 77]. That is, they extended not only the real number field but also the corresponding set theory, which is much more than we need and want to do regarding our intuitions and intentions. We make only those assumptions necessary to model defaults and defeasible inference in a more fme grained probabilistic context. The adequacy of our way to proceed is guaranteed by the previously mentioned model theoretic results. There have been other proposals combin ing the probabilistic with the ranking measure perspective, e.g. counterfactual probabilities [Bou 93] and cumulative measures [Wey 94], but these heterogeneous approaches are sometimes less adequate. For practical purposes, of course, they may still constitute a real alternative.\n4 DEFAULTKNOWLEDGE\nInfmitesimal probabilities are certainly a very natural way to encode the notion of \"negligibility\" or \"extreme small ness\", which is at the center of our descriptive default philosophy. However, interpreting defaults by nonarcbi medean conditional probability constraints is not entirely straightforward. The reason is that we have to deal with a multitude of possible interpretations, which may produce different results. In what follows, let L be a classical pro positional or flrst-order language and P be a nonarcbime dean IR'-valued probability measure on the correspond ing Lindenbaum algebra iJL. To implement the attribute extreme smallness, a subset of [0, l]R\u00b7 must satisfy at least two conditions, namely downwards closure and infini tesimality. Therefore, in our framework, the translation of a default <p -\u00bb'I' (<p, 'I' L-formulas) will always take the form of an infmitesimal positive initial interval constraint,\n\u2022 <p -\u00bb 'If : P( --iV I <p) E l.p _,. v\u2022 with\n[0, l]R\u00b7 ;.;;;2 \ufffdq,_,. v,;:. 0, s.t. for all v E lcp-\u2022 v\u2022 0::; v << 1 and 0 S w $ v implies w E Iq, _,. v\u00b7\nObserve that we allow our intervals to depend on <p -\u00bb'I'\u00b7 To define lcp _,. v\u2022 we may now choose between different comparison relations (<, s, <<,\ufffdand bounds (infmi tesimal terms, 1). But, what are the most appropriate stra tegies for translating the descriptive content of defaults by such constraints ? Here, we have to take into account not only our intuitions about the intended logical behaviour of defaults, e.g. w.r.t. Lehmann's conditional axioms, but\n544 Weydert\nalso the possible defeasible inference policies which will have to exploit this encoding of default knowledge. An obvious choice would be to use the canonical ranking measure construction and to transfer the corresponding default interpretations. This would give us three kinds of constraints based on the magnitude relation <<. Let Eql-10 'II\n<< 1. Note that\ufffd stands for not>>.\n\u2022 cp -\u00bb'I': P(--\\Jf I cp) << 1 (<<-classical),\n\u2022 cp-\u00bb'lf: P(-lJf l cp)<<Eql-\u00bb 'lf (<<-bounded),\n\u2022 cp -\u00bb'I': P(-\\jll cp) \ufffdEql-10 'II \ufffd-bounded).\nRecalling what we said about ranking measures and the fact that e << ...Je << 1 for e << 1, we see that the ftrst scheme guarantees the rules of rational, the second and the third one only those of preferential conditional logic. So, only the <<-classical interpretation shows the desired behaviour at the level of monotonic inference for defaults. The corresponding proof theory then allows us to trans form given default knowledge bases without changing their intended nonarchimedean probabilistic meaning. The precise syntactic form doesn't matter under these con ditions. Most old-fashioned default formalisms, like Rei ter's default logic, do not support such natural, content preserving manipulations. To simplify the overall valua tion context, we may try to restrict the values P can take, e.g. to polynomials over a ftxed infinitesimal \u00a3. In the context of ranking measures, this would correspond to the choice of a discrete ranking algebra.\nAnother possibility would be to interpret all our defaults through infmitesimal, but otherwise classical threshold constraints using < or 5.\n\u2022 cp-\u00bb'1': P(-\\jflcp)<Eql-\u00bb\\11 (<-bounded),\n\u2022 cp -\u00bb'I' : P( -lJf I cp) \ufffd Eql-10 'II (\ufffd-bounded).\nThese constraints, however, strongly conflict with almost all the classical postulates for default conditionals (e.g. conjunction on the right, reasoning by cases, cautious mo notony). Nevertheless, that doesn't make this interpreta tion worthless. For instance, \ufffd-boundedness is appropriate for implementing defeasible inference strategies based on generalized probabilistic techniques like nonarchimedean entropy maximization. Furthermore, on an intuitive level, this interpretation and the <<-classical one are quite close to each other. In fact, asking P(-\\jll cp) to be<< 1, i.e. to be\ufffd than some (generic) infinitesimal, doesn\ufffd seem to be so different from asking it to be\ufffd than a fixed (generic) infinitesimal, given the indiscernibility result from Prop. 3.2. Of course, this argument breaks down if we consider not one but several infinitesimal bounds in parallel (e.g. P(-lJI 1 cp) \ufffd e and P(-lJI' I cp') \ufffd e' = e2). So, it is not completely obvious which translation strategy serves our needs best, because there seems to be a surprising trade off between having a suitable monotonic logic for defaults and getting nice nonmonotonic inference relations to ex ploit them.\n5 DEFAULT INFERENCE\nIn its traditional form, defeasible reasoning has been mainly concerned with finite splitted knowledge bases of the type I: u a, where I:= { cpi I i \ufffd nl:} is a set of facts from L and a = {'l'i -\u00bb 'l''i I i \ufffd n11} a collection of defaults over L, whose role is to guide a plausible reason ing process starting at I:. The task of any - necessarily nonmonotonic - plausible inference relation II= is to tell us, given facts and defaults, what we might reasonably ex pect or assume beyond certainty, i.e. which 'I' should be defeasibly inferred (is a plausible consequence ofllia).\n\u2022 { cpj I i \ufffd nl:} u {'l'i -\u00bb 'l''i I i \ufffd nil} II=\"'.\nFor each a, we can then define a finitary consequence relation l=a on L by setting I: l=a 'I' iff I: u a II= 'I'\u00b7 Currently, a rather broad consensus has been reached about the minimal requirements for such factual inference relations l=a (but not for interactions between defaults). They are at least required to be preferential for ftnite premise sets [KLM 90]. We do not ask for rational mono tony, because sometimes, we may want to construct new inference relations by taking the intersection of existing ones and to see I= a as giving only a partial approximation of some ideal plausible relationships, which conflicts with the above principle. Concerning the full relation II= and the impact of a, however, there doesn't exist a general agreement about which postulates to adopt, in particular if the basic default notion is allowed to violate the rules of preferential conditional logic. For us, the primary criterion is the existence of a semantic justification which doesn't conflict with the probabilistic perspective and might even be justified by probabilistic considerations. So, let's tum now to default inference in the context of our nonarchi medean framework of IR' -valued probability measures.\nThe basic idea governing the defeasible inference philo sophy we are going to adopt here is that\nDefault reasoning slwuld be anchored - on an abstract, ideal level - in the comparison of nonarchimedean pro babilistic belief valuations ordered according to some suitable version of the minimal informational commitment principle.\nMore precisely, we adopt the following four-step strategy.\n1. Finiteness. Fix a background language L, closed under the usual propositional connectives, and a corresponding monotonic inference relation 1-, closed under the rules of propositional logic, s.t. 1- induces on L a ftnite Linden baum algebra \ufffdL <BL being the quotient of L over logical equivalence -11-). This should be enough for \ufffdost practical representational purposes. We may then restnct ourselves to fmitary measures, which greatly simpliftes matters.\n2. Nonarchimedean constraints. Translate a given finite default collection a into a set of infinitesimal conditional probability constraints. Several important choices have to be made in this context. First of all, if explicit default strengths or preferences have not been given, we must decide whether our default translation procedure 'T should\nfix a single infinitesimal bound for all the defaults or in troduce a particular one for each individual default. Next, we have to choose suitable numerical comparison rela tions linking bounds and admissible values.\n\u2022 'J: A= {'Vi-\u00bb 'l''i I i :$; n,i} -> {P(--ll''i I 'l'i)E IiI i :$; n,i}\nThe main question here is whether the constraint areas Ii should be closed under addition (requiring <<). If not, the intervals should at least be closed w.r.t the order-topolo gy induced by < (requiring :s;). This gives us three major interpretation strategies 'J for A, resulting in\n\u2022 A'T\u00abl = {P(--lV'i I 'l'i) << 1 I i :$; n,i},\n\u2022 A 'J'l.,s = {P( --ll''i I 'lfi) :s; e I i :s; n.i} fore << 1,\n\u2022 A 'J'l.,p = {P( --ll''i I 'lfi) :s; Ei I i :s; n.i} for Ei << 1.\nNote that the constraints from the last two sets are open schemes with infinitesimal parameters e, Ei. We call 'J \u00ab1 the classical-bounded, 'J \ufffd 8 the single-bounded and 'J \ufffd the plural-bounded nonarchimedean probabilistic defauft translation policy.\n3. Preferential entropy reasoning. Implement the mi nimal information principle within a preferential frame work. This is achieved by defming a partial order on fini tary IR'-valued probability distributions which gives prio rity to informationally less committed valuations, i.e. to those making the least additional assumptions given the knowledge at hand. To this end, we have to choose an ap propriate information measure. The standard proposal in the literature is the entropy concept Entropy is in fact a privilegued measure for the lack of information which can be justified by uniqueness results for different sets of postulates. Similarly, entropy maximization is a distin guished method in probabilistic inference [Jay 78, PV 90]. Because the axioms for RCE-fields already guarantee the existence of a logarithm function corresponding to exp, we may directly use the classical entropy definition in IR' as well. Let P be a nonarchimedean IR'-valued probability distribution on a finite boolean algebra .n with atoms A1, ... ,An. Then the (IR'-)entropy H(P) of P is defined by\n\u2022 H(P) = -:Dli where Si = P(Ai) x log(P(Ai)) if P(Ai) ::F- 0 and Si = 0 if P(Ai) = 0 Oimit value).\nH(P) takes its maximum log(n) for the uniform distribu tion po on .n (with P0(Ai) = 1/n). Note that H is invariant under atom permutations. For the standard valuation structure, it is well-known that every set of linear :S;- cons traints {l:P(Ai)aji :s; bj, 0 :s; P(Ai), l:P(Ai) = 1 I 1 :s; i :s; n, jeJ} over (P(AJ), ... , P(An)), which determines a convex subset of [0, l ]n, has a unique entropy-maximizing solu tion. But note that we may think of other interesting con ditions, e.g. concerned with independence, which cannot be expressed by linear means and may require at least polynomial expressions. Now, because IR' is an elemen tary extension of IRe, all the LEp(R)-expressible results about entropy-maximization in classical fmitary contexts, can be immediately transferred to IR', where they hold as well. Based on H, we can then defme our entropy-based\nDefaults 545\ninformation-ordering <E, a total pre-order on fmitary IR' valued probability distributions P: B -> [0, l]R'\u00b7\n\u2022 P <E P' iff H(P') < H(P)\nIt is easy to see that the constraint sets A 'J provided by the plural- or single-bounded translation policy can be writ ten as sets of linear weak inequality constraints over the atom probabilities P(Ai). Therefore, assuming satisfiabili ty, there exists a unique <\ufffdminimallR'-valued distribu tion P* over .n verifying 11 \u2022 For constraints based on <, << or \ufffd. however, there might be no <E-minima at all. Fortunately, we may still use the partial order <E to defme a preferential consequence relation II=E on all LEp(R') sentences with constants P(Ai). All we have to do - be cause <E is neither well-ordered nor otherwise well-be haved - is to adopt the limit evaluation strategy. That is, if P.n is the set of IR'-valued distributions over .n,\n\u2022 <I>(P(AJ), ... , P(An)) II=E 'lf(P(AJ); ... , P(An)) iff\nin PB, for every P validating <1>, there is a P' :s;E P\nverifying <I>, s.t for aliP\" :s;E P' satisfying <I>, 'If holds.\n4. Defeasible entailment relations. Defme for every in terpretation strategy 'J a corresponding defeasible entail ment notion II='J based on II=E\u00b7 i.e. entropy maximization in nonarchimedean contexts. The general idea is to deter mine the relationship between a fmite premise set I:, re presented by the conjunction of its elements cp, and a po tential plausible conclusion 'I' by evaluating the conditio nal probability P(-ijf I cp) for the <E-most-preferred distri butions verifying A'T. The nonmonotonic inference step should succeed iff P(--ll' I cp) is extremely small for all these P, i.e. P(--ll' I cp) << 1. This guarantees that II='J is preferential for fixed A. Rational monotony cannot be re quired because usually, l: offers only a partial (defeasible) description of P. Given that we may not omit any possible instan-tiation of the free unspecified infinitesimal parameters Ei (i :s; n,i) in A'T, we are going to accept only defeasible conclusions which are independent from the exact values. To realize this, we proceed by universal quantification. Now, we are ready for our general defeasible entailment scheme. Let I: = { cpj I i :s; n,J, A = {'Vi-\u00bb 'l''i I i :s; n,i} and cp = cpo& .. &cpnt. Then,\n\u2022 l: u A II='J 'I' iff for all f{), \u2022\u2022. , Ent << 1,\nA'T(f{), ... , Env II=E P(-,'lfl cp) << 1.\nIt immediately follows from this definition that II='J is finitarily preferential for ftxed A'T, for 'J = 'J\ufffd.s. 'J\ufffd.p\u00b7 'J \u00ab1\u00b7 In this paper, we have and will mainly consider standard finite default sets. But, it is important to note that our entailment strategy is in fact applicable to arbitrary constraint sets over nonarchimedean probability measures on some given fmite boolean algebra. Furthermore, if we are willing to accept a technically slightly more deman ding framework, it is even possible to handle infinite boolean algebras. For practical (normative) purposes, however, fixed finite contexts seem to be sufficient.\n546 Weydert\n6 EXAMPLES AND COMPARISONS\nTo become more acquainted with our nonarchimedean en tropy-maximizing entailment strategies, we are now going to illustrate the core approaches ll=::;;,p, 11=:::;,8 and ll=\u00abt by their impact on some relevant inference patterns and - roughly - investigate their links with other popular ac counts from the literature. Notably preferential ll=pc and rational closure II=Rc [LM 92] (equivalently, for finite sets of conditionals, system Z [Pea 90] or elementary hy perentailment [Wey 93, 95]), lexicographic closure II=Lc [Leh 92, BCDLP 93], conditional entailment ll=cE [GP 92], maximum-entropy entailment II=ME [GMP 90, Gol 92] and random-worlds entailment II=Rw [BGHK 93]. The maximum-entropy-based formalism for defeasible reasoning investigated by Goldszmidt is - contrasting with ours - mainly defined for simple default sets interpreted according to 'J::;;,s and based on a slightly more cumber some limiting probability strategy. Nonetheless, it gives the same results as ll=s;,s for standard default knowledge bases. This readily follows from Prop. 3.2. A similar re mark holds for the random-worlds approach of [BGHK 93], which in fact intends to handle more powerful first order languages. It combines a somewhat limited appro ximation methodology with a limiting counting strategy based on very far-reaching indifference assumptions for finite first-order models. This causes difficulties with pre mises lacking finite models. Last but not least, it should be mentioned that the consequence relation lh<t, which is based on the most liberal interpretation strategy 'J \u00abl\u2022 turns out to be equivalent to preferential closure ll=pc. Because the constraint interval here is just the set of all positive in finitesimals (in the given IR'), there is a lot of variability when doing entropy-maximization. That's why only the basic <<-relationships restrict the set of admissible non archimedean probability distributions (through the rules of probability theory and the definition of\u00ab).\nTo begin with, we describe several - in our eyes desirable - strict and defeasible (non-)inference features. This list is, of course, not intended to be exhaustive or even represen tative in a precise sense. It only collects some interesting principles helping us to discriminate, situate and evaluate plausible consequence relations, providing at least a sur vey of the relevant issues. In the following, we shall as sume that ex, \ufffd. \"(, cp, 'I' are logically independent formulas from L. Also, we concentrate on standard finite default sets Ll.\nRat0 : Object-level rationality.\n\u2022 -\u00bb satisfies the rules of rational conditional logic.\nThese are intuitively appealing for normal implication.\nPre : Meta-level preferentiality.\n\u2022 For constant Ll, II= defmes a preferential consequence relation on facts.\nFundamental requirement for factual plausible inference.\nRat: Meta-level rationality.\n\u2022 For constant Ll, II= defmes a rational consequence relation on facts.\nAt the meta-level, rational monotony is not really re quired, but sometimes practical.\nES : Extended specificity.\n\u2022 { cp} u { cp -\u00bb ex, ex-\u00bb \ufffd. \ufffd -\u00bb 'If, ex -\u00bb--W} II= -\\If.\nExtended version of the most-specific-subclass principle, a basic postulate for plausible reasoning with defaults.\nEI : Inheritance through exceptional subclasses.\n\u2022 {ex} u { ex->> \ufffd. ex -\u00bb--lJf, \ufffd -\u00bb 'If, \ufffd -\u00bb cp} II= cp.\nThe formalism should be able to handle implicit indepen dence assumptions coherently. The exceptionality of ex in \ufffd should not affect the inheritance of cp.\nGE : Geffner's example.\n\u2022 { ex&\ufffd&y} u { ex->>--,\ufffd r ->)--,\ufffd ex&\ufffd->> 'I'} Ill= 'I'\u00b7 -,'I\u00a3\nBecause ex&\ufffd and r are unrelated, there is no reason to prefer 'I' or --iJf, given ex&\ufffd&y.\nAP : Anti-prioritization.\n\u2022 { exv(\ufffd&y)} u {T ->)--,li T ->>-f(, exv\ufffd ->>-a:} Ill= --, ex.\nIf \ufffd and y are negligible w.r.t. T and ex w.r.t. \ufffd. then ex pecting ex to be negligible w.r.t. \ufffd&y seems completely unjustifiable given the presumable exceptionality of \ufffd&y w.r.t. \ufffd (cf. EO. Recall that A->> B = \"A&--,B is negligible w.r.t. A(&B)\".\nRE : Redundancy.\n\u2022 { --,cp} u {T ->> cp, T ->> cpV'If} Ill== 'I'\u00b7\nAssuming right weakening for ->>, T ->> cpV'If would be come redundant, so we should infer nothing which does not already follow from --cp and T ->> cp (\"syntax-indepen dency\").\nNE : Neutralization.\n\u2022 { --,cp} u {T ->> cp, T ->>'If, --cpv--, '1'->>--,cp} Ill= 'I'\nFrom the perspective of preferential conditional logic, the default T ->> 'I' is the only one which may be considered redundant. But then, given --cp, there is no longer any reason to expect 'I'\u00b7 Note that this argument holds in par ticular in a situation where the default --cpv--,'1' -\u00bb--ip has been replaced by the corresponding strict implication and the similarity with RE becomes more obvious.\nNow, let's see how the defeasible entailment relations mentioned above are handling these \"benchmark tests\". This will give us a general impression of their overall be haviour and help us to detect major strengths and weak nesses. To compute the inferences for 11=:::;,8 and ll=::;;,p, we may use the algorithms described in [Gol 92]. Satis faction is indicated by 1, violation by 0.\nPrinciples : Rat0 Pre Rat ES EI GE AP RE NE\n\u2022 ll=\ufffd.p 0 1 0 1 1 1 1 1 1\n\u2022 ll=\ufffd,s 0 1 1 1 1 1 1 1 1\n\u2022 ll=\u00abt 1 1 0 1 0 1 1 1 1\n\u2022 II=Rc 1 1 1 1 0 0 0 1 1\n\u2022 ll=cE 0 1 0 1 1 1 0 0 0\n\u2022 II=Lc 0 1 1 1 1 0 0 0 0\nAll this seems to support our view that nonmonotonic reasoning based on infinitesimal probabilistic knowledge is a natural generalization of classical probabilistic infer ence and defeasible reasoning with default conditionals. It is particularly well-suited to exploit entropy maximiza tion techniques, offering us very plausible results backed by foundational considerations which are also valid for in finitesimal contexts. Of course, on the other hand, we must see that existing ME-based formalisms are computa tionally reasonable only or mainly for irredundant default sets, i.e. where no abnormality part A&-,B is covered by the abnormality parts of the remaining defaults. But such a restriction is hardly appealing. However, recent results have indicated that it could be possible to approximate pure ME-default inference by using more practical rank ing-based consequence relations admitting nonarchime dean probabilistic justifications. They would allow ?s to combine rational conditional logic for defaults w1th a mechanism for exploiting implicit independence assump tions, e.g. to realize Rat0 and El, which is beyond the scope of the formalisms mentioned above [W ey 95].\nACKNOWLEDGEMENTS\nThanks to the anonymous referees, who have helped to make this paper a less obscure one.\nREFERENCES\n[Bac 90] F. Bacchus. Representing and Reasoning with Probabilistic Knowledge. MIT Press, Cambridge, Mass., 1990. [BCDLP 93] S. Benferhat, C. Cayrol, D. Dubois, J. Lang, H. Prade. Inconsistency management and prioritized syntax-based entailment In Proceedings of IJCAI 93, Morgan Kaufmann, 1993. [BGHK93] F. Bacchus, AJ. Grove, J. Y. Halpern. Statis tical foundations for default reasoning. In Proceed ings of IJCAI 93, Morgan Kaufmann, 1993. [Bou 93] C. Boutilier. The probability of a possibility. Adding uncertainty to default rules. In Ninth Con ference on Uncertainty in Artificial Intelligence. Morgan Kaufmann, 1993. [DP 88] D. Dubois, H. Prade. Possibility Theory. Plenum Press, New York 1988. [Dri 94] L. van den Dries. The elementary theory of restricted analytic fields with exponentiation. In Annals of Mathematics, 140 (1994), 182-205. [Gol 92] M. Goldszmidt Qualitative Probabilities. A Nor mative Framework for Commonsense Reasoning.\nDefaults 547\nPhD thesis. TR-190, Cognitive Systems Laboratory, Computer Science, UCLA, Los Angeles, 1992. [GP 92] H. Geffner, J. Pearl. Conditional entailment : bridging two approaches to default reasoning. Artifi cial Intelligence, 53: 209 - 244, 1992. [GMP 90] M. Goldsmidt, P. Morris, J. Pearl. A maximum entropy approach to nonmonotonic reasoning. In Pro ceedings of AAAI 90. Morgan Kaufmann 1990. [Jay 78] E. T. Jaynes. Where do we stand on maximum entropy ? In R.D. Levine and M. Tribus (eds.), The Maximum Entropy Formalism. MIT-Press 1978. [KLM 90] S. Kraus, D. Lehmann, M. Magidor. Nonmonotonic reasoning, preferential models and cumula tive logics. Artificial Intelligence, 44: 167-207, 1\ufffd90. [LM 92] D. Lehmann, M. Magidor. What does a condi tional knowledge base entail ? Artificial Intelligence, 55:1-60, 1992. [Leh 92] D. Lehmann. Another perspective on default . reasoning. Technical report TR-92-12, Hebrew Um\nversity, Jerusalem, 1992. [Mak 94] D. Makinson. General patterns in nonmonotonic\nreasoning. In Handbook of Logic in Artificial Intel ligence and Logic Programming ( vol II), eds. D.Gab bay, C. Hogger. Oxford University Press, 1994.\n[Pea 90] J. Pearl. System Z. In M.Vardi (ed) Proceedings of the Third Conference of Theoretical Aspects of Reasoning about Knowledge. Morgan Kaufmann. [PV 90] J.B. Paris and A. Vencovska Anote on the ine vitability of maximum entropy. In Intemationl Jour nal of Approximate Reasoning, 4:183-223, 1990. [Rot 91] H. Rott A nonmonotonic conditional logic for belief revision I. In The Logic of Theory Gange, LNCS 465, Springer, Berlin, p 135 -181.\n[Rot 94] H. Rott Drawing inference from conditionals. Konstanzer Berichte Logik und Information, 38, Konstanz, 1994. [Spo 90] W. Spohn. A general non-probabilistic theory of inductive reasoning. In R.D. Shachter et al. (eds.), Uncertainty in Artificial Intelligence 4, North-Hol land, Amsterdam 1990. [Sto 77] K.D. Stoyan. Infinitesimal Analysis of Curves and Surfaces. In Handbook of Mathematical Logic, J. Barwise (ed.), North Holland, 1977. [Wey 91] E. Weydert. Qualitative magnitude reasoning. Towards a new semantics for default reasoning. In J. Dix et al. (eds.), Nonmonotonic and Inductive Rea soning. Springer-Verlag 1991. [Wey 93] E. Weydert. Plausible inference for default con ditionals. InProceedings of the ECSQARU 93 .. Springer, Berlin, 1993. [Wey 94] E. Weydert. General belief measures. In Tenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann, 1994. [Wey 95] E. Weydert. Default entailment. A preferential construction semantics for defeasible inference (to appear)."}], "references": [{"title": "Representing and Reasoning with Probabilistic Knowledge", "author": ["F. Bacchus"], "venue": null, "citeRegEx": "Bacchus.,? \\Q1990\\E", "shortCiteRegEx": "Bacchus.", "year": 1990}, {"title": "Inconsistency management and prioritized syntax-based entailment", "author": ["S. Benferhat", "C. Cayrol", "D. Dubois", "J. Lang", "H. Prade"], "venue": "In Proceedings of IJCAI", "citeRegEx": "Benferhat et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Benferhat et al\\.", "year": 1993}, {"title": "Statis\u00ad tical foundations for default reasoning", "author": ["F. Bacchus", "AJ. Grove", "J.Y. Halpern"], "venue": "In Proceed\u00ad ings of IJCAI", "citeRegEx": "Bacchus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 1993}, {"title": "The probability of a possibility. Adding uncertainty to default rules", "author": ["C. Boutilier"], "venue": "In Ninth Con\u00ad ference on Uncertainty in Artificial Intelligence", "citeRegEx": "Boutilier.,? \\Q1993\\E", "shortCiteRegEx": "Boutilier.", "year": 1993}, {"title": "The elementary theory of restricted analytic fields with exponentiation", "author": ["Dri 94] L. van den Dries"], "venue": "In Annals of Mathematics,", "citeRegEx": "Dries.,? \\Q1994\\E", "shortCiteRegEx": "Dries.", "year": 1994}], "referenceMentions": [], "year": 2011, "abstractText": "We develop a new semantics for defeasible infer\u00ad ence based on extended probability measures al\u00ad lowed to take infinitesimal values, on the inter\u00ad pretation of defaults as generalized conditional probability constraints and on a preferred-model implementation of entropy-maximization.", "creator": "pdftk 1.41 - www.pdftk.com"}}}