{"id": "1303.5753", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Compressed Constraints in Probabilistic Logic and Their Revision", "abstract": "in probabilistic logic entailments, even moderate size problems must yield linear constraint systems with so many variables that exact correction are impractical. this difficulty can be remedied in many cases of which involves introducing pure three valued logic ( true, false, fail \" don't care \" ). the three - valued idea avoids the construction of \" compressed \" conditional systems which have the same solution sets as their two - valued counterparts, but which may involve dramatically fewer loops. techniques to calculate point estimates for the posterior probabilities of entailed sentences are investigated.", "histories": [["v1", "Wed, 20 Mar 2013 15:33:38 GMT  (239kb)", "http://arxiv.org/abs/1303.5753v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["paul snow"], "accepted": false, "id": "1303.5753"}, "pdf": {"name": "1303.5753.pdf", "metadata": {"source": "CRF", "title": "Compressed Constraints in Probabilistic Logic and Their Revision", "authors": ["Paul Snow"], "emails": ["paulsnow@oz.plymouth.edu"], "sections": [{"heading": null, "text": "1. PROLIFERATION OF WORLDS An entailment problem in Nilsson's (1986) probabilistic\nlogic derives an estimate for the prior probability of one sentence (hereafter, the \"target\") from the priors for a set of other (\"source\") sentences. The prior beliefs about the source sentences establish constraints of the form\nP=VW L wi = l\nWi <': 0\nsum over all \"worlds\"\nfor all \"worlds\"\nHere, P is the column vector of the sentences' priors. V is\na matrix derived from an inventory of all consistent\npatterns of truth assignments ( 1 = true, 0 = false) for the\nsource and target sentences. For instance, for source sentences Q and Q=> R, and target sentence R, the consistent patterns (or \"possible worlds\") are the columns\nof the matrix:\nThe linear constraint system says that the prior of each sentence is the sum of the priors for the possible worlds in which the sentence is true. The system thus implies a constraint on the target sentence's prior. In the example, the prior for target sentence R is w 1 + w 3. The linear system, by constraining W, also constrains this sum. The estimate for the target may either be a probability interval (computed using two linear programs), or else a point probability (perhaps maximizing entropy over the wis). A practical difficulty with this scheme is the large number of possible worlds that can arise with even a modest number of source sentences: ten sentences can yield a thousand worlds. Exponential complexity is inherent in the approach (Halpern, 1989).\nOne response to this difficulty is to introduce approximations, as Nilsson himself did. Kane (1989) suggested assessing conditional probabilities (rather than simply zero or one) in the entailment expression for the target sentence. Kane's method doesn't address proliferation of worlds on account of the source sentences, and requires additional assessments beyond priors for the source sentences.\nThe approach taken in this paper represents prior constraints without approximations or assessments beyond the source priors. Instead, a method is discussed that often derives smaller linear systems to convey constraints on the target sentence probability equivalent to those in Nilsson's original proposal. The effect should be that many moderate-sized entailment problems become practical for solution by exact means.\nA method for revision which uses these compressed systems is also discussed. Because of the complicated interaction of conditionals and priors, simplifying assumptions are used.\nCompressed Constraints in Probabilistic Logic and Their Revision 387\n2. OVERVIEW AND EXAMPLE\nThe method presented here derives linear constraint systems based on a three-valued logic common in engineering work (true, false, and \"don't care,\" hereafter \"d.c.\"). The opportunity to use the d.c. value arises whenever two worlds' truth assignment vectors differ in only one component, one world having true and the other false. In the example of the last section, worlds 1 and 3 could form one world with assignments [ d.c., 1, 1 ]. In effect, [ d.c., 1, 1 ] is a \"shorthand\" for the assertion that both [ 0, 1, 1] and [ 1, 1, 1] are possible worlds.\nFor example, consider a modus ponens with a conjunctive antecedent used by Kane (1989). The source sentences are independent AI, A2, A3, and the implication sentence AI & A2 & A3 =>B. The target is B. In this problem, there are 16 possible worlds:\nAI A2 A3\nAI & A2 & A3 => B B 1100000000111111 1100001111000011 1100110011001100 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1010101010101010\nWith three-valued logic, these sixteen worlds can be compressed to five. The enumeration of these worlds follows readily from our understanding of the \"conjunctive antecedent implies consequent\" inference schema. An elementary understanding of what assignments are possible in that schema allows the specification of the possible worlds with n conjuncts as follows:\n1. one world where all sentences are true; 2. one world where all antecedents are true, and the\nimplication and consequent are false; 3. for each i from 1 through n, a world where:\nconjunct #i is false conjuncts numbered< i are true con juncts numbered > i are d.c. the implication is true the consequent is d.c.\nFigure 1 shows a semantic tree for three antecedents constructed according to this plan. Figure 2 expresses the same information in a \"matrix\" format. The resulting five worlds for the example problem can inform a system of ordinary linear constraints. Where the original system contained equation constraints, the new system contains inequalities. Assuming that point priors are available for the Pr (A i)'s, the specific system to bound Pr (B) is (in\naddition to the usual non-negativity and total probability constraints):\nPr (AI) [ 1, 1, 1, l,O]\u2022W' Pr (A2) \ufffd [ I, 1, 1, 0, 0] \u2022 W' Pr(A2) s [ 1, 1, 1, 0, I]\u2022 W' Pr (A3) \ufffd [ 1, 1,0, O,O]\u2022W' Pr (A3) s [ 1,1,0, I,I]\u2022W' Pr (=>) [ 1, 0, 1, I, 1 ] \u2022 W' Pr(B) s [ 1, 0, I, I, I]\u2022 W' Pr (B) \ufffd [ 1,0, O,O,O]\u2022W'\n(Values arising from d. c. assignments are in bold face; the operator \"\u2022\" indicates the scalar product; W' is the transpose of W .) Discussion of the derivation of linear constraints from the semantic tree appears in section 4 below.\nThe possible solutions for Pr (B) in the above system are identical to those that would be found by the corresponding two-valued system. Nevertheless, the \"size\" of the system (total number of vector or matrix components, a rough but fair estimator of the difficulty of the linear programs) is half that of the original.\n3. COMPRESSION USING KNOWLEDGE AND USING SEARCH\nThree-valued constraint systems are simplest to construct when they reflect a well-understood inference schema. The \"conjunctive antecedent implies consequent\" schema is, of course, at the heart of production rule-based systems. From this, the required constraint system can easily be constructed.\nBased on the specification described in the last section, we see that n antecedents yield only n+2 possible worlds. This compares with 2n + 1 worlds in the two-valued system! (The total size of the linear system will be O(n2).) In the case of arbitrary source and target sentences, tree construction will not be guided by an understanding of the possible truth assignments. Standard algorithms, like the Quine-McCluskey procedure, proceed by constructing the ordinary, two-value, semantic tree, and then search that tree for opportunities to combine worlds.\nNote that compression may turn out to be impossible. Consider m independent sentences S I ... Sm as the source and the \"parity function\" (S 1 xor S 2 xor ... xor\nSm) as the target. There are 2m possible assignments, and each assignment has a Hamming distance of at least 2\n388 Snow\nFigure 1. Compressed Semantic Tree for Kane (1989) Example.\nw1 Wz w3 w4 Ws\nA1 1 1 1 1 0 Az 1 1 1 0 d.c. A3 1 1 0 d.c. d.c.\nA1A2A3 => B 1 0 1 1 1\nB 1 0 d.c. d.c. d.c.\nFigure 2. Same Information as Fig. 1, in \"Matrix\" Format.\nCompressed Constraints in Probabilistic Logic and Their Revision 389\nfrom any other assignment. That is, no two assignments differ in only one component, and so no opportunity to use d.c. arises. In other cases, even if some compression were possible, the amount could be disappointing.\nThe effort involved can be considerable. Just constructing the two-value semantic tree is worst-case exponential, to say nothing of the search. Still, if extensive compression is achieved, the overall effort of the entailment problem may be reduced. That's because the linear programming steps are computationally intensive compared to tree construction, so the potential pay-off for finding a reasonable size constraint system can be handsome.\n4. EXPRESSING THE CONSTRAINTS\nOnce the possible worlds are found, we can derive linear constraints. Returning to the Kane example, sentences like A 1 which have no d.c. values are handled just as they would be in Nilsson's proposal. For a sentence with a d.c. value, A2, the semantic tree tells us that the possible\nassignments are [ 1, 1, 1, 0, d.c. ]. This can be read to mean that the prior probabilities wi of the worlds must be such that there is a number p in the closed unit interval where\nPr(A2)=wl +w2+w3+pw5\nThis reading follows immediately from the way the fifth world was constructed. Of the total weight that was assigned to the original worlds that became world 5, some portion of that weight contributes to the sum that is A2's\nprior. For any p and W which satisfy the equation constraint just given, there is some apportionment of weight to the original worlds that achieves p and which satisfies the two-valued system. Conversely, any solution of the two-valued system has a p that satisfies the above equation.\nThe equation, in turn, is equivalent to two simultaneous inequality constraints\nPr ( A2):;;w1 +w2+w3+w5 Pr ( A2)\ufffdw1 +w2+w3\nA solution of the equation constraint for any admissible p solves both inequalities, and for any solution of the inequalities, there is an admissible p that satisfies the equation constraint.\nThe assignment for sentence A 3, [ I, I, 0, d.c., d.c. ], asserts the existence of two numbers (not necessarily\ndistinct) q and r in the closed unit interval, such that\nEven though r above and the earlier p pertain to the same compressed world, their values are independent, a fact which follows from the way that the fifth world was constructed. Thus, the two constraints do not interfere with each other when they obtain simultaneously. The last equation can be translated into the inequalities\nPr(A3) :;;w1+w2+w4+w5 Pr(A3) \ufffd w1 +w2\nThese simultaneous inequalities are equivalent to their equation constraint. Further, these inequalities can hold simultaneously with the inequalities derived earlier for the other sentence. (The full translation of this problem into simultaneous linear constraints was given near the end of section 2 above.)\nTo summarize, each equation constraint involving d.c. assignments can be expanded into two weak inequalities of opposite sense. The upper bounding sum includes the probabilities for the merged worlds; the lower bounding sum omits them. (For source sentences, the quantity being bound is the given prior for the sentence; for the target sentence, the bounds are on the unknown prior being sought).\nPrior beliefs can be weak inequalities (for instance, to express bounds on the prior probability for a sentence). A weak inequality expression of belief in the original system yields one weak inequality of the same sense in the new system. If the inequality bounds a prior from above, then any merged worlds contribute to the sum; if from below, they do not.\n5. REVISION WITH CONDITIONALS\nIf evidence E is observed that bears on some sentence S, then we would wish to revise our probability estimates to reflect our new beliefs. Nilsson assumed that the effect of E on any world j depended only on whether S was true in j or not. In particular:\nPr(Eij ) =Pr(EIS ) = Pr ( E I --.S) if S is true in j if S is false in j (Ia) ( I b)\n(Nilsson made the assumption in a different, but equivalent, form: for sentences S, T, and evidence E that bears on S, Pr( T I S, E ) = Pr( T I S ) and Pr( T I --.S, E ) = Pr( T 1--.S ) .) This assumption is frequently encountered in inference work. For a discussion of the motivation of\n390 Snow\nthe assumption in an A.I. context, see for example Pearl (1986). The assumption isn't strained, and is surely useful, but its main attraction is its simple relationship between the known conditionals given sentences and the unknown conditionals given worlds.\nWhen S is d.c. in j, we can extend this assumption by simple probabilistic identities to\nPr(Eij) = Pr( Sij)*Pr(EIS )+ Pr ( -,S I j ) * Pr ( E I -,S) (2)\nBy the way that the d.c. value was arrived at, knowledge that j obtained would provide no constraint on the probability of S that depends only on j and S. This contrasts with the simpler situation when S is either definitely true or definitely false in each world.\nThe sort of revision discussed in Nilsson's original proposal was to find a single feasible W vector and then apply Bayes' formula to that prior. This yields a single point posterior for the distribution over worlds, and hence the target posterior. Another useful posterior constraint system describes all posterior distributions consistent with the prior constraints and assumption (1). This kind of estimate can be obtained from the uncompressed system by the straightforward application of a procedure for revising a linear prior system by a point conditional\n(Snow, 1991). The compressed system, unfortunately, generally doesn't easily support this kind of estimation. The conditional that comports with assumption (1) depends on the prior to which the conditional would be applied.\nRevision using a chosen prior solution is not impeded by the interaction of conditionals and priors. The steps are:\no Choose a representative prior which solves the compressed system.\no Use that prior and the tableau ( e.g. figure 2 ) to assess feasible Pr( S I j ) values to replace d.c.'s among the sentences about which evidence may be seen.\no When evidence is observed, use equation (2) and the Pr( S I j ) values to compute a consistent conditional distribution over worlds.\no Apply Bayes rule using the chosen prior and the consistent conditional to compute the posterior estimate.\n6. AN EXAMPLE OF REVISION\nA complete inference problem would include estimates for the prior probability of each of the source sentences. Typically, the resulting prior constraint system will have many solutions for W, the possible prior distribution over the (compressed) worlds. The method described here doesn't depend on how the analyst selects which solution will serve as the representative prior over the worlds. Possibilities include the maximum entropy solution, or a solution which yields a prior for the target in the middle of its interval of possible values.\nSuppose the estimated priors for the source sentences are:\nPr( A 1) =0.8 Pr(A2 )=0.7 Pr ( A3 )= 0.6 Pr ( => ) = 0.8\nand the analyst chooses from among the solutions for the distributions over the worlds the representative solution\n( 0.2, 0.2, 0.2, 0.2, 0.2 )\nThis choice is consistent with any estimate for the prior of B in the range [ 0.2, 0.8]. The next step is replace the d.c. markers in figure 2 with estimated values of Pr( S 1 j ).\nExpecting to view evidence bearing on the source sentences, the analyst replaces their d.c.'s with\nPr ( A2 I 5 ) = 0.5 Pr(A314 )=0.5\nPr ( A3l5) = 0.5\nThese choices aren't unique. They are consistent in the sense that\nPr ( A2 ) = w 1 + w 2 + w3 + Pr ( A21 5) w 5 and 0.7 = 0.2 + 0.2 + 0.2 + 0.5 * 0.2\nNow suppose evidence E is observed that bears on sentence A3, and suppose that\nApplying (1) and (2) gives as world conditionals\nCompressed Constraints in Probabilistic Logic and Their Revision 391\nPr ( E I 1 ) = 0.8 Pr ( E I 2 ) = 0.8 Pr ( E I 3 ) = 0.4 Pr ( E 14 ) = 0.6 Pr ( E I 5 ) = 0.6\nWhen these conditionals are applied to the representative prior, the corresponding posterior works out to\n( 0.25, 0.25, 0.12, 0.19, 0.19)\nand the posterior estimate for B can be anything in the interval [ 0.25, 0.75].\nNote that if evidence were observed that bore directly on B, the analyst would also have to assess Pr( B I j ) for each of the three d.c.'s pertaining to B. This, of course, is just what Kane suggested. In that case, there would be a specific point estimate for the entailed posterior of the target, rather than an interval.\n7. REVISION USING POSTERIORS\nIn his original proposal, Nilsson considered revision using posterior probabilities for sentences. The methods developed here for conditional revision can be applied directly to the case where a posterior for sentence S given evidence E rather than a conditional is known.\nAssuming that neither Pr( S ) , Pr( --,S ), nor Pr( E ) is zero, then we have the probabilistic identities\nPr(EIS ) IPr(E)=Pr(SIE) IPr(S ) Pr ( E I --,S ) I Pr ( E ) = Pr ( --,S I E ) I Pr ( --,S )\nSince we are assumed to know the truth status of S in each world, we know (or can assess consistent values for) the priors and any needed Pr( S I j )'s. Since division of all conditionals by the same constant (i.e., Pr( E ) ) has no effect on the calculated posterior, we can use the posterior to prior ratios in place of the conditionals in Bayes' formula. If S is d.c. in world j, and assumption (I) holds, then the appropriate conditional ratio is the average of the posterior to prior ratios weighted by Pr( S I j ) and its complement, Pr( --,S I j ) .\n8. CONCLUSIONS\nThe introduction of a third logical value can greatly reduce the number of variables seen in linear constraint systems for entailment in probabilistic logic. The method is most useful when the inference problem involves some easy-to-analyze inference schema, thus avoiding search in the construction of a semantic tree. Modus oonens with a conjunctive antecedent was emphasized here because of its importance in rule-based inference. Similar points\ncould have been made using other common schemata involving modus tollens, modus tollendo ponens and other Latin friends for illustration.\nFor arbitrary entailments, a resort to search may be needed. The method in that case is something of a gamble: search costs can be heavy, and there is no guarantee that a substantial savings will be realized. Nevertheless, the gamble may be attractive in moderate sized problems which tax the means of linear programming. If successful, the analyst will trade many simple steps to avoid an impractical linear program.\nThe extra effort of applying the new method is substantially confined to the creation of a suitable three value semantic tree. The derivation of a linear constraint system from the tree is conceptually simple, and no worse than twice as hard as deriving a two-value system from a conventional semantic tree with the same number of worlds.\nConstraints obtained from three-valued systems, like other linear constraint systems, can be revised in the face of evidence using Bayesian approaches. Calculation of a point posterior from a chosen feasible prior is straightforward. Compressed systems do not appear to allow the conceptually easy calculation of an exact posterior constraint system because of the interaction between conditionals and priors. Uncompressed systems do have this capability. This difference between the two kinds of prior constraints is of limited practical import, since the size of uncompressed systems effectively precludes the actual use of the exact posterior system except in small problems.\nLiterature Cited\nHalpern, J. Y., An analysis of first-order logics of probability, Proceedings IJCAI-89, 1375-1381, 1989.\nKane, T., Maximum entropy in Nilsson's probabilistic logic, Proceedings IJCAI-89, 452-457, 1989.\nLevi, 1., The Enterprise of Knowledge, Cambridge, MA: MIT Press, 1980.\nNilsson, N. J., Probabilistic logic, Artificial Intelligence 28, 71-87, 1986.\nPearl, J., On evidential reasoning in a hierarchy of hypotheses, Artificia/lntelligence 28, 9-15, 1986.\nSnow, P., Improved posterior probability estimates from prior and conditional linear constraint systems, l EEE Transactions Systems, Man, & Cybernetics 21, April 1991 (to appear)."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "In probabilistic logic entailments, even moderate size problems can yield linear constraint systems with so many variables that exact methods are impractical. This difficulty can be remedied in many cases of interest by introducing a three\u00ad valued logic (true, false, and \"don't care\"). The three-valued approach allows the construction of \"compressed\" constraint systems which have the same solution sets as their two-valued counterparts, but which may involve dramatically fewer variables. Techniques to calculate point estimates for the posterior probabilities of entailed sentences are discussed. 1. PROLIFERATION OF WORLDS An entailment problem in Nilsson's (1986) probabilistic logic derives an estimate for the prior probability of one sentence (hereafter, the \"target\") from the priors for a set of other (\"source\") sentences. The prior beliefs about the source sentences establish constraints of the form P=VW L wi = l Wi <': 0 sum over all \"worlds\"", "creator": "pdftk 1.41 - www.pdftk.com"}}}