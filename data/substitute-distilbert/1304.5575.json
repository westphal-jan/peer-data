{"id": "1304.5575", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2013", "title": "Inverse Density as an Inverse Problem: the Fredholm Equation Approach", "abstract": "in this paper we address mathematical problem currently estimating conversion rate $ \\ frac { q } { p } $ where $ p $ is lower density function and $ q $ is another density, or, more generally an arbitrary function. knowing or approximating this ratio is needed in determining problems about inference interval integration, in particular, when one needs to average a function with respect to one probability signal, given a sample from another. it is often referred as { \\ eta importance sampling } in statistical sciences and is also negatively related to statistical problem of { \\ it covariate shift } in transfer learning as well as to various mcmc statistics. it may also be useful for separating the underlying geometry of a space, say a manifold, from the density function occurring on it.", "histories": [["v1", "Sat, 20 Apr 2013 00:57:35 GMT  (3438kb,D)", "https://arxiv.org/abs/1304.5575v1", null], ["v2", "Thu, 25 Apr 2013 11:46:51 GMT  (3438kb,D)", "http://arxiv.org/abs/1304.5575v2", "Fixing a few typos in last version"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["qichao que", "mikhail belkin"], "accepted": true, "id": "1304.5575"}, "pdf": {"name": "1304.5575.pdf", "metadata": {"source": "CRF", "title": "Inverse Density as an Inverse Problem: the Fredholm Equation Approach", "authors": ["Qichao Que", "Mikhail Belkin"], "emails": [], "sections": [{"heading": null, "text": "In this paper we address the problem of estimating the ratio q p\nOur approach is based on reformulating the problem of estimating q p\nas an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically.\nThe resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement.\nWe provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel in the case of densities defined on Rd, compact domains in Rd and smooth d-dimensional sub-manifolds of the Euclidean space.\nWe also show experimental results including applications to classification and semi-supervised learning within the covariate shift framework and demonstrate some encouraging experimental comparisons. We also show how the parameters of our algorithms can be chosen in a completely unsupervised manner."}, {"heading": "1 Introduction", "text": "Density estimation is one of the best-studied and most useful problems in statistical inference. The question is to estimate the probability density function p(x) from a sample x1, . . . , xn. There is a rich literature on the subject (e.g., see the review [11]), particularly, dealing with a class of non-parametric kernel estimators going back to the work of Parzen [20].\nIn this paper we address the related problem of estimating the ratio of two functions, q(x)p(x) where p is given by a sample and q(x) is either a known function or another probability density function given by a sample. We note that estimating such ratio is necessary when one attempts to integrate a function with respect to one density, given its values on a sample obtained from another distribution. This is typical when the process generating the data is different from the averaging problem we wish to address. To give a very simple practical example of such a situation, consider a cleaning robot equipped with a dirt sensor. We would like to know how well the robot performs cleaning, however, the probability density of the robot location p(x) depends on the program and is clearly not uniform. To obtain the cleaning quality, we need to average the sensor readings with respect to the uniform density over the floor rather than the location distribution, which requires estimating the inverse 1p (here q(x) it the constant function 1).\nAn important class of applications for density ratios relates to various Markov Chain Monte Carlo (MCMC) integration techniques used in various applications, in particular, in many tasks of Bayesian inference. It is often hard to sample directly from the desirable probability distribution but it may be possible to construct an approximation which is easier to sample from. The class of techniques related to the importance sampling (see, e.g., [16]) deals with this problem by using a ratio of two densities (which is typically assumed to be known in that literature).\nar X\niv :1\n30 4.\n55 75\nv2 [\ncs .L\nG ]\n2 5\nA pr\n2 01\n3\nRecently there have been a significant amount of work on estimating the density ratio (also known as te importance function) from sampled data, e.g., [8, 13, 10, 28, 3]. Many of these papers consider this problem in the context of covariate shift assumption [24] or the so-called selection bias [36]. Our Fredholm Inverse Regularized Estimator (FIRE) framework introduces a very general and flexible approach to this problem which leads to more efficient algorithms design, provides very competitive experimental results and makes possible theoretical analysis in terms of the sample complexity and convergence rates.\nWe will provide a more detailed discussion of these and other related papers and connections to our work in Section 2, where we also discuss how the Kernel Mean Matching algorithm [8, 10] can be viewed within our framework.\nThe approach taken in our paper is based on reformulating the density ratio estimation as an integral equation, known as the Fredholm equation of the first kind (in the classical one-dimensional case), and solving it using the tools of regularization and Reproducing Kernel Hilbert Spaces. That allows us to develop simple and flexible algorithms for density ratio estimation within the popular kernel learning framework. In addition the integral operator approach separates estimation and regularization problems, thus allowing us to address certain settings where the existing methods are not applicable. The connection to the classical operator theory setting makes it easier to apply the standard tools of spectral analysis to obtain theoretical results.\nWe will now briefly outline the main idea of this paper. We start with the following simple equality underlying the importance sampling method:\nEq(h) =\n\u222b h(x)q(x)dx = \u222b h(x) q(x)\np(x) p(x)dx = Ep\n( h(x) q(x)\np(x)\n) (1)\nBy replacing the function h(x) with a kernel k(x, y), we obtain\nKp q\np (x) :=\n\u222b k(x, y) q(y)\np(y) p(y)dy =\n\u222b k(x, y)q(y)dy := Kq1(x). (2)\nThinking of the function q(x)p(x) as an unknown quantity and assuming that the right hand side is known this becomes an integral equation (known as the Fredholm equation of the first type). Note that the right-hand side can be estimated given a sample from q while the operator on the left can be estimated using a sample from p.\nTo push this idea further, suppose kt(x, y) is a \u201clocal\u201d kernel, (e.g., the Gaussian, kt(x, y) = 1 (2\u03c0t)d/2 e\u2212 \u2016x\u2212y\u20162 2t ) such that \u222b Rd kt(x, y)dx = 1. Convolution with such a kernel is close to the \u03b4-function, i.e., \u222b Rd kt(x, y)f(x)dx = f(y) +O(t). Thus we get another (approximate) integral equality:\nKt,p q\np (y) := \u222b Rd kt(x, y) q(x) p(x) p(x)dx \u2248 q(y). (3)\nIt becomes an integral equation for q(x)p(x) , assuming that q is known or can be approximated.\nWe address these inverse problems by formulating them within the classical framework1 of TiknonovPhilips regularization with the penalty term corresponding to the norm of the function in the Reproducing Kernel Hilbert Space H with kernel kH used in many machine learning algorithms.\n[Type I]: q\np \u2248 arg min f\u2208H \u2016Kpf \u2212Kq1\u20162L2,p + \u03bb\u2016f\u2016 2 H [Type II]:\nq p \u2248 arg min f\u2208H \u2016Kt,pf \u2212 q\u20162L2,p + \u03bb\u2016f\u2016 2 H\nImportantly, given a sample x1, . . . , xn from p, the integral operator Kpf applied to a function f can be approximated by the corresponding discrete sum Kpf(x) \u2248 1n \u2211 i f(xi)K(xi, x), while L2,p norm is approximated by an average: \u2016f\u20162L2,p \u2248 1 n \u2211 i f(xi)\n2. Of course, the same holds for a sample from q. Thus, we see that the Type I formulation is useful when q is a density and samples from both p and q are available, while the Type II is useful, when the values of q (which does not have to be a density function at all2) are known at the data points sampled from p.\nSince all of these involve only function evaluations at the sample points, by an application of the usual representer theorem for Reproducing Kernel Hilbert Spaces, both Type I and II formulations lead to simple, explicit and easily implementable algorithms, representing the solution of the optimization problem as linear combinations of the kernels over the points of the sample \u2211 i \u03b1ikH(xi, x) (see Section 3). We call the resulting algorithms FIRE for Fredholm Inverse Regularized Estimator.\n1In fact our formulation is quite close to the original formulation of Tikhonov. 2This could be important in various sampling procedures, for example, when the normalizing coefficients are hard to estimate.\nSome remarks would be useful at this point. Remark 1: Other norms and loss functions. Norms and loss functions other that L2,p can also be used in our setting as long as they can be approximated from a sample using function evaluations.\n\u2022 Perhaps, the most interesting is the norm L2,q norm available in the Type I setting, when a sample from the probability distribution q is available. In fact, given a sample from both p and q we can use the combined empirical norm \u03b3L2,p + (1\u2212 \u03b3)L2,q. Optimization using those norms leads to some interesting looking kernel algorithms described in Section 3. We note that the solution is still a linear combination of kernel functions on centered on the sample from p and can still be written explicitly.\n\u2022 In the Type I formulation, if the kernels k(x, y) and kH(x, y) coincide, it is possible to use the RKHS norm \u2016 \u00b7 \u2016H instead of L2,p. This formulation (see Section 3) also yields an explicit formula and is related to the Kernel Mean Matching algorithm [10] (see the discussion in Section 2), although with a different optimization procedure. We note that the solution in our framework has a natural out-of-sample extension, which becomes important for proper parameter selection.\n\u2022 Other norms/loss functions, e.g., L1,p, L1,q, -insensitive loss from the SVM regression, etc., can also be used in our framework as long as they can be approximated from a sample using function evaluations. We note that some of these may have advantages in terms of the sparsity of the resulting solution. On the other hand, a standard advantage of using the square norm is the ease of cross-validation with respect to the parameter \u03bb.\nRemark 2: Other regularization methods. Several regularization methods other than TikhonovPhilips regularization are available. We will briefly discuss the spectral cut-off regularization and its potential advantages in Section 3. We note that other methods, such as early stopping (e.g., [34, 1]) can be used and may have computational advantages. Remark 3. We note that an intermediate \u201cType 1.5\u201d formulation is also available. Specifically, for two \u201d\u03b4-kernels\u201d K and K \u2032, we have Kp qp \u2248 K \u2032 q1, thus using two different kernels in the Type I formulation\nq p \u2248 arg min f\u2208H \u2016Kpf \u2212K\u2032q1\u20162L2,p + \u03bb\u2016f\u2016 2 H (4)\nThe ability to use kernels with different bandwidth for p and q may be potentially important in practice, especially when the samples from p and q have very different cardinality. The resulting algorithms for this setting are described in in Section 3. Of course, the previous two remarks apply to this setting as well.\nSince we are dealing with a classical inverse problem for integral operators, our formulation allows for theoretical analysis using the methods of spectral theory. In Section 4 we prove concentration and error bounds as well as convergence rates for our algorithms when data are sampled from a distribution defined in Rd, a domain in Rd with boundary or a compact d-dimensional sub-manifold of a Euclidean space RN for the case of the Gaussian kernel.\nIt is interesting to note that unlike the usual density estimation problem the width of the kernel does not need to go to zero for convergence. However, it is necessary if we want a polynomial convergence rate. This is related to the exponential decay of eigenvalues for the Gaussian kernel.\nFinally, in Section 6 we discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [10] and LSIF [13] as well as the base-line thresholded inverse kernel density estimator3 (TIKDE) and importance sampling (when available).\nWe summarize the contributions of the paper as follows:\n1. We provide a formulation of estimating the density ratio (importance function) as a classical inverse problem, known as the Fredholm equation, establishing a connections to the methods of classical analysis. The underlying idea is to \u201clinearize\u201d the properties of the density by studying an associated integral operator.\n2. To solve the resulting inverse problems we apply regularization with an RKHS norm penalty. This provides a flexible and principled framework, with a variety of different norms and regularization techniques available. It separates the underlying inverse problem from the necessary regularization and leads to a family of very simple and direct algorithms within the kernel learning framework in machine learning. We call the resulting algorithms FIRE for Fredholm Inverse Regularized Estimator.\n3Obtained by dividing the standard kernel density estimator for q by a thresholded kernel density estimator for p Interestingly, despite its simplicity it performs quite well in many settings.\n3. Using the techniques of spectral analysis and concentration, we provide a detailed theoretical analysis for the case of the Gaussian kernel, for Euclidean case as well as distributions supported on a submanifold. We prove error bounds and as well as the convergence rates (as far as we know, it is the first convergence rate analysis for density ratio estimation). We also comment on other kernels and potential extensions of our analysis.\n4. We evaluate and compare our methods on several real-world and artificial different data sets and in several settings and demonstrate strong performance and better computational efficiency compared to the alternatives. We also propose a completely unsupervised technique for cross-validating the parameters of our algorithm and demonstrate its usefulness, thus addressing in our setting one of the most thorny issues in unsupervised/semi-supervised learning.\n5. Finally, our framework allows us to address several different settings related to a number of problems in areas from covariate shift classification in transfer learning to importance sampling in MCMC to geometry estimation and numerical integration. Some of these connections are explored in this paper and some we hope to address in the future work."}, {"heading": "2 Related work", "text": "The problem of density estimation has a long history in classical statistical literature and a rich variety of methods are available [11]. However, as far as we know the problem of estimating the inverse density or density ratio from a sample has not been studied extensively until quite recently. Some of the related older work includes density estimation for inverse problems [7] and the literature on deconvolution, e.g., [4].\nIn the last few years the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [19] and, in particular to the form of transfer learning known as covariate shift [24]. To give a brief summary, given the feature space X and the label space Y , two probability distributions p and q on X\u00d7Y satisfy the covariate assumption if for all x, y, p(y|x) = q(y|x). It is easy to see that training a classifier to minimize the error for q, given a sample from p requires estimating the ratio of the marginal distributions qX(x)pX(x) . Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10]. KMM is based on the observation that Eq(\u03a6(x)) = Ep( q p\u03a6(x)), where \u03a6 is the feature map corresponding to an RKHS H. It is rewritten as an optimization problem\nq(x) p(x) = arg min \u03b2\u2208L2,\u03b2(x)>0,Ep(\u03b2)=1 \u2016Eq(\u03a6(x))\u2212 Ep(\u03b2(x)\u03a6(x))\u2016H (5)\nThe quantity on the right can be estimated given a sample from p and a sample from q and the minimization becomes a quadratic optimization problem over the values of \u03b2 at the points sampled from p. Writing down the feature map explicitly, i.e., recalling that \u03a6(x) = KH(x, \u00b7), we see that the equality Eq(\u03a6(x)) = Ep( q p\u03a6(x)) is equivalent to the integral equation Eq. 2 considered as an identity in the Hilbert space H. Thus the problem of KMM can be viewed within our setting Type I (see the Remark 2 in the introduction), with a RKHS norm but a different optimization algorithm.\nHowever, while the KMM optimization problem in Eq. 5 uses the RKHS norm, the weight function \u03b2 itself is not in the RKHS. Thus, unlike most other algorithms in the RKHS framework (in particular, FIRE), the empirical optimization problem resulting from Eq. 5 does not have a natural out-of-sample extension4.\nAlso, since there is no regularizing term, the problem is less stable (see Section 6 for some experimental comparisons) and the theoretical analysis is harder (however, see [8] and the recent paper [35] for some nice theoretical analysis of KMM in certain settings).\nAnother related recent algorithm is Least Squares Importance Sampling (LSIF) [13], which attempts to estimate the density ratio by choosing a parametric linear family of functions and choosing a function from this family to minimize the L2,p distance to the density ratio. A similar setting with the Kullback-Leibler distance (KLIEP) was proposed in [29]. This has an advantage of a natural out-of-sample extension property. We note that our method for unsupervised parameter selection in Section 6 is related to their ideas. However, in our case the set of test functions does not need to form a good basis since no approximation is required.\nWe note that our methods are closely related to a large body of work on kernel methods in machine learning and statistical estimation (e.g., [26, 22, 21]). Many of these algorithms can be interpreted as inverse problems, e.g., [5, 25] in the Tikhonov regularization or other regularization frameworks. In particular,\n4In particular, this becomes an issue for model selection, see Section 6.\nwe note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].\nWe also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [33, 23]. There are also connections to geometry and density-dependent norms for semi-supervised learning, e.g., [2].\nFinally, the setting in this paper is connected to the large literature on integral equations [15]. In particular, we note [32], which analyzes the classical Fredholm problem using regularization for noisy data."}, {"heading": "3 Settings and Algorithms", "text": ""}, {"heading": "3.1 Some preliminaries", "text": "We start by introducing some objects and function spaces important for our development. As usual, the space of square-integrable functions with respect to a measure \u03c1, is defined as follows:\nL2,\u03c1 =\n{ f : \u222b \u2126 |f(x)|2d\u03c1 <\u221e } .\nThis is a Hilbert space with the inner product defined in the usual way by \u3008f, g\u30092,\u03c1 = \u222b\n\u2126 f(x)g(x)d\u03c1.\nGiven a function of two variables k(x, y) (a kernel), we define the operator K\u03c1: K\u03c1f(y) := \u222b\n\u2126\nk(x, y)f(x)d\u03c1(x).\nWe will use the notation Kt,\u03c1 to explicitly refer to the parameter of the kernel function kt(x, y), when it is a \u03b4-family.\nIf the function k(x, y) is symmetric and positive definite, then there is a corresponding Reproducing Kernel Hilbert space (RKHS) H. We recall the key property of the kernel kH: for any f \u2208 H, \u3008f, kH(x, \u00b7)\u3009H = f(x). The direct consequence of this is the Representer Theorem, which allows us to write solutions to various optimization problems over H in terms of linear combinations of kernels supported on sample points (see [26] for an in-depth discussion or the RKHS theory and the issues related to learning).\nIt is important to note that in some of our algorithms the RKHS kernel kH will be different from the kernel of the integral operator k.\nGiven a sample x1, . . . , xn from p, one can approximate the L2,p norm of a function 5 f by \u2016f\u201622,p \u2248\n1 n \u2211 i |f(xi)|2. Similarly, the integral operator Kpf(x) \u2248 1 n \u2211 i k(xi, x)f(xi). These approximate equalities can be made precise by using appropriate concentration inequalities."}, {"heading": "3.2 The FIRE Algorithms", "text": "As discussed in the introduction, the starting point for our development is the integral equality\n[Type I]: Kp q\np (x) = \u222b \u2126 k(x, y) q(y) p(y) p(y)dy = Kq1(x). (6)\nNotice that in Type I, the kernel is not necessary to be in \u03b4-family. For example, it could be linear kernel. Thus, we omit the t in the kernel for the Type I case.\nMoreover, if the kernel kt(x, y) is a Gaussian, which we will analyze in detail, or another \u03b4-family and for f sufficiently smooth Kt,qf(x) \u2248 f(x)p(x) + o(1) and hence\n[Type II]: Kt,p q\np (x) = \u222b \u2126 kt(x, y) q(y) p(y) p(y)dy = q(x) + o(1). (7)\nIn fact, for the Gaussian kernel, the o(1) term is of the order t. Since it is important that the kernel kt is in the \u03b4-family with bandwidth t, so we keep t in the notation in this case.\nAssuming that either Kq1 or q are known (for simplicity we will refer to these settings as Type I and Type II, respectively) these Eqs. 6,7 become integral equations for pq , known as the Fredholm equations of the first kind.\nTo address the problem of estimating pq we need to obtain an approximation to the solution which (a) can be obtained computationally from sampled data, (b) is stable with respect to sampling and other\n5f needs to be in a function class where point evaluations are defined.\nperturbation of the input function6 and, preferably, (c) can be analyzed using the standard machinery of functional analysis.\nTo provide a framework for solving these inverse problems we apply the classical techniques of regularization combined with the RKHS norm popular in machine learning. In particular a simple formulation of Eq.6 in terms of Tikhonov regularization with the L2,p norm is as follows:\n[Type I]: f I\u03bb = arg min f\u2208H \u2016Kpf \u2212Kq1\u201622,p + \u03bb\u2016f\u20162H (8)\nHere H is an appropriate Reproducing Kernel Hilbert Space. Similarly Eq. 7 can be written as\n[Type II]: f II\u03bb = arg min f\u2208H \u2016Kt,pf \u2212 q\u201622,p + \u03bb\u2016f\u20162H (9)\nWe will now discuss the empirical versions of these equations and the resulting algorithms in different settings and for different norms."}, {"heading": "3.3 Algorithms for the Type I setting.", "text": "Given an iid sample from p, zp = {x1, x2, . . . , xn} and an iid sample from q, zq = {x\u20321, x\u20322, . . . , x\u2032m} (we will denote the combined sample by z) we can approximate the integral operators Kp and Kq by\nKzpf(x) = 1\nn \u2211 xi\u2208zp k(xi, x)f(xi) and Kzqf(x) = 1 m \u2211 x\u2032i\u2208zq k(x\u2032i, x)f(x \u2032 i). (10)\nThus the empirical version of Eq. 8 becomes\nf I\u03bb,z = arg min f\u2208H\n1\nn \u2211 xi\u2208zp ((Kzpf)(xi)\u2212 (Kzq1)(xi))2 + \u03bb\u2016f\u20162H (11)\nWe observe that the first term of the optimization problem involves only evaluations of the function f at the points of the sample zp.\nThus, using the Representer Theorem and the standard matrix algebra manipulation we obtain the following solution:\nf I\u03bb,z(x) = \u2211 xi\u2208zp kH(xi, x)vi and v = ( K2p,pKH + n\u03bbI )\u22121 Kp,pKp,q1zq . (12)\nwhere the kernel matrices are defined as follows: (Kp,p)ij = 1 nk(xi, xj), (KH)ij = kH(xi, xj) for xi, xj \u2208 zp and Kp,q is defined as (Kp,q)ij = 1 mk(xi, x \u2032 j) for xi \u2208 zp and x\u2032j \u2208 zq.\nTo compute the whole regularization path for all \u03bb\u2019s, or computing the inverse for every \u03bb, we can use the following formula for v:\nv = Q(\u039b + n\u03bbI)\u22121Q\u22121Kp,pKp,q1zq ,\nwhere K2p,pKH = Q\u039bQ \u22121 is a diagonalization7 of K2p,pKH (i.e., \u039b is diagonal).\nWhen KH and Kp,p are obtained using the same kernel function k, i.e. 1 nKH = Kp,p, the expression\nsimplifies:\nv = 1\nn\n( K3p,p + \u03bbI )\u22121 Kp,pKp,q1zq .\nIn that case (or, more, generally, if they commute) the diagonalization is obtained by computing the eigendecomposition of Kp,p = Q\u039bQ\nT , where Q is an orthogonal matrix. Then the solution could be computed using the following formula:\nf I\u03bb,z(x) = 1\nn \u2211 xi\u2208zp k(xi, x)vi and v = Q ( \u039b3 + \u03bbI )\u22121 \u039bQTKp,q1zq .\nSimilarly to many other algorithms based on the square loss function, this formulation allows us to efficiently compute the solution for many values of the parameter \u03bb simultaneously, which is very useful for cross-validation.\n6Especially in Eq. 7, where the identity has an error term depending on t. 7Strictly speaking, an arbitrary matrix can only be reduced to the Jordan canonical form, but an arbitrarily small pertur-\nbation of any matrix can be diagonalized over the complex numbers."}, {"heading": "3.3.1 Algorithms for \u03b3L2,p + (1\u2212 \u03b3)L2,q norm.", "text": "Depending on the setting, we may want to minimize the error of the estimate over the probability distribution p, q or over some linear combination of these. A significant potential benefit of using a linear combination is that both samples can be used at the same time in the loss function. First we state the continuous version of the problem:\nf*\u03bb = arg min f\u2208H \u03b3\u2016Kpf \u2212Kq1\u201622,p + (1\u2212 \u03b3)\u2016Kpf \u2212Kq1\u201622,q + \u03bb\u2016f\u20162H (13)\nGiven a sample from p, zp = {x1, x2, . . . , xn} and a sample from q, zq = {x\u20321, x\u20322, . . . , x\u2032m} we obtain an empirical version of the Eq. 13:\nf\u2217\u03bb,z(x) = arg min f\u2208H\n\u03b3\nn \u2211 xi\u2208zp ( (Kzpf)(xi)\u2212 (Kzq1)(xi) )2 + 1\u2212 \u03b3 m \u2211 x\u2032i\u2208zq ( (Kzpf)(x\u2032i)\u2212 (Kzq1)(x\u2032i) )2 + \u03bb\u2016f\u20162H\nUsing the Representer Theorem we can derive: f\u2217\u03bb,z(x) = \u2211 xi\u2208zp vikH(xi, x) v = (K + n\u03bbI) \u22121 K11zq\nwhere\nK =\n( \u03b3\nn (Kp,p) 2 + 1\u2212 \u03b3 m KTq,pKq,p\n) KH and K1 = ( \u03b3\nn Kp,pKp,q + 1\u2212 \u03b3 m KTq,pKq,q ) Here (Kp,p)ij = 1 nk(xi, xj), (KH)ij = kH(xi, xj) for xi, xj \u2208 zp. Kp,q and Kq,p are defined as (Kp,q)ij =\n1 mk(xi, x \u2032 j) and (Kq,p)ji = 1 nk(x \u2032 j , xi) for xi \u2208 zp,x\u2032j \u2208 zq.\nWe see that despite the loss function combining both samples, the solution is still a summation of kernels over the points in the sample from p."}, {"heading": "3.3.2 Algorithms for the RKHS norm.", "text": "In addition to using the RKHS norm for regularization norm, we can also use it as a loss function:\nf*\u03bb = arg min f\u2208H \u2016Kpf \u2212Kq1\u20162H\u2032 + \u03bb\u2016f\u20162H (14)\nHere the Hilbert space H\u2032 must correspond to the kernel K and can potentially be different from the space H used for regularization. Note that this formulation is only applicable in the Type I setting since it requires the function q to belong to the RKHS H\u2032.\nGiven two samples zp, zq, it is straightforward to write down the empirical version of this problem, leading to the following formula:\nf\u2217\u03bb,z(x) = \u2211 xi\u2208zp vikH(xi, x) v = (Kp,pKH + n\u03bbI) \u22121 Kp,q1zq . (15)\nThe result is somewhat similar to our Type I formulation with the L2,p norm. We note the connection between this formulation of using the RKHS norm as a loss function and the KMM algorithm [10]. The Eq. 15 can be viewed as a regularized version of KMM (with a different optimization procedure), when the kernels K and KH are the same.\nInterestingly a somewhat similar formula arises in [13] as unconstrained LSIF, with a different functional basis (kernels centered at the points of the sample zq) and in a setting not directly related to RKHS inference."}, {"heading": "3.4 Algorithms for the Type II and 1.5 settings.", "text": "In the Type II setting we assume that we have a sample z = {x1, x2, . . . , xn} drawn from p and that we know the function values q(xi) at the points of the sample.\nReplacing the norm and the integral operator with their empirical versions, we obtain the following optimization problem:\nf II\u03bb,z = arg min f\u2208H\n1\nn \u2211 xi\u2208z (Kt,zf(xi)\u2212 q(xi))2 + \u03bb\u2016f\u20162H (16)\nRecall that Kt,z is the empirical version of Kt,p defined by\nKt,zf(x) = 1\nn \u2211 xi\u2208z kt(xi, x)f(xi)\nAs before, using the Representer Theorem we obtain an analytical formula for the solution: f II\u03bb,z(x) = \u2211 xi\u2208z kH(xi, x)vi where v = ( K2KH + n\u03bbI )\u22121 Kq. (17)\nwhere the kernel matrix K is defined by Kij = 1 nkt(xi, xj), (KH)ij = kH(xi, xj) and qi = q(xi)."}, {"heading": "3.4.1 Type 1.5: The setting and the algorithm.", "text": "This case (see Eq. 4) is intermediate between Type I and Type II. The setting is the same as in Type I, in that we are given two samples zp from p and zq from q. But similarly to Type II, we use the fact that Kp qp \u2248 K \u2032 q1 when Kp and K\u2032q are different \u03b4-function-like kernels (e.g., two Gaussians of different bandwidth). The algorithm is similar to that for Type I with the difference that the kernel matrix K \u2032q,q is computed using the kernel k\u2032(x, y): (K \u2032q,q)ij = 1 mk \u2032(xi, x \u2032 j).\nf1.5\u03bb,z(x) = \u2211 xi\u2208zp kH(xi, x)vi and v = ( K2p,pKH + n\u03bbI )\u22121 Kp,pK \u2032 q,q1zq ."}, {"heading": "3.5 Spectral Cutoff Regularization", "text": "In this section we briefly discuss an alternative form of regularization, based on thresholding the spectrum of the kernel matrix. It also leads to simple algorithms comparable to those for Tikhonov regularization and may have certain computational advantages.\nSince Kp is a compact self-adjoint operator on L2,p, its eigenfunctions {u0, u1, . . . } form a complete orthogonal basis for L2,p. An alternative method of regularization is the so-called spectral cutoff where the problem is restricted to the subspace spanned by the top few eigenfunctions of Kp Thus the regularization problems become\nf I,spec\u03bb = arg min f\u2208Hk \u2016Kpf \u2212Kq1\u201622,p\nf II,spec\u03bb = arg min f\u2208Ht,k \u2016Kt,pf \u2212 q\u201622,p\nwhere Hk and Ht,k is the finite dimensional subspace of L2,p spanned by the eigenvectors of Kp and Kt,p corresponding to the k largest eigenvalues.\nWithout going into detail, it can be seen that the corresponding empirical optimization problems are\nf I,spec\u03bb,z = arg min f\u2208Hk,z\n1\nn \u2211 xi\u2208zp (Kzpf(xi)\u2212Kt,zq1(xi))2 (18)\nf II,spec\u03bb,z = arg min f\u2208Ht,k,z\n1\nn \u2211 xi\u2208z (Kt,zpf(xi)\u2212 q(xi))2 (19)\nwhere the span of eigenvectors of the kernel matrix K is taken instead of the eigenfunctions of Kp or Kt,p. For this algorithm, we assume KH and K1 use the same kernel. Then the solution to the empirical regularization problems given in Eqs. 18,19 are respectively\nf I,spec\u03bb,z (x) = 1\nn \u2211 xi\u2208zp k(xi, x)vi\nv = Qk\u039b \u22122 k Q T kK21zq\n(20)\nf II,spec\u03bb,z (x) = 1\nn \u2211 xi\u2208z kt(xi, x)vi\nv = Qk\u039b \u22122 k Q T k q\n(21)\nwhere K1 = Q\u039bQ T is the eigendecomposition of K1 with orthogonal matrix Q and diagonal matrix \u039b, and Qk and \u039bk is the submatrices of Q and \u039b corresponding to the k largest eigenvalues of the kernel matrix K1 and the remaining objects are defined in the previous subsection.\nWe note that spectral regularization can be faster computationally as it requires to compute only the top few eigenvectors of the kernel matrix. There are several efficient algorithms for computing eigendecomposition when only the first k eigenvalues are needed. Thus spectral regularization can be more computationally efficient than the Tikhonov regularization which potentially requires a full eigen-decomposition or matrix multiplication."}, {"heading": "3.6 Comparison of type I and type II settings.", "text": "While at first glance the type II, setting may appear to be more restrictive than type I, there are a number of important differences in their applicability.\n1. In Type II setting q does not have to be a density function (i.e., non-negative and integrate to one).\n2. Eq. 11 of the Type I setting cannot be easily solved in the absence of a sample zq from q, since estimating Kq requires either sampling from q (if it is a density) or estimating the integral in some other way, which may be difficult in high dimension but perhaps of interest in certain low-dimensional application domains.\n3. There are a number of problems (e.g., many problems involving MCMC) where q(x) is known explicitly (possibly up to a multiplicative constant), while sampling from q is expensive or even impossible computationally [17].\n4. Unlike Eq. 8, Eq. 9 has an error term depending on the kernel, which is essentially the difference between the kernel and the \u03b4-function. For example, in the important case of the Gaussian kernel, the error is of the order O(t), where t is the variance.\n5. While a number of different norms are available in the Type I setting, only the L2,p norm is available for Type II."}, {"heading": "4 Theoretical analysis: bounds and convergence rates for Gaus-", "text": "sian Kernels\nIn this section, we state our main results on bounds and convergence rates for our algorithm based on Tikhonov regularization with a Gaussian kernel. We consider both Type I and Type II settings for the Euclidean and manifold cases and make a remark on the Euclidean domains with boundary.\nTo simplify the theoretical development the integral operator and the RKHS H will correspond to the same Gaussian kernel kt(x, y). Most of the proofs will be given in the next Section 5. We note that two Gaussian kernels with different bandwidth parameters can be analyzed using only minor modifications to our arguments."}, {"heading": "4.1 Assumptions", "text": "Before proceeding to the main results, we will state the assumptions on the density functions p and q and the basic setting for our theorems:\n1. The set \u2126 where the density function p is defined could be one of the following: (1) the whole Rd; (2) a compact smooth Riemannian sub-manifold M of d-dimension in Rn. In both cases, we need 0 < p(x) < \u0393 for any x \u2208 \u2126. The function q should satisfy q \u2208 L2,p and needs to be bounded from above. We will also make some remarks about a compact domain in Rd with boundary.\n2. We also require q(x)p(x) \u2208 W 2 2 (\u2126) and q \u2208 W 22 (\u2126), where W 22 (\u2126) is the Sobolev space of functions on \u2126\n(e.g., [30]). Certain properties of W 22 (\u2126) will be discussed later in the proof.\nIt will be important for us that H is isometric to L2,p under the map K1/2p : L2,p \u2192 H, that is, \u2016f\u2016H = \u2016K\u22121/2p f\u2016L2,p for any f \u2208 H. Here the integral operator Kp uses the RKHS kernel corresponding to H."}, {"heading": "4.2 Main Theorems", "text": ""}, {"heading": "4.2.1 Type I setting", "text": "We will provide theoretical results for our setting Type I, where both the operator and the regularization\nkernel are Gaussian kt(x, y) = 1 (2\u03c0t)d/2 e\u2212 \u2016x\u2212y\u20162 2t with the same bandwidth parameter t.\nTheorem 1. Let p and q be two density functions on \u2126 and q be another density over \u2126 satisfying the assumption in Sec. 4.1. Given n points, zp = {x1, x2, . . . , xn}, i.i.d. sampled from p and m points, zq = {x\u20321, x\u20322, . . . , x\u2032m}, i.i.d. sampled from q, and for small enough t, for the solution to the optimization problem in (11), with confidence at least 1\u2212 2e\u2212\u03c4 , we have\n(1) If the domain \u2126 is Rd,\u2225\u2225\u2225\u2225f I\u03bb,z \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n\u2264C1 ( t\n\u2212 log \u03bb\n) s 2\n+ C2\u03bb 1\u2212\u03b1 2 (Approximating Error)\n+ C3\n\u221a \u03c4\n\u03bbtd/2 ( 1\u221a m + 1 \u03bb1/6 \u221a n ) (Sampling Error),\n(22)\nwhere C1, C2, C3 are constants independent of t, \u03bb. (2) If the domain \u2126 is a compact manifold without boundary of d dimension,\u2225\u2225\u2225\u2225f I\u03bb,z \u2212 qp \u2225\u2225\u2225\u2225 2,p \u2264C1t+ C2\u03bb1/2 (Approximating Error)\n+ C3\n\u221a \u03c4\n\u03bbtd/2 ( 1\u221a m + 1 \u03bb1/6 \u221a n ) (Sampling Error),\n(23)\nwhere C1, C2, C3 are constants independent of t, \u03bb.\nProof. See Section 5.\nRemark 1: convergence for fixed t. For the Euclidean case in Eq. 22, with fixed kernel width t, the error will converge to 0, as \u03bb \u2192 0 given sufficiently many data points. However the required number of points is exponential in 1\u03bb . This is related to the fact the eigen-values of the operator Kp decay exponentially fast, when the kernel is Gaussian. On the other hand choosing both t and \u03bb as a function of n leads to a much better polynomial rate given below. Remark 2. A minor modification of the proof provides the following simpler version of Eq. 22:\u2225\u2225\u2225\u2225f I\u03bb,z \u2212 qp \u2225\u2225\u2225\u2225 2,p \u2264 C1t s 2 + C2\u03bb 1 2 + C3 \u221a \u03c4 \u03bbtd/2 ( 1\u221a m + 1 \u03bb1/6 \u221a n ) (24)\nAs a consequence we obtain the following corollary establishing the convergence rates:\nCorollary 2. Assuming m > \u03bb1/3n, with confidence at least 1\u2212 2e\u2212\u03c4 , we have the following: (1) If \u2126 = Rd, \u2225\u2225\u2225\u2225f I\u03bb,z \u2212 qp \u2225\u2225\u2225\u22252\n2,p\n= O (\u221a \u03c4n\u2212 s 3.5s+d ) (2) If \u2126 is a d-dimensional sub-manifold of a Euclidean space,\u2225\u2225\u2225\u2225f I\u03bb,z \u2212 qp \u2225\u2225\u2225\u22252 2,p = O (\u221a \u03c4n\u2212 1 3.5+d/2 )\nProof. For the Euclidean space, set t = n \u2212 110.5 3 s+d , \u03bb = n \u2212 s10.5 3 s+d and apply Theorem 1 (Eq. 24 for the Euclidean case). For the sub-manifold case set t = n\u2212 1 7+d , \u03bb = n\u2212 2 7+d ."}, {"heading": "4.2.2 Type II setting", "text": "In this section we provide an analysis for the Type II setting and also make a remark about the error analysis for the compact domains in Rd.\nRecall that in Type II setting we have a set of points sampled from p and assume that the values of q on those points are known. Note, that q does not have to be a density function.\nTheorem 3. Let p be a density function on \u2126 and q be a function satisfying the assumptions in Sec. 4.1. Given n points z = {x1, x2, . . . , xn} sampled i.i.d. from p, and for sufficiently small t, for the solution to the optimization problem in (16), with confidence at least 1\u2212 2e\u2212\u03c4 , we have\n(1) If the domain \u2126 is Rd,\u2225\u2225\u2225\u2225f II\u03bb,z \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n\u2264C1 ( t\n\u2212 log \u03bb\n) s 2\n+ C2\u03bb 1\u2212\u03b1 2 + C3\u03bb \u2212 13 \u2016Kt,q1\u2212 q\u20162,p + C4 \u221a \u03c4\n\u03bb3/2td/2 \u221a n , (25)\nwhere C1, C2, C3, C4 are constants independent of t, \u03bb. Moreover, \u2016Kt,q1\u2212 q\u20162,p = O(t).\n(2) If \u2126 is a d-dimensional sub-manifold of a Euclidean space,\u2225\u2225\u2225\u2225f II\u03bb,z \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n\u2264C1t+ C2\u03bb1/2 + C3\u03bb\u2212 1 3 \u2016Kt,q1\u2212 q\u20162,p + C4\n\u221a \u03c4\n\u03bb3/2td/2 \u221a n , (26)\nwhere C1, C2, C3, C4 are constants independent of t, \u03bb. Moreover, \u2016Kt,q1\u2212 q\u20162,p = O(t 1\u2212\u03b5) for any \u03b5 > 0.\nRemark. It can be shown that if \u2126 is a compact subset with sufficiently smooth boundary in Rd, we have the same bound with (1) except for \u2016Kt,q1\u2212 q\u20162,p = O(t 1 4\u2212\u03b5) for any any \u03b5 > 0.\nAs before, we obtain the rates as a corollary:\nCorollary 4. With confidence at least 1\u2212 2e\u2212\u03c4 , we have: (1) If \u2126 = Rd, \u2225\u2225\u2225\u2225f II\u03bb,z \u2212 qp \u2225\u2225\u2225\u22252\n2,p\n= O (\u221a \u03c4n \u2212 1 4+ 5 6 d ) (2) If \u2126 is a d-dimensional sub-manifold of a Euclidean space, than for any 0 < \u03b5 < 1\u2225\u2225\u2225\u2225f II\u03bb,z \u2212 qp \u2225\u2225\u2225\u22252 2,p = O (\u221a \u03c4n \u2212 1\u2212\u03b5 4\u22124\u03b5+5 6 d )\nProof. For the case of Rd, set t = n\u2212 1 4.8+d , \u03bb = n \u2212 1 4+ 5 6 d . For case of sub-manifold case, set t = n\u2212 1\u2212\u03b5 4.8\u22124.8\u03b5+d , \u03bb = n \u2212 1\u2212\u03b5 4\u22124\u03b5+5 6 d . Apply Theorem 3."}, {"heading": "5 Proofs of Theorems", "text": "In this section, we provide a proof for the our main Theorem 1 for setting I. The proof for the Theorem 3 for the setting type II is along similar lines and can be found in the appendix."}, {"heading": "5.1 Basics about RKHS", "text": "Since Kt,\u03c1 is a self-adjoint operator, its eigenfunctions {u0,t, u1,t, . . . } form a complete orthogonal basis for L2,\u03c1. Denote the eigenvalues of Kt,\u03c1 by {\u03c30,t, \u03c30,t, . . . }. The norm of Kt,\u03c1, \u2016Kt,\u03c1\u2016L2,\u03c1\u2192L2,\u03c1 \u2264 maxi \u03c3i,t < c for a constant c. We know that Ht is isometric to L2,\u03c1 under the map K1/2t,\u03c1 : L2,\u03c1 \u2192 Ht, i.e. \u2016f\u2016Ht = \u2016K\u22121/2t,\u03c1 f\u2016L2,\u03c1 for any f \u2208 Ht, and this is the definition we use for the norm \u2016 \u00b7 \u2016Ht of Ht. This also implies that \u2016K\u22121/2t,\u03c1 f\u2016L2,\u03c1 <\u221e for any f \u2208 Ht. And Kt,\u03c1 is defined using the spectrum of Kt,\u03c1,\nKt,\u03c1f = \u2211 i \u03c3i,\u03c1\u3008f, ui,t\u3009ui,t"}, {"heading": "5.2 Proof of Theorem 1", "text": "Proof. Recall the definition of f I\u03bb and f I \u03bb,z in Eq. 8 and Eq. 11. By the triangle inequality, we have\u2225\u2225\u2225\u2225qp \u2212 f I\u03bb,z \u2225\u2225\u2225\u2225 2,p \u2264 \u2225\u2225\u2225\u2225qp \u2212 f I\u03bb \u2225\u2225\u2225\u2225 2,p + \u2225\u2225f I\u03bb \u2212 f I\u03bb,z\u2225\u22252,p .\n=(Approximation Error) + (Sampling Error)\n(27)\nThe approximation error \u2225\u2225\u2225f I\u03bb \u2212 qp\u2225\u2225\u2225\n2,p is a measure of the distance between qp and the optimal approximation given by algorithm (8) given infinite number of data. The sampling error term \u2225\u2225\u2225f I\u03bb \u2212 f I\u03bb,z\u2225\u2225\u2225\n2,p the difference\nbetween f I\u03bb and f I \u03bb,z, depending on the data points.\nAs typical in these types of estimates our proof consists of two parts: bounding the approximating error,\u2225\u2225\u2225f I\u03bb \u2212 qp\u2225\u2225\u2225 2,p in Lemma 7 and providing a concentration bound for \u2225\u2225\u2225f I\u03bb,z \u2212 f I\u03bb\u2225\u2225\u2225 2,p in Lemma 8. The theorem follows immediately by putting these two results together."}, {"heading": "5.2.1 Bound for Approximation Error", "text": "First of all, let present two lemmas that are useful for bounding the approximation error.\nLemma 5. Let \u03bb > 0. If function f \u2208W 22 (Rd) and p(x) > 0 for any x \u2208 Rd, then\narg min g\u2208L2,p (\u2225\u2225\u2225f \u2212K1/2t,p g\u2225\u2225\u22252 2,p + \u03bb\u2016g\u201622,p ) \u2264 D1ts + \u03bbD2 \u2016f\u201622 . (28)\nfor constants D1, D2.\nProof. See Appendix A.\nLemma 6. Let \u03bb > 0. If function f \u2208W 22 (M) defined on a compact Riemann sub-manifold of d-dimension in a Euclidean space, then\narg min g\u2208L2,p (\u2225\u2225\u2225f \u2212K1/2t,p g\u2225\u2225\u22252 2,p + \u03bb\u2016g\u201622,p ) \u2264 D1 \u2016f\u20162,p t 2 + \u03bbD2 \u2016f\u201622,p . (29)\nfor constants D1, D2.\nProof. See Appendix B\nNow we can present the lemma that gives the bound of the approximation error in the following lemma.\nLemma 7. Let p, q be two density functions of probability measure over a domain X satisfying the assumptions in 4.1. The solution to the optimization problem in (8), f I\u03bb, satisfies the following inequality,\n(1) when the domain X is Rd,\u2225\u2225\u2225\u2225f I\u03bb \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n\u2264 C1 ( t\nlog( 1\u03bb )\n)s/2 + C2\u03bb 1/2\nfor constants C1, C2 which are independent of \u03bb and t. (2) when the domain X is a compact Riemannian sub-manifold M of d dimension in RN ,\u2225\u2225\u2225\u2225f I\u03bb \u2212 qp \u2225\u2225\u2225\u2225 2,p \u2264 C1t+ C2\u03bb1/2\nfor constants C1, C2 which are independent of \u03bb and t.\nProof. Recall the equation (8). By functional calculus, we have analytical formula for f I\u03bb as follows,\nf I\u03bb = \u2211 i \u03c32i,t \u03c33i,t + \u03bb \u3008Kt,q1, ui,t\u30092ui,t = ( K3t,p + \u03bbI )\u22121K2t,pKt,q1 = (K3t,p + \u03bbI)\u22121K3t,p qp . The last equation is because\nKt,q1 = Kt,p q\np .\nThus the approximating error is\u2225\u2225\u2225\u2225f I\u03bb \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n= \u2225\u2225\u2225\u2225(K3t,p + \u03bbI)\u22121K3t,p qp \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n(30)\nNotice that ( K3t,p + \u03bbI )\u22121K3t,p qp in (30) can also be rewritten as ( K3t,p + \u03bbI )\u22121K3t,p qp = arg ming\u2208L2,p \u2225\u2225\u2225\u2225qp \u2212K3/2t,p g \u2225\u2225\u2225\u22252 2,p + \u03bb\u2016g\u201622,p\nThus, \u2225\u2225\u2225\u2225(K3t,p + \u03bbI)\u22121K3t,p qp \u2212 qp \u2225\u2225\u2225\u22252\n2,p\n\u2264 min g\u2208L2,p \u2225\u2225\u2225\u2225qp \u2212K3/2t,p g \u2225\u2225\u2225\u22252\n2,p\n+ \u03bb\u2016g\u201622,p (31)\nThe minimum of the above optimization problem can always be bounded by any specific g \u2208 L2,p. And we will expend the above formula such that we can take advantages of Lemma 5 and 6. To this end, we define an operator\ng\u2217\u03bb = T (f, \u03bb) = arg min g\u2208L2,p \u2225\u2225\u2225f \u2212K1/2t,p g\u2225\u2225\u22252 2,p + \u03bb\u2016g\u201622,p\nBy functional calculus, it is not hard to see that g\u2217\u03bb = (Kt,p + \u03bbI) \u22121Kt,pf . If f \u2208 W 22 , so is g\u2217\u03bb, this is because Kt,p is an integral operator with Gaussian kernel and Gaussian kernel is in W s2 for any s > 0. Also, we should have \u2016g\u2217\u03bb\u20162,p \u2264 \u2016f\u20162,p, because \u2016(Kt,p + \u03bbI)\u22121Kt,p\u2016L2,p\u2192L2,p \u2264 1.\nNow let g\u22171 = T ( q p , \u03bb ) , g\u22172 = T (g\u22171 , \u03bb) , g\u22173 = T (g\u22172 , \u03bb). We have g\u22171 , g\u22172 is also in W 22 and \u2016g\u22172\u20162,p \u2264\n\u2016g\u22171\u20162,p \u2264 \u2016 q p\u20162,p. Now we could expend (31),\nmin g\u2208K2,p \u2225\u2225\u2225\u2225qp \u2212K3/2t,p g \u2225\u2225\u2225\u22252\n2,p\n+ \u03bb\u2016g\u201622,p \u2264 \u2225\u2225\u2225\u2225qp \u2212K3/2t,p g\u22173 \u2225\u2225\u2225\u22252 2,p + \u03bb\u2016g\u22173\u201622,p\n= \u2225\u2225\u2225\u2225qp \u2212K1/2t,p g1 +K1/2t,p g1 \u2212Kt,pg2 +Kt,pg1 \u2212K3/2t,p g3 \u2225\u2225\u2225\u22252\n2,p\n+ \u03bb\u2016g\u22173\u201622,p\nBy inequality (a+ b+ c)2 \u2264 3(a2 + b2 + c3), we have\nmin g\u2208K2,p \u2225\u2225\u2225\u2225qp \u2212K3/2t,p g \u2225\u2225\u2225\u22252\n2,p\n+ \u03bb\u2016g\u201622,p\n\u22643 (\u2225\u2225\u2225\u22251p \u2212K1/2t,p g\u22171 \u2225\u2225\u2225\u22252\n2,p\n+ \u03bb\u2016g\u22171\u201622,p\n) + 3 (\u2225\u2225\u2225K1/2t,p (g\u22171 \u2212K1/2t,p g\u22172)\u2225\u2225\u22252 2,p + \u03bb\u2016g\u22172\u201622,p )\n+ 3 (\u2225\u2225\u2225Kt,p (g\u22172 \u2212K1/2t,p g\u22173)\u2225\u2225\u22252 2,p + \u03bb\u2016g\u22173\u201622,p )\n\u22643 (\u2225\u2225\u2225\u2225qp \u2212K1/2t,p g\u22171 \u2225\u2225\u2225\u22252\n2,p\n+ \u03bb\u2016g\u22171\u201622,p\n) + 3c1/2 (\u2225\u2225\u2225g\u22171 \u2212K1/2t,p g\u22172\u2225\u2225\u22252 2,p + \u03bb\u2016g\u22172\u201622,p )\n+ 3c (\u2225\u2225\u2225g\u22172 \u2212K1/2t,p g\u22173\u2225\u2225\u22252 2,p + \u03bb\u2016g\u22173\u201622,p )\nThe last inequality is because \u2016Kt,p\u2016L2\u2192L2 < c for constant c > 1. Up to now, the proof is valid for both cases in the theorem. And we can apply Lemma 5 and 6 to get the bounds for both cases. By Lemma 5, for the densities p, q over Rd, we have\u2225\u2225\u2225\u2225(K3t,p + \u03bbI)\u22121K3t,p qp \u2212 qp \u2225\u2225\u2225\u22252 2,p \u2264 min g\u2208K2,p \u2225\u2225\u2225\u22251p \u2212K3/2t,p g \u2225\u2225\u2225\u22252 2,p + \u03bb\u2016g\u201622,p \u2264 9cD1 \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u22252 2,p ( t log( 1\u03bb ) )s + 9cD2\u03bb 1\u2212\u03b1 \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u22252 2,p\n(32)\nRecall (30), we have\u2225\u2225\u2225\u2225f I\u03bb \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n\u2264 \u221a 9cD1 \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u22252\n2,p\nts + 9cD2\u03bb \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u22252\n2,p\n\u2264 C1 ( t\nlog( 1\u03bb )\n)s/2 + C2\u03bb (1\u2212\u03b1)/2 (33)\nwhere C1 = 3 \u221a cD1 \u2225\u2225\u2225 qp\u2225\u2225\u2225 2,p , C2 = 3 \u221a cD2 \u2225\u2225\u2225 qp\u2225\u2225\u2225 2,p .\nApplying Lemma 6, we will have the result for manifold case,\u2225\u2225\u2225\u2225f I\u03bb \u2212 qp \u2225\u2225\u2225\u2225\n2,p\n\u2264 \u221a 9cD1 \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u22252\n2,p\nt2 + 9cD2\u03bb \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u22252\n2,p \u2264C1t+ C2\u03bb1/2 (34)\nwhere C1 = 3 \u221a cD1 \u2225\u2225\u2225 qp\u2225\u2225\u2225 2,p , C2 = 3 \u221a cD2 \u2225\u2225\u2225 qp\u2225\u2225\u2225 2,p ."}, {"heading": "5.2.2 Bound for Sampling Error", "text": "In the next lemma, we will give concentration of the sampling error, \u2016f I\u03bb \u2212 f I\u03bb,z\u20162,p. Lemma 8. Let p be a density of a probability measure over a domain X and q another density function. They satisfy the assumptions in 4.1. Consider f I\u03bb and f I \u03bb,z defined in (8) and (11), with confidence at least\n1\u2212 2e\u2212\u03c4 , we have \u2225\u2225f I\u03bb \u2212 f I\u03bb,z\u2225\u22252,p \u2264 C3(\u03bat\u221a\u03c4\u03bb\u221am + kt \u221a \u03c4 \u03bb7/6 \u221a n ) where \u03bat = supx\u2208\u2126 kt(x, x) = 1 (2\u03c0t)d/2\nProof. Recall that, f I\u03bb = arg min\nf\u2208Ht \u2016Kt,pf \u2212Kt,q1\u201622,p + \u03bb\u2016f\u20162Ht\nand\nf I\u03bb,z = arg min f\u2208Ht,z\n1\nn \u2211 xpi\u2208zp ( (Kzpf)(x p i )\u2212 (Kzq1)(x p i ) )2 + \u03bb\u2016f\u20162Ht\nUsing functional calculus, we will get the explicit formula for f I\u03bb and f I \u03bb,z as follows, f I\u03bb = ( K3p + \u03bbI )\u22121K2pKq1 and\nf I\u03bb,z = ( K3zp + \u03bbI )\u22121 K2zpKzq1.\nThen the bound for sampling error is to bound the above two objects. Let f\u0303 = ( K3zp + \u03bbI )\u22121 K2pKq1. We\nhave f I\u03bb \u2212 f I\u03bb,z = f I\u03bb \u2212 f\u0303 + f\u0303 \u2212 f I\u03bb,z. For f I\u03bb \u2212 f\u0303 , using the fact that ( K3p + \u03bbI ) f I\u03bb = K2pKq1, we have\nf I\u03bb \u2212 f\u0303 =f I\u03bb \u2212 ( K3zp + \u03bbI )\u22121 ( K3p + \u03bbI ) f I\u03bb\n= ( K3zp + \u03bbI )\u22121 ( K3zp \u2212K 3 p ) f I\u03bb\nAnd\nf\u0303 \u2212 f I\u03bb,z = ( K3zp + \u03bbI )\u22121 K2pKq1\u2212 ( K3zp + \u03bbI )\u22121 K2zpKzq1\n= ( K3zp + \u03bbI )\u22121 ( K2pKq \u2212K2zpKzq ) 1\nNotice that we have K3zp \u2212 K 3 p and K2zpKzq \u2212 K 2 pKq in the identity we get. For these two objects, it is\nnot hard to verify the following identities,\nK3zp \u2212K 3 p = ( Kzp \u2212Kp )3 +Kp ( Kzp \u2212Kp )2 + ( Kzp \u2212Kp ) Kp ( Kzp \u2212Kp ) + ( Kzp \u2212Kp )2Kp +K2p ( Kzp \u2212Kp ) +Kp ( Kzp \u2212Kp ) Kp + ( Kzp \u2212Kp ) K2p.\nAnd\nK2zpKzq \u2212K 2 pKq = ( Kzp \u2212Kp )2 (Kzq \u2212Kq)+Kp (Kzp \u2212Kp) (Kzq \u2212Kq)+ (Kzp \u2212Kp)Kp (Kzq \u2212Kq) +K2p ( Kzq \u2212Kq ) + ( Kzp \u2212Kp\n)2Kq +Kp (Kzp \u2212Kp)Kq + (Kzp \u2212Kp)KpKq. Thus, in these two identities, the only two random variables are Kzp \u2212 Kp and Kzq \u2212 Kq. By results\nabout concentration of Kzp and Kzq , we have with probability 1\u2212 2e\u2212\u03c4 ,\n\u2016Kzp \u2212Kp\u2016H\u2192H \u2264 \u03bat \u221a \u03c4\u221a n , \u2016Kzq \u2212Kq\u2016H\u2192H \u2264 \u03bat \u221a \u03c4\u221a m ,\n\u2225\u2225Kzq1\u2212Kq1\u2225\u2225H \u2264 \u03bat \u221a 2\u03c4\u221a m\n(35)\nAnd we know that for a large enough constant c which is independent of t and \u03bb, \u2016Kp\u2016H\u2192H < c, \u2016Kq\u2016H\u2192H < c, \u2225\u2225\u2225\u2225(K3zp + \u03bbI)\u22121\u2225\u2225\u2225\u2225\nH\u2192H \u2264 1 \u03bb , \u2016Kq1\u2016H < c\nand\n\u2016f I\u03bb\u20162H = \u2211 i \u03c35i (\u03c33i + \u03bb) 2 \u2329 q p , ui \u232a2 \u2264 ( sup \u03c3>0\n\u03c35\n(\u03c33 + \u03bb)2 )\u2211 i \u2329 q p , ui \u232a2 \u2264 c2 1 \u03bb1/3 \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u22252 2,p\nthus, \u2016f I\u03bb\u2016H \u2264 c\u03bb1/6 \u2225\u2225\u2225 qp\u2225\u2225\u2225\n2,p . Notice that \u2225\u2225\u2225(Kzp \u2212Kp)2\u2225\u2225\u2225H \u2264 \u2225\u2225Kzp \u2212Kp\u2225\u22252H and \u2225\u2225\u2225(Kzp \u2212Kp)3\u2225\u2225\u2225H \u2264 \u2225\u2225Kzp \u2212Kp\u2225\u22253H, both of this could\nbe of smaller order compared with \u2225\u2225Kzp \u2212Kp\u2225\u2225H. For simplicity we hide the term including them in the final bound without changing the dominant order. We could also hide the terms with the product of any two the random variables in Eq. 40, which is of prior order compared to the term with only one random variable. Now let us put everything together,\n\u2016f I\u03bb \u2212 f I\u03bb,z\u20162,p \u2264 c1/2\u2016f I\u03bb \u2212 f I\u03bb,z\u2016Ht \u2264c1/2 ( c3\u03bat \u221a \u03c4\n\u03bb7/6 \u221a n \u2225\u2225\u2225\u2225qp \u2225\u2225\u2225\u2225\n2,p\n+ c2\u03bat \u221a \u03c4\n\u03bb \u221a m\n)\n\u2264C3 ( \u03bat \u221a \u03c4\n\u03bb \u221a m\n+ \u03bat \u221a \u03c4\n\u03bb7/6 \u221a n\n)\nwhere C3 = c 5/2 max ( c \u2225\u2225\u2225 qp\u2225\u2225\u2225\n2,p , 1\n) ."}, {"heading": "6 Experiments", "text": "In this section we explore the empirical performance of our methods under various settings. We will primarily concentrate on our setting Type II and use the same Gaussian kernel for the integral operator and the regularization term to simplify model selection.\nThis section is organized as follows. In Subsection 6.1 we describe a completely unsupervised procedure for parameter selection, which will be used throughout the experimental section. In Subsection 6.2 we briefly describe the data sets and the re-sampling procedures we use. In Subsection 6.3 we provide a comparison between our methods using different norms and other methods based on the expected performance under our evaluation criteria. In Subsection 6.4 we provide a number of experiments comparing our method to different methods on several different data sets for classification and regression tasks. Finally in Subsection 6.5 we study performance of different kernels in both Type-I and Type-II setting using two simulated data sets."}, {"heading": "6.1 Experimental Setting and Model Selection", "text": "The setting: In our experiments, we have a set of a data set Xp = {xpi , i = 1, 2, . . . , n} and another set of instances Xq = {xqj , j = 1, 2, . . . ,m}. The goal is to estimate q p under the assumption that X\np is sampled from p and Xq is sampled from q.\nWe note that our algorithms typically has two parameters, which need to be selected, the kernel width t and the regularization parameter \u03bb. In general choosing parameters in a unsupervised or semi-supervised setting is a hard problem as it may be difficult to validate the resulting classifier/estimator. However, certain features of our setting allow us to construct an adequate unsupervised proxy for the performance of the algorithm. Now we construct a performance measure for the quality of the estimator. Performance Measure. We describe a set of performance measures to use for parameter selection.\nFor a given function u, we have the following importance sampling equality (Eq. 1): Eq(u(x)) = Ep ( u(x) q(x)\np(x)\n) .\nIf f(x) is an approximation of the true ratio qp , using the samples from X p and Xq respectively, we will have the following approximation to the above equation:\n1\nn n\u2211 i=1 u(xpi )f(x p i ) \u2248 1 m m\u2211 j=1 u(xqj).\nTherefore, after obtaining an estimate f of the ratio, we can validate it by using a set of test functions U = {u1, u2, . . . , uF } using the following performance measure:\nJ(f ;Xp, Xq, U) = 1\nF F\u2211 l=1  n\u2211 i=1 ul(x p i )f(x p i )\u2212 m\u2211 j=1 ul(x q j) 2 (36) where U = {u1, u2, . . . , uF } is a collection of function chosen as criterion. Using this performance measure allows various cross-validation procedures to be sued for parameter selection.\nWe note that this way of measuring the error is related to the LSIF [13] and KLIEP [29], algorithms. However, there a similar measure is used to construct an approximation to the ratio fracqp using functions u1, . . . , uF as a basis. In our setting, to choose parameters, we can use validations sets (such as linear functions) which are poorly suited as a basis for approximating the density ratio. Choice of validation function sets for parameter selection. In principle, any set of (sufficiently wellbehaved) functions can be used as a validation set. From a practical point of view we would like functions to be simple to compute and readily available for different data sets.\nIn the our experiments, we will use the following two families of functions for parameter tuning:\n(1) Sets of random linear functions u(x) = \u03b2Tx where \u03b2 \u223c N(0, Id).\n(2) Sets of random half-space indicator functions, u(x) = 1\u03b2T x>0 where \u03b2 \u223c N(0, Id).\nRemark 1: We have also tried (a) coordinates functions, (b) random combination of kernel functions, and (c) random combination of kernel functions with thresholding. In our experience the coordinate functions are not rich enough for adequate parameter tuning. On the other hand, using the kernel functions significantly increases the complexity of the procedure (due to the necessity of choosing the kernel width and other parameters) without increasing the performance significantly. Remark 2: Note that for linear functions, the cardinality of the set should not exceed the dimension of the space due to linear dependence. Remark 3: It appears that linear functions work well for regression tasks, while half-spaces are well-suited for classification.\nProcedures for parameter selection. We optimize the performance using cross-validation by splitting the data set in two parts Xp,train and Xq,train used for training and Xp,cv and Xq,cv used for validation, and repeating this process five times to find the optimal values of parameters8.\nFor the two parameters which need to be tuned, the kernel width t and the regularization parameter \u03bb, we specify a parameter grid as follows. The range for kernel width t is (t0, 2t0, . . . , 2\n9t0), where t0 is the average distance of the 10 nearest neighbors, and regularization parameter \u03bb is (1e\u2212 5, 1e\u2212 6, . . . , 1e\u2212 10)."}, {"heading": "6.2 Data sets and Resampling", "text": "In our experiments, several data sets are considered: Bank8FM, CPUsmall and Kin8nm for regression; and USPS and 20 news groups for classification.\nFor each data set, we assume they are i.i.d. sampled from a distribution denoted by p. We draw the first 500 or 1000 points from the original data set as Xp. To obtain Xq, we apply a resampling scheme on the remaining points of the original data set. Two ways of resampling, using the features of the data and using the label information, are used (along the lines similar to those proposed in [8]).\nSpecifically, given a set of data points with labels {(x1, y1), (x2, y2), . . . , (xn, yn)} we resample as follows:\n\u2022 Resampling using feature information (labels yi are not used). We subsample the data points so that the probability Pi of selecting the instance i, is defined by the following (sigmoid) function:\nPi = e(a\u3008xi,e1\u3009\u2212b)/\u03c3v\n1 + e(a\u3008xi,e1\u3009\u2212b)/\u03c3v\nwhere a, b are the resampling parameters, e1 is the first principal component, and \u03c3v is the standard deviation of the projection to e1. Note that in this resampling scheme, the probability of taking one point is only conditioned on the feature information xi. This resampling method will be denoted by PCA(a, b).\n8We note that this procedure cannot be used with KMM as it has no out-of-sample extension. Therefore in subsection 6.3 we do not compare our method with KMM since there is no obvious way to extend the results to the validation data set.\n\u2022 Resampling using label information. The probability of selecting the i\u2019th instance, denoted by Pi, is defined by\nPi = { 1 y1 \u2208 Lq 0 Otherwise.\nwhere yi \u2208 L = {1, 2, . . . , k} and Lq is a subset of the complete label set L. We apply this for binary problems obtained by aggregating different classes in the multi-class setting."}, {"heading": "6.3 Testing the FIRE algorithm", "text": "In first experiment, we test our method for selecting the parameters, which is described in Section 6.1, by focusing on the the error J(f ;Xp, Xq, U) in Eq. 36 for different function classes U . We use different families of functions for tuning parameters and validation. This measure is important because in practice the functions we are interested may not be in the collection we chosen for validation. To avoid confusion, we denote the function for cross validation by f cv and the function for measuring error by f err.\nWe use the CPUsmall and USPS hand-written digits data sets. For each of them, we generate two data sets Xp and Xq using the resampling method, PCA(a, \u03c3v), describe in Section 6.2. We compare FIRE with several methods including TIKDE, LSIF. Figure 1 gives an illustration of the procedure and usage of data for the experiments. And the results are shown in Table 1 and 2. The numbers in the table are the average errors defined in Eq. 36 on held-out set Xerr over 5 trials, using different criterion functions f cv(Columns) and error-measuring functions f err(Row). N is the number of random function we are using for the cross-validation.\nFor the error-measuring functions, we have several choices as follows:\n(1) Linear: Sets of Random linear functions f(x) = \u03b2Tx where \u03b2 \u223c N(0, Id).\n(2) Half-space: Sets of random half-space indicator functions, f(x) = 1\u03b2T x>0 where \u03b2 \u223c N(0, Id).\n(3) Kernel: Sets of random linear combination of kernel functions centered at the training data, f(x) = \u03b3TK where \u03b3 \u223c N(0, Id) and Kij = k(xi, xj) where xi are points from the data set.\n(4) K-indicator: Sets of random kernel indicator functions centered at the training data, f = 1\u03b3TK>0 where \u03b3 \u223c N(0, Id) and Kij = k(xi, xj) where xi are points from the data set.\n(5) Coord: Sets of coordinate functions."}, {"heading": "6.4 Supervised Learning: Regression and Classification", "text": "In our experiments, we compare our method FIRE with several methods under the setting of supervised learning, i.e. regression and classification. More specifically, we consider the situation part or all of the training set Xp are labeled and all of Xq are unlabeled. In the following experiments, we will estimate the density ratio function using 1000 points in Xp and use the labeled data from Xp to build a regression function or classifier on Xq."}, {"heading": "6.4.1 Regression", "text": "Given data sets (Xp, Y p) where Xp is for independent variable, and Y p is for dependent variable, and a test data set Xq with a different distribution, the regression problem is to obtain a function a predictor on Xq. To make the comparison between unweighted regression method and different weighting schemes, we use the simplest regression method, the least square linear regression. With this method, the regression function is of the form\nf(x, \u03b2) = \u03b2tx,\nwhere \u03b2 = (XWXT )+XWY and (\u00b7)+ denotes the pseudo-inverse of a matrix. Here W is a diagonal matrix with the estimated density ratio on the diagonal. These are estimated using FIRE and other density ratio estimation methods for comparison. The results on 3 regression data sets are shown in Table 5, 3 and 4."}, {"heading": "6.4.2 Classification", "text": "Similarly to the case of regression the density ratio can also be used for building a classifier such as SVM. Given a set of labeled data, {(x1, y1),(x2, y2), . . . , (xn, yn)} and xi \u223c q, we building a linear classifier f by\nthe weighted linear SVM algorithm as follows:\nf = arg min \u03b2\u2208Rd\nC\nn n\u2211 i=1 wi(\u03b2 Txi \u2212 yi)+ + \u2016\u03b2\u201622\nThe weights wi\u2019s are obtained by various density ratios estimation algorithms using two data sets X p and Xq. Note that estimating the density ratios using Xp and Xq is completely independent of the label information. We also explore the performance of these weighted SVM as the number of labeled points used for training classifier changes. In the experiments, we first estimate the density ratios on the whole Xp with the parameters selected by cross validation. Then we subsample a portion of Xp and use their labels to train the classifier. And the performance of the classifier in terms of prediction error is estimated using all the points in Xq. The results on USPS hand-written digits and 20 news groups are shown in Table 6, 7, 8 and 9."}, {"heading": "6.5 Simulated Examples", "text": ""}, {"heading": "6.5.1 Simulated Dataset 1.", "text": "We use a simple example, where the two densities are known, to demonstrate the properties of our methods and how the number of data points influences the performance.\nFor this experiment, we suppose p = 0.5N(\u22122, 12)+0.5N(2, 0.52) and q = N(0, 0.52) and fix |Xq| = 2000, and vary |Xp| from 50 to 1000. We compare our method with the other two methods for the same problem: TIKDE and KMM. For all the methods we consider in this experiment, we will choose the optimal parameter based on the empirical L2 norm of the difference between the estimated ratio and the true ratio, which is supposed to be known in this toy example. Figure 2 gives the reader an intuition about how the estimated ratios behave for different methods.\nAnd Figure 3 shows how different methods perform when |Xp| varies from 50 to 1000 and |Xq| is fixed\nto be 2000. The boxplot is also a good way to illustrate the stability of the methods over 50 independent repetitions."}, {"heading": "6.5.2 Simulated Dataset 2.", "text": "In the second simulated example, we will test our method for various kernels and different norms as the cost function. More specifically, we suppose p = N(0, 0.52) and q = Unif([\u22121, 1]). We will use this example to explore the power of our methods with different kernels. Three settings are considered in this experiments: (1)Different kernels kh for the RKHS. We use polynomial kernels of degree 1, 5 and 20, exponential kernel and Gaussian kernel; (2) Type-I setting and Type-II setting; (3) Different norm for the cost function in the algorithm, i.e. \u2016 \u00b7 \u20162,p and \u2016 \u00b7 \u20162,q. In this example, \u2016 \u00b7 \u20162,p focuses on the region close 0, but still has penalty outside interval [\u22121, 1]; \u2016 \u00b7 \u20162,p has uniform penalty on [\u22121, 1] and has no penalty at all outside the interval.\nIn all settings, we fix the convolution kernel to be Gaussian, kt. When the RKHS kernel is exponential and Gaussian, we also need to decide their width. For simplicity, we just fix their width to be 20t, where t is the width of the convolution kernel kt. For setting Type-I, we will set |Xp| = 500 and |Xq| = 500; for Type-II setting, we only specify |Xp| = 500. The results are shown in Figure 4."}, {"heading": "Acknowledgements", "text": "We are grateful to Yusu Wang for many valuable discussions and suggestions. We also thank Lorenzo Rosasco for very useful discussions and Christoph Lampert for pointing us to important related papers.\nWe thank the Austrian Institute of Technology (ISTA) and Herbert Edelsbrunner\u2019s research group for their hospitality during writing of the paper.\nThe work was partially supported by the NSF Grants IIS 0643916 and IIS 1117707."}, {"heading": "A Proof for Lemma 5", "text": "Proof. RKHS is unique for a given domain and kernel, so is independent of the measure used to define the L2,\u03c1. Thus for any g \u2208 L2,p, there should be h \u2208 L2 such that L1/2t,p g = L 1/2 t h and\n\u2016g\u20162,p = \u2016 L1/2t,p g\u2016Ht = \u2016 L 1/2 t h\u2016Ht = \u2016h\u20162.\nSince this is true for arbitrary g \u2208 L2,p, we have\nmin g\u2208L2,p \u2225\u2225\u2225f \u2212 L1/2t,p g\u2225\u2225\u22252 2,p + \u03bb\u2032\u2016g\u201622,p = min h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2,p + \u03bb\u2032\u2016h\u201622\nBecause \u2016 \u00b7 \u20162,p \u2264 \u0393\u2016 \u00b7 \u20162,\nmin h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2,p + \u03bb\u2032\u2016h\u201622 \u2264 \u03932 ( min h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2 + \u03bb\u2032 \u03932 \u2016h\u201622 ) (37)\nTo bound\nmin h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2 + \u03bb\u2032 \u03932 \u2016h\u201622\nWe need the Fourier transform F : L2(Rd)\u2192 L2(Rd), defined as\nf\u0302(\u03be) = \u222b Rd e\u2212i\u03bexf(x)dx.\nLt on Rd is the heat operator, thus L1/2t = L t2 . And\nLtf(x) = \u222b Rd kt(x, y)f(y)dy = (kt \u2217 f)(x),\nSo, F ( Ltf) = k\u0302tf\u0302 . Note that F is an isometry. Thus it is the same to transform the (37) using Fourier transform. Then we have\nmin h\u0302\u2208L2\n\u2225\u2225\u2225f\u0302 \u2212 k\u0302 t 2 h\u0302 \u2225\u2225\u22252 2 + \u03bb\u2032 \u03932 \u2016h\u0302\u201622\nwhere k\u0302t(\u03be) = e \u2212\u2016\u03be\u20162t 2 . And let\nf\u0303(\u03be) =\n{ f\u0302(\u03be) if \u2016\u03be\u20162 < 4\u03b1 log( 1 \u03bb\u2032 )\nt\n0 Otherwise.\nand h\u0303 = ( k\u0302 t\n2\n)\u22121 f\u0303 . It is obvious that \u2016h\u0303\u201622 \u2264 1\u03bb\u2032\u03b1 \u2016f\u0302\u2016 2 2 = 1 \u03bb\u2032\u03b1 \u2016f\u2016\n2 2. And\u2225\u2225\u2225f\u0302 \u2212 k\u0302 t 2 h\u0303 \u2225\u2225\u22252 2 = \u2225\u2225\u2225f\u0302 \u2212 f\u0303\u2225\u2225\u22252 2\nNow we recall the definition of Sobolev space using Fourier transform, which states that f\u0302(\u03be) = 1 (1+\u2016\u03be\u20162)s/2 u\u0302(\u03be) for some u\u0302 \u2208 L2. Thus, \u2225\u2225\u2225f\u0302 \u2212 f\u0303\u2225\u2225\u22252 2 = (\u222b \u2016\u03be\u20162< 4\u03b1 log( 1 \u03bb\u2032 )\nt\n1\n(1 + \u2016\u03be\u20162)s/2 u\u0302(\u03be)d\u03be )2 \u2264 \u222b \u2016\u03be\u20162< 4\u03b1 log( 1 \u03bb\u2032 )\nt\n( 1\n(1 + \u2016\u03be\u20162)s/2 u\u0302(\u03be)\n)2 d\u03be \u2264 t s\n4\u03b1s ( log( 1\u03bb\u2032 ) )s \u2016u\u0302\u201622 Thus, we have\nmin h\u0302\u2208L2\n\u2225\u2225\u2225f\u0302 \u2212 k\u0302 t 2 h\u0302 \u2225\u2225\u22252 2 + \u03bb\u2032 \u03932 \u2016h\u0302\u201622 \u2264 \u2225\u2225\u2225f\u0302 \u2212 k\u0302 t 2 h\u0303 \u2225\u2225\u22252 2 + \u03bb\u2032 \u03932 \u2016h\u0303\u201622 \u2264\nts 4\u03b1s ( log( 1\u03bb\u2032 ) )s \u2016u\u0302\u201622 + \u03bb\u20321\u2212\u03b1\u03932 \u2016f\u201622 Let D1 = \u03932\u2016u\u0302\u201622 4\u03b1s and D2 = 1, we have the lemma."}, {"heading": "B Proof for Lemma 6", "text": "For the compact manifold case, we also need to have similar lemma as the above one. However, the definition of Fourier transform is obscure, thus we need to consider alternative way to get the same bound. We can use the Laplace-Beltrami operator on the compact manifold. It has discrete spectrum and satisfies the Weyl\u2019s Law, see Chapter 8 in [30], about the spectrum of the Laplace-Beltrami operator \u2206, which is discrete if the manifold is compact. It states the following: the number of eigenvalues of Laplacian operator over a bounded domain with Neumann Bounday condition that are less or equal than x, denoted by N(x), satisfies\nlim x\u2192\u221e\nN(x)\nxd/2 = C\nfor a constant C depending on the dimensionality and volume of the domain. This implies there exists M such that for any i > M ,\nc1i 2/d \u2264 \u03b7i \u2264 c2i2/d.\nAlso, we can redefine the Sobolev space on a compact manifold using Laplace-Beltrami operator. W s2 = {f \u2208 L2 : \u2225\u2225\u2225\u2206s/2f\u2225\u2225\u2225\n2 <\u221e}.\nAnd this definition of W s2 is equivalent to common definition of Sobolev space using differentiation, see [27] for the details for this equivalence.\nFirst we need the following lemma. Lemma 9. Suppose f \u2208W s2 and Nt = ( 1 t )\u03b1 , then we have\u2211\ni>Nt\n\u3008f, vi\u300922 \u2264 Ct2\u03b1s/d\nwhere vi is the eigenfunctions of Laplacian operator \u2206 and C is a constant independent of t.\nProof. First let proof that \u221e\u2211 i=0 \u3008f, vi\u300922 i 2s/d <\u221e.\nUsing the implication of Weyl\u2019s Law, we have for i > M , i2s/d \u2264 \u03b7 s i\nc1 . Thus,\n\u221e\u2211 i=0 \u3008f, vi\u300922 i 2s/d = \u221e\u2211 i\u2264M \u3008f, vi\u300922 i 2s/d + \u221e\u2211 i>M \u3008f, vi\u300922 i 2s/d\n\u2264 \u221e\u2211 i\u2264M \u3008f, vi\u300922 i 2s/d + \u221e\u2211 i>M \u3008f, vi\u300922 \u03b7si c1\n\u2264 \u221e\u2211 i\u2264M \u3008f, vi\u300922 i 2s/d + 1 c1 \u2225\u2225\u2225\u2206s/2f\u2225\u2225\u22252 2 <\u221e\nLet \u2211\u221e i=0 \u3008f, vi\u3009 2 2 i 2s/d = C. For Nt = ( 1 t )\u03b1 . We have\nN 2s/d t \u2211 i>Nt \u3008f, vi\u300922 < \u2211 i>Nt \u3008f, vi\u300922 i 2s/d \u2264 C.\nThus, \u2211 i>Nt \u3008f, vi\u300922 < C N 2s/d t = Ct2\u03b1s/d.\nNow we can give the proof for Lemma 6.\nProof. RKHS is unique for a given domain and kernel, so is independent of the measure used to define the L2,\u03c1. Thus for any g \u2208 L2,p, there should be h \u2208 L2 such that L1/2t,p g = L 1/2 t h and\n\u2016g\u20162,p = \u2016 L1/2t,p g\u2016Ht = \u2016 L 1/2 t h\u2016Ht = \u2016h\u20162.\nSince this is true for arbitrary g \u2208 L2,p, we have\nmin g\u2208L2,p \u2225\u2225\u2225f \u2212 L1/2t,p g\u2225\u2225\u22252 2,p + \u03bb\u2032\u2016g\u201622,p = min h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2,p + \u03bb\u2032\u2016h\u201622\nBecause \u2016 \u00b7 \u20162,p \u2264 \u0393\u2016 \u00b7 \u20162,\nmin h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2,p + \u03bb\u2032\u2016h\u201622 \u2264 \u03932 ( min h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2 + \u03bb\u2032 \u03932 \u2016h\u201622 ) Now, let\nh\u2217\u03bb\u2032 = arg min h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2 + \u03bb\u2032\u2016h\u201622\nExpend f using the eigenfunctions v0, v1, . . . of \u2206, we have\nf = \u221e\u2211 i=0 \u3008f, vi\u30092 vi\nDenote the eigenvalues of \u2206 as \u03b70, \u03b71, . . . , the heat operator defined as Ht = e \u2212\u2206t having eigenvalues as e\u2212\u03b70t, e\u2212\u03b71t, . . . . Recall the Weyl\u2019s law, we have there exists M such that for any i > M , c1i 2/d \u2264 \u03b7i \u2264 c2i2/d. When t is small enough, we will have Nt = 1 td/2 > M . Since we order \u03b7i in non-decreasing order, for any i < Nt, we have \u03b7i \u2264 \u03b7Nt \u2264 c2N 2/d t = c2/t, also e\n\u2212\u03b7it > e\u2212c2 . Now denote PN be the operator that projects function f \u2208 L2 to the subspace spanned by first N eigenfunctions of \u2206. Thus\nPNtf = \u2211 i\u2264Nt \u3008f, vi\u30092 vi\nwhere vi is the eigenfunction of \u2206. And let\nh\u0302 = H \u22121/2 t PNtf = Nt\u2211 i=0 e \u03b7it 2 \u3008f, vi\u30092 vi\nThus, we have\narg min h\u2208L2 \u2225\u2225\u2225f \u2212 L1/2t h\u2225\u2225\u22252 2 + \u03bb\u2032\u2016h\u201622 \u2264 \u2225\u2225\u2225f \u2212 L1/2t h\u0302\u2225\u2225\u22252 2 + \u03bb\u2032\u2016h\u0302\u201622\n= \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i>Nt \u3008f, vi\u30092 vi + \u2211 i\u2264Nt \u3008f, vi\u30092 vi \u2212 L 1/2 t h\u0302 \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\u2032\u2016h\u0302\u201622\n\u2264 \u2225\u2225\u2225\u2225\u2225\u2211 i>Nt \u3008f, vi\u30092 vi \u2225\u2225\u2225\u2225\u2225 2 + \u2225\u2225\u2225H1/2H\u22121/2PNtf \u2212 L1/2t H\u22121/2PNtf\u2225\u2225\u2225 2 2 + \u03bb\u2032 Nt\u2211 i=1 e\u03b7it \u2329 f, vti \u232a2 2\n(38)\nNow let us proceed by bound the above formula. By Lemma 9 with Nt = 1 td/2 and s = 2, we have\n\u221e\u2211 i=Nt+1 \u3008f, vi\u300922 \u2264 Ct 2\nAlso, for i < Nt, e \u03b7it \u2264 ec2 , thus \u2016H\u22121/2PNtf\u20162 \u2264 ec2/2 \u2016PNtf\u20162 \u2264 ec2/2 \u2016f\u20162. Recall we have \u2016H 1/2 t \u2212\nL 1/2 t \u2016L2\u2192L2 \u2264 C \u2032tfor a constant C \u2032. Thus, we have\u2225\u2225\u2225H1/2H\u22121/2PNtf \u2212 L1/2t H\u22121/2PNtf\u2225\u2225\u2225\n2 \u2264 C \u2032ec2/2 \u2016f\u20162 t\nFor the third term in (38), we have\n\u03bb\u2032 Nt\u2211 i=1 e\u03b7it \u3008f, vi\u300922 \u2264 \u03bb \u2032ec2 Nt\u2211 i=1 \u3008f, vi\u300922 \u2264 \u03bb \u2032ec2 \u2016f\u201622\nHence, \u2225\u2225\u2225f \u2212 L1/2t,p g\u2217\u03bb\u2032\u2225\u2225\u22252 2,p + \u03bb\u2032\u2016g\u2217\u03bb\u2032\u201622,p \u2264 \u03932 ( Cts + C \u2032ec2/2 \u2016f\u20162 t )2 + \u03bb\u2032ec2 \u2016f\u201622\nWhen t is small enough, t2 \u2264 t, letting D1 = \u03932(C + C \u2032ec2/2), D2 = ec2 , we prove the lemma."}, {"heading": "C Proof for Theorems in 4.2.2", "text": "In the second case, since we do not have samples from q, we replace Kt,q,zq by q. Consider corresponding f II\u03bb ,\nf II\u03bb =(K3t,p + \u03bbI)\u22121K2t,pq = (K3t,p + \u03bbI)\u22121K2t,p ( q \u2212Kt,p q\np +Kt,p\nq\np ) =(K3t,p + \u03bbI)\u22121K2t,p ( q \u2212Kt,p q\np\n) + (K3t,p + \u03bbI)\u22121K3t,p q\np Thus, we need to bound the extra term (K3t,p+\u03bbI)\u22121K2t,p ( q \u2212Kt,p qp ) . Let d = q\u2212Kt,p qp and \u2016d\u20162,p = \u03b4t,\nwe have \u2225\u2225( L3t,p + \u03bbI)\u22121 L2t,pd\u2225\u22252,p = ( \u221e\u2211 i=1 ( \u03c32i \u3008ui, d\u30092,p \u03c33i + \u03bb )2) 12 \u2264 max \u03c3>0 ( 1 \u03c3 + \u03bb\u03c32 ) \u03b4t \u2264 \u03b4t( 2 1 3 + 2\u2212 2 3 ) \u03bb 1 3 \u2264 \u03bb\u2212 13 \u03b4t\nThe bound for \u03b4t is given in the following lemma.\nLemma 10. Suppose p, q are two density function of probability measures of the domain \u2126 and satisfying the assumptions we gave in section 4.1. We have the following: (1) When \u2126 is Rd and q \u2208W 22 (R2), we have\u2225\u2225\u2225\u2225Kt,p qp \u2212 q \u2225\u2225\u2225\u2225 2,p = O(t)\n(2) When Omega is a manifold M without boundary of d dimension and q \u2208W 22 (M), we have\u2225\u2225\u2225\u2225Kt,p qp \u2212 q \u2225\u2225\u2225\u2225\n2,p\n= O(t1\u2212\u03b5)\nfor any 0 < \u03b5 < 1.\nProof. By definition of Kt,p, we have\nKt,p q\np \u2212 q = \u222b Rd kt(x, y) q(y) p(y) p(y)dy \u2212 q(x) = \u222b Rd kt(x, y)(q(y)\u2212 q(x))dy = (Kt \u2212 I)q\nBy results in [?], we have (Kt\u2212I)q = t\u2206q+ o(t) when q is twice differentiable. Due to q \u2208W 22 (Rd), we have \u2016\u2206q\u20162 <\u221e. Thus, we have\n\u2016Kt,p q\np \u2212 q\u20162,p \u2264 \u0393\u2016Kt,p\nq p \u2212 q\u20162 = \u0393\u2016t\u2206q + o(t)\u20162 \u2264 \u0393t\u2016\u2206q\u20162 + o(t) = O(t).\nFor manifold case, we have (Kt \u2212D)q = t\u2206q + o(t), where Df = \u222b M kt(x, y)dyf(x). Thus,\n(Kt \u2212 I)q = (Kt \u2212D)q + (D \u2212 I)q.\nFor the first term, we have the same rate with Rd. Now we proceed by bounding the second term. \u2016(D \u2212 I)q\u20162 = \u2225\u2225\u2225\u2225(\u222b\nM kt(\u00b7, y)dy \u2212 1\n) q(\u00b7) \u2225\u2225\u2225\u2225\n2\n\u2264 \u2225\u2225\u2225\u2225\u222b M kt(\u00b7, y)dy \u2212 1 \u2225\u2225\u2225\u2225 2 \u2016q\u20162\nWe know that \u2016q\u20162 <\u221e. Let Bt(x) = {y \u2208 M : \u2016x \u2212 y\u20162 < t 1 2\u2212\u03b5} and Rt(x) is the projection of Bt(x) on the TxM. In the following proof, we need to use change of variables to converting integral over a manifold to the integral over the tangent space at a specific point. For two points x, y \u2208 M, let y\u2032 = \u03c0x(y) be the projection of y in the tangent space Tx of M at x. Let J\u03c0x |y denote the Jacobian of the map \u03c0x at point y \u2208 M and J\u03c0\u22121x \u2223\u2223\u2223 y\u2032 is the inverse. For y sufficiently close to x, we have\n\u2016x\u2212 y\u2016 = \u2016x\u2212 y\u2032\u2016+O(\u2016x\u2212 y\u2032\u20163)\u2223\u2223\u2223J\u03c0x |y \u2212 1\u2223\u2223\u2223 = O(\u2016x\u2212 y\u20162)\u2223\u2223\u2223\u2223J\u03c0\u22121x \u2223\u2223\u2223y\u2032 \u2212 1 \u2223\u2223\u2223\u2223 = O(\u2016x\u2212 y\u2032\u20162).\nThus, it is true that the points in Rt(x) are still no further than 2t 1 2\u2212\u03b5, when t is small enough. Since kt has exponential decay, the integral \u222b Bt(x) kt(y, \u00b7)dy is of order O(e\u2212t \u2212\u03b5 ), and so is \u222b Rt(x) kt(y \u2032, \u00b7)dy\u2032. Thus, for any point x \u2208M, \u2223\u2223\u2223\u2223\u222b M kt(x, \u00b7)dx\u2212 1\n\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 \u222b Bt(x) kt(y, \u00b7)dy \u2212 1 +O(e\u2212t \u2212\u03b5 ) \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 \u222b Bt(x) kt(y, \u00b7)dy \u2212 \u222b TxM kt(y \u2032, \u00b7)dy\u2032 +O(e\u2212t \u2212\u03b5 ) \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 \u222b R(x) kt(y \u2032, \u00b7)J\u03c0\u22121 |y\u2032dy\u2032 \u2212 \u222b R(x) kt(x, \u00b7)dx+O(e\u2212t \u2212\u03b5 ) \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 \u222b R(x) kt(x, \u00b7)(J\u03c0\u22121 |x \u2212 1)dx+O(e\u2212t \u2212\u03b5 ) \u2223\u2223\u2223\u2223\u2223 =O(t1\u22122\u03b5)\n\u222b R(x) kt(x, \u00b7)dx+O(e\u2212t \u2212\u03b5 )\n=O(t1\u22122\u03b5) (\u222b TxM kt(x, \u00b7)dx+O(e\u2212t \u2212\u03b5 ) ) +O(e\u2212t \u2212\u03b5 )\n=O(t1\u22122\u03b5) ( 1 +O(e\u2212t \u2212\u03b5 ) ) +O(e\u2212t \u2212\u03b5 )\n=O(t1\u22122\u03b5)\nAbusing the notation of \u03b5, we have \u2016 \u222b M kt(\u00b7, y)dy \u2212 1\u20162 \u2264 O(t 1\u2212\u03b5) where 0 < \u03b5 < 1.\nFor the concentration of \u2016f II\u03bb \u2212 f II\u03bb,z\u20162,p, we will consider their close formulas\nf II\u03bb = ( K3p + \u03bbI )\u22121K2pq f II\u03bb,z = ( K3zp + \u03bbI )\u22121 K2zpq\n(39)\nBy the similar argument to that in Lemma 8, we will have the following lemma gives the concentration bound.\nLemma 11. Let p be a density of a probability measure over a domain X and q another density function. They satisfy the assumptions in 4.1. Consider f II\u03bb and f II \u03bb,z defined in Eq. 39, with confidence at least 1\u22122e\u2212\u03c4 ,\nwe have \u2225\u2225f II\u03bb \u2212 f II\u03bb,z\u2225\u22252,p \u2264 C4( \u03bat\u221a\u03c4\u03bb3/2\u221an + \u03bat \u221a \u03c4 \u03bb \u221a n ) where \u03bat = supx\u2208\u2126 kt(x, x) = 1 (2\u03c0t)d/2\nProof. Let f\u0303 = ( K3zp + \u03bbI )\u22121 K2pq. We have f II\u03bb \u2212 f II\u03bb,z = f II\u03bb \u2212 f\u0303 + f\u0303 \u2212 f II\u03bb,z. For f II\u03bb \u2212 f\u0303 , using the fact that(\nK3p + \u03bbI ) f II\u03bb = K2pq, we have\nf II\u03bb \u2212 f\u0303 =f II\u03bb \u2212 ( K3zp + \u03bbI )\u22121 ( K3p + \u03bbI ) f II\u03bb\n= ( K3zp + \u03bbI )\u22121 ( K3zp \u2212K 3 p ) f II\u03bb\nAnd\nf\u0303 \u2212 f II\u03bb,z = ( K3zp + \u03bbI )\u22121 K2pq \u2212 ( K3zp + \u03bbI )\u22121 K2zpq\n= ( K3zp + \u03bbI )\u22121 ( K2p \u2212K2zp ) q\nNotice that we have K3zp \u2212K 3 p and K2zp \u2212K 2 p in the identity we get. For these two objects, it is not hard\nto verify the following identities,\nK3zp \u2212K 3 p = ( Kzp \u2212Kp )3 +Kp ( Kzp \u2212Kp )2 + ( Kzp \u2212Kp ) Kp ( Kzp \u2212Kp ) + ( Kzp \u2212Kp )2Kp +K2p ( Kzp \u2212Kp ) +Kp ( Kzp \u2212Kp ) Kp + ( Kzp \u2212Kp ) K2p.\nAnd\nK2zp \u2212K 2 p = ( Kzp \u2212Kp )2 +Kp ( Kzp \u2212Kp ) + ( Kzp \u2212Kp ) Kp\nThus, in these two identities, the only two random variables are Kzp\u2212Kp. By results about concentration of Kzp and Kzq , we have with probability 1\u2212 2e\u2212\u03c4 ,\n\u2016Kzp \u2212Kp\u2016H\u2192H \u2264 \u03bat \u221a \u03c4\u221a n ,\n\u2225\u2225Kzpq \u2212Kpq\u2225\u2225H \u2264 \u03bat\u2016q\u2016\u221e \u221a 2\u03c4\u221a n\n(40)\nAnd we know that for a large enough constant c which is independent of t and \u03bb, \u2016Kp\u2016H\u2192H < c, \u2225\u2225\u2225\u2225(K3zp + \u03bbI)\u22121\u2225\u2225\u2225\u2225\nH\u2192H \u2264 1 \u03bb , \u2016Kpq\u2016H < c\u2016q\u20162,p\nand\n\u2016f II\u03bb \u20162H = \u2211 i \u03c33i (\u03c33i + \u03bb) 2 \u3008q, ui\u30092 \u2264 ( sup \u03c3>0\n\u03c33\n(\u03c33 + \u03bb)2 )\u2211 i \u3008q, ui\u30092 \u2264 c2 \u03bb \u2016q\u201622,p\nthus, \u2016f II\u03bb \u2016H \u2264 c\u03bb1/2 \u2016q\u20162,p. Notice that \u2225\u2225\u2225(Kzp \u2212Kp)2\u2225\u2225\u2225H \u2264 \u2225\u2225Kzp \u2212Kp\u2225\u22252H and \u2225\u2225\u2225(Kzp \u2212Kp)3\u2225\u2225\u2225H \u2264 \u2225\u2225Kzp \u2212Kp\u2225\u22253H, both of this could be of smaller order compared with\n\u2225\u2225Kzp \u2212Kp\u2225\u2225H. For simplicity we hide the term including them in the final bound without changing the dominant order. We could also hide the terms with the product of any two the random variables in Eq. 40, which is of prior order compared to the term with only one random variable. Now let us put everything together,\n\u2016f II\u03bb \u2212 f II\u03bb,z\u20162,p \u2264 c1/2\u2016f II\u03bb \u2212 f II\u03bb,z\u2016Ht \u2264c1/2 ( c3\u03bat \u221a \u03c4\n\u03bb3/2 \u221a n \u2016q\u20162,p +\nc2\u03bat \u221a \u03c4\n\u03bb \u221a n \u2016q\u2016\u221e ) \u2264C4 ( \u03bat \u221a \u03c4\n\u03bb3/2 \u221a n\n+ \u03bat \u221a \u03c4\n\u03bb \u221a n ) where C4 = c 5/2 max ( c \u2016q\u20162,p , \u2016q\u2016\u221e ) .\nGiven the above lemmas, the main theorem for the second case follows."}], "references": [{"title": "On regularization algorithms in learning theory", "author": ["Frank Bauer", "Sergei Pereverzev", "Lorenzo Rosasco"], "venue": "Journal of complexity,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Discriminative learning for differing training and test distributions", "author": ["Steffen Bickel", "Michael Br\u00fcckner", "Tobias Scheffer"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Optimal rates of convergence for deconvolving a density", "author": ["Raymond J Carroll", "Peter Hall"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Learning from examples as an inverse problem", "author": ["Ernesto De Vito", "Lorenzo Rosasco", "Andrea Caponnetto", "Umberto De Giovannini", "Francesca Odone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Spectral regularization for support estimation", "author": ["Ernesto De Vito", "Lorenzo Rosasco", "Alessandro Toigo"], "venue": "Advances in Neural Information Processing Systems, NIPS Foundation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Maximum smoothed likelihood density estimation for inverse problems", "author": ["P. Eggermont", "V. LaRicca"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Covariate shift by kernel mean matching", "author": ["Arthur Gretton", "Alex Smola", "Jiayuan Huang", "Marcel Schmittfull", "Karsten Borgwardt", "Bernhard Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Conditional mean embeddings as regressors", "author": ["S Gr\u00fcnew\u00e4lder", "G Lever", "L Baldassarre", "S Patterson", "A Gretton", "M Pontil"], "venue": "In Proceedings of the 29th International Conference on Machine Learning, ICML 2012,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Jiayuan Huang", "Alexander J. Smola", "Arthur Gretton", "Karsten M. Borgwardt", "Bernhard Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Review papers: Recent developments in nonparametric density estimation", "author": ["Alan Julian Izenman"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "A least-squares approach to direct importance estimation", "author": ["Takafumi Kanamori", "Shohei Hido", "Masashi Sugiyama"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Robust kernel density estimation", "author": ["Joo Seuk Kim", "Clayton Scott"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Linear integral equations, volume 82", "author": ["Rainer Kress"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Metropolized independent sampling with comparisons to rejection sampling and importance sampling", "author": ["Jun Liu"], "venue": "Statistics and Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Annealed importance sampling", "author": ["Radford M Neal"], "venue": "Statistics and Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization", "author": ["XuanLong Nguyen", "Martin J Wainwright", "Michael I Jordan"], "venue": "Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1962}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Kernel methods for pattern analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": "Cambridge university press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Data spectroscopy: Eigenspaces of convolution operators and clustering", "author": ["Tao Shi", "Mikhail Belkin", "Bin Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Improving predictive inference under covariate shift by weighting the loglikelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "On a kernel-based method for pattern recognition, regression, approximation, and operator inversion", "author": ["Alex J Smola", "Bernhard Sch\u00f6lkopf"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Support vector machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Analysis of the laplacian on the complete riemannian manifold", "author": ["R.S. Strichartz"], "venue": "Journal of Functional Analysis,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1983}, {"title": "Covariate shift adaptation by importance weighted cross validation", "author": ["Masashi Sugiyama", "Matthias Krauledat", "Klaus-Robert M\u00fcller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["Masashi Sugiyama", "Shinichi Nakajima", "Hisashi Kashima", "Paul Von Buenau", "Motoaki Kawanabe"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Partial Differential Equation", "author": ["M.E. Taylor"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Support vector method for multivariate density estimation", "author": ["Vladimir Vapnik", "Sayan Mukherjee"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Practical approximate solutions to linear operator equations when the data are noisy", "author": ["Grace Wahba"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1977}, {"title": "The effect of the input density distribution on kernel-based classifiers", "author": ["Christopher Williams", "Matthias Seeger"], "venue": "In Proceedings of the 17th International Conference on Machine Learning. Citeseer,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "On early stopping in gradient descent learning", "author": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto"], "venue": "Constructive Approximation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Analysis of kernel mean matching under covariate shift", "author": ["Yaoliang Yu", "Csaba Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": ", see the review [11]), particularly, dealing with a class of non-parametric kernel estimators going back to the work of Parzen [20].", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": ", see the review [11]), particularly, dealing with a class of non-parametric kernel estimators going back to the work of Parzen [20].", "startOffset": 128, "endOffset": 132}, {"referenceID": 14, "context": ", [16]) deals with this problem by using a ratio of two densities (which is typically assumed to be known in that literature).", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 11, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 9, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 26, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 2, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 22, "context": "Many of these papers consider this problem in the context of covariate shift assumption [24] or the so-called selection bias [36].", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "We will provide a more detailed discussion of these and other related papers and connections to our work in Section 2, where we also discuss how the Kernel Mean Matching algorithm [8, 10] can be viewed within our framework.", "startOffset": 180, "endOffset": 187}, {"referenceID": 9, "context": "We will provide a more detailed discussion of these and other related papers and connections to our work in Section 2, where we also discuss how the Kernel Mean Matching algorithm [8, 10] can be viewed within our framework.", "startOffset": 180, "endOffset": 187}, {"referenceID": 9, "context": "This formulation (see Section 3) also yields an explicit formula and is related to the Kernel Mean Matching algorithm [10] (see the discussion in Section 2), although with a different optimization procedure.", "startOffset": 118, "endOffset": 122}, {"referenceID": 32, "context": ", [34, 1]) can be used and may have computational advantages.", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": ", [34, 1]) can be used and may have computational advantages.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": "Finally, in Section 6 we discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [10] and LSIF [13] as well as the base-line thresholded inverse kernel density estimator (TIKDE) and importance sampling (when available).", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Finally, in Section 6 we discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [10] and LSIF [13] as well as the base-line thresholded inverse kernel density estimator (TIKDE) and importance sampling (when available).", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "The problem of density estimation has a long history in classical statistical literature and a rich variety of methods are available [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Some of the related older work includes density estimation for inverse problems [7] and the literature on deconvolution, e.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": ", [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": "In the last few years the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [19] and, in particular to the form of transfer learning known as covariate shift [24].", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "In the last few years the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [19] and, in particular to the form of transfer learning known as covariate shift [24].", "startOffset": 238, "endOffset": 242}, {"referenceID": 2, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 7, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 11, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 26, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 9, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 27, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 16, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 9, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 222, "endOffset": 226}, {"referenceID": 7, "context": "Also, since there is no regularizing term, the problem is less stable (see Section 6 for some experimental comparisons) and the theoretical analysis is harder (however, see [8] and the recent paper [35] for some nice theoretical analysis of KMM in certain settings).", "startOffset": 173, "endOffset": 176}, {"referenceID": 33, "context": "Also, since there is no regularizing term, the problem is less stable (see Section 6 for some experimental comparisons) and the theoretical analysis is harder (however, see [8] and the recent paper [35] for some nice theoretical analysis of KMM in certain settings).", "startOffset": 198, "endOffset": 202}, {"referenceID": 11, "context": "Another related recent algorithm is Least Squares Importance Sampling (LSIF) [13], which attempts to estimate the density ratio by choosing a parametric linear family of functions and choosing a function from this family to minimize the L2,p distance to the density ratio.", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "A similar setting with the Kullback-Leibler distance (KLIEP) was proposed in [29].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": ", [26, 22, 21]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 20, "context": ", [26, 22, 21]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 19, "context": ", [26, 22, 21]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 4, "context": ", [5, 25] in the Tikhonov regularization or other regularization frameworks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 23, "context": ", [5, 25] in the Tikhonov regularization or other regularization frameworks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 29, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 63, "endOffset": 67}, {"referenceID": 5, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 12, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 207, "endOffset": 211}, {"referenceID": 8, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 236, "endOffset": 239}, {"referenceID": 31, "context": "We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [33, 23].", "startOffset": 136, "endOffset": 144}, {"referenceID": 21, "context": "We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [33, 23].", "startOffset": 136, "endOffset": 144}, {"referenceID": 1, "context": ", [2].", "startOffset": 2, "endOffset": 5}, {"referenceID": 13, "context": "Finally, the setting in this paper is connected to the large literature on integral equations [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 30, "context": "In particular, we note [32], which analyzes the classical Fredholm problem using regularization for noisy data.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "The direct consequence of this is the Representer Theorem, which allows us to write solutions to various optimization problems over H in terms of linear combinations of kernels supported on sample points (see [26] for an in-depth discussion or the RKHS theory and the issues related to learning).", "startOffset": 209, "endOffset": 213}, {"referenceID": 9, "context": "We note the connection between this formulation of using the RKHS norm as a loss function and the KMM algorithm [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Interestingly a somewhat similar formula arises in [13] as unconstrained LSIF, with a different functional basis (kernels centered at the points of the sample zq) and in a setting not directly related to RKHS inference.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": ", many problems involving MCMC) where q(x) is known explicitly (possibly up to a multiplicative constant), while sampling from q is expensive or even impossible computationally [17].", "startOffset": 177, "endOffset": 181}, {"referenceID": 28, "context": ", [30]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "We note that this way of measuring the error is related to the LSIF [13] and KLIEP [29], algorithms.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "We note that this way of measuring the error is related to the LSIF [13] and KLIEP [29], algorithms.", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "Two ways of resampling, using the features of the data and using the label information, are used (along the lines similar to those proposed in [8]).", "startOffset": 143, "endOffset": 146}], "year": 2013, "abstractText": "In this paper we address the problem of estimating the ratio q p where p is a density function and q is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another. It is often referred as importance sampling in statistical inference and is also closely related to the problem of covariate shift in transfer learning as well as to various MCMC methods. It may also be useful for separating the underlying geometry of a space, say a manifold, from the density function defined on it. Our approach is based on reformulating the problem of estimating q p as an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel in the case of densities defined on R, compact domains in R and smooth d-dimensional sub-manifolds of the Euclidean space. We also show experimental results including applications to classification and semi-supervised learning within the covariate shift framework and demonstrate some encouraging experimental comparisons. We also show how the parameters of our algorithms can be chosen in a completely unsupervised manner.", "creator": "LaTeX with hyperref package"}}}