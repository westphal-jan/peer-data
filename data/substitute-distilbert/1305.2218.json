{"id": "1305.2218", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2013", "title": "Stochastic gradient descent algorithms for strongly convex functions at O(1/T) convergence rates", "abstract": "with a weighting scheme proportional to t, a traditional stochastic gradient approximation ( sgd ) algorithm achieves a high probability convergence estimation of o ( { \\ kappa } / t ) for strongly convex functions, instead of o ( { \\ kappa } ln ( t ) / \u03b3 ). we also recall that an automated sgd decomposition also assumes a rate across o ( { \\ kappa } / t ).", "histories": [["v1", "Thu, 9 May 2013 21:31:47 GMT  (17kb,D)", "http://arxiv.org/abs/1305.2218v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["shenghuo zhu"], "accepted": false, "id": "1305.2218"}, "pdf": {"name": "1305.2218.pdf", "metadata": {"source": "CRF", "title": "Stochastic gradient descent algorithms for strongly convex functions at O(1/T ) convergence rates", "authors": ["Shenghuo Zhu"], "emails": ["zsh@nec-labs.com"], "sections": [{"heading": null, "text": "1 Introduction\nConsider a stochastic optimization problem\nmin x\u2208X {f(x) := E\u03beF (x, \u03be)}\nwhere X \u2282 Rd is a nonempty bounded closed convex set, \u03be is a random variable, F is a smooth convex function, f is a smooth strongly-convex function. The requirement of smoothness simplifies the analysis. If the objective function is nonsmooth but satisfies Lipschitz continuity, stochastic gradient descent algorithms can replace gradients with subgradients, but the analysis has to introduce an additional term in the same order as the variance term. Some nonsmooth cases have been studied in (Lan, 2008) and (Ghadimi & Lan, 2012).\nAssume that the domain is bounded, i.e. supx,y\u2208X \u2016x\u2212y\u20162 \u2264 D2. Let G(x, \u03be) be a stochastic gradient of function f at x with a random variable \u03be. Then g(x) := E\u03beG(x, \u03be) is a gradient of f(x). Assume that \u2016g(x)\u2212g(y)\u2016\u2217 \u2264 L\u2016x\u2212y\u2016, where L is known as the Lipschitz constant. We only consider strongly convex function in this note, thus assume that there is \u00b5 > 0, such that f(y) \u2265 f(x) + \u3008g(x), y \u2212 x\u3009 + \u00b52 \u2016y \u2212 x\u2016\n2. We assume that stochastic gradients are bounded, i.e., there exists Q > 0, such that sup\u03be \u2016G(x, \u03be)\u2212 g(x)\u2016\u2217 \u2264 Q.\nWe are interested in the conditional number \u03ba, which is defined as L/\u00b5. The conditional number, \u03ba, could be as large as \u221a N , where N is the number of samples and T = N . One reference case is regularized linear classifiers (Smale & Zhou, 2003), where the regularization factor could be as large as \u221a N . The other reference case is the conditional number of a N \u00d7 n random matrix (Rudelson & Vershynin, 2009), where the smallest singular value is O( \u221a N \u2212 \u221a n\u2212 1). When \u03ba = \u0398( \u221a T ), O(\u03ba/T ) = O(1/ \u221a T ), which bridges the gap between the convergence rate for strongly convex functions and that for those without strongly convex condition. In this note, we assume \u03ba = O(T ). We use big-O notation in term of T and \u03ba and hide the factors D2L, Q2/L and DQ besides constants.\nNotation\nDenote {1 \u00b7 \u00b7 \u00b7T} by [T ]. Let {\u03bet : t \u2208 [T ]} be a sequence of independent random variables. Denote E|t\u22121{\u00b7} := E{\u00b7|\u03be1, \u00b7 \u00b7 \u00b7 , \u03bet\u22121}. We define l\u0303n(T, t) = \u2211T \u03c4=t+1 1 \u03c4 . Then l\u0303n(T, t) \u2264 1 t+1 + ln(T/(t + 1)), and for t \u2265 1, l\u0303n(T, t) \u2264 ln(T/t).\n2 Stochastic gradient descent algorithm\nAlgorithm 1 shows the stochastic gradient descent method. Unlike the conventional averaging by equal weights wt = 1/T , we use a weighting scheme wt = \u03b1t \u220fT \u03c4=t+1(1\u2212\u03b1\u03c4 ) = t/(2T (T+1)), where \u03b1t = 2/(t+1) . Theorem 1 shows a convergence rate of O(\u03ba/T ), assuming that T > \u03ba. Let At = \u2016xt \u2212 x\u2217\u20162, Bt = \u3008\u03b4t, xt\u22121 \u2212 x\u2217\u3009 /Q, Ct = \u2016\u03b4t\u20162\u2217/Q2,\nar X\niv :1\n30 5.\n22 18\nv1 [\ncs .L\nG ]\n9 M\nay 2\n01 3\nAlgorithm 1 Stochastic gradient descent algorithm\n1: Input: initial solution x0, step sizes {\u03b3t > 0 : t \u2208 [T ]} and averaging factor {\u03b1t > 0 : t \u2208 [T ]}. 2: for t \u2208 [T ] do 3: Let sample gradient g\u0302k = G(xt\u22121, \u03bet), where \u03bet is independent from {\u03be\u03c4 : \u03c4 \u2208 [t\u2212 1]}.\n4: Let xt = arg min x\u2208X\n{ \u3008g\u0302t, x\u3009+ 1\n2\u03b3t \u2016x\u2212 xt\u22121\u20162\n} ;\n5: Set x\u0304t = x\u0304t\u22121 + \u03b1t(xt \u2212 x\u0304t\u22121); 6: end for 7: Output: x\u0304T .\nand the coefficients bt = O(1) and ct = O(1/t). The informal argument is that the weighting scheme equalizes the variance of each iteration, since var(btBt) and ctCt are O(1/t) assuming that At = O(1/t).\nTheorem 1. Assume that the underlying function f is strongly convex, i.e., \u00b5 > 0. Let \u03ba = L/\u00b5. If \u03b1t = 2 t+1 , \u03b3t = 2\n\u00b5(t+2\u03ba) , then it holds for Algorithm 1 that for \u03b8 > 0,\nPr{f(x\u0304T )\u2212 f(x\u2217) \u2265 K\u0304(T ) + \u221a 2\u03b8K\u0303(T ) + \u03b8K\u0302(T )} \u2264 exp{\u2212\u03b8}, (1)\nwhere\nK\u0304(T ) := D2L\nT +\n2\u03baQ2\nLT = O(\u03ba/T ),\nK\u0303(T ) := 4DQ(\u03ba+ 1)\nT 3/2 +\n2 \u221a 2\u03baQ2\nLT +\n4 \u221a 2\u03ba3/2Q2 \u221a 1 + lnT\nLT 3/2 = O(\u03ba/T ),\nK\u0302(T ) := 10\u03baQ2\nLT = O(\u03ba/T ).\nSimilarly with traditional equal weighting scheme, wt = 1/T , we have a convergence rate of O(\u03ba ln(T )/T ) in Proposition 2. Informally, var( \u2211 t wtbtBt) = ln(T )/T implies a convergence rate of O(ln(T )/T ). Proposition 2. Assume that \u00b5 > 0. Let \u03ba = L/\u00b5. If \u03b1t = 1 t , \u03b3t = 1 \u00b5(t+\u03ba) , then for \u03b8 > 0,\nPr{f(x\u0304T )\u2212 f(x\u2217) \u2265 K\u0304(T ) + \u221a 2\u03b8K\u0303(T ) + \u03b8K\u0302(T )} \u2264 exp{\u2212\u03b8},\nwhere\nK\u0304(T ) := LD2 2T + \u03baQ2 2LT (1 + lnT ), K\u0303(T ) :=\nDQ \u221a \u03ba+ 1\nT + \u03baQ2 LT \u221a 1 + lnT , K\u0302(T ) := 6\u03baQ2 LT .\nProposition 3 shows that if the optimal solution x\u2217 is an interior point, it is possible to simply take the nonaveraged solution, xT . The convergence rate is O(\u03ba 2/T ). However, if \u03ba = \u0398( \u221a T ), O(\u03ba2/T ) means not convergent, just like the non-averaged SGD solution without strongly convex conditions.\nProposition 3. Assume that \u00b5 > 0 and the optimal solution x\u2217 is an interior point. Let \u03ba = L/\u00b5. If \u03b3t = 1\n\u00b5(t+\u03ba) ,\nthen for \u03b8 > 0, Pr{f(xT )\u2212 f(x\u2217) \u2265 K\u0304(T ) + \u221a 2\u03b8K\u0303(T ) + \u03b8K\u0302(T )} \u2264 exp{\u2212\u03b8},\nwhere\nK\u0304(T ) := D2L(\u03ba+ 1)2 2(T + \u03ba)2 + \u03ba2Q2(T + \u03ba(1 + lnT )) 2L(T + \u03ba)2 = O(\u03ba2/T ),\nK\u0303(T ) := DQ(\u03ba+ 1)2\u221a 2(T + \u03ba)3/2 + \u03ba2Q2 2L(T + \u03ba) + \u03ba2Q2\n\u221a \u03baT (1 + ln(T )) 2L(T + \u03ba)2 = O(\u03ba2/T ),\nK\u0302(T ) := 6\u03ba2Q2\nL(T + \u03ba) = O(\u03ba2/T ).\nRemark 1. There are studies on the high probability convergence rate of stochastic algorithm on strongly convex functions, such as (Rakhlin et al., 2012). The convergence rate usefully is O(polylog(T )/T ). Here, we prove a convergence rate of O( \u03baT ) with proper weighting scheme.\n3 Accelerated Stochastic Gradient Descent Algorithm\nAlgorithm 2 Accelerated Stochastic Gradient Descent algorithm\n1: Input: x0, \u00b5, {\u03b1t \u2265 0}, {\u03b3t > 0}; 2: Let x\u03040 = x0; 3: for k \u2208 [T ] do 4: Let yt\u22121 = \u03b1txt\u22121 + (1\u2212 \u03b1t)x\u0304t\u22121; 5: Let g\u0302t = G(yt\u22121, \u03bet), where {\u03bet} is a sample;\n6: Let xt = arg min x\u2208X\n{ \u3008g\u0302t \u2212 \u00b5(yt\u22121 \u2212 xt\u22121), x\u3009+ 1\n2\u03b3t \u2016x\u2212 xt\u22121\u20162\n} ;\n7: Set x\u0304t = x\u0304t\u22121 + \u03b1t(xt \u2212 x\u0304t\u22121); 8: end for 9: Output: x\u0304t.\nAlgorithm 2 is a stochastic variant of Nesterov\u2019s accelerated methods. The convergence rate is also O(\u03ba/T ).\nComparing with Theorem 1, the determinant part in Theorem 4 have a better rate, i.e. LD 2\nT 2 .\nTheorem 4. Assume that \u00b5 > 0. If \u03b1t = 2 t+1 , \u03b3t = 1 \u00b5(2\u03ba/t+1/\u03b1t) , then for \u03b8 > 0,\nPr{f(x\u0304T )\u2212 f(x\u2217) > K\u0304(T ) + \u221a 2\u03b8K\u0303(T ) + \u03b8K\u0302(T )} \u2264 exp{\u2212\u03b8},\nwhere\nK\u0304(T ) := 2D2L\nT 2 +\n2\u03baQ2\nLT , K\u0303(T ) :=\n\u221a 20\u03baDQ\nT 3/2 +\n\u221a 10\u03baQ2\n2LT , K\u0302(T ) :=\n8\u03baQ2\nLT .\nRemark 2. The paper (Ghadimi & Lan, 2012) has its strongly convex version for AC-SA for sub-Gaussian gradient assumption, but its proof relies on a multi-stage algorithm.\nAlthough SAGE (Hu et al., 2009) also provided a stochastic algorithm based on Nesterov\u2019s method for strongly convexity, the high probability bound was not given in the paper.\n4 A note on weighting schemes\nIn this study, we find the interesting property of weighting scheme with \u03b1t = 2 t+1 , i.e. wt = 2t T (T+1) . The scheme takes advantage of a sequence with variance at the decay rate of 1t . Now let informally investigate a sequence with homogeneous variance, say 1. With a constant weighting scheme, \u03b1t = 1/t, i.e. wt = 1/T , the averaged variance is 1/T . With an exponential weighting scheme, \u03b11 = 1, \u03b1t = \u03b1, i.e. w1 = (1 \u2212 \u03b1)T\u22121 and wt = \u03b1(1 \u2212 \u03b1)T\u2212t, the averaged variance is \u03b12\u2212\u03b1 (1+(1\u2212\u03b1)\n2T\u22121) \u2248 \u03b12\u2212\u03b1 , which is translated to that the number of effective tail samples is a constant 2\u03b1 \u2212 1. With the weighting scheme \u03b1t = 2 t+1 or wt = 2t/(T (T + 1)), the averaged variance is 2(2T+1) 3T (T+1) \u2248 4 3T , which is translated to 3T4 effective tail samples. This is a trade-off between sample efficiency and recency. To make\nother trade-offs, We can use a generalized scheme1, \u03b1t = tr\u2211t \u03c4=1 \u03c4 r or wt = tr\u2211T \u03c4=1 \u03c4 r . Then the averaged variance is approximately (1+r) 2\n(1+2r)T .\n5 Proofs\nThe proof strategy is first to construct inequalities from the algorithms in Lemma 6 and 7, then to apply Lemma 5 to derive the probability inequalities.\n1An alternative scheme is \u03b1t = 1+r t+r or wt = (1+r)\u0393(t+r;t) \u0393(T+r+1;T ) , where \u0393(T ; t) := \u0393(T )/\u0393(t).\nLemma 5. Assume that Bt is martingale difference, wt \u2265 0, a\u0303t \u2265 0, c\u0303t \u2265 0, at \u2265 0, ct \u2265 0, dt > 0, A0 \u2264 D2, At \u2265 0, and\nXt = wt(a\u0303tAt\u22121 + 2b\u0303tBt + c\u0303tCt), (2) At \u2264 dt(atAt\u22121 + 2btBt + ctCt), (3) B2t \u2264 At\u22121Ct, Ct \u2264 1.\nIf the following conditions hold\n1. for u \u2208 (0, 1 2R\u0302T ),\nE|T exp(uXT+1) \u2264 exp((uP\u0304T + 2u2P\u0303 2T\n1\u2212 uR\u0302T )AT + uR\u0304T + 2u2R\u03032T 1\u2212 uR\u0302T ), (4)\n2. for t \u2208 [T ], atdtP\u0304t + wta\u0303t \u2264 P\u0304t\u22121, R\u0304t + wtc\u0303t + ctdtP\u0304t \u2264 R\u0304t\u22121, atdtP\u0303 2 t + 4(wtb\u0303t + btdtP\u0304t)\n2 \u2264 P\u0303 2t\u22121, R\u03032t + ctdtP\u0303 2 t \u2264 R\u03032t\u22121,\nR\u0302t \u2264 R\u0302t\u22121, atdtP\u0303 2 t R\u0302t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t \u2264 P\u0303 2t\u22121R\u0302t\u22121, atdtP\u0303 2 t R\u0302 2 t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t R\u0302t + 2b 2 td 2 t P\u0303 4 t \u2264 P\u0303 2t\u22121R\u03022t\u22121,\n(5)\nthen for \u03b8 > 0,\nPr{ T+1\u2211 t=1 Xt \u2265 P\u03040D2 + R\u03040 + \u221a 2\u03b8(P\u03030D 2 + R\u03030) + 2\u03b8R\u03020} \u2264 exp{\u2212\u03b8}. (6)\nProof. We will prove the following inequality by induction,\nE|t exp(u T+1\u2211 \u03c4=t+1 X\u03c4 ) \u2264 exp((uP\u0304t + 2u2P\u0303 2t 1\u2212 uR\u0302t )At + uR\u0304t + 2u2R\u03032t 1\u2212 uR\u0302t ), \u2200u \u2208 (0, 1 2R\u0302t ). (7)\nEq. 4 implies that Eq. (7) holds for t = T . For u \u2208 (0, 1 2R\u0302t\u22121 ), E|t\u22121 exp(u T+1\u2211 \u03c4=t X\u03c4 ) \u2264 E|t\u22121 exp(uXt + (uP\u0304t + u2P\u0303 2t 2(1\u2212 uR\u0302t) )At + uR\u0304t +\nu2R\u03032t\n2(1\u2212 uR\u0302t) ) (8)\n\u2264 E|t\u22121 exp(uwt(a\u0303tAt\u22121 + 2b\u0303tBt + c\u0303tCt) + (uP\u0304t + u2P\u0303 2t\n2(1\u2212 uR\u0302t) )dt(atAt\u22121 + 2btBt + ctCt) + uR\u0304t +\nu2R\u03032t\n2(1\u2212 uR\u0302t) )\n(9)\n\u2264 exp((u(P\u0304tdtat + pta\u0303t) + u2P\u0303 2t dtat\n2(1\u2212 uR\u0302t) )At\u22121 + u(R\u0304t + ptct + P\u0304tdtct) +\nu2R\u03032t\n2(1\u2212 uR\u0302t) +\nu2P\u0303 2t dtct\n2(1\u2212 uR\u0302t) ) (10)\n\u00d7 E|t\u22121 exp(2u(wtb\u0303t + btdtP\u0304t + ubtdtP\u0303\n2 t\n2(1\u2212 uR\u0302t) )Bt)\n\u2264 exp((u(P\u0304tdtat + pta\u0303t) + u2P\u0303 2t dtat\n2(1\u2212 uR\u0302t) + 2u2(wtb\u0303t + btdtP\u0304t +\nubtdtP\u0303 2 t\n2(1\u2212 uR\u0302t) )2)At\u22121\n+ u(R\u0304t + wtc\u0303t + P\u0304tdtct) + u2R\u03032t\n2(1\u2212 uR\u0302t) +\nu2P\u0303 2t dtct\n2(1\u2212 uR\u0302t) ) (11)\n\u2264 exp((u(P\u0304tdtat + pta\u0303t) + u2P\u0303 2t dtat\n2(1\u2212 uR\u0302t) + 2u2(wtb\u0303t + btdtP\u0304t)\n2 + u3(wtb\u0303t + btdtP\u0304t)btdtP\u0303 2 t\n2(1\u2212 uR\u0302t) +\n2u4b2td 2 t P\u0303 4 t 2(1\u2212 uR\u0302t) )At\u22121\n+ u(R\u0304t + wtc\u0303t + P\u0304tdtct) + u2R\u03032t\n2(1\u2212 uR\u0302t) +\nu2P\u0303 2t dtct\n2(1\u2212 uR\u0302t) ) (12)\n\u2264 exp((uP\u0304t\u22121 + u2P\u0303 2t\u22121\n2(1\u2212 uR\u0302t\u22121) )At\u22121 + uR\u0304t\u22121 +\nu2R\u03032t\u22121\n2(1\u2212 uR\u0302t\u22121) ), (13)\nwhere Eq. (8) is due to the assumption of induction; Eq. (9) is due to Eq. (2,3); Eq. (10) is due to Ct \u2264 1; Eq. (11) is due to E|t\u22121Bt = 0, B2t \u2264 At\u22121Ct \u2264 At\u22121, and Hoeffding\u2019s lemma, thus E|t\u22121 exp(2vBt) \u2264 exp(2v2At\u22121); Eq. (12) is due to 1\n1\u2212uR\u0302t \u2264 2R\u0302t\u22121 2R\u0302t\u22121\u2212R\u0302t \u2264 2; Eq. (13) is due to Eqs. (5). Then for u \u2208 (0, 1 2R\u0302t ),\nE exp(u T+1\u2211 \u03c4=1 X\u03c4 ) \u2264 exp((uP\u03040 + u2P\u0303 20 2(1\u2212 uR\u03020) )A0 + uR\u03040 +\nu2R\u030320\n2(1\u2212 uR\u03020) ) \u2264 exp(u(P\u03040D2 + R\u03040) +\nu2(P\u0303 20D 2 + R\u030320)\n2(1\u2212 2uR\u03020) ).\nEq. (6) follows Lemma 8.\nWe prove Lemma 6, which is the same as Lemma 7 of (Lan, 2008) except for the strong convexity.\nLemma 6. Let \u03b4t = G(xt\u22121, \u03bet)\u2212 g(xt\u22121), At = \u2016xt \u2212 x\u2217\u20162, Bt = \u3008\u03b4t, xt\u22121 \u2212 x\u2217\u3009 /Q, Ct = \u2016\u03b4t\u20162\u2217/Q2. If \u03b3t > 0 and \u03b3tL < 1, it holds for Algorithm 1 that\nf(xt)\u2212 f(x\u2217) \u2264 1\u2212 \u03b3t\u00b5\n2\u03b3t At\u22121 \u2212\n1\n2\u03b3t At \u2212QBt + \u03b3t 2(1\u2212 \u03b3tL) Q2Ct.\nProof. Let dt = xt \u2212 xt\u22121.\nf(xt) \u2264 f(xt\u22121) + \u3008g(xt\u22121), dt\u3009+ L\n2 \u2016dt\u20162 (14)\n\u2264 f(x\u2217) + \u3008g(xt\u22121), xt \u2212 x\u2217\u3009 \u2212 \u00b5\n2 \u2016xt\u22121 \u2212 x\u2217\u20162 +\nL 2 \u2016dt\u20162 (15)\n= f(x\u2217) + \u3008g\u0302t, xt \u2212 x\u2217\u3009 \u2212 \u00b5\n2 \u2016xt\u22121 \u2212 x\u2217\u20162 +\nL 2 \u2016dt\u20162 \u2212 \u3008\u03b4t, xt \u2212 x\u2217\u3009\n\u2264 f(x\u2217) + 1\u2212 \u03b3t\u00b5\n2\u03b3t \u2016xt\u22121 \u2212 x\u2217\u20162 \u2212\n1\n2\u03b3t \u2016xt \u2212 x\u2217\u20162 \u2212 1\u2212 \u03b3tL 2\u03b3t \u2016dt\u20162 \u2212 \u3008\u03b4t, dt\u3009 \u2212 \u3008\u03b4t, xt\u22121 \u2212 x\u2217\u3009 (16)\n\u2264 f(x\u2217) + 1\u2212 \u03b3t\u00b5\n2\u03b3t \u2016xt\u22121 \u2212 x\u2217\u20162 \u2212\n1\n2\u03b3t \u2016xt \u2212 x\u2217\u20162 + \u03b3t 2(1\u2212 \u03b3tL) \u2016\u03b4t\u20162\u2217 \u2212 \u3008\u03b4t, xt\u22121 \u2212 x\u2217\u3009 . (17)\nEq. (14) is due to the Lipschitz continuity of f , Eq. (15) due to the strong convexity of f , Eq. (16) due to the optimality of Step 4.\nProof of Theorem 1. Because \u03b3tL = 2\u03ba t+2\u03ba < 1, it follows Lemma 6 that\nf(xt)\u2212 f(x\u2217) \u2264 1\u2212 \u03b3t\u00b5\n2\u03b3t At\u22121 \u2212\n1\n2\u03b3t At \u2212QBt +\n\u03b3tQ 2\n2(1\u2212 \u03b3tL)\n\u2264 (t+ 2\u03ba\u2212 2)\u00b5At\u22121 4 \u2212 (t+ 2\u03ba)\u00b5At 4 \u2212QBt +\nQ2 \u00b5t .\nAs f(xt)\u2212 f(x\u2217) \u2265 \u00b52At it follows Lemma 6 that\nAt \u2264 dt(atAt\u22121 + 2btBt + ctCt),\nwhere at = \u00b5(t+2\u03ba\u22122) 4 , bt = \u2212 Q 2 , ct = Q2 \u00b5t and dt = 4 \u00b5(t+2\u03ba+2) . Let wt = \u03b1t \u220fT \u03c4=t+1(1 \u2212 \u03b1\u03c4 ) = 2t T (T+1) . Assume that \u03b10 = 0 and \u03b30 = 1. Then\nf(x\u0304T )\u2212 f(x\u2217) \u2264 T\u2211 t=1 wt(f(xt)\u2212 f(x\u2217)) \u2264 T\u2211 t=1 wt ( 1\u2212 \u03b3t\u00b5 2\u03b3t At\u22121 \u2212 1 2\u03b3t At \u2212QBt +\n\u03b3tQ 2\n2(1\u2212 \u03b3tL)\n)\n\u2264 T\u2211 t=1 wt ( 1\u2212 \u03b3t\u00b5 2\u03b3t \u2212 wt\u22121 2wt\u03b3t\u22121 ) At\u22121 \u2212 T\u2211 t=1 wtQBt + T\u2211 t=1 wt \u03b3tQ 2 2(1\u2212 \u03b3tL)\n\u2264 T\u2211 t=1 wt ( L 2t At\u22121 \u2212QBt + Q2 \u00b5t ) \u2264 LD 2 T + T\u2211 t=1 wt ( \u2212QBt + Q2 \u00b5t ) .\nNote that we use the factor At\u22121 \u2264 D2 for simplicity. Let a\u0303t = 0, b\u0303t = bt, c\u0303t = ct, XT+1 = LD 2\nT , and\nP\u0304t = 0, R\u0304t = LD2\nT + 2\u03baQ2(T \u2212 t) LT 2 ,\nP\u0303 2t = 4Q2(T \u2212 t)(t+ 2\u03ba+ 2)(t+ 2\u03ba\u2212 1)\nT 2(T + 1)2 ,\nR\u03032t = Q4\u03ba2\nL2T 2(T + 1)2 (8(T \u2212 t)(T \u2212 t\u2212 1) + 32\u03baT l\u0303n(T, t)),\nR\u0302t = 5\u03baQ2(T \u2212 t)\nLT 2 .\nGiven the facts that \u03ba \u2265 1, (t + 2\u03ba \u2212 2)(t + 2\u03ba \u2212 1) \u2264 (t + 2\u03ba + 1)(t + 2\u03ba \u2212 2), (T \u2212 t + 1) \u2212 (T \u2212 t) = 1, (T \u2212 t+ 1)2 \u2212 (T \u2212 t)2 \u2265 2(T \u2212 t), (T \u2212 t+ 1)3 \u2212 (T \u2212 t)3 \u2265 3(T \u2212 t)2, the proof of Eq. (23) follows from Lemma 5, because for t \u2265 1,\natdtP\u0304t + wta\u0303t = 0 = P\u0304t\u22121,\nR\u0304t + wtct + ctdtP\u0304t \u2264 R\u0304t + 2t T 2 Q2 \u00b5t \u2264 R\u0304t\u22121,\natdtP\u0303 2 t + 4(wtb\u0303t + btdtP\u0304t) 2 \u2264 t+ 2\u03ba\u2212 2 t+ 2\u03ba+ 2 P\u0303 2t + 4t2Q2 T 2(T + 1)2 \u2264 Q 2 T 2(T + 1)2 (4(T \u2212 t)(t+ 2\u03ba+ 1)(t+ 2\u03ba\u2212 2) + 4t2)\n\u2264 P\u0303 2t\u22121 \u2212 Q2\nT 2(T + 1)2 (4(t+ 2\u03ba+ 1)(t+ 2\u03ba\u2212 2)\u2212 4t2)\n= P\u0303 2t\u22121 \u2212 Q2\nT 2(T + 1)2 (4(2\u03ba\u2212 1)t+ 16\u03ba2 \u2212 8\u03ba\u2212 8) \u2264 P\u0303 2t\u22121,\nR\u03032t + ctdtP\u0303 2 t \u2264 R\u03032t + 16Q4(T \u2212 t)(t+ 2\u03ba+ 2)(t+ 2\u03ba\u2212 1) \u00b52T 2(T + 1)2t(t+ 2\u03ba+ 2)\n\u2264 Q 4 \u00b52T 2(T + 1)2 (8(T \u2212 t)(T \u2212 t\u2212 1) + 32\u03baT l\u0303n(T, t) + 16(T \u2212 t) + 16(2\u03ba\u2212 1) t ) \u2264 R\u03032t\u22121,\nand\natdtP\u0303 2 t R\u0302t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t \u2264 4Q2(T \u2212 t)(t+ 2\u03ba\u2212 1)(t+ 2\u03ba\u2212 2) T 4 R\u0302t + 32Q4t(T \u2212 t)(t+ 2\u03ba\u2212 1) \u00b5T 6\n\u2264 Q 4\n\u00b5T 6 (20(T \u2212 t)2(t+ 2\u03ba\u2212 1)(t+ 2\u03ba\u2212 2) + 32t(T \u2212 t)(t+ 2\u03ba\u2212 1))\n\u2264 P\u0303 2t\u22121R\u0302t\u22121 \u2212 Q4(T \u2212 t) \u00b5T 6 (2\u00d7 20(t+ 2\u03ba+ 1)(t+ 2\u03ba\u2212 2)\u2212 32t(t+ 2\u03ba\u2212 1)) = P\u0303 2t\u22121R\u0302t\u22121 \u2212 Q4(T \u2212 t) \u00b5T 6 (8t2 \u2212 8t+ 16\u03ba(6t\u2212 5) + 160\u03ba2 \u2212 80) \u2264 P\u0303 2t\u22121R\u0302t\u22121.\natdtP\u0303 2 t R\u0302 2 t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t R\u0302t + 2b 2 td 2 t P\u0303 4 t\n\u2264 4Q 2(T \u2212 t)(t+ 2\u03ba+ 1)(t+ 2\u03ba\u2212 2)\nT 4 R\u03022t + 32Q4t(T \u2212 t)(t+ 2\u03ba\u2212 1) \u00b5T 6 R\u0302t + 128Q6(T \u2212 t)2(t+ 2\u03ba\u2212 1)2 \u00b52T 8\n\u2264 P\u0303 2t\u22121R\u03022t\u22121 \u2212 Q6(T \u2212 t)2\n\u00b52T 8 (3\u00d7 100(t+ 2\u03ba+ 1)(t+ 2\u03ba\u2212 2)\u2212 160t(t+ 2\u03ba\u2212 1)\u2212 128(t+ 2\u03ba\u2212 1)2)\n= P\u0303 2t\u22121R\u0302 2 t\u22121 \u2212\nQ6(T \u2212 t)2\n\u00b52T 8 (12(t\u2212 1)2 + 368(t\u2212 1)(\u03ba\u2212 1) + 688(\u03ba\u2212 1)2 + 508(t\u2212 1) + 1656(\u03ba\u2212 1) + 368)\n\u2264 P\u0303 2t\u22121R\u03022t\u22121.\nProof of Proposition 2. Because \u03b3tL < 1, it follows Lemma 6 that\nf(xt)\u2212 f(x\u2217) \u2264 1\u2212 \u03b3t\u00b5\n2\u03b3t At\u22121 \u2212\n1\n2\u03b3t At \u2212QBt +\n\u03b3tQ 2\n2(1\u2212 \u03b3tL)\n\u2264 (L+ \u00b5(2t\u2212 1))At\u22121 2 \u2212 (L+ 2\u00b5t)At 2 \u2212QBt +\nQ2 4\u00b5t .\nAs the strong convexity implies that f(xt)\u2212 f(x\u2217) \u2265 \u00b52At, it follows Lemma 6 that At \u2264 dt(atAt\u22121 + 2btBt + ctCt),\nwhere at = \u00b5(t+\u03ba\u22121) 2 , bt = \u2212 Q 2 , ct = Q2 2\u00b5t and dt = 2 \u00b5(t+\u03ba+1) . Let wt = \u03b1t \u220fT \u03c4=t+1(1\u2212 \u03b1\u03c4 ) = 1 T . Assume that \u03b10 = 0 and \u03b30 = 1. Then\nf(x\u0304T )\u2212 f(x\u2217) \u2264 T\u2211 t=1 wt(f(xt)\u2212 f(x\u2217)) \u2264 T\u2211 t=1 wt ( 1\u2212 \u03b3t\u00b5 2\u03b3t At\u22121 \u2212 1 2\u03b3t At \u2212QBt +\n\u03b3tQ 2\n2(1\u2212 \u03b3tL)\n)\n\u2264 T\u2211 t=1 wt ( 1\u2212 \u03b3t\u00b5 2\u03b3t \u2212 wt\u22121 2wt\u03b3t\u22121 ) At\u22121 \u2212 T\u2211 t=1 wtQBt + T\u2211 t=1 wt \u03b3tQ 2 2(1\u2212 \u03b3tL)\n\u2264 LA0 2T + T\u2211 t=1 wt ( \u2212QBt + Q2 4\u00b5t ) .\nLet a\u0303t = 0, b\u0303t = bt, c\u0303t = ct, XT+1 = LD2\n2T , and\nP\u0304t = 0, R\u0304t = Q2\n2\u00b5T l\u0303n(T, t),\nP\u0303 2t = Q2(t+ \u03ba+ 1)\nT 2 ,\nR\u03032t = Q4\n\u00b52T 2 l\u0303n(T, t),\nR\u0302t = 3Q2\n\u00b5T .\nThe proof follows from Lemma 5, because for k \u2265 1,\nP\u0304tdtat + pta\u0303t = 0 = P\u0304t\u22121, R\u0304t + wtct + P\u0304tdtct \u2264 Q2\n2\u00b5T ln T t + 1 T\nQ2 2\u00b5t \u2264 R\u0304t\u22121,\nP\u0303 2t dtat + 4(wt + P\u0304tdt) 2b2t \u2264\nQ2(\u03ba+ t+ 1) T 2 t+ \u03ba\u2212 1 t+ \u03ba+ 1 + Q2 T 2 = Q2(t+ \u03ba) T 2(t+ \u03ba+ 1) = P\u0303 2t\u22121,\nR\u03032t + P\u0303 2 t dtct \u2264\nQ4\n\u00b52T 2 ln T t +\n2Q2 \u00b5T 2 Q2 2\u00b5t \u2264 R\u03032t\u22121,\nand\natdtP\u0303 2 t R\u0302t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t \u2264 Q2(t+ \u03ba\u2212 1)(t+ \u03ba+ 1) T 2(t+ \u03ba+ 1) R\u0302t + 2Q4(t+ \u03ba+ 1) \u00b5T 3(t+ \u03ba+ 1) \u2264 Q 2(t+ \u03ba) T 2 R\u0302t\u22121.\natdtP\u0303 2 t R\u0302 2 t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t R\u0302t + 2b 2 td 2 t P\u0303 4 t\n\u2264 Q 2(t+ \u03ba\u2212 1)(t+ \u03ba+ 1)\nT 2(t+ \u03ba+ 1) R\u03022t +\n2Q4(t+ \u03ba+ 1) \u00b5T 3(t+ \u03ba+ 1) R\u0302t + 2Q6(t+ \u03ba+ 1)2 \u00b52T 4(t+ \u03ba+ 1)2 \u2264 Q 2(t+ \u03ba) T 2 R\u03022t\u22121.\nProof of Proposition 3. Because \u03b3tL < 1, it follows Lemma 6 that\nf(xt)\u2212 f(x\u2217) \u2264 1\u2212 \u03b3t\u00b5\n2\u03b3t At\u22121 \u2212\n1\n2\u03b3t At \u2212QBt +\n\u03b3tQ 2\n2(1\u2212 \u03b3tL)\n\u2264 (L+ \u00b5(t\u2212 1))At\u22121 2 \u2212 (L+ \u00b5t)At 2 \u2212QBt +\nQ2 2\u00b5t .\nAs the strong convexity implies that f(xt)\u2212 f(x\u2217) \u2265 \u00b52At, it follows Lemma 6 that\nAt \u2264 dt(atAt\u22121 + 2btBt + ctCt),\nwhere at = \u00b5(t+\u03ba\u22121) 2 , bt = \u2212 Q 2 , ct = Q2 2\u00b5t and dt = 2 \u00b5(t+\u03ba+1) . Because the solution is an interior point, we have\nf(xT )\u2212 f(x\u2217) \u2264 L\n2 AT .\nLet wt = 0, XT+1 = L 2AT , and\nP\u0304t = L(t+ \u03ba)(t+ \u03ba+ 1)\n2(T + \u03ba)(T + \u03ba+ 1) ,\nR\u0304t = \u03ba2Q2\n2L(T + \u03ba)(T + \u03ba+ 1) (T \u2212 t+ \u03bal\u0303n(T, t)),\nP\u0303 2t = Q2\u03ba2(T \u2212 t)(t+ \u03ba)(t+ \u03ba+ 1)\n2(T + \u03ba)2(T + \u03ba+ 1)2 ,\nR\u03032t = \u03ba4Q4\n4L2(T + \u03ba)2(T + \u03ba+ 1)2 ((T \u2212 t)(T \u2212 t\u2212 1) + \u03baT l\u0303n(T, t)),\nR\u0302t = 2\u03ba2Q2(T \u2212 t)\nL(T + \u03ba)(T + \u03ba+ 1) .\nThe proof follows from Lemma 5, because\nP\u0304tdtat = L(t+ \u03ba)(t+ \u03ba\u2212 1) 2(T + \u03ba)(T + \u03ba+ 1) = P\u0304t\u22121,\nR\u0304t + P\u0304tdtct \u2264 R\u0304t + L(t+ \u03ba)(t+ \u03ba+ 1)\n2(T + \u03ba)(T + \u03ba+ 1)\n2\n\u00b5(t+ \u03ba+ 1)\nQ2\n2\u00b5t\n\u2264 \u03ba 2Q2 2L(T + \u03ba)(T + \u03ba+ 1) (T \u2212 t+ \u03bal\u0303n(T, t) + t+ \u03ba t ) \u2264 R\u0304t\u22121,\nP\u0303 2t dtat + P\u0304 2 t d 2 t b 2 t \u2264 t+ \u03ba\u2212 1 t+ \u03ba+ 1 P\u0303 2t + \u03ba2Q2(t+ \u03ba)2 4(T + \u03ba)2(T + \u03ba+ 1)2\n\u2264 P\u0303 2t\u22121 \u2212 \u03ba2Q2 (T + \u03ba)2(T + \u03ba+ 1)2 ( 1 2 (t+ \u03ba\u2212 1)(t+ \u03ba)\u2212 1 4 (t+ \u03ba)2) \u2264 P\u0303 2t\u22121 \u2212 \u03ba2Q2\n(T + \u03ba)2(T + \u03ba+ 1)2 ( 1 4 (t+ \u03ba)(t+ \u03ba\u2212 2)) \u2264 P\u0303 2t\u22121, [t \u2265 1 and \u03ba \u2265 1]\nR\u03032t + P\u0303 2 t dtct \u2264 R\u03032t + Q4\u03ba2(T \u2212 t)(t+ \u03ba) 2\u00b52(T + \u03ba)2(T + \u03ba+ 1)2t\n\u2264 Q 4\u03ba2 4\u00b52(T + \u03ba)2(T + \u03ba+ 1)2 ((T \u2212 t)(T \u2212 t\u2212 1) + \u03baT l\u0303n(T, t) + 2(T \u2212 t) + (T \u2212 t)\u03ba t ) \u2264 R\u03032t\u22121,\nand\natdtP\u0303 2 t R\u0302t + 4b 2 td 2 t P\u0304tP\u0303 2 t\n\u2264 Q 2\u03ba2\n(T + \u03ba)2(T + \u03ba+ 1)2\n( 1\n2 (T \u2212 t)(t+ \u03ba)(t+ \u03ba\u2212 1)R\u0302t + (T \u2212 t)(t+ \u03ba)\nLQ2(t+ \u03ba)\n\u00b52(T + \u03ba)(T + \u03ba+ 1) ) \u2264 P\u0303 2t\u22121R\u0302t\u22121 \u2212 Q4\u03ba4 L(T + \u03ba)3(T + \u03ba+ 1)3 ( 2(T \u2212 t)(t+ \u03ba)(t+ \u03ba\u2212 1)\u2212 (T \u2212 t)(t+ \u03ba)2\n) \u2264 P\u0303 2t\u22121R\u0302t\u22121 \u2212 Q4\u03ba4\nL(T + \u03ba)3(T + \u03ba+ 1)3(T \u2212 t) (t+ \u03ba)(t+ \u03ba\u2212 2) \u2264 P\u0303 2t\u22121R\u0302t\u22121.\natdtP\u0303 2 t R\u0302 2 t + 4b 2 td 2 t P\u0304tP\u0303 2 t R\u0302t + 2b 2 td 2 t P\u0303 4 t\n\u2264 Q 2\u03ba2 (T + \u03ba)2(T + \u03ba+ 1)2 ( 1 2 (T \u2212 t)(t+ \u03ba)(t+ \u03ba\u2212 1)R\u03022t + (T \u2212 t)(t+ \u03ba)\nLQ2(t+ \u03ba)\n\u00b52(T + \u03ba)(T + \u03ba+ 1) R\u0302t\n+ Q4\u03ba2(T \u2212 t)2(t+ \u03ba)2\n4\u00b52(T + \u03ba)2(T + \u03ba+ 1)2 )\n\u2264 P\u0303 2t\u22121R\u03022t\u22121 \u2212 Q6\u03ba6 L2(T + \u03ba)4(T + \u03ba+ 1)4 (6(T \u2212 t)2(t+ \u03ba)(t+ \u03ba\u2212 1)\u2212 2(T \u2212 t)2(t+ \u03ba)\u2212 1 4 (T \u2212 t)2(t+ \u03ba)2) \u2264 P\u0303 2t\u22121R\u03022t\u22121 \u2212 Q6\u03ba6(T \u2212 t)2(t+ \u03ba)\nL2(T + \u03ba)4(T + \u03ba+ 1)4 ( 15 4 (t+ \u03ba)\u2212 6) \u2264 P\u0303 2t\u22121R\u03022t\u22121.\nSimilar to Lemma 9 of (Lan, 2008), we have the following lemma for Algorithm 2 with the consideration of strongly convex cases.\nLemma 7. Let \u03b4t = G(yt\u22121, \u03bet)\u2212 g(yt\u22121), At = \u2016xt \u2212 x\u2217\u20162, Bt = \u3008\u03b4t, xt\u22121 \u2212 x\u2217\u3009 /Q, Ct = \u2016\u03b4t\u20162\u2217/Q2. If 0 < \u03b1t < 1, \u03b3t > 0 and \u03b3t(\u03b1tL+ \u00b5) < 1, it holds for Algorithm 2 that\nf(x\u0304t)\u2212 f(x\u2217) \u2264 (1\u2212 \u03b1t)(f(x\u0304t\u22121)\u2212 f(x\u2217)) + \u03b1t(1\u2212 \u03b3t\u00b5)\n2\u03b3t At\u22121 \u2212 \u03b1t 2\u03b3t At \u2212 \u03b1tQBt + \u03b1t\u03b3t 2(1\u2212 \u03b1t\u03b3tL\u2212 \u03b3t\u00b5) Q2Ct.\nProof. Let dt = xt \u2212 xt\u22121 and vt = xt\u22121 + \u03b3t\u00b5(yt\u22121 \u2212 xt\u22121). Note that x\u0304t \u2212 yt\u22121 = \u03b1tdt.\nf(x\u0304t) \u2264 f(yt\u22121) + \u3008g(yt\u22121), x\u0304t \u2212 yt\u22121\u3009+ L\n2 \u2016x\u0304t \u2212 yt\u22121\u20162 (18)\n= (1\u2212 \u03b1t)[f(yt\u22121) + \u3008g(yt\u22121), x\u0304t\u22121 \u2212 yt\u22121\u3009] + \u03b1t[f(yt\u22121) + \u3008g(yt\u22121), xt \u2212 yt\u22121\u3009] + \u03b12tL\n2 \u2016dt\u20162\n\u2264 (1\u2212 \u03b1t)f(x\u0304t\u22121) + \u03b1tf(x\u2217) + \u03b1t \u3008g(yt\u22121), xt \u2212 x\u2217\u3009 \u2212 \u03b1t\u00b5\n2 \u2016yt\u22121 \u2212 x\u2217\u20162 +\n\u03b12tL\n2 \u2016dt\u20162 (19)\n= (1\u2212 \u03b1t)f(x\u0304t\u22121) + \u03b1tf(x\u2217) + \u03b1t \u3008g\u0302t, xt \u2212 x\u2217\u3009 \u2212 \u03b1t\u00b5\n2 \u2016yt\u22121 \u2212 x\u2217\u20162 +\n\u03b12tL\n2 \u2016dt\u20162 \u2212 \u03b1t \u3008\u03b4t, xt \u2212 x\u2217\u3009\n\u2264 (1\u2212 \u03b1t)f(x\u0304t\u22121) + \u03b1tf(x\u2217) + \u03b1t \u03b3t \u3008xt \u2212 vt, x\u2217 \u2212 xt\u3009 \u2212 \u03b1t\u00b5 2 \u2016yt\u22121 \u2212 x\u2217\u20162 +\n\u03b12tL\n2 \u2016dt\u20162 \u2212 \u03b1t \u3008\u03b4t, xt \u2212 x\u2217\u3009 (20)\n= (1\u2212 \u03b1t)f(x\u0304t\u22121) + \u03b1tf(x\u2217) + \u03b1t(1\u2212 \u03b3t\u00b5)\n2\u03b3t \u2016xt\u22121 \u2212 x\u2217\u20162 \u2212 \u03b1t 2\u03b3t \u2016xt \u2212 x\u2217\u20162 \u2212 \u03b1t\u00b5 2 \u2016yt\u22121 \u2212 xt\u20162\n\u2212 \u03b1t(1\u2212 \u03b3t\u00b5\u2212 \u03b1t\u03b3tL) 2\u03b3t \u2016dt\u20162 \u2212 \u03b1t \u3008\u03b4t, dt\u3009 \u2212 \u03b1t \u3008\u03b4t, xt\u22121 \u2212 x\u2217\u3009\n\u2264 (1\u2212 \u03b1t)f(x\u0304t\u22121) + \u03b1tf(x\u2217) + \u03b1t(1\u2212 \u03b3t\u00b5)\n2\u03b3t \u2016xt\u22121 \u2212 x\u2217\u20162 \u2212 \u03b1t 2\u03b3t \u2016xt \u2212 x\u2217\u20162\n+ \u03b1t\u03b3t\n2(1\u2212 \u03b3t\u00b5\u2212 \u03b1t\u03b3tL) \u2016\u03b4t\u20162\u2217 \u2212 \u03b1t \u3008\u03b4t, xt\u22121 \u2212 x\u2217\u3009 . (21)\nEq. (24) is due to the Lipschitz continuity of f , Eq. (25) due to the strong convexity of f , Eq. (20) due to the optimality of Step 6.\nProof of Theorem 4. Let \u03bbt = \u220fT \u03c4=t+1(1\u2212 \u03b1t) = t(t+1) T (T+1) . We have and\n\u03bbt\u03b1t(1\u2212 \u03b3t\u00b5) \u03b3t \u2212 \u03bbt\u22121\u03b1t\u22121 \u03b3t\u22121 = 2t T (T + 1) ( 2L t + \u00b5(t+ 1) 2 \u2212 \u00b5)\u2212 2(t\u2212 1) T (T + 1) ( 2L t\u2212 1 + \u00b5t 2 ) = 0, \u2200t > 1.\nLet at = \u00b5(4\u03ba+t(t\u22121)) 2t , bt = \u2212 Q 2 , ct = Q2 \u00b5t , and dt = 2t \u00b5(4\u03ba+t(t+1)) . Summing up the inequality in Lemma 7 weighted by \u03bbt, we have\nf(x\u0304t)\u2212 f(x\u2217) \u2264 \u03bb1\u03b11(1\u2212 \u03b31\u00b5)\n2\u03b31 A0 \u2212 \u03bbt\u03b1t 2\u03b3t At + t\u2211 \u03c4=1 \u03bb\u03c4\u03b1\u03c4 (\u2212QBt + \u03b3\u03c4 2(1\u2212 \u03b1\u03c4\u03b3\u03c4L\u2212 \u03b3\u03c4\u00b5) Q2C\u03c4 )\n\u2264 2L T (T + 1) A0 \u2212 2t T (T + 1) At dt + t\u2211 \u03c4=1\n2\u03c4\nT (T + 1) (2b\u03c4B\u03c4 + c\u03c4C\u03c4 ) .\n(22)\nLet A\u0303t := dt t { LA0 + \u2211t \u03c4=1 (2\u03c4b\u03c4Bt + \u03c4ctCt) } . Because f(x\u0304t)\u2212 f(x\u2217) \u2265 0, we have\nt\ndt At \u2264\nt\ndt A\u0303t = t\u2212 1 dt\u22121 A\u0303t\u22121 + 2tbtBt + tctCt = tatA\u0303t\u22121 + 2tbtBt + tctCt\nThen At \u2264 A\u0303t = dt(atA\u0303t\u22121 + 2btBt + ctCt). (23)\nGiven Eq. (22) and Eq. (23), letting wt = 2t T (T+1) , a\u0303t = 0, b\u0303t = bt, c\u0303t = ct, XT+1 = 2LD2 T (T+1) , and\nP\u0304t = 0, R\u0304t = 2LD2\nT 2 + 2\u03baQ2(T \u2212 t) LT 2 ,\nP\u0303 2t = 5Q2(T \u2212 t)(t(t+ 1) + 4\u03ba)\nT 4 ,\nR\u03032t = 5\u03ba2Q4(T \u2212 t)(T \u2212 t\u2212 1)\n2L2T 4 ,\nR\u0302t = 4\u03baQ2(T \u2212 t)\nLT 2 ,\nthe proof follows from Lemma 5, because\natdtP\u0304t + wta\u0303t = 0 = P\u0304t\u22121,\nR\u0304t + wtc\u0303t + ctdtP\u0304t \u2264 R\u0304t + 2t T 2 Q2 \u00b5t \u2264 R\u0304t\u22121,\natdtP\u0303 2 t + 4(wtb\u0303t + btdtP\u0304t) 2 \u2264 t(t\u2212 1) + 4\u03ba t(t+ 1) + 4\u03ba P\u0303 2t + 4t2Q2 T 4\n\u2264 Q 2\nT 4 (6(t(t\u2212 1) + 4\u03ba)(T \u2212 t) + 4t2) \u2264 P\u0303 2t\u22121 \u2212\nQ2 T 4 (5(t(t\u2212 1) + 4\u03ba)\u2212 4t2)\n\u2264 P\u0303 2t\u22121 \u2212 Q2\nT 4 (t2 \u2212 5t+ 20\u03ba) \u2264 P\u0303 2t\u22121 \u2212\nQ2 T 4 (3t) \u2264 P\u0303 2t\u22121,\nR\u03032t + ctdtP\u0303 2 t \u2264 R\u03032t + 5Q4(T \u2212 t) \u00b52T 4 \u2264 R\u03032t\u22121,\nand\natdtP\u0303 2 t R\u0302t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t \u2264 Q2(t(t\u2212 1) + 4\u03ba)(T \u2212 t) T 4 R\u0302t + 4t2Q4(T \u2212 t) \u00b5T 6\n\u2264 Q 4\n\u00b5T 6 (4(t(t\u2212 1) + 4\u03ba)(T \u2212 t)2 + 4t2(T \u2212 t))\n\u2264 P\u0303 2t\u22121R\u0302t\u22121 \u2212 Q4(T \u2212 t) \u00b5T 6 (2\u00d7 4(t(t\u2212 1) + 4\u03ba)\u2212 4t2) = P\u0303 2t\u22121R\u0302t\u22121 \u2212 Q4(T \u2212 t) \u00b5T 6 (4t2 \u2212 8t+ 32\u03ba) \u2264 P\u0303 2t\u22121R\u0302t\u22121 \u2212 Q4(T \u2212 t) \u00b5T 6 (14t) \u2264 P\u0303 2t\u22121R\u0302t\u22121\natdtP\u0303 2 t R\u0302 2 t + 4btdt(wtb\u0303t + btdtP\u0304t)P\u0303 2 t R\u0302t + 2b 2 td 2 t P\u0303 4 t\n\u2264 5Q 2(t(t\u2212 1) + 4\u03ba)(T \u2212 t)\nT 4 R\u03022t + 20t2Q4(T \u2212 t) \u00b5T 6 R\u0302t + 100t2Q6(T \u2212 t)2 \u00b52T 8\n\u2264 Q 6\n\u00b52T 8 (80(t(t\u2212 1) + 4\u03ba)(T \u2212 t)3 + 80t2(T \u2212 t)2 + 100t2(T \u2212 t)2)\n\u2264 P\u0303 2t\u22121R\u03022t\u22121 \u2212 Q6(T \u2212 t)2\n\u00b52T 8 (3\u00d7 80(t(t\u2212 1) + 4\u03ba)\u2212 80t2 \u2212 100t2)\n= P\u0303 2t\u22121R\u0302 2 t\u22121 \u2212\nQ6(T \u2212 t)2\n\u00b52T 8 (60t2 \u2212 240t+ 960\u03ba) \u2264 P\u0303 2t\u22121R\u03022t\u22121 \u2212\n(T \u2212 t)2Q6\n\u00b52T 8 (240t) \u2264 P\u0303 2t\u22121R\u03022t\u22121.\nSupporting lemma\nWe use part of the proof of Lemma 8 in (Birge\u0301 & Massart, 1998).\nLemma 8. Let B > 0 and \u03c3 > 0. If the log-moment generating function satisfies\nlogE exp{uZ} \u2264 \u03c3 2u2\n2(1\u2212 uB) for all 0 \u2264 u < 1/B,\nthen\nPr{Z \u2265 } \u2264 exp{\u2212 2\n2\u03c32 + 2 B } for all \u2265 0, (24)\nand Pr{Z \u2265 \u221a 2\u03b8\u03c32 + \u03b8B} \u2264 exp{\u2212\u03b8} for all \u03b8 \u2265 0. (25)\nProof. It follows Markov\u2019s inequality that\nPr{Z \u2265 } \u2264 inf u E exp{\u2212u + uZ} = exp{\u2212h( )},\nwhere h( ) := supu u \u2212 \u03c3 2u2 2(1\u2212uB) . Also, the supremum is achieved for\n= \u03c32u\n1\u2212 uB +\n\u03c32u2B\n2(1\u2212 uB)2 =\n\u03c32u\n2(1\u2212 uB) +\n\u03c32u\n2(1\u2212 uB)2 ,\ni.e. u = B\u22121[1\u2212 \u03c3(2 B + \u03c32)\u22121/2] < 1/B. Then we prove Eq. (24), as\nh( ) = 2\nB + \u03c32 + \u03c32(1 + 2 B/\u03c32)1/2 \u2265\n2\n2 B + 2\u03c32 .\nLet\n\u03b8 := \u03c32u2\n2(1\u2212 uB)2 = h( ).\nThen we prove Eq. (25), as\n\u221a 2\u03b8\u03c32 + \u03b8B =\n\u03c32u\n(1\u2212 uB) +\n\u03c32u2B\n2(1\u2212 uB)2 = .\nReferences\nBirge\u0301, L., & Massart, P. (1998). Minimum contrast estimators on sieves: exponential bounds and rates of convergence. Bernoulli, 4, 329\u2013375.\nGhadimi, S., & Lan, G. (2012). Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: a generic algorithmic framework. Optimization-online.\nHu, C., Kwok, J. T., & Pan, W. (2009). Accelerated gradient methods for stochastic optimization and online learning. NIPS\u201909: Neural Information Processing Systems.\nLan, G. (2008). Efficient methods for stochastic composite optimization. SIAM Journal on Optimization.\nRakhlin, A., Shamir, O., & Sridharan, K. (2012). Making gradient descent optimal for strongly convex stochastic optimization. ICML 2012.\nRudelson, M., & Vershynin, R. (2009). Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics, 62, 1707\u20131739.\nSmale, S., & Zhou, D.-X. (2003). Estimating the approximation error in learning theory. Anal. Appl. (Singap.), 1, 17\u201341."}], "references": [{"title": "Minimum contrast estimators on sieves: exponential bounds and rates of convergence", "author": ["L. Birg\u00e9", "P. Massart"], "venue": null, "citeRegEx": "Birg\u00e9 and Massart,? \\Q1998\\E", "shortCiteRegEx": "Birg\u00e9 and Massart", "year": 1998}, {"title": "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: a generic algorithmic framework. Optimization-online", "author": ["S. Ghadimi", "G. Lan"], "venue": null, "citeRegEx": "Ghadimi and Lan,? \\Q2012\\E", "shortCiteRegEx": "Ghadimi and Lan", "year": 2012}, {"title": "Accelerated gradient methods for stochastic optimization and online learning. NIPS\u201909", "author": ["C. Hu", "J.T. Kwok", "W. Pan"], "venue": "Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Efficient methods for stochastic composite optimization", "author": ["G. Lan"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Lan,? \\Q2008\\E", "shortCiteRegEx": "Lan", "year": 2008}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Rudelson and Vershynin,? \\Q2009\\E", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2009}, {"title": "Estimating the approximation error in learning theory", "author": ["S. Smale", "Zhou", "D.-X"], "venue": "Anal. Appl. (Singap.),", "citeRegEx": "Smale et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Smale et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Some nonsmooth cases have been studied in (Lan, 2008) and (Ghadimi & Lan, 2012).", "startOffset": 42, "endOffset": 53}, {"referenceID": 4, "context": "There are studies on the high probability convergence rate of stochastic algorithm on strongly convex functions, such as (Rakhlin et al., 2012).", "startOffset": 121, "endOffset": 143}, {"referenceID": 2, "context": "Although SAGE (Hu et al., 2009) also provided a stochastic algorithm based on Nesterov\u2019s method for strongly convexity, the high probability bound was not given in the paper.", "startOffset": 14, "endOffset": 31}, {"referenceID": 3, "context": "We prove Lemma 6, which is the same as Lemma 7 of (Lan, 2008) except for the strong convexity.", "startOffset": 50, "endOffset": 61}, {"referenceID": 3, "context": "Similar to Lemma 9 of (Lan, 2008), we have the following lemma for Algorithm 2 with the consideration of strongly convex cases.", "startOffset": 22, "endOffset": 33}], "year": 2013, "abstractText": "With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O(\u03ba/T ) for strongly convex functions, instead of O(\u03ba ln(T )/T ). We also prove that an accelerated SGD algorithm also achieves a rate of O(\u03ba/T ).", "creator": "TeX"}}}