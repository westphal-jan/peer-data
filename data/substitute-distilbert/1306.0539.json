{"id": "1306.0539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "On the Performance Bounds of some Policy Search Dynamic Programming Algorithms", "abstract": "we consider its infinite - sequence discounted optimal control network formalized by markov decision equations. we focus on policy search algorithms, as specify exactly approximately optimal policy by ordering the standard policy iteration ( pi ) scheme via an - approximate greedy operator ( kakade and langford, 2002 ; lazaric que al., 2010 ). we describe existing and a few new posterior bounds for direct policy iteration ( dpi ) ( lagoudakis and parr, 2013 ; fern et am., 2006 ; lazaric et al., 2010 ) and conservative policy iteration ( cpi ) ( kakade and langford, 1903 ). by paying a particular attention to the concentrability constants involved besides such guarantees, we adequately argue that the guarantee of cpi is much better than that of dpi, plus here comes at the cost of a relative - - exponential in $ \\ frac { 1 } { \\ epsilon } $ - - increase of time complexity. we significantly describe an algorithm, necessity - stationary independent policy iteration ( nsdpi ), that can otherwise be seen as 1 ) a variation of policy search by dynamic programming by bagnell et al. ( 1972 ) to the zero horizon situation or 2 ) a generalized version of the non - stationary pi with continuous period of pearson and lesner ( 1966 ). which provide an amendment of this algorithm, that shows in particular that it enjoys the best of both worlds : its performance guarantee is similar to that possible cpi, but within a time complexity similar versus that of dpi.", "histories": [["v1", "Mon, 3 Jun 2013 19:13:53 GMT  (785kb,D)", "http://arxiv.org/abs/1306.0539v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["bruno scherrer"], "accepted": false, "id": "1306.0539"}, "pdf": {"name": "1306.0539.pdf", "metadata": {"source": "CRF", "title": "On the Performance Bounds of some Policy Search Dynamic Programming Algorithms", "authors": ["Bruno Scherrer"], "emails": ["bruno.scherrer@inria.fr"], "sections": [{"heading": null, "text": "\u2014\nincrease of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI."}, {"heading": "1 Introduction", "text": "The study of approximation in Dynamic Programming algorithms for infinite-horizon discounted Markov Decision Processes (MDP) has a rich history (Bertsekas and Tsitsiklis, 1996; Szepesva\u0301ri, 2010). Some of the first important results, gathered by Bertsekas and Tsitsiklis (1996), provide bounds on the closeness to optimality of the computed policy as a function of the max-norm errors during iterations. If Value or Policy Iteration are run with some error k, it is known that the value v\u03c0k of the policies \u03c0k generated by the algorithm can get close to the optimal policy \u03c0\u2217 if the errors are small enough since we have\nlim sup k\u2192\u221e\n\u2016v\u03c0\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 2\u03b3\n(1\u2212 \u03b3)2 sup k \u2016 k\u2016\u221e.\nUnfortunately, such results have a limited range since in practice, most implementations of Dynamic Programming algorithms involve function approximation (like classification or regression) that controls some \u03bd-weighted Lp norm \u2016 \u00b7 \u2016\u03bd,p instead of the max-norm \u2016 \u00b7 \u2016\u221e. Starting with the works of Munos (2003, 2007), this motivated a recent trend of research (Antos et al., 2008; Munos and Szepesva\u0301ri, 2008; Farahmand et al., 2009, 2010; Lazaric et al., 2010, 2011; Scherrer et al., 2012) that provide generalizations of the above result of the form\nlim sup k\u2192\u221e\n\u2016v\u03c0\u2217 \u2212 v\u03c0k\u2016\u00b5,p \u2264 2\u03b3C1/p\n(1\u2212 \u03b3)2 sup k \u2016 k\u2016\u03bd,p, (1)\nwhere \u00b5 and \u03bd are some distributions. The possibility to express the right hand side with respect to some \u03bd-weighted Lp norm comes at the price of a constant C, called concentrability coefficient, that measures the stochastic smoothness of the MDP: the more the MDP dynamics may concentrate in some parts of\nar X\niv :1\n30 6.\n05 39\nv1 [\ncs .A\nI] 3\nJ un\n2 01\n3\nthe state space, the bigger C (see Munos and Szepesva\u0301ri (2008) for a detailed discussion). Though efforts have been employed to improve these constants (Farahmand et al., 2010; Scherrer et al., 2012), the fact that it may be infinite (for instance when the MDP is deterministic) constitutes a severe limitation.\nInterestingly, one work (Kakade and Langford, 2002; Kakade, 2003)\u2014anterior to those of Munos (2003, 2007)\u2014proposed an approximate Dynamic Programming algorithm, Conservative Policy Iteration (CPI), with a performance bounds similar to Equation (1) but with a constant that is\u2014as we will argue precisely in this paper (Remarks 1, 2 and 3)\u2014better than all the others: it only involves the mismatch between some input measure of the algorithm and a baseline distribution corresponding roughly to the frequency visitation of the optimal policy, a natural piece of information that an expert of the domain may provide. The main motivation of this paper is to emphasize the importance of these concentrability constants regarding the significance of the performance bounds.\nIn Section 3, we will describe Direct Policy Iteration (DPI), a very simple Dynamic Programming algorithm proposed by Lagoudakis and Parr (2003); Fern et al. (2006); Lazaric et al. (2010) that is similar to CPI; for this algorithm, we will provide and extend the analysis developed by Lazaric et al. (2010). We will then consider CPI in Section 4, describe the theoretical properties originally given by Kakade and Langford (2002), as well as some new bounds that will ease the comparison with DPI. In particular, as a corollary of our analysis, we will obtain an original bound for CPI(\u03b1), a practical variation of CPI that uses a fixed stepsize. We will argue that the concentrability constant involved in the analysis of CPI is better than those of DPI. This improvement of quality unfortunately comes at some price: the number of iterations required by CPI can be exponentially bigger than that of DPI. This will motivate the introduction of another algorithm: we describe in Section 5 Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplification of the Non-Stationary PI with growing period algorithm of Scherrer and Lesner (2012). We will analyze this algorithm and prove in particular that it enjoys the best of both worlds: a guarantee similar to that of CPI and a fast rate like that of DPI. The next section begins by providing the background and the precise setting considered."}, {"heading": "2 Background", "text": "We consider an infinite-horizon discounted Markov Decision Process Puterman (1994); Bertsekas and Tsitsiklis (1996) (S,A, P, r, \u03b3), where S is a possibly infinite state space, A is a finite action space, P (ds\u2032|s, a), for all (s, a), is a probability kernel on S, r : S \u2192 [\u2212Rmax, Rmax] is a reward function bounded by Rmax, and \u03b3 \u2208 (0, 1) is a discount factor. A stationary deterministic policy \u03c0 : S \u2192 A maps states to actions. We write P\u03c0(ds\n\u2032|s) = P (ds\u2032|s, \u03c0(s)) for the stochastic kernel associated to policy \u03c0. The value v\u03c0 of a policy \u03c0 is a function mapping states to the expected discounted sum of rewards received when following \u03c0 from any state: for all s \u2208 S,\nv\u03c0(s) = E [ \u221e\u2211 t=0 \u03b3tr(st) \u2223\u2223\u2223\u2223\u2223s0 = s, st+1 \u223c P\u03c0(\u00b7|st) ] .\nThe value v\u03c0 is clearly bounded by Vmax = Rmax/(1\u2212 \u03b3). It is well-known that v\u03c0 can be characterized as the unique fixed point of the linear Bellman operator associated to a policy \u03c0: T\u03c0 : v 7\u2192 r + \u03b3P\u03c0v. Similarly, the Bellman optimality operator T : v 7\u2192 max\u03c0 T\u03c0v has as unique fixed point the optimal value v\u2217 = max\u03c0 v\u03c0. A policy \u03c0 is greedy w.r.t. a value function v if T\u03c0v = Tv, the set of such greedy policies is written Gv. Finally, a policy \u03c0\u2217 is optimal, with value v\u03c0\u2217 = v\u2217, iff \u03c0\u2217 \u2208 Gv\u2217, or equivalently T\u03c0\u2217v\u2217 = v\u2217.\nIn this paper, we focus on algorithms that use an approximate greedy operator, G , that takes as input a distribution \u03bd and a function v : S \u2192 R and returns a policy \u03c0 that is ( , \u03bd)-approximately greedy with respect to v in the sense that:\n\u03bd(Tv \u2212 T\u03c0v) = \u03bd(max \u03c0\u2032 T\u03c0\u2032v \u2212 T\u03c0v) \u2264 .\nIn practice, this can be achieved through a L1-regression of the so-called advantage function Kakade and Langford (2002); Kakade (2003) or through a sensitive classification problem Lazaric et al. (2010), in both case generating the learning problems through rollout trajectories induced by policy \u03c0. For all\nconsidered algorithms, we will provide bounds on the expected loss Es\u223c\u00b5[v\u03c0\u2217(s)\u2212 v\u03c0(s)] = \u00b5(v\u03c0\u2217 \u2212 v\u03c0) of using some generated policy \u03c0 instead of the optimal policy \u03c0\u2217 for some distribution \u00b5 of interest as a function of the errors k at each iteration."}, {"heading": "3 Direct Policy Iteration (DPI)", "text": "We begin by describing Direct Policy Iteration (DPI) introduced by Lagoudakis and Parr (2003); Fern et al. (2006) and analyzed by Lazaric et al. (2010). The analysis of this algorithm relies on the following\nAlgorithm 1 DPI\ninput: an initial policy \u03c00, a distribution \u03bd for k = 0, 1, 2, . . . do \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c0k) end for return: v\u03c0k\ncoefficients relating \u00b5 (the distribution of interest for measuring the loss) and \u03bd (the parameter of the algorithm):\nDefinition 1. Let c(1), c(2), . . . be the smallest coefficients in [1,\u221e) \u222a {\u221e} such that for all i and all sets of policies \u03c01, \u03c02, . . . , \u03c0i, \u00b5P\u03c01P\u03c02 . . . P\u03c0i \u2264 c(i)\u03bd, and let C(1) and C(2) be the following coefficients in [1,\u221e) \u222a {\u221e}\nC(1) = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic(i), C(2) = (1\u2212 \u03b3)2 \u221e\u2211 i=0 \u221e\u2211 j=i \u03b3jc(j) = (1\u2212 \u03b3)2 \u221e\u2211 i=0 (i+ 1)\u03b3ic(i).\nRemark 1. A bound involving C(1) is in general better than one involving C(2), in the sense that we always have (i) C(1) \u2264 11\u2212\u03b3C\n(2) = O(C(2)) while (ii) we may have C(1) <\u221e and C(2) =\u221e. (i) holds because, using the fact that c(i) \u2265 1, we have\nC(1) = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic(i) \u2264 (1\u2212 \u03b3) \u221e\u2211 i=0 \u221e\u2211 j=0 \u03b3i+jc(i) = 1 1\u2212 \u03b3 C(2).\nNow (ii) can be obtained in any situation where c(i) = \u0398( 1i2\u03b3i ), since the generic term of C (1) is \u03b3ic(i) = \u0398( 1i2 ) (the infinite sum converges) while that of C (2) is (i + 1)\u03b3ic(i) = \u0398( 1i ) (the infinite sum diverges to infinity).\nWith these coefficients in hand, we can prove the following performance bounds.\nTheorem 1. At each iteration k of DPI (Algorithm 1), the expected loss satisfies:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 C(2)\n(1\u2212 \u03b3)2 max 1\u2264i\u2264k\ni + \u03b3 kVmax and \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264\nC(1)\n1\u2212 \u03b3 k\u2211 i=1 i + \u03b3 kVmax.\nProof. A proof of the first bound can be found in Lazaric et al. (2010). For completeness, we provide one in Appendix A, along with the proof of the (original) second bound, all the more that they share a significant part of the arguments.\nThough the second bound involves the sum of the errors instead of the max value, as noted in Remark 1 its coefficient C(1) is better than C(2). As stated in the following corollary, the above theorem implies that the asymptotic performance bound is approched after a small number (or order O(log 1 )) of iterations.\nCorollary 1. Write = max1\u2264i\u2264k i.\nIf k \u2265 log Vmax\n1\u2212 \u03b3 , then \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264\n( C(2)\n(1\u2212 \u03b3)2 + 1\n) .\nIf k =\n\u2308 log Vmax\n1\u2212 \u03b3\n\u2309 , then \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 ( kC(1)\n1\u2212 \u03b3 + 1\n) \u2264 (( log Vmax + 1 ) C(1)\n(1\u2212 \u03b3)2 + 1\n) ."}, {"heading": "4 Conservative Policy Iteration (CPI)", "text": "We now turn to the description of Conservative Policy Iteration (CPI) proposed by Kakade and Langford (2002). At iteration k, CPI (described in Algorithm 2) uses the distribution d\u03c0k,\u03bd = (1 \u2212 \u03b3)\u03bd(I \u2212 \u03b3P\u03c0k)\n\u22121\u2014the discounted occupancy measure induced by \u03c0k when starting from \u00b5\u2014for calling the approximate greedy operator and for deciding whether to stop or not. Furthermore, it uses an adaptive stepsize \u03b1 to generate a stochastic mixture of all the policies that are returned by the successive calls to the approximate greedy operator, which explains the adjective \u201cconservative\u201d.\nAlgorithm 2 CPI\ninput: a distribution \u03bd, an initial policy \u03c00, \u03c1 > 0 for k = 0, 1, . . . do \u03c0\u2032k+1 \u2190 G k+1(d\u03c0k,\u03bd , v\u03c0k) Compute a \u03c13 -accurate estimate A\u0302k+1 of Ak+1 = d\u03c0k,\u03bd(T\u03c0\u2032k+1v\u03c0k \u2212 v\u03c0k) if A\u0302k+1 \u2264 2\u03c13 then\nreturn: \u03c0k end if \u03b1k+1 \u2190 (1\u2212\u03b3)(A\u0302k+1\u2212 \u03c13 )\n4\u03b3Vmax\n\u03c0k+1 \u2190 (1\u2212 \u03b1k+1)\u03c0k + \u03b1k+1\u03c0\u2032k+1 end for\nThe analysis here relies on the following coefficient:\nDefinition 2. Let C\u03c0\u2217 be the smallest coefficient in [1,\u221e) \u222a {\u221e} such that d\u03c0\u2217,\u00b5 \u2264 C\u03c0\u2217\u03bd.\nRemark 2. Our motivation for revisiting CPI is related to the fact that the constant C\u03c0\u2217 that will appear soon in its analysis is better than C(1) (and thus also, by Remark 1, better that C(2)) of algorithms like DPI1 in the sense that (i) we always have C\u03c0\u2217 \u2264 C(1) and (ii) we may have C\u03c0\u2217 < \u221e and C(1) = \u221e; moreover, if for any MDP and distribution \u00b5, there always exists a parameter \u03bd such that C\u03c0\u2217 < \u221e, there might not exist a \u03bd such that C(2) <\u221e. (i) holds because\nd\u03c0\u2217,\u00b5 = (1\u2212 \u03b3)\u00b5(I \u2212 \u03b3P\u03c0\u2217)\u22121 = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3i\u00b5(P\u03c0\u2217) i \u2264 (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic(i)\u03bd = C(1)\u03bd\nand C\u03c0\u2217 is the smallest coefficient satisfying the above inequality. Now consider (ii). The positive part of the claim (\u201calways exists\u201d) is trivial: it is sufficient to take \u03bd = d\u03c0\u2217,\u00b5 and we have C\u03c0\u2217 = 1. The negative part (\u201cthere might not exist\u201d) can be shown by considering an MDP defined on N, \u00b5 equal to the dirac measure \u03b4({0}) on state 0 and an infinite number of actions a \u2208 N that result in a deterministic transition from 0 to a. As in Definition 1, let c(1) \u2208 [1,\u221e) \u222a {\u221e} be such that for all \u03c0, \u00b5P\u03c0 \u2264 c(1)\u03bd. Then for all actions a, we have \u03b4({a}) \u2264 c(1)\u03bd. As a consequence, 1 = \u2211 i\u2208N \u03bd(i) \u2265 1 c(1) \u2211 i\u2208N 1 and thus necessarily c(1) =\u221e. As a consequence C(2) =\u221e.\nThe constant C\u03c0\u2217 will be small when the parameter distribution \u03bd is chosen so that it fits as much as possible d\u03c0\u2217,\u00b5 that is the discounted expected long-term occupancy of the optimal policy \u03c0\u2217 starting from\n1Though we do not develop this here, it can be seen that concentrability coefficients that have been introduced for other approximate Dynamic Programming algorithms like Value Iteration (see Munos and Szepesva\u0301ri (2008); Farahmand et al. (2010)) or Modified Policy Iteration (see Scherrer et al. (2012)) are equal to C(2).\n\u00b5. A good domain expert should be able to provide such an estimate, and thus the condition C\u03c0\u2217 <\u221e is rather mild. We have the following performance bound2.\nTheorem 2. CPI (Algorithm 2) has the following properties: (i) The expected values \u03bdv\u03c0k = E[v\u03c0k(s)|s \u223c \u03bd] of policies \u03c0k starting from distribution \u03bd are monotonically increasing: \u03bdvk+1 > \u03bdvk + \u03c12\n72\u03b3Vmax .\n(ii) The (random) iteration k\u2217 at which the algorithm stops is such that k\u2217 \u2264 72\u03b3Vmax 2\n\u03c12 .\n(iii) The policy \u03c0k\u2217 that is returned satisfies\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k\u2217 ) \u2264 C\u03c0\u2217\n(1\u2212 \u03b3)2 ( k\u2217+1 + \u03c1).\nProof. The proof follows the lines of that Kakade and Langford (2002) and is provided in Appendix B for completeness.\nWe have the following immediate corollary that shows that CPI obtains a performance bounds similar to those of DPI (in Corollary 1)\u2014at the exception that it involves a different (better) concentrability constant\u2014after O( 1 2 ) iterations.\nCorollary 2. If CPI is run with parameter \u03c1 = = max1\u2264i\u2264k i, then CPI stops after at most 72\u03b3Vmax\n2\n2\niterations and returns a policy \u03c0k\u2217 that satisfies:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k\u2217 ) \u2264 2C\u03c0\u2217\n(1\u2212 \u03b3)2 .\nWe also provide a complementary original analysis of this algorithm that further highlights its connection with DPI.\nTheorem 3. At each iteration k < k\u2217 of CPI (Algorithm 2), the expected loss satisfies:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 C(1) (1\u2212 \u03b3)2 k\u2211 i=1 \u03b1i i + e {(1\u2212\u03b3)\u2211ki=1 \u03b1i}Vmax.\nProof. The proof is a natural but tedious extension of the analysis of DPI to the situation where conservative steps are made, and is deferred to Appendix C.\nSince in the proof of Theorem 2 (in Appendix B), one shows that the learning steps of CPI satisfy \u03b1k \u2265 (1\u2212\u03b3)\u03c112\u03b3Vmax , the right term e {(1\u2212\u03b3)\u2211ki=1 \u03b1i} above tends 0 exponentially fast, and we get the following corollary that shows that CPI has a performance bound with the coefficient C(1) of DPI in a number of iterations O( log 1 ).\nCorollary 3. Assume CPI is run with parameter \u03c1 = = max1\u2264i\u2264k i. The smallest (random) iteration k\u2020 such that log Vmax 1\u2212\u03b3 \u2264 \u2211k\u2020 i=1 \u03b1i \u2264 log Vmax 1\u2212\u03b3 + 1 is such that k \u2020 \u2264 12\u03b3Vmax log Vmax (1\u2212\u03b3)2 and the policy \u03c0k\u2020 satisfies:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k\u2020 ) \u2264\nC(1) (\u2211k\u2020 i=1 \u03b1i ) (1\u2212 \u03b3)2 + 1  \u2264 (C(1) (log Vmax + 1) (1\u2212 \u03b3)3 + 1 ) .\nIn practice, the choice for the learning step \u03b1k in Algorithm 2 is very conservative, which makes CPI (as it is) a very slow algorithm. Natural solutions to this problem, that have for instance been considered in a variation of CPI for search-based structure prediction problems (III et al., 2009; Daume\u0301 III et al., 2006), is to either use a line-search (to optimize the learning step \u03b1k \u2208 (0, 1)) or even to use a fixed value\n2Note that there are two small differences between the algorithm and analysis described by Kakade and Langford (2002) and the ones we give here: 1) the stepsize \u03b1 is a factor 1\n\u03b3 bigger in our description, and thus the number of iterations is\nslightly better (smaller by a factor \u03b3); 2) our result is stated in terms of the error k that may not be known in advance and the input parameter \u03c1 while Kakade and Langford (2002) assume a uniform bound on the errors ( k) is known and equal to the parameter \u03c1.\n\u03b1 (e.g. \u03b1 = 0.1) for all iterations. This latter solution, that one may call CPI(\u03b1), works indeed well in practice, and is significantly simpler to implement since one is relieved of the necessity to estimate A\u0302k+1 through rollouts (see Kakade and Langford (2002) for the description of this process); indeed it becomes almost as simple as DPI except that one uses the distribution d\u03c0k,\u03bd and conservative steps. Since the proof is based on a generalization of the analysis of DPI and thus does not use any of the specific properties of CPI, it turns out that the results we have just given (Corollary 3) can straightforwardly be specialized to the case of this algorithm.\nCorollary 4. Assume we run CPI(\u03b1) for some \u03b1 \u2208 (0, 1), that is CPI (Algorithm 2) with \u03b1k = \u03b1 for all k. Write = max1\u2264i\u2264k i.\nIf k = \u2308 log Vmax \u03b1(1\u2212 \u03b3) \u2309 , then \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 \u03b1(k + 1)C(1) (1\u2212 \u03b3)2 \u2264 ( C(1) ( log Vmax + 1 ) (1\u2212 \u03b3)3 + 1 ) .\nWe see here that the parameter \u03b1 directly controls the rate of CPI(\u03b1)3. Furthermore, if one sets \u03b1 to a sufficiently small value, one should recover the nice properties of Theorem 2-Corollary 2: monotonicity of the expected value and a performance bound with respect to the best constant C\u03c0\u2217 , though we do not formalize this here.\nIn summary, Corollary 2 and Remark 2 tell us that CPI has a performance guarantee that can be arbitrarily better than that of DPI, though the opposite is not true. This, however, comes at the cost of a significant exponential increase of time complexity since Corollary 2 states that there might be a number of iterations that scales in O( 1 2 ), while the guarantee of DPI (Corollary 1) only requires O ( log 1\n) iterations. When the analysis of CPI is relaxed so that the performance guarantee is expressed in terms of the (worse) coefficient C(1) of DPI (Corollary 3), we were able to slightly improve the rate\u2014by a factor O\u0303( )\u2014, though it is still exponentially slower than that of DPI. The algorithm that we present in the next section will be the best of both worlds: it will enjoy a performance guarantee involving the best constant C\u03c0\u2217 , but with a time complexity similar to that of DPI."}, {"heading": "5 Non-stationary Direct Policy Iteration (NSDPI)", "text": "We are now going to describe an algorithm that has a flavour similar to DPI \u2013 in the sense that at each step it does a full step towards a new policy \u2013 but also has a conservative flavour like CPI \u2013 in the sense that the policies will evolve more and more slowly. This algorithm is based on finite-horizon non-stationary policies. We will write \u03c3 = \u03c01\u03c02 . . . \u03c0k the k-horizon policy that makes the first action according to \u03c01, then the second action according to \u03c02, etc. Its value is v\u03c3 = T\u03c01T\u03c02 . . . T\u03c0kr. We will write \u03c3 = \u2205 the \u201cempty\u201d non-stationary policy. Note that v\u2205 = r and that any infinite-horizon policy that begins with \u03c3 = \u03c01\u03c02 . . . \u03c0k, which we will denote \u201c\u03c3 . . . \u201d has a value v\u03c3... \u2265 v\u03c3 \u2212 \u03b3kVmax.\nThe last algorithm we consider, named here Non-Stationary Direct Policy Iteration (NSDPI) because it behaves as DPI but builds a non-stationary policy by iteratively concatenating the policies that are returned by the approximate greedy operator, is described in Algorithm 3.\nAlgorithm 3 NSDPI\ninput: a distribution \u03bd initialization: \u03c30 = \u2205 for k = 0, 1, 2, . . . do \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c3k) \u03c3k+1 \u2190 \u03c0k+1\u03c3k end for\nWe are going to state a performance bound for this algorithm with respect to the constant C\u03c0\u2217 , but also an alternative bound based on the following new concentrability coefficients.\n3The performance bound of CPI(1) (with \u03b1 = 1) does not match the bound of DPI (Corollary 1), but is a factor 1 1\u2212\u03b3 worse. This amplification is due to the fact that the approximate greedy operator uses the distribution d\u03c0k,\u03bd \u2265 (1 \u2212 \u03b3)\u03bd instead of \u03bd (for DPI).\nDefinition 3. Let c\u03c0\u2217(1), c\u03c0\u2217(2), . . . be the smallest coefficients in [1,\u221e) \u222a {\u221e} such that for all i, \u00b5(P\u03c0\u2217) i \u2264 c\u03c0\u2217(i)\u03bd. and let C (1) \u03c0\u2217 be the following coefficient in [1,\u221e) \u222a {\u221e}:\nC(1)\u03c0\u2217 = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic\u03c0(i).\nRemark 3. A bound involving C\u03c0\u2217 is in general better than one involving C (1) \u03c0\u2217 in the sense that (i) we always have C\u03c0\u2217 \u2264 C (1) \u03c0\u2217 while (ii) we may have C\u03c0\u2217 < \u221e and C (1) \u03c0\u2217 = \u221e. Similarly, a bound involving C (1) \u03c0\u2217 is in general better than one involving C (1) since (iii) we have C (1) \u03c0\u2217 \u2264 C(1) while (iv) we may have C (1) \u03c0\u2217 <\u221e and C(1) =\u221e. (i) holds because (very similarly to Remark 2-(i))\nd\u03c0\u2217,\u00b5 = (1\u2212 \u03b3)\u00b5(I \u2212 \u03b3P\u03c0\u2217)\u22121 = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3i\u00b5(P\u03c0\u2217) i \u2264 (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic\u03c0\u2217(i)\u03bd = C (1) \u03c0\u2217 \u03bd\nand C\u03c0\u2217 is the smallest coefficient satisfying the above inequality. (ii) can easily be obtained by designing a problem with c\u03c0\u2217(1) =\u221e as in Remark 2-(ii). (iii) is a consequence of the fact that for all i, c\u03c0\u2217(i) \u2264 c(i). Finally, (iv) is trivially obtained by considering two different policies.\nWith this notations in hand, we are ready to state that NSDPI (Algorithm 3) enjoys two guarantees that have a fast rate like those of DPI (Theorem 1), one expressed in terms of the concentrability C\u03c0\u2217 that was introduced for CPI (in Definition 2), and the other in terms of the constant C (1) \u03c0\u2217 we have just introduced.\nTheorem 4. At each iteration k of NSDPI (Algorithm 3), the expected loss of running an infinitely-long policy that begins by \u03c3k satisfies\n\u00b5(v\u03c0\u2217 \u2212 v\u03c3k...) \u2264 C\n(1) \u03c0\u2217\n1\u2212 \u03b3 max 1\u2264i\u2264k\ni + 2\u03b3 kVmax and \u00b5(v\u03c0\u2217 \u2212 v\u03c3k...) \u2264 C\u03c0\u2217 1\u2212 \u03b3 k\u2211 i=1 i + 2\u03b3 kVmax.\nProof. The proof of Theorem 4 is rather simple (much simpler than the previous ones), and is deferred to Appendix D.\nAs shown in the following immediate corollary of Theorem 4, these relations constitute guarantees that small errors i in the greedy operator quickly induce good policies.\nCorollary 5. Write = max1\u2264i\u2264k i.\nIf k \u2265 log 2Vmax\n1\u2212 \u03b3 then \u00b5(v\u03c0\u2217 \u2212 v\u03c3k) \u2264\n( C (1) \u03c0\u2217\n1\u2212 \u03b3 + 1\n) .\nIf k =\n\u2308 log 2Vmax\n1\u2212 \u03b3\n\u2309 then \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 ( kC\u03c0\u2217 1\u2212 \u03b3 + 1 ) \u2264 (( log 2Vmax + 1 ) C\u03c0\u2217 (1\u2212 \u03b3)2 + 1 ) .\nThe first bound has a better dependency with respect to 11\u2212\u03b3 , but (as explained in Remark 3) is expressed in terms of the worse coefficient C (1) \u03c0\u2217 . The second guarantee is almost as good as that of CPI (Corollary 2) since it only contains an extra log 1 term, but it has the nice property that it holds quickly: in time log 1 instead of O( 1 2 ), that is exponentially faster.\nWe devised NSDPI as a DPI-like simplified variation of one of the non-stationary dynamic programming algorithm recently introduced by Scherrer and Lesner (2012), the Non-Stationary PI algorithm with growing period. The main difference with NSDPI is that one considers there the value v(\u03c3k)\u221e of the policy that loops infinitely on \u03c3k instead of the value v\u03c3k of the only first k steps here. Following the intuition that when k is big, these two values will be close to each other, we ended up focusing on NSDPI because it is simpler. Remarkably, NSDPI turns out to be almost identical to an older algorithm, the Policy Search by Dynamic Programming (PSDP) algorithm Bagnell et al. (2003). It should\nhowever be noted that PSDP was introduced for a slightly different control problem where the horizon is finite, while we are considering here the infinite-horizon problem. Given a problem with horizon T , PSDP comes with a guarantee that is essentially identical to the first bound in Corollary 5, but requiring as many input distributions as there are time steps4. Our main contribution with respect to PSDP is that by considering the infinite-horizon case, we managed to require only one input parameter (the distribution \u03bd that should match as much as possible the measure d\u03c0\u2217,\u00b5, recall Definition 1)\u2014a much milder assumption\u2014and provide the second performance bound with respect to C\u03c0\u2217 , that is better (cf. Remark 3)."}, {"heading": "6 Discussion, Conclusion and Future Work", "text": "In this article, we have described two algorithms of the literature, DPI and CPI, and introduced the NSDPI algorithm that borrows ideas from both DPI and CPI, while also having some very close algorithmic connections with PSDP Bagnell et al. (2003) and the Non-Stationary PI algorithm with growing period of Scherrer and Lesner (2012). Figure 1 synthesizes the theoretical guarantees we have discussed about these algorithms. For each such guarantee, we provide the dependency of the performance bound and the number of iterations with respect to , 11\u2212\u03b3 and the concentrability coefficients (for CPI, we assume as Kakade and Langford (2002) that \u03c1 = ). We highlight in red the bounds that are to our knowledge new5.\nOne of the most important message of our work is that what is usually hidden in the constants of the performance bounds does matter. The constants we discussed can be sorted from the worst to the best (cf. Remarks 1, 2 and 3) as follows: C(2), C(1), C (1) \u03c0\u2217 , C\u03c0\u2217 . To our knowledge, this is the first time that such an in-depth comparison of the bounds is done, and our hierarchy of constants has interesting implications that go beyond to the policy search algorithms we have been focusing on in this paper. As a matter of fact, several dynamic programming algorithms\u2014AVI (Munos, 2007), API (Munos, 2003), \u03bbPI Scherrer (2013), AMPI (Scherrer et al., 2012)\u2014come with guarantees involving the worst constant C(2), that can easily be made infinite. On the positive side, we have argued that the guarantee for CPI can be arbitrarily stronger than the other state-of-the-art algorithms. We have identified the NSDPI algorithm as having a similar nice property. Furthermore, we can observe that DPI and NSDPI both have the best time complexity guarantee. Thus, NSDPI turns out to have the overall best guarantees.\nAt the technical level, several of our bounds come in pair, and this is due to the fact that we have introduced a new proof technique in order to derive new bounds with various constants. This led to a new bound for DPI, that is better since it involves the C(1) constant instead of C(2). It also enabled us to derive new bounds for CPI (and its natural algorithmic variant CPI(\u03b1)) that is worse in terms of\n4 It is assumed by Bagnell et al. (2003) that a set of baseline distributions \u03bd1, \u03bd2, . . . , \u03bdT provide estimates of where the optimal policy is leading the system from some distribution \u00b5 at all time steps 1, 2, . . . , T ; then, the authors measure the mismatch through coefficients c\u03c0\u2217 (i) such that \u00b5P i \u03c0\u2217 \u2264 c\u03c0\u2217 (i)\u03bdi. This kind of assumption is essentially identical to the concentrability assumption underlying the constant C (1) \u03c0\u2217 in Definition 3, the only difference being that we only refer to one measure \u03bd. 5We do not highlight the second bound of NSDPI, acknowledging that it already appears in a very close form in Bagnell et al. (2003) for PSDP.\nguarantee but has a better time complexity (O\u0303( 1 ) instead of O( 1 )). We believe this new technique may be helpful in the future for the analysis of other algorithms. The main limitation of our analysis lies in the assumption, considered all along the paper, that all algorithms have at disposal an -approximate greedy operator. This is in general not realistic, since it may be hard to control directly the quality level . Furthermore, it may be unreasonable to compare all algorithms on this basis, since the underlying optimization problems may have slightly different complexities: for instance, methods like CPI look in a space of stochastic policies while DPI moves in a space of deterministic policies. Digging and understanding in more depth what is potentially hidden in the term \u2014as we have done here for the concentrability constants\u2014constitutes a very natural research direction.\nLast but not least, on the practical side, we have run preliminary numerical experiments that somewhat support our theoretical argument that algorithms with a better concentrability constant should be preferred. On simulations on relatively small problems, CPI+ (CPI with a crude line-search mechanism), CPI(\u03b1) and NSDPI were shown to always perform significantly better than DPI, NSDPI always displayed the least variability, and CPI(\u03b1) performed the best on average. We refer the reader to Appendix E for further details. Running and analyzing similar experiments on bigger domains constitutes interesting future work."}, {"heading": "A Proof of Theorem 1", "text": "Our proof is here even slightly more general than what is required: we provide the result for any reference policy \u03c0 (and not only for the optimal policy \u03c0\u2217). Writing ek+1 = max\u03c0\u2032 T\u03c0\u2032v\u03c0k \u2212 T\u03c0k+1v\u03c0k , we can first see that:\nv\u03c0 \u2212 v\u03c0k+1 = T\u03c0v\u03c0 \u2212 T\u03c0v\u03c0k + T\u03c0v\u03c0k \u2212 T\u03c0k+1v\u03c0k + T\u03c0k+1v\u03c0k \u2212 T\u03c0k+1v\u03c0k+1 \u2264 \u03b3P\u03c0(v\u03c0 \u2212 v\u03c0k) + ek+1 + \u03b3P\u03c0k+1(v\u03c0k \u2212 v\u03c0k+1). (2)\nUsing the fact that v\u03c0k+1 = (I \u2212 \u03b3P\u03c0k+1)\u22121r, one can notice that:\nv\u03c0k \u2212 v\u03c0k+1 = (I \u2212 \u03b3P\u03c0k+1)\u22121(v\u03c0k \u2212 \u03b3P\u03c0k+1v\u03c0k \u2212 r) = (I \u2212 \u03b3P\u03c0k+1)\u22121(T\u03c0kv\u03c0k \u2212 T\u03c0k+1v\u03c0k) \u2264 (I \u2212 \u03b3P\u03c0k+1)\u22121ek+1.\nPutting this back in Equation (A) we get:\nv\u03c0 \u2212 v\u03c0k+1 = \u03b3P\u03c0(v\u03c0 \u2212 v\u03c0k) + (I \u2212 \u03b3P\u03c0k+1)\u22121ek+1.\nBy induction on k we obtain:\nv\u03c0 \u2212 v\u03c0k = k\u22121\u2211 i=0 (\u03b3P\u03c0) i(I \u2212 \u03b3P\u03c0k\u2212i)\u22121ek\u2212i + (\u03b3P\u03c0)k(v\u03c0 \u2212 v\u03c00).\nMultiplying both sides by \u00b5 (and observing that ek \u2265 0) and using Definition 1 and the fact that \u03bdej \u2264 j , we obtain:\n\u00b5(v\u03c0 \u2212 v\u03c0k) \u2264 k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0) i(I \u2212 \u03b3P\u03c0k\u2212i)\u22121ek\u2212i + \u03b3kVmax\n\u2264 k\u22121\u2211 i=0  \u221e\u2211 j=0 \u03b3i+jc(i+ j) k\u2212i + \u03b3kVmax =\nk\u22121\u2211 i=0 \u221e\u2211 j=i \u03b3jc(j) k\u2212i + \u03b3 kVmax (3)\n\u2264 k\u22121\u2211 i=0 \u221e\u2211 j=i \u03b3jc(j) max 1\u2264l\u2264k l + \u03b3 kVmax,\nwhich provides the first bound. Starting back on Equation (A), we get\n\u00b5(v\u03c0 \u2212 v\u03c0k) \u2264 k\u22121\u2211 i=0 \u221e\u2211 j=i \u03b3jc(j) k\u2212i + \u03b3 kVmax\n\u2264 k\u22121\u2211 i=0 \u221e\u2211 j=0 \u03b3jc(j) k\u2212i + \u03b3 kVmax\n= \u221e\u2211 j=0 \u03b3jc(j) k\u2211 i=1 i + \u03b3 kVmax,\nwhich proves the second bound."}, {"heading": "B Proof of Theorem 2", "text": "We include a concise and self-contained proof that essentially follows the steps in (Kakade and Langford, 2002).\n(i) We first show that the value \u03b7k = \u03bdv\u03c0k is increasing along the iterations. Consider some iteration k of the algorithm. Using the facts that T\u03c0k+1v\u03c0k = (1 \u2212 \u03b1k+1)v\u03c0k + \u03b1k+1T\u03c0\u2032k+1v\u03c0k and P\u03c0k+1 = (1 \u2212 \u03b1k+1)P\u03c0k + \u03b1k+1P\u03c0\u2032k+1 , we can see that:\n\u03b7k+1 \u2212 \u03b7k = \u03bd(v\u03c0k+1 \u2212 v\u03c0k) = \u03bd[(I \u2212 \u03b3P\u03c0k+1)\u22121r \u2212 v\u03c0k ] = \u03bd(I \u2212 \u03b3P\u03c0k)\u22121(I \u2212 \u03b3P\u03c0k)(I \u2212 \u03b3P\u03c0k+1)\u22121[r \u2212 (I \u2212 \u03b3P\u03c0k+1)v\u03c0k ] = \u03bd(I \u2212 \u03b3P\u03c0k)\u22121(I \u2212 \u03b3P\u03c0k+1 + \u03b3P\u03c0k+1 \u2212 \u03b3P\u03c0k)(I \u2212 \u03b3P\u03c0k+1)\u22121(T\u03c0k+1v\u03c0k \u2212 v\u03c0k)\n= 1\n1\u2212 \u03b3 d\u03c0k,\u03bd\n[ I + \u03b3\u03b1k+1(P\u03c0\u2032k+1 \u2212 P\u03c0k)(I \u2212 \u03b3P\u03c0k+1) \u22121 ] \u03b1k+1(T\u03c0\u2032k+1v\u03c0k \u2212 v\u03c0k)\n= \u03b1k+1 1\n1\u2212 \u03b3 d\u03c0k,\u03bd(T\u03c0\u2032k+1v\u03c0k \u2212 v\u03c0k) + \u03b1 2 k+1\n\u03b3\n1\u2212 \u03b3 d\u03c0k,\u03bd(P\u03c0\u2032k+1 \u2212 P\u03c0k)(I \u2212 \u03b3P\u03c0k+1) \u22121(T\u03c0\u2032k+1v\u03c0k \u2212 v\u03c0k)\n\u2265 \u03b1k+1 1\n1\u2212 \u03b3 (A\u0302k+1 \u2212\n\u03c1 3 )\u2212 2\u03b12k+1\n\u03b3\n(1\u2212 \u03b3)2 Vmax\nwhere we eventually used the fact that T\u03c0\u2032k+1v\u03c0k \u2212 v\u03c0k \u2208 (\u2212Vmax, Vmax). Now, it can be seen that the setting \u03b1k+1 = (1\u2212\u03b3)(A\u0302k+1\u2212 \u03c13 )\n4\u03b3Vmax of Algorithm 2 is the one that maximizes the above right hand side. With\nthis setting we get:\n\u03b7k+1 \u2212 \u03b7k \u2265 (A\u0302k+1 \u2212 \u03c13 ) 2\n8\u03b3Vmax\n> \u03c12\n72\u03b3Vmax\nsince as long as the algorithm iterates, A\u0302k+1 > 2\u03c13 . (ii) The second point is a very simple consequence of (i): since \u03b7k = \u03bdv\u03c0k \u2264 Vmax, the number of iterations of the algorithm cannot exceed 72\u03b3Vmax 2\n\u03c12 .\n(iii) We now prove the performance bound. Write e = max\u03c0\u2032 T\u03c0\u2032v\u03c0k\u2217 \u2212 v\u03c0k\u2217 . We have:\nv\u03c0\u2217 \u2212 v\u03c0k\u2217 = T\u03c0\u2217v\u03c0\u2217 \u2212 T\u03c0\u2217v\u03c0k\u2217 + T\u03c0\u2217v\u03c0k\u2217 \u2212 v\u03c0k\u2217 \u2264 \u03b3P\u03c0\u2217(v\u03c0\u2217 \u2212 v\u03c0k\u2217 ) + e \u2264 (I \u2212 \u03b3P\u03c0\u2217)\u22121e.\nMultiplying by \u00b5, using Definition 2 and the facts that e = max\u03c0\u2032 T\u03c0\u2032v\u03c0k\u2217 \u2212 T\u03c0k\u2217 v\u03c0k\u2217 \u2265 0 and d\u03c0k\u2217 ,\u03bd \u2265 (1\u2212 \u03b3)\u03bd, we obtain\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k\u2217 ) \u2264 1\n1\u2212 \u03b3 d\u03c0\u2217,\u00b5e\n\u2264 C\u03c0\u2217 1\u2212 \u03b3 \u03bde\n\u2264 C\u03c0\u2217 (1\u2212 \u03b3)2 d\u03c0k\u2217 ,\u03bde\n= C\u03c0\u2217\n(1\u2212 \u03b3)2 d\u03c0k\u2217 ,\u03bd(max \u03c0\u2032 T\u03c0\u2032v\u03c0k\u2217 \u2212 T\u03c0\u2032k\u2217+1v\u03c0k\u2217 + T\u03c0\u2032k\u2217+1v\u03c0k\u2217 \u2212 v\u03c0k\u2217 )\n\u2264 C\u03c0\u2217 (1\u2212 \u03b3)2 ( k\u2217 +Ak+1).\nThe result follows by observing that the advantage satisfies Ak+1 \u2264 A\u0302k+1 + \u03c13 \u2264 \u03c1 since A\u0302k+1 \u2264 2\u03c1 3 when the algorithms stops."}, {"heading": "C Proof of Theorem 3", "text": "Using the facts that T\u03c0k+1v\u03c0k = (1\u2212 \u03b1k+1)v\u03c0k + \u03b1k+1T\u03c0k+1v\u03c0k and the notation ek+1 = max\u03c0\u2032 T\u03c0\u2032v\u03c0k \u2212 T\u03c0\u2032k+1v\u03c0k , we have:\nv\u03c0 \u2212 v\u03c0k+1 = v\u03c0 \u2212 T\u03c0k+1v\u03c0k + T\u03c0k+1v\u03c0k \u2212 T\u03c0k+1v\u03c0k+1 = v\u03c0 \u2212 (1\u2212 \u03b1k+1)v\u03c0k \u2212 \u03b1k+1T\u03c0\u2032k+1v\u03c0k + \u03b3P\u03c0k+1(v\u03c0k \u2212 v\u03c0k+1)\n= (1\u2212 \u03b1k+1)(v\u03c0 \u2212 v\u03c0k) + \u03b1k+1(T\u03c0v\u03c0 \u2212 T\u03c0v\u03c0k) + \u03b1k+1(T\u03c0v\u03c0k \u2212 T\u03c0\u2032k+1v\u03c0k) + \u03b3P\u03c0k+1(v\u03c0k \u2212 v\u03c0k+1)\n\u2264 [(1\u2212 \u03b1k+1)I + \u03b1k+1\u03b3P\u03c0] (v\u03c0 \u2212 v\u03c0k) + \u03b1k+1ek+1 + \u03b3P\u03c0k+1(v\u03c0k \u2212 v\u03c0k+1). (4)\nUsing the fact that v\u03c0k+1 = (I \u2212 \u03b3P\u03c0k+1)\u22121r, we can see that\nv\u03c0k \u2212 v\u03c0k+1 = (I \u2212 \u03b3P\u03c0k+1)\u22121(v\u03c0k \u2212 \u03b3P\u03c0k+1v\u03c0k \u2212 r) = (I \u2212 \u03b3P\u03c0k+1)\u22121(T\u03c0kv\u03c0k \u2212 T\u03c0k+1v\u03c0k) \u2264 (I \u2212 \u03b3P\u03c0k+1)\u22121\u03b1k+1ek+1.\nPutting this back in Equation (C), we obtain:\nv\u03c0 \u2212 v\u03c0k+1 \u2264 [(1\u2212 \u03b1k+1)I + \u03b1k+1\u03b3P\u03c0] (v\u03c0 \u2212 v\u03c0k) + \u03b1k+1(I \u2212 \u03b3P\u03c0k+1)\u22121ek+1.\nDefine the matrix Qk = [(1\u2212 \u03b1k)I + \u03b1k\u03b3P\u03c0], the set Ni,k = {j; k \u2212 i + 1 \u2264 j \u2264 k} (this set contains exactly i elements), the matrix Ri,k = \u220f j\u2208Ni,k Qj , and the coefficients \u03b2k = 1 \u2212 \u03b1k(1 \u2212 \u03b3) and \u03b4k =\u220fk\ni=1 \u03b2k. We get by induction\nv\u03c0 \u2212 v\u03c0k \u2264 k\u22121\u2211 i=0 Ri,k\u03b1k\u2212i(I \u2212 \u03b3P\u03c0k\u2212i)\u22121ek\u2212i + \u03b4kVmax. (5)\nLet Pj(Ni,k) the set of subsets of Ni,k of size j. With this notation we have\nRi,k = i\u2211 j=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k(\u03b3P\u03c0) j\nwhere for all subset I of Ni,k, we wrote\n\u03b6I,i,k = (\u220f n\u2208I \u03b1n ) \u220f n\u2208Ni,k\\I (1\u2212 \u03b1n)  .\nTherefore, by multiplying Equation (C) by \u00b5, using Definition 1, and the facts that \u03bd \u2264 (1\u2212 \u03b3)dnu,\u03c0k+1 , we obtain:\n\u00b5(v\u03c0 \u2212 v\u03c0k) \u2264 1\n1\u2212 \u03b3 k\u22121\u2211 i=0 i\u2211 j=0 \u221e\u2211 l=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k\u03b3 j+lc(j + l)\u03b1k\u2212i k\u2212i + \u03b4kVmax.\n= 1\n1\u2212 \u03b3 k\u22121\u2211 i=0 i\u2211 j=0 \u221e\u2211 l=j \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k\u03b3 lc(l)\u03b1k\u2212i k\u2212i + \u03b4kVmax\n\u2264 1 1\u2212 \u03b3 k\u22121\u2211 i=0 i\u2211 j=0 \u221e\u2211 l=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k\u03b3 lc(l)\u03b1k\u2212i k\u2212i + \u03b4kVmax\n= 1\n1\u2212 \u03b3 ( \u221e\u2211 l=0 \u03b3lc(l) ) k\u22121\u2211 i=0  i\u2211 j=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k \u03b1k\u2212i k\u2212i + \u03b4kVmax = 1\n1\u2212 \u03b3 ( \u221e\u2211 l=0 \u03b3lc(l) ) k\u22121\u2211 i=0  \u220f j\u2208Ni,k (1\u2212 \u03b1j + \u03b1j) \u03b1k\u2212i k\u2212i + \u03b4kVmax = 1\n1\u2212 \u03b3 ( \u221e\u2211 l=0 \u03b3lc(l) )( k\u22121\u2211 i=0 \u03b1k\u2212i k\u2212i ) + \u03b4kVmax.\nNow, using the fact that for x \u2208 (0, 1), log(1\u2212 x) \u2264 \u2212x, we can observe that\nlog \u03b4k = log k\u220f i=1 \u03b2i\n= k\u2211 i=1 log \u03b2i\n= k\u2211 i=1 log(1\u2212 \u03b1i(1\u2212 \u03b3))\n\u2264 \u2212(1\u2212 \u03b3) k\u2211 i=1 \u03b1i\nAs a consequence, we get \u03b4k \u2264 e\u2212(1\u2212\u03b3) \u2211k i=1 \u03b1i ."}, {"heading": "D Proof of Theorem 4", "text": "We begin by the first relation. For all k, we have\nv\u03c0 \u2212 v\u03c3k = T\u03c0v\u03c0 \u2212 T\u03c0v\u03c3k\u22121 + T\u03c0v\u03c3k\u22121 \u2212 T\u03c0kv\u03c3k\u22121 \u2264 \u03b3P\u03c0(v\u03c0 \u2212 v\u03c3k\u22121) + ek\nwhere we defined ek = max\u03c0\u2032 T\u03c0\u2032v\u03c3k\u22121 \u2212 T\u03c0kv\u03c3k\u22121 . By induction, we deduce that\nv\u03c0 \u2212 v\u03c3k \u2264 k\u22121\u2211 i=0 (\u03b3P\u03c0) iek\u2212i + \u03b3 kVmax.\nBy multiplying both sides of by \u00b5, using Definition 3 and the fact that \u03bdjej \u2264 , we get:\n\u00b5(v\u03c0 \u2212 v\u03c3k) \u2264 k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0) iek\u2212i + \u03b3 kVmax (6)\n\u2264 k\u22121\u2211 i=0 \u03b3ic(i) k\u2212i + \u03b3 kVmax\n\u2264 ( k\u22121\u2211 i=0 \u03b3ic(i) ) max 1\u2264i\u2264k i + \u03b3 kVmax.\nWe now prove the second relation. Starting back in Equation (D) and using Definition 2 (in particular\nthe fact that for all i, \u00b5(\u03b3P\u03c0) i \u2264 11\u2212\u03b3 d\u03c0\u2217,\u00b5 \u2264 C\u03c0\u2217 1\u2212\u03b3 \u03bd) and the fact that \u03bdej \u2264 j , we get:\n\u00b5(v\u03c0 \u2212 v\u03c3k) \u2264 k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0) iek\u2212i + \u03b3 kVmax \u2264 C\u03c0\u2217 1\u2212 \u03b3 k\u2211 i=1 i + \u03b3 kVmax\nand the result is obtained by using the fact that v\u03c3k... \u2265 v\u03c3k \u2212 \u03b3kVmax."}, {"heading": "E Experiments", "text": "In this section, we present some experiments in order to illustrate the different algorithms discussed in the paper and to get some insight on their empirical behaviour. CPI as it is described in Algorithm 2 can be very slow (in one sample experiment on a 100 state problem, it made very slow progress and took several millions of iterations before it stopped) and we did not evaluate it further. Instead, we considered two variations: CPI+ that is identical to CPI except that it chooses the step \u03b1 at each iteration by doing a line-search towards the policy output by the classifier6, and CPI(\u03b1) with \u03b1 = 0.1, that makes \u201crelatively but not too small\u201d steps at each iteration. We begin by describing the domain and the approximation considered.\nDomain and Approximations In order to assess their quality, we consider finite problems where the exact value function can be computed. More precisely, we consider Garnet problems (Archibald et al., 1995), which are a class of randomly constructed finite MDPs. They do not correspond to any specific application, but are totally abstract while remaining representative of the kind of MDP that might be encountered in practice. In our experiments, a Garnet is parameterized by 4 parameters and is written G(nS , nA, b, p): nS is the number of states, nA is the number of actions, b is a branching factor specifying how many possible next states are possible for each state-action pair (b states are chosen uniformly at random and transition probabilities are set by sampling uniform random b\u22121 cut points between 0 and 1) and p is the number of features (for linear function approximation). The reward is state-dependent: for a given randomly generated Garnet problem, the reward for each state is uniformly sampled between 0 and 1. Features are chosen randomly: \u03a6 is a nS\u00d7p feature matrix of which each component is randomly and uniformly sampled between 0 and 1. The discount factor \u03b3 is set to 0.99 in all experiments.\nAll the algorithms we have discussed in the paper need to repeatedly compute G (\u03bd, v). In other words, they must be able to make calls to an approximate greedy operator applied to the value v of some policy for some distribution \u03bd. To implement this operator, we compute a noisy estimate of the value v with a uniform white noise u(\u03b9) of amplitude \u03b9, then projects this estimate onto a Fourier basis of the value function space with F < n coefficients with respect to the \u03bd-quadratic norm (projection that we write \u03a0F,\u03bd), and then applies the (exact) greedy operator on this projected estimate. In a nutshell, one call to the approximate greedy operator G (\u03bd, v) amounts to compute G\u03a0F,\u03bd(v + u(\u03b9)).\n6We implemented a crude line-search mechanism, that looks on the set 2i\u03b1 where \u03b1 is the minimal step estimated by CPI to ensure improvement.\nSimulations We have run series of experiments, in which we callibrated the perturbations (noise, approximations) so that the algorithm are significantly perturbed but no too much (we do not want their behavior to become too erratic). After trial and error, we ended up considering the following setting. We used garnet problems G(ns, na, b, p) with the number of states ns \u2208 {100, 200}, the number of actions na \u2208 {2, 5}, the branching factor b \u2208 {1, ns50 } (b = 1 corresponds to deterministic problems), the number of features to approximate the value p = ns10 , and the noise level \u03b9 = 0.05 (5%).\nFor each of these 23 = 8 parameter instances, we generated 30 (random) MDPs. For each such MDP, we ran DPI, CPI+, CPI(0.1) and NSDPI 30 times and estimated (for all iterations) the average and the standard deviation (on these 30 runs for a fixed MDP) of the performance (the error \u00b5(v\u03c0\u2217 \u2212 v\u03c0k)). Figures 2 to 5 display statistics of the average performance of the algorithms and of their standard deviation: statistics are here used because the average and the standard performance of an algorithm on an MDP are random variables since the MDP is itself random. For ease of comparison, all curves are displayed with the same x and y range. Figure 2 shows the statistics overall for the 23 = 8 parameter instances. Figure 3, 4 and 5 display statistics that are respectively conditional on the values of ns, na and b, which also gives some insight on the influence of these parameters.\nFrom these experiments and statistics, we made the following general observations: 1) In all experiments, CPI+ converged in very few iterations (most of the time less than 10, and always less than 20). 2) DPI is much more variable than the other algorithms and tends to provide the worst average performance. 3) If CPI+ and NSDPI have a similar average performance, the standard deviation of NSDPI is consistantly smaller than that of CPI and is thus more robust. 4) CPI(0.1) tend to provide the best average results, though its standard deviation is bigger than that of NSDPI. 5) All these relative behaviors tend to be amplified in the more difficult instances, that is when the state and actions spaces are big (ns = 200, na = 5) and the dynamics deterministic (b = 1).\nns = 100\nna = 2\nb = 1 (deterministic)"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on Policy Search algorithms, that compute an approximately optimal policy by following the standard Policy Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford, 2002; Lazaric et al., 2010). We describe existing and a few new performance bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI) (Kakade and Langford, 2002). By paying a particular attention to the concentrability constants involved in such guarantees, we notably argue that the guarantee of CPI is much better than that of DPI, but this comes at the cost of a relative\u2014exponential in 1 \u2014 increase of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI.", "creator": "LaTeX with hyperref package"}}}