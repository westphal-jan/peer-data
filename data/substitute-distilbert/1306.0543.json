{"id": "1306.0543", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "Predicting Parameters in Deep Learning", "abstract": "we demonstrate that there is maximum redundancy in measuring parameterization of several deep learning models. given only a few weight values for each feature it is possible to accurately predict the squared values. moreover, we show that this only can no parameter values be predicted, but many of events need not be learned quite all. we train several different architectures by learning slightly a small area of weights and predicting the rest. in the best case we are able to predict more than 95 % of the weights of a network without any drop function accuracy.", "histories": [["v1", "Mon, 3 Jun 2013 19:16:26 GMT  (324kb,D)", "https://arxiv.org/abs/1306.0543v1", null], ["v2", "Mon, 27 Oct 2014 11:49:08 GMT  (340kb,D)", "http://arxiv.org/abs/1306.0543v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["misha denil", "babak shakibi", "laurent dinh", "marc'aurelio ranzato", "nando de freitas"], "accepted": true, "id": "1306.0543"}, "pdf": {"name": "1306.0543.pdf", "metadata": {"source": "CRF", "title": "Predicting Parameters in Deep Learning", "authors": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "emails": ["misha.denil@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk", "laurent.dinh@umontreal.ca", "ranzato@fb.com"], "sections": [{"heading": "1 Introduction", "text": "Recent work on scaling deep networks has led to the construction of the largest artificial neural networks to date. It is now possible to train networks with tens of millions [13] or even over a billion parameters [7, 16].\nThe largest networks (i.e. those of Dean et al. [7]) are trained using asynchronous SGD. In this framework many copies of the model parameters are distributed over many machines and updated independently. An additional synchronization mechanism coordinates between the machines to ensure that different copies of the same set of parameters do not drift far from each other.\nA major drawback of this technique is that training is very inefficient in how it makes use of parallel resources [1]. In the largest networks of Dean et al. [7], where the gains from distribution are largest, distributing the model over 81 machines reduces the training time per mini-batch by a factor of 12, and increasing to 128 machines achieves a speedup factor of roughly 14. While these speedups are very significant, there is a clear trend of diminishing returns as the overhead of coordinating between the machines grows. Other approaches to distributed learning of neural networks involve training in batch mode [8], but these methods have not been scaled nearly as far as their online counterparts.\nIt seems clear that distributed architectures will always be required for extremely large networks; however, as efficiency decreases with greater distribution, it also makes sense to study techniques for learning larger networks on a single machine. If we can reduce the number of parameters which must be learned and communicated over the network of fixed size, then we can reduce the number of machines required to train it, and hence also reduce the overhead of coordination in a distributed framework.\nIn this work we study techniques for reducing the number of free parameters in neural networks by exploiting the fact that the weights in learned networks tend to be structured. The technique we present is extremely general, and can be applied to a broad range of models. Our technique is also completely orthogonal to the choice of activation function as well as other learning optimizations; it can work alongside other recent advances in neural network training such as dropout [12], rectified units [20] and maxout [9] without modification.\nar X\niv :1\n30 6.\n05 43\nv2 [\ncs .L\nG ]\n2 7\nThe intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13]. Given this structure, representing the value of each pixel in the feature separately is redundant, since it is highly likely that the value of a pixel will be equal to a weighted average of its neighbours. Taking advantage of this type of structure means we do not need to store weights for every input in each feature. This intuition is illustrated in Figures 1 and 2.\nThe remainder of this paper is dedicated to elaborating on this observation. We describe a general purpose technique for reducing the number of free parameters in neural networks. The core of the technique is based on representing the weight matrix as a low rank product of two smaller matrices. By factoring the weight matrix we are able to directly control the size of the parameterization by controlling the rank of the weight matrix.\nNa\u0131\u0308ve application of this technique is straightforward but tends to reduce performance of the networks. We show that by carefully constructing one of the factors, while learning only the other factor, we can train networks with vastly fewer parameters which achieve the same performance as full networks with the same structure.\nThe key to constructing a good first factor is exploiting smoothness in the structure of the inputs. When we have prior knowledge of the smoothness structure we expect to see (e.g. in natural images), we can impose this structure directly through the choice of factor. When no such prior knowledge is available we show that it is still possible to make a good data driven choice.\nWe demonstrate experimentally that our parameter prediction technique is extremely effective. In the best cases we are able to predict more than 95% of the parameters of a network without any drop in predictive accuracy.\nThroughout this paper we make a distinction between dynamic and static parameters. Dynamic parameters are updated frequently during learning, potentially after each observation or mini-batch. This is in contrast to static parameters, whose values are computed once and not altered. Although the values of these parameters may depend on the data and may be expensive to compute, the computation need only be done once during the entire learning process.\nThe reason for this distinction is that static parameters are much easier to handle in a distributed system, even if their values must be shared between machines. Since the values of static parameters do not change, access to them does not need to be synchronized. Copies of these parameters can be safely distributed across machines without any of the synchronization overhead incurred by distributing dynamic parameters."}, {"heading": "2 Low rank weight matrices", "text": "Deep networks are composed of several layers of transformations of the form h = g(vW), where v is an nv-dimensional input, h is an nh-dimensional output, and W is an nv \u00d7 nh matrix of parameters. A column of W contains the weights connecting each unit in the visible layer to a single unit in the hidden layer. We can to reduce the number of free parameters by representing W as the product of two matrices W = UV, where U has size nv \u00d7 n\u03b1 and V has size n\u03b1 \u00d7 nh. By making n\u03b1 much smaller than nv and nh we achieve a substantial reduction in the number of parameters.\nIn principle, learning the factored weight matrices is straightforward. We simply replace W with UV in the objective function and compute derivatives with respect to U and V instead of W. In practice this na\u0131\u0308ve approach does not preform as well as learning a full rank weight matrix directly.\nMoreover, the factored representation has redundancy. If Q is any invertible matrix of size n\u03b1\u00d7 n\u03b1 we have W = UV = (UQ)(Q\u22121V) = U\u0303V\u0303. One way to remove this redundancy is to fix the value of U and learn only V. The question remains what is a reasonable choice for U? The following section provides an answer to this question."}, {"heading": "3 Feature prediction", "text": "We can exploit the structure in the features of a deep network to represent the features in a much lower dimensional space. To do this we consider the weights connected to a single hidden unit as a function w : W \u2192 R mapping weight space to real numbers estimate values of this function using regression. In the case of p \u00d7 p image patches,W is the coordinates of each pixel, but other structures forW are possible. A simple regression model which is appropriate here is a linear combination of basis functions. In this view the columns of U form a dictionary of basis functions, and the features of the network are linear combinations of these features parameterized by V. The problem thus becomes one of choosing a good base dictionary for representing network features."}, {"heading": "3.1 Choice of dictionary", "text": "The base dictionary for feature prediction can be constructed in several ways. An obvious choice is to train a single layer unsupervised model and use the features from that model as a dictionary. This approach has the advantage of being extremely flexible\u2014no assumptions about the structure of feature space are required\u2014but has the drawback of requiring an additional training phase.\nWhen we have prior knowledge about the structure of feature space we can exploit it to construct an appropriate dictionary. For example when learning features for images we could choose U to be a selection of Fourier or wavelet bases to encode our expectation of smoothness.\nWe can also build U using kernels that encode prior knowledge. One way to achieve this is via kernel ridge regression [25]. Let w\u03b1 denote the observed values of the weight vector w on a restricted subset of its domain \u03b1 \u2282 W . We introduce a kernel matrix K\u03b1, with entries (K\u03b1)ij = k(i, j), to model the covariance between locations i, j \u2208 \u03b1. The parameters at these locations are (w\u03b1)i and (w\u03b1)j . The kernel enables us to make smooth predictions of the parameter vector over the entire domainW using the standard kernel ridge predictor:\nw = kT\u03b1(K\u03b1 + \u03bbI) \u22121w\u03b1 ,\nwhere k\u03b1 is a matrix whose elements are given by (k\u03b1)ij = k(i, j) for i \u2208 \u03b1 and j \u2208 W , and \u03bb is a ridge regularization coefficient. In this case we have U = kT\u03b1(K\u03b1 + \u03bbI) \u22121 and V = w\u03b1."}, {"heading": "3.2 A concrete example", "text": "In this section we describe the feature prediction process as it applies to features derived from image patches using kernel ridge regression, since the intuition is strongest in this case. We defer a discussion of how to select a kernel for deep layers as well as for non-image data in the visible layer to a later section. In those settings the prediction process is formally identical, but the intuition is less clear.\nIf v is a vectorized image patch corresponding to the visible layer of a standard neural network then the hidden activity induced by this patch is given by h = g(vW), where g is the network nonlinearity and W = [w1, . . . ,wnh ] is a weight matrix whose columns each correspond to features which are to be matched to the visible layer.\nWe consider a single column of the weight matrix, w, whose elements are indexed by i \u2208 W . In the case of an image patch these indices are multidimensional i = (ix, iy, ic), indicating the spatial location and colour channel of the index i. We select locations \u03b1 \u2282 W at which to represent the filter explicitly and use w\u03b1 to denote the vector of weights at these locations.\nThere are a wide variety of options for how \u03b1 can be selected. We have found that choosing \u03b1 uniformly at random from W (but tied across channels) works well; however, it is possible that performance could be improved by carefully designing a process for selecting \u03b1.\nWe can use values for w\u03b1 to predict the full feature as w = kT\u03b1(K\u03b1 + \u03bbI) \u22121w\u03b1. Notice that we can predict the entire feature matrix in parallel using W = kT\u03b1(K\u03b1 + \u03bbI) \u22121W\u03b1 where W\u03b1 = [(w1)\u03b1, . . . , (wnh)\u03b1].\nFor image patches, where we expect smoothness in pixel space, an appropriate kernel is the squared exponential kernel\nk(i, j) = exp ( \u2212 (ix \u2212 jx)\n2 + (iy \u2212 jy)2 2\u03c32 ) where \u03c3 is a length scale parameter which controls the degree of smoothness.\nHere \u03b1 has a convenient interpretation as the set of pixel locations in the image, each corresponding to a basis function in the dictionary defined by the kernel. More generically we will use \u03b1 to index a collection of dictionary elements in the remainder of the paper, even when a dictionary element may not correspond directly to a pixel location as in this example."}, {"heading": "3.3 Interpretation as pooling", "text": "So far we have motivated our technique as a method for predicting features in a neural network; however, the same approach can also be interpreted as a linear pooling process.\nRecall that the hidden activations in a standard neural network before applying the nonlinearity are given by g\u22121(h) = vW. Our motivation has proceeded along the lines of replacing W with U\u03b1W\u03b1 and discussing the relationship between W and its predicted counterpart.\nAlternatively we can write g\u22121(h) = v\u03b1W\u03b1 where v\u03b1 = vU\u03b1 is a linear transformation of the data. Under this interpretation we can think of a predicted layer as being composed to two layers internally. The first is a linear layer which applies a fixed pooling operator given by U\u03b1, and the second is an ordinary fully connected layer with |\u03b1| visible units."}, {"heading": "3.4 Columnar architecture", "text": "The prediction process we have described so far assumes that U\u03b1 is the same for all features; however, this can be too restrictive. Continuing with the intuition that filters should be smooth local edge detectors we might want to choose \u03b1 to give high resolution in a local area of pixel space while using a sparser representation in the remainder of the space. Naturally, in this case we would want to choose several different \u03b1\u2019s, each of which concentrates high resolution information in different regions.\nIt is straightforward to extend feature prediction to this setting. Suppose we have several different index sets \u03b11, . . . , \u03b1J corresponding to elements from a dictionary U. For each \u03b1j we can form the sub-dictionary U\u03b1j and predicted the feature matrix Wj = U\u03b1jW\u03b1j . The full predicted feature matrix is formed by concatenating each of these matrices blockwise W = [W1, . . . ,WJ ]. Each block of the full predicted feature matrix can be treated completely independently. Blocks Wi and Wj share no parameters\u2014even their corresponding dictionaries are different.\nEach \u03b1j can be thought of as defining a column of representation inside the layer. The input to each column is shared, but the representations computed in each column are independent. The output of the layer is obtained by concatenating the output of each column. This is represented graphically in Figure 3.\nIntroducing additional columns into the network increases the number of static parameters but the number of dynamic parameters remains fixed. The increase in static parameters comes from the fact that each column has its own dictionary. The reason that there is not a corresponding increase in the number of dynamic parameters is that for a fixed size hidden layer the hidden units are divided between the columns. The number of dynamic parameters depends only on the number of hidden units and the size of each dictionary.\nIn a convolutional network the interpretation is similar. In this setting we have g\u22121(h) = v \u2217W\u2217, where W\u2217 is an appropriately sized filter bank. Using W to denote the result of vectorizing the filters of W\u2217 (as is done in non-convolutional models) we can again write W = U\u03b1w\u03b1, and using a slight abuse of notation1 we can write g\u22121(h) = v \u2217U\u03b1w\u03b1. As above, we re-order the operations to obtain g\u22121(h) = v\u03b1w\u03b1 resulting in a structure similar to a layer in an ordinary MLP. This structure is illustrated in Figure 3.\nNote that v is first convolved with U\u03b1 to produce v\u03b1. That is, preprocessing in each column comes from a convolution with a fixed set of filters, defined by the dictionary. Next, we form linear combinations of these fixed convolutions, with coefficients given by w\u03b1. This particular order of operations may result in computational improvements if the number of hidden channels is larger than n\u03b1, or if the elements of U\u03b1 are separable [22]."}, {"heading": "3.5 Constructing dictionaries", "text": "We now turn our attention to selecting an appropriate dictionary for different layers of the network. The appropriate choice of dictionary inevitably depends on the structure of the weight space.\nWhen the weight space has a topological structure where we expect smoothness, for example when the weights correspond to pixels in an image patch, we can choose a kernel-based dictionary to enforce the type of smoothness we expect.\nWhen there is no topological structure to exploit, we propose to use data driven dictionaries. An obvious choice here is to use a shallow unsupervised feature learning, such as an autoencoder, to build a dictionary for the layer.\nAnother option is to construct data-driven kernels for ridge regression. Easy choices here are using the empirical covariance or empirical squared covariance of the hidden units, averaged over the data.\nSince the correlations in hidden activities depend on the weights in lower layers we cannot initialize kernels in deep layers in this way without training the previous layers. We handle this by pre-training each layer as an autoencoder. We construct the kernel using the empirical covariance of the hidden units over the data using the pre-trained weights. Once each layer has been pre-trained in this way\n1The vectorized filter bank W = U\u03b1w\u03b1 must be reshaped before the convolution takes place.\nwe fine-tune the entire network with backpropagation, but in this phase the kernel parameters are fixed.\nWe also experiment with other choices for the dictionary, such as random projections (iid Gaussian dictionary) and random connections (dictionary composed of random columns of the identity)."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Multilayer perceptron", "text": "We perform some initial experiments using MLPs [24] in order to demonstrate the effectiveness of our technique. We train several MLP models on MNIST using different strategies for constructing the dictionary, different numbers of columns and different degrees of reduction in the number of dynamic parameters used in each feature. We chose to explore these permutations on MNIST since it is small enough to allow us to have broad coverage.\nThe networks in this experiment all have two hidden layers with a 784\u2013500\u2013500\u201310 architecture and use a sigmoid activation function. The final layer is a softmax classifier. In all cases we preform parameter prediction in the first and second layers only; the final softmax layer is never predicted. This layer contains approximately 1% of the total network parameters, so a substantial savings is possible even if features in this layer are not predicted.\nFigure 4 (left) shows performance using several different strategies for constructing the dictionary, each using 10 columns in the first and second layers. We divide the hidden units in each layer equally between columns (so each column connects to 50 units in the layer above).\nThe different dictionaries are as follows: nokernel is an ordinary model with no feature prediction (shown as a horizontal line). LowRank is when both U and V are optimized. RandCon is random connections (the dictionary is random columns of the identity). RandFixU is random projections using a matrix of iid Gaussian entries. SE is ridge regression with the squared exponential kernel with length scale 1.0. Emp is ridge regression with the covariance kernel. Emp2 is ridge regression with the squared covariance kernel. AE is a dictionary pre-trained as an autoencoder. The SE\u2013Emp and SE-Emp2 architectures preform substantially better than the alternatives, especially with few dynamic parameters.\nFor consistency we pre-trained all of the models, except for the LowRank, as autoencoders. We did not pretrain the LowRank model because we found the autoencoder pretraining to be extremely unstable for this model.\nFigure 4 (right) shows the results of a similar experiment on TIMIT. The raw speech data was analyzed using a 25-ms Hamming window with a 10-ms fixed frame rate. In all the experiments, we represented the speech using 12th-order Mel frequency cepstral coefcients (MFCCs) and energy, along with their first and second temporal derivatives. The networks used in this experiment have two hidden layers with 1024 units. Phone error rate was measured by performing Viterbi decoding\nthe phones in each utterance using a bigram language model, and confusions between certain sets of phones were ignored as described in [19]."}, {"heading": "4.2 Convolutional network", "text": "Figure 5 shows the performance of a convnet [17] on CIFAR-10. The first convolutional layer filters the 32 \u00d7 32 \u00d7 3 input image using 48 filters of size 8 \u00d7 8 \u00d7 3. The second convolutional layer applies 64 filters of size 8 \u00d7 8 \u00d7 48 to the output of the first layer. The third convolutional layer further transforms the output of the second layer by applying 64 filters of size 5 \u00d7 5 \u00d7 64. The output of the third layer is input to a fully connected layer with 500 hidden units and finally into a softmax layer with 10 outputs. Again we do not reduce the parameters in the final softmax layer. The convolutional layers each have one column and the fully connected layer has five columns.\nConvolutional layers have a natural topological structure to exploit, so we use an dictionary constructed with the squared exponential kernel in each convolutional layer. The input to the fully connected layer at the top of the network comes from a convolutional layer so we use ridge regression with the squared exponential kernel to predict parameters in this layer as well."}, {"heading": "4.3 Reconstruction ICA", "text": "Reconstruction ICA [15] is a method for learning overcomplete ICA models which is similar to a linear autoencoder network. We demonstrate that we can effectively predict parameters in RICA on both CIFAR-10 and STL-10. In order to use RICA as a classifier we follow the procedure of Coates et al. [6].\nFigure 6 (left) shows the results of parameter prediction with RICA on CIFAR-10 and STL-10. RICA is a single layer architecture, and we predict parameters a squared exponential kernel dictionary with a length scale of 1.0. The nokernel line shows the performance of RICA with no feature prediction on the same task. In both cases we are able to predict more than half of the dynamic parameters without a substantial drop in accuracy.\nFigure 6 (right) compares the performance of two RICA models with the same number of dynamic parameters. One of the models is ordinary RICA with no parameter prediction and the other has 50% of the parameters in each feature predicted using squared exponential kernel dictionary with a length scale of 1.0; since 50% of the parameters in each feature are predicted, the second model has twice as many features with the same number of dynamic parameters."}, {"heading": "5 Related work and future directions", "text": "Several other methods for limiting the number of parameters in a neural network have been explored in the literature. An early approach is the technique of \u201cOptimal Brain Damage\u201d [18] which uses approximate second derivative information to remove parameters from an already trained network. This technique does not apply in our setting, since we aim to limit the number of parameters before training, rather than after.\nThe most common approach to limiting the number of parameters is to use locally connected features [6]. The size of the parameterization of locally connected networks can be further reduced by using tiled convolutional networks [10] in which groups of feature weights which tile the input\nspace are tied. Convolutional neural networks [13] are even more restrictive and force a feature to have tied weights for all receptive fields.\nTechniques similar to the one in this paper have appeared for shallow models in the computer vision literature. The double sparsity method of Rubinstein et al. [23] involves approximating linear dictionaries with other dictionaries in a similar manner to how we approximate network features. Rigamonti et al. [22] study approximating convolutional filter banks with linear combinations of separable filters. Both of these works focus on shallow single layer models, in contrast to our focus on deep networks.\nThe techniques described in this paper are orthogonal to the parameter reduction achieved by tying weights in a tiled or convolutional pattern. Tying weights effectively reduces the number of feature maps by constraining features at different locations to share parameters. Our approach reduces the number of parameters required to represent each feature and it is straightforward to incorporate into a tiled or convolutional network.\nCires\u0327an et al. [3] control the number of parameters by removing connections between layers in a convolutional network at random. They achieve state-of-the-art results using these randomly connected layers as part of their network. Our technique subsumes the idea of random connections, as described in Section 3.5.\nThe idea of regularizing networks through prior knowledge of smoothness is not new, but it is a delicate process. Lang and Hinton [14] tried imposing explicit smoothness constraints through regularization but found it to universally reduce performance. Na\u0131\u0308vely factoring the weight matrix and learning both factors tends to reduce performance as well. Although the idea is simple conceptually, execution is difficult. Gu\u0308lc\u0327ehre et al. [11] have demonstrated that prior knowledge is extremely important during learning, which highlights the importance of introducing it effectively.\nRecent work has shown that state of the art results on several benchmark tasks in computer vision can be achieved by training neural networks with several columns of representation [2, 13]. The use of different preprocessing for different columns of representation is of particular relevance [2]. Our approach has an interpretation similar to this as described in Section 3.4. Unlike the work of [2], we do not consider deep columns in this paper; however, collimation is an attractive way for increasing parallelism within a network, as the columns operate completely independently. There is no reason we could not incorporate deeper columns into our networks, and this would make for a potentially interesting avenue of future work.\nOur approach is superficially similar to the factored RBM [21, 26], whose parameters form a 3- tensor. Since the total number of parameters in this model is prohibitively large, the tensor is represented as an outer product of three matrices. Major differences between our technique and the factored RBM include the fact that the factored RBM is a specific model, whereas our technique can be applied more broadly\u2014even to factored RBMs. In addition, in a factored RBM all factors are learned, whereas in our approach the dictionary is fixed judiciously.\nIn this paper we always choose the set \u03b1 of indices uniformly at random. There are a wide variety of other options which could be considered here. Other works have focused on learning receptive fields directly [5], and would be interesting to incorporate with our technique.\nIn a similar vein, more careful attention to the selection of kernel functions is appropriate. We have considered some simple examples and shown that they preform well, but our study is hardly exhaustive. Using different types of kernels to encode different types of prior knowledge on the weight space, or even learning the kernel functions directly as part of the optimization procedure as in [27] are possibilities that deserve exploration.\nWhen no natural topology on the weight space is available we infer a topology for the dictionary from empirical statistics; however, it may be possible to instead construct the dictionary to induce a desired topology on the weight space directly. This has parallels to other work on inducing topology in representations [10] as well as work on learning pooling structures in deep networks [4]."}, {"heading": "6 Conclusion", "text": "We have shown how to achieve significant reductions in the number of dynamic parameters in deep models. The idea is orthogonal but complementary to recent advances in deep learning, such as dropout, rectified units and maxout. It creates many avenues for future work, such as improving large scale industrial implementations of deep networks, but also brings into question whether we have the right parameterizations in deep learning."}], "references": [{"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Technical Report arXiv:1305.0445, Universit\u00e9 de Montr\u00e9al", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Schmidhuber"], "venue": "IEEE Computer Vision and Pattern Recognition, pages 3642\u20133649", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "High-performance neural networks for visual object classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Masci"], "venue": "arXiv:1102.0183", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Emergence of object-selective features in unsupervised feature learning", "author": ["A. Coates", "A. Karpathy", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 2690\u20132698", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 2528\u20132536", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "Artificial Intelligence and Statistics", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 1232\u20131240", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable stacking and learning for building deep architectures", "author": ["L. Deng", "D. Yu", "J. Platt"], "venue": "International Conference on Acoustics, Speech, and Signal Processing, pages 2133\u20132136", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Emergence of complex-like cells in a temporal product network with local receptive fields", "author": ["K. Gregor", "Y. LeCun"], "venue": "arXiv preprint arXiv:1006.0448", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["C. G\u00fcl\u00e7ehre", "Y. Bengio"], "venue": "International Conference on Learning Representations", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "CoRR, abs/1207.0580", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, pages 1106\u20131114", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Dimensionality reduction and prior knowledge in e-set recognition", "author": ["K. Lang", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 24:1017\u20131025", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Building highlevel features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimal brain damage", "author": ["Y. LeCun", "J.S. Denker", "S. Solla", "R.E. Howard", "L.D. Jackel"], "venue": "Advances in Neural Information Processing Systems, pages 598\u2013605", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1990}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["K.-F. Lee", "H.-W. Hon"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(11):1641\u20131648", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proc. 27th International Conference on Machine Learning, pages 807\u2013814. Omnipress Madison, WI", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Factored 3-way restricted Boltzmann machines for modeling natural images", "author": ["M. Ranzato", "A. Krizhevsky", "G.E. Hinton"], "venue": "Artificial Intelligence and Statistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning separable filters", "author": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"], "venue": "IEEE Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Double sparsity: learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, 58:1553\u20131564", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(6088):533\u2013536", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1986}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "B. Marlin", "N. Freitas"], "venue": "International Conference on Machine Learning, pages 1201\u20131208", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "A neural support vector network architecture with adaptive kernels", "author": ["P. Vincent", "Y. Bengio"], "venue": "International Joint Conference on Neural Networks, pages 187\u2013192", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 12, "context": "It is now possible to train networks with tens of millions [13] or even over a billion parameters [7, 16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "It is now possible to train networks with tens of millions [13] or even over a billion parameters [7, 16].", "startOffset": 98, "endOffset": 105}, {"referenceID": 15, "context": "It is now possible to train networks with tens of millions [13] or even over a billion parameters [7, 16].", "startOffset": 98, "endOffset": 105}, {"referenceID": 6, "context": "[7]) are trained using asynchronous SGD.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "A major drawback of this technique is that training is very inefficient in how it makes use of parallel resources [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "[7], where the gains from distribution are largest, distributing the model over 81 machines reduces the training time per mini-batch by a factor of 12, and increasing to 128 machines achieves a speedup factor of roughly 14.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Other approaches to distributed learning of neural networks involve training in batch mode [8], but these methods have not been scaled nearly as far as their online counterparts.", "startOffset": 91, "endOffset": 94}, {"referenceID": 11, "context": "Our technique is also completely orthogonal to the choice of activation function as well as other learning optimizations; it can work alongside other recent advances in neural network training such as dropout [12], rectified units [20] and maxout [9] without modification.", "startOffset": 209, "endOffset": 213}, {"referenceID": 19, "context": "Our technique is also completely orthogonal to the choice of activation function as well as other learning optimizations; it can work alongside other recent advances in neural network training such as dropout [12], rectified units [20] and maxout [9] without modification.", "startOffset": 231, "endOffset": 235}, {"referenceID": 8, "context": "Our technique is also completely orthogonal to the choice of activation function as well as other learning optimizations; it can work alongside other recent advances in neural network training such as dropout [12], rectified units [20] and maxout [9] without modification.", "startOffset": 247, "endOffset": 250}, {"referenceID": 5, "context": "The intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13].", "startOffset": 252, "endOffset": 259}, {"referenceID": 12, "context": "The intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13].", "startOffset": 252, "endOffset": 259}, {"referenceID": 24, "context": "One way to achieve this is via kernel ridge regression [25].", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "This particular order of operations may result in computational improvements if the number of hidden channels is larger than n\u03b1, or if the elements of U\u03b1 are separable [22].", "startOffset": 168, "endOffset": 172}, {"referenceID": 23, "context": "We perform some initial experiments using MLPs [24] in order to demonstrate the effectiveness of our technique.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "the phones in each utterance using a bigram language model, and confusions between certain sets of phones were ignored as described in [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "Figure 5 shows the performance of a convnet [17] on CIFAR-10.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Reconstruction ICA [15] is a method for learning overcomplete ICA models which is similar to a linear autoencoder network.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "An early approach is the technique of \u201cOptimal Brain Damage\u201d [18] which uses approximate second derivative information to remove parameters from an already trained network.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "The most common approach to limiting the number of parameters is to use locally connected features [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "The size of the parameterization of locally connected networks can be further reduced by using tiled convolutional networks [10] in which groups of feature weights which tile the input", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Convolutional neural networks [13] are even more restrictive and force a feature to have tied weights for all receptive fields.", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "[23] involves approximating linear dictionaries with other dictionaries in a similar manner to how we approximate network features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] study approximating convolutional filter banks with linear combinations of separable filters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] control the number of parameters by removing connections between layers in a convolutional network at random.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Lang and Hinton [14] tried imposing explicit smoothness constraints through regularization but found it to universally reduce performance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "[11] have demonstrated that prior knowledge is extremely important during learning, which highlights the importance of introducing it effectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Recent work has shown that state of the art results on several benchmark tasks in computer vision can be achieved by training neural networks with several columns of representation [2, 13].", "startOffset": 181, "endOffset": 188}, {"referenceID": 12, "context": "Recent work has shown that state of the art results on several benchmark tasks in computer vision can be achieved by training neural networks with several columns of representation [2, 13].", "startOffset": 181, "endOffset": 188}, {"referenceID": 1, "context": "The use of different preprocessing for different columns of representation is of particular relevance [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "Unlike the work of [2], we do not consider deep columns in this paper; however, collimation is an attractive way for increasing parallelism within a network, as the columns operate completely independently.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "Our approach is superficially similar to the factored RBM [21, 26], whose parameters form a 3tensor.", "startOffset": 58, "endOffset": 66}, {"referenceID": 25, "context": "Our approach is superficially similar to the factored RBM [21, 26], whose parameters form a 3tensor.", "startOffset": 58, "endOffset": 66}, {"referenceID": 4, "context": "Other works have focused on learning receptive fields directly [5], and would be interesting to incorporate with our technique.", "startOffset": 63, "endOffset": 66}, {"referenceID": 26, "context": "Using different types of kernels to encode different types of prior knowledge on the weight space, or even learning the kernel functions directly as part of the optimization procedure as in [27] are possibilities that deserve exploration.", "startOffset": 190, "endOffset": 194}, {"referenceID": 9, "context": "This has parallels to other work on inducing topology in representations [10] as well as work on learning pooling structures in deep networks [4].", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "This has parallels to other work on inducing topology in representations [10] as well as work on learning pooling structures in deep networks [4].", "startOffset": 142, "endOffset": 145}], "year": 2014, "abstractText": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy.", "creator": "LaTeX with hyperref package"}}}