{"id": "1306.3729", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2013", "title": "Spectral Experts for Estimating Mixtures of Linear Regressions", "abstract": "discriminative latent - variable models are typically learned using em based gradient - based optimization, which emerge from local optima. in this paper, we make a new computationally efficient and provably consistent estimator for a subgroup of linear regressions, a simple instance of a discriminative latent - variable model. our approach relies on a low - yielding linear regression to recover a symmetric tensor, which can be factorized into elastic parameters using a tensor inversion method. we prove rates of convergence for our statistics both provide implicit empirical evaluation illustrating its strengths relative to static optimization ( mmm ).", "histories": [["v1", "Mon, 17 Jun 2013 03:02:05 GMT  (2364kb,D)", "http://arxiv.org/abs/1306.3729v1", "Accepted at ICML 2013. Includes supplementary material"]], "COMMENTS": "Accepted at ICML 2013. Includes supplementary material", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["arun tejasvi chaganty", "percy liang"], "accepted": true, "id": "1306.3729"}, "pdf": {"name": "1306.3729.pdf", "metadata": {"source": "META", "title": "Spectral Experts for Estimating Mixtures of Linear Regressions", "authors": ["Arun Tejasvi Chaganty", "Percy Liang"], "emails": ["chaganty@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": null, "text": "Last Modified: June 18, 2013"}, {"heading": "1. Introduction", "text": "Discriminative latent-variable models, which combine the high accuracy of discriminative models with the compact expressiveness of latent-variable models, have been widely applied to many tasks, including object recognition (Quattoni et al., 2004), human action recognition (Wang & Mori, 2009), syntactic parsing (Petrov & Klein, 2008), and machine translation (Liang et al., 2006). However, parameter estimation in these models is difficult; past approaches rely on local optimization (EM, gradient descent) and are vulnerable to local optima.\nOur broad goal is to develop efficient provably consistent estimators for discriminative latent-variable models. In this paper, we provide a first step in this direction by proposing a new algorithm for a simple model, a mixture of linear regressions (Viele & Tong, 2002).\nRecently, method of moments estimators have been\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\ndeveloped for generative latent-variable models, including mixture models, HMMs (Anandkumar et al., 2012b), Latent Dirichlet Allocation (Anandkumar et al., 2012a), and parsing models (Hsu et al., 2012). The basic idea of these methods is to express the unknown model parameters as a tensor factorization of the third-order moments of the model distribution, a quantity which can be estimated from data. The moments have a special symmetric structure which permits the factorization to be computed efficiently using the robust tensor power method (Anandkumar et al., 2012c).\nIn a mixture of linear regressions, using third-order moments does not directly reveal the tensor structure of the problem, so we cannot simply apply the above tensor factorization techniques. Our approach is to employ low-rank linear regression (Negahban & Wainwright, 2009; Tomioka et al., 2011) to predict the second and third powers of the response. The solution to these regression problems provide the appropriate symmetric tensors, on which we can then apply the tensor power method to retrieve the final parameters.\nThe result is a simple and efficient two-stage algorithm, which we call Spectral Experts. We prove that our algorithm yields consistent parameter estimates under certain identifiability conditions. We also conduct an empirical evaluation of our technique to understand its statistical properties (Section 5). While Spectral Experts generally does not outperform EM, presumably due to its weaker statistical efficiency, it serves as an effective initialization for EM, significantly outperforming EM with random initialization."}, {"heading": "1.1. Notation", "text": "Let [n] = {1, . . . , n} denote the first n positive integers. We use O(f(n)) to denote a function g(n) such that limn\u2192\u221e g(n)/f(n) <\u221e.\nWe use x\u2297p to represent the p-th order tensor formed by taking the tensor product of x \u2208 Rd; i.e. x\u2297pi1...ip = xi1 \u00b7 \u00b7 \u00b7xip . We will use \u3008\u00b7, \u00b7\u3009 to denote the gener-\nar X\niv :1\n30 6.\n37 29\nv1 [\ncs .L\nG ]\n1 7\nJu n\n20 13\nalized dot product between two p-th order tensors: \u3008X,Y \u3009 = \u2211 i1,...ip Xi1,...ipYi1,...ip . A tensor X is symmetric if for all i, j \u2208 [d]p which are permutations of each other, Xi1\u00b7\u00b7\u00b7ip = Xj1\u00b7\u00b7\u00b7jp (all tensors in this paper will be symmetric). For a p-th order tensor X \u2208 (Rd)\u2297p, the mode-i unfolding of X is a matrix X(i) \u2208 Rd\u00d7d p\u22121 , whose j-th row contains all the elements of X whose i-th index is equal to j.\nFor a vector X, let \u2016X\u2016op denote the 2-norm. For a matrix X, let \u2016X\u2016\u2217 denote the nuclear (trace) norm (sum of singular values), \u2016X\u2016F denote the Frobenius norm (square root of sum of squares of singular values), \u2016X\u2016max denote the max norm (elementwise maximum), \u2016X\u2016op denote the operator norm (largest singular value), and \u03c3k(X) be the k-th largest singular value of X. For a p-th order tensor X, let \u2016X\u2016\u2217 = 1 p \u2211p i=1 \u2016X(i)\u2016\u2217 denote the average nuclear norm over\nall p unfoldings, and let \u2016X\u2016op = 1p \u2211p i=1 \u2016X(i)\u2016op denote the average operator norm over all p unfoldings.\nLet vec(X) be the vectorization of a p-th order tensor. For example, if X \u2208 (R2)\u22973, vec(X) = (X111, X112, \u00b7 \u00b7 \u00b7 , X222). For a tensor X \u2208 (Rd)\u2297p, let cvec(X) \u2208 RN(d,p), N(d, p) = ( d+p\u22121 p ) be the collapsed vectorization of X. For example, if X \u2208 Rd\u00d7d, cvec(X) = (Xii : i \u2208 [d]; Xij+Xji\u221a2 : i, j \u2208 [d], i < j). In general, each component of cvec(X) is indexed by a vector of counts (c1, . . . , cd) with total sum \u2211 i ci = p. The value of that component is 1\u221a |K(c)| \u2211 k\u2208K(c)Xk1\u00b7\u00b7\u00b7kp , where K(c) = {k \u2208 [d]p : \u2200i \u2208 [d], ci = |{j \u2208 [p] : kj = i}|} are the set of index vectors k whose count profile is c. We note that for a symmetric tensor X and any tensor Y , \u3008X,Y \u3009 = \u3008cvec(X), cvec(Y )\u3009; this property is not true in general though. Later, we\u2019ll see that vectorization allow us to perform regression on tensors, and collapsing simplifies our identifiability condition."}, {"heading": "2. Model", "text": "The mixture of linear regressions model (Viele & Tong, 2002) defines a conditional distribution over a response y \u2208 R given covariates x \u2208 Rd. Let k be the number of mixture components. The generation of y given x involves three steps: (i) draw a mixture component h \u2208 [k] according to mixture proportions \u03c0 = (\u03c01, . . . , \u03c0k); (ii) draw observation noise from a known zero-mean noise distribution E , and (iii) set y deterministically based on h and . More compactly:\nh \u223c Multinomial(\u03c0), (1) \u223c E , (2) y = \u03b2Th x+ . (3)\nThe parameters of the model are \u03b8 = (\u03c0,B), where \u03c0 \u2208 Rd are the mixture proportions and B = [\u03b21 | \u00b7 \u00b7 \u00b7 | \u03b2k] \u2208 Rd\u00d7k are the regression coefficients. Note that the choice of mixture component h and the observation noise are independent. The learning problem is stated as follows: given n i.i.d. samples (x(1), y(1)), . . . , (x(n), y(n)) drawn from the model with some unknown parameters \u03b8\u2217, return an estimate of the parameters \u03b8\u0302.\nThe mixture of linear regressions model has been applied in the statistics literature for modelling music perception, where x is the actual tone and y is the tone perceived by a musician (Viele & Tong, 2002). The model is an instance of the hierarchical mixture of experts (Jacobs et al., 1991), in which the mixture proportions are allowed to depend on x, known as a gating function. This dependence allow the experts to be localized in input space, providing more flexibility, but we do not consider this dependence in our model.\nThe estimation problem for a mixture of linear regressions is difficult because the mixture components h are unobserved, resulting in a non-convex log marginal likelihood. The parameters are typically learned using expectation maximization (EM) or Gibbs sampling (Viele & Tong, 2002), which suffers from local optima. In the next section, we present a new algorithm that sidesteps the local optima problem entirely."}, {"heading": "3. Spectral Experts algorithm", "text": "In this section, we describe our Spectral Experts algorithm for estimating model parameters \u03b8 = (\u03c0,B). The algorithm consists of two steps: (i) low-rank regression to estimate certain symmetric tensors; and (ii) tensor factorization to recover the parameters. The two steps can be performed efficiently using convex optimization and tensor power method, respectively.\nTo warm up, let us consider linear regression on the response y given x. From the model definition, we have y = \u03b2>h x+ . The challenge is that the regression coefficients \u03b2h depend on the random h. The first key step is to average over this randomness by defining average\nregression coefficients M1 def = \u2211k h=1 \u03c0h\u03b2h. Now we can express y as a linear function of x with non-random coefficients M1 plus a noise term \u03b71(x):\ny = \u3008M1, x\u3009+ (\u3008\u03b2h \u2212M1, x\u3009+ )\ufe38 \ufe37\ufe37 \ufe38 def = \u03b71(x) . (4)\nThe noise \u03b71(x) is the sum of two terms: (i) the mixing noise \u3008M1\u2212\u03b2h, x\u3009 due to the random choice of the mixture component h, and (ii) the observation noise \u223c E . Although the noise depends on x, it still has zero mean\nconditioned on x. We will later show that we can perform linear regression on the data {x(i), y(i)}ni=1 to produce a consistent estimate of M1. But clearly, knowing M1 is insufficient for identifying all the parameters \u03b8, as M1 only contains d degrees of freedom whereas \u03b8 contains O(kd).\nIntuitively, performing regression on y given x provides only first-order information. The second key insight is that we can perform regression on higher-order powers to obtain more information about the parameters. Specifically, for an integer p \u2265 1, let us define the average p-th order tensor power of the parameters as follows:\nMp def = k\u2211 h=1 \u03c0h\u03b2 \u2297p h . (5)\nNow consider performing regression on y2 given x\u22972. Expanding y2 = (\u3008\u03b2h, x\u3009 + )2, using the fact that \u3008\u03b2h, x\u3009p = \u3008\u03b2\u2297ph , x\u2297p\u3009, we have:\ny2 = \u3008M2, x\u22972\u3009+ E[ 2] + \u03b72(x), (6) \u03b72(x) = \u3008\u03b2\u22972h \u2212M2, x \u22972\u3009+ 2 \u3008\u03b2h, x\u3009+ ( 2 \u2212 E[ 2]).\nAgain, we have expressed y2 has a linear function of x\u22972 with regression coefficients M2, plus a known bias E[ 2] and noise.1 Importantly, the noise has mean zero; in fact each of the three terms has zero mean by definition of M2 and independence of and h.\nPerforming regression yields a consistent estimate of M2, but still does not identify all the parameters \u03b8. In particular, B is only identified up to rotation: if B = [\u03b21 | \u00b7 \u00b7 \u00b7 | \u03b2k] satisfies B diag(\u03c0)B> = M2 and \u03c0 is uniform, then (BQ) diag(\u03c0)(Q>B>) = M2 for any orthogonal matrix Q.\nLet us now look to the third moment for additional information. We can write y3 as a linear function of x\u22973 with coefficients M3, a known bias 3E[ 2]\u3008M\u03021, x\u3009+ E[ 3] and some noise \u03b73(x):\ny3 = \u3008M3, x\u22973\u3009+ 3E[ 2]\u3008M\u03021, x\u3009+ E[ 3] + \u03b73(x), \u03b73(x) = \u3008\u03b2\u22973h \u2212M3, x \u22973\u3009+ 3 \u3008\u03b2\u22972h , x \u22972\u3009 (7)\n+ 3( 2\u3008\u03b2h, x\u3009 \u2212 E[ 2]\u3008M\u03021, x\u3009) + ( 3 \u2212 E[ 3]).\nThe only wrinkle here is that \u03b73(x) does not quite have zero mean. It would if M\u03021 were replaced with M1, but M1 is not available to us. Nonetheless, as M\u03021 concentrates around M1, the noise bias will go to zero.\n1If E[ 2] were not known, we could treat it as another coefficient to be estimated. The coefficients M2 and E[ 2] can be estimated jointly provided that x does not already contain a bias (xj must be non-constant for every j \u2208 [d]).\nAlgorithm 1 Spectral Experts\ninput Datasets Dp = {(x(1), y(1)), \u00b7 \u00b7 \u00b7 , (x(n), y(n))} for p = 1, 2, 3; regularization strengths \u03bb (2) n , \u03bb (3) n ;\nobservation noise moments E[ 2],E[ 3]. output Parameters \u03b8\u0302 = (\u03c0\u0302, [\u03b2\u03021 | \u00b7 \u00b7 \u00b7 | \u03b2\u0302k]). 1: Estimate compound parameters M2,M3 using\nlow-rank regression:\nM\u03021 = arg min M1\n(8)\n1\n2n \u2211 (x,y)\u2208D1 (\u3008M1, x\u3009 \u2212 y)2,\nM\u03022 = arg min M2\n\u03bb(2)n \u2016M2\u2016\u2217+ (9)\n1\n2n \u2211 (x,y)\u2208D2 (\u3008M2, x\u22972\u3009+ E[ 2]\u2212 y2)2,\nM\u03023 = arg min M3\n\u03bb(3)n \u2016M3\u2016\u2217+ (10)\n1\n2n \u2211 (x,y)\u2208D3 (\u3008M3, x\u22973\u3009+ 3E[ 2]\u3008M\u03021, x\u3009+ E[ 3]\u2212 y3)2.\n2: Estimate parameters \u03b8 = (\u03c0,B) using tensor factorization:\n(a) Compute whitening matrix W\u0302 \u2208 Rd\u00d7k (such that W\u0302>M\u03022W\u0302 = I) using SVD.\n(b) Compute eigenvalues {a\u0302h}kh=1 and eigenvectors {v\u0302h}kh=1 of the whitened tensor M\u03023(W\u0302 , W\u0302 , W\u0302 ) \u2208 Rk\u00d7k\u00d7k by using the robust tensor power method.\n(c) Return parameter estimates \u03c0\u0302h = a\u0302 \u22122 h and\n\u03b2\u0302h = (W\u0302 >)\u2020(a\u0302hv\u0302h).\nPerforming this regression yields an estimate of M3. We will see shortly that knowledge of M2 and M3 are sufficient to recover all the parameters.\nNow we are ready to state our full algorithm, which we call Spectral Experts (Algorithm 1). First, we perform three regressions to recover the compound parameters M1 (4), M2 (6), and M3 (7). Since M2 and M3 both only have rank k, we can use nuclear norm regularization (Tomioka et al., 2011; Negahban & Wainwright, 2009) to exploit this low-rank structure and improve our compound parameter estimates. In the algorithm, the regularization strengths \u03bb (2) n and \u03bb (3) n are set to\nc\u221a n\nfor some constant c.\nHaving estimated the compound parameters M1, M2 and M3, it remains to recover the original parame-\nters \u03b8. Anandkumar et al. (2012c) showed that for M2 and M3 of the forms in (5), it is possible to efficiently accomplish this. Specifically, we first compute a whitening matrix W based on the SVD of M2 and use that to construct a tensor T = M3(W,W,W ) whose factors are orthogonal. We can use the robust tensor power method to compute all the eigenvalues and eigenvectors of T , from which it is easy to recover the parameters \u03c0 and {\u03b2h}.\nRelated work In recent years, there has a been a surge of interest in \u201cspectral\u201d methods for learning latent-variable models. One line of work has focused on observable operator models (Hsu et al., 2009; Song et al., 2010; Parikh et al., 2012; Cohen et al., 2012; Balle et al., 2011; Balle & Mohri, 2012) in which a re-parametrization of the true parameters are recovered, which suffices for prediction and density estimation. Another line of work is based on the method of moments and uses eigendecomposition of a certain tensor to recover the parameters (Anandkumar et al., 2012b;a; Hsu et al., 2012; Hsu & Kakade, 2013). Our work extends this second line of work to models that require regression to obtain the desired tensor.\nIn spirit, Spectral Experts bears some resemblance to the unmixing algorithm for estimation of restricted PCFGs (Hsu et al., 2012). In that work, the observations (moments) provided a linear combination over the compound parameters. \u201cUnmixing\u201d involves solving for the compound parameters by inverting a mixing matrix. In this work, each data point (appropriately transformed) provides a different noisy projection of the compound parameters.\nOther work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers). Similar to Spectral Experts, Balle & Mohri (2012) used a two-step approach, where convex optimization is first used to estimate moments (the Hankel matrix in their case), after which these moments are subjected to spectral decomposition. However, these methods are developed in the observable operator framework, whereas we consider parameter estimation.\nThe idea of performing low-rank regression on y2 has been explored in the context of signal recovery from magnitude measurements (Candes et al., 2011; Ohlsson et al., 2012). There, the actual observed response was y2, whereas in our case, we deliberately construct powers y, y2, y3 to identify the underlying parameters."}, {"heading": "4. Theoretical results", "text": "In this section, we provide theoretical guarantees for the Spectral Experts algorithm. Our main result shows that the parameter estimates \u03b8\u0302 converge to \u03b8 at a 1\u221a\nn rate that depends polynomially on the bounds on the parameters, covariates, and noise, as well the k-th smallest singular values of the compound parameters and various covariance matrices.\nTheorem 1 (Convergence of Spectral Experts). Assume each dataset Dp (for p = 1, 2, 3) consists of n i.i.d. points independently drawn from a mixture of linear regressions model with parameter \u03b8\u2217.2 Further, assume \u2016x\u20162 \u2264 R, \u2016\u03b2\u2217h\u20162 \u2264 L for all h \u2208 [k], | | \u2264 S and B is rank k. Let \u03a3p def = E[cvec(x\u2297p)\u22972], and assume \u03a3p 0 for each p \u2208 {1, 2, 3}. Let < 12 . Suppose the number of samples is n = max(n1, n2) where\nn1 = \u2126\n( R12 log(1/\u03b4)\nminp\u2208[3] \u03c3min(\u03a3p)2\n)\nn2 = \u2126\n( \u22122 k2\u03c02max\u2016M2\u2016 1/2 op \u2016M3\u20162opL6S6R12\n\u03c3k(M2)5\u03c3min(\u03a31)2 log(1/\u03b4)\n) .\nIf each regularization strength \u03bb (p) n is set to\n\u0398\n( LpSpR2p\n\u03c3min(\u03a31)2\n\u221a log(1/\u03b4)\nn\n) ,\nfor p \u2208 2, 3, then the parameter estimates \u03b8\u0302 = (\u03c0\u0302, B\u0302) returned by Algorithm 1 (with the columns appropriately permuted) satisfies\n\u2016\u03c0\u0302 \u2212 \u03c0\u2016\u221e \u2264 \u2016\u03b2\u0302h \u2212 \u03b2h\u20162 \u2264\nfor all h \u2208 [k].\nWhile the dependence on some of the norms (L6, S6, R12) looks formidable, it is in some sense unavoidable, since we need to perform regression on third-order moments. Classically, the number of samples required is squared norm of the covariance matrix, which itself is bounded by the squared norm of the data, R3. This third-order dependence also shows up in the regularization strengths; the cubic terms bound each of 3, \u03b23h and \u2016(x\u22973)\u22972\u2016F with high probability.\nThe proof of the theorem has two parts. First, we bound the error in the compound parameters estimates M\u03022, M\u03023 using results from Tomioka et al. (2011). Then we use results from Anandkumar et al. (2012c) to convert this error into a bound on the actual parameter estimates \u03b8\u0302 = (\u03c0\u0302, B\u0302) derived from the robust tensor power method. But first, let us study a more basic property: identifiability.\n2Having three independent copies simplifies the analysis."}, {"heading": "4.1. Identifiability from moments", "text": "In ordinary linear regression, the regression coefficients \u03b2 \u2208 Rd are identifiable if and only if the data has full rank: E[x\u22972] 0, and furthermore, identifying \u03b2 requires only moments E[xy] and E[x\u22972] (by observing the optimality conditions for (4)). However, in mixture of linear regressions, these two moments only allow us to recover M1. Theorem 1 shows that if we have the higher order analogues, E[x\u2297py\u2297p] and E[x\u22972p] for p \u2208 {1, 2, 3}, we can then identify the parameters \u03b8 = (\u03c0,B), provided the following identifiability condition holds: E[cvec(x\u2297p)\u22972] 0 for p \u2208 {1, 2, 3}.\nThis identifiability condition warrants a little care, as we can run into trouble when components of x are dependent on each other in a particular algebraic way. For example, suppose x = (1, t, t2), the common polynomial basis expansion, so that all the coordinates are deterministically related. While E[x\u22972] 0 might be satisfied (sufficient for ordinary linear regression), E[cvec(x\u22972)\u22972] is singular for any data distribution. To see this, note that cvec(x\u22972) = [1 \u00b7 1, t \u00b7 t, 2(1 \u00b7 t2), 2(t\u00b7t2), (t2\u00b7t2)] contains components t\u00b7t and 2(1\u00b7t2), which are linearly dependent. Therefore, Spectral Experts would not be able to identify the parameters of a mixture of linear regressions for this data distribution.\nWe can show that some amount of unidentifiability is intrinsic to estimation from low-order moments, not just an artefact of our estimation procedure. Suppose x = (t, . . . , td). Even if we observed all moments E[x\u2297py\u2297p] and E[x\u22972p] for p \u2208 [r] for some r, all the resulting coordinates would be monomials of t up to only degree 2dr, and thus the moments live in a 2drdimensional subspace. On the other hand, the parameters \u03b8 live in a subspace of at least dimension dk. Therefore, at least r \u2265 k/2 moments are required for identifiability of any algorithm for this monomial example."}, {"heading": "4.2. Analysis of low-rank regression", "text": "In this section, we will bound the error of the compound parameter estimates \u2016\u22062\u20162F and \u2016\u22063\u20162F , where \u22062 def = M\u03022 \u2212 M2 and \u22063 def = M\u03023 \u2212 M3. Our analysis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Negahban & Wainwright (2009) for matrices. The main calculation involved is controlling the noise \u03b7p(x), which involves various polynomial combinations of the mixing noise and observation noise.\nLet us first establish some notation that unifies the three regressions ((8), (9), and (10)). Define the observation operator Xp(Mp) : Rd \u2297p \u2192 Rn mapping com-\npound parameters Mp:\nXp(Mp;D)i def = \u3008Mp, x\u2297pi \u3009, (xi, yi) \u2208 D. (11)\nLet \u03ba(Xp) be the restricted strong convexity constant, and let X\u2217p(\u03b7p;D) = \u2211 (x,y)\u2208D \u03b7p(x)x \u2297p be the adjoint.\nLemma 1 (Tomioka et al. (2011), Theorem 1). Suppose there exists a restricted strong convexity constant \u03ba(Xp) such that\n1 n \u2016Xp(\u2206)\u201622 \u2265 \u03ba(Xp)\u2016\u2206\u20162F and \u03bb(p)n \u2265 2\u2016X\u2217p(\u03b7p)\u2016op n .\nThen the error of M\u0302p is bounded as follows:\n\u2016M\u0302p \u2212Mp\u2016F \u2264 32\u03bb\n(p) n \u221a k\n\u03ba(Xp) .\nGoing forward, we need to lower bound the restricted strong convexity constant \u03ba(Xp) and upper bound the operator norm of the adjoint operator \u2016X\u2217p(\u03b7p)\u2016op. The proofs of the following lemmas follow from standard concentration inequalities and are detailed in Appendix A.\nLemma 2 (lower bound on restricted strong convexity constant). If\nn = \u2126 ( max p\u2208[3] R4p(p!)2 log(1/\u03b4) \u03c3min(\u03a3p)2 ) ,\nthen with probability at least 1\u2212 \u03b4:\n\u03ba(Xp) \u2265 \u03c3min(\u03a3p)\n2 ,\nfor each p \u2208 [3]. Lemma 3 (upper bound on adjoint operator). If\nn = \u2126 max p\u2208[3] L2pS2pR4p log(1/\u03b4)\n\u03c3min(\u03a31)2 ( \u03bb (p) n\n)2  ,\nthen with probability at least 1\u2212 \u03b4:\n\u03bb(p)n \u2265 1 n \u2016X\u2217p(\u03b7p)\u2016op,\nfor each p \u2208 [3]."}, {"heading": "4.3. Analysis of the tensor factorization", "text": "Having bounded the error of the compound parameter estimates M\u03022 and M\u03023, we will now study how this error propagates through the tensor factorization step of Algorithm 1, which includes whitening, applying the robust tensor power method (Anandkumar et al., 2012c), and unwhitening.\nLemma 4. Let M3 = \u2211k h=1 \u03c0h\u03b2 \u22973 h . Let \u2016M\u03022\u2212M2\u2016op and \u2016M\u03023 \u2212M3\u2016op both be less than\n\u03c3k(M2) 5/2\nk\u03c0max\u2016M2\u20161/2op \u2016M3\u2016op ,\nfor some < 12 . Then, there exists a permutation of indices such that the parameter estimates found in step 2 of Algorithm 1 satisfy the following with probability at least 1\u2212 \u03b4:\n\u2016\u03c0\u0302 \u2212 \u03c0\u2016\u221e \u2264\n\u2016\u03b2\u0302h \u2212 \u03b2h\u20162 \u2264 .\nfor all h \u2208 [k].\nThe proof follows by applying standard matrix perturbation results for the whitening and unwhitening operators and can be found in Appendix B."}, {"heading": "4.4. Synthesis", "text": "Together, these lemmas allow us to control the compound parameter error and the recovery error. We now apply them in the proof of Theorem 1:\nProof of Theorem 1 (sketch). By Lemma 1, Lemma 2 and Lemma 3, we can control the Frobenius norm of the error in the moments, which directly upper bounds the operator norm: If n \u2265 max{n1, n2}, then\n\u2016M\u0302p \u2212Mp\u2016op = O ( \u03bb(p)n \u221a k\u03c3min(\u03a3p) \u22121 ) . (12)\nWe complete the proof by applying Lemma 4 with the above bound on \u2016M\u0302p \u2212Mp\u2016op."}, {"heading": "5. Empirical evaluation", "text": "In the previous section, we showed that Spectral Experts provides a consistent estimator. In this section, we explore the empirical properties of our algorithm on simulated data. Our main finding is that Spectral Experts alone attains higher parameter error than EM, but this is not the complete story. If we initialize EM with the estimates returned by Spectral Experts, then we end up with much better estimates than EM from a random initialization."}, {"heading": "5.1. Experimental setup", "text": "Algorithms We experimented with three algorithms. The first algorithm (Spectral) is simply the Spectral Experts. We set the regularization strengths\n\u03bb (2) n = 1 105 \u221a n and \u03bb (3) n = 1 103 \u221a n ; the algorithm was not very sensitive to these choices. We solved the low-rank regression to estimate M2 and M3 using an off-the-shelf convex optimizer, CVX (Grant & Boyd, 2012). The second algorithm (EM) is EM where the \u03b2\u2019s are initialized from a standard normal and \u03c0 was set to the uniform distribution plus some small perturbations. We ran EM for 1000 iterations. In the final algorithm (Spectral+EM), we initialized EM with the output of Spectral Experts.\nData We generated synthetic data as follows: First, we generated a vector t sampled uniformly over the b-dimensional unit hypercube [\u22121, 1]b. Then, to get the actual covariates x, we applied a non-linear function of t that conformed to the identifiability criteria discussed in Section 3. The true regression coefficients {\u03b2h} were drawn from a standard normal and \u03c0 is set to the uniform distribution. The observation noise is drawn from a normal with variance \u03c32. Results are presented below for \u03c32 = 0.1, but we did not observe any qualitatively different behavior for choices of \u03c32 in the range [0.01, 0.4].\nAs an example, one feature map we considered in the one-dimensional setting (b = 1) was x = (1, t, t4, t7). The data and the curves fit using Spectral Experts, EM with random initialization and EM initialized with the parameters recovered using Spectral Experts are shown in Figure 2. We note that even on wellseparated data such as this, EM converged to the correct basin of attraction only 13% of the time.\nTable 1. Parameter error \u2016\u03b8\u2217\u2212 \u03b8\u0302\u2016F (n = 500, 000) as the number of base variables b, number of features d and the number of components k increases. While Spectral by itself does not produce good parameter estimates, Spectral+EM improves over EM significantly.\nVariables (b) Features (d) Components (k) Spectral EM Spectral + EM\n1 4 2 2.45 \u00b1 3.68 0.28 \u00b1 0.82 0.17 \u00b1 0.57 2 5 2 1.38 \u00b1 0.84 0.00 \u00b1 0.00 0.00 \u00b1 0.00 2 5 3 2.92 \u00b1 1.71 0.43 \u00b1 1.07 0.31 \u00b1 1.02 2 6 2 2.33 \u00b1 0.67 0.63 \u00b1 1.29 0.01 \u00b1 0.01\n103 104 105 106 107 n\n0\n1\n2\n3\n4\n5\n6\n7\n8\nPa ra\nm et\ner E\nrr or\nEM Spectral Spectral+EM\n(a) Well-specified data\n102 103 104 105 106 n\n0\n1\n2\n3\n4\n5\nPa ra\nm et\ner E\nrr or\nEM Spectral Spectral+EM\n(b) Misspecified data\nFigure 3. Learning curves: parameter error as a function of the number of samples n (b = 1, d = 5, k = 3)."}, {"heading": "5.2. Results", "text": "Table 1 presents the Frobenius norm of the difference between true and estimated parameters for the model, averaged over 20 different random instances for each feature set and 10 attempts for each instance. The experiments were run using n = 500, 000 samples.\nOne of the main reasons for the high variance is the variation across random instances; some are easy for EM to find the global minima and others more difficult. In general, while Spectral Experts did not recover parameters by itself extremely well, it provided a good initialization for EM.\nTo study the stability of the solutions returned by Spectral Experts, consider the histogram in Figure 1, which shows the recovery errors of the algorithms over 170 attempts on a dataset with b = 1, d = 4, k = 3. Typically, Spectral Experts returned a stable solution. When these parameters were close enough to the true parameters, we found that EM almost always converged to the global optima. Randomly initialized EM only finds the true parameters a little over 10% of the time and shows considerably higher variance.\nEffect of number of data points In Figure 3, we show how the recovery error varies as we get more data. Each data point shows the mean error over 10 attempts, with error bars. We note that the recovery performance of EM does not particularly improve; this suggests that EM continues to get stuck in a local optima. The spectral algorithm\u2019s error decays slowly, and as it gets closer to zero, EM initialized at the spectral parameters finds the true parameters more often as well. This behavior highlights the trade-off between statistical and computational error.\nMisspecified data To evaluate how robust the algorithm was to model mis-specification, we removed large contiguous sections from x \u2208 [\u22120.5,\u22120.25] \u222a [0.25, 0.5] and ran the algorithms again. Table 2 reports recovery errors in this scenario. The error in the estimates grows larger for higher d."}, {"heading": "6. Conclusion", "text": "In this paper, we developed a computationally efficient and statistically consistent estimator for mixture of linear regressions. Our algorithm, Spectral Experts, regresses on higher-order powers of the data with a regularizer that encourages low rank structure, followed by tensor factorization to recover the actual parameters. Empirically, we found Spectral Experts to be an excellent initializer for EM.\nAcknowledgements We would like to thank Lester Mackey for his fruitful suggestions and the anonymous reviewers for their helpful comments."}, {"heading": "A. Proofs: Regression", "text": "Let us review the regression problem set up in Section 3. We assume we are given data (xi, yi) \u2208 Dp generated by the following process,\nyi = \u3008Mp, x\u2297pi \u3009+ bp + \u03b7p(xi),\nwhere Mp = \u2211k h=1 \u03c0h\u03b2 \u2297p h , the expected value of \u03b2 \u2297p h , bp is an estimable bias and \u03b7p(x) is zero mean noise. In particular, for p \u2208 {1, 2, 3}, we showed that bp and \u03b7p(x) were,\nb1 = 0 b2 = E[ 2]\nb3 = 2E[ ]\u3008M\u03021, x\u3009+ E[ 3] \u03b71(x) = \u3008\u03b2h \u2212M1, x\u3009+ (13) \u03b72(x) = \u3008\u03b2\u22972h \u2212M2, x\n\u22972\u3009+ 2 \u3008\u03b2h, x\u3009+ ( 2 \u2212 E[ 2]) (14) \u03b73(x) = \u3008\u03b2\u22973h \u2212M3, x \u22973\u3009+ 3 \u3008\u03b2\u22972h , x \u22972\u3009+ 3( 2\u3008\u03b2h, x\u3009 \u2212 E[ 2]\u3008M1, x\u3009) + ( 3 \u2212 E[ 3]). (15)\nWe then defined the observation operator Xp(Mp) : Rd \u2297p \u2192 Rn,\nXp(Mp;Dp)i def = \u3008Mp, x\u2297pi \u3009,\nfor (xi, yi) \u2208 Dp. This let us succinctly represent the low-rank regression problem for p = 2, 3 as follows,\nM\u0302p = arg min Mp\u2208Rd\u2297p\n1\n2n \u2016y \u2212 bp \u2212 Xp(Mp;Dp)\u201622 + \u03bbp\u2016Mp\u2016\u2217.\nLet us also recall the adjoint of the observation operator, X\u2217p : Rn \u2192 Rd p ,\nX\u2217p(\u03b7p;Dp) = \u2211 x\u2208Dp \u03b7p(x)x \u2297p,\nwhere we have used \u03b7p to represent the vector [\u03b7p(x)]x\u2208Dp .\nTomioka et al. (2011) showed that error in the estimated M\u0302p can be bounded as follows;\nLemma 1 (Tomioka et al. (2011), Theorem 1). Suppose there exists a restricted strong convexity constant \u03ba(Xp) such that\n1\n2n \u2016Xp(\u2206)\u201622 \u2265 \u03ba(Xp)\u2016\u2206\u20162F and \u03bbn \u2265 \u2016X\u2217p(\u03b7p)\u2016op n .\nThen the error of M\u0302p is bounded as follows: \u2016M\u0302p \u2212M\u2217p \u2016F \u2264 \u03bbn \u221a k\n\u03ba(Xp) .\nIn this section, we will derive an upper bound on \u03ba(Xp) and a lower bound on 1 n\u2016X \u2217 p(\u03b7p)\u2016op, allowing us to apply Lemma 1 in the particular context of our noise setting.\nLemma 2 (Lower bound on restricted strong convexity). Let \u03a3p def = E[cvec(x\u2297p)\u22972]. If\nn \u2265 16(p!) 2R4p\n\u03c3min(\u03a3p)2\n( 1 + \u221a log(1/\u03b4)\n2\n)2 ,\nthen, with probability at least 1\u2212 \u03b4,\n\u03ba(Xp) \u2265 \u03c3min(\u03a3p)\n2 .\nProof. Recall that \u03ba(Xp) is defined to be a constant such that the following inequality holds,\n1 n \u2016Xp(\u2206)\u201622 \u2265 \u03ba(Xp)\u2016\u2206\u20162F ,\nwhere\n\u2016Xp(\u2206)\u201622 = \u2211\n(x,y)\u2208Dp\n\u3008\u2206, x\u2297p\u30092.\nTo proceed, we will unfold the tensors \u2206 and x\u2297p to get a lower bound in terms of \u2016\u2206\u20162F . This will allow us to choose an appropriate value for \u03ba(Xp) that will hold with high probability.\nFirst, note that x\u2297p is symmetric, and thus \u3008\u2206, x\u2297p\u3009 = \u3008cvec \u2206, cvecx\u2297p\u3009. This allows us to simplify \u2016Xp(\u2206)\u201622 as follows,\n1 n \u2016Xp(\u2206)\u201622 = 1 n \u2211 (x,y)\u2208Dp \u3008\u2206, x\u2297p\u30092\n= 1\nn \u2211 (x,y)\u2208Dp \u3008cvec(\u2206), cvec(x\u2297p)\u30092\n= 1\nn \u2211 (x,y)\u2208Dp tr(cvec(\u2206)\u22972 cvec(x\u2297p)\u22972)\n= tr cvec(\u2206)\u22972 1 n \u2211 (x,y)\u2208Dp cvec(x\u2297p)\u22972  . Let \u03a3\u0302p def = 1n \u2211 (x,y)\u2208Dp cvec(x \u2297p)\u22972, so that 1n\u2016Xp(\u2206)\u2016 2 2 = tr(cvec(\u2206)\n\u22972\u03a3\u0302p). For symmetric \u2206 , \u2016 cvec(\u2206)\u20162 = \u2016\u2206\u2016F 3. Then, we have\n1 n \u2016Xp(\u2206)\u201622 = tr(cvec(\u2206)\u22972\u03a3\u0302p)\n\u2265 \u03c3min(\u03a3\u0302p)\u2016\u2206\u20162F .\nBy Weyl\u2019s theorem,\n\u03c3min(\u03a3\u0302p) \u2265 \u03c3min(\u03a3p)\u2212 \u2016\u03a3\u0302p \u2212 \u03a3p\u2016op.\nSince \u2016\u03a3\u0302p \u2212 \u03a3p\u2016op \u2264 \u2016\u03a3\u0302p \u2212 \u03a3p\u2016F , it suffices to show that the empirical covariance concentrates in Frobenius norm. Applying Lemma 5, with probability at least 1\u2212 \u03b4,\n\u2016\u03a3\u0302p \u2212 \u03a3p\u2016F \u2264 2\u2016\u03a3p\u2016F\u221a\nn\n( 1 + \u221a log(1/\u03b4)\n2\n) .\nNow we seek to control \u2016\u03a3p\u2016F . Since \u2016x\u20162 \u2264 R, we can use the bound\n\u2016\u03a3p\u2016F \u2264 p!\u2016 vec(x\u2297p)\u22972\u2016F \u2264 p!R2p.\nFinally, \u2016\u03a3\u0302p \u2212 \u03a3p\u2016op \u2264 \u03c3min(\u03a3p)/2 with probability at least 1\u2212 \u03b4 if,\nn \u2265 16(p!) 2R4p\n\u03c3min(\u03a3p)2\n( 1 + \u221a log(1/\u03b4)\n2\n)2 .\n3 The \u2206 correspond to residuals M\u0302p \u2212Mp, which can easily be shown to always be symmetric.\nLemma 3 (Upper bound on adjoint operator). With probability at least 1\u2212 \u03b4, the following holds,\n1 n \u2016X\u22171(\u03b71)\u2016op \u2264 2 R(2LR+ S)\u221a n\n( 1 + \u221a log(3/\u03b4)\n2 ) 1 n \u2016X\u22172(\u03b72)\u2016op \u2264 2 (4L2R2 + 2SLR+ 4S2)R2\u221a n ( 1 + \u221a log(3/\u03b4) 2\n) 1 n \u2016X\u22173(\u03b73)\u2016op \u2264 2 (8L3R3 + 3L2R2S + 6LRS2 + 2S3)R3\u221a n ( 1 + \u221a log(6/\u03b4) 2 )\n+ 3R4S2\n( 128R(2LR+ S)\n\u03c3min(\u03a31) \u221a n\n( 1 + \u221a log(6/\u03b4)\n2\n)) .\nIt follows that, with probability at least 1\u2212 \u03b4,\n1 n \u2016X\u2217p(\u03b7p)\u2016op = O\n( LpSpR2p\u03c3min(\u03a31) \u22121 \u221a log(1/\u03b4)\nn\n) ,\nfor each p \u2208 {1, 2, 3}.\nProof. Let E\u0302p[f(x, , h)] denote the empirical expectation over the examples in dataset Dp (recall the Dp\u2019s are independent to simplify the analysis). By definition,\n1 n \u2016X\u2217p(\u03b7p)\u2016op = \u2225\u2225\u2225E\u0302p [\u03b7p(x)x\u2297p]\u2225\u2225\u2225 op\nfor p \u2208 {1, 2, 3}. To proceed, we will bound each \u03b7p(x) and use Lemma 5 to bound \u2016E\u0302p[\u03b7p(x)x\u2297p]\u2016F . The Frobenius norm to bounds the operator norm, completing the proof.\nBounding \u03b7p(x). Using the assumptions that \u2016\u03b2h\u20162 \u2264 L, \u2016x\u20162 \u2264 R and | | \u2264 S, it is easy to bound each \u03b7p(x),\n\u03b71(x) = \u3008\u03b2h \u2212M1, x\u3009+ \u2264 \u2016\u03b2h \u2212M1\u20162\u2016x\u20162 + | | \u2264 2LR+ S \u03b72(x) = \u3008\u03b2\u22972h \u2212M2, x \u22972\u3009+ 2 \u3008\u03b2h, x\u3009+ ( 2 \u2212 E[ 2])\n\u2264 \u2016\u03b2\u22972h \u2212M2\u2016F \u2016x \u22972\u2016F + 2| |\u2016\u03b2h\u20162\u2016x\u20162 + | 2 \u2212 E[ 2]| \u2264 (2L)2R2 + 2SLR+ (2S)2\n\u03b73(x) = \u3008\u03b2\u22973h \u2212M3, x \u22973\u3009+ 3 \u3008\u03b2\u22972h , x \u22972\u3009 + 3 ( 2\u3008\u03b2h, x\u3009 \u2212 E[ 2]\u3008M\u03021, x\u3009 ) + ( 3 \u2212 E[ 3])\n\u2264 \u2016\u03b2\u22973h \u2212M3\u2016F \u2016x \u22973\u2016F + 3| |\u2016\u03b2\u22972h \u2016F \u2016x \u22972\u2016F + 3 ( | 2| \u2016\u03b2h\u2016F \u2016x\u2016F + \u2223\u2223E[ 2]\u2223\u2223 \u2016M\u03021\u20162\u2016x\u20162)+ | 3|+ \u2223\u2223E[ 3]\u2223\u2223 \u2264 (2L)3R3 + 3SL2R2 + 3(S2LR+ S2LR) + 2S3.\nWe have used inequality \u2016M1 \u2212 \u03b2h\u20162 \u2264 2L above.\nSpectral Experts Bounding \u2225\u2225\u2225E\u0302[\u03b7p(x)x\u2297p]\u2225\u2225\u2225\nF . We may now apply the above bounds on \u03b7p(x) to bound \u2016\u03b7p(x)x\u2297p\u2016F , using the\nfact that \u2016cX\u2016F \u2264 c\u2016X\u2016F . By Lemma 5, each of the following holds with probability at least 1\u2212 \u03b41,\u2225\u2225\u2225E\u03021[\u03b71(x)x]\u2225\u2225\u2225 2 \u2264 2R(2LR+ S)\u221a n ( 1 + \u221a log(1/\u03b41) 2 ) \u2225\u2225\u2225E\u03022[\u03b72(x)x\u22972]\u2225\u2225\u2225\nF \u2264 2(4L 2R2 + 2SLR+ 4S2)R2\u221a n\n( 1 + \u221a log(1/\u03b42)\n2 ) \u2225\u2225\u2225E\u03023[\u03b73(x)x\u22973]\u2212 E[\u03b73(x)x\u22973 | x]\u2225\u2225\u2225\nF \u2264 2(8L 3R3 + 3L2R2S + 6LRS2 + 2S3)R3\u221a n\n( 1 + \u221a log(1/\u03b43)\n2\n) .\nRecall that \u03b73(x) does not have zero mean, so we must bound the bias:\n\u2016E[\u03b73(x)x\u22973 | x]\u2016F = \u20163E[ 2]\u3008M1 \u2212 M\u03021, x\u3009x\u22973\u2016F \u2264 3E[ 2]\u2016M1 \u2212 M\u03021\u20162\u2016x\u20162\u2016x\u22973\u2016F .\nNote that in all of this, both M\u03021 and M1 are treated as constants. Further, by applying Lemma 1 to M1, we have a bound on \u2016M1 \u2212 M\u03021\u20162; with probability at least 1\u2212 \u03b43,\n\u2016M1 \u2212 M\u03021\u20162 \u2264 32\u03bb\n(1) n \u03ba(X1)\n\u2264 322R(2LR+ S)\u221a n\n( 1 + \u221a log(1/\u03b43)\n2\n) 2\n\u03c3min(\u03a31) .\nSo, with probability at least 1\u2212 \u03b43, \u2016E[\u03b73(x)x\u22973 | x]\u2016F \u2264 3R4S2 ( 128R(2LR+ S)\n\u03c3min(\u03a31) \u221a n\n( 1 + \u221a log(1/\u03b43)\n2\n)) .\nFinally, taking \u03b41 = \u03b4/3, \u03b42 = \u03b4/3, \u03b43 = \u03b4/6, and taking the union bound over the bounds for p \u2208 {1, 2, 3}, we get our result."}, {"heading": "B. Proofs: Tensor Decomposition", "text": "Once we have estimated the moments from the data through regression, we apply the robust tensor eigendecomposition algorithm to recover the parameters, \u03b2h and \u03c0. However, the algorithm is guaranteed to work only for symmetric matrices with (nearly) orthogonal eigenvectors, so, as a first step, we will need to whiten the third-order moment tensor using the second moments. We then apply the tensor decomposition algorithm to get the eigenvalues and eigenvectors. Finally, we will have to undo the transformation by applying an un-whitening step. In this section, we present error bounds for each step, and combine them to prove the following lemma,\nLemma 4 (Tensor Decomposition with Whitening). Let M2 = \u2211k h=1 \u03c0h\u03b2 \u22972 h , M3 = \u2211k h=1 \u03c0h\u03b2 \u22973 h . Let \u03b5M2 def = \u2016M\u03022 \u2212M2\u2016op and \u03b5M3 def = \u2016M\u03023 \u2212M3\u2016op both be such that,\nmax{\u03b5M2 , \u03b5M3} \u2264 min\n{ \u03c3k(M2)\n2 ,\n15k\u03c05/2max ( 24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2 )\n2\u03c3k(M2)3/2 \u22121 , (\n4 \u221a 3/2\u2016M2\u20161/2op \u03c3k(M2)\u22121 + 8k\u03c0max\u2016M2\u20161/2op \u03c3k(M2)\u22123/2 (\n24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2\n))\u22121 }\nfor some < 12\u221a\u03c0max .\nThen, there exists a permutation of indices such that the parameter estimates found in step 2 of Algorithm 1 satisfy the following with probability at least 1\u2212 \u03b4,\n\u2016\u03c0\u0302 \u2212 \u03c0\u2016\u221e \u2264\n\u2016\u03b2\u0302h \u2212 \u03b2h\u20162 \u2264 . for all h \u2208 [k].\nProof. We will use the general notation, \u03b5X def = \u2016X\u0302 \u2212X\u2016op to represent the error of the estimate, X\u0302, of X in the operator norm.\nThrough the course of the proof, we will make some assumptions on errors that allow us simplify portions of the expressions. At the end of the proof, we will collect these conditions together to state the assumptions on above.\nStep 1: Whitening Much of this matter has been presented in Hsu & Kakade (2013, Lemma 11, 12). We present our own version for completeness.\nLet W and W\u0302 be the whitening matrices for M2 and M\u03022 respectively. Also define W \u2020 and W\u0302 \u2020 to be their pseudo-inverses.\nWe will first show that the whitened tensor T = M3(W,W,W ) is symmetric with orthogonal eigenvectors. Recall that M2 = \u2211 h \u03c0h\u03b2 \u22972 h , so,\nI = WTM2W = \u2211 h \u03c0hW T\u03b2\u22972h W\n= \u2211 h ( \u221a \u03c0hW T\u03b2h\ufe38 \ufe37\ufe37 \ufe38 vh )\u22972.\nThus W\u03b2h = vh\u221a \u03c0h , where vh form an orthonormal basis. Applying the same whitening transform to M3, we get, M3 = \u2211 h \u03c0h\u03b2 \u22973 h\nM3(W,W,W ) = \u2211 h \u03c0h(W T\u03b2h) \u22973\n= \u2211 h 1 \u221a \u03c0h v\u22973h .\nConsequently, T has an orthogonal decomposition with eigenvectors vh and eigenvalues 1/ \u221a \u03c0h.\nLet us now study how far T\u0302 = M\u03023(W\u0302 , W\u0302 , W\u0302 ) differs from T , in terms of the errors of M2 and M3, following Anandkumar et al. (2012c). We note that while T\u0302 is also symmetric, it may not have an orthogonal decomposition. To do so, we use the triangle inequality to break the difference into a number of simple terms, that differ in exactly one element. We will then apply \u2016M3(W,W,W \u2212 W\u0302 )\u2016op \u2264 \u2016M3\u2016op\u2016W\u2016op\u2016W\u2016op\u2016W \u2212 W\u0302\u2016op.\n\u03b5T = \u2016M3(W,W,W )\u2212 M\u03023(W\u0302 , W\u0302 , W\u0302 )\u2016op \u2264 \u2016M3(W,W,W )\u2212M3(W,W, W\u0302 )\u2016op + \u2016M3(W,W, W\u0302 )\u2212M3(W, W\u0302 , W\u0302 )\u2016op\n+ \u2016M3(W, W\u0302 , W\u0302 )\u2212M3(W\u0302 , W\u0302 , W\u0302 )\u2016op + \u2016M3(W\u0302 , W\u0302 , W\u0302 )\u2212 M\u03023(W\u0302 , W\u0302 , W\u0302 )\u2016op \u2264 \u2016M3(W,W,W \u2212 W\u0302 )\u2016op + \u2016M3(W,W \u2212 W\u0302 , W\u0302 )|op + \u2016M3(W \u2212 W\u0302 , W\u0302 , W\u0302 )|op \u2264 \u2016M3\u2016op\u2016W\u20162op\u03b5W + \u2016M3\u2016op\u2016W\u0302\u2016op\u2016W\u2016op\u03b5W + \u2016M3\u2016op\u2016W\u0302\u20162op\u03b5W + \u03b5M3\u2016W\u0302\u20163op \u2264 \u2016M3\u2016op(\u2016W\u20162op + \u2016W\u0302\u2016op\u2016W\u2016op + \u2016W\u0302\u20162op)\u03b5W + \u03b5M3\u2016W\u0302\u20163op\nWe can relate \u2016W\u0302\u2016 and \u03b5W to \u03b5M2 using Lemma 6, for which we need the following condition,\nCondition 1. Let \u03b5M2 < \u03c3k(M2)/3.\nThen,\n\u2016W\u0302\u2016op \u2264 \u03c3k(M2) \u22121/2\u221a 1\u2212 \u03b5M2\u03c3k(M2)\n\u2264 \u221a 2\u03c3k(M2) \u22121/2\n\u03b5W \u2264 2\u03c3k(M2)\u22121/2 \u03b5M2 \u03c3k(M2)\n1\u2212 \u03b5M2\u03c3k(M2) \u2264 4\u03c3k(M2)\u22123/2\u03b5M2 .\nThus,\n\u03b5T \u2264 6\u2016M3\u2016op\u2016W\u20162op(4\u03c3k(M2)\u22123/2)\u03b5M2 + \u03b5M32 \u221a\n2\u2016W\u20163op \u2264 24\u2016M3\u2016op\u03c3k(M2)\u22125/2\u03b5M2 + 2 \u221a 2\u03c3k(M2) \u22123/2\u03b5M3\n\u2264 \u03c3k(M2)\u22123/2 (\n24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2\n) max{\u03b5M2 , \u03b5M3}.\nStep 2: Decomposition We have constructed T to be a symmetric tensor with orthogonal eigenvectors. We can now apply the results of Anandkumar et al. (2012c, Theorem 5.1) to bound the error in the eigenvalues, \u03bbW , and eigenvectors, \u03c9, returned by the robust tensor power method;\n\u2016\u03bbW \u2212 \u03bb\u0302W \u2016\u221e \u2264 5k\u03b5T\n(\u03bbW )min\n\u2016\u03c9h \u2212 \u03c9\u0302h\u20162 \u2264 8k\u03b5T\n(\u03bbW )2min ,\nfor all h \u2208 [k], where (\u03bbW )min is the smallest eigenvalue of T .\nStep 3: Unwhitening Finally, we need to invert the whitening transformation to recover \u03c0 and \u03b2h from \u03bbW and \u03c9h. Let us complete the proof by studying how this inversion relates the error in \u03c0 and \u03b2 to the error in \u03bbW and \u03c9.\nFirst, we will bound the error in the \u03b2s,\n\u2016\u03b2\u0302h \u2212 \u03b2h\u20162 = \u2016W\u0302 \u2020\u03c9\u0302 \u2212W \u2020\u03c9\u20162 \u2264 \u03b5W \u2020\u2016\u03c9\u0302h\u20162 + \u2016W \u2020\u20162\u2016\u03c9\u0302h \u2212 \u03c9h\u20162. (Triangle inequality)\nOnce more, we can apply the results of Lemma 6,\n\u2016W\u0302 \u2020\u2016op \u2264 \u221a \u03c31(M2) \u221a 1 +\n\u03b5M2 \u03c3k(M2)\n\u03b5W \u2020 \u2264 2 \u221a \u03c31(M2) (\u221a 1 +\n\u03b5M2 \u03c31(M2)\n) \u03b5M2 \u03c3k(M2)\n1\u2212 \u03b5M2\u03c3k(M2) .\nUsing Condition 1, this simplifies to, \u2016W\u0302 \u2020\u2016op \u2264 \u221a\n3/2\u2016M2\u20161/2op \u03b5W \u2020 \u2264 4 \u221a 3/2\u2016M2\u20161/2op \u03c3k(M2)\u22121\u03b5M2 .\nThus,\n\u2016\u03b2\u0302h \u2212 \u03b2h\u20162 \u2264 4 \u221a 3/2\u2016M2\u20161/2op \u03c3k(M2)\u22121\u03b5M2 + 8\u2016M2\u20161/2op k\u03b5T\n(\u03bbW )2min \u2264 4 \u221a\n3/2\u2016M2\u20161/2op \u03c3k(M2)\u22121\u03b5M2 + 8\u2016M2\u20161/2op k\u03c0max\u03c3k(M2)\u22123/2 (\n24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2\n) max{\u03b5M2 , \u03b5M3}.\nNext, let us bound the error in \u03c0,\n|\u03c0\u0302h \u2212 \u03c0h| = \u2223\u2223\u2223\u2223\u2223 1(\u03bbW )2h \u2212 1(\u03bb\u0302W )2h \u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u2223\u2223 ( (\u03bbW )h + (\u03bb\u0302W )h )( (\u03bbW )h \u2212 (\u03bb\u0302W )h ) (\u03bbW )2h(\u03bb\u0302W ) 2 h \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 (2(\u03bbW )h \u2212 \u2016\u03bbW \u2212 \u03bb\u0302W \u2016\u221e)\n(\u03bbW )2h ( (\u03bbW )h + \u2016\u03bbW \u2212 \u03bb\u0302W \u2016\u221e )2 \u2016\u03bbW \u2212 \u03bb\u0302W \u2016\u221e. To simplify the above expression, we would like that \u2016\u03bbW \u2212 \u03bb\u0302W \u2016\u221e \u2264 (\u03bbW )min/2 or \u2016\u03bbW \u2212 \u03bb\u0302W \u2016\u221e \u2264 12\u221a\u03c0max , recalling that (\u03bbW )h = \u03c0 \u22121/2 h . Thus, we would like to require the following condition to hold on ;\nCondition 2. \u2264 12\u221a\u03c0max .\nNow,\n|\u03c0\u0302h \u2212 \u03c0h| \u2264 (3/2)(\u03bbW )h\n(\u03bbW )4h \u2016\u03bbW \u2212 \u03bb\u0302W \u2016\u221e\n\u2264 3 2(\u03bbW )3h 5k\u03b5T (\u03bbW )2min \u2264 3\u03c0 3/2 max\n2 5k\u03c0max\u03c3k(M2)\n\u22123/2 (\n24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2\n) max{\u03b5M2 , \u03b5M3}\n\u2264 15 2 \u03c05/2maxk\u03c3k(M2)\n\u22123/2 (\n24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2\n) max{\u03b5M2 , \u03b5M3}.\nFinally, we complete the proof by requiring that the bounds \u03b5M2 and \u03b5M3 imply that \u2016\u03c0\u0302 \u2212 \u03c0\u2016\u221e \u2264 and \u2016\u03b2\u0302h \u2212 \u03b2h\u20162 \u2264 , i.e.\nmax{\u03b5M2 , \u03b5M3} \u2264 ( 15\n2 \u03c05/2maxk\u03c3k(M2)\n\u22123/2 (\n24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2\n))\u22121\nmax{\u03b5M2 , \u03b5M3} \u2264 ( 4 \u221a 3/2\u2016M2\u20161/2op \u03c3k(M2)\u22121\u03b5M2 + 8\u2016M2\u20161/2op k\u03c0max\u03c3k(M2)\u22123/2 (\n24 \u2016M3\u2016op \u03c3k(M2) + 2 \u221a 2\n))\u22121 ."}, {"heading": "C. Basic Lemmas", "text": "In this section, we have included some standard results that we employ for completeness. Lemma 5 (Concentration of vector norms). Let X,X1, \u00b7 \u00b7 \u00b7 , Xn \u2208 Rd be i.i.d. samples from some distribution with bounded support (\u2016X\u20162 \u2264M with probability 1). Then with probability at least 1\u2212 \u03b4,\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Xi \u2212 E[X] \u2225\u2225\u2225\u2225\u2225 2 \u2264 2M\u221a n ( 1 + \u221a log(1/\u03b4) 2 ) .\nProof. Define Zi = Xi \u2212 E[X].\nThe quantity we want to bound can be expressed as follows:\nf(Z1, Z2, \u00b7 \u00b7 \u00b7 , Zn) = \u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Zi \u2225\u2225\u2225\u2225\u2225 2 .\nLet us check that f satisfies the bounded differences inequality:\n|f(Z1, \u00b7 \u00b7 \u00b7 , Zi, \u00b7 \u00b7 \u00b7 , Zn)\u2212 f(Z1, \u00b7 \u00b7 \u00b7 , Z \u2032i, \u00b7 \u00b7 \u00b7 , Zn)| \u2264 1\nn \u2016Zi \u2212 Z \u2032i\u20162\n= 1\nn \u2016Xi \u2212X \u2032i\u20162\n\u2264 2M n ,\nby the bounded assumption of Xi and the triangle inequality.\nBy McDiarmid\u2019s inequality, with probability at least 1\u2212 \u03b4, we have: Pr[f \u2212 E[f ] \u2265 ] \u2264 exp (\n\u22122 2\u2211n i=1(2M/n) 2\n) .\nRe-arranging: \u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Zi \u2225\u2225\u2225\u2225\u2225 2 \u2264 E [\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Zi \u2225\u2225\u2225\u2225\u2225 2 ] +M \u221a 2 log(1/\u03b4) n .\nNow it remains to bound E[f ]. By Jensen\u2019s inequality, E[f ] \u2264 \u221a E[f2], so it suffices to bound E[f2]:\nE  1 n2 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 Zi \u2225\u2225\u2225\u2225\u2225 2  = E[ 1 n2 n\u2211 i=1 \u2016Zi\u201622 ] + E  1 n2 \u2211 i 6=j \u3008Zi, Zj\u3009  \u2264 4M 2\nn + 0,\nwhere the cross terms are zero by independence of the Zi\u2019s.\nPutting everything together, we obtain the desired bound:\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Zi \u2225\u2225\u2225\u2225\u2225 \u2264 2M\u221an +M \u221a 2 log(1/\u03b4) n .\nRemark: The above result can be directly applied to the Frobenius norm of a matrix M because \u2016M\u2016F = \u2016 vec(M)\u20162. Lemma 6 (Perturbation Bounds on Whitening Matrices). Let A be a rank-k d\u00d7 d matrix, W\u0302 be a d\u00d7 k matrix that whitens A\u0302, i.e. W\u0302T A\u0302W\u0302 = I. Suppose W\u0302TAW\u0302 = UDUT , then define W = W\u0302UD\u2212 1 2UT . Note that W is also a d\u00d7 k matrix that whitens A. If \u03b1A = \u03b5A\u03c3k(A) < 1 3 then,\n\u2016W\u0302\u2016op \u2264 \u2016W\u2016op\u221a 1\u2212 \u03b1A\n\u2016W\u0302 \u2020\u2016op \u2264 \u2016W \u2020\u2016op \u221a 1 + \u03b1A\n\u03b5W \u2264 \u2016W\u2016op \u03b1A\n1\u2212 \u03b1A \u03b5W \u2020 \u2264 \u2016W \u2020\u2016op \u221a 1 + \u03b1A\n\u03b1A 1\u2212 \u03b1A .\nProof. This lemma has also been proved in Hsu & Kakade (2013, Lemma 10), but we present it differently here for completeness. First, note that for a matrix W that whitens A = V \u03a3V T , W = V \u03a3\u2212 1 2V T and W \u2020 = V \u03a3 1 2V T . Thus, by rotational invariance,\n\u2016W\u2016op = 1\u221a \u03c3k(A)\n\u2016W \u2020\u2016op = \u221a \u03c31(A)\nThis allows us to bound the operator norms of W\u0302 and W\u0302 \u2020 in terms of W and W \u2020,\n\u2016W\u0302\u2016op = 1\u221a \u03c3k(A\u0302)\n\u2264 1\u221a \u03c3k(A)\u2212 \u03b5A\n(By Weyl\u2019s Theorem)\n\u2264 1 1\u2212 \u03b1A 1\u221a \u03c3k(A) = \u2016W\u2016op\u221a 1\u2212 \u03b1A\n\u2016W\u0302 \u2020\u2016op = \u221a \u03c31(A\u0302)\n\u2264 \u221a \u03c31(A) + \u03b5A (By Weyl\u2019s Theorem) \u2264 \u221a 1 + \u03b1A \u221a \u03c31(A) = \u221a 1 + \u03b1A\u2016W \u2020\u2016op.\nTo find \u03b5W , we will exploit the rotational invariance of the operator norm.\n\u03b5W = \u2016W\u0302 \u2212W\u2016op = \u2016WUD 12UT \u2212W\u2016op (W\u0302 = WUD\u2212 1 2UT )\n\u2264 \u2016W\u2016op\u2016I \u2212 UD 1 2UT \u2016op . (Sub-multiplicativity)\nWe will now bound \u2016I \u2212 UD 12UT \u2016op. Note that by rotational invariance, \u2016I \u2212 UD 1 2UT \u2016op = \u2016I \u2212D 1 2 \u2016op. By Weyl\u2019s inequality, |1 \u2212 \u221a Dii| \u2264 \u2016I \u2212D 1 2 \u2016op; put differently, \u2016I \u2212D 1 2 \u2016op bounds the amount \u221a Dii can diverge from 1. Using the property that |(1 + x)\u22121/2 \u2212 1| \u2264 |x| for all |x| \u2264 1/2, we will take an alternate approach and bound Dii separately, and use it to bound \u2016I \u2212D 1 2 \u2016op.\n\u2016I \u2212D\u2016op = \u2016I \u2212 UDUT \u2016op (Rotational invariance) = \u2016W\u0302T A\u0302W\u0302 \u2212 W\u0302TAW\u0302\u2016op (By definition) = \u2016W\u0302T (A\u0302\u2212A)W\u0302\u2016op \u2264 \u2016W\u0302\u20162op\u03b5A\n\u2264 1 \u03c3k(A) \u03b5A 1\u2212 \u03b1A \u2264 \u03b1A 1\u2212 \u03b1A .\nTherefore, if \u03b1A1\u2212\u03b5A < 1/2, or \u03b5A < \u03c3k(A) 3 ,\n\u2016I \u2212 UD1/2UT \u2016op \u2264 \u03b1A\n1\u2212 \u03b1A .\nWe can now complete the proof of the bound on \u03b5W ,\n\u03b5W = \u2016W\u2016op\u2016I \u2212 UD1/2UT \u2016op (Rotational invariance)\n\u2264 \u2016W\u2016op \u03b1A\n1\u2212 \u03b1A .\nSimilarly, we can bound the error on the un-whitening transform, W \u2020,\n\u03b5W \u2020 = \u2016W\u0302 \u2020 \u2212W \u2020\u2016op = \u2016W\u0302 \u2020 \u2212 UD 12UT W\u0302 \u2020\u2016op (W \u2020 = UD1/2UT W\u0302 \u2020)\n\u2264 \u2016W\u0302 \u2020\u2016op\u2016I \u2212 UD 1 2UT \u2016op \u2264 \u2016W \u2020\u2016op \u221a 1 + \u03b1A \u03b1A\n1\u2212 \u03b1A ."}], "references": [{"title": "A method of moments for mixture models and hidden Markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Anandkumar", "Anima", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "CoRR, abs/1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Spectral learning of general weighted automata via constrained matrix completion", "author": ["B. Balle", "M. Mohri"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Balle and Mohri,? \\Q2012\\E", "shortCiteRegEx": "Balle and Mohri", "year": 2012}, {"title": "A spectral learning algorithm for finite state transducers", "author": ["B. Balle", "A. Quattoni", "X. Carreras"], "venue": "In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "Balle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2011}, {"title": "Learning mixtures of spher", "author": ["D. Hsu", "S.M. Kakade"], "venue": "http://cvxr.com/cvx,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Modeling with mix", "author": ["Viele", "Kert", "Tong", "Barbara"], "venue": null, "citeRegEx": "Viele et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Viele et al\\.", "year": 2011}, {"title": "\u03b7p(x)x \u2297p, where we have used \u03b7p to represent the vector [\u03b7p(x)]x\u2208Dp", "author": ["Tomioka"], "venue": null, "citeRegEx": "Tomioka,? \\Q2011\\E", "shortCiteRegEx": "Tomioka", "year": 2011}, {"title": "2012c, Theorem 5.1) to bound the error in the eigenvalues", "author": ["Anandkumar"], "venue": null, "citeRegEx": "Anandkumar,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Anandkumar et al. (2012c) showed that for M2 and M3 of the forms in (5), it is possible to efficiently accomplish this.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "One line of work has focused on observable operator models (Hsu et al., 2009; Song et al., 2010; Parikh et al., 2012; Cohen et al., 2012; Balle et al., 2011; Balle & Mohri, 2012) in which a re-parametrization of the true parameters are recovered, which suffices for prediction and density estimation.", "startOffset": 59, "endOffset": 178}, {"referenceID": 3, "context": "Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers).", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": "Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers).", "startOffset": 66, "endOffset": 177}, {"referenceID": 3, "context": "Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers). Similar to Spectral Experts, Balle & Mohri (2012) used a two-step approach, where convex optimization is first used to estimate moments (the Hankel matrix in their case), after which these moments are subjected to spectral decomposition.", "startOffset": 66, "endOffset": 304}, {"referenceID": 2, "context": "First, we bound the error in the compound parameters estimates M\u03022, M\u03023 using results from Tomioka et al. (2011). Then we use results from Anandkumar et al.", "startOffset": 65, "endOffset": 113}, {"referenceID": 0, "context": "Then we use results from Anandkumar et al. (2012c) to convert this error into a bound on the actual parameter estimates \u03b8\u0302 = (\u03c0\u0302, B\u0302) derived from the robust tensor power method.", "startOffset": 25, "endOffset": 51}, {"referenceID": 4, "context": "In this section, we will bound the error of the compound parameter estimates \u2016\u22062\u2016F and \u2016\u22063\u2016F , where \u22062 def = M\u03022 \u2212 M2 and \u22063 def = M\u03023 \u2212 M3. Our analysis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Negahban & Wainwright (2009) for matrices.", "startOffset": 79, "endOffset": 226}, {"referenceID": 4, "context": "In this section, we will bound the error of the compound parameter estimates \u2016\u22062\u2016F and \u2016\u22063\u2016F , where \u22062 def = M\u03022 \u2212 M2 and \u22063 def = M\u03023 \u2212 M3. Our analysis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Negahban & Wainwright (2009) for matrices.", "startOffset": 79, "endOffset": 284}, {"referenceID": 5, "context": "Lemma 1 (Tomioka et al. (2011), Theorem 1).", "startOffset": 9, "endOffset": 31}], "year": 2013, "abstractText": "Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima. In this paper, we develop a new computationally efficient and provably consistent estimator for a mixture of linear regressions, a simple instance of a discriminative latentvariable model. Our approach relies on a lowrank linear regression to recover a symmetric tensor, which can be factorized into the parameters using a tensor power method. We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM). Last Modified: June 18, 2013", "creator": "LaTeX with hyperref package"}}}