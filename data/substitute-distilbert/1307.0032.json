{"id": "1307.0032", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2013", "title": "Memory Limited, Streaming PCA", "abstract": "we consider streaming, one - pass principal component analysis ( pca ), in the q - dimensional regime, with limited memory. here, $ p $ - dimensional samples are presented sequentially, and the responsibility is to produce the $ k $ - dimensional subspace that best approximates these points. standard algorithms require $ br ( p ^ 2 ) $ memory ; meanwhile no algorithm can cycle fast than $ o ( pg ) $ memory, since this is what the output library requires. memory ( implies storage ) complexity is most meaningful when evaluated in quantitative context of computational and sample complexity. sample complexity for high - dimensional answers is typically desirable in the setting of g { \\ em spiked computation model }, where $ p $ - dimensional surveys are generated from a population image equal to the identity ( initial noise ) by a low - dimensional target ( the r ) p is the signal to was recovered. it is now well - understood that the spike can be recovered once the number of samples, $ n $, scales faster with the dimension, $ p $. yet, all algorithms that provably incorporate this, have memory complexity $ o ( p ^ 2 ) $. meanwhile, algorithms that memory - complexity $ o ( kp ) $ don't have provable assumptions on sample complexity comparable to $ p $. we present numerical algorithm that achieves both : it uses $ hk ( kp ) $ memory ( meaning storage of any kind ) and is able to compute the $ k $ - \\ spike with $ og ( p \\ log p ) $ sample - bandwidth - - the first application of its type. while our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more fuzzy models as the data.", "histories": [["v1", "Fri, 28 Jun 2013 21:38:17 GMT  (46kb)", "http://arxiv.org/abs/1307.0032v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["ioannis mitliagkas", "constantine caramanis", "prateek jain 0002"], "accepted": true, "id": "1307.0032"}, "pdf": {"name": "1307.0032.pdf", "metadata": {"source": "CRF", "title": "Memory Limited, Streaming PCA", "authors": ["Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n00 32\nv1 [\nst at\n.M L\n] 2"}, {"heading": "1 Introduction", "text": "Principal component analysis is a fundamental tool for dimensionality reduction, clustering, classification, and many more learning tasks. It is a basic preprocessing step for learning, recognition, and estimation procedures. The core computational element of PCA is performing a (partial) singular value decomposition, and much work over the last half century has\n\u2217Email: {ioannis,constantine}@utexas.edu;prajain@microsoft.com\nfocused on efficient algorithms (e.g., [7] and references therein) and hence on computational complexity.\nThe recent focus on understanding high-dimensional data, where the dimensionality of the data scales together with the number of available sample points, has led to an exploration of the sample complexity of covariance estimation. This direction was largely influenced by Johnstone\u2019s spiked covariance model, where data samples are drawn from a distribution whose (population) covariance is a low-rank perturbation of the identity matrix [11]. Work initiated there, and also work done in [19] (and references therein) has explored the power of batch PCA in the p-dimensional setting with sub-Gaussian noise, and demonstrated that the singular value decomposition (SVD) of the empirical covariance matrix succeeds in recovering the principal components (extreme eigenvectors of the population covariance) with high probability, given n = O(p) samples.\nThis paper brings the focus on another critical quantity: memory/storage. This is relevant in the so-called streaming data model, where the samples xt \u2208 Rp are collected sequentially, and unless we store them, they are irretrievably gone.1 The only currently available algorithms with provable sample complexity guarantees either store all n = O(p) samples (note that for more than a single pass over the data, the samples must all be stored) or explicitly form the empirical p\u00d7 p (typically dense) covariance matrix. Either case requires at least O(p2) storage. Despite the availability of massive local and distributed storage systems, for high-dimensional applications (e.g., where data points are high resolution photographs, biometrics, video, etc.), p could be on the order of 1010 \u2212 1012, making O(p2) prohibitive, if not in fact impossible to manage. Indeed, at multiple computing scales, manipulating vectors of length O(p) is possible, when storage of O(p2) is not. A typical desktop may have 10-20 GB of RAM, but will not have more than a few TB of total storage. A modern smart-phone may have as much as a GB of RAM, but has a few GB, not TB, of storage.\nWe consider the streaming data setting, where data points are generated sequentially, and are never stored. In the setting of the so-called spiked covariance model (and natural generalizations) we show that a simple algorithm requiring O(kp) storage \u2013 the best possible \u2013 performs as well as batch algorithms (namely, SVD on the empirical covariance matrix), with sample complexity O(p log p). To the best of our knowledge, this is the only algorithm with both storage complexity and sample complexity guarantees. We discuss the connection to past work in more detail in Section 2. We introduce the model with all related details in Section 3, and present the solution to the rank 1 case, the rank k case, and the perturbedrank-k case in Sections 4.1, 4.2 and 4.3, respectively. In Section 5 we provide simulations that not only confirm the theoretical results, but demonstrate that our algorithm works well outside the assumptions of our main theorems."}, {"heading": "2 Related Work", "text": "Memory- and computation-efficient algorithms that operate on streaming data are plentiful in the literature and many seem to do well in practice. However, there is no algorithm that provably recovers the principal components in the same noise and sample-complexity regime as the batch PCA algorithm does and maintains a provably light memory footprint. Because\n1This is similar to what is sometimes referred to as the single pass model.\nof the practical relevance, there has been renewed interest recently in this problem, and the fact that this is an important unresolved issue has been pointed out in numerous places, e.g., [21, 1].\nA large body of work has focused on the non-statistical data paradigm that deals with a fixed pool of samples. This includes work on online PCA and low-rank matrix approximation in the streaming scenario, including sketching and dimensionality-reduction based techniques.\nOnline-PCA for regret minimization has been considered in several papers, most recently in [21], where the multiplicative weights approach is adapted for this problem (now experts correspond to subspaces). The goal there is to control the regret, improving on the natural follow-the-leader algorithm that performs batch-PCA at each step. However, the algorithm can require O(p2) memory, in order to store the multiplicative weights. A memory-light variant described in [1] typically requires much less memory, but there are no guarantees for this, and moreover, for certain problem instances, its memory requirement is on the order of p2.\nSub-sampling, dimensionality-reduction and sketching form another family of low-complexity and low-memory techniques, see, e.g., [5, 13, 8]. These save on memory and computation by performing SVD on the resulting smaller matrix. The results in this line of work provide worst-case guarantees over the pool of data, and typically require a rapidly decaying spectrum (which we do not have in our setting) to produce good bounds. More fundamentally, these approaches are not appropriate for data coming from a statistical model such as the spiked covariance model. It is clear that subsampling approaches, for instance, simply correspond to discarding most of the data, and for fundamental sample complexity reasons, cannot work. Sketching produces a similar effect: each column of the sketch is a random (+/\u2212) sum of the data points. If the data points are, e.g., independent Gaussian vectors, then so will each element of the sketch, and thus this approach again runs against fundamental sample complexity constraints. Indeed, it is straightforward to check that the guarantees presented in ([5, 8]) are not strong enough to guarantee recovery of the spike. This is not because the results are weak; it is because they geared towards worst-case bounds.\nAlgorithms focused on sequential SVD (e.g., [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.e., each time a new data sample arrives) but without performing full-blown SVD at each step. While these algorithms indeed reduce both the computational and memory burden of batch-PCA, there are no rigorous guarantees on the quality of the principal components or on the statistical performance of these methods.\nIn a Bayesian mindset, some researchers have come up with expectation maximization approaches [16, 18], that can be used in an incremental fashion. The finite sample behavior is not known.\nStochastic-approximation-based algorithms along the lines of [15] are also quite popular, because of their low computational and memory complexity, and excellent performance in practice. They go under a variety of names, including Incremental PCA (though the term Incremental has been used in the online setting as well [10]), Hebbian learning, and stochastic power method [1]. The basic algorithms are some version of the following: upon receiving data point xt at time t, update the estimate of the top k principal components via:\nU (t+1) = Proj(U (t) + \u03b7txtx \u22a4 t U (t)), (1)\nwhere Proj(\u00b7) denotes the \u201cprojection\u201d that takes the SVD of the argument, and sets the top k singular values to 1 and the rest to zero (see [1] for further discussion).\nWhile empirically these algorithms perform well, to the best of our knowledge - and efforts - there does not exist any rigorous finite sample guarantee for these algorithms. The analytical challenge seems to be the high variance at each step, which makes direct analysis difficult.\nIn summary, while much work has focused on memory-constrained PCA, there has as of yet been no work that simultaneously provides sample complexity guarantees competitive with batch algorithms, and also memory/storage complexity guarantees close to the minimal requirement of O(kp) \u2013 the memory required to store only the output. We present an algorithm that provably does both."}, {"heading": "3 Problem Formulation and Notation", "text": "We consider a streaming model, where at each time step t, we receive a point xt \u2208 Rp. Furthermore, any vector that is not explicitly stored can never be revisited. Now, our goal is to compute the top k principal components of the data: the k-dimensional subspace that offers the best squared-error estimate for the points. We assume a probabilistic generative model, from which the data is sampled at each step t. Specifically, we assume,\nxt = Azt +wt, (2)\nwhere A \u2208 Rp\u00d7k is a fixed matrix, zt \u2208 Rk\u00d71 is a multivariate normal random variable, i.e.,\nzt \u223c N (0k\u00d71, Ik\u00d7k),\nand vector wt \u2208 Rp\u00d71 is the \u201cnoise\u201d vector and is also sampled from a multivariate normal distribution, i.e., wt \u223c N (0p\u00d71, \u03c32Ip\u00d7p). Furthermore, we assume that all 2n random vectors (zt,wt, \u22001 \u2264 t \u2264 n) are mutually independent.\nIn this regime, it is well-known that batch-PCA is asymptotically consistent (hence recovering A up to unitary transformations) with number of samples scaling as n = O(p) [20]. It is interesting to note that in this high-dimensional regime, the signal-to-noise ratio quickly approaches zero, as the signal, or \u201celongation\u201d of the major axis, \u2016Az\u20162, is O(1), while the noise magnitude, \u2016w\u20162, scales as O( \u221a p). The central goal of this paper is to provide finite sample guarantees for a streaming algorithm that requires memory no more than O(kp) and matches the consistency results of batch PCA in the sampling regime n = O(p) (possibly with additional log factors, or factors depending on \u03c3 and k).\nWe denote matrices by capital letters (e.g. A) and vectors by lower-case bold-face letters (x). \u2016x\u2016q denotes the \u2113q norm of x; \u2016x\u2016 denotes the \u21132 norm of x. \u2016A\u2016 or \u2016A\u20162 denotes the spectral norm of A while \u2016A\u2016F denotes the Frobenius norm of A. Without loss of generality (WLOG), we assume that: \u2016A\u20162 = 1, where \u2016A\u20162 = max\u2016x\u20162=1 \u2016Ax\u20162 denotes the spectral norm of A. Finally, we write \u3008a,b\u3009 = a\u22a4b for the inner product between a, b. In proofs the constant C is used loosely and its value may vary from line to line.\nAlgorithm 1 Block-Stochastic Power Method Block-Stochastic Orthogonal Iteration input {x1, . . . ,xn}, Block size: B 1: q0 \u223c N (0, Ip\u00d7p) (Initialization) H i \u223c N (0, Ip\u00d7p), 1 \u2264 i \u2264 k (Initialization) 2: q0 \u2190 q0/\u2016q0\u20162 H \u2190 Q0R0 (QR-decomposition) 3: for \u03c4 = 0, . . . , n/B \u2212 1 do 4: s\u03c4+1 \u2190 0 S\u03c4+1 \u2190 0 5: for t = B\u03c4 + 1, . . . , B(\u03c4 + 1) do 6: s\u03c4+1 \u2190 s\u03c4+1 + 1B \u3008q\u03c4 ,xt\u3009xt S\u03c4+1 \u2190 S\u03c4+1 + 1Bxtx\u22a4t Q\u03c4 7: end for 8: q\u03c4+1 \u2190 s\u03c4+1/\u2016s\u03c4+1\u20162 S\u03c4+1 = Q\u03c4+1R\u03c4+1 (QR-decomposition) 9: end for output"}, {"heading": "4 Algorithm and Guarantees", "text": "In this section, we present our proposed algorithm and its finite sample analysis. It is a block-wise stochastic variant of the classical power-method. Stochastic versions of the power method are already popular in the literature and are known to have good empirical performance; see [1] for a nice review of such methods. However, the main impediment to the analysis of such stochastic algorithms (as in (1)) is the potentially large variance of each step, due primarily to the high-dimensional regime we consider, and the vanishing SNR.\nThis motivated us to consider a modified stochastic power method algorithm, that has a variance reduction step built in. At a high level, our method updates only once in a \u201cblock\u201d and within one block we average out noise to reduce the variance.\nBelow, we first illustrate the main ideas of our method as well as our sample complexity proof for the simpler rank-1 case. The rank-1 and rank-k algorithms are so similar, that we present them in the same panel. We provide the rank-k analysis in Section 4.2. We note that, while our algorithm describes {x1, . . . ,xn} as \u201cinput,\u201d we mean this in the streaming sense: the data are no-where stored, and can never be revisited unless the algorithm explicitly stores them."}, {"heading": "4.1 Rank-One Case", "text": "We first consider the rank-1 case for which each sample xt is generated using: xt = uzt +wt where u \u2208 Rp is the principal component that we wish to recover. Our algorithm is a block-wise method where all the n samples are divided in n/B blocks (for simplicity we assume that n/B is an integer). In the (\u03c4 + 1)-st block, we compute\ns\u03c4+1 =\n\n\n1\nB\nB(\u03c4+1) \u2211\nt=B\u03c4+1\nxtx \u22a4 t\n\nq\u03c4 . (3)\nThen, the iterate q\u03c4 is updated using q\u03c4+1 = s\u03c4+1/\u2016s\u03c4+1\u20162. Note that, s\u03c4+1 can be easily computed in an online manner where O(p) operations are required per step. Furthermore, storage requirements are also linear in p."}, {"heading": "4.1.1 Analysis", "text": "We now present the sample complexity analysis of our proposed method (Algorithm 1). We show that, using O(\u03c34p log(p)/\u01eb2) samples, Algorithm 1 obtains a solution qT of accuracy \u01eb, i.e. \u2016qT \u2212 u\u20162 \u2264 \u01eb.\nTheorem 1. Denote the data stream by x1, . . . ,xn, where xt \u2208 Rp, \u2200t is generated by (2). Set the total number of iterations T = \u2126( log(p/\u01eb)\nlog((\u03c32+.75)/(\u03c32+.5)) ) and the block size B =\n\u2126( (1+3(\u03c3+\u03c32)\n\u221a p)2 log(T )\n\u01eb2 ). Then, with probability 0.99, \u2016qT \u2212 u\u20162 \u2264 \u01eb, where qT is the T -th\niterate of Algorithm 1. That is, Algorithm 1 obtains an \u01eb-accurate solution with number of samples (n) given by:\nn = \u2126\u0303\n( (1 + 3(\u03c3 + \u03c32) \u221a p)2 log(p/\u01eb)\n\u01eb2 log((\u03c32 + .75)/(\u03c32 + .5))\n)\n.\nNote that in the total sample complexity, we use the notation \u2126\u0303(\u00b7) to suppress the extra log(T ) factor for clarity of exposition, as T already appears in the expression linearly.\nProof. The proof decomposes the current iterate into the component of the current iterate, q\u03c4 , in the direction of the true principal component (the spike) u, and the perpendicular component, showing that the former eventually dominates. Doing so hinges on three key components: (a) for large enough B, the empirical covariance matrix F\u03c4+1 = 1 B \u2211B(\u03c4+1) t=B\u03c4+1 xtx \u22a4 t is close to the true covariance matrix M = uu\u22a4 + \u03c32I, i.e., \u2016F\u03c4+1 \u2212 M\u20162 is small. In the process, we obtain \u201ctighter\u201d bounds for \u2016u\u22a4(F\u03c4+1 \u2212M)u\u2016 for fixed u; (b) with probability 0.99 (or any other constant probability), the initial point q0 has a component of at least O(1/ \u221a p) magnitude along the true direction u; (c) after \u03c4 iterations, the error in estimation is at most O(\u03b3\u03c4 ) where \u03b3 < 1 is a constant. There are several results that we use repeatedly, which we collect here, and prove individually in the appendix. Lemmas 4, 5 and 6. Let B, T and the data stream {xi} be as defined in the theorem. Then:\n\u2022 (Lemma 4): With probability 1\u2212 C/T , for C a universal constant, we have: \u2225\n\u2225 \u2225 \u2225 \u2225 1 B \u2211\nt\nxtx \u22a4 t \u2212 uu\u22a4 \u2212 \u03c32I\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u2264 \u01eb.\n\u2022 (Lemma 5): With probability 1\u2212 C/T , for C a universal constant, we have:\nu\u22a4s\u03c4+1 \u2265 u\u22a4q\u03c4 (1 + \u03c32) ( 1\u2212 \u01eb 4(1 + \u03c32) ) ,\nwhere st = 1 B\n\u2211 B\u03c4<t\u2264B(\u03c4+1) xtx \u22a4 t q\u03c4 .\n\u2022 (Lemma 6): Let q0 be the initial guess for u, given by Steps 1 and 2 of Algorithm 1. Then, w.p. 0.99: |\u3008q0,u\u3009| \u2265 C0\u221ap , where C0 > 0 is a universal constant.\nStep (a) is proved in Lemmas 4 and 5, while Lemma 6 provides the required result for the initial vector q0. Using these lemmas, we next complete the proof of the theorem. We note that both (a) and (b) follow from well-known results; we provide them for completeness.\nLet q\u03c4 = \u221a 1\u2212 \u03b4\u03c4u + \u221a \u03b4\u03c4g\u03c4 , 1 \u2264 \u03c4 \u2264 n/B, where g\u03c4 is the component of q\u03c4 that is\nperpendicular to u and \u221a 1\u2212 \u03b4\u03c4 is the magnitude of the component of q\u03c4 along u. Note that g\u03c4 may well change at each iteration; we only wish to show \u03b4\u03c4 \u2192 0. Now, using Lemma 5, the following holds with probability at least 1\u2212 C/T :\nu\u22a4s\u03c4+1 \u2265 \u221a 1\u2212 \u03b4\u03c4 (1 + \u03c32) ( 1\u2212 \u01eb 4(1 + \u03c32) ) . (4)\nNext, we consider the component of s\u03c4+1 that is perpendicular to u:\ng\u22a4\u03c4+1s\u03c4+1 = g \u22a4 \u03c4+1\n\n\n1\nB\nB(\u03c4+1) \u2211\nt=B\u03c4+1\nxtx \u22a4 t\n\nq\u03c4 = g \u22a4 \u03c4+1(M + E\u03c4 )q\u03c4 ,\nwhere M = uu\u22a4 + \u03c32I and E\u03c4 is the error matrix: E\u03c4 = M \u2212 1B \u2211B(\u03c4+1) t=B\u03c4+1 xtx \u22a4 t . Using Lemma 4, \u2016E\u03c4\u20162 \u2264 \u01eb (w.p. \u2265 1\u2212 C/T ). Hence, w.p. \u2265 1\u2212 C/T :\ng\u22a4\u03c4+1s\u03c4+1 = \u03c3 2g\u22a4\u03c4+1q\u03c4 + \u2016g\u03c4+1\u20162\u2016E\u03c4\u20162\u2016q\u03c4\u20162 \u2264 \u03c32\n\u221a\n\u03b4\u03c4 + \u01eb. (5)\nNow, since q\u03c4+1 = s\u03c4+1/\u2016s\u03c4+1\u20162,\n\u03b4\u03c4+1 = (g \u22a4 \u03c4+1q\u03c4+1)\n2 = (g\u22a4\u03c4+1s\u03c4+1) 2\n(u\u22a4s\u03c4+1)2 + (g\u22a4\u03c4+1s\u03c4+1) 2 ,\n(i) \u2264 (g \u22a4 \u03c4+1s\u03c4+1) 2\n(1\u2212 \u03b4\u03c4 ) ( 1 + \u03c32 \u2212 \u01eb 4 )2 + (g\u22a4\u03c4+1s\u03c4+1)\n2 ,\n(ii) \u2264 (\u03c3 2 \u221a \u03b4\u03c4 + \u01eb) 2\n(1\u2212 \u03b4\u03c4 ) ( 1 + \u03c32 \u2212 \u01eb 4 )2 + (\u03c32 \u221a \u03b4\u03c4 + \u01eb)2 , (6)\nwhere, (i) follows from (4) and (ii) follows from (5) along with the fact that x c+x\nis an increasing function in x for c, x \u2265 0.\nAssuming \u221a \u03b4\u03c4 \u2265 2\u01eb and using (6) and bounding the failure probability with a union\nbound, we get (w.p. \u2265 1\u2212 \u03c4 \u00b7 C/T )\n\u03b4\u03c4+1 \u2264 \u03b4\u03c4 (\u03c3\n2 + 1/2)2 (1\u2212 \u03b4\u03c4 )(\u03c32 + 3/4)2 + \u03b4\u03c4 (\u03c32 + 1/2)2 (i) \u2264 \u03b3 2\u03c4\u03b40 1\u2212 (1\u2212 \u03b32\u03c4 )\u03b40 (ii) \u2264 C1\u03b32\u03c4p, (7)\nwhere \u03b3 = \u03c3 2+1/2\n\u03c32+3/4 and C1 > 0 is a global constant. Inequality (ii) follows from Lemma 6;\nto prove (i), we need one final result: the following lemma shows that the recursion given by (7) decreases \u03b4\u03c4 at a fast rate. Interestingly, the rate of decrease in error \u03b4\u03c4 initially (for small \u03c4) might be sub-linear but for large enough \u03c4 the rate turns out to be linear. We defer the proof to the appendix.\nLemma 2. If for any \u03c4 \u2265 0 and 0 < \u03b3 < 1, we have \u03b4\u03c4+1 \u2264 \u03b3 2\u03b4\u03c4\n1\u2212\u03b4\u03c4+\u03b32\u03b4\u03c4 , then,\n\u03b4\u03c4+1 \u2264 \u03b32t+2\u03b40\n1\u2212 (1\u2212 \u03b32t+2)\u03b40 .\nHence, using the above equation after T = O (log(p/\u01eb)/ log (1/\u03b3)) updates, with probability at least 1\u2212C, \u221a\u03b4T \u2264 2\u01eb. The result now follows by noting that \u2016u\u2212qT\u20162 \u2264 2 \u221a \u03b4T .\nRemark: Note that in Theorem 1, the probability of accurate principal component recovery is a constant and does not decay with p. One can correct this by either paying a price of O(log p) in storage, or in sample complexity: for the former, we can run O(log p) instances of Algorithm 1 in parallel; alternatively, we can run Algorithm 1 O(log p) times on fresh data each time, using the next block of data to evaluate the old solutions, always keeping the best one. Either approach guarantees a success probability of at least 1\u2212 1\npO(1) .\n4.2 General Rank-k Case\nIn this section, we consider the general rank-k PCA problem where each sample is assumed to be generated using the model of equation (2), where A \u2208 Rp\u00d7k represents the k principal components that need to be recovered. Let A = U\u039bV \u22a4 be the SVD of A where U \u2208 Rp\u00d7k, \u039b, V \u2208 Rk\u00d7k. The matrices U and V are orthogonal, i.e., U\u22a4U = I, V \u22a4V = I, and \u03a3 is a diagonal matrix with diagonal elements \u03bb1 \u2265 \u03bb2 \u00b7 \u00b7 \u00b7 \u2265 \u03bbk. The goal is to recover the space spanned by A, i.e., span(U). Without loss of generality, we can assume that \u2016A\u20162 = \u03bb1 = 1.\nSimilar to the rank-1 problem, our algorithm for the rank-k problem can be viewed as a streaming variant of the classical orthogonal iteration used for SVD. But unlike the rank1 case, we require a more careful analysis as we need to bound spectral norms of various quantities in intermediate steps and simple, crude analysis can lead to significantly worse bounds. Interestingly, the analysis is entirely different from the standard analysis of the orthogonal iteration as there, the empirical estimate of the covariance matrix is fixed while in our case it varies with each block.\nFor the general rank-k problem, we use the largest-principal-angle-based distance function between any two given subspaces:\ndist (span(U), span(V )) = dist(U, V ) = \u2016U\u22a4\u22a5V \u20162 = \u2016V \u22a4\u22a5 U\u20162,\nwhere U\u22a5 and V\u22a5 represent an orthogonal basis of the perpendicular subspace to span(U) and span(V ), respectively. For the spiked covariance model, it is straightforward to see that this is equivalent to the usual PCA figure-of-merit, the expressed variance.\nTheorem 3. Consider a data stream, where xt \u2208 Rp for every t is generated by (2), and the SVD of A \u2208 Rp\u00d7k is given by A = U\u039bV \u22a4. Let, wlog, \u03bb1 = 1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbk > 0. Let,\nT = \u2126\n(\nlog(p/k\u01eb)/ log\n(\n\u03c32 + 0.75\u03bb2k \u03c32 + 0.5\u03bb2k\n))\n, B = \u2126\n\n \n( (1 + \u03c3)2 \u221a k + \u03c3 \u221a 1 + \u03c32k \u221a p )2 log(T )\n\u03bb4k\u01eb 2\n\n  .\nThen, after T B-size-block-updates, w.p. 0.99, dist(U,QT ) \u2264 \u01eb. Hence, the sufficient number of samples for \u01eb-accurate recovery of all the top-k principal components is:\nn = \u2126\u0303\n\n \n( (1 + \u03c3)2 \u221a k + \u03c3 \u221a 1 + \u03c32k \u221a p )2 log(p/k\u01eb)\n\u03bb4k\u01eb 2 log\n(\n\u03c32+0.75\u03bb2 k\n\u03c32+0.5\u03bb2 k\n)\n\n  .\nAgain, we use \u2126\u0303(\u00b7) to suppress the extra log(T ) factor.\nThe key part of the proof requires the following additional lemmas that bound the energy of the current iterate along the desired subspace and its perpendicular space (Lemmas 8 and 9), and Lemma 10, which controls the quality of the initialization.\nLemmas 8, 9 and 10. Let the data stream, A, B, and T be as defined in Theorem 3, \u03c3 be the variance of noise, F\u03c4+1 = 1 B \u2211 B\u03c4<t\u2264B(\u03c4+1) xtx \u22a4 t and Q\u03c4 be the \u03c4 -th iterate of Algorithm 1.\n\u2022 (Lemma 8): \u2200 v \u2208 Rk and \u2016v\u20162 = 1, w.p. 1\u2212 5C/T we have:\n\u2016U\u22a4F\u03c4+1Q\u03c4v\u20162 \u2265 (\u03bb2k + \u03c32 \u2212 \u03bb2k\u01eb 4 ) \u221a 1\u2212 \u2016U\u22a4\u22a5Q\u03c4\u201622.\n\u2022 (Lemma 9): With probability at least 1\u22124C/T , \u2016U\u22a4\u22a5F\u03c4+1Q\u03c4\u20162 \u2264 \u03c32\u2016U\u22a4\u22a5Q\u03c4\u20162+\u03bb2k\u01eb/2.\n\u2022 (Lemma 10): Let Q0 \u2208 Rp\u00d7k be sampled uniformly at random as in Algorithm 1. Then, w.p. at least 0.99: \u03c3k(U \u22a4Q0) \u2265 C \u221a 1 kp .\nWe provide the proof of the lemmas and theorem in the appendix."}, {"heading": "4.3 Perturbation-tolerant Subspace Recovery", "text": "While our results thus far assume A has rank exactly k, and k is known a priori, here we show that both these can be relaxed; hence our results hold in a quite broad setting.\nLet xt = Azt +wt be the t-th step sample, with A = U\u039bV T \u2208 Rp\u00d7r and U \u2208 Rp\u00d7r where r \u2265 k is the true rank of A which is unknown. However, we run Algorithm 1 with rank k and the goal is to recover a subspace QT , s.t., QT is contained in U .\nWe first observe that the largest-principal angle based distance function that we use in the previous section can directly be used for our more general setting. That is, dist(U,QT ) = \u2016UT\u22a5QT\u20162 measures the component of QT \u201coutside\u201d the subspace U and the goal is to show that component is \u2264 \u01eb.\nNow, our analysis can be easily modified to handle this more general setting as crucially our distance function does not change. Naturally, now the number of samples we require increases according to r. In particular, if\nn = \u2126\u0303\n\n\n( (1 + \u03c3)2 \u221a r + \u03c3 \u221a 1 + \u03c32r \u221a p )2 log(p/r\u01eb)\n\u03bb4r\u01eb 2 log\n(\n\u03c32+0.75\u03bb2r \u03c32+0.5\u03bb2r\n)\n\n ,\nthen dist(U,QT ) \u2264 \u01eb. Furthermore, if we assume r \u2265 C \u00b7 k (or a large enough constant C > 0) then the initialization step provides us better distance, i.e., dist(U,Q0) \u2264 C \u2032/\u221ap rather than dist(U,Q0) \u2264 C \u2032/ \u221a kp bound if r = k. This initialization step enables us to give tighter sample complexity as the r \u221a p in the numerator above can be replaced by \u221a rp."}, {"heading": "5 Experiments", "text": "In this section, we show that, as predicted by our theoretical results, our algorithm performs close to the optimal batch SVD. We provide the results from simulating the spiked covariance model, and demonstrate the phase-transition in the probability of successful recovery that is inherent to the statistical problem. Then we stray from the analyzed model and performance metric and test our algorithm on real world\u2013and some very big\u2013datasets, using the metric of explained variance.\nIn the experiments for Figures 1 (a)-(b), we draw data from the generative model of\n(2). Our results are averaged over at least 200 independent runs. Algorithm 1 uses the block size prescribed in Theorem 3, with the empirically tuned constant of 0.2. As expected, our algorithm exhibits linear scaling with respect to the ambient dimension p \u2013 the same as the batch SVD. The missing point on batch SVD\u2019s curve (Figure 1(a)), corresponds to p > 2.4 \u00b7 104. Performing SVD on a dense p \u00d7 p matrix, either fails or takes a very long time on most modern desktop computers; in contrast, our streaming algorithm easily runs on this size problem. The phase transition plot in Figure 1(b) shows the empirical sample complexity on a large class of problems and corroborates the scaling with respect to the noise variance we obtain theoretically.\nFigures 1 (c)-(d) complement our complete treatment of the spiked covariance model, with some out-of-model experiments. We used three bag-of-words datasets from [14]. We evaluated our algorithm\u2019s performance with respect to the fraction of explained variance metric: given the p\u00d7k matrix V output from the algorithm, and all the provided samples in matrix X , the fraction of explained variance is defined as Tr(V TXXTV )/Tr(XXT ). To be consistent with our theory, for a dataset of n samples of dimension p, we set the number of blocks to be T = \u2308log(p)\u2309 and the size of blocks to B = \u230an/T \u230b in our algorithm. The NIPS dataset is the smallest, with 1500 documents and 12K words and allowed us to compare our algorithm with the optimal, batch SVD. We had the two algorithms work on the document space (p = 1500) and report the results in Figure 1(c). The dashed line represents the optimal using B samples. The figure is consistent with our theoretical result: our algorithm performs as well as the batch, with an added log(p) factor in the sample complexity.\nFinally, in Figure 1 (d), we show our algorithm\u2019s ability to tackle very large problems. Both the NY Times and PubMed datasets are of prohibitive size for traditional batch methods \u2013 the latter including 8.2 million documents on a vocabulary of 141 thousand words \u2013 so we just report the performance of Algorithm 1. It was able to extract the top 7 components for each dataset in a few hours on a desktop computer. A second pass was made on the data to evaluate the results, and we saw 7-10 percent of the variance explained on spaces with p > 104."}, {"heading": "A Lemmas from Section 4.1", "text": "We first give the statement of all the Lemmas whose proofs we omitted in the body of the paper. Then we provide some results from the literature \u2013 what we call Preliminaries \u2013 and then we prove Theorem 3 and the supporting lemmas.\nLemma 4. Let B, T and the data stream {xt} be as defined in Theorem 1. Then, w.p. 1\u2212 C/T we have:\n\u2225 \u2225 \u2225 \u2225 \u2225 1 B \u2211\nt\nxtx \u22a4 t \u2212 uu\u22a4 \u2212 \u03c32I\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u2264 \u01eb.\nLemma 5. Let B, T and the data stream {xt} be as defined in Theorem 1. Then, w.p. 1\u2212 C/T we have:\nu\u22a4s\u03c4+1 \u2265 u\u22a4q\u03c4 (1 + \u03c32) ( 1\u2212 \u01eb 4(1 + \u03c32) ) ,\nwhere st = 1 B\n\u2211 B\u03c4<t\u2264B(\u03c4+1) xtx \u22a4 t q\u03c4 .\nLemma 6. Let q0 be the initial guess for u, given by Steps 1 and 2 of Algorithm 1. Then, w.p. 0.99: |\u3008q0,u\u3009| \u2265 C0\u221ap , where C0 > 0 is a universal constant.\nLemma 7. If for any \u03c4 \u2265 0 and 0 < \u03b3 < 1, we have \u03b4\u03c4+1 \u2264 \u03b3 2\u03b4\u03c4\n1\u2212\u03b4\u03c4+\u03b32\u03b4\u03c4 , then,\n\u03b4\u03c4+1 \u2264 \u03b32t+2\u03b40\n1\u2212 (1\u2212 \u03b32t+2)\u03b40 ."}, {"heading": "B Lemmas from Section 4.2", "text": "Lemma 8. Let X , A, B, and T be as defined in Theorem 3. Also, let \u03c3 be the variance of noise, F\u03c4+1 = 1 B \u2211 B\u03c4<t\u2264B(\u03c4+1) xtx \u22a4 t and Q\u03c4 be the \u03c4 -th iterate of Algorithm 1. Then, \u2200 v \u2208 Rk and \u2016v\u20162 = 1, w.p. 1\u2212 5C/T we have:\n\u2016U\u22a4F\u03c4+1Q\u03c4v\u20162 \u2265 (\u03bb2k + \u03c32 \u2212 \u03bb2k\u01eb 4 ) \u221a 1\u2212 \u2016U\u22a4\u22a5Q\u03c4\u201622.\nLemma 9. Let X , A, B, F\u03c4+1, Q\u03c4 be as defined in Lemma 8. Then, w.p. 1 \u2212 4C/T , \u2016U\u22a4\u22a5F\u03c4+1Q\u03c4\u20162 \u2264 \u03c32\u2016U\u22a4\u22a5Q\u03c4\u20162 + \u03bb2k\u01eb/2.\nLemma 10. Let Q0 \u2208 Rp\u00d7k be sampled uniformly at random from the set of all k-dimensional subspaces (see Initialization Steps of Algorithm 1). Then, w.p. at least 0.99: \u03c3k(U\n\u22a4Q0) \u2265 C \u221a\n1 kp , where C > 0 is a global constant."}, {"heading": "C Preliminaries", "text": "Lemma 11 (Lemma 5.4 of [20]). Let A be a symmetric k\u00d7 k matrix, and let N\u01eb be an \u01eb-net of Sk\u22121 for some \u01eb \u2208 [0, 1). Then,\n\u2016A\u20162 \u2264 1\n(1\u2212 2\u01eb) supx\u2208N\u01eb |\u3008Ax,x\u3009|.\nLemma 12 (Proposition 2.1 of [19]). Consider independent random vectors x1, . . . ,xn in R\np, n \u2265 p, which have sub-Gaussian distribution with parameter 1. Then for every \u03b4 > 0 with probability at least 1\u2212 \u03b4 one has,\n\u2016 1 n\nn \u2211\ni=1\nxix T i \u2212 E[xixTi ]\u20162 \u2264 C\n\u221a\nlog(2/\u03b4)\n\u221a\np n .\nLemma 13 (Corollary 3.5 of [20]). Let A be an N \u00d7n matrix whose entries are independent standard normal random variables. Then for every t \u2265 0, with probability at least 1 \u2212 2 exp(\u2212t2/2) one has,\n\u221a N \u2212\u221an\u2212 t \u2264 \u03c3k(A) \u2264 \u03c31(A) \u2264 \u221a N + \u221a n+ t.\nLemma 14 (Theorem 1.2 of [17]). Let \u03b61, . . . , \u03b6n be independent centered real random variables with variances at least 1 and subgaussian moments bounded by B. Let A be an k \u00d7 k matrix whose rows are independent copies of the random vector (\u03b61, . . . , \u03b6n). Then for every \u01eb \u2265 0 one has\nPr(\u03c3min(A) \u2264 \u01eb/ \u221a k) \u2264 C\u01eb+ cn,\nwhere C > 0 and c \u2208 (0, 1) depend only on B. Note that B = 1 for the standard Gaussian variables.\nLemma 15. Let xi \u2208 Rm, 1 \u2264 i \u2264 B be i.i.d. standard multivariate normal variables. Also, yi \u2208 Rn are also i.i.d. normal variables and are independent of xi, \u2200i. Then, w.p. 1\u2212 \u03b4,\n\u2225 \u2225 \u2225 \u2225 \u2225 1 B \u2211\ni\nxiy \u22a4 i\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n\u2264 \u221a Cmax(m,n) log(2/\u03b4)\nB .\nProof. Let M = \u2211 i xiy T i and let m > n. Then, the goal is to show that, the following holds w.p. 1\u2212 \u03b4: 1 B \u2016Mv\u20162 \u2264 \u221a Cm log(2/\u03b4) B\nfor all v \u2208 Rn s.t. \u2016v\u20162 = 1. We prove the lemma by first showing that the above mentioned result holds for any fixed\nvector v and then use standard epsilon-net argument to prove it for all v. Let N be the 1/4-net of Sn\u22121. Then, using Lemma 5.4 of [20] (see Lemma 11),\n\u2016 1 Bm MTM\u20162 \u2264 2max v\u2208N 1 Bm \u2016Mv\u201622. (8)\nNow, for any fixed v: Mv = \u2211 i xiy T i v = \u2211 i xici, where ci = y T i v \u223c N(0, 1). Hence,\n\u2016Mv\u201622 = m \u2211\n\u2113=1\n( B \u2211\ni=1\nxi\u2113ci) 2.\nNow, \u2211B i=1 xi\u2113ci \u223c N(0, \u2016c\u201622) where cT = [c1 c2 \u00b7 \u00b7 \u00b7 cB]. Hence, \u2211B i=1 xi\u2113ci = \u2016c\u20162h\u2113 where h\u2113 \u223c N(0, 1).\nTherefore, \u2016Mv\u201622 = \u2016c\u201622\u2016h\u201622 where hT = [h1 h2 \u00b7 \u00b7 \u00b7hB]. Now,\nPr( \u2016c\u201622\u2016h\u201622 Bm \u2265 1 + \u03b3) \u2264 Pr(\u2016c\u2016 2 2 B \u2265 \u221a 1 + \u03b3) + Pr( \u2016h\u201622 m \u2265 \u221a 1 + \u03b3)\n\u03b61 \u2264 2 exp(\u2212B\u03b3\n2\n32 ) + 2 exp(\u2212m\u03b3\n2\n32 ) \u2264 4 exp(\u2212m\u03b3\n2\n32 ), (9)\nwhere 0 < \u03b3 < 3 and \u03b61 follows from Lemma 13.\nUsing (8), (9), the following holds with probability (1\u2212 9n+1e\u2212m\u03b3 2 32 ):\n\u2016M\u201622 Bm \u2264 1 + 2\u03b3. (10)\nThe result now follows by setting \u03b3 appropriately and assuming n < Cm for small enough C."}, {"heading": "D Proof of Theorem 3", "text": "Recall that our algorithm proceeds in a blockwise manner; for each block of samples, we compute\nS\u03c4+1 =\n\n\n1\nB\nB(\u03c4+1) \u2211\nt=B\u03c4+1\nxtx \u22a4 t\n\nQ\u03c4 , (11)\nwhere Q\u03c4 \u2208 Rp\u00d7k is the \u03c4 -th block iterate and is an orthogonal matrix, i.e., Q\u22a4\u03c4 Q\u03c4 = Ik\u00d7k. Given S\u03c4+1, the next iterate, Q\u03c4+1, is computed by the QR-decomposition of S\u03c4+1. That is,\nS\u03c4+1 = Q\u03c4+1R\u03c4+1, (12)\nwhere R\u03c4+1 \u2208 Rk\u00d7k is an upper-triangular matrix. Proof. By using update for Q\u03c4+1 (see (11), (12)):\nQ\u03c4+1R\u03c4+1 = F\u03c4+1Q\u03c4 , (13)\nwhere F\u03c4+1 = 1 B\n\u2211 B\u03c4<t\u2264B(\u03c4+1) xtx \u22a4 t . That is,\nU\u22a4\u22a5Q\u03c4+1R\u03c4+1v = U \u22a4 \u22a5F\u03c4+1Q\u03c4v, \u2200v \u2208 Rk, (14)\nwhere U\u22a5 is an orthogonal basis of the subspace orthogonal to span(U). Now, let v1 be the singular vector corresponding to the largest singular value, then:\n\u2016U\u22a4\u22a5Q\u03c4+1\u201622 = \u2016U\u22a4\u22a5Q\u03c4+1v1\u201622\n\u2016v1\u201622 = \u2016U\u22a4\u22a5Q\u03c4+1R\u03c4+1v\u03031\u201622 \u2016R\u03c4+1v\u03031\u201622\n(i) = \u2016U\u22a4\u22a5Q\u03c4+1R\u03c4+1v\u03031\u201622 \u2016U\u22a4Q\u03c4+1R\u03c4+1v\u03031\u201622 + \u2016U\u22a4\u22a5Q\u03c4+1R\u03c4+1v\u03031\u201622\n(ii) = \u2016U\u22a4\u22a5F\u03c4+1Q\u03c4 v\u03031\u201622 \u2016U\u22a4F\u03c4+1Q\u03c4 v\u03031\u201622 + \u2016U\u22a4\u22a5F\u03c4+1Q\u03c4 v\u03031\u201622 . (15)\nwhere v\u03031 = R\u22121\u03c4+1v1\n\u2016R\u22121\u03c4+1v1\u20162 . (i) follows as Q\u03c4+1 is an orthogonal matrix and [U U\u22a5] form a complete\northogonal basis; (ii) follows by using (13). The existence of R\u22121\u03c4+1 follows using Lemma 8 along with the fact that \u03c3k(R\u03c4+1) = \u2016R\u03c4+1\u03b60\u20162 \u2265 \u2016U\u22a4Q\u03c4+1R\u03c4+1\u03b60\u20162 = \u2016U\u22a4F\u03c4+1Q\u03c4\u03b60\u20162 > 0, where \u03b60 is the singular vector of R\u03c4+1 corresponding to its smallest singular value, \u03c3k(R\u03c4+1).\nNow, using (15) with Lemmas 8, 9 and using the fact that x/(x + c) is an increasing function of x, for all x > 0, we get (w.p. \u2265 1\u2212 2C/T ):\n\u2016U\u22a4\u22a5Q\u03c4+1\u201622 \u2264 (\u03c32\u2016U\u22a4\u22a5Q\u03c4\u20162 + \u03bb2k\u01eb/2)2\n(\u03bb2k + \u03c3 2 \u2212 \u03bb\n2 k \u01eb\n4 ) 2(1\u2212 \u2016U\u22a4\u22a5Q\u03c4\u201622) + (\u03c32\u2016U\u22a4\u22a5Q\u03c4\u20162 + 0.5\u03bb2k\u01eb)2\n.\nNow, assuming \u01eb \u2264 \u2016U\u22a4\u22a5Q\u03c4\u201622, using the above equation and by using union bound, we get (w.p. \u2265 1\u2212 2\u03c4C/T ):\n\u2016U\u22a4\u22a5Q\u03c4+1\u201622 \u2264 \u03b32\u2016U\u22a4\u22a5Q\u03c4\u201622\n1\u2212 \u2016U\u22a4\u22a5Q\u03c4\u201622 + \u03b32\u2016U\u22a4\u22a5Q\u03c4\u201622 , (16)\nwhere \u03b3 = \u03c32+\u03bb2 k /2\n\u03c32+3\u03bb2 k /4\n< 1 for \u03bbk > 0. Using Lemma 2 along with the above equation, we get\n(w.p. \u2265 1\u2212 2\u03c4C/T ): \u2016U\u22a4\u22a5Q\u03c4+1\u201622 \u2264 \u03b32\u03c4\n\u2016U\u22a4\u22a5Q0\u201622 1\u2212 \u2016U\u22a4\u22a5Q0\u201622 .\nNow, using Lemma 10 we know that \u2016U\u22a4\u22a5Q0\u201622 is at most 1 \u2212 \u2126(1/(kp)). Hence, for T = O(log(p/\u01eb)/ log(1/\u03b3), we get: \u2016U\u22a4\u22a5QT\u201622 \u2264 \u01eb. Furthermore, we require B (as mentioned in the Theorem) samples per block. Hence, the total sample complexity bound is given by O(BT ), concluding the proof."}, {"heading": "E Proof of Lemma 4", "text": "Proof. Note that,\n1\nB\n\u2211\nt\nxtx \u22a4 t \u2212 uu\u22a4 \u2212 \u03c32I = uu\u22a4\n1\nB\n\u2211\nt\n(z2t \u2212 1)+\n1\nB\n\u2211\nt\n(wtw \u22a4 t \u2212 \u03c32I) +\n1\nB\n\u2211\nt\nztwtu \u22a4 +\n1 B u \u2211\nt\nztw \u22a4 t . (17)\nWe now individually bound each of the above given terms in the RHS. Using standard tail bounds for covariance estimation (see Lemma 12), we can bound the first two terms (w.p. 1\u2212 2C/T ):\n1\nB\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nt\n(z2t \u2212 1) \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2264 \u221a C log(T )\nB ,\n\u2016 1 B \u2211\nt\n(wtw \u22a4 t \u2212 \u03c32I)\u20162 \u2264 \u03c32\n\u221a\nC1p log(T )\nB . (18)\nSimilarly, using Lemma 15, we can bound the last two terms in (17) (w.p. 1\u2212 2C/T ):\n\u2016 1 B \u2211\nt\nztwtu \u22a4\u20162 = \u2016\n1 B u \u2211\nt\nztw \u22a4 t \u20162 \u2264 \u03c3\n\u221a\nC1p log(T )\nB . (19)\nThe lemma now follows by using (17), (18), (19) along with B as given by Theorem 1."}, {"heading": "F Proof of Lemma 5", "text": "Proof. Let q\u03c4 = \u221a 1\u2212 \u03b4\u03c4u+ \u221a \u03b4\u03c4u \u22a5 \u03c4 , where u \u22a5 \u03c4 is the component of q\u03c4 that is orthogonal to u. Now,\nu\u22a4s\u03c4+1 = 1\nB\n\u2211\nt\n(u\u22a4xt)(x \u22a4 t qt)\n= 1\nB\n\u2211\nt\n(zt + u \u22a4wt)( \u221a 1\u2212 \u03b4\u03c4 (zt + u\u22a4wt) + \u221a \u03b4\u03c4w \u22a4 t u \u22a5 \u03c4 )\n= \u221a 1\u2212 \u03b4\u03c4 B \u2211\nt\n(zt + u \u22a4wt) 2 + \u221a \u03b4\u03c4 B \u2211\nt\n(zt + u \u22a4wt)w \u22a4 t u \u22a5 \u03c4 . (20)\nNow, the first term above is a summation of B i.i.d. chi-square variables and hence using standard results (see Lemma 13), w.p. (1\u2212 C/T ):\n1\nB\n\u2211\nt\n(zt + u \u22a4wt) 2 \u2265 (1 + \u03c32)(1\u2212 \u221a C log(2T )\nB ). (21)\nAlso, w\u22a4t u and w \u22a4 t u \u22a5 \u03c4 are independent random variables, as both w \u22a4 t u, w \u22a4 t u \u22a5 \u03c4 are Gaussians and E[w\u22a4t u \u22a5 \u03c4 u\n\u22a4wt] = 0. Hence, using Lemma 15, the following holds with probability \u2265 1\u2212 4C/T :\n\u2016 1 B \u2211\nt\n(zt + u \u22a4wt)w \u22a4 t u \u22a5 \u03c4 \u20162 \u2264 \u03c3\n\u221a 1 + \u03c32\n\u221a\nC log(T )\nB\n(i) \u2264 \u03c3 \u221a 1 + \u03c32\n\u221a\nC1p log(T )\nB(1\u2212 \u03b40) \u221a 1\u2212 \u03b4\u03c4 ,\n(22)\nwhere (i) follows by using inductive hypothesis (i.e., \u221a 1\u2212 \u03b4\u03c4 > \u221a\n1\u2212 \u03b4\u03c4\u22121, induction step follows as we show that the error decreases at each step) and Lemma 6.\nThe lemma now follows by using (20), (21), (22) and by setting B, T appropriately."}, {"heading": "G Proof of Lemma 6", "text": "Proof. Using standard tail bounds for Gaussians (see Lemma 13), \u2016q0\u20162 \u2264 2\u221ap with probability 1 \u2212 exp(\u2212C1p), where C1 > 0 is a universal constant. Furthermore, (\u2016q0\u20162q0)\u22a4u \u223c N(0, 1). Hence, there exists C0 > 0, s.t., with probability 0.99, |(\u2016q0\u20162q0)Tu| \u2265 C0. Hence, |q\u22a40 u| \u2265 C02\u221ap ."}, {"heading": "H Proof of Lemma 2", "text": "Proof. We prove the lemma using induction. The base case (for \u03c4 = 0) follows trivially.\nNow, by the inductive hypothesis, \u03b4\u03c4 \u2264 \u03b3 2t\u03b40\n1\u2212(1\u2212\u03b32t)\u03b40 . That is,\n1 \u03b4\u03c4 \u2265 1\u2212 (1\u2212 \u03b3 2t)\u03b40 \u03b32t\u03b40 .\nFinally, by assumption,\n\u03b4\u03c4+1 \u2264 \u03b32\n1 \u03b4\u03c4\n\u2212 (1\u2212 \u03b32) \u2264 \u03b32\n1\u2212(1\u2212\u03b32t)\u03b40 \u03b32t\u03b40\n\u2212 (1\u2212 \u03b32) .\nThe lemma follows after simplification of the above given expression."}, {"heading": "I Proof of Lemma 8", "text": "Proof. Using the generative model (2), we get:\nU\u22a4F\u03c4+1Q\u03c4v = \u039b\n(\n1\nB\n\u2211\nt\nztz \u22a4 t\n)\n\u039bU\u22a4Q\u03c4v +\n(\n1\nB\n\u2211\nt\nU\u22a4wtw \u22a4 t U\n)\nU\u22a4Q\u03c4v\n+( 1\nB\n\u2211\nt\nU\u22a4wtz \u22a4 t )\u039bU \u22a4Q\u03c4v+\u039b( 1\nB\n\u2211\nt\nztw \u22a4 t U)U \u22a4Q\u03c4v+\n(\n1\nB\n\u2211\nt\n(\u039bzt + U \u22a4wt)w \u22a4 t U\u22a5U \u22a4 \u22a5Q\u03c4\n)\nv.\n(23)\nNote that in the equation and rest of the proof, t varies from B\u03c4 < t \u2264 B(\u03c4 + 1). We now show that each of the five terms in the above given equation concentrate around their respective means. Also, let yt = U \u22a4wt and y\u22a5t = U \u22a4 \u22a5wt. Note that, yt \u223c N(0, \u03c32Ik\u00d7k) and y\u22a5t \u223c N(0, \u03c32I(p\u2212k)\u00d7(p\u2212k)). (a): Consider the first term in (23). Using \u2016Av\u20162 \u2264 \u2016A\u20162\u2016v\u20162 and the assumption that \u03bb1 = 1, we get: \u2016\u039b ( 1 B \u2211 t ztz \u22a4 t \u2212 I ) \u039bU\u22a4Q\u03c4v\u20162 \u2264 \u2016 ( 1 B \u2211 t ztz \u22a4 t \u2212 I )\n\u20162\u2016U\u22a4Q\u03c4v\u20162. Using Lemma 12 we get (w.p. 1\u2212 C/T ):\n\u2016 1 B \u2211\nt\nztz \u22a4 t \u2212 I\u20162 \u2264\n\u221a\nC1k log(T )\nB .\nThat is,\n\u2016\u039b( 1 B \u2211\nt\nztz \u22a4 t \u2212 I)\u039bU\u22a4Q\u03c4v\u20162 \u2264\n\u221a\nC1k log(T )\nB \u2016U\u22a4Q\u03c4v\u20162. (24)\n(b): Similarly, the second term in (23) can be bounded as (w.p. 1\u2212 C/T ):\n\u2016 ( 1\nB\n\u2211\nt\nU\u22a4wtw \u22a4 t U \u2212 \u03c32I\n)\nU\u22a4Q\u03c4v\u20162 \u2264 \u03c32 \u221a C1k log(T )\nB \u2016U\u22a4Q\u03c4v\u20162. (25)\n(c): Now consider the third and the fourth term. Now wt and zt are independent 0-mean Gaussians, hence using Lemma 15, we get: \u2016 1 B \u2211 t U \u22a4wtz\u22a4t \u20162 \u2264 \u03c3 \u221a C1k log(T ) B . Hence, w.p. 1\u2212 2C/T ,\n\u2016\u039b( 1 B \u2211\nt\nztw \u22a4 t U)U \u22a4Q\u03c4v\u2016+ \u2016( 1\nB\n\u2211\nt\nU\u22a4wtzt)\u039bU \u22a4Q\u03c4v\u2016 \u2264 2\u03c3\n\u221a\nC1k log(T )\nB \u2016U\u22a4Q\u03c4v\u20162. (26)\n(d): Finally, we consider the last term in (23). Note that, (\u039bzt + U \u22a4wt) \u223c N(0, D) where\nD is a diagonal matrix with Dii = \u03bb 2 i + \u03c3 2. Also, Q\u22a4U\u22a5U\u22a4\u22a5wt \u223c N(0, \u03c32I(p\u2212k)\u00d7(p\u2212k)) and is independent of (\u039bzt + U \u22a4wt) as E[Q\u22a4U\u22a5U\u22a4\u22a5wtw \u22a4 t U ] = 0; recall that for Gaussian RVs, covariance is zero iff RVs are independent. Hence, using Lemma 15, w.p. \u2265 1\u2212 C/T :\n\u2016( 1 B \u2211\nt\n(\u039bzt + U \u22a4wt)w \u22a4 t U\u22a5U \u22a4 \u22a5Q\u03c4 )v\u20162 \u2264\n\u221a 1 + \u03c32\u03c3\n\u221a\nC1k log(T )\nB . (27)\nNow, using (23), (24), (25), (26), (27) (w.p. \u2265 1\u2212 5C/T )\n\u2016U\u22a4F\u03c4+1Q\u03c4v\u20162 \u2265 \u2016(\u039b2+\u03c32I)U\u22a4Q\u03c4v\u20162\u2212 \u221a C1k log(T )\nB \u2016U\u22a4Q\u03c4v\u20162\n( (1 + \u03c3)2 + \u03c3 \u221a 1 + \u03c32\n\u2016U\u22a4Q\u03c4v\u20162\n)\n.\n(28)\nNow, \u2016U\u22a4Q\u03c4v\u20162 \u2265 \u03c3k(U\u22a4Q\u03c4v). Next, by using the inductive hypothesis (i.e., \u03c3k(UTQ\u03c4 ) \u2265 \u03c3k(U\nTQ\u03c4\u22121), induction step follows as we show that the error decreases at each step) and Lemma 10, we have \u2016U\u22a4Q\u03c4v\u20162 \u2265 \u03c3k(U\u22a4Q0) \u2265 C\u221apk with probability \u2265 0.99.\nAlso, \u2016(\u039b2+\u03c32I)U\u22a4Q\u03c4v\u20162 \u2265 (\u03bb2k+\u03c32)\u2016U\u22a4Q\u03c4v\u20162. Additionally, \u2016U\u22a4Q\u03c4v\u20162 \u2265 \u221a 1\u2212 \u2016U\u22a4\u22a5Q\u03c4\u201622. Hence, lemma follows by using these facts with (28) and by selecting B as given in Theorem 3."}, {"heading": "J Proof of Lemma 9", "text": "Proof. Similar to our proof for Lemma 8, we separate out the \u201cerror\u201d or deviation terms in \u2016U\u22a4\u22a5F\u03c4+1Q\u03c4\u20162 and bound them using concentration bounds. Now,\n\u2016U\u22a4\u22a5F\u03c4+1Q\u03c4v\u20162 = \u2016U\u22a4\u22a5 (U\u039b2U\u22a4 + \u03c32I + E\u03c4 )Q\u03c4v\u20162 \u2264 \u2016\u03c32U\u22a4\u22a5Q\u03c4v\u20162 + \u2016U\u22a4\u22a5E\u03c4Q\u03c4v\u20162 \u2264 \u03c32\u2016U\u22a4\u22a5Q\u03c4v\u20162 + \u2016E\u03c4\u20162, (29)\nwhere E\u03c4 is the error matrix representing deviation of the estimate F\u03c4+1 from its mean. That is,\nE = 1\nB\n\u2211\nt\nxtx \u22a4 t \u2212 U\u039b2U\u22a4 \u2212 \u03c32I\n= U\u039b( 1\nB\n\u2211\nt\nztz \u22a4 t \u2212 I)\u039bU\u22a4 + (\n1\nB\n\u2211\nt\nwtw \u22a4 t \u2212 \u03c32I)\n+ U\u039b 1\nB\n\u2211\nt\nztw \u22a4 t +\n1\nB\n\u2211\nt\nwtz \u22a4 t \u039bU. (30)\nNote that the above given four terms correspond to similar four terms in (23) and hence can be bounded in similar fashion. In particular, the following holds with probability 1\u2212 4C/T :\n\u2016E\u20162 \u2264 \u221a C1k log(T )\nB + \u03c32\n\u221a\nC1p log(T )\nB + 2\u03c3\n\u221a\nC1p log(T )\nB \u2264 \u03bb2k\u01eb/2, (31)\nwhere the second inequality follows by setting B as required by Theorem 3. The lemma now follows using (29), (30), (31)."}, {"heading": "K Proof of Lemma 10", "text": "Proof. Using Step 2 of Algorithm 1: H = Q0R0. Let vk be the singular vector of U \u22a4Q0 corresponding to the smallest singular value. Then,\n\u03c3k(U \u22a4Q0) = \u2016U\u22a4Q0R0R\u221210 vk\u20162 \u2016R\u221210 vk\u20162 \u2016R\u221210 vk\u20162\n\u2265 \u03c3k(U\u22a4Q0R0)\u03c3k(R\u221210 ). (32)\nNow, \u03c3k(R \u22121 0 ) = 1 \u2016R0\u20162 = 1 \u2016Q0R0\u20162 = 1 \u2016H\u20162 . Note that \u2016H\u20162 is the spectral norm of a random matrix with i.i.d. Gaussian entries and hence can be easily bounded using standard results. In particular, using Lemma 13, we get: \u2016H\u20162 \u2264 C1\u221ap w.p. \u2265 1 \u2212 e\u2212C2p, where C1, C2 > 0 are global constants.\nBy Theorem 1.1 of [17] (see Lemma 14), w.p. \u2265 0.99, \u03c3k(U\u22a4Q0R0) = \u03c3k(H) \u2265 C/ \u221a k.\nThe lemma now follows using the above two bounds with (32)."}], "references": [{"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "In 50th Allerton Conference on Communication,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["L. Balzano", "R. Nowak", "B. Recht"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Fast low-rank modifications of the thin singular value decomposition", "author": ["M. Brand"], "venue": "Linear algebra and its applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Incremental singular value decomposition of uncertain data with missing values", "author": ["Matthew Brand"], "venue": "Vision\u2014ECCV", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Numerical linear algebra in the streaming model", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the 41st annual ACM symposium on Theory of computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Tracking a few extreme singular values and vectors in signal processing", "author": ["P. Comon", "G.H. Golub"], "venue": "Proceedings of the IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM review,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Online robust subspace tracking from partial information", "author": ["J. He", "L. Balzano", "J. Lui"], "venue": "arXiv preprint arXiv:1109.3827,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K. Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "On the distribution of the largest eigenvalue in principal components analysis.(english", "author": ["Iain M. Johnstone"], "venue": "Ann. Statist,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "On incremental and robust subspace learning", "author": ["Y. Li"], "venue": "Pattern recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Finite sample approximation results for principal component analysis: a matrix perturbation approach", "author": ["Boaz Nadler"], "venue": "The Annals of Statistics, page 2791\u20132817,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Fast collapsed gibbs sampling for latent dirichlet allocation", "author": ["Ian Porteous", "David Newman", "Alexander Ihler", "Arthur Asuncion", "Padhraic Smyth", "MaxWelling"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1951}, {"title": "EM algorithms for PCA and SPCA. Advances in neural information processing systems, page", "author": ["Sam Roweis"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Probabilistic principal component analysis", "author": ["Michael E. Tipping", "Christopher M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "How close is the sample covariance matrix to the actual covariance matrix", "author": ["R. Vershynin"], "venue": "Journal of Theoretical Probability,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "This direction was largely influenced by Johnstone\u2019s spiked covariance model, where data samples are drawn from a distribution whose (population) covariance is a low-rank perturbation of the identity matrix [11].", "startOffset": 207, "endOffset": 211}, {"referenceID": 17, "context": "Work initiated there, and also work done in [19] (and references therein) has explored the power of batch PCA in the p-dimensional setting with sub-Gaussian noise, and demonstrated that the singular value decomposition (SVD) of the empirical covariance matrix succeeds in recovering the principal components (extreme eigenvectors of the population covariance) with high probability, given n = O(p) samples.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": ", [21, 1].", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "A memory-light variant described in [1] typically requires much less memory, but there are no guarantees for this, and moreover, for certain problem instances, its memory requirement is on the order of p.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": ", [5, 13, 8].", "startOffset": 2, "endOffset": 12}, {"referenceID": 11, "context": ", [5, 13, 8].", "startOffset": 2, "endOffset": 12}, {"referenceID": 6, "context": ", [5, 13, 8].", "startOffset": 2, "endOffset": 12}, {"referenceID": 4, "context": "Indeed, it is straightforward to check that the guarantees presented in ([5, 8]) are not strong enough to guarantee recovery of the spike.", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "Indeed, it is straightforward to check that the guarantees presented in ([5, 8]) are not strong enough to guarantee recovery of the spike.", "startOffset": 73, "endOffset": 79}, {"referenceID": 3, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 5, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 37, "endOffset": 43}, {"referenceID": 7, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 37, "endOffset": 43}, {"referenceID": 14, "context": "In a Bayesian mindset, some researchers have come up with expectation maximization approaches [16, 18], that can be used in an incremental fashion.", "startOffset": 94, "endOffset": 102}, {"referenceID": 16, "context": "In a Bayesian mindset, some researchers have come up with expectation maximization approaches [16, 18], that can be used in an incremental fashion.", "startOffset": 94, "endOffset": 102}, {"referenceID": 13, "context": "Stochastic-approximation-based algorithms along the lines of [15] are also quite popular, because of their low computational and memory complexity, and excellent performance in practice.", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "They go under a variety of names, including Incremental PCA (though the term Incremental has been used in the online setting as well [10]), Hebbian learning, and stochastic power method [1].", "startOffset": 133, "endOffset": 137}, {"referenceID": 0, "context": "They go under a variety of names, including Incremental PCA (though the term Incremental has been used in the online setting as well [10]), Hebbian learning, and stochastic power method [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 0, "context": "where Proj(\u00b7) denotes the \u201cprojection\u201d that takes the SVD of the argument, and sets the top k singular values to 1 and the rest to zero (see [1] for further discussion).", "startOffset": 141, "endOffset": 144}, {"referenceID": 18, "context": "In this regime, it is well-known that batch-PCA is asymptotically consistent (hence recovering A up to unitary transformations) with number of samples scaling as n = O(p) [20].", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "Stochastic versions of the power method are already popular in the literature and are known to have good empirical performance; see [1] for a nice review of such methods.", "startOffset": 132, "endOffset": 135}, {"referenceID": 12, "context": "We used three bag-of-words datasets from [14].", "startOffset": 41, "endOffset": 45}], "year": 2013, "abstractText": "We consider streaming, one-pass principal component analysis (PCA), in the highdimensional regime, with limited memory. Here, p-dimensional samples are presented sequentially, and the goal is to produce the k-dimensional subspace that best approximates these points. Standard algorithms require O(p2) memory; meanwhile no algorithm can do better than O(kp) memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the spiked covariance model, where p-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, n, scales proportionally with the dimension, p. Yet, all algorithms that provably achieve this, have memory complexity O(p2). Meanwhile, algorithms with memory-complexity O(kp) do not have provable bounds on sample complexity comparable to p. We present an algorithm that achieves both: it uses O(kp) memory (meaning storage of any kind) and is able to compute the k-dimensional spike with O(p log p) sample-complexity \u2013 the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.", "creator": "LaTeX with hyperref package"}}}