{"id": "1307.1662", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2013", "title": "Polyglot: Distributed Word Representations for Multilingual NLP", "abstract": "distributed word representations ( word embeddings ) have recently contributed to enhanced performance in language modeling and several communication modeling. in a work, we trace word embeddings for more then 100 languages using their corresponding categories. we quantitatively demonstrate the effect of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. experts find their performance to be competitive with near state - of - art methods in english, danish and canadian. sometimes, we investigate the semantic features extracted by these embeddings through the proximity of fuzzy groupings. we will release these embeddings publicly to join researchers in the development and enhancement of multilingual applications.", "histories": [["v1", "Fri, 5 Jul 2013 16:52:09 GMT  (345kb,D)", "http://arxiv.org/abs/1307.1662v1", "10 pages, 2 figures, Proceedings of Conference on Computational Natural Language Learning CoNLL'2013"], ["v2", "Fri, 27 Jun 2014 17:31:33 GMT  (345kb,D)", "http://arxiv.org/abs/1307.1662v2", "10 pages, 2 figures, Proceedings of Conference on Computational Natural Language Learning CoNLL'2013"]], "COMMENTS": "10 pages, 2 figures, Proceedings of Conference on Computational Natural Language Learning CoNLL'2013", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rami al-rfou", "bryan perozzi", "steven skiena"], "accepted": false, "id": "1307.1662"}, "pdf": {"name": "1307.1662.pdf", "metadata": {"source": "CRF", "title": "Polyglot: Distributed Word Representations for Multilingual NLP", "authors": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "emails": ["skiena}@cs.stonybrook.edu"], "sections": [{"heading": "1 Introduction", "text": "Building multilingual processing systems is a challenging task. Every NLP task involves different stages of preprocessing and calculating intermediate representations that will serve as features for later stages. These stages vary in complexity and requirements for each individual language. Despite recent momentum towards developing multilingual tools (Nivre et al., 2007; Hajic\u030c et al., 2009; Pradhan et al., 2012), most of NLP research still focuses on rich resource languages. Common NLP systems and tools rely heavily on English specific features and they are infrequently tested on multiple datasets. This makes them hard to port to new languages and tasks (Blitzer et al., 2006).\nA serious bottleneck in the current approach for developing multilingual systems is the require-\nment of familiarity with each language under consideration. These systems are typically carefully tuned with hand-manufactured features designed by experts in a particular language. This approach can yield good performance, but tends to create complicated systems which have limited portability to new languages, in addition to being hard to enhance and maintain.\nRecent advancements in unsupervised feature learning present an intriguing alternative. Instead of relying on expert knowledge, these approaches employ automatically generated taskindependent features (or word embeddings) given large amounts of plain text. Recent developments have led to state-of-art performance in several NLP tasks such as language modeling (Bengio et al., 2006; Mikolov et al., 2010), and syntactic tasks such as sequence tagging (Collobert et al., 2011). These embeddings are generated as a result of training \u201cdeep\u201d architectures, and it has been shown that such representations are well suited for domain adaptation tasks (Glorot et al., 2011; Chen et al., 2012).\nWe believe two problems have held back the research community\u2019s adoption of these methods. The first is that learning representations of words involves huge computational costs. The process usually involves processing billions of words over weeks. The second is that so far, these systems have been built and tested mainly on English.\nIn this work we seek to remove these barriers to entry by generating word embeddings for over a hundred languages using state-of-the-art techniques. Specifically, our contributions include:\n\u2022 Word embeddings - We will release word embeddings for the hundred and seventeen languages that have more than 10,000 articles on Wikipedia. Each language\u2019s vocabulary will contain up to 100,000 words. The embeddings will be publicly available at\nar X\niv :1\n30 7.\n16 62\nv1 [\ncs .C\nL ]\n5 J\nul 2\n01 3\n(www.cs.stonybrook.edu/\u02dcdsl), for the research community to study their characteristics and build systems for new languages. We believe our embeddings represent a valuable resource because they contain a minimal amount of normalization. For example, we do not lower case words for European languages as other studies have done for English. This preserves features of the underlying language.\n\u2022 Quantitative analysis - We investigate the embedding\u2019s performance on a part-ofspeech (PoS) tagging task, and conduct qualitative investigation of the syntactic and semantic features they capture. Our experiments represent a valuable chance to evaluate distributed word representations for NLP as the experiments are conducted in a consistent manner and a large number of languages are covered. As the embeddings capture interesting linguistic features, we believe the multilingual resource we are providing gives researchers a chance to create multilingual comparative experiments.\n\u2022 Efficient implementation - Training these models was made possible by our contributions to Theano (machine learning library (Bergstra et al., 2010)). These optimizations empower researchers to produce word embeddings under different settings or for different corpora than Wikipedia.\nThe rest of this paper is as follows. In Section 2, we give an overview of semi-supervised learning and learning representations related work. We then describe, in Section 3, the network used to generate the word embeddings and its characteristics. Section 4 discusses the details of the corpus collection and preparation steps we performed. Next, in Section 5, we discuss our experimental setup and the training progress over time. In Section 6 we discuss the semantic features captured by the embeddings by showing examples of the word groupings in multiple languages. Finally, in Section 7 we demonstrate the quality of our learned features by training a PoS tagger on several languages and then conclude."}, {"heading": "2 Related Work", "text": "There is a large body of work regarding semisupervised techniques which integrate unsuper-\nvised feature learning with discriminative learning methods to improve the performance of NLP applications. Word clustering has been used to learn classes of words that have similar semantic features to improve language modeling (Brown et al., 1992) and knowledge transfer across languages (Ta\u0308ckstro\u0308m et al., 2012). Dependency parsing and other NLP tasks have been shown to benefit from such a large unannotated corpus (Koo et al., 2008), and a variety of unsupervised feature learning methods have been shown to unilaterally improve the performance of supervised learning tasks (Turian et al., 2010). (Klementiev et al., 2012) induce distributed representations for a pair of languages jointly, where a learner can be trained on annotations present in one language and applied to test data in another.\nLearning distributed word representations is a way to learn effective and meaningful information about words and their usages. They are usually generated as a side effect of training parametric language models as probabilistic neural networks. Training these models is slow and takes a significant amount of computational resources (Bengio et al., 2006; Dean et al., 2012). Several suggestions have been proposed to speed up the training procedure, either by changing the model architecture to exploit an algorithmic speedup (Mnih and Hinton, 2009; Morin and Bengio, 2005) or by estimating the error by sampling (Bengio and Senecal, 2008).\n(Collobert and Weston, 2008) shows that word embeddings can almost substitute NLP common features on several tasks. The system they built, SENNA, offers part of speech tagging, chunking, named entity recognition, semantic role labeling and dependency parsing (Collobert, 2011). The system is built on top of word embeddings and performs competitively compared to state of art systems. In addition to pure performance, the system has a faster execution speed than comparable NLP pipelines (Al-Rfou\u2019 and Skiena, 2012).\nTo speed up the embedding generation process, SENNA embeddings are generated through a procedure that is different from language modeling. The representations are acquired through a model that distinguishes between phrases and corrupted versions of them. In doing this, the model avoids the need to normalize the scores across the vocabulary to infer probabilities. (Chen et al., 2013) shows that the embeddings generated by SENNA\nperform well in a variety of term-based evaluation tasks. Given the training speed and prior performance on NLP tasks in English, we generate our multilingual embeddings using a similar network architecture to the one SENNA used.\nHowever, our work differs from SENNA in the following ways. First, we do not limit our models to English, we train embeddings for a hundred and seventeen languages. Next, we preserve linguistic features by avoiding excessive normalization to the text. For example, our English model places \u201cApple\u201d closer to IT companies and \u201capple\u201d to fruits. More examples of linguistic features preserved by our model are shown in Table 1. This gives us the chance to evaluate the embeddings performance over PoS tagging without the need for manufactured features. Finally, we release the embeddings and the resources necessary to generate them to the community to eliminate any barriers.\nDespite the progress made in creating distributed representations, combining them to produce meaning is still a challenging task. Several approaches have been proposed to address feature compositionality for semantic problems such as paraphrase detection (Socher et al., 2011), and sentiment analysis (Socher et al., 2012) using word embeddings."}, {"heading": "3 Distributed Word Representation", "text": "Distributed word representations (word embeddings) map the index of a word in a dictionary to a feature vector in high-dimension space. Every dimension contributes to multiple concepts, and every concept is expressed by a combination of subset of dimensions. Such mapping is learned by back-propagating the error of a task through the model to update random initialized embeddings. The task is usually chosen such that examples can be automatically generated from unlabeled data (i.e so it is unsupervised). In case of language modeling, the task is to predict the last word of a phrase that consists of n words.\nIn our work, we start from the example construction method outlined in (Bengio et al., 2009). They train a model by requiring it to distinguish between the original phrase and a corrupted version of the phrase. If it does not score the original one higher than the corrupted one (by a margin), the model will be penalized. More precisely, for a given sequence of words S = [wi\u2212n . . . wi . . . wi+n] observed in the corpus T , we will construct another corrupted sequence S\u2032 by replacing the word in the middle wi with a word wj chosen randomly from the vocabulary. The neural network represents a function score that scores each phrase, the model is penalized through the hinge loss function J(T ) as shown in 1.\nJ(T ) = 1 |T | \u2211 i\u2208T |1\u2212score(S\u2032)+score(S)|+ (1)\nFigure 1 shows a neural network that takes a sequence of words with size 2n + 1 to compute a score. First, each word is mapped through a vocabulary dictionary with the size |V | to an index that is used to index a shared matrix C with the size |V |\u2217M where M is the size of the vector representing the word. Once the vectors are retrieved, they are concatenated into one vector called projection layer P with size (2n + 1) \u2217M . The projection layer plays the role of an input to a hidden layer with size |H|, the activations A of which are calculated according to equation 3, where W1, b1 are the weights and bias of the hidden layer.\nA = tanh(W1P + b1) (2)\nTo calculate the phrase score, a linear combination of the hidden layer activations A is computed using W2 and b2.\nscore(P ) = W2A+ b2 (3)\nTherefore, the five parameters that have to be learned are W1, W2, b1, b2, C with a total number of parameters (2n+ 1) \u2217M \u2217H +H +H + 1+ |V | \u2217M \u2248M \u2217 (nH + |V |) ."}, {"heading": "4 Corpus Preparation", "text": "We have chosen to generate our word embeddings from Wikipedia. In addition to size, there are other desirable properties that we wish for the source of our language model to have:\n\u2022 Size and variety of languages - As of this writing (April, 2013), 42 languages had more than 100,000 article pages, and 117 languages had more than 10,000 article pages. \u2022 Well studied - Wikipedia is a prolific re-\nsource in the literature, and has been used for a variety of problems. Particularly, Wikipedia is well suited for multilingual applications (Navigli and Ponzetto, 2010). \u2022 Quality - Wikipedians strive to write arti-\ncles that are readable, accurate, and consist of good grammar. \u2022 Openly accessible - Wikipedia is a resource\navailable for free use by researchers \u2022 Growing - As technology becomes more ac-\ncessible, the size and scope of the multilingual Wikipedia effort continues to expand.\nTo process Wikipedia markup, we first extract the text using a modified version of the Bliki en-\ngine1. Next we must tokenize the text. We rely on an OpenNLP probabilistic tokenizer whenever possible, and default to the Unicode text segmentation2 algorithm offered by Lucene when we have no such OpenNLP model. After tokenization, we normalize the tokens to reduce their sparsity. We have two main normalization rules. The first replaces digits with the symbol #, so \u201c1999\u201d becomes ####. In the second, we remove hyphens and brackets that appear in the middle of a token. As an additional rule for English, we map nonLatin characters to their unicode block groups.\nIn order to capture the syntactic and semantic features of words, we must observe each word several times in each of its valid contexts. This requirement, when combined with the Zipfian distribution of words in natural language, implies that learning a meaningful representation of a language requires a huge amount of unstructured text. In practice we deal with this limitation by restricting ourselves to considering the most frequently occurring tokens in each language.\nTable 2 shows the size of each language corpus in terms of tokens, number of word types and coverage of text achieved by building a vocabulary out of the most frequent 100,000 tokens, |V |. Out of vocabulary (OOV) words are replaced with a special token \u3008UNK\u3009.\nWhile Wikipedia has 284 language specific encyclopedias, only five of them have more than a million articles. The size drops dramatically, such that the 42nd largest Wikipedia, Hindi, has slightly above 100,000 articles and the 100th, Tatar, has slightly over 16,000 articles3.\nSignificant Wikipedias in size have a word coverage over 92% except for German, Russian, Arabic and Czech which shows the effect of heavy usage of morphological forms in these languages on the word usage distribution.\nThe highest word coverage we achieve is unsurprisingly for Chinese. This is expected given the limited size vocabulary of the language - the number of entries in the Contemporary Chinese Dictionary are estimated to be 65 thousand words (Shuxiang, 2004).\n1Java Wikipedia API (Bliki engine) - http://code. google.com/p/gwtwiki/\n2http://www.unicode.org/reports/tr29/ 3http://meta.wikimedia.org/w/index.\nphp?title=List_of_Wikipedias&oldid= 5248228"}, {"heading": "5 Training", "text": "For our experiments, we build a model as the one described in Section 3 using Theano (Bergstra et al., 2010). We choose the following parameters, context window size 2n + 1 = 5, vocabulary |V | = 100, 000, word embedding size M = 64, and hidden layer size H = 32. The intuition, here, is to maximize the relative size of the embeddings compared to the rest of the network. This might force the model to store the necessary information in the embeddings matrix instead of the hidden layer. Another benefit is that we will avoid overfitting on the smaller Wikipedias. Increasing the window size or the embedding size slows down the training speed, making it harder to converge within a reasonable time.\nThe examples are generated by sweeping a window over sentences. For each sentence in the corpus, all unknown words are replaced with a special token \u3008UNK\u3009 and sentences are padded with \u3008S\u3009, \u3008/S\u3009 tokens. In case the window exceeds the edges of a sentence, the missing slots are filled with our padding token, \u3008PAD\u3009.\nTo train the model, we consider the data in minibatches of size 16. Every 16 examples, we estimate the gradient using stochastic gradient descent (Bottou, 1991), and update the parameters which contributed to the error using backpropagation (Rumelhart et al., 2002). Calculating an exact gradient is prohibitive given that the dataset size is in millions of examples. We calculate the development error by sampling randomly 10000 minibatches from the development dataset.\nFor each language, we set the batch size to 16 examples, and the learning rate to be 0.1. Following, (Collobert et al., 2011)\u2019s advice, we divide each layer by the fan in of that layer, and we consider the embeddings layer to have a fan in of 1. We divide the corpus to three sets, training, development and testing with the following percentages 90, 5, 5 respectively.\nOne disadvantage of the approach used by (Collobert et al., 2011) is that there is no clear stopping criteria for the model training process. We have noticed that after a few weeks of training, the model\u2019s performance reaches the point where there is no significant decrease in the average loss over the development set, and when this occurs we manually stop the training. An interesting property of this model is that we did not notice any sign of overfitting for large Wikipedias. This could be explained by the infinite amount of examples we can generate by randomly choosing the re-\nplacement word in the corrupted phrase. Figure 2 shows a typical learning curve of the training. As the number of examples have been seen so far increased both the training error and the development error go down."}, {"heading": "6 Qualitative Analysis", "text": "In order to understand how the embeddings space is organized, we examine the subtle information captured by the embeddings through investigating the proximity of word groups. This information has the potential to help researchers develop applications that use such semantic and syntactic information. The embeddings not only capture syntactic features, as we will demonstrate in Section 4, but also demonstrate the ability to capture interesting semantic information. Table 3 shows different words in several languages. For each word on top of each list, we rank the vocabulary according to their Euclidean distance from that word and show the closest five neighboring words. \u2022 French & Spanish - Expected groupings of\ncolors and professions is clearly observed. \u2022 English - The example shows how the em-\nbedding space is aware of the name change that happened to a group of Indian cities. \u201cMumbai\u201d used to be called \u201cBombay\u201d, \u201cChennai\u201d used to be called \u201cMadras and \u201cKolkata\u201d used to be called \u201cCalcutta\u201d. On the other hand, \u201cHyderabad\u201d stayed at a similar distance from both names as they point to the same conceptual meaning. \u2022 Arabic - The first example shows the word\n\u201cThanks\u201d. Despite not removing the diacrit-\nics from the text, the model learned that the two surface forms of the word mean similar things and, therefore, grouped them together. In Arabic, conjunction words do not get separated from the following word. Usually, \u201dand thanks\u201d serves as a letter signature as \u201csincerely\u201d is used in English. The model learned that both words {\u201cand thanks\u201d, \u201cthanks\u201d } are similar, regardless their different forms. The second example illustrates a specific syntactic morphological feature of Arabic, where enumeration of couples has its own form. \u2022 German - The example demonstrates that the\ncompositional semantics of multi-unit words are still preserved. \u2022 Russian - The model learned to group Rus-\nsian/Soviet leaders and other figures related to the Soviet history together. \u2022 Chinese - The list contains three solar terms\nthat are part of the traditional East Asian lunisolar calendars. The remaining two terms correspond to traditional holidays that occur at the same dates of these solar terms. \u2022 Italian - The model learned that the lower\nand upper cases of the word has similar meaning."}, {"heading": "7 Sequence Tagging", "text": "Here we analyze the quality of the models we have generated. To test the quantitative performance of the embeddings, we use them as the sole features for a well studied NLP task, part of speech tagging.\nTo demonstrate the capability of the learned dis-\ntributed representations in extracting useful word features, we train a PoS tagger over the subset of languages that we were able to acquire free annotated resources for. We choose our tagger for this task to be a neural network because it has a fast convergence rate based on our initial experiments.\nThe part of speech tagger has similar architecture to the one used for training the embeddings. However we have changed some of the network parameters, specifically, we use a hidden layer of size 300 and learning rate of 0.3. The network is trained by minimizing the negative of the log likelihood of the labeled data. To tag a specific word wi we consider a window with size 2n where n in our experiment is equal to 2. Equation 4 shows how we construct a feature vector F by concatenating (\u2295) the embeddings of the words occurred in the window, where C is the matrix that contains the embeddings of the language vocabulary.\nF = i+2\u2295\nj=i\u22122 C[wj ] (4)\nThe feature vector will be fed to the network and the error will back propagated back to the embeddings.\nThe results of this experiment are presented in Table 4. We train and test our models on the universal tagset proposed by (Petrov et al., 2012). This universal tagset maps each original tag in a treebank to one out of twelve general PoS tags. This simplifies the comparison of classifiers performance across languages. We compare our results to a similar experiment conducted in their\nwork, where they trained a TnT tagger (Brants, 2000) on several treebanks. The TnT tagger is based on Markov models and depends on trigram counts observed in the labeled data. It was chosen for its fast speed and (near to) state-of-the-art accuracy, without language specific tuning.\nThe performance of embeddings is competitive in general. Surprisingly, it is doing better than the TnT tagger in English and Danish. Moreover, our performance is so close in the case of Swedish. This task is hard for our tagger for two reasons. The first is that we do not add OOV words seen during training of the tagger to our vocabulary. The second is that all OOV words are substituted with one representation, \u3008UNK\u3009 and there is no character level information used to inform the tagger about the characteristic of the OOV words.\nOn the other hand, the performance on the known words is strong and consistent showing the value of the features learned about these words from the unsupervised stage. Although the word coverage of German and Czech are low in the original Wikipedia corpora (See Table 2), the features learned are achieving great accuracy on the known words. They both achieve above 98.5% accuracy. It is noticeable that the Slovene model performs the worst, under both known and unknown words categories. It achieves only 93.46% accuracy on the test dataset. Given that the Slovene embeddings were trained on the least amount of data among all other embeddings we test here, we expect the quality to go lower for the other smaller Wikipedias not tested here.\nIn Table 5, we present how well the vocabulary of each language\u2019s embeddings covered the part of speech datasets. The datasets come from a different domain than Wikipedia, and this is reflected in the results.\nIn Table 6, we present the results of training the same neural network part of speech tagger without using our embeddings as initializations. We found that the embeddings benefited all the languages we considered, and observed the greatest benefit in languages which had a small number of training examples. We believe that these results illustrate the performance"}, {"heading": "8 Conclusion", "text": "Distributed word representations represent a valuable resource for any language, but particularly for resource-scarce languages. We have demonstrated how word embeddings can be used as off-the-shelf solution to reach near to state-of-art performance over a fundamental NLP task, and we believe that our embeddings will help researchers to develop tools in languages with which they have no expertise.\nMoreover, we showed several examples of interesting semantic relations expressed in the embeddings space that we believe will lead to interesting applications and improve tasks as semantic compositionality.\nWhile we have only considered the properties of word embeddings as features in this work, it has been shown that using word embeddings in conjunction with traditional NLP features can signifi-\ncantly improve results on NLP tasks (Turian et al., 2010; Collobert et al., 2011). With this in mind, we believe that the entire research community can benefit from our release of word embeddings for over 100 languages.\nWe hope that these resources will advance the study of possible pair-wise mappings between embeddings of several languages and their relations. Our future work in this area includes improving the models by increasing the size of the context window and their domain adaptivity through incorporating other sources of data. We will be investigating better strategies for modeling OOV words. We see improvements to OOV word handling as essential to ensure robust performance of the embeddings on real-world tasks."}, {"heading": "Acknowledgments", "text": "This research was partially supported by NSF Grants DBI-1060572 and IIS-1017181, with additional support from TexelTek."}], "references": [{"title": "Floresta sint\u00e1 (c) tica\u201d: a treebank for portuguese", "author": ["Susana Afonso", "Eckhard Bick", "Renato Haber", "Diana Santos."], "venue": "Proc. of the Third Intern. Conf. on Language Resources and Evaluation (LREC), pages 1698\u20131703.", "citeRegEx": "Afonso et al\\.,? 2002", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "Speedread: A fast named entity recognition pipeline", "author": ["Rami Al-Rfou", "Steven Skiena"], "venue": null, "citeRegEx": "Al.Rfou. and Skiena.,? \\Q2012\\E", "shortCiteRegEx": "Al.Rfou. and Skiena.", "year": 2012}, {"title": "Prague Dependency Treebank 2.5 \u2013 a revisited version of PDT 2.0", "author": ["Eduard Bej\u010dek", "Jarmila Panevov\u00e1", "Jan Popelka", "Pavel Stra\u0148\u00e1k", "Magda \u0160ev\u010d\u0131\u0301kov\u00e1", "Jan \u0160t\u011bp\u00e1nek", "Zden\u011bk \u017dabokrtsk\u00fd"], "venue": "In Proceedings of COLING", "citeRegEx": "Bej\u010dek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bej\u010dek et al\\.", "year": 2012}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Yoshua Bengio", "J-S Senecal."], "venue": "Neural Networks, IEEE Transactions on, 19(4):713\u2013722.", "citeRegEx": "Bengio and Senecal.,? 2008", "shortCiteRegEx": "Bengio and Senecal.", "year": 2008}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.S. Sen\u00e9cal", "F. Morin", "J.L. Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013 186.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston."], "venue": "International Conference on Machine Learning, ICML.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Stochastic gradient learning in neural networks", "author": ["L\u00e9on Bottou."], "venue": "Proceedings of Neuro-N\u0131\u0302mes 91, Nimes, France. EC2.", "citeRegEx": "Bottou.,? 1991", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "The tiger treebank", "author": ["Sabine Brants", "Stefanie Dipper", "Silvia Hansen", "Wolfgang Lezius", "George Smith."], "venue": "IN PROCEEDINGS OF THE WORKSHOP ON TREEBANKS AND LINGUISTIC THEORIES, pages 24\u201341.", "citeRegEx": "Brants et al\\.,? 2002", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "Tnt: a statistical part-ofspeech tagger", "author": ["Thorsten Brants."], "venue": "Proceedings of the sixth conference on Applied natural language processing, pages 224\u2013231. Association for Computational Linguistics.", "citeRegEx": "Brants.,? 2000", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational linguistics, 18(4):467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha."], "venue": "John Langford and", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "The expressive power of word embeddings. CoRR, abs/1301.3226", "author": ["Yanqing Chen", "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston."], "venue": "International Conference on Machine Learning, ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "J. Mach. Learn. Res., 12:2493\u20132537, November.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep learning for efficient discriminative parsing", "author": ["Ronan Collobert."], "venue": "AISTATS.", "citeRegEx": "Collobert.,? 2011", "shortCiteRegEx": "Collobert.", "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc Le", "Mark Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Ng"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Towards a slovene dependency treebank", "author": ["Sa\u0161o D\u017eeroski", "Toma\u017e Erjavec", "Nina Ledinek", "Petr Pajas", "Zdenek \u017dabokrtsky", "Andreja \u017dele."], "venue": "Proc. of the Fifth Intern. Conf. on Language Resources and Evaluation (LREC).", "citeRegEx": "D\u017eeroski et al\\.,? 2006", "shortCiteRegEx": "D\u017eeroski et al\\.", "year": 2006}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proceedings of the Twenty-eight International Conference on Machine Learning (ICML\u201911), volume 27,", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "Proceedings of COLING 2012, pages 1459\u20131474, Mumbai, India, December. The COLING 2012 Organizing Committee.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "In Proc. ACL/HLT.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "The danish dependency treebank and the dtag treebank tool", "author": ["Matthias Trautner Kromann."], "venue": "Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT), page 217.", "citeRegEx": "Kromann.,? 2003", "shortCiteRegEx": "Kromann.", "year": 2003}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernocky", "S. Khudanpur."], "venue": "Proceedings of Interspeech.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, 21:1081\u2013 1088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of the international workshop on artificial intelligence and statistics, pages 246\u2013252.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Babelnet: Building a very large multilingual semantic network", "author": ["Roberto Navigli", "Simone Paolo Ponzetto."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 216\u2013225. Association for Computational Lin-", "citeRegEx": "Navigli and Ponzetto.,? 2010", "shortCiteRegEx": "Navigli and Ponzetto.", "year": 2010}, {"title": "Talbanken05: A swedish treebank with phrase structure and dependency annotation", "author": ["Joakim Nivre", "Jens Nilsson", "Johan Hall."], "venue": "Proceedings of the fifth International Conference on Language Resources and Evaluation (LREC), pages 1392\u20131395.", "citeRegEx": "Nivre et al\\.,? 2006", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet U\u011fur Do\u011fan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "author": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang."], "venue": "Proceedings of the Sixteenth Conference on Computational Natu-", "citeRegEx": "Pradhan et al\\.,? 2012", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Learning representations by backpropagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams."], "venue": "Cognitive modeling, 1:213.", "citeRegEx": "Rumelhart et al\\.,? 2002", "shortCiteRegEx": "Rumelhart et al\\.", "year": 2002}, {"title": "The Contemporary Chinese Dic", "author": ["Lu Shuxiang"], "venue": null, "citeRegEx": "Shuxiang.,? \\Q2004\\E", "shortCiteRegEx": "Shuxiang.", "year": 2004}], "referenceMentions": [{"referenceID": 29, "context": "Despite recent momentum towards developing multilingual tools (Nivre et al., 2007; Haji\u010d et al., 2009; Pradhan et al., 2012), most of NLP", "startOffset": 62, "endOffset": 124}, {"referenceID": 31, "context": "Despite recent momentum towards developing multilingual tools (Nivre et al., 2007; Haji\u010d et al., 2009; Pradhan et al., 2012), most of NLP", "startOffset": 62, "endOffset": 124}, {"referenceID": 7, "context": "This makes them hard to port to new languages and tasks (Blitzer et al., 2006).", "startOffset": 56, "endOffset": 78}, {"referenceID": 4, "context": "Recent developments have led to state-of-art performance in several NLP tasks such as language modeling (Bengio et al., 2006; Mikolov et al., 2010), and syntactic tasks such as sequence tagging (Collobert et al.", "startOffset": 104, "endOffset": 147}, {"referenceID": 24, "context": "Recent developments have led to state-of-art performance in several NLP tasks such as language modeling (Bengio et al., 2006; Mikolov et al., 2010), and syntactic tasks such as sequence tagging (Collobert et al.", "startOffset": 104, "endOffset": 147}, {"referenceID": 19, "context": "These embeddings are generated as a result of training \u201cdeep\u201d architectures, and it has been shown that such representations are well suited for domain adaptation tasks (Glorot et al., 2011; Chen et al., 2012).", "startOffset": 169, "endOffset": 209}, {"referenceID": 12, "context": "These embeddings are generated as a result of training \u201cdeep\u201d architectures, and it has been shown that such representations are well suited for domain adaptation tasks (Glorot et al., 2011; Chen et al., 2012).", "startOffset": 169, "endOffset": 209}, {"referenceID": 6, "context": "\u2022 Efficient implementation - Training these models was made possible by our contributions to Theano (machine learning library (Bergstra et al., 2010)).", "startOffset": 126, "endOffset": 149}, {"referenceID": 11, "context": "Word clustering has been used to learn classes of words that have similar semantic features to improve language modeling (Brown et al., 1992) and knowledge transfer across languages (T\u00e4ckstr\u00f6m et al.", "startOffset": 121, "endOffset": 141}, {"referenceID": 21, "context": "Dependency parsing and other NLP tasks have been shown to benefit from such a large unannotated corpus (Koo et al., 2008), and a variety of unsupervised feature learning methods have been shown to unilaterally improve the performance of supervised learning tasks (Turian et al.", "startOffset": 103, "endOffset": 121}, {"referenceID": 20, "context": "(Klementiev et al., 2012) induce distributed representations for a pair of languages jointly, where a learner can be trained on annotations present in one language and applied to test data in another.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "icant amount of computational resources (Bengio et al., 2006; Dean et al., 2012).", "startOffset": 40, "endOffset": 80}, {"referenceID": 17, "context": "icant amount of computational resources (Bengio et al., 2006; Dean et al., 2012).", "startOffset": 40, "endOffset": 80}, {"referenceID": 25, "context": "Several suggestions have been proposed to speed up the training procedure, either by changing the model architecture to exploit an algorithmic speedup (Mnih and Hinton, 2009; Morin and Bengio, 2005) or by esti-", "startOffset": 151, "endOffset": 198}, {"referenceID": 26, "context": "Several suggestions have been proposed to speed up the training procedure, either by changing the model architecture to exploit an algorithmic speedup (Mnih and Hinton, 2009; Morin and Bengio, 2005) or by esti-", "startOffset": 151, "endOffset": 198}, {"referenceID": 3, "context": "mating the error by sampling (Bengio and Senecal, 2008).", "startOffset": 29, "endOffset": 55}, {"referenceID": 14, "context": "(Collobert and Weston, 2008) shows that word embeddings can almost substitute NLP common features on several tasks.", "startOffset": 0, "endOffset": 28}, {"referenceID": 16, "context": "The system they built, SENNA, offers part of speech tagging, chunking, named entity recognition, semantic role labeling and dependency parsing (Collobert, 2011).", "startOffset": 143, "endOffset": 160}, {"referenceID": 1, "context": "In addition to pure performance, the system has a faster execution speed than comparable NLP pipelines (Al-Rfou\u2019 and Skiena, 2012).", "startOffset": 103, "endOffset": 130}, {"referenceID": 13, "context": "(Chen et al., 2013) shows that the embeddings generated by SENNA", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "In our work, we start from the example construction method outlined in (Bengio et al., 2009).", "startOffset": 71, "endOffset": 92}, {"referenceID": 27, "context": "Particularly, Wikipedia is well suited for multilingual applications (Navigli and Ponzetto, 2010).", "startOffset": 69, "endOffset": 97}, {"referenceID": 33, "context": "This is expected given the limited size vocabulary of the language - the number of entries in the Contemporary Chinese Dictionary are estimated to be 65 thousand words (Shuxiang, 2004).", "startOffset": 168, "endOffset": 184}, {"referenceID": 6, "context": "For our experiments, we build a model as the one described in Section 3 using Theano (Bergstra et al., 2010).", "startOffset": 85, "endOffset": 108}, {"referenceID": 8, "context": "scent (Bottou, 1991), and update the parameters which contributed to the error using backpropagation (Rumelhart et al.", "startOffset": 6, "endOffset": 20}, {"referenceID": 32, "context": "scent (Bottou, 1991), and update the parameters which contributed to the error using backpropagation (Rumelhart et al., 2002).", "startOffset": 101, "endOffset": 125}, {"referenceID": 15, "context": "Following, (Collobert et al., 2011)\u2019s advice, we divide each layer by the fan in of that layer, and we consider the embeddings layer to have a fan in of 1.", "startOffset": 11, "endOffset": 35}, {"referenceID": 15, "context": "One disadvantage of the approach used by (Collobert et al., 2011) is that there is no clear stopping criteria for the model training process.", "startOffset": 41, "endOffset": 65}, {"referenceID": 9, "context": "Language Source Test TnT Unknown Known All German Tiger\u2020 (Brants et al., 2002) 89.", "startOffset": 57, "endOffset": 78}, {"referenceID": 2, "context": "5 (Bej\u010dek et al., 2012) 71.", "startOffset": 2, "endOffset": 23}, {"referenceID": 22, "context": "10% Danish DDT\u2020 (Kromann, 2003) 73.", "startOffset": 16, "endOffset": 31}, {"referenceID": 23, "context": "00% English PennTreebank (Marcus et al., 1993) 75.", "startOffset": 25, "endOffset": 46}, {"referenceID": 0, "context": "80% Portuguese Sint(c)tica\u2020 (Afonso et al., 2002) 75.", "startOffset": 28, "endOffset": 49}, {"referenceID": 18, "context": "80% Slovene SDT\u2020 (D\u017eeroski et al., 2006) 68.", "startOffset": 17, "endOffset": 40}, {"referenceID": 28, "context": "60% Swedish Talbanken05\u2020 (Nivre et al., 2006) 83.", "startOffset": 25, "endOffset": 45}, {"referenceID": 30, "context": "The results are compared to the TnT tagger results reported by (Petrov et al., 2012).", "startOffset": 63, "endOffset": 84}, {"referenceID": 30, "context": "We train and test our models on the universal tagset proposed by (Petrov et al., 2012).", "startOffset": 65, "endOffset": 86}, {"referenceID": 10, "context": "We compare our results to a similar experiment conducted in their work, where they trained a TnT tagger (Brants, 2000) on several treebanks.", "startOffset": 104, "endOffset": 118}, {"referenceID": 15, "context": "cantly improve results on NLP tasks (Turian et al., 2010; Collobert et al., 2011).", "startOffset": 36, "endOffset": 81}], "year": 2013, "abstractText": "Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.", "creator": "TeX"}}}