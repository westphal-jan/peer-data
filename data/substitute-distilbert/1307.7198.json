{"id": "1307.7198", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2013", "title": "Self-Learning for Player Localization in Sports Video", "abstract": "this paper introduces a novel self - learning framework but automates human label acquisition process for improving instruments for detecting players accurately broadcast footage of sports games. unlike most previous process - learning approaches for improving appearance - based object detectors from videos, operations allow an unknown, specific number of stationary objects in a more generalized display signal with non - static camera clusters. our self - learning approach uses a latent factor learning algorithm and deformable part models to represent the strength and colour information of players, constraining their motions, and learns the colour of the playing field by a gentle adaboost algorithm. lets combine those image cues and discover additional representations automatically from unlabelled data. in our experiments, our program exploits both labelled and unlabelled data examining sparsely labelled videos comparing sports games, providing a mean performance improvement of over 20 % in the average precision for detecting sports players and improved tracking, when videos require very few labelled images.", "histories": [["v1", "Sat, 27 Jul 2013 00:34:41 GMT  (38096kb,D)", "http://arxiv.org/abs/1307.7198v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["kenji okuma", "david g lowe", "james j little"], "accepted": false, "id": "1307.7198"}, "pdf": {"name": "1307.7198.pdf", "metadata": {"source": "CRF", "title": "Self-Learning for Player Localization in Sports Video", "authors": ["Kenji Okuma", "David G. Lowe", "James J. Little"], "emails": ["little}@cs.ubc.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nRecent advances in object detection have enabled computers to detect many classes of objects, such as faces, pedestrians, and cars. Modern digital cameras and video conferencing systems often have a built-in face detection system to automatically focus on faces. Pedestrian detection has been employed for monitoring surveillance videos and supporting safer driving of cars. However, these machine learning methods suffer from a major drawback \u2014 they require a large amount of training data. In order to achieve performance levels that are high enough for practical commercial applications, it is common that more than a million labelled instances are used for the training, which must be acquired at great expense.\nOne way to resolve this issue is to employ abundant unlabelled data. Active learning has been adopted to train object detectors without much human effort (Okuma et al, 2011; Vijayanarasimhan and Grauman, 2011). With abundant unlabelled data, crowdsourcing is also a powerful tool to utilize human labour efficiently with reduced cost for obtaining abundant labels. LabelMe (Russell et al, 2008) and other interactive user interfaces on Amazon Mechanical Turk such as one by (Sorokin and Forsyth, 2008) and the Visipedia project (Welinder and Perona, 2010) address inexpensive acquisition of labels from a large pool of thousands of unlabelled images. Recently, crowdsourcing has also been utilized for annotating a collection of video data. Interactive annotation tools on the Web such as VATIC, a video annotation tool by (Vondrick et al, 2010), and LabelMe video (Yuen et al, 2009) have become publicly available in the computer vision community to foster large scale labelling of unlabelled\nvideo data. However, those crowdsourcing tools are designed primarily for reducing the overall labelling cost in terms of time and money. They consider neither the impact of each label for improved performance of a classification model nor reducing the size of training data.\nAnother way to resolve the shortage of labelled data is to exploit both labelled and unlabelled data. There has been, especially in recent years, a significant interest in semisupervised learning, which exploits both labelled and unlabelled data to efficiently train a classifier. Semi-supervised learning approaches have shown success in various domains such as text classification (Nigam et al, 2000), handwritten digits recognition (Lawrence and Jordan, 2005), track classfication (Teichman and Thrun, 2012), and object detection (Ali et al, 2011; Leistner et al, 2007; Rosenberg et al, 2005; Siva et al, 2012; Yao et al, 2012). There is a large literature on methods of semi-supervised learning, which originally dates back to the work of Scudder (Scudder, 1965).\nIn this paper, we use semi-supervised learning for improving an appearance-based model of target objects. Most of the recent approaches (Ali et al, 2011; Leistner et al, 2007; Yao et al, 2012) exploit a relatively small amount of labelled data to discover a meaningful portion of training samples for improving object localization in video sequences. None of these approaches, however, address the use of video data with non-stationary camera views. Combined motions from both a non-stationary camera and moving target objects cause inherent localization difficulties. We show that our approach improves player localization on broadcast footage of sports, which allows an unknown, unconstrained number of target objects in more generalized video sequences with non-static camera views. For improving player localization, we address how to maximize the impact of labels by selecting examples that are most likely to be misclassified by the current classification function, and to reduce the overall labelling cost by making the labelling process fully automatic."}, {"heading": "II. WEAKLY-SUPERVISED SELF-LEARNING FOR PLAYER LOCALIZATION", "text": "Given sparsely labelled video data that consists of n different video sequences {Vi}ni=1 where each sequence contains a different number of image frames, Vi = {x1, . . . ,xni}, the task is to train an initial model H : X 7\u2192 Y from a small set of labels L = {(x1, y1), . . . , (xl, yl)} and exploit additional unlabelled data U = {xl+1, . . . ,xl+m} for improving the model, assuming that\nar X\niv :1\n30 7.\n71 98\nv1 [\ncs .C\nV ]\n2 7\nJu l 2\n01 3\nx \u2208 X , y \u2208 Y , and l m. In this paper, we will use hockey and basketball video data for learning an appearancebased model of sports players. This can be viewed as a weakly-supervised learning problem because we deal with videos without localization of the target objects. Unlike most previous semi-supervised learning methods, we allow an unconstrained, unknown number of players that appear in each frame of a video sequence.\nWe propose to use self-learning, which is one of the most commonly used semi-supervised learning methods (Chapelle et al, 2006), to lower the requirement for extensive labelling. Self-learning is a wrapper algorithm that repeatedly uses a supervised learning method. It starts with a small set of labels to train the initial model. In each iteration, the model is used to evaluate unlabelled data and to obtain predictions. The model is then retrained with a selected portion of predictions as additional labels. This process is repeated until some stopping criterion is met.\nOur self-learning system has several stages as shown in Figure 1. The training procedure starts from initializing with a small set of labelled images and a large set of unlabelled images from sparsely labelled video data. Then the system iterates over the following steps. First, a small set of labelled data is used to train initial part-based models for detecting players and classifying their team colour (section IV and section V). Second, these appearance-based models are applied to the unlabelled data and generate a set of detection bounding windows. Third, these bounding windows are linked by a Kalman filter and generate a set of tracklets (section VI). Finally figure-ground segmentation is applied to validate these tracklets. The resulting set of validated tracklets is used as additional labels to re-train current classification models. Algorithm 1 summarizes this process.\nThere are several reasons why we particularly focus on sports player detection in sports videos. Sports videos are highly structured because the domain knowledge is quite specific (e.g, team colours, the player uniform, the colour of the playing field). But they are still challenging enough to be an interesting problem. For example, Figure 2 shows several major challenges for detecting hockey players. Videos in sports \u2014 especially team sports such as hockey (6 onfield players per team), basketball (5 on-field players per team), and soccer (11 on-field players per team) \u2014 are a rich source of labels for learning the appearance of sports players since each frame of a video almost always contains multiple labels. Furthermore, accurate localization of sports players is a fundamental requirement for tackling other interesting problems such as action recognition and player recognition. To the best of our knowledge, our work is the first large scale study of a self-learning framework for learning the appearance of sports players in broadcast footage."}, {"heading": "III. SEMI-SUPERVISED LEARNING IN VIDEOS", "text": "Many algorithms in semi-supervised learning assume that the unlabelled data are independent samples. However, in a video sequence, the trajectory of object instances, defined by\nAlgorithm 1 : Self-learning for player localization in videos Given training video sequences {Vi}ni=1, randomly select m labelled images that contain an initial set of labelled data L = {(x1, y1, c1), . . . , (xl, yl, cl)} where x is a window descriptor, y is a class label, and c is a team colour label. The number of self-learning sessions is set as ns = 5.\n1: Initialize U with all image frames that are unlabelled in {Vi}ni=1. 2: for ns self-learning sessions do 3: Training classifiers:\nGiven labelled data L, train a player detector (section IV) and colour classifiers (section V)\n4: Player detection and team classification: Run the trained classifiers for unlabelled data U (Figure 4) 5: Player tracking: Run a Kalman filter to link detection bounding windows (section VI) 6: Data selection: Select a new dataset Lnew (section VII) and add to existing data: L = L \u2229 Lnew 7: end for\nthe location of the bounding windows, suggests the spatiotemporal structure of subsequent labels.\nIn order to exploit the dependent structure of the video data, several tracking-by-detection approaches (Babenko et al, 2009; Kalal et al, 2010; Leistner et al, 2011) have been proposed to learn an appearance model of an object from videos. These approaches have the stringent assumption of having only one instance of the target object class in each frame of a video sequence. Such an assumption strictly limits applications to detection of a single instance of the target object class, where an instance with the highest confidence is identified as a positive label and all remaining instances are labelled as negative. For learning the appearance of an object class such as pedestrians or faces, videos that contain multiple pedestrians in each frame are much more effective than videos with one person in each frame, because they capture occlusion relationships that are not present in single object videos. But localization of multiple target objects remains difficult, and it prevents most trackingby-detection approaches from exploiting unlabelled data that are available from such videos. Nonetheless, there are a few approaches that have considered exploiting unlabelled video data with multiple target objects such as (Ali et al, 2011; Ramanan et al, 2007).\nRamanan et al (2007) proposed a semi-supervised method for building a large collection of labelled faces from archival video of the television show Friends. Their final collection contains 611,770 faces. Their approach used the Viola et al.\u2019s face detector to detect faces, grouping them with colour histograms of body appearance (i.e, hair, face, and torso) and tracking them using a part-based colour tracker for multiple\nObject Detection\nTeam classification\nData Selection\nFigure-ground segmentation\nObject Tracking\nSparsely labelled video\nVideo after self-learning\nFig. 1: System overview of our self-learning framework. Black boxes mean that models are not updated during the training process and treated as a black box. The system takes a sparsely labelled video with a small set of fully labelled image frames as input and trains initial classification models. Our self-learning approach uses these models to explore the unlabelled portion of data, collecting additional training labels, and updates these models for improved performance. This process is repeated multiple times and produces a more complete set of labels in colour-specific tracklets in the video.\nmotion blur object pose occlusion illumination\nFig. 2: Challenges in player detection. These include motion blur, wide pose variation, occlusion, and sudden illumination change.\npeople in videos. Although their approach is effective with large scale data, they performed only one iteration of exploring the unlabelled data for building a large collection of faces and never used the acquired collection for improving the classifiers they used.\nRecently, Ali et al (2011) implemented self-learning on sparsely labelled videos, which allows any number of instances of the target object class. The approach described in (Ali et al, 2011) is most related to our approach. But it uses a different learning approach and has a number of limitations that we address. It has the major limitation that an appearance of target objects must have a single scale where we need to improve player localization for sports players with various sizes. Furthermore, it assumes a simpler form of video input that could not be applied to broadcast footage of sports. Their model is based on a rather simple, smooth motion of walking pedestrians in their surveillance data of a stationary camera view. Sports players have much more complicated, unpredictable motions with more frequent, complex interactions. Secondly, their approach differs significantly from ours. They used simple edge based features for representing the shape of pedestrians and used a boosting algorithm and linear programming to exploit the temporal coherence of videos. We adopt a latent SVM formulation for learning the shape and colour of sports players who have a variety of different poses (i.e., running, jumping, walking, and etc). We use Kalman filters to link a sparse set of detection boxes, and use figure-ground segmentation as additional information to validate the unlabelled data. Our work is the first to apply self-learning to videos which contain multiple target objects\nof a moving camera view."}, {"heading": "IV. PLAYER DETECTION", "text": "In order to detect hockey players, we adopt the recent latent SVM (LSVM) approach of Felzenszwalb et al (2009). The goal of a supervised learning algorithm is to take n training samples and design a classifier that is capable of distinguishing M different classes. For a given training set (x1, y1), . . . , (xn, yn) with xi \u2208 <N and yi \u2208 {\u22121,+1} in their simplest form with two classes, LSVM is a classifier that scores a sample x with the following function,\nf\u03b2(x) = max z\u2208Z(x)\n\u03b2 \u00b7 \u03a6(x, z) (1)\nHere \u03b2 is a vector of model parameters and z are latent values. The set Z(x) defines possible latent values for a sample x. Training \u03b2 then becomes the optimization problem. We approximate the posterior probability P (y = 1|x) of the decision function in a parametric form of a sigmoid (Lin et al, 2003; Platt, 2000).\nP (y = 1|x) \u2248 P (y = 1|f) = 1 1 + exp(fA+B)\nwhere f = f\u03b2(x)\n(2)\nWe used their code for detection and augment it with a colour classifier as described below."}, {"heading": "V. TEAM CLASSIFICATION", "text": "Our shape-based deformable part model (DPM) gives a tight bounding window of the object (i.e., a hockey player) as well as a set of smaller bounding windows of its corresponding parts. Given these bounding windows as prior knowledge, the model learns a colour classification function based on deformable parts with the following function:\nf\u03b3(x) = \u03b3 \u00b7 \u03a6(x, z\u03b2) (3)\nwhere \u03b3 is a vector of model parameters and z\u03b2 are latent values specified by the shape-based DPM detector. Following (Lu et al, 2009; Okuma et al, 2004; Pe\u0301rez et al, 2002), we use Hue-Saturation-Value (HSV) colour histograms. Thus, a feature vector x is composed of a set of HSV colour histograms, each of which has N = NhNs + Nv bins and corresponds to a unique part of the deformable part models. A distribution K(R) , {k(n;R)}n=1,...,N of the colour histogram in a bounding window R is given as follows:\nk(n;R) = \u03b7 \u2211 d\u2208R \u03b4[b(d)\u2212 n] (4)\nwhere d is any pixel position within R, and b(d) \u2208 {1, . . . , N} as the bin index. \u03b4 is the delta function. We set the size of bins Nh, Ns, and Nv as 10. The normalizing constant \u03b7 ensures that all the bin values are [0, 1.0]. It is important to note that K(R) is not a probability distribution and is only locally contrast normalized1, maxK(R) = 1.0.\n1In our experiments which are not shown here, we tested our classification model with the distribution of the colour histograms which are normalized to be probability distributions. However, results were much worse than ones with local contrast normalization.\nWe train a colour model for each team label: \u201cMTL\u201d for Montreal Canadiens, \u201cNYR\u201d for New York Rangers, and \u201cref\u201d for referees. Figure 3 shows two component deformable part models for the Montreal Canadiens team. The posterior probability of the decision function for each colour classification model is approximated by fitting a sigmoid function (Lin et al, 2003; Platt, 2000). Finally, our team colour classification function is formulated as the maximum likelihood of three binary colour classification models.\ny\u2217 = argmax y\u2208Y P (y|x, z\u03b2) (5)\nwhere y is a team label and Y = {\u201cMTL\u201d, \u201cNYR\u201d, \u201cref\u201d, \u201cothers\u201d}. These part-based colour models are highly discriminative since they use the learned latent values z\u03b2 (i.e., location and size of multiple parts of an object) based on the shape-based DPM detector. Furthermore, these colour models are efficiently trained without optimizing over a large space of latent values, which is the bottleneck of training the latent SVM.\nFor team colour classification, part-based colour models are particularly effective when two teams, the Montreal Canadiens and the New York Rangers, have a similar distribution of colours (e.g., red and blue) in their uniform (Figure 2). Figure 3 shows how multi-part weighted histograms preserve the spatial information of colour distributions, where a single holistic representation cannot. In the figure, there are two different part-based colour models for the Montreal Canadiens, where each model has weighted multipart colour histograms. Parts with more discriminative colour are learned to have higher weights. Figure 4 shows results of team colour classification, which improves detection results of the shape-based model by suppressing those detection windows that do not have the learned team colour labels. In this case, we had 79% precision and 57% recall without team classification (a) and 89% precision and 54% recall with team classification by suppressing false positive detection windows (b)."}, {"heading": "VI. FIGURE-GROUND SEGMENTATION AND PLAYER TRACKING", "text": "We developed an interactive labelling tool to learn a figureground segmentation model based on a boosting algorithm. Given a small set of manually labelled foreground pixels and background pixels on the first image, we used the OpenCV implementation of Gentle Adaboost to learn a set of 150 weighted decision trees2 where the maximum depth of these trees is 10. We then use the initial model on an additional few images, interactively labelling wrongly classified pixels and update the model with these additional labels. The process is repeated a few times with no more than 5 images.\nWe also tested a saliency measure called \u201cobjectness\u201d (Alexe et al, 2010) because it has been used in state-ofthe-art weakly supervised approaches for localizing generic\n2Learning and inference of the model can be further sped up by using decision stumps (i.e., one level decision trees) instead of multi-level decision trees, or reducing the number of weak features.\nobjects. However, \u201cobjectness\u201d did not work well in a hockey video mainly due to a small size of hockey players and weak contrast of the colour of hockey players and the rink.\nOnce detected players have their team label, the next step is to associate detected bounding windows into a set of \u201ctracklets\u201d where a tracklet represents a sequence of bounding windows that share the same identity over time. To achieve this, we employ a tracking-by-detection approach\nand adopt the tracking system of (Lu et al, 2011) based on a Kalman filter (Kalman, 1960). In our self-learning process, we do not update parameters of a tracking model and treat player tracking as a black box. Therefore, our system also works with other tracking-by-detection approaches such as a data-driven MCMC (Khan and Shah, 2006) or the boosted particle filter (BPF) (Okuma et al, 2004)."}, {"heading": "VII. DATA SELECTION", "text": "As described, a set of tracklets {T }kj=1 is obtained by combining detection and tracking results of hockey players. These tracklets are used as a pool of candidate data C from which we collect a set of training labels for improving performance of classification models. Since this selection process is fully automatic, we need a selection criterion which effectively discovers additional training labels without accumulating incorrect labels.\nOur selection criterion combines several image cues including detection, colour classification, tracking of players, and pixel-wise figure-ground segmentations. The selection process is performed with the following steps. First, we prune away short tracklets with less than 10 bounding windows because these tracklets are often produced by very sparse detection results, and often include incorrect labels. After pruning, we have a refined set of tracklets {T }mj=1 where m < k. We initialize a pool of candidate data C with bounding windows of these tracklets. Second, we compute the shape confidence of these predicted bounding windows by running our shape-based DPM detector on each bounding box. Third, we compute a foreground score af \u2208 [0, 1.0] to measure a proportion of foreground pixels (i.e., player pixels) within each predicted bounding window Rp in the candidate data C:\naf = 1\narea(Rp) \u2211 di\u2208Rp f(di) (6)\nwhere area(Rp) denotes the area of the bounding window Rp in terms of the total number of pixels within the window, and f is a binary function which uses the decision value of our figure-ground segmentation model H as follows: f(di) = 1 if H(di) \u2265 0, or 0 otherwise. We use a foreground score af to determine whether or not the corresponding predicted bounding windowRp is added to a set of additional data Lnew. For making this decision, we use labelled data and derive a set of two thresholds \u03c4lower = \u00b5af \u2212 \u03c3af and \u03c4upper = \u00b5af + \u03c3af where \u00b5af is a mean foreground score and \u03c3af is a standard deviation. These thresholds represent how likely Rp contains the foreground object in terms of the proportion of foreground pixels within the window and are computed based on all positive instances in ground-truth data. Consequently, we add a predicted bounding window Rp to Lnew if \u03c4lower \u2264 af \u2264 \u03c4upper.\nThe selected candidate data Lnew is added to labelled data L by simply taking the union of these two datasets, L = L \u222a Lnew. This union produces many bounding windows that significantly overlap with each other. We reduce these duplicates by prioritizing those instances in Lnew and discarding existing instances in L. Assuming that classification models improve every iteration, we utilize this process for eliminating some of the incorrect localization labels. However, such an assumption may not hold if the selection process accumulates too many noisy labels. In the following experiments, we show that our assumption still holds in our self-learning framework.\nAlgorithm 2 : Data selection Given a set of tracklets {T }kj=1 and a figure-ground segmentation model H , the goal is to select a portion of data as candidate labels for the next iteration of self-learning as described in 1. Every iteration, we set the maximum number of additional labels to be added as nmax = 2000.\n1: Tracklet selection: Discard short tracklets and initialize candidate data C from {T }mj=1. 2: Estimate the shape confidence of selected tracklets: Run our shape-based DPM detector for each bounding window in C. Sort them in ascending order of the predicted shape confidence. 3: Apply figure-ground segmentation: For each bounding window Rp, compute a segmentation score af using Equation 6. 4: Final selection: Select a new dataset Lnew (nmax additional labels) and merge datasets, L: L = L \u222a Lnew"}, {"heading": "VIII. EXPERIMENTS", "text": "Data: Our system was tested on our hockey dataset consisting of 7 different video sequences which sum to 4,627 image frames of broadcast footage, and our basketball dataset consisting of 7 different video sequences which sum to 4,818 image frames of broadcast footage. The data are split into two separate sets: 3 sequences (2,249 frames in hockey, 2,486 frames in basketball) for training and 4 sequences (2,378 frames in hockey, 2,332 frames in basketball) for testing. In the training data, the annotations are given in rectangular boxes with the category label, identification (i.e., the number of their jersey) and team colour label.\nIn our experiments, we prepared 6 different sets of fully labelled images: 5 sets of m randomly selected fully labelled images where m = {5, 10, 20, 40, 100} and the fully supervised set of all 2,249 images for hockey and 2,486 images for basketball. For each initial labelled dataset, we first trained the initial shape-based DPM detector and part-based colour classifiers. Then we applied our self-learning framework to collect additional training labels from the unlabelled data and improve initial classifiers iteratively for up to four iterations.\nPlayer detection: We adopted the PASCAL VOC criterion (Everingham et al, 2010) and used average precision (AP) for evaluating our detection results because it has been well defined and widely used in the vision community. Figure 6 shows the result of our system on our hockey data. We ran the entire process five times and show the mean and variance for each labelled dataset. The blue line shows the baseline performance based on only fully supervised data. The red line shows the performance after our system collected additional labels from unlabelled parts of the video. The results show a large performance gain \u2014 about 20% in the mean average precision \u2014 in cases with a small number of labelled images (e.g., using 5 and 10 labelled images).\nHowever, the performance gain gradually decreases or is eliminated with larger labelled datasets.\nFigure 6 shows the average number of labels used for each labelled dataset in the x-axis using a logarithmic scale. We plot the average number of labelled bounding windows from each set of m labelled images where m = {5, 10, 20, 40, 100}. Note that each image typically contains multiple labels.\nPlayer tracking: Figure 7 shows the result of the weakly supervised training for 5 labelled images. In the figure, more hockey players are discovered and tracked successfully after four self-learning iterations of our system in the case of 5 labelled images. Secondly, the performance of tracking hockey players quickly converges to the best performance in the case of fully labelled images (e.g., compare one in 100 labelled images and one in fully labelled images). This fast convergence is also evident in the detection result of Figure 6.\nData selection: Figure 8 shows representative candidate bounding windows in each iteration of the self-learning process. The figure shows the most confident bounding windows with a high detection score and the least confident bounding windows with a low detection score among candidate bounding windows that are selected by our data selection algorithm 2. The localization of hockey players is improved gradually in each iteration. The difference is especially obvious between the iteration 1 and 4, where there is an improvement of 12% in the average precision. Importantly, many of these candidate bounding windows are typically false negatives of the player detector. The detector alone cannot identify these misclassification examples, but they are quite effective at improving the classification performance (Okuma et al, 2011). Our approach is able to select them by tracking players\u2019 motions and segmenting the colour of the playing field.\nComputation time:: Our experiments were performed on an 8-core (Intel Xeon 2.66GHz) machine with 32GB of RAM. The weakly supervised case had four additional learning iterations on top of the strongly supervised case which required only one iteration for training and testing. It took about 4 days of CPU time to run our system on all labelled datasets, where over 80% of time was spent for training a detector and running it on both training and test images to obtain detection bounding windows. It takes about 7 to 10 seconds to run our DPM detector on an image of 960\u00d7 540. To speed up the detection process, the size prior of sports players was estimated from training data and used to focus computational resources within a limited range of scales \u2014 in our case, [\u00b5s \u2212 \u03c3s, \u00b5s + \u03c3s] where \u00b5s is the mean size and \u03c3s is a standard deviation."}, {"heading": "IX. CONCLUSIONS", "text": "Our self-learning approach combines several image cues such as the appearance information (i.e., shape and colour) of players, the constraints on their motions, and the colour of the playing field for discovering additional labels automatically from unlabelled data. We use the constraints of\nplayers\u2019 motions to explore unlabelled portions of sports videos and discover useful labels that the appearance-based player detector is unable to find with the current classification performance. The playing field segmentation is effective for eliminating erroneous labels. Our experimental results show that our approach is particularly effective when there is very little labelled data.\nThis paper shows that it is possible to realize fully automatic acquisition of labels if a small amount of label data is available even in realistic, challenging videos from broadcast footage of sports. An immediate future direction is to use a game-specific player detector for re-targeting other games (e.g., classic games that have been recorded in the past) by re-learning the confidence score of the detector without additional manual labels as in (Wang et al, 2012). Ideally, the label acquisition process should be fully automatic, which will be a difficult goal to achieve in general. Although we showed the possibilities in sports video, there are still many challenges that need to be resolved in order to realize fully automatic acquisition of labels for solving the problem of generic object detection."}], "references": [{"title": "What is an object", "author": ["B Alexe", "T Deselaers", "V Ferrari"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Alexe et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Alexe et al\\.", "year": 2010}, {"title": "FlowBoost - Appearance Learning from Sparsely Annotated Videos", "author": ["K Ali", "D Hasler", "F Flueret"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Ali et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ali et al\\.", "year": 2011}, {"title": "Visual Tracking with Online Multiple Instance Learning", "author": ["B Babenko", "MH Yang", "S Belongie"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Babenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Babenko et al\\.", "year": 2009}, {"title": "The PASCAL Visual Object Classes (VOC) Challenge", "author": ["M Everingham", "LV Gool", "CKI Williams", "J Winn", "A Zisserman"], "venue": "International Journal of Computer Vision 88(2),", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Object Detection with Discriminatively Trained Part Based Models", "author": ["PF Felzenszwalb", "RB Girshick", "D McAllester", "D Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2009}, {"title": "P-N learning: Bootstrapping Binary Classiers by Structural Constraints", "author": ["Z Kalal", "J Matas", "K Mikolajczy"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kalal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalal et al\\.", "year": 2010}, {"title": "A New Approach to Linear Filtering and Prediction Problems", "author": ["RE Kalman"], "venue": "Transactions of the ASME Journal of Basic Engineering", "citeRegEx": "Kalman,? \\Q1960\\E", "shortCiteRegEx": "Kalman", "year": 1960}, {"title": "A Multiview Approach to Tracking People in Crowded Scenes using a Planar Homography Constraints", "author": ["SM Khan", "M Shah"], "venue": "European Conference on Computer Vision", "citeRegEx": "Khan and Shah,? \\Q2006\\E", "shortCiteRegEx": "Khan and Shah", "year": 2006}, {"title": "Semi-Supervised Boosting using Visual Similarity Learning", "author": ["C Leistner", "H Grabner", "H Bischof"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Leistner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Leistner et al\\.", "year": 2007}, {"title": "Improving Classifiers with Unlabeled Weakly-Related Videos", "author": ["C Leistner", "M Godec", "S Schulter", "A Saffari", "M Werlberger", "H Bischof"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Leistner et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Leistner et al\\.", "year": 2011}, {"title": "A Note on Platts Probabilistic Outputs for Support Vector Machines", "author": ["HT Lin", "CJ Lin", "RC Weng"], "venue": "Tech. rep., Department of Computer Science and Information Engineering,", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Tracking and Recognizing Actions of Multiple Hockey Players using the Boosted Particle Filter", "author": ["WL Lu", "K Okuma", "JJ Little"], "venue": "Image and Vision Computing", "citeRegEx": "Lu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2009}, {"title": "Identifying Players in Broadcast Sports Videos using Conditional Random Fields", "author": ["WL Lu", "JA Ting", "KP Murphy", "JJ Little"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Lu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2011}, {"title": "Text Classication from Labeled and Unlabeled Documents using EM", "author": ["K Nigam", "AK McCallum", "ST andTom Mitchell"], "venue": "Machine Learning", "citeRegEx": "Nigam et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "A Boosted Particle Filter: Multitarget Detection and Tracking", "author": ["K Okuma", "A Taleghani", "N de Freitas", "JJ Little", "DG Lowe"], "venue": "European Conference on Computer Vision", "citeRegEx": "Okuma et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Okuma et al\\.", "year": 2004}, {"title": "An adaptive interface for active localization", "author": ["K Okuma", "E Brochu", "DG Lowe", "JJ Little"], "venue": "In: International Conference on Computer Vision Theory and Applications", "citeRegEx": "Okuma et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Okuma et al\\.", "year": 2011}, {"title": "ColorBased Probabilistic Tracking", "author": ["P P\u00e9rez", "C Hue", "J Vermaak", "M Gangnet"], "venue": null, "citeRegEx": "P\u00e9rez et al\\.,? \\Q2002\\E", "shortCiteRegEx": "P\u00e9rez et al\\.", "year": 2002}, {"title": "Probabilities for SV machines", "author": ["JC Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt,? \\Q2000\\E", "shortCiteRegEx": "Platt", "year": 2000}, {"title": "Leveraging archival video for building face datasets", "author": ["D Ramanan", "S Baker", "S Kakade"], "venue": "IEEE International Conference on Computer Vision", "citeRegEx": "Ramanan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ramanan et al\\.", "year": 2007}, {"title": "SemiSupervised Self-Training of Object Detection Models", "author": ["C Rosenberg", "M Hebert", "H Schneiderman"], "venue": "Seventh IEEE Workshop on Applications of Computer Vision", "citeRegEx": "Rosenberg et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2005}, {"title": "Labelme: a database and web-based tool for image annotation", "author": ["BC Russell", "A Torralba", "KP Murphy", "WT Freeman"], "venue": "International Journal of Computer Vision", "citeRegEx": "Russell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2008}, {"title": "Probability of error of some adaptive pattern-recognition machines", "author": ["HJ Scudder"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "Scudder,? \\Q1965\\E", "shortCiteRegEx": "Scudder", "year": 1965}, {"title": "Defence of Negative Mining for Annotating Weakly Labelled Data", "author": ["P Siva", "C Russell", "T Xiang"], "venue": "European Conference on Computer Vision", "citeRegEx": "Siva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Siva et al\\.", "year": 2012}, {"title": "Utility data annotation with Amazon", "author": ["A Sorokin", "D Forsyth"], "venue": "Mechanical Turk. In: Workshop on Internet Vision", "citeRegEx": "Sorokin and Forsyth,? \\Q2008\\E", "shortCiteRegEx": "Sorokin and Forsyth", "year": 2008}, {"title": "Tracking-based semisupervised learning", "author": ["A Teichman", "S Thrun"], "venue": "International Journal of Robotics Research", "citeRegEx": "Teichman and Thrun,? \\Q2012\\E", "shortCiteRegEx": "Teichman and Thrun", "year": 2012}, {"title": "Large-Scale Live Active Learning: Training Object Detectors with Crawled Data and Crowds", "author": ["S Vijayanarasimhan", "K Grauman"], "venue": "IEEE Computer Society International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Vijayanarasimhan and Grauman,? \\Q2011\\E", "shortCiteRegEx": "Vijayanarasimhan and Grauman", "year": 2011}, {"title": "Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces", "author": ["C Vondrick", "D Ramanan", "D Patterson"], "venue": "European Conference on Computer Vision", "citeRegEx": "Vondrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2010}, {"title": "Transferring a generic", "author": ["M Wang", "WL Xiaogang", "Wang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Online crowdsourcing: rating", "author": ["P Welinder", "P Perona"], "venue": null, "citeRegEx": "Welinder and Perona,? \\Q2010\\E", "shortCiteRegEx": "Welinder and Perona", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "object detectors without much human effort (Okuma et al, 2011; Vijayanarasimhan and Grauman, 2011).", "startOffset": 43, "endOffset": 98}, {"referenceID": 23, "context": "LabelMe (Russell et al, 2008) and other interactive user interfaces on Amazon Mechanical Turk such as one by (Sorokin and Forsyth, 2008) and the Visipedia project (Welinder and Perona, 2010) address inexpensive acquisition of labels from a large pool of thousands of unlabelled images.", "startOffset": 109, "endOffset": 136}, {"referenceID": 28, "context": "LabelMe (Russell et al, 2008) and other interactive user interfaces on Amazon Mechanical Turk such as one by (Sorokin and Forsyth, 2008) and the Visipedia project (Welinder and Perona, 2010) address inexpensive acquisition of labels from a large pool of thousands of unlabelled images.", "startOffset": 163, "endOffset": 190}, {"referenceID": 24, "context": "Semi-supervised learning approaches have shown success in various domains such as text classification (Nigam et al, 2000), handwritten digits recognition (Lawrence and Jordan, 2005), track classfication (Teichman and Thrun, 2012), and object detection (Ali et al, 2011; Leistner et al, 2007; Rosenberg et al, 2005; Siva et al, 2012; Yao et al, 2012).", "startOffset": 203, "endOffset": 229}, {"referenceID": 21, "context": "There is a large literature on methods of semi-supervised learning, which originally dates back to the work of Scudder (Scudder, 1965).", "startOffset": 119, "endOffset": 134}, {"referenceID": 17, "context": "We approximate the posterior probability P (y = 1|x) of the decision function in a parametric form of a sigmoid (Lin et al, 2003; Platt, 2000).", "startOffset": 112, "endOffset": 142}, {"referenceID": 17, "context": "The posterior probability of the decision function for each colour classification model is approximated by fitting a sigmoid function (Lin et al, 2003; Platt, 2000).", "startOffset": 134, "endOffset": 164}, {"referenceID": 6, "context": "To achieve this, we employ a tracking-by-detection approach and adopt the tracking system of (Lu et al, 2011) based on a Kalman filter (Kalman, 1960).", "startOffset": 135, "endOffset": 149}, {"referenceID": 7, "context": "works with other tracking-by-detection approaches such as a data-driven MCMC (Khan and Shah, 2006) or the boosted particle filter (BPF) (Okuma et al, 2004).", "startOffset": 77, "endOffset": 98}], "year": 2013, "abstractText": "This paper introduces a novel self-learning framework that automates the label acquisition process for improving models for detecting players in broadcast footage of sports games. Unlike most previous self-learning approaches for improving appearance-based object detectors from videos, we allow an unknown, unconstrained number of target objects in a more generalized video sequence with non-static camera views. Our self-learning approach uses a latent SVM learning algorithm and deformable part models to represent the shape and colour information of players, constraining their motions, and learns the colour of the playing field by a gentle Adaboost algorithm. We combine those image cues and discover additional labels automatically from unlabelled data. In our experiments, our approach exploits both labelled and unlabelled data in sparsely labelled videos of sports games, providing a mean performance improvement of over 20% in the average precision for detecting sports players and improved tracking, when videos contain very few labelled images.", "creator": "LaTeX with hyperref package"}}}