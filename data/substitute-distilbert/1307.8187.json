{"id": "1307.8187", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2013", "title": "Towards Minimax Online Learning with Unknown Time Horizon", "abstract": "we consider online learning when real time horizon is unknown. we apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown - horizon settings, one that assumes the horizon is chosen randomly according'some known distribution, and the other which allows the adversary full control over the horizon. for given random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. and for the adversarial horizon setting, we prove a nontrivial lower bound thus shows that the adversary obtains strictly more recently than else the horizon is fixed and known. based forth the minimax solution of the random horizon setting, we then propose a new adaptive algorithm for \" pretends \" whereas the worm is drawn past a distribution from a special family, but will matter how small actual horizon is predicted, the worst - case regret is of the optimal rate. furthermore, our algorithm can be generalized in many ways, including handling other unknown locations and other online learning settings. experiments show that gi algorithm defeats many other scheduling algorithms in typical online mathematical optimization setting.", "histories": [["v1", "Wed, 31 Jul 2013 01:49:50 GMT  (33kb)", "http://arxiv.org/abs/1307.8187v1", null], ["v2", "Sun, 6 Oct 2013 18:49:58 GMT  (47kb)", "http://arxiv.org/abs/1307.8187v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "robert e schapire"], "accepted": true, "id": "1307.8187"}, "pdf": {"name": "1307.8187.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Unknown Time Horizon", "authors": ["Haipeng Luo"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n81 87\nv1 [\ncs .L"}, {"heading": "1 Introduction", "text": "We study online learning problems with unknown time horizon with the aim of developing algorithms and approaches for the realistic case that the number of time steps is initially unknown.\nWe first adopt the standard Hedge setting [1] where the learner chooses a distribution over N actions on each round, and the losses for each action are then selected by an adversary. The learner incurs loss equal to the expected loss of the actions in terms of the distribution it chose for this round, and its goal is to minimize the regret, the difference between its cumulative loss and that of the best action after T rounds.\nVarious algorithms are known to achieve the optimal (up to a constant) upper bound O( \u221a T lnN) on the regret. Most of them assume that the horizon T is known ahead\nof time, especially those which are minimax optimal. When the horizon is unknown, the so-called doubling trick [2] is a general technique to make a learning algorithm adaptive and still achieve O( \u221a T lnN) regret uniformly for any T . The idea is to first guess a horizon, and once the actual horizon exceeds this guess, double it and restart the algorithm. Although, in theory, it is widely applicable, the doubling trick is aesthetically inelegant, and intuitively wasteful, since it repeatedly restarts itself, entirely forgetting all the preceding information. Other approaches have also been proposed, as we discuss shortly.\nIn this paper, we study the problem of learning with unknown horizon in a gametheoretic framework. We consider a number of variants of the problem, and make progress toward a minimax solution. Based on this approach, we give a new general technique which can also make other algorithms adaptive and achieve low regret. The resulting algorithm is still not exactly optimal, but it makes use of all the previous information on each round and achieves much lower regret in experiments.\nWe view this online learning problem as a repeated game between the learner and the adversary. In [3] and [4], an exact minimax optimal solution was proposed for a slightly different game with binary losses which assumes that the loss of the best action is at most some fixed constant. They derived the solution under a very simple type of loss space; that is, on each round only one action suffers one unit loss. We call this the basis vector loss space. As a preliminary of this paper, we also derive a similar minimax solution under this simple loss space for our setting where the horizon T is fixed and known to the learner ahead of time.\nWe then move on to the primary interest of this paper, that is, the case when the horizon is unknown to the learner. We study this unknown horizon setting in the minimax framework, with the aim of ultimately deriving game-theoretically optimal algorithms. Two types of models are studied. The first one assumes the horizon is chosen according to some known distribution, and the learner\u2019s goal is to minimize the expected regret. We show the exact minimax solution for the basis vector loss space in this case. It turns out that the distribution the learner should choose on each round is simply the conditional expectation of the distributions the learner would have chosen for the fixed horizon case.\nThe second model we study gives the adversary the power to decide the horizon on the fly, which is possibly the most adversarial case. In this case, we no longer use the regret as the performance measure. Otherwise the adversary would obviously choose an infinite horizon. Instead, we use a scaled regret to measure the performance. Specifically, we scale the regret at time t by the optimal regret under fixed horizon t. The exact optimal solution in this case is unfortunately not found and remains an open problem, even for the extremely simple case. However, we give a lower bound for this setting to show that the optimal regret is strictly greater than the one in the fixed horizon game. That is, the adversary does obtain strictly more power if allowed to pick the horizon.\nWe then propose our new adaptive algorithm based on the minimax solution in the random horizon setting. One might doubt how realistic a random horizon is in practice. Even if the true horizon is indeed drawn from a fixed distribution, how can we know this distribution? We address these problems at the same time. Specifically, we prove that no matter how the horizon is chosen, if we assume it is drawn from a distribution\nfrom a special family, and let the learner play in a way similar to the one in the random horizon setting, then the worst-case regret at any time T (not the expected regret) can still be of the optimal order. In other words, although the learner is behaving as if the horizon is random, its regret will be small even if the horizon is actually controlled by an adversary.\nOur idea can be combined with not only the minimax algorithm, but also the exponential weights algorithm, and even other online optimization settings [5, 6]. Moreover, our technique can not only deal with unknown horizon, but also other unknown information such as the loss of the best action, thus leading to a first order regret bound that depends on the loss of the best action [7]. Like the doubling trick, this seems to be a quite general way to make an algorithm adaptive. Furthermore, we conduct experiments showing that our algorithm outperforms many other existing algorithms, including the doubling trick, in an online linear optimization setting within an \u21132 ball where our algorithm has an explicit closed form.\nThe rest of the paper is organized as follows. We define the Hedge setting formally in Section 2, and derive the minimax solution for the fixed horizon setting as the preliminary of this paper in Section 3. In Section 4, we study two unknown horizon settings in the minimax framework. Our new adaptive algorithm is presented in Section 5, and several generalizations and an experiment are shown in Section 6. We omit most of the proofs in the paper due to space limitations, but all details can be found in the supplementary material.\nRelated work Besides the doubling trick, other adaptive algorithms have been studied [8, 9, 10, 11]. Auer et al. [8] showed that for algorithms such as the exponential weights algorithm [12, 1, 13], where a learning rate \u03b7 should be set as a function of the horizon, typically in the form \u221a\n(b lnN)/T for some constant b, one can simply set the learning rate adaptively as \u221a\n(b lnN)/t, where t is the current number of rounds. In other words, this algorithm always pretends the current round is the last round. Although this idea works with the exponential weights algorithm, we remark that assuming the current round is the last round does not always work. Specifically, one can show that it will fail if applied to the minimax algorithm (see Section 6.1). In another approach to online learning with unknown horizon, Chaudhuri et al. [11] proposed an adaptive algorithm based on a novel potential function reminiscent of the half-normal distribution.\nOther performance measures different from the usual regret were studied before. Foster and Vohra [14] introduced internal regret comparing the loss of an online algorithm to the loss of a modified algorithm which consistently replaces one action by another. In [15] and [16], the learner\u2019s loss is compared with the best k-shifting expert instead of the best one. Hazan and Seshadhri [17] studied the usual regret within any time interval, which they called adaptive regret. To the best of our knowledge, the form of scaled regret that we study is new. Lower bounds on anytime regret in terms of the quadratic variations for any loss sequence (instead of the worst case sequence this paper considers) were studied in [18]."}, {"heading": "2 Repeated Games", "text": "We first consider the following repeated game between a learner and an adversary. The learner has access to N actions. On each round t = 1, . . . , T , (1) the learner chooses a distribution Pt over the actions; (2) the adversary reveals the loss vector Zt = (Zt,1, . . . , Zt,N) \u2208 LS, where Zt,i is the loss for action i for this round, and the loss space LS is a subset of [0, 1]N ; (3) the learner suffers loss \u2113t = Pt \u00b7 Zt for this round.\nNotice that the adversary can choose the losses on round t with full knowledge of the history P1:t and Z1:t\u22121, that is, all the previous choices of the learner and the adversary. We also denote the cumulative loss up to round t for the learner and the actions by Lt = \u2211t t\u2032=1 \u2113t\u2032 and Mt = \u2211t t\u2032=1 Zt\u2032 respectively. The goal for the learner is to minimize the difference between its total loss and that of the best action at the end of the game. In other words, the goal of the learner is to minimize Reg(LT ,MT ), where we define the regret function Reg(L,M) , L \u2212 mini Mi, for L \u2208 R and M \u2208 RN . The number of rounds T is called the horizon.\nRegarding the loss space LS, perhaps the simplest one is {e1, . . . , eN}, the N standard basis vectors in N dimensions. Playing with this loss space means that on each round, the adversary chooses one single action to incur one unit loss. In Sections 3, 4 and 5, we mainly focus on this basis vector loss space, but we return to the most general case [0, 1]N in Section 6."}, {"heading": "3 Minimax Solution for Fixed Horizon", "text": "Although our primary interest in this paper is the case when the horizon is unknown to the learner, we first present some preliminary results on the setting where the horizon is known to both the learner and the adversary ahead of time. These will later be useful for the unknown horizon case.\nIf we treat the learner as an algorithm Alg that takes the information of previous rounds as inputs, and outputs a distribution Pt = Alg(P1:t\u22121,Z1:t\u22121) that the learner is going to play with, then finding the optimal solution in this fixed horizon setting can be viewed as solving the minimax expression\ninf Alg sup Z1:T Reg(LT ,MT ). (1)\nAlternatively, we can recursively define a function V (M, r) as:\nV (M, 0) , \u2212min i Mi ; V (M, r) , min P\u2208\u2206(N) max Z\u2208LS (P \u00b7 Z+ V (M+ Z, r \u2212 1)) ,\nwhere M \u2208 RN is a loss vector, r is a nonnegative integer, and \u2206(N) is the N dimensional simplex. By a simple argument, one can show that the value of V (M, r) is the regret of a game with r rounds starting from the situation that each action has initial loss Mi, and assuming both the learner and the adversary will play optimally. In fact, the value of Eq. (1) is exactly V (0, T ), and the optimal learner algorithm is the one that chooses the P\u2217 which realizes the minimum in the definition of V (M, r) when the\nactions\u2019 cumulative loss vector is M and there are r rounds left. We call V (0, T ) the value of the game.\nWe now consider the basis vector loss space 1, that is, LS = {e1, . . . , eN}. It turns out that under this loss space, the value function V has a nice closed form. Similar to the results from [2] and [3], V can also be expressed in terms of a random walk. Suppose R(M, r) is the expectation of the loss of the best action if the adversary chooses each ei uniformly randomly for the remaining r rounds, starting from loss vector M. Formally, R(M, r) can be defined in a recursive way: R(M, 0) , mini Mi ;R(M, r) , 1 N \u2211N i=1 R(M+ ei, r \u2212 1). The connection between V and R, and the optimal algorithm are then shown by the following theorem.\nTheorem 1. If LS = {e1, . . . , eN}, then for any vector M and integer r \u2265 0,\nV (M, r) = r\nN \u2212R(M, r).\nLet cN = 1N \u221a 2(N \u2212 1) lnN . Then the value of the game satisfies\nV (0, T ) \u2264 cN \u221a T . (2)\nMoreover, on round t, the optimal learner algorithm is the one that chooses weight Pt,i = V (Mt\u22121, r)\u2212V (Mt\u22121+ei, r\u22121) for each action i, where Mt\u22121 is the current cumulative loss vector and r is the number of remaining rounds, that is, r = T \u2212 t+1.\nTheorem 1 tells us that under the basis vector loss space, the best way to play is to assume that the adversary is playing uniformly randomly, because r/N and R(M, r) are exactly the expected loss for the learner and the best action respectively. In practice, computing R(M, r) needs exponential time. However, we can estimate it by sampling (see similar work in [3]). Note that cN is decreasing when N \u2265 4 (with maximum value about 0.72). So contrary to the O( \u221a T lnN) regret bound for the general loss space [0, 1]N which is increasing in N , here V (0, T ) is of order O( \u221a T ).\nBefore we move on, we point out a few properties of the function V , which we will use later.\nProposition 1. If LS = {e1, . . . , eN}, then for any vector M and integer r, Property 1. V (M, r) = V ((M1 \u2212 a, . . . ,MN \u2212 a), r)\u2212 a for any real number a and r \u2265 0. Property 2. V (M, r) is non-increasing in Mi for each i = 1, . . . , N .\nProperty 3. V (M, r) is non-decreasing in r."}, {"heading": "4 Playing without Knowing the Horizon", "text": "We turn now to the case in which the horizon T is unknown to the learner, which is often more realistic in practice. There are several ways of modeling this setting. For example, the horizon can be chosen ahead of time according to some fixed distribution, or it can even be chosen by the adversary. We will discuss these two variants separately.\n1For other loss spaces, finding the minimax solutions seems difficult. Nevertheless, we show the relation of the values of the game under different loss spaces in the supplementary file, see Theorem 9."}, {"heading": "4.1 Random Horizon", "text": "Suppose the horizon T is chosen according to some fixed distribution Q which is known to both the learner and the adversary. Before the game starts, a random T is drawn, and neither the learner nor the adversary knows the actual value of T . The game stops after T rounds, and the learner aims to minimize the expectation of the regret. Using our earlier notation, the problem can be formally defined as\ninf Alg sup Z1:\u221e ET\u223cQ[Reg(LT ,MT )],\nwhere we assume the expectation is always finite. We sometimes omit the subscript T \u223c Q for simplicity. For the basis vector loss space, we again show the exact minimax solution, which has a strong connection with the one for the fixed horizon setting.\nTheorem 2. If LS = {e1, . . . , eN}, then\ninf Alg sup Z1:\u221e ET\u223cQ[Reg(LT ,MT )] = ET\u223cQ[inf Alg sup Z1:T Reg(LT ,MT )]. (3)\nMoreover, on round t, the optimal learner plays with the distributionPt = ET\u223cQ[PTt |T \u2265 t], where PTt is the optimal distribution the learner would play if the horizon is T , that is, PTt,i = V (Mt\u22121, T \u2212 t+ 1)\u2212 V (Mt\u22121 + ei, T \u2212 t).\nEq. (3) tells us that if the horizon is drawn from some distribution, then even though the learner does not know the actual horizon before playing the game, as long as the adversary does not know this information either, it can still do as well as the case when they are both aware of the horizon.\nHowever, so far this model does not seem to be quite useful in practice for several reasons. First of all, the horizon might not be chosen according to a distribution. Even if it is, this distribution is probably unknown. Secondly, what we really care about is the performance which holds uniformly for any horizon, instead of the expected regret. Fortunately, we address all these problems and develop new adaptive algorithms based on the result in this section. We discuss these in Section 5 after first introducing the fully adversarial model."}, {"heading": "4.2 Adversarial Horizon", "text": "The most adversarial setting is the one where the horizon is completely controlled by the adversary. That is, we let the adversary decide whether to continue or stop the game on each round according to the current situation. However, notice that the value of the game is increasing in the horizon. So if the adversary can determine the horizon and its goal is still to maximize the regret, then the problem would not make sense because the adversary would clearly choose to play the game forever and never stop leading to infinite regret. One reasonable way to address this issue is to scale the regret by the value of the fixed horizon game V (0, T ), so that the scaled regret Reg(LT ,MT )/V (0, T ) indicates how many times worse is the regret compared to the\none that is optimal given the horizon. Under this setting, the corresponding minimax expression is\nV\u0303 = inf Alg sup T sup Z1:T\nReg(LT ,MT )\nV (0, T ) . (4)\nUnfortunately, finding the minimax solution to this setting seems to be quite challenging, even for the simplest case N = 2. It is clear, however, that V\u0303 is at most some constant due to the existence of adaptive algorithms such as the doubling trick, which can achieve the optimal regret bound up to a constant without knowing T . Another clear fact is V\u0303 \u2265 1, since it is impossible for the learner to do better than the case when it is aware of the horizon. Below, we derive a nontrivial lower bound that is greater than 1, thus proving that the adversary does gain strictly more power when it can stop the game whenever it wants. Theorem 3. If N = 2 and LS = [0, 1]2, then V\u0303 \u2265 \u221a 2. That is, for every algorithm, there exists an adversary and a horizon T such that the regret of the learner after T rounds is at least \u221a 2V (0, T )."}, {"heading": "5 A New Adaptive Algorithm", "text": "We study next how the random-horizon algorithm of Section 4.1 can be used when the horizon is entirely unknown. In Theorem 2, we proposed an algorithm that simply takes the conditional expectation of the distributions we would have played if the horizon were given. Notice that even though it is derived from the random horizon setting, it can still be used in any setting as an adaptive algorithm in the sense that it does not require the horizon as a parameter. However, to use this algorithm, we should ask two questions: What distribution should we use? What can we say about the algorithm\u2019s performance for an arbitrary horizon instead of in expectation?\nAs a first attempt, suppose we use a uniform distribution over 1, . . . , T0, where T0 is a huge integer. From what we observe in some numerical calculations, Pt = E[PTt |T \u2265 t] tends to be a uniform distribution in this case. Clearly it cannot be a good algorithm if for each round, it just places equal weights for each action regardless of the actions\u2019 behaviors. In fact, one can verify that the exponential distribution (that is, Pr[T = t] \u221d \u03b1t for some constant 0 < \u03b1 < 1) also does not work. These examples show that even though this algorithm gives us the optimal expected regret, it can still suffer a big regret for a particular trial of the game, which we definitely want to avoid.\nNevertheless, it turns out that there does exist a family of distributions that can guarantee the regret to be of order O( \u221a T ) for any T . Below, we first give a general upper bound on the regret that holds for any distribution Q and has no dependence on the choices of the adversary. After that we will show what the appropriate distributions are to make this upper bound be O( \u221a T ).\nTheorem 4. Let LS = {e1, . . . , eN}, V\u0304t(M) = ET\u223cQ[V (M, T \u2212 t + 1)|T \u2265 t], and qt = PrT\u223cQ[T = t|T \u2265 t]. Suppose on round t, the learner chooses Pt = ET\u223cQ[PTt |T \u2265 t], where PTt,i = V (Mt\u22121, T \u2212 t+ 1)\u2212 V (Mt\u22121 + ei, T \u2212 t). Then\nfor any Ts, the regret after Ts rounds is at most\nV\u03041(0) +\nTs\u2211\nt=1\nqtV\u0304t+1(0).\nProof. By the definition of PTt and the fact that Pr[T = t \u2032|T \u2265 t] = (1 \u2212 qt) Pr[T = t\u2032|T \u2265 t+ 1] for any t\u2032 > t, we have Pt,i = E[V (Mt\u22121, T \u2212 t+ 1)\u2212 V (Mt\u22121 + ei, T \u2212 t)|T \u2265 t]\n= V\u0304t(Mt\u22121)\u2212 qtV (Mt\u22121 + ei, 0)\u2212 (1\u2212 qt)V\u0304t+1(Mt\u22121 + ei) \u2264 V\u0304t(Mt\u22121)\u2212 V\u0304t+1(Mt\u22121 + ei) + qtV\u0304t+1(0). (5)\nHere the inequality holds because for anyM, if we let M\u2032 = (M1\u2212R(M, 0), . . . ,MN\u2212 R(M, 0)) so that M \u2032i \u2265 0 for each i, then by Property 1 and 2 in Proposition 1, V\u0304t+1(M) \u2212 V (M, 0) = E[V (M\u2032, T \u2212 t)|T \u2265 t + 1] \u2264 V\u0304t+1(0). Replacing M with Mt\u22121 + ei shows Eq. (5). Now let the choice of the adversary on round t be it, so that Mt = Mt\u22121 + eit . The regret at the end of round Ts is\nV (MTs , 0) +\nTs\u2211\nt=1\nPt,it \u2264 V (MTs , 0) + Ts\u2211\nt=1\n( V\u0304t(Mt\u22121)\u2212 V\u0304t+1(Mt) + qtV\u0304t+1(0) )\n= V (MTs , 0) + V\u03041(0)\u2212 V\u0304Ts+1(MTs) + Ts\u2211\nt=1\nqtV\u0304t+1(0),\nwhich is at most V\u03041(0) + \u2211Ts\nt=1 qtV\u0304t+1(0) because for any M and t, by Property 3 in Proposition 1, V\u0304t(M)\u2212 V (M, 0) = E[V (M, T \u2212 t+ 1)\u2212 V (M, 0)|T \u2265 t] \u2265 0.\nAs a direct corollary, we now show an appropriate choice of Q. We emphasize that even though we let T be a random variable drawn from Q, the actual horizon Ts is not necessarily chosen according to this distribution. It can even be chosen adversarially.\nTheorem 5. Under the same conditions of Theorem 4, if Pr[T = t] \u221d 1/td where d > 32 is a constant, then for any Ts, the regret after Ts rounds is at most\n\u0393(d\u2212 32 ) \u0393(d) (d\u2212 1)2cN \u221a \u03c0Ts + o( \u221a Ts),\nwhere cN is defined in Eq. (2) and \u0393 is the gamma function. Choosing d \u2248 2.35 approximately minimizes the main term in the bound, leading to regret 3cN \u221a Ts +\no( \u221a Ts).\nTheorem 5 tells us that pretending that the horizon is drawn from the distribution Pr[T = t] \u221d 1/td (d > 3/2) can always achieve low regret. Although we do not have a simple closed form for E[PTt |T \u2265 t] (this can be addressed under some specific settings though, see Section 6.3), computing the sum of the first sufficient number of terms in the series can be a good estimate since the weight for each term decreases rapidly. Also notice that the constant 3 in the bound for the term cN \u221a Ts is less than\nthe one for the doubling trick with the fixed horizon optimal algorithm, which is 2+ \u221a 2 [7]. We will see in Section 6.3 an experiment on a different but related online learning setting, which shows that our algorithm performs much better than the doubling trick."}, {"heading": "6 Generalizations", "text": "It turns out that the idea of using a \u201cpretend prior distribution\u201d is very applicable in online learning. Below we will discuss a couple of generalizations, including Hedge with general loss space, handling other unknown information besides the horizon, and an application to online linear optimization within an \u21132 ball."}, {"heading": "6.1 Generalizing the Exponential Weights Algorithm", "text": "Up until now, we mainly focused on the basis vector loss space, but in fact, we can also deal with the most general loss space [0, 1]N . The optimal solution for the loss space [0, 1]N is unknown even for the fixed horizon setting. However, generalizing the weighted majority algorithm of [12], Freund and Schapire [1, 13] presented an algorithm using exponential weights that can deal with this general loss space and achieve the O( \u221a T lnN) bound on the regret. The algorithm takes the horizon T as a parameter, and on round t, it simply chooses Pt,i \u221d exp(\u2212\u03b7Mt\u22121,i), where \u03b7 =\u221a (8 lnN)/T is the learning rate. It is shown that the regret of this algorithm is at most \u221a\n(T lnN)/2. Auer et al. [8] proposed a way to make this algorithm adaptive by simply setting a time-varying learning rate \u03b7 = \u221a\n(8 lnN)/t, where t is the current round, leading to a regret bound of \u221a T lnN for any T (see Chapter 2.5 of [19]). In other words, the algorithm always treats the current round as the last round. Below, we show that our \u201cpretend distribution\u201d idea can also be used to make this exponential weights algorithm adaptive, and is in fact a generalization of the adaptive learning rate algorithm in [8].\nTheorem 6. Let LS = [0, 1]N , Pr[T = t] \u221d 1/td (d > 3/2) and \u03b7T = \u221a\n(b lnN)/T , where b is a constant. If on round t, the learner assigns weight ET\u223cQ[PTt,i|T \u2265 t] to each action i, where PTt,i \u221d exp(\u2212\u03b7TMt\u22121,i), then for any Ts, the regret after Ts rounds is at most\n(\u221a b(d\u2212 1)\n4(d\u2212 1/2) + d\u2212 1 (d\u2212 3/2) \u221a b\n) \u221a\nTs lnN + o( \u221a Ts lnN).\nSetting b = 4d\u22122d\u22123/2 minimizes the main term, which approaches 1 as d \u2192 \u221e.\nNote that different from Theorem 5 where the regret is minimized when d \u2248 2.35, Theorem 6 implies that the bigger we choose d, the smaller will be the leading term in this bound. Also note that if d \u2192 \u221e, our algorithm simply becomes the one of [8], because Pr[T = \u03c4 |T \u2265 t] is 1 if \u03c4 = t and 0 otherwise. Therefore, our algorithm can be viewed as a generalization of the idea of treating the current round as the last round. However, we emphasize that the way we deal with unknown horizon is more applicable in the sense that if we try to make the minimax algorithm in Theorem 1 adaptive by treating each round as the last round, one can construct an adversary that leads to linear regret (just consider a game of two actions and the adversary choosing e1 and e2 alternatively)."}, {"heading": "6.2 First Order Regret Bound", "text": "So far all the regret bounds we have discussed are in terms of the horizon, which are also called the zeroth order bound. More refined bounds have been studied in the literature [7]. For example, the first order bound, that depends on the loss of the best action m\u2217 at the end of the game, usually is of order O( \u221a m\u2217 lnN). Again, using the exponential weights algorithm with a slightly different learning rate \u03b7 = ln(1 + \u221a (2 lnN)/m\u2217), one can show that the regret is at most \u221a 2m\u2217 lnN + lnN . Here, m\u2217 is prior information on the loss sequence similar to the horizon. To avoid exploiting this information that is not available in practice, one can again use techniques like the doubling trick or the time-varying learning rate. Alternatively, we show that the \u201cpretend distribution\u201d technique can also be used here. Instead of using a discrete distribution on the horizon, it now makes more sense to assign a continuous distribution on the loss of the best action.\nTheorem 7. Let LS = [0, 1]N , mt = miniMt,i + 1, \u03b7m = \u221a\n(lnN)/m, and m \u2265 1 be a continuous random variable with probability density f(m) \u221d 1/md (d > 3/2). If on round t, the learner assigns weight E[Pmt,i |m \u2265 mt\u22121] to each action i, where Pmt,i \u221d exp(\u2212\u03b7mMt\u22121,i), then for any Ts, the regret after Ts rounds is at most\n3(d\u2212 7/6)(d\u2212 1) (d\u2212 3/2)(d\u2212 1/2) \u221a m\u2217 lnN + (1 + (d\u2212 1) ln(m\u2217 + 1)) lnN + o( \u221a m\u2217 lnN),\nwhere m\u2217 = miniMTs,i is the loss of the best action after Ts rounds. Setting d = 5/2 + \u221a 2 minimizes the main term, which becomes (3/2 + \u221a 2) \u221a m\u2217 lnN ."}, {"heading": "6.3 Online Linear Optimization", "text": "All the examples we have shown previously do not have an explicit form to compute the learner\u2019s strategy. In this section, however, we will show an explicit algorithm for online linear optimization within an \u21132 ball using the technique of pretended distribution, which also illustrates the generality of our technique. Specifically, we consider the following repeated game adopted from [6] (all the norms are \u21132 norms). Let B = {x \u2208 RN : \u2016x\u2016 \u2264 1}. On each round t = 1, . . . , T , the learner first chooses a vector xt \u2208 B, then the adversary chooses another vector wt \u2208 B. The learner suffers loss xt \u00b7 wt for this round, and the regret of the learner after T rounds is\n\u2211T t=1 xt \u00b7 wt \u2212 inf\nx\u2208B\n\u2211T t=1 x \u00b7 wt = \u2211T t=1 xt \u00b7 wt + \u2016WT\u2016, where we define\nWt = \u2211t\nt\u2032=1 wt\u2032 . In [6], a simple but exact minimax optimal algorithm for the fixed horizon setting was shown: on each round t, choose\nxTt = \u2212Wt\u22121 /\u221a \u2016Wt\u22121\u20162 + (T \u2212 t+ 1) . (6)\nThis strategy guarantees the regret to be at most \u221a T . To make this algorithm adaptive, we again assign a distribution over the horizon. However, in order to get an explicit form for E[xTt |T \u2265 t], a continuous distribution on T is necessary. It does not seem to make sense at first glance since the horizon is always an integer, but keep in mind that\nthe random variable T is merely an artifact of our algorithm. As long as the output of the learner is in the set B, our algorithm is valid. Specifically, we show the following:\nTheorem 8. Let T \u2265 1 be a continuous random variable with probability density f(T ) \u221d 1/T 2. If the learner chooses xt = E[xTt |T \u2265 t] on round t, where xTt is defined by Eq. (6), then the regret after Ts rounds is at most \u03c0 \u221a Ts + o( \u221a Ts) for any Ts. Moreover, xt has the following explicit form\nxt =\n \n\n( t\u00b7tanh\u22121 (\u221a 1\u2212t/c )\n(c\u2212t)3/2 \u2212 \u221a c c\u2212t\n)\nWt\u22121, if c 6= t\n\u2212 2t 3c3/2\nWt\u22121, else (7)\nwhere c = 1 + \u2016Wt\u22121\u20162.\nThe algorithm we are proposing in Eq. (7) looks quite inexplicable if one does not realize that it comes from the expression E[xTt |T \u2265 t] with an appropriate distribution. Yet the algorithm not only enjoys a low theoretic regret bound as shown in Theorem 8, but also achieves very good performance in simulated experiments. To show this, we conduct an experiment that compares the regrets of five algorithms at any time step within 1000 rounds against an adversary that chooses points in B uniformly at random (N = 10). The results are shown in Figure 1, where each data point is the maximum regret over a thousand randomly generated adversaries for the corresponding algorithm and horizon. The\nfive algorithms are: the minimax algorithm in Eq. (6) (OPT); the one we proposed in Theorem 8 (DIST); online gradient descent, a general algorithm for online optimization (see [5]) (OGD); the algorithm in Eq. (6) with T replaced by t (OPT2); the doubling trick with the minimax algorithm (DOUBLE). Note that OPT is not really an adaptive algorithm: it \u201ccheats\u201d by knowing the horizon T = 1000 in advance, and thus performs best at the end of the game. We include this algorithm merely as a baseline. Figure 1 shows that our algorithm DIST achieves consistently much lower regret than any other adaptive algorithm. OPT2 adapts the idea of treating the current round as the last round, which can actually be forced to achieve linear regret under the worst case scenario (similar to what we discussed at the end of Section 6.1). It is surprising though that its performance is comparable to OGD under a uniformly random adversary. Finally, we remark that although the doubling trick is widely applicable in theory, in experiments it is beaten by most of the other algorithms."}, {"heading": "A Proof of Theorem 1 and Proposition 1", "text": "We first state a few properties of the function R:\nProposition 2. For any vector M of N dimensions and integer r,\nProperty 4. R(M, r) = a+R((M1 \u2212 a, . . . ,MN \u2212 a), r) for any real number a and r \u2265 0.\nProperty 5. R(M, r) is non-decreasing in Mi for each i = 1, . . . , N .\nProperty 6. If r > 0, R(M, r)\u2212R(M, r \u2212 1) \u2264 1/N .\nProperty 7. If r > 0, andPi = 1N+R(M+ei, r\u22121)\u2212R(M, r) for each i = 1, . . . , N , then P = (P1, . . . , PN ) is a distribution in the simplex \u2206(N).\nProof of Proposition 2. We omit the proof for Property 4 and 5, since it is straightforward. We prove Property 6 by induction. For the base case r = 1, let S = {j : Mj = mini Mi}. If |S| = 1, then R(M + ei, 0) is R(M, 0) for i /\u2208 S and R(M, 0) + 1 otherwise. If |S| > 1, then R(M + ei, 0) is simply R(M, 0) for all i. In either case, we have\nR(M, 1) = 1\nN\nN\u2211\ni=1\nR(M+ ei, 0) \u2264 1\nN (1 +\nN\u2211\ni=1\nR(M, 0)) = 1\nN +R(M, 0),\nproving the base case. Now for r > 1, by definition of R and induction,\nR(M, r)\u2212R(M, r\u22121) = 1 N\nN\u2211\nj=1\n(R(M+ ei, r \u2212 1)\u2212R(M+ ei, r \u2212 2)) \u2264 1\nN\nN\u2211\nj=1\n1\nN =\n1\nN ,\ncompleting the induction. For Property 7, it suffices to prove Pi \u2265 0 for each i and\u2211N i=1 Pi = 1. The first part can be shown using Property 5 and 6:\nPi = 1\nN +R(M+ei, r\u22121)\u2212R(M, r) \u2265\n1 N +R(M, r\u22121)\u2212( 1 N +R(M, r\u22121)) = 0.\nThe second part is also easy to show by definition of R:\nN\u2211\ni=1\nPi = 1+\nN\u2211\ni=1\nR(M+ei, r\u22121)\u2212NR(M, r) = 1+NR(M, r)\u2212NR(M, r) = 1.\nProof of Theorem 1. First proveV (M, r) = r/N\u2212R(M, r) for any r \u2265 0 inductively. The base case r = 0 is trivial by definition. For r > 0,\nV (M, r) = min P\u2208\u2206(N) max Z\u2208LS\n(P \u00b7 Z+ V (M+ Z, r \u2212 1))\n= min P\u2208\u2206(N) max i\u2208[N ]\n(Pi + V (M + ei, r \u2212 1)) (LS = {e1, . . . , eN})\n= min P\u2208\u2206(N) max i\u2208[N ]\n(\nPi + r \u2212 1 N \u2212R(M + ei, r \u2212 1) )\n(by induction)\nDenote Pi + (r \u2212 1)/N \u2212 R(M + ei, r \u2212 1) by g(P, i). Notice that the average of g(P, i) over all i is irrelevant to P : 1N \u2211N i=1 g(P, i) = r/N \u2212 R(M, r). Therefore, maxi g(P, i) \u2265 r/N \u2212R(M, r) for any P, and\nV (M, r) = min P max i\ng(P, i) \u2265 r/N \u2212R(M, r). (8)\nOn the other hand, from Proposition 2, we know that P \u2217i = 1/N+R(M+ei, r\u22121)\u2212 R(M, r) (i \u2208 [N ]) is a valid distribution. Also,\nV (M, r) = min P max i g(P, i) \u2264 max i g(P\u2217, i) = max i\n( r\nN \u2212R(M, r)\n)\n= r\nN \u2212R(M, r).\n(9) So from Eq. (8) and (9) we have V (M, r) = r/N \u2212 R(M, r), and also P \u2217i = 1/N + R(M + ei, r \u2212 1) \u2212 R(M, r) = V (M, r) \u2212 V (M + ei, r \u2212 1) realizes the minimum, and thus is the optimal strategy.\nIt remains to prove V (0, T ) \u2264 cN \u221a T . Let Z1, . . . ,ZT be independent uniform\nrandom variables taking values in {e1, . . . , eN}. By what we proved above,\nV (0, T ) = T\nN \u2212 E[min i\u2208[N ]\nT\u2211\nt=1\nZt,i] = E[max i\u2208[N ]\nT\u2211\nt=1\n(1/N \u2212 Zt,i)].\nLet yt,i = 1/N \u2212 Zt,i. Then each yt,i is a random variable that takes value 1/N with probability 1 \u2212 1/N and 1/N \u2212 1 with probability 1/N . Also, for a fixed i, y1,i, . . . , yT,i are independent (note that this is not true for yt,1, . . . , yt,N for a fixed t). It is not hard to verify that each yt,i satisfies\nE[exp(\u03bbyt,i)] \u2264 exp( \u03bb2\u03c32\n2 ), \u2200\u03bb > 0,\nwhere \u03c32 = (N \u2212 1)/N2 is the variance of yt,i. So if we let Yi = \u2211T\nt=1 yt,i, by the independence of each term, we have\nE[exp(\u03bbYi)] = E[\nT\u220f\nt=1\nexp(\u03bbyt,i)] =\nT\u220f\nt=1\nE[exp(\u03bbyt,i)] \u2264 exp( \u03bb2\u03c32T\n2 ), \u2200\u03bb > 0.\nNow using Lemma A.13 in [7], we arrive at\nE[max i\u2208[N ]\nYi] \u2264 \u03c3 \u221a 2T lnN = cN \u221a T .\nWe conclude the proof by pointing out\nV (0, T ) = E[max i\u2208[N ]\nT\u2211\nt=1\n(1/N \u2212 Zt,i)] = E[max i\u2208[N ]\nYi] \u2264 cN \u221a T .\nProof of Proposition 1. Based on the equality V (M, r) = r/N \u2212R(M, r) from Theorem 1, the three properties in Proposition 1 are direct corollaries of Property 4, 5 and 6 in Proposition 2."}, {"heading": "B Proof of Theorem 2", "text": "Proof. Define V\u0304t(M) = E[V (M, T \u2212 t+ 1)|T \u2265 t] and q(t\u2032, t) = Pr[T = t\u2032|T \u2265 t]. We will prove an important property of the V\u0304 function:\nV\u0304t(M) = min P\u2208\u2206(N) max i\u2208[N ]\n( Pi + q(t, t)V (M+ ei, 0) + (1\u2212 q(t, t)) V\u0304t+1(M + ei) ) .\n(10) This equation shows that V\u0304t(M) is the conditional expectation of the regret given T \u2265 t, starting from cumulative loss vector M and assuming both the learner and the adversary are optimal. This is similar to the function V in the fixed horizon case, and again the value of the game infAlg supZ1:\u221e ET\u223cQ[Reg(LT ,MT )] is simply V\u03041(0).\nTo prove Eq. (10), we plug the definition of V\u0304t+1(M+ ei) into the right hand side and get minP maxi g(P, i) where g(P, i) is\nPi + q(t, t)V (M+ ei, 0) + (1\u2212 q(t, t))E[V (M+ ei, T \u2212 t)|T \u2265 t+ 1].\nUsing the fact that for any t\u2032 \u2265 t+ 1,\n(1\u2212q(t, t))q(t\u2032, t+1) = Pr[T > t|T \u2265 t] Pr[T = t\u2032|T \u2265 t+1] = Pr[T = t\u2032|T \u2265 t] = q(t\u2032, t),\ng(P, i) can be simplified in the following way:\ng(P, i) = Pi + q(t, t)V (M+ ei, 0) + (1\u2212 q(t, t)) \u221e\u2211\nT=t+1\n(q(T, t+ 1)V (M+ ei, T \u2212 t))\n= Pi + q(t, t)V (M+ ei, 0) +\n\u221e\u2211\nT=t+1\n(q(T, t)V (M + ei, T \u2212 t))\n= Pi + E[V (M+ ei, T \u2212 t)|T \u2265 t]. (11)\nAlso, the average of g(P, i) over all i is independent of P:\n1\nN\nN\u2211\ni=1\ng(P, i) = 1\nN +\n1\nN\nN\u2211\ni\nE[V (M+ ei, T \u2212 t)|T \u2265 t]\n= E\n[\n1\nN +\n1\nN\nN\u2211\ni\nV (M + ei, T \u2212 t)|T \u2265 t ]\n= E\n[\n1\nN +\n1\nN\nN\u2211\ni\n( T \u2212 t N \u2212R(M + ei, T \u2212 t) ) |T \u2265 t ]\n= E[ T \u2212 t+ 1\nN \u2212R(M, T \u2212 t+ 1)|T \u2265 t] (by definition of R)\n= E[V (M, T \u2212 t+ 1)|T \u2265 t]\nwhich implies\nmin P\u2208\u2206(N) max i\u2208[N ]\ng(P, i) \u2265 E[V (M, T \u2212 t+ 1)|T \u2265 t]. (12)\nOn the other hand, let P\u2217 = E[PT |T \u2265 t], where PTi = V (M, T \u2212 t+ 1)\u2212 V (M+ ei, T \u2212 t). P\u2217 is a valid distribution since PT is a distribution for any T . Also, by plugging into Eq. (11),\ng(P\u2217, i) = E[V (M, T \u2212 t+ 1)\u2212 V (M+ ei, T \u2212 t)|T \u2265 t] + E[V (M+ ei, T \u2212 t)|T \u2265 t] = E[V (M, T \u2212 t+ 1)|T \u2265 t].\nTherefore,\nmin P\u2208\u2206(N) max i\u2208[N ] g(P, i) \u2264 max i\u2208[N ] g(P\u2217, i) = E[V (M, T \u2212 t+ 1)|T \u2265 t]. (13)\nEq. (12) and (13) show that minP maxi g(P, i) = E[V (M, T \u2212 t+ 1)|T \u2265 t], which agrees with the left hand side of Eq. (10). We thus prove\ninf Alg sup Z1:\u221e\nET\u223cQ[Reg(LT ,MT )] = V\u03041(0) = E[V (0, T )|T \u2265 1]\n= ET\u223cQ[inf Alg sup Z1:T Reg(LT ,MT )],\nand P\u2217 is the optimal strategy."}, {"heading": "C Proof of Theorem 3", "text": "To prove Theorem 3, we need to find out what V (0, T ) is under the general loss space [0, 1]2. Note that Theorem 1 only tells us the case of the basis vector loss space. Fortunately, it turns out that they are the same if N = 2. To be more specific, we will show\nlater in Theorem 9 that if N = 2 and LS = [0, 1]2, then V (0, T ) = T/2 \u2212 R(0, T ), which can be further simplified as\nV (0, T ) = T 2 \u2212 1 2T\nT\u2211\nm=0\n( T\nm\n)\nmin{m,T \u2212m} = T 2T ( T \u2212 1 \u230aT2 \u230b ) .\nWe can now prove Theorem 3 using this explicit scaling factor, denoted by S(T ) for simplicity.\nProof of Theorem 3. Again, solving Eq. (4) is equivalent to finding the value function V\u0303 defined on each state of the game, similar to the functions V and V\u0304 we had before. The difference is that V\u0303 should be a function of not only the index of the current round t and the cumulative loss vector M, but also the cumulative loss L for the learner. Moreover, to obtain a base case for the recursive definition, it is convenient to first assume that T is at most T0, where T0 is some fixed integer. Under these conditions, we define V\u0303 T0t (L,M) recursively as:\nV\u0303 T0T0 (L,M) , minP\u2208\u2206(N) max Z\u2208LS Reg(L+P \u00b7 Z,M+ Z) V (0, T0) ,\nV\u0303 T0t (L,M) , min P\u2208\u2206(N) max Z\u2208LS max\n{ Reg(L+P \u00b7 Z,M + Z)\nV (0, t) , V\u0303 T0t+1(L+P \u00b7 Z,M+ Z)\n}\n,\nwhich is the scaled regret starting from round t with cumulative loss L for the learner and M for the actions, assuming both the learner and the adversary will play optimally from this round on. The value of the game V\u0303 is now lim\nT0\u2192+\u221e V\u0303 T01 (0,0).\nTo simplify this value function, we will need three facts. First, the base case can be related to V (M, 1):\nV\u0303 T0T0 (L,M) = minP\u2208\u2206(N) max Z\u2208LS Reg(L+P \u00b7 Z,M+ Z) V (0, T0)\n=\n(\nL+ min P\u2208\u2206(N) max Z\u2208LS\nReg(P \u00b7 Z,M+ Z) )/\nV (0, T0)\n=\n(\nL+ min P\u2208\u2206(N) max Z\u2208LS\nP \u00b7 Z+ V (M+ Z, 0) )/\nV (0, T0)\n= L+ V (M, 1)\nV (0, T0) .\nSecond, for any L and M, one can inductively show that\nV\u0303 T0t (L,M) = V\u0303 T0 t (L \u2212R(M, 0),M\u2032), (14)\nwhere M \u2032i = Mi \u2212R(M, 0). (We omit the details since it is straightforward.)\nThird, when M = 0, by symmetry, one has\nV\u0303 T0t (L,0) = max Z\u2208LS max\n{ Reg(L+Pu \u00b7 Z,Z)\nV (0, t) , V\u0303 T0t+1(L+Pu \u00b7 Z,Z)\n}\n(Pu = ( 1N , . . . , 1 N ))\n\u2265 max { L+ 1N V (0, t) , V\u0303 T0t+1(L+ 1 N , e1) } . (15)\nNow we can make use of the condition N = 2 to lower bound V\u0303 . The key point is to consider a restricted adversary who can only place one unit more loss on one of the action than the other, if not stopping the game. Clearly the value of this restricted game serves as a lower bound of V\u0303 . Specifically, consider the value of V\u0303 T0t (L, e1) for t \u2264 T0 \u2212 2:\nV\u0303 T0t (L, e1) \u2265 min p\u2208[0,1] max\n{ Reg(L+ p, 2e1)\nS(t) , V\u0303 T0t+1(L+ 1\u2212 p, e1 + e2)\n}\n(restricted adversary)\n= min p\u2208[0,1] max\n{ L+ p\nS(t) , V\u0303 T0t+1(L\u2212 p,0)\n}\n(by Eq. (14))\n\u2265 min p\u2208[0,1] max\n{ L+ p\nS(t) , L+ 1/2\u2212 p S(t+ 1) , V\u0303 T0t+2(L+ 1 2 \u2212 p, e1)\n}\n(by Eq. (15))\n\u2265 min p\u2208R max\n{ L+ p\nS(t) , V\u0303 T0t+2(L+\n1 2 \u2212 p, e1)\n}\nTherefore, if we assume T0 is even without loss of generality and define function GT0t (L) recursively as:\nGT0T0(L) , V\u0303 T0 T0\n(L, e1) = L+ V (e1, 1)\nS(T0) =\nL\nS(T0)\nGT0t (L) , min p\u2208R max\n{ L+ p\nS(t) , GT0t+2(L +\n1 2 \u2212 p)\n}\n,\nthen it is clear that V\u0303 T0t (L, e1) \u2265 GT0t (L), and thus by 15,\nV\u0303 T01 (0,0) \u2265 max{1, V\u0303 T02 ( 1\n2 , e1)} \u2265 max{1, GT02 (\n1 2 )}.\nIt remains to compute GT02 ( 1 2 ). By some elementary computations and the fact that\nfor two linear functionsh1(p) and h2(p) of different signs of slopes, minp max{h1(p), h2(p)} = h1(p\n\u2217) where p\u2217 is such that h1(p\u2217) = h2(p\u2217), one can inductively prove that for t = 2, 4, . . . , T0,\nGT0t (L) = 2\nT0\u2212t\n2 (L+ 12 )\u2212 12\nS(T0) + (T0\u2212t)/2\u2211\nk=1\n(2k\u22121S(T0 \u2212 2k)) .\nPlugging S(t) = t2t ( t\u22121 \u230at/2\u230b ) and letting T0 \u2192 \u221e, we arrive at\nlim T0\u2192\u221e GT02 (1/2) = lim T0\u2192\u221e\n\n\nT0/2\u22121\u2211\nk=1\n( 2k\u2212T0/2S(T0 \u2212 2k) )\n\n\n\u22121\n= lim T0\u2192\u221e\n\n\nT0/2\u22121\u2211\nk=1\n( S(2k)\n2k\n) \n\n\u22121\n=\n\n\n\u221e\u2211\nj=0\nj\n8j\n( 2j\nj\n) \n\n\u22121\n.\nDefine G(x) = \u2211\u221e\nj=0 ( 2j j ) xj and F (x) = x \u00b7 G\u2032(x). Note that what we want to\ncompute above is exactly 1/F (18 ). In [20], it was shown that G(x) = (1 \u2212 4x)\u22121/2. Therefore, F (x) = 2x \u00b7 (1\u2212 4x)\u22123/2 and\nlim T0\u2192\u221e\nGT02 (1/2) = 1/F (1/8) = \u221a 2.\nWe conclude the proof by pointing out\nV\u0303 = lim T0\u2192\u221e V\u0303 T01 (0,0) \u2265 max{1, lim T0\u2192\u221e\nGT02 (1/2)} = \u221a 2.\nAs we mentioned at the beginning of this section, the last thing we need to show is that the value V (0, T ) is the same under the two loss spaces. In fact, we will prove stronger results in the following theorem claiming that this is true only if N = 2.\nTheorem 9. Let LS1,LS2,LS3 be the three loss spaces {e1, . . . , eN}, {0, 1}N and [0, 1]N respectively, and VLS(0, T ) be the value of the game V (0, T ) under the loss space LS. If N > 2, we have for any T ,\nVLS1(0, T ) < VLS2(0, T ) = VLS3(0, T ).\nHowever, the three values above are the same if N = 2.\nProof. We first inductively show that for anyM and r, VLS2(M, r) = VLS3(M, r) and VLS3(M, r) is convex in M. For the base case r = 0, by definition, VLS2(M, 0) = VLS3(M, 0) = \u2212mini Mi. Also, for any two loss vectors M and M\u2032, and \u03bb \u2208 [0, 1],\nVLS3(\u03bbM + (1\u2212 \u03bb)M\u2032, 0) = \u2212min i (\u03bbMi + (1 \u2212 \u03bb)M \u2032i)\n\u2264 \u2212min i (\u03bbMi)\u2212min i ((1\u2212 \u03bb)M \u2032i) = \u03bbVLS3(M, 0) + (1\u2212 \u03bb)VLS3(M\u2032, 0),\nshowing VLS3(M, 0) is convex in M. For r > 0,\nVLS3(M, r) = min P\u2208\u2206(N) max Z\u2208LS3\n(P \u00b7 Z+ VLS3(M+ Z, r \u2212 1)) .\nNotice that P \u00b7Z+ VLS3(M+Z, r \u2212 1) is equal to P \u00b7Z+ VLS2(M+ Z, r \u2212 1) and is convex in Z by induction. Therefore the maximum is always achieved at one of the corner points of LS3, which is in LS2. In other words,\nVLS3(M, r) = min P\u2208\u2206(N) max Z\u2208LS2\n(P \u00b7 Z+ VLS2(M+ Z, r \u2212 1)) = VLS2(M, r).\nOn the other hand, by introducing a distribution Q over all the elements in LS2, we have\nVLS3(M, r) = min P\u2208\u2206(N) max Z\u2208LS2\n(P \u00b7 Z+ VLS3(M+ Z, r \u2212 1))\n= min P\u2208\u2206(N) max Q\nEZ\u223cQ [P \u00b7 Z+ VLS3(M+ Z, r \u2212 1)]\n= max Q min P\u2208\u2206(N)\nEZ\u223cQ [P \u00b7 Z+ VLS3(M+ Z, r \u2212 1)]\n= max Q\n(\nEZ\u223cQVLS3(M + Z, r \u2212 1) + min P\u2208\u2206(N)\nP \u00b7 EZ\u223cQ[Z] ) ,\nwhere we switch the min and max by Corollary 37.3.2 of [21]. Note that the last expression is the maximum over a family of linear combinations of convex functions in M, which is still a convex function in M, completing the induction step. To conclude, VLS2(0, T ) = VLS3(0, T ) for any N and T .\nWe next prove if N = 2, VLS1(0, T ) = VLS2(0, T ). Again, we inductively prove VLS1(M, r) = VLS2(M, r) for any M and r. The base case is clear. For r > 0, let P \u2217i = VLS1(M, r)\u2212 VLS1(M+ ei, r \u2212 1) (i = 1, 2). By induction,\nVLS2(M, r) = min P\u2208\u2206(2) max Z\u2208LS2\n(P \u00b7 Z+ VLS1(M + Z, r \u2212 1))\n\u2264 max Z1,Z2\u2208{0,1} (P \u22171Z1 + P \u2217 2Z2 + VLS1(M+ (Z1, Z2), r \u2212 1)) = max {VLS1(M, r \u2212 1), 1 + VLS1(M+ (1, 1), r \u2212 1), VLS1(M, r)} = max {VLS1(M, r \u2212 1), VLS1(M, r)}\n(by Property 1 in Proposition 1)\n= VLS1(M, r). (by Property 3 in Proposition 1)\nHowever, it is clear that VLS2(M, r) \u2265 VLS1(M, r). Therefore, VLS1(M, r) = VLS2(M, r).\nFinally, to prove VLS1(0, T ) < VLS2(0, T ) for N > 2, we inductively prove VLS1((T \u2212 r)e1, r) < VLS2((T \u2212 r)e1, r) for r = 1, . . . , T . For the base case r = 1, VLS1((T \u2212 1)e1, 1) = 1/N \u2212R((T \u2212 1)e1, 1) = 1/N , while\nVLS2((T \u2212 1)e1, 1) = min P\u2208\u2206(N) max Z\u2208LS2 (P \u00b7 Z+ VLS2((T \u2212 1)e1 + Z, 0))\n\u2265 min P\u2208\u2206(N) max i\u2208[N ] (1\u2212 Pi + VLS2((T \u2212 1)e1 + 1\u2212 ei, 0))\n= min P\u2208\u2206(N)\nmax {\u2212P1, 1\u2212 P2, . . . , 1\u2212 PN} .\nWe claim that the value of the last minimax expression above, denoted by v, is (N \u2212 2)/(N \u2212 1), which is strictly greater than 1/N if N > 2 and thus proves the base case. To show that, notice that for any P \u2208 \u2206(N), there must exist i \u2208 {2, . . . , N} such that Pi \u2264 1/(N \u2212 1) and\nmax {\u2212P1, 1\u2212 P2, . . . , 1\u2212 PN} \u2265 1\u2212 Pi \u2265 N \u2212 2 N \u2212 1 ,\nshowing v \u2265 (N \u2212 2)/(N \u2212 1). On the other hand, the equality is realized by the distribution P\u2217 = (0, 1N\u22121 , . . . , 1 N\u22121).\nFor r > 1, we have\nVLS2((T \u2212 r)e1, r) = min P\u2208\u2206(N) max Z\u2208LS2 (P \u00b7 Z+ VLS2((T \u2212 r)e1 + Z, r \u2212 1))\n\u2265 min P\u2208\u2206(N) max i\u2208[N ] (Pi + VLS2((T \u2212 r)e1 + ei, r \u2212 1))\n\u2265 min P\u2208\u2206(N)\n1\nN\nN\u2211\ni=1\n(Pi + VLS2((T \u2212 r)e1 + ei, r \u2212 1))\n= 1\nN +\n1\nN\nN\u2211\ni=1\nVLS2((T \u2212 r)e1 + ei, r \u2212 1)\n> 1\nN +\n1\nN\nN\u2211\ni=1\nVLS1((T \u2212 r)e1 + ei, r \u2212 1)\n= VLS1((T \u2212 r)e1, r).\nHere, the last strict inequality holds because for i = 1, VLS2((T \u2212 r + 1)e1, r \u2212 1) > VLS1((T \u2212 r+1)e1, r\u2212 1) by induction; for i 6= 1, it is trivial that VLS2((T \u2212 r)e1 + ei, r \u2212 1) \u2265 VLS1((T \u2212 r)e1 + ei, r \u2212 1). Therefore, we complete the induction step and thus prove VLS1(0, T ) < VLS2(0, T )."}, {"heading": "D Proof of Theorem 5", "text": "The proof (and the one of Theorem 6) relies heavily on a common technique to approximate a sum using an integral, which we state without proof as the following claim.\nClaim 1. Let f(x) be a non-increasing nonnegative function defined on R+. Then the following inequalities hold for any integer 0 < j \u2264 k.\n\u222b k+1\nj\nf(x) dx \u2264 k\u2211\ni=j\nf(i) \u2264 \u222b k\nj\u22121 f(x) dx\nProof of Theorem 5. By Theorem 4, it suffices to upper bound V\u03041(0) and \u2211Ts\nt=1 qtV\u0304t+1(0). Let St = \u2211\u221e t\u2032=t 1/t \u2032d. By applying Claim 1 multiple times, we have\n1\nSt \u2264\n(\u222b \u221e\nt\ndx\nxd\n)\u22121 = td\u22121(d\u2212 1); (16)\nqt = 1 St \u00b7 td \u2264 d\u2212 1 t ;\nV\u03041(0) = E[V (0, T )|T \u2265 1] \u2264 cN S1\n\u221e\u2211\nT=1\n1\nT d\u2212 1 2\n(by Theorem 1)\n\u2264 cN S1\n(\n1 +\n\u222b \u221e\n1\ndx\nxd\u2212 1 2\n)\n(by Claim 1)\n= cN (d\u2212 12 ) S1(d\u2212 32 ) \u2264 cN (d\u2212 1)(d\u2212 1 2 )\nd\u2212 32 = O(1); (by Eq. (16))\nV\u0304t+1(0) = E[V (0, T \u2212 t)|T \u2265 t+ 1]\n\u2264 cN St+1\n\u222b \u221e\n0\n\u221a xdx\n(t+ x)d\n= cN St+1\n\u00b7 \u0393(d\u2212 3 2 ) 2\u0393(d) \u00b7 \u221a \u03c0 td\u2212 3 2\n\u2264 (d\u2212 1)cN \u221a \u03c0 \u00b7 \u0393(d\u2212 3 2 )\n2\u0393(d) \u00b7 (t+ 1)\nd\u22121\ntd\u2212 3 2\n;\nTs\u2211\nt=1\nqtV\u0304t+1(0) \u2264 (d\u2212 1)2cN \u221a \u03c0 \u00b7 \u0393(d\u2212 3 2 )\n2\u0393(d)\nTs\u2211\nt=1\ntd\u22121 + o(td\u22121)\ntd\u2212 1 2\n\u2264 \u0393(d\u2212 3 2 )\n\u0393(d) (d\u2212 1)2cN\n\u221a \u03c0Ts + o( \u221a Ts),\nwhich proves the theorem."}, {"heading": "E Proof of Theorem 6", "text": "Proof. We will first show that\nReg(LTs ,MTs) \u2264 (lnN) \u00b7 E [ 1\n\u03b7T |T \u2265 TS + 1\n]\n\ufe38 \ufe37\ufe37 \ufe38\nA\n+ 1\n8\nTs\u2211\nt=1 E[\u03b7T |T \u2265 t] \ufe38 \ufe37\ufe37 \ufe38\nB\n. (17)\nLet \u03a6Tt = 1 \u03b7T\nln ( \u2211N i=1 exp(\u2212\u03b7TMt\u22121,i) ) . The key point of the proof for the non-\nadaptive version of the exponential weights algorithm is to use \u03a6Tt as a \u201cpotential\u201d function, and bound the change in potential before and after a single round [7]. Specifically, they showed that\nPTt \u00b7 Zt \u2264 \u03b7T 8 + \u03a6Tt \u2212 \u03a6Tt+1.\nWe also base our proof on this inequality. The total loss of the learner after Ts rounds is\nLTs =\nTs\u2211\nt=1\nE[PTt |T \u2265 t] \u00b7 Zt = Ts\u2211\nt=1\nE[PTt \u00b7 Zt|T \u2265 t]\n\u2264 B + Ts\u2211\nt=1\nE[\u03a6Tt \u2212 \u03a6Tt+1|T \u2265 t].\nDefine Ut = E[\u03a6Tt |T \u2265 t]. We do the following transformation:\nE[\u03a6Tt \u2212 \u03a6Tt+1|T \u2265 t] = Ut \u2212 ET [\u03a6Tt+1|T \u2265 t] = Ut \u2212 qt\u03a6tt+1 \u2212 (1\u2212 qt)Ut+1 = Ut \u2212 Ut+1 + qt(Ut+1 \u2212 \u03a6tt+1) = Ut \u2212 Ut+1 + qt \u00b7 E[\u03a6Tt+1 \u2212 \u03a6tt+1|T \u2265 t+ 1] = Ut \u2212 Ut+1 + qt \u00b7 E[FT,t(Mt)|T \u2265 t+ 1],\nwhere we define\nFT,t(M) = ln (\n\u2211\ni exp(\u2212\u03b7TMi)) \u03b7T \u2212 ln ( \u2211 i exp(\u2212\u03b7tMi)) \u03b7t .\nA key observation is\nmax M\u2208RN\n+ \u03b7T <\u03b7t\nFT,t(M) = lnN \u03b7T \u2212 lnN \u03b7t , (18)\nwhich can be verified by a standard derivative analysis that we omit. (An alternative approach using KL-divergence can be found in Chapter 2.5 of [19].)\nWe further define another potential function \u03a6\u0304Tt = (lnN)/\u03b7T and also U\u0304t = E[\u03a6\u0304Tt |T \u2265 t]. Note that the new potential \u03a6\u0304Tt has no dependence on t and thus\n\u03a6\u0304Tt = \u03a6\u0304 T t\u2032 for any t, t \u2032. We now have\nTs\u2211\nt=1\nE[\u03a6Tt \u2212 \u03a6Tt+1|T \u2265 t]\n=\nTs\u2211\nt=1\n( Ut \u2212 Ut+1 + qt \u00b7 E[\u03a6Tt+1 \u2212 \u03a6tt+1|T \u2265 t+ 1] )\n= U1 \u2212 UTs+1 + Ts\u2211\nt=1\n( qt \u00b7 E[\u03a6Tt+1 \u2212 \u03a6tt+1|T \u2265 t+ 1] )\n\ufe38 \ufe37\ufe37 \ufe38\nC\n(19)\n\u2264 U1 \u2212 UTs+1 + Ts\u2211\nt=1\n(\nqt \u00b7 E[ lnN \u03b7T \u2212 lnN \u03b7t |T \u2265 t+ 1]\n)\n(by Eq. (18))\n= U\u03041 \u2212 U\u0304Ts+1 + Ts\u2211\nt=1\n( qt \u00b7 E[\u03a6\u0304Tt+1 \u2212 \u03a6\u0304tt+1|T \u2265 t+ 1] )\n\ufe38 \ufe37\ufe37 \ufe38\nD\n+U\u0304Ts+1 \u2212 UTs+1.\n(\u2235 U1 = U\u03041)\nNotice that D has the exact same form as C except for a different definition of the potential, and also Eq. (19) is an equality. Therefore, by a reverse transformation, we have\nTs\u2211\nt=1\nE[\u03a6Tt \u2212 \u03a6Tt+1|T \u2265 t] = Ts\u2211\nt=1\nE[\u03a6\u0304Tt \u2212 \u03a6\u0304Tt+1|T \u2265 t] + U\u0304Ts+1 \u2212 UTs+1\n= U\u0304Ts+1 \u2212 UTs+1 (\u2235 \u03a6\u0304Tt = \u03a6\u0304Tt+1)\nU\u0304Ts+1 is exactly A in Eq. (17), and UTs+1 can be related to the loss of the best action:\nUTs+1 = E\n[\n1\n\u03b7T ln\nN\u2211\ni=1\nexp(\u2212\u03b7TMTs,i) | T \u2265 Ts + 1 ]\n\u2265 E [ 1\n\u03b7T ln exp(\u2212\u03b7TR(MTs , 0)) | T \u2265 Ts + 1\n]\n= \u2212R(MTs , 0).\nThe regret is therefore\nReg(LTs ,MTs) = LTS \u2212R(MTs , 0) \u2264 A+B \u2212 UTs+1 \u2212R(MTs , 0) \u2264 A+B,\nproving Eq. (17). The rest of the proof is merely to plug in the distribution and \u03b7T = \u221a\n(b lnN)/T , and upper bound Eq. (17) using Claim 1. Adopting the notation St = \u2211\u221e t\u2032=t 1/t \u2032d and\nthe result of Eq. (16) in the proof of Theorem 5, we have\nA =\n\u221a lnN\nSTs+1 \u221a b\n\u221e\u2211\nT=Ts+1\n1\nT d\u22121/2\n\u2264 (d\u2212 1) \u221a lnN\u221a\nb (Ts + 1)\nd\u22121 (\u222b \u221e\nTs+1\ndx\nxd\u22121/2 +\n1\n(Ts + 1)d\u22121/2\n)\n= d\u2212 1\n(d\u2212 3/2) \u221a b\n\u221a Ts lnN + o( \u221a Ts lnN);\nB =\n\u221a b lnN\n8\nTs\u2211\nt=1\n1\nSt\n\u221e\u2211\nT=t\n1\nT d+1/2\n\u2264 (d\u2212 1) \u221a b lnN\n8\nTs\u2211\nt=1\ntd\u22121 (\u222b \u221e\nt\ndx\nxd+1/2 +\n1\ntd+1/2\n)\n\u2264 (d\u2212 1) \u221a b lnN\n8\nTs\u2211\nt=1\n( 1\n(d\u2212 1/2) \u221a t +\n1\ntd+3/2\n)\n\u2264 \u221a b(d\u2212 1)\n4(d\u2212 1/2) \u221a Ts lnN + o( \u221a Ts lnN).\nCombining the bounds above for A and B proves the theorem."}, {"heading": "F Proof of Theorem 7", "text": "Proof. The main idea resembles the one of Theorem 6, but the details are much more technical. Let us first define several notations:\nSt ,\n\u222b \u221e\nmt\ndm md =\n1\n(d\u2212 1)md\u22121t ,\nqt , Pr[m < mt|m \u2265 mt\u22121] = 1\nSt\u22121\n\u222b mt\nmt\u22121\ndm md = 1\u2212 ( mt\u22121 mt )d\u22121 ,\nY mt ,\nN\u2211\ni=1\nexp(\u2212\u03b7mMt\u22121,i),\n\u03a6mt ,\n(\n1 + 1\n\u03b7m\n)\nlnY mt , Ut , E[\u03a6 m t |m \u2265 mt\u22121].\nThe proof starts from the following property of the exponential weights algorithm [7]:\nPmt \u00b7 Zt \u2264 1 1\u2212 e\u2212\u03b7m ( lnY mt \u2212 lnY mt+1 )\n\u2264 \u03a6mt \u2212 \u03a6mt+1. (\u2235 \u03b7m \u2265 ln(1 + \u03b7m))\nBy the fact that fm\u2265mt\u22121(m \u2032) = (1 \u2212 qt)fm\u2265mt(m\u2032) for any m\u2032 \u2265 mt, where fm\u2265mt\u22121 and fm\u2265mt are conditional density functions, the loss of the learner after Ts rounds is\nLTs =\nTs\u2211\nt=1\nE[Pmt \u00b7 Zt|m \u2265 mt\u22121]\n\u2264 Ts\u2211\nt=1\nE[\u03a6mt \u2212 \u03a6mt+1|m \u2265 mt\u22121]\n=\nTs\u2211\nt=1\n(\nUt \u2212 \u222b mt\nmt\u22121\n\u03a6mt+1fm\u2265mt\u22121(m)dm+ (1 \u2212 qt)Ut+1 )\n\u2264 Ts\u2211\nt=1\n(\nUt \u2212 \u03a6mt\u22121t+1 \u222b mt\nmt\u22121\nfm\u2265mt\u22121(m)dm+ (1\u2212 qt)Ut+1 )\n= U1 \u2212 UTs+1 + Ts\u2211\nt=1\nqt(Ut+1 \u2212 \u03a6mt\u22121t+1 ),\nHere the last inequality holds because\u03a6mt is increasing in m. To show this, we consider the following\n(\n1 + 1\n\u03b7\n)\nln\nN\u2211\ni=1\nexp(\u2212\u03b7ai) = ( 1 + 1\n\u03b7\n)(\n\u2212\u03b7a1 + ln N\u2211\ni=1\nexp(\u2212\u03b7(ai \u2212 a1)) )\n= \u2212(\u03b7 + 1)a1 + ( 1 + 1\n\u03b7\n)\nln N\u2211\ni=1\nexp(\u2212\u03b7(ai \u2212 a1)),\nwhere \u03b7, a1, . . . , aN are positive. Since ln \u2211\ni exp(\u2212\u03b7(ai \u2212 a1)) \u2265 0, the expression above is decreasing in \u03b7, which along with the fact that \u03b7m decreases in m shows that \u03a6mt increases in m.\nWe now compute U1 and UTs+1:\nU1 = E[(1 + \u221a m/ lnN) lnN | m \u2265 1] = lnN + d\u2212 1 d\u2212 3/2 \u221a lnN\nUTs+1 = E\n[\n(1 + 1/\u03b7m) ln \u2211\ni\nexp(\u2212\u03b7mMTs,i) | m \u2265 mTs\n]\n\u2265 E[(1 + 1/\u03b7m)(\u2212\u03b7mm\u2217) | m \u2265 mTs ] = \u2212m\u2217 (1 + E[\u03b7m | m \u2265 mTs ]) = \u2212m\u2217 ( 1 + d\u2212 1\nd\u2212 1/2\n\u221a\nlnN\nmTs\n)\n\u2265 \u2212m\u2217 \u2212 d\u2212 1 d\u2212 1/2 \u221a m\u2217 lnN (\u2235 mTs = m \u2217 + 1)\nFor Ut+1 \u2212 \u03a6mt\u22121t+1 = E[\u03a6mt+1 \u2212 \u03a6 mt\u22121 t+1 | m \u2265 mt], we first upper bound the part\ninside the expectation:\n\u03a6mt+1 \u2212 \u03a6 mt\u22121 t+1\n= ( lnY mt+1 \u03b7m \u2212 lnY mt\u22121 t+1 \u03b7mt\u22121 ) + (\u03b7mt\u22121 \u2212 \u03b7m)min i Mt,i + ln \u2211 e\u2212\u03b7m(Mt,i\u2212mini Mt,i) \u2211 e\u2212\u03b7mt\u22121(Mt,i\u2212mini Mt,i) .\nThe first term above is at most (\n1 \u03b7m \u2212 1\u03b7mt\u22121 ) lnN = \u221a lnN( \u221a m \u2212 \u221amt\u22121) by\nEq. (18). The second term is at most \u221a lnN( 1\u221amt\u22121 \u2212 1\u221a m )mt\u22121 since miniMt,i = mt \u2212 1 \u2264 mt\u22121, and the last term is at most lnN since the numerator is at most N while the denominator is at least 1. Therefore, we have\nUt+1 \u2212 \u03a6mt\u22121t+1 \u2264 lnN + \u221a lnN \u00b7 E[ \u221a m\u2212 mt\u22121\u221a\nm | m \u2265 mt]\n= lnN + \u221a lnN ( d\u2212 1 d\u2212 3/2 \u221a mt \u2212 d\u2212 1 d\u2212 1/2 mt\u22121\u221a mt ) \u2264 lnN + \u221a lnN\n( d\u2212 1 d\u2212 3/2 \u221a mt \u2212 d\u2212 1 d\u2212 1/2 mt \u2212 1\u221a mt )\n= lnN + (d\u2212 1)\n\u221a mt lnN\n(d\u2212 3/2)(d\u2212 1/2) + d\u2212 1 d\u2212 1/2\n\u221a\nlnN\nmt .\nIt remains to compute \u2211Ts t=1 qt(Ut+1\u2212\u03a6 mt\u22121 t+1 ), which, using the above, can be done by computing A = \u2211Ts\nt=1 qt, B = \u2211Ts t=1 qt \u221a mt and C = \u2211Ts t=1 qt/ \u221a mt. By inequality\n1\u2212 x \u2264 \u2212 lnx for any x > 0, we have\nA =\nTs\u2211\nt=1\n(\n1\u2212 ( mt\u22121 mt\n)d\u22121)\n\u2264 \u2212(d\u22121) Ts\u2211\nt=1\n(lnmt\u22121 \u2212 lnmt) = (d\u22121) ln(m\u2217+1).\nFor B, we first show qt \u221a mt \u2264 2(d\u2212 1)( \u221a mt \u2212\u221amt\u22121), which is equivalent to\nqt \u221a mt\u221a\nmt \u2212\u221amt\u22121 =\n( mt\nmt\u22121\n)d\u22121 \u2212 1\n( mt\nmt\u22121\n)d\u22121 \u2212 (\nmt mt\u22121\n)d\u22123/2 \u2264 2(d\u2212 1)\nif mt 6= mt\u22121 (it is trivial otherwise). Define h(x) = (xd\u22121\u2212 1)/(xd\u22121\u2212xd\u22123/2) for x \u2208 [1, 2] (note that mt/mt\u22121 is within this interval). One can verify that h\u2032(x) < 0 and thus h(x) \u2264 limx\u21921 h(x) = 2(d \u2212 1). So we prove qt \u221a mt \u2264 2(d \u2212 1)(\n\u221a mt \u2212\u221a\nmt\u22121) and\nB \u2264 2(d\u2212 1) Ts\u2211\nt=1\n( \u221a mt \u2212 \u221a mt\u22121) = 2(d\u2212 1)( \u221a mTs \u2212 1) \u2264 2(d\u2212 1) \u221a m\u2217.\nA simple comparison of B and C shows C = o( \u221a m\u2217). We finally conclude the proof by combining all we have\nReg(LTs ,MTs)\n\u2264 U1 \u2212 UTs+1 + Ts\u2211\nt=1\nqt(Ut+1 \u2212 \u03a6mt\u22121t+1 )\u2212m\u2217\n= (1 + (d\u2212 1) ln(m\u2217 + 1)) lnN + ( d\u2212 1 d\u2212 1/2 + 2(d\u2212 1)2 (d\u2212 3/2)(d\u2212 1/2) )\u221a m\u2217 lnN + o( \u221a m\u2217 lnN) = 3(d\u2212 7/6)(d\u2212 1) (d\u2212 3/2)(d\u2212 1/2) \u221a m\u2217 lnN + (1 + (d\u2212 1) ln(m\u2217 + 1)) lnN + o( \u221a m\u2217 lnN)."}, {"heading": "G Proof of Theorem 8", "text": "Proof. Let \u03a6Tt = \u221a\n\u2016Wt\u22121\u20162 + (T \u2212 t+ 1) be the potential function for this setting. The key property of the minimax algorithm Eq. (6) shown in [6] is the following:\nxTt \u00b7wt \u2264 \u03a6Tt \u2212 \u03a6Tt+1.\nBased on this property, the loss of our algorithm after Ts rounds is\nTs\u2211\nt=1\nE[xTt |T \u2265 t] \u00b7wt = Ts\u2211\nt=1\nE[xTt \u00b7wt|T \u2265 t] \u2264 Ts\u2211\nt=1\nE[\u03a6Tt \u2212 \u03a6Tt+1|T \u2265 t].\nNow define Ut = E[\u03a6Tt |T \u2265 t] and qt = Pr[T < t + 1|T \u2265 t]. By the fact that fT\u2265t(t\u2032) = (1\u2212 qt)fT\u2265t+1(t\u2032) for any t\u2032 \u2265 t+ 1, where fT\u2265t and fT\u2265t+1 are condi-\ntional density functions, we have\nTs\u2211\nt=1\nE[xTt |T \u2265 t] \u00b7wt\n\u2264 Ts\u2211\nt=1\n( Ut \u2212 E[\u03a6Tt+1|T \u2265 t] )\n=\nTs\u2211\nt=1\n( Ut \u2212 \u222b t+1\nt\n\u03a6Tt+1fT\u2265t(T )dT \u2212 (1\u2212 qt)Ut+1 )\n\u2264 Ts\u2211\nt=1\n( Ut \u2212 \u03a6tt+1 \u222b t+1\nt\nfT\u2265t(T )dT \u2212 (1 \u2212 qt)Ut+1 ) (\u2235 \u03a6Tt+1 increases in T )\n=\nTs\u2211\nt=1\n(Ut \u2212 Ut+1 + qt(Ut+1 \u2212 \u2016Wt\u22121\u2016)) (\u2235 \u03a6tt+1 = \u2016Wt\u22121\u2016)\n= U1 \u2212 UTs+1 + Ts\u2211\nt=1\nqtE [\u221a \u2016Wt\u22121\u20162 + (T \u2212 t)\u2212 \u2016Wt\u22121\u2016 | T \u2265 t+ 1 ]\n\u2264 U1 \u2212 UTs+1 + Ts\u2211\nt=1\nqtE [\u221a T \u2212 t | T \u2265 t+ 1 ] . (\u2235 \u221a a+ b\u2212\u221aa \u2264 \u221a b)\nNote that UTs+1 \u2265 \u2016WT \u2016, and thus it remains to plug in the distribution and compute U1 and \u2211Ts t=1 qtE[ \u221a T \u2212 t | T \u2265 t+ 1], which is almost the same process as what we did in the proof of Theorem 5 if one realizes qt \u2264 (d\u2212 1)/t also holds here. In a word, the regret can be bounded by\n\u0393(d\u2212 32 ) \u0393(d) (d\u2212 1)2 \u221a \u03c0Ts + o( \u221a Ts),\nwhich is \u03c0 \u221a Ts + o( \u221a Ts) if d = 2. The explicit form in Eq. (7) comes from a direct calculation."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We consider online learning when the time horizon is unknown. We apply a<lb>minimax analysis, beginning with the fixed horizon case, and then moving on to<lb>two unknown-horizon settings, one that assumes the horizon is chosen randomly<lb>according to some known distribution, and the other which allows the adversary<lb>full control over the horizon. For the random horizon setting with restricted losses,<lb>we derive a fully optimal minimax algorithm. And for the adversarial horizon set-<lb>ting, we prove a nontrivial lower bound which shows that the adversary obtains<lb>strictly more power than when the horizon is fixed and known. Based on the mini-<lb>max solution of the random horizon setting, we then propose a new adaptive algo-<lb>rithm which \u201cpretends\u201d that the horizon is drawn from a distribution from a special<lb>family, but no matter how the actual horizon is chosen, the worst-case regret is of<lb>the optimal rate. Furthermore, our algorithm can be generalized in many ways,<lb>including handling other unknown information and other online learning settings.<lb>Experiments show that our algorithm outperforms many other existing algorithms<lb>in an online linear optimization setting.", "creator": "LaTeX with hyperref package"}}}