{"id": "1310.1934", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2013", "title": "Discriminative Features via Generalized Eigenvectors", "abstract": "representing examples in statistical portfolio that is tied with the underlying classifier can greatly enhance the performance of a learning system. in this paper studies investigate scalable techniques for investigating discriminative features by taking advantage of simple second order structure in the data. theorists focus on multiclass classification and recognizes that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. we evaluate classifiers built some objective features on three different tasks, obtaining reflections into the derived results.", "histories": [["v1", "Mon, 7 Oct 2013 20:05:52 GMT  (113kb,D)", "http://arxiv.org/abs/1310.1934v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nikos karampatziakis", "paul mineiro"], "accepted": true, "id": "1310.1934"}, "pdf": {"name": "1310.1934.pdf", "metadata": {"source": "META", "title": "Discriminative Features via Generalized Eigenvectors", "authors": ["Nikos Karampatziakis", "Paul Mineiro"], "emails": ["nikosk@microsoft.com", "pmineiro@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Supervised learning has been a great success story for machine learning, both in theory and in practice. In theory, we have a good understanding of the conditions under which supervised learning can succeed (Vapnik, 1998). In practice, supervised learning approaches are profitably employed in many domains, from movie recommendation to speech and image recognition (Koren et al., 2009; Hinton et al., 2012a; Krizhevsky et al., 2012). The success of all of these systems crucially hinges on the compatibility between the model and the representation used to solve the problem.\nFor some problems, the kinds of representations and models that lead to good performance are well-known. In text classification, for example, unigram and bigram features together with linear classifiers are known\nto work well for a variety of related tasks (Halevy et al., 2009). For other problems, such as drug design, speech, and image recognition, far less is known about which combinations are effective. This has fueled interest in methods that can learn the appropriate representations directly from the raw signal, with techniques such as dictionary learning (Mairal et al., 2008) and deep learning (Krizhevsky et al., 2012; Hinton et al., 2012a) achieving state of the art performance in many important problems.\nIn this work, we explore conceptually and computationally simple ways to create discriminative features that can scale to a large number of examples, even when data is distributed across many machines. Our techniques are not a panacea. They are exploiting simple second order structure in the data and it is very easy to come up with sufficient conditions under which they will not give any advantage over learning using the raw signal. Nevertheless, they empirically work remarkably well.\nOur setup is the usual multiclass setting where we are given labeled data {xi, yi}ni=1, sampled iid from a distribution D on Rd\u00d7 [k], and we need to come up with a classifier h : Rd \u2192 [k] with low generalization error PD(h(x) 6= y). Abusing notation, we will sometimes use y to refer to the one hot encoding of y that identifies each class with one of the vertices of the standard k\u2212 1-simplex. To keep the focus on the quality of our feature representation we will restrict ourselves to h being linear, such as a multiclass linear SVM or multinomial logistic regression. We suspect representations that improve the performance of linear classifiers will also beneficially compose with nonlinear techniques."}, {"heading": "2. Method", "text": "One of the simplest possible statistics involving both features and labels is the matrix E[xy>], which in multiclass classification is the collection of classconditional mean feature vectors. This statistic has been thoroughly explored, e.g., Fisher LDA (Fisher,\nar X\niv :1\n31 0.\n19 34\nv1 [\ncs .L\nG ]\n7 O\nct 2\n01 3\n1936) and Sliced Inverse Regression (Li, 1991). However, in many practical applications we expect that the data distribution contains much more information than that contained in the first moment statistics. The natural next object of study is the tensor E[x\u2297x\u2297 y].\nIn multiclass classification, the tensor E[x \u2297 x \u2297 y] is simply a collection of the conditional second moment matrices Ci = E[xx>|y = i]. There are many standard ways of extracting features from these matrices. For example, one could try per-class PCA (Wold & Sjostrom, 1977) which will find directions that maximize v>E[xx>|y = i]v = E[(v>x)2|y = i], or VCA (Livni et al., 2013) which will find directions that minimize the same quantity. The subtlety here is that there is no reason to believe that these directions are specific to class i. In other words, the directions we find might be very similar for all classes and, therefore, not be discriminative.\nA simple alternative is to work with the quotient\nRij(v) = E[(v>x)2|y = i] E[(v>x)2|y = j] = v>Civ v>Cjv , (1)\nwhose local maximizers are the generalized eigenvectors solving Civ = \u03bbCjv. Despite the non-convexity, efficient and robust routines for solving these types of problems are part of mature software packages such as LAPACK.\nSince objective (1) is homogeneous in v, we will assume that each eigenvector v is scaled such that v>Cjv = 1. Then we have that v>Civ = \u03bb, i.e. on average, the squared projection of an example from class i on v will be \u03bb while the squared projection of an example from class j will be 1. As long as \u03bb is far from 1, this gives us a direction along which we expect to be able to discriminate the two classes by simply using the magnitude of the projection. Moreover, if there are many eigenvalues substantially different from 1 all associated eigenvectors can be used as feature detectors."}, {"heading": "2.1. Useful Properties", "text": "The feature detectors resulting from maximizing equation (1) have two useful properties which we list below. For simplicity we state the results assuming full rank exact conditional moment matrices, and then discuss the impact of regularization and finite samples.\nProposition 1. (Invariance) Under the above assumptions, the embedding v>x is invariant to invertible linear transformations of x.\nProof. Let A \u2208 Rd\u00d7d be invertible and x\u2032 = Ax be the transformed input. Let Cm = E[xx>|y = m] be\nthe second moment matrix given y = m for the original data. For a class pair (i, j), a generalized eigenvector v satisfies Civ = \u03bbCjv. Using the Cholesky factorization Cj = LjL > j and setting v = L \u2212> j u we have\nL\u22121j CiL \u2212> j u = \u03bbu, (2)\ni.e., u is an eigenvector of L\u22121j CiL \u2212> j . Moreover, the embedding for the original data involves only\nv>x = u>L\u22121j x. (3)\nFor the transformed data, the conditional second moments are E[x\u2032x\u2032>|y = m] = AE[xx>|y = m]A> = ACmA\n> and the corresponding generalized eigenvector v\u2032 satisfies ACiA >v\u2032 = \u03bbACjA >v\u2032. Letting v\u2032 = A\u2212>L\u2212>j u \u2032 we see that u\u2032 satisfies L\u22121j CiL \u2212> j u \u2032 = \u03bbu\u2032 which is the same as (2). Therefore u\u2032 can be chosen such that u\u2032 = u. Finally, the embedding involves only v\u2032>x\u2032 = u\u2032>L\u22121j A\n\u22121Ax = u>L\u22121j x which is the same as the embedding (3) for the original data.\nIt is worth pointing out that the results of some popular methods, such as PCA, are not invariant to linear transformations of the inputs. For such methods, differences in preprocessing and normalization can lead to vastly different results. The practical utility of an \u201coff the shelf\u201d classifier is greatly improved by this invariance, which provides robustness to data specification, e.g., differing units of measurement across the original features.\nProposition 2. (Diversity) Two feature detectors v1 and v2 extracted from the same ordered class pair (i, j) have uncorrelated responses E[(v>1 x)(v>2 x)|y = j] = 0.\nProof. This follows from the orthogonality of the eigenvectors in the induced problem L\u22121j CiL \u2212> j u = \u03bbu (c.f. proof of Proposition 1) and the connection v = L\u2212>j u. If u1 and u2 are eigenvectors of L \u22121 j CiL \u2212> j then 0 = u>1 u2 = v > 1 LjL > j v2 = v > 1 E[xx>|y = j]v2 = E[(v>1 x)(v>2 x)|y = j].\nDiversity indicates the different generalized eigenvectors per class pair provide complementary information, and that techniques which only use the first generalized eigenvector are not maximally exploiting the data."}, {"heading": "2.2. Finite Sample Considerations", "text": "Even though we have shown the properties of our method assuming knowledge of the expectations E[xx>|y = m], in practice we estimate these quantities from our training samples. The empirical average\nC\u0302m = \u2211n i=1 I[yi = m]xix>i\u2211n\ni=1 I[yi = m] (4)\nconverges to the expectation at a rate of O(n\u22121/2). Here and below we are suppressing the dependence upon the dimensionality d, which we consider fixed. Typical finite sample tail bounds become meaningful once n = O(d log d) (Vershynin, 2010).\nGiven C\u0302m = Cm + Em with ||Em||2 = O(n\u22121/2), we can use results from matrix perturbation theory to establish that our finite sample results cannot be too far from those obtained using the expected values. For example, if the Crawford number\nc(Ci, Cj) . = min ||v||=1 (v>Civ) 2 + (v>Cjv) 2 > 0,\nand the perturbations Ei and Ej satisfy\n||Ei||22 + ||Ej ||22 < c(Ci, Cj),\nthen (Golub & Van Loan, 2012) for all q \u2208 [d]\ntan(| tan\u22121(\u03bbq)\u2212 tan\u22121(\u03bb\u0302q)|) \u2264 O (\n1\u221a nc(Ci, Cj)\n) ,\nwhere \u03bbq, \u03bb\u0302q are the q-th generalized eigenvalues of the matrix pairs Ci, Cj and C\u0302i, C\u0302j respectively. Similar results apply to the sine of the angle between an estimated generalized eigenvector and the true one (Demmel et al., 2000) Section 5.7."}, {"heading": "2.3. Regularization", "text": "An additional concern with finite samples is that C\u0302m may not be full rank as we have assumed until now. In particular, if there are fewer than d examples in class m, then C\u0302m is guaranteed to be rank deficient. When such a matrix appears in the denominator of (1), estimation of the eigenvectors can be unstable and overly sensitive to the sample at hand. A common solution (Platt et al., 2010) is to regularize the denominator matrix by adding a multiple of the identity to the denominator, i.e., maximizing\nR\u03b3ij(v) = v>C\u0302iv\nv>(C\u0302j + \u03b3I)v , (5)\nwhich is equivalent to maximizing equation (1) with an additional upper-bound constraint on the norm of v. We typically set \u03b3 to be a small multiple of the average eigenvalue of C\u0302j (Friedman, 1989) which can be easily obtained as the trace of C\u0302j divided by d. In Section 4 we find this strategy empirically effective."}, {"heading": "2.4. An Algorithm", "text": "We are left with specifying a full algorithm for multiclass classification. First we need to specify how to\nAlgorithm 1 Generalized Eigenvectors for Multiclass Require: S = {(xi, yi)}ni=1, \u03b8 \u2265 0 and \u03b3 \u2265 0 1: F \u2190 \u2205 2: for (i, j 6= i) \u2208 {1, . . . , k}2 do 3: Solve C\u0302iV = (C\u0302j + \u03b3 d Trace(C\u0302j)I)V \u039b\n4: F \u2190 F \u222a {Vq|\u039bqq \u2265 \u03b8} 5: end for 6: \u03c8v,\u03b1,\u03b4(x) . = max(0, \u03b4v>x)\u03b1/2 7: \u03c6(x) . = [\u03c8v,\u03b1,\u03b4(x)|v, \u03b1, \u03b4 \u2208 F \u00d7 {1, 2, 3} \u00d7 {\u22121, 1}] 8: w = MultiLogit({(\u03c6(x), y)|(x, y) \u2208 S})\nuse the eigenvectors {vi}. The eigenvectors define an embedding for each example x using the projection magnitudes {v>i x} as new coordinates. However the embedding is linear, therefore composition with a linear classifier is equivalent to learning a linear classifier in the original space, perhaps with a different regularization. This motivates the use of nonlinear functions of the projection magnitude.\nTo construct nonlinear maps, we can get inspiration from the optimization criterion in equation (1), i.e., the ratio of expected projection magnitudes conditional on different class labels. For example, we could use a nonlinear map such as (v>x)2. This type of nonlinearity can be sensitive (for example, it is not Lipschitz) so in practice more robust proxies can be used such as |v>x| or even |v>x|1/2.1 In principle, smoothing splines or any other flexible set of univariate basis functions could be used. In our experiments we simply fit a piecewise cubic polynomial on |v>x|1/2. The polynomial has only two pieces, one for v>x > 0 and one for v>x \u2264 0. We briefly experimented with interaction terms between projection magnitudes, but did not find them beneficial.\nAdditionally, we need to address from which class pairs to extract eigenvectors. A simple and empirically effective approach, suitable when the number of classes is modest, is to just use all ordered pairs of classes. This can be wasteful if two classes are never confused. The alternative, however, of leaving out a pair (i, j) is that the classifier might have no way of distinguishing between these two classes. Since we do not know upfront which pairs of classes will be confused, our brute force approach is just a safe way to endow the classifier with enough flexibility to deal with any pair of classes that could potentially be confused. Of course, as the number of classes grows, this brute force approach becomes less viable both computationally (due to the quadratic increase in generalized eigenvalue problems)\n1 These choices are simple and yield only slightly worse results than what we report in our experiments.\nand statistically (due to the increase in the number of features for the final classifier). We discuss issues regarding large numbers of classes in Section 5.\nFinally, the generalized eigenvalues can guide us in picking a subset of the d generalized eigenvectors we could extract from each class pair, i.e., generalized eigenvalues are useful for feature selection. A generalized eigenvector v with eigenvalue \u03bb has E[(v>x)2|y] equal to 1 for the denominator class y = j and equal to \u03bb for the numerator class y = i. Therefore, eigenvalues far from 1 correspond to highly discriminative features. Similar to (Platt et al., 2010), we extract the top few eigenvectors, as top eigenspaces are cheaper to compute than bottom eigenspaces. To guard against picking non-discriminative eigenvectors, we discard those whose eigenvalues are less than a threshold \u03b8 > 1.\nThe above observations lead to the GEM procedure outlined in Algorithm 1. Although Algorithm 1 has proven sufficiently versatile for the experiments described herein, it is merely an example of how to use generalized eigenvalue based features for multiclass classification. Other classification techniques could benefit from using the raw projection values without any nonlinear manipulation, e.g., decision trees; additionally the generalized eigenvectors could be used to initialize a neural network architecture as a form of pre-training.\nWe remark that each step in Algorithm 1 is highly amenable to distributed implementation: empirical class-conditional second moment matrices can be computed using map-reduce techniques, the generalized eigenvalue problems can be solved independently in parallel, and the logistic regression optimization is convex and therefore highly scalable (Agarwal et al., 2011)."}, {"heading": "3. Related Work", "text": "Our approach resembles many existing methods that work by finding eigenvectors of matrices constructed from data. One can think of all these approaches as procedures for finding directions v that maximize the signal to noise ratio\nR(v) = v>Sv\nv>Nv , (6)\nwhere the symmetric matrices S and N are such that the quadratic forms v>Sv and v>Nv represent the signal and the noise, respectively, captured along direction v. In Table 1 we present many well known approaches that could be cast in this framework. Principal Component Analysis (PCA) finds the directions of maximal variance without any particular noise model. The recently proposed Vanishing Component Analysis (VCA) (Livni et al., 2013) finds the directions on which the projections vanish so it can be thought as swapping the roles of signal and noise in PCA. Fisher LDA maximizes the variability in the class means while minimizing the within class variance. Sliced Inverse Regression first whitens x, and then uses the second moment matrix of the conditional whitened means as the signal and, like PCA, has no particular noise model. Finally, oriented PCA (Diamantaras & Kung, 1996; Platt et al., 2010) is a very general framework in which the noise matrix can be the correlation matrix of any type of noise z meaningful to the task at hand.\nBy closely examining the signal and noise matrices, it is clear that each method can be further distinguished according to two other capabilities: whether it is possible to extract many directions, and whether the directions are discriminative. For example, PCA and VCA can extract many directions but these are not discriminative. In contrast, Fisher LDA and SIR are discriminative but they work with rank-k matrices so the number of directions that could be extracted is limited by the number of classes. Furthermore both of these methods lose valuable fidelity about the data by using the conditional means.\nOriented PCA is sufficiently general to encompass our technique as a special case. Nonetheless, to the best of our knowledge, the specific signal and noise models in this paper are novel and, as we show in Section 4, they empirically work very well."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. MNIST", "text": "We begin with the MNIST database of handwritten digits (LeCun et al., 1998), for which we can visu-\nalize the generalized eigenvectors, providing intuition regarding the discriminative nature of the computed directions. For each of the ten classes, we estimated Cm = E[xx>|y = m] using (4) and then extracted generalized eigenvectors for each class pair (i, j) by solving C\u0302iv = \u03bb( \u03b3 d Trace(C\u0302j)I + C\u0302j)v. Figure 1 shows a sample of results from this procedure for five class pairs (one in each row) and \u03b3 = 0.5. In the top row we use class pair (3, 2) and we observe that the eigenvectors are sensitive to the circular stroke of a typical 3 while remaining insensitive to the areas where 2s and 3s overlap. Similar results are seen in the second and third rows where we use class pairs (8, 5) and (3, 5): the strokes we find are along areas used by the first class and mostly avoided by the second class. In the fourth row we use class pair (8, 0). Here we observe two patterns. First, a dot in the center that avoids the 0s. The other 4 detectors consist of positive (red) and negative (blue) strokes arranged in a way that would cancel each other if we take the inner product of the detector with a radially symmetric pattern such as a 0. Similarly in the bottom row with class pair (4, 9), the detector attempts to cancel the horizontal stroke corresponding to the top of the 9, where a typical 4 would be open.\nFigure 2 shows for each of the ten classes the distribution of values obtained by projecting the training examples in that class onto the first eigenvector for class pair (3, 2), i.e., the top left image in Figure 1. The projection pattern inspires two comments. First, while the magnitude of the projection is itself discriminative for distinguishing between 2s and 3s, there is additional information in knowing the sign of the pro-\njection. This motivates our particular choice of nonlinear expansion in Algorithm 1. Second, the detector is discriminative for class 3 vs. class 2 as per design, but also useful for distinguishing other classes from 2s. However certain classes such as 1s and 7s would be completely confused with 2s were this the only feature. The number of classes in MNIST is modest (k = 10) so we can easily afford to extract features for all k(k\u2212 1) class pairs for excellent discrimination. For problems with a large number of classes, however, we need to carefully pick the subproblems we need to solve so that the resulting set of features is discriminative, diverse, and complete. We revisit this topic in Section 5.\nTable 2 contains results for algorithm 1 on the MNIST test set. To determine the hyperparameter settings \u03b3 and \u03b8, we held out a fraction of the training set for validation. Once \u03b3 and \u03b8 were determined, we trained on the entire training set.\nFor \u201cdeep GEM\u201d we applied GEM to the representation created by GEM, i.e., line 7 of Algorithm 1. Because of the intermediate nonlinearity this is not equivalent to a single application of GEM, and we do observe an improvement in generalization. Subsequent recursive compositions of GEM degrade generalization, e.g., 3 levels of GEM yields 110 test errors. We would like to better understand the conditions under which composing GEM with itself is beneficial.\nOur results occupy an intermediate position amongst state of the art results on MNIST. For comparison we include results from other permutation-invariant methods from (Wan et al., 2013) and (Goodfellow et al., 2013). These methods rely on generic non-\nconvex optimization techniques and face challenging scaling issues in a distributed setting (Dean et al., 2012). While maximization of the Rayleigh quotient (1) is non-convex, mature implementations are computationally efficient and numerically robust. The final classifier is built using convex techniques and our pipeline is particularly well suited to the distributed setting, as discussed in Section 5."}, {"heading": "4.2. Covertype", "text": "Covertype is a multiclass data set whose task is to predict one of 7 forest cover types using 54 cartographic variables (Blackard & Dean, 1999). RBF kernels provide state of the art performance on Covertype, and consequently it has been a benchmark dataset for fast approximate kernel techniques (Rahimi & Recht, 2007; Jose et al., 2013). Here, we demonstrate that generalized eigenvector extraction composes well with randomized feature maps in the primal. This approximates generalized eigenfunction extraction in the RKHS, while retaining the speed and compactness of primal approaches.\nCovertype does not come with a designated test set, so we randomly permuted the data set and used the last 10% for testing, utilizing the same train-test split for all experiments. We followed the same experimental protocol as the previous section, i.e., held out a portion of the training set for validation to select hyperparameters.\nTable 3 summarizes the results.2 GEM and deep GEM are exactly the same as in the previous section, i.e., Algorithm 1 without and with self-composition respectively. RFF stands for Random Fourier Features (Rahimi & Recht, 2007), in which the Gaussian kernel is approximated in the primal by a randomized cosine map; we used logistic regression for the primal learning algorithm. We treated the bandwidth and number of cosines as hyperparameters to be optimized.\nThe relatively poor classification performance of RFF\n2When comparing with other published results, be aware that many authors adjust the task to be a binary classification task.\non Covertype has been noted before (Rahimi & Recht, 2007), a result we reproduce here. Instead of using the randomized feature map directly, however, we can apply Algorithm 1 to the representation induced by RFF, which we denote GEM + RFF. This improves the classification error with only modest increase in computation cost, e.g., in MATLAB it takes 8 seconds to compute the randomized Fourier features, 58 seconds to (sequentially) solve the generalized eigenvalue problems and compute the GEM feature representation, and 372 seconds to optimize the logistic regression. The final error rate of 8.4% is a new record for this task."}, {"heading": "4.3. TIMIT", "text": "TIMIT is a corpus of phonemically and lexically annotated speech of English speakers of multiple genders and dialects (Fisher et al., 1986). Although the ultimate problem is sequence annotation, there is a derived multiclass classification problem of predicting the phonemic annotation associated with a short segment of audio. Such a classifier can be composed with standard sequence modeling techniques to produce an overall solution, which has made the multiclass problem a subject of research (Hinton et al., 2012b; Hutchinson et al., 2012). In this experiment we focus exclusively on the multiclass problem.\nWe use a standard preprocessing of TIMIT as our initial representation (Hutchinson et al., 2012). Specifically the speech is converted into feature vectors via the first to twelfth Mel frequency cepstral coefficients and energy plus first and second temporal derivatives. This results in 39 coefficients per frame, which is concatenated with 5 preceding and 5 following frames to produce a 429 coefficient input to the classifier. The targets for the classifier are the 183 phone states (i.e., 61 phones each in 3 possible states).\nWe use the standard training, development, and test sets of TIMIT. As in previous experiments herein, hyperparameters are optimized on the development set (using cross-entropy as the objective), but unlike previous experiments we do not retrain with the development set once hyperparameters are determined, in\ncorrespondence with the experimental protocol used with the T-DSN (Hutchinson et al., 2012).\nWith 183 classes the all-pairs approach for generalized eigenvector extraction is unwieldy, so we used a randomized procedure to select from which class pairs to extract features, by randomly positioning the class labels on a hypercube and extracting generalized eigenvectors only for immediate hyperneighbors. For k classes this results in O(k log k) generalized eigenvalue problems. Although we did not attempt a thorough exploration of different strategies for subproblem selection, the hypercube heuristic yielded better results for a given feature budget than either uniform random selection over all class pairs or stratified random selection over class pairs ensuring equal numbers of denominator or numerator classes. The resulting performance for five different choices of random hypercube is shown in the row of Table 4 denoted GEM. We show both multiclass error rate as well as cross entropy, the objective we are actually optimizing.\nThe random subproblem selection creates an opportunity to ensemble, and empirically the resulting classifiers are sufficiently diverse that ensembling yields a substantial improvement. In Table 4, denoted GEM ensemble, we show the performance of the ensemble prediction of the 5 classifiers using the geometric mean prediction (this is the prediction that minimizes its average KL-divergence to each element of the ensemble). The result matches the classification error and improves upon the cross-entropy loss of the best published T-DSN. This is remarkable considering the T-DSN is a deep architecture employing between 8 and 13 stacked layers of nonlinear transformations, whereas the GEM procedure produces a shallow architecture with a single nonlinear layer."}, {"heading": "5. Discussion", "text": "Given the simplicity and empirical success of our method, we were surprised to find considerable work on methods that only extract the first generalized eigenvector (Mika et al., 2003) but very little work on using the top m generalized eigenvectors. Our experience is that additional eigenvectors provide complementary information. Empirically, their inclusion in\nthe final classifier far outweighs the necessary increase in sample complexity, especially given typical modern data set sizes. Thus we believe this technique should be valuable in other domains.\nOf course our method will not be able to extract anything useful if all classes have the same second moment but different higher order statistics. While our limited experience here suggests second moments are informative for natural datasets, there are potential benefits in using higher order moments. For example, we could replace our class-conditional second moment matrix with a second moment matrix conditioned on other events, informed by higher order moments.\nAs the number of class labels increases, say k \u2265 1000, our brute force all-pairs approach, which scales as O(k2), becomes increasingly difficult both computationally and statistically: we need to solveO(k2) eigenvector problems (possibly in parallel) and deal with O(k2) features in the ultimate classifier. Taking a step back, the object of our attention is the tensor E[x\u2297x\u2297y] and in this paper we only studied one way of selecting pairs of slices from it. In particular, our slices are tensor contractions with one of the standard basis vectors in Rk. Clearly, contracting the tensor with any vector u in Rk is possible. This contraction leads to a d \u00d7 d second moment matrix which averages the examples of the different classes in the way prescribed by u. Any sensible, data-dependent way of picking a good set of vectors u should be able to reduce the dependence on k2.\nThe same issues also arise with a continuous y: how to define and estimate the pairs of matrices whose generalized eigenvectors should be extracted is not immediately clear. Still, the case where y is multidimensional (vector regression) can be reduced to the case of univariate y using the same technique of contraction with a vector u. Feature extraction from a continuous y can be done by discretization (solely for the purpose of feature extraction), which is much easier in the univariate case than in the multivariate case.\nIn domains where examples exhibit large variation, or when labeled data is scarce, incorporating prior knowledge is extremely important. For example, in image recognition, convolutions and local pooling are popular ways to generate representations that are invariant to localized distortions. Directly exploiting the spatial or temporal structure of the input signal, as well as incorporating other kinds of invariances in our framework, is a direction for future work.\nHigh dimensional problems create both computational and statistical challenges. Computationally, when\nd > 106, the solution of generalized eigenvalue problems can only be performed via specialized libraries such as ScaLAPACK, or via randomized techniques, such as those outlined in (Halko et al., 2011; Saibaba & Kitanidis, 2013). Statistically, the finite-sample second moment estimates can be inaccurate when the number of dimensions overwhelms the number of examples. The effect of this inaccuracy on the extracted eigenvectors needs further investigation. In particular, it might be unimportant for datasets encountered in practice, e.g., if the true class-conditional second moment matrices have low effective rank (Bunea & Xiao, 2012).\nFinally, our approach is simple to implement and well suited to the distributed setting. Although a distributed implementation is out of the scope of this paper, we do note that aspects of Algorithm 1 were motivated by the desire for efficient distributed implementation. The recent success of non-convex learning systems has sparked renewed interest in non-convex representation learning. However, generic distributed nonconvex optimization is extremely challenging. Our approach first decomposes the problem into tractable non-convex subproblems and then subsequently composes with convex techniques. Ultimately we hope that judicious application of convenient non-convex objectives, coupled with convex optimization techniques, will yield competitive and scalable learning algorithms."}, {"heading": "6. Conclusion", "text": "We have shown a method for creating discriminative features via solving generalized eigenvalue problems, and demonstrated empirical efficacy via multiple experiments. The method has multiple computational and statistical desiderata. Computationally, generalized eigenvalue extraction is a mature numerical primitive, and the matrices which are decomposed can be estimated using map-reduce techniques. Statistically, the method is invariant to invertible linear transformations, estimation of the eigenvectors is robust when the number of examples exceeds the number of variables, and estimation of the resulting classifier parameters is eased due to the parsimony of the derived representation.\nDue to this combination of empirical, computational, and statistical properties, we believe the method introduced herein has utility for a wide variety of machine learning problems."}, {"heading": "Acknowledgments", "text": "We thank John Platt and Li Deng for helpful discussions and assistance with the TIMIT experiments."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Agarwal", "Alekh", "Chapelle", "Olivier", "Dud\u0301\u0131k", "Miroslav", "Langford", "John"], "venue": "CoRR, abs/1110.4198,", "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables", "author": ["Blackard", "Jock A", "Dean", "Denis J"], "venue": "Computers and Electronics in Agriculture,", "citeRegEx": "Blackard et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Blackard et al\\.", "year": 1999}, {"title": "On the sample covariance matrix estimator of reduced effective rank population matrices, with applications to fPCA", "author": ["F. Bunea", "L. Xiao"], "venue": null, "citeRegEx": "Bunea and Xiao,? \\Q2012\\E", "shortCiteRegEx": "Bunea and Xiao", "year": 2012}, {"title": "Templates for the solution of algebraic eigenvalue problems: a practical guide", "author": ["Demmel", "James", "Dongarra", "Jack", "Ruhe", "Axel", "van der Vorst", "Henk", "Bai", "Zhaojun"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Demmel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Demmel et al\\.", "year": 2000}, {"title": "Principal component neural networks", "author": ["Diamantaras", "Konstantinos I", "Kung", "Sun Y"], "venue": null, "citeRegEx": "Diamantaras et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Diamantaras et al\\.", "year": 1996}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["Fisher", "Ronald A"], "venue": "Annals of eugenics,", "citeRegEx": "Fisher and A.,? \\Q1936\\E", "shortCiteRegEx": "Fisher and A.", "year": 1936}, {"title": "The DARPA speech recognition research database: Specification and status", "author": ["W. Fisher", "G. Doddington", "Marshall", "Goudie K"], "venue": "In Proceedings of the DARPA Speech Recognition Workshop,", "citeRegEx": "Fisher et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1986}, {"title": "Regularized discriminant analysis", "author": ["Friedman", "Jerome H"], "venue": "Journal of the American statistical association,", "citeRegEx": "Friedman and H.,? \\Q1989\\E", "shortCiteRegEx": "Friedman and H.", "year": 1989}, {"title": "The unreasonable effectiveness of data", "author": ["Halevy", "Alon", "Norvig", "Peter", "Pereira", "Fernando"], "venue": "Intelligent Systems, IEEE,", "citeRegEx": "Halevy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halevy et al\\.", "year": 2009}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Tropp", "Joel A"], "venue": "SIAM review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition", "author": ["Hutchinson", "Brian", "Deng", "Li", "Yu", "Dong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hutchinson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2012}, {"title": "Local deep kernel learning for efficient non-linear svm prediction", "author": ["Jose", "Cijo", "Goyal", "Prasoon", "Aggrwal", "Parv", "Varma", "Manik"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Jose et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jose et al\\.", "year": 2013}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sliced inverse regression for dimension reduction", "author": ["Li", "Ker-Chau"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li and Ker.Chau.,? \\Q1991\\E", "shortCiteRegEx": "Li and Ker.Chau.", "year": 1991}, {"title": "Supervised dictionary learning", "author": ["Mairal", "Julien", "Bach", "Francis", "Ponce", "Jean", "Sapiro", "Guillermo", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:0809.3083,", "citeRegEx": "Mairal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2008}, {"title": "Translingual document representations from discriminative projections", "author": ["Platt", "John C", "Toutanova", "Kristina", "Yih", "Wentau"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Platt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Platt et al\\.", "year": 2010}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Randomized square-root free algorithms for generalized hermitian eigenvalue problems", "author": ["Saibaba", "Arvind K", "Kitanidis", "Peter K"], "venue": "arXiv preprint arXiv:1307.6885,", "citeRegEx": "Saibaba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saibaba et al\\.", "year": 2013}, {"title": "Introduction to the nonasymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin and Roman.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2010}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Simca: a method for analyzing chemical data in terms of similarity and analogy", "author": ["Wold", "Svante", "Sjostrom", "Michael"], "venue": "Chemometrics: theory and application,", "citeRegEx": "Wold et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Wold et al\\.", "year": 1977}], "referenceMentions": [{"referenceID": 13, "context": "In practice, supervised learning approaches are profitably employed in many domains, from movie recommendation to speech and image recognition (Koren et al., 2009; Hinton et al., 2012a; Krizhevsky et al., 2012).", "startOffset": 143, "endOffset": 210}, {"referenceID": 14, "context": "In practice, supervised learning approaches are profitably employed in many domains, from movie recommendation to speech and image recognition (Koren et al., 2009; Hinton et al., 2012a; Krizhevsky et al., 2012).", "startOffset": 143, "endOffset": 210}, {"referenceID": 8, "context": "In text classification, for example, unigram and bigram features together with linear classifiers are known to work well for a variety of related tasks (Halevy et al., 2009).", "startOffset": 152, "endOffset": 173}, {"referenceID": 17, "context": "This has fueled interest in methods that can learn the appropriate representations directly from the raw signal, with techniques such as dictionary learning (Mairal et al., 2008) and deep learning (Krizhevsky et al.", "startOffset": 157, "endOffset": 178}, {"referenceID": 14, "context": ", 2008) and deep learning (Krizhevsky et al., 2012; Hinton et al., 2012a) achieving state of the art performance in many important problems.", "startOffset": 26, "endOffset": 73}, {"referenceID": 3, "context": "Similar results apply to the sine of the angle between an estimated generalized eigenvector and the true one (Demmel et al., 2000) Section 5.", "startOffset": 109, "endOffset": 130}, {"referenceID": 18, "context": "A common solution (Platt et al., 2010) is to regularize the denominator matrix by adding a multiple of the identity to the denominator, i.", "startOffset": 18, "endOffset": 38}, {"referenceID": 18, "context": "Similar to (Platt et al., 2010), we extract the top few eigenvectors, as top eigenspaces are cheaper to compute than bottom eigenspaces.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "We remark that each step in Algorithm 1 is highly amenable to distributed implementation: empirical class-conditional second moment matrices can be computed using map-reduce techniques, the generalized eigenvalue problems can be solved independently in parallel, and the logistic regression optimization is convex and therefore highly scalable (Agarwal et al., 2011).", "startOffset": 344, "endOffset": 366}, {"referenceID": 18, "context": "Finally, oriented PCA (Diamantaras & Kung, 1996; Platt et al., 2010) is a very general framework in which the noise matrix can be the correlation matrix of any type of noise z meaningful to the task at hand.", "startOffset": 22, "endOffset": 68}, {"referenceID": 15, "context": "We begin with the MNIST database of handwritten digits (LeCun et al., 1998), for which we can visu-", "startOffset": 55, "endOffset": 75}, {"referenceID": 22, "context": "For comparison we include results from other permutation-invariant methods from (Wan et al., 2013) and (Goodfellow et al.", "startOffset": 80, "endOffset": 98}, {"referenceID": 12, "context": "RBF kernels provide state of the art performance on Covertype, and consequently it has been a benchmark dataset for fast approximate kernel techniques (Rahimi & Recht, 2007; Jose et al., 2013).", "startOffset": 151, "endOffset": 192}, {"referenceID": 12, "context": "result is from (Jose et al., 2013) where they also use a", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "TIMIT is a corpus of phonemically and lexically annotated speech of English speakers of multiple genders and dialects (Fisher et al., 1986).", "startOffset": 118, "endOffset": 139}, {"referenceID": 11, "context": "Such a classifier can be composed with standard sequence modeling techniques to produce an overall solution, which has made the multiclass problem a subject of research (Hinton et al., 2012b; Hutchinson et al., 2012).", "startOffset": 169, "endOffset": 216}, {"referenceID": 11, "context": "We use a standard preprocessing of TIMIT as our initial representation (Hutchinson et al., 2012).", "startOffset": 71, "endOffset": 96}, {"referenceID": 11, "context": "result from (Hutchinson et al., 2012).", "startOffset": 12, "endOffset": 37}, {"referenceID": 11, "context": "correspondence with the experimental protocol used with the T-DSN (Hutchinson et al., 2012).", "startOffset": 66, "endOffset": 91}, {"referenceID": 9, "context": "d > 10, the solution of generalized eigenvalue problems can only be performed via specialized libraries such as ScaLAPACK, or via randomized techniques, such as those outlined in (Halko et al., 2011; Saibaba & Kitanidis, 2013).", "startOffset": 179, "endOffset": 226}], "year": 2013, "abstractText": "Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results.", "creator": "LaTeX with hyperref package"}}}