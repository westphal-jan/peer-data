{"id": "1311.6809", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2013", "title": "A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic Cost", "abstract": "we introduce a novel family of adaptive precision algorithms based on a relative probability cost. the new family intrinsically connects arbitrary higher and lower order measures of the error into a single continuous update based on the iteration amount. we introduce important members regarding converge family as algorithms such as the least mean logarithmic square ( lmls ) and least scaled absolute difference ( llad ) algorithms that improve the convergence performance of almost conventional algorithms. however, our approach and analysis are generic such although they cover other well - known cost functions as indicated in the paper. the lmls process achieves comparable convergence performance with less least weighted fourth ( lmf ) algorithm and extends the stability bound on the step size. the llad and least mean square ( lms ) algorithms demonstrate similar convergence performance in impulse - free noise environments while the llad algorithm is robust detecting impulsive interferences and outperforms the sign algorithm ( sa ). measurements monitor the transient, steady state and tracking performance of the introduced algorithms there demonstrate the match of the theoretical analyzes performance simulation results. we show the extended stability bound of the lmls algorithm and analyze the best of the llad algorithm against impulsive interferences. finally,... demonstrate the performance of new algorithms in different scenarios demonstrating method learning.", "histories": [["v1", "Tue, 26 Nov 2013 10:02:20 GMT  (271kb)", "http://arxiv.org/abs/1311.6809v1", "Submitted to IEEE Transactions on Signal Processing"]], "COMMENTS": "Submitted to IEEE Transactions on Signal Processing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["muhammed o sayin", "n denizcan vanli", "suleyman s kozat"], "accepted": false, "id": "1311.6809"}, "pdf": {"name": "1311.6809.pdf", "metadata": {"source": "CRF", "title": "A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic Cost", "authors": ["Muhammed O. Sayin", "N. Denizcan Vanli", "Suleyman S. Kozat"], "emails": ["sayin@ee.bilkent.edu.tr,", "vanli@ee.bilkent.edu.tr,", "kozat@ee.bilkent.edu.tr)."], "sections": [{"heading": null, "text": "ar X\niv :1\n31 1.\n68 09\nv1 [\ncs .L\nG ]\n2 6\nN ov\n2 01\n3 1\nIndex Terms\u2014Logarithmic cost function, robustness against impulsive noise, stable adaptive method.\nEDICS Category: MLR-LEAR, ASP-ANAL, MLR-APPL\nI. INTRODUCTION\nADAPTIVE filtering applications such as channel equal-ization, noise removal or echo cancellation utilize a certain statistical measure of the error signal1 et denoting the difference between the desired signal dt and the estimation output d\u0302t. Usually, the mean square error E[e2t ] is used as the cost function due to its mathematical tractability and relative ease of analysis. The least mean square (LMS) and normalized least mean square (NLMS) algorithms are the members of this class [1]. In the literature, different powers of the error are commonly used as the cost function in order to provide stronger convergence or steady-state performance than the least-squares algorithms under certain settings [1].\nThe least mean fourth (LMF) algorithm and its family use the even powers of the error as the cost function, i.e., E[e2nt ] [2]. This family achieves a better trade-off between the\nThis work is in part supported by the Outstanding Researcher Programme of Turkish Academy of Sciences and TUBITAK project 112E161.\nThe authors are with the Department of Electrical and Electronics Engineering, Bilkent University, Bilkent, Ankara 06800 Turkey, Tel: +90 (312) 290-2336, Fax: +90 (312) 290-1223 (e-mail: sayin@ee.bilkent.edu.tr, vanli@ee.bilkent.edu.tr, kozat@ee.bilkent.edu.tr).\n1Time index appears as a subscript.\ntransient and steady-state performance, however, has stability issues [3]\u2013[5]. The stability of the LMF algorithm depends on the input and noise power, and the initial value of the adaptive filter weights [6]. On the other hand, the stability of the conventional LMS algorithm depends only on the input power for a given step-size [1]. The normalized filters improve the performance of the algorithms under certain settings by removing dependency to the input statistics in the updates [7]. However, note that the normalized least mean fourth (NLMF) algorithm does not solve the stability issues [6]. In [6], authors propose the stable NLMF algorithm, which might also be derived through the proposed relative logarithmic error cost framework as shown in this paper.\nThe performance of the least-squares algorithms degrades severely when the input and desired signal pairs are perturbed by heavy tailed impulsive interferences, e.g., in applications involving high power noise signals [8]. In this context, we define robustness as the insensitivity of the algorithms against the impulsive interferences encountered in the practical applications and provide a theoretical framework [9]. Note that, usually, the algorithms using lower-order measures of the error in their cost function are relatively less sensitive to such perturbations. For example, the well-known sign algorithm (SA) uses the L1 norm of the error and is robust against impulsive interferences since its update involves only the sign of et. However, the SA usually exhibits slower convergence performance especially for highly correlated input signals [10].\nThe mixed-norm algorithms minimize a combination of different error norms in order to achieve improved convergence performance [11], [12]. For example, [12] combines the robust L1 norm and the more sensitive but better converging L2 norm through a mixing parameter. Even though the combination parameter brings in an extra degree of freedom, the design of the mixed norm filters requires the optimization of the mixing parameter based on a priori knowledge of the input and noise statistics. On the other hand, the mixture of experts algorithms adaptively combine different algorithms and provide improved performance irrespective of the environment statistics [13]\u2013 [16]. However, note that such mixture approaches require to operate several different algorithms on parallel, which may be infeasible in different applications [17]. In [18], authors propose an adaptive combination of L1 and L2 norms of the error in parallel, however, the resulting algorithm demonstrates impulsive perturbations on the learning curves. This results since the impulsive interferences severely degrade the algorithmic updates. In general, the samples contaminated with impulses contain little useful information [9]. Hence, the robust algorithms need to be less sensitive only against\nlarge perturbations on the error and can be as sensitive as the conventional least squares algorithms for small error values. The switched-norm algorithms switch between the L1 and L2 norms based on the error amount such as the robust Huber filter [19]. This approach combines the better convergence of L2 and the robustness of L1 together in a discrete manner with a breaking point in the cost function, however, requires optimization of certain parameters as detailed in this paper.\nIn this paper, we use diminishing return functions, e.g., the logarithm function, as a normalization (or a regularization) term, i.e., as a subtracting term, in the cost function in order to improve the convergence performances. We particularly choose the logarithm function as the normalizing diminishing return function [20] in our cost definitions since the logarithmic function is differentiable and results efficient and mathematically tractable adaptive algorithms. As shown in the paper, by using the logarithm function, we are able to use of the higher-order statistics of the error for small perturbations. On the other hand, for larger error values, the introduced algorithms seek to minimize the conventional cost functions due to the decreasing weight of the logarithmic term with the increasing error amount. In this sense, the new framework is akin to a continuous generalization of the switched norm algorithms, hence greatly improve the convergence performance of the mixed-norm methods as shown in this paper.\nOur main contributions include: 1) We propose the least mean logarithmic square (LMLS) algorithm, which achieves a similar trade-off between the transient and steady-state performance of the LMF algorithm and as stable as the LMS algorithm; 2) We propose the least logarithmic absolute difference (LLAD) algorithm, which significantly improves the convergence performance of the SA while exhibiting comparable performance with the SA in the impulsive noise environments; 3) We analyze the transient, steady-state and tracking performance of the introduced algorithms; 4) We demonstrate the extended stability bound on the step-sizes with the logarithmic error cost framework; 5) We introduce an impulsive noise framework and analyze the robustness of the LLAD algorithm in the impulsive noise environments; 6) We demonstrate the significantly improved convergence performances of the introduced algorithms in several different scenarios in our simulations.\nWe organize the paper as follows. In Section II, we introduce the relative logarithmic error cost framework. In Section III, the important members of the novel family are derived. We analyze the transient, steady-state and tracking performances of those members in Section IV. In Section V, we compare the stability bound on the step-sizes and the robustness of the proposed algorithms. In Section VI, we provide the numerical examples demonstrating the improved performance of the conventional algorithms in the new logarithmic error cost framework. We conclude the paper in Section VII with several remarks.\nNotation: Bold lower (or upper) case letters denote the vectors (or matrices). For a vector a (or matrix A), aT (or AT ) is its ordinary transpose. \u2016 \u00b7 \u2016 and \u2016 \u00b7 \u2016A denote the L2 norm and the weighted L2 norm with the matrix A, respectively\no w\nt w\nt x\nt d\nt n\nt d\u0302\nt e\n!\n(provided that A is positive-definite). | \u00b7 | is the absolute value operator. We work with real data for notational simplicity. For a random variable x (or vector x), E[x] (or E[x]) represents its expectation. Here, Tr(A) denotes the trace of the matrix A and \u2207xf(x) is the gradient operator."}, {"heading": "II. COST FUNCTION WITH LOGARITHMIC ERROR", "text": "We consider the system identification framework shown in Fig. 1, where we denote the input signal by xt and the desired signal by dt. Here, we observe an unknown vector2 wo \u2208 Rp through a linear model\ndt = w T o xt + nt,\nwhere nt represents the noise and we define the error signal as et \u25b3 = dt\u2212d\u0302t = dt\u2212wTt xt. In this framework, adaptive filtering algorithms estimate the unknown system vector wo through the minimization of a certain cost function. The gradient descent methods usually employ convex and uni-modal cost functions in order to converge to the global minimum of the error surfaces, e.g., the mean square error E[e2t ] [1]. The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.\nIn this framework, we use a normalized error cost function using the logarithm function given by\nJ (et) \u25b3 = F (et)\u2212\n1 \u03b1 ln (1 + \u03b1F (et)) , (1)\nwhere \u03b1 > 0 is a design parameter and F (et) is a conventional cost function of the error signal et, e.g., F (et) = E [|et|]. As an illustration, in Fig. 2, we compare |et| and |et| \u2212 ln(1 + |et|). From this plot, we observe that the logarithm based cost function is less steep for small perturbations on the error while both logarithmic square and absolute difference cost functions exhibit comparable steepness for large error values. Indeed, this new family intrinsically combines the benefits of using lower and higherorder measures of the error into a single adaptation algorithm. Our algorithms provide comparable convergence rate with a conventional algorithm minimizing the cost function F (et) and achieve smaller steady-state mean square errors through the use of higher-order statistics for small perturbations of the error.\n2Although we assume a time invariant unknown system vector here, we also provide the tracking performance analysis for certain non-stationary models later in the paper.\n3 \u22125 0 5 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\nError Signal e t\nS to\nch as\ntic C\nos t F\nun ct\nio ns\nStochastic Error Cost Functions vs. Error Signal\n|e t | \u03c1(e t ) ref.(6) |e t | \u2212 ln(1+|e t |)\nFig. 2: Here, we plot stochastic cost functions to illustrate decreased steepness of the least squares algorithms in the logarithmic error cost framework for small error amounts.\nRemark 2.1: In [21], the authors propose a stochastic cost function using the logarithm function as follows\nJ [21](et) \u25b3 =\n1\n2\u03b3 ln\n(\n1 + \u03b3\n(\net\n\u2016xt\u2016\n)2 )\n.\nNote that the cost function J [21](et) is the subtracted term in (1) for F (et) = e2t\n\u2016xt\u20162 . The Hessian matrix of J [21](et) is given by\nH ( J [21](et) ) = xtx\nT t\n\u2016xt\u20162 ( 1 + \u03b3 (\net \u2016xt\u2016\n)2 )\n\u00d7\n\n  \n1\u2212 2\u03b3e 2 t\n\u2016xt\u20162 ( 1 + \u03b3 (\net \u2016xt\u2016\n)2 )\n\n   .\nWe emphasize that H ( J [21](et) ) is positive semi-definite provided that \u03b3 (\net \u2016xt\u2016\n)2\n\u2264 1, thus, the parameter \u03b3 should be chosen carefully to be able to efficiently use the gradient descent algorithms. On the other hand, we show that the new cost function in (1) is a convex function enabling the use of the diminishing return property [20] of the logarithm function for stable and robust updates.\nThe relative logarithmic error cost we introduce in (1) can also be expressed as\nJ(et) = 1\n\u03b1 ln\n(\nexp (\u03b1F (et))\n1 + \u03b1F (et)\n)\n. (2)\nSince exp(\u03b1F (et)) = \u2211\u221e m=0 1 m!\u03b1 mFm(et), we obtain\nJ(et) = 1\n\u03b1 ln\n(\n1 + \u03b12 2! F 2(et) + \u03b13 3! F 3(et) + \u00b7 \u00b7 \u00b7\n1 + \u03b1F (et)\n)\n. (3)\nSince F (et) is a non-negative function, J(et) is also a non-negative function by (3).\nRemark 2.2: The Hessian matrix of J(et) is given by\nH (J(et))=H (F (et)) \u03b1F (et) 1 + \u03b1F (et) + \u03b1\u2207wF (et)\u2207wF (et)T (1 + \u03b1F (et)) 2 ,\nwhich is positive semi-definite provided that H (F (et)) is a positive semi-definite matrix.\nWe obtain the first gradient of (1) as follows\n\u2207wJ(et) = \u2207wF (et) \u03b1F (et)\n1 + \u03b1F (et) ,\nwhich yields zero if \u2207wF (et) or F (et) is zero. Note that the optimal solution for the cost function F (et) minimizes F (et) and is obtained by\n\u2207w=woF (et) = 0. Since F (et) is a non-negative convex function, the global minimum and the value yielding zero gradient coincide if the latter exits. Hence, the optimal solution for the relative logarithmic error cost function is the same with the cost function F (et) since as shown in Remark 2.2 the Hessian matrix of the logarithmic cost function is positive semi-definite. For example, the mean-square error cost function F (et) = E[e2t ] yields to the Wiener solution wo = E[xtx T t ] \u22121E[xtdt].\nRemark 2.3: By Maclaurin series of the natural logarithm for \u03b1F (et) \u2264 1, (1) yields\nJ(et) = F (et)\u2212 1\n\u03b1\n(\n\u03b1F (et)\u2212 \u03b12\n2 F 2(et) + \u00b7 \u00b7 \u00b7\n)\n= \u03b1\n2 F 2(et)\u2212\n\u03b12\n3 F 3(et) + \u00b7 \u00b7 \u00b7 , (4)\nwhich is an infinite combination of the conventional cost function for small values of F (et). We emphasize that the cost function (4) yields to the second power of the cost function F (et) for small values of the error while for large error values, the cost function J(et) resembles F (et) as follows:\nF (et)\u2212 1\n\u03b1 ln (1 + \u03b1F (et)) \u2192 F (et) as et \u2192 \u221e.\nHence, the new methods are the combinations of the algorithms with mainly F 2(et) or F (et) cost functions based on the error amount. It is important to note that the objective functions F 2(et), e.g., E[e2t ] 2, and F (e2t ), e.g., E[e 4 t ], yields the same stochastic gradient update after removing the expectation in this paper. The switched norm algorithms also combine two different norms into a single update in a discrete manner based on the error amount. As an example, the Huber objective function combining L1 and L2 norms of the error is defined as [19]\n\u03c1(et) \u25b3 =\n{\n1 2e 2 t for |et| \u2264 \u03b3, \u03b3|et| \u2212 12\u03b32 for |et| > \u03b3, (5)\nwhere \u03b3 > 0 denotes the cut-off value. In Fig. 2, we also compare the Huber objective function (for \u03b3 = 1) and the introduced cost (1) with F (et) = E[|et|] (for \u03b1 = 1). Note that (5) uses a piecewise-function combining two different algorithms based on the comparison of the error with the cut-off value \u03b3. On the other hand, logarithm based cost\n4 function J(et) intrinsically combines the functions with different order of powers in a continuous manner into a single update and avoids possible anomalies that might arise due to the breaking point in the cost function.\nRemark 2.4: Instead of a logarithmic normalization term, it is also possible to use various functions having diminishing returns property in order to provide stability and robustness to the conventional algorithms. For example, one can choose the cost function as\nJarctan(et) \u25b3 = F (et)\u2212\n1 \u03b1 arctan (\u03b1F (et)) (6)\nand the Taylor series expansion of the second term in (6) around F (et) = 0 is given by\n1 \u03b1 arctan (1 + \u03b1F (et)) = F (et)\u2212\n\u03b12\n3 F 3(et) + \u00b7 \u00b7 \u00b7 .\nThus, the resulting algorithm combines the algorithms using mainly F 3(et) (for small perturbations on the error) and F (et). We note that the algorithms using (6) are also as stable as F (et), however, they behave like minimizing the higher-order measures, i.e., F 3(et), for small error values.\nIn the next section, we propose important members of this novel adaptive filter family."}, {"heading": "III. NOVEL ALGORITHMS", "text": "Based on the gradient of J(et) we obtain the general steepest descent update as\nwt+1 = wt \u2212 \u00b5\u2207wF (et) \u03b1F (et)\n1 + \u03b1F (et) ,\nwhere \u00b5 > 0 is the step size and \u03b1 > 0 is the design parameter.\nRemark 3.1: In the previous section, we motivate the logarithm based error cost framework as a continuous generalization of the switched norm algorithms. The switched norm update involves a cut-off \u03b3 in the comparison of the error amount. Similarly, we utilize a design parameter \u03b1 in (1) in order to determine the asymptotic cut-off value. For example, a larger \u03b1 decreases the weight of the logarithmic term in the cost (1) and the resulting algorithm behaves more like minimizing the cost F (et). In the performance analyzes, we show that a sufficiently small design parameter, i.e., \u03b1 = 1, does not have determinative influence on the steady-state convergence performance under the Gaussian noise signal assumption. Hence, in the following algorithms we choose \u03b1 = 1. On the other hand, we resort to the usage of \u03b1 in order to facilitate the performance analyzes of the algorithms. Additionally, in the impulsive noise environments, we show that the optimization of \u03b1 improves the steady-state convergence performance of the introduced algorithms.\nIf we assume that after removing the expectation to generate stochastic gradient updates F (et) yields f(et), e.g., F (et) =\nE[f(et)], then the general stochastic gradient update is given by\nwt+1 = wt \u2212 \u00b5\u2207wet\u2207etf(et) f(et)\n1 + f(et) ,\n= wt + \u00b5xt\u2207etf(et) f(et)\n1 + f(et) . (7)\nIn the following subsections, we introduce algorithms improving the performance of the conventional algorithms such as the LMS (i.e. f(et) = e2t ), sign algorithm (i.e. f(et) = |et|) and normalized updates."}, {"heading": "A. The Least Mean Logarithmic Square (LMLS) Algorithm", "text": "For F (et) = E[e2t ], the stochastic gradient update yields\nwt+1 = wt + \u00b5xtet e2t\n1 + e2t\n= wt + \u00b5 xte\n3 t\n1 + e2t . (8)\nNote that we include the multiplier \u20182\u2019 coming from the gradient \u2207ete2t = 2et into the step-size \u00b5. The algorithm (8) resembles a least-mean fourth update for the small error values while it behaves like the least-mean square algorithm for large perturbations on the error. This provides smaller steady-state mean square error thanks to the fourth-order statistics of the error for small perturbations and stability of the least-squares algorithms for large perturbations. Hence, the LMLS algorithm intrinsically combines the least mean-square and least-mean fourth algorithms based on the error amount instead of mixed LMF + LMS algorithms [11] that need artificial combination parameter in the cost definition."}, {"heading": "B. The Least Logarithmic Absolute Difference (LLAD) Algorithm", "text": "The SA utilizes F (et) = E[|et|] as the cost function, which provides robustness against impulsive interferences [1]. However, the SA has slower convergence rate since the L1 norm is the smallest possible error power for a convex cost function. In the logarithmic cost framework, for F (et) = E[|et|], (7) yields\nwt+1 = wt + \u00b5xtsign(et) |et|\n1 + |et| = wt + \u00b5 xtet\n1 + |et| . (9)\nThe algorithm (9) combines the LMS algorithm and SA into a single robust algorithm with improved convergence performance. We note that in Section V we calculate the optimum \u03b1opt in order to achieve better convergence performance than the SA in the impulsive noise environments."}, {"heading": "C. Normalized Updates", "text": "We introduce normalized updates with respect to the regressor signal in order to provide independence from the input data correlation statistics under certain settings. We define the new objective function as\nJnew(et) \u25b3 = F\n(\net\n\u2016xt\u2016\n)\n\u2212 1 \u03b1 ln\n(\n1 + \u03b1F\n(\net\n\u2016xt\u2016\n))\n,\n5 for example F (\net \u2016xt\u2016\n) = E [ e2t \u2016xt\u20162 ] . The Hessian matrix of\nthe new cost function Jnew(et) is also positive semi-definite provided that the Hessian matrix of F (\net \u2016xt\u2016\n)\nis positive semidefinite as shown in Remark 2.2.\nThe steepest-descent update is given by\nwt+1 = wt \u2212 \u00b5\u2207wF ( et\n\u2016xt\u2016\n) \u03b1F (\net \u2016xt\u2016\n)\n1 + \u03b1F (\net \u2016xt\u2016\n) .\nFor F ( et\u2016xt\u2016 ) = E [( et \u2016xt\u2016\n) 2 ]\n, we get the normalized least mean logarithmic square (NLMLS) algorithm given by\nwt+1 = wt + \u00b5 xte\n3 t\n\u2016xt\u20162 (\u2016xt\u20162 + e2t ) . (10)\nWe point out that (10) is also proposed as the stable normalized least mean-fourth algorithm in [6].\nFor F ( et\u2016xt\u2016 ) = E [ |et| \u2016xt\u2016 ]\n, we obtain the normalized least logarithmic absolute difference (NLLAD) algorithm as\nwt+1 = wt + \u00b5xtet\n\u2016xt\u2016 (\u2016xt\u2016+ |et|) .\nIn the next section, we analyze the transient and steady state performance of the introduced algorithms."}, {"heading": "IV. PERFORMANCE ANALYSIS", "text": "We define a priori estimation error and the weighted form as\nea,t \u25b3 = xTt w\u0303t and e \u03a3 a,t \u25b3 = xTt \u03a3w\u0303t,\nwhere w\u0303t \u25b3 = wo \u2212wt and \u03a3 is a symmetric positive definite weighting matrix. Different choice of \u03a3 leads to the different performance measures of the algorithm [1]. In the analyzes, we include the design parameter \u03b1 in order to facilitate the theoretical analyzes. After some algebra, we obtain the weighted-energy recursion [1], [22], [23] as\nE [ \u2016w\u0303t+1\u20162\u03a3 ] = E [ \u2016w\u0303t\u20162\u03a3 ]\n\u2212\u00b52E [ e\u03a3a,t\u2207etf(et) \u03b1f(et)\n1 + \u03b1f(et)\n]\n+\u00b52E\n[\n\u2016xt\u20162\u03a3 ( \u2207etf(et) \u03b1f(et)\n1 + \u03b1f(et)\n)2 ]\n. (11)\nFor notational simplicity, we define\ng(et) \u25b3 = \u2207etf(et)\n\u03b1f(et)\n1 + \u03b1f(et) . (12)\nThen, (11) yields the general weighted-energy recursion [23] as follows\nE [ \u2016w\u0303t+1\u20162\u03a3 ] =E [ \u2016w\u0303t\u20162\u03a3 ] \u2212 \u00b52E [ e\u03a3a,tg(et) ]\n+ \u00b52E [\n\u2016xt\u20162\u03a3g 2(et)\n]\n. (13)\nIn the subsequent analysis of (11), we use the following assumptions:\nAssumption 1: The observation noise nt is zero-mean independently and identically distributed (i.i.d.) Gaussian random\nvariable and independent from xt. The regressor signal xt is also zero-mean i.i.d. Gaussian random variable with the auto-correlation matrix R \u25b3 = E [\nxtx T t\n]\n. Assumption 2:\nThe estimation error et and the noise nt are jointly Gaussian. The Gaussian estimation error assumption is acceptable for sufficiently small step size \u00b5 and through the Assumption 1 [1].\nAssumption 3: The estimation error et is jointly Gaussian with the weighted a priori estimation error e\u03a3a,t for any constant matrix \u03a3. The assumption is reasonable for long filters, i.e. p is large, sufficiently small step size \u00b5 [23], and by Assumption 2. Assumption 4: The random variables \u2016xt\u20162\u03a3 and g\n2(et) are uncorrelated, which enables the following split as\nE [\n\u2016xt\u20162\u03a3g 2(et)\n] = E [ \u2016xt\u20162\u03a3 ] E [ g2(et) ] .\nWe next analyze the transient behavior of the new algorithms through the energy recursion (11)."}, {"heading": "A. Transient Analysis", "text": "In the following we evaluate (11) term by term. We first consider the second term in the right hand side (RHS) of (13) and introduce the following lemma\nLemma 1: Under Assumptions 1-4, we have\nE[e\u03a3a,tg(et)] = E[e \u03a3 a,tet]\nE[etg(et)]\nE[e2t ] . (14)\nProof: The proof of Lemma 1 follows from the Price\u2019s result [24], [25]. That is, for any Borel function g(b) we can write\nE[xg(y)] = E[xy]\nE[y2] E[yg(y)],\nwhere x and y are zero-mean jointly Gaussian random variables [26]. Hence by Assumptions 2 and 3, we obtain (14) and the proof is concluded.\nSince et = ea,t + nt, we obtain\nE [\ne\u03a3a,tet\n] = E [\ne\u03a3a,tea,t\n] = E [ \u2016w\u0303t\u20162\u03a3xtxTt ] , (15)\nby Assumption 1. Additionally, by the independence assumption for the regressor xt (i.e., Assumptions 1 and 4), we can simplify the third term in the RHS of (13). Hence, the weighted-error recursion (13) could be written as follows [23]\nE [ \u2016w\u0303t+1\u20162\u03a3 ] =E [ \u2016w\u0303t\u20162\u03a3 ] \u2212 \u00b52hG (et)E [ \u2016w\u0303t\u20162\u03a3R ]\n+ \u00b52E [ \u2016xt\u20162\u03a3 ] hU (et) , (16)\nwhere\nhG(et) \u25b3 =\nE[etg(et)]\nE[e2t ] , hU (et)\n\u25b3 = E [ g2(et) ] .\nRemark 4.1: In the Appendices we evaluate the functions hG(et) and hU (et) for the LMLS and LLAD algorithms and tabulate the evaluated results with the results for the LMS algorithm, LMF algorithm and SA in Table I.\n6\nUsing (16), in the following we construct the learning curves for the new algorithms:\ni) For the white regression data for which R = \u03c32xI, the time-evolution of the mean square deviation (MSD) E[\u2016w\u0303t\u20162] is given by\nE [ \u2016w\u0303t+1\u20162 ] = ( 1\u2212 \u00b52\u03c32xhG(et) ) E [ \u2016w\u0303t\u20162 ] +\u00b52p\u03c32xhU (et).\nThis completes the transient analysis of the MSD for the white regressor data since hU (et) and hG(et) are given in Table I, and the right hand side only depends on E[\u2016w\u0303t\u20162].\nii) For the correlated regression data, by the CayleyHamilton theorem after some algebra we get the state-space recursion\nWt+1 = AWt + \u00b52Y\nwhere the vectors are defined as\nWt \u25b3 =\n\n  \nE [ \u2016w\u0303t\u20162 ]\n...\nE [ \u2016w\u0303t\u20162Rp\u22121 ]\n\n   , Y \u25b3= hU (et)\n\n  \nE [ \u2016xt\u20162 ]\n...\nE [ \u2016xt\u20162\u03a3p\u22121 ]\n\n   .\nThe coefficient matrix A is given by\nA \u25b3=\n\n   \n1 \u22122\u00b5hG(et) \u00b7 \u00b7 \u00b7 0 0 1 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 2\u00b5c0hG(et) 2\u00b5c1hG(et) \u00b7 \u00b7 \u00b7 1 + 2\u00b5cp\u22121hG(et)\n\n    .\nwhere the ci\u2019s for i \u2208 {0, 1, ..., p\u22121} are the coefficients of the characteristic polynomial of R. Note that the top entry of the state vector Wt yields the time-evolution of the mean square deviation E [ \u2016w\u0303t\u20162 ]\nand the second entry gives the learning curves for the excess mean square error E [\ne2a,t ]\n. In the following subsection, we analyze the steady state excess mean square error (EMSE) and MSD of the LMLS and LLAD algorithms."}, {"heading": "B. Steady State Analysis", "text": "At the steady state, (11) and (15) yields\n\u00b5E [ \u2016xt\u20162\u03a3 ] hU (et) = 2 hG(et)E [ e\u03a3a,tea,t ] . (17)\nWithout loss of generality, we set the weight matrix \u03a3 = I, then (17) leads the steady state EMSE\n\u03b6 \u25b3 = E[e2a,t]\n= \u00b5\n2 E [\n\u2016xt\u20162 ] hU (et)\nhG(et)\n= \u00b5\n2 Tr(R)\nhU (et) hG(et) . (18)\nBy Assumption 1, the steady state MSD is given by [23]\n\u03b7 \u25b3 = E [ \u2016w\u0303t\u20162 ]\n= p\nTr(R) \u03b6,\nwhere p denotes the filter length. At the steady state, we additionally use the following assumptions, which directly follow from the property of a learning algorithm that as t goes to infinity, et goes to zero.\nAssumption 5: For sufficiently small \u00b5, hG(et) and hU (et) functions of the LMLS algorithm as t \u2192 \u221e is given by\nhG(et) = 1\n\u03c32e E\n[\n\u03b1e4t 1 + \u03b1e2t\n]\n\u2192 \u03b1 \u03c32e E [ e4t ] ,\nhU (et) = E\n[\n\u03b12e6t\n(1 + \u03b1e2t ) 2\n]\n\u2192 \u03b12E [ e6t ] .\nAssumption 6: For sufficiently small \u00b5, hG(et) and hU (et) functions of the LLAD algorithm as t \u2192 \u221e is given by\nhG(et) = 1\n\u03c32e E\n[\n\u03b1e2t 1 + \u03b1|et|\n]\n\u2192 \u03b1 \u03c32e E [ e2t ] ,\nhU (et) = E\n[\n\u03b12e2t\n(1 + \u03b1|et|)2\n]\n\u2192 \u03b12E [ e2t ] .\nNow, we explicitly derive the steady state analysis of the LMLS and LLAD algorithms, respectively. The LMLS Algorithm: For the LMLS algorithm, by Assumption 5, (18) leads\n\u03b6LMLS = \u00b5\n2 \u03b1Tr(R)\u03c32e\nE [ e6t ] E [e4t ] . (19)\n8 By Assumption 2, et is a Gaussian random variable and \u03c32e = \u03b6 + \u03c32n, we have\n\u03b6LMLS = \u00b5\n2 \u03b1Tr(R)\u03c32e 15\u03c36e 3\u03c34e ,\n= 5\u00b5\n2 \u03b1Tr(R)\n(\n\u03b6LMLS + \u03c3 2 n\n)2 .\nHence, after some algebra, the EMSE and MSD for the LMLS algorithm are given by\n\u03b6LMLS = 1\u2212 5\u03b1\u00b5Tr(R)\u03c32n \u00b1\n\u221a\n1\u2212 10\u03b1\u00b5Tr(R)\u03c32n 5\u03b1\u00b5Tr(R) , (20)\n\u03b7LMLS = p 1\u2212 5\u03b1\u00b5Tr(R)\u03c32n \u00b1\n\u221a\n1\u2212 10\u03b1\u00b5Tr(R)\u03c32n 5\u03b1\u00b5Tr(R)2 ,\nwhere the smaller roots match with the simulations. Note that (20) for \u03b1 = 1 is the same with the EMSE of the LMF algorithm [23].\nRemark 4.2: In (20), let \u00b5\u0303 \u25b3 = \u00b5\u03b1, then\n\u03b6LMLS = 1\u2212 5\u00b5\u0303Tr(R)\u03c32n \u00b1\n\u221a\n1\u2212 10\u00b5\u0303Tr(R)\u03c32n 5\u00b5\u0303Tr(R) . (21)\nBy (21), we could achieve similar steady state convergence performance for different \u03b1 by changing the step size \u00b5, e.g., \u00b5\u0303 = \u00b5\u03b1 = \u00b51010\u03b1, however, smaller \u03b1 results in a slower convergence rate. Hence, without loss of generality, we propose the algorithms with \u03b1 = 1 under the Gaussianity assumption.\nThe LLAD Algorithm: Similarly, for the LLAD algorithm, by Assumption 6, (18) yields\n\u03b6LLAD = \u00b5\n2 Tr(R)\u03c32e\u03b1\nE[e2t ] E[e2t ] ,\n= \u00b5\u03b1\n2 Tr(R)\u03c32e .\nBy Assumption 2, the EMSE and MSD for the LLAD algorithm is given by\n\u03b6LLAD = \u00b5\u03b1Tr(R)\u03c32n 2\u2212 \u00b5\u03b1Tr(R) . (22) \u03b7LLAD = \u00b5\u03b1p\u03c32n\n2\u2212 \u00b5\u03b1Tr(R) Note that (22) is the same with the EMSE of the LMS algorithm [23]. Hence, for sufficiently small \u03b1, the LLAD algorithm achieves similar steady-state convergence performance with the LMS algorithm under the zero-mean Gaussian error signal assumption.\nIn Fig. 3, we plot the theoretical and simulated MSD vs. step size for the LMLS and LLAD algorithms. In the system identification framework, we choose the regressor and noise signals as i.i.d. zero mean Gaussian with the variances \u03c32x = 1 and \u03c3 2 n = 0.01, respectively. The parameter of interest wo \u2208 R5 is randomly chosen. We observe that the theoretical steady-state MSD matches with the simulation results generated through the ensemble average of the last 103 iterations of 105 (for the LMLS algorithm) and 104 (for the LLAD algorithm) iterations of 200 independent trials. In. Fig. 4 and Fig. 5, under the same configurations, we compare\nthe simulated MSD and EMSE curves generated through the ensemble average of 200 independent trials with the theoretical results for the step-size \u00b5 = 0.1. We note that theoretical performance analyzes match with our simulation results."}, {"heading": "C. Tracking Performance", "text": "In this subsection, we investigate the tracking performance of the introduced algorithms in a non-stationary environment. We assume a random walk model [1] for wo,t such that\nwo,t+1 = wo,t + qt (23)\nwhere qt \u2208 Rp is a zero-mean vector process with covariance matrix E[qtq T t ] = Q. We note that the model (23) has not changed the definitions of a priori error. Hence, by the Assumption 5, the tracking EMSE of the LMLS is the same with the tracking EMSE of the LMF and is approximately given by [1]\n\u03b6\u2032LMLS \u2248 3\u03b1\u00b5\u03c34nTr(R) + \u00b5 \u22121Tr(Q)\n6\u03c32n .\nSimilarly, through the Assumption 6, we obtain the tracking EMSE of the LLAD as\n\u03b6\u2032LLAD = \u03b1\u00b5\u03c32nTr(R) + \u00b5 \u22121Tr(Q)\n2\u2212 \u03b1\u00b5Tr(R) .\nIn the next section, we compare the new algorithms with the conventional LMS and SA in terms of the stability bound and robustness."}, {"heading": "V. COMPARISON WITH THE CONVENTIONAL ALGORITHMS", "text": "We re-emphasize that the cost function J(et) intrinsically combines the costs, mainly, F (et) and F 2(et) based on the relative error amount since for small perturbations on the error, the updates are mainly using the cost F 2(et). Based on our stochastic gradient approach, i.e., removing the expectation in the gradient descent, F 2(et) and F (e2t ) results in the same algorithm. Hence, in this section we compare the stability of the LMLS algorithm with the LMF and LMS algorithms and analyze the robustness of the LLAD algorithm in the impulsive noise environments."}, {"heading": "A. Stability Bound for the LMLS Algorithm", "text": "We again refer to the stochastic gradient update (7), which we rewrite as\nwt+1 = wt + \u00b5 \u2032xt\u2207etf(et),\nwhere \u00b5\u2032 \u25b3 = \u00b5 \u03b1f(et)1+\u03b1f(et) . Note that \u00b5 \u2032 \u2264 \u00b5 irrespective of the design parameter \u03b1. Hence, intuitively we can state that for the introduced algorithms the step-size bound is at least as large as the step-size bound for the corresponding conventional algorithm.\nAnalytically, for stable updates the step size \u00b5 should satisfy\nE [ \u2016w\u0303t+1\u20162 ] \u2264 E [ \u2016w\u0303t\u20162 ] .\nBy (11), the Assumption 3, and \u03a3 = I, the stability bound on the step size is given by\n\u00b5 \u2264 2 E [\u2016xt\u20162] inf E[e2a,t]\u2208\u2126\n{\nE[ea,tet] hG(et)\nhU (et)\n}\n,\n9 where\n\u2126 \u25b3 =\n{\nE[e2a,t] : \u03bb \u2264 E[e2a,t] \u2264 1\n4 Tr(R)E[\u2016w\u03030\u20162]\n}\n,\nwith the Cramer-Rao lower bound \u03bb [27]. For example the step size bound for the LMLS yields\n\u00b5 \u2264 1 E [\u2016xt\u20162] inf E[e2a,t]\u2208\u2126\n{\nE[ea,tet]\nE [e2t ] \u03b2\n}\n,\nwhere\n\u03b2 \u25b3 =\nE [ \u03b1e4t 1+\u03b1e2t ]\nE [ \u03b12e6t (1+\u03b1e2t ) 2 ]\n= E [ \u03b1e4t (1+\u03b1e2t ) 2 ] + E [ \u03b12e6t (1+\u03b1e2t ) 2 ]\nE [\n\u03b12e6t (1+\u03b1e2t ) 2\n] \u2265 1.\nWe re-emphasize that the LMLS extends the stability bound of the LMS algorithm (the same bound with \u03b2 = 1) while performing comparable performance with the LMF algorithm, which has several stability issues [3]\u2013[5]."}, {"heading": "B. Robustness Analysis for the LLAD Algorithm", "text": "Although the performance analysis of the adaptive filters assumes the white Gaussian noise signals, in practical applications the impulsive noise is a common problem [8]. In order to analyze the performance in the impulsive noise environments, we use the following model.\nImpulsive noise model: We model the noise as a summation of two independent random terms [28], [29] as\nnt = no,t + btni,t,\nwhere no,t is the ordinary noise signal that is zero-mean Gaussian with variance \u03c32no and ni,t is the impulse-noise that is also zero-mean Gaussian with significantly large variance \u03c32ni . Here, bt is generated through a Bernoulli random process and determines the occurrence of the impulses in the noise signal with pB(bt = 1) = \u03bdi and pB(bt = 0) = 1\u2212 \u03bdi where \u03bdi is the frequency of the impulses in the noise signal. The corresponding probability density function is given by\npn(nt) = 1\u2212 \u03bdi\u221a 2\u03c0\u03c3no exp\n(\n\u2212 n 2 t\n2\u03c32no\n)\n+ \u03bdi\u221a 2\u03c0\u03c3n exp\n(\n\u2212 n 2 t\n2\u03c32n\n)\n,\nwhere \u03c32n = \u03c3 2 no + \u03c32ni .\nWe particularly analyze the steady-state performance of the LLAD algorithm (for which f(et) = |et|) in the impulsive noise environments since we motivate the LLAD algorithm as improving the steady state convergence performance of the SA. Since the noise is not a Gaussian random variable in impulsive noise environment, the Gaussianity assumption of the estimation error et and the Price\u2019s Theorem are not applicable. At the steady-state, for \u03a3 = I, (11) yields\nE [ \u2016xt\u20162 ] = 2E\n[\n\u03b1ea,tet 1+\u03b1|et|\n]\n\u00b5E [\n\u03b12e2t (1+\u03b1|et|) 2\n] . (24)\nSteady\u2212state MSD vs. step size for the LLAD algorithm\nWe now evaluate the each term in (24) separately. We first consider the nominator of the RHS of (24), and write\nE\n[\n\u03b1ea,tet\n1 + \u03b1|et|\n]\n=\n\u222b \u221e\n\u2212\u221e\n\u222b \u221e\n\u2212\u221e\n\u03b1ea,t(ea,t + nt) 1 + \u03b1|ea,t + nt|\nexp\n(\n\u2212 e 2 a,t\n2\u03c32ea\n)\n\u221a 2\u03c0\u03c3ea pn(nt)dea,tdnt.\n= \u03b1\n\u222b \u221e\n\u2212\u221e\n\u222b \u221e\n\u2212\u221e\nea,tet\nexp\n(\n\u2212 e 2 a,t\n2\u03c32ea \u2212 n\n2 t\n2\u03c32no\n)\n2\u03c0\u03c3ea\u03c3no (1\u2212 \u03bdi)dea,tdnt\n+\n\u222b \u221e\n\u2212\u221e\n\u222b \u221e\n\u2212\u221e\nea,tsign(ea,t + nt)\nexp\n(\n\u2212 e 2 a,t\n2\u03c32ea \u2212 n\n2 t\n2\u03c32n\n)\n2\u03c0\u03c3ea\u03c3n \u03bdidea,tdnt,\nwhere in the last step of the equation we assume that in the impulse-free environment, \u03b1ea,tet1+\u03b1|et| \u2248 \u03b1ea,tet since at steady state, the error is assumed to take relatively small values whereas if the impulse-noise occurs, \u03b1ea,tet1+\u03b1|et| \u2248 ea,tsign(et) due to the large perturbation on the error. Hence, since \u03c32n \u226b \u03c32ea , the expectation leads\nE\n[\n\u03b1ea,tet\n1 + \u03b1|et|\n] = \u03b1(1\u2212 \u03bdi)\u03c32ea + \u221a 2 \u03c0 \u03bdi \u03c32ea \u03c3n . (25)\nFollowing similar steps for the denominator of the RHS of (24), we obtain\nE\n[\n\u03b12e2t\n(1 + \u03b1|et|)2\n]\n= \u03b12(1\u2212 \u03bdi) ( \u03c32ea + \u03c3 2 no ) + \u03bdi. (26)\nBy (24), (25) and (26), the EMSE of the LLAD algorithm in the impulsive noise environment is given by\n\u03b6\u2217LLAD = \u00b5Tr(R)\n(\n\u03bdi + \u03b1 2(1 \u2212 \u03bdi)\u03c32no\n)\n\u03b1(1 \u2212 \u03bdi)(2 \u2212 \u03b1\u00b5Tr(R)) + \u221a 8 \u03c0 \u03bdi \u03c3n\n. (27)\nNote that for \u03bdi = 0 (impulse-free) (27) yields (22).\nRemark 5.1: Increasing \u03bdi or in other words more frequent impulses cause larger steady state EMSE. However, through\n10\nthe optimization of \u03b1, we can minimize the steady state EMSE. After some algebra, the optimum design parameter in impulsive noise environment is roughly given by\n\u03b1opt \u2248 \u221a \u03bdi 1\u2212 \u03bdi 1 \u03c3no .\nIn Fig. 6, we plot the dependence of the steady-state MSD with the step size in 5%, i.e., \u03bdi = 0.05, impulsive noise environment where \u03c32x = 1, \u03c3 2 no = 0.01 and \u03c32ni = 10 4 after 200 independent trials. We observe that \u03b1opt improves the convergence performance and the theoretical analyzes through the impulsive noise model matches with the simulation results. We next demonstrate the performance of the introduced algorithms in different applications."}, {"heading": "VI. NUMERICAL EXAMPLES", "text": "In this section, we particularly compare the convergence rate of the algorithms for the same steady state MSD through the specific choice of the step sizes for a fair comparison. Here, we have a stationary data dt = wTo xt + nt where xt is zeromean Gaussian i.i.d. regression signal with variance \u03c32x = 1, nt represent zero-mean i.i.d. noise signal and the parameter of interest wo \u2208 R5 is randomly chosen. In following scenarios, we compare the algorithms under Gaussian noise and impulsive noise models subsequently.\nScenario 1 (impulse-free environment): In that scenario, we use a zero-mean Gaussian i.i.d. noise signal with the variance \u03c32n = 0.01 and the design parameter \u03b1 = 1. In Fig. 7, we compare the convergence rate of the LMLS, LMF and LMS algorithms for relatively small step sizes. We observe that LMLS and LMF algorithms achieve comparable performance and LMLS achieves better convergence performance than the LMS algorithm. In Fig. 8, we compare the LMLS and LMS algorithms for relatively large step sizes, i.e., \u00b5LMLS = 0.1 and \u00b5LMS = 0.0047. We only compare the LMLS and LMS algorithms since the LMF algorithm is not stable for such a step-size. Hence, the LMLS algorithm demonstrate comparable convergence performance with the LMF algorithm with extended stability bound.\nIn Fig. 9, we compare the LLAD, SA and LMS algorithms in impulse-free noise environment. We observe that the LLAD algorithm shows comparable convergence performance with the LMS algorithm, in other words, the logarithmic error cost framework improves the convergence performance of the SA.\nScenario 2 (impulsive noise environment): Here, we use the impulsive noise model with \u03c32ni = 10\n4. In that configuration, we resort to the design parameter since through the optimization of \u03b1, the LLAD algorithm could achieve smaller steady-state MSD. In Fig. 10, we plot sample desired signals in 1%, 2% and 5% impulsive noise environments and Fig. 11 shows the corresponding time evolution of the MSD of the LLAD, SA and LMS algorithms. The step sizes are chosen as \u00b5LLAD = \u00b5LMS = 0.0097, 0.007, 0.0043 for 1%, 2% and 5% impulsive noise environments, respectively, and \u00b5SA = 0.0015. The figures show that in the impulsive noise environments, the LMS algorithm does not converge while the LLAD algorithm, which achieves comparable convergence performance with the LMS algorithm in the impulse free environment, performs still better than the SA."}, {"heading": "VII. CONCLUDING REMARKS", "text": "In this paper, we present a novel family of adaptive filtering algorithms based on the logarithmic error cost framework. We propose important members of the new family, i.e., the LMLS and LLAD algorithms. The LMLS algorithm achieves comparable convergence performance with the LMF algorithm with far larger stability bound on the step size. In the impulse-free environment, the LLAD algorithm has a similar convergence performance with the LMS algorithm. Furthermore, the LLAD algorithm is robust against impulsive interferences and outperforms the SA. We also provide comprehensive performance analyzes of the introduced algorithms, which match with our simulation results. For example, the steady-state analyzes in the impulse-free and impulsive noise environments. Finally, we show the improved convergence performance of the new algorithms in several different system identification scenarios.\n11"}, {"heading": "APPENDIX A", "text": "EVALUATION OF hG(et)\nThe LMLS algorithm: We have\nhG(et) = 1\n\u03c32e E\n[\n\u03b1e4t 1 + \u03b1e2t\n]\n,\n= 1\n\u03c32e\n( \u03c32e \u2212 \u03b1\u22121 + \u03b1\u22121E [\n1\n1 + \u03b1e2t\n])\n, (28)\nwhere \u03c32e = E[e 2 t ] and the first line of the equation follows according to the definition of g(et) in (12). According to Assumption 2, we obtain the last term in (28) as follows\nE\n[\n1\n1 + \u03b1e2t\n]\n= 1\u221a 2\u03c0\u03c3e\n\u222b \u221e\n\u2212\u221e\n1\n1 + \u03b1e2t exp\n(\n\u2212 e 2 t\n2\u03c32e\n)\ndet\n= 1\u221a\n2\u03b1\u03c0\u03c3e\n\u222b \u221e\n\u2212\u221e\nexp ( \u2212\u03bbu2 )\n1 + u2 du\n= 1\u221a\n2\u03b1\u03c0\u03c3e \u03c0exp(\u03bb)erfc(\n\u221a \u03bb), (29)\nwhere u \u25b3 = \u221a \u03b1et, \u03bb\n\u25b3 = 12\u03b1\u03c32e\n, and the third line follows from [30] with erfc(\u00b7) denoting the complementary error function. Hence, putting (29) in (28), we obtain hG(et) for the LMLS update\nhG(et) = 1\u2212 2\u03bb ( 1\u2212 \u221a \u03c0\u03bbexp(\u03bb)erfc( \u221a \u03bb) ) .\nThe LLAD algorithm: We have\nhG(et) = 1\n\u03c32e E\n[\n\u03b1e2t 1 + \u03b1|et|\n]\n,\n= 1\n\u03c32e\n( E[|et|]\u2212 \u03b1\u22121 + \u03b1\u22121E [\n1\n1 + \u03b1|et|\n])\n, (30)\nwhere the first line follows according to the definition of g(et) in (12). According to Assumption 2, we obtain the last term\n12\nin (30) as follows\nE\n[\n1\n1 + \u03b1|et|\n]\n= 1\u221a 2\u03c0\u03c3e\n\u222b \u221e\n\u2212\u221e\n1\n1 + \u03b1|et| exp\n(\n\u2212 e 2 t\n2\u03c32e\n)\ndet\n= 1\u221a\n2\u03c0\u03b1\u03c3e\n\u222b \u221e\n\u2212\u221e\n1\n1 + |u|exp ( \u2212\u03bau2 ) du\n= 1\u221a\n2\u03c0\u03b1\u03c3e\n\u03c0erfi( \u221a \u03ba)\u2212 Ei(\u03ba)\nexp(\u03ba) , (31)\nwhere u \u25b3 = \u03b1et, and \u03ba \u25b3 = 12\u03b12\u03c32e\n, and the third line follows from [30] with erfi(z) = \u2212jerf(jz) denoting the imaginary error function and Ei(x) denoting the exponential integral, i.e.,\nEi(x) = \u2212 \u222b \u221e\n\u2212x\nexp(\u2212t) t dt.\nAs a result, putting (31) in (30), we obtain hG(et) for the LLAD update\nhG(et) = 1\n\u03c3e\n\u221a\n2\n\u03c0\n( 1\u2212 \u221a \u03ba\u03c0 + \u03ba\n\u03c0erfi( \u221a \u03ba)\u2212 Ei(\u03ba)\nexp(\u03ba)\n)\n."}, {"heading": "APPENDIX B", "text": "EVALUATION OF hU (et)\nThe LMLS Algorithm: We have\nhU (et) = E\n[\n\u03b12e6t\n(1 + \u03b1e2t ) 2\n]\n= E\n[\n\u2212\u03b12 \u2202 \u2202\u03b1\n(\ne4t 1 + \u03b1e2t\n)]\n= \u2212\u03b12 \u2202 \u2202\u03b1\n(\nE\n[\ne4t 1 + \u03b1e2t\n])\n,\nwhere in the last line we applied the interchange of integration and differentiation property since \u03b8(et, \u03b1) \u25b3 =\ne4t 1+\u03b1e2t\nand \u2202\u03b8(et,\u03b1)\n\u2202\u03b1 are both continuous in R2. From Appendix A, we\nobtain\nhU (et) = \u2212\u03b12 \u2202\n\u2202\u03b1\n(\n\u03b1\u22121E\n[\n\u03b1e4t 1 + \u03b1e2t\n])\n= \u2212\u03b12 \u2202 \u2202\u03b1 ( \u03b1\u22121\u03c32ehG(et) )\n= \u03c32e\n( 1\u2212 2\u03bb(\u03bb+ 2) + \u03bb(2\u03bb+ 5) \u221a \u03c0\u03bbexp(\u03bb)erfc (\u221a \u03bb )) .\nThe LLAD Algorithm: Following similar lines to LMLS algorithm, we have\nhU (et) = E\n[\n\u03b12e2t\n(1 + \u03b1|et|)2\n]\n= E\n[\n\u2212\u03b12 \u2202 \u2202\u03b1 ( |et| 1 + \u03b1|et| )]\n= \u2212\u03b12 \u2202 \u2202\u03b1\n(\nE [ |et| 1 + \u03b1|et| ]) ,\nwhere in the last line we applied the interchange of integration and differentiation property since \u03b8(et, \u03b1) \u25b3 = |et|1+\u03b1|et| and\n\u2202\u03b8(et,\u03b1) \u2202\u03b1 are both continuous in R2. From Appendix A, we obtain\nhU (et) = \u2212\u03b12 \u2202\n\u2202\u03b1\n(\n\u03b1\u22121E\n[\n\u03b1|et| 1 + \u03b1|et|\n])\n= \u2212\u03b12 \u2202 \u2202\u03b1\n( \u03b1\u22121 ( 1\u2212 E [\n1\n1 + \u03b1|et|\n]))\n= \u2212\u03b12 \u2202 \u2202\u03b1\n( \u03b1\u22121 ( 1\u2212 1\u221a 2\u03c0\u03b1\u03c3e\n\u03c0erfi( \u221a \u03ba)\u2212 Ei(\u03ba)\nexp(\u03ba)\n))\n= 1\u2212 2\u03ba+ 2 \u221a \u03ba\n\u03c0\n( 1 + (\u03ba\u2212 1)\u03c0erfi ( \u221a \u03ba)\u2212 Ei(\u03ba)\nexp(\u03ba)\n)\n,\nwhere the third line follows from (31)."}], "references": [{"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "The least mean fourth (LMF) adaptive algorithm and its family,", "author": ["E. Walach", "B. Widrow"], "venue": "IEEE Trans. Inform. Theory, vol. 30,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1984}, {"title": "When is the least-mean fourth algorithm mean-square stable?", "author": ["V. Nascimento", "J. Bermudez"], "venue": "in Acoustics, Speech, and Signal Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Probability of divergence for the least-mean fourth algorithm,", "author": ["V. Nascimento", "J.C.M. Bermudez"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A mean-square stability analysis of the least mean fourth adaptive algorithm,", "author": ["P. Hubscher", "J. Bermudez", "V. Nascimento"], "venue": "IEEE Trans. on Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "A simple model for the effect of normalization on the convergence rate of adaptive filters,", "author": ["V. Nascimento"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Signal processing with fractional lower order moments: stable processes and their applications,", "author": ["M. Shao", "C. Nikias"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Adaptive robust impulse noise filtering,", "author": ["S.R. Kim", "A. Efron"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Improved convergence analysis of stochastic gradient adaptive filters using the sign algorithm,", "author": ["V.J. Mathews", "S.-H. Cho"], "venue": "IEEE Trans. Acoust., Speech, Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1987}, {"title": "Least mean mixednorm adaptive filtering,", "author": ["J. Chambers", "O. Tanrikulu", "A. Constantinides"], "venue": "Electron. Lett., vol. 30,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "A robust mixed-norm adaptive filter algorithm,", "author": ["J. Chambers", "A. Avlonitis"], "venue": "IEEE Signal Processing Lett., vol. 4,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Separate-variable adaptive combination of lms adaptive filters for plant identification,", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "M. Martinez-Ramon", "A. Figueiras-Vidal"], "venue": "IEEE 13th Workshop on Neural Networks for Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "New algorithms for improved adaptive convex combination of lms transversal filters,", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "A. Figueiras-Vidal"], "venue": "IEEE Trans. Instrumentation and Measurement,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Mean-square performance of a convex combination of two adaptive filters,", "author": ["J. Arenas-Garcia", "A. Figueiras-Vidal", "A. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Improving the tracking capability of adaptive filters via convex combination,", "author": ["M.T.M. Silva", "V. Nascimento"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Steady-state mse performance analysis of mixture approaches to adaptive filtering,", "author": ["S. Kozat", "A. Erdogan", "A. Singer", "A. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Adaptive combination of normalised filters for robust system identification,", "author": ["J. Arenas-Garcia", "A. Figueiras-Vidal"], "venue": "Electron. Lett., vol. 41,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Robust huber adaptive filter,", "author": ["P. Petrus"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Scherbert, Introduction to Real Analysis", "author": ["D.R.R.G. Bartle"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A normalized least mean squares algorithm with a step-size scaler against impulsive measurement noise,", "author": ["I. Song", "P. Park", "R. Newcomb"], "venue": "IEEE Trans. Circuits Syst. II: Express Briefs, vol. 60,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Transient analysis of data-normalized adaptive filters,", "author": ["T.Y. Al-Naffouri", "A. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "A useful theorem for nonlinear devices having gaussian inputs,", "author": ["R. Price"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1958}, {"title": "An extension of price\u2019s theorem (corresp.),", "author": ["E. McMahon"], "venue": "IEEE Trans. Inform. Theory, vol. 10,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1964}, {"title": "Efficient methods of estimate correlation functions of gaussian processes and their performance analysis,", "author": ["T. Koh", "E. Powers"], "venue": "IEEE Trans. Acoust., Speech, Signal Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1985}, {"title": "Joint channel estimation and symbol detection in rayleigh flat-fading channels with impulsive noise,", "author": ["X. Wang", "H. Poor"], "venue": "IEEE Comm. Lett.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "A recursive least m-estimate algorithm for robust adaptive filtering in impulsive noise: fast algorithm and convergence performance analysis,", "author": ["S.-C. Chan", "Y.-X. Zou"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "The least mean square (LMS) and normalized least mean square (NLMS) algorithms are the members of this class [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "In the literature, different powers of the error are commonly used as the cost function in order to provide stronger convergence or steady-state performance than the least-squares algorithms under certain settings [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 1, "context": ", E[e t ] [2].", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "transient and steady-state performance, however, has stability issues [3]\u2013[5].", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "transient and steady-state performance, however, has stability issues [3]\u2013[5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "On the other hand, the stability of the conventional LMS algorithm depends only on the input power for a given step-size [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "The normalized filters improve the performance of the algorithms under certain settings by removing dependency to the input statistics in the updates [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": ", in applications involving high power noise signals [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "In this context, we define robustness as the insensitivity of the algorithms against the impulsive interferences encountered in the practical applications and provide a theoretical framework [9].", "startOffset": 191, "endOffset": 194}, {"referenceID": 8, "context": "However, the SA usually exhibits slower convergence performance especially for highly correlated input signals [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "The mixed-norm algorithms minimize a combination of different error norms in order to achieve improved convergence performance [11], [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "The mixed-norm algorithms minimize a combination of different error norms in order to achieve improved convergence performance [11], [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "For example, [12] combines the robust L1 norm and the more sensitive but better converging L2 norm through a mixing parameter.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "On the other hand, the mixture of experts algorithms adaptively combine different algorithms and provide improved performance irrespective of the environment statistics [13]\u2013 [16].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "On the other hand, the mixture of experts algorithms adaptively combine different algorithms and provide improved performance irrespective of the environment statistics [13]\u2013 [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "However, note that such mixture approaches require to operate several different algorithms on parallel, which may be infeasible in different applications [17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "In [18], authors propose an adaptive combination of L1 and L2 norms of the error in parallel, however, the resulting algorithm demonstrates impulsive perturbations on the learning curves.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In general, the samples contaminated with impulses contain little useful information [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 17, "context": "The switched-norm algorithms switch between the L1 and L2 norms based on the error amount such as the robust Huber filter [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "We particularly choose the logarithm function as the normalizing diminishing return function [20] in our cost definitions since the logarithmic function is differentiable and results efficient and mathematically tractable adaptive algorithms.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": ", the mean square error E[et ] [1].", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "1: In [21], the authors propose a stochastic cost function using the logarithm function as follows", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "J [21](et) \u25b3 = 1 2\u03b3 ln (", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "Note that the cost function J [21](et) is the subtracted term in (1) for F (et) = e2t \u2016xt\u2016 .", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The Hessian matrix of J [21](et) is given by H (", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "J [21](et) )", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "J [21](et) )", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "On the other hand, we show that the new cost function in (1) is a convex function enabling the use of the diminishing return property [20] of the logarithm function for stable and robust updates.", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "As an example, the Huber objective function combining L1 and L2 norms of the error is defined as [19] \u03c1(et) \u25b3 = { 1 2e 2 t for |et| \u2264 \u03b3, \u03b3|et| \u2212 1 2\u03b32 for |et| > \u03b3, (5) where \u03b3 > 0 denotes the cut-off value.", "startOffset": 97, "endOffset": 101}, {"referenceID": 9, "context": "Hence, the LMLS algorithm intrinsically combines the least mean-square and least-mean fourth algorithms based on the error amount instead of mixed LMF + LMS algorithms [11] that need artificial combination parameter in the cost definition.", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "The Least Logarithmic Absolute Difference (LLAD) Algorithm The SA utilizes F (et) = E[|et|] as the cost function, which provides robustness against impulsive interferences [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "Different choice of \u03a3 leads to the different performance measures of the algorithm [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "After some algebra, we obtain the weighted-energy recursion [1], [22], [23] as E [", "startOffset": 60, "endOffset": 63}, {"referenceID": 20, "context": "After some algebra, we obtain the weighted-energy recursion [1], [22], [23] as E [", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "The Gaussian estimation error assumption is acceptable for sufficiently small step size \u03bc and through the Assumption 1 [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 21, "context": "(14) Proof: The proof of Lemma 1 follows from the Price\u2019s result [24], [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "(14) Proof: The proof of Lemma 1 follows from the Price\u2019s result [24], [25].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "That is, for any Borel function g(b) we can write E[xg(y)] = E[xy] E[y2] E[yg(y)], where x and y are zero-mean jointly Gaussian random variables [26].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "We assume a random walk model [1] for wo,t such that wo,t+1 = wo,t + qt (23) where qt \u2208 R is a zero-mean vector process with covariance matrix E[qtq T t ] = Q.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Hence, by the Assumption 5, the tracking EMSE of the LMLS is the same with the tracking EMSE of the LMF and is approximately given by [1] \u03b6 LMLS \u2248 3\u03b1\u03bc\u03c3 nTr(R) + \u03bc Tr(Q) 6\u03c32 n .", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "We re-emphasize that the LMLS extends the stability bound of the LMS algorithm (the same bound with \u03b2 = 1) while performing comparable performance with the LMF algorithm, which has several stability issues [3]\u2013[5].", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "We re-emphasize that the LMLS extends the stability bound of the LMS algorithm (the same bound with \u03b2 = 1) while performing comparable performance with the LMF algorithm, which has several stability issues [3]\u2013[5].", "startOffset": 210, "endOffset": 213}, {"referenceID": 6, "context": "Robustness Analysis for the LLAD Algorithm Although the performance analysis of the adaptive filters assumes the white Gaussian noise signals, in practical applications the impulsive noise is a common problem [8].", "startOffset": 209, "endOffset": 212}, {"referenceID": 24, "context": "Impulsive noise model: We model the noise as a summation of two independent random terms [28], [29] as nt = no,t + btni,t, where no,t is the ordinary noise signal that is zero-mean Gaussian with variance \u03c3 no and ni,t is the impulse-noise that is also zero-mean Gaussian with significantly large variance \u03c3 ni .", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "Impulsive noise model: We model the noise as a summation of two independent random terms [28], [29] as nt = no,t + btni,t, where no,t is the ordinary noise signal that is zero-mean Gaussian with variance \u03c3 no and ni,t is the impulse-noise that is also zero-mean Gaussian with significantly large variance \u03c3 ni .", "startOffset": 95, "endOffset": 99}], "year": 2013, "abstractText": "We introduce a novel family of adaptive filtering algorithms based on a relative logarithmic cost. The new family intrinsically combines the higher and lower order measures of the error into a single continuous update based on the error amount. We introduce important members of this family of algorithms such as the least mean logarithmic square (LMLS) and least logarithmic absolute difference (LLAD) algorithms that improve the convergence performance of the conventional algorithms. However, our approach and analysis are generic such that they cover other well-known cost functions as described in the paper. The LMLS algorithm achieves comparable convergence performance with the least mean fourth (LMF) algorithm and extends the stability bound on the step size. The LLAD and least mean square (LMS) algorithms demonstrate similar convergence performance in impulse-free noise environments while the LLAD algorithm is robust against impulsive interferences and outperforms the sign algorithm (SA). We analyze the transient, steady state and tracking performance of the introduced algorithms and demonstrate the match of the theoretical analyzes and simulation results. We show the extended stability bound of the LMLS algorithm and analyze the robustness of the LLAD algorithm against impulsive interferences. Finally, we demonstrate the performance of our algorithms in different scenarios through numerical examples.", "creator": "LaTeX with hyperref package"}}}