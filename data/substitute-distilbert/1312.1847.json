{"id": "1312.1847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2013", "title": "Understanding Deep Architectures using a Recursive Convolutional Network", "abstract": "convolutional neural network models have recently been shown could achieve good performance on global recognition benchmarks. however, like many deep models, there is little guidance on how the architecture of the model should be selected. important hyper - parameters such as the degree of parameter sharing, number of layers, units per layer, and overall number of parameters must be selected manually through trial - and - bust. towards address this, we introduce a novel subset of recursive neural network dynamics is complicated in nature. its similarity to database structure models allows us for tease guess the important architectural factors that influence modeling. researchers find that across a given parameter budget, deeper models are preferred over heavier ones, and models with more parameters are convenient to those with fewer. surprisingly and perhaps counterintuitively, we find that performance is independent of the number of units, so long as the network depth and number network parameters stay held constant. this concerns that, computational efficiency considerations aside, parameter sharing within deep networks may not be so beneficial as previously supposed.", "histories": [["v1", "Fri, 6 Dec 2013 12:55:05 GMT  (195kb,D)", "https://arxiv.org/abs/1312.1847v1", null], ["v2", "Wed, 19 Feb 2014 17:55:37 GMT  (151kb,D)", "http://arxiv.org/abs/1312.1847v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david eigen", "jason rolfe", "rob fergus", "yann lecun"], "accepted": false, "id": "1312.1847"}, "pdf": {"name": "1312.1847.pdf", "metadata": {"source": "CRF", "title": "Understanding Deep Architectures using a Recursive Convolutional Network", "authors": ["David Eigen", "Jason Rolfe", "Rob Fergus", "Yann LeCun"], "emails": ["deigen@cs.nyu.edu", "rolfe@cs.nyu.edu", "fergus@cs.nyu.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Convolutional networks have recently made significant progress in a variety of image classification and detection tasks [13, 2, 23], with further gains and applications continuing to be realized. At the same time, performance of these models is determined by many interrelated architectural factors; among these are the number of layers, feature map dimension, spatial kernel extents, number of parameters, and pooling sizes and placement. Notably, multiple layers of unpooled convolution [13, 30] have been utilized lately with considerable success. These architectures must be carefully designed and sized using good intuition along with extensive trial-and-error experiments on a validation set. But are there any characteristics to convolutional layers\u2019 performance that might be used to help inform such choices? In this paper, we focus on disentangling and assessing the independent effects of three important variables: the numbers of layers, feature maps per layer, and parameters.\nWe accomplish this via a series of three experiments using a novel type of recursive network model. This model has a convolutional architecture and is equivalent to a deep convolutional network where all layers have the same number of feature maps and the filters (weights) are tied across layers. By aligning the architecture of this model to existing convolutional approaches, we are able to tease apart these three factors that determine performance. For example, adding another layer increases the number of parameters, but it also puts an additional non-linearity into the system. But would the extra parameters be better used expanding the size of the existing layers? To provide a general answer to this type of issue is difficult since multiple factors are conflated: the capacity of the model (and of each layer) and its degree of non-linearity. However, we can design a recursive model to have the same number of layers and parameters as the standard convolutional model, and thereby see if the number of feature maps (which differs) is important or not. Or we can match the number of feature maps and parameters to see if the number of layers (and number of non-linearities) matters.\nar X\niv :1\n31 2.\n18 47\nv2 [\ncs .L\nG ]\n1 9\nFe b\n20 14\nWe consider convolutional models exclusively in this paper for several reasons. First, these models are widely used for image recognition tasks. Second, they are often large, making architecture search tedious. Third, and most significantly, recent gains have been found by using stacks of multiple unpooled convolution layers. For example, the convolutional model proposed by Krizhevsky et al. [13] for ImageNet classification has five convolutional layers which turn out to be key to its performance. In [30], Zeiler and Fergus reimplemented this model and adjusted different parts in turn. One of the largests effects came from changing two convolutional layers in the middle of the model: removing them resulted in a 4.9% drop in performance, while expanding them improved performance by 3.0%. By comparison, removing the top two densely connected layers yielded a 4.3% drop, and expanding them a 1.7% gain, even though they have far more parameters. Hence the use of multiple convolutional layers is vital and the development of superior models relies on understanding their properties. Our experiments have particular bearing in characterizing these layers."}, {"heading": "1.1 Related Work", "text": "The model we employ has relations to recurrent neural networks. These are are well-studied models [11, 21, 27], naturally suited to temporal and sequential data. For example, they have recently been shown to deliver excellent performance for phoneme recognition [8] and cursive handwriting recognition [7]. However, they have seen limited use on image data. Socher et al. [26] showed how image segments could be recursively merged to perform scene parsing. More recently [25], they used a convolutional network in a separate stage to first learn features on RGB-Depth data, prior to hierarchical merging. In these models the input dimension is twice that of the output. This contrasts with our model which has the same input and output dimension.\nOur network also has links to several auto-encoder models. Sparse coding [18] uses iterative algorithms, such as ISTA [1], to perform inference. Rozell et al. [20] showed how the ISTA scheme can be unwrapped into a repeated series of network layers, which can be viewed as a recursive net. Gregor & LeCun [9] showed how to backpropagate through such a network to give fast approximations to sparse coding known as LISTA. Rolfe & LeCun [19] then showed in their DrSAE model how a discriminative term can be added. Our network can be considered a purely discriminative, convolutional version of LISTA or DrSAE.\nThere also are interesting relationships with convolutional Deep Belief Networks [15], as well as Multi-Prediction Deep Boltzmann Machines [6]. As pointed out by [6], mean field inference in such models can be unrolled and viewed as a type of recurrent network. In contrast to the model we use, however, [15] trains unsupervised using contrastive divergence, while [6] is nonconvolutional and focuses on conditioning on random combinations of inputs and targets."}, {"heading": "2 Approach", "text": "Our investigation is based on a multilayer Convolutional Network [14], for which all layers beyond the first have the same size and connection topology. All layers use rectified linear units (ReLU) [3, 4, 16]. We perform max-pooling with non-overlapping windows after the first layer convolutions and rectification; however, layers after the first use no explicit pooling. We refer to the number of feature maps per layer as M , and the number of layers after the first as L. To emphasize the difference between the pooled first layer and the unpooled higher layers, we denote the first convolution kernel by V and the kernels of the higher layers by Wl. A per-map bias bl is applied in conjunction with the convolutions. A final classification matrix C maps the last hidden layer to softmax inputs.\nSince all hidden layers have the same size, the transformations at all layers beyond the first have the same number of parameters (and the same connection topology). In addition to the case where all layers are independently parameterized, we consider networks for which the parameters of the higher layers are tied between layers, so that Wi = Wj and bi = bj for all i, j. As shown in Fig. 1, tying the parameters across layers renders the deep network dynamics equivalent to recurrence: rather than projecting through a stack of distinct layers, the hidden representation is repeatedly processed by a consistent nonlinear transformation. The convolutional nature of the transformation performed at each layer implies another set of ties, enforcing translation-invariance among the parameters. This novel recursive, convolutional architecture is reminiscent of LISTA [9], but without a direct projection from the input to each hidden layer.\n...\nconvolution\nW\nZ3\nM\n8\nM"}, {"heading": "2.1 Instantiation on CIFAR-10 and SVHN", "text": "We describe our models for the CIFAR-10 [12] and SVHN [17] datasets used in our experiments. In both cases, each image Xn is of size 32\u00d7 32\u00d7 3. In the equations below, we drop the superscript n indicating the index in the dataset for notational simplicity. The first layer applies a set of M kernels Vm of size 8 \u00d7 8 \u00d7 3 via spatial convolution with stride one (denoted as \u2217), and per-map bias b0m, followed by the element-wise rectification nonlinearity. We use a \u201csame\u201d convolution (i.e. zeropadding the edges), yielding a same-size representation P of 32 \u00d7 32 \u00d7M . This representation is then max-pooled within each feature map with non-overlapping 4\u00d7 4 windows, producing a hidden layer Z1 of size 8\u00d7 8\u00d7M .\nPm = max ( 0,b0m +Vm \u2217X ) , Z1i,j,m = max\ni\u2032,j\u2032\u2208{0,\u00b7\u00b7\u00b7 ,3} (P4\u00b7i+i\u2032,4\u00b7j+j\u2032,m)\nAll L succeeding hidden layers maintain this size, applying a set of M kernels Wlm of size 3 \u00d7 3 \u00d7M , also via \u201csame\u201d spatial convolution with stride one, and per-map bias blm, followed by the rectification nonlinearity:\nZlm = max ( 0,bl\u22121m +W l\u22121 m \u2217 Zl\u22121 ) In the case of the tied model (see Fig. 1(b)), the kernels W l (and biases bl) are constrained to be the same. The final hidden layer is subject to pixel-wise L2 normalization and passed into a logistic classifier to produce a prediction Y:\nYk = exp(Y \u2032k)\u2211 k exp(Y \u2032 k)\nwhere Y\u2032k = \u2211 i,j,m Cki,j,m \u00b7 ZL+1i,j,m/||Z L+1 i,j ||\nThe first-layer kernels Vm are initialized from a zero-mean Gaussian distribution with standard deviation 0.1 for CIFAR-10 and 0.001 for SVHN. The kernels of the higher layers Wlm are initialized to the identity transformation Wi\u2032,j\u2032,m\u2032,m = \u03b4i\u2032,0 \u00b7 \u03b4j\u2032,0 \u00b7 \u03b4m\u2032,m, where \u03b4 is the Kronecker delta. The network is trained to minimize the logistic loss function L = \u2211 n log(Y n k(n)) and k(n) is the true class of the nth element of the dataset. The parameters are not subject to explicit regularization. Training is performed via stochastic gradient descent with minibatches of size 128, learning rate 10\u22123, and momentum 0.9:\ng = 0.9 \u00b7 g + \u2211\nn\u2208minibatch\n\u2202Ln\n\u2202 {V,W,b} ; {V,W,b} = {V,W,b} \u2212 10\u22123 \u00b7 g"}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Performance Evaluation", "text": "We first provide an overview of the model\u2019s performance at different sizes, with both untied and tied weights, in order to examine basic trends and compare with other current systems. For CIFAR-10, we tested the models using M = 32, 64, 128, or 256 feature maps per layer, and L = 1, 2, 4, 8, or 16 layers beyond the first. For SVHN, we used M = 32, 64, 128, or 256 feature maps and L = 1, 2, 4, or 8 layers beyond the first. That we were able to train networks at these large depths is due to the initialization of all W lm to the identity: this initially copies activations at the first layer up to the last layer, and gradients from the last layer to the first. Both untied and tied models had trouble learning with zero-centered Gaussian initializations at some of the larger depths.\nResults are shown in Figs. 2 and 3. Here, we plot each condition on a grid according to numbers of feature maps and layers. To the right of each point, we show the test error (top) and training error (bottom). Contours show curves with a constant number of parameters: in the untied case, the number of parameters is determined by the number of feature maps and layers, while in the tied case it is determined solely by the number of feature maps; Section 3.2.1 examines the behavior along these lines in more detail.\nFirst, we note that despite the simple architecture of our model, it still achieves competitive performance on both datasets, relative to other models that, like ours, do not use any image transformations or other regularizations such as dropout [10, 28], stochastic pooling [29] or maxout [5] (see Table 1). Thus our simplifications do not entail a departure from current methods in terms of performance.\nWe also see roughly how the numbers of layers, feature maps and parameters affect performance of these models at this range. In particular, increasing any of them tends to improve performance, particularly on the training set (a notable exception to CIFAR-10 at 16 layers in the tied case, which goes up slightly). We now examine the independent effects of each of these variables in detail."}, {"heading": "3.2 Effects of the Numbers of Feature maps, Parameters and Layers", "text": "In a traditional untied convolutional network, the number of feature maps M , layers L and parameters P are interrelated: Increasing the number of feature maps or layers increases the total number of parameters in addition to the representational power gained by higher dimensionality (more feature maps) or greater nonlinearity (more layers). But by using the tied version of our model, we can investigate the effects of each of these three variables independently.\nTo accomplish this, we consider the following three cases, each of which we investigate with the described setup:\n1. Control for M and P , vary L: Using the tied model (constant M and P ), we evaluate performance for different numbers of layers L.\n2. Control for M and L, vary P : Compare pairs of tied and untied models with the same numbers of feature maps M and layers L. The number of parameters P increases when going from tied to untied model for each pair.\n3. Control for P and L, vary M : Compare pairs of untied and tied models with the same number of parameters P and layers L. The number of feature maps M increases when going from the untied to tied model for each pair.\nNote the number of parameters P is equal to the total number of independent weights and biases over all layers, including initial feature extraction and classification. This is given by the formula below for the untied model (for the tied case, substitute L = 1 regardless of the number of layers):\nP = 8 \u00b7 8 \u00b7 3 \u00b7M + 3 \u00b7 3 \u00b7M2 \u00b7 L + M \u00b7 (L+ 1) + 64 \u00b7M \u00b7 10 + 10 (first layer) (higher layers) (biases) (classifier)"}, {"heading": "3.2.1 Case 1: Number of Layers", "text": "We examine the first of these cases in Fig. 4. Here, we plot classification performance at different numbers of layers using the tied model only, which controls for the number of parameters. A different curve is shown for different numbers of feature maps. For both CIFAR-10 and SVHN, performance gets better as the number of layers increases, although there is an upward tick at 8 layers for CIFAR-10 test error. The predominant cause of this appears to be overfitting, since the training error still goes down. At these depths, therefore, adding more layers alone tends to increase performance, even though no additional parameters are introduced. This is because additional layers allow the network to learn more complex functions by using more nonlinearities.\nThis conclusion is further supported by Fig. 5, which shows performance of the untied model according to numbers of parameters and layers. Note that vertical cross-sections of this figure correspond to the constant-parameter contours of Fig. 2. Here, we can also see that for any given number of parameters, the best performance is obtained with a deeper model. The exception to this is again the 8-layer models on CIFAR-10 test error, which suffer from overfitting.\nExperiment 1a: Error by Layers and Features (tied model)\nExperiment 1b: Error by Parameters and Layers (untied model)"}, {"heading": "3.2.2 Case 2: Number of Parameters", "text": "To vary the number of parameters P while holding fixed the number of feature mapsM and layersL, we consider pairs of tied and untied models where M and L remain the same within each pair. The number of parameters P is then greater for the untied model.\nThe result of this comparison is shown in Fig. 6. Each point corresponds to a model pair; we show classification performance of the tied model on the x axis, and performance of the untied model on the y axis. Since the points fall below the y = x line, classification performance is better for the untied model than it is for the tied. This is not surprising, since the untied model has more total parameters and thus more flexibility. Note also that the two models converge to the same test performance as classification gets better \u2014 this is because for the largest numbers of L and M , both models have enough flexibility to achieve maximum test performance and begin to overfit.\nExperiment 2: Same Feature Maps and Layers, Varied Parameters"}, {"heading": "3.2.3 Case 3: Number of Feature Maps", "text": "We now consider the third condition from above, the effect of varying the number of feature mapsM while holding fixed the numbers of layers L and parameters P .\nFor a given L, we find model pairs whose numbers of parameters P are very close by varying the number of feature maps. For example, an untied model with L = 3 layers andM = 71 feature maps has P = 195473 parameters, while a tied model with L = 3 layers and M = 108 feature maps has P = 195058 parameters \u2014 a difference of only 0.2%. In this experiment, we randomly sampled model pairs having the same number of layers, and where the numbers of parameters were within 1.0% of each other. We considered models where the number of layers beyond the first was between 2 and 8, and the number of feature maps was between 16 and 256 (for CIFAR-10) or between 16 and 150 (for SVHN).\nFig. 7 shows the results. As before, we plot a point for each model pair, showing classification performance of the tied model on the x axis, and of the untied model on the y axis. This time, however, each pair has fixed P and L, and tied and untied models differ in their number of feature maps M . We find that despite the different numbers of feature maps, the tied and untied models\nExperiment 3: Same Parameters and Layers, Varied Feature Maps\nperform about the same in each case. Thus, performance is determined by the number of parameters and layers, and is insensitive to the number of feature maps."}, {"heading": "4 Discussion", "text": "Above we have demonstrated that while the numbers of layers and parameters each have clear effects on performance, the number of feature maps has little effect, once the number of parameters is taken into account. This is perhaps somewhat counterintuitive, as we might have expected the use of higher-dimensional representations to increase performance; instead we find that convolutional layers are insensitive to this size.\nThis observation is also consistent with Fig. 5: Allocating a fixed number of parameters across multiple layers tends to increase performance compared to putting them in few layers, even though this comes at the cost of decreasing the feature map dimension. This is precicesly what one might expect if the number of feature maps had little effect compared to the number of layers.\nOur analysis employed a special tied architecture and comes with some important caveats, however. First, while the tied architecture serves as a useful point of comparison leading to several interesting conclusions, it is new and thus its behaviors are still relatively unknown compared to the common untied models. This may particularly apply to models with a large number of layers (L > 8), or very small numbers of feature maps (M < 16), which have been left mostly unexamined in this paper. Second, our experiments all used a simplified architecture, with just one layer of pooling. While we believe the principles found in our experiments are likely to apply in more complex cases as well, this is unclear and requires further investigation to confirm.\nThe results we have presented provide empirical confirmation within the context of convolutional layers that increasing layers alone can yield performance benefits (Experiment 1a). They also indicate that filter parameters may be best allocated in multilayer stacks (Experiments 1b and 3), even at the expense of having fewer feature maps. In conjunction with this, we find the feature map dimension itself has little effect on convolutional layers\u2019 performance, with most sizing effects coming from the numbers of layers and parameters (Experiments 2 and 3). Thus, focus would be best placed on these variables when determining model architectures."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Flexible", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "high performance convolutional neural networks for image classification. In IJCAI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "ICML, volume 8, pages 921\u2013928", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS, volume 15, pages 315\u2013323", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Maxout networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-prediction deep boltzmann machines", "author": ["I.J. Goodfellow", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel connectionist system for improved unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "PAMI, 31(5)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "ICASSP", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastave", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical Report TR-2009, University of Toronto", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A. Ng"], "venue": "ICML, volume 26", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML, pages 807\u2013814", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS Workshop", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "37(23):3311\u20133325", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Discriminative recurrent sparse auto-encoders", "author": ["J. Rolfe", "Y. LeCun"], "venue": "ICLR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse coding via thresholding and local competition in neural circuits", "author": ["C.J. Rozell", "D.H. Johnson", "R.G. Baraniuk", "B.A. Olshausen"], "venue": "Neural Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Training recurrent networks by evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F. Gomez"], "venue": "Neural Computation, 19(3):757\u2013779", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "ICPR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Overfeat: Integrated recognition", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "localization and detection using convolutional networks. http://arxiv.org/abs/1312.6229", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical bayesian optimzation of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R. Adams"], "venue": "NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional-Recursive Deep Learning for 3D Object Classification", "author": ["R. Socher", "B. Huval", "B. Bhat", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "C.C. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporal kernel recurrent neural networks", "author": ["I. Sutskever", "G. Hinton"], "venue": "Neural Networks, 23:239\u2013243", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "Z. Sixin", "Y. LeCun", "R. Fergus"], "venue": "ICML", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic pooling", "author": ["M. Zeiler", "R. Fergus"], "venue": "ICLR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Arxiv.org 1131.2901v3", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Convolutional networks have recently made significant progress in a variety of image classification and detection tasks [13, 2, 23], with further gains and applications continuing to be realized.", "startOffset": 135, "endOffset": 146}, {"referenceID": 1, "context": "1 Introduction Convolutional networks have recently made significant progress in a variety of image classification and detection tasks [13, 2, 23], with further gains and applications continuing to be realized.", "startOffset": 135, "endOffset": 146}, {"referenceID": 22, "context": "1 Introduction Convolutional networks have recently made significant progress in a variety of image classification and detection tasks [13, 2, 23], with further gains and applications continuing to be realized.", "startOffset": 135, "endOffset": 146}, {"referenceID": 12, "context": "Notably, multiple layers of unpooled convolution [13, 30] have been utilized lately with considerable success.", "startOffset": 49, "endOffset": 57}, {"referenceID": 29, "context": "Notably, multiple layers of unpooled convolution [13, 30] have been utilized lately with considerable success.", "startOffset": 49, "endOffset": 57}, {"referenceID": 12, "context": "[13] for ImageNet classification has five convolutional layers which turn out to be key to its performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "In [30], Zeiler and Fergus reimplemented this model and adjusted different parts in turn.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "These are are well-studied models [11, 21, 27], naturally suited to temporal and sequential data.", "startOffset": 34, "endOffset": 46}, {"referenceID": 20, "context": "These are are well-studied models [11, 21, 27], naturally suited to temporal and sequential data.", "startOffset": 34, "endOffset": 46}, {"referenceID": 26, "context": "These are are well-studied models [11, 21, 27], naturally suited to temporal and sequential data.", "startOffset": 34, "endOffset": 46}, {"referenceID": 7, "context": "For example, they have recently been shown to deliver excellent performance for phoneme recognition [8] and cursive handwriting recognition [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "For example, they have recently been shown to deliver excellent performance for phoneme recognition [8] and cursive handwriting recognition [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 25, "context": "[26] showed how image segments could be recursively merged to perform scene parsing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "More recently [25], they used a convolutional network in a separate stage to first learn features on RGB-Depth data, prior to hierarchical merging.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Sparse coding [18] uses iterative algorithms, such as ISTA [1], to perform inference.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Sparse coding [18] uses iterative algorithms, such as ISTA [1], to perform inference.", "startOffset": 59, "endOffset": 62}, {"referenceID": 19, "context": "[20] showed how the ISTA scheme can be unwrapped into a repeated series of network layers, which can be viewed as a recursive net.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Gregor & LeCun [9] showed how to backpropagate through such a network to give fast approximations to sparse coding known as LISTA.", "startOffset": 15, "endOffset": 18}, {"referenceID": 18, "context": "Rolfe & LeCun [19] then showed in their DrSAE model how a discriminative term can be added.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "There also are interesting relationships with convolutional Deep Belief Networks [15], as well as Multi-Prediction Deep Boltzmann Machines [6].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "There also are interesting relationships with convolutional Deep Belief Networks [15], as well as Multi-Prediction Deep Boltzmann Machines [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "As pointed out by [6], mean field inference in such models can be unrolled and viewed as a type of recurrent network.", "startOffset": 18, "endOffset": 21}, {"referenceID": 14, "context": "In contrast to the model we use, however, [15] trains unsupervised using contrastive divergence, while [6] is nonconvolutional and focuses on conditioning on random combinations of inputs and targets.", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "In contrast to the model we use, however, [15] trains unsupervised using contrastive divergence, while [6] is nonconvolutional and focuses on conditioning on random combinations of inputs and targets.", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "Our investigation is based on a multilayer Convolutional Network [14], for which all layers beyond the first have the same size and connection topology.", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "All layers use rectified linear units (ReLU) [3, 4, 16].", "startOffset": 45, "endOffset": 55}, {"referenceID": 3, "context": "All layers use rectified linear units (ReLU) [3, 4, 16].", "startOffset": 45, "endOffset": 55}, {"referenceID": 15, "context": "All layers use rectified linear units (ReLU) [3, 4, 16].", "startOffset": 45, "endOffset": 55}, {"referenceID": 8, "context": "This novel recursive, convolutional architecture is reminiscent of LISTA [9], but without a direct projection from the input to each hidden layer.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "1 Instantiation on CIFAR-10 and SVHN We describe our models for the CIFAR-10 [12] and SVHN [17] datasets used in our experiments.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "1 Instantiation on CIFAR-10 and SVHN We describe our models for the CIFAR-10 [12] and SVHN [17] datasets used in our experiments.", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "First, we note that despite the simple architecture of our model, it still achieves competitive performance on both datasets, relative to other models that, like ours, do not use any image transformations or other regularizations such as dropout [10, 28], stochastic pooling [29] or maxout [5] (see Table 1).", "startOffset": 246, "endOffset": 254}, {"referenceID": 27, "context": "First, we note that despite the simple architecture of our model, it still achieves competitive performance on both datasets, relative to other models that, like ours, do not use any image transformations or other regularizations such as dropout [10, 28], stochastic pooling [29] or maxout [5] (see Table 1).", "startOffset": 246, "endOffset": 254}, {"referenceID": 28, "context": "First, we note that despite the simple architecture of our model, it still achieves competitive performance on both datasets, relative to other models that, like ours, do not use any image transformations or other regularizations such as dropout [10, 28], stochastic pooling [29] or maxout [5] (see Table 1).", "startOffset": 275, "endOffset": 279}, {"referenceID": 4, "context": "First, we note that despite the simple architecture of our model, it still achieves competitive performance on both datasets, relative to other models that, like ours, do not use any image transformations or other regularizations such as dropout [10, 28], stochastic pooling [29] or maxout [5] (see Table 1).", "startOffset": 290, "endOffset": 293}, {"referenceID": 23, "context": "[24] 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] 15.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] 16.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "6 Coates & Ng [3] 18.", "startOffset": 14, "endOffset": 17}, {"referenceID": 28, "context": "1 Zeiler & Fergus (max pool) [29] 3.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "[22] 4.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.", "creator": "LaTeX with hyperref package"}}}