{"id": "1312.4637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2013", "title": "Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations", "abstract": "lp relaxation - based message passing algorithms provide an useful tool for branching inference via probabilistic graphical representations. however, different lp relaxations often have different objective functions and variables of differing dimensions, which presents a barrier to effective comparison and analysis. in addition, the computational complexity of lp relaxation - based methods raises quickly with the number constraint constraints. reducing the number of constraints without consuming the quality requiring the solutions is thus desirable.", "histories": [["v1", "Tue, 17 Dec 2013 04:44:04 GMT  (16118kb,D)", "https://arxiv.org/abs/1312.4637v1", null], ["v2", "Mon, 21 Apr 2014 09:30:20 GMT  (12599kb,D)", "http://arxiv.org/abs/1312.4637v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["zhen zhang", "qinfeng shi", "yanning zhang", "chunhua shen", "anton van den hengel"], "accepted": false, "id": "1312.4637"}, "pdf": {"name": "1312.4637.pdf", "metadata": {"source": "CRF", "title": "Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations", "authors": ["Zhen Zhang", "Qinfeng Shi", "Chunhua Shen", "Anton van den Hengel"], "emails": [], "sections": [{"heading": null, "text": "We propose a unified formulation under which existing MAP LP relaxations may be compared and analysed. Furthermore, we propose a new tool called Marginal Polytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited such as node redundancy and edge equivalence. We show that using Marginal Polytope Diagrams allows the number of constraints to be reduced without loosening the LP relaxations. Then, using Marginal Polytope Diagrams and constraint reduction, we develop three novel message passing algorithms, and demonstrate that two of these show a significant improvement in speed over state-of-art algorithms while delivering a competitive, and sometimes higher, quality of solution.\nKeywords Constraint Reduction \u00b7 Higher Order Potential \u00b7 Message Passing \u00b7 Probabilistic Graphical Models \u00b7 MAP inference\n1 Introduction\nLinear Programming (LP) relaxations have been used to approximate the maximum a posteriori (MAP) inference of Probabilistic Graphical Models (PGMs)\n1. School of Computer Science and Technology, Northwestern Polytechnical University, Xi\u2019an, China, 710129 Shaanxi Provincial Key Laboratory of Speech & Image Information Processing, Xi\u2019an, China, 710129 2. School of Computer Science, the University of Adelaide, Adelaide, Australia, SA, 5005\nar X\niv :1\n31 2.\n46 37\nv2 [\ncs .C\nV ]\n2 1\nA pr\n2 01\n2 Z. Zhanget al.\n[7] by enforcing local consistency over edges or clusters. An attractive property of this approach is that it is guaranteed to find the optimal MAP solution when the labels are integers. This is particularly significant in light of the fact that Kumar et al. showed that LP relaxation provides a better approximation than Quadratic Programming relaxation and Second Order Cone Programming relaxation [12]. Despite their success, there remain a variety of large-scale problems that off-the-shelf LP solvers can not solve [25]. Moreover, it has been shown [25, 19] that LP relaxations have a large gap between the dual objective and the decoded primal objective and fail to find the optimal MAP solution in many real-world problems.\nIn response to this shortcoming a number of dual message passing methods have been proposed including Dual Decompositions [10, 20, 21] and Generalised Max Product Linear Programming (GMPLP) [3]. These methods can still be computationally expensive when there are a large number of constraints in the LP relaxations. It is desirable to reduce the number of constraints in order to reduce computational complexity without sacrificing the quality of the solution. However, this is non-trivial, because for a MAP inference problem the dimension of the primal variable can be different in various LP relaxations. This also presents a barrier for effectively comparing the quality of two LP relaxations and their corresponding message passing methods. Furthermore, these message-passing methods may get stuck in non-optimal solutions due to the non-smooth dual objectives [18, 4, 16].\nOur contributions are: 1) we propose a unified form for MAP LP relaxations, under which existing MAP LP relaxations can be rewritten as constrained optimisation problems with variables of the same dimension and objective; 2) we present a new tool which we call the Marginal Polytope Diagram to effectively compare different MAP LP relaxations. We show that any MAP LP relaxation in the above unified form has a Marginal Polytope Diagram, and vice versa. We establish propositions to conveniently show the equivalence of seemingly different Marginal Polytope Diagrams; 3) Using Marginal Polytope Diagrams, we show how to safely reduce the number of constraints (and consequently the number of messages) without sacrificing the quality of the solution, and propose three new message passing algorithms in the dual; 4) we show how to perform message passing in the dual without computing and storing messages (via updating the beliefs only and directly); 5) we propose a new cluster pursuit strategy.\n2 MAP Inference and LP Relaxations\nWe consider MAP inference over factor graphs with discrete states. For generality, we will use higher order potentials (where possible) throughout the paper.\nConstraint Reduction using Marginal Polytope Diagrams 3\n2.1 MAP inference\nAssume that there are n variables X1, \u00b7 \u00b7 \u00b7 , Xn, each taking discrete states xi \u2208 Vals(Xi). Let V = {1, \u00b7 \u00b7 \u00b7 , n} denote the node set, and let C be a collection of subsets of V. C has an associated group of potentials \u03b8 = {\u03b8c(xc) \u2208 R|c \u2208 C}, where xc = [xi]i\u2208c. Given a graph G = (V,C) and potentials \u03b8, we consider the following exponential family distribution [22]:\np(x |\u03b8) = 1 Z exp\n(\u2211\nc\u2208C \u03b8c(xc)\n) , (1)\nwhere x = [x1, x2, . . . , xn] \u2208 X, and Z = \u2211 x\u2208X exp( \u2211 c\u2208C \u03b8c(xc)) is known as a normaliser, or partition function. The goal of MAP inference is to find the MAP assignment, x\u2217, that maximises p(x |\u03b8). That is\nx\u2217 = argmax x\n\u2211 c\u2208C \u03b8c(xc). (2)\nHere we slightly generalise the notation of xc to xs = [xi]i\u2208s, xt = [xi]i\u2208t and xf = [xi]i\u2208f where s, t, f are subsets of V reserved for later use.\n2.2 Linear Programming Relaxations\nBy introducing\n\u00b5 = (\u00b5c(xc))c\u2208C, (3)\nthe MAP inference problem can be written as an equivalent Linear Programming (LP) problem as follows\n\u00b5\u2217 = argmax \u00b5\u2208M(G)\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc), (4)\nin which the feasible set, M(G), is known as the marginal polytope [22], defined as follows\n  \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223 q(x) > 0, \u2200x\u2211 x q(x) = 1\u2211 xV \\c q(x) = \u00b5c(xc),\u2200c \u2208 C,xc    . (5)\nHere the first two groups of constraints specify that q(x) is a distribution over X, and we refer to the last group of constraints as the global marginalisation constraint, which guarantees that for arbitrary \u00b5 in M(G), all \u00b5c(xc), c \u2208 C can be obtained by marginalisation from a common distribution q(x) over X. In general, exponentially many inequality constraints (i.e. q(x) > 0, \u2200x) are required to define a marginal polytope, which makes the LP hard to solve.\n4 Z. Zhanget al.\nThus (4) is often relaxed with a local marginal polytope ML(G) to obtain the following LP relaxation\n\u00b5\u2217 = argmax \u00b5\u2208ML(G)\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc). (6)\nDifferent LP relaxation schemes define different local marginal polytopes. A typical local marginal polytope defined in [20, 4] is as follows:\n  \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 xc\\{i} \u00b5c(xc) = \u00b5i(xi),\u2200c \u2208 C, i \u2208 c, xi \u00b5c(xc) > 0,\n\u2211 xc \u00b5c(xc) = 1,\u2200c \u2208 C,xc\n   . (7)\nCompared to the marginal polytope, for arbitrary \u00b5 in a local marginal polytope, all \u00b5c(xc) may not be the marginal distributions of a common distribution q(x) over X, but there are much fewer constraints in local marginal polytope. As a result, the LP relaxation can be solved more efficiently. This is of particular practical significance because state-of-the-art interior point or simplex LP solvers can only handle problems with up to a few hundred thousand variables and constraints while many real-world datasets demand far more variables and constraints [25, 12].\nSeveral message passing-based approximate algorithms [3, 19, 21] have been proposed to solve large scale LP relaxations. Each of them applies coordinate descent to the dual objective of an LP relaxation problem with a particular local marginal polytope. Different local marginal polytopes use different local marginalisation constraints, which leads to different dual problems and hence different message updating schemes.\n2.3 Generalised Max Product Linear Programming\nGloberson and Jaakkola [3] showed that LP relaxations can also be solved by message passing, known as Max Product LP (MPLP) when only node and edge potentials are considered, or Generalised MPLP (GMPLP) (see Section 6 of [3]) when potentials over clusters are considered.\nIn GMPLP, they define I = {s|s = c \u2229 c\u2032; c, c\u2032 \u2208 C}, and \u00b5g = (\u00b5c(xc), \u00b5s(xs))c\u2208C,s\u2208I. (8)\nThen they consider the following LP relaxation\n\u00b5g\u2217 = argmax \u00b5g\u2208MgL(G)\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc), (9)\nwhere the local marginal polytope MgL(G) is defined as\n   \u00b5g \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u00b5c(xc) > 0,\u2200c \u2208 C,xc\u2211 xc \u00b5c(xc) = 1, \u2200c \u2208 C \u2211 xc\\s \u00b5c(xc) = \u00b5s(xs),\u2200c \u2208 C, s \u2208 Sg(c),xs   \n(10)\nConstraint Reduction using Marginal Polytope Diagrams 5\nwith Sg(c) = {s|s \u2208 I, s \u2286 c}. To derive a desirable dual formulation, they replace the third group of constraints with the following equivalent constraints\n\u00b5c\\s,s(xc\\s,xs) = \u00b5c(xc), \u2200c \u2208 C, s \u2208 Sg(c),xc,\u2211\nxc\\s\n\u00b5c\\s,s(xc\\s,xs) = \u00b5s(xs), \u2200c \u2208 C, s \u2208 Sg(c),xs\nwhere \u00b5c\\s,s(xc\\s,xs) is known as the copy variable. Let \u03b2c\\s,s(xc\\s,xs) be the dual variable associated with the first group of the new constraints above, using standard Lagrangian yields the following dual problem:\nmin \u03b2\n\u2211 s\u2208I max xs\n\u2211\nc\u2208C,s\u2208Sg(c) max xc\\s \u03b2c\\s,s(xc\\s,xs)\ns.t. \u03b8c(xc) = \u2211\ns\u2208Sg(c) \u03b2c\\s,s(xc\\s,xs), \u2200c \u2208 C,xc . (11)\nLet \u03bbc\u2192s(xs) = maxxc\\s \u03b2c\\s,s(xc\\s,xs), they use a coordinate descent method to minimise the dual by picking up a particular c \u2208 C and updating all \u03bb\u2217c\u2192s(xs) as following:\n\u03bb\u2217c\u2192s(xs) = \u2212\u03bb\u2212cs (xs)+ 1 |Sg(c)| max xc\\s [ \u03b8c(xc)+ \u2211\ns\u0302\u2208Sg(c) \u03bb\u2212cs\u0302 (xs\u0302)\n] ,\u2200s \u2208 Sg(c),xs (12)\nwhere \u03bb\u2212cs (xs) = \u2211 c\u0302\u2208{c\u0304|c\u0304\u2208C,c\u0304 6=c,s\u2208Sg(c\u0304)} \u03bbc\u0302\u2192s(xs). At each iteration the dual objective always decreases, thus guaranteeing convergence. Under certain conditions GMPLP finds the exact solution. Sontag et al. [19] extended this idea by iteratively adding clusters and reported faster convergence empirically.\n2.4 Dual Decomposition\nDual Decomposition [10, 20] explicitly splits node potentials (those potentials of order 1) from cluster potentials with order greater than 1, and rewrites the MAP objective (2) as \u2211\ni\u2208V \u03b8i(xi) +\n\u2211 f\u2208F \u03b8f (xf ), (13)\nwhere F = {f |f \u2208 C, |f | > 1}. By defining \u00b5d = (\u00b5i(xi), \u00b5f (xf ))i\u2208V,f\u2208F, they consider the following LP relaxation:\nmax \u00b5d\u2208MdL(G)\nfd(\u00b5 d)\nfd(\u00b5 d) =\n\u2211\ni\u2208V\n\u2211\nxi\n\u00b5i(xi)\u03b8i(xi) + \u2211\nf\u2208F\n\u2211\nxf\n\u00b5f (xf )\u03b8f (xf ) (14)\n6 Z. Zhanget al.\nwith a different local marginal polytope MdL(G) defined as\n{ \u00b5d > 0 \u2223\u2223\u2223\u2223 \u2211 xi \u00b5i(xi) = 1,\u2200i \u2208 V\u2211\nxf/{i} \u00b5f (xf ) = \u00b5i(xi),\u2200f \u2208 F, i \u2208 f, xi\n} . (15)\nLet \u03bbfi(xi) be the Lagrangian multipliers corresponding to each \u2211\nxf\\{i} \u00b5f (xf ) =\n\u00b5i(xi) for each f \u2208 F, i \u2208 f, xi, one can show that the standard Lagrangian duality is\nL(\u03bb) = \u2211\ni\u2208V max xi\n( \u03b8i(xi) +\n\u2211\nf\u2208{f \u2032|f \u2032\u2208F,i\u2208f \u2032} \u03bbfi(xi)\n)\n+ \u2211\nf\u2208F max xf\n( \u03b8f (xf )\u2212 \u2211\ni\u2208f \u03bbfi(xi)\n) . (16)\nSubgradient or coordinate descent can be used to minimise the dual objective. Since the Dual Decomposition using coordinate descent is closely related to GMPLP and the unified form which we will present, we give the update rule derived by coordinate descent below,\n\u03bb\u2217fi(xi) = \u2212\u03b8i(xi)\u2212 \u03bb\u2212fi (xi)+ 1\n|f | maxxf\\{i} [ \u03b8f (xf )+ \u2211\ni\u0302\u2208f \u03b8i\u0302(xi\u0302)+\n\u2211 i\u0302\u2208f \u03bb\u2212f i\u0302 (xi\u0302) ] ,\u2200i\u2208f,xi (17)\nwhere f is a particular cluster from F, and \u03bb\u2212fi (xi) = \u2211 f\u0302\u2208{f\u0304 |f\u0304\u2208F,f\u0304 6=f,i\u2208f\u0304} \u03bbf\u0302 i(xi).\nCompared to GMPLP, the local marginal polytope in the Dual Decomposition has much fewer constraints. In general for an arbitrary graph G = (V,C), MdL(G) is looser than M g L(G) (i.e. M d L(G) \u2287MgL(G)) .\n2.5 Dual Decomposition with cycle inequalities\nRecently, Sontag et al. [21] proposed a Dual Decomposition with cycle inequalities considering the following LP relaxation\n\u00b5d\u2217 = max \u00b5\u2208MoL(G) fd(\u00b5 d) (18)\nwith a local marginal polytope MoL(G),    \u00b5d \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 xi \u00b5i(xi) = 1,\u2200i \u2208 V, xi\u2211 xf/{i} \u00b5f (xf )=\u00b5i(xi),\u2200f \u2208 F, i \u2208 f, xi \u2211 xf/e \u00b5f (xf )=\u00b5e(xe),\u2200f, e\u2208F, e\u2282f,xe, |e|=2, |f |\u22653\n\u00b5d > 0\n  \nThey added cycle inequalities to tighten the problem. Reducing the primal feasible set may reduce the maximum primal objective, which reduces the\nConstraint Reduction using Marginal Polytope Diagrams 7\nminimum dual objective. They showed that finding the \u201ctightest\u201d cycles, which maximise the decrease in the dual objective, is NP-hard. Thus, instead, they looked for the most \u201cfrustrated\u201d cycles, which correspond to the cycles with the smallest LHS of their cycle inequalities. Searching for \u201cfrustrated\u201d cycles, adding the cycles\u2019 inequalities and updating the dual is repeated until the algorithm converges.\n3 A Unified View of MAP LP Relaxations\nIn different LP relaxations, not only the formulations of the objective, but also the dimension of primal variable may vary, which makes comparison difficult. By way of illustration, note that the primal variable in GMPLP is \u00b5g = (\u00b5c(xc), \u00b5s(xs))c\u2208C,s\u2208I, while in Dual Decomposition the primal variable is \u00b5d = {\u00b5i(xi), \u00b5f (xf )}i\u2208V,f\u2208F. Although \u00b5d can be reformulated to (\u00b5c(xc))c\u2208C if C = {{i}|i \u2208 V} \u222a F, the variables {\u00b5s(xs)}s\u2208I (corresponding to intersections) in GMPLP still do not appear in Dual Decomposition. This shows that the dimensions of the primal variables in GMPLP and Dual Decomposition are different.\n3.1 A Unified Formulation\nWhen using the local marginal polytope the objective of the LP relaxation depends only on those \u00b5c(xc), c \u2208 C. We thus reformulate the LP Relaxation into a unified formulation as follows:\n\u00b5\u2217 = argmax \u00b5\u2208ML(G,C\u2032,S(C\u2032))\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc), (19)\nwhere \u00b5 is defined in (3). The local marginal polytope, ML(G,C \u2032, S(C\u2032)), can be defined in a unified formulation as\n   \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u00b5c(xc) > 0,\u2200c \u2208 C\u2032,xc\u2211 xc \u00b5c(xc) = 1,\u2200c \u2208 C\u2032 \u2211 xc\\s \u00b5c(xc) = \u00b5s(xs),\u2200c \u2208 C\u2032, s \u2208 S(c),xs    . (20)\nHere C\u2032 is what we call an extended cluster set, where each c \u2208 C\u2032 is called an extended cluster. S(C\u2032) = (S(c))c\u2208C\u2032 , where each s \u2208 S(c) is a subset of c, which we refer to as a sub-cluster. The choices of C\u2032 and S(C\u2032) correspond to existing or even new inference algorithms, which will be shown later, and when specifying C\u2032 and S(C\u2032), we require C\u2032 \u222a(\u222ac\u2208C\u2032 S(c)) \u2287 C.\nThe first two groups of constraints in ML(G,C \u2032, S(C\u2032)) ensure that \u2200c \u2208 C\u2032, \u00b5c(xc) is a distribution over Vals(xc). We refer to the third group of constraints as local marginalisation constraints.\n8 Z. Zhanget al.\nRemarks The LP formulation in (1) and (2) of [24] may look similar to ours. However, the work of [24] is in fact a special case of ours. In their work, an additional restriction s \u2282 c for (20) must be satisfied (see (4) in [24]). As a result, their work does not cover the LP relaxations in [19] and GMPLP, where redundant constraints like \u00b5c(xc) = \u00b5c(xc) are used to derive a message from one cluster to itself (see Figure 1 of [19]). Our approach, however, is in fact a generalisation of [19], GMPLP and [24].\n3.2 Reformulating GMPLP and Dual Decomposition\nHere we show that both GMPLP and Dual Decomposition can be reformulated by (19).\nLet us start with GMPLP first. Let C\u2032 be C and S(C\u2032) be Sg(C) = (Sg(c))c\u2208C. GMPLP (9) can be reformulated as follows\n\u00b5\u2217 = argmax \u00b5\u2208ML(G,C,Sg(C))\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc), (21)\nwhere ML(G,C, Sg(C)) is defined as\n   \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u00b5c(xc) > 0,\u2200c \u2208 C,xc\u2211 xc \u00b5c(xc) = 1,\u2200c \u2208 C \u2211 xc\\s \u00b5c(xc) = \u00b5s(xs),\u2200c \u2208 C, s \u2208 Sg(c),xs    . (22)\nWe can see that (22) and (10) only differ in the dimensions of their variables \u00b5 and \u00b5g (see (3) and (8)). Since the objectives in (21) and (9) do not depend on \u00b5s(xs), s \u2208 I directly, the solutions of the two optimisation problems (21) and (9) are the same on \u00b5.\nFor Dual Decomposition, we let Cd = {{i}|i \u2208 V} \u222a F, and\nSd(c) = { \u2205, |c| = 1 {{i}|i \u2208 C} |c| > 1 . (23)\nLet C\u2032 be Cd and S(C \u2032) be Sd(Cd) = (Sd(c))c\u2208Cd . Dual Decomposition (14) can be reformulated as\n\u00b5\u2217 = argmax \u00b5\u2208ML(G,Cd,Sd(Cd))\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc), (24)\nwhere ML(G,Cd, Sd(Cd)) is defined as\n   \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u00b5c(xc) > 0, \u2200c \u2208 Cd,xc\u2211 xc \u00b5c(xc) = 1, \u2200c \u2208 Cd \u2211 xc\\s \u00b5c(xc) = \u00b5s(xs), \u2200c \u2208 Cd, s \u2208 Sd(c),xs    . (25)\nConstraint Reduction using Marginal Polytope Diagrams 9\nSimilarly, for Dual Decomposition with cycle inequalities in (18), we define So(c) as follows\nSo(c) =    \u2205, |c| = 1 {{i, j}|{i, j} \u2282 c} |c| = 3 {{i}} |c| > 1, |c| 6= 3 . (26)\nLet C\u2032 be Cd and S(C \u2032) be So(Cd) = (So(c))c\u2208Cd , we reformulate the problem in (18) as\n\u00b5\u2217 = argmax \u00b5\u2208ML(G,Cd,So(Cd))\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc). (27)\n3.3 Generalised Dual Decomposition\nNote that MdL(G) and M o L(G) in Dual Decomposition are looser than M g L(G). This suggests that for some \u03b8 Dual Decomposition may achieve a lower quality solution or slower convergence (in terms of number of iterations) than GMPLP 1. We show using the unified formulation of LP Relaxation in (19), Dual Decomposition can be derived on arbitrary local marginal polytopes (including those tighter than MgL(G), M d L(G) and M o L(G)). We refer to this new type of Dual Decomposition as Generalised Dual Decomposition (GDD), which forms a basic framework for more efficient algorithms to be presented in Section 6.\n3.3.1 GDD Message Passing\nLet \u03bbc\u2192s(xs) be the Lagrangian multipliers (dual variables) corresponding to the local marginalisation constraints \u2211 xc\\s\n\u00b5c(xc) = \u00b5s(xs) for each c \u2208 C\u2032, s \u2208 S(c),xs. Define\nT = C\u2032 \u222a[ \u222a c\u0302\u2208C\u2032 S(c\u0302)], (28)\nand the following variables \u2200t \u2208 T,xt:\n\u03b8\u0302t(xt) = 1(t \u2208 C)\u03b8t(xt) , (29a) \u03b3t(xt) = 1(t \u2208 C\u2032)\n\u2211 s\u0302\u2208S(t)\\{t} \u03bbt\u2192s\u0302(xs\u0302) , (29b)\n\u03bbt(xt) = \u2211\nc\u2208{c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}} \u03bbc\u2192t(xt) , (29c)\nbt(xt) = \u03b8\u0302t(xt) + \u03bbt(xt)\u2212 \u03b3t(xt) , (29d) 1 This does not contradict the result reported in [21], where Dual Decomposition with cycle inequalities converges faster in terms of running time than GMPLP. In [21], on all their datasets MoL(G) = M g L(G) as the order of clusters are at most 3. Dual Decomposition with cycle inequalities runs faster because it has a better cluster pursuit strategy. On datasets with higher order potentials, it may have worse performance than GMPLP.\n10 Z. Zhanget al.\nwhere 1(S) is the indicator function, which is equal to 1 if the statement S is true and 0 otherwise. Define \u03bb = (\u03bbc\u2192s(xs))c\u2208C\u2032,s\u2208S(c), we have the dual problem (see derivation in Section 1 of the supplementary).\ng(\u03bb) = max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n[\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc)+\n\u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\n\u2211\nxs\n( \u00b5s(xs)\u2212 \u2211\nxc\\s\n\u00b5c(xc) ) \u03bbc\u2192s(xs) ]\n= \u2211\nt\u2208T max xt bt(xt). (30)\nIn (30), if c \u2208 S(c) for some c \u2208 C\u2032, the variable \u03bbc\u2192c(xc) will always be cancelled out2. As a result, \u03bbc\u2192c(xc) can be set to arbitrary value. To optimise (30), we use coordinate descent. For any c \u2208 C\u2032 fixing all \u03bbc\u2032\u2192s(xs), c\u2032 \u2208 C\u2032, s \u2208 S(c\u2032) except \u03bbc,S(c) = (\u03bbc\u2192s(xs))s\u2208S(c)\\{c} yeilds a sub-optimisation problem,\nargmin \u03bbc,S(c) gc(\u03bbc,S(c)) ,\ngc(\u03bbc,S(c))= [ max xc [ \u03b8\u0302c(xc)\u2212 \u2211 s\u2208S(c)\\{c} \u03bbc\u2192s(xs)+\u03bbc(xc) ] +\n\u2211 s\u2208S(c)\\{c} max xs [ \u03b8\u0302s(xs)\u2212\u03b3s(xs) +\u03bb\u2212cs (xs) +\u03bbc\u2192s(xs)\n]] , (31)\nwhere \u2200s \u2208 S(c) \\ {c},xs\n\u03bb\u2212cs (xs)= \u2211\nc\u0302\u2208{c\u2032|c\u2032\u2208C,c\u2032 6=c,s\u2208S(c\u2032)\\{c\u2032}} \u03bbc\u0302\u2192s(xs). (32)\nA solution is provided in the proposition below.\nProposition 1 \u2200s \u2208 S(c) \\ {c},xs, let\n\u03bb\u2217c\u2192s(xs) = \u2212\u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc)+\n\u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302) )] , (33)\nthen \u03bb\u2217c,S(c) = (\u03bb \u2217 c\u2192s(xs))s\u2208S(c)\\{c} is a solution of (31).\n2 In dual objective other than (30), \u03bbc\u2192c(xc) may not be cancelled out (e.g. the dual objective used in GMPLP).\nConstraint Reduction using Marginal Polytope Diagrams 11\nAlgorithm 1: GDD Message Passing\ninput : G = (V,C), ML(G,C \u2032,S(C\u2032)), C\u2032, S(C\u2032), \u03b8, Tg , Kmax output: \u03bb = (\u03bbc\u2192s(xs))c\u2208C\u2032,s\u2208S(c)\\{c}\n1 k = 0, g0(\u03bb) = +\u221e, \u03bb = 0; 2 repeat 3 k = k + 1; 4 for c \u2208 C\u2032 do 5 Compute \u03bb\u2217c,S(c) using (33); update \u03bbc,S(c) = \u03bb \u2217 c,S(c); 6 Computing b using (29);\n7 gk(\u03bb) = \u2211 t\u2208T maxxt bt(xt);\n/* By Proposition 3, g(\u03bb) always converges. */\n8 until |gk(\u03bb)\u2212 gk\u22121(\u03bb)| < Tg or k > Kmax;\nThe derivation of (30) and (31), and the proof of Proposition 1 are provided in Section 1 in the supplementary material. The bt(xt) are often referred to as beliefs, and \u03bbc\u2192s(xs) messages (see [3, 19]). In (33), \u03bbc(xc) and \u03b3s(xs), \u03bb\u2212cs (xs),\u2200s \u2208 S(c) \\ {c} are known, and they do not depend on \u03bbc\u2192s(xs). We summarise the message updating procedure in Algorithm 1. Dual Decomposition can be seen as a special case of GDD with a specific local marginal polytope ML(G,C, Sd(C)) in (25).\nDecoding The beliefs bt(xt), t \u2208 T are computed via (29d) to evaluate the dual objective and decode an integer solution of the original MAP problem. For a g(\u03bb) obtained via GDD based message passing, we find x\u2217 (so called decoding) via\nx\u2217t \u2208 argmax xt bt(xt),\u2200t \u2208 T . (34)\nHere we use \u2208 instead of = is because there may be multiple maximisers. In fact, if a node i \u2208 V is also an extended cluster or sub-cluster (i.e. \u2203t \u2208 T, s.t. t = {i}), then we perform more efficient decoding via\nx\u2217i \u2208 argmax xi bi(xi). (35)\nFurther discussion on decoding is deferred to Proposition 4 and Section 3.5.\n3.3.2 Convergence and Decoding Consistency\nIn this part we analyse the convergence and decoding consistency of GDD message passing.\nGDD essentially iterates over c \u2208 C\u2032, and updates the messages via (33). The dual decrease defined below\nd(c) =gc(\u03bbc,S(c))\u2212 gc(\u03bb\u2217c,S(c)) (36)\nplays a role in the analysis of GDD.\n12 Z. Zhanget al.\nProposition 2 (Dual Decrease) For any c \u2208 C\u2032, the dual decrease d(c) = max\nxc bc(xc) +\n\u2211\ns\u2208S(c)\\{c} max xs bs(xs)\n\u2212max xc\n[ bc(xc) + \u2211\ns\u2208S(c)\\{c} bs(xs)\n] > 0. (37)\nThe proof is provided in Section 2 of the supplementary. A natural question is whether GDD is convergent, which is answered by the following proposition.\nProposition 3 (Convergence) GDD always converges.\nProof According to duality and LP relaxation, we have for arbitrary \u03bb,\ng(\u03bb) > max x\n\u2211 c\u2208C \u03b8c(xc). (38)\nBy Proposition 2 in each single step of coordinate descent, the dual decrease d(c) is non-negative. Thus GDD message passing produces a monotonically decreasing sequence of g(\u03bb). Since the sequence has a lower bound, the sequence must converge.\nNote that Proposition 3 does not guarantee g(\u03bb) reaches the limit in finite steps in GDD (GMPLP and Dual Decomposition have the same issue). However, in practice we observe that GDD often converges in finite steps. The following proposition in part explains why the decoding in (35) is reasonable.\nProposition 4 (Decoding Consistency) If GDD reaches a fixed point in finite steps, then \u2200c \u2208 C\u2032, s \u2208 S(c) \\ {c}, there exist x\u0302c \u2208 argmaxxc bc(xc), and x\u0304s \u2208 argmaxxs bs(xs), s.t. x\u0302s = x\u0304s. Proof If GDD reaches a fixed point, \u2200c \u2208 C\u2032, d(c) = 0 (see (37) ). Otherwise a non-zero dual decrease means GDD would not stop. Thus \u2200c \u2208 C\u2032,\nmax xc\nbc(xc) + \u2211\ns\u2208S(c)\\{c} max xs bs(xs) =\nmax xc\n[ bc(xc) + \u2211\ns\u2208S(c)\\{c} bs(xs)\n] , (39)\nwhich completes the proof.\nProposition 4 essentially states that there exist two maximisers that agree on xs. This in part justifies decoding via (35) (e.g. s is a node). Further discussion on decoding is provided in Section 3.5.\nIt\u2019s obvious that the solution of GDD is exact, if the gap between the dual objective and the decoded primal objective is zero. Here we show that the other requirements for the exact solution also hold.\nProposition 5 If there exists x that maximises bt(xt),\u2200t \u2208 T, the solution of GDD is exact.\nThe proof is provided in Section 3 of the supplementary. Proposition 4 and 5 generalise the results of Section 1.7 in [20].\nConstraint Reduction using Marginal Polytope Diagrams 13\nAlgorithm 2: Belief Propagation Without Messages\ninput : G = (V,C), ML(G,C \u2032,S(C\u2032)), C\u2032, S(C\u2032), \u03b8, Tg , Kmax, output: b = (bt(xt))t\u2208T ;\n1 b = (\u03b8\u0302t(xt))t\u2208T , k = 0, g 0(\u03bb) = +\u221e; 2 repeat 3 k = k + 1; 4 for c \u2208 C\u2032 do 5 Compute b\u2217c,S(c) using (40); update bc,S(c) = b \u2217 c,S(c);\n6 gk(\u03bb) = \u2211 t\u2208T maxxt bt(xt);\n/* By Proposition 3, g(\u03bb) always converges. */\n7 until |gk(\u03bb)\u2212 gk\u22121(\u03bb)| < Tg or k > Kmax;\n3.4 Belief Propagation Without Messages\nGDD involves updating many messages (e.g. \u03bbc(xc) and \u03b3s(xs), \u03bb \u2212s c (xs),\u2200c \u2208 C\u2032, s \u2208 S(c) \\ {c}). These messages are then used to compute the beliefs bt(xt),\u2200t \u2208 T (see (29d)). Here we show that we can directly update the beliefs without computing and storing messages.\nWhen optimising (31), bc(xc) and bs(xs), s \u2208 S(c) \\ {c} are determined by \u03bbc,S(c)(xc) (see (78) in supplementary). Thus let bc,S(c) = (bc(xc), bs(xs))s\u2208S(c)\\{c} be the beliefs determined by \u03bbc,S(c), and b \u2217 c,S(c) be the beliefs determined by \u03bb\u2217c,S(c). We have the following proposition. Proposition 6 When optimising (31), the beliefs b\u2217c,S(c) can be computed from a bc,S(c) determined by arbitrary \u03bbc,S(c) as following:\nb\u2217s(xs)= max xc\\s\n[ bc(xc)+ \u2211 s\u0302\u2208S(c)\\{c}bs\u0302(xs\u0302) ]\n| S(c) \\ {c}| ,\u2200s\u2208S(c)\\{c},xs\nb\u2217c(xc) =bc(xc)+ \u2211\ns\u0302\u2208S(c)\\{c} bs\u0302(xs\u0302)\u2212\n\u2211\ns\u0302\u2208S(c)\\{c} b\u2217s\u0302(xs\u0302),\u2200xc . (40)\nThe proof is provided in Section 4 of the supplementary. The reason that b\u2217c,S(c) can be computed using bc,S(c) from an arbitrary \u03bbc,S(c), is because \u03bbc\u2192s(xs), s \u2208 S(c) \\ {c} is cancelled out (e.g. see (86) in supplementary) during the calculation. In the first iteration, beliefs are initialised via bt(xt) = \u03b8\u0302t(xt),\u2200t \u2208 T, and then we can use bc,S(c) from previous iterations to compute b\u2217c,S(c) without messages and the potentials.\nMemory Conservation For message updating based methods such as GDD message passing in Algorithm 1, Max-Sum Diffusion [11, 24], GMPLP[3] and etc, both messages and potentials need to be stored in order to compute new messages (see (33) for example). However, the proposed belief propagation without messages (summarised in Algorithm 2) only needs to store beliefs. The beliefs can simple take the space of potentials (i.e. initialisation), and then update. For a graph G, we assume that each node takes k = | Vals(Xi)| states, and let ML(G,C \u2032, S(C\u2032)) be a local marginal polytope with specific C\u2032\n14 Z. Zhanget al.\nand S(C\u2032). Then we need the following space to store beliefs, potentials and messages:\nMembeliefs = \u2211\nt\u2208T k|t|, (41a)\nMempotentials = \u2211\nc\u2208C k|c|, (41b)\nMemmessages = \u2211\nc\u2208C\n\u2211\ns\u2208S(c)\\{c} k|s|. (41c)\nRecall the definition of T in (28), it is easy to show that\nMembeliefs = \u2211\nc\u2208C k|c| +\n\u2211\ns\u2208T \\C k|s|\n= \u2211\nc\u2208C k|c| +\n\u2211\ns\u2208(C\u222a(\u222ac\u2208C S(c))\\C k|s|\n6 \u2211\nc\u2208C k|c| +\n\u2211\ns\u2208\u222ac\u2208C S(c) k|s|\n6 \u2211\nc\u2208C k|c| +\n\u2211\nc\u2208C\n\u2211\ns\u2208S(c)\\{c} k|s|\n=Mempotentials + Memmessages. (42)\nThis means that belief propagation without messages always uses less memory than message passing. If we choose T = C, and S(t) = {t\u2032|t\u2032 \u2282 t}, the memory for message passing has a simpler form\nMempotentials + Memmessages = \u2211\nt\u2208T k|t| +\n\u2211\nt\u2208T\n\u2211 s\u2282t k|s|,\n= \u2211\nt\u2208T k|t| +\n\u2211\nt\u2208T\n[ (1 + k)|t| \u2212 k|t| \u2212 1 ]\n= \u2211\nt\u2208T\n[ (1 + k)|t| \u2212 1 ] . (43)\nLet k = 2, |t| = 10, we have k|t| = 1024, and (k + 1)|t| \u2212 1 = 59048, where message updating based methods uses approximately 59 times memory as belief propagation without messages.\n3.5 \u201cStealth\u201d Cluster Pursuit\nIf GDD does not find the exact solution, then there exists a gap between the dual objective of GDD and the decoded primal objective. Various approaches, including [9, 23, 19, 1], try to tighten the gap. These approaches typically involve two steps: 1) creating a dictionary of clusters, and then 2) search for a best cluster in the dictionary w.r.t. some score function. They typically\nuse a fixed dictionary of clusters. For example, Sontag et al [19] consider the dictionary as all possible triplets; Werner [23] considers all order 4 cycles for grids type graphs.\nDual decrease is a popular choice of the score function. For example, Sontag et al [19], brutal force search over the dictionary for the cluster with maximum dual decrease. Batra et al [1] accelerate the process by computing primal dual gap for all clusters in the dictionary first (computing primal dual gap is much cheaper than computing dual decrease), and then only search over the clusters with non-zero primal dual gaps for the cluster with maximum dual decrease.\nIn this section, we show a new cluster pursuit strategy, which dynamically generates a dictionary of clusters instead of using a fixed one. The new cluster pursuit strategy is based on a special type of cluster which we call \u201cstealth\u201d clusters.\nDefinition 1 (\u201cStealth\u201d cluster) \u2200c1, c2 \u2208 C\u2032, we say c = c1 \u222a c2 is a \u201cstealth\u201d cluster if\n1. \u2203s \u2208 T s.t. s \u2208 S(c1) \\ {c1}, s \u2208 S(c2) \\ {c2}, and 2. @c\u0302 \u2208 C\u2032 s.t. c1 \u2208 S(c\u0302) \\ {c\u0302}, c2 \u2208 S(c\u0302) \\ {c\u0302}.\nProposition 4 essentially says there exist two maximisers that agree on xs. There is however a situation where the decoding from different clusters may disagree. Let us consider two clusters c1, c2 in Definition 1. According to Proposition 4 there exists a maximiser x\u2032s of bs(xs) that agrees with bc1(xc1), and a maximiser x\u2032\u2032s of bs(xs) that agrees with bc2(xc2). However, if bs(xs) has multiple maximisers, the maximisers x\u2032s and x \u2032\u2032 s could be different. This means bc1(xc1) and bc2(xc2) may disagree on their overlap. Adding c1\u222ac2 into C\u2032 with S(c1 \u222a c2) = {s|s \u2208 T, s \u2282 (c1 \u222a c2)} at least one maximiser of bc1(xc1) and bc2(xc2) will become the same according to Proposition 4. This observation yields a strategy to dynamically generate dictionary of clusters to tighten the relaxation.\nWith similar derivation as in [19], adding a new \u201cstealth\u201d cluster c = c1\u222ac2 with S(c) = {s|s \u2208 T, s \u2282 c}, the dual decrease after one message updating for\n16 Z. Zhanget al.\nAlgorithm 3: GDD with \u201cStealth\u201d Cluster Pursuit\ninput : G = (V,C), ML(G,C \u2032,S(C\u2032)), C\u2032, S(C\u2032), \u03b8,\nthreshold Tg, Ta, max iterations K 1 max and K 2 max,\nmax running timeTmax, cluster addition size n output: x\u2217\n1 l = 0; 2 Initialise \u03bb0 = 0; b0 := (b0t (xt))t\u2208T = (\u03b8\u0302t(xt))t\u2208T ; 3 repeat 4 l = l + 1; T = C\u2032 \u222a(\u222ac\u2208C\u2032 S(c)); P = \u2205; 5 K = 1(l = 1)K1max + 1(l > 1)K 2 max; 6 Run Algo. 1 (\u03bbl,bl) =GDD(G,ML,C \u2032,S(C\u2032), \u03b8, Tg, K,\u03bb l\u22121); 7 or Algo. 2 bl =BP(G,ML,C \u2032,S, \u03b8, Tg, K,b l\u22121); 8 for t \u2208 T do 9 for c1, c2 \u2208 {c|c \u2208 C\u2032, t \u2208 S(c) \\ {c}} do\n10 {x\u0304c1} = argmaxxc1 b l c1 (xc1 ); 11 {x\u0302c2} = argmaxxc1 b l c2 (xc2 ); 12 if @x\u0304c1 , x\u0302c2 , s.t. x\u0302t = x\u0304t then 13 P = P \u222a {c1 \u222a c2}; 14 S(c1 \u222a c2) = {s|s \u2208 T, s \u2282 (c1 \u222a c2)}; 15 Compute d1(c) according to (44);\n16 Add the n clusters in P with largest d1(c) to C \u2032; 17 For all new added c, \u2200s \u2208 S(c) \\ {c},xs, \u03bblc\u2192s(xs) = 0; 18 Decode x\u2217 using (34) or (35); 19 g(\u03bb) = \u2211 t\u2208T maxxt bt(xt); 20 until |g(\u03bb)\u2212 p(x\u2217 |\u03b8)| 6 Ta or running time> Tmax;\nc is\nd1(c) =gc(\u03bbc,S(c))\u2212 gc(\u03bb\u2217c,S(c)) = \u2211\ns\u2208S(c)\\{c} max xs bs(xs)\u2212max xc\n\u2211\ns\u2208S(c)\\{c} bs(xs). (44)\nIn practice, we add clusters that will lead to largest dual decrease. With these observations, a new cluster pursuit strategy, which we call \u201cstealth\u201d cluster pursuit strategy, is summarised in Algorithm 3. In a nutshell, we search for disagreeing clusters which maximise the dual decrease. In the worst case, this can be slow if too many disagreeing \u201cstealth\u201d clusters exist. However, in practice it is very fast. As we can see from Figure 1, the number of disagreeing \u201cstealth\u201d clusters is far less (about 1% to 10%) than the total number of \u201cstealth\u201d clusters, which leads to a significant speed up. More importantly, \u201cstealth\u201d cluster pursuit makes our feasible set tighter than that of LP relaxation in (7) and GMPLP, which in turn are tighter than Dual Decomposition [20].\n\u201cStealth\u201d cluster pursuit may get bigger and bigger clusters which would become prohibitively expensive to solve. In our experiments, it\u2019s always computationally affordable. When it isn\u2019t, one can use low order terms to approximate bt. Furthermore both the frustrated cycle search strategy in [21] and acceleration via evaluating primal dual gap first in [1] are applicable to GDD as well.\nConstraint Reduction using Marginal Polytope Diagrams 17\nConvergence and Consistency \u201cStealth\u201d cluster pursuit can be seen as adding new constraints only. No matter which clusters are added to C\u2032, the dual decrease d1(c) is always no-negative. Thus GDD with \u201cstealth\u201d cluster pursuit still have the same convergence and consistency properties as original GDD presented in Section 3.3.2.\n4 Marginal Polytope Diagrams\nLP relaxation based message passings can be slow if there are too many constraints. This motivates us to seek ways of reducing the number of constraints to reduce computational complexity without sacrificing the quality of the solution. Here we first propose a new tool which we call marginal polytope diagrams. Then with this tool, we show how to reduce constraints without loosening the optimisation problem.\nDefinition 2 (Marginal Polytope Diagram) Given a graph G = (V,C), GM = (VM ,EM ) with node set VM and edge set EM is said to be a marginal polytope diagram of G if\n1. C \u2286 VM \u2286 2V and 2. a directed edge from c to s deonted by (c\u2192 s), belongs to EM only nodes\nif c, s \u2208 VM , s \u2286 c.\nRemarks Previous work in this vein includes Region graphs [26, 7] and Hasse diagrams (a.k.a poset diagrams) [17, 22, 15]. What distinguishes marginal polytope diagrams, however, is the fact that the receivers of an edge can be a subset of the senders, where Region graphs and Hasse diagrams require that the edge\u2019s receivers must be a proper subset of the senders. For example, the definition of region graph in Page 419 of [7], requires that a receiver must be a proper subset of a sender in region graph. Likewise in Page 16 of [26], the authors state that a region graph must be a directed acyclic graph, which means that the edge\u2019s receivers must be a proper subset of the senders (otherwise there would be a loop from a region to itself). This is particularly significant since some dual message passing algorithms (including GMPLP) send messages from a cluster c to itself. Hasse diagrams [17] have a further restriction, which corresponds to a special case of Marginal Polytope Diagram, where for arbitrary v1, v2 \u2208 VM , v1 \u2282 v2 edge (v1 \u2192 v2) \u2208 EM if and only if @v3 \u2208 VM , s.t. v2 \u2282 v3 \u2282 v1. In some LP relaxation based message passing algorithms, some local marginalisation constraints from a cluster to itself are required, thus violating the proper subset requirement. Both Hasse diagrams and Region graphs are inapplicable in this case. For example, in Section 6 of [3], GMPLP sends messages from one cluster to itself, which requires a local marginalisation constraint from one cluster to itself. In [19], the message \u03bbij\u2192ij (in their Figure 1) is from the edge ij to itself, which requires a local marginalisation constraint from the edge to itself. The proposed marginal polytope diagram not only handles the above situations, but also provides\na natural correspondence among marginal polytope diagram, local marginal polytope and MAP message passing.\nIn marginal polytope diagrams, we use rectangles to represent nodes (to differentiate from a graph of graphical models) similar to Hasse diagrams (see Section 4.2.1 in [22]). An example is given in Figure 2. For the marginal polytope diagram associated with GMPLP (in Figure 2 middle) has edges from a node to itself, which are not allowed in both Region graphs and Hasse diagrams. Also edges like ({2, 3, 4} \u2192 {3}) are not allowed in Hasse diagrams. We choose to use the term diagram instead of graph in order to distinguish from graphs in graphical models.\nFrom local marginal polytope to diagram Given a graph G = (V,C) and arbitrary local marginal polytope ML(G,C\n\u2032, S(C\u2032)), the corresponding marginal polytope diagram GM = (VM ,EM ) can be constructed as:\n1. VM = T (T is defined in (28)), 2. \u2200c \u2208 C\u2032, if s \u2208 S(c), then (c\u2192 s) \u2208 EM .\nFrom diagram to local marginal polytope Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ), the corresponding local marginal polytope ML(G,C \u2032, S(C\u2032)) can be recovered as follows:\nC\u2032 = {c|c \u2208 VM ,\u2203(c\u2192 s) \u2208 EM}, S(c) = {s|(c\u2192 s) \u2208 EM},\u2200c \u2208 C\u2032 . (45)\n4.1 Equivalent Edges\nDefinition 3 (Edge equivalence) For arbitrary G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, \u2200c1, c2, t \u2208 VM , t \u2286 c1, t \u2286 c2, given\nU = {\u2211\nxc\\s\n\u00b5c(xc) = \u00b5s(xs),\u2200(c\u2192 s) \u2208\n(EM \\{(c\u0302\u2192 t)|c\u0302 \u2208 VM , t \u2286 c\u0302}),xs } , (46)\nConstraint Reduction using Marginal Polytope Diagrams 19\nFig. 3 Two types of edge equivalence in Proposition 7. Left: Type 1 (c \u2192 t) \u21d4 (s \u2192 t). Right: Type 2 (s1 \u2192 t)\u21d4 (s2 \u2192 t).\nFig. 4 A counter example for the simpler edge equivalence definition.\nedges (c1 \u2192 t) and (c2 \u2192 t) are said to be equivalent w.r.t GM denoted by (c1 \u2192 t)\u21d4 (c2 \u2192 t), if the following holds:\n{ \u00b5 |U\u222a{ \u2211\nxc1\\t\n\u00b5c1(xc1) = \u00b5t(xt),\u2200xt} }\n= { \u00b5 |U\u222a{ \u2211\nxc2\\t\n\u00b5c2(xc2) = \u00b5t(xt),\u2200xt} } . (47)\nNote that the definition of edge equivalence does not require (c1 \u2192 t) and (c2 \u2192 t) from EM . Checking whether two edges are equivalent via Definition 3 might be inconvenient. In fact, edge equivalence can be read directly from a marginal polytope diagram.\nProposition 7 Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ) of G, we have\n1. \u2200c, s, t \u2208 VM , t \u2282 s \u2282 c, if (c\u2192 s) \u2208 EM , then (c\u2192 t)\u21d4 (s\u2192 t); 2. If (c \u2192 s1), (c \u2192 s2) \u2208 EM , then \u2200t \u2208 VM , t \u2282 s1, t \u2282 s2, (s1 \u2192 t) \u21d4\n(s2 \u2192 t). The proof is provided in Section 5 of supplementary. In Figure 3 we give examples of the two types of edge equivalence in Proposition 7. Furthermore, the following proposition always holds.\nProposition 8 For arbitrary G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, edge equivalence w.r.t. GM is an equivalence relation.\nThe proof is provided in Section 6 of supplementary.\nSimpler edge equivalence definition? Readers may wonder why in Definition 3, all edges sent to t are removed (see (46)), instead of only removing two edges (c1 \u2192 t), (c2 \u2192 t). The answer is that if we did so, the resulting edge equivalence (we call it simpler edge equivalence) would no longer be an equivalence relation. To see this, we can replace EM \\{(c\u0302 \u2192 t)|c\u0302 \u2208 VM} with EM \\{(c1 \u2192 t), (c2 \u2192 t)} in (46), and see a counter example in Figure 4. In considering whether (s1 \u2192 t) /\u2208 EM and (s4 \u2192 t) /\u2208 EM are equivalent, we need to consider two constraint sets Ua = { \u2211 xc\\s\n\u00b5c(xc) = \u00b5s(xs),\u2200(c \u2192 s) \u2208 EM \u222a{(s1 \u2192 t)},xs} and Ub = { \u2211 xc\\s\n\u00b5c(xc) = \u00b5s(xs),\u2200(c \u2192 s) \u2208 EM \u222a{(s4 \u2192 t)},xs}. Then by the fact (s1 \u2192 t) \u21d4 (s2 \u2192 t), (s3 \u2192 t) \u21d4 (s4 \u2192 t), (s2 \u2192 t) \u2208\n20 Z. Zhanget al.\nEM , (s3 \u2192 t) \u2208 EM , we have {\u00b5 |Ua} = {\u00b5 |Ub}. Thus (s1 \u2192 t) \u21d4 (s4 \u2192 t) by the simpler edge equivalence definition. However, by the simpler edge equivalence definition (s2 \u2192 t), (s3 \u2192 t) are not equivalent in general. This means transitivity does not hold. Thus the simpler edge equivalence is not an equivalence relation. Figure 4 is not a counter example for Definition 3, because (s1 \u2192 t), (s4 \u2192 t) are not equivalent by Definition 3.\nWith edge equivalence, we can see that given a marginal polytope diagram GM = (VM ,EM ) of a graph G = (V,C), the following two operations would not change the corresponding local marginal polytope.\n1. Adding a new edge that is equivalent to an existing edge in EM ; 2. Removing one of two existing equivalent edges in EM .\nBy composing the two operations above, we are able to derive a series of operations which would not change the corresponding local marginal polytope. For illustration, using Operation 1) first and then using Operation 2) would lead to an operation: replacing an existing edge in EM with an equivalent edge. Repeating Operation 2) we can merge several equivalent edges in EM to one edge. Repeating Operation 2) and then using Operation 1) we can replace a group of equivalent edges in EM with a new equivalent edge.\n4.2 Redundant Nodes\nThere is a type of node, the removal of which from a marginal polytope diagram does not change the local marginal polytope.\nDefinition 4 (Redundant Node) For any graph G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, we say v \u2208 VM \\C is a redundant node w.r.t. GM , if\n{ \u00b5 | \u2211\nxc\\s\n\u00b5c(xc) = \u00b5s(xs),\u2200(c\u2192 s) \u2208 EM ,xs }\n= { \u00b5 | \u2211\nxc\\s\n\u00b5c(xc) = \u00b5s(xs),\u2200(c\u2192 s) \u2208 E\u0302 M ,xs\n}\nwhere\nE\u0302 M = [ EM \\({(c\u2192 v) \u2208 EM} \u222a {(v \u2192 s) \u2208 EM}) ]\n\u222a { (c\u2192 s)|(c\u2192 v) \u2208 EM , (v \u2192 s) \u2208 EM } .\nThe following proposition provides an easy way to find redundant nodes in a marginal polytope diagram.\nProposition 9 Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ), v \u2208 VM \\C is a redundant node w.r.t. GM if either of the following statements is true:\n1. There is only one (c\u2192 v) \u2208 EM ; 2. All (c\u2192 v) \u2208 EM are equivalent w.r.t. GM according to Definition 3.\nProof Let us consider the first case where there is only one (c\u2192 v) \u2208 EM . It is easy to check that\n   \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2200(v \u2192 s) \u2208 EM ,xs, \u2211 xc\\s \u00b5c(xc) = \u00b5s(xs); \u2200xv, \u2211 xc\\v \u00b5c(xc) = \u00b5v(xv)   \n(48a)\n=    \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2200xv, \u2211 xc\\v \u00b5c(xc) = \u00b5v(xv); \u2200(v \u2192 s) \u2208 EM ,xs, \u2211 xv\\s \u00b5v(xv) = \u00b5s(xs)    . (48b)\nSince v /\u2208 C, \u00b5v(xv) is not part of \u00b5 in (3). Note that removing \u2211\nxc\\v \u00b5c(xc) =\n\u00b5v(xv), \u2200xv from (48a) would not change the set of \u00b5 described by (48a) as no other variables depend on \u00b5v(xv). That is,\n{ \u00b5 | \u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302),\u2200(c\u0302\u2192 s\u0302) \u2208 EM1 ,xs\u0302 }\n= { \u00b5 | \u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302),\u2200(c\u0302\u2192 s\u0302) \u2208 EM2 ,xs\u0302 } ,\nwhere EM1 = {(c\u2192 s)|(v \u2192 s) \u2208 EM}, and EM2 = {(c\u2192 v)}\u222a{(v \u2192 s) \u2208 EM}. Let A = {\u00b5 |\u2211xc\u0302\\s\u0302 \u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302),\u2200(c\u0302\u2192 s\u0302) \u2208 E M \\EM2 ,xs\u0302}. We have\n{ \u00b5 | \u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302),\u2200(c\u0302\u2192 s\u0302) \u2208 EM1 ,xs\u0302 } \u2229A\n= { \u00b5 | \u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302),\u2200(c\u0302\u2192 s\u0302) \u2208 EM2 ,xs\u0302 } \u2229A.\nSince EM = (EM \\EM2 ) \u222a EM2 and E\u0302 M\n= (EM \\EM2 ) \u222a EM1 , { \u00b5 | \u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302),\u2200(c\u0302\u2192 s\u0302) \u2208 E\u0302 M ,xs\u0302\n}\n= { \u00b5 | \u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302),\u2200(c\u0302\u2192 s\u0302) \u2208 EM ,xs\u0302 } .\nHence v is a redundant node by definition. Now let us consider the second case, where there are multiple equivalent (c \u2192 v) \u2208 EM . By edge equivalence we can keep one of (c \u2192 v) \u2208 EM and remove the rest without changing the local marginal polytope, which becomes the first case.\n5 Constraint Reduction\nEquivalent edges offer an effective way to reduce constraints. By composing the two basic operations in Section 4.1, we can get a series of operations which would not change the corresponding local marginal polytope. For illustration we can partition EM into several equivalent classes by equivalence between edges, and we can simply pick up arbitrarily many edges in each equivalent class to get a new marginal polytope diagram with fewer edges. As each edge corresponds to a local marginalisation constraint, the number of constraints can be efficiently reduced by the above operations. As shown in Figure 5, all edges to node {3} are equivalent, thus we can keep just one of them in the diagram to keep the local marginal polytope unaltered yet with fewer constraints.\nGiven a graph G = (V,C), and a marginal polytope diagram GM = (VM ,EM ) of G, if a node v \u2208 VM is a redundant node, we can reduce the number of constraints by removing v from VM to obtain a marginal polytope diagram as follows:\nGMR =(V M R ,E M R ),V M R = V M \\{v}, EMR =[E\nM \\({(c\u2192 v) \u2208 EM} \u222a {(v \u2192 s) \u2208 EM})] \u222a {(c\u2192 s)|(c\u2192 v), (v \u2192 s) \u2208 EM}, (49)\nConstraint Reduction using Marginal Polytope Diagrams 23\nand according to the definition of redundant nodes, GMR and G M correspond to the same local marginal polytope. Figure 6 gives an example of redundant node removal. In the middle diagram, one can see that nodes {2, 4, 5}, {2, 5, 6}, {4, 5, 8}, and {5, 6, 8} are redundant because their indegrees are 1. For node {5}, as all edges to {5} are equivalent, node {5} is also redundant. Removal of these redundant nodes leads to fewer constraints without changing the local marginal polytope.\nIt is worth mentioning that [8] is perhaps closest idea to ours in spirit. However, [8, Proposition 2.1] considers the removal of edges only, whereas ours considers the removal of both edges and nodes.\n5.1 Is the Minimal Number of Constraints Always a Good Choice?\nUsing marginal polytope diagrams one can safely reduce the number of constraints without altering the local marginal polytope. On one hand, fewer constraints means fewer belief updates on bt(xt), t \u2208 T, which leads to lower run time per iteration. On the other hand, fewer constraints means fewer coordinates (i.e. search directions), and thus that the algorithm is more likely to get stuck at corners due to the non-smoothness of the dual objective and the nature of coordinate descent (this has been often observed empirically too). This means that minimal number of constraints is not always a good choice. As we shall see in the next section, a trade-off between the minimal number of constraints and the maximal number of constraints performs best.\n6 From Constraint Reduction To New Message Passing Algorithms\nHere we propose three new efficient algorithms for MAP inference, using different constraint reduction strategies (via marginal polytope diagrams). All three algorithms are based on the GDD belief propagation procedure which is equivalent to GDD message passing, thus the theoretical properties in Section 3.3.2 also hold for these three algorithms.\nTo derive new algorithms, we first construct a local marginal polytope as an initial local marginal polytope, and then by different constraint reduction strategies we get three different algorithms. For arbitrary graph G = (V,C), we let C\u2032 be C0 = {c\u2032|c\u2032 \u2286 c, c \u2208 C} and S(C\u2032) be S0(C0) = (S0(c))c\u2208C0 with S0(c) = {s|s \u2208 C0, s \u2282 c} to construct a local marginal polytope ML(G,C0, S0(C0)) as a initial local marginal polytope. Thus the marginal polytope diagram is GM0 = (V M 0 ,E M 0 ) where\nVM0 = {v|v \u2286 c, c \u2208 C}, EM0 = {(c\u2192 s)|c, s \u2208 VM0 , s \u2282 c}. (50)\nThe marginal polytope diagram GM0 and GDD provide the base for all three algorithms.\n24 Z. Zhanget al.\n6.1 Power Set Algorithm\nIn the first algorithm which we call Power Set algorithm, we do not remove any redundant nodes in GM0 . One can see that \u2200(c \u2192 t) \u2208 EM0 , |c| \u2212 |t| > 1, \u2203s \u2208 VM0 , s.t.|s| = |t| + 1, (c \u2192 s) \u2208 EM0 , which suggests (c \u2192 t) \u21d4 (s \u2192 t). Using equivalent edges we get a marginal polytope diagram GMp = (V M p ,E M p ) as follows\nVMp = V M 0 , EMp = {(c\u2192 s)|c, s \u2208 VMp , s \u2282 c, |c| = |s|+ 1}, (51)\nwhich corresponds to the same local marginal polytope as GM0 . Thus we define Cp = {c|c \u2286 c\u0302, c\u0302 \u2208 C}, Sp(c) = {s|(c \u2192 s) \u2208 EMp }, and let C\u2032 be Cp, and S(C\u2032) be Sp(Cp) = (Sp(c))c\u2208C\u2032 , the corresponding local marginal polytope becomes ML(G,Cp, Sp(Cp)). By the fact that c /\u2208 Sp(c),\u2200c \u2208 Cp, applying belief propagation without messages to ML(G,Cp, Sp(Cp)) yields the following belief propagation form \u2200c \u2208 Cp:\nb\u2217s(xs) = 1 |c| maxxc\\s [bc(xc) + \u2211\ns\u0302\u2208Sp(c) bs\u0302(xs\u0302)],\u2200s \u2208 Sp(c),xs\nb\u2217c(xc) = bc(xc)+ \u2211\ns\u0302\u2208Sp(c) bs\u0302(xs\u0302)\u2212\n\u2211\ns\u0302\u2208Sp(c) b\u2217s\u0302(xs\u0302),\u2200xc . (52)\n6.2 \u03c0-System Algorithm\nThe second algorithm which we call \u03c0-System algorithm, is based on the \u03c0system [5] extended from C. Such a \u03c0-system denoted by C\u03c0 has the following properites:\n1. if c \u2208 C, then c \u2208 C\u03c0; 2. if c1, c2 \u2208 C\u03c0, then c1 \u2229 c2 \u2208 C\u03c0. We can construct C\u03c0 using the properties above by assigning all elements in C to C\u03c0 and adding intersections repeatedly to C\u03c0.\nProposition 10 All v \u2208 VM0 \\C\u03c0 are redundant nodes w.r.t GM0 .\nProof Since C \u2286 C\u03c0, for any v \u2208 VM0 \\C\u03c0, we have v \u2208 VM0 \\C. Now we prove the proposition by proving that all edges to v are equivalent.\nLet Pv = {p|(p\u2192 v) \u2208 EM0 }, and Cv = {c|c \u2208 C, v \u2282 c}. We let s = \u2229c\u2208Cvc, and we must have v \u2286 s. Moreover, if v = s we have v = \u2229c\u2208Cvc \u2208 C\u03c0, this contradicts the fact that v \u2208 VM \\C\u03c0. Thus we must have v \u2282 s. Then, by the definition of VM0 and E M 0 in (50), \u2200p \u2208 Pv, \u2203c \u2208 Cv, s.t. p \u2286 c. By the fact that s = \u2229c\u0302\u2208Cv c\u0302, we have s \u2286 c. Thus if p = c = s, then (p \u2192 v) \u21d4 (s \u2192 v) naively holds. If only one of p and s is equal to c, we have (p \u2192 v) \u21d4 (s \u2192 v) by Proposition 7 (the first case). If both p and s are not equal to c, by\nConstraint Reduction using Marginal Polytope Diagrams 25\nProposition 7 (the second case) we have (p \u2192 v) \u21d4 (s \u2192 v). As a result, all (p \u2192 v), p \u2208 Pv are equivalent, which implies that v is redundant node w.r.t. GM0 by Proposition 9.\nBy edge equivalence, we construct a marginal polytope diagram GM\u03c0 = (VM\u03c0 ,E M \u03c0 ) below,\nVM\u03c0 = C\u03c0, (53) EM\u03c0 = {(c\u2192 s)|c, s \u2208 C\u03c0, s \u2282 c,@t \u2208 C\u03c0, s.t. s \u2282 t \u2282 c}.\nThus we define S\u03c0(c) = {s|(c\u2192 s) \u2208 EM\u03c0 }. Let C\u2032 be C\u03c0, and S(C\u2032) be S\u03c0(C\u03c0) = (S\u03c0(c))c\u2208C\u03c0 the corresponding marginal polytope becomes ML(G,C\u03c0, S\u03c0(C\u03c0)). By the fact that c /\u2208 S\u03c0(c),\u2200c \u2208 C\u03c0, applying belief propagation without messages on ML(G,C\u03c0, S\u03c0(C\u03c0)) results in the following belief propagation form \u2200c \u2208 C\u03c0:\nb\u2217s(xs)= 1\n| S\u03c0(c)| max xc\\s\n[bc(xc)+ \u2211\ns\u0302\u2208S\u03c0(c) bs\u0302(xs\u0302)],\u2200s \u2208 S\u03c0(c),xs\nb\u2217c(xc)=bc(xc) + \u2211\ns\u0302\u2208S\u03c0(c) bs\u0302(xs\u0302)\u2212\n\u2211\ns\u0302\u2208S\u03c0(c) b\u2217s\u0302(xs\u0302),\u2200xc . (54)\nNote that a node in the \u03c0-system may still be a redundant node.\n6.3 Maximal-Cluster Intersection algorithm\nHere we remove more redundant nodes by introducing the notion of maximal clusters.\nDefinition 5 (maximal cluster) Given a graph G = (V,C), a cluster c is said to be a maximal cluster of C, if c \u2208 C,@c\u0302 \u2208 C, s.t. c \u2282 c\u0302. The intersection of all maximal clusters is\nIm = {s|s = c \u2229 c\u2032, c, c\u2032 \u2208 Cm}, (55)\nwhere\nCm = {c|c \u2208 C,@c\u0302 \u2208 C, s.t. c \u2282 c\u0302}. (56)\nProposition 11 All v \u2208 VM0 \\{C\u222a Im} are redundant nodes w.r.t GM0 .\nThe proof is provided in Section 7 of the supplementary material. By edge equivalence, we construct a marginal polytope diagram GMm = (VMm ,E M m ) with\nVMm = C\u222a Im EMm = {(c\u2192 s)|c \u2208 Cm, s \u2208 C\u222a Im, s \u2282 c}. (57)\n26 Z. Zhanget al.\nThus we define Sm(c) = {s|(c \u2192 s) \u2208 EMm }. Let C\u2032 be Cm, and S(C) be Sm(Cm) = (Sm(c))c\u2208Cm , the corresponding local marginal polytope becomes ML(G,Cm, Sm(Cm)). By the fact that c /\u2208 Sm(c),\u2200c \u2208 Cm, applying belief propagation without messages to ML(G,Cm, Sm(Cm)) yields the following belief propagation \u2200c \u2208 CM :\nb\u2217s(xs)= 1\n| Sm(c)| max xc\\s\n[bc(xc)+ \u2211\ns\u0302\u2208Sm(c) bs\u0302(xs)],\u2200s \u2208 Sm(c),xs\nb\u2217c(xc)=bc(xc)+ \u2211\ns\u0302\u2208Sm(c) bs\u0302(xs)\u2212\n\u2211\ns\u0302\u2208Sm(c) b\u2217s\u0302(xs\u0302),\u2200xc . (58)\n7 Experiments\nMAP LP relaxation can be solved using standard LP solvers such as CPLEX, Gurobi, LPSOLVE etc.. However, for the inference problems in our experiments the LP relaxations typically have more than 105 variables and 106 constraints. It is very slow to use standard LP solvers in this case. Even stateof-the-art commercial LP solvers such as CPLEX have been reported to be slower than message passing based algorithms [25]. Thus we only compare our methods against message passing based algorithms.\nWe compare the proposed algorithms (all with \u201cstealth\u201d cluster pursuit) which are Power Set algorithm (PS), \u03c0-System algorithm (\u03c0-S) and MaximalCluster Intersection algorithm (MI), with 3 competitors: GMPLP [3] with \u201ctriplet\u201d cluster pursuit [19] (GMPLP+T), GMPLP with our \u201cstealth\u201d cluster pursuit (GMPLP+S), and Dual Decomposition with \u201ctriplet\u201d and \u201ccycle\u201d cluster pursuit [21] (Sontag12). All algorithms run belief propagation/message passing with all original constraints (including the ones with higher order potentials). After several iterations of belief propagation, if there is a gap between the dual and decoded primal, different cluster pursuit strategy are applied to tighten the LP relaxations. A brief summary of these methods is provided in Table 1. Max-Sum Diffusion (MSD) [23, 24] has been shown empirically inferior to GMPLP [see 20, Figure 1.5]. Similarly TRW-S of [8] has been shown to be inferior to GMPLP in the higher order potential case [see 8, Section 5]. Thus we compare primarily with Sontag12 and GMPLP. Note that Sontag12 is considered the state-of-the-art.\nWe implement our algorithms and GMPLP in C++. For Sontag12, we use their released C++ code3. As all algorithms use a framework similar to Algorithm 3 (with different the message updating and cluster pursuit), we can describe the termination criteria for these algorithms using the notation from Algorithm 3. For all algorithms, the threshold of inner loop Tg is set to 10\n\u22128, and the maximum number of iterations K1max = 1000. We adopt K 2 max = 20 in light of the faster convergence observed empirically in [19]. The threshold for the outer loop Ta is set to 10\n\u22126. In each cluster pursuit we add n = 20 new clusters, and the maximum running time Tmax is set to 1 hour.\nGDD based algorithms can be implemented as either a message passing procedure or a belief propagation procedure without messages. We implemented both, and observed that both have similar speed (see Section 8.5 in the supplementary material). Of course, the latter uses less storage. For presentation clarity, we only report the result of GDD using belief propagation without messages here.\n7.1 Synthetic data\nWe generate a synthetic graphical model with a structure commonly used in image segmentation and denoising. The structure is a 128 \u00d7 128 grid shown in Figure 7(a) with 3 types of potentials: node potentials, edge potentials and higher order potentials. We consider the problem below,\nmax x\n[\u2211\ni\u2208V \u03b8i(xi) +\n\u2211 ij\u2208E \u03b8ij(xi, xj) + \u2211 f\u2208F \u03b8f (xf ) ] ,\nwhere each xi \u2208 {1, 2, 3} and |f | = 4. All potentials are generated from normal distribution N(0, 1). For clusters with order \u2265 4, Sontag12 only enforces local marginalisation constraints from clusters to nodes, thus its initial local\n3 For computational efficiency, we optimised Sontag et al.\u2019s released code (achieving the same output but with 2-3 times speed up). This is done for GDD and GMPLP as well to ensure a fair comparison. All algorithms are compiled with option \u201c-O3 -fomit-frame-pointer -pipe -ffast-math\u201d using \u201cclang-4.2\u201d, and all experiments are running in single thread with I7 3610QM and 16GB RAM.\nmarginal polytope is looser than that of GMPLP and our methods. As shown in Figure 7(b), the proposed methods, converge much faster than GMPLP+S, GMPLP+T and Sontag12. Also Sontag12\u2019s gap between the dual objective and the decoded primal objective is much larger than that of our methods and GMPLP (even with cluster pursuit to tighten the local marginal polytope).\n7.2 Protein-Protein Interaction\nHere we consider 8 Protein-Protein Interaction (PPI) inference problems (from protein1 to protein8 ) used in [21]. In each problem, there are typically over 14000 nodes, and more than 42000 potentials defined on nodes, edges and triplets. Since the highest order of the potentials is only 3 (triplets), the local marginal polytopes (without cluster pursuit) of all methods here are the same tight. Thus performance difference here is mainly due to different cluster pursuit strategies and computational complexity per iteration.\nWe test all methods on all 8 problems. The average running time for one iteration of updating all beliefs or messages in Table 2 (i.e. steps 4-7 in Algorithm 2 for ours, and the counterpart for the competitors similar to steps 4-8 in Algorithm 1). We can see that the proposed methods have the smallest average running time, followed by Sontag12, and then by GMPLP.\nConstraint Reduction using Marginal Polytope Diagrams 29\nWe present dual objective plots on two problems in Figure 8 here, and provide the results for all problems in the supplementary (Section 8.2). Overall, the proposed methods converge fastest and two of them (\u03c0-S and MI) find exact solutions on 3 problems: protein2, protein4 and protein8. Sontag12 finds exact solutions on protein4 and protein8, and achieved the smallest dual objective values on the problems where all methods failed to find the exact solutions. In terms of convergence, Sontag12 converges slower than proposed methods and faster than GMPLP. GMPLP+T does not find an exact solution on any of the 8 problems, and GMPLP+S finds the exact solution on protein2 only.\n7.3 Image Segmentation\nImage segmentation is often seen as a MAP inference problem over PGMs. Following [6], we consider the MAP problem below\nmax x\n[\u2211\ni\u2208V \u03b8i(xi) +\n\u2211 ij\u2208E \u03b8ij(xi, xj) + \u2211 f\u2208F \u03b8f (xf ) ] .\nwhere |f | = 4. We use the same graph structure as in Figure 7(a), and follow the potentials in [6], where the colour terms in \u03b8i(xi) are computed as in [2]. More details including parameter settings are provided in the supplementary material. Here we segment three images: banana1, book and bool in the MSRC Grabcut dataset 4. The resolution of the images varies from 520 \u00d7 450 to 640 \u00d7 480, and each of the inference problems has about 2 \u00d7 105 to 3 \u00d7 105 nodes and more than 106 potentials. Sontag12 failed to find exact solutions in all 3 images. GMPLP+S and GMPLP+T find exact solution on book only. The proposed methods, PS, \u03c0-S and MI, find the exact solution on all three problems. The result is shown in Figure 9. From the third row of Figure 9 (primal-dual objetives), we can see that the proposed method converges much faster than the competitors. From the fourth row of Figure 9, we can see that the inference error rate (against the exact solution) reduced quickest to zero for the proposed methods.\n7.4 Image Matching\nHere we consider key point based image matching between two images (a source image and a destination image). First we detect key points from both images via SIFT [14] detector. Assume that there are m key points from the source image and n key points from the destination image. Let {p(i) \u2208 R2}i=1,2,...,m and {q(i) \u2208 R2}i=1,2,...,n be the coordinates of key points in the source and the destination images respectively. Let {h(i)}i=1,2,...,m and\n4 http://research.microsoft.com/en-us/um/cambridge/projects/visionimagevideoediting/ segmentation/grabcut.htm\n{g(i)}i=1,2,...,n be the SIFT feature vectors of the source and the destination images respectively. Assume m \u2264 n (otherwise swap the source image and the destination image to guarantee so). The task is for each key point i \u2208 {1, 2, . . . ,m} in the source image, to find a corresponding key point xi \u2208 {1, 2, . . . , n} in the destination image. When there are a large number of points involved, a practical way is to restrict a corresponding key point xi \u2208 {1, . . . , k}\u222a{\u22121}. Here if i has a corresponding point, we restrict it from its k-nearest neighbours of the SIFT feature vector h(i) in {q(i) \u2208 R2}i=1,2,...,n. If i has no corresponding point, we let xi = \u22121.\nLet V = {1, 2, . . .m}, and the matching problem can be formulated as a MAP problem similar to [13],\nmax x\n{\u2211\ni\u2208V \u03b8i(xi) +\n\u2211 f\u2208F \u03b8f (xf ) } , (59)\nConstraint Reduction using Marginal Polytope Diagrams 31\nwhere |f | = 4, xi \u2208 {1, . . . , k}\u222a{\u22121}, and constructing of F is provided in the supplementary. In [13], key points in the source image are filtered and reduced to less than 100 (see Section 3 of [13]), thus they often have corresponding key points in the destination image. As a result they did not use \u22121 to handle the no correspondence case. However, this strategy gives rise to a danger that potentially important key points may be removed too. Also the small scale of their problem allows them to let xi take all n states. In our experiment, we keep all key points (often over 103 in both source and destination images). In that situation, we face two issues: 1) each f \u2208 F needs O((k + 1)4) storage for potentials and beliefs; 2) some key points in the source image have no corresponding points in the destination image. For the first issue, we set k = 4 for computational efficiency. For the second issue, we extend the models in [13] to handle the potential lack of correspondence. The node and higher order potentials are defined as follows:\n\u03b8i(xi) =\n{ \u2212\u03b7 xi = \u22121\n\u2212\u03b4i||h(i)\u2212 g(xi)||22 otherwise\n\u03b8f (xf ) =\n{ 0 \u2203i \u2208 f, s.t.xi = \u22121\n\u2212||PxfWf ||1 otherwise\nwhere \u03b7 and \u03b4i are user specified parameters, Pxf = [q(xi)]i\u2208f \u2208 R2\u00d74, and Wf \u2208 R4 is a column vector computed via solving (5) in [13] (details provided in supplementary).\nWe set \u03b7 = \u221225, and \u03b4i = 100/maxxi ||h(i) \u2212 g(xi)||22,\u2200i \u2208 V. We test all algorithms on 6 image sequences from Affine Covariant Regions Datasets 5. Each inference problem has about 1 \u00d7 103 to 3 \u00d7 103 nodes and 2 \u00d7 103 to 6 \u00d7 103 potentials. All algorithms find exact solutions. GMPLP converges before using cluster pursuit, thus GMPLP+S and GMPLP+T became the same (reported as GMPLP). From Figure 10 we can see that our \u03c0-S converges fastest among all methods in all images, followed by our MI. Both the total running time and the number of iterations required to reach an exact solution for the matching problems are reported in Tables 3 and 4. In several cases \u03c0-S takes an abnormally long time because it was trapped at a local optimum and cluster pursuit had to be applied to escape its basin of attraction. Two of the proposed methods, PS and \u03c0-S, take less running time and iterations in most cases as number of constraints and variables is sufficiently reduced without loosening the local marginal polytope.\n8 Conclusion\nWe have proposed a unified formulation of MAP LP relaxations which allows to conveniently compare different LP relaxation with different formulations of objectives and different dimensions of primal variables. With the unified formulation, a new tool, the Marginal Polytope Diagram, is proposed to describe\n5 http://www.robots.ox.ac.uk/~vgg/data/data-aff.html\nLP relaxations. With a group of propositions, we can easily find equivalence between different marginal polytope diagrams. Thus constraint reduction can be carried out via the removal of redundant nodes and replacement of equivalent edges in the marginal polytope diagram. Together with the unified formulation and constraint reduction, we have also proposed three new message\nConstraint Reduction using Marginal Polytope Diagrams 33\n1. Batra D, Nowozin S, Kohli P (2011) Tighter relaxations for map-mrf inference: A local primal-dual gap based separation algorithm. In: International Conference on Artificial Intelligence and Statistics, pp 146\u2013154 2. Blake A, Rother C, Brown M, Perez P, Torr P (2004) Interactive image segmentation using an adaptive gmmrf model. In: Computer Vision-ECCV 2004, Springer, pp 428\u2013441 3. Globerson A, Jaakkola T (2007) Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In: NIPS, vol 21 4. Hazan T, Shashua A (2010) Norm-product belief propagation: Primal-dual message-passing for approximate inference. Information Theory, IEEE Transactions on 56(12):6294\u20136316 5. Kallenberg O (2002) Foundations of modern probability. Springer Verlag\n34 Z. Zhanget al.\n6. Kohli P, Ladicky\u0300 L, Torr PH (2009) Robust higher order potentials for enforcing label consistency. IJCV 82(3):302\u2013324 7. Koller D, Friedman N (2009) Probabilistic graphical models: principles and techniques. MIT press 8. Kolmogorov V, Schoenemann T (2012) Generalized sequential treereweighted message passing. arXiv preprint arXiv:12056352 9. Komodakis N, Paragios N (2008) Beyond loose lp-relaxations: Optimizing mrfs by repairing cycles. In: Computer Vision\u2013ECCV 2008, Springer, pp 806\u2013820 10. Komodakis N, Paragios N, Tziritas G (2007) MRF optimization via dual decomposition: Message-passing revisited. In: ICCV, IEEE, pp 1\u20138 11. Kovalevsky V, Koval V (1975) A diffusion algorithm for decreasing energy of max-sum labeling problem. Glushkov Institute of Cybernetics, Kiev, USSR 12. Kumar MP, Kolmogorov V, Torr PH (2009) An analysis of convex relaxations for MAP estimation of discrete MRFs. The Journal of Machine Learning Research 10:71\u2013106\nConstraint Reduction using Marginal Polytope Diagrams 35\n13. Li H, Kim E, Huang X, He L (2010) Object matching with a locally affineinvariant constraint. In: CVPR, IEEE, pp 1641\u20131648 14. Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795\u2013804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R\u00a9 in Machine Learning 1(1-2):1\u2013305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1\u20138 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474\u20131488 25. Yanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation\u2013An Empirical Study. JMLR 7:1887\u20131907 26. Yedidia J, Freeman W, Weiss Y (2005) Constructing free-energy approximations and generalized belief propagationalgorithms. Information Theory, IEEE Transactions on 51(7):2282\u20132312\n1 Derivation of GDD Message updating\nFor convenience, we add several redundant constraints to reformulate (20) as\n   \u00b5 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u00b5t(xt) > 0,\u2200t \u2208 T,xt\u2211 xt \u00b5t(xt) = 1,\u2200t \u2208 T \u2211 xc\\s \u00b5c(xc) = \u00b5s(xs),\u2200c \u2208 C\u2032, s \u2208 S(c),xs    . (58)\nNow we keep the first two groups of constraints (thus not correspond to any Lagrangian multipliers), and introduce Lagrangian multipliers {\u03bbc\u2192s(xs)|\u2200c \u2208 C\u2032, s \u2208 S(c),xs} to the third group of the constraints. By standard Lagrangian duality, we have the dual objective below,\ng(\u03bb) = max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n[\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc) + \u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\n\u2211\nxs\n( \u00b5s(xs)\u2212 \u2211\nxc\\s\n\u00b5c(xc) ) \u03bbc\u2192s(xs) ]\n= max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n[\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc) + \u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\\{c}\n\u2211\nxs\n( \u00b5s(xs)\u2212 \u2211\nxc\\s\n\u00b5c(xc) ) \u03bbc\u2192s(xs) ] . (59)\nHere the last equation holds because if c \u2208 S(c) for some c \u2208 C\u2032, \u03bbc\u2192c(xc) is cancelled out. Rearranging variables in (59), the dual objective of GDD becomes:\ng(\u03bb) = max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n[\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc) + \u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\\{c}\n\u2211\nxs\n\u00b5s(xs)\u03bbc\u2192s(xs)\u2212 \u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\\{c}\n\u2211\nxc\n\u00b5c(xc)\u03bbc\u2192s(xs)\n]\n= max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n[\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc) + \u2211\ns\u2208 [ \u222ac\u2032\u2208C\u2032 ( S(c\u2032)\\{c\u2032}\n)] \u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,s\u2208S(c\u2032)\\{c\u2032}\n} \u2211\nxs\n\u00b5s(xs)\u03bbc\u2192s(xs)\n\u2212 \u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\\{c}\n\u2211\nxc\n\u00b5c(xc)\u03bbc\u2192s(xs)\n] . (60)\nBy definition of \u03b8\u0302t(xt), t \u2208 T in (29a) and the fact that C \u2286 T, we have\n\u2200t \u2208 T \\C, \u03b8\u0302t(xt) = 1(t \u2208 C)\u03b8t(xt) = 0,\u2200xt \u2200t \u2208 C, \u03b8\u0302t(xt) = 1(t \u2208 C)\u03b8t(xt) = \u03b8t(xt),\u2200xt (61)\nThus the first term in the most RHS of (60) can be reformulated as:\n\u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8c(xc) = \u2211\nc\u2208C\n\u2211\nxc\n\u00b5c(xc)\u03b8\u0302c(xc) + \u2211\nt\u2208T\\C\n\u2211\nxt\n\u00b5t(xt)\u03b8\u0302t(xt)\n= \u2211\nt\u2208T\n\u2211\nxt\n\u00b5t(xt)\u03b8\u0302t(xt). (62)\nBy the definition of T in (28) it is easy to verify that:\n\u2200t \u2208 T \\ [ \u222ac\u2032\u2208C\u2032 ( S(c\u2032) \\ {c\u2032} )] ,@c \u2208 C\u2032, s.t. t \u2208 S(c) \\ {c}. (63)\nThus we have \u2211\nt\u2208T \\ [ \u222ac\u2032\u2208C\u2032 ( S(c\u2032)\\{c\u2032}\n)] \u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}\n} \u2211\nxt\n\u00b5t(xt)\u03bbc\u2192t(xt) = 0. (64)\n1\nAs a result, the second term in the most RHS of (60) can be reformulated as:\n\u2211\ns\u2208 [ \u222ac\u2032\u2208C\u2032 ( S(c\u2032)\\{c\u2032}\n)] \u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,s\u2208S(c\u2032)\\{c\u2032}\n} \u2211\nxs\n\u00b5s(xs)\u03bbc\u2192s(xs)\n= \u2211\ns\u2208 [ \u222ac\u2032\u2208C\u2032 ( S(c\u2032)\\{c\u2032}\n)] \u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,s\u2208S(c\u2032)\\{c\u2032}\n} \u2211\nxs\n\u00b5s(xs)\u03bbc\u2192s(xs)\n+ \u2211\nt\u2208T \\ [ \u222ac\u2032\u2208C\u2032 ( S(c\u2032)\\{c\u2032}\n)] \u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}\n} \u2211\nxt\n\u00b5t(xt)\u03bbc\u2192t(xt)\n= \u2211\nt\u2208T\n\u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}\n} \u2211\nxt\n\u00b5t(xt)\u03bbc\u2192t(xt). (65)\nFor the third term in the most RHS of (60), we simply reformulate it as:\n\u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\\{c}\n\u2211\nxc\n\u00b5c(xc)\u03bbc\u2192s(xs) = \u2211\nt\u2208T\n\u2211\ns\u2208S(t)\\{t}\n\u2211\nxt\n1(t \u2208 C\u2032)\u00b5t(xt)\u03bbt\u2192s(xs) (66)\nUsing (62), (65) and (66), we have\ng(\u03bb) = max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n[\u2211\nt\u2208T\n\u2211\nxt\n\u00b5t(xt)\u03b8\u0302t(xt) + \u2211\nt\u2208T\n\u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}\n} \u2211\nxt\n\u00b5t(xt)\u03bbc\u2192t(xt)\n\u2212 \u2211\nt\u2208T\n\u2211\ns\u2208S(t)\\{t}\n\u2211\nxt\n1(t \u2208 C\u2032)\u00b5t(xt)\u03bbt\u2192s(xs) ] .\n= max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n\u2211\nt\u2208T\n\u2211\nxt\n\u00b5t(xt) [ \u03b8\u0302t(xt) +\n\u2211\nc\u2208 { c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}\n}\u03bbc\u2192t(xt)\u2212 1(t \u2208 C \u2032)\n\u2211\ns\u2208S(t)\\{t} \u03bbt\u2192s(xs)\n]\n(67a)\n= max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n\u2211\nt\u2208T\n\u2211\nxt\n\u00b5t(xt) [ \u03b8\u0302t(xt) + \u03bbt(xt)\u2212 \u03b3t(xt) ] (67b)\n= max \u2200t \u2208 T,xt,\u00b5t(xt) > 0,\u2211\nxt \u00b5t(xt) = 1\n\u2211\nt\u2208T\n\u2211\nxt\n\u00b5t(xt)bt(xt) (67c)\n= \u2211\nt\u2208T max xt bt(xt). (67d)\nHere from (67a) to (67c) we use the definition of \u03b3t(xt), \u03bbt(xt) and bt(xt), t \u2208 T in (29). From (67c) to (67d), as \u2200t \u2208 T,xt, \u00b5t(xt) > 0, \u2211 xt \u00b5t(xt) = 1, the maximum can be attained by letting \u00b5t(x \u2217 t ) = 1 for some x \u2217 t \u2208 argmaxxt bt(xt). When applying coordinate descent to optimise the above problem, we pick a particular c \u2208 C \u2032 and then fix all \u03bb except those \u03bbc\u2192s(xs), s \u2208 S(c). Recall the definition of T in (28), we can reformulate (28) as\nT = \u222a c\u2032\u2208C\u2032\n({c\u2032} \u222a S(c\u2032)) = [ ({c} \u222a ( S(c) \\ {c} )] \u222a (T \\({c} \u222a S(c))). (68)\n2\nThus by definition of bt(xt) in (29d), g(\u03bb) can be decomposed to three parts as follows:\ng(\u03bb) = \u2211\nt\u2208T max xt\n[ \u03b8\u0302t(xt)\u2212 \u03b3t(xt) + \u03bbt(xt) ]\n= max xc\n[ \u03b8\u0302c(xc)\u2212 \u03b3c(xc) + \u03bbc(xc) ] + \u2211\ns\u2208S(c)\\{c} max xs\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bbs(xs) ]\n+ \u2211\nt\u2208T \\({c}\u222aS(c)) max xt\n[ \u03b8\u0302t(xt)\u2212 \u03b3t(xt) + \u03bbt(xt) ]\n= max xc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs) + \u03bbc(xc)\n] (69a)\n+ \u2211\ns\u2208S(c)\\{c} max xs\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bbc\u2192s(xs) ] (69b)\n+ \u2211\nt\u2208T \\({c}\u222aS(c)) max xt\n[ \u03b8\u0302t(xt)\u2212 \u03b3t(xt) + \u03bbt(xt) ] (69c)\nNote that only (69a) and (69b) depend on \u03bbc\u2192s(xs), s \u2208 S(c), thus minimising g(\u03bb) over all \u03bbc\u2192s(xs), s \u2208 S(c) is equivalent to the sub-optimisation problem in (31).\nAn optimal solution of (31) is provided in the following proposition.\nProposition 1 \u2200s \u2208 S(c) \\ {c},xs, let\n\u03bb\u2217c\u2192s(xs) = \u2212\u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302) )] ,\nthen \u03bb\u2217c,S(c) = (\u03bb \u2217 c\u2192s(xs))s\u2208S(c)\\{c} is a solution of (31).\nProof Considering the objective of (31), we have:\ngc(\u03bbc,S(c)) = max xc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs) + \u03bbc(xc)\n] (70a)\n+ \u2211\ns\u2208S(c)\\{c} max xs\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bbc\u2192s(xs) ] (70b)\n>max xc\n{[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs) + \u03bbc(xc)\n]\n+ \u2211\ns\u2208S(c)\\{c}\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bbc\u2192s(xs)\n]}\n= max xc\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u2208S(c)\\{c}\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) ]] . (70c)\nClearly the RHS of (70c) is a lower bound of gc(\u03bbc,S(c)) for arbitrary \u03bbc,S(c). Now we show that lower bound is attained when \u03bbc,S(c) = \u03bb \u2217 c,S(c).\n3\nWhen \u03bbc\u2192s(xs) = \u03bb\u2217c\u2192s(xs),\u2200xs for each s \u2208 S(c) \\ {c}, we have\n\u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bb\u2217c\u2192s(xs) =\u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs)+{ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302)\n)]}\n= 1\n| S(c) \\ {c}| maxxc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302) )] ,\u2200xs . (71)\nThus when \u03bbc,S(c) = \u03bb \u2217 c,S(c), (70b) becomes:\n\u2211\ns\u2208S(c)\\{c} max xs\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bb\u2217c\u2192s(xs) ]\n= \u2211\ns\u2208S(c)\\{c} max xs\n1\n| S(c) \\ {c}| maxxc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302)\n)]\n= max xc\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302) )] . (72)\nThus gc(\u03bbc,S(c)) becomes:\ngc(\u03bb \u2217 c,S(c)) = max\nxc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bb\u2217c\u2192s(xs) + \u03bbc(xc)\n] + \u2211\ns\u2208S(c)\\{c} max xs\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bb\u2217c\u2192s(xs) ]\n= max xc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bb\u2217c\u2192s(xs) + \u03bbc(xc)\n]\n+ max xc\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302) )] . (73)\nAs the RHS of (70c) is a lower bound of gc(\u03bbc,S(c)) for arbitrary \u03bbc,S(c), thus we must have\nmax xc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c) \u03bb\u2217c\u2192s(xs) + \u03bbc(xc)\n] > 0, (74)\nwhich implies that the RHS of (70a) is non-negative.\n4\nNow we show that the RHS of (70a) is also non-positive.\nmax xc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bb\u2217c\u2192s(xs) + \u03bbc(xc)\n]\n= max xc\n[ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bb\u2217c\u2192s(xs)\n]\n= max xc\n{ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c}\n[ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs)\n+ 1\n| S(c) \\ {c}| maxxc\\s\n[ \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302) ) + \u03b8\u0302c(xc) + \u03bbc(xc)\n]]}\n= 1 | S(c) \\ {c}| maxxc \u2211\ns\u2208S(c)\\{c}\n{ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c}\n[ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs) ]\n\u2212max xc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c}\n[ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs) ]]}\n6 1| S(c) \\ {c}| \u2211\ns\u2208S(c)\\{c} max xc\n{ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c}\n[ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs) ]\n\u2212max xc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c}\n[ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs) ]]}\n= 1 | S(c) \\ {c}| \u2211\ns\u2208S(c)\\{c} max xs\n{ max xc\\s [ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c}\n[ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs) ]]\n\u2212max xc\\s\n[ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c}\n[ \u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs) ]]}\n=0. (75)\nThus using (74) and (75) we have\nmax xc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bb\u2217c\u2192s(xs) + \u03bbc(xc)\n] = 0. (76)\nThus using (73) and (76), we have\ngc(\u03bb \u2217 c,S(c)) = max\nxc\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u2208S(c)\\{c}\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) ]] , (77)\nwhich implies by specifying \u03bbc,S(c) = \u03bb \u2217 c,S(c), gc(\u03bb \u2217 c,S(c)) achieves the lower bound shown in RHS of (70c).\n2 Dual Decrease in a single coordinate descent step\nProposition 2 (Dual Decrease) For any c \u2208 C\u2032, the dual decrease\nd(c) = max xc\nbc(xc) + \u2211\ns\u2208S(c)\\{c} max xs bs(xs)\n\u2212max xc\n[ bc(xc) + \u2211\ns\u2208S(c)\\{c} bs(xs)\n] > 0.\n5\nProof Considering the sub-optimisation problem in (31), by definition of bt(xt), t \u2208 T we have\nbc(xc) = \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs),\u2200xc, (78a)\nbs(xs) = \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u2211\nc\u0302\u2208 { c\u2032|c\u2032\u2208C\u2032,s\u2208S(c\u2032)\\{c\u2032}\n}\u03bbc\u0302\u2192s(xs)\n= \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u2211\nc\u0302\u2208 { c\u2032|c\u2032\u2208C\u2032,c\u2032 6=c,s\u2208S(c\u2032)\\{c\u2032}\n}\u03bbc\u0302\u2192s(xs) + \u03bbc\u2192s(xs)\n= \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bbc\u2192s(xs),\u2200s \u2208 S(c) \\ {c},xs . (78b)\nWhen considering the sub-optimisation problem in (31), only \u03bbc\u2192s(xs), s \u2208 S(c) in (78) are flexible. Thus bc(xc) and bs(xs), s \u2208 S(c) can be determined by \u03bbc,S(c), and the following equation always holds:\ngc(\u03bbc,S(c)\\{c}) = max xc\n[ \u03b8\u0302c(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs) + \u03bbc(xc)\n] + \u2211\ns\u2208S(c)\\{c} max xs\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bbc\u2192s(xs) ]\n= max xc\nbc(xc) + \u2211\ns\u2208S(c)\\{c} max xs bs(xs). (79)\nNow we evaluate the optimal objective. By (77) and definition of bt(xt), t \u2208 T in (29d) we have\ngc(\u03bb \u2217 c,S(c)) = max\nxc\n[ \u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u2208S(c)\\{c}\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs)\n]]\n= max xc\n[ \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs) +\n\u2211\ns\u2208S(c)\\{c}\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bbc\u2192s(xs)\n]]\n= max xc\n[ bc(xc) + \u2211\ns\u2208S(c)\\{c} bs(xs)\n] . (80)\nThus the dual decrease in a single coordinate descent step is\nd(c) =gc(\u03bbc,S(c))\u2212 gc(\u03bb\u2217c,S(c))\n= max xc\nbc(xc) + \u2211\ns\u2208S(c)\\{c} max xs bs(xs)\u2212max xc\n[ bc(xc) + \u2211\ns\u2208S(c)\\{c} bs(xs)\n]\n>0. (81)\n3 Proof of Proposition 5\nProposition 5 If there exists x that maximises bt(xt),\u2200t \u2208 T, the solution of GDD is exact.\nProof Under the above assumptions, we have\ng(\u03bb) = \u2211\nt\u2208T max xc bc(xc) = max x\n\u2211 t\u2208T bt(xt)\n= max x\n[\u2211\nt\u2208T \u03b8\u0302t(xt) +\n\u2211 t\u2208T \u03bbt(xt)\u2212 \u2211 t\u2208T \u03b3t(xt) ]\n= max x\n[\u2211\nt\u2208T 1(t \u2208 C)\u03b8t(xt) +\n\u2211\nt\u2208T\n\u2211\nc\u2208{c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}} \u03bbc\u2192t(xt)\u2212\n\u2211 t\u2208T 1(t \u2208 C\u2032) \u2211 s\u0302\u2208S(t)\\{t} \u03bbt\u2192s\u0302(xs\u0302) ] .\n6\nAs it is easy to verify that\n\u2211\nt\u2208T\n\u2211\nc\u2208{c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}} \u03bbc\u2192t(xt) =\n\u2211\nc\u2208\u222at\u2208T{c\u2032|c\u2032\u2208C\u2032,t\u2208S(c\u2032)\\{c\u2032}}\n\u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs)\n= \u2211\nc\u2208C\u2032\n\u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs)\n= \u2211\nt\u2208T 1(t \u2208 C\u2032)\n\u2211\ns\u0302\u2208S(t)\\{t} \u03bbt\u2192s\u0302(xs\u0302).\nThus we have\ng(\u03bb) = max x\n[\u2211\nt\u2208T 1(t \u2208 C)\u03b8t(xt)\n] = max\nx\n\u2211 c\u2208C \u03b8c(xc),\nwhich completes the proof.\n4 Derivation of Belief Propagation Without Messages\nProposition 6 When optimising (31), the beliefs b\u2217c,S(c) can be computed from a bc,S(c) determined by arbitrary \u03bbc,S(c) as following:\nb\u2217s(xs) = max xc\\s\n[ bc(xc) + \u2211 s\u0302\u2208S(c)\\{c} bs\u0302(xs\u0302) ]\n| S(c) \\ {c}| ,\u2200s \u2208 S(c) \\ {c},xs\nb\u2217c(xc) =bc(xc) + \u2211\ns\u0302\u2208S(c)\\{c} bs\u0302(xs\u0302)\u2212\n\u2211\ns\u0302\u2208S(c)\\{c} b\u2217s\u0302(xs\u0302),\u2200xc .\nProof By (78) we have:\nb\u2217c(xc) = \u03b8\u0302c(xc) + \u03bbc(xc)\u2212 \u2211\ns\u2208S(c)\\{c} \u03bb\u2217c\u2192s(xs),\u2200xc . (82a)\nb\u2217s(xs) = \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bb\u2217c\u2192s(xs), s \u2208 S(c) \\ {c},xs (82b)\nAccording to (82b) and (78b), we have:\nb\u2217s(xs)\u2212 bs(xs) = \u03bb\u2217c\u2192s(xs)\u2212 \u03bbc\u2192s(xs),\u2200s \u2208 S(c) \\ {c},xs . (83)\nAccording to (82a) and (78a), we have:\nb\u2217c(xc)\u2212 bc(xc) = \u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs)\u2212\n\u2211\ns\u2208S(c)\\{c} \u03bb\u2217c\u2192s(xs),\u2200xc . (84)\nRearranging (78b) yields:\nbs(xs)\u2212 \u03bbc\u2192s(xs) = \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs),\u2200s \u2208 S(c) \\ {c},xs . (85)\nBy (78a) and (78b), we have: bc(xc) + \u2211\ns\u2208S(c)\\{c} bs(xs) =\u03b8\u0302c(xc) + \u03bbc(xc)\u2212\n\u2211\ns\u2208S(c)\\{c} \u03bbc\u2192s(xs) +\n\u2211\ns\u2208S(c)\\{c}\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) + \u03bbc\u2192s(xs) ]\n=\u03b8\u0302c(xc) + \u03bbc(xc) + \u2211\ns\u2208S(c)\\{c}\n[ \u03b8\u0302s(xs)\u2212 \u03b3s(xs) + \u03bb\u2212cs (xs) ] ,\u2200xc . (86)\n7\nUsing (85) and (86), we can reformulate (33) to:\n\u03bb\u2217c\u2192s(xs) =\u2212 \u03b8\u0302s(xs) + \u03b3s(xs)\u2212 \u03bb\u2212cs (xs) + 1\n| S(c) \\ {c}| maxxc\\s\n[ \u2211\ns\u0302\u2208S(c)\\{c}\n( \u03b8\u0302s\u0302(xs\u0302)\u2212 \u03b3s\u0302(xs\u0302) + \u03bb\u2212cs\u0302 (xs\u0302) ) + \u03b8\u0302c(xc) + \u03bbc(xc)\n]\n=\u2212 ( bs(xs)\u2212 \u03bbc\u2192s(xs) ) +\n1\n| S(c) \\ {c}| maxxc\\s\n[ bc(xc) + \u2211\ns\u0302\u2208S(c)\\{c} bs\u0302(xs\u0302)\n] , \u2200s \u2208 S(c) \\ {c},xs . (87)\nReplacing \u03bb\u2217c\u2192s(xs) in (83) with the most RHS of (87) results in:\nb\u2217s(xs) = bs(xs) + \u03bb \u2217 c\u2192s(xs)\u2212 \u03bbc\u2192s(xs)\n= bs(xs)\u2212 ( bs(xs)\u2212 \u03bbc\u2192s(xs) ) +\n1\n| S(c) \\ {c}| maxxc\\s\n[ bc(xc) + \u2211\ns\u0302\u2208S(c)\\{c} bs\u0302(xs\u0302)\n] \u2212 \u03bbc\u2192s(xs)\n= 1\n| S(c) \\ {c}| maxxc\\s\n[ bc(xc) + \u2211\ns\u0302\u2208S(c)\\{c} bs\u0302(xs\u0302)\n] ,\u2200s \u2208 S(c) \\ {c},xs, (88)\nand by reformulating (84) using (83) we get\nb\u2217c(xc) =bc(xc) + \u2211\ns\u0302\u2208S(c)\\{c} \u03bbc\u2192s\u0302(xs\u0302)\u2212\n\u2211\ns\u0302\u2208S(c)\\{c} \u03bb\u2217c\u2192s\u0302(xs\u0302)\n=bc(xc) + \u2211\ns\u0302\u2208S(c)\\{c} bs\u0302(xs\u0302)\u2212\n\u2211\ns\u0302\u2208S(c)\\{c} b\u2217s\u0302(xs\u0302),\u2200xc . (89)\nTogether with (88) and (89) we finished the proof.\n5 Proof of Proposition 7\nProposition 9 Given a graph G = (V,C) and a marginal polytope diagram GM = (VM ,EM ) of G, we have\n1. \u2200c, s, t \u2208 VM , t \u2282 s \u2282 c, if (c\u2192 s) \u2208 EM , then (c\u2192 t)\u21d4 (s\u2192 t); 2. If (c\u2192 s1), (c\u2192 s2) \u2208 EM , then \u2200t \u2208 VM , t \u2282 s1, t \u2282 s2, (s1 \u2192 t)\u21d4 (s2 \u2192 t).\nProof To show the first case, we let Ua be\nUa = {\u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302), \u2200xs\u0302 |(c\u0302\u2192 s\u0302) \u2208 (EM \\{(c\u0304\u2192 t)|c\u0304 \u2208 VM}) } . (90)\nBy the fact that (c \u2192 s) \u2208 EM , we know that (c \u2192 s) \u2208 EM \\{(c\u0304 \u2192 t)|c\u0304 \u2208 VM}. Thus for arbitrary \u00b5\u0304 \u2208 {\u00b5 |Ua}, constraints\n\u2211\nxc\\s\n\u00b5\u0304c(xc) = \u00b5\u0304s(xs),\u2200xs (91)\nmust be satisfied. Thus if a \u00b5\u0304 \u2208 {\u00b5 |Ua} also satisfies \u2211\nxc\\t \u00b5\u0304c(xc) = \u00b5\u0304t(xt),\u2200xt, we must have\n\u2211\nxs\\t\n\u00b5\u0304s(xs) = \u2211\nxs\\t\n\u2211\nxc\\s\n\u00b5\u0304c(xc) = \u2211\nxc\\t\n\u00b5\u0304c(xc) = \u00b5\u0304t(xt),\u2200xt, (92)\nwhich implies such a \u00b5\u0304 also satisfies \u2211\nxs\\t \u00b5\u0304s(xs) = \u00b5\u0304t(xt),\u2200xt.\nOn the other hand, if a \u00b5\u0304 \u2208 {\u00b5 |Ua} also satisfies \u2211\nxs\\t \u00b5\u0304s(xs) = \u00b5\u0304t(xt),\u2200xt, we must have\n\u2211\nxc\\t\n\u00b5\u0304c(xc) = \u2211\nxs\\t\n\u2211\nxc\\s\n\u00b5\u0304c(xc) = \u2211\nxs\\t\n\u00b5\u0304s(xs) = \u00b5\u0304t(xt),\u2200xt, (93)\n8\nwhich implies such a \u00b5\u0304 also satisfies \u2211\nxc\\t \u00b5\u0304c(xc) = \u00b5\u0304t(xt),\u2200xt. Thus we have\n{ \u00b5 |Ua \u222a{ \u2211\nxc\\t\n\u00b5c(xc) = \u00b5t(xt),\u2200xt} } = { \u00b5 |Ua \u222a{ \u2211\nxs\\t\n\u00b5s(xs) = \u00b5t(xt),\u2200xt} } ,\nwhich shows that edges (c\u2192 t) and (s\u2192 t) are equivalent by definition. For the second case, we define\nUb = {\u2211\nxc\u0302\\s\u0302\n\u00b5c\u0302(xc\u0302) = \u00b5s\u0302(xs\u0302), \u2200xs\u0302 |(c\u0302\u2192 s\u0302) \u2208 (EM \\{(c\u0304\u2192 t)|c \u2208 VM}) } . (94)\nBy the fact that (c \u2192 s1), (c \u2192 s2) \u2208 EM , we must have (c \u2192 s1), (c \u2192 s2) \u2208 EM \\{(c\u0304 \u2192 t)|c \u2208 VM}. Thus for arbitrary \u00b5\u0304 \u2208 {\u00b5|Ub}, constraints\n\u2211\nxc\\s1\n\u00b5\u0304c(xc) = \u00b5\u0304s1(xs1), \u2211\nxc\\s2\n\u00b5\u0304c(xc) = \u00b5\u0304s2(xs2),\u2200xs1 ,xs2\nmust be satisfied. Thus if a \u00b5\u0304 \u2208 {\u00b5 |Ub} also satisfies \u2211\nxs1\\t \u00b5\u0304s1(xs1) = \u00b5\u0304t(xt),\u2200xt, we have\n\u2211\nxs2\\t\n\u00b5\u0304s2(xs2) = \u2211\nxs2\\t\n\u2211\nxc\\s2\n\u00b5\u0304c(xc) = \u2211\nxc\\t\n\u00b5\u0304c(xc) = \u2211\nxc\\s1\n\u2211\nxs1\\t\n\u00b5\u0304c(xc) = \u00b5\u0304t(xt),\u2200xt, (95)\nwhich implies such \u00b5\u0304 also satisfies \u2211\nxs2\\t \u00b5\u0304s2(xs2) = \u00b5\u0304t(xt).\nOn the other hand, if a \u00b5\u0304 \u2208 {\u00b5 |Ub} also satisfies \u2211\nxs2\\t \u00b5\u0304s2(xs2) = \u00b5\u0304t(xt), we have\n\u2211\nxs1\\t\n\u00b5\u0304s1(xs1) = \u2211\nxs1\\t\n\u2211\nxc\\s1\n\u00b5\u0304c(xc) = \u2211\nxc\\t\n\u00b5\u0304c(xc) = \u2211\nxs2\\t\n\u2211\nxc\\s2\n\u00b5\u0304c(xc) = \u00b5\u0304t(xt), (96)\nwhich implies such \u00b5\u0304 also satisfies \u2211\nxs1\\t \u00b5\u0304s1(xs1) = \u00b5\u0304t(xt). Thus we have\n{ \u00b5 |Ub \u222a{ \u2211\nxs1\\t\n\u00b5s1(xs1) = \u00b5t(xt),\u2200xt} } = { \u00b5 |Ub \u222a{ \u2211\nxs2\\t\n\u00b5s2(xs2) = \u00b5t(xt),\u2200xt} } ,\nwhich shows that (s1 \u2192 t) and (s2 \u2192 t) are equivalent by definition.\n6 Proof of Proposition 8\nProposition 10 For arbitrary G = (V,C), and marginal polytope diagram GM = (VM ,EM ) of G, edge equivalence w.r.t. GM is an equivalence relation.\nProof By definition, reflexivity and symmetry naively holds. Thus we only prove the transitivity by proving the claim that \u2200c1, c2, c3, t \u2208 VM , t \u2286 c1, t \u2286 c2, t \u2286 c3, if (c1 \u2192 t)\u21d4 (c2 \u2192 t) and (c2 \u2192 t)\u21d4 (c3 \u2192 t) are true, then (c1 \u2192 t)\u21d4 (c3 \u2192 t) must be true.\nNow we prove the claim. Let U be\nU = {\u2211\nxc\\s\n\u00b5c(xc) = \u00b5s(xs),\u2200(c\u2192 s) \u2208 (EM \\{(c\u0302\u2192 t)|c\u0302 \u2208 VM , t \u2286 c\u0302}),xs } .\nThen by definition of edge equivalence, we must have { \u00b5 |U\u222a{ \u2211\nxc1\\t\n\u00b5c1(xc1) = \u00b5t(xt),\u2200xt} } = { \u00b5 |U\u222a{ \u2211\nxc2\\t\n\u00b5c2(xc2) = \u00b5t(xt),\u2200xt} } ,\n{ \u00b5 |U\u222a{ \u2211\nxc2\\t\n\u00b5c2(xc2) = \u00b5t(xt),\u2200xt} } = { \u00b5 |U\u222a{ \u2211\nxc3\\t\n\u00b5c3(xc3) = \u00b5t(xt),\u2200xt} } , (97)\n9\nwhich implies that\n{ \u00b5 |U\u222a{ \u2211\nxc1\\t\n\u00b5c1(xc1) = \u00b5t(xt),\u2200xt} } = { \u00b5 |U\u222a{ \u2211\nxc3\\t\n\u00b5c3(xc3) = \u00b5t(xt).\u2200xt} } . (98)\nThus we must have (c1 \u2192 t)\u21d4 (c3 \u2192 t) by definition. Over all, edge equivalence w.r.t. GM is an equivalence relation.\n7 Proof of Proposition 11\nProposition 15 All v \u2208 VM0 \\(C\u222a Im) are redundant nodes w.r.t. GM0 .\nProof Since C \u2286 (C\u222a Im), for any v \u2208 VM0 \\(C\u222a Im), we have v \u2208 VM0 \\C. Now we prove the proposition by proving that all edges to v are equivalent. Let Pv = {p|(p \u2192 v) \u2208 EM0 }. By definition of Cm, we have \u2200p1, p2 \u2208 Pv, \u2203c1, c2 \u2208 Cm, s.t. p1 \u2286 c1, p2 \u2286 c2. Thus let s = c1 \u2229 c2, by the fact that v \u2286 p1 \u2286 c1 and v \u2286 p2 \u2286 c2, we must have v \u2286 s. Moreover, if v = s we must have v = c1 \u2229 c2 \u2208 Im, which contradict to the fact that v \u2208 VM \\(C\u222a Im). Thus we must have v \u2282 s. By the fact s = c1 \u2229 c2, we have s \u2286 ci, i \u2208 {1, 2}. Thus \u2200i \u2208 {1, 2}, if pi = ci = s, (pi \u2192 v) \u21d4 (s \u2192 v) naively holds; if only one of pi and s is equal to ci, we have (pi \u2192 v) \u21d4 (s \u2192 v) by Proposition 7 (the first case); if both pi and s are not equal to ci, by Proposition 7 (the second case) we have (pi \u2192 v) \u21d4 (s \u2192 v). Thus by transitivity we have all (p\u2192 v), p \u2208 Pv are equivalent, which implies that v is redundant node w.r.t. GM0 by Proposition 9.\n8 Experiment\nWe present more experiment here.\n8.1 Results on Synthetic Data\nAdditional results on the synthetic data for the convergence in terms of both running time and the number of iterations are provided in Figure 1, which consistently shows faster convergence of the proposed methods.\n8.2 Results on PPI dataset\nAdditional results on PPI dataset for the convergence in terms of both running time and the number of iterations are provided in Figure 2.\n10\n8.3 Image segmentation\nHere the node potentials \u03b8i(xi), i \u2208 V are computed according to (5) in Kohli et al [2009] as follows,\n\u03b8i(xi) = \u03b8T\u03d5T (xi) + \u03b8col\u03d5col(xi) + \u03b8l\u03d5l(xi). (99)\nWe choose \u03b8T = 0, \u03b8col = 1 and \u03b8l = 0, thus we have \u03b8i(xi) = \u03d5col(xi). We learn \u03d5col(xi) from the data using Gaussian Mixture Models as in Blake et al [2004]. We follow (12) and (10) in Kohli et al [2009] to compute edge potentials and high order potentials (with \u03b8\u03b1 = 0, \u03b8 h p = 0, \u03b8 h v = 25 and \u03b8 h \u03b2 being set to the reciprocal of variance of all pixels i.e. grey value in [0, 255]). Additional results are provided in Figures 3 and 4. We can see that the proposed methods, PS, \u03c0-S and MI, not only find the exact solution on all three problems, but also converge much faster than others.\n11\n12\n8.4 Image Matching\nWe first detect key points from source images and destination images by using SIFT detectors implemented in OpenCV with default parameters. As in Li et al [2010], for every p(i), i \u2208 V, if its nearest 3 neighbours p(i1), p(i2), p(i3) are not in a line, there must exist a column vector W \u2032 f in R3 s.t.\np(i) = [p(i1), p(i2), p(i3)]W \u2032 f . (100)\nThen let Wf = [W \u2032 f ,\u22121] we must have\n[p(i1), p(i2), p(i3), p(i)]Wf = 0, (101)\nand the equation is invariant to affine transformation Li et al [2010]. Thus for every p(i), i \u2208 V, if its nearest 3 nearest neighbours p(i1), p(i2), p(i3) are not in a line, there is an order-4 cluster f = {i1, i2, i3, i}. Let Pxf = [q(xi1), q(xi2), q(xi3), q(xi)]. ||PxfWf ||1 can be used as a geometry cost.\nAdditional results are provided in the following figures. Although all algorithms achieves exact solutions on all data sets, the proposed methods often show better convergence rate in terms of both iterations and running time. The plot of dataset ubc45 is excluded since all algorithms achieve the exact solution at the first iteration.\n13\n14\n15\n16\n8.5 Comparing Belief Propagation and Message Passing\nOur GDD based algorithms can be implemented as either a message passing (MP) procedure or a belief propagation procedure without messages. We implement both, and observe that both have similar speed as shown in Figure 15. Of course, the latter uses less storage.\n17\nReferences\nBatra D, Nowozin S, Kohli P (2011) Tighter relaxations for map-mrf inference: A local primal-dual gap based separation algorithm. In: International Conference on Artificial Intelligence and Statistics, pp 146\u2013154\nBlake A, Rother C, Brown M, Perez P, Torr P (2004) Interactive image segmentation using an adaptive gmmrf model. In: Computer Vision-ECCV 2004, Springer, pp 428\u2013441\nGloberson A, Jaakkola T (2007) Fixing max-product: Convergent message passing algorithms for MAP LPrelaxations. In: NIPS, vol 21\nHazan T, Shashua A (2010) Norm-product belief propagation: Primal-dual message-passing for approximate inference. Information Theory, IEEE Transactions on 56(12):6294\u20136316\nKallenberg O (2002) Foundations of modern probability. Springer Verlag\nKohli P, Ladicky\u0300 L, Torr PH (2009) Robust higher order potentials for enforcing label consistency. IJCV 82(3):302\u2013 324\nKoller D, Friedman N (2009) Probabilistic graphical models: principles and techniques. MIT press\nKolmogorov V, Schoenemann T (2012) Generalized sequential tree-reweighted message passing. arXiv preprint arXiv:12056352\n18\nKomodakis N, Paragios N (2008) Beyond loose lp-relaxations: Optimizing mrfs by repairing cycles. In: Computer Vision\u2013ECCV 2008, Springer, pp 806\u2013820\nKomodakis N, Paragios N, Tziritas G (2007) MRF optimization via dual decomposition: Message-passing revisited. In: ICCV, IEEE, pp 1\u20138\nKovalevsky V, Koval V (1975) A diffusion algorithm for decreasing energy of max-sum labeling problem. Glushkov Institute of Cybernetics, Kiev, USSR\nKumar MP, Kolmogorov V, Torr PH (2009) An analysis of convex relaxations for MAP estimation of discrete MRFs. The Journal of Machine Learning Research 10:71\u2013106\nLi H, Kim E, Huang X, He L (2010) Object matching with a locally affine-invariant constraint. In: CVPR, IEEE, pp 1641\u20131648\nLowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157\nMcEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299\nMeshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031\nPakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873\nSchwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS\nSontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using messagepassing. In: UAI, AUAI Press, pp 503\u2013510\nSontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press\nSontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795\u2013804\nWainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R\u00a9 in Machine Learning 1(1-2):1\u2013305\nWerner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1\u20138\nWerner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474\u20131488\nYanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation\u2013An Empirical Study. JMLR 7:1887\u20131907\nYedidia J, Freeman W, Weiss Y (2005) Constructing free-energy approximations and generalized belief propagationalgorithms. Information Theory, IEEE Transactions on 51(7):2282\u20132312\n19"}], "references": [{"title": "Tighter relaxations for map-mrf inference: A local primal-dual gap based", "author": ["D Batra", "S Nowozin", "P Kohli"], "venue": null, "citeRegEx": "Batra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Batra et al\\.", "year": 2011}, {"title": "Interactive image segmentation using an adaptive gmmrf", "author": ["A Blake", "C Rother", "M Brown", "P Perez", "P Torr"], "venue": null, "citeRegEx": "Blake et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blake et al\\.", "year": 2004}, {"title": "Fixing max-product: Convergent message passing algorithms for MAP LP", "author": ["A Globerson", "T Jaakkola"], "venue": null, "citeRegEx": "Globerson and Jaakkola,? \\Q2007\\E", "shortCiteRegEx": "Globerson and Jaakkola", "year": 2007}, {"title": "A (2010) Norm-product belief propagation: Primal-dual message-passing for approximate infer", "author": ["Hazan T", "Shashua"], "venue": null, "citeRegEx": "T and Shashua,? \\Q2010\\E", "shortCiteRegEx": "T and Shashua", "year": 2010}, {"title": "Foundations of modern probability", "author": ["O Kallenberg"], "venue": null, "citeRegEx": "Kallenberg,? \\Q2002\\E", "shortCiteRegEx": "Kallenberg", "year": 2002}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["P Kohli", "L Ladick\u1ef3", "PH Torr"], "venue": null, "citeRegEx": "Kohli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kohli et al\\.", "year": 2009}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D Koller", "N Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Generalized sequential tree-reweighted message passing", "author": ["V Kolmogorov", "T Schoenemann"], "venue": null, "citeRegEx": "Kolmogorov and Schoenemann,? \\Q2012\\E", "shortCiteRegEx": "Kolmogorov and Schoenemann", "year": 2012}, {"title": "Beyond loose lp-relaxations: Optimizing mrfs by repairing cycles", "author": ["N Komodakis", "N Paragios"], "venue": "Vision\u2013ECCV", "citeRegEx": "Komodakis and Paragios,? \\Q2008\\E", "shortCiteRegEx": "Komodakis and Paragios", "year": 2008}, {"title": "MRF optimization via dual decomposition: Message-passing revisited", "author": ["N Komodakis", "N Paragios", "G Tziritas"], "venue": "In: ICCV,", "citeRegEx": "Komodakis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Komodakis et al\\.", "year": 2007}, {"title": "A diffusion algorithm for decreasing energy of max-sum labeling problem", "author": ["V Kovalevsky", "V Koval"], "venue": "Glushkov Institute of Cybernetics,", "citeRegEx": "Kovalevsky and Koval,? \\Q1975\\E", "shortCiteRegEx": "Kovalevsky and Koval", "year": 1975}, {"title": "An analysis of convex relaxations for MAP estimation of discrete MRFs", "author": ["MP Kumar", "V Kolmogorov", "PH Torr"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Object matching with a locally affine-invariant constraint", "author": ["H Li", "E Kim", "X Huang", "L He"], "venue": "In: CVPR,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Object recognition from local scale-invariant features", "author": ["DG Lowe"], "venue": "ICCV 1999, IEEE,", "citeRegEx": "Lowe,? \\Q1999\\E", "shortCiteRegEx": "Lowe", "year": 1999}, {"title": "Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance", "author": ["RJ McEliece", "M Yildirim"], "venue": null, "citeRegEx": "McEliece and Yildirim,? \\Q2003\\E", "shortCiteRegEx": "McEliece and Yildirim", "year": 2003}, {"title": "Convergence rate analysis of map coordinate minimization algorithms", "author": ["O Meshi", "T Jaakkola", "A Globerson"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Meshi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2012}, {"title": "Estimation and marginalization using the kikuchi approximation methods", "author": ["P Pakzad", "V Anantharam"], "venue": "Neural Computation", "citeRegEx": "Pakzad and Anantharam,? \\Q2005\\E", "shortCiteRegEx": "Pakzad and Anantharam", "year": 2005}, {"title": "Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins", "author": ["AG Schwing", "T Hazan", "M Pollefeys", "R Urtasun"], "venue": null, "citeRegEx": "Schwing et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwing et al\\.", "year": 2012}, {"title": "Tightening LP relaxations for MAP using messagepassing", "author": ["D Sontag", "T Meltzer", "A Globerson", "Y Weiss", "T Jaakkola"], "venue": null, "citeRegEx": "Sontag et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2008}, {"title": "Introduction to dual decomposition for inference", "author": ["D Sontag", "A Globerson", "T Jaakkola"], "venue": "SJ (eds) Optimization for Machine Learning,", "citeRegEx": "Sontag et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2011}, {"title": "Efficiently Searching for Frustrated Cycles in MAP Inference", "author": ["D Sontag", "DK Choe", "Y Li"], "venue": null, "citeRegEx": "Sontag et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2012}, {"title": "Graphical models, exponential families, and variational inference. Foundations and Trends R", "author": ["MJ Wainwright", "MI Jordan"], "venue": null, "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Linear Programming Relaxations and Belief Propagation\u2013An", "author": ["C Yanover", "T Meltzer", "Y Weiss"], "venue": null, "citeRegEx": "Yanover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yanover et al\\.", "year": 2006}, {"title": "Constructing free-energy approximations and generalized belief propaga", "author": ["W Freeman", "Y Weiss"], "venue": null, "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "Kallenberg O (2002) Foundations of modern probability.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features.", "startOffset": 0, "endOffset": 15}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets.", "startOffset": 0, "endOffset": 147}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms.", "startOffset": 0, "endOffset": 344}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods.", "startOffset": 0, "endOffset": 518}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins.", "startOffset": 0, "endOffset": 680}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing.", "startOffset": 0, "endOffset": 840}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference.", "startOffset": 0, "endOffset": 974}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference.", "startOffset": 0, "endOffset": 1142}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795\u2013804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference.", "startOffset": 0, "endOffset": 1272}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795\u2013804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R \u00a9 in Machine Learning 1(1-2):1\u2013305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf).", "startOffset": 0, "endOffset": 1419}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795\u2013804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R \u00a9 in Machine Learning 1(1-2):1\u2013305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1\u20138 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction.", "startOffset": 0, "endOffset": 1607}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795\u2013804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R \u00a9 in Machine Learning 1(1-2):1\u2013305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1\u20138 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474\u20131488 25. Yanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation\u2013An Empirical Study.", "startOffset": 0, "endOffset": 1812}, {"referenceID": 13, "context": "Lowe DG (1999) Object recognition from local scale-invariant features. In: ICCV 1999, IEEE, vol 2, pp 1150\u20131157 15. McEliece RJ, Yildirim M (2003) Belief propagation on partially ordered sets. In: Mathematical systems theory in biology, communications, computation, and finance, Springer, pp 275\u2013299 16. Meshi O, Jaakkola T, Globerson A (2012) Convergence rate analysis of map coordinate minimization algorithms. In: Advances in Neural Information Processing Systems 25, pp 3023\u20133031 17. Pakzad P, Anantharam V (2005) Estimation and marginalization using the kikuchi approximation methods. Neural Computation 17(8):1836\u20131873 18. Schwing AG, Hazan T, Pollefeys M, Urtasun R (2012) Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In: Proc. NIPS 19. Sontag D, Meltzer T, Globerson A, Weiss Y, Jaakkola T (2008) Tightening LP relaxations for MAP using message-passing. In: UAI, AUAI Press, pp 503\u2013510 20. Sontag D, Globerson A, Jaakkola T (2011) Introduction to dual decomposition for inference. In: Sra S, Nowozin S, Wright SJ (eds) Optimization for Machine Learning, MIT Press 21. Sontag D, Choe DK, Li Y (2012) Efficiently Searching for Frustrated Cycles in MAP Inference. In: UAI, AUAI Press, pp 795\u2013804 22. Wainwright MJ, Jordan MI (2008) Graphical models, exponential families, and variational inference. Foundations and Trends R \u00a9 in Machine Learning 1(1-2):1\u2013305 23. Werner T (2008) High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In: CVPR 2008. IEEE Conferenceon, IEEE, pp 1\u20138 24. Werner T (2010) Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. PAMI, IEEE Transactions on 32(8):1474\u20131488 25. Yanover C, Meltzer T, Weiss Y (2006) Linear Programming Relaxations and Belief Propagation\u2013An Empirical Study. JMLR 7:1887\u20131907 26. Yedidia J, Freeman W, Weiss Y (2005) Constructing free-energy approximations and generalized belief propagationalgorithms.", "startOffset": 0, "endOffset": 1944}], "year": 2014, "abstractText": "LP relaxation-based message passing algorithms provide an effective tool for MAP inference over Probabilistic Graphical Models. However, different LP relaxations often have different objective functions and variables of differing dimensions, which presents a barrier to effective comparison and analysis. In addition, the computational complexity of LP relaxation-based methods grows quickly with the number of constraints. Reducing the number of constraints without sacrificing the quality of the solutions is thus desirable. We propose a unified formulation under which existing MAP LP relaxations may be compared and analysed. Furthermore, we propose a new tool called Marginal Polytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited such as node redundancy and edge equivalence. We show that using Marginal Polytope Diagrams allows the number of constraints to be reduced without loosening the LP relaxations. Then, using Marginal Polytope Diagrams and constraint reduction, we develop three novel message passing algorithms, and demonstrate that two of these show a significant improvement in speed over state-of-art algorithms while delivering a competitive, and sometimes higher, quality of solution.", "creator": "LaTeX with hyperref package"}}}