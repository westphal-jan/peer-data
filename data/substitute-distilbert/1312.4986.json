{"id": "1312.4986", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2013", "title": "A Comparative Evaluation of Curriculum Learning with Filtering and Boosting", "abstract": "not some instances in a data set are equally beneficial for inferring a model of the data. specific instances ( such as outliers ) are detrimental against inferring a model of the data. several computer learning techniques treat topics in a data set differently affecting conditions such as curriculum learning, filtering, and detection. however, an automated method for assessed how beneficial an instance is for inferring a parameter of the instance does not exist. in this paper, we present an automated method that orders the instances in a data set by complexity based on the their likelihood of not misclassified ( instance hardness ). the underlying assumption inside this method says viewing instances with a high likelihood actually being misclassified represent more complex concepts in a data set. ordering the instances in a data set allows a learning algorithm to focus on recalling chosen unusual instances and ignore the only ones. we compare ordering the instances in a data set in curriculum learning, filtering and boosting. we find that detecting the instances significantly increases classification accuracy and that filtering has the largest impact affect classification accuracy. on a set of 52 data sets, ordering the instances increases given average accuracy from 81 % to 84 %.", "histories": [["v1", "Tue, 17 Dec 2013 22:12:52 GMT  (23kb)", "http://arxiv.org/abs/1312.4986v1", "19 pages, 2 figures, 6 tables"]], "COMMENTS": "19 pages, 2 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michael r smith", "tony martinez"], "accepted": false, "id": "1312.4986"}, "pdf": {"name": "1312.4986.pdf", "metadata": {"source": "CRF", "title": "An Extensive Evaluation of Filtering Misclassified Instances in Supervised Classification Tasks", "authors": ["Michael R. Smith", "Tony Martinez"], "emails": ["msmith@axon.cs.byu.edu,", "martinez@cs.byu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n49 86\nv1 [\ncs .L\nG ]\n1 7\nKeywords: curriculum learning, instance hardness, outlier filtering, boosting"}, {"heading": "1 Introduction", "text": "The goal of supervised machine learning is to model a task by \u201cteaching\u201d a learning algorithm through the presentation of labeled instances from a data set. The training instances are generally presented to a learning algorithm in no particular order and are generally treated as being equally important for inferring a model of the data. This can be problematic for many machine learning algorithms in deciding which initial search direction will lead to the optimal solution. Without guidance, the learning algorithm may choose an inappropriate initial direction to search the hypothesis space from which it may never be able to fully correct due to an inability to unlearn previously learned concepts.\nConsider the hypothetical two-dimensional data set in Figure 1. Instances A, B, C, and D could be considered outliers and represent differing degrees of their extent of being an outlier. The instances in the dotted oval represent the border points, which could be used to define the classification boundary. Many learning algorithms have a built in mechanism to avoid overfitting the outliers, however, the presence of outliers could still affect the inferred classification border. Knowing before training begins which instances are the most informative instances could improve learning.\nA number of methods have been developed that treat individual instances in a dataset differently during training to focus on the most informative ones. Filtering identifies and removes noisy instances and outliers from a data set prior to training and generally results in an increase in classification accuracy on non-filtered test data (Brodley and Friedl, 1999; Gamberger et al., 2000; Smith and Martinez, 2011). Boosting also treats instances differently during training by incrementally adjusting the weights of the instances during training (Schapire, 1990; Freund, 1990). Boosting iteratively trains m base learners and reweights the training data after each model is inferred such that the probability for selecting instances that are misclassified increases. Boosting, however, has been shown to be prone to overfitting outliers and noisy instances.\nCurriculum learning was recently formalized by Bengio et al. (2009) as a means of using an ordering of the training data from simplest to most complex to train a learning algorithm. Instances representing simple concepts are given a weight of 1 while instances that represent complex concepts are initially given a weight of 0, similar to filtering. As training progresses and the simper training instances are learned, a subset of the more complex training instances receive a weight of 1. This process continues until all of the training instances receive a weight of 1. From a cognitive point of view, curriculum learning is based on how humans acquire knowledge. For example, in schools, subject matter is\norganized into curricula such that simpler or foundational ideas are presented first. As learning progresses, more complex concepts and ideas can be learned by using the already learned simpler ideas. The main deficiencies of most previous work in curriculum learning are: 1) that there is no general method for ordering the instances by complexity and 2) that there is no method for determining when to add more complex instances to the training set. In previous work for curriculum learning, the ordering was done by hand or by some heuristic specific to the learning task (e.g., the number of words in a sentence).\nIn this paper, we present an automated method for ordering the instances in a data set based on their likelihood of being misclassified (instance hardness (Smith et al., 2013)) which we call a hardness ordering. The underlying assumption of ordering the instances by their hardness is that harder instances represent more complex and/or outlier instances. We use a hardness ordering to examine curriculum learning, filtering, and avoiding overfitting in boosting on a set of 52 UCI data sets using multilayer perceptrons (MLPs) and decision trees (DTs). As filtering and boosting have received considerable attention (Krause and Singer, 2004; Liu et al., 2004), we focus primarily on developing curriculum learning and comparing curriculum learning with filtering and boosting. We also examine when to add more complex instances during the training process in curriculum learning.\nWe find that ordering the instances in a data set by complexity significantly improves classification accuracy. Specifically, curriculum learning significantly increases classification accuracy for MLPs and significantly decreases the classification accuracy for DTs. Curriculum learning is better suited to learning algorithms that can be incrementally updated similar to MLPs. Filtering the most complex instances achieves higher classification accuracy than curriculum learning and boosting. We also examine boosting and curriculum learning with filtering. Boosting and curriculum learning with filtering significantly increases the accuracy over boosting and curriculum learning without filtering. Filtering has the most significant effect on the accuracy. We postulate that the significant change in accuracy when filtering is due to the fact that filtering creates a simpler surface for a learning algorithm to model by removing outliers and noisy instances. This in turn increases the generalization accuracy. By contrast, curriculum learning assumes that previously unlearnable complex instances become learnable once the simpler and foundational instances are learned. By adding the more complex instances into the training process, curriculum learning generates more complex models and the generalization accuracy decreases.\nThe contributions of this paper include: 1) an automated method for ordering the instances in a data set based on their likelihood of being misclassified, 2) an examination of curriculum learning in general machine learning problems, whereas, previously, curriculum learning has only been examined in limited situations, and 3) a comparison of curriculum learning, filtering and boosting.\nThe remainder of the paper is organized as follows: In Section 2, we review related works. In Section 3, we present how to generate a hardness ordering. Implementation details of how we implement curriculum learning using a hardness ordering and a comparison with filtering and boosting are presented in\nSection 4. Section 5 gives conclusions and directions for future work."}, {"heading": "2 Related Works", "text": "Emphasizing the most important instances has led to success in a number of previous works. Our work is motivated by the lack of a method to order the instances in curriculum learning and the importance of the early stages of training a learning algorithm. Elman (1993) realized that the early stages of training dictate what solutions are possible in multilayer perceptrons. This is true for most gradient descent or greedy learning algorithms. During the early stages of training, the initial direction to search the hypothesis space is chosen. All practical machine learning algorithms avoid the computational cost of storing all possible hypotheses consistent with the training data by choosing a promising hypothesis at step t. The hypothesis space is then searched in a step-wise fashion, meaning that the hypothesis found at step t+1 is a continuation of the search of the hypothesis space at step t. It can also be difficult for most machine learning algorithms to backtrack and unlearn learned concepts. This adds more importance to the initial search direction that is chosen to follow. To aid in this problem, Elman introduced the idea of \u201cstarting small\u201d meaning that only simple concepts should be used during the early stages of training. The simple concepts guide the initial search direction of the learning algorithm and can facilitate learning more complex concepts for some situations (Elman used grammar rules). Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011). Each approach shares the idea of breaking the learning task into subcomponents and then, starting with the simplest concepts, train by gradually increasing concept complexity.\nBengio et al. (2009) formalized these ideas in curriculum learning. The idea behind curriculum learning is to first optimize a smoothed objective function and gradually reduce the degree of smoothing during the training process. At a more concrete level, curriculum learning is a weighting scheme for training. Each training instance is assigned a weight which controls how the instance is used in training. Initially, the weights on the training instances favor the \u201ceasier\u201d instances or those that represent simpler concepts. As training proceeds, the weights on the training instances are updated such that \u201charder\u201d instances and more complex concepts are introduced into the training set. This continues until all of the instances in the target training set are uniformly weighted.\nKumar et al. (2010) recently presented self-paced learning for latent variable models, building on the idea of curriculum learning. In self-paced learning, a set of latent variables are learned that indicate which instances should be included for training. The choice of which instances are used in training is left to the optimization technique and the number of instances used is controlled with a variable which is annealed to eventually use all of the instances. They assume that the simpler instances are used first for training, but no explicit ordering on the instances is generated. The primary shortcomings of previous work in\ncurriculum learning are that there is no automated method that orders the instances in a data set by complexity and that there is no method to determine when to add more complex instances to the training set.\nBoosting is another approach that incrementally adjusts the weights of the instances during training (Schapire, 1990; Freund, 1990). Boosting is an algorithm designed to target misclassified instances during training such that as training continues, uninformative instances that lie within a cluster of instances of the same class are suppressed. Boosting iteratively trains m base learners on a subset of the training data to form an ensemble. After an iteration of training, the data is weighted such that the probability of selecting instances that are misclassified increases and the probability of selecting the instances that are classified correctly decreases. These techniques assume that the misclassified instances are the most informative and should be weighted more. However, this can lead to overfitting noise and new methods were proposed to ignore suspected outliers and weight the more informative or boundary instances higher (Krause and Singer, 2004). Boosting differs from curriculum learning in that it is an ensemble method and the influence of the easier instances decrease as training progresses.\nFiltering (removing outlier or noisy instances prior to training) is a similar approach to curriculum learning. Outliers and noisy instances have been observed to adversely affect an induced model (Smith and Martinez, 2011). Thus, the goal of filtering is to reduce the effects of outlier or noisy instances by removing them prior to training. One of the difficulties with outlier and noise identification is that there is no agreed-upon definition of what constitutes an outlier or noise. As such, a variety of different noise and outlier detection methods exist, such as statistical methods (Barnett and Lewis, 1978), densitybased clustering (Breunig et al., 2000), and classification-based methods (John, 1995; Brodley and Friedl, 1999). These methods have been used to identify and remove outliers prior to training, resulting in higher classification accuracy (Brodley and Friedl, 1999; Gamberger et al., 2000; Liu et al., 2004)."}, {"heading": "3 Ordering the Instances", "text": "Many machine learning techniques could benefit from knowing how beneficial an instance is to inferring a model of the data. In this paper, we use instance hardness (Smith et al., 2013) to order the instances by complexity. Instance hardness posits that each instance in a data set has a hardness property that indicates the likelihood that it will be misclassified. Instances with high instance hardness are frequently misclassified. For example, outliers and mislabeled instances are expected to have high instance hardness since a learning algorithm will have to overfit to classify them correctly. The instance hardness of an instance is primarily dependent on its relationship to the other instances in the data set and, by extension, to the underlying data distribution. Obviously, some instances are harder for some learning algorithms than for others. For example, some instances are harder for a linear classifier than for a non-linear classifier\nbecause a non-linear classifier is capable of producing more complex decision boundaries. Ideally, instance hardness should be calculated with respect to the underlying data distribution. However, as we do not have access to the underlying data distribution, instance hardness is estimated empirically using a set of learning algorithms that have been successfully applied to a wide range of problems and, because of their success, they are commonly used in practice. Thus, instance hardness is also dependent on the method used to estimate the likelihood of a instance being misclassified.\nFormally, instance hardness is the probability that an instance xi will be misclassified with respect to the underlying data distribution p(x, y) of a task:\nIH(xi, yi) = 1\u2212 p(yi|xi, p(x, y)) (1)\nwhere yi is the assigned class label for xi. The unknown distribution p(x, y) is estimated using an observed training set t from T . We assume that t is the only available sampling of p(x, y). The training set t is used in place of p(x, y) proceeding forward. Instance hardness estimates p(yi|xi, t) empirically using a set of learning algorithms. To be explicit, p(yi|xi, t) is estimated as p(yi|xi, t, g) which is that probability of yi given xi as input to learning algorithm g trained on t. Using a single learning algorithm to calculate the probability of yi given xi incorporates the bias of the learning algorithm g. To lessen the bias of a single learning algorithm and to gain insight into the objective function of the task, we use a set of learning algorithms to estimate p(yi|xi, t). Integrating over the set of all learning algorithms G and weighting each learning algorithm by p(g) instance hardness is:\nIH(xi, yi) = 1\u2212 \u222b G p(g)p(yi|xi, t, g) dg. (2)\nThe quality of the estimate of instance hardness depends in large part on the prior distribution of learning algorithms p(g). The distribution p(g) represents the probability that a particular learning algorithm will be used to infer a model of the task. Not all learning algorithms are equally likely. For example, modeling a task with a decision tree is more likely than using a learning algorithm that always predicts the same class. If all learning algorithms were equally likely, then all instances would have the same instance hardness value under the no free lunch theorem (Wolpert, 1996). In this paper, instance hardness is estimated using learning algorithms that are successfully and commonly used in practice. We refer to this set as the set of empirically successful learning algorithms (ESLAs). A natural way to approximate p(g) is to weight ESLAs with a non-zero weight and to weight the non-ESLAs with a weight of 0. Since it is impossible to precisely define the evolving set of ESLAs, it is approximated by selecting a diverse subset of the set of ESLAs (L). Although we use ESLAs to calculate instance hardness for general machine learning algorithms, other sets of learning algorithms could be used to calculate instance hardness such as those algorithms that are used in text mining or the set of neural network algorithms.\nGiven the set L of ESLAs, stochastic integration can be used to approximate the integral from Equation 2:\nIH(xi, yi) \u2248 1\u2212 1\n|L| |L|\u2211 j=1 p(yi|xi, t, gj) (3)\nwhere p(g) is approximated as 1|L| . Since all of the learning algorithms in L do not produce a probability distribution, we use the indicator function to approximate p(yi|xi, t, g).\nFor instance hardness, the diversity of the learning algorithms is determined using unsupervised meta-learning (Lee and Giraud-Carrier, 2011). Unsupervised meta-learning uses Classifier Output Difference (COD) (Peterson and Martinez, 2005) to measure the diversity between learning algorithms. COD measures the distance between two learning algorithms as the probability that the learning algorithms make different predictions. Unsupervised meta-learning then clusters the learning algorithms based on their COD scores with hierarchical agglomerative clustering. We considered 20 commonly used learning algorithms. The dendrogram from clustering the considered learning algorithms using unsupervised meta-learning is shown in Figure 2. The height of the line connecting two clusters corresponds to the distance (COD value) between them. The COD value 0.18 was chosen because the value produced clusters that included learning algorithms from different families of learning algorithms (e.g. decision tree algorithms, nearest-neighbor algorithms, etc.). A representative algorithm from each cluster was used to create L as shown in Table 1. The actual number of learning algorithms is not as important as selecting a set of learning algorithms that approximately represents the set of ESLAs. We have found that using a small, diverse set of ESLAs produces significant improvements in accuracy.\nThe learning algorithms are used as implemented in Weka with their default parameters (Hall et al., 2009). By adjusting the parameters, some instances may be correctly classified more consistently. However, parameter optimization is an expensive process. Hence, using default parameters gives insight into which instances are misclassified in most practical scenarios. The learning algorithms are evaluated using 5 by 10-fold cross-validation1. We use five folds to better measure the instance hardness of each instance and to protect against the dependency on the data used in each fold."}, {"heading": "4 Empirical Evaluation", "text": "With the ordering of the instances provided by instance hardness, we examine how to use a hardness ordering in the learning process. We examine using a hardness ordering on a set of 52 UCI data sets (Frank and Asuncion, 2010) shown in Table 2. We use a hardness ordering in curriculum learning, filtering and boosting. As curriculum learning is less fully explored, we focus on developing curriculum learning. We then compare curriculum learning with filtering and boosting. We implement the methods using multilayer perceptrons (MLPs) trained with backpropagation and decision trees (DTs) trained using C4.5 (Quinlan, 1993). We chose a MLP because the training set can be augmented during training, which is a natural fit for curriculum learning. Other incremental learning algorithms could also be used. We train a MLP until convergence, where convergence is determined by a diminishing learning rate. Initially, the learning rate is set to 0.3 and it is reduced by 70% if the error on the training data does not decrease over an entire epoch. Training continues until the learning rate is less than 0.001. This helps protect against the choice of learning rate. We also examine curriculum learning in DTs to show that curriculum learning is not appropriate for all learning algorithms and to demonstrate the effects of filtering and boosting in multiple learning algorithms.\n15 by 10-fold cross-validation runs 10-fold cross-validation 5 times, each time with a dif-\nferent random seed for selecting the 10 partitions of the data."}, {"heading": "4.1 Curriculum Learning", "text": "Curriculum learning trains a learning algorithm on the simplest concepts prior to training on the more complex instances, analogous to teaching a child a subject such as mathematics: addition and subtraction are taught prior to algebra which is taught prior to calculus, etc. One of the shortcomings of curriculum learning is that there is no automated way to generate curricula for curriculum learning. Instance hardness is a natural fit for curriculum learning, providing an ordering of the instances from easiest to hardest."}, {"heading": "4.1.1 Implementation Details", "text": "Even with an ordering for curriculum learning, there still remain a couple of questions that need to be addressed for general machine learning problems: 1) the initial complexity of the training instances, and 2) when to add more complex training instances.\nThe complexity of the initial training instances has a high likelihood of affecting the final model since the initial training instances determine the search direction that will be continued as more instances are added to the training set. Providing a learning algorithm with instances that are too easy and uninformative could lead to a learning algorithm choosing an arbitrary and/or incorrect gradient to follow. On the other hand, if the initial complexity is too complex, subconcepts may not be learned since they are grouped with the concepts that build on them.\nAs training begins, instances with an instance hardness value below a threshold are added to the training set. To gain insight into curriculum learning, we try different values for the initial complexity of the training set: 0, 0.25, 0.5, and 0.75. Each time new instances are added to the training set, the instance hardness threshold (IH ) is incremented by 0.1 until all of the training instances are used for training.\nWhen to add more complex instances to a training set could also have an impact on the final outcome of the induced model. If a learning algorithm does not train on the initial instances long enough, a learning algorithm may not find the optimal gradient to follow. Training too long on a subset of the instances has the potential for a learning algorithm to overfit the subset. This could hinder learning the more complex instances that will be introduced later in the training process.\nFor MLPs, we consider two techniques of when to add more complex instances to the training set.\nEvery n epochs. For this method, instances with increasing instance hardness values are added to the training set every n epochs. This method is simple and it provides an indication of whether it may be better to let the MLP train more or less before adding more instances to the training set. We set n to 25, 50, 100, 200, 300, 400, and 500 to give an indication of how long to train before adding more instances. Other values could have been used as well.\nConvergence. This method trains the MLP to convergence on the training set before adding more complex instances to the training set.\nFor DTs, more complex instances are added to the training set after a DT is induced using the given training set. Once more complex instances are added to the training set, the more complex training instances are propagated to a leaf node and the instances are evaluated to determine if the tree should be expanded at this leaf node. As the training set is augmented, the previously trained portions of the DT are not modified. For DTs, we consider pruning and not pruning the tree before adding more complex instances to the training set. For pruning at a given node, the error is estimated for its descendant branches as well as if the node was a leaf node. If the estimated error of the node as a leaf node is lower that the estimated error of the descendant branches, the node\u2019s descendants are pruned."}, {"heading": "4.1.2 Results", "text": "The results from implementing curriculum learning in MLPs and DTs are shown in Table 3. Table 3 provides the aggregate statistics for each method of adding more complex instances to the training set (average over all datasets, p-value from the Wilcoxon signed-rank test, and number of times that the accuracy from curriculum learning is greater than, equal to, or less than the original).\nFor MLPs, curriculum learning significantly increases classification accuracy with an alpha value of 0.05 for all of the methods of when to add more complex instances to the training set examined in this work. Adding more complex instances to the training set after training for 100 epochs is the most significant although adding more more complex instances after 50 training epochs results in the highest average classification accuracy. The average accuracy increases from 81.41% to 82.02% when more complex instances are added to the training set after training for 100 epochs. It is interesting to note that curriculum learning considerably decreases the classification accuracy on some of the datasets. The nursery dataset decreases in accuracy from 99.82% to around 97.42% regardless of when more complex training instances are added to the training set. The chess dataset decreases in accuracy as well. The reasoning behind this could be that there are no subconcepts in the dataset. On other datasets, curriculum learning increases classification accuracy considerably. For example, the postoperativePatient dataset increases in accuracy from 68.22% to 71.11% and the contact lenses dataset increases from 60% to 79.17%.\nFor DTs, curriculum learning significantly decreases classification accuracy. Despite this, on a few datasets, curriculum learning considerably increases classification accuracy. The labor dataset increases from 73.68%, to 85.26% and the anneal.ORIG dataset increases from 90.98% to 94.32%. However, other datasets show a considerable decrease in classification accuracy such as the breast-cancer dataset which decreases from 75.52% to less than 69%. Clearly, DTs are not as well suited for curriculum learning as MLPs are. This may be due to the use of entropy in C4.5 and the inability to backtrack and recover from splitting on a\nAverage 81.41 82.32 82.57 82.02 82.23 82.20 81.53 82.13 82.15 p-values 0.002 0.004 0.001 0.003 0.002 0.002 0.003 0.034 > \u2212 = \u2212 < 36-1-15 32-2-18 35-2-15 34-2-16 36-2-14 33-2-17 33-2-17 31-2-19\nDT Orig Prune NoPrune\nAverage 80.12 78.62 76.56 p-values 0.998 1 > \u2212 = \u2212 < 15-1-36 7-0-45\nsuboptimal attribute. MLPs, on the other hand, can partially recover from this through weight updates.\nOne potential problem is that there may not be enough information provided in the initial training set to infer a model of the data. To test this, we adjusted the instance hardness value of the initial instances in the training set. (Originally, only instances with an instance hardness of 0 were used in the initial training set). The aggregate results from setting the instance hardness value of the initial training instances to 0, 0.25, 0.5, and 0.75 are shown in Table 4 for adding more complex instances to the training set after 100 training epochs and training until convergence for MLPs and for pruning and not pruning before adding more complex instances in DTs. The p-values and counts are with respect to the initial training instances having an instance hardness value of 0.\nFor MLPs, increasing the instance hardness value of the initial training set significantly increases the classification accuracy with an alpha value of 0.05 when training until convergence before adding more complex instances. When adding more complex instances to the training set after 100 epochs, increasing the initial complexity did not significantly increase the classification accuracy. For DTs with pruning, the initial instance hardness value has very little effect\u2013all of the datasets have the same accuracy for initial instance hardness values of 0.25 and 0.5 and only three datasets change in classification accuracy with an initial hardness value of 0.75. Thus, pruning before adding more complex instances to the training set appears to lessen the affects of the initial instance hardness value. When DTs are not pruned before adding more complex instance to the training set, an initial instance hardness of 0.75 significantly increases classification accuracy over having an initial instance hardness value of 0. (It should be remembered that the original classification accuracy for DTs is 80.12%, thus, any form of curriculum learning for DTs examined so far does not increase the accuracy over the original)."}, {"heading": "4.2 Comparison with Filtering and Boosting", "text": "In this section, we compare curriculum learning with filtering and boosting. The filtering technique employed removes any instance with an instance hardness value greater than or equal to a threshold (we use 0.75) (Smith and Martinez, 2012). We denote the filtering method as IH.75. Evaluation for filtering is done on unfiltered test data. We compare curriculum learning with two boosting techniques: AdaBoost (Freund and Schapire, 1996) (AB) and MultiBoost (Webb, 2000) (MB).\nFor curriculum learning with MLPs, we use the method of adding more complex instances to the training set after training for 100 epochs since it was the most significant. For DTs, pruning is done before adding more complex instances to the training set since it achieves higher classification accuracy. A statistical comparison of curriculum learning, filtering, and boosting for MLPs and DTs is given in Tables 5 and 6 respectively.\nThe first row shows the average accuracy for each method. Each following pair of rows gives the p-value from the Wilcoxon signed-rank test and the number of times that a method achieved an accuracy greater than-equal to-less than the method in the column heading. Curriculum learning (CL) significantly increases classification accuracy over boosting for MLPs. For both MLPs and DTs, filtering and boosting significantly increases the classification accuracy over the original.\nWe also examined filtering prior to boosting and curriculum learning. Curriculum learning with filtering is denoted as CL.75 and boosting with filtering is denoted as AB.75 and MB.75 for AdaBoost and MultiBoost. For curriculum learning, we set the initial complexity level to 0.5. When a method is augmented with filtering, the filtered method significantly outperforms (in terms of classi-\nfication accuracy) the method without filtering. This is particularly apparent with AdaBoost. AdaBoost does not significantly increase the classification accuracy over any of the other methods for MLPs and only over the original for DTs. Yet, with filtering, AB.75 significantly increases the classification accuracy over the other methods. This shows how prone AdaBoost is to overfitting. These results also show the importance of treating instances differently during training, particularly in the case of AdaBoost and filtering. Filtering has the most significant effect on accuracy, achieving higher classification accuracy than the original algorithm, curriculum learning, and boosting. Applying filtering to curriculum learning and boosting also increases the classification accuracy. AB.75 achieves the highest classification accuracy out of all of the investigated methods increasing the accuracy to 84% for MLPs and DTs.\nConcerning curriculum learning, these results lead to the question of what is gained by using curriculum learning and under what circumstances is using curriculum learning most appropriate. Only 2 of the 52 datasets (autos and labor) have a considerably higher classification accuracy for curriculum learning than filtering for both MLPs and DTs. Originally, curriculum learning was proposed to be used in specific tasks as a continuation method that were assumed to be non-convex. However, it is difficult to determine if a data set represents a\nnon-convex problem. A common approach for determining convexity is taking the difference in accuracy between a linear classifier and a nonlinear classifier. We used the relative percentage difference between the accuracy of a MLP trained with backpropagation and a perceptron (MLP-per) and between the accuracy of a random forest and a linear SVM (RF-SVM). Non-convexity is assumed if the non-linear classifier significantly out performs the linear classifier (although this really determines non-linearity).\nWe compared the convexity measures for each data set with the accuracies from curriculum learning with filtering and boosting. The convexity measures are inconclusive. For example, for the autos dataset, curriculum learning provides a boost in accuracy as is expected since both MLP-per and RF-SVM are positive. The classification accuracy for the autos dataset is considerably higher for CL Prune (using a DT), increasing almost 5 percent On the other hand, a boost is expected on the colic dataset, yet the classification accuracy decreased from about 86% to 82%. Non-convexity, therefore, is not a sufficient condition for curriculum learning to outperform filtering the dataset. It is difficult to know when to use curriculum learning for general machine learning tasks, especially when a simple method such as filtering produces similar results."}, {"heading": "5 Conclusions", "text": "In this paper we presented a method for ordering the instances in a data set by complexity (hardness ordering). A hardness ordering uses instance hardness to order the instances in a data set based on the their likelihood of being misclassified. The hardness ordering allows a learning algorithm to focus on the most informative instances.\nWe integrated the hardness ordering for a data set into the learning process in curriculum learning. One of the main shortcomings of curriculum learning is the lack of a method for developing curricula. As curriculum learning is a relatively new approach, we examined using a hardness ordering as a general approach to implement curriculum learning. We examined curriculum learning on a set of 52 UCI data sets using MLPs and DTs and compared curriculum learning with filtering and boosting.\nOur exploration with curriculum learning has shed interesting, and unexpected, light that curriculum learning performs strikingly similar to filtering and boosting in MLPs. The similarity of curriculum learning in MLPs to filtering is somewhat expected. Elman (1993) pointed out that one of the reasons starting small is so important is due to backpropagation\u2019s inflexibility of learning late in the learning process. As training progresses, weights become rigid and only very small changes are made during training. As complex instances are not trained on in curriculum learning until late in the learning process, they will only have a very minor (if any) affect on the trained model, especially with a large number of training epochs between adding more instances. Thus, the harder instances are not expected to have a large impact on the final model.\nFor the DTs, the hope was that by starting with easier instances, more appropriate attribute splits would be chosen early in the learning process. As more difficult (and possibly noisy) instances are added later they would have less of an effect on the tree. DTs may not be an ideal candidate for curriculum learning because they are somewhat robust to noise due to using entropy to choose which attribute to split on and because pruning helps avoid overfitting the data.\nThe claim that curriculum learning is well suited for non-convex problems may or may not be true. One difficult aspect of this claim is determining the convexity of a dataset. Curriculum learning has an intuitive motivation and has proven to work well in specific past applications. For general machine learning problems, however, it would appear that the benefits may not be as great as originally intended. Curriculum learning may be well suited for tasks that can be broken down into subtasks.\nOur finding that curriculum learning does not out perform filtering and boosting in general machine learning problems may be an artifact of how we ordered the instances in a data set. Using instance hardness to develop curricula orders the instances based on how hard they are to correctly classify but may not take into account if some concepts are sub-concepts, how concepts are related, etc. A better understanding of how the concepts in a data set are related could aid in creating more appropriate curricula for a data set. Future work\nfor curriculum learning includes better understanding of how the instances in a data set are related to each other and developing curricula that accounts for the concepts contained in the instances.\nWe showed the positive impact of filtering instances prior to training. Filtering the noisy and outlier instances prior to training significantly increases the accuracy for the original learning algorithm as well as for curriculum learning and boosting. In fact, filtering prior to using AdaBoost produce significantly higher classification accuracy than all other methods investigated for MLPs and DTs. By filtering the noisy and outlier instances prior to training, AdaBoost does not overfit the noise since it is no longer in the training data. AdaBoost further increases classification accuracy by focusing on the instances that are hardest to correctly classify."}], "references": [{"title": "Outliers in statistical data", "author": ["V. Barnett", "T. Lewis"], "venue": "John Wiley & Sons Ltd.,", "citeRegEx": "Barnett and Lewis,? \\Q1978\\E", "shortCiteRegEx": "Barnett and Lewis", "year": 1978}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Lof: identifying density-based local outliers", "author": ["M.M. Breunig", "Kriegel", "H.-P", "R.T. Ng", "J. Sander"], "venue": null, "citeRegEx": "Breunig et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Breunig et al\\.", "year": 2000}, {"title": "Identifying mislabeled training data", "author": ["C.E. Brodley", "M.A. Friedl"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Brodley and Friedl,? \\Q1999\\E", "shortCiteRegEx": "Brodley and Friedl", "year": 1999}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["J.L. Elman"], "venue": null, "citeRegEx": "Elman,? \\Q1993\\E", "shortCiteRegEx": "Elman", "year": 1993}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "In Proceedings of the Third Annual Workshop on Computational Learning Theory,", "citeRegEx": "Freund,? \\Q1990\\E", "shortCiteRegEx": "Freund", "year": 1990}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Proceedings of the 13th International Conference on Machine Learning,", "citeRegEx": "Freund and Schapire,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1996}, {"title": "Noise detection and elimination in data preprocessing: Experiments in medical domains", "author": ["D. Gamberger", "N. Lavra\u010d", "S. D\u017eeroski"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "Gamberger et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gamberger et al\\.", "year": 2000}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Robust decision trees: Removing outliers from databases", "author": ["G.H. John"], "venue": "In Knowledge Discovery and Data Mining,", "citeRegEx": "John,? \\Q1995\\E", "shortCiteRegEx": "John", "year": 1995}, {"title": "Leveraging the margin more carefully", "author": ["N. Krause", "Y. Singer"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "Krause and Singer,? \\Q2004\\E", "shortCiteRegEx": "Krause and Singer", "year": 2004}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "A metric for unsupervised metalearning", "author": ["J. Lee", "C. Giraud-Carrier"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Lee and Giraud.Carrier,? \\Q2011\\E", "shortCiteRegEx": "Lee and Giraud.Carrier", "year": 2011}, {"title": "On-line outlier detection and data cleaning", "author": ["H. Liu", "S. Shah", "W. Jiang"], "venue": "Computers & Chemical Engineering,", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Learning complex motions by sequencing simpler motion templates", "author": ["G. Neumann", "W. Maass", "J. Peters"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Neumann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Neumann et al\\.", "year": 2009}, {"title": "Estimating the potential for combining learning models", "author": ["A.H. Peterson", "T.R. Martinez"], "venue": "In Proceedings of the ICML Workshop on Meta-Learning,", "citeRegEx": "Peterson and Martinez,? \\Q2005\\E", "shortCiteRegEx": "Peterson and Martinez", "year": 2005}, {"title": "Neural network learning control of robot manipulators using gradually increasing task difficulty", "author": ["T.D. Sanger"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Sanger,? \\Q1994\\E", "shortCiteRegEx": "Sanger", "year": 1994}, {"title": "The strength of weak learnability", "author": ["R.E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Schapire,? \\Q1990\\E", "shortCiteRegEx": "Schapire", "year": 1990}, {"title": "Improving classification accuracy by identifying and removing instances that should be misclassified", "author": ["M.R. Smith", "T. Martinez"], "venue": "In Proceedings of the IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Smith and Martinez,? \\Q2011\\E", "shortCiteRegEx": "Smith and Martinez", "year": 2011}, {"title": "Increasing task accuracy with adaptive filter sets", "author": ["M.R. Smith", "T. Martinez"], "venue": null, "citeRegEx": "Smith and Martinez,? \\Q2012\\E", "shortCiteRegEx": "Smith and Martinez", "year": 2012}, {"title": "An instance level analysis of data complexity", "author": ["M.R. Smith", "T. Martinez", "C. Giraud-Carrier"], "venue": "Machine Learning,", "citeRegEx": "Smith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "From baby steps to leapfrog: \u201chow less is more\u201d in unsupervised dependency parsing", "author": ["V.I. Spitkovsky", "H. Alshawi", "D. Jurafsky"], "venue": "In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Spitkovsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "On the utility of curricula in unsupervised learning of probabilistic grammars", "author": ["K. Tu", "V. Honavar"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence", "citeRegEx": "Tu and Honavar,? \\Q2011\\E", "shortCiteRegEx": "Tu and Honavar", "year": 2011}, {"title": "Multiboosting: A technique for combining boosting and wagging", "author": ["G.I. Webb"], "venue": "Machine Learning,", "citeRegEx": "Webb,? \\Q2000\\E", "shortCiteRegEx": "Webb", "year": 2000}, {"title": "The lack of a priori distinctions between learning algorithms", "author": ["D.H. Wolpert"], "venue": "Neural Computation,", "citeRegEx": "Wolpert,? \\Q1996\\E", "shortCiteRegEx": "Wolpert", "year": 1996}], "referenceMentions": [{"referenceID": 3, "context": "Filtering identifies and removes noisy instances and outliers from a data set prior to training and generally results in an increase in classification accuracy on non-filtered test data (Brodley and Friedl, 1999; Gamberger et al., 2000; Smith and Martinez, 2011).", "startOffset": 186, "endOffset": 262}, {"referenceID": 7, "context": "Filtering identifies and removes noisy instances and outliers from a data set prior to training and generally results in an increase in classification accuracy on non-filtered test data (Brodley and Friedl, 1999; Gamberger et al., 2000; Smith and Martinez, 2011).", "startOffset": 186, "endOffset": 262}, {"referenceID": 18, "context": "Filtering identifies and removes noisy instances and outliers from a data set prior to training and generally results in an increase in classification accuracy on non-filtered test data (Brodley and Friedl, 1999; Gamberger et al., 2000; Smith and Martinez, 2011).", "startOffset": 186, "endOffset": 262}, {"referenceID": 17, "context": "Boosting also treats instances differently during training by incrementally adjusting the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 131, "endOffset": 161}, {"referenceID": 5, "context": "Boosting also treats instances differently during training by incrementally adjusting the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 131, "endOffset": 161}, {"referenceID": 1, "context": "Curriculum learning was recently formalized by Bengio et al. (2009) as a means of using an ordering of the training data from simplest to most complex to train a learning algorithm.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "In this paper, we present an automated method for ordering the instances in a data set based on their likelihood of being misclassified (instance hardness (Smith et al., 2013)) which we call a hardness ordering.", "startOffset": 155, "endOffset": 175}, {"referenceID": 10, "context": "As filtering and boosting have received considerable attention (Krause and Singer, 2004; Liu et al., 2004), we focus primarily on developing curriculum learning and comparing curriculum learning with filtering and boosting.", "startOffset": 63, "endOffset": 106}, {"referenceID": 13, "context": "As filtering and boosting have received considerable attention (Krause and Singer, 2004; Liu et al., 2004), we focus primarily on developing curriculum learning and comparing curriculum learning with filtering and boosting.", "startOffset": 63, "endOffset": 106}, {"referenceID": 16, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 14, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 21, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 22, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 3, "context": "Elman (1993) realized that the early stages of training dictate what solutions are possible in multilayer perceptrons.", "startOffset": 0, "endOffset": 13}, {"referenceID": 1, "context": "Bengio et al. (2009) formalized these ideas in curriculum learning.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bengio et al. (2009) formalized these ideas in curriculum learning. The idea behind curriculum learning is to first optimize a smoothed objective function and gradually reduce the degree of smoothing during the training process. At a more concrete level, curriculum learning is a weighting scheme for training. Each training instance is assigned a weight which controls how the instance is used in training. Initially, the weights on the training instances favor the \u201ceasier\u201d instances or those that represent simpler concepts. As training proceeds, the weights on the training instances are updated such that \u201charder\u201d instances and more complex concepts are introduced into the training set. This continues until all of the instances in the target training set are uniformly weighted. Kumar et al. (2010) recently presented self-paced learning for latent variable models, building on the idea of curriculum learning.", "startOffset": 0, "endOffset": 806}, {"referenceID": 17, "context": "Boosting is another approach that incrementally adjusts the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 101, "endOffset": 131}, {"referenceID": 5, "context": "Boosting is another approach that incrementally adjusts the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 101, "endOffset": 131}, {"referenceID": 10, "context": "However, this can lead to overfitting noise and new methods were proposed to ignore suspected outliers and weight the more informative or boundary instances higher (Krause and Singer, 2004).", "startOffset": 164, "endOffset": 189}, {"referenceID": 18, "context": "Outliers and noisy instances have been observed to adversely affect an induced model (Smith and Martinez, 2011).", "startOffset": 85, "endOffset": 111}, {"referenceID": 0, "context": "As such, a variety of different noise and outlier detection methods exist, such as statistical methods (Barnett and Lewis, 1978), densitybased clustering (Breunig et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 2, "context": "As such, a variety of different noise and outlier detection methods exist, such as statistical methods (Barnett and Lewis, 1978), densitybased clustering (Breunig et al., 2000), and classification-based methods (John, 1995; Brodley and Friedl, 1999).", "startOffset": 154, "endOffset": 176}, {"referenceID": 9, "context": ", 2000), and classification-based methods (John, 1995; Brodley and Friedl, 1999).", "startOffset": 42, "endOffset": 80}, {"referenceID": 3, "context": ", 2000), and classification-based methods (John, 1995; Brodley and Friedl, 1999).", "startOffset": 42, "endOffset": 80}, {"referenceID": 3, "context": "These methods have been used to identify and remove outliers prior to training, resulting in higher classification accuracy (Brodley and Friedl, 1999; Gamberger et al., 2000; Liu et al., 2004).", "startOffset": 124, "endOffset": 192}, {"referenceID": 7, "context": "These methods have been used to identify and remove outliers prior to training, resulting in higher classification accuracy (Brodley and Friedl, 1999; Gamberger et al., 2000; Liu et al., 2004).", "startOffset": 124, "endOffset": 192}, {"referenceID": 13, "context": "These methods have been used to identify and remove outliers prior to training, resulting in higher classification accuracy (Brodley and Friedl, 1999; Gamberger et al., 2000; Liu et al., 2004).", "startOffset": 124, "endOffset": 192}, {"referenceID": 20, "context": "In this paper, we use instance hardness (Smith et al., 2013) to order the instances by complexity.", "startOffset": 40, "endOffset": 60}, {"referenceID": 24, "context": "If all learning algorithms were equally likely, then all instances would have the same instance hardness value under the no free lunch theorem (Wolpert, 1996).", "startOffset": 143, "endOffset": 158}, {"referenceID": 12, "context": "For instance hardness, the diversity of the learning algorithms is determined using unsupervised meta-learning (Lee and Giraud-Carrier, 2011).", "startOffset": 111, "endOffset": 141}, {"referenceID": 15, "context": "Unsupervised meta-learning uses Classifier Output Difference (COD) (Peterson and Martinez, 2005) to measure the diversity between learning algorithms.", "startOffset": 67, "endOffset": 96}, {"referenceID": 8, "context": "The learning algorithms are used as implemented in Weka with their default parameters (Hall et al., 2009).", "startOffset": 86, "endOffset": 105}, {"referenceID": 19, "context": "75) (Smith and Martinez, 2012).", "startOffset": 4, "endOffset": 30}, {"referenceID": 6, "context": "We compare curriculum learning with two boosting techniques: AdaBoost (Freund and Schapire, 1996) (AB) and MultiBoost (Webb, 2000) (MB).", "startOffset": 70, "endOffset": 97}, {"referenceID": 23, "context": "We compare curriculum learning with two boosting techniques: AdaBoost (Freund and Schapire, 1996) (AB) and MultiBoost (Webb, 2000) (MB).", "startOffset": 118, "endOffset": 130}, {"referenceID": 4, "context": "Elman (1993) pointed out that one of the reasons starting small is so important is due to backpropagation\u2019s inflexibility of learning late in the learning process.", "startOffset": 0, "endOffset": 13}], "year": 2013, "abstractText": "Not all instances in a data set are equally beneficial for inferring a model of the data. Some instances (such as outliers) are detrimental to inferring a model of the data. Several machine learning techniques treat instances in a data set differently during training such as curriculum learning, filtering, and boosting. However, an automated method for determining how beneficial an instance is for inferring a model of the data does not exist. In this paper, we present an automated method that orders the instances in a data set by complexity based on the their likelihood of being misclassified (instance hardness). The underlying assumption of this method is that instances with a high likelihood of being misclassified represent more complex concepts in a data set. Ordering the instances in a data set allows a learning algorithm to focus on the most beneficial instances and ignore the detrimental ones. We compare ordering the instances in a data set in curriculum learning, filtering and boosting. We find that ordering the instances significantly increases classification accuracy and that filtering has the largest impact on classification accuracy. On a set of 52 data sets, ordering the instances increases the average accuracy from 81% to 84%.", "creator": "LaTeX with hyperref package"}}}