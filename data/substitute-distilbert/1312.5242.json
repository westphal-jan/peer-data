{"id": "1312.5242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Unsupervised feature learning by augmenting single images", "abstract": "when aspect scaling is applied to visual pattern recognition, data augmentation being often used to generate additional training data without extra labeling cost. it helps to reduce overfitting and increase the color of the display. in this paper we investigate if it is possible to use data sequences as the geometric core of an unsupervised feature learning architecture. to that end we sample a set of random image patches and declare each of modules to be a generic single - image surrogate class. we then extend these trivial one - element classes by applying a variety possible transformations within the initial'seed'task. finally we train a convolutional neural network to discriminate between these surrogate classes. the best representation algorithm by the creator can then be used in various vision tasks. we follow that this simple feature recognition algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets ( stl - 10, cifar - 10, caltech - 101 ).", "histories": [["v1", "Wed, 18 Dec 2013 17:44:17 GMT  (124kb,D)", "https://arxiv.org/abs/1312.5242v1", "6 pages, 4 figures, 1 table"], ["v2", "Fri, 24 Jan 2014 18:02:09 GMT  (124kb,D)", "http://arxiv.org/abs/1312.5242v2", "ICLR 2014 workshop track submission (6 pages, 4 figures, 1 table)"], ["v3", "Sun, 16 Feb 2014 13:07:23 GMT  (134kb,D)", "http://arxiv.org/abs/1312.5242v3", "ICLR 2014 workshop track submission (7 pages, 4 figures, 1 table)"]], "COMMENTS": "6 pages, 4 figures, 1 table", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["alexey dosovitskiy", "jost tobias springenberg", "thomas brox"], "accepted": false, "id": "1312.5242"}, "pdf": {"name": "1312.5242.pdf", "metadata": {"source": "CRF", "title": "Unsupervised feature learning by augmenting single images", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg"], "emails": ["dosovits@cs.uni-freiburg.de", "springj@cs.uni-freiburg.de", "brox@cs.uni-freiburg.de"], "sections": [{"heading": null, "text": "When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial \u2019seed\u2019 patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101)."}, {"heading": "1 Introduction", "text": "Deep convolutional neural networks trained via backpropagation have recently been shown to perform well on image classification tasks containing millions of images and thousands of categories [17, 24]. While deep convolutional neural networks have been known to yield good results on supervised image classification tasks such as MNIST for a long time [18], the recent successes are made possible through optimized implementations, efficient model averaging and data augmentation techniques [17]. The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9]. This capability to generalize to new datasets indicates that supervised discriminative learning is currently the best known algorithm for visual feature learning. The downside of this approach is the need for expensive labeling, as the amount of required labels grows quickly the larger the model gets. For this reason unsupervised learning, although currently underperforming, remains an appealing paradigm, since it can make use of raw unlabeled images and videos which are readily available in virtually infinite amounts.\nIn this work we aim to combine the power of discriminative supervised learning with the simplicity of unsupervised data acquisition. The main novelty of our approach is the way we obtain training data for a convolutional network in an unsupervised manner. In the standard supervised setting there exists a large set of labeled images, which may be further augmented by small translations, rotations or color variations to generate even more (and more diverse) training data.\nar X\niv :1\n31 2.\n52 42\nv3 [\ncs .C\nV ]\n1 6\nFe b\nIn contrast, our method does not require any labeled data at all: we use the augmentation step alone to create surrogate training data from a set of unlabeled images. We start with trivial surrogate classes consisting of one random image patch each, and then augment the data by applying a random set of transformations to each patch. After that we train a convolutional neural network to classify these surrogate classes. The feature representation learned by the network is, by construction, discriminative and at the same time invariant to typical data transformations. Nevertheless it is not immediately clear: Would the feature representation learned from this surrogate task perform well on general image classification problems? Our experiments show that, indeed, this simple unsupervised feature learning algorithm achieves competitive or state of the art results on several benchmarks.\nBy performing image augmentation we provide prior knowledge about natural image distribution to the training algorithm. More precisely, by assigning the same label to all transformed versions of an image patch we force the learned feature representation to be invariant to the transformations applied. This can be seen as an indirect form of supervision: our algorithm needs some expert knowledge about which transformations the features should be invariant to. However, similar expert knowledge is used in most other unsupervised feature learning algorithms. Features are usually learned from small image patches, which assumes translational invariance. Turning images to grayscale assumes invariance to color changes. Whitening or contrast normalization assumes invariance to contrast changes and, largely, color variations."}, {"heading": "1.1 Related work", "text": "Our approach is related to a large body of work on unsupervised learning and convolutional neural networks. In contrast to our method, most unsupervised learning approaches, e.g. [13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner. Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15].\nOur proposed method can be related to work on metric learning, for example [10, 12]. However, instead of enforcing a metric on the feature representation directly, as in [12], we only implicitly force the representation of transformed images to be mapped close together through the introduced surrogate labels. This enables us to use discriminative training for learning a feature representation which performs well in classification tasks.\nLearning invariant features with a discriminative objective was previously considered in early work on tangent propagation [21], which aims to learn features invariant to small predefined transformations by directly penalizing the derivative of the network output with respect to the parameters of the transformation. In contrast to their work, our algorithm does not rely on labeled data and is less dependent on a small magnitude of the applied transformations. Tangent propagation has been successfully combined with an unsupervised feature learning algorithm in [20] to build a classifier exploiting information about the manifold structure of the learned representation. This, however, again comes with the disadvantages of reconstruction-based training.\nLoosely related to our work is research on using unlabeled data for regularizing supervised algorithms, for example self-training [2] or entropy regularization [11, 19]. In contrast to these semisupervised methods, our training procedure, as mentioned before, does not make any use of labeled data. Finally, the idea of creating a pseudo-task to improve the performance of a supervised algorithm is used in [1]."}, {"heading": "2 Learning algorithm", "text": "Here we describe in detail our feature learning pipeline. The two main stages of our approach are generating the surrogate training data and training a convolutional neural network using this data."}, {"heading": "2.1 Data acquisition", "text": "The input to our algorithm is a set of unlabeled images, which come from roughly the same distribution as the images we later aim to classify. We randomly sample N \u2208 [50, 32000] random patches of size 32\u00d7 32 pixels from different images, at varying positions and scales. We only sample from regions with considerable gradient energy to avoid getting uniformly colored patches. Then we apply K \u2208 [1, 100] random transformations to each of the sampled patches. Each of these random transformations is a composition of four random \u2019elementary\u2019 transformations from the following list:\n\u2022 Translation: translate the patch by a distance within 0.25 of the patch size vertically and horizontally. \u2022 Scale: multiply the scale of the patch by a factor between 0.7 and 1.4. \u2022 Color: multiply the projection of each patch pixel onto the principal components of the\nset of all pixels by a factor between 0.5 and 2 (factors are independent for each principal component and the same for all pixels within a patch).\n\u2022 Contrast: raise saturation and value (S and V components of the HSV color representation) of all pixels to a power between 0.25 and 4 (same for all pixels within a patch).\nWe do not apply any preprocessing to the obtained patches other than subtracting the mean of each pixel over the whole training dataset. Examples of patches sampled from the STL-10 unlabeled dataset are shown in Fig. 1. Examples of transformed versions of one patch are shown in Fig. 2."}, {"heading": "2.2 Training", "text": "As a result of the procedure described above, to each patch xi \u2208 X from the set of initially sampled patches X = {x1, . . . xN} we apply a set of transformations Ti = {T 1i , . . . , TKi } and get a set of its transformed versions Sxi = Tixi = {T j i xi|T j i \u2208 Ti}. We then declare each of these sets to be a class by assigning label i to the class Sxi and train a convolutional neural network to discriminate between these surrogate classes. Formally, we minimize the following loss function:\nL(X) = \u2211 xi\u2208X \u2211 T ji \u2208Ti l(i, T ji xi), (1)\nwhere l(i, T ji xi) is the loss on the sample T j i xi with (surrogate) true label i. We use a convolutional neural network with cross entropy loss on top of the softmax output layer of the network, hence in our case\nl(i, T ji xi) = CE(ei, f(T j i xi)), CE(y, f) = \u2212 \u2211 k yk log fk, (2)\nwhere f denotes the function computing the values of the output layer of the neural network given the input data, and ei is the ith standard basis vector.\nFor training the network we use an implementation based on the fast convolutional neural network code from [17], modified to support dropout. We use a fixed network architecture in all experiments: 2 convolutional layers with 64 filters of size 5 \u00d7 5 each followed by 1 fully connected layer of 128 neurons with dropout and a softmax layer on top. We perform 2\u00d72 max-pooling after convolutional layers and do not perform any contrast normalization between layers. We start with a learning rate of 0.01 and gradually decrease the learning rate during training. That is, we train until there is no improvement in validation error, then decrease the learning rate by a factor of 3, and repeat this procedure several times until there is no more significant improvement in validation error."}, {"heading": "2.2.1 Pre-training", "text": "In some of our experiments, in which the number of surrogate classes is large relative to the number of training samples per surrogate class, we observed that during the training process the training error does not significantly decrease compared to initial chance level. To alleviate this problem, before training the network on the whole surrogate dataset we pre-train it on a subset with fewer surrogate classes, typically 100. We stop the pre-training as soon as the training error starts falling, indicating that the optimization found a direction towards a good local minimum. We then use the weights learned by this pre-training phase as an initialization for training on the whole surrogate dataset."}, {"heading": "2.3 Testing", "text": "When the training procedure is finished, we apply the learned feature representation to classification tasks on \u2019real\u2019 datasets, consisting of images which may differ in size from the surrogate training images. To extract features from these new images, we convolutionally compute the responses of all the network layers except the top softmax and form a 3-layer spatial pyramid of them. We then train a linear support vector machine (SVM) on these features. We select the hyperparameters of the SVM via crossvalidation."}, {"heading": "3 Experiments", "text": "We report our classification results on the STL-10, CIFAR-10 and Caltech-101 datasets, approaching or exceeding state of the art for unsupervised algorithms on each of them. We also evaluate the effects of the number of surrogate classes and the number of training samples per surrogate class in the training data. For training the network in all our experiments we generate a surrogate dataset using patches extracted from the STL-10 unlabeled dataset.\nFor STL-10 we use the usual testing protocol of averaging the results over 10 pre-defined folds of training data and report the mean and the standard deviation. For CIFAR-10 we report two results: \u2019CIFAR-10\u2019 means training on the whole CIFAR-10 training set and \u2019CIFAR-10-reduced\u2019 means the average over 10 random selections of 400 training samples per class. For Caltech-101 we follow the usual protocol with selecting 30 random samples per class for training and not more than 50 training samples per class for testing, repeated 10 times."}, {"heading": "3.1 Classification results", "text": "In Table 1 we compare our classification results to other recent work. Our network is trained on a surrogate dataset with 8000 surrogate classes containing 150 samples each. We remind that for extracting features during test time we use the first 3 layers of the network with 64, 64 and 128 filters respectively. The feature representation is hence considerably more compact than in most competing approaches. We do not list the results of supervised methods on CIFAR-10 (the best of which currently exceed 90% accuracy), since those are not directly comparable to our unsupervised feature learning method.\nAs can be seen in the table, our results are comparable to state of the art on CIFAR-10 and exceed the performance of many unsupervised algorithms on Caltech-101. On STL-10 for which the image\n1As mentioned, we do not compare to the methods which use supervised information for learning features on the full CIFAR-10 dataset\n2There are two ways to compute the accuracy on Caltech-101: simply averaging the accuracy over the whole test set or calculating the accuracy for each class separately and then averaging these values. These methods differ because for many classes less than 50 test samples are available. It seems that most researchers in the machine learning field use the first method, which is what we report in the table. When using the second method, our performance drops to 74.1%\u00b1 0.6%\ndistribution of the test dataset is closest to the surrogate samples our algorithm reaches 67.4%\u00b10.6% accuracy outperforming all other approaches by a large margin."}, {"heading": "3.2 Influence of the data acquisition on classification performance", "text": "Our pipeline lets us easily vary the number of surrogate classes in the training data and the number of training samples per surrogate class. We use this to measure the effect of these factors on the quality of the resulting features. We vary the number of surrogate classes between 50 and 32000 and the number of training samples per surrogate class between 1 and 100. The results are shown in Fig. 3 and 4. In Fig. 4 we also show, as a baseline, the classification performance of random filters (all weights are sampled from a normal distribution with standard deviation 0.001, all biases are set to zero). Initializing the random filters does not require any training data and can hence be seen as using 0 samples per surrogate class. Error bars in Fig. 3 show the standard deviations computed when testing on 10 folds of the STL-10 dataset.\nAn apparent trend in Fig. 3 is that increasing the number of surrogate classes results in an increase in classification accuracy until it reaches an optimum at around 8000 surrogate classes. When the number of surrogate classes is further increased the classification results do not change or slightly decrease. One explanation for this behavior is that the larger the number of surrogate classes becomes, the more these classes overlap. As a result of this overlap the classification problem becomes more difficult and adapting the network to the surrogate task no longer succeeds. To check the validity of this explanation we also plot in Fig. 3 the classification error on the validation set (taken from the surrogate data) computed after training the network. It rapidly grows as the number of surrogate classes increases, supporting the claim that the task quickly becomes more difficult as the number of surrogate classes increases.\nFig. 4 shows that classification accuracy increases with increasing number of samples per surrogate class and saturates around 100 samples. It can also be seen that when training with small numbers of samples per surrogate class, there is no clear indication that having more classes lead to better performance. We hypothesize that the reason may be that with few training samples per class the surrogate classification problem is too simple and hence the network can severely overfit, which results in poor and unstable generalization to real classification tasks. However, starting from around 8\u221216 samples per surrogate class, the surrogate task gets sufficiently complicated and the networks with more diverse training data (more surrogate classes) perform consistently better."}, {"heading": "4 Discussion", "text": "We proposed a simple unsupervised feature learning approach based on data augmentation that shows good results on a variety of classification tasks. While our approach sets the state of the art on STL-10 it remains to be seen whether this success can be translated into consistently better performance on other datasets.\nThe performance of our method saturates when the number of surrogate classes increases. One probable reason for this is that the surrogate task we use is relatively simple and does not allow the network to learn complex invariances such as 3D viewpoint invariance or inter-instance invariance. We hypothesize that our unsupervised feature learning method could learn more powerful higherlevel features if the surrogate data were more similar to real-world labeled datasets. This could be achieved by using extra weak supervision provided for example by video data or a small number of labeled samples. Another possible way of obtaining richer surrogate training data would be (unsupervised) merging of similar surrogate classes. We see these as interesting directions for future work."}, {"heading": "Acknowledgements", "text": "We acknowledge funding by the ERC Starting Grant VideoLearn (279401)."}], "references": [{"title": "Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks", "author": ["A. Ahmed", "K. Yu", "W. Xu", "Y. Gong", "E. Xing"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Semi supervised logistic regression", "author": ["M.-R. Amini", "P. Gallinari"], "venue": "In ECAI, pages 390\u2013394,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Unsupervised Feature Learning for RGB-D Based Object Recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In ISER,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Multipath sparse coding using hierarchical matching pursuit", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Ask the locals: multi-way local pooling for image recognition", "author": ["Y. Boureau", "N. Le Roux", "F. Bach", "J. Ponce", "Y. LeCun"], "venue": "In Proc. International Conference on Computer Vision (ICCV\u201911)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In NIPS, pages 3248\u20133256,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S.T. Roweis", "G.E. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Entropy regularization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. Lecun"], "venue": "In In Proc. Computer Vision and Pattern Recognition Conference", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Direct modeling of complex invariances for visual object features", "author": ["K.Y. Hui"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML- 13),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Y. Jia", "C. Huang", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks", "author": ["D.-H. Lee"], "venue": "In Workshop on Challenges in Representation Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y.N. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Tangent prop - a formalism for specifying selected invariances in an adaptive network", "author": ["P. Simard", "B. Victorri", "Y. LeCun", "J.S. Denker"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Deep learning of invariant features via simulated fixations in video", "author": ["W.Y. Zou", "A.Y. Ng", "S. Zhu", "K. Yu"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Deep convolutional neural networks trained via backpropagation have recently been shown to perform well on image classification tasks containing millions of images and thousands of categories [17, 24].", "startOffset": 192, "endOffset": 200}, {"referenceID": 23, "context": "Deep convolutional neural networks trained via backpropagation have recently been shown to perform well on image classification tasks containing millions of images and thousands of categories [17, 24].", "startOffset": 192, "endOffset": 200}, {"referenceID": 17, "context": "While deep convolutional neural networks have been known to yield good results on supervised image classification tasks such as MNIST for a long time [18], the recent successes are made possible through optimized implementations, efficient model averaging and data augmentation techniques [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 16, "context": "While deep convolutional neural networks have been known to yield good results on supervised image classification tasks such as MNIST for a long time [18], the recent successes are made possible through optimized implementations, efficient model averaging and data augmentation techniques [17].", "startOffset": 289, "endOffset": 293}, {"referenceID": 23, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 247, "endOffset": 254}, {"referenceID": 6, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 247, "endOffset": 254}, {"referenceID": 23, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 268, "endOffset": 272}, {"referenceID": 6, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 301, "endOffset": 304}, {"referenceID": 6, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 341, "endOffset": 344}, {"referenceID": 8, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 378, "endOffset": 381}, {"referenceID": 12, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15].", "startOffset": 165, "endOffset": 177}, {"referenceID": 24, "context": "Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15].", "startOffset": 165, "endOffset": 177}, {"referenceID": 14, "context": "Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15].", "startOffset": 165, "endOffset": 177}, {"referenceID": 9, "context": "Our proposed method can be related to work on metric learning, for example [10, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "Our proposed method can be related to work on metric learning, for example [10, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "However, instead of enforcing a metric on the feature representation directly, as in [12], we only implicitly force the representation of transformed images to be mapped close together through the introduced surrogate labels.", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "Learning invariant features with a discriminative objective was previously considered in early work on tangent propagation [21], which aims to learn features invariant to small predefined transformations by directly penalizing the derivative of the network output with respect to the parameters of the transformation.", "startOffset": 123, "endOffset": 127}, {"referenceID": 19, "context": "Tangent propagation has been successfully combined with an unsupervised feature learning algorithm in [20] to build a classifier exploiting information about the manifold structure of the learned representation.", "startOffset": 102, "endOffset": 106}, {"referenceID": 1, "context": "Loosely related to our work is research on using unlabeled data for regularizing supervised algorithms, for example self-training [2] or entropy regularization [11, 19].", "startOffset": 130, "endOffset": 133}, {"referenceID": 10, "context": "Loosely related to our work is research on using unlabeled data for regularizing supervised algorithms, for example self-training [2] or entropy regularization [11, 19].", "startOffset": 160, "endOffset": 168}, {"referenceID": 18, "context": "Loosely related to our work is research on using unlabeled data for regularizing supervised algorithms, for example self-training [2] or entropy regularization [11, 19].", "startOffset": 160, "endOffset": 168}, {"referenceID": 0, "context": "Finally, the idea of creating a pseudo-task to improve the performance of a supervised algorithm is used in [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Then we apply K \u2208 [1, 100] random transformations to each of the sampled patches.", "startOffset": 18, "endOffset": 26}, {"referenceID": 16, "context": "For training the network we use an implementation based on the fast convolutional neural network code from [17], modified to support dropout.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "STL-10 CIFAR-10-reduced CIFAR-10 Caltech-101 K-means [6] 60.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "0 \u2014 Multi-way local pooling [5] \u2014 \u2014 \u2014 77.", "startOffset": 28, "endOffset": 31}, {"referenceID": 24, "context": "6 Slowness on videos [25] 61.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Receptive field learning [16] \u2014 \u2014 [83.", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "7 Hierarchical Matching Pursuit (HMP) [3] 64.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Multipath HMP [4] \u2014 \u2014 \u2014 82.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "5 Sum-Product Networks [8] 62.", "startOffset": 23, "endOffset": 26}, {"referenceID": 14, "context": "View-Invariant K-means [15] 63.", "startOffset": 23, "endOffset": 27}], "year": 2014, "abstractText": "When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial \u2019seed\u2019 patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).", "creator": "LaTeX with hyperref package"}}}