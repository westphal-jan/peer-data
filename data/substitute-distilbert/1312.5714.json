{"id": "1312.5714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Avoiding Confusion between Predictors and Inhibitors in Value Function Approximation", "abstract": "reinforcement learning treats each actor, feature, or stimulus as having a positive balanced null reward value. some stimuli, however, negate or inhibit the benefit of certain other predictors ( excitors ) when occurring with them, regardless are otherwise neutral. we show that both linear and non - contingent value - response only assign parameter features a strong value with the opposite valence of the predictor it inhibits ( i. e., inhibitor = - selector ). in one circumstance, this gives a correct prediction ( i. e., catalyst + companion = neutral source ). importantly, however, value - function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor judgment is simply followed before biased neutral outcome. essentially, we show that having reward actions as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. we noticed that this problem can be easily avoided meant the reinforcement learning problem is broken into 1 ) a supervised learning module that predicts the positive appearance of primary reinforcements and 2 ) a reinforcement learning module potentially sums their error - defined values.", "histories": [["v1", "Thu, 19 Dec 2013 19:52:52 GMT  (51kb,D)", "http://arxiv.org/abs/1312.5714v1", null], ["v2", "Wed, 18 Feb 2015 15:35:56 GMT  (265kb,D)", "http://arxiv.org/abs/1312.5714v2", "14 pages, 3 figures, 23 references, Workshop paper in ICLR 2014 (updated based on reviewer comments)"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["patrick c connor", "thomas p trappenberg"], "accepted": false, "id": "1312.5714"}, "pdf": {"name": "1312.5714.pdf", "metadata": {"source": "CRF", "title": "An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning", "authors": ["Patrick Connor", "Thomas Trappenberg"], "emails": ["patrick.connor@dal.ca", "tt@cs.dal.ca"], "sections": [{"heading": "1 Introduction", "text": "Value-function approximators1 (VFAs) are used to map multi-dimensional real-world states to values in the practical application of reinforcement learning (RL) algorithms. A variety of function approximation techniques have been applied to this domain (e.g., [2, 3, 4]). Among applications, the input representation will vary, but the output representation or prediction target remains the same: reward value. The reward value scale ranges from reward (positive values) through neutral (zero value) to punishment (negative values).\nIn biological RL, as depicted in classical conditioning experiments, there is a distinction between stimuli (i.e., features) that predict punishment and those that cancel a prediction of reward. In the conditioned inhibition paradigm [5, 6], animals receive alternated presentations of a stimulus followed by a reinforcer, say, a reward (A+) and a combination of that stimulus and another, followed by no reward (AX-). Stimulus A becomes excitatory and stimulus X cancels or inhibits A\u2019s reward prediction. However, the two are not viewed as entirely symmetric opposites in this field. For example, the extinction of the inhibitor (X) is generally more complicated than mere non-reinforced presentations [7, 8], which is effective on excitatory stimuli such as A. In contrast, the VFA does see excitors and inhibitors as perfectly symmetric opposites.\nIf trained on the same data as in the conditioned inhibition paradigm (i.e., [1 0]\u21921, [1 1]\u21920), an approximator will subsequently predict a negative reinforcement or punishment when presented with\n1For a useful introduction to value-function approximation, see [1]\nar X\niv :1\n31 2.\n57 14\nv1 [\ncs .A\nI] 1\n9 D\nec 2\nX alone (i.e., [0 1];-1) instead of the correct zero valued outcome (since X is actually a predictor of reward omission, not punishment). In this way, there exists a confusion between predictors of an outcome and inhibitors of the oppositely valenced outcome. Making the distinction between them is important. Besides imposing an increase in prediction error when the inhibitor is presented alone (or in certain combinations), such circumstances could be read incorrectly by an agent and lead to irrational actions (e.g., fear-driven actions instead of disappointment-driven actions and vice versa).\nHere, we demonstrate this issue in concrete terms and offer a simple, generic solution. Instead of predicting the reward value, we train function approximators to predict specific primary reinforcements (e.g., food, charging station, etc.). Then, the agent-defined reward values of the expected reinforcements are summed to give a reward value prediction. This approach breaks the reinforcement-learning problem into an intermediate supervised learning layer and a RL layer readout. The intermediate prediction of real-world primary reinforcements recognizes the importance of developing high-level representations of the world, though doing so is not the focus of the present work. Instead, we show that the choice of what a supervised learner predicts, or its output representation, is equally important, and is reflected in the resulting affect on prediction accuracy."}, {"heading": "2 Predicting the future state instead of values", "text": "Figure 1A depicts the typical value-function approximation approach used in RL. The input features which express the environmental state are submitted to the approximator and it predicts the total expected future reward value from that state. It is updated according to one of a variety of RL paradigms (e.g., Sarsa, Q-learning, TD(\u03bb), etc.) and the specifics of the approximator (e.g., gradient descent for a linear approximator). In contrast, Figure 1B depicts the deeper, two-staged RL architecture that this work advocates: multiple supervised learning models, which predict aspects of the future state (predicted features) and, in particular, the appearance of primary reinforcements. A key difference between the predicted feature and reward value targets is that the former has positive values only (a feature is either present or not) and the latter can have both positive and negative values. This rectification is a crucial detail of the proposed approach.\nThe notion of predicting future observations or states is not new to RL [9, 10]. For example, in temporal difference (TD) networks, Sutton and Tanner [9] demonstrate the use of TD learning to predict future observations, not just reward. Although more computation is required than for predicting reward alone, the richer predictive output obviously has potential uses, including being able to foresee future states and allow the agent to act in anticipation of them. We show below how such predictions can be used to resolve the problem described in the previous section, but there is also another benefit for RL. Each agent places value on certain kinds of stimuli in the world. For some of these stimuli, the value changes dynamically based on the internal state of the agent. For example, when a robotic agent has a low battery, the charging station ought to have a higher value than when the robot is fully charged. A simple VFA would be forced to learn the value of approaching the charging station depending on its battery state. This is certainly possible, but we might take a cue from biological systems, where it seems that the dynamic revaluation of a primary reinforcer due to satiety is second nature. Satiety can be integrated in the proposed approach simply \u2013 the reward value of any primary reinforcer can be weighted by a satiety parameter in the RL module. Being fully sated would neutralize the reward value otherwise attributed to the associated reinforcer."}, {"heading": "3 Function approximators", "text": "In our evaluation, the standard VFA is instantiated as a linear function via least mean squares (LMS) linear regression and as a non-linear function via support-vector regression (SVR) [11]. This choice was made because both models required very little adjusting of parameters, which might otherwise complicate the interpretation of the results. For the two-stage architecture, we use a slightly different version of LMS, which we derive and relate to LMS below.\nThe form of SVR used here, called the epsilon-SVR, involves fitting a hyperplane with margins (i.e., a hyper-rectangular prism) so that it encompasses all data while at the same time minimizing the hyperplane\u2019s slope [11]. This is often infeasible, so that outliers are permitted but at a cost, leading to a trade-off between minimization of the slope and the acceptance of outliers. Since explanations of simulation results will not rely on the formal definition of this non-linear model, we direct the\ninterested reader to Smola and Scho\u0308lkopf [11]. We used LIBSVM [12] in our simulations (Settings: cost = 10, epsilon = 1e\u2212 5, radial basis function kernel with \u03b3 = 1Num.Features ). Underlying LMS linear regression is the goal to maximize the likelihood of the given data being generated by a Gaussian random variable with a parameterized n-dimensional linear mean. This function can be expressed as\ny = \u03c6Tx+N (0, \u03c3) (1) where y is the outcome being predicted, x is a vector of inputs, \u03c6 is the vector of model parameters, and 0 and \u03c3 represent the mean and standard deviation of the Gaussian random variable, respectively. The probability density function (PDF) becomes\np(y, x|\u03c6) = 1\u221a 2\u03c0\u03c3 e\u2212 (y\u2212\u03c6T x)2 2\u03c32 (2)\nFor the two-stage architecture, the reinforcer outcome being predicted cannot be less than zero,\ny = G(\u03c6Tx+N (0, \u03c3)) (3)\nwhere G() is the threshold-linear function [13] which returns its argument whenever it is positive and returns zero otherwise. The PDF becomes\np(y, x|\u03c6) =  1\u221a 2\u03c0\u03c3 e\u2212 (y\u2212\u03c6T x)2 2\u03c32 , for y > 0 1 2 (1\u2212 erf( \u03c6T x\u221a 2\u03c3 ), for y = 0\n0, for y < 0\n(4)\ndiffering from the other PDF by zeroing the probability of negative y values and instead heaping it onto the y = 0 case. From this PDF, let us derive the two-stage function approximator\u2019s learning rule and, indirectly, for LMS as well. Given data generated from Equation 3, we can infer the values\nof \u03c6 using maximum likelihood estimation. The probability or likelihood that a certain training data set is generated from the PDF is\nL(\u03c6) = p(y(1), ...y(m), x(1), ...x(m)|\u03c6)\n= m\u220f i=1 p(y(i), x(i)|\u03c6) (5)\nwhere m is the number of training data points. We can maximize this convex likelihood function by taking its log and ascending its gradient,\nlog L(\u03c6) = log \u220fm i=1 p(y (i), x(i)|\u03c6) = \u2211m i=1 log p(y\n(i), x(i)|\u03c6) = \u2212my>0log( \u221a 2\u03c0\u03c3)\u2212 12\u03c32 \u2211 i,y(i)>0(y (i) \u2212 \u03c6Tx(i))2\n\u2212my=0log(2) + \u2211 i,y(i)=0 log(1\u2212 erf( \u03c6T x(i)\u221a 2\u03c3 )) (6)\nwhere my>0 and my=0 are the number of data points when y > 0 and y = 0, respectively. Taking the gradient of this function with respect to each \u03c6j gives\n\u2202 log L(\u03c6)\n\u2202\u03c6j =\n1\n\u03c32 \u2211 i,y(i)>0 (y(i) \u2212 \u03c6Tx(i))x(i)j \u2212 \u221a 2 \u03c0\u03c32 \u2211 i,y(i)=0 e\u2212 (\u03c6T x(i))2 2\u03c32 1\u2212 erf(\u03c6T x(i)\u221a 2\u03c3 ) x (i) j (7)\nThe gradient can be ascended by iteratively updating the model parameters,\n\u03c6j =: \u03c6j + \u03b1 \u2202 log L(\u03c6)\n\u2202\u03c6j (8)\nwhere the learning rate, \u03b1 = \u03c3 2\nn+2 . The learning rule for can be similarly derived for standard LMS (Equation 2), resulting in\n\u2202 log L(\u03c6)\n\u2202\u03c6j =\n1\n\u03c32 M\u2211 i=1 (y(i) \u2212 \u03c6Tx(i))x(i)j (9)\nwhere M is the total number of data points. The practical feature of the rectified learning rule that distinguishes it is what happens for data points where y = 0: when the weighted sum \u03c6Tx is negative, learning is essentially deactivated, especially for small \u03c3. For our simulations, we use a low noise variance of \u03c3 = 1e\u2212 4. The second or RL stage of our architecture computes a weighted sum of its inputs. For simplicity in our simulations, rewarding reinforcers will be given weights of +1, and punishing reinforcers will have weights of \u22121."}, {"heading": "4 Simulation and results", "text": "To show the difference between the two architectures, a single-step RL problem is simulated. The structure is captured in Table 1. In brief, it is the combination of two simultaneous conditioned inhibition experiments, where one reinforcement is rewarding and the other is punishing. The experiment is arranged to show how well these architectures distinguish between inhibitory features and excitatory features of the opposite valence, in a case where both rewards and punishments do occur. As a RL problem, there are no actions to take; an agent is forced from one state to the next. As an aside, however, one can imagine an agent\u2019s actions also being represented as input features, for which the process of learning to predict outcomes based on proposed actions would then be the same.\nSpecifically, there are 4 binary features, such that each feature is associated with a specific outcome that follows its presentation: feature PR is associated with delivery of a positive (rewarding) reinforcer (feature P), OP is associated with omission of P, NR is associated with delivery of a negative (punishing) reinforcer (N), and ON is associated with the omission of N. There are a total of 24 = 16 unique data points. Training on the \u201cfull\u201d dataset includes all 16 data points. The \u201cpartial\u201d dataset is the \u201cfull\u201d dataset excluding the data points in the lightly shaded rows of the table (numbered 2, 5, 7,"}, {"heading": "3 0 0 1 0 0 1 -1 -.5 -1 -1 -1 -1 -1", "text": ""}, {"heading": "7 0 1 1 0 0 1 -1 -1 -2 -1 -1.6 -1 -1", "text": "8, 10, and 14). The removed data points contain evidence of the non-linear nature of the inhibitory features OP and ON.\nA positive reinforcer is delivered (P = 1) after a data point is presented where feature PR is present and feature OP is not, as shown in the column P of the table. A negative reinforcer is given when NR is present and ON is not, as shown in column N. Both positive and negative reinforcers may be given on the same trial. Finally, the reward value (column RV) for a data point is the sum of the positive reinforcement minus the negative reinforcement values predicted for that data point (i.e., P \u2212N ), since the associated second-stage weights are +1 and \u22121, respectively. The learning cycle for the standard VFA in this experiment is the following:\n\u2022 the 4-feature state vector is presented as input. \u2022 the reward value prediction is made \u2022 in the next time step, the actual reward value outcome is detected and used to update the\nmodel\nThe learning cycle for the two-stage architecture in this experiment is the following:\n\u2022 the 4-feature state vector is presented as input. \u2022 the prediction of the positive reinforcer is made by one function approximator and the\nprediction of the negative reinforcer is made by a second approximator\n\u2022 the reward value prediction is computed as the positive reinforcer prediction minus the negative reinforcer prediction (P \u2212N )\n\u2022 in the next time step, the absence/presence of each reinforcer is detected and used to update the associated function approximator.\nNote that simulations revealed that presenting multiple copies of the data to these models did not affect the results and neither did the order of the data points.\nThe table contains the predictions after training for each approach and each data point, allowing us to see where poor predictions occur. In the bottom row, the average prediction error for each approach is recorded. The columns \u201cLR-F\u201d and \u201cLR-P\u201d denote the reward value predictions for\nLMS when the full dataset and the partial dataset are used for training, respectively. In the LR-F case, the predictions are routinely incorrect by 0.5. By looking at results for data points numbered 2, 3, 5, and 9, we can see that the LMS weights were 0.5 for the predictor of reward (PR) as well as for the predictor for the omission of punishment (ON). The same confusion occurs between the predictor of punishment and predictor of the omission of reward, both having a weight of -0.5. The LR-P results fair no better, the difference being that the weights are exactly doubled. In both cases, the OP and NR signals and the PR and ON signals have equivalent roles in the eyes of a linear VFA.\nIn the results labeled SV-F (full dataset), the additional data points identify the non-linearity in the system and the model captures this information perfectly. Not so, however, when the partial dataset is used (SV-P results). Here, the SV-P model behaves like the linear model, since the partial dataset gives no hint of a non-linear facet. There is a small distinction between the SV-P and LR-P results; in SV-P, the inhibitory features are not as potent as the excitatory predictive features (0.6 versus 1.0).\nIn contrast to these standard value-function approximation results, the two-stage architecture using the rectified LMS function approximator perfectly predicts the correct outcomes, regardless of whether the full (2S-F results) or partial (2S-P results) dataset is used."}, {"heading": "5 Discussion", "text": "Why does the two-stage architecture work? Recall that each function approximator predicts the presence/absence of a single reinforcer and that the prediction cannot be negative (e.g., less than zero food) because the output is rectified. Without this rectification (i.e, using standard LMS), the two-stage architecture gives exactly the same results as LR-F and LR-P (not shown in the table).\nIn both tests (2S-F, 2S-P), the internal rectified LMS weight of the predictor of a rewarding reinforcer (PR) is +1 and the weight of predictor of omission of the rewarding reinforcer (OP) is \u22121. This is different from the unrectified LMS weights (0.5 and \u22120.5) in the LR-F case. The reason for this is that when the inhibitor OP is presented alone, the learning rule shuts off learning so that it\u2019s weight cannot be shrunk toward -0.5. This outcome of rectification resonates with the animal learning literature (but see [14]), which generally finds that non-reinforced presentations of such inhibitors do not extinguish them (i.e., they keep their inhibitory quality), which is in contrast to excitors which are easily extinguished using the same procedure.\nFrom the LR-P results, however, we saw that having weights of +1 and \u22121 for PR and OP, respectively, did not equate to perfect results. The difference here is that when OP is presented alone, the rectified LMS prediction of the reinforcer is \u201ctruncated\u201d at zero to reflect the associated PDF (i.e., no negative outcome values) from which the model is derived. This avoids the erroneous result in data point #5. The same rectification applies to the predictor of omission of the negative reinforcer (data point # 2) and other similar scenarios (data points 7, 8, 10, and 14). In contrast, such a rectification does not make sense in direct value-function approximation where both positive (rewarding) and negative (punishing) outcomes occur.\nAlthough the final output of the two-stage architecture is reward value, our approach is not simply changing the contents of the VFA to encapsulate multiple function approximators and a RL module. Instead, they are fundamentally different in terms of the quantity they predict (reward vs. presence of specific reinforcer). In the same vein, the architecture we propose is not trained using a scalar reward prediction error, but rather a vector of state feature-specific prediction errors, one scalar per feature/internal function approximator.\nApart from the rectification, the model we used to demonstrate the two-stage architecture learns only linear relationships. However, this is not to say that one could not use in its place a model that learns non-linear relationships. In fact, it will be necessary to have non-linear predictive abilities for specific combinations of features (e.g., XOR or AND cases). Here, the rectified linear model was used to demonstrate the advantage of our 2-stage structure. Non-linear models that are used in its place must also rectify their output, even during learning. For example, the use of SVR without rectification for the partial dataset, but in the two-stage architecture, gives the same results as shown in SV-P. Such rectification is non-native to most regression models. One exception is Dual Pathway Regression [15]. Unlike LMS, which integrates inhibitory features by summing their effects with excitatory features, it multiplies the excitatory feature sum by a factor between zero and one, which is smaller for greater inhibitory feature sums. As a result, inhibition can only shrink predictions, not\nmake them negative. This model is better than LMS at eliminating the effects of irrelevant features and can capture certain non-linear relationships when appropriately extended [16]. In the present experiment, it gives the correct predictions without modification when embedded in the two-stage architecture for both the full and partial datasets.\nThe input representation which would best support the two-stage architecture is likely a hierarchical one. The best deep learning representations here would seem to be those with varying-levels feature complexity that correlate with the primary reinforcers meaningful to the agent. For the thirsty agent, a low-level bank of edge-detectors would seem sufficient to identify a pool or puddle with numerous ripples. In the case of a calm pool, however, a very high-level representation that captures the mirror-like effect of the water would be more discriminative. A hierarchical representation affords such varying levels of complexity and thus would seem to effectively support the prediction of future reinforcements."}, {"heading": "6 Conclusion", "text": "We have demonstrated how a two-stage RL architecture can overcome a fundamental issue faced by VFAs: the inability to distinguish between predictors of reinforcement and inhibitors of reinforcement of the opposite valence. The prediction of reward value is replaced by the prediction of primary reinforcers, which themselves have a positive salience only. Such a rectified output representation allows an inhibitor (or predictor of omission) to have a negative influence when presented together with a predictor of reinforcement, but no influence when it is presented alone. This eliminates its confusion with predictors of reinforcement with the opposite valence."}], "references": [{"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems, pp. 1038\u20131044, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Value function approximation with diffusion wavelets and Laplacian eigenfunctions", "author": ["S. Mahadevan", "M. Maggioni"], "venue": "Advances in Neural Information Processing systems, 2005, pp. 843\u2013850.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Value function approximation in reinforcement learning using the fourier basis.", "author": ["G. Konidaris", "S. Osentoski", "P.S. Thomas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Conditioned reflexes: an investigation of the physiological activity of the cerebral cortex", "author": ["I.P. Pavlov"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1927}, {"title": "Pavlovian conditioned inhibition.", "author": ["R.A. Rescorla"], "venue": "Psychological Bulletin,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1969}, {"title": "Extinction of Pavlovian conditioned inhibition.", "author": ["C.L. Zimmer-Hart", "R.A. Rescorla"], "venue": "Journal of Comparative and Physiological Psychology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1974}, {"title": "Inhibition as a \u201cslave\u201d process: deactivation of conditioned inhibition through extinction of conditioned excitation", "author": ["D.T. Lysle", "H. Fowler"], "venue": "Journal of Experimental Psychology. Animal Behavior Processes, vol. 11, no. 1, pp. 71\u201394, 1985.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1985}, {"title": "Temporal-difference networks", "author": ["R.S. Sutton", "B. Tanner"], "venue": "Advances in Neural Information Processing Systems, 2004, pp. 1377\u20131384.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Using predictive representations to improve generalization in reinforcement learning.", "author": ["E.J. Rafols", "M.B. Ring", "R.S. Sutton", "B. Tanner"], "venue": "in IJCAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A tutorial on support vector regression", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "Statistics and Computing, vol. 14, no. 3, pp. 199\u2013222, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Extinction and blocking of conditioned inhibition in human causal learning", "author": ["I. Baetu", "A.G. Baker"], "venue": "Learning & Behavior, vol. 38, pp. 394\u2013407, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Biologically plausible feature selection through relative correlation", "author": ["P. Connor", "T. Trappenberg"], "venue": "Proceedings of the 2013 International Joint Conference on Neural Networks (IJCNN), 2013, pp. 759\u2013766.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A dual pathway approach for solving the spatial credit assignment problem in a biological way", "author": ["P. Connor"], "venue": "Ph.D. dissertation, Dalhousie University, November 2013. 7", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": ", [2, 3, 4]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ", [2, 3, 4]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": ", [2, 3, 4]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 4, "context": "In the conditioned inhibition paradigm [5, 6], animals receive alternated presentations of a stimulus followed by a reinforcer, say, a reward (A+) and a combination of that stimulus and another, followed by no reward (AX-).", "startOffset": 39, "endOffset": 45}, {"referenceID": 5, "context": "In the conditioned inhibition paradigm [5, 6], animals receive alternated presentations of a stimulus followed by a reinforcer, say, a reward (A+) and a combination of that stimulus and another, followed by no reward (AX-).", "startOffset": 39, "endOffset": 45}, {"referenceID": 6, "context": "For example, the extinction of the inhibitor (X) is generally more complicated than mere non-reinforced presentations [7, 8], which is effective on excitatory stimuli such as A.", "startOffset": 118, "endOffset": 124}, {"referenceID": 7, "context": "For example, the extinction of the inhibitor (X) is generally more complicated than mere non-reinforced presentations [7, 8], which is effective on excitatory stimuli such as A.", "startOffset": 118, "endOffset": 124}, {"referenceID": 0, "context": ", [1 0]\u21921, [1 1]\u21920), an approximator will subsequently predict a negative reinforcement or punishment when presented with", "startOffset": 2, "endOffset": 7}, {"referenceID": 0, "context": ", [1 0]\u21921, [1 1]\u21920), an approximator will subsequently predict a negative reinforcement or punishment when presented with", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": ", [1 0]\u21921, [1 1]\u21920), an approximator will subsequently predict a negative reinforcement or punishment when presented with", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": "For a useful introduction to value-function approximation, see [1]", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": ", [0 1];-1) instead of the correct zero valued outcome (since X is actually a predictor of reward omission, not punishment).", "startOffset": 2, "endOffset": 7}, {"referenceID": 8, "context": "The notion of predicting future observations or states is not new to RL [9, 10].", "startOffset": 72, "endOffset": 79}, {"referenceID": 9, "context": "The notion of predicting future observations or states is not new to RL [9, 10].", "startOffset": 72, "endOffset": 79}, {"referenceID": 8, "context": "For example, in temporal difference (TD) networks, Sutton and Tanner [9] demonstrate the use of TD learning to predict future observations, not just reward.", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "In our evaluation, the standard VFA is instantiated as a linear function via least mean squares (LMS) linear regression and as a non-linear function via support-vector regression (SVR) [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": ", a hyper-rectangular prism) so that it encompasses all data while at the same time minimizing the hyperplane\u2019s slope [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "interested reader to Smola and Sch\u00f6lkopf [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "We used LIBSVM [12] in our simulations (Settings: cost = 10, epsilon = 1e\u2212 5, radial basis function kernel with \u03b3 = 1 Num.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "where G() is the threshold-linear function [13] which returns its argument whenever it is positive and returns zero otherwise.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "This outcome of rectification resonates with the animal learning literature (but see [14]), which generally finds that non-reinforced presentations of such inhibitors do not extinguish them (i.", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "One exception is Dual Pathway Regression [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "This model is better than LMS at eliminating the effects of irrelevant features and can capture certain non-linear relationships when appropriately extended [16].", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "Reinforcement learning treats each input, feature, or stimulus as having a positive or negative reward value. Some stimuli, however, negate or inhibit the values of certain other predictors (excitors) when presented with them, but are otherwise neutral. We show that both linear and non-linear value-function approximators assign inhibitory features a strong value with the opposite valence of the predictor it inhibits (i.e., inhibitor= -excitor). In one circumstance, this gives a correct prediction (i.e., excitor + inhibitor = neutral outcome). Importantly, however, value-function approximators incorrectly predict that when the inhibitor is presented alone, a negative or oppositely valenced outcome will follow whereas the inhibitor alone is actually followed by a neutral outcome. Essentially, we show that having reward value as a direct predictive target can make inhibitors indistinguishable from excitors that predict the oppositely valenced outcome. We show that this problem can be easily avoided if the reinforcement learning problem is broken into 1) a supervised learning module that predicts the positive appearance of primary reinforcements and 2) a reinforcement learning module which sums their agent-defined values.", "creator": "LaTeX with hyperref package"}}}