{"id": "1312.5851", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Fast Training of Convolutional Networks through FFTs", "abstract": "convolutional networks are one of the most widely employed architectures in computer vision science machine learning. in order because leverage their ability to learn complex functions, large amounts of data are required for training. training a large convolutional network by produce state - of - the - art simulations can take weeks, even when using modern machinery. producing labels using a trained network can also be costly ( dealing with web - scale datasets. using pure work, we present with sequential algorithm which accelerates training and inference displaying a significant efficiency, whilst can minimize improvements of probability an order of magnitude compared to existing state - of - the - art implementations. this is done by computing convolutions as pointwise products in the fourier domain while reusing the same transformed feature map many times. the metric is implemented on a gpu architecture and addresses a plurality of related challenges.", "histories": [["v1", "Fri, 20 Dec 2013 08:42:21 GMT  (282kb,D)", "http://arxiv.org/abs/1312.5851v1", null], ["v2", "Wed, 22 Jan 2014 00:28:06 GMT  (132kb,D)", "http://arxiv.org/abs/1312.5851v2", null], ["v3", "Tue, 28 Jan 2014 01:33:21 GMT  (133kb,D)", "http://arxiv.org/abs/1312.5851v3", null], ["v4", "Tue, 18 Feb 2014 03:20:51 GMT  (134kb,D)", "http://arxiv.org/abs/1312.5851v4", null], ["v5", "Thu, 6 Mar 2014 23:27:18 GMT  (134kb,D)", "http://arxiv.org/abs/1312.5851v5", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["michael mathieu", "mikael henaff", "yann lecun"], "accepted": true, "id": "1312.5851"}, "pdf": {"name": "1312.5851.pdf", "metadata": {"source": "CRF", "title": "Fast Training of Convolutional Networks through FFTs", "authors": ["Michael Mathieu", "Mikael Henaff"], "emails": ["mathieu@cs.nyu.edu", "mbh305@nyu.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "As computer vision and machine learning aim to solve increasingly challenging tasks, models of greater complexity are required. This in turn requires orders of magnitude more data to take advantage of these powerful models while avoiding overfitting. While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1]. This brings about new challenges as to how to train networks in a feasible amount of time. Even using parallel computing environments, training a network on ImageNet can take weeks [6]. In addition, although inference of labels using a trained network is comparatively fast, real-world applications such as producing labels for all images on the internet can represent a significant cost in terms of time and resources. Therefore, there is an important need to develop fast algorithms for training and inference.\nIn this work, we present a simple algorithm which accelerates training and inference using convolutional networks. The idea is based on performing convolutions as products in the Fourier domain, and reusing transformed feature maps many times. The significant operations in training convolutional networks can all be viewed as convolutions between pairs of 2-D matrices, which can represent input and output feature maps, gradients of the loss with respect to feature maps, or weight kernels. Typically, convolutions are performed for all pairings between two sets of 2-D matrices. By computing the Fourier transforms of the matrices in each set once, we can efficiently perform all convolutions as pairwise products. Although it has long been known that convolutions can be\nar X\niv :1\n31 2.\n58 51\nv1 [\ncs .C\nV ]\ncomputed as products in the Fourier domain, until recently the number of feature maps used in convolutional networks has been too small to make a method like ours effective. When the number of feature maps is large, this method accelerates training and inference by a significant factor, and can lead to a speedup of over an order of magnitude."}, {"heading": "2 Theory", "text": ""}, {"heading": "2.1 Backpropagation", "text": "The backpropagation algorithm [7] is the standard method to compute the gradient when training a convolutional network. During training, each layer performs three tasks, which we now describe. First we fix some notation: for a given layer, we have a set of input feature maps xf indexed by f , each one being a 2-D image of dimensions n\u00d7n. The output is a set of feature maps yf \u2032 indexed by f \u2032, which are also 2-D images whose dimension depends on the convolutional kernel and its stride. The layer\u2019s trainable parameters consist of a set of weights wf \u2032f , each of which is a small kernel of dimensions k \u00d7 k. In the forward pass, each output feature map is computed as a sum of the input feature maps convolved with the corresponding trainable weight kernel:\nyf \u2032 = \u2211 f xf \u2217 wf \u2032f (1)\nDuring the backward pass, the gradients with respect to the inputs are computed by convolving the transposed weight kernel with the gradients with respect to the outputs:\n\u2202L\n\u2202xf =\n\u2202L\n\u2202yf \u2032 \u2217 wTf \u2032f (2)\nThis step is necessary for computing the gradients in (3) for the previous layer. Finally, the gradients of the loss with respect to the weight are computed by convolving each input feature map with the gradients with respect to the outputs:\n\u2202L\nwf \u2032f =\n\u2202L\n\u2202yf \u2032 \u2217 xf (3)\nNote that \u2202L\u2202yf\u2032 is a 2-D matrix with the same dimensions as the output feature map yf \u2032 , and that all operations consist of convolutions between various sets of 2-D matrices."}, {"heading": "2.2 Algorithm", "text": "The well-known Convolution Theorem states that convolutions in the spatial domain are equivalent to pointwise products in the Fourier domain. Letting F denote the Fourier transform and F\u22121 its inverse, we can compute convolutions between functions f and g as follows:\nf \u2217 g = F\u22121(F(f) \u00b7 F(g))\nTypically, this method is used when the size of the convolution kernel is close to that of the input image. Note that a convolution of an image of size n \u00d7 n with a kernel of size k \u00d7 k using the direct method requires O(n2k2) operations. The complexity of the FFT-based method requires O(6n2 log n+n2) operations: each FFT requiresO(n2 log n2) = O(2n2 log n), and the pointwise product in the frequency domain requires n2. When the kernel is small, it is usually more efficient to do the convolution in the spatial domain.\nOur algorithm is based on the observation that in all of the operations (1), (2), (3), each of the matrices indexed by f is convolved with each of the matrices indexed by f \u2032. We can therefore compute the FFT of each matrix once, and all pairwise convolutions can be performed as products\nin the frequency domain. Even though using the FFT-based method may be less efficient for a given convolution, we can effectively reuse our FFTs many times which more than compensates for the overhead.\nThe following analysis makes this idea precise. Assume we have f input feature maps, f \u2032 output feature maps, images consisting of n \u00d7 n pixels and kernels of k \u00d7 k pixels. Also assume we are performing updates over minibatches of size S, and that C represents the hidden constant in the FFT complexity. As an example, using the direct approach (1) will take a total of S \u00b7f \u2032 \u00b7f \u00b7(n\u2212k+1)2 \u00b7k2 operations. Our approach requires (2C \u00b7 n2 log n)(S \u00b7 f + f \u2032 \u00b7 f) operations to transform the input feature maps and kernels to the Fourier domain, a total of S \u00b7n2 \u00b7 f \u2032 \u00b7 f additions and multiplications in the Fourier domain, and S \u00b7 f \u2032 \u00b7 (2C \u00b7 n2 log n) operations to transform the output feature maps back to the spatial domain. The same analysis yields similar complexity estimates for the other operations:\nDirect convolution Our Method\u2211 f xf \u2217 wf \u2032f S \u00b7 f \u2032 \u00b7 f \u00b7 n\u20322 \u00b7 k2 2Cn2 log n[f \u2032 \u00b7 S + f \u00b7 S + f \u2032 \u00b7 f ] + S \u00b7 f \u2032 \u00b7 f \u00b7 n2\n\u2202L \u2202yf\u2032 \u2217 wTf \u2032f S \u00b7 f \u2032 \u00b7 f \u00b7 n2 \u00b7 k2 2Cn\u20322 log n\u2032[f \u2032 \u00b7 S + f \u00b7 S + f \u2032 \u00b7 f ] + S \u00b7 f \u2032 \u00b7 f \u00b7 n\u20322\n\u2202L \u2202yf\u2032 \u2217 xf S \u00b7 f \u2032 \u00b7 f \u00b7 k2 \u00b7 n\u20322 2Cn log n2[f \u2032 \u00b7 S + f \u00b7 S + f \u2032 \u00b7 f ] + S \u00b7 f \u2032 \u00b7 f \u00b7 n2\nHere n\u2032 = (n\u2212 k + 1) represents the size of the output feature map. Note that the high complexity of the direct method for convolution comes from the product of five terms, whereas our method has a sum of products with at most four terms. Figure 2 shows the theoretical number of operations for direct convolution and our FFT method for various input sizes."}, {"heading": "2.3 Implementation", "text": "Although conceptually straighforward, a number of challenges relating to GPU implementation needed to be addressed. First, current GPU implementations of the FFT such as cuFFT are designed to parallelize over individual transforms. This can be useful for computing a limited number of transforms on large inputs, but is not suitable for our task since we are performing many FFTs over relatively small inputs. Therefore, we developed a custom CUDA implementation of the CooleyTukey FFT algorithm which enabled us to specify how many threads to allocate to each FFT, while also parallelizing over feature maps and minibatches. Two-dimensional transforms were performed by computing one-dimensional transforms over rows and columns, and was done in-place to save memory on the GPU. As another means to save memory, we used symmetry properties of FFTs of real inputs to only store half of the data in the frequency domain."}, {"heading": "3 Experiments", "text": "To test our analysis, we ran a series of experiments comparing the speed of our method compared to the GPU implementations of [6] and of the Torch 7 machine learning environment [3]. Specifically, we compared how each method performed with varying kernel sizes, input sizes and minibatch sizes. The results are shown in Figure 3. For all experiments, we chose 96 input feature maps and 256 output features, which represents a typical configuration of a deep network\u2019s second layer. The functions updateOutput, updateGradInput and accGradParameters correspond to the operations in (1), (2) and (3) respectively. All times are measured in seconds.\nWe see that our method significantly outperforms the other two in nearly all cases. The improvement is especially pronounced for the accGradParameters operation, which is the most computationally expensive. This is likely due to the fact that the convolution we are computing has a large kernel, for which FFTs are better suited in any case, and due to an additional heuristic whereby we sum over minibatches in the Fourier domain, thus minimizing the number of inverse FFTs. Also note that our method performs the same regardless of kernel size, since we pad the kernel to be the same size as the input image before applying the FFT. This enables the use of much larger kernels, which we intend to explore in future work."}, {"heading": "4 Discussion", "text": "We have presented a simple and fast algorithm for training and inference using convolutional networks. It outperforms known state-of-the-art implementations in terms of speed, as verified by numerical experiments. In the future we plan to explore the possibilities of learning kernels directly in the Fourier domain, which could also remove the need for specifying kernel sizes as a hyperparameter."}], "references": [{"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel P.W. Ellis", "Brian Whitman", "Paul Lamere"], "venue": "In Proceedings of the 12th International Conference on Music Information Retrieval", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Representing shape with a spatial pyramid kernel", "author": ["A. Bosch", "A. Zisserman", "X. Munoz"], "venue": "In Proceedings of the ACM International Conference on Image and Video Retrieval,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Clement Farabet"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object", "author": ["L. Fei-Fei", "R. Fergus", "Pietro Perona"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].", "startOffset": 103, "endOffset": 112}, {"referenceID": 1, "context": "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].", "startOffset": 103, "endOffset": 112}, {"referenceID": 7, "context": "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].", "startOffset": 103, "endOffset": 112}, {"referenceID": 3, "context": "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].", "startOffset": 160, "endOffset": 166}, {"referenceID": 0, "context": "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].", "startOffset": 160, "endOffset": 166}, {"referenceID": 5, "context": "Even using parallel computing environments, training a network on ImageNet can take weeks [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "1 Backpropagation The backpropagation algorithm [7] is the standard method to compute the gradient when training a convolutional network.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "3 Experiments To test our analysis, we ran a series of experiments comparing the speed of our method compared to the GPU implementations of [6] and of the Torch 7 machine learning environment [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "3 Experiments To test our analysis, we ran a series of experiments comparing the speed of our method compared to the GPU implementations of [6] and of the Torch 7 machine learning environment [3].", "startOffset": 192, "endOffset": 195}], "year": 2016, "abstractText": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.", "creator": "LaTeX with hyperref package"}}}