{"id": "1312.6204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "One-Shot Adaptation of Supervised Deep Convolutional Models", "abstract": "dataset bias remains a significant barrier towards general real world computer vision tasks. though deep convolutional networks have proven to be a competitive approach for image classification, a question remains : have these models have solved the dataset bias problem? in application, training or fine - tuning a state - of - the - art deep model on a new domain requires a significant amount of data, which for many applications is easily not clear. transfer of models directly to new algorithms without adaptation has historically led to poor recognition performance. in this paper, we pose the following disagreement : is a single image dataset, much larger studies previously explored for adaptation, comprehensive enough to extend general control models algorithms may be not applied to new image domains? in other words, are deep cnns trained on large amounts of labeled data as meaningful beyond dataset bias as previous methods have been shown to increase? we show that a generic supervised deep cnn model trained on a large dataset reduces, then does not remove, dataset failure. furthermore, we propose these steps for adaptation with deep models that are able to operate remarkably little ( one example r category ) or no labeled domain specific data. our experiments show that completion of deep treatments on benchmark selective domain diversity datasets can need a significant performance boost.", "histories": [["v1", "Sat, 21 Dec 2013 04:32:51 GMT  (63kb,D)", "https://arxiv.org/abs/1312.6204v1", null], ["v2", "Tue, 18 Feb 2014 02:57:42 GMT  (63kb,D)", "http://arxiv.org/abs/1312.6204v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["judy hoffman", "eric tzeng", "jeff donahue", "yangqing jia", "kate saenko", "trevor darrell"], "accepted": false, "id": "1312.6204"}, "pdf": {"name": "1312.6204.pdf", "metadata": {"source": "CRF", "title": "One-Shot Adaptation of Supervised Deep Convolutional Models", "authors": ["Judy Hoffman", "Eric Tzeng", "Jeff Donahue", "Yangqing Jia"], "emails": ["jhoffman@eecs.berkeley.edu", "etzeng@eecs.berkeley.edu", "jdonahue@eecs.berkeley.edu", "jiayq@google.com", "saenko@cs.uml.edu", "trevor@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Supervised deep convolutional neural networks (CNNs) trained on large-scale classification tasks have been shown to learn impressive mid-level structures and obtain high levels of performance on contemporary classification challenges [3, 23]. These models generally assume extensive training using labeled data, and testing is limited to data from the same domain. In practice, however, the images we would like to classify are often produced under different imaging conditions or drawn from a different distribution, leading to a domain shift. Scaling such models to new domains remains an open challenge.\nDeep CNNs require large amounts of training data to learn good mid-level convolutional models and final fully-connected classifier stages. While the continuing expansion of web-based datasets like ImageNet [3] promises to produce labeled data for almost any desired category, such large-scale supervised datasets may not include images of the category across all domains of practical interest. Earlier deep learning efforts addressed this challenge by learning layers in an unsupervised fashion using unlabeled data to discover salient mid-level structures [6, 8]. While such approaches are\n\u2217This work was completed while Yangqing Jia was a graduate student at UC Berkeley\nar X\niv :1\n31 2.\n62 04\nv2 [\ncs .C\nV ]\n1 8\nappealing, they have heretofore been unable to match the level of performance of supervised models, and unsupervised training of networks with the same level of depth as [17] remains a challenge.\nUnfortunately, image datasets are inherently biased [21]. Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution. Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models. Evaluation for image category classification across visually distinct domains has focused on the Office dataset, which contains 31 image categories and 3 domains [20]. Recently, [9] showed that using the deep mid-level features learned on ImageNet, instead of the more conventional bag-of-words features, effectively removed the bias in some of the domain adaptation settings in the Office dataset [20]. However, [9] limited their experiments to small-scale source domains found only in Office, and evaluated on only a subset of relevant layers.\nYet until now, almost none of the previous domain adaptation studies used ImageNet as the source domain, nor utilized the full set of parameters of a deep CNN trained on source data. Recent work by Rodner et al. [19] attempted to adapt from ImageNet to the SUN dataset, but did not take advantage of deep convolutional features.\nIn this paper, we ask the question: will deep models still suffer from dataset bias when trained with all layers of the CNN and a truly large scale source dataset? Here, we provide the first evaluation of domain adaptation with deep learned representations in its most natural setting, in which all of ImageNet is used as source data for a target category. We use the 1.2 million labeled images available in the 2012 ImageNet 1000-way classification dataset [3] to train the model in [17] and evaluate its generalization to the Office dataset. This constitutes a three orders of magnitude increase in source data compared to the several thousand images available for the largest domain in Office.\nWe find that it is easier to adapt from ImageNet than from previous smaller source domains, but that dataset bias remains a major issue. Fine-tuning the parameters on the small amount of labeled target data (we consider one-shot adaptation) turns out to be unsurprisingly problematic. Instead, we propose a simple yet intuitive adaptation method: train a final domain-adapted classification \u201clayer\u201d using various layers of the pre-trained network as features, without any fine-tuning its parameters. We provide a comprehensive evaluation of existing methods for classifier adaptation as applied to each of the fully connected layers of the network, including the last, task-specific classification layer. When adapting from ImageNet to Office, it turns out to be possible to achieve target domain performance on par with source domain performance using only a single labeled example per target category.\nWe examine both the setting where there are a few labeled examples from the target domain (supervised adaptation) and the setting where there are no labeled target examples (unsupervised adaptation). We also describe practical solutions for choosing between the various adaptation methods based on experimental constraints such as limited computation time."}, {"heading": "2 Background: Deep Domain Adaptation Approaches", "text": "For our task we consider adapting between a large source domain and a target domain with few or or no labeled examples. A typical approach to domain adaptation or transfer learning with deep architectures is to take the representation learned via back-propagation on a large dataset, and then transfer the representation to a smaller dataset by fine-tuning, i.e. backpropagation at a lower learning rate [11, 23]. However, fine-tuning requires an ample amount of labeled target data and so should not be expected to work well when we consider the very sparse label condition, such as the one-shot learning scenario we evaluate below, where we have just one labeled example per category in the target domain.\nIn fact, in our experiments under this setting, fine-tuning actually reduces performance. Specifically, on the ImageNet\u2192Webcam task reported in Section 4, using the final output layer as a predictor in the target domain received 66% accuracy, while using the final output layer after fine tuning produced a degraded accuracy of 61%.\nA separate method that was recently proposed for deep adaptation is called Deep Learning for domain adaptation by Interpolating between Domains (DLID) [5]. This method learns multiple unsupervised deep models directly on the source, target, and combined datasets and uses a representation\nwhich is the concatenation of the outputs of each model as its adaptation approach. While this was shown to be an interesting approach, it is limited by its use of unsupervised deep structures.\nIn general, unsupervised deep convolutional models have been unable to achieve the performance of supervised deep CNNs. However, training a supervised deep model requires sufficient labeled data. Our insight is that the extensive labeled data available in the source domain can be exploited using a supervised model without requiring a significant amount of labeled target data.\nTherefore, we propose using a supervised deep source model with supervised or unsupervised adaptation algorithms that are applied to models learned on the target data directly. This hybrid approach will utilize the strong representation available from the supervised deep model trained on a large source dataset while requiring only enough target labeled data to train a shallow model with far fewer parameters. Specifically, we consider training a convolutional neural network (CNN) on the source domain and using that network to extract features on the target data that can then be used to train an auxiliary shallow learner. For extracting features from the deep source model, we follow the setup of Donahue et al. [9], which extracts a visual feature DeCAF from the ImageNet-trained architecture of [17]."}, {"heading": "3 Adapting Deep CNNs with Few Labeled Target Examples", "text": "We propose a general framework for selectively adapting the parameters of a convolutional neural network (CNN) whose representation and classifier weights are trained on a large-scale source domain, such as ImageNet. Our framework adds a final domain-adaptive classification \u201clayer\u201d that takes the activations of one of the existing network\u2019s layers as input features. Note that the network cannot be effectively fine-tuned without access to more labeled target data. This adapted layer is a linear classifier that combines source and target training data using an adaptation method. To demonstrate the generality of our framework, we select a representative set of popular linear classifier adaptation approaches that we empirically evaluate in Section 4. We separate our discussion into the set of supervised and unsupervised adaptation settings.\nBelow we denote the features extracted over the source domain asX and the features extracted over the target domain as X\u0303 . Similarly, we denote the source domain image classifier as \u03b8 and the target domain image classifier as \u03b8\u0303."}, {"heading": "3.1 Unsupervised Adaptation", "text": "Many unsupervised adaptation techniques seek to minimize the distance between subspaces that represent the source and target domains. We denote these subspaces as U and U\u0303 , respectively.\nGFK [12] The Geodesic Flow Kernel (GFK) method [12] is an unsupervised domain adaptation approach which seeks embeddings for the source and target points that minimize domain shift. Inputs to the method are U and U\u0303 , lower-dimensional embeddings of the source and target domains (e.g. from principal component analysis). The method constructs the geodesic flow \u03c6(t) along the manifold of subspaces such that U = \u03c6(0) and U\u0303 = \u03c6(1). Finally, a transformation G is constructed by computing G = \u222b 1 0 \u03c6(t)\u03c6(t)\u1d40dt using a closed-form solution, and classification is performed by training an SVM on the source dataX and transformed target data GX\u0303 .\nSA [10] The Subspace Alignment (SA) method [10] also begins with low-dimensional embeddings of the source and target domains U and U\u0303 , respectively. It seeks to minimize in M , a transformation matrix, the objective \u2016UM \u2212 U\u0303\u20162F . The analytical solution to this objective is M\u2217 = U\u1d40U\u0303 . Given M\u2217, an SVM is trained on source data X and transformed target data UM\u2217U\u0303\u1d40X\u0303 ."}, {"heading": "3.2 Supervised Adaptation", "text": "Late Fusion Perhaps the simplest supervised adaptation method is to independently train a source and target classifier and combine the scores of the two to create a final scoring function. We call this approach Late Fusion. It has been explored by many for a simple adaptation approach. Let us\ndenote the score from the source classifier as vs and the score from the target classifier as vt. For our experiments we explore two methods of combining these scores, which are described below:\n\u2022 Max: Produce the scores of both the source and target classifier and simply choose the max of the two as the final score for each example. Therefore, vadapt = max(vs, vt).\n\u2022 Linear Interpolation: Set the score for a particular example to equal the convex combination of the source and target classifier scores, vadapt = (1 \u2212 \u03b1)vs + \u03b1vt. This method requires setting a hyperparameter, \u03b1, which determines the weights of the source and target classifiers.\nLate Fusion has two major advantages: it is easy to implement, and the source classifier it uses may be precomputed to make adaptation very fast. In the case of the linear interpolation combination rule, however, this method can potentially suffer from having a sensitive hyperparameter. We show a hyperparameter analysis in Section 4.\nDaume\u0301 III [7] This simple feature replication method was proposed for domain adaptation by [7]. The method augments feature vectors with a source component, a target component, and a shared component. Each source data point x is augmented to x\u2032 = (x;x;0), and each target data point x\u0303 is augmented to x\u0303\u2032 = (x\u0303;0; x\u0303). Finally, an SVM is trained on the augmented source and target data\u2014a relatively expensive procedure given the potentially large size of the source domain and the tripled augmented feature dimensionality.\nPMT [1] This classifier adaptation method, Projective Model Transfer (PMT), proposed by [1], is a variant of adaptive SVM. It takes as input a classifier \u03b8 pre-trained on the source domain. PMTSVM learns a target domain classifier \u03b8\u0303 by adding an extra term to the usual SVM objective which regularizes the angle \u03b1(\u03b8\u0303,\u03b8) = cos\u22121 ( \u03b8\u1d40\u03b8\u0303 \u2016\u03b8\u2016\u2016\u03b8\u0303\u2016 ) between the target and source hyperplanes. This results in the following loss function:\nLPMT (\u03b8\u0303) = 1\n2 \u2016\u03b8\u0303\u201622 +\n\u0393 2 \u2016\u03b8\u0303\u201622 sin2 \u03b1(\u03b8\u0303,\u03b8) + `hinge(X\u0303, Y\u0303 ; \u03b8\u0303) , (1)\nwhere `hinge(X,Y ;\u03b8) denotes the SVM hinge loss of a data matrixX , label vector Y , and classifier hyperplane \u03b8, and \u0393 is a hyperparameter which, as it increases, enforces more transfer from the source classifier.\nMMDT [15] The Max-margin Domain Transforms (MMDT) method from [15] jointly optimizes an SVM-like objective over a feature transformation matrix A mapping target points to the source feature space and classifier parameters \u03b8 in the source feature space. In particular, MMDT minimizes the following loss function (assuming a binary classification task to simplify notation, and with `hinge defined as in PMT):\nLMMDT (\u03b8, A) = 1\n2 \u2016\u03b8\u201622 +\n1 2 \u2016A\u2212 I\u20162F + Cs`hinge(X,Y ;\u03b8) + Ct`hinge(AX\u0303, Y\u0303 ;\u03b8) , (2)\nwhere Cs and Ct are hyperparameters controlling the importance of correctly classifying the source and target points (respectively)."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Datasets", "text": "The Office [20] dataset is a collection of images from three distinct domains: Amazon, DSLR, and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. Of these 31 categories, 16 overlap with the categories present in the 1000-category ImageNet classification task1. Thus, for our experiments, we limit ourselves to these 16 classes. In our experiments using Amazon as a source domain, we follow the standard training protocol for this dataset of using 20 source examples per category [20, 12], for a total of 320 images.\n1 The 16 overlapping categories are backpack, bike helmet, bottle, desk lamp, desktop computer, file cabinet, keyboard, laptop computer, mobile phone, mouse, printer, projector, ring binder, ruler, speaker, and trash can.\nImageNet [3] is the largest available dataset of image category labels. We use 1000 categories\u2019 worth of data (1.2M images) to train the network, and use the 16 categories that overlap with Office (approximately 1200 examples per category or \u224820K images total) as labeled source classifier data."}, {"heading": "4.2 Experimental Setup & Baselines", "text": "For our experiments, we use the fully trained deep CNN model described in Section 2, extracting feature representations from three different layers of the CNN. We then train a source classifier using these features on one of two source domains, and adapt to the target domain.\nThe source domains we consider are either the Amazon domain, or the corresponding 16-category ImageNet subset where each category has many more examples. We focus on the Webcam domain as our target (test) domain, as Amazon-to-Webcam was shown to be the only challenging shift in [9] (the DSLR domain is much more similar to Webcam and did not require adaptation when using deep mid-level features). This combination exemplifies the shift from online web images to realworld images taken in typical office/home environments. Note that, regardless of the source domain chosen to learn the classifier, ImageNet data from all 1000 categories was used to train the network.\nIn addition, for the supervised adaptation setting we assume access to only a single example per category from the target domain (Webcam).\nEach method is then evaluated across 20 random train/test splits, and we report averages and standard errors for each setting. For each random train/test split we choose one example for training and 10 other examples for testing (so there is a balanced test set across categories). Therefore, each test split has 160 examples. The unsupervised adaptation methods operate in a transductive setting, so the target subspaces are learned from the unlabeled test data.\nNon-adaptive Baselines In addition to the adaptation methods outlined in Section 3, we also evaluate using the following non-adaptive baselines.\n\u2022 SVM (source only): A support vector machine trained only on source data. \u2022 SVM (target only): A support vector machine trained only on target data. \u2022 SVM (source and target): A support vector machine trained on both source and target\ndata. To account for the large discrepancy between the number of training data points in the source and target domains, we weighted the data points such that the constraints from the source and target domains effectively contribute equally to the optimization problem. Specifically, each source data point receives a weight of ntns+nt , and each target data point receives a weight of nsns+nt , where ns, nt denote the number of data points in the source and target, respectively.\nMany of the adaptation methods we evaluate have hyperparameters that must be cross-validated for use in practice, so we set the parameters of the adaptation techniques as follows.\nFirst, the C value used for C-SVM in the classifier for all methods is set to C = 1. Without any validation data we are not able to tune this parameter properly, so we choose to leave it as the default value. Since all methods we report require setting of this parameter, we feel that the relative comparisons between methods is sound even if the absolute numbers could be improved with a new setting for C. For Daume\u0301 III and MMDT, which look at the source and target data simultaneously, we use the same weighting scheme as we did for the source and target SVM. Late Fusion with the linear interpolation combination rule is reported across hyperparameter settings in Figure 1(a) to help understand how performance varies as we trade off emphasis between the learned classifiers from the source and target domains. Again, we do not have the validation data to tune this parameter so we report in the tables the performance averaged across parameter settings. The plot vs \u03b1 indicates that there is usually a best parameter setting that could be learned with more available data. For PMT, we choose \u0393 = 1000, which corresponds to allowing a large amount of transfer from the source classifier to the target classifier. We do this because the source-only classifier is stronger than the target-only classifier (with ImageNet source). For the unsupervised methods GFK and SA, again we evaluated a variety of subspace dimensionalities and Figure 1(b) shows that the overall method performance does not vary significantly with the dimensionality choice."}, {"heading": "4.3 Effect of Source Domain Size", "text": "Previous studies considered source domains from the Office dataset. In this section, we ask what happens when an orders-of-magnitute larger source dataset is used.\nFor completeness we begin by evaluating Amazon as a source domain. Preliminary results on this setting are reported in [9], here we extend the comparison here by presenting the results with more adaptation algorithms and more complete evaluation of hyperparameter settings. Table 1 presents multiclass accuracies for each algorithm using either layer 6 or 7 from the deep network, which corresponds to the output from each of the fully connected layers.\nAn SVM trained using only Amazon data achieves 78.6% in-domain accuracy (tested on the same domain) when using the DeCAF6 feature and 80.2% in-domain accuracy when using the DeCAF7 feature. These numbers are significantly higher than the performance of the same classifier on Webcam test data, indicating that even with the DeCAF features, there is a still a domain shift between the Amazon and Webcam datasets.\nNext, we consider an unsupervised adaptation setting where no labeled examples are available from the target dataset. In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK [12] and SA [10]. Both of these methods make use of a subspace dimensionality hyperparameter. We show the results using a 100-dimensional subspace and leave the discussion of setting this parameter until Section 4.6. For this shift the adaptation algorithms increase performance when using the layer 6 feature, but offer no additional improvement when using the layer 7 feature.\nWe finally assume that a single example per category is available in the target domain. As the bottom rows of Table 1 show, supervised adaptation algorithms are able to provide significant improvement regardless of the feature space chosen, even in the one-shot scenario. For this experiment we noticed that using the second fully connected layer (DeCAF7) was a stronger overall feature in general."}, {"heading": "4.4 Adapting with a Large Scale Source Domain", "text": "We next address one of the main questions of this paper: Is there still a domain shift when using a large source dataset such as ImageNet? To begin to answer this question we follow the same experimental paradigm as the previous experiment, but use ImageNet as our source dataset. The results are shown in Table 2.\nAgain, we first verify that the source only SVM achieves higher performance when tested on indomain data than on Webcam data. Indeed, for the 16 overlapping labels, the source SVM produces 62.50% accuracy on ImageNet data using DeCAF6 features and 74.50% accuracy when using\nDeCAF7 features. Compare this to the 54% and 59% for Webcam evaluation and a dataset bias is still clearly evident.\nNote that when using ImageNet as a source domain, overall performance of all algorithms improves. In addition, unsupervised adaptation approaches are more effective than for the smaller source domain experiment."}, {"heading": "4.5 Adapting a Pre-trained Classifier to a New Label Set", "text": "DeCAF8 differs from the other DeCAF features in that it constitutes the 1000 activations corresponding to the 1000 labels in the ImageNet classification task. In the CNN proposed by [17], these activations are fed into a softmax unit to compute the label probabilities. We instead experiment with using the DeCAF8 activations directly as a feature representation, which is akin to training another classifier using the output of the 1000-way CNN classifier.\nTable 3 shows results for various adaptation techniques using both ImageNet and Amazon as source domains. We use the same setup as before, but instead use DeCAF8 as the feature representation.\nThe ImageNet results are uniformly better with DeCAF8 than with DeCAF6 or DeCAF7, likely due to the fact that DeCAF8 was explicitly trained on ImageNet data to effectively discriminate between ImageNet categories. Because it can more effectively classify images from the source domain, it is able to better adapt from the source domain to the target domain.\nHowever, we see a negligible difference in performance for Amazon, with performance actually decreasing with respect to DeCAF7 for certain adaptation methods. We believe this is because the final activation vector is too specific to the 1000-way ImageNet task, and that DeCAF7 provides a more general representation that is better suited to the Amazon domain. This, in turn, results in improved adaptation. In general, however, the difference between the various DeCAF representations with Amazon as a source are small enough to be insignificant."}, {"heading": "4.6 Analysis and Practical Considerations", "text": "Our adaptation experiments show that, despite its large size, even ImageNet is not large enough to cover all domains, and that traditional domain adaptation methods go a long way in increasing performance and mitigating the effects of this shift. Depending on the characteristics of the problem at hand, our results suggest different methods may be most suitable.\nIf no labels exist in the target domain, then there are unsupervised adaptation algorithms that are easy to use and fast to compute at adaptation time, yet still achieve increased performance over sourceonly methods. For this scenario, we experimented with two subspace alignment based methods that both require setting a parameter that indicates the dimensionality of the input subspaces. Figure 1(b) shows the effect that changing the subspace dimensionality has on the overall method performance. In general, we noticed that these methods were not particularly sensitive to this parameter so long as the dimensionality remains larger than the number of categories in our label set. Below this threshold, the subspace is less likely to capture all important discriminative information needed for classification.\nIn the case where we have a large source dataset and a limited number of labeled target examples, it may be preferable to compute source classifier parameters in advance, then examine only the source parameters and the target data at adaptation time. Examples of these kinds of methods are Late Fusion and PMT. These methods are unaffected by the number of data points in the source domain at adaptation time, and can thus be applied quickly. In our experiments, we found that a properly tuned Late Fusion classifier with linear interpolation was the fastest and most effective approach. Figure 1(a) shows the performance of linear interpolation Late Fusion as we vary the hyperparameter \u03b1. Although the method is sensitive to \u03b1, we found that for both source domains, the basic strategy of setting \u03b1 around 0.8 provides a close approximation to optimal performance. This setting can be interpreted as trusting the target classifier more than the source, but not so much as to completely discount the information available from the source classifier. In each table we report both the performance of linear interpolation both averaged across hyper parameter settings \u03b1 \u2208 [0, 1] as well as the performance of linear interpolation with the best possible setting of \u03b1 per experiment \u2013 this is denoted as \u201cOracle\u201d performance.\nIf there are no computational constraints and there are very few labels in the target domain, the best-performing method seems to be the \u201cfrustratingly easy\u201d approach originally proposed by Daume\u0301 III [7] and applied again for deep models in [5].\nFinally, we found that feature representation can have a significant impact on adaptation performance. Our results show that ImageNet as source performs best with the DeCAF8 representation, whereas Amazon as source performs best with the DeCAF7 representation. This, combined with our intuition, seems to indicate that for adaptation from source domains other than ImageNet, an intermediate representation other than DeCAF8 is more powerful for adaptation, whereas ImageNet classification works best with the full representation that was trained on it."}, {"heading": "5 Conclusion", "text": "In this paper, we presented the first evaluation of domain adaptation from a large-scale source dataset with deep features. We demonstrated that, although using ImageNet as a source domain generalizes\nbetter than other smaller source domains, there is still a domain shift when adapting to other visual domains.\nOur experimental results show that deep adaptation methods can go a long way in mitigating the effects of this domain shift. Based on our results, we also provided a set of practical recommendations for choosing a feature representation and adaptation method accounting for constraints on runtime and accuracy.\nThere are a number of interesting directions to take given our results. First we notice that though DeCAF8 is the strongest feature to use for learning a classifier on ImageNet data, DeCAF7 is actually a better feature to use with the Amazon source domain and the Webcam target domain. This could lead to a hybrid approach where one uses different feature representations for the various domains and produces a combined adapted model. Another interesting direction that should be explored is to integrate the adaption algorithms into the deep models explicitly and even allow for feedback between the two stages. Current deep models although allow information flow between the final classifier and the representation learning architecture. We feel that the next step is to have a separate task specific adaptable layer that does not simply learn a new final layer, but instead learns a separate, but equivalent final layer, that is regularized by the final layer learned on the source dataset.\nThis future work is a natural extension of the result we have shown in this paper: that pre-trained deep representations with large source domains can be effectively adapted to new target domains using only shallow, linear adaptation methods, and that in cases where the target data is limited, this approach is the best way to mitigate dataset bias."}], "references": [{"title": "Tabula rasa: Model transfer for object category detection", "author": ["Y. Aytar", "A. Zisserman"], "venue": "In Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Analysis of representations for domain adaptation", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": "Proc. NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["A. Berg", "J. Deng", "L. Fei-Fei"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning bounds for domain adaptation", "author": ["John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman"], "venue": "In Proc. NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "DLID: Deep learning for domain adaptation by interpolating between domains", "author": ["S. Chopra", "S. Balakrishnan", "R. Gopalan"], "venue": "In ICML Workshop on Challenges in Representation Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Emergence of object-selective features in unsupervised feature learning", "author": ["A. Coates", "A. Karpathy", "A. Ng"], "venue": "In Proc. NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daum\u00e9 III"], "venue": "In ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "In Proc. NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv e-prints,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "In Proc. ICCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "arXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "In Proc. CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "In Proc. ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Discovering latent domains for multisource domain adaptation", "author": ["J. Hoffman", "B. Kulis", "T. Darrell", "K. Saenko"], "venue": "In Proc. ECCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Efficient learning of domain-invariant image representations", "author": ["J. Hoffman", "E. Rodner", "J. Donahue", "K. Saenko", "T. Darrell"], "venue": "In Proc. ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Undoing the damage of dataset bias", "author": ["A. Khosla", "T. Zhou", "T. Malisiewicz", "A. Efros", "A. Torralba"], "venue": "In Proc. ECCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In Proc. CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Towards adapting imagenet to reality: Scalable domain adaptation with implicit low-rank transformations", "author": ["Erik Rodner", "Judy Hoffman", "Jeff Donahue", "Trevor Darrell", "Kate Saenko"], "venue": "CoRR, abs/1308.4200,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In Proc. ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Adapting SVM classifiers to data with shifted distributions", "author": ["J. Yang", "R. Yan", "A. Hauptmann"], "venue": "In ICDM Workshops,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "ArXiv e-prints,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Supervised deep convolutional neural networks (CNNs) trained on large-scale classification tasks have been shown to learn impressive mid-level structures and obtain high levels of performance on contemporary classification challenges [3, 23].", "startOffset": 234, "endOffset": 241}, {"referenceID": 22, "context": "Supervised deep convolutional neural networks (CNNs) trained on large-scale classification tasks have been shown to learn impressive mid-level structures and obtain high levels of performance on contemporary classification challenges [3, 23].", "startOffset": 234, "endOffset": 241}, {"referenceID": 2, "context": "While the continuing expansion of web-based datasets like ImageNet [3] promises to produce labeled data for almost any desired category, such large-scale supervised datasets may not include images of the category across all domains of practical interest.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Earlier deep learning efforts addressed this challenge by learning layers in an unsupervised fashion using unlabeled data to discover salient mid-level structures [6, 8].", "startOffset": 163, "endOffset": 169}, {"referenceID": 7, "context": "Earlier deep learning efforts addressed this challenge by learning layers in an unsupervised fashion using unlabeled data to discover salient mid-level structures [6, 8].", "startOffset": 163, "endOffset": 169}, {"referenceID": 16, "context": "appealing, they have heretofore been unable to match the level of performance of supervised models, and unsupervised training of networks with the same level of depth as [17] remains a challenge.", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "Unfortunately, image datasets are inherently biased [21].", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 12, "endOffset": 18}, {"referenceID": 3, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 12, "endOffset": 18}, {"referenceID": 19, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 46, "endOffset": 54}, {"referenceID": 20, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 46, "endOffset": 54}, {"referenceID": 6, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 21, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 0, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 19, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 17, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 15, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 12, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 11, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 13, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 14, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 19, "context": "Evaluation for image category classification across visually distinct domains has focused on the Office dataset, which contains 31 image categories and 3 domains [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 8, "context": "Recently, [9] showed that using the deep mid-level features learned on ImageNet, instead of the more conventional bag-of-words features, effectively removed the bias in some of the domain adaptation settings in the Office dataset [20].", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Recently, [9] showed that using the deep mid-level features learned on ImageNet, instead of the more conventional bag-of-words features, effectively removed the bias in some of the domain adaptation settings in the Office dataset [20].", "startOffset": 230, "endOffset": 234}, {"referenceID": 8, "context": "However, [9] limited their experiments to small-scale source domains found only in Office, and evaluated on only a subset of relevant layers.", "startOffset": 9, "endOffset": 12}, {"referenceID": 18, "context": "[19] attempted to adapt from ImageNet to the SUN dataset, but did not take advantage of deep convolutional features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "2 million labeled images available in the 2012 ImageNet 1000-way classification dataset [3] to train the model in [17] and evaluate its generalization to the Office dataset.", "startOffset": 88, "endOffset": 91}, {"referenceID": 16, "context": "2 million labeled images available in the 2012 ImageNet 1000-way classification dataset [3] to train the model in [17] and evaluate its generalization to the Office dataset.", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "backpropagation at a lower learning rate [11, 23].", "startOffset": 41, "endOffset": 49}, {"referenceID": 22, "context": "backpropagation at a lower learning rate [11, 23].", "startOffset": 41, "endOffset": 49}, {"referenceID": 4, "context": "A separate method that was recently proposed for deep adaptation is called Deep Learning for domain adaptation by Interpolating between Domains (DLID) [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "[9], which extracts a visual feature DeCAF from the ImageNet-trained architecture of [17].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[9], which extracts a visual feature DeCAF from the ImageNet-trained architecture of [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "GFK [12] The Geodesic Flow Kernel (GFK) method [12] is an unsupervised domain adaptation approach which seeks embeddings for the source and target points that minimize domain shift.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "GFK [12] The Geodesic Flow Kernel (GFK) method [12] is an unsupervised domain adaptation approach which seeks embeddings for the source and target points that minimize domain shift.", "startOffset": 47, "endOffset": 51}, {"referenceID": 9, "context": "SA [10] The Subspace Alignment (SA) method [10] also begins with low-dimensional embeddings of the source and target domains U and \u0168 , respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "SA [10] The Subspace Alignment (SA) method [10] also begins with low-dimensional embeddings of the source and target domains U and \u0168 , respectively.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Daum\u00e9 III [7] This simple feature replication method was proposed for domain adaptation by [7].", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Daum\u00e9 III [7] This simple feature replication method was proposed for domain adaptation by [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "PMT [1] This classifier adaptation method, Projective Model Transfer (PMT), proposed by [1], is a variant of adaptive SVM.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "PMT [1] This classifier adaptation method, Projective Model Transfer (PMT), proposed by [1], is a variant of adaptive SVM.", "startOffset": 88, "endOffset": 91}, {"referenceID": 14, "context": "MMDT [15] The Max-margin Domain Transforms (MMDT) method from [15] jointly optimizes an SVM-like objective over a feature transformation matrix A mapping target points to the source feature space and classifier parameters \u03b8 in the source feature space.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "MMDT [15] The Max-margin Domain Transforms (MMDT) method from [15] jointly optimizes an SVM-like objective over a feature transformation matrix A mapping target points to the source feature space and classifier parameters \u03b8 in the source feature space.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "The Office [20] dataset is a collection of images from three distinct domains: Amazon, DSLR, and Webcam.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "In our experiments using Amazon as a source domain, we follow the standard training protocol for this dataset of using 20 source examples per category [20, 12], for a total of 320 images.", "startOffset": 151, "endOffset": 159}, {"referenceID": 11, "context": "In our experiments using Amazon as a source domain, we follow the standard training protocol for this dataset of using 20 source examples per category [20, 12], for a total of 320 images.", "startOffset": 151, "endOffset": 159}, {"referenceID": 2, "context": "ImageNet [3] is the largest available dataset of image category labels.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "We focus on the Webcam domain as our target (test) domain, as Amazon-to-Webcam was shown to be the only challenging shift in [9] (the DSLR domain is much more similar to Webcam and did not require adaptation when using deep mid-level features).", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "GFK [12] Amazon 53.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "1 SA [10] Amazon 51.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "3 Daum\u00e9 III [7] Amazon+Webcam 68.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "4 PMT [1] Amazon+Webcam 64.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "8 MMDT [15] Amazon+Webcam 65.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "We show here multiclass accuracy on the target domain test set for both supervised and unsupervised adaptation experiments across the two fully connected layer features (similar to [9], but with one labeled target example).", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "Preliminary results on this setting are reported in [9], here we extend the comparison here by presenting the results with more adaptation algorithms and more complete evaluation of hyperparameter settings.", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK [12] and SA [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK [12] and SA [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "GFK [12] ImageNet 65.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "4 SA [10] ImageNet 59.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "1 Daum\u00e9 III [7] ImageNet+Webcam 59.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "5 PMT [1] ImageNet+Webcam 66.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "8 MMDT [15] ImageNet+Webcam 59.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "GFK [12] Source 68.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "2 SA [10] Source 66.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "3 Daum\u00e9 III [7] Source+Webcam 77.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "7 PMT [1] Source+Webcam 70.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "1 MMDT [15] Source+Webcam 73.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "In the CNN proposed by [17], these activations are fed into a softmax unit to compute the label probabilities.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "In each table we report both the performance of linear interpolation both averaged across hyper parameter settings \u03b1 \u2208 [0, 1] as well as the performance of linear interpolation with the best possible setting of \u03b1 per experiment \u2013 this is denoted as \u201cOracle\u201d performance.", "startOffset": 119, "endOffset": 125}, {"referenceID": 6, "context": "If there are no computational constraints and there are very few labels in the target domain, the best-performing method seems to be the \u201cfrustratingly easy\u201d approach originally proposed by Daum\u00e9 III [7] and applied again for deep models in [5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "If there are no computational constraints and there are very few labels in the target domain, the best-performing method seems to be the \u201cfrustratingly easy\u201d approach originally proposed by Daum\u00e9 III [7] and applied again for deep models in [5].", "startOffset": 241, "endOffset": 244}], "year": 2014, "abstractText": "Dataset bias remains a significant barrier towards solving real world computer vision tasks. Though deep convolutional networks have proven to be a competitive approach for image classification, a question remains: have these models have solved the dataset bias problem? In general, training or fine-tuning a state-ofthe-art deep model on a new domain requires a significant amount of data, which for many applications is simply not available. Transfer of models directly to new domains without adaptation has historically led to poor recognition performance. In this paper, we pose the following question: is a single image dataset, much larger than previously explored for adaptation, comprehensive enough to learn general deep models that may be effectively applied to new image domains? In other words, are deep CNNs trained on large amounts of labeled data as susceptible to dataset bias as previous methods have been shown to be? We show that a generic supervised deep CNN model trained on a large dataset reduces, but does not remove, dataset bias. Furthermore, we propose several methods for adaptation with deep models that are able to operate with little (one example per category) or no labeled domain specific data. Our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a significant performance boost.", "creator": "LaTeX with hyperref package"}}}