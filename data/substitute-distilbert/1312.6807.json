{"id": "1312.6807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2013", "title": "Iterative Nearest Neighborhood Oversampling in Semisupervised Learning from Imbalanced Data", "abstract": "transductive graph - based semi - supervised learning methods usually build an information chain utilizing both labeled and null samples as vertices. those methods propagate label information of labeled labels to neighbors through their edges in vain to get the predicted labels of unlabeled samples. instead typically incomplete - supervised learning approaches are sensitive against limiting label distribution happened in imbalanced labeled datasets. the class boundary will be progressively skewed by the majority classes needing an exponential classification. in 1962 paper, we proposed a simple and effective approach to alleviate the unfavorable influence of imbalance regions by iteratively selecting a matched unlabeled samples and adding them into the minority classes to form a balanced labeled dataset for the learning methods afterwards. the experiments on uci datasets and mnist handwritten digits both showed that the proposed approach outperforms somewhat existing state - of - art methods.", "histories": [["v1", "Tue, 24 Dec 2013 12:24:30 GMT  (478kb)", "http://arxiv.org/abs/1312.6807v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fengqi li", "chuang yu", "nanhai yang", "feng xia", "guangming li", "fatemeh kaveh-yazdy"], "accepted": false, "id": "1312.6807"}, "pdf": {"name": "1312.6807.pdf", "metadata": {"source": "CRF", "title": "Iterative Nearest Neighborhood Oversampling in Semi-supervised Learning from Imbalanced Data", "authors": ["Fengqi Li", "Chuang Yu", "Nanhai Yang", "Feng Xia", "Guangming Li", "Fatemeh Kaveh-Yazdy"], "emails": ["f.xia@ieee.org"], "sections": [{"heading": null, "text": "If the dataset only contains two classes, a binary classification, the class that has more samples is called the majority class, and the other one is called the minority class. Many popular SSL methods are sensitive to the initial labeled dataset and are suffered from a severe skew of data to the majority classes. In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.\nThe imbalance learning problem [7] puzzles many machine learning methods established on the assumption that every class has the same or approximate same quantity of samples in raw data. There are various methods proposed to deal with the imbalance classification problems. These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].\nRe-sampling methods include oversampling [19, 21] and undersampling [14] approaches, in which the class distribution is balanced by adding a few of samples to minority class or removing a few of samples from majority class, respectively.\nMost existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4]. The bias caused by differing class balances can be systematically adjusted by re-weighting [15, 16] or re-sampling [17].\nFocused on the bad performance of SSL algorithm to the imbalanced learning problem, we propose a novel approach based on oversampling in consideration of the SSL\u2019s characteristic that there are abounds of unlabeled samples. Li et al. combined active learning with SSL methods that sample a few of most helpful modules for learning a prediction model in [20]. Based on above considerations, Iterative Nearest Neighborhood Oversampling (INNO) algorithm we propose in this paper tries to convert a few unlabeled samples to labeled samples for minority classes, consequently constructing a balanced or approximately balanced labeled dataset for standard graph-based SSL methods afterwards. Therefore, we aim to alleviate the unfavorable impact of typical classifiers in dealing with imbalanced dataset in SSL domain.\nIn this paper, we provide an effective and efficient heuristic method to eliminate the \u2018injustice\u2019 brought by imbalanced labeled dataset. As the samples with a close affinity in a low dimension feature space will probably have the same label, we propose an iterative search approach to simply oversample a few unlabeled samples around known labeled samples in order to form a balanced labeled dataset. Extensive experiments on synthetic and real datasets confirm the effectiveness and efficiency of our proposed algorithms.\nThe remainder of this paper is organized as follows. In Section 2, we provide a brief review of existing studies of semi-supervised learning and their applications on imbalanced problem. We give the motivation behind the proposed INNO in Section 3. In Section 4, we revisit some popular algorithms by giving a graph transduction regularization framework, and then we introduce our proposed algorithm INNO in details. The experimental results on some imbalanced dataset are presented in Section 5. Finally, we conclude the paper in Section 6."}, {"heading": "2 Related Work", "text": "As SSL accomplishes an inspiring performance in combining a small scale of labeled samples and a mass mount of unlabeled samples effectively, it has been utilized in many real-world applications such as topic detection, multimedia information identification and object recognition. For the past few years, graph-based SSL approaches have attracted increasing attention due to their good performance and ease of implementation. Graph-based SSL regards both labeled and unlabeled samples as vertices in a graph and builds edges between pairwise vertices, and the weight of edge represents the similarity between the corresponding vertices. Transductive graph-based SSL methods predict the label for unlabeled samples via graph partition or label propagation using a small portion of seed labels provided by initial labeled dataset [22]. Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9]. Recent researches on graph-based SSL include ensemble manifold regularization [18] and relevance feedback [12]. However, these graph-based SSL methods developed with smoothness, clustering assumption, and manifold assumption [1] frequently perform a bad classification if provided an imbalanced dataset.\nWang et al. [15] proposed a node regularizer to balance the inequitable influence of labels from different classes, which can be regarded as a re-weighting method. They developed an alternating minimization procedure to interleave optimize the node regularizer and classification function, and greedily searched the largest negative gradient of cost function to determinate the label of an unlabeled samples during each minimization step until acquiring all predicted labels of unlabeled samples. Nevertheless, the time complexity of the algorithm is O(n3), and also it would be suffered from error occured in classification progress, during iteration. Its modified algorithm LDST [16] revises the unilateral greedy search strategy into a bidirectional manner, which can drive wrong label correction in addition to eliminate imbalance problem.\nOther graph-based SSL algorithms solve imbalance problem mainly by re-sampling methods. Li et al. [2] proposed semi-supervised learning with dynamic subspace generation algorithm based on undersampling to handle imbalanced classification. They constructed several subspace classifiers on the corresponding balanced subset by iteratively performing under-sampling without duplication on majority class to form a balanced subnet. However, the algorithm features high complexity in computational time."}, {"heading": "3 Motivation", "text": "Transductive graph-based SSL methods propagate label information of labeled samples to their\nneighbors through edges to get the predicted labels of unlabeled samples. Once there is an imbalanced distribution of classes in labeled dataset, the class boundary will severely skew to the majority classes, which have a more possibility to influence the predicted labels of unlabeled samples. We draw the influence of imbalance classification result to three popular transductive GSSL methods on the two-moon toy dataset in Figure 1. The symbols \u2018\u25a1\u2019 and \u2018\u25bd\u2019 stand for class \u2018+1\u2019 and \u2019-1\u2019 respectively in raw data, and we use solid symbol to depict labeled data. Originally, class \u2018+1\u2019 contains one labeled samples and class \u2018-1\u2019 contains ten labeled data. In Figure 1, it can be seen that the impact of imbalance label distribution to aforementioned algorithms even on a well-separated dataset. The conventional transductive graph-based SSL algorithms, such as GFHF [10], LGC [11], and GTAM [15], fail to give the acceptable classification result.\nOversampling methods have been shown to be very successful in handling with imbalanced problem. However, Barua et al. [19] reported some cases of insufficiencies and inappropriateness in existing methods. They proposed MWMOTE that generated synthetic minority samples by using clustering approach to select samples according to data importance around a subnet of the minority class; however it achieves to select minority samples around the class boundary under a large number of training set. Plessis and Sugiyama [17] proposed a semi-supervised learning method to estimate the class ratio for test dataset by combining train and test dataset in supervised learning. However, these methods are inapplicable in SSL scenario.\nIn order to handle with the imbalance problem of labeled dataset in SSL scenario, considering the problem of abundant unlabeled samples in SSL domain, we proposed a simple and effective method, called Iterative Nearest Neighborhood Oversampling, to convert a few of unlabeled samples to labeled samples for minority class, which can construct a balanced labeled samples for learning methods. We integrate the proposed algorithm with two popular transductive graph-based SSL methods to perform a robust classification to imbalanced problem, and the processing flow can be described as Figure 2."}, {"heading": "4 Iterative Nearest Neighborhood Oversampling", "text": ""}, {"heading": "4.1 Graph-based SSL Formulation", "text": "Given a raw dataset X = {XL\u222aXU} containing n samples, where XL = {(x1,y1), (x2,y2),\u2026, (xl,yl)} is\nthe labeled dataset with cardinality |XL| = l and XU = {xl+1, xl+2,\u2026, xl+u}is the unlabeled dataset with cardinality |XU| = u, where l+u = n and typically l << u. Define the labels correlation to labeled dataset is YL = {y1, y2,\u2026, yl}, where yi \u2208 {1,2,...,c}(i=1...l) and c is the count of different classes. The goal of SSL is to infer YU = {yl+1, yl+2,..., yl+u} of unlabeled samples by combining XL and XU. Graph-based SSL formulate an undirected graph G = {X, E, W} in which the vertex set is X = {xi \u2208 Rd} (i=1...n, d is the number of features) and the edge set between vertex is E = {eij}. The samples are treated as vertex and the edges eij can be weighted by Wij = k(xi, xj), where k(xi, xj) could be a similarity measure such as Euclidean distance, RBF distance or cosine distance; thus, the weight matrix can be represented as W = {Wij}.\nDefine the graph Laplacian \u0394 = D\u2212W and the normalized graph Laplacian is L = D-1/2\u0394D-1/2 = I \u2212 D-1/2WD-1/2, where D = diag{D11, D22,..., Dnn}is the node degree matrix with diagonal element Dii = \u2211jWij. The binary label matrix Y = {Yij \u2208 B n\u00d7c } is set as Yij = 1 if xi is labeled as class j and Yij = 0 otherwise. Most Graph-based SSL methods perform label propagation procedure based on manifold assumption, that is, the labels are smooth on the graph. Therefore, they essentially estimate a classification function {F: X \u2192 Rn\u00d7c} constrained to give the true label for labeled samples and given smooth labels over the whole graph. Mathematically, Graph-based SSL methods formulate a regularization framework by a cost function as follows: Q{F} = Ql(F) + Qs(F) (1) where Ql(F) is a loss function to penalize the deviation from the given labels, and Qs(F) is regarded as the smooth regularizer to prefer the label smoothness. The optimal * arg min (F)\nF F Q    can\nbe calculated by minimization the cost function Q{F}. Therefore, different Graph-based SSL methods can be obtained by assigning different loss functions and regularizers to Ql(F) and Qs(F)."}, {"heading": "4.2 Methodology", "text": "In real-world applications, labeled samples are always sampled according with normalization distribution. Labels of samples in some classes are easy to obtain, and in others that are not, even if they are of the same important level. To deal with the imbalance classification problem in semi-supervised learning scenarios, we assume that there are lots of unlabeled samples around a labeled sample in a low feature dimension space. Therefore, we can select a few unlabeled samples for minority class to form a balanced dataset. We describe our oversampling model as follows: Consider the multi-class classification scenarios, let r = {r1, r2,..., rc} denotes the size set of labeled samples in labeled dataset, where rj (j = 1...c) is the number of labeled samples in class j. We use standard variance var(r) represent the dispersion degree of the quantity of labeled samples in each class, and the imbalance ratio var(r) can be describe as follows:\n1/ 2\n2\n1\n1var( ) ( ) c\nj j r r r c          (2)\nwhere 1 sumr rc  ,\n1\nc\nsum j j r r   . We propose a novel approach to iteratively increase the labeled samples of the minority class, named Iterative Nearest Neighborhood Oversampling (INNO), in order to eliminate the adverse influence of imbalanced labeled dataset. During iteration, we obtain the class j containing the smallest number of labeled data, and traverse k neighbors of each labeled samples in class j to select the most similar sample to all labeled samples of class j in the unlabeled dataset. The most similar sample can be defined as:\nmax maxarg ( , ) k ux X jk kx k x x (3) where xj is the labeled samples in class j.\nTo avoid xmaxk deriving from classification boundary, we skip the samples which are connected to labeled samples of remainder classes. Then, we simply label the sample xmaxk with class j, remove it from unlabeled dataset XU and add it to the labeled dataset XL. We formalize the INNO approach as algorithm 1.\nAlgorithm 1 Iterative Nearest Neighborhood Oversampling (INNO) Input: k NN graph, affinity matrix W, stop parameter s, imbalanced labeled dataset XL and unlabeled dataset XU; Output: balanced or approximate balanced labeled dataset. Procedure: 1 while var(r) > s 2 Initialization\n1... minj j cr r , max = \u2212\u221e, maxk = 0;\n3 for each labeled sample xj in class j 4 for each neighbors xk of xj 5 skip the xk if it is in XL or xk has edges between labeled samples in other class; 6 if Wij > max, then update max, maxk 7 end for 8 end for 9 if maxk=0 // all the neighbors of labeled samples in class j have edges with labeled samples in other classes, then rj = rmax, continue; 10 label xmaxk with class j, remove it from XU, add it to XL, rj = rj + 1; 11 end while As labeled dataset is very scarce scale compared with unlabeled samplesset in the background of semi-supervised learning, it\u2019s difficult to infer the class boundary by a small number of labeled data, caused by intrinsic sample selection bias or inevitable non-stationarity. Therefore, classic oversampling methods [19, 23] is not capable in this situation, because they need to judge of the informative data close to class boundary, in order to synthetically generate new samples for minority class. In contrary, we try to skip the unlabeled samples close to class boundary to reduce the risk of introducing reckless mistakes in SSL scenarios. So, we simply set rj = rmax if the iteration finds all the neighbors of labeled samples in class j have edges with labeled samples in other classes, that is, no more samples will be introduced for class j. Moreover, our method is capable of multi-class classification, though most sampling methods are used to diagnose between-class imbalance problem.\nHere we consider a binary classification demonstration in Figure 3, where the stars and circles represent the samples of majority and minority class respectively and the yellow points are unlabeled samples. The imbalance ratio of labeled dataset between class \u2018+1\u2019 and \u2018-1\u2019 is r+1:r-1 = 2:4. We employ a k-nearest neighbor classifier on the graph (assuming k = 2) and only consider the neighborhood connections in class \u2018+1\u2019. We set the stop parameter s = 0, that is, the iteration will stop when all classes have the same quantity of samples. As we can see, sample A and B are the initial labeled samples in minority class \u2018+1\u2019, and then we show the process of INNO algorithm to balance the labeled dataset. The algorithm searches all neighbor unlabeled samples of A and B, finding the closest sample C which is not in labeled dataset and have no connections to labeled samples of class \u2018-1\u2019, therefore, label C with \u2018+1\u2019 and remove it from unlabeled dataset and add it into labeled dataset. The algorithm continues to search the neighbors of A, B and C to find the sample D, but the D is connected to labeled sample of class \u2018-1\u2019, so it skips D and E as\nwell. Thereby it finds sample F which satisfies all search conditions. At this moment, a balanced labeled dataset is obtained, and the algorithm ends with s = 0."}, {"heading": "4.3 Complexity analysis", "text": "Our method query k neighbors of every labeled sample in each iteration, the time of query is (rsum + rsum\u00d7k) \u00d7 k, and the time of iteration in the worst situation is rmax\u00d7c \u2212 rmin\u00d7(c-1), where rmax and rmin is the largest and smallest number of labels. The time complexity of the proposed algorithm is 2 3 3max maxmax max ( 1)\n( ) ( ) 2\nr r O c r k k O ck r       . As the scale of labeled samples is small,\nthus the rmax<l<<n inequality is held. Clearly, class number c and neighbor number k are very small, thus resulting in low computational complexity for our algorithm."}, {"heading": "5 Experiments", "text": "There are many accuracy measures for evaluating the two-class classification problems, such as precision, recall, geometric-mean (G-mean) and F-measure [12]. To evaluate the classifier performance, we calculate the accuracy by a confusion matrix as illustrated in Table 1.\nAccording to Table 1, many performance measures can be derived and domain classes are regarded as positive and negative classes. One of the most common criteria is overall accuracy which is used for two class classification problems in this paper.\nIt can be defined as TP TNaccuracy\np n   \n(4)\nThis measure provides a simple way of describing a classifier\u2019s performance on a given data set. Meanwhile, we apply RBF kernel function Wij = exp(\u03a3d |xid - xjd|2/\u03c32) to calculate the similarity measure between samples, and set parameter \u03b1=0.99 in LGC [9] and \u03bc = 99 in GTAM [14] during the experiments and the results are the average results of 50 runs. All the experiments are run on a Dell Optiplex-380 PC with Intel Pentium dual-core processors 2.93GHz and main memory of 3GB."}, {"heading": "5.1 UCI datasets", "text": "Firstly we evaluate the effectiveness of our proposed INNO algorithm combined with SSL methods on IRIS and IONOSPHER datasets from UCI repository. IRIS dataset consists of three different categories of flower, \u2018setosa\u2019, \u2018versicolor\u2019 and \u2018virginica\u2019. Each category contains 50 samples, and the feature dimension of a sample is 4. We fix the number of labeled samples in category \u2018setosa\u2019 at 10, but range the number of labeled samples in category \u2018versicolor\u2019 from 1 to 10, and also range the number of labeled samples in category \u2018virginica\u2019 from 10 to 20. Stop parameter s is set to zero on this dataset, namely the balance algorithm stops when the labeled dataset are completely balanced. Set \u03c3 = 0.26 in RBF kernel function and the neighbors k = 5 in k-NN. Figure 5(a) shows the classification result on IRIS. IONOSPHERE dataset has 351samples of \u2018g\u2019 and \u2018b\u2019 categories. Category \u2018g\u2019 and \u2018b\u2019 contains 225, 126 respectively, and each sample has 34 dimensions. We set the stop parameter s=0 in INNO algorithm. As the class distribution in the original data set are not balanced, so that the label balance algorithm can stop at the right point where the imbalance ratio of labeled dataset are consistent with the original dataset. Zhu et al. proposed in CMN method in [10] to solve the negative influence to the classification result caused by imbalanced labeled dataset. We compared it with our algorithm in this paper. In this experiment, we set the size of labeled samples in category \u2018g\u2019 by ranging from 12 to 21 and set the size of labeled samples in category \u2018b\u2019 range of 2~11. Set \u03c3 = 1 in RBF kernel function and k = 10 in k-NN. Figure 5(b) shows the classification accuracy on IONOSPHERE.\nIt can be seen in Figure 4(a) that all algorithms have high classification accuracy when each class has the same number of labeled data. As the imbalanced ratio increases between different classes in labeled dataset, classification accuracy drops gradually in GFHF, LGC and GTAM algorithm, around 70% when imbalance ratio is about 9, while the proposed INNO+GFHF and INNO+LGC algorithms remain stable, basically maintained about 95%. Therefore, INNO algorithm shows a better robustness when dealing with imbalanced labeled dataset. We can get similar results from Figure 4(b), although GRF+CMN method can reduce the influence of imbalanced labeled dataset to classification results to same extent, it\u2019s under the assumption that the labeled dataset have the same distribution with the original data from the class definitely. Therefore, the classification accuracy drops when the class distribution in the original data is different from the class distribution in labeled dataset."}, {"heading": "5.2 Handwritten digit dataset", "text": "In this section, we conduct two classification experiments on MNIST handwritten digit dataset. MNIST handwritten dataset has a training set of 60000 samples and a test set of 10000 samples, each sample has a pixel of 28\u00d728, and each pixel is a grayscale range from 0 to 255. In these experiments, we combined the training and test set together and the pixel values of the image were used directly as features, i.e. each sample has a feature number of 784. We randomly selected 200 samples of the number \u20180\u2019 to \u20189\u2019 from the entire data set, so the sample set has 2000 samples. We used the parameter \u03c3 = 380 that Zhu et al. [8] set in MNIST and set the stop parameter s = 0 in INNO. We selected digit \u20185\u2019 to \u20189\u2019 to conduct a 5-class experiment and digit \u20180\u2019 to \u20189\u2019 to conduct a 10-class experiment. Figure 5 shows that the classification accuracy of each algorithm curves as we continue increasing the imbalanced ratio of labeled dataset.\nFigure 5 illustrates the label balance algorithm is not necessary when the imbalanced ratio\napproximates to 0, therefore, LGC ,INNO+LGC, GFHF and INNO+GFHF algorithm have the same classification accuracy, namely the algorithm we propose will not affect the accuracy of the original algorithm when the labeled dataset is balanced at the beginning. While the classification accuracy of GFHF, LGC and GTAM is decreased significantly when the imbalance ratio increases gradually, INNO+GFHF and INNO+LGC raised in the figure show a stable performance. It also can be seen that the experiments on digit \u20185\u2019 to \u20189\u2019 and digit \u20180\u2019 to \u20189\u2019 show that the accuracy of GTAM algorithm decreases along with the class numbers increasing obviously, while others are not."}, {"heading": "5.3 Parameter discussion", "text": "Intuitively, the number of neighbors, k, will affect the result of INNO algorithm. To validate this conjecture, we perform an experiment on the UCI datasets. We fix the imbalanced ratio of labeled dataset at 10:1:20 with categories \u2018setosa\u2019, \u2018versicolor\u2019 and \u2018virginica\u2019 on IRIS dataset, so var(r) = 9.50, and fix the imbalanced ratio of labeled dataset at 23:2 between class \u2018g\u2019 and \u2018b\u2019 on IONOSPHERE dataset, so var(r) = 14.85. Set the stop parameter s to zero, the classification accuracy trends are shown in Figure 6.\nThe classification accuracy is not high when k value is too small, so by increasing k, the classification accuracy increases drastically, and then the classification accuracy on IRIS fluctuates lenitively around 90%~95% when k remains in a certain region. However, if k continues to increase, the classification accuracy begins to drop severely down to 75%. It\u2019s because when k is too large, the number of nearest neighbors is excessive, so INNO algorithm will find that all the neighbors are connected to other categories. Then INNO algorithm is unable to balance the labeled samples, resulting in lower classification accuracy. Moreover, we consider the influence of stop parameter s to classification accuracy. We perform another experiment on IRIS and IONOSPHERE with k to 5 and 10 respectively and fix the imbalanced ratio as same as above. We change the stop parameter s to observe the oscillation on classification accuracy in Figure 7. As we can see from Figure 7(a), it shows little improvement of the classification accuracy when the stop parameter and the original imbalanced ratio are nearly the same. At this point, INNO algorithm doesn\u2019t convert enough unlabeled samples into labeled data. The labeled dataset tends to become more and more balanced and the classification accuracy increases quickly and falls in a certain range, when the step parameter is decreased.\nImproved classification accuracy could be achieved by choosing smaller stop condition values. But the results are not as supposed and oscillate in a certain range, as shown in Figure 7(b). One possible reason is that the INNO algorithm skips the classification boundary of unlabeled samples at algorithm step 5 if all the neighbors of labeled samples in minority class have edges with labeled samples in other classes. When the stop parameter becomes smaller, the more unlabeled samples will be selected and the probability of the unlabeled samples close to classification boundary is higher. As the algorithm searched the neighbors of labeled samples in minority class, the labeled number of the current class will be simply set to rmax at the algorithm step 9; therefore, it will not introduce new labeled samples in this class. And as well taking into account of the randomness of labeled samples selected from raw data by algorithm, we cannot foresee the occurrence of such a situation [17], namely the imbalanced output may also occur even the stop condition s is 0, so the classification accuracy ranges in a certain scope."}, {"heading": "6 Conclusion", "text": "In classification scenarios, state-of-art semi-supervised learning methods estimate a classification function on the assumption that there is a balanced distribution in labeled and unlabeled dataset. However, the class boundary will be severely skewed by majority class in an imbalance between-class are, which is proved by the experiments on UCI datasets and MNIST digit recognition. As the bias caused by disproportionately imbalanced dataset adjusted by re-sampling or re-weighting, we proposed the INNO algorithm to settle down this imbalance problem simply and effectively, which eliminates the \u2018injustice\u2019 brought by imbalanced labeled dataset to popular transductive graph-based SSL methods. Our method iteratively searches the neighbors of labeled samples in minority class to seek out the nearest neighbor to all labeled samples of minority class, and try to skip the unlabeled samples close to the class boundary. Therefore, we can construct a balanced or approximately balanced labeled dataset for the learning methods. The experiments show a better classification result of SSL methods combined with INNO."}], "references": [{"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6kopf", "A. Zien", "Eds"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Semi-supervised learning for imbalanced sentiment classification.", "author": ["S. Li", "Z. Wang"], "venue": "Proceedings of the Twenty-Second international joint conference on Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Parameter Estimation of One-Class SVM on Imbalance Text Classification.", "author": ["L. Zhuang", "H. Dai"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A comprehensive survey of data mining-based fraud detection research.", "author": ["C. Phua", "V. Lee", "K. Smith", "R. Gayler"], "venue": "Artificial Intelligence Review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Outside the Closed World: On Using Machine Learning for Network Intrusion Detection", "author": ["R. Summer", "V. Paxson"], "venue": "Proc. 2010 IEEE Symp. Security and Privacy, IEEE CS Press, 2010, pp. 305\u2013316.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Classification and knowledge discovery in protein databases.", "author": ["P. Radivojac", "N.V. Chawla", "A.K. Dunker", "Z. Obradovic"], "venue": "J. Biomedical Informatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Toward Scalable Learning with Non-Uniform Class and Cost Distributions", "author": ["P. Chan", "S. Stolfo"], "venue": "Proc. Int\u2019l Conf. Knowledge Discovery and Data Mining, pp. 164-168, 1998.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "A Multiple Resampling Method for Learning from Imbalanced Data  Sets", "author": ["A. Estabrooks", "T. Jo", "N. Japkowicz"], "venue": "Computational Intelligence, vol. 20, pp. 18-36, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Machine Learning Research., vol. 7, pp. 2399\u20132434, Nov. 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "Proc. Int. Conf. Mach. Learn., pp. 912\u2013919, 2003", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems, vol. 16. Cambridge, MA: MIT Press, 2004, pp. 321\u2013328.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A multimedia retrieval framework based on semi-supervised ranking and relevance feedback", "author": ["Y. Yang", "F. Nie", "D. Xu", "J. Luo", "Y. Zhuang", "Y. Pan"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 723\u2013742, Apr. 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning from Imbalanced Data", "author": ["H. Haibo", "E.A. Garcia"], "venue": "IEEE Trans. on Knowl. Data Eng.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Exploratory under-sampling for class-imbalance learning", "author": ["X.-Y. Liu", "J. Wu", "Z.-H. Zhou"], "venue": "In Proc. of the International Conference on Data Mining (ICDM),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Graph transduction via alternating minimization", "author": ["J. Wang", "T. Jebara", "S.-F. Chang"], "venue": "Proc. Int. Conf. Mach. Learn., pp. 1144\u20131151, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Label diagnosis through self tuning for web image search", "author": ["J. Wang", "Y.-G. Jiang", "S.-F. Chang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1390\u20131397, 2009", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching", "author": ["M. Plessis", "M. Sugiyama"], "venue": "Proc. Int. Conf. Mach. Learn., pp. 823-830, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensemble Manifold Regularization", "author": ["D. Tao", "C. Xu", "L. Yang", "X.S. Hua"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., pp 1227-1223, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "MWMOTE-Majority Weighted Minority Oversampling Technique for Imbalaced Data Set Learning", "author": ["S. Barua", "M. Isam", "X. Yao", "K. Murase"], "venue": "IEEE Trans. Knowl. Data Eng., vol ,2012", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Sample-based software defect prediction with active and semi-supervised learning", "author": ["M. Li", "H. Zhang", "R. Wu", "Z.H. Zhou"], "venue": "Autom Softw Eng, vol 19, pp. 201\u2013230, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "SMOTE: Synthetic Minority Over-Sampling Technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "J. Artificial Intelligence Research, vol. 16, pp. 321-357, 2002", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust and Scalable Graph-Based Semisupervised Learning", "author": ["W. Liu", "J. Wang", "S.-F. Chang."], "venue": "Proceedings of the IEEE , vol.100, no.9, pp.2624-2638, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "An Instance-Weighting Method to Induce Cost-Sensitive Trees", "author": ["K.M. Ting"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 14, no. 3, pp. 659-665, 2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A Kernel-Based Two-Class Classifier for Imbalanced Data Sets", "author": ["X. Hong", "S. Chen", "C.J. Harris"], "venue": "IEEE Trans. Neural Networks, vol. 18, no. 1, pp. 28-41, Jan. 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning on the Border: Active Learning in Imbalanced Data Classification", "author": ["S. Ertekin", "J. Huang", "L. Bottou", "L. Giles"], "venue": "Proc. ACM Conf. Information and Knowledge Management, pp. 127-136, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "In machine learning, semi-supervised learning (SSL) methods [1] train a classifier by combining labeled and unlabeled samples together, which has attracted attentions due to their advantage of reducing the need for labeled samples and improving accuracy in comparison with most of supervised learning methods.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "However, although most existing methods have shown encouraging success in many applications, they assume that the distribution between classes in both labeled and unlabeled datasets are balanced, which may not satisfy the reality [2].", "startOffset": 230, "endOffset": 233}, {"referenceID": 2, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 163, "endOffset": 166}, {"referenceID": 6, "context": "The imbalance learning problem [7] puzzles many machine learning methods established on the assumption that every class has the same or approximate same quantity of samples in raw data.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 139, "endOffset": 147}, {"referenceID": 24, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 139, "endOffset": 147}, {"referenceID": 18, "context": "Re-sampling methods include oversampling [19, 21] and undersampling [14] approaches, in which the class distribution is balanced by adding a few of samples to minority class or removing a few of samples from majority class, respectively.", "startOffset": 41, "endOffset": 49}, {"referenceID": 20, "context": "Re-sampling methods include oversampling [19, 21] and undersampling [14] approaches, in which the class distribution is balanced by adding a few of samples to minority class or removing a few of samples from majority class, respectively.", "startOffset": 41, "endOffset": 49}, {"referenceID": 13, "context": "Re-sampling methods include oversampling [19, 21] and undersampling [14] approaches, in which the class distribution is balanced by adding a few of samples to minority class or removing a few of samples from majority class, respectively.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 13, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 18, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 22, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 3, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 207, "endOffset": 210}, {"referenceID": 14, "context": "The bias caused by differing class balances can be systematically adjusted by re-weighting [15, 16] or re-sampling [17].", "startOffset": 91, "endOffset": 99}, {"referenceID": 15, "context": "The bias caused by differing class balances can be systematically adjusted by re-weighting [15, 16] or re-sampling [17].", "startOffset": 91, "endOffset": 99}, {"referenceID": 16, "context": "The bias caused by differing class balances can be systematically adjusted by re-weighting [15, 16] or re-sampling [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "combined active learning with SSL methods that sample a few of most helpful modules for learning a prediction model in [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "Transductive graph-based SSL methods predict the label for unlabeled samples via graph partition or label propagation using a small portion of seed labels provided by initial labeled dataset [22].", "startOffset": 191, "endOffset": 195}, {"referenceID": 9, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 333, "endOffset": 336}, {"referenceID": 17, "context": "Recent researches on graph-based SSL include ensemble manifold regularization [18] and relevance feedback [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Recent researches on graph-based SSL include ensemble manifold regularization [18] and relevance feedback [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "However, these graph-based SSL methods developed with smoothness, clustering assumption, and manifold assumption [1] frequently perform a bad classification if provided an imbalanced dataset.", "startOffset": 113, "endOffset": 116}, {"referenceID": 14, "context": "[15] proposed a node regularizer to balance the inequitable influence of labels from different classes, which can be regarded as a re-weighting method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Its modified algorithm LDST [16] revises the unilateral greedy search strategy into a bidirectional manner, which can drive wrong label correction in addition to eliminate imbalance problem.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "[2] proposed semi-supervised learning with dynamic subspace generation algorithm based on undersampling to handle imbalanced classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The conventional transductive graph-based SSL algorithms, such as GFHF [10], LGC [11], and GTAM [15], fail to give the acceptable classification result.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "The conventional transductive graph-based SSL algorithms, such as GFHF [10], LGC [11], and GTAM [15], fail to give the acceptable classification result.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "The conventional transductive graph-based SSL algorithms, such as GFHF [10], LGC [11], and GTAM [15], fail to give the acceptable classification result.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "[19] reported some cases of insufficiencies and inappropriateness in existing methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Plessis and Sugiyama [17] proposed a semi-supervised learning method to estimate the class ratio for test dataset by combining train and test dataset in supervised learning.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "Therefore, classic oversampling methods [19, 23] is not capable in this situation, because they need to judge of the informative data close to class boundary, in order to synthetically generate new samples for minority class.", "startOffset": 40, "endOffset": 48}, {"referenceID": 22, "context": "Therefore, classic oversampling methods [19, 23] is not capable in this situation, because they need to judge of the informative data close to class boundary, in order to synthetically generate new samples for minority class.", "startOffset": 40, "endOffset": 48}, {"referenceID": 11, "context": "5 Experiments There are many accuracy measures for evaluating the two-class classification problems, such as precision, recall, geometric-mean (G-mean) and F-measure [12].", "startOffset": 166, "endOffset": 170}, {"referenceID": 8, "context": "99 in LGC [9] and \u03bc = 99 in GTAM [14] during the experiments and the results are the average results of 50 runs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "99 in LGC [9] and \u03bc = 99 in GTAM [14] during the experiments and the results are the average results of 50 runs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "proposed in CMN method in [10] to solve the negative influence to the classification result caused by imbalanced labeled dataset.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "[8] set in MNIST and set the stop parameter s = 0 in INNO.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "And as well taking into account of the randomness of labeled samples selected from raw data by algorithm, we cannot foresee the occurrence of such a situation [17], namely the imbalanced output may also occur even the stop condition s is 0, so the classification accuracy ranges in a certain scope.", "startOffset": 159, "endOffset": 163}], "year": 2013, "abstractText": "Transductive graph-based semi-supervised learning methods usually build an undirected graph utilizing both labeled and unlabeled samples as vertices. Those methods propagate label information of labeled samples to neighbors through their edges in order to get the predicted labels of unlabeled samples. Most popular semi-supervised learning approaches are sensitive to initial label distribution happened in imbalanced labeled datasets. The class boundary will be severely skewed by the majority classes in an imbalanced classification. In this paper, we proposed a simple and effective approach to alleviate the unfavorable influence of imbalance problem by iteratively selecting a few unlabeled samples and adding them into the minority classes to form a balanced labeled dataset for the learning methods afterwards. The experiments on UCI datasets and MNIST handwritten digits dataset showed that the proposed approach outperforms other existing state-of-art methods.", "creator": null}}}