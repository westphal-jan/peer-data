{"id": "1401.1247", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2014", "title": "Tractability through Exchangeability: A New Perspective on Efficient Probabilistic Inference", "abstract": "exchangeability is a functional notion in general and probability theory. the assumption that saving infinite billions of data points is valid is at the core for bayesian statistics. however, finite exchangeability as useful statistical property that renders probabilistic inference tractable is less un - understood. we realize a theory of finite exchangeability and its relation to convex probabilistic inference. the theory is complementary to that called finance promoting conditional independence. we show that tractable inference in probabilistic models with high coefficients and millions of variables can be understood using the notion of finite ( partial ) exchangeability. we also show that existing lifted inference algorithms implicitly utilize a combination of conditional independence and partial exchangeability.", "histories": [["v1", "Tue, 7 Jan 2014 00:30:25 GMT  (27kb)", "https://arxiv.org/abs/1401.1247v1", null], ["v2", "Tue, 22 Apr 2014 22:21:16 GMT  (48kb,D)", "http://arxiv.org/abs/1401.1247v2", "In Proceedings of the 28th AAAI Conference on Artificial Intelligence"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mathias niepert", "guy van den broeck"], "accepted": true, "id": "1401.1247"}, "pdf": {"name": "1401.1247.pdf", "metadata": {"source": "META", "title": "Tractability through Exchangeability: A New Perspective on Efficient Probabilistic Inference", "authors": ["Mathias Niepert", "Guy Van den Broeck"], "emails": ["mniepert@cs.washington.edu", "guyvdb@cs.ucla.edu"], "sections": [{"heading": "1 Introduction", "text": "Probabilistic graphical models such as Bayesian and Markov networks explicitly represent conditional independencies of a probability distribution with their structure (Pearl 1988; Lauritzen 1996; Koller and Friedman 2009; Darwiche 2009). Their wide-spread use in research and industry can largely be attributed to this structural property and their declarative nature, separating representation and inference algorithms. Conditional independencies often lead to a more concise representation and facilitate efficient algorithms for parameter estimation and probabilistic inference. It is wellknown, for instance, that probabilistic graphical models with a tree structure admit efficient inference. In addition to conditional independencies, modern inference algorithms exploit contextual independencies (Boutilier et al. 1996) to speed up probabilistic inference.\nThe time complexity of classical probabilistic inference algorithms is exponential in the treewidth (Robertson and Seymour 1986) of the graphical model. Independence and its various manifestations often reduce treewidth and treewidth has been used in the literature as the decisive factor for assessing the tractability of probabilistic inference (cf. Koller and Friedman (2009), Darwiche (2009)). However, recent algorithmic developments have shown that inference in probabilistic graphical models can be highly tractable, even\n\u2217Both authors contributed equally to the paper. Guy Van den Broeck is also at KU Leuven, Belgium.\nin high-treewidth models without any conditional independencies. For instance, lifted probabilistic inference algorithms (Poole 2003; Kersting 2012) often perform efficient inference in densely connected graphical models with millions of random variables. With the success of lifted inference, understanding these algorithms and their tractability on a more fundamental level has become a central challenge. The most pressing question concerns the underlying statistical principle that allows inference to be tractable in the absence of independence.\nThe present paper contributes to a deeper understanding of the statistical properties that render inference tractable. We consider an inference problem tractable when it is solved by an efficient algorithm, running in time polynomial in the number of random variables. The crucial contribution is a comprehensive theory that relates the notion of finite partial exchangeability (Diaconis and Freedman 1980a) to tractability. One instance is full exchangeability where the distribution is invariant under variable permutations. We develop a theory of exchangeable decompositions that results in novel tractability conditions. Similar to conditional independence, partial exchangeability decomposes a probabilistic model so as to facilitate efficient inference. Most importantly, the notions of conditional independence and partial exchangeability are complementary, and when combined, define a much larger class of tractable models than the class of models rendered tractable by conditional independence alone.\nConditional and contextual independence are such powerful concepts because they are statistical properties of the distribution, regardless of the representation used. Partial exchangeability is such a statistical property that is independent of any representation, be it a joint probability table, a Bayesian network, or a statistical relational model. We introduce novel forms of exchangeability, discuss their sufficient statistics, and efficient inference algorithms. The resulting exchangeability framework allows us to state known liftability results as corollaries, providing a first statistical characterization of exact lifted inference. As an additional contribution, we connect the semantic notion of exchangeability to syntactic notions of tractability by showing that liftable statistical relational models have the required exchangeability properties due to their syntactic symmetries. We thereby unify notions of lifting from the exact and approximate inference community into a common framework. ar X\niv :1\n40 1.\n12 47\nv2 [\ncs .A\nI] 2\n2 A\npr 2\n01 4"}, {"heading": "2 A Case Study: Markov Logic", "text": "The analysis of exchangeability and tractable inference is developed in the context of arbitrary discrete probability distributions, independent of a particular representational formalism. Nevertheless, for the sake of accessibility, we will provide examples and intuitions for Markov logic, a wellknown statistical relational language that exhibits several forms of exchangeability. Hence, after the derivation of the theoretical results in each section, we apply the theory to the problem of inference in Markov logic networks. This also allows us to link the theory to existing results from the lifted probabilistic inference literature."}, {"heading": "Markov Logic Networks", "text": "We first introduce some standard concepts from functionfree first-order logic. An atom P (t1, . . . , tn) consists of a predicate P/n of arity n followed by n argument terms ti, which are either constants, {A,B, . . . } or logical variables {x, y, . . . }. A unary atom has one argument and a binary atom has two. A formula combines atoms with connectives (e.g., \u2227, \u21d4). A formula is ground if it contains no logical variables. The groundings of a formula are obtained by instantiating the variables with particular constants.\nMany statistical relational languages have been proposed in recent years (Getoor and Taskar 2007; De Raedt et al. 2008). We will work with one such language, called Markov logic networks (MLN) (Richardson and Domingos 2006). An MLN is a set of tuples (w, f), where w is a real number representing a weight and f is a formula in first-order logic. Let us, for example, consider the following MLN\n1.3 Smokes(x)\u21d2 Cancer(x) (1) 1.5 Smokes(x) \u2227 Friends(x, y)\u21d2 Smokes(y) (2)\nwhich states that smokers are more likely to (1) get cancer and (2) be friends with other smokers.\nGiven a domain of constants D, a first-order MLN \u2206 induces a grounding, which is the MLN obtained by replacing each formula in \u2206 with all its groundings (using the same weight). Take for example the domain D = {A,B} (e.g., two people, Alice and Bob), the above first-order MLN represents the following grounding.\n1.3 Smokes(A)\u21d2 Cancer(A) 1.3 Smokes(B)\u21d2 Cancer(B) 1.5 Smokes(A) \u2227 Friends(A,A)\u21d2 Smokes(A) 1.5 Smokes(A) \u2227 Friends(A,B)\u21d2 Smokes(B) 1.5 Smokes(B) \u2227 Friends(B,A)\u21d2 Smokes(A) 1.5 Smokes(B) \u2227 Friends(B,B)\u21d2 Smokes(B)\nThis ground MLN contains eight different random variables, which correspond to all groundings of atoms Smokes(x), Cancer(x) and Friends(x, y). This leads to a distribution over 28 possible worlds. The weight of each world is the product of the expressions exp(w), where (w, f) is a ground MLN formula and f is satisfied by the world. The probabilities of worlds are obtained by normalizing their weights. Without loss of generality (Jha et al. 2010), we assume that first-order formulas contain no constants."}, {"heading": "Lifted Probabilistic Inference", "text": "The advent of statistical relational languages such as Markov logic has motivated a new class of lifted inference algorithms (Poole 2003). These algorithms exploit the high-level structure and symmetries of the first-order logic formulas to speed up inference (Kersting 2012). Surprisingly, they perform tractable inference even in the absence of conditional independencies. For example, when interpreting the above MLN as an (undirected) probabilistic graphical model, all pairs of random variables in {Smokes(A), Smokes(B), . . . } are connected by an edge due to the groundings of Formula 2. The model has no conditional or contextual independencies between the Smokes variables. Nevertheless, lifted inference algorithms exactly compute its single marginal probabilities in time linear in the size of the corresponding graphical model (Van den Broeck et al. 2011), scaling up to millions of random variables.\nAs lifted inference research makes algorithmic progress, the quest for the source of tractability and its theoretical properties becomes increasingly important. For exact lifted inference, most theoretical results are based on the notion of domain-lifted inference (Van den Broeck 2011).\nDefinition 1 (Domain-lifted). Domain-lifted inference algorithms run in time polynomial in |D|. Note that domain-lifted algorithms can be exponential in other parameters, such as the number of formulas and predicates. Our current understanding of exact lifted inference is that syntactic properties of MLN formulas permit domainlifted inference (Van den Broeck 2011; Jaeger and Van den Broeck 2012; Taghipour et al. 2013). We will review these results where relevant. Moreover, the (fractional) automorphisms of the graphical model representation have been related to lifted inference (Niepert 2012b; Bui, Huynh, and Riedel 2012; Noessner, Niepert, and Stuckenschmidt 2013; Mladenov and Kersting 2013). While there are deep connections between automorphisms and exchangeability (Niepert 2012b; 2013; Bui, Huynh, and Riedel 2012; Bui, Huynh, and de Salvo Braz 2012), we refer these to future work."}, {"heading": "3 Finite Exchangeability", "text": "This section provides some background on the concept of finite partial exchangeability. We proceed by showing that particular forms of finite exchangeability permit tractable inference. For the sake of simplicity and to provide links to statistical relational models such as MLNs, we present\nthe theory for finite sets of (upper-case) binary random variables X = {X1, . . . , Xn}. However, the theory applies to all distributions over finite valued discrete random variables. Lower-case x denote an assignments to X.\nWe begin with the most basic form of exchangeability. Definition 2 (Full Exchangeability). A set of variables X = {X1, ..., Xn} is fully exchangeable if and only if Pr(X1 = x1, . . . , Xn = xn) = Pr(X1 = x\u03c0(1), . . . , Xn = x\u03c0(n)) for all permutations \u03c0 of {1, . . . , n}. Full exchangeability is best understood in the context of a finite sequence of binary random variables such as a number of coin tosses. Here, exchangeability means that it is only the number of heads that matters and not their particular order. Figure 1 depicts an undirected graphical model with 9 finitely exchangeable dependent Bernoulli variables."}, {"heading": "Finite Partial Exchangeability", "text": "The assumption that all variables of a probabilistic model are exchangeable is often too strong. Fortunately, exchangeability can be generalized to the concept of partial exchangeability using the notion of a sufficient statistic (Diaconis and Freedman 1980b; Lauritzen et al. 1984; Lauritzen 1988). Particular instances of exchangeability such as full finite exchangeability correspond to particular statistics. Definition 3 (Partial Exchangeability). Let Di be the domain of Xi, and let T be a finite set. A set of random variables X is partially exchangeable with respect to the statistic T : D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Dn \u2192 T if and only if\nT (x) = T (x\u2032) implies Pr(x) = Pr(x\u2032).\nThe following theorem states that the joint distribution of a set of random variables that is partially exchangeable with a statistic T is a unique mixture of uniform distributions. Theorem 1 (Diaconis and Freedman (1980a)). Let T be a finite set and let T : {0, 1}n \u2192 T be a statistic of a partially exchangeable set X. Moreover, let St = {x \u2208 D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Dn | T (x) = t}, letUt be the uniform distribution over St, and let Pr(St) = Pr(T (x) = t). Then,\nPr(X) = \u2211 t\u2208T Pr(St)Ut(X).\nHence, a distribution that is partially exchangeable with respect to a statistic T can be parameterized as a unique mixture of uniform distributions. We will see that several instances of partial exchangeability render probabilistic inference tractable. Indeed, the major theme of the present paper can be summarized as finding methods for constructing the above representation and exploiting it for tractable probabilistic inference for a given probabilistic model.\nLet [[\u00b7]] be the indicator function. The uniform distribution of each equivalence class St is Ut(X) = [[T (X) = t]]/|St|; and the probability of St is Pr(St) = Pr(x)|St| for every x \u2208 St. Hence, every value of the statistic T corresponds to one equivalence class St of joint assignments with identical probability. We will refer to these equivalence classes as orbits. We write x \u223c e when assignments x and e agree on the values of their shared variables (Darwiche 2009).\nThe suborbit St,e \u2286 St for some evidence state e is the set of those states in St that are compatible with e, that is, St,e = {x | T (x) = t and x \u223c e}."}, {"heading": "Partial Exchangeability and Probabilistic Inference", "text": "We are now in the position to relate finite partial exchangeability to tractable probabilistic inference, using notions from Theorem 1. The inference tasks we consider are \u2013 MPE inference, i.e., finding arg maxy Pr(y, e) for any\ngiven assignment e to variables E \u2286 X, and \u2013 marginal inference, i.e., computing Pr(e) for any given e. For a set of variables X, we say that P (x) can be computed efficiently iff it can be computed in time polynomial in |X|. We make the following complexity claims Theorem 2. Let X be partially exchangeable with statistic T . If we can efficiently \u2013 for all x, evaluate Pr(x), and \u2013 for all e and t \u2208 T decide whether there exists an x \u2208 St,e, and if so, construct it,\nthen the complexity of MPE inference is polynomial in |T |. If we can additionally compute |St,e| efficiently, then the complexity of marginal inference is also polynomial in |T |.\nProof. For MPE inference, we construct an xt \u2208 St,e for each t \u2208 T , and return the one maximizing Pr(xt). For marginal inference, we return \u2211 t\u2208T Pr(xt)|St,e|.\nIf the above conditions for tractable inference are fulfilled we say that a distribution is tractably partially exchangeable for MPE or marginal inference. We will present notions of exchangeability and related statistics T that make distributions tractably partially exchangeable. Please note that Theorem 2 generalizes to situations in which we can only efficiently compute Pr(x) up to a constant factor Z, as is often the case in undirected probabilistic graphical models."}, {"heading": "Markov Logic Case Study", "text": "Exchangeability and independence are not mutually exclusive. Independent and identically distributed (iid) random variables are also exchangeable. Take for example the MLN\n1.5 Smokes(x)\nThe random variables Smokes(A), Smokes(B), . . . are independent. Hence, we can compute their marginal probabilities independently as\nPr(Smokes(A)) = Pr(Smokes(B)) = exp(1.5)\nexp(1.5) + 1\nThe variables are also finitely exchangeable. For example, the probability that A smokes and B does not is equal to the probability that B smokes and A does not. The sufficient statistic T (x) counts how many people smoke in the state x and the probability of a state in which n out of N people smoke is exp(1.5n)/(exp(1.5) + 1)N .\nExchangeability can occur without independence, as in the following Markov logic network\n1.5 Smokes(x) \u2227 Smokes(y)\nThis distribution has neither independencies nor conditional independencies. However, its variables are finitely exchangeable and the probability of a state x is only a function of the sufficient statistic T (x) counting the number of smokers in x. The probability of a state now increases by a factor of exp(1.5) with every pair of smokers. When n people smoke there are n2 pairs and, hence, Pr(x) = exp ( 1.5n2 ) /Z, where Z is a normalization constant. Let\nY consist of all Smokes(x) variables except for Smokes(A), and let y be an assignment to Y in which m people smoke. The probability that A smokes given y is\nPr (Smokes(A) | y) = exp\n( 1.5(m+ 1)2 ) exp (1.5(m+ 1)2) + exp (1.5m2) ,\nwhich clearly depends on the number of smokers in y. Hence, Smokes(A) is not independent of Y but the random variables are exchangeable with sufficient statistic n. Figure 1 depicts the graphical representation of the corresponding ground Markov logic network."}, {"heading": "4 Exchangeable Decompositions", "text": "We now present novel instances of partial exchangeability that render probabilistic inference tractable. These instances generalize exchangeability of single variables to exchangeability of sets of variables. We describe the notion of an exchangeable decomposition and prove that it fulfills the tractability requirements of Theorem 2. We proceed by demonstrating that these forms commonly occur in MLNs."}, {"heading": "Variable Decompositions", "text": "The notions of independent and exchangeable decompositions are at the core of the developed theoretical results.\nDefinition 4 (Variable Decomposition). A variable decomposition X = {X1, . . . ,Xk} partitions X into subsets Xi. We call w = maxi |Xi| the width of the decomposition. Definition 5 (Independent Decomposition). A variable decomposition is independent if and only if Pr factorizes as\nPr (X) = k\u220f i=1 Qi(Xi).\nDefinition 6 (Exchangeable Decomposition). A variable decomposition is exchangeable iff for all permutations \u03c0,\nPr (X1 = x1, . . . ,Xk = xk) = Pr ( X1 = x\u03c0(1), . . . ,Xk = x\u03c0(k) ) .\nFigure 2 depicts an example distribution Pr with 20 random variables and a decomposition into 5 subsets of width 4. The joint distribution is invariant under permutations of the 5 sequences. The corresponding sufficient statistic T counts the number of occurrences of each binary vector of length 4 and returns a tuple of counts. Please note that the definition of full finite exchangeability (Definition 2) is the special case when the exchangeable decomposition has width 1. Also note that the size of all subsets in an exchangeable decomposition equal the width.\nTractable Variable Decompositions The core observation of the present work is that variable decompositions that are exchangeable and/or independent result in tractable probabilistic models. For independent decompositions, the following tractability guarantee is used in most existing inference algorithms. Proposition 3. Given an independent decomposition of X with bounded width, and a corresponding factorized representation of the distribution (cf. Definition 5), the complexity of MPE and marginal inference is polynomial in |X|.\nWhile the decomposition into independent components is a well-understood concept, the combination with finite exchangeability has not been previously investigated as a statistical property that facilitates tractable probabilistic inference. We can now prove the following result. Theorem 4. Suppose we can compute Pr(x) in time polynomial in |X|. Then, given an exchangeable decomposition of X with bounded width, the complexity of MPE and marginal inference is polynomial in |X|.\nProof. Following Theorem 2, we have to show that there exists a statistic T so that (a) |T | is polynomial in |X|; (b) we can efficiently decide whether an x \u2208 St,e exists and if so, construct it; and (c) efficiently compute |St,e| for all e and t \u2208 T . Statements (b) and (c) ensure that the assumptions of Theorem 2 hold for exchangeable decompositions, which combined with (a) proves the theorem.\nTo prove (a), let us first construct a sufficient statistic for exchangeable decompositions. A full joint assignment X = x decomposes into assignments X1 = x1, . . . ,Xk = xk in accordance with the given variable decomposition. Each x` is a bit string b \u2208 {0, 1}w. Consider a statistic T (x) = (c1, . . . , c2w), where each ci has a corresponding unique bit string bi \u2208 {0, 1}w and ci = \u2211k `=1[[x` = bi]]. The value ci of the statistic thus represents the number of components in the decomposition that are assigned bit string bi. Hence, we have \u22112w i=1 ci = k, and we prove (a) by observing that\n|T | = ( k + 2w \u2212 1\n2w \u2212 1\n) \u2264 k2 w\u22121 \u2264 n2 w\u22121.\nTo prove statements (b) and (c) we have to find, for each partial assignment E = e, an algorithm that generates an\nx \u2208 St,e and that computes |St,e| in time polynomial in |X|. To hint at the proof strategy, we give the formula for the orbit without evidence |St|:\n\u2223\u2223S(c1,...,c2w )\u2223\u2223 = ( kc1 )( k \u2212 c1 c2 ) . . . ( k \u2212 \u22112w\u22122 i=1 ci c2w\u22121 ) .\nThe proof is very technical and deferred to the appendix."}, {"heading": "Markov Logic Case Study", "text": "Let us consider the following MLN\n1.3 Smokes(x)\u21d2 Cancer(x) 1.5 Smokes(x) \u2227 Smokes(y)\nIt models a distribution in which every non-smoker or smoker with cancer, that is, every x satisfying the first formula, increases the probability by a factor of exp(1.3). Each pair of smokers increases the probability by a factor of exp(1.5). This model is not fully exchangeable: swapping Smokes(A) and Cancer(A) in a state yields a different probability. There are no (conditional) independencies between the Smokes(x) atoms.\nThe variables in this MLN do have an exchangeable decomposition whose width is two, namely\nX = {{Smokes(A), Cancer(A)}, {Smokes(B), Cancer(B)}, {Smokes(C), Cancer(C)}, . . . }.\nThe sufficient statistic of this decomposition counts the number of people in each of four groups, depending on whether they smoke, and whether they have cancer. The probability of a state only depends on the number of people in each group and swapping people between groups does not change the probability of a state. For example,\nPr(Smokes(A) = 0, Cancer(A) = 1,\nSmokes(B) = 1, Cancer(B) = 0)\n= Pr(Smokes(A) = 1, Cancer(A) = 0,\nSmokes(B) = 0, Cancer(B) = 1).\nTheorem 4 says that this MLN permits tractable inference. The fact that this MLN has an exchangeable decomposition is not a coincidence. In general, we can show this for MLNs of unary atoms, which are called monadic MLNs.\nTheorem 5. The variables in a monadic MLN have an exchangeable decomposition. The width of this decomposition is equal to the number of predicates.\nThe proof builds on syntactic symmetries of the MLN, called renaming automorphisms (Bui, Huynh, and Riedel 2012; Niepert 2012a). Please see the appendix for further details.\nIt now follows as a corollary of Theorems 4 and 5 that MPE and marginal inference in monadic MLNs is polynomial in |X|, and therefore also in the domain size |D|. Corollary 6. Inference in monadic MLNs is domain-lifted."}, {"heading": "5 Marginal and Conditional Exchangeability", "text": "Many distributions are not decomposable into independent or exchangeable decompositions. Similar to conditional independence, the notion of exchangeability can be extended to conditional exchangeability. We generalize exchangeability to conditional distributions, and state the corresponding tractability guarantees."}, {"heading": "Marginal and Conditional Decomposition", "text": "Tractability results for exchangeable decompositions on all variables under consideration also extend to subsets.\nDefinition 7 (Marginal Exchangeability). When a subset Y of the variables under consideration has an exchangeable decomposition Y , we say that Y is marginally exchangeable. This means that Y is still an exchangeable decomposition when considering the distribution Pr(Y) = \u2211 X\\Y Pr(X).\nTheorem 7. Suppose we are given a marginally exchangeable decomposition of Y with bounded width and let Z = X \\ Y. If computing Pr(y) = \u2211 z Pr(y, z) is polynomial in |X| for all y, then the complexity of MPE and marginal inference over variables Y is polynomial in |X|.\nProof. Let T be the statistic associated with the given decomposition, and let e be evidence given on E \u2286 Y. Then,\nP (e) = \u2211 t\u2208T |St,e| \u00b7 \u2211 z Pr(yt,e, z), where yt,e \u2208 St,e.\nBy the assumption that Y is marginally exchangeable and the proof of Theorem 4, we can compute |St,e| and an yt,e \u2208 St,e in time polynomial in |X|. An analogous argument holds for MPE inference on Pr(Y).\nWe now need to identify distributions Pr(Y,Z) for which we can compute Pr(y) efficiently. This implies tractable probabilistic inference over Y. Given a particular y, we have already seen sufficient conditions: when Z decomposes exchangeably or independently conditioned on y, Proposition 3 and Theorem 4 guarantee that computing Pr(y) is tractable. This suggests the following general notion.\nDefinition 8 (Conditional Decomposability). Let X be a set of variables with Y \u2286 X and Z = X \\ Y. We say that Z is exchangeably (independently) decomposable given Y if and only if for each assignment y to Y, there exists an exchangeable (independent) decomposition Zy of Z. Furthermore, we say that Z is decomposable with bounded width iff the width w of each Zy is bounded. When the decomposition can be computed in time polynomial in |X| for all y, we say that Z is efficiently decomposable.\nTheorem 8. Let X be a set of variables with Y \u2286 X and Z = X \\Y. Suppose we are given a marginally exchangeable decomposition of Y with bounded width. Suppose further that Z is efficiently (exchangeably or independently) decomposable given Y with bounded width. If we can compute Pr(x) efficiently, then the complexity of MPE and marginal inference over variables Y is polynomial in |X|.\nProof. Following Theorem 7, we only need to show that we can compute Pr(y) = \u2211 z Pr(y, z) in time polynomial in |X| for all y. When Z is exchangeably decomposable given Y, this follows from constructing Zy and employing the arguments made in the proof of Theorem 4. The case when Z is independently decomposable is analogous.\nTheorems 7 and 8 are powerful results and allow us to identify numerous probabilistic models for which inference is tractable. For instance, we will prove liftability results for Markov logic networks. However, we are only at the beginning of leveraging these tractability results to their fullest extent. Especially Theorem 7 is widely applicable because the computation of \u2211 z Pr(y, z) can be tractable for many reasons. For instance, conditioned on the variables Y, the distributions Pr(y,Z) could be bounded treewidth graphical models, such as tree-structured Markov networks. Tractability for Pr(Y) follows immediately from Theorem 7.\nSince Theorem 7 only speaks to the tractability of querying variables in Y, there is the question of when we can also efficiently query the variables Z. Results from the lifted inference literature may provide a solution by bounding or approximating queries and evidence that includes Z to maintain marginal exchangeability (Van den Broeck and Darwiche 2013). The next section shows that certain restricted situations permit tractable inference on the variables in Z."}, {"heading": "Markov Logic Case Study", "text": "Let us again consider the MLN\n1.3 Smokes(x)\u21d2 Cancer(x) 1.5 Smokes(x) \u2227 Friends(x, y)\u21d2 Smokes(y)\nhaving the marginally exchangeable decomposition\nY = {{Smokes(A)}, {Smokes(B)}, {Smokes(C)}, . . . }\nwhose width is one. To intuitively see why this decomposition is marginally exchangeable, let us consider two states y and y\u2032 of the Smokes(x) atoms in which only the values of two atoms, for example Smokes(A) and Smokes(B), are swapped. There is a symmetry of the MLNs joint distribution that swaps these atoms: the renaming automorphism (Bui, Huynh, and Riedel 2012; Niepert 2012a) that swaps constants A and B in all atoms. For marginal exchangeability, we need that \u2211 z Pr(y, z) = \u2211 z Pr(y\n\u2032, z). But this holds since the renaming automorphism is an automorphism of the set of states {yz | z \u2208 DZ} \u2013 for every y, y\u2032, and z there exists an automorphism that maps yz to y\u2032z\u2032 with Pr(y, z) = Pr(y\u2032, z\u2032).\nThe given MLN has several marginally exchangeable decompositions, with the most general one being\nY = {{Smokes(A), Cancer(A), Friends(A,A)}, {Smokes(B), Cancer(B), Friends(B,B)}, {Smokes(C), Cancer(C), Friends(C,C)}, . . . }.\nFor that decomposition, the remaining Z variables\n{Friends(A,B), Friends(B,A), Friends(A,C), . . . }\nare independently decomposable given Y. The Z variables appear at most once in any formula. In a probabilistic graphical model representation, evidence on the Y variables would therefore decompose the graph into independent components. Thus, it follows from Theorem 8 that we can efficiently answer any query over the variables in Y.\nThis insight generalizes to a large class of MLNs, called the two-variable fragment. It consists of all MLNs whose formulas contain at most two logical variables.\nTheorem 9. In a two-variable fragment MLN, let Y and Z be the ground atoms with one and two distinct arguments respectively. Then there exists a marginally exchangeable decomposition of Y, and Z is efficiently independently decomposable given Y. Each decomposition\u2019s width is at most twice the number of predicates.\nThe proof of Theorem 9 is rather technical and we refer the reader to the appendix for a detailed proof. It now follows from Theorems 8 and 9 that the complexity of inference over the unary atoms in the two-variable fragment is polynomial in the domain size |D|.\nWhat happens if our query involves variables from Z \u2013 the binary atoms? It is known in the lifted inference literature that we cannot expect efficient inference of general queries that involve the binary atoms. Assignments to the Z variables break symmetries and therefore break marginal exchangeability. This causes inference to become #P-hard as a function of the query (Van den Broeck and Davis 2012). Nevertheless, if we bound the number of binary atoms involved in the query, we can use the developed theory to show a general liftability result.\nTheorem 10. For any MLN in the two-variable fragment, MPE and marginal inference over the unary atoms and a bounded number of binary atoms is domain-lifted.\nThis corresponds to one of the strongest known theoretical results in the lifted inference literature (Jaeger and Van den Broeck 2012). We refer the interested reader to the appendix for the proof. A consequence of Theorem 10 is that we can efficiently compute all single marginals in the two-variable fragment, given arbitrary evidence on the unary atoms."}, {"heading": "6 Discussion and Conclusion", "text": "We conjecture that the concept of (partial) exchangeability has potential to contribute to a deeper understanding of tractable probabilistic models. The important role conditional independence plays in the research field of graphical models is evidence for this hypothesis. Similar to conditional independence, it might be possible to develop a theory of exchangeability that mirrors that of independence. For instance, there might be a (graphical) structural representations of particular types of partial exchangeability and corresponding logical axiomatizations (Pearl 1988). Moreover, it would be interesting to develop graphical models with exchangeability and independence, and notions like d-separation to detect marginal exchangeability and conditional decomposability from a structural representation. The first author has taken steps in this direction by introducing exchangeable variable models, a class of (non-relational)\nprobabilistic models based on finite partial exchangeability (Niepert and Domingos 2014).\nRecently, there has been considerable interest in computing and exploiting the automorphisms of graphical models (Niepert 2012b; Bui, Huynh, and Riedel 2012). There are several interesting connections between automorphisms, exchangeability, and lifted inference (Niepert 2012a). Moreover, there are several group theoretical algorithms that one could apply to the automorphism groups to discover the structure of exchangeable variable decompositions from the structure of the graphical models. Since we presently only exploit renaming automorphisms, there is a potential for tractable inference in MLNs that goes beyond what is known in the lifted inference literature.\nPartial exchangeability is related to collective graphical models (Sheldon and Dietterich 2011) (CGMs) and cardinality-based potentials (Gupta, Diwan, and Sarawagi 2007) as these models also operate on sufficient statistics. However, probabilistic inference for CGMs is not tractable and there are no theoretical results that identify tractable CGMs models. The presented work may help to identify such situations. The presented theory generalizes the statistics of cardinality-based potentials.\nLifted Inference and Exchangeability Our case studies identified a deep connection between lifted probabilistic inference and the concepts of partial, marginal and conditional exchangeability. In this new context, it appears that exact lifted inference algorithms (de Salvo Braz, Amir, and Roth 2005; Milch et al. 2008; Jha et al. 2010; Van den Broeck et al. 2011; Gogate and Domingos 2011; Taghipour et al. 2012) can all be understood as performing essentially three steps: (i) construct a sufficient statistic T (x), (ii) generate all possible values of the sufficient statistic, and (iii) count suborbit sizes for a given statistic. For an example of (i), we can show that a compiled firstorder circuit (Van den Broeck et al. 2011) or the trace of probabilistic theorem proving (Gogate and Domingos 2011) encode a sufficient statistic in their existential quantifier and splitting nodes. Steps (ii) and (iii) are manifested in all these algorithms through summations and binomial coefficients.\nBetween Corollary 6 and Theorem 10, we have re-proven almost the entire range of liftability results from the lifted inference literature (Jaeger and Van den Broeck 2012) within the exchangeability framework, and extended these to MPE inference. There is an essential difference though: liftability results make assumptions about the syntax (e.g., MLNs), whereas our exchangeability theorems apply to all distributions. We expect Theorem 8 to be used to show liftability, and more general tractability results for many other representation languages, including but not limited to the large number of statistical relational languages that have been proposed (Getoor and Taskar 2007; De Raedt et al. 2008)."}, {"heading": "Acknowledgments", "text": "This work was partially supported by ONR grants #N0001412-1-0423, #N00014-13-1-0720, and #N00014-12-1-0312; NSF grants #IIS-1118122 and #IIS-0916161; ARO grant #W911NF-08-1-0242; AFRL contract #FA8750-13-2-0019;\nthe Research Foundation-Flanders (FWO-Vlaanderen); and a Google research award to MN."}, {"heading": "A Continued Proof of Theorem 4", "text": "Proof. To prove statements (b) and (c), we need to represent partial assignments E = e with E \u2286 X. The partial assignments decompose into partial assignments E1 = e1, . . . ,Ek = ek in accordance with X . Each e` corresponds to a string m \u2208 {0, 1, \u2217}w where characters 0 and 1 encode assignments to variables in E` and \u2217 encodes an unassigned variable in X` \u2212E`. In this case, we say that e` is of type m. Please note that there are 2w distinct b and 3w distinctm. We say that x agrees with e, denoted by x \u223c e, if and only if their shared variables have identical assignments.\nA completion c of e to x is a bijection c : {e1, ..., ek} \u2192 {x1, ...,xk} such that c(ei) = xj implies ei \u223c xj . Every completion corresponds to a unique way to assign elements in {0, 1} to unassigned variables so as to turn the partial assignment e into the full assignment x.\nLet t = (c1, ..., c2w) \u2208 T , let E \u2286 X, and let e \u2208 DE. Moreover, let dj = \u2211k `=1[[e` = mj ]] for each mj \u2208 {0, 1, \u2217}w. Consider the set of matrices\nAt,e = { A \u2208M(2w, 3w) \u2223\u2223\u2223 \u2211 i ai,j = dj , \u2211 j ai,j = ci\nand ai,j = 0 if bi 6\u223c mj } .\nEvery A \u2208 At,e represents a set of completions from e to x for which T (x) = t. The value ai,j indicates that each completion represented by A maps ai,j elements in {e1, ..., ek} of type mj to ai,j elements in {x1, ...,xk} of type bi. We write \u03b3(A) for the set of completions represented by A.\nWe have to prove the following statements\n1. For every A \u2208 At,e and every c \u2208 \u03b3(A) there exists an x \u2208 DX with T (x) = t and c is a completion of e to x; 2. For every x \u2208 DX with T (x) = t and every completion c of e to x there exists an A \u2208 At,e such that c \u2208 \u03b3(A); 3. For all A,A\u2032 \u2208 At,e with A 6= A\u2032 we have that \u03b3(A) \u2229 \u03b3(A\u2032) = \u2205;\n4. For every A \u2208 At,e, we can efficiently compute |\u03b3(A)|, the size of the set of completions represented by A.\nTo prove statement (1), let A \u2208 At,e and c \u2208 \u03b3(A). By the definition of At,e, c maps ai,j elements in {e1, ..., ek} of type mj to ai,j elements in {x1, ...,xk} of type bi. By the conditions \u2211 i ai,j = dj and \u2211 j ai,j = ci of the definition of At,e we have that c is a bijection. By the condition ai,j = 0 if bi 6\u223c mj of the definition of At,e we have that c(ei) = xj implies ei \u223c xj and, therefore, e \u223c x. Hence, c is a completion. Moreover, c completes e to an x with\u2211k `=1[[x` = bi]] = \u2211 j ai,j = ci by the definition of At,e. Hence, T (x) = t. To prove statement (2), let x \u2208 DX with T (x) = t and let c be a completion of e to x. We construct an A with c \u2208 \u03b3(A) as follows. Since c is a completion we have that c(ei) = xj implies ei \u223c xj and, hence, we set ai,j = 0 if bi 6\u223c mj . For all other entries in A we set ai,j = |{ej |\nc(ej) = xi}|. Since c is surjective, we have that \u2211 j ai,j =\nci and since c is injective, we have that \u2211 i ai,j = dj . Hence, A \u2208 At,e. To prove statement (3), let A,A\u2032 \u2208 At,e with A 6= A\u2032. Since A 6= A\u2032 we have that there exist i, j such that, without loss of generality, ai,j < a\u2032i,j . Hence, every c \u2208 A maps fewer elements of type mj to elements of type bi than every c\u2032 \u2208 A\u2032. Hence, c 6= c\u2032 for every c \u2208 A and every c\u2032 \u2208 A\u2032.\nTo prove statement (4), let A \u2208 At,e. Every c \u2208 A maps ai,j elements in {e1, ..., ek} of type mj to ai,j elements in {x1, ...,xk} of type bi. Hence, the size of the set of completions represented by A is, for each 1 \u2264 j \u2264 3w, the number of different ways to place ai,j balls of color i, 1 \u2264 i \u2264 2w, into dj urns. Hence,\n|\u03b3(A)| = 3w\u220f j=1 2w\u220f i=1 ( dj \u2212 \u2211i\u22121 q=1 ai,q ai,j ) .\nFrom the statements (1)-(4) we can conclude that |St,e| = \u2211\nA\u2208At,e\n|\u03b3(A)|.\nThis allows us to prove (b) and (c). We can constructAt,e in time polynomial in n as follows. There are 2w3w entries in a 2w \u00d7 3w matrix and each entry has at most k different values. Hence, we can enumerate all k6\nw \u2264 n6w possible matrices A \u2208 M(2w, 3w). We simply select those A for which the conditions in the definition of At,e hold. For one A \u2208 At,e we can efficiently construct one c and the x that it completes e to. This proves (b). Finally, we compute |St,e|. This proves (c)."}, {"heading": "B Proof of Theorem 5", "text": "Proof. Let P1, ..., PN be the N unary predicates of a given MLN and let D = {1, ..., k} be the domain. After grounding, there are k ground atoms per predicate. We write Pj(i) to denote the ground atom that resulted from instantiating predicate Pj with domain element i. Let X = {X1, ...,Xk} be a decomposition of the set of ground atoms with Xi = {P1(i), P2(i), ..., PN (i)} for every 1 \u2264 i \u2264 k. A renaming automorphism (Bui, Huynh, and Riedel 2012; Niepert 2012a) is a permutation of the ground atoms that results from a permutation of the domain elements. The joint distribution over all ground atoms remains invariant under these permutations. Consider the permutation of ground atoms that results from swapping two domain elements i \u2194 i\u2032. This permutation acting on the set of ground atoms permutes the components Xi and Xi\u2032 and leaves all other components invariant. Since this is possible for each pair i, i\u2032 \u2208 {1, ..., k} it follows that the decomposition X is exchangeable."}, {"heading": "C Proof of Theorem 9", "text": "Proof. Let P1, ..., PM be the M unary predicates and let Q1, ..., QN be the N binary predicates of a given MLN and let D = {1, ..., k} be the domain. After grounding, there are k ground atoms per unary and k2 ground atoms per binary predicate. We write P`(i) to denote the ground atom\nthat resulted from instantiating unary predicate P` with domain element i and Q`(i, j) to denote the ground atom that resulted from instantiating binary predicate Q` with domain elements i and j.\nLet X be the set of all ground atoms, let Y = {P`(i) | 1 \u2264 i \u2264 k, 1 \u2264 ` \u2264 M} \u222a {Q`(i, i) | 1 \u2264 i \u2264 k, 1 \u2264 ` \u2264 N} and let Z = X \u2212Y. Moreover, let Y = {Y1, ...,Yk} with Yi = {P`(i) | 1 \u2264 ` \u2264 M} \u222a {Q`(i, i) | 1 \u2264 ` \u2264 N}. We can make the same arguments as in the proof of Theorem 5 to show that Y is exchangeable.\nNow, we prove that the variables Z are independently decomposable given Y. Let Zi,j = {Q1(i, j), ..., QN (i, j), Q1(j, i), ..., QN (j, i)} for all 1 \u2264 i < j \u2264 k. Now, let f be any ground formula and let G be the set of ground atoms occurring in both f and Z. Then, either G \u2286 Zi,j or G \u2229 Zi,j = \u2205 since every formula in the MLN has at most two variables. Hence, {Zi,j | 1 \u2264 i < j \u2264 k} is a decomposition of Z with ( k 2\n) components, width 2N , and Pr(Z,y) factorizes as\nPr(Z,y) = \u220f i,j i<j Qi,j(Zi,j ,y).\nBy the properties of MLNs, we have that the Qi,j(Zi,j ,y) are computable in time exponential in the width of the decomposition but polynomial in k."}, {"heading": "D Proof of Theorem 10", "text": "Proof. Suppose that the query e contains a bounded number of binary atoms whose arguments are constants from the setK. Consider the set of variables Q consisting of all unary atoms whose argument comes from K, and all binary atoms whose arguments both come from K. The unary atoms in Q are no long marginally exchangeable, because their arguments can appear asymmetrically in e. We can now answer the query by simply enumerating all states q of Q and performing inference in each Pr(Y,Z,q) separately, were all variables Y have again become marginally exchangeable, and all variables Z have become independently decomposable given Y. The construction of Y and Z is similar to the proof of Theorem 9, except that some additional binary atoms are now in Y instead of Z. These atoms have one argument in K, and one not in K, and are treated as unary. When we bound the number of binary atoms in the query, the size of Q will not be a function of the domain size, and enumerating over all states q is domain-lifted."}], "references": [{"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "Proceedings of the 12th Conference on Uncertainty in artificial intelligence (UAI), 115\u2013123.", "citeRegEx": "Boutilier et al\\.,? 1996", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Exact lifted inference with distinct soft evidence on every object", "author": ["H. Bui", "T. Huynh", "R. de Salvo Braz"], "venue": "In Proceedings of the 26th Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Bui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2012}, {"title": "Automorphism groups of graphical models and lifted variational inference", "author": ["H. Bui", "T. Huynh", "S. Riedel"], "venue": null, "citeRegEx": "Bui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2012}, {"title": "Modeling and reasoning with Bayesian networks", "author": ["A. Darwiche"], "venue": "Cambridge University Press.", "citeRegEx": "Darwiche,? 2009", "shortCiteRegEx": "Darwiche", "year": 2009}, {"title": "Probabilistic inductive logic programming: theory and applications", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "Muggleton", "eds"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Lifted firstorder probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Braz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Braz et al\\.", "year": 2005}, {"title": "De Finetti\u2019s generalizations of exchangeability", "author": ["P. Diaconis", "D. Freedman"], "venue": "Studies in Inductive Logic and Probability, volume II.", "citeRegEx": "Diaconis and Freedman,? 1980a", "shortCiteRegEx": "Diaconis and Freedman", "year": 1980}, {"title": "Finite exchangeable sequences", "author": ["P. Diaconis", "D. Freedman"], "venue": "The Annals of Probability 8(4):745\u2013764.", "citeRegEx": "Diaconis and Freedman,? 1980b", "shortCiteRegEx": "Diaconis and Freedman", "year": 1980}, {"title": "Introduction to Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar"], "venue": "The MIT Press.", "citeRegEx": "Getoor and Taskar,? 2007", "shortCiteRegEx": "Getoor and Taskar", "year": 2007}, {"title": "Probabilistic theorem proving", "author": ["V. Gogate", "P. Domingos"], "venue": "Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI), 256\u2013265.", "citeRegEx": "Gogate and Domingos,? 2011", "shortCiteRegEx": "Gogate and Domingos", "year": 2011}, {"title": "Efficient inference with cardinality-based clique potentials", "author": ["R. Gupta", "A.A. Diwan", "S. Sarawagi"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML), 329\u2013336.", "citeRegEx": "Gupta et al\\.,? 2007", "shortCiteRegEx": "Gupta et al\\.", "year": 2007}, {"title": "Liftability of probabilistic inference: Upper and lower bounds", "author": ["M. Jaeger", "G. Van den Broeck"], "venue": "Proceedings of the 2nd International Workshop on Statistical Relational AI.", "citeRegEx": "Jaeger and Broeck,? 2012", "shortCiteRegEx": "Jaeger and Broeck", "year": 2012}, {"title": "Lifted inference seen from the other side: The tractable features", "author": ["A. Jha", "V. Gogate", "A. Meliou", "D. Suciu"], "venue": "Proceedings of the 24th Conference on Neural Information Processing Systems (NIPS).", "citeRegEx": "Jha et al\\.,? 2010", "shortCiteRegEx": "Jha et al\\.", "year": 2010}, {"title": "Lifted probabilistic inference", "author": ["K. Kersting"], "venue": "Proceedings of European Conference on Artificial Intelligence (ECAI).", "citeRegEx": "Kersting,? 2012", "shortCiteRegEx": "Kersting", "year": 2012}, {"title": "Probabilistic Graphical Models", "author": ["D. Koller", "N. Friedman"], "venue": "The MIT Press.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Extreme point models in statistics", "author": ["S.L. Lauritzen", "O.E. Barndorff-Nielsen", "A.P. Dawid", "P. Diaconis", "S. Johansen"], "venue": "Scandinavian Journal of Statistics 11(2).", "citeRegEx": "Lauritzen et al\\.,? 1984", "shortCiteRegEx": "Lauritzen et al\\.", "year": 1984}, {"title": "Extremal families and systems of sufficient statistics", "author": ["S.L. Lauritzen"], "venue": "Lecture notes in statistics. Springer-Verlag.", "citeRegEx": "Lauritzen,? 1988", "shortCiteRegEx": "Lauritzen", "year": 1988}, {"title": "Graphical Models", "author": ["S.L. Lauritzen"], "venue": "Oxford University Press.", "citeRegEx": "Lauritzen,? 1996", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Lifted probabilistic inference with counting formulas", "author": ["B. Milch", "L. Zettlemoyer", "K. Kersting", "M. Haimes", "L. Kaelbling"], "venue": "Proceedings of the 23rd AAAI Conference on Artificial Intelligence 1062\u20131068.", "citeRegEx": "Milch et al\\.,? 2008", "shortCiteRegEx": "Milch et al\\.", "year": 2008}, {"title": "Lifted inference via k-locality", "author": ["M. Mladenov", "K. Kersting"], "venue": "Proceedings of the 3rd International Workshop on Statistical Relational AI.", "citeRegEx": "Mladenov and Kersting,? 2013", "shortCiteRegEx": "Mladenov and Kersting", "year": 2013}, {"title": "Exchangeable variable models", "author": ["M. Niepert", "P. Domingos"], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Niepert and Domingos,? 2014", "shortCiteRegEx": "Niepert and Domingos", "year": 2014}, {"title": "Lifted probabilistic inference: An MCMC perspective", "author": ["M. Niepert"], "venue": "Proceedings of StaRAI.", "citeRegEx": "Niepert,? 2012a", "shortCiteRegEx": "Niepert", "year": 2012}, {"title": "Markov chains on orbits of permutation groups", "author": ["M. Niepert"], "venue": "Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI).", "citeRegEx": "Niepert,? 2012b", "shortCiteRegEx": "Niepert", "year": 2012}, {"title": "Symmetry-aware marginal density estimation", "author": ["M. Niepert"], "venue": "Proceedings of the 27th Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Niepert,? 2013", "shortCiteRegEx": "Niepert", "year": 2013}, {"title": "RockIt: Exploiting Parallelism and Symmetry for MAP Inference in Statistical Relational Models", "author": ["J. Noessner", "M. Niepert", "H. Stuckenschmidt"], "venue": "Proceedings of the 27th Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Noessner et al\\.,? 2013", "shortCiteRegEx": "Noessner et al\\.", "year": 2013}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 985\u2013991.", "citeRegEx": "Poole,? 2003", "shortCiteRegEx": "Poole", "year": 2003}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning 62(1-2):107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Graph minors", "author": ["N. Robertson", "P. Seymour"], "venue": "II. Algorithmic aspects of tree-width. Journal of algorithms 7(3):309\u2013322.", "citeRegEx": "Robertson and Seymour,? 1986", "shortCiteRegEx": "Robertson and Seymour", "year": 1986}, {"title": "Collective graphical models", "author": ["D. Sheldon", "T. Dietterich"], "venue": "Advances in Neural Information Processing Systems (NIPS). 1161\u20131169.", "citeRegEx": "Sheldon and Dietterich,? 2011", "shortCiteRegEx": "Sheldon and Dietterich", "year": 2011}, {"title": "Lifted variable elimination with arbitrary constraints", "author": ["N. Taghipour", "D. Fierens", "J. Davis", "H. Blockeel"], "venue": "Proceedings of the fifteenth international conference on Artificial Intelligence and Statistics, volume 22, 1194\u20131202.", "citeRegEx": "Taghipour et al\\.,? 2012", "shortCiteRegEx": "Taghipour et al\\.", "year": 2012}, {"title": "Completeness results for lifted variable elimination", "author": ["N. Taghipour", "D. Fierens", "G. Van den Broeck", "J. Davis", "H. Blockeel"], "venue": "Proceedings of the 16th Conference on Artificial Intelligence and Statistics, 572\u2013580.", "citeRegEx": "Taghipour et al\\.,? 2013", "shortCiteRegEx": "Taghipour et al\\.", "year": 2013}, {"title": "On the complexity and approximation of binary evidence in lifted inference", "author": ["G. Van den Broeck", "A. Darwiche"], "venue": "Advances in Neural Information Processing Systems 26 (NIPS).", "citeRegEx": "Broeck and Darwiche,? 2013", "shortCiteRegEx": "Broeck and Darwiche", "year": 2013}, {"title": "Conditioning in first-order knowledge compilation and lifted probabilistic inference", "author": ["G. Van den Broeck", "J. Davis"], "venue": "Proceedings of the 26th Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Broeck and Davis,? 2012", "shortCiteRegEx": "Broeck and Davis", "year": 2012}, {"title": "Lifted probabilistic inference by firstorder knowledge compilation", "author": ["G. Van den Broeck", "N. Taghipour", "W. Meert", "J. Davis", "L. De Raedt"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI), 2178\u20132185.", "citeRegEx": "Broeck et al\\.,? 2011", "shortCiteRegEx": "Broeck et al\\.", "year": 2011}, {"title": "On the completeness of firstorder knowledge compilation for lifted probabilistic inference", "author": ["G. Van den Broeck"], "venue": "Advances in Neural Information Processing Systems 24 (NIPS),, 1386\u20131394.", "citeRegEx": "Broeck,? 2011", "shortCiteRegEx": "Broeck", "year": 2011}], "referenceMentions": [{"referenceID": 25, "context": "Probabilistic graphical models such as Bayesian and Markov networks explicitly represent conditional independencies of a probability distribution with their structure (Pearl 1988; Lauritzen 1996; Koller and Friedman 2009; Darwiche 2009).", "startOffset": 167, "endOffset": 236}, {"referenceID": 17, "context": "Probabilistic graphical models such as Bayesian and Markov networks explicitly represent conditional independencies of a probability distribution with their structure (Pearl 1988; Lauritzen 1996; Koller and Friedman 2009; Darwiche 2009).", "startOffset": 167, "endOffset": 236}, {"referenceID": 14, "context": "Probabilistic graphical models such as Bayesian and Markov networks explicitly represent conditional independencies of a probability distribution with their structure (Pearl 1988; Lauritzen 1996; Koller and Friedman 2009; Darwiche 2009).", "startOffset": 167, "endOffset": 236}, {"referenceID": 3, "context": "Probabilistic graphical models such as Bayesian and Markov networks explicitly represent conditional independencies of a probability distribution with their structure (Pearl 1988; Lauritzen 1996; Koller and Friedman 2009; Darwiche 2009).", "startOffset": 167, "endOffset": 236}, {"referenceID": 0, "context": "In addition to conditional independencies, modern inference algorithms exploit contextual independencies (Boutilier et al. 1996) to speed up probabilistic inference.", "startOffset": 105, "endOffset": 128}, {"referenceID": 28, "context": "The time complexity of classical probabilistic inference algorithms is exponential in the treewidth (Robertson and Seymour 1986) of the graphical model.", "startOffset": 100, "endOffset": 128}, {"referenceID": 0, "context": "In addition to conditional independencies, modern inference algorithms exploit contextual independencies (Boutilier et al. 1996) to speed up probabilistic inference. The time complexity of classical probabilistic inference algorithms is exponential in the treewidth (Robertson and Seymour 1986) of the graphical model. Independence and its various manifestations often reduce treewidth and treewidth has been used in the literature as the decisive factor for assessing the tractability of probabilistic inference (cf. Koller and Friedman (2009), Darwiche (2009)).", "startOffset": 106, "endOffset": 545}, {"referenceID": 0, "context": "In addition to conditional independencies, modern inference algorithms exploit contextual independencies (Boutilier et al. 1996) to speed up probabilistic inference. The time complexity of classical probabilistic inference algorithms is exponential in the treewidth (Robertson and Seymour 1986) of the graphical model. Independence and its various manifestations often reduce treewidth and treewidth has been used in the literature as the decisive factor for assessing the tractability of probabilistic inference (cf. Koller and Friedman (2009), Darwiche (2009)).", "startOffset": 106, "endOffset": 562}, {"referenceID": 26, "context": "For instance, lifted probabilistic inference algorithms (Poole 2003; Kersting 2012) often perform efficient inference in densely connected graphical models with millions of random variables.", "startOffset": 56, "endOffset": 83}, {"referenceID": 13, "context": "For instance, lifted probabilistic inference algorithms (Poole 2003; Kersting 2012) often perform efficient inference in densely connected graphical models with millions of random variables.", "startOffset": 56, "endOffset": 83}, {"referenceID": 6, "context": "The crucial contribution is a comprehensive theory that relates the notion of finite partial exchangeability (Diaconis and Freedman 1980a) to tractability.", "startOffset": 109, "endOffset": 138}, {"referenceID": 8, "context": "Many statistical relational languages have been proposed in recent years (Getoor and Taskar 2007; De Raedt et al. 2008).", "startOffset": 73, "endOffset": 119}, {"referenceID": 27, "context": "We will work with one such language, called Markov logic networks (MLN) (Richardson and Domingos 2006).", "startOffset": 72, "endOffset": 102}, {"referenceID": 12, "context": "Without loss of generality (Jha et al. 2010), we assume that first-order formulas contain no constants.", "startOffset": 27, "endOffset": 44}, {"referenceID": 26, "context": "The advent of statistical relational languages such as Markov logic has motivated a new class of lifted inference algorithms (Poole 2003).", "startOffset": 125, "endOffset": 137}, {"referenceID": 13, "context": "These algorithms exploit the high-level structure and symmetries of the first-order logic formulas to speed up inference (Kersting 2012).", "startOffset": 121, "endOffset": 136}, {"referenceID": 31, "context": "Our current understanding of exact lifted inference is that syntactic properties of MLN formulas permit domainlifted inference (Van den Broeck 2011; Jaeger and Van den Broeck 2012; Taghipour et al. 2013).", "startOffset": 127, "endOffset": 203}, {"referenceID": 22, "context": "Moreover, the (fractional) automorphisms of the graphical model representation have been related to lifted inference (Niepert 2012b; Bui, Huynh, and Riedel 2012; Noessner, Niepert, and Stuckenschmidt 2013; Mladenov and Kersting 2013).", "startOffset": 117, "endOffset": 233}, {"referenceID": 19, "context": "Moreover, the (fractional) automorphisms of the graphical model representation have been related to lifted inference (Niepert 2012b; Bui, Huynh, and Riedel 2012; Noessner, Niepert, and Stuckenschmidt 2013; Mladenov and Kersting 2013).", "startOffset": 117, "endOffset": 233}, {"referenceID": 22, "context": "While there are deep connections between automorphisms and exchangeability (Niepert 2012b; 2013; Bui, Huynh, and Riedel 2012; Bui, Huynh, and de Salvo Braz 2012), we refer these to future work.", "startOffset": 75, "endOffset": 161}, {"referenceID": 7, "context": "Fortunately, exchangeability can be generalized to the concept of partial exchangeability using the notion of a sufficient statistic (Diaconis and Freedman 1980b; Lauritzen et al. 1984; Lauritzen 1988).", "startOffset": 133, "endOffset": 201}, {"referenceID": 15, "context": "Fortunately, exchangeability can be generalized to the concept of partial exchangeability using the notion of a sufficient statistic (Diaconis and Freedman 1980b; Lauritzen et al. 1984; Lauritzen 1988).", "startOffset": 133, "endOffset": 201}, {"referenceID": 16, "context": "Fortunately, exchangeability can be generalized to the concept of partial exchangeability using the notion of a sufficient statistic (Diaconis and Freedman 1980b; Lauritzen et al. 1984; Lauritzen 1988).", "startOffset": 133, "endOffset": 201}, {"referenceID": 6, "context": "Theorem 1 (Diaconis and Freedman (1980a)).", "startOffset": 11, "endOffset": 41}, {"referenceID": 3, "context": "We write x \u223c e when assignments x and e agree on the values of their shared variables (Darwiche 2009).", "startOffset": 86, "endOffset": 101}, {"referenceID": 21, "context": "The proof builds on syntactic symmetries of the MLN, called renaming automorphisms (Bui, Huynh, and Riedel 2012; Niepert 2012a).", "startOffset": 83, "endOffset": 127}, {"referenceID": 21, "context": "There is a symmetry of the MLNs joint distribution that swaps these atoms: the renaming automorphism (Bui, Huynh, and Riedel 2012; Niepert 2012a) that swaps constants A and B in all atoms.", "startOffset": 101, "endOffset": 145}, {"referenceID": 25, "context": "For instance, there might be a (graphical) structural representations of particular types of partial exchangeability and corresponding logical axiomatizations (Pearl 1988).", "startOffset": 159, "endOffset": 171}, {"referenceID": 20, "context": "probabilistic models based on finite partial exchangeability (Niepert and Domingos 2014).", "startOffset": 61, "endOffset": 88}, {"referenceID": 22, "context": "Recently, there has been considerable interest in computing and exploiting the automorphisms of graphical models (Niepert 2012b; Bui, Huynh, and Riedel 2012).", "startOffset": 113, "endOffset": 157}, {"referenceID": 21, "context": "There are several interesting connections between automorphisms, exchangeability, and lifted inference (Niepert 2012a).", "startOffset": 103, "endOffset": 118}, {"referenceID": 29, "context": "Partial exchangeability is related to collective graphical models (Sheldon and Dietterich 2011) (CGMs) and cardinality-based potentials (Gupta, Diwan, and Sarawagi 2007) as these models also operate on sufficient statistics.", "startOffset": 66, "endOffset": 95}, {"referenceID": 18, "context": "In this new context, it appears that exact lifted inference algorithms (de Salvo Braz, Amir, and Roth 2005; Milch et al. 2008; Jha et al. 2010; Van den Broeck et al. 2011; Gogate and Domingos 2011; Taghipour et al. 2012) can all be understood as performing essentially three steps: (i) construct a sufficient statistic T (x), (ii) generate all possible values of the sufficient statistic, and (iii) count suborbit sizes for a given statistic.", "startOffset": 71, "endOffset": 220}, {"referenceID": 12, "context": "In this new context, it appears that exact lifted inference algorithms (de Salvo Braz, Amir, and Roth 2005; Milch et al. 2008; Jha et al. 2010; Van den Broeck et al. 2011; Gogate and Domingos 2011; Taghipour et al. 2012) can all be understood as performing essentially three steps: (i) construct a sufficient statistic T (x), (ii) generate all possible values of the sufficient statistic, and (iii) count suborbit sizes for a given statistic.", "startOffset": 71, "endOffset": 220}, {"referenceID": 9, "context": "In this new context, it appears that exact lifted inference algorithms (de Salvo Braz, Amir, and Roth 2005; Milch et al. 2008; Jha et al. 2010; Van den Broeck et al. 2011; Gogate and Domingos 2011; Taghipour et al. 2012) can all be understood as performing essentially three steps: (i) construct a sufficient statistic T (x), (ii) generate all possible values of the sufficient statistic, and (iii) count suborbit sizes for a given statistic.", "startOffset": 71, "endOffset": 220}, {"referenceID": 30, "context": "In this new context, it appears that exact lifted inference algorithms (de Salvo Braz, Amir, and Roth 2005; Milch et al. 2008; Jha et al. 2010; Van den Broeck et al. 2011; Gogate and Domingos 2011; Taghipour et al. 2012) can all be understood as performing essentially three steps: (i) construct a sufficient statistic T (x), (ii) generate all possible values of the sufficient statistic, and (iii) count suborbit sizes for a given statistic.", "startOffset": 71, "endOffset": 220}, {"referenceID": 9, "context": "2011) or the trace of probabilistic theorem proving (Gogate and Domingos 2011) encode a sufficient statistic in their existential quantifier and splitting nodes.", "startOffset": 52, "endOffset": 78}, {"referenceID": 8, "context": "We expect Theorem 8 to be used to show liftability, and more general tractability results for many other representation languages, including but not limited to the large number of statistical relational languages that have been proposed (Getoor and Taskar 2007; De Raedt et al. 2008).", "startOffset": 237, "endOffset": 283}, {"referenceID": 21, "context": "A renaming automorphism (Bui, Huynh, and Riedel 2012; Niepert 2012a) is a permutation of the ground atoms that results from a permutation of the domain elements.", "startOffset": 24, "endOffset": 68}], "year": 2014, "abstractText": "Exchangeability is a central notion in statistics and probability theory. The assumption that an infinite sequence of data points is exchangeable is at the core of Bayesian statistics. However, finite exchangeability as a statistical property that renders probabilistic inference tractable is less well-understood. We develop a theory of finite exchangeability and its relation to tractable probabilistic inference. The theory is complementary to that of independence and conditional independence. We show that tractable inference in probabilistic models with high treewidth and millions of variables can be explained with the notion of finite (partial) exchangeability. We also show that existing lifted inference algorithms implicitly utilize a combination of conditional independence and partial exchangeability.", "creator": "TeX"}}}