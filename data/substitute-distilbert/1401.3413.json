{"id": "1401.3413", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Infinite Mixed Membership Matrix Factorization", "abstract": "rating and recommendation systems have become a popular application area for applying a suite of distributed learning software. current models rely closely on probabilistic interpretations and extensions of matrix factorization, which factorizes a user - item ratings matrix into latent user and external vectors. most of these methods fail to model significant variations in item ratings from otherwise healthy users, a phenomenon locally as the \" lazy dynamite \" effect. recent efforts have addressed this problem by adding a contextual bias term to the rating, which captures the mood under which a user rates an item or the context in where an icon is rated by a user. in later work, we extend this model in a nonparametric sense by learning the optimal number of moods or contexts from the data, and develop gibbs sampling inference procedures for our model. we evaluate our approach on the movielens 1m dataset, and show significant improvements over the optimal parametric baseline, another than twice the deviation previously encountered for this task. we alternately extract and evaluate candidate dblp dataset, wherein we predict the number judgments papers associate - authored by multiple judges, and demonstrate improvements over relevant parametric baseline on said alternative domain as well.", "histories": [["v1", "Wed, 15 Jan 2014 02:39:15 GMT  (217kb,D)", "http://arxiv.org/abs/1401.3413v1", "For ICDM 2013 Workshop Proceedings"]], "COMMENTS": "For ICDM 2013 Workshop Proceedings", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["avneesh saluja", "mahdi pakdaman", "dongzhen piao", "ankur p parikh"], "accepted": false, "id": "1401.3413"}, "pdf": {"name": "1401.3413.pdf", "metadata": {"source": "CRF", "title": "Infinite Mixed Membership Matrix Factorization", "authors": ["Avneesh Saluja", "Mahdi Pakdaman", "Dongzhen Piao", "Ankur P. Parikh"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014recommender systems; graphical models;\nI. INTRODUCTION\nGenerating recommendations for users based on their previously revealed preferences and the ratings of similar users has increased in popularity as a machine learning problem, primarily due to the activities of internet companies such as Amazon, or the coverage of competitions such as the Netflix prize. Most of these methods have at their core a matrix factorization-based approach. We can view the users and their ratings for items as a U\u00d7M matrix R (subsequently referred to as the ratings matrix or the user-item matrix), which contains the ratings for U users over M items, and where every item is rated by at least one user. The idea is to factorize R into its latent factors: R \u2248 ATB, where A \u2208 RD\u00d7U is the latent user matrix, and B \u2208 RD\u00d7M is the latent item matrix. These lower dimensional representations reflect the intuition that ratings are based on a small number of factors (with dimension D), and the idea is to complete the matrix using this low-dimensional formulation, by taking the inner product between user and item latent factors for items not rated by users during the prediction or \u201cmatrix completion\u201d step.\nThis general approach is also known as collaborative filtering, since one not only utilizes the user\u2019s previous preferences but also the preferences of similar users (hence the \u201ccollabora-\ntive\u201d aspect). Probabilistic formulations of matrix factorization for recommendation systems generally improve upon the performance of their non-probabilistic counterparts, by treating the latent factors as random variables. However, such methods are unable to model context explicitly, since most probabilistic models of this form generate the ratings from a normal distribution with the mean set to the inner product between the user and item latent factors. As a result, phenomena like the Napoleon Dynamite effect, originally proposed in the context of the Netflix prize, wherein certain movies (or more generally, items) receive high variance ratings from users that otherwise rate other items similarly (the movie Napoleon Dynamite being a prime example), cannot be handled well in the traditional probabilistic matrix factorization framework.\nThe recently proposed mixed membership matrix factorization (M3F) model [1] aims to tackle this problem by explicitly modeling ratings not only with static latent factors, but with an additional bias term that takes into account the context of the rating. M3F represents this bias term by modeling it through a type of mixed membership stochastic blockmodel: each user and each item can be represented as a discrete distribution over user and item topics, representing contexts or moods under which users rate items. When a user u rates an item j, the contextual bias of this rating is a function of the user\u2019s bias for the item\u2019s topic for that particular rating (intuitively, the context under which item j is being rated). It is also a function of the item\u2019s bias for the user\u2019s topic for that rating (intuitively, the mood of the user u at the time of rating). Thus, M3F models a user\u2019s rating for an item as a function of both the static latent factors and the user-item group and user group-item based contextual biases.\nThe M3F model however, is limited in the sense that the number of user and item topics for the contextual bias term is set beforehand and not learned from the data. In our proposed extension to this model, which we refer to as the infinite mixed membership matrix factorization model (iM3F), we sample the user and item topic assignments from an infinite-dimensional prior, allowing us to learn the optimal number of user and item topics from the data. Ideally, we would like the user and item topics to be shared across all users and items, but that each user and item maintain their own topic proportions over these topics. Thus, we propose to use a hierarchical Dirichlet process prior on the number of item and user topics, and\nar X\niv :1\n40 1.\n34 13\nv1 [\ncs .L\nG ]\n1 5\nJa n\n20 14\nsample assignments through a Chinese restaurant franchise representation. A Gibbs sampling procedure is derived and implemented for this model, and we evaluate its performance on two datasets from different domains: a movie ratings dataset (MovieLens), and a publication co-authorship database that we extracted ourselves from the DBLP database. Compared to the baseline M3F model on the former dataset, we decrease root mean square error (RMSE) by 0.0065, which is two times higher than M3F\u2019s improvement over its baseline. We also significantly outperform M3F on the DBLP dataset."}, {"heading": "II. RELATED WORK", "text": "As a way to attack recommender system problems, matrix factorization has been one of the popular approaches along with content-based filtering [2], and in recent years has proven to be the superior alternative of the two [3]. Probabilistic matrix factorization (PMF) was first introduced by Salakhutdinov and Mnih [4], whereby ratings are generated from a Gaussian with a mean computed as the dot product of the user and item latent factors, with fixed variance. A fully Bayesian version of PMF (BPMF) was subsequently proposed by the same authors [5], where they assumed the latent factors were themselves generated by Gaussian distributions, with normalWishart priors on the hyperparameters.\nOur work is primarily based on extending the M3F model [1] . In M3F, the user and item latent factors are modeled as in BPMF, but when generating a rating, in addition to using the dot product of the latent factors as the mean, we add a contextual bias term. More formally, we generate a rating by a user u for an item j as1:\nruj \u223c N (\u03c70 + cku + dij + au \u00b7 bj , \u03c32) (1)\nwhere cku is the latent rating bias of user u under item topic k, dij denotes the bias for item j under user topic i, \u03c70 is a fixed global bias, and au and bj are the latent user and item factors respectively. The M3F framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].\nCollaborative topic regression [10] also combines matrix factorization and mixed membership topic modeling, but their method solves a different problem, the cold start issue of outof-matrix predictions, rather than contextual bias.\nThere has been a good amount of work on general nonparametric techniques in matrix factorization and in particular, applications in collaborative filtering. However, all of these approaches focus on the parameters corresponding to the number of latent factors, and aim to learn the optimal latent dimension from the data. [11] do not use a probabilistic approach and instead use SVD, whereas [12], [13] adopt an Indian Buffett Process to model the latent factor dimensionality, and\n1note that this is one of two variants proposed in the original paper, the Topic-Indexed Bias (TIB) model. The other model, the Topic-Indexed Factor (TIF) model, was found to underperform the TIB model.\nimplement variational inference-based solutions. We note that such non-parametric approaches are orthogonal to our method, since they target the latent dimensionality, and can thus be easily integrated and combined with our approach."}, {"heading": "III. APPROACH", "text": "Our proposed method, iM3F, is based on extending the M3F approach for probabilistic modeling of recommendation systems. In iM3F, we add an infinite-dimensional prior for user and item topics. This extension is realized in our model by using a nonparametric Bayesian prior for the topic distributions, providing the model the ability to have a potentially infinite number of user and item topics. We present the schematic and symbolic views of our model, and derive a Gibbs sampling inference scheme using the Chinese restaurant representation of the nonparametric prior."}, {"heading": "A. Chinese Restaurants", "text": "The nonparametric prior on the mixtures over topics for users and items is motivated by its ability to allow the data to dictate the optimal number of user and item groups, instead of parameterizing these values beforehand. Using such a prior makes it unnecessary to conduct extensive \u201ctuning\u201d experiments for the number of topics, and is arguably conceptually simpler and more elegant. We provide a brief background of the ideas, with an emphasis on the intuition. For a more rigorous introduction, we refer the reader to [14]. For the purposes of exposition, we discuss the case of users being defined as mixtures over user topics (i.e., a user is defined as a mixture of various \u201cmoods\u201d), and note that the item case is completely symmetrical.\nIn the finite case, we can represent this mixture as a multinomial distribution over the various user topics. The conjugate prior for a multinomial distribution is the Dirichlet distribution, sometimes seen as a \u201cdistribution over distributions\u201d or a distribution over finitely many points on the real line. If we take the number of mixture components to infinity, then the conjugate prior becomes a Dirichlet process (DP) [15], a distribution over an infinitely fine-grained real line. In\npractice, we do not have a density defined for the infinitedimensional DP prior, so we define it indirectly via a sampling scheme that is used to generate samples from this prior [14]. If we are interested in both the weights and the partitions, then we use a stick-breaking construction to generate samples. Otherwise, if we are simply interested in partition or cluster assignments for the data, we can marginalize the mixture weights and sample the assignments directly, through a process known as a Chinese restaurant process (CRP) [16]. In our situation, for every rating ruj , we can simply sample from the CRP prior to ascertain which item and user topics were used to generate the contextual bias for the rating.\nA naive way to incorporate the CRP prior is to assume that each user and item maintain their own priors over item and user topics2. Ideally though, we would like each user or item to maintain their own mixture weights over topics, but have a global set of topics that are shared across all users and items. This requirement necessitates a hierarchical Dirichlet process (HDP) prior [17] on the mixture components. In the Chinese restaurant metaphor, we now use a Chinese restaurant franchise (CRF) prior. Intuitively, this is a two-stage Dirichlet process, where we have one process for a global set of \u201cdishes\u201d, or topics that are shared by all users (or all items), and another user-specific process for the table assignments for individual customers (or ratings). The base distribution for each user-specific CRP is the process corresponding to the shared set of dishes, which forces each user (or item) to maintain a distribution over the shared set of topics."}, {"heading": "B. Generative Process", "text": "We present the generative process of the iM3F model, depicted in Figure 2. For the purposes of comparison, the schematic view of the finite, parametric case is presented in Figure 1, which corresponds to the original M3F model. First, the hyperparameters for the latent user and item factors are\n2Such a \u201cseparate\u201d CRP model was implemented and evaluated, but the results were worse than a parametric solution.\ngenerated:\n\u039bU \u223cWishart(W0, v0), \u039bM \u223cWishart(W0, v0) \u00b5U \u223c N (\u00b50, (\u03bb0\u039bU )\u22121), \u00b5M \u223c N (\u00b50, (\u03bb0\u039bM )\u22121)\nWe also generate the global probability measures that are used to define a set of shared clusters for both the user and item topics:\nG0 \u223c DP(\u03b2,H0), G1 \u223c DP(\u03b2,H1)\nNote that we assume the DPs associated with the item and user global dishes have the same concentration parameter \u03b2, for the purposes of simplicity, and that the base measures are discrete and uniform. Now that we have the hyperparameters, we can sample the latent factor au for each user and the latent factor bj for each item by sampling from a normal distribution. Furthermore, since we have the global base measures, we can sample the user- and item-specific mixture distributions from DPs with discrete base measures G0 and G1:\nFor each u \u2208 {1, . . . , U} : au \u223c N (\u00b5U , (\u039bU )\u22121) Gu \u223c DP(\u03b3,G0)\nFor each j \u2208 {1, . . . ,M} : bj \u223c N (\u00b5M , (\u039bM )\u22121) Gj \u223c DP(\u03b3,G1)\nOnce again, we assume the DPs associated with the item and user-specific mixtures have the same concentration parameter \u03b3. With a slight abuse of notation, we can generate the user group and item group topic assignment indicator variables from CRPs with appropriate base distributions:\nFor each rating ruj :\nzUuj \u223c CRP(\u03b3,Gu) zMuj \u223c CRP(\u03b3,Gj)\nwhere zUuj is the user cluster or topic that user u belongs to when rating item j (user u\u2019s mood at the time of rating item j). Similarly, zMuj is the item cluster or topic that item j, when rated by user u, belongs to. Note that in practice the topic assignments are sampled by marginalizing out the base distributions Gu and Gj , and the only relevant parameter is \u03b3, but for notational clarity we include the base measure. Letting k = zMuj and i = z U uj , we can generate c k u, the bias of user u for item topic k, and similarly generate dij , the bias of a specific item j for the user topic i as follows:\ndij \u223c N (\u00b5i, \u03c320) cku \u223c N (\u00b5k, \u03c320)\nFinally, we have all the pieces in place to generate the rating:\nruj \u223c N (\u03b2ikuj + au \u00b7 bj , \u03c32)\nwhere \u03b2ikuj = \u03c70 + c k u + d i j .\nC. Inference via Gibbs Sampling\nOur sampling procedure is similar to the M3F sampling scheme, except for the cluster assignment indicators zU and zM , as they are generated from a CRF (in iM3F) instead of a multinomial with a Dirichlet prior.\nWe first sample the hyperparameters given the other variables in the model:\n\u039bU |rest\\{\u00b5U} \u223cWishart((W\u221210 + U\u2211\nu=1\n(au \u2212 a\u0304)(au \u2212 a\u0304)T )\n+ \u03bb0U\n\u03bb0 + U (\u00b50 \u2212 a\u0304)(\u00b50 \u2212 a\u0304)T )\u22121, \u03bd0 + U)\n(2)\n\u039bM |rest\\{\u00b5M} \u223cWishart((W\u221210 + M\u2211 j=1 (bj \u2212 b\u0304)(bj \u2212 b\u0304)T )\n+ \u03bb0M\n\u03bb0 +M (\u00b50 \u2212 b\u0304)(\u00b50 \u2212 b\u0304)T )\u22121, \u03bd0 +M)\n(3)\nwhere in Equation 2, a\u0304 = 1U \u2211U\nu=1 au and in Equation 3, b\u0304 = 1U \u2211M j=1 bj . We then sample parameters \u00b5 U and \u00b5M :\n\u00b5U |rest \u223c N\n( \u03bb0\u00b50 + \u2211U u=1 au\n\u03bb0 + U , (\u039bU (\u03bb0 + U))\n\u22121\n)\n\u00b5M |rest \u223c N\n( \u03bb0\u00b50 + \u2211M j=1 bj\n\u03bb0 +M , (\u039bM (\u03bb0 +M))\n\u22121 ) Then, we sample the item bias for a given user u and for i \u2208 {1, . . . ,KM}:\nciu|rest \u223cN  c0 \u03c320 + \u2211 j\u2208Vu \u03c3 \u22122zMuji ( ruj \u2212 \u03c70 \u2212 d zUuj j \u2212 au \u00b7 bj ) \u03c3\u221220 + \u2211 j\u2208Vu \u03c3 \u22122zMuji ,\n1 \u03c3\u221220 + \u2211 j\u2208Vu \u03c3 \u22122zMuji\n) (4)\nwhere Vu is the set of items rated by user u, and zMuji is an selector variable which is 1 if the item topic assignment for user u and item j is i. Note that KM is conceptually infinite, but in practice we go through the item clusters that exist in the data thus far and generate ciu. Due to the symmetry of the user and item biases, the user bias for an item j, with i \u2208 {1, . . . ,KU} this time (KU is also conceptually infinite), is:\ndij |rest \u223cN  d0 \u03c320 + \u2211 u:j\u2208Vu \u03c3 \u22122zUuji ( ruj \u2212 \u03c70 \u2212 c zMuj u \u2212 au \u00b7 bj ) \u03c3\u221220 + \u2211 u:j\u2208Vu \u03c3 \u22122zUuji ,\n1 \u03c3\u221220 + \u2211 u:j\u2208Vu \u03c3 \u22122zUuji\n) (5)\nFor notational convenience, let us define the empirical means and variances of the values sampled through Equations 4 and 5 as (\u00b5c , \u03c3c) and (\u00b5d , \u03c3d) respectively.\nUsing the conjugacy property of the Gaussian and Wishart distributions, we can sample the latent factors. We update au\nand bj as follows:\nFor each u, au|rest \u223cN ( (\u039bU\u2217u ) \u22121 (\u039bU\u00b5U+\u2211 j\u2208Vu \u03c3\u22122bj ( ruj \u2212 \u03c70 \u2212 c zMuj u \u2212 d zUuj j ) , (\u039bU\u2217u )\u22121  (6) For each j,\nbj |rest \u223cN ( (\u039bM\u2217j ) \u22121 (\u039bM\u00b5M+\u2211 u:j\u2208Vu \u03c3\u22122au ( ruj \u2212 \u03c70 \u2212 c zMuj u \u2212 d zUuj j ) , (\u039bU\u2217u )\u22121 \n(7)\nwhere in Equation 6, \u039bU\u2217u = \u039b U + \u2211 j\u2208Vu \u03c3 \u22122bjb T j , and\nin Equation 7, \u039bM\u2217j = \u039b M + \u2211 u:j\u2208Vu \u03c3 \u22122aua T u .\nFinally, we draw the topic assignment indicators, which is where the HDP prior enters the picture. In the original M3F model, user and item topic assignment indicators zUuj and z M uj were sampled from a finite multinomial distribution, with the hyperparameters of this multinomial sampled from a Dirichlet prior multiplied by a likelihood term. In the CRF case, user topic and item topic assignment sampling is done via a twostage process: sample the global dish assignments (per table) available to the franchise, and then sample the local table assignments (per customer) for a restaurant. Note that for the problem, it is not the entire rating that is modeled by the MMSB, but rather the rating residual, which is the rating minus the score predicted by the latent factors. Therefore, let the rating residual be defined as xuj = ruj \u2212 \u03c70 \u2212 au \u00b7 bj . Tying the restaurant analogy to our problem, customers are ratings (or rather, rating residuals, as only the c and d variables are affected by the topic assignments), tables are user-specific proportions over topics, and finally dishes are the topics themselves, in that they represent the parameters used to generate the data (residuals).\nThe corresponding Gibbs sampler thus has two sets of state variables: the first being the table assignment indicator t of the observed residuals to tables, and the second is the dish assignment indicator k associated with each table, which is just a group of residuals. We sample the table assignments as follows:\nP(tuj = t) \u221d Nu,\u2212jt P(xuj |\u00b5 \u2212uj kt )\nP(t = t\u0304) \u221d \u03b3\n(\u2211K k=1 P(xuj |\u00b5\n\u2212uj k )Mk + P(xuj |c0)\u03b2\u2211K\nk=1Mk + \u03b2 ) where t\u0304 corresponds to sampling a new table, \u03b2 and \u03b3 are the hyperparameters for the CRPs corresponding to the dish and table assignments respectively, Nu,\u2212jt is the number of customers (rating residuals) associated with restaurant (user) u sitting on existing table t except the jth customer (i.e., except the rating residual of user u for item j), and Mk is the number of tables over all restaurants (users) on which the dish number\nk is being served. In addition, \u00b5\u2212ujkt is shorthand for the rating residual empirical mean, computed over all residuals across all users assigned to a table with dish k, except for the current residual that we are sampling for. c0 is the prior for the bias or the residual term. When sampling a new table for a residual xuj , if we happen to sample a previously nonexistent table, then we immediately sample a new dish for that table.\nWe sample dishes for each table as follows: P(kut = k) \u221dM\u2212utk \u220f\ntuj=t\nP(xuj |\u00b5\u2212utk ) (8)\nP(kut = k\u0304) \u221d \u03b2 \u220f\ntuj=t\nP(xuj |c0) (9)\nwhere k\u0304 corresponds to sampling a new dish, M\u2212utk represents the count of the tables assigned to dish k except the current table that we are sampling a dish assignment for, and \u00b5\u2212utk is shorthand for the rating residual empirical mean, computed over all residuals across all users assigned to a table with dish k except all residuals on the current table we are sampling for. Note that when computing the likelihoods, we compute the joint likelihood over all the residuals associated with the current table, since in the dish assignment we assign a dish for a group of residuals.\nWhen computing the probability of dish assignments as per Equations 8 and 9, we used the MAP estimate in the predictive likelihood term for our experiments (Section IV). To be completely Bayesian, the actual likelihood term is:\nf(xtuj=t|rest) = \u222b\nP(\u00b5\u2212utk |{xu\u2032j\u2032 |ku\u2032j\u2032 = k, tu\u2032j\u2032 6= t})\u00b7\u220f tuj=t P(xuj |\u00b5\u2212utk )d\u00b5 \u2212ut k (10)\n= \u222b P(\u03b8|X ) n\u220f i=1 P(xi|\u03b8)d\u03b8 (11)\n\u221d \u222b exp ( \u2212(\u03b8 \u2212 \u00b5P )2\n2\u03c32P\n) n\u220f i=1 exp ( \u2212(xi \u2212 \u03b8)2 2\u03c32 ) d\u03b8\n(12)\nwhere in Equation 10, xtuj=t is the set of customers (residual ratings) sitting on table t (the table we are sampling the dish for), and in Equation 11, n is the number of customers (residuals) sitting on table t, X = {xu\u2032j\u2032 |ku\u2032j\u2032 = k, tu\u2032j\u2032 6= t} is set of all other residuals across all users that are associated with dish or topic k (index by l) except those sitting on our special table, and \u03b8 = \u00b5\u2212utk . In Equation 12, we use the assumption that the data (residual) is generated from a Gaussian with a conjugate prior, and \u00b5P and \u03c32P are defined as before. From Equation 12, we can say that the joint probability over P(\u03b8,xtuj=t) is a multivariate Gaussian, and marginalizing over \u03b8 yields a Gaussian as well. Using the laws of total expectation, variance, and covariance, one can show that:\nE[xi|X ] = E[E[xi|\u03b8,X ]|X ] = E[\u03b8|X ] = \u00b5P Var[xi|X ] = Var[E[xi|\u03b8,X ]|X ] + E[Var[xi|\u03b8,X ]|X ]\n= \u03c32 + \u03c32P\nCov[xi, xj |X ] = E[Cov[xi, xj |\u03b8,X ]|X ] + Cov[E[xi|\u03b8,X ],E[xj |\u03b8,X ]|X ]\n= 0 + Cov[\u03b8, \u03b8] = Var[\u03b8] = \u03c32P\nThus, we can write the predictive likelihood term as:\nf(xtuj=t|rest) \u221d 1 | \u2211 | 12 exp\n( \u22121\n2 (xtuj=t \u2212 \u00b5)T \u00b7\u2211\u22121 (xtuj=t \u2212 \u00b5) ) (13)\nwhere \u00b5 is an n-element vector with all entries equal to \u00b5P (the posterior mean), and \u2211 is the covariance matrix for which all diagonal elements are equal to \u03c32 + \u03c32P and all off diagonal elements are equal to \u03c32P .\nWe can also analytically obtain the determinant of the covariance matrix \u2211 : | \u2211 | = \u03c32n + n\u03c32(n\u22121)\u03c32P , where n is the dimensionality of the matrix or the number of customers sitting at the table whose dish we are sampling for."}, {"heading": "IV. EXPERIMENTS", "text": "The proposed iM3F model was evaluated on the MovieLens and DBLP datasets. On the MovieLens set, we broadly divided our results into internal comparisons, meaning hyperparameter variation experiments, and external comparisons, meaning the improvements vis-a\u0300-vis M3F. For the DBLP dataset, we present final numbers in comparison to M3F. While previous work evaluated their approaches on the Netflix Prize dataset, we are unable to do so due to the unavailability of the data. All experiments were run on a quad-core 2.5 GHz Linux machine with 8GB RAM, and the implementation was done in MATLAB and C."}, {"heading": "A. Datasets", "text": "The MovieLens3 dataset is a movie rating set, similar to those used in the Netflix prize. We used two versions of the datasets (ml100k and ml1m) that contain 100,000 and 1 million ratings respectively. Table I shows some summary statistics.\nIn both datasets, each user has rated at least 20 movies, with each rating being an integer from 1 to 5. When comparing the performance of different models, we measured the Root Mean Square Error (RMSE) on a held-out test set that is generated using a leave-one-out strategy: for each user, one random rating is chosen out of her movie ratings and put into the test set; the rest remain in the training set. We performed this random split twice, and all results for the MovieLens experiments are averaged over the two splits.\n3http://www.grouplens.org\nWe have also extracted a publication dataset from the latest DBLP XML data4 as of January 6, 2013. The extracted coauthorship dataset can be made publicly available for comparison purposes. The dataset consists of around 4.5 million ratings; each rating is between two authors, and is simply a count of the number of papers that they have co-authored. For the purposes of our experiments, we selected the first 2 million ratings between 1 and 10, which consisted of 644,256 authors, and split the dataset into training and test components such that every author is represented at least once in the test set. In this dataset, the interpretation of the topics is akin to clusterings of researchers based on a specific sub-field, or geographical location.\nB. Internal Comparisons We use the ml100k dataset with 100 sampling iterations to look at how varying the hyperparameters \u03b2 and \u03b3 affect the final number of topics estimated from the data. The larger ml1m dataset with 500 sampling iterations was used to evaluate RMSE for both the hyperparameter variation experiments and the final comparison with M3F. When comparing against M3F, we use the hyperparameter settings that yield the best performance as in the original paper [1]: \u2022 Number of user topics KU : 2 \u2022 Number of item topics KM : 1 \u2022 Latent factor dimensionality D: 40\nAs we do not alter the latent factor terms, we maintain D = 40 for the iM3F model. The impact of our CRF concentration parameters \u03b2 and \u03b3, which correspond to the CRPs of the dish and table processes respectively, are presented in Figure 3. In Figures 3a to 3c, we show the final topic numbers after running 100 iterations of the Gibbs sampler for the CRF model as we varied \u03b3 and \u03b2. The overall trend is as expected, with higher \u03b2 and \u03b3 values resulting in more topics. Figure 4 presents the train and test set RMSE on ml1m for certain settings of the concentration parameters \u03b2 and \u03b3. The lowest training RMSE was obtained when \u03b3 = 1, \u03b2 = 0.1 (0.6904), with KU = 94.5 and KM = 100, which we refer to as \u2018TrainBest\u2019 in subsequent exposition. The lowest test RMSE was when \u03b3 = 0.1, \u03b2 = 0.1 (0.7718), i.e., \u2019TestBest\u2019, with KU = 64, and KM = 77.5.5\nC. iM3F and M3F We then compared our CRF iM3F model with the M3F implementation of [1]. For the ml1m data, the M3F model also shows a better fit to the data, and achieves a much lower train RMSE of 0.60. However, in the test set comparison in Figure 5, both of our CRF models outperformed M3F after 500 rounds of Gibbs sampling. The final test RMSE for \u2018TestBest\u2019 (our best result) was 0.7718, while for the M3F model it was 0.77836, the difference being statistically significant according\n4http://dblp.uni-trier.de/xml/ 5fractional values are obtained due to averaging between the two ML1M splits 6note that this is different than the result on the same dataset published in previous work, however, we were unable to replicate the previous number despite running the original implementation directly.\nto the paired t-test. To put this improvement in context, the test set gain of 0.0065 is more than twice the improvement that M3F achieved over its baseline, BPMF, the difference between M3F and BPMF being the addition of the MMSB to model the contextual bias. \u2018TrainBest\u2019 achieved a test RMSE of 0.7750, also an improvement greater than the M3F-BPMF improvement.\nOn the DBLP dataset, for M3F we carried out an extensive three-way grid search, varying the number of latent factors (with the values {5, 10, 15, 20, 25, 30}), and user and item topics (with the values {1, 2, 4, 8, 16, 32}), and found the best configuration to be D = 25,KU = 2,KM = 1. The very fact that we had to implement an extensive grid search for these parameters already indicates the potential advantages of the nonparametric approach. For the iM3F model, guided by previous results we set \u03b3 = 0.1 and tried \u03b2 = 0.1 and 1.\nThe final results are presented in Table II, along with the final number of user and item topics. It is interesting to note that the number of user and item topics that achieved the best test RMSE in iM3F is quite a bit higher than the optimal parameters for M3F.\nThe performance gain achievable through iM3F comes at a cost, in that Gibbs sampling with nonparametric models is computationally more expensive than with parametric ones, as one always has to consider growing the number of clusters in light of more data. However, we note that recent efforts that develop exact, distributed MCMC procedures for Dirichlet process-based models [18] can easily be applied in our scenario to accelerate Gibbs sampling, which is a simple solution for today\u2019s multicore computers. We emphasize however, that we have not added additional parameters to the model; rather, KU and KM have been replaced with a pair of hyperparameters, \u03b2 and \u03b3, providing the model more flexibility to learn."}, {"heading": "V. FUTURE WORK & CONCLUSION", "text": "With the CRF prior, it is also possible to experiment with variants based on the nature of the data itself. For example, reciprocal datasets, wherein users rate other users ratings are based on mutual compatibility, are becoming increasingly more widespread, e.g., academic publication databases like our\nDBLP dataset or the Microsoft Academic Scholar database, or online dating websites like eHarmony or OkCupid. In the future, we would like to model the co-authorship dataset released (and other similar datasets) in more direct \u2018relational\u2019 manner. The bidirectional ratings need to be combined somehow to incorporate the principle of reciprocity in these ratings. Handling such modifications is straightforward in our work, e.g., Figure 6.\nIn this paper, we designed and implemented an important addition to the general field of probabilistic matrix factorization. We proposed a nonparametric prior to the mixed membership matrix factorization model, and in the process derived a Gibbs sampler using a Chinese restaurant franchise representation. We then validated our model by decreasing RMSE on the MovieLens 1M dataset compared to M3F\u2019s RMSE, a much more significant improvement than what had been done previously in the literature. We also extracted a co-authorship database from DBLP, and showed RMSE improvements over M3F there as well.\nWhat the results suggest is that when utilizing and incorporating notions of mixed membership topic modeling into probabilistic matrix factorization, a nonparametric prior is a key assumption to make and model, let alone the convenience factor of not having to perform a grid search to find parameter values."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Lester Mackey for assistance in modifying his original codebase to the nonparametric version, Eric P. Xing for his comments, and the anonymous reviewers for their feedback."}], "references": [{"title": "Mixed membership matrix factorization", "author": ["L. Mackey", "D. Weiss", "M.I. Jordan"], "venue": "Proceedings of the 27th International Conference on Machine Learning, June 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["J.L. Herlocker", "J.A. Konstan", "L.G. Terveen", "John", "T. Riedl"], "venue": "ACM Transactions on Information Systems, vol. 22, pp. 5\u201353, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, IEEE Computer Society, vol. 42, no. 8, pp. 30\u201337, Aug. 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Advances in Neural Information Processing Systems, vol. 20, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "author": ["\u2014\u2014"], "venue": "Proceedings of the International Conference on Machine Learning, vol. 25, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Mixed membership stochastic blockmodels", "author": ["E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1981\u20132014, Jun. 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1981}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, Mar. 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J.B. Tenenbaum", "T. Griffiths", "T. Yamada", "N. Ueda"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Estimation and prediction for stochastic blockstructures", "author": ["K. Nowicki", "T.A.B. Snijders"], "venue": "Journal of the American Statistical Association, vol. 96, no. 455, pp. 1077\u20131087, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201911, 2011, pp. 448\u2013456.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast nonparametric matrix factorization for large-scale collaborative filtering", "author": ["K. Yu", "S. Zhu", "J. Lafferty", "Y. Gong"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, ser. SIGIR \u201909. New York, NY, USA: ACM, 2009, pp. 211\u2013218.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonparametric bayesian matrix factorization by power-ep", "author": ["N. Ding", "Y.A. Qi", "R. Xiang", "I. Molloy", "N. Li"], "venue": "Journal of Machine Learning Research - Proceedings Track, vol. 9, pp. 169\u2013176, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric max-margin matrix factorization for collaborative prediction", "author": ["M. Xu", "J. Zhu", "B. Zhang"], "venue": "NIPS, 2012, pp. 64\u201372.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Exact and approximate sum representations for the dirichlet process", "author": ["H. Ishwaran", "M. Zarepour"], "venue": "Canadian Journal of Statistics, vol. 30, no. 2, pp. 269\u2013283, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "A Bayesian Analysis of Some Nonparametric Problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics, vol. 1, no. 2, pp. 209\u2013230, 1973.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1973}, {"title": "Ferguson distributions via P\u00f3lya urn schemes", "author": ["D. Blackwell", "J.B. Macqueen"], "venue": "The Annals of Statistics, vol. 1, pp. 353\u2013355, 1973.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1973}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association, vol. 101, no. 476, pp. 1566\u20131581, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Exact and Efficient Parallel Inference for Nonparametric Mixture Models", "author": ["S.A. Williamson", "A. Dubey", "E.P. Xing"], "venue": "ICML 2013, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The recently proposed mixed membership matrix factorization (MF) model [1] aims to tackle this problem by explicitly modeling ratings not only with static latent factors, but with an additional bias term that takes into account the context of the rating.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "RELATED WORK As a way to attack recommender system problems, matrix factorization has been one of the popular approaches along with content-based filtering [2], and in recent years has proven to be the superior alternative of the two [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "RELATED WORK As a way to attack recommender system problems, matrix factorization has been one of the popular approaches along with content-based filtering [2], and in recent years has proven to be the superior alternative of the two [3].", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "Probabilistic matrix factorization (PMF) was first introduced by Salakhutdinov and Mnih [4], whereby ratings are generated from a Gaussian with a mean computed as the dot product of the user and item latent factors, with fixed variance.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "A fully Bayesian version of PMF (BPMF) was subsequently proposed by the same authors [5], where they assumed the latent factors were themselves generated by Gaussian distributions, with normalWishart priors on the hyperparameters.", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Our work is primarily based on extending the MF model [1] .", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 271, "endOffset": 274}, {"referenceID": 7, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 304, "endOffset": 307}, {"referenceID": 8, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 309, "endOffset": 312}, {"referenceID": 9, "context": "Collaborative topic regression [10] also combines matrix factorization and mixed membership topic modeling, but their method solves a different problem, the cold start issue of outof-matrix predictions, rather than contextual bias.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "[11] do not use a probabilistic approach and instead use SVD, whereas [12], [13] adopt an Indian Buffett Process to model the latent factor dimensionality, and", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11] do not use a probabilistic approach and instead use SVD, whereas [12], [13] adopt an Indian Buffett Process to model the latent factor dimensionality, and", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "[11] do not use a probabilistic approach and instead use SVD, whereas [12], [13] adopt an Indian Buffett Process to model the latent factor dimensionality, and", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "For a more rigorous introduction, we refer the reader to [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "If we take the number of mixture components to infinity, then the conjugate prior becomes a Dirichlet process (DP) [15], a distribution over an infinitely fine-grained real line.", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "1: The original MF model, as proposed in [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 13, "context": "practice, we do not have a density defined for the infinitedimensional DP prior, so we define it indirectly via a sampling scheme that is used to generate samples from this prior [14].", "startOffset": 179, "endOffset": 183}, {"referenceID": 15, "context": "Otherwise, if we are simply interested in partition or cluster assignments for the data, we can marginalize the mixture weights and sample the assignments directly, through a process known as a Chinese restaurant process (CRP) [16].", "startOffset": 227, "endOffset": 231}, {"referenceID": 16, "context": "This requirement necessitates a hierarchical Dirichlet process (HDP) prior [17] on the mixture components.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "When comparing against MF, we use the hyperparameter settings that yield the best performance as in the original paper [1]: \u2022 Number of user topics K : 2 \u2022 Number of item topics K : 1 \u2022 Latent factor dimensionality D: 40 As we do not alter the latent factor terms, we maintain D = 40 for the iMF model.", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "iMF and MF We then compared our CRF iMF model with the MF implementation of [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 17, "context": "However, we note that recent efforts that develop exact, distributed MCMC procedures for Dirichlet process-based models [18] can easily be applied in our scenario to accelerate Gibbs sampling, which is a simple solution for today\u2019s multicore computers.", "startOffset": 120, "endOffset": 124}], "year": 2014, "abstractText": "Rating and recommendation systems have become a popular application area for applying a suite of machine learning techniques. Current approaches rely primarily on probabilistic interpretations and extensions of matrix factorization, which factorizes a user-item ratings matrix into latent user and item vectors. Most of these methods fail to model significant variations in item ratings from otherwise similar users, a phenomenon known as the \u201cNapoleon Dynamite\u201d effect. Recent efforts have addressed this problem by adding a contextual bias term to the rating, which captures the mood under which a user rates an item or the context in which an item is rated by a user. In this work, we extend this model in a nonparametric sense by learning the optimal number of moods or contexts from the data, and derive Gibbs sampling inference procedures for our model. We evaluate our approach on the MovieLens 1M dataset, and show significant improvements over the optimal parametric baseline, more than twice the improvements previously encountered for this task. We also extract and evaluate a DBLP dataset, wherein we predict the number of papers co-authored by two authors, and present improvements over the parametric baseline on this alternative domain as well.", "creator": "LaTeX with hyperref package"}}}