{"id": "1401.3432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments", "abstract": "this paper proposes testing experimentally validates a bayesian network model of a range finder adapted to harsh environments. all modeling components are rigorously explained, and all model parameters have a physical form. this approach results in a scientific and intuitive model. with restriction to the state of the art beam model this paper : ( i ) proposes radically different functional form for the probability of range measurements caused by the objects, ( ii ) intuitively explains the discontinuity encountered in te state nor the standard beam diagram, and ( iii ) relates the number of model parameters, while maintaining the minimal representational power since experimental data. the proposed beam model is called rbbm, short for rigorously descriptive beam model. a maximum likelihood and inverse variational bayesian estimator ( both based on expectation - maximization ) represent proposed to learn the model parameters.", "histories": [["v1", "Wed, 15 Jan 2014 04:49:23 GMT  (3761kb)", "http://arxiv.org/abs/1401.3432v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tinne de laet", "joris de schutter", "herman bruyninckx"], "accepted": false, "id": "1401.3432"}, "pdf": {"name": "1401.3432.pdf", "metadata": {"source": "CRF", "title": "A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments", "authors": ["Tinne De Laet", "Joris De Schutter", "Herman Bruyninckx"], "emails": ["tinne.delaet@mech.kuleuven.be", "joris.deschutter@mech.kuleuven.be", "herman.bruyninckx@mech.kuleuven.be"], "sections": [{"heading": null, "text": "Furthermore, the RBBM is extended to a full scan model in two steps: first, to a full scan model for static environments and next, to a full scan model for general, dynamic environments. The full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter. In contrast to Gaussian-based state of the art models, the proposed full scan model uses a sample-based approximation. This sample-based approximation enables handling dynamic environments and capturing multimodality, which occurs even in simple static environments."}, {"heading": "1. Introduction", "text": "In a probabilistic approach, inaccuracies are embedded in the stochastic nature of the model, particularly in the conditional probability density representing the measurement process. It is of vital importance that all types of inaccuracies affecting the measurements are incorporated in the probabilistic sensor model. Inaccuracies arise from sensor limitations, noise, and the fact that most complex environments can only be represented and perceived in a limited way. The dynamic nature of the environment in particular is an important source of inaccuracies. This dynamic nature results from the presence of unmodeled and possibly moving objects and people.\nThis paper proposes a probabilistic range finder sensor model for dynamic environments. Range finders, which are widely used in mobile robotics, measure the distances z to objects in the environment along certain directions \u03b8 relative to the sensor. We derive the sensor\nc\u00a92008 AI Access Foundation. All rights reserved.\nmodel in a form suitable for mobile robot localization, i.e.: P (Z = z |X = x,M = m)1, where Z indicates the measured range, X the position of the mobile robot (and of the sensor mounted on it), and M the environment map. The presented model is however useful in other applications of range sensors as well.\nFirst, this paper derives a probabilistic sensor model for one beam of a range finder, i.e. the beam model. In particular, this paper gives a rigorously Bayesian derivation using a Bayesian network model while stating all model assumptions and giving a physical interpretation for all model parameters. The obtained model is named RBBM, short for Rigorously Bayesian Beam Model. The innovations of the presented approach are (i) to introduce extra state variables A = a for the positions of unmodeled objects in the probabilistic sensor model P (z | x,m, a), and (ii) to marginalize out these extra state variables from the total probability before estimation. The latter is required because extra variables (exponentially!) increase the computational complexity of state estimation while in a lot of applications estimating the position of unmodeled objects is not of primary interest. In summary, the marginalization avoids the increase in complexity to infer the probability distributions P (x) and P (m), while maintaining the modeling of the dynamic nature of the environment.\nThis paper furthermore presents a maximum-likelihood and a variational Bayesian estimator (both based on expectation-maximization) to learn the model parameters of the RBBM.\nNext, the paper presents an extension of the RBBM to a full scan model i.e.: P (z | \u03b8, x,m) where z and \u03b8 contain all the measured distances and beam angles, respectively. This full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter. In contrast to Gaussian-based state of the art models, the proposed full scan model uses a sample-based approximation. The sample-based approximation allows us to capture the multi-modality of the full scan model, which is shown to occur even in simple static environments."}, {"heading": "1.1 Paper Overview", "text": "This paper is organized as follows. Section 2 gives an overview of the related work. Section 3 (i) presents a Bayesian beam model for range finders founded on Bayesian networks, the RBBM, (ii) mathematically derives an analytical formula for the probabilistic sensor model while clearly stating all assumptions, (iii) provides useful insights in the obtained beam model and (iv) shows that the obtained analytical sensor model agrees with the proposed Bayesian network. Section 4 presents a maximum likelihood and a variational Bayesian estimator (both based on expectation-maximization) to learn the model parameters. In Section 5 the model parameters of the RBBM are learned from experimental data and the resulting model is compared with the state of the art beam model proposed by Thrun, Burgard, and Fox (2005), further on called Thrun\u2019s model. Section 6 extends the RBBM to an adaptive full scan model for dynamic environments. Section 7 discusses the obtained RBBM and adaptive full scan model and compares them with previously proposed range finder sensor models.\n1. To simplify notation, the explicit mention of the random variable in the probabilities is omitted whenever possible, and replaced by the common abbreviation P (x) instead of writing P (X = x)."}, {"heading": "2. Related Work", "text": "Three basic approaches to deal with dynamic environments exist in the literature (Fox, Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor model and outlier detection.\nIn state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun (2003) developed an algorithm \u2018SLAM with DATMO\u2019, short for SLAM with the detection and tracking of moving objects. State augmentation however is often infeasible since the computational complexity of state estimation increases exponentially with the number of independent state variables to estimate. A closely related solution consists of adapting the map according to the changes in the environment. Since such approaches assume that the environment is almost static, they are unable to cope with real dynamics as in populated environments (Fox et al., 1999). A more recent, related approach proposed by Wolf and Sukhatme (2004) maintains two coupled occupancy grids of the environment, one for the static map and one for the moving objects, to account for environment dynamics.\nProbabilistic approaches are to some extent robust to unmodeled dynamics, since they are able to deal with sensor noise. In such approaches however, the sensor noise should reflect the real uncertainty due to the unmodeled dynamics of the environment. Therefore, a second approach for dealing with dynamic environments is to adapt the sensor model to correctly reflect situations in which measurements are affected by the unmodeled environment dynamics. Fox et al. (1999) show that such approaches are only capable to model such noise on average, and, while these approaches work reliably with occasional sensor blockage, they are inadequate in situations where more than fifty percent of the measurements are corrupted.\nTo handle measurement corruption more effectively, an approach based on outlier detection can be used. This approach uses an adapted sensor model, as explained in the previous paragraph. The idea is to investigate the cause of a sensor measurement and to reject measurements that are likely to be affected by unmodeled environment dynamics. Ha\u0308hnel, Schulz, and Burgard (2003a) and Ha\u0308hnel, Triebel, Burgard, and Thrun (2003b) studied the problem of performing SLAM in environments with many moving objects using the EM algorithm for filtering out affected measurements. By doing so, they were able to acquire maps in the environment where conventional SLAM techniques failed. Fox et al. (1999) propose two different kinds of filters: an entropy filter, suited for an arbitrary sensor, and a distance filter, designed for proximity sensors. These filters detect whether a measurement is corrupted or not, and discard sensor readings resulting from objects that are not contained in the map.\nThis paper focuses on (sonar and laser) range finders, whose physical principle is the emission of a sound or light wave, followed by the recording of its echo. Highly accurate sensor models would include physical parameters such as surface curvature and material absorption coefficient. These parameters are however difficult to estimate robustly in unstructured environments. Hence, the literature typically relies on purely basic geometric models.\nThe range finder sensor models available from the literature are traditionally divided in three main groups: feature-based approaches, beam-based models and correlation-based\nmethods. Feature-based approaches extract a set of features from the range scan and match them to features contained in an environmental model. Beam-based models, also known as ray cast models, consider each distance measurement along a beam as a separate range measurement. These models represent the one-dimensional distribution of the distance measurement by a parametric function, which depends on the expected range measurement in the respective beam directions. In addition, these models are closely linked to the geometry and the physics involved in the measurement process. They often result in overly peaked likelihood functions due to the underlying assumption of independent beams. The last group of range finder sensor models, correlation-based methods, build local maps from consecutive scans and correlate them with a global map. The simple and efficient likelihood field models or end point model (Thrun, 2001) are related to these correlation-based methods. Plagemann, Kersting, Pfaff, and Burgard (2007) nicely summarize the advantages and drawbacks of the different range finder sensor models.\nRange finder sensor models can also be classified according to whether they use discrete geometric grids (Ha\u0308hnel et al., 2003a, 2003b; Fox et al., 1999; Burgard, Fox, Hennig, & Schmidt, 1996; Moravec, 1988) or continuous geometric models (Thrun et al., 2005; Choset, Lynch, Hutchinson, Kantor, Burgard, Kavraki, & Thrun, 2005; Pfaff, Burgard, & Fox, 2006). Moravec proposed non-Gaussian measurement densities over a discrete grid of possible distances measured by sonar; the likelihood of the measurements has to be computed for all possible positions of the mobile robot at a given time. Even simplified models (Burgard et al., 1996) in this approach turned out to be computationally too expensive for real-time application. Therefore, Fox et al. proposed a beam model consisting of a mixture of two physical causes for a measurement: a hit with an object in the map, or with an object not yet modeled in the map. The last cause accounts for the dynamic nature of the environment. An analogous mixture (Thrun et al., 2005; Choset et al., 2005) adds two more physical causes: a sensor failure and an unknown cause resulting in a \u2018max-range\u2019 measurement and a \u2018random\u2019 measurement, respectively. While Thrun et al. and Pfaff et al. use a continuous model, Choset et al. present the discrete analog of the mixture, taking into account the limited resolution of the range sensor. Pfaff et al. extend the basic mixture model for use in Monte Carlo localization. To overcome problems due to the combination of the limited representational power and the peaked likelihood of the accurate range finder, they propose an adaptive likelihood model. The likelihood model is smooth during global localization and more peaked during tracking.\nRecently, different researchers tried to tackle the problems associated with beam-based models, caused by the independence assumptions between beams. Plagemann et al. (2007) propose a sensor model for the full scan. The model treats the sensor modeling task as a non-parametric Bayesian regression problem, and solves it using Gaussian processes. It is claimed that the Gaussian beam processes combine the advantages of the beam-based and the correlation-based models. Due to the underlying assumption that the measurements are jointly Gaussian distributed, the Gaussian beam processes are not suited to take into account the non-Gaussian uncertainty due to the dynamic nature of the environment. An alternative approach to handle the overly-peaked likelihood functions resulting from the traditional beam models is proposed by Pfaff, Plagemann, and Burgard (2007). A locationdependent full scan model takes into account the approximation error of the sample-based representation, and explicitly models the correlations between individual beams introduced\nby the pose uncertainty. The measurements are assumed to be jointly Gaussian distributed just as Plagemann et al. proposed. While Plagemann et al. represent the covariance matrix as a parametrized covariance function using Gaussian processes whose parameters are learned from data, Pfaff et al. learn the full covariance matrix being less restrictive in this manner. Despite the modeled correlation between beams, the measurements are still assumed to be jointly Gaussian distributed, which again limits the applicability in dynamic environments.\nThis paper proposes a rigorously Bayesian modeling of the probabilistic range sensor beam model for dynamic environments, referred to as RBBM. Similar to the work of Thrun et al. (2005) and Pfaff et al. (2006) the sensor model is derived for a continuous geometry. Unlike previous models of Thrun et al. (2005), Pfaff et al. (2006), Fox et al. (1999) and Choset et al. (2005), the mixture components are founded on a Bayesian modeling. This modeling makes use of probabilistic graphical models, in this case Bayesian networks. Such graphical models provide a simple way to visualize the structure of a probabilistic model, and can be used to design and motivate new models (Bishop, 2006). By inspection of the graph, insights of the model, including conditional independence properties are obtained. Next, inspired by the adaptive full scan models in the literature (Pfaff et al., 2006, 2007; Plagemann et al., 2007), the RBBM is extended to an adaptive full scan model. The underlying sample-based approximation of the full scan model, in contrast to the Gaussianbased approximation proposed by Pfaff et al. (2007) and Plagemann et al., enables handling dynamic environments and capturing multi-modality, which occurs even in simple static environments."}, {"heading": "3. Beam Model", "text": "We model the probabilistic beam model P (Z = z |X = x,M = m) for dynamic environments as a Bayesian network. We introduce extra state variables A = a for the positions of unmodeled objects in the probabilistic sensor model P (z | x,m, a). To prevent an exponential increase of the computational complexity of the state estimation due to the extra variables, these variables are marginalized out from the total probability before estimation. The marginalization:\nP (z | x,m) =\n\u222b\na\nP (z | x,m, a)P (a) da, (1)\navoids increasing the complexity to infer the conditional probability distributions of interest, P (x) and P (m), while it maintains the modeling of the dynamic nature of the environment. Section 3.1 explains which extra state variables are physically relevant, while Section 3.3 explains the marginalization of these extra state variables. Section 3.5 summarizes all assumptions and approximations. Finally, Section 3.6 provides useful insights in the obtained beam model, called RBBM, and in its derivation. Section 3.7 shows that the obtained analytical expression for the RBBM agrees with the proposed Bayesian network by means of a Monte Carlo simulation."}, {"heading": "3.1 Bayesian Model", "text": "Bayesian networks graphically represent probabilistic relationships between variables in a mathematical model, to structure and facilitate probabilistic inference computations with those variables (Jensen & Nielsen, 2007; Neapolitan, 2004). A Bayesian network is defined as follows: (i) a set of nodes, each with an associated random variable, connected by directed edges forming a directed acyclic graph (DAG); (ii) each discrete (continuous) random variable has a finite (infinite) set of mutually exclusive states; (iii) each random variable A with parents B1, . . . , BN has a conditional probability distribution P (A |B1, . . . , Bn) (known as conditional probability table in the case of discrete variables). Although the definition of Bayesian networks does not refer to causality, and there is no requirement that the directed edges represent causal impact, a well-known way of structuring variables for reasoning under uncertainty is to construct a graph representing causal relations (Jensen & Nielsen, 2007). In this case the graphical models are also known as generative models (Bishop, 2006), since they capture the causal process generating the random variables.\nIn this application, the range sensor ideally measures z\u22c6, the distance to the closest object in the map. An unknown number n of unmodeled objects, possibly preventing the measurement of the closest object in the map, are however present in the environment. Depending on the position of the jth unmodeled object along the measurement beam, xNj , the unmodeled object occludes the map or not. The unmodeled object only occludes the map if it is located in front of the closest object contained in the map. k is the total number of occluding objects out of the n unmodeled objects. The positions of these occluding objects on the measurement beam are denoted by {xKi}i=1:k. If the map is occluded by an unmodeled object, the range sensor will ideally measure z\u22c6occl = xKc, with xKc the position of the closest occluding object.\nThe following extra state variables, a in Eq. (1), are included in the Bayesian model: N is the discrete random variable indicating the unknown number of unmodeled objects in the environment; XNj is the continuous random variable for the position of the jth unmodeled object on the measurement beam; K is the discrete random variable indicating the number of objects occluding the measurement of the map; XKi is the continuous random variable for the position of the ith occluding object on the measurement beam; and Z\u22c6occl is the continuous random variable indicating the ideal range measurement of the closest occluding object. Fig. 1 shows the Bayesian network for the probabilistic range finder sensor model with the variables Z,X and M that occur in the probabilistic sensor model (defined in Section 1), all the extra variables N,XN = {XNj}j=1:n,K,XK = {XKi}i=1:k, Z \u22c6 occl and the model parameters p and \u03c3m (defined in Section 3.2).\nThe directed edges in the graphical model represent causal relationships between the variables. For example, X and M unambiguously determine the measured range Z for a perfect sensor in the absence of unmodeled occluding objects. The number of occluding objectsK depends on the total number N of unmodeled objects and their positionsXN with respect to the measurement beam. X and M also have a causal impact on K: the larger the expected measurement z\u22c6, the higher the possibility that one or more unmodeled objects are occluding the modeled object corresponding to the expected measurement. The positions along the measurement beam XK of the occluding objects are equal to the positions of the K of N unmodeled objects occluding the map. Therefore, random variables XK are\nnot only influenced by K but also by XN . Since the K objects are occluding the map, their positions along the measurement beam are limited to the interval [0, z\u22c6], so XK has a causal dependency on X and M . The ideal measurement z\u22c6occl of an occluding object is the position of the occluding object closest to the sensor, so Z\u22c6occl depends on the positions XK of the occluding objects. Finally, measurement Z also depends on the ideal measurement of the occluding object Z\u22c6occl and the number of occluding objects K. In case of occlusion (k \u2265 1), z\u22c6occl is ideally measured, else (no occlusion, k = 0) z \u22c6 is ideally measured."}, {"heading": "3.2 Conditional Probability Distributions", "text": "Inferring the probability distribution of the extra state variables such as P (n) is often infeasible. Marginalization of the extra state variables Z,X,M,N,XN ,K,XK , Z \u22c6 occl avoids the increase in complexity of the estimation problem, but still takes into account the dynamic nature of the environment. Marginalization requires the modelling of all conditional probability tables and conditional probability distributions (pdf) of each random variable conditionally on its parents.\nFirst of all, some assumptions have to be made for P (n). Assume that the probability of the number of unmodeled objects decreases exponentially, i.e. P (n) is given by:\nP (n) = (1\u2212 p) pn, (2)\nwith p a measure for the degree of appearance of unmodeled objects. More precisely, p is the probability that at least one unmodeled object is present. While p is indicated in Fig. 1, Fig. 2 shows the resulting distribution P (n).\nSecondly, assume that nothing is known a priori about the position of the unmodeled objects along the measurement beam. Hence each unmodeled object\u2019s position is assumed to be uniformly distributed over the measurement beam (Fig. 3):\nP (xNj) =\n{ 1\nzmax if xNj \u2264 zmax 0 otherwise, (3)\nwith zmax the maximum range of the range sensor. Thirdly, assume the positions of the unmodeled objects are independent :\nP (xN | n) = n\u220f\nj=1\nP (xNj) . (4)\nNext, an expression is needed for the conditional probability: P (k | n, xN , x,m), i.e. the probability that k of the n unmodeled objects are occluding the map m. An unmodeled object is occluding the map m if it is located along the measurement beam and in front of the closest object in the map. It is straightforward to show that P (k | n, xN , x,m) is a binomial distribution:\nP (k | n, xN , x,m) =\n \n\n(\nn\nk\n)\nuk (1\u2212 u)n\u2212k if k \u2264 n\n0 otherwise,\n(5)\nwhere u is the probability that an unmodeled object is occluding the map and\n( n\nk\n)\n=\nn! (n\u2212k)!k! is the number of ways of selecting k objects out of a total of n objects. Fig. 4 shows\nthis binomial distribution. Since it was assumed that the positions of the unmodeled objects were uniformly distributed, u, the probability that an unmodeled object is occluding the map is:\nu = P (xNj < z \u22c6) =\nz\u22c6\nzmax , (6)\nas depicted in Fig. 3.\nFurthermore, an analytical expression for P (xK | xN , k) is necessary. The positions of the occluding objects xK are equal to the positions of the unmodeled objects xN that are occluding the map, as shown in Fig. 5. In other words, xKi equals xNj if and only if the unmodeled object is occluding the map, i.e. if xNj \u2264 z \u22c6:\nP (xKi | xNj , k, x,m) =\n \n\n1 P(xNj\u2264z\u22c6) \u03b4 (xKi \u2212 xNj) = zmax z\u22c6 \u03b4 (xKi \u2212 xNj) if xNj \u2264 z \u22c6 0 otherwise, (7)\nwith \u03b4 the Dirac function and xKi the occluding object corresponding to xNj .\nIn case of occlusion, the range sensor ideally measures the distance to the closest occluding object xKc:\nP (z\u22c6occl | xK) = \u03b4 (z \u22c6 occl \u2212 xKc) . (8)\nWhile range finders are truly quite deterministic since the measurements are to a great extent explainable by underlying physical phenomena such as specular reflections, inference, ... these underlying phenomena are complex and therefore costly to model. On top of these underlying phenomena additional uncertainty on the measurement is due to (i) uncertainty in the sensor position, (ii) inaccuracies of the world model and (iii) inaccuracies of the sensor itself. So far only disturbances on the measurements due to unmodeled objects in the environment are included. To capture the additional uncertainty, additional measurement noise is added. After taking into account the disturbances by unmodeled objects, unexplainable measurements and sensor failures (Section 3.4), there is no physical reason to expect that the mean value of the true measurements deviates from the expected measurement and that the true measurements are distributed asymmetrically around the mean. Therefore\nsymmetrical noise with mean value zero is added. Two facts justify the modeling of the measurement noise as a normal distribution: (i) the normal distribution maximizes the information entropy among all distributions with known mean and variance, making it the natural choice of underlying distribution for data summarized in terms of sample mean and variance; and (ii) if the underlying phenomena are assumed to have a small, independent effect on the measurement, the central limit theorem states that under certain conditions (such as being independent and identically distributed with finite variance), the sum of a large number of random variables is approximately normally distributed. If the measurement noise is modeled by a zero mean Gaussian with standard deviation \u03c3m, the conditional probability P (z | x,m, z\u22c6occl, k) is:\nP (z | x,m, z\u22c6occl, k) =\n{\nN (z; z\u22c6, \u03c3m) if k = 0 N (z; z\u22c6occl, \u03c3m) if k \u2265 1, (9)\nwhere the conditional probability P (z | x,m, z\u22c6occl, k) has two main cases, the first for k = 0 where no occlusion is present and the sensor is observing the map m, and the second case for k \u2265 1 where the sensor observes an occluding object. \u03c3m is included in the Bayesian network of Fig. 1."}, {"heading": "3.3 Marginalization", "text": "This section shows the different steps needed to marginalize out the extra state variables in Eq. (1), and motivates the approximation that leads to an analytical sensor model.\nThe product rule rewrites the sensor model P (z | x,m) as:\nP (z | x,m) = P (z, x,m)\nP (x,m) =\nP (z, x,m)\nP (x)P (m) , (10)\nsince X and M are independent. The numerator is obtained by marginalizing the joint probability of the whole Bayesian network pjoint = P (z, x,m, xN , n, xK , k, z \u22c6 occl) over xN , n, xK , k and z \u22c6 occl:\nP (z, x,m) =\n\u222b\nz\u22c6occl\n\u2211\nk\n\u222b\nxK\n\u2211\nn\n\u222b\nxN\npjoint dxN dxK dz \u22c6 occl. (11)\nUsing the chain rule to factorize the joint distribution while making use of the conditional dependencies in the Bayesian network (Fig. 1) yields:\npjoint = P (z | x,m, z \u22c6 occl, k)P (z \u22c6 occl | xK)P (k | n, xN , x,m)\nP (xK | xN , k, x,m)P (xN | n)P (n)P (x)P (m) . (12)\nSubstituting (12) and then (11) into (10) gives:\nP (z | x,m) =\n\u222b\nz\u22c6occl\n\u2211\nk\nP (z | x,m, z\u22c6occl, k)\n\u222b\nxK\nP (z\u22c6occl | xK)\n\u2211\nn\nP (k | n, x,m)P (n)P (xK | n, k, x,m) dxK dz \u22c6 occl, (13)\nwhere\nP (xK | n, k, x,m) =\n\u222b\nxN\nP (xK | xN , k, x,m)P (xN | n) dxN . (14)\nSince the binomial distribution P (k | n, xN , x,m) of Eq. (5) is independent of xN , it is moved out of the integral over xN in (14), and is further on denoted by P (k | n, x,m).\nMarginalizing xN Now study the integral over xN in Eq. (14) and focus on xNj , the position of one unmodeled object. Substituting (3) and (7) results in:\nP (xKi | n, k, x,m) =\n\u222b z\u22c6\nxNj=0\nzmax\nz\u22c6 \u03b4 (xKi \u2212 xNj)\n1\nzmax dxNj\n=\n{ 1 z\u22c6 if xKi \u2264 z \u22c6\n0 otherwise. (15)\nThis equation expresses that xKi is uniformly distributed when conditioned on n, k, x and m as shown in Fig. 6. Since all occluding objects are considered independent:\nP (xK | n, k, x,m) =\n{( 1 z\u22c6 )k if \u2200 0 \u2264 i \u2264 k : xKi \u2264 z \u22c6\n0 otherwise. (16)\nThis equation shows that P (xK | n, k, x,m) is independent of n and thus can be moved out of the summation over n in Eq. (13):\nP (z | x,m) =\n\u222b\nz\u22c6occl\n\u2211\nk\nP (z | x,m, z\u22c6occl, k)P (z \u22c6 occl | k, x,m)P (k | x,m) dz \u22c6 occl, (17)\nwith\nP (k | x,m) = \u2211\nn\nP (k | n, x,m)P (n) , (18)\nand\nP (z\u22c6occl | n, k, x,m) =\n\u222b\nxK\nP (z\u22c6occl | xK)P (xK | k, x,m) dxK . (19)\nMarginalizing n First focus on the summation over n in Eq. (18) and substitute (2) and (5):\nP (k | x,m) = \u221e\u2211\nn=k\n[( n\nk\n) uk (1\u2212 u)n\u2212k (1\u2212 p) pn ] . (20)\nAppendix A proves that this infinite sum simplifies to:\nP (k | x,m) = ( 1\u2212 p\u2032 ) p\u2032k, (21)\nwith\np\u2032 = up\n1\u2212 (1\u2212 u) p . (22)\nMarginalizing xK Now focus on the integral over xK in Eq. (19). Substituting (8) into this equation results in:\nP (z\u22c6occl | k, x,m) =\n\u222b\nxKc\n\u03b4 (z\u22c6occl \u2212 xKc)P (xKc | k) dxKc\n= P (xKc = z \u22c6 occl | k, x,m) . (23)\nThis equation shows that the conditional probability P (z\u22c6occl | k, x,m) represents the probability that the perfect measurement of the nearest occluding object is z\u22c6occl, i.e. the probability that the nearest occluding object is located at z\u22c6occl. This is only the case when one of the k objects along the measurement beam is located such that z\u22c6occl is measured, while all other objects along the measurement beam are located behind the occluding object, or expressed in probabilities:\nP (z\u22c6occl | k, x,m) = k\u2211\ni=1\nP (xK 6=i \u2265 z \u22c6 occl | k, x,m)P (xKi = z \u22c6 occl | k, x,m) . (24)\nSince xK is uniformly distributed over [0, z \u22c6] as shown by Eq. (15), it follows that:\nP (xKi = z \u22c6 occl | k, x,m) =\n1\nz\u22c6 , (25)\nP (xKi \u2265 z \u22c6 occl | k, x,m) = z\u22c6 \u2212 z\u22c6occl z\u22c6 , (26)\nand (24) can be written as:\nP (z\u22c6occl | k, x,m) = k 1\nz\u22c6\n( z\u22c6 \u2212 z\u22c6occl\nz\u22c6\n)k\u22121\n. (27)\nMarginalizing k After obtaining expressions for P (k | x,m) (Eq. (21)) and P (z\u22c6occl | k, x,m) (Eq. (27)) we turn the attention to the summation over k in Eq. (17):\nP (z, z\u22c6occl | x,m) = \u2211\nk\nP (z | x,m, z\u22c6occl, k)P (z \u22c6 occl | k, x,m)P (k | x,m) . (28)\nSplit this summation in two parts: one for k = 0, when there is no occlusion, and one for k \u2265 1, and substitute the expressions for P (k | x,m) and P (z | x,m, z\u22c6occl, k) given by Eq. (21) and Eq. (9), respectively:\nP (z, z\u22c6occl | x,m) = N (z; z \u22c6, \u03c3m)P (z \u22c6 occl | k = , x,m)P (k =  | x,m) +\nN (z; z\u22c6occl, \u03c3m)P (z \u22c6 occl | k \u2265 , x,m)P (k \u2265  | x,m)\n= N (z; z\u22c6, \u03c3m)P (z \u22c6 occl | k = , x,m)\n( \u2212 p\u2032 ) +\nN (z; z\u22c6occl, \u03c3m)\u03b1 (z \u22c6 occl | x,m) , (29)\nwhere \u03b1 (z\u22c6occl | x,m) = P (z \u22c6 occl | k \u2265 1, x,m)P (k \u2265 1 | x,m)\n= \u221e\u2211\nk=1\nP (z\u22c6occl | k, x,m) ( 1\u2212 p\u2032 ) p\u2032k. (30)\nSubstituting (27) into (30) results in:\n\u03b1 (z\u22c6occl | x,m) = \u221e\u2211\nk=1\nk 1\nz\u22c6\n( z\u22c6 \u2212 z\u22c6occl\nz\u22c6\n)k\u22121 ( 1\u2212 p\u2032 ) p\u2032k, (31)\nwhich is simplified using Eq. (114) in Appendix A:\n\u03b1 (z\u22c6occl | x,m) = 1\nz\u22c6\n( 1\u2212 p\u2032 ) p\u2032\n\u221e\u2211\nk=1\nk\n( z\u22c6 \u2212 z\u22c6occl z\u22c6 p\u2032 )k\u22121\n= p\u2032 (1\u2212 p\u2032)\nz\u22c6 [ 1\u2212 ( z\u22c6\u2212z\u22c6occl z\u22c6 p\u2032 )]2 . (32)\nSubstituting (32) into (29) gives:\nP (z, z\u22c6occl | x,m) = N (z; z \u22c6, \u03c3m)P (z \u22c6 occl | k = , x,m)\n( \u2212 p\u2032 ) +\nN (z; z\u22c6occl, \u03c3m) (\u2212 p\u2032) p\u2032\nz\u22c6 [ \u2212 ( z\u22c6\u2212z\u22c6occl z\u22c6 p\u2032 )] . (33)\nMarginalizing z\u22c6 occl Substituting (33) into (17) shows that only the integration over z\u22c6occl still has to be carried out:\nP (z | x,m) = (1\u2212 p\u2032)N (z; z\u22c6, \u03c3m) + p \u2032\n\u222b z\u22c6\nz\u22c6occl= N (z; z\u22c6occl, \u03c3m)\n\u2212 p\u2032\nz\u22c6 [ \u2212 ( z\u22c6\u2212z\u22c6 occl z\u22c6 p\u2032 )] dz\n\u22c6 occl .(34)\nThe first term of the right hand side is a Gaussian distribution around the ideal measurement, multiplied with the probability of no occlusion (k = 0). The second term is an integration over all possible positions of the occluding object of a scaled Gaussian distribution centered at the ideal measurement of the occluding object (z\u22c6occl). The scaling factor represents the probability that the occluding objects are located such that z\u22c6occl is measured. From Eq. (20) and Eq. (32) it follows that the scaling factor can be written as:\n\u03b1 (z\u22c6occl | x,m) = p (1\u2212 p)\nzmax\n[ 1\u2212 ( 1\u2212 z\u22c6 occl\nzmax\n) p ]2 , (35)\nwhich is independent of z\u22c6. Until now, no approximations where made to obtain Eq. (34) for the beam model P (z | x,m). The integral over the scaled Gaussian distributions however, cannot be obtained analytically. Therefore, a first approximation in the marginalization is made by neglecting the noise on the range measurement in case of occlusion, i.e.: N (z; z\u22c6occl, \u03c3m) \u2248 \u03b4 (z \u2212 z\u22c6occl). Using this approximation the second term in the right hand side of Eq. (34) becomes:\np\u2032 (1\u2212 p\u2032)\nz\u22c6 [ 1\u2212 z\n\u22c6\u2212z z\u22c6\np\u2032 ]2 =\np (1\u2212 p)\nzmax\n[ 1\u2212 (\n1\u2212 z zmax\n) p ]2 . (36)\nFig. 7 shows the quality of the approximation of the integral in Eq. ( 34) compared to a finite sum approximation with small step size. The approximation introduces a discontinuity around z = z\u22c6. Using the proposed approximation for the integral the resulting beam model is:\nP (z | x,m) =\n \n\n(1\u2212 p\u2032)N (z; z\u22c6, \u03c3m) + p \u2032 (\u2212p \u2032)\nz\u22c6[\u2212( z \u22c6 \u2212z\nz\u22c6 p\u2032)]\n if z \u2264 z\u22c6\n(1\u2212 p\u2032)N (z; z\u22c6, \u03c3m) otherwise, (37)\nas shown in Fig. 8. The RBBM can be written as a mixture of two components:\nP (z | x,m) = \u03c01Phit (z | x,m) + \u03c02Poccl (z | x,m) , (38)\nwith \u03c01 = ( 1\u2212 p\u2032 ) (39)\n\u03c02 = p \u2032 (40)\nPhit (z | x,m) = N (z; z \u22c6, \u03c3m) (41)\nPoccl (z | x,m) =\n \n\n1 z\u22c6\n1\u2212p\u2032 [1\u2212( z \u22c6 \u2212z\nz\u22c6 p\u2032)]\n2 if 0 \u2264 z \u2264 z \u22c6\n0 otherwise. (42)"}, {"heading": "3.4 Extra Components", "text": "Occasionally, range finders produce unexplainable measurements, caused by phantom readings when sonars bounce off walls, or suffer from cross-talk (Thrun et al., 2005). Furthermore additional uncertainty on the measurements is caused by (i) uncertainty in the sensor position, (ii) inaccuracies of the world model and (iii) inaccuracies of the sensor itself. These unexplainable measurements are modeled using a uniform distribution spread over the entire measurement range [0, zmax]:\nPrand (z | x,m) =\n{ 1\nzmax if 0 \u2264 z \u2264 zmax, 0 otherwise. (43)\nFurthermore, sensor failures typically produce max-range measurements, modeled as a point-mass distribution centered around zmax:\nPmax (z | x,m) = I (zmax) =\n{\n1 if z = zmax, 0 otherwise. (44)\nThese two extra components can be added to Eq. (38), resulting in the final RBBM:\nP (z | x,m) = \u03c01 Phit (z | x,m) + \u03c02 Poccl (z | x,m) + \u03c03 Prand (z | x,m) + \u03c04 Pmax (z | x,m) , (45) where \u03c03 and \u03c04 are the probabilities that the range finder returns an unexplainable measurement and a maximum reading, respectively. Furthermore,\n\u03c01 = ( 1\u2212 p\u2032 ) (1\u2212 \u03c03 \u2212 \u03c04) and (46) \u03c02 = p \u2032(1\u2212 \u03c03 \u2212 \u03c04), (47)\nwhile Phit (z | x,m), Poccl (z | x,m), Prand (z | x,m) and Pmax (z | x,m) are given by (41), (42), (43) and (44) respectively."}, {"heading": "3.5 Assumptions and Approximations", "text": "This section summarizes the assumptions and approximations made to arrive at the RBBM of Eq. (45). Section 3.2 makes four assumptions:\n(i) the probability of the number of unmodeled objects decreases exponentially, Eq. (2); (ii) the unmodeled object\u2019s position is uniformly distributed over the measurement beam (Fig. 3, Eq. (3)); (iii) the positions of the unmodeled objects are independent, Eq. (4); and (iv) the measurement noise is zero mean normally distributed with standard deviation \u03c3m (Eq. 9). Furthermore, Section 3.3 makes one approximation to obtain an analytical expression by neglecting the noise on the range measurement in case of occlusion (Eq. (34))."}, {"heading": "3.6 Interpretation", "text": "The following paragraphs give some insights in the RBBM and its derivation. The mixture representation (45) shows the four possible causes of a range measurement: a hit with the map, a hit with an unmodeled object, an unknown cause resulting in a random measurement and a sensor failure resulting in a maximum reading measurement.\nThe derivation of Section 3.3 shows that the position of each of the occluding objects is uniformly distributed between the sensor and the ideally measured object in the environment (Eq. (15), Fig. 6). This is perfectly reasonable considering the assumption of uniformly distributed unmodeled objects.\nFurthermore, some insights are provided concerning \u03b1 (z\u22c6occl | x,m) (Eq. (35), Fig. 7), the probability that the occluding objects are located such that z\u22c6occl is measured. First of all, this probability is independent of the location of the ideally measured object in the environment (z\u22c6) (except that this probability is zero for z > z\u22c6). This agrees with intuition, since one expects the measurements caused by the occluding objects to be independent of z\u22c6, the measurement in the case of no occlusion. Second, the probability of sensing unmodeled objects decreases with the range, as expected. This is easily explained with the following thought experiment: if two objects are present with the same likelihood in the perception field of the range finder, and the first object is closest to the range sensor, then the sensor is more likely to measure the first object. To measure the second object, the second object\nshould be present and the first object should be absent (Thrun et al., 2005). Moreover, the rate of decrease of the likelihood of sensing unmodeled objects is only dependent on p, the degree of appearance of unmodeled objects.\nThe probability of measuring a feature of the map, and therefore the integral under the scaled Gaussian (1\u2212p\u2032)Phit (z | x,m) (45), decreases with the expected range. This is easily explained since the probability that the map is not occluded decreases when the feature is located further away.\nFinally, the discontinuity of the RBBM (Fig. 8) was shown to be caused by the only approximation made (Section 3.5). Since the state of the art range sensors are very accurate, neglecting the measurement noise on the measurement of an occluding object is an acceptable approximation. This is also shown by the experiments presented in Section 5.\nWith respect to the state of the art beam model of Thrun et al. (2005), the model proposed here, Eq. (45), has: (i) a different functional form for the probability of range measurements caused by unmodeled objects, (ii) an intuitive explanation for the discontinuity encountered in the cited paper, and (iii) a reduction in the number of model parameters. Thrun et al. find that Poccl (z | x,m) has an exponential distribution. This exponential distribution results from the following underlying assumptions (although not revealed by the authors): (i) the unmodeled objects are equally distributed in the environment and (ii) a beam is reflected with a constant probability at any range. The last assumption equals assuming that the probability that an unmodeled object is located at a certain distance is constant. This assumption fails to capture that the number of unmodeled object is finite and that it is more probable to have a limited number of unmodeled objects than a huge number of them. While we also assume that unmodeled objects are equally distributed in the environment (Eq. (3)), we assume that the number of unmodeled objects is geometrically distributed (Eq. (2)) capturing the finiteness of the number of unmodeled objects and the higher probability of a smaller number of unmodeled objects. The modeling of the finiteness of the number of unmodeled objects and the higher probability of a smaller number of unmodeled objects results in a quadratic decay of Poccl (z | x,m), instead of the exponential decay of Poccl (z | x,m) found by Thrun et al..\nAs stated in the previous paragraph, the discontinuity of the RBBM (Fig. 8) is caused by an approximation. While Thrun\u2019s model considers the rate of decay of Poccl (z | x,m) to be independent of \u03c02, the probability of an occlusion, it is shown here that they both depend on the same parameter p\u2032 (Eq. (42), Eq. (47)). Therefore the RBBM has fewer parameters than Thrun\u2019s model."}, {"heading": "3.7 Validation", "text": "The goal of this section is to show by means of a Monte Carlo simulation2 that the RBBM, Eq. (45), agrees with the Bayesian network in Fig. 1. A Monte Carlo simulation is an approximate inference method for Bayesian networks. The idea behind the Monte Carlo simulation is to draw random configurations of the network variables Z, X, M , N , XN = {XNj}j=1:n, K, XK = {XKi}i=1:k and Z \u22c6 occl and to do this a sufficient number of times. Random configurations are selected by ancestral sampling (Bishop, 2006), i.e. by successively sampling\n2. A Monte Carlo simulation is also known as stochastic simulation in the Bayesian network literature (Jensen & Nielsen, 2007).\nthe states of the variables following the causal model defined by the directed acyclic graph of the Bayesian network.\nFig. 9 shows that the RBBM agrees with a Monte Carlo simulation with 500 samples of the proposed Bayesian network."}, {"heading": "4. Variational Bayesian Learning of the Model Parameters", "text": "The RBBM, Eq. (45), depends on four independent model parameters:\n\u0398 = [ \u03c3m, p \u2032, \u03c03, \u03c04 ] , (48)\nwhile zmax is a known sensor characteristic. This set of parameters has a clear physical interpretation; \u03c3m is the standard deviation of the zero mean Gaussian measurement noise in Eq. (9) governing Phit (z | x,m) (Eq. (41)); p\n\u2032, defined in Eq. (21), is the probability that the map is occluded (P (k \u2265 1 | x,m)); \u03c03 and \u03c04 are the probabilities that the range finder returns an unexplainable measurement (unknown cause) and a maximum reading (sensor failure), respectively.\nAn alternative non-minimal set of parameters containing all the mixing coefficients \u03c0 = [\u03c01, \u03c02, \u03c03, \u03c04] could be used: \u0398 \u2032 = [\u03c3m,\u03c0], provided that the constraint:\nS=4\u2211\ns=1\n\u03c0s = 1, (49)\nis taken into account. The set of minimal parameters \u0398 straightforwardly follows from the non-minimal set \u0398\u2032 since:\np\u2032 = \u03c02\n1\u2212 \u03c03 \u2212 \u03c04 , (50)\nas can be seen from Eq. (47).\nThe physical interpretation of the parameters \u0398 allows us to initialize them by hand with plausible values. However, another, more flexible way is to learn the model parameters from actual data containing J measurements Z = {z\u03b9}\u03b9=1:J with corresponding states X = {x\u03b9}\u03b9=1:J and map m. Furthermore, learning the model parameters is also a validation for the proposed analytical model: if the learning algorithm succeeds in finding model parameters such that the resulting distribution gives a good explanation of the data, the analytical model is likely to agree well with reality.\nIn this paper two different estimators3, a maximum likelihood (ML) (Dempster, Laird, & Rubin, 1977; McLachlan & Krishnan, 1997; Bishop, 2006) and a variational Bayesian (VB) (Beal & Ghahramani, 2003; Bishop, 2006) estimator, are presented to learn the model parameters from data. Section 4.1 derives a maximum likelihood estimator, which is a known approach for this problem, but reformulated for the RBBM. This ML estimator only provides point estimates of the parameters and leads to overfitting since the likelihood function is generally higher for more complex model structures. Therefore, we propose a variational Bayesian (VB) estimator in Section 4.2, which is a new approach for learning the parameters for beam models. The VB estimator is a fully Bayesian learning approach; priors over the unknown parameters are included, complex (overfitting) models are punished, and a full probability distribution over the parameters is obtained.\n3. This paper approximately follows the notation by Bishop (2006)."}, {"heading": "4.1 Maximum Likelihood Learning", "text": "A maximum likelihood estimator is proposed to identify the model parameters \u0398 that maximize the likelihood of the data Z with corresponding X and map m:\n\u0398 = argmax \u0398 logP (Z |X,m,\u0398) . (51)\nWhen using the mixture representation of the RBBM (Eq. (45)), the estimation problem can be formulated as finding the ML estimates for the parameters \u0398\u2032 = [\u03c3m,\u03c0] provided that the constraint in Eq. (49) is included. In general it is not known which of the four possible causes actually caused each of the measurements. In that case the ML estimation problem is difficult and lacks a closed-form solution. If however, the corresponding causes of each of the measurements are known, the solution is easily obtained in closed form. Therefore, introduce a latent correspondence variable d = [d1, d2, d3, d4], representing the unknown cause, using a 1-of-S representation. The elements ds of d give the probability that the measurement is a result of the s\u2019th cause. The graphical representation of the mixture formulation including the latent correspondence d variable is shown in Fig. 10. Although the ML estimation problem lacks a closed-form solution due to the unknown correspondences, an expectation-maximization approach (EM) can solve the problem by iterating an expectation and a maximization step. The expectation step calculates an expectation for the correspondence variables ds while the maximization step computes the other model parameters under these expectations.\nAlgorithm 1 ML estimator for model parameters\nwhile convergence criterion not satisfied do for all z\u03b9 in Z, with \u03b9 = 1 : J , where J = |Z| \u22121 do\ncalculate z\u22c6m \u03b7 = [ \u03c01 Phit (z\u03b9 | x\u03b9,m) + \u03c02 Poccl (z\u03b9 | x\u03b9,m) + \u03c03 Prand (z\u03b9 | x\u03b9,m)+\n\u03c04 Pmax (z\u03b9 | x\u03b9,m)] \u22121\n\u03b3 (d\u03b91) = \u03b7 \u03c01 Phit (z\u03b9 | x\u03b9,m) \u03b3 (d\u03b92) = \u03b7 \u03c02 Poccl (z\u03b9 | x\u03b9,m) \u03b3 (d\u03b93) = \u03b7 \u03c03 Prand (z\u03b9 | x\u03b9,m) \u03b3 (d\u03b94) = \u03b7 \u03c04 Pmax (z\u03b9 | x\u03b9,m)\nend for \u03c01 = J \u22121 \u2211\n\u03b9 \u03b3 (d\u03b91) \u03c02 = J \u22121 \u2211\n\u03b9 \u03b3 (d\u03b92) \u03c03 = J \u22121 \u2211\n\u03b9 \u03b3 (d\u03b93) \u03c04 = J \u22121 \u2211\n\u03b9 \u03b3 (d\u03b94) p\u2032 = \u03c021\u2212\u03c03\u2212\u03c04\n\u03c3m =\n\u221a P\n\u03b9 \u03b3(d\u03b91)(z\u03b9\u2212z \u22c6 \u03b9 ) 2 P\n\u03b9 \u03b3(d\u03b91)\nend while return \u0398 = [\u03c3m, p \u2032, \u03c03, \u03c04]\nThe marginal distribution over the correspondence variable d is specified in terms of the mixing coefficients \u03c0s such that:\nP (ds = 1) = \u03c0s, (52)\nwhere the parameters \u03c0 must satisfy the following two conditions: {\n0 \u2264 \u03c0s \u2264 1, \u2211S\ns=1 \u03c0s = 1. (53)\nSince d uses a 1-of-S representation, the marginal distribution can be written as:\nP (d) = S\u220f\ns=1\n\u03c0dss . (54)\nThe EM-algorithm expresses the complete-data log likelihood, i.e. the log likelihood of the observed and the latent variables:\nlogP ( Z,D |X,\u0398\u2032,m ) =\nJ\u2211\n\u03b9=1\n(d\u03b91 (log \u03c01 + logPhit (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7\nd\u03b92 (log \u03c02 + logPoccl (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7\nd\u03b93 (log \u03c03 + logPrand (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7\nd\u03b94 (log \u03c04 + logPmax (z\u03b9 | x\u03b9,m))) , (55)\nwhere Z = {z\u03b9}\u03b9=1:J is the vector containing the observed data and D = {d\u03b9} is the vector containing the matching correspondences.\nExpectation step: Taking the expectation of the complete-data log likelihood in Eq. (55) with respect to the posterior distribution of the latent variables gives:\nQ(\u0398\u2032,\u0398\u2032old) = ED [ logP ( Z,D |X, \u0398\u2032,m )]\n= J\u2211\n\u03b9=1\n(\u03b3 (d\u03b91) (log \u03c01 + logPhit (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7\n\u03b3 (d\u03b92) (log \u03c02 + logPoccl (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7\n\u03b3 (d\u03b93) (log \u03c03 + logPrand (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7\n\u03b3 (d\u03b94) (log \u03c04 + logPmax (z\u03b9 | x\u03b9,m))) , (56)\nwhere \u03b3 (d\u03b9s) = E [d\u03b9s] is the discrete posterior probability, or responsibility (Bishop, 2006), of cause s for data point z\u03b9. In the E-step, these responsibilities are evaluated using Bayes\u2019 theorem, which takes the form:\n\u03b3 (d\u03b91) = E [d\u03b9] = \u03c01Phit (z\u03b9 | x\u03b9,m)\nNorm , (57)\n\u03b3 (d\u03b92) = E [d\u03b9] = \u03c02Poccl (z\u03b9 | x\u03b9,m)\nNorm , (58)\n\u03b3 (d\u03b93) = E [d\u03b9] = \u03c03Prand (z\u03b9 | x\u03b9,m)\nNorm , and (59)\n\u03b3 (d\u03b94) = E [d\u03b9] = \u03c04Pmax (z\u03b9 | x\u03b9,m)\nNorm , (60)\nwhere Norm is the normalization constant:\nNorm = \u03c01Phit (z\u03b9 | x\u03b9,m) + \u03c02Poccl (z\u03b9 | x\u03b9,m) + \u03c03Prand (z\u03b9 | x\u03b9,m) + \u03c04Pmax (z\u03b9 | x\u03b9,m) . (61)\nTwo measures are derived from the responsibilities:\nJs = J\u2211\n\u03b9=1\n\u03b3 (d\u03b9s) , and (62)\nz\u0304s = 1\nJs\nJ\u2211\n\u03b9=1\n\u03b3 (d\u03b9s) z\u03b9, (63)\nwhere Js is the effective number of data points associated with cause s, and z\u0304s is the mean of the effective data points associated with cause s.\nMaximization step: In the M-step the expected complete-data log likelihood in Eq. (56) is maximized with respect to the parameters \u0398\u2032 = [\u03c3m,\u03c0]:\n\u0398\u2032new = argmax \u0398\u2032 Q(\u0398\u2032,\u0398\u2032old). (64)\nMaximization with respect to \u03c0s using a Lagrange multiplier to enforce the constraint \u2211\ns \u03c0s = 1 results in:\n\u03c0s = Js\nJ , (65)\nwhich is the effective fraction of points in the data set explained by cause s. Maximization with respect to \u03c3m results in:\n\u03c3m =\n\u221a \u221a \u221a \u221a 1\nJ1\nJ\u2211\n\u03b9=1\n\u03b3d\u03b91 (z\u03b9 \u2212 z \u22c6 \u03b9 )\n2. (66)\nAlgorithm 1 summarizes the equations for the ML estimator, further on called ML-EM algorithm."}, {"heading": "4.2 Variational Bayesian Learning", "text": "The ML estimator only provides point estimates of the parameters and is sensitive to overfitting (Bishop, 2006). Therefore, we propose a variational Bayesian (VB) estimator, which is a new approach for learning the parameters for beam models. The VB estimator is a fully Bayesian learning approach; priors over the unknown parameters are included, complex (overfitting) models are punished, and a full probability distribution over the parameters is obtained. The VB estimator has only a little computational overhead as compared to the ML estimator (Bishop, 2006).\nThe Bayesian approach attempts to integrate over the possible values of all uncertain quantities rather than optimize them as in the ML approach (Beal, 2003; Beal & Ghahramani, 2003). The quantity that results from integrating out both the latent variables and the\nparameters is known as the marginal likelihood4: P (Z) = \u222b P (Z |D,\u0398)P (D,\u0398) d(D,\u0398), where P (D,\u0398) is a prior over the latent variables and the parameters of the model. Integrating out the parameters penalizes models with more degrees of freedom, since these models can a priori model a larger range of data sets. This property of Bayesian integrations is known as Occam\u2019s razor, since it favors simpler explanations for the data over complex ones (Jeffreys & Berger, 1992; Rasmussen & Ghahramani, 2000).\nUnfortunately, computing the marginal likelihood, P (Z), is intractable for almost all models of interest. The variational Bayesian method constructs a lower bound on the marginal likelihood, and attempts to optimize this bound using an iterative scheme that has intriguing similarities to the standard EM algorithm. To emphasize the similarity with ML-EM, the algorithm based on variational Bayesian inference is further on called VB-EM.\nBy introducing the distribution Q over the latent variables the complete log marginal likelihood can be decomposed as (Bishop, 2006):\nlogP (Z) = L (Q) +KL (Q||P ) , (67)\nwhere\nL (Q) =\n\u222b\nQ (D, \u0398) log\n( P (Z,D, \u0398)\nQ (D, \u0398)\n)\nd (D, \u0398) , (68)\nand KL (Q||P ) is the KL-divergence between Q and P . Since this KL-divergence is always greater or equal than zero, L (Q) is a lower bound on the log marginal likelihood. Maximizing this lower bound with respect to the distribution Q (D,\u0398) is equivalent to minimizing the KL-divergence. If any possible choice for Q (D,\u0398) is allowed, the maximum of the lower bound would occur when the KL-divergence vanishes, i.e. when Q (D,\u0398) is equal to the posterior distribution P (D,\u0398 | Z). Working with the true posterior distribution is however often intractable in practice. One possible approximate treatment considers only a restricted family of distributions Q (D,\u0398) and seeks the member of this family minimizing the KL-divergence. The variational Bayesian treatment uses a factorized approximation, in this case between the latent variables D and the parameter \u0398:\nQ (D,\u0398) = QD(D)Q\u0398 (\u0398) . (69)\nThe variational approach makes a free form (variational) optimization of L (Q) with respect to the distributions QD(D) and Q\u0398 (\u0398), by optimizing with respect to each of the factors in turn. The general expressions for the optimal factors are (Bishop, 2006):\nlogQ\u22c6D(D) = E\u0398 [logP (Z,D, \u0398)] + C te, and (70)\nlogQ\u22c6\u0398 (\u0398) = ED [logP (Z,D, \u0398)] + C te, (71)\nwhere \u22c6 indicates the optimality. These expressions give no explicit solution for the factors, because the optimal distribution for one of the factors depends on the expectation computed with respect to the other factor. Therefore an iterative procedure, similar to EM, that cycles through the factors and replaces each in turn with a revised optimal estimate is used.\n4. To avoid overloading the notation the conditioning on the map m and the positions X = {x\u03b9} associated with the data Z = {z\u03b9} is not explicitly written.\nIntroducing priors Since the variational Bayesian approach is a fully Bayesian approach, priors have to be introduced over the parameters \u0398\u2032\u2032 = [\u00b5, \u03c3m,\u03c0]. Remark that in the variational Bayesian estimator not only the standard deviation \u03c3m governing Phit (z | x,m) (Eq. (41)) is estimated but also the means, referred to as \u00b5 further on. Since the analysis is considerably simplified if conjugate prior distributions are used, a Dirichlet prior is chosen for the mixing coefficients \u03c0:\nP (\u03c0) = Dir (\u03c0|\u03b10) , (72)\nas well as an independent Gaussian-Wishart prior5 for the mean \u00b5 and the precision \u03bbm = \u03c3\u22121m of the Gaussian distribution Phit (z | x,m) (Eq. (41)):\nP (\u00b5, \u03bbm) = N ( \u00b5|\u00b5\u0304, (\u03b2\u03bbm) \u2212 ) W (\u03bbm|W, \u03bd) . (73)\n\u03b10 gives the effective prior number of observations associated with each component of the mixture. Therefore, if the value of \u03b10 is set small, the posterior distribution will be mainly influenced by the data rather than by the prior.\nExpectation step Using these conjugate priors, it can be shown that the factor Q\u22c6 D (D) can be written as:\nQ\u22c6D(D) = J\u220f\n\u03b9=1\nrd\u03b91\u03b91 r d\u03b92 \u03b92 r d\u03b93 \u03b93 r d\u03b94 \u03b94 , (74)\nwhere the quantities r\u03b9s are responsibilities analogous to the \u03b3\u03b9s of Eq. (57) and are given by:\nr\u03b9s = \u03c1\u03b9s\n\u03c1\u03b91 + \u03c1\u03b92 + \u03c1\u03b93 + \u03c1\u03b94 , (75)\nwhere\nlog \u03c1\u03b91 = E [log \u03c01] + E [logPhit (z\u03b9 | x\u03b9,m)] , (76)\nlog \u03c1\u03b92 = E [log \u03c02] + E [logPoccl (z\u03b9 | x\u03b9,m)] , (77)\nlog \u03c1\u03b93 = E [log \u03c03] + E [logPrand (z\u03b9 | x\u03b9,m)] , and (78)\nlog \u03c1\u03b94 = E [log \u03c04] + E [logPmax (z\u03b9 | x\u03b9,m)] . (79)\nThe above equations can be rewritten as:\nlog \u03c1\u03b91 = E [log \u03c01] + \n E [log |\u03bbm|]\u2212\n  log (\u03c0)\u2212   E\u00b5,\u03bbm\n[\n(z\u03b9 \u2212 \u00b5) T \u03bb (z\u03b9 \u2212 \u00b5)\n]\n, (80)\nlog \u03c1\u03b92 = E [log \u03c02] + logPoccl (z\u03b9 | x\u03b9,m) , (81)\nlog \u03c1\u03b93 = E [log \u03c03] + logPrand (z\u03b9 | x\u03b9,m) , and (82)\nlog \u03c1\u03b94 = E [log \u03c04] + logPmax (z\u03b9 | x\u03b9,m) , (83)\n5. The parameters are as defined by Bishop (2006).\nin which the expectations can be calculated as follows:\nE [log \u03c0s] = \u03a8 (\u03b1s)\u2212\u03a8(\u03b11 + \u03b12 + \u03b13 + \u03b14) , (84)\nE [log |\u03bbm|] = \u03a8 (\u03bd\n2\n)\n+ log 2 + log |W |, and (85)\nE\u00b5,\u03bbm\n[\n(z\u03b9 \u2212 \u00b5) T \u03bb (z\u03b9 \u2212 \u00b5)\n]\n= \u03b2\u22121 + \u03bd (z\u03b9 \u2212 \u00b5\u0304) T W (z\u03b9 \u2212 \u00b5\u0304) , (86)\nwhere \u03a8 is the digamma function.\nThree measures are derived from the responsibilities:\nJs =\nJ\u2211\n\u03b9=1\nr\u03b9s, (87)\nz\u0304s = 1\nJs\nJ\u2211\n\u03b9=1\nr\u03b9sz\u03b9, and (88)\nCs = 1\nJs\nJ\u2211\n\u03b9=1\nr\u03b9s (z\u03b9 \u2212 z\u0304s) (z\u03b9 \u2212 z\u0304s) T , (89)\nwhere Js is the effective number of data points associated with cause s, z\u0304s is the mean of the effective data points associated with cause s and Cs the covariance of the effective data points associated with cause s. Due to the similarity with the E-step of the EM-algorithm, the step of calculating the responsibilities in the variational Bayesian inference is known as the variational E-step.\nMaximization step In accordance with the graphical representation in Fig. 10, it can be shown that the variational posterior Q\u0398 (\u0398) factorizes as Q (\u03c0)Q (\u00b51, \u03c3m) and that the first optimal factor is given by a Dirichlet distribution:\nQ\u22c6 (\u03c0) = Dir (\u03c0|\u03b1) , (90)\nwith\n\u03b1s = \u03b10 + Js. (91)\nThe second optimal factor is given by a Gaussian-Wishart distribution:\nQ\u22c6 (\u00b51, \u03bbm) = N ( \u00b5|\u00b5\u0304, (\u03b2\u03bbm) \u2212 ) W (\u03bbm|W, \u03bd) , (92)\nwith\n\u03b2 = \u03b20 + J1, (93) \u00b5\u0304 = 1\n\u03b2 (\u03b20\u00b5\u03040 + J1z\u03041) , (94)\nW\u22121 = W\u221210 + J1C1 + \u03b20J1\n\u03b20 + J1 (z\u03041 \u2212 \u00b5\u03040) (z\u03041 \u2212 \u00b5\u03040)\nT , and (95)\n\u03bd = \u03bd0 + J1. (96)\nThese update equations are analogous to the M-step of the EM-algorithm for the maximum likelihood solution and is therefore known as the variational M-step. The variational M-step computes a distribution over the parameters (in the conjugate family) rather than a point estimate as in the case of the maximum likelihood estimator. This distribution over the parameters allows us to calculate a predictive density P (z | Z).\nDue to the use of conjugate priors, the integrals in the predictive density can be calculated analytically:\nP (z |Z) = \u03b11St\n( z|\u00b5\u0304, \u03bd\u03b21+\u03b2W, \u03bd ) + \u03b12Poccl (z | x,m) + \u03b13Prand (z | x,m) + \u03b14Pmax (z | x,m)\n\u03b11 + \u03b12 + \u03b13 ,\n(97) where St (.) is a Student\u2019s t-distribution. When the size J of the data is large, the Student\u2019s t-distribution approximates a Gaussian and the predictive distribution can be rewritten as:\nP (z |Z) = \u03b11N (z|\u00b5, \u03bbm) + \u03b1Poccl (z | x,m) + \u03b1Prand (z | x,m) + \u03b1Pmax (z | x,m)\n\u03b11 + \u03b12 + \u03b13 + \u03b14 . (98)\nIf point estimates are desired for the parameters, maximum a posteriori estimates can be obtained as follows:\n\u03c0\u0302s = E [\u03c0s] = \u03b1s\n\u03b1 + \u03b1 + \u03b1 + \u03b1 (99)\n\u03c3m =\n( \u03bd\u03b2\n1 + \u03b2 W\n)\u2212 1 2\n, and (100)\np\u2032 = \u03c0\u03022\n1\u2212 \u03c0\u03023 \u2212 \u03c0\u03024 . (101)\nAlgorithm 2 summarizes the equations for the VB-EM estimator."}, {"heading": "5. Experiments", "text": "The goal of this section is threefold: (i) to learn the model parameters (Eq. (48)) of the RBBM (Eq. (45)) from experimental data, (ii) to compare the results of the proposed ML-EM and VB-EM estimator (Section 4), and (iii) to compare the results of the proposed estimators with the learning approach of Thrun\u2019s model proposed by Thrun et al. (2005). To this end two experimental setups from different application areas in robotics are used. The data for the first learning experiment is gathered during a typical mobile robot application in which the robot is equipped with a laser scanner and is travelling in an office environment. The data for the second learning experiment is gathered during a typical industrial pickand-place operation in a human populated environment. A laser scanner is mounted on the industrial robot to make it aware of people and other unexpected objects in the robot\u2019s workspace.\nTo see how well the learned model explains the experiment, the learned continuous pdf P (z | x,m,\u0398) of Eq. (45) has to be compared with the discrete pdf of the experimental data (histogram) H (z). To this end, the learned pdf is first discretized using the same bins {zf}f=1:F as the experimental pdf. To quantize the difference between the learned and the\nAlgorithm 2 Variational Bayesian estimator for model parameters\nwhile convergence criterion not satisfied for all z\u03b9 in Z, with \u03b9 = 1 : J , where J = |Z| \u22121\ncalculate z\u22c6m \u03c1\u03b91 = exp [ \u03a8(\u03b11)\u2212\u03a8(\u03b11 + \u03b12 + \u03b13 + \u03b14) + 1 2 ( \u03a8 ( \u03bd 2 ) + log 2 + log |W | ) \u2212 12 log (2\u03c0) . . .\n\u00b7 \u00b7 \u00b7 \u2212 12\n(\n\u03b2\u22121 + \u03bd (z\u03b9 \u2212 \u00b5\u0304) T W (z\u03b9 \u2212 \u00b5\u0304)\n)]\n\u03c1\u03b92 = exp [\u03a8 (\u03b12)\u2212\u03a8(\u03b11 + \u03b12 + \u03b13 + \u03b14) + logPoccl (z\u03b9 | x,m)] \u03c1\u03b93 = exp [\u03a8 (\u03b13)\u2212\u03a8(\u03b11 + \u03b12 + \u03b13 + \u03b14) + logPrand (z\u03b9 | x,m)] \u03c1\u03b94 = exp [\u03a8 (\u03b14)\u2212\u03a8(\u03b11 + \u03b12 + \u03b13 + \u03b14) + logPmax (z\u03b9 | x,m)] \u03b7 = \u03c1\u03b91 + \u03c1\u03b92 + \u03c1\u03b93 + \u03c1\u03b94 r\u03b91 = \u03b7\n\u22121\u03c1\u03b91 r\u03b92 = \u03b7\n\u22121\u03c1\u03b92 r\u03b93 = \u03b7\n\u22121\u03c1\u03b93 r\u03b94 = \u03b7\n\u22121\u03c1\u03b94 end for J1 = \u2211J \u03b9=1 r\u03b91 J2 = \u2211J \u03b9=1 r\u03b92 J3 = \u2211J \u03b9=1 r\u03b93 J4 = \u2211J \u03b9=1 r\u03b94 z\u03041 = 1 J1 \u2211J j=1 r\u03b91z\u03b9 C1 = 1 J1 \u2211J \u03b9=1 r\u03b91 (z\u03b9 \u2212 z\u03041) (z\u03b9 \u2212 z\u03041) T , \u03b11 = \u03b10 + J1. \u03b12 = \u03b10 + J2. \u03b13 = \u03b10 + J3. \u03b14 = \u03b10 + J4. \u03b2 = \u03b20 + J1 \u00b5\u0304 = 1\n\u03b2 (\u03b20\u00b5\u03040 + J1z\u03041)\nW\u22121 = W\u221210 + J1C1 + \u03b20J1 \u03b20+J1 (z\u03041 \u2212 \u00b5\u03040) (z\u03041 \u2212 \u00b5\u03040) T \u03bd = \u03bd0 + J1\n\u03c01 = \u03b11 \u03b11+\u03b12+\u03b13+\u03b14 \u03c02 =\n\u03b12 \u03b11+\u03b12+\u03b13+\u03b14\n\u03c03 = \u03b13 \u03b11+\u03b12+\u03b13+\u03b14 \u03c04 =\n\u03b14 \u03b11+\u03b12+\u03b13+\u03b14\np\u2032 = \u03c021\u2212\u03c03\u2212\u03c04 \u03c3m = ( \u03bd\u03b2 1+\u03b2W )\u2212 1 2\nend while return {\u03b11, \u03b12, \u03b13, \u03b14, \u03b2, \u00b5\u0304,W, \u03bd,\u0398 \u2032\u2032 = [\u00b5, \u03c3m, p \u2032, \u03c03, \u03c04]}\nexperimental pdf two \u2018distance\u2019 measures are used: the discrete KL-divergence:\nd1 = KL (H||P ) \u2248 F\u2211\nf=1\nH (zf ) log H (zf )\nP (zf | x,m,\u0398) , (102)\nand the square root of the discrete Hellinger distance:\nd2 = DH (H||P ) \u2248\n\u221a \u221a \u221a \u221a F\u2211\nf=1\n(\nH (zf ) 1 2 \u2212 P (zf | x,m,\u0398) 1 2 )2 . (103)\nThe latter is known to be a valid symmetric distance metric (Bishop, 2006)."}, {"heading": "5.1 First Learning Experiment", "text": "In a first learning experiment, the experimental data reported by Thrun et al. (2005) is used. The data consists of two series of measurements obtained with a mobile robot traveling through a typical office environment. From the set of measurements, 10000 measurements that are centered around two different expected ranges, are selected. The two obtained sets with different expected ranges are shown in Fig. 11. The parameters of the learning algorithms are listed in Table 1. Fig. 12 and Table 2 show the results of the ML-EM and VB-EM estimators for the RBBM compared to the results of the ML estimator for Thrun\u2019s model (Thrun et al., 2005) for these two sets. The results are obtained by running the learning algorithms for 30 iteration steps.\nThe proposed ML-EM and VB-EM estimator outperform the ML-EM estimator for Thrun\u2019s model for the studied data sets. Despite the reduced number of parameters of the RBBM compared to Thrun\u2019s model (Section 3.6), the RBBM has at least the same representational power."}, {"heading": "5.2 Second Learning Experiment", "text": "The data for the second learning experiment is gathered during the execution of a typical industrial pick-and-place operation in a human-populated environment. A Sick LMS 200 laser scanner is mounted on the first axis of an industrial Kuka 361 robot (Fig. 13). The laser scanner provides measurements of the robot environment and therefore of people and other unexpected objects in the robot\u2019s workspace. Processing these measurements is a first step towards making industrial robots aware of their possibly changing environment and as such of moving these robots out of their cages.\nIn a first step, a map (Fig. 14) is build of the robot\u2019s static environment, i.e. without any unexpected objects or people moving around, by rotating the first axis of the industrial robot. Next, the robot performs a pick-and-place operation while a number of people are walking around at random in the robot environment. Different sets of measurements are acquired each with a different number of people. Similar to the first learning experiment, measurements are selected centered around different expected ranges from the acquired data. The studied expected ranges in the second learning experiment range from 3.0m to 4.5m in steps of 0.1m (Fig. 14). For safety reasons, people were not allowed to move closer than 1m to the robot, i.e. the safety region (Fig. 14). Therefore, measurements smaller than 1m are discarded.\nFrom the data, the model parameters are learning using the same learning parameters as in the first learning experiment (Table. 1).\nTable 3 shows the Kullback Leibler divergence (Eq. (102)) and the Hellinger distance (Eq. (103)) averaged for the studied expected range for the different set of measurements after running the ML-EM and VB-EM estimators for the RBBM and the ML estimator for Thrun\u2019s model (Thrun et al., 2005). The results were obtained after running each of the learning algorithms for 30 iteration steps.\nThe proposed ML-EM and VB-EM estimator outperform the ML-EM estimator for Thrun\u2019s model for the studied data sets. Despite the reduced number of parameters of the RBBM compared to Thrun\u2019s model (Section 3.6), the RBBM has at least the same representational power."}, {"heading": "6. Adaptive Full Scan Model", "text": "This section extends the RBBM to an adaptive full scan model for dynamic environments; adaptive, since it automatically adapts to the local density of samples when using samplebased representations; full scan, since the model takes into account the dependencies between individual beams.\nIn many applications using a range finder, the posterior is approximated by a finite set of samples (histogram filter, particle filters). The peaked likelihood function associated with a range finder (small \u03c3m due to its accuracy) is problematic when using such finite set of samples. The likelihood P (z | x,m) is evaluated at all samples, which are approximately distributed according to the posterior estimate. Basic sensor models typically assume that the estimate x and the map m are known exactly, that is, they assume that one of the samples corresponds to the true value. This assumption, however, is only valid in the limit of infinitely many samples. Otherwise, the probability that a value exactly corresponds to the true location is virtually zero. As a consequence, these peaked likelihood functions do not adequately model the uncertainty due to the finite, sample-based representation of the posterior (Pfaff et al., 2007). Furthermore, the use of a basic range finder model typically results in even more peaked likelihood models, especially when using a large number of beams per measurement, due to multiplication of probabilities. In practice, the problem\nof peaked likelihoods, is dealt with in various ways: sub-sampling the measurement (fewer beams); introducing minimal likelihoods for beams; inflating the measurement uncertainty; or other means of regularization of the resulting likelihoods. These solutions are not satisfactory however, since the additional uncertainty due to the sample-based representation is not known in advance. The additional uncertainty strongly varies with the number of samples and the uncertainty of the estimate (Pfaff et al., 2006). Fox (2003) proposes to dynamically adapt the number of samples by means of KLD sampling (KLD stands for Kullback-Leibler divergence). For very peaked likelihoods however, this might result in a huge number of samples. Lenser and Veloso (2000) and Thrun, Fox, Burgard, and Dellaert (2001) ensure that a critical mass of samples is located at the important parts of the state space by sampling from the observation model. Sampling from the observation model however, is often only possible in an approximate and inaccurate way. Pfaff et al. (2006) introduced an adaptive beam model for dynamic environments, which explicitly takes location uncertainty due to the sample-based representation into account. They compute the additional uncertainty due to the sample-based representation, using techniques from density estimation. When evaluating the likelihood function at a sample, they consider a certain region around the sample, depending on the sample density at that location. Then, depending on the area covered by the sample, the variance of the Gaussian, \u03c3m, governing the beam model in Eq. (38), is calculated for each sample. As a result, the beam model automatically adapts to the local density of samples. Such a location dependent model results in a smooth likelihood function during global localization and a more peaked function during position tracking without changing the number of samples.\nPlagemann et al. (2007) and Pfaff et al. (2007) showed that by considering a region around samples, the individual beams become statistically dependent. The degree of dependency depends on the geometry of the environment and on the size and location of the considered region. Beam models, such as the RBBM, implicitly assume however that the beams are independent, that is:\nP (z | \u03b8, x,m) = B\u220f\nb=1\nP (zb | \u03b8b, x,m) , (104)\nwhere z = {zb}b=1:B and \u03b8 = {\u03b8b}b=1:B are the vectors containing the measured ranges and the angles of the different beams respectively; zb is the range measured at the beam with angle \u03b8b; B is the total number of beams and P (zb | \u03b8b, x,m) is for instance the RBBM (Eq. (45)). By neglecting the dependency between beams, the resulting likelihoods P (z | \u03b8, x,m) are overly peaked. Models taking into account the dependencies between beams consider the full range scan and are therefore called full scan models further on. The full scan models proposed by Plagemann et al. (2007) and Pfaff et al. (2007) both assume that the beams of a range scan are jointly Gaussian distributed. The off-diagonal elements of the covariance matrix associated with the Gaussian distribution represent the dependency. To learn the model parameters, both methods draw samples from the region around a sample and perform ray-casting using these samples. Plagemann et al. train a Gaussian process which models the full scan, while Pfaff et al. directly provide a maximum likelihood estimate for the mean and covariance of the Gaussian.\nSection 6.1 shows that the dependency between beams may introduce multi-modality, even for simple static environments. Multi-variate Gaussian models as proposed by Plage-\nmann et al. (2007) and Pfaff et al. (2007) cannot handle this multi-modality. Therefore, a new sample-based method for obtaining an adaptive full scan model from a beam model, able to handle multi-modality, is proposed. Section 6.2 extends the adaptive full scan model for dynamic environments by taking into account non-Gaussian model uncertainty."}, {"heading": "6.1 Sample-based Adaptive Full Scan Model for Static Environments", "text": "Plagemann et al. (2007) and Pfaff et al. (2007) estimate the full scan model, P (z | x,m)6, based on a local environment U (x) of the exact estimate x:\nP (z | x,m) \u2248\n\u222b\nP (x\u0303 | x)Phit (z | x\u0303,m) dx\u0303, (105)\nwith P (x\u0303 | x) the distribution representing the probability that x\u0303 is an element of the environment U (x), i.e: x\u0303 \u2208 U (x). The environment U (x) is modeled as a circular region around x. Since this section does not consider the dynamics of the environment, only one component of the RBBM in Eq. (37) is used: Phit (z | x,m). The marginalization over the environment U (x) in Eq.(105) introduces dependencies between the measurements zb of the measurement vector z.\nThe environment U (x), as explained above, depends on the sample density around the sample x under consideration. Pfaff et al. (2006) proposed to use a circular region with diameter dU(x), which is a weighted sum of the Euclidean distance and the angular difference. Like Plagemann et al. (2007) and Pfaff et al. (2007), an approximation of the above likelihood can be estimated online for each sample x by simulating L complete range scans at locations drawn from U (x) using the given map m of the environment. Contrary to the multivariate Gaussian approximation proposed by Plagemann et al. and Pfaff et al., we propose a sample-based approximation, able to handle multi-modality. Sampling from the environment U (x) immediately results in a sample-based approximation of P (x\u0303 | x):\nP (x\u0303 | x) \u2248 1\nL\nL\u2211\nl=1\n\u03b4x\u0303(l) , (106)\nwhere \u03b4x\u0303(l) denotes the delta-Dirac mass located in x\u0303 (l), and the samples are distributed according to P (x\u0303 | x):\nx\u0303(l) \u223c P (x\u0303 | x) . (107)\nUsing this sample-based approximation of P (x\u0303 | x) the likelihood of Eq. (105) can be approximated as:\nP (z | x,m) \u2248 1\nL\nL\u2211\nl=1\nPhit\n( z | x\u0303(l),m ) . (108)\nSince this sample-based approximation has to be calculated online, the number of samples has to be limited. If the used environment U (x) is large, the resulting approximation will be\n6. To simplify the notation \u03b8 and \u03b8 are omitted from P (z | \u03b8, x,m) and P (zb | \u03b8b, x,m), respectively.\nbad. To smooth the undesired bumpy behavior due to the limited number of samples, the measurement noise \u03c3m governing Phit (z | x,m) in Eq. (45), is artificially increased depending on the size of U (x) by multiplying it with a factor:\n1 + C \u221a\ndU(x). (109)\nIn further experiments, C was set to 20."}, {"heading": "6.1.1 Experiment", "text": "A simple environment consisting of a rectangular \u2018room\u2019 with an object (a Kuka KR 15/2 robot) in the middle (Fig. 16) is used to show that the marginalization over (even small) U (x) to obtain the true likelihood not only introduces dependencies between the beams but also multi-modality. The U (x) results from a local uncertainty on the x- and y\u2212position of 0.01m and a rotational uncertainty of 5\u25e6. To obtain a reference, a Sick LMS 200 range finder is used to take a large number of measurements (L = 1500) at random locations sampled in U (x). To allow for exact positioning, the Sick LMS 200 is placed on a Kuka 361 industrial robot. The Sick LMS 200 range finder is connected to a laptop that controls the motion of the Kuka 361 industrial robot over the network using Corba-facilities in the Open Robot Control Software, Orocos (Bruyninckx, 2001; Soetens, 2006). A simplified map of the environment (Fig. 16) is built to simulate the 150 complete range scans needed to construct a full scan model. The marginal P (zb | x,m) of two selected beams are studied in more detail. The marginal likelihoods for the selected beam using the proposed sample-based approximation of Eq. (108) and the Gaussian approximation proposed by Pfaff et al. (2007), are compared in Fig. 17(b)-17(c). The histogram of the measurements of the selected beam in this figure clearly shows the multi-modality of the likelihood caused by the dependency between beams. In contrast to the Gaussian-based state of the art full scan model, the proposed sample-based approximation is able to handle the multi-modality of the range finder\ndata. Fig. 17(d) shows the difference for all beams between the experimentally obtained cumulative marginal (L = 1500) and the Gaussian-based and sample-based approximation for all beams. The mean difference with the experimental data for the sample-based approximation is 2.8 times smaller than the difference for the Gaussian-based approximation, even for the simple static environment of Fig. 16 and the small U (x)."}, {"heading": "6.2 Sample-based Adaptive Full Scan Model for Dynamic Environments", "text": "The adaptive beam model proposed by Pfaff et al. (2006) is suited for use in dynamic environments since it uses the four component mixture beam model (Thrun et al., 2005; Choset et al., 2005). To date however, the adaptive full scan likelihood models of Pfaff et al. (2007) and Plagemann et al. (2007) have not been adapted for dynamic environments. The assumption that the beams are jointly Gaussian distributed, unable to capture the nonGaussian uncertainty due to environment dynamics, prevents the straightforward extension for dynamic environments. In contrast, the sample-based approximation of the full scan likelihood, as proposed in Section 6.1, can be extended to include environment dynamics. To this end, replace Phit (z | x,m) in Eq. (105) and Eq. (108) by the full mixture of Eq. (38)."}, {"heading": "6.2.1 Experiment", "text": "Fig. 18(a) and Fig. 18(b) compare the marginals for the selected beams (Fig. 17(a)) obtained from the adaptive full scan model for dynamic environments using the proposed samplebased approximation and the Gaussian approximation proposed by Pfaff et al. (2007). In contrast to the Gaussian-based state of the art full scan model, the proposed sample-based approximation is able to handle the multi-modality of the range finder data. Fig. 18(c) shows a probability map of the adaptive full scan model (sample-based approximation) suited for dynamic environments for the example environment of Fig. 16. The probability map plots P (z | x,m) as a function of the position in the map and shows that the marginalization over the environment U (x) of a sample in Eq. (105) not only introduces dependency between beams but also introduces multi-modality."}, {"heading": "7. Discussion", "text": "This paper proposed and experimentally validated the RBBM, a rigorously Bayesian network model of a range finder adapted to dynamic environments. All modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. This approach resulted in a transparent and intuitive model. The rigorous modeling revealed all underlying assumptions and parameters. This way a clear physical interpretation of all parameters is obtained providing intuition for the parameter choices. In contrast to the model of Thrun et al. (2005), the assumption underlying the non-physical discontinuity in the RBBM is discovered. Furthermore, the paper proposes a different functional form for the probability of range measurements caused by unmodeled objects Poccl (z | x,m) (Eq. (45)), i.e. quadratic rather than exponential as proposed by Thrun et al. Furthermore, compared to the work of Thrun et al. (2005), Choset et al. (2005), Pfaff et al. (2006) the RBBM depends on fewer parameters, while maintaining the same representational power for experimental data. Bayesian modeling revealed that both the rate of decay of Poccl (z | x,m) and\nthe probability of an occluded measurement \u03c02 depend on one parameter p \u2032. State of the art sensor models however, assume independency of these two parameters. Finally, a maximumlikelihood and a variational Bayesian estimator (both based on expectation-maximization) were proposed to learn the model parameters of the RBBM. Learning the model parameters from experimental data benefits from the RBBM\u2019s reduced number of parameters. Using two sets of learning experiments from different application areas in robotics (one reported by Thrun et al. (2005)) the RBBM was shown to explain the obtained measurements at least as well as the state of the art model of Thrun et al.\nFurthermore, the paper extended the RBBM to an adaptive full scan model in two steps: first, to a full scan model for static environments and next, to a full scan model for general, dynamic environments. The full scan model adapts to the local sample density when using a particle filter, and accounts for the dependency between beams. In contrast to the Gaussian-based state of the art models of Plagemann et al. (2007) and Pfaff et al. (2007), the proposed full scan model uses a sample-based approximation, which can cope with dynamic environments and with multi-modality (which was shown to occur even in simple static environments)."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their thorough and constructive reviews. The authors also thank Wilm Decre\u0301, Pauwel Goethals, Goele Pipeleers, Ruben Smits, Bert Stallaert, Lieboud Van den Broeck, Marnix Volckaert and Hans Wambacq for participating in the experiments. All authors gratefully acknowledge the financial support by K.U.Leuven\u2019s Concerted Research Action GOA/05/10 and the Research Council K.U.Leuven, CoE EF/05/006 Optimization in Engineering (OPTEC). Tinne De Laet is a Doctoral Fellow of the Fund for Scientific Research\u2013Flanders (F.W.O.) in Belgium."}, {"heading": "Appendix A. Simplification of Infinite Sum", "text": "To goal of this appendix is to prove that\n\u2211\nn\nP (k | n, x,m)P (n) = \u221e\u2211\nn=k\n[( n\nk\n) uk (1\u2212 u)n\u2212k (1\u2212 p) pn ] , (110)\ncan be simplified to: \u2211\nn\nP (k | n, x,m)P (n) = ( 1\u2212 p\u2032 ) p\u2032k, (111)\nwith p\u2032 = up1\u2212(1\u2212u)p .\nExpand Eq. (110) and move terms out of the summation so that:\n\u2211\nn\nP (k | n, x,m)P (n) = (1\u2212 p)ukpk \u221e\u2211\nn=k\n[ n!\n(n\u2212 k)!k! [(1\u2212 u) p]n\u2212k\n]\n. (112)\nNext introduce variables t = n\u2212 k and a e = (1\u2212 u) p:\n\u2211\nn\nP (k | n, x,m)P (n) = (1\u2212 p)ukpk \u221e\u2211\nt=0\n[ (t+ k)! t!k! et ] . (113)\nThe next step is to prove by induction that:\n\u221e\u2211\nt=0\n[ (t+ k)! t!k! et ] = 1 (1\u2212 e)k+1 . (114)\nFirst, show that the above equality holds for k = 0:\n\u221e\u2211\nt=0\nt! t! et =\n\u221e\u2211\nt=0\net, (115)\nwhich is the well-known geometric series, so:\n\u221e\u2211\nt=0\net = 1\n1\u2212 e , (116)\nshowing that equality (114) indeed holds for k = 0. Next it is proved that, if the expression holds for k\u2212 1, it also holds for k. Introduce variable V for the solution of the infinity sum for k and split up (114) in two parts:\nV = \u221e\u2211\nt=0\n[ (t+ k)! t!k! et ] = \u221e\u2211\nt=0\n[ (t+ k)!\nt!k! \u2212\n(t+ k \u2212 1)!\nt! (k \u2212 1)!\n]\net + \u221e\u2211\nt=0\n[ (t+ k \u2212 1)! t! (k \u2212 1)! et ] \ufe38 \ufe37\ufe37 \ufe38 1\n(1\u2212e)k\n, (117)\nwhere the fact is used that the equality (114) holds for k\u2212 1. Simplify the first term of the summation to:\n\u221e\u2211\nt=0\n[ (t+ k)!\nt!k! \u2212\n(t+ k \u2212 1)!\nt! (k \u2212 1)!\n]\net = \u221e\u2211\nt=0\n[ (t+ k \u2212 1)!\nt!k! tet\n]\n. (118)\nThe term in the summation for t = 0 is equal to zero. Hence, introduce the variable t\u2032 = t\u22121 and simplify:\n\u221e\u2211\nt=0\n[ (t+ k)!\nt!k! \u2212\n(t+ k \u2212 1)!\nt! (k \u2212 1)!\n]\net = \u221e\u2211\nt\u2032=0\n[ (t\u2032 + k)!\n(t\u2032 + 1)!k!\n( t\u2032 + 1 ) et \u2032+1\n]\n(119)\n= e \u221e\u2211\nt\u2032=0\n[ (t\u2032 + k)!\nt\u2032!k! et\n\u2032\n]\n\ufe38 \ufe37\ufe37 \ufe38\nV\n, (120)\nfrom which the series V we were looking for is recognized. Substitute the above result in Eq. (117) so that:\nV = eV + 1\n(1\u2212 e)k . (121)\nSolving this equation for V gives:\nV = 1\n(1\u2212 e)k+1 , (122)\nproving that equality (114) holds for k if it assumed to hold for k\u2212 1, and closing the proof by induction.\nNow substitute Eq. (114) in Eq. (113):\n\u2211\nn\nP (k | n, x,m)P (n) = (1\u2212 p)ukpk\n[1\u2212 (1\u2212 u) p]k+1 , (123)\nor rewrite as: \u2211\nn\nP (k | n, x,m)P (n) = ( 1\u2212 p\u2032 ) p\u2032k, (124)\nwhere p\u2032 = up1\u2212(1\u2212u)p , what is exactly what had to be proved."}], "references": [{"title": "Variational algorithms for approximate Bayesian inference", "author": ["M.J. Beal"], "venue": "Ph.D. thesis,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "The variational bayesian em algorithm for incomplete data: with application for scoring graphical model structures", "author": ["M.J. Beal", "Z. Ghahramani"], "venue": "In Valencia International Meeting on Bayesian Statistics,", "citeRegEx": "Beal and Ghahramani,? \\Q2003\\E", "shortCiteRegEx": "Beal and Ghahramani", "year": 2003}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Open RObot COntrol Software. http://www.orocos.org", "author": ["H. Bruyninckx"], "venue": null, "citeRegEx": "Bruyninckx,? \\Q2001\\E", "shortCiteRegEx": "Bruyninckx", "year": 2001}, {"title": "Estimating the absolute position of a mobile robot using position probability grids", "author": ["W. Burgard", "D. Fox", "D. Hennig", "T. Schmidt"], "venue": "In Proc. of the National Conference on Artificial Intelligence", "citeRegEx": "Burgard et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Burgard et al\\.", "year": 1996}, {"title": "Principles of Robot Motion: Theory, Algorithms, and Implementations", "author": ["H. Choset", "K.M. Lynch", "S. Hutchinson", "G.A. Kantor", "W. Burgard", "L.E. Kavraki", "S. Thrun"], "venue": null, "citeRegEx": "Choset et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Choset et al\\.", "year": 2005}, {"title": "Maximum likelihood from incomplete data via the EM algorithm (with discussion)", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society (Series B),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Adapting the Sample Size in Particle Filters Through KLD-Sampling", "author": ["D. Fox"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Fox,? \\Q2003\\E", "shortCiteRegEx": "Fox", "year": 2003}, {"title": "Markov localization for mobile robots in dynamic environments", "author": ["D. Fox", "W. Burgard", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fox et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Fox et al\\.", "year": 1999}, {"title": "Mobile robot mapping in populated environments and sensor planning", "author": ["D. H\u00e4hnel", "D. Schulz", "W. Burgard"], "venue": "Journal of the Advanced Robotics,", "citeRegEx": "H\u00e4hnel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "H\u00e4hnel et al\\.", "year": 2003}, {"title": "Map building with mobile robots in dynamic environments", "author": ["D. H\u00e4hnel", "R. Triebel", "W. Burgard", "S. Thrun"], "venue": "In Proceedings of the 2003 IEEE International Conference on Robotics and Automation,", "citeRegEx": "H\u00e4hnel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "H\u00e4hnel et al\\.", "year": 2003}, {"title": "Ockham\u2019s razor and bayesian analysis", "author": ["W. Jeffreys", "J. Berger"], "venue": "American Scientist,", "citeRegEx": "Jeffreys and Berger,? \\Q1992\\E", "shortCiteRegEx": "Jeffreys and Berger", "year": 1992}, {"title": "Bayesian Networks and Decision Graphs", "author": ["F.V. Jensen", "T.D. Nielsen"], "venue": null, "citeRegEx": "Jensen and Nielsen,? \\Q2007\\E", "shortCiteRegEx": "Jensen and Nielsen", "year": 2007}, {"title": "Sensor resetting localization for poorly modelled mobile robots", "author": ["S. Lenser", "M. Veloso"], "venue": "In Proceedings of the 2000 IEEE International Conference on Robotics and Automation,", "citeRegEx": "Lenser and Veloso,? \\Q2000\\E", "shortCiteRegEx": "Lenser and Veloso", "year": 2000}, {"title": "The EM algorithm and extensions", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan,? \\Q1997\\E", "shortCiteRegEx": "McLachlan and Krishnan", "year": 1997}, {"title": "Sensor fusion in certainty grids for mobile robots", "author": ["H.P. Moravec"], "venue": "AI Magazine,", "citeRegEx": "Moravec,? \\Q1988\\E", "shortCiteRegEx": "Moravec", "year": 1988}, {"title": "Learning Bayesian Networks", "author": ["R.E. Neapolitan"], "venue": null, "citeRegEx": "Neapolitan,? \\Q2004\\E", "shortCiteRegEx": "Neapolitan", "year": 2004}, {"title": "Robust Monte-Carlo localization using adaptive likelihood models", "author": ["P. Pfaff", "W. Burgard", "D. Fox"], "venue": "European Robotics Symposium,", "citeRegEx": "Pfaff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pfaff et al\\.", "year": 2006}, {"title": "Improved likelihood models for probabilistic localization based on range scans", "author": ["P. Pfaff", "C. Plagemann", "W. Burgard"], "venue": "In Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Pfaff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pfaff et al\\.", "year": 2007}, {"title": "Gaussian beam processes: A nonparametric Bayesian measurement model for range finders", "author": ["C. Plagemann", "K. Kersting", "P. Pfaff", "W. Burgard"], "venue": "In Robotics: Science and Systems (RSS),", "citeRegEx": "Plagemann et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Plagemann et al\\.", "year": 2007}, {"title": "Occam\u2019s razor. In Advances in Neural Information Processing 13, Denver, Colorado", "author": ["C. Rasmussen", "Z. Ghahramani"], "venue": null, "citeRegEx": "Rasmussen and Ghahramani,? \\Q2000\\E", "shortCiteRegEx": "Rasmussen and Ghahramani", "year": 2000}, {"title": "A Software Framework for Real-Time and Distributed Robot and Machine Control", "author": ["P. Soetens"], "venue": "Ph.D. thesis,", "citeRegEx": "Soetens,? \\Q2006\\E", "shortCiteRegEx": "Soetens", "year": 2006}, {"title": "A probabilistic online mapping algorithm for teams of mobile robots", "author": ["S. Thrun"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Thrun,? \\Q2001\\E", "shortCiteRegEx": "Thrun", "year": 2001}, {"title": "Robust monte carlo localization for mobile robots", "author": ["S. Thrun", "D. Fox", "W. Burgard", "F. Dellaert"], "venue": "Artificial Intelligence,", "citeRegEx": "Thrun et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 2001}, {"title": "Online simultaneous localization and mapping with detection and tracking of moving objects: Theory and results from a ground vehicle in crowded urban areas", "author": ["Wang", "C.-C", "C. Thorpe", "S. Thrun"], "venue": "In Proceedings of the 2003 IEEE International Conference on Robotics and Automation, Taipeh, Taiwan. ICRA2003", "citeRegEx": "Wang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Mobile robot simultaneous localization and mapping in dynamic environments", "author": ["D.F. Wolf", "G.S. Sukhatme"], "venue": "In Proceedings of the 2004 IEEE International Conference on Robotics and Automation,", "citeRegEx": "Wolf and Sukhatme,? \\Q2004\\E", "shortCiteRegEx": "Wolf and Sukhatme", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "In Section 5 the model parameters of the RBBM are learned from experimental data and the resulting model is compared with the state of the art beam model proposed by Thrun, Burgard, and Fox (2005), further on called Thrun\u2019s model.", "startOffset": 186, "endOffset": 197}, {"referenceID": 8, "context": "Since such approaches assume that the environment is almost static, they are unable to cope with real dynamics as in populated environments (Fox et al., 1999).", "startOffset": 140, "endOffset": 158}, {"referenceID": 7, "context": "Related Work Three basic approaches to deal with dynamic environments exist in the literature (Fox, Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor model and outlier detection. In state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun (2003) developed an algorithm \u2018SLAM with DATMO\u2019, short for SLAM with the detection and tracking of moving objects.", "startOffset": 95, "endOffset": 391}, {"referenceID": 7, "context": "Related Work Three basic approaches to deal with dynamic environments exist in the literature (Fox, Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor model and outlier detection. In state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun (2003) developed an algorithm \u2018SLAM with DATMO\u2019, short for SLAM with the detection and tracking of moving objects. State augmentation however is often infeasible since the computational complexity of state estimation increases exponentially with the number of independent state variables to estimate. A closely related solution consists of adapting the map according to the changes in the environment. Since such approaches assume that the environment is almost static, they are unable to cope with real dynamics as in populated environments (Fox et al., 1999). A more recent, related approach proposed by Wolf and Sukhatme (2004) maintains two coupled occupancy grids of the environment, one for the static map and one for the moving objects, to account for environment dynamics.", "startOffset": 95, "endOffset": 1015}, {"referenceID": 7, "context": "Related Work Three basic approaches to deal with dynamic environments exist in the literature (Fox, Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor model and outlier detection. In state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun (2003) developed an algorithm \u2018SLAM with DATMO\u2019, short for SLAM with the detection and tracking of moving objects. State augmentation however is often infeasible since the computational complexity of state estimation increases exponentially with the number of independent state variables to estimate. A closely related solution consists of adapting the map according to the changes in the environment. Since such approaches assume that the environment is almost static, they are unable to cope with real dynamics as in populated environments (Fox et al., 1999). A more recent, related approach proposed by Wolf and Sukhatme (2004) maintains two coupled occupancy grids of the environment, one for the static map and one for the moving objects, to account for environment dynamics. Probabilistic approaches are to some extent robust to unmodeled dynamics, since they are able to deal with sensor noise. In such approaches however, the sensor noise should reflect the real uncertainty due to the unmodeled dynamics of the environment. Therefore, a second approach for dealing with dynamic environments is to adapt the sensor model to correctly reflect situations in which measurements are affected by the unmodeled environment dynamics. Fox et al. (1999) show that such approaches are only capable to model such noise on average, and, while these approaches work reliably with occasional sensor blockage, they are inadequate in situations where more than fifty percent of the measurements are corrupted.", "startOffset": 95, "endOffset": 1637}, {"referenceID": 7, "context": "Related Work Three basic approaches to deal with dynamic environments exist in the literature (Fox, Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor model and outlier detection. In state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun (2003) developed an algorithm \u2018SLAM with DATMO\u2019, short for SLAM with the detection and tracking of moving objects. State augmentation however is often infeasible since the computational complexity of state estimation increases exponentially with the number of independent state variables to estimate. A closely related solution consists of adapting the map according to the changes in the environment. Since such approaches assume that the environment is almost static, they are unable to cope with real dynamics as in populated environments (Fox et al., 1999). A more recent, related approach proposed by Wolf and Sukhatme (2004) maintains two coupled occupancy grids of the environment, one for the static map and one for the moving objects, to account for environment dynamics. Probabilistic approaches are to some extent robust to unmodeled dynamics, since they are able to deal with sensor noise. In such approaches however, the sensor noise should reflect the real uncertainty due to the unmodeled dynamics of the environment. Therefore, a second approach for dealing with dynamic environments is to adapt the sensor model to correctly reflect situations in which measurements are affected by the unmodeled environment dynamics. Fox et al. (1999) show that such approaches are only capable to model such noise on average, and, while these approaches work reliably with occasional sensor blockage, they are inadequate in situations where more than fifty percent of the measurements are corrupted. To handle measurement corruption more effectively, an approach based on outlier detection can be used. This approach uses an adapted sensor model, as explained in the previous paragraph. The idea is to investigate the cause of a sensor measurement and to reject measurements that are likely to be affected by unmodeled environment dynamics. H\u00e4hnel, Schulz, and Burgard (2003a) and H\u00e4hnel, Triebel, Burgard, and Thrun (2003b) studied the problem of performing SLAM in environments with many moving objects using the EM algorithm for filtering out affected measurements.", "startOffset": 95, "endOffset": 2263}, {"referenceID": 7, "context": "Related Work Three basic approaches to deal with dynamic environments exist in the literature (Fox, Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor model and outlier detection. In state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun (2003) developed an algorithm \u2018SLAM with DATMO\u2019, short for SLAM with the detection and tracking of moving objects. State augmentation however is often infeasible since the computational complexity of state estimation increases exponentially with the number of independent state variables to estimate. A closely related solution consists of adapting the map according to the changes in the environment. Since such approaches assume that the environment is almost static, they are unable to cope with real dynamics as in populated environments (Fox et al., 1999). A more recent, related approach proposed by Wolf and Sukhatme (2004) maintains two coupled occupancy grids of the environment, one for the static map and one for the moving objects, to account for environment dynamics. Probabilistic approaches are to some extent robust to unmodeled dynamics, since they are able to deal with sensor noise. In such approaches however, the sensor noise should reflect the real uncertainty due to the unmodeled dynamics of the environment. Therefore, a second approach for dealing with dynamic environments is to adapt the sensor model to correctly reflect situations in which measurements are affected by the unmodeled environment dynamics. Fox et al. (1999) show that such approaches are only capable to model such noise on average, and, while these approaches work reliably with occasional sensor blockage, they are inadequate in situations where more than fifty percent of the measurements are corrupted. To handle measurement corruption more effectively, an approach based on outlier detection can be used. This approach uses an adapted sensor model, as explained in the previous paragraph. The idea is to investigate the cause of a sensor measurement and to reject measurements that are likely to be affected by unmodeled environment dynamics. H\u00e4hnel, Schulz, and Burgard (2003a) and H\u00e4hnel, Triebel, Burgard, and Thrun (2003b) studied the problem of performing SLAM in environments with many moving objects using the EM algorithm for filtering out affected measurements.", "startOffset": 95, "endOffset": 2311}, {"referenceID": 7, "context": "Related Work Three basic approaches to deal with dynamic environments exist in the literature (Fox, Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor model and outlier detection. In state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun (2003) developed an algorithm \u2018SLAM with DATMO\u2019, short for SLAM with the detection and tracking of moving objects. State augmentation however is often infeasible since the computational complexity of state estimation increases exponentially with the number of independent state variables to estimate. A closely related solution consists of adapting the map according to the changes in the environment. Since such approaches assume that the environment is almost static, they are unable to cope with real dynamics as in populated environments (Fox et al., 1999). A more recent, related approach proposed by Wolf and Sukhatme (2004) maintains two coupled occupancy grids of the environment, one for the static map and one for the moving objects, to account for environment dynamics. Probabilistic approaches are to some extent robust to unmodeled dynamics, since they are able to deal with sensor noise. In such approaches however, the sensor noise should reflect the real uncertainty due to the unmodeled dynamics of the environment. Therefore, a second approach for dealing with dynamic environments is to adapt the sensor model to correctly reflect situations in which measurements are affected by the unmodeled environment dynamics. Fox et al. (1999) show that such approaches are only capable to model such noise on average, and, while these approaches work reliably with occasional sensor blockage, they are inadequate in situations where more than fifty percent of the measurements are corrupted. To handle measurement corruption more effectively, an approach based on outlier detection can be used. This approach uses an adapted sensor model, as explained in the previous paragraph. The idea is to investigate the cause of a sensor measurement and to reject measurements that are likely to be affected by unmodeled environment dynamics. H\u00e4hnel, Schulz, and Burgard (2003a) and H\u00e4hnel, Triebel, Burgard, and Thrun (2003b) studied the problem of performing SLAM in environments with many moving objects using the EM algorithm for filtering out affected measurements. By doing so, they were able to acquire maps in the environment where conventional SLAM techniques failed. Fox et al. (1999) propose two different kinds of filters: an entropy filter, suited for an arbitrary sensor, and a distance filter, designed for proximity sensors.", "startOffset": 95, "endOffset": 2579}, {"referenceID": 22, "context": "The simple and efficient likelihood field models or end point model (Thrun, 2001) are related to these correlation-based methods.", "startOffset": 68, "endOffset": 81}, {"referenceID": 8, "context": "Range finder sensor models can also be classified according to whether they use discrete geometric grids (H\u00e4hnel et al., 2003a, 2003b; Fox et al., 1999; Burgard, Fox, Hennig, & Schmidt, 1996; Moravec, 1988) or continuous geometric models (Thrun et al.", "startOffset": 105, "endOffset": 206}, {"referenceID": 15, "context": "Range finder sensor models can also be classified according to whether they use discrete geometric grids (H\u00e4hnel et al., 2003a, 2003b; Fox et al., 1999; Burgard, Fox, Hennig, & Schmidt, 1996; Moravec, 1988) or continuous geometric models (Thrun et al.", "startOffset": 105, "endOffset": 206}, {"referenceID": 4, "context": "Even simplified models (Burgard et al., 1996) in this approach turned out to be computationally too expensive for real-time application.", "startOffset": 23, "endOffset": 45}, {"referenceID": 5, "context": "An analogous mixture (Thrun et al., 2005; Choset et al., 2005) adds two more physical causes: a sensor failure and an unknown cause resulting in a \u2018max-range\u2019 measurement and a \u2018random\u2019 measurement, respectively.", "startOffset": 21, "endOffset": 62}, {"referenceID": 12, "context": "The simple and efficient likelihood field models or end point model (Thrun, 2001) are related to these correlation-based methods. Plagemann, Kersting, Pfaff, and Burgard (2007) nicely summarize the advantages and drawbacks of the different range finder sensor models.", "startOffset": 69, "endOffset": 177}, {"referenceID": 4, "context": "Even simplified models (Burgard et al., 1996) in this approach turned out to be computationally too expensive for real-time application. Therefore, Fox et al. proposed a beam model consisting of a mixture of two physical causes for a measurement: a hit with an object in the map, or with an object not yet modeled in the map. The last cause accounts for the dynamic nature of the environment. An analogous mixture (Thrun et al., 2005; Choset et al., 2005) adds two more physical causes: a sensor failure and an unknown cause resulting in a \u2018max-range\u2019 measurement and a \u2018random\u2019 measurement, respectively. While Thrun et al. and Pfaff et al. use a continuous model, Choset et al. present the discrete analog of the mixture, taking into account the limited resolution of the range sensor. Pfaff et al. extend the basic mixture model for use in Monte Carlo localization. To overcome problems due to the combination of the limited representational power and the peaked likelihood of the accurate range finder, they propose an adaptive likelihood model. The likelihood model is smooth during global localization and more peaked during tracking. Recently, different researchers tried to tackle the problems associated with beam-based models, caused by the independence assumptions between beams. Plagemann et al. (2007) propose a sensor model for the full scan.", "startOffset": 24, "endOffset": 1315}, {"referenceID": 4, "context": "Even simplified models (Burgard et al., 1996) in this approach turned out to be computationally too expensive for real-time application. Therefore, Fox et al. proposed a beam model consisting of a mixture of two physical causes for a measurement: a hit with an object in the map, or with an object not yet modeled in the map. The last cause accounts for the dynamic nature of the environment. An analogous mixture (Thrun et al., 2005; Choset et al., 2005) adds two more physical causes: a sensor failure and an unknown cause resulting in a \u2018max-range\u2019 measurement and a \u2018random\u2019 measurement, respectively. While Thrun et al. and Pfaff et al. use a continuous model, Choset et al. present the discrete analog of the mixture, taking into account the limited resolution of the range sensor. Pfaff et al. extend the basic mixture model for use in Monte Carlo localization. To overcome problems due to the combination of the limited representational power and the peaked likelihood of the accurate range finder, they propose an adaptive likelihood model. The likelihood model is smooth during global localization and more peaked during tracking. Recently, different researchers tried to tackle the problems associated with beam-based models, caused by the independence assumptions between beams. Plagemann et al. (2007) propose a sensor model for the full scan. The model treats the sensor modeling task as a non-parametric Bayesian regression problem, and solves it using Gaussian processes. It is claimed that the Gaussian beam processes combine the advantages of the beam-based and the correlation-based models. Due to the underlying assumption that the measurements are jointly Gaussian distributed, the Gaussian beam processes are not suited to take into account the non-Gaussian uncertainty due to the dynamic nature of the environment. An alternative approach to handle the overly-peaked likelihood functions resulting from the traditional beam models is proposed by Pfaff, Plagemann, and Burgard (2007). A locationdependent full scan model takes into account the approximation error of the sample-based representation, and explicitly models the correlations between individual beams introduced", "startOffset": 24, "endOffset": 2006}, {"referenceID": 2, "context": "Such graphical models provide a simple way to visualize the structure of a probabilistic model, and can be used to design and motivate new models (Bishop, 2006).", "startOffset": 146, "endOffset": 160}, {"referenceID": 19, "context": "Next, inspired by the adaptive full scan models in the literature (Pfaff et al., 2006, 2007; Plagemann et al., 2007), the RBBM is extended to an adaptive full scan model.", "startOffset": 66, "endOffset": 116}, {"referenceID": 13, "context": "represent the covariance matrix as a parametrized covariance function using Gaussian processes whose parameters are learned from data, Pfaff et al. learn the full covariance matrix being less restrictive in this manner. Despite the modeled correlation between beams, the measurements are still assumed to be jointly Gaussian distributed, which again limits the applicability in dynamic environments. This paper proposes a rigorously Bayesian modeling of the probabilistic range sensor beam model for dynamic environments, referred to as RBBM. Similar to the work of Thrun et al. (2005) and Pfaff et al.", "startOffset": 135, "endOffset": 586}, {"referenceID": 13, "context": "represent the covariance matrix as a parametrized covariance function using Gaussian processes whose parameters are learned from data, Pfaff et al. learn the full covariance matrix being less restrictive in this manner. Despite the modeled correlation between beams, the measurements are still assumed to be jointly Gaussian distributed, which again limits the applicability in dynamic environments. This paper proposes a rigorously Bayesian modeling of the probabilistic range sensor beam model for dynamic environments, referred to as RBBM. Similar to the work of Thrun et al. (2005) and Pfaff et al. (2006) the sensor model is derived for a continuous geometry.", "startOffset": 135, "endOffset": 610}, {"referenceID": 13, "context": "represent the covariance matrix as a parametrized covariance function using Gaussian processes whose parameters are learned from data, Pfaff et al. learn the full covariance matrix being less restrictive in this manner. Despite the modeled correlation between beams, the measurements are still assumed to be jointly Gaussian distributed, which again limits the applicability in dynamic environments. This paper proposes a rigorously Bayesian modeling of the probabilistic range sensor beam model for dynamic environments, referred to as RBBM. Similar to the work of Thrun et al. (2005) and Pfaff et al. (2006) the sensor model is derived for a continuous geometry. Unlike previous models of Thrun et al. (2005), Pfaff et al.", "startOffset": 135, "endOffset": 711}, {"referenceID": 13, "context": "represent the covariance matrix as a parametrized covariance function using Gaussian processes whose parameters are learned from data, Pfaff et al. learn the full covariance matrix being less restrictive in this manner. Despite the modeled correlation between beams, the measurements are still assumed to be jointly Gaussian distributed, which again limits the applicability in dynamic environments. This paper proposes a rigorously Bayesian modeling of the probabilistic range sensor beam model for dynamic environments, referred to as RBBM. Similar to the work of Thrun et al. (2005) and Pfaff et al. (2006) the sensor model is derived for a continuous geometry. Unlike previous models of Thrun et al. (2005), Pfaff et al. (2006), Fox et al.", "startOffset": 135, "endOffset": 732}, {"referenceID": 5, "context": "(2006), Fox et al. (1999) and Choset et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 4, "context": "(1999) and Choset et al. (2005), the mixture components are founded on a Bayesian modeling.", "startOffset": 11, "endOffset": 32}, {"referenceID": 2, "context": "Such graphical models provide a simple way to visualize the structure of a probabilistic model, and can be used to design and motivate new models (Bishop, 2006). By inspection of the graph, insights of the model, including conditional independence properties are obtained. Next, inspired by the adaptive full scan models in the literature (Pfaff et al., 2006, 2007; Plagemann et al., 2007), the RBBM is extended to an adaptive full scan model. The underlying sample-based approximation of the full scan model, in contrast to the Gaussianbased approximation proposed by Pfaff et al. (2007) and Plagemann et al.", "startOffset": 147, "endOffset": 589}, {"referenceID": 16, "context": "1 Bayesian Model Bayesian networks graphically represent probabilistic relationships between variables in a mathematical model, to structure and facilitate probabilistic inference computations with those variables (Jensen & Nielsen, 2007; Neapolitan, 2004).", "startOffset": 214, "endOffset": 256}, {"referenceID": 2, "context": "In this case the graphical models are also known as generative models (Bishop, 2006), since they capture the causal process generating the random variables.", "startOffset": 70, "endOffset": 84}, {"referenceID": 22, "context": "should be present and the first object should be absent (Thrun et al., 2005). Moreover, the rate of decrease of the likelihood of sensing unmodeled objects is only dependent on p, the degree of appearance of unmodeled objects. The probability of measuring a feature of the map, and therefore the integral under the scaled Gaussian (1\u2212p)Phit (z | x,m) (45), decreases with the expected range. This is easily explained since the probability that the map is not occluded decreases when the feature is located further away. Finally, the discontinuity of the RBBM (Fig. 8) was shown to be caused by the only approximation made (Section 3.5). Since the state of the art range sensors are very accurate, neglecting the measurement noise on the measurement of an occluding object is an acceptable approximation. This is also shown by the experiments presented in Section 5. With respect to the state of the art beam model of Thrun et al. (2005), the model proposed here, Eq.", "startOffset": 57, "endOffset": 937}, {"referenceID": 2, "context": "Random configurations are selected by ancestral sampling (Bishop, 2006), i.", "startOffset": 57, "endOffset": 71}, {"referenceID": 2, "context": "In this paper two different estimators3, a maximum likelihood (ML) (Dempster, Laird, & Rubin, 1977; McLachlan & Krishnan, 1997; Bishop, 2006) and a variational Bayesian (VB) (Beal & Ghahramani, 2003; Bishop, 2006) estimator, are presented to learn the model parameters from data.", "startOffset": 67, "endOffset": 141}, {"referenceID": 2, "context": "In this paper two different estimators3, a maximum likelihood (ML) (Dempster, Laird, & Rubin, 1977; McLachlan & Krishnan, 1997; Bishop, 2006) and a variational Bayesian (VB) (Beal & Ghahramani, 2003; Bishop, 2006) estimator, are presented to learn the model parameters from data.", "startOffset": 174, "endOffset": 213}, {"referenceID": 2, "context": "This paper approximately follows the notation by Bishop (2006).", "startOffset": 49, "endOffset": 63}, {"referenceID": 2, "context": "\u03b9=1 (\u03b3 (d\u03b91) (log \u03c01 + logPhit (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7 \u03b3 (d\u03b92) (log \u03c02 + logPoccl (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7 \u03b3 (d\u03b93) (log \u03c03 + logPrand (z\u03b9 | x\u03b9,m)) + \u00b7 \u00b7 \u00b7 \u03b3 (d\u03b94) (log \u03c04 + logPmax (z\u03b9 | x\u03b9,m))) , (56) where \u03b3 (d\u03b9s) = E [d\u03b9s] is the discrete posterior probability, or responsibility (Bishop, 2006), of cause s for data point z\u03b9.", "startOffset": 276, "endOffset": 290}, {"referenceID": 2, "context": "2 Variational Bayesian Learning The ML estimator only provides point estimates of the parameters and is sensitive to overfitting (Bishop, 2006).", "startOffset": 129, "endOffset": 143}, {"referenceID": 2, "context": "The VB estimator has only a little computational overhead as compared to the ML estimator (Bishop, 2006).", "startOffset": 90, "endOffset": 104}, {"referenceID": 0, "context": "The Bayesian approach attempts to integrate over the possible values of all uncertain quantities rather than optimize them as in the ML approach (Beal, 2003; Beal & Ghahramani, 2003).", "startOffset": 145, "endOffset": 182}, {"referenceID": 2, "context": "By introducing the distribution Q over the latent variables the complete log marginal likelihood can be decomposed as (Bishop, 2006): logP (Z) = L (Q) +KL (Q||P ) , (67) where L (Q) = \u222b Q (D, \u0398) log ( P (Z,D, \u0398) Q (D, \u0398) ) d (D, \u0398) , (68)", "startOffset": 118, "endOffset": 132}, {"referenceID": 2, "context": "The general expressions for the optimal factors are (Bishop, 2006): logQD(D) = E\u0398 [logP (Z,D, \u0398)] + C , and (70) logQ\u0398 (\u0398) = ED [logP (Z,D, \u0398)] + C , (71) where \u22c6 indicates the optimality.", "startOffset": 52, "endOffset": 66}, {"referenceID": 2, "context": "The parameters are as defined by Bishop (2006).", "startOffset": 33, "endOffset": 47}, {"referenceID": 22, "context": "(45)) from experimental data, (ii) to compare the results of the proposed ML-EM and VB-EM estimator (Section 4), and (iii) to compare the results of the proposed estimators with the learning approach of Thrun\u2019s model proposed by Thrun et al. (2005). To this end two experimental setups from different application areas in robotics are used.", "startOffset": 203, "endOffset": 249}, {"referenceID": 22, "context": "Figure 11: Data for second learning experiment reported by Thrun et al. (2005). These data consist of two series of measurements obtained with a mobile robot traveling through a typical office environment.", "startOffset": 59, "endOffset": 79}, {"referenceID": 2, "context": "The latter is known to be a valid symmetric distance metric (Bishop, 2006).", "startOffset": 60, "endOffset": 74}, {"referenceID": 22, "context": "1 First Learning Experiment In a first learning experiment, the experimental data reported by Thrun et al. (2005) is used.", "startOffset": 94, "endOffset": 114}, {"referenceID": 18, "context": "As a consequence, these peaked likelihood functions do not adequately model the uncertainty due to the finite, sample-based representation of the posterior (Pfaff et al., 2007).", "startOffset": 156, "endOffset": 176}, {"referenceID": 17, "context": "The additional uncertainty strongly varies with the number of samples and the uncertainty of the estimate (Pfaff et al., 2006).", "startOffset": 106, "endOffset": 126}, {"referenceID": 7, "context": "Fox (2003) proposes to dynamically adapt the number of samples by means of KLD sampling (KLD stands for Kullback-Leibler divergence).", "startOffset": 0, "endOffset": 11}, {"referenceID": 7, "context": "Fox (2003) proposes to dynamically adapt the number of samples by means of KLD sampling (KLD stands for Kullback-Leibler divergence). For very peaked likelihoods however, this might result in a huge number of samples. Lenser and Veloso (2000) and Thrun, Fox, Burgard, and Dellaert (2001) ensure that a critical mass of samples is located at the important parts of the state space by sampling from the observation model.", "startOffset": 0, "endOffset": 243}, {"referenceID": 7, "context": "Fox (2003) proposes to dynamically adapt the number of samples by means of KLD sampling (KLD stands for Kullback-Leibler divergence). For very peaked likelihoods however, this might result in a huge number of samples. Lenser and Veloso (2000) and Thrun, Fox, Burgard, and Dellaert (2001) ensure that a critical mass of samples is located at the important parts of the state space by sampling from the observation model.", "startOffset": 0, "endOffset": 288}, {"referenceID": 7, "context": "Fox (2003) proposes to dynamically adapt the number of samples by means of KLD sampling (KLD stands for Kullback-Leibler divergence). For very peaked likelihoods however, this might result in a huge number of samples. Lenser and Veloso (2000) and Thrun, Fox, Burgard, and Dellaert (2001) ensure that a critical mass of samples is located at the important parts of the state space by sampling from the observation model. Sampling from the observation model however, is often only possible in an approximate and inaccurate way. Pfaff et al. (2006) introduced an adaptive beam model for dynamic environments, which explicitly takes location uncertainty due to the sample-based representation into account.", "startOffset": 0, "endOffset": 546}, {"referenceID": 7, "context": "Fox (2003) proposes to dynamically adapt the number of samples by means of KLD sampling (KLD stands for Kullback-Leibler divergence). For very peaked likelihoods however, this might result in a huge number of samples. Lenser and Veloso (2000) and Thrun, Fox, Burgard, and Dellaert (2001) ensure that a critical mass of samples is located at the important parts of the state space by sampling from the observation model. Sampling from the observation model however, is often only possible in an approximate and inaccurate way. Pfaff et al. (2006) introduced an adaptive beam model for dynamic environments, which explicitly takes location uncertainty due to the sample-based representation into account. They compute the additional uncertainty due to the sample-based representation, using techniques from density estimation. When evaluating the likelihood function at a sample, they consider a certain region around the sample, depending on the sample density at that location. Then, depending on the area covered by the sample, the variance of the Gaussian, \u03c3m, governing the beam model in Eq. (38), is calculated for each sample. As a result, the beam model automatically adapts to the local density of samples. Such a location dependent model results in a smooth likelihood function during global localization and a more peaked function during position tracking without changing the number of samples. Plagemann et al. (2007) and Pfaff et al.", "startOffset": 0, "endOffset": 1429}, {"referenceID": 7, "context": "Fox (2003) proposes to dynamically adapt the number of samples by means of KLD sampling (KLD stands for Kullback-Leibler divergence). For very peaked likelihoods however, this might result in a huge number of samples. Lenser and Veloso (2000) and Thrun, Fox, Burgard, and Dellaert (2001) ensure that a critical mass of samples is located at the important parts of the state space by sampling from the observation model. Sampling from the observation model however, is often only possible in an approximate and inaccurate way. Pfaff et al. (2006) introduced an adaptive beam model for dynamic environments, which explicitly takes location uncertainty due to the sample-based representation into account. They compute the additional uncertainty due to the sample-based representation, using techniques from density estimation. When evaluating the likelihood function at a sample, they consider a certain region around the sample, depending on the sample density at that location. Then, depending on the area covered by the sample, the variance of the Gaussian, \u03c3m, governing the beam model in Eq. (38), is calculated for each sample. As a result, the beam model automatically adapts to the local density of samples. Such a location dependent model results in a smooth likelihood function during global localization and a more peaked function during position tracking without changing the number of samples. Plagemann et al. (2007) and Pfaff et al. (2007) showed that by considering a region around samples, the individual beams become statistically dependent.", "startOffset": 0, "endOffset": 1453}, {"referenceID": 17, "context": "The full scan models proposed by Plagemann et al. (2007) and Pfaff et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 17, "context": "(2007) and Pfaff et al. (2007) both assume that the beams of a range scan are jointly Gaussian distributed.", "startOffset": 11, "endOffset": 31}, {"referenceID": 17, "context": "(2007) and Pfaff et al. (2007) cannot handle this multi-modality.", "startOffset": 11, "endOffset": 31}, {"referenceID": 17, "context": "1 Sample-based Adaptive Full Scan Model for Static Environments Plagemann et al. (2007) and Pfaff et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 17, "context": "(2007) and Pfaff et al. (2007) estimate the full scan model, P (z | x,m)6, based on a local environment U (x) of the exact estimate x:", "startOffset": 11, "endOffset": 31}, {"referenceID": 17, "context": "Pfaff et al. (2006) proposed to use a circular region with diameter dU(x), which is a weighted sum of the Euclidean distance and the angular difference.", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "Pfaff et al. (2006) proposed to use a circular region with diameter dU(x), which is a weighted sum of the Euclidean distance and the angular difference. Like Plagemann et al. (2007) and Pfaff et al.", "startOffset": 0, "endOffset": 182}, {"referenceID": 17, "context": "Pfaff et al. (2006) proposed to use a circular region with diameter dU(x), which is a weighted sum of the Euclidean distance and the angular difference. Like Plagemann et al. (2007) and Pfaff et al. (2007), an approximation of the above likelihood can be estimated online for each sample x by simulating L complete range scans at locations drawn from U (x) using the given map m of the environment.", "startOffset": 0, "endOffset": 206}, {"referenceID": 3, "context": "The Sick LMS 200 range finder is connected to a laptop that controls the motion of the Kuka 361 industrial robot over the network using Corba-facilities in the Open Robot Control Software, Orocos (Bruyninckx, 2001; Soetens, 2006).", "startOffset": 196, "endOffset": 229}, {"referenceID": 21, "context": "The Sick LMS 200 range finder is connected to a laptop that controls the motion of the Kuka 361 industrial robot over the network using Corba-facilities in the Open Robot Control Software, Orocos (Bruyninckx, 2001; Soetens, 2006).", "startOffset": 196, "endOffset": 229}, {"referenceID": 3, "context": "The Sick LMS 200 range finder is connected to a laptop that controls the motion of the Kuka 361 industrial robot over the network using Corba-facilities in the Open Robot Control Software, Orocos (Bruyninckx, 2001; Soetens, 2006). A simplified map of the environment (Fig. 16) is built to simulate the 150 complete range scans needed to construct a full scan model. The marginal P (zb | x,m) of two selected beams are studied in more detail. The marginal likelihoods for the selected beam using the proposed sample-based approximation of Eq. (108) and the Gaussian approximation proposed by Pfaff et al. (2007), are compared in Fig.", "startOffset": 197, "endOffset": 611}, {"referenceID": 5, "context": "(2006) is suited for use in dynamic environments since it uses the four component mixture beam model (Thrun et al., 2005; Choset et al., 2005).", "startOffset": 101, "endOffset": 142}, {"referenceID": 16, "context": "2 Sample-based Adaptive Full Scan Model for Dynamic Environments The adaptive beam model proposed by Pfaff et al. (2006) is suited for use in dynamic environments since it uses the four component mixture beam model (Thrun et al.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": ", 2005; Choset et al., 2005). To date however, the adaptive full scan likelihood models of Pfaff et al. (2007) and Plagemann et al.", "startOffset": 8, "endOffset": 111}, {"referenceID": 5, "context": ", 2005; Choset et al., 2005). To date however, the adaptive full scan likelihood models of Pfaff et al. (2007) and Plagemann et al. (2007) have not been adapted for dynamic environments.", "startOffset": 8, "endOffset": 139}, {"referenceID": 17, "context": "17(a)) obtained from the adaptive full scan model for dynamic environments using the proposed samplebased approximation and the Gaussian approximation proposed by Pfaff et al. (2007). In contrast to the Gaussian-based state of the art full scan model, the proposed sample-based approximation is able to handle the multi-modality of the range finder data.", "startOffset": 163, "endOffset": 183}, {"referenceID": 19, "context": "In contrast to the model of Thrun et al. (2005), the assumption underlying the non-physical discontinuity in the RBBM is discovered.", "startOffset": 28, "endOffset": 48}, {"referenceID": 19, "context": "In contrast to the model of Thrun et al. (2005), the assumption underlying the non-physical discontinuity in the RBBM is discovered. Furthermore, the paper proposes a different functional form for the probability of range measurements caused by unmodeled objects Poccl (z | x,m) (Eq. (45)), i.e. quadratic rather than exponential as proposed by Thrun et al. Furthermore, compared to the work of Thrun et al. (2005), Choset et al.", "startOffset": 28, "endOffset": 415}, {"referenceID": 5, "context": "(2005), Choset et al. (2005), Pfaff et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 5, "context": "(2005), Choset et al. (2005), Pfaff et al. (2006) the RBBM depends on fewer parameters, while maintaining the same representational power for experimental data.", "startOffset": 8, "endOffset": 50}, {"referenceID": 18, "context": "z[m] Experimental data (1500 samples) Gaussian approximation (Pfaff et al., 2007) Sample-based approximation", "startOffset": 61, "endOffset": 81}, {"referenceID": 18, "context": "z[m] Experimental data (1500 samples) Gaussian approximation (Pfaff et al., 2007) Sample-based approximation", "startOffset": 61, "endOffset": 81}, {"referenceID": 18, "context": "beam \u2206 P c (z | ,m ) Gaussian-based approximation (Pfaff et al., 2007) Sample-based approximation", "startOffset": 50, "endOffset": 70}, {"referenceID": 17, "context": "(b) and (c) show the marginal likelihood P (zb | x,m) for the two selected beams together with the histogram of the experimentally recorded range finder data, the Gaussian-based approximation (L = 150) of Pfaff et al. (2007), and the sample-based approximation (L = 150) of this paper.", "startOffset": 205, "endOffset": 225}, {"referenceID": 17, "context": "17(a) together with the Gaussian-based approximation (L = 150) of Pfaff et al. (2007) and the sample-based approximation (L = 150) extended for the use in dynamic environments.", "startOffset": 66, "endOffset": 86}, {"referenceID": 19, "context": "Using two sets of learning experiments from different application areas in robotics (one reported by Thrun et al. (2005)) the RBBM was shown to explain the obtained measurements at least as well as the state of the art model of Thrun et al.", "startOffset": 101, "endOffset": 121}, {"referenceID": 17, "context": "In contrast to the Gaussian-based state of the art models of Plagemann et al. (2007) and Pfaff et al.", "startOffset": 61, "endOffset": 85}, {"referenceID": 17, "context": "(2007) and Pfaff et al. (2007), the proposed full scan model uses a sample-based approximation, which can cope with dynamic environments and with multi-modality (which was shown to occur even in simple static environments).", "startOffset": 11, "endOffset": 31}], "year": 2008, "abstractText": "This paper proposes and experimentally validates a Bayesian network model of a range finder adapted to dynamic environments. All modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. This approach results in a transparent and intuitive model. With respect to the state of the art beam model this paper: (i) proposes a different functional form for the probability of range measurements caused by unmodeled objects, (ii) intuitively explains the discontinuity encountered in the state of the art beam model, and (iii) reduces the number of model parameters, while maintaining the same representational power for experimental data. The proposed beam model is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood and a variational Bayesian estimator (both based on expectation-maximization) are proposed to learn the model parameters. Furthermore, the RBBM is extended to a full scan model in two steps: first, to a full scan model for static environments and next, to a full scan model for general, dynamic environments. The full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter. In contrast to Gaussian-based state of the art models, the proposed full scan model uses a sample-based approximation. This sample-based approximation enables handling dynamic environments and capturing multimodality, which occurs even in simple static environments.", "creator": null}}}