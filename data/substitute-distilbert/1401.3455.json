{"id": "1401.3455", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Monte Carlo Sampling Methods for Approximating Interactive POMDPs", "abstract": "partially specified markov decision processes ( pomdps ) provide a principled framework facilitating sequential systems in uncertain single agent settings. an extension of pomdps to multiagent searches, called interactive pomdps ( i - pomdps ), replaces pomdp query spaces with interactive prediction belief systems which represent an agent's belief about the physical world, about discussions of other situations, and about their thinking about others'beliefs. this modification makes the difficulties of obtaining solutions due to comparison of shared belief and policy spaces even more acute.! describe you general method for obtaining approximate solutions of i - pomdps based on particle filtering ( pf ). we introduce the interactive pf, which descends the levels like the interactive belief hierarchies and samples and propagates experiments at cloud level. though interactive pf is able help mitigate the belief space degradation, but specifically does not address its policy space level. & mitigate the policy space complexity - - sometimes also called the curse of history - - we utilize any complementary method based on sampling likely observations against building the look ahead reachability tree. while this approach does not completely address the curse of history, it beats back the curse's impact substantially. we provide experimental results and chart future work.", "histories": [["v1", "Wed, 15 Jan 2014 05:14:08 GMT  (574kb)", "http://arxiv.org/abs/1401.3455v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prashant doshi", "piotr j gmytrasiewicz"], "accepted": false, "id": "1401.3455"}, "pdf": {"name": "1401.3455.pdf", "metadata": {"source": "CRF", "title": "Monte Carlo Sampling Methods for Approximating Interactive POMDPs", "authors": ["Prashant Doshi", "Piotr J. Gmytrasiewicz"], "emails": ["PDOSHI@CS.UGA.EDU", "PIOTR@CS.UIC.EDU"], "sections": [{"heading": "1. Introduction", "text": "Interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005; Seuken & Zilberstein, 2008) are a generalization of POMDPs to multiagent settings and offer a principled decision-theoretic framework for sequential decision making in uncertain multiagent settings. I-POMDPs are applicable to autonomous self-interested agents who locally compute what actions they should execute to optimize their preferences given what they believe while interacting with others with possibly conflicting objectives. Though POMDPs can be used in multiagent settings, it is so only under the strong assumption that the other agent\u2019s behavior be adequately represented implicitly (say, as noise) within the POMDP model (see Boutilier, Dean, & Hanks, 1999; Gmytrasiewicz & Doshi, 2005, for examples). The approach adopted in I-POMDPs is to expand the traditional state space to include models of other agents. Some of these models are the sophisticated intentional models, which ascribe beliefs, preferences, and rationality to others and are analogous to the notion of agent\nc\u00a92009 AI Access Foundation. All rights reserved.\ntypes in Bayesian games (Harsanyi, 1967; Mertens & Zamir, 1985). Other models, such as finite state machines, do not ascribe beliefs or rationality to other agents and we call them subintentional models. An agent\u2019s beliefs within I-POMDPs are called interactive beliefs, and they are nested analogously to the hierarchical belief systems considered in game theory (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Heifetz & Samet, 1998; Aumann, 1999), in theoretical computer science (Fagin, Halpern, Moses, & Vardi, 1995) and to the hyper-priors in hierarchical Bayesian models (Gelman, Carlin, Stern, & Rubin, 2004). Since the interactive beliefs may be infinitely nested, Gmytrasiewicz and Doshi (2005) defined finitely nested I-POMDPs as computable specializations of the infinitely nested ones. Solutions of finitely nested I-POMDPs map an agent\u2019s states of belief about the environment and other agents\u2019 models to policies. Consequently, I-POMDPs find important applications in agent, human, and mixed agent-human environments. Some potential applications include path planning in multi-robot environments, coordinating troop movements in battlefields, planning the course of a treatment in a multi-treatment therapy, and explaining commonly observed social behaviors (Doshi, Zeng, & Chen, 2007).\nHowever, optimal decision making in uncertain multiagent settings is computationally very hard requiring significant time and memory resources. For example, the problem of solving decentralized POMDPs has been shown to lie in the NEXP-complete class (Bernstein, Givan, Immerman, & Zilberstein, 2002). Expectedly, exact solutions of finitely nested I-POMDPs are difficult to compute as well, due to two primary sources of intractability: (i) The complexity of the belief representation which is proportional to the dimensions of the belief simplex, sometimes called the curse of dimensionality. (ii) The complexity of the space of policies, which is proportional to the number of possible future beliefs, also called the curse of history.\nBoth these sources of intractability exist in POMDPs also (see Pineau, Gordon, & Thrun, 2006; Poupart & Boutilier, 2004) but the curse of dimensionality is especially more acute in I-POMDPs. This is because in I-POMDPs the complexity of the belief space is even greater; the beliefs may include beliefs about the physical environment, and possibly the agent\u2019s beliefs about other agents\u2019 beliefs, about their beliefs about others\u2019, and so on. Thus, a contributing factor to the curse of dimensionality is the level of belief nesting that is considered. As the total number of agent models grows exponentially with the increase in nesting level, so does the solution complexity.\nWe observe that one approach to solving a finitely nested I-POMDP is to investigate collapsing the model to a traditional POMDP, and utilize available approximation methods that apply to POMDPs. However, the transformation into a POMDP is not straightforward. In particular, it does not seem possible to model the update of other agents\u2019 nested beliefs as a part of the transition function in the POMDP. Such a transition function would include nested beliefs and require solutions of others\u2019 models in defining it, and thus be quite different from the standard ones to which current POMDP approaches apply.\nIn this article, we present the first set of generally applicable methods for computing approximately optimal policies for the finitely nested I-POMDP framework while demonstrating computational savings. Since an agent\u2019s belief is defined over other agents\u2019 models, which may be a complex infinite space, sampling methods which are able to approximate distributions over large spaces to arbitrary accuracy are a promising approach. We adopt the particle filter (Gordon, Salmond, & Smith, 1993; Doucet, Freitas, & Gordon, 2001) as our point of departure. There is growing empirical evidence (Koller & Lerner, 2001; Daum & Huang, 2002) that particle filters are unable to significantly reduce the adverse impact of increasing state spaces. Specifically, the number of particles needed to maintain the error from the exact state estimation increases as the number of dimensions increase.\nHowever, the rate of convergence of the approximate posterior to the true one is independent of the dimensions of the state space (Crisan & Doucet, 2002) under weak assumptions. In other words, while we may need more particles to maintain error as the state space increases, the rate at which the error reduces remains unchanged, regardless of the state space. Furthermore, sampling approaches allow us to focus resources on the regions of the state space that are considered more likely in an uncertain environment, providing a strong potential for computational savings.\nWe generalize the particle filter, and more specifically the bootstrap filter (Gordon et al., 1993), to the multiagent setting, resulting in the interactive particle filter (I-PF). The generalization is not trivial: We do not simply treat the other agent as an automaton whose actions follow a fixed and known distribution. Rather, we consider the case where other agents are intentional \u2013 they possess beliefs, capabilities and preferences. Subsequently, the propagation step in the I-PF becomes more complicated than in the standard PF. In projecting the subject agent\u2019s belief over time, we must project the other agent\u2019s belief, which involves predicting its action and anticipating its observations. Mirroring the hierarchical character of interactive beliefs, the interactive particle filtering involves sampling and propagation at each of the hierarchical levels of the beliefs. We empirically demonstrate the ability of the I-PF to flexibly approximate the state estimation in I-POMDPs, and show the computational savings obtained in comparison to a regular grid based implementation. However, as we sample an identical number of particles at each nesting level, the total number of particles and the associated complexity, continues to grow exponentially with the nesting level.\nWe combine the I-PF with value iteration on sample sets thereby providing a general way to solve finitely nested I-POMDPs. Our approximation method is anytime and is applicable to agents that start with a prior belief and optimize over finite horizons. Consequently, our method finds applications for online plan computation. We derive error bounds for our approach that are applicable to singly-nested I-POMDPs and discuss the difficulty in generalizing the bounds to multiply nested beliefs. We empirically demonstrate the performance and computational savings obtained by our method on standard test problems as well as a larger uninhabited aerial vehicle (UAV) reconnaissance problem.\nWhile the I-PF is able to flexibly mitigate the belief space complexity, it does not address the policy space complexity. In order to mitigate the curse of history, we present a complementary method based on sampling observations while building the look ahead reachability tree during value iteration. This translates into considering only those future beliefs during value iteration that an agent is likely to have from a given belief. This approach is similar in spirit to the sparse sampling techniques used in generating partial look ahead trees for action selection during reinforcement learning (Kearns, Mansour, & Ng, 2002; Wang, Lizotte, Bowling, & Schuurmans, 2005) and for online planning in POMDPs (Ross, Pineau, Paquet, & Chaib-draa, 2008). While these approaches were applied in single agent reinforcement learning problems, we focus on a multiagent setting and recursively apply the technique to solve models of all agents at each nesting level. Observation sampling was also recently utilized in DEC-POMDPs (Seuken & Zilberstein, 2007), where it was shown to improve the performance on large problems. We note that this approach does not completely address the curse of history, but beats back its impact on the difficulty of computing the I-POMDP solutions, substantially. We report on the additional computational savings obtained when we combine this method with the I-PF, and provide empirical results in support.\nRest of this article is structured in the following manner. We review the various state estimation methods and their relevance, and the use of particle filters in previous works in Section 2. In Section 3, we review the traditional particle filtering technique concentrating on bootstrap filters in\nparticular. We briefly outline the finitely nested I-POMDP framework in Section 4 and the multiagent tiger problem used for illustration in Section 5. In Section 6, we discuss representations for the nested beliefs and the inherent difficulty in formulating them. In order to facilitate understanding, we give a decomposition of the I-POMDP belief update in Section 7. We then present the I-PF that approximates the finitely nested I-POMDP belief update in Section 8. This is followed by a method that utilizes the I-PF to compute solutions to I-POMDPs, in Section 9. We also comment on the asymptotic convergence and compute error bounds of our approach. In Section 10, we report on the performance of our approximation method on simple and larger test problems. In Section 11, we provide a technique for mitigating the curse of history, and report on some empirical results. Finally, we conclude this article and outline future research directions in Section 12."}, {"heading": "2. Related Work", "text": "Several approaches to nonlinear Bayesian estimation exist. Among these, the extended Kalman filter (EKF) (Sorenson, 1985), is most popular. The EKF linearises the estimation problem so that the Kalman filter can be applied. The required probability density function (p.d.f.) is still approximated by a Gaussian, which may lead to filter divergence, and therefore an increase in the error. Other approaches include the Gaussian sum filter (Sorenson & Alspach, 1971), and superimposing a grid over the state space with the belief being evaluated only over the grid points (Kramer & Sorenson, 1988). In the latter approach, the choice of an efficient grid is non-trivial, and the method suffers from the curse of dimensionality: The number of grid points that must be considered is exponential in the dimensions of the state space. Recently, techniques that utilize Monte Carlo (MC) sampling for approximating the Bayesian state estimation problem have received much attention. These techniques are general enough, in that, they are applicable to both linear, as well as, non-linear problem dynamics, and the rate of convergence of the approximation error to zero is independent of the dimensions of the underlying state space. Among the spectrum of MC techniques, two that have been particularly well-studied in sequential settings are Markov chain Monte Carlo (MCMC) (Hastings, 1970; Gelman et al., 2004), and particle filters (Gordon et al., 1993; Doucet et al., 2001). Approximating the I-POMDP belief update using the former technique, may turn out to be computationally exhaustive. Specifically, MCMC algorithms that utilize rejection sampling (e.g. Hastings, 1970) may cause a large number of intentional models to be sampled, solved, and rejected, before one is utilized for propagation. In addition, the complex estimation process in I-POMDPs makes the task of computing the acceptance ratio for rejection sampling computationally inefficient. Although Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, it would involve sampling from a conditional distribution of the physical state given the observation history and model of other, and from the distribution of the other\u2019s model given the physical state. However, these distributions are neither efficient to compute nor easy to derive analytically. Particle filters need not reject solved models and compute a new model in replacement, propagating all solved models over time and resampling them. They are intuitively amenable to approximating the I-POMDP belief update and produce reasonable approximations of the posterior while being computationally feasible.\nParticle filters previously have been successfully applied to approximate the belief update in continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001). While Thrun (2000) integrates particle filtering with Q-learning to learn the policy, Poupart et al. (2001) assume the prior existence of an exact value function and present an error bound analysis of substituting the POMDP belief update with particle filters. Loosely related to our work are\nthe sampling algorithms that appear in (Ortiz & Kaelbling, 2000) for selecting actions in influence diagrams, but this work does not focus on sequential decision making. In the multiagent setting, particle filters have been employed for collaborative multi-robot localization (Fox, Burgard, Kruppa, & Thrun, 2000). In this application, the emphasis was on predicting the position of the robot, and not the actions of the other robots, which is a critical step in our approach. Additionally, to facilitate fast localization, beliefs of other robots encountered during motion were considered to be fully observable to enable synchronization.\nWithin the POMDP literature, approaches other than sampling methods have also appeared that address the curse of dimensionality. An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) to uncover a low dimensional belief subspace that usually encompasses a robot\u2019s potential beliefs. The method is based on the observation that beliefs along many real-world trajectories exhibit only a few degrees of freedom. The effectiveness of these methods is problem specific; indeed, it is possible to encounter problems where no substantial belief compression may occur. When applied to the I-POMDP framework, the effectiveness of the compression techniques would depend, for example, on the existence of agent models whose likelihoods within the agent\u2019s belief do not change after successive belief updates or on the existence of correlated agent models. Whether such models exist in practice is a topic of future work.\nTechniques that address the curse of history in POMDPs also exist. Poupart and Boutilier (2004) generate policies via policy iteration using finite state controllers with a bounded number of nodes. Pineau et al. (2006) perform point-based value iteration (PBVI) by selecting a small subset of reachable belief points at each step from the belief simplex and planning only over these belief points. Doshi and Perez (2008) outline the challenges and develop PBVI for I-POMDPs. Though our method of mitigating the curse of history is conceptually close to point based selection methods, we focus on plan computation when the initial belief is known while the previously mentioned methods are typically utilized for offline planning. An approximate way of solving POMDPs online is the RTBSS approach (Paquet, Tobin, & Chaib-draa, 2005; Ross et al., 2008) that adopts the branch-andbound technique for pruning the look ahead reachability tree. This approach focuses on selecting the best action to expand which is complementary to our approach of sampling the observations. Further, its extension to the multiagent setting as formalized by I-POMDPs may not be trivial due to the need for a bounding heuristic function whose formulation in multiagent settings remains to be investigated."}, {"heading": "3. Background: Particle Filter for the Single Agent Setting", "text": "To act rationally in uncertain settings, agents need to track the evolution of the state over time, based on the actions they perform and the available observations. In single agent settings, the state estimation is usually accomplished with a technique called the Bayes filter (Russell & Norvig,\n2003). A Bayes filter allows the agent to maintain a belief about the state of the world at any given time, and update this belief each time an action is performed and new sensory information arrives. The convenience of this approach lies in the fact that the update is independent of the past percepts and action sequences. This is because the agent\u2019s belief is a sufficient statistic: it fully summarizes all of the information contained in past actions and observations.\nThe operation of a Bayes filter can be decomposed into a two-step process:\n\u2022 Prediction: When an agent performs a new action, at\u22121, its prior belief state is updated:\nPr(st|at\u22121, bt\u22121) =\n\u222b\nst\u22121 bt\u22121(st\u22121)T (st|st\u22121, at\u22121)dst\u22121 (1)\n\u2022 Correction: Thereafter, when an observation, ot, is received, the intermediate belief state, Pr(\u00b7|at\u22121, bt\u22121), is corrected:\nPr(st|ot, at\u22121, bt\u22121) = \u03b1O(ot|st, at\u22121)Pr(st|at\u22121, bt\u22121) (2)\nwhere \u03b1 is the normalizing constant, T is the transition function that gives the uncertain effect of performing an action on the physical state, and O is the observation function which gives the likelihood of receiving an observation from a state on performing an action.\nParticle filters (PF) (Gordon et al., 1993; Doucet et al., 2001) are specific implementations of Bayes filters tailored toward making Bayes filters applicable to non-linear dynamic systems. Rather than sampling directly from the target distribution which is often difficult, PFs adopt the method of importance sampling (Geweke, 1989), which allows samples to be drawn from a more tractable distribution called the proposal distribution, \u03c0. For example, if Pr(St|ot, at\u22121, bt\u22121) is the target posterior distribution, and \u03c0(St|ot, at\u22121, bt\u22121) the proposal distribution, and the support of \u03c0(St|ot, at\u22121, bt\u22121) includes the support of Pr(St|ot, at\u22121, bt\u22121), we can approximate the target posterior by sampling N i.i.d. particles {s(n), n = 1...N} according to \u03c0(St|ot, at\u22121, bt\u22121) and assigning to each particle a normalized importance weight:\nw(n) = w\u0303(s(n))\n\u2211N n=1 w\u0303(s (n)) where w\u0303(s(n)) =\nPr(s(n)|ot, at\u22121, bt\u22121)\n\u03c0(s(n)|ot, at\u22121, bt\u22121)\nEach true probability, Pr(s|ot, at\u22121, bt\u22121), is then approximated by:\nPrN (s|o t, at\u22121, bt\u22121) =\nN\u2211\nn=1\nw(n)\u03b4D(s\u2212 s (n))\nwhere \u03b4D(\u00b7) is the Dirac-delta function. As N \u2192 \u221e, PrN (s|ot, at\u22121, bt\u22121) a.s. \u2192 Pr(s|ot, at\u22121, bt\u22121). When applied recursively over several steps, importance sampling leads to a large variance in the weights. To avoid this degeneracy, Gordon et al. (1993) suggested inserting a resampling step, which would increase the population of those particles that had high importance weights. This has the beneficial effect of focusing the particles in the high likelihood regions supported by the observations and increasing the tracking ability of the PF. Since particle filtering extends importance sampling sequentially and appends a resampling step, it has also been called sequential importance sampling and resampling (SISR).\nThe general algorithm for the particle filtering technique is given by Doucet et al. (2001). We concentrate on a specific implementation of this algorithm, that has previously been studied under various names such as MC localization, survival of the fittest, and the bootstrap filter. The implementation maintains a set of N particles denoted by b\u0303t\u22121 independently sampled from the prior, bt\u22121, and takes an action and observation as input. Each particle is then propagated forwards in time, using the transition kernel T of the environment. Each particle is then weighted by the likelihood of perceiving the observation from the state that the particle represents, as given by the observation function O. This is followed by the (unbiased) resampling step, in which particles are picked proportionately to their weights, and a uniform weight is subsequently attached to each particle. We outline the algorithm of the bootstrap filter in Fig. 1. Crisan and Doucet (2002) outline a rigorous proof of the convergence of this algorithm toward the true posterior as N \u2192 \u221e.\nLet us understand the working of the PF in the context of a simple example \u2013 the single agent tiger problem (Kaelbling, Littman, & Cassandra, 1998). The single agent tiger problem resembles a game show in which the agent has to choose to open one of two doors behind which lies either a valuable prize or a dangerous tiger. Apart from actions that open doors, the subject has the option of listening for the tiger\u2019s growl coming from the left, or the right door. However, the subject\u2019s hearing is imperfect, with given percentages (say, 15%) of false positive and false negative occurrences. Following Kaelbling et al. (1998), we assume that the value of the prize is 10, that the pain associated with encountering the tiger can be quantified as -100, and that the cost of listening is -1.\nLet the agent have a prior belief according to which it is uninformed about the location of the tiger. In other words, it believes with a probability of 0.5 that the tiger is behind the left door (TL), and with a similar probability that the tiger is behind the right door (TR). We will see how the agent approximately updates its belief using the particle filter when, say, it listens (L) and hears a growl from the left (GL). Fig. 2 illustrates the particle filtering process. Since the agent is uninformed about the tiger\u2019s location, we start with an equal number of particles (samples) denoting TL (lightly\nshaded) and TR (darkly shaded). The initial sample set is approximately representative of the agent\u2019s prior belief of 0.5. Since listening does not change the location of the tiger, the composition of the sample set remains unchanged after propagation. On hearing a growl from the left, the light particles denoting TL will be tagged with a larger weight (0.85) because they are more likely to be responsible for GL, than the dark particles denoting TR (0.15). Here, the size of the particle is proportional to the weight attached to the particle. Finally, the resampling step yields the sample set at time step t, which contains more particles denoting TL than TR. This sample set approximately represents the updated belief of 0.85 of the agent that the tiger is behind the left door. Note that the propagation carries out the task of prediction as shown in Eq. 1 approximately, while the correction step (Eq. 2) is approximately performed by weighting and resampling."}, {"heading": "4. Overview of Finitely Nested I-POMDPs", "text": "I-POMDPs (Gmytrasiewicz & Doshi, 2005) generalize POMDPs to handle multiple agents. They do this by including models of other agents in the state space. We focus on finitely nested I-POMDPs here, which are the computable counterparts of I-POMDPs in general. For simplicity of presentation let us consider an agent, i, that is interacting with one other agent, j. The arguments generalize to a setting with more than two agents in a straightforward manner.\nDefinition 1 (I-POMDPi,l). A finitely nested interactive POMDP of agent i, I-POMDPi,l, is:\nI-POMDPi,l = \u3008ISi,l, A, Ti,\u2126i, Oi, Ri\u3009\nwhere: \u2022 ISi,l is a set of interactive states defined as ISi,l = S \u00d7Mj,l\u22121, l \u2265 1, and ISi,0 = S,1 where S is the set of states of the physical environment, and Mj,l\u22121 is the set of possible models of agent j.\n1. If there are more agents participating in the interaction, K > 2, then ISi,l = S \u00d7K\u22121j=1 Mj,l\u22121\nEach model, mj,l\u22121 \u2208 Mj,l\u22121, is defined as a triple, mj,l\u22121 = \u3008hj , fj , Oj\u3009, where fj : Hj \u2192 \u2206(Aj) is agent j\u2019s function, assumed computable, which maps possible histories of j\u2019s observations, Hj , to distributions over its actions. hj is an element of Hj , and Oj is a function, also computable, specifying the way the environment is supplying the agent with its input. For simplicity, we may write model mj,l\u22121 as mj,l\u22121 = \u3008hj , m\u0302j\u3009, where m\u0302j consists of fj and Oj .\nA specific class of models are the (l\u22121)th level intentional models, \u0398j,l\u22121, of agent j: \u03b8j,l\u22121 = \u3008bj,l\u22121, A,\u2126j , Tj , Oj , Rj , OCj\u3009. bj,l\u22121 is agent j\u2019s belief nested to the level l\u22121, bj,l\u22121 \u2208 \u2206(ISj,l\u22121), and OCj is j\u2019s optimality criterion. Rest of the notation is standard. We may rewrite \u03b8j,l\u22121 as, \u03b8j,l\u22121 = \u3008bj,l\u22121, \u03b8\u0302j\u3009, where \u03b8\u0302j \u2208 \u0398\u0302j includes all elements of the intentional model other than the belief and is called the agent j\u2019s frame. The intentional models are analogous to types as used in Bayesian games (Harsanyi, 1967).\nAs mentioned by Gmytrasiewicz and Doshi (2005), we may also ascribe the subintentional models, SMj , which constitute the remaining models in Mj,l\u22121. Examples of subintentional models are finite state controllers and fictitious play models (Fudenberg & Levine, 1998). While we do not consider these models here, they could be accommodated in a straightforward manner.\nIn order to promote understanding, let us define the finitely nested interactive state space in an inductive manner:\nISi,0 = S, \u0398j,0 = {\u3008bj,0, \u03b8\u0302j\u3009 : bj,0 \u2208 \u2206(ISj,0), A = Aj}, ISi,1 = S \u00d7\u0398j,0, \u0398j,1 = {\u3008bj,1, \u03b8\u0302j\u3009 : bj,1 \u2208 \u2206(ISj,1)}, . . . . . .\nISi,l = S \u00d7\u0398j,l\u22121, \u0398j,l = {\u3008bj,l, \u03b8\u0302j\u3009 : bj,l \u2208 \u2206(ISj,l)}.\nRecursive characterizations of state spaces analogous to above have appeared previously in the game-theoretic literature (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Battigalli & Siniscalchi, 1999) where they have led to the definitions of hierarchical belief systems. These have been proposed as mathematical formalizations of type spaces in Bayesian games. Additionally, the nested beliefs are, in general, analogous to hierarchical priors utilized for Bayesian analysis of hierarchical data (Gelman et al., 2004). Hierarchical priors arise when unknown priors are assumed to be drawn from a population distribution, whose parameters may themselves be unknown thereby motivating a higher level prior.\n\u2022 A = Ai \u00d7Aj is the set of joint moves of all agents.\n\u2022 Ti is a transition function, Ti : S\u00d7A\u00d7S \u2192 [0, 1] which describes the results of the agent\u2019s actions on the physical states of the world. (It is assumed that actions can directly change the physical state only, see Gmytrasiewicz & Doshi, 2005).\n\u2022 \u2126i is the set of agent i\u2019s observations.\n\u2022 Oi is an observation function, Oi : S \u00d7A\u00d7 \u2126i \u2192 [0, 1] which gives the likelihood of perceiving observations in the state resulting from performing the action. (It is assumed that only the physical state is directly observable, and not the models of the other agent.)\n\u2022 Ri is defined as, Ri : ISi \u00d7A \u2192 R. While an agent is allowed to have preferences over physical states and models of other agents, usually only the physical state will matter."}, {"heading": "4.1 Belief Update", "text": "Analogous to POMDPs, an agent within the I-POMDP framework also updates its belief as it acts and observes. However, there are two differences that complicate a belief update in multiagent settings, when compared to single agent ones. First, since the state of the physical environment depends on the actions performed by both agents, the prediction of how the physical state changes has to be made based on the predicted actions of the other agent. The probabilities of other\u2019s actions are obtained based on its models. Second, changes in the models of the other agent have to be included in the update. Specifically, since the other agent\u2019s model is intentional the update of the other agent\u2019s beliefs due to its new observation has to be included. In other words, the agent has to update its beliefs based on what it anticipates that the other agent observes and how it updates. The belief update function for an agent in the finitely nested I-POMDP framework is:\nbti(is t) = \u03b1\n\u222b\nist\u22121:\u03b8\u0302t\u22121j =\u03b8\u0302 t j\nbt\u22121i,l (is t\u22121)\n\u2211\nat\u22121j\nPr(at\u22121j |\u03b8 t\u22121 j,l\u22121) Oi(s t, at\u22121, oti) Ti(s t\u22121, at\u22121, st)\n\u00d7 \u2211 otj \u03b4D(SE\u03b8\u0302tj (bt\u22121j,l\u22121, a t\u22121 j , o t j)\u2212 b t j,l\u22121) Oj(s t, at\u22121, otj) d is t\u22121\n(3) where \u03b1 is the normalization constant, \u03b4D is the Dirac-delta function, SE\u03b8\u0302tj (\u00b7) is an abbreviation denoting the belief update, and Pr(at\u22121j |\u03b8 t\u22121 j,l\u22121) is the probability that a t\u22121 j is Bayes rational for the agent described by \u03b8t\u22121j,l\u22121. If j is also modeled as an I-POMDP, then i\u2019s belief update invokes j\u2019s belief update (via the term SE \u03b8\u0302tj (bt\u22121j,l\u22121, a t\u22121 j , o t j)), which in turn invokes i\u2019s belief update and so on. This recursion in belief nesting bottoms out at the 0th level. At this level, belief update of the agent reduces to a POMDP based belief update. 2 For an illustration of the belief update, additional details on I-POMDPs, and how they compare with other multiagent planning frameworks, see (Gmytrasiewicz & Doshi, 2005).\nIn a manner similar to the belief update in POMDPs, the following proposition holds for the I-POMDP belief update. The proposition results from noting that Eq. 3 expresses the belief in terms of parameters of the previous time step only. A complete proof of the belief update and this proposition is given by Gmytrasiewicz and Doshi (2005).\nProposition 1. (Sufficiency) In a finitely nested I-POMDPi,l of agent i, i\u2019s current belief, i.e., the probability distribution over the set S \u00d7 \u0398j,l\u22121, is a sufficient statistic for the past history of i\u2019s observations."}, {"heading": "4.2 Value Iteration", "text": "Each level l belief state in I-POMDPi,l has an associated value reflecting the maximum payoff the agent can expect in this belief state:\nU t(\u3008bi,l, \u03b8\u0302i\u3009) = max ai\u2208Ai { \u222b is\u2208ISi,l ERi(is, ai)bi,l(is)d is+\n\u03b3 \u2211\noi\u2208\u2126i\nPr(oi|ai, bi,l)U t\u22121(\u3008SE\n\u03b8\u0302i (bi,l, ai, oi), \u03b8\u0302i\u3009)\n} (4)\n2. The 0th level model is a POMDP: Other agent\u2019s actions are treated as exogenous events and folded into T, O, and R.\nwhere, ERi(is, ai) = \u2211\naj Ri(is, ai, aj)Pr(aj |\u03b8j,l\u22121) (since is = (s, \u03b8j,l\u22121)).\nEq. 4 is a basis for value iteration in I-POMDPs, and can be succinctly rewritten as U t = HU t\u22121, where H is commonly known as the value backup operator. Analogous to POMDPs, H is both isotonic and contracting, thereby making the value iteration convergent (Gmytrasiewicz & Doshi, 2005).\nAgent i\u2019s optimal action, a\u2217i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT (\u03b8i), defined as:\nOPT (\u3008bi,l, \u03b8\u0302i\u3009) = argmax ai\u2208Ai { \u222b is\u2208ISi,l ERi(is, ai)bi,l(is)d is+\n\u03b3 \u2211\noi\u2208\u2126i\nPr(oi|ai, bi,l)U(\u3008SE\u03b8\u0302i(bi,l, ai, oi), \u03b8\u0302i\u3009)\n} (5)"}, {"heading": "5. Example: The Multiagent Tiger Problem", "text": "To illustrate our approximation methods, we utilize the multiagent tiger problem as an example. The multiagent tiger problem is a generalization of the single agent tiger problem outlined in Section 3 to the multiagent setting. For the sake of simplicity, we restrict ourselves to a two-agent setting, but the problem is extensible to more agents in a straightforward way.\nIn the two-agent tiger problem, each agent may open doors or listen. To make the interaction more interesting, in addition to the usual observation of growls, we added an observation of door creaks, which depends on the action executed by the other agent. Creak right (CR) is likely due to the other agent having opened the right door, and similarly for creak left (CL). Silence (S) is a good indication that the other agent did not open doors and listened instead. We assume that the accuracy of creaks is 90%, while the accuracy of growls is 85% as before. Again, the tiger location is chosen randomly in the next time step if any of the agents opened any doors in the current step. We also assume that the agent\u2019s payoffs are analogous to the single agent version. Note that the result of this assumption is that the other agent\u2019s actions do not impact the original agent\u2019s payoffs directly, but rather indirectly by resulting in states that matter to the original agent. Table 1 quantifies these factors.\nWhen an agent makes its choice in the multiagent tiger problem, it may find it useful to consider what it believes about the location of the tiger, as well as whether the other agent will listen or open a door, which in turn depends on the other agent\u2019s beliefs, preferences and capabilities. In particular, if the other agent were to open any of the doors, the tiger\u2019s location in the next time step would be chosen randomly. The information that the agent had about the tiger\u2019s location till then, would reduce to zero. We simplify the situation somewhat by assuming that all of the agent j\u2019s properties, except for beliefs, are known to i, and that j\u2019s time horizon is equal to i\u2019s. In other words, i\u2019s uncertainty pertains only to j\u2019s beliefs and not to its frame."}, {"heading": "6. Representing Prior Nested Beliefs", "text": "As we mentioned, there is an infinity of intentional models of an agent. Since an agent is unaware of the true models of interacting agents ex ante, it must maintain a belief over all possible candidate models. The complexity of this space precludes practical implementations of I-POMDPs for all\nbut the simplest settings. Approximations based on sampling use a finite set of sample points to represent a complete belief state.\nIn order to sample from nested beliefs we first need to represent them. Agent i\u2019s level 0 belief, bi,0 \u2208 \u2206(S), is a vector of probabilities over each physical state: bi,0 def = \u3008 pi,0(s1), pi,0(s2), . . ., pi,0(s|S|) \u3009. The first and second subscripts of bi,0 denote the agent and the level of nesting, respectively. Since belief is a probability distribution, \u2211|S|\nq=1 pi,0(sq) = 1. We refer to this constraint\nas the simplex constraint. As we may write, pi,0(s|S|) = 1 \u2212 \u2211|S|\u22121\nq=1 pi,0(sq), subsequently, only |S| \u2212 1 probabilities are needed to specify a level 0 belief.\nFor the tiger problem, let s1 = TL and s2 = TR. An example level 0 belief of i for the tiger problem, bi,0 def = \u3008pi,0(TL), pi,0(TR)\u3009, is \u30080.7, 0.3\u3009 that assigns a probability of 0.7 to TL and 0.3 to TR. Knowing pi,0(TL) is sufficient for a complete specification of the level 0 belief.\nAgent i\u2019s first level belief, bi,1 \u2208 \u2206(S \u00d7 \u0398j,0), is a vector of densities over j\u2019s level 0 beliefs, one for each combination of state and j\u2019s frame and possibly distinct from each other. Hence,\nthere are |S||\u0398\u0302j | such densities: bi,1 def = \u3008 p \u3008s,\u03b8\u0302j\u30091 i,1 , p \u3008s,\u03b8\u0302j\u30092 i,1 , . . ., p\n\u3008s,\u03b8\u0302j\u3009|S||\u0398\u0302j | i,1 \u3009, where \u3008s, \u03b8\u0302j\u3009k,\nk = 1, . . . , |S||\u0398\u0302j | is a particular state and j\u2019s frame combination. Because of the simplex constraint, the sum of integrals of the level 1 densities over the level 0 beliefs must be 1. We observe that the level 1 densities may be represented using any family of probability distributions such as exponential family (Dobson, 2002) or polynomials that allow approximation of any function up to arbitrary accuracy. As such, the densities could exhibit any shape given that they satisfy the simplex constraint.\nof the tiger ( \u222b 0.5 0 p \u3008TL,\u03b8\u0302\u2032\u3009 i,1 (pj,0)dpj,0 \u226a \u222b 1 0.5 p \u3008TL,\u03b8\u0302\u2032\u3009 i,1 (pj,0)dpj,0), though i itself is unaware of it as the marginal of each plot is 0.5.\nAn example level 1 belief of i, bi,1 def = \u3008p\n\u3008TL,\u03b8\u0302\u2032j\u3009 i,1 , p \u3008TR,\u03b8\u0302\u2032j\u3009\ni,1 \u3009, in the tiger problem is one according to which i is uninformed about j\u2019s level 0 beliefs and about the location of the tiger (see Fig. 3(a)). The superscript, \u03b8\u0302\u2032j , is agent j\u2019s frame that is known to i. Another example level 1 belief of i is one according to which it believes that j likely knows the location of the tiger (Fig. 3(b)).\nAgent i\u2019s second level belief, bi,2 \u2208 \u2206(S\u00d7\u0398j,1), is a vector of densities over j\u2019s level 1 beliefs for each state and j\u2019s intentional frame. In comparison to level 0 and level 1 beliefs, representing doubly-nested beliefs and beliefs with deeper nestings is not trivial. This is because these are distributions over density functions whose representations need not be finite. For example, let j\u2019s\nsingly-nested belief densities be represented using the family of polynomials. Then, i\u2019s doublynested belief over j\u2019s densities is a vector of normalized mathematical functions of variables where the variables are the parameters of lower-level densities. Because the lower level densities are polynomials which could be of any degree and therefore any number of coefficients, the functions that represent doubly-nested beliefs may have an indefinite number of variables. Thus computable representations of i\u2019s level 2 beliefs are not trivially obtained. We formalize this observation using Proposition 2, which shows that multiply-nested beliefs are necessarily partial functions that fail to assign a probability to some elements (lower level beliefs) in their domain.\nProposition 2. Agent i\u2019s multiply nested belief, bi,l, l \u2265 2, is strictly a partial recursive function.\nProof. We briefly revisit the definition of nested beliefs: bi,l \u2208\u2206(ISi,l) = \u2206(S\u00d7\u0398j,l\u22121) = \u2206(S\u00d7 \u3008Bj,l\u22121, \u0398\u0302j\u3009), where Bj,l\u22121 is the level l \u2212 1 belief simplex and \u0398\u0302j is the set of frames of j. As the basis case, let l = 2, then bi,2 \u2208 \u2206(S \u00d7 \u3008Bj,1, \u0398\u0302j\u3009). Because the state and frame spaces are discrete, bi,2 may be represented using a collection of density functions on j\u2019s beliefs, one for each discrete state and j\u2019s frame combination, p \u3008s,\u03b8\u0302j\u3009 i,2 (bj,1), where bj,1 \u2208 Bj,1. Notice that, bj,1 being singly-nested, is itself a collection of densities over j\u2019s level 0 beliefs, one for each state and i\u2019s\nframe combination. Thus, as mentioned before let bj,1 def = \u3008 p \u3008s,\u03b8\u0302i\u30091 j,1 , p \u3008s,\u03b8\u0302i\u30092 j,1 , . . ., p \u3008s,\u03b8\u0302i\u3009|S||\u0398\u0302i| j,1 \u3009.\nRecall from Section 4 that the models and therefore the belief density functions are assumed computable. Let x be the program of length in bits, l(x), that encodes, say p\u3008s,\u03b8\u0302i\u30091j,1 , in the language g. Then define the complexity of the density function, p\u3008s,\u03b8\u0302i\u30091j,1 , as: Cg(p \u3008s,\u03b8\u0302i\u30091 j,1 ) = min {l(x) : g(x) = p \u3008s,\u03b8\u0302i\u30091 j,1 }. Cg(\u00b7) is the minimum length program in language g that computes the argument. 3 We observe that l(x) is proportional to the number of parameters that describe p\u3008s,\u03b8\u0302i\u30091j,1 . Because the number of parameters of a density need not be bounded, l(x) and consequently the complexity of the density may not be finite. Intuitively, this is equivalent to saying that the density could have \u201cany shape\u201d.\nAssume, by way of contradiction, that the level 2 density function, p \u3008s,\u03b8\u0302j\u3009 i,2 is a total recursive function. Construct a Turing machine, T , that computes it. Because p \u3008s,\u03b8\u0302j\u3009 i,2 is total, T will halt on all inputs. Specifically, T will read the set of symbols on its input tape that describe the level 1 density function (the program, x), and once it has finished reading it halts and leaves a number between 0 and 1 on the output tape. This number is the output of the density function encoded by T . Note that T does not execute the input program x, but simply parses it to enable identification. Thus T is not a universal Turing machine. As we mentioned previously, the minimum length program (and hence the complexity) that encodes the level l density function may be infinite. Thus the size of the set of symbols on the input tape of T , l(x), may be infinite, and T may not halt. But this is a contradiction. Thus, p \u3008s,\u03b8\u0302j\u3009 i,2 is a partial recursive function.\nThe argument may be extended inductively to further levels of nesting.\nAs multiply-nested beliefs in their general form are partial recursive functions that are not defined for every possible lower level belief in their domain, restrictions on the complexity of the\n3. Note that the complexity, Cg , is within a constant of the Kolmogorov complexity (Li & Vitanyi, 1997) of the density\nfunction, p\u3008s,\u03b8\u0302i\u30091j,1 .\nnested beliefs (where complexity is as defined in Proposition 2) are needed to allow for computability and so that they are well-defined. One way is to focus our attention on limited representations involving a bounded number of parameters.\nBecause of these complications, a general-purpose language for representing nested beliefs is beyond the scope of this article and we do not attempt it here; it is a topic of continuing investigations. Instead, we utilize specific examples of doubly-nested and more deeply nested beliefs for our experiments in the remainder of this article."}, {"heading": "7. Decomposing the I-POMDP Belief Update", "text": "Analogous to Eqs. 1 and 2 of the Bayes filter, we decompose the I-POMDP belief update into two steps. The decomposition not only facilitates a better understanding of the belief update, but also plays a pivotal role in the development of the approximation.\n\u2022 Prediction: When an agent, say i, performs an action at\u22121i , and agent j performs a t\u22121 j , the\npredicted belief state is:\nPr(ist|at\u22121i , a t\u22121 j , b t\u22121 i,l ) = \u222b ISt\u22121:\u03b8\u0302t\u22121j =\u03b8\u0302 t j bt\u22121i,l (is t\u22121)Pr(at\u22121j |\u03b8 t\u22121 j,l\u22121)\n\u00d7Ti(s t\u22121, at\u22121i , a t\u22121 j , s\nt) \u2211\notj Oj(s\nt, at\u22121i , a t\u22121 j , o t j)\n\u00d7\u03b4D(SE\u03b8\u0302tj (bt\u22121j,l\u22121, a t\u22121 j , o t j)\u2212 b t j,l\u22121) d is t\u22121\n(6)\nwhere \u03b4D is the Dirac-delta function, SE\u03b8\u0302tj (\u00b7) is an abbreviation for the belief update, Pr(at\u22121j | \u03b8t\u22121j,l\u22121) is the probability that a t\u22121 j is Bayes rational for the agent described by \u03b8 t\u22121 j,l\u22121.\n\u2022 Correction: When agent i perceives an observation, oti, the corrected belief state is a weighted sum of the predicted belief states for each possible action of j:\nPr(ist|oti, a t\u22121 i , b t\u22121 i,l ) = \u03b1\n\u2211\nat\u22121j\nOi(s t, at\u22121i , a t\u22121 j , o t i)Pr(is t|at\u22121i , a t\u22121 j , b t\u22121 i,l ) (7)\nwhere \u03b1 is the normalizing constant.\nEquations 6 and 7 along with Proposition 1 may be seen as a generalization of the single agent Bayes filter (see Section 3) to the multiagent setting. At a general level, these equations represent an application of the update of hierarchical priors given observed data (Gelman et al., 2004) to the problem of state estimation in multiagent settings."}, {"heading": "8. Interactive Particle Filter for the Multiagent Setting", "text": "We presented the algorithm for the traditional bootstrap filter in Section 3. As we mentioned before, the bootstrap filter is a MC sampling based randomized implementation of the POMDP belief update (Bayes filter). We generalize this implementation so as to approximate the I-POMDP belief update steps presented previously in Section 7."}, {"heading": "8.1 Description", "text": "Our generalization of the PF to the multiagent case, which we call an interactive particle filter (I-PF), similar to the basic PF, involves the key steps of importance sampling and selection. The resulting algorithm inherits the convergence properties of the original algorithm (Doucet et al., 2001). Specifically, the approximate posterior belief generated by the filter converges to the truth (as computed by Eqs. 6 and 7) as the number of particles (N ) tends to infinity. Note that the presence of other agents does not affect the convergence because, (a) the exact belief update that provides the stationary point similarly reasons about the presence of other agents, and (b) we explicitly model other agents\u2019 actions due to which the nonstationarity in the environment vanishes.\nThe extension of the PF to the multiagent setting turns out to be nontrivial because we are faced with predicting the other agent\u2019s action(s), which requires us to deal with an interactive belief hierarchy. Analogously to the I-POMDP belief update, the I-PF reduces to the traditional PF when there is only one agent in the environment.\nThe I-PF, described in Fig. 4, requires an initial set of N particles, b\u0303t\u22121k,l , that is approximately representative of the agent\u2019s prior belief, along with the action, at\u22121k , the observation, o t k, and the level of belief nesting, l > 0. As per our convention, k will stand for either agent i or j, and \u2212k for the other agent, j or i, as appropriate. Each particle, is(n)k , in the sample set represents the agent\u2019s possible interactive state, in which the other agent\u2019s belief may itself be a set of particles. Formally, is\n(n) k = \u3008s (n), \u03b8 (n) \u2212k \u3009, where \u03b8 (n) \u2212k = \u3008\u0303b (n) \u2212k,l\u22121, \u03b8\u0302 (n) \u2212k \u3009. Note that b\u0303 (n) k,0 is a probability distribution over the physical state space. We generate b\u0303t\u22121k,l by sampling N particles from the prior nested belief. Given a prior nested belief, this is a simple recursive procedure that first uses the marginals over the physical states and frames at the current nesting level to sample the state and frame of the other agent. We then sample N particles from the density over the lower level beliefs, conditioned on the sampled state and frame combination. If the belief is multiply nested, this operation is recursively performed bottoming out at the lowest level where the other agent\u2019s flat (level 0) beliefs are sampled.\nThe interactive particle filtering proceeds by propagating each particle forward in time. However, as opposed to the traditional particle filtering, this is not a one-step process: (i) In order to perform the propagation, other agent\u2019s action must be known. As the model ascribed to the other agent is intentional, this is obtained by solving the other agent\u2019s model (using the algorithm APPROXPOLICY described later in Section 9) to find a distribution over its actions, from which its action is sampled (lines 3\u20134 in Fig. 4). Specifically, if OPT is the set of optimal actions obtained by solving the model, then Pr(a\u2212k) = 1|OPT | if a\u2212k is in OPT, 0 otherwise. (ii) Additionally, analogous to the exact belief update, for each of the other agent\u2019s possible observations, we must update its model (line 6). Because the model is intentional, we must update its belief state. If l > 1, updating the other agent\u2019s belief requires recursively invoking the I-PF for performing its belief update (lines 12\u201314). This recursion in depth of the belief nesting terminates when the level of nesting becomes one, and a LEVEL0BELIEFUPDATE, described in Fig. 5, is performed (lines 8\u201310).4 In addition to using the agent\u2019s own observation for weighting, the other agent\u2019s observations also participate in the weighting process (lines 15\u201316). The latter allows us to distinguish j\u2019s beliefs that are more likely given the physical state. Though the propagation and weighting steps generate |\u2126\u2212k|N appropriately weighted particles, we resample N particles out of these (line 19), using an unbiased\n4. If the physical state space is also continuous or very large, then we would replace the level 0 belief update with a traditional particle filter. However, in doing so, we would loose the theoretical bounds given in Section 9.1\nresampling scheme. Lines 2\u201315 represent a simulation of the prediction step (Eq. 6), while lines 16\u201320 are the simulated implementation of the correction step (Eq. 7).\nAn alternative approach within the propagation step is to sample the other agent\u2019s observation because it is a hidden variable. We may then update its belief given the sampled observation. Although statistically equivalent to our approach, it involves an additional step of sampling, which further contributes to the sources of error in the I-PF. In particular, for lesser number of particles, other agent\u2019s beliefs resulting from low probability observations may not appear in the resampled posterior. This is because other agent\u2019s low probability observations are less likely to be sampled. Because the original agent\u2019s observation is independent of the other\u2019s belief, particles with identical physical states but different beliefs of the other are weighted equally. As the beliefs resulting from low probability observations are less frequent in the sample set, they are less likely to be picked in the resampled posterior. In comparison, weighting using the other agent\u2019s observation removes\nthe intermediate sampling step and a source of error but at the expense of temporarily generating a larger number of particles. Based on a preference for reduced approximation error or computational efficiency, one of the two alternative steps may be used.\nA visualization of our I-PF implementation is shown in Fig. 6. Note that the number of particles grows exponentially with the nesting level, due to which the approach becomes intractable for a larger number of levels. A method to limit the number of particles as we descend through the nesting level is needed to address this source of complexity. This is one line of our future work.\nNotice that the I-PF could also be viewed as a recursive implementation of an approximately Rao-Blackwellised particle filter (RBPF) (Doucet, de Freitas, Murphy, & Russell, 2000), where the conditional distribution over models is itself updated using a RBPF. Doshi (2007) presented a Rao-Blackwellised I-PF (RB-IPF), where the conditional distribution is updated using a variational Kalman filter. Although the performance of the RB-IPF improves on the I-PF, it is restricted to prior beliefs that are Gaussian and could not be generalized to beyond a single level of nesting."}, {"heading": "8.2 Illustration of the I-PF", "text": "We illustrate the operation of the I-PF using the multiagent tiger problem introduced in Section 5. For the sake of simplicity we consider i\u2019s prior belief to be singly nested, i.e. l = 1. The procedure is recursively performed for more deeply nested beliefs. Let i be uninformed about j\u2019s level 0 beliefs, and about the location of the tiger (see Fig. 3(a)). We demonstrate the operation of the I-PF for the case when i listens (L) and hears a growl from the left and no creaks, \u3008GL,S\u3009.\nIn Fig. 7, we show the initial sample set, b\u0303t\u22121i,1 , consisting of N = 2 particles that is approximately representative of i\u2019s singly-nested beliefs. Since we assume that j\u2019s frame is known, each\n~ b i,1 t-1\nparticle is an interactive state consisting of the tiger\u2019s location and j\u2019s level 0 belief. Let a belief of 0.5 of j be sampled from each of the flat line densities.\n~ bi,1 t-1\nAs we mentioned before, the propagation of the particles from time step t\u2212 1 to t is a two-step process. As the first step, we solve j\u2019s POMDP to compute its optimal action when its belief is 0.5. j\u2019s action is to listen since it does know the location of the tiger. We depict this in Fig. 8.\nThe second step of the propagation is to sample the next physical state for each particle using the transition function. Since both i and j listen, the location of the tiger remains unchanged. Next, we must update j\u2019s beliefs. We do this by anticipating what j might observe, and updating its belief exactly given its optimal action of listening. Since j could receive one of two possible observations \u2013 GL or GR \u2013 each particle \u201dsplits\u201d into two. This is shown using the dashed arrows going from particles in the initial sample set to the particles in the propagated sample set, in Fig. 9. When j hears a GL, its updated belief is 0.85 (that the tiger is behind the left door), otherwise it is 0.15 when it hears a GR. If the level of belief nesting is greater than one, j\u2019s belief update would be performed by recursively invoking the interactive particle filter on j\u2019s beliefs.\nAs part of the weighting, we will first weight each particle with the probability of j receiving its observations. Particles with larger weights contain beliefs of j that are more likely. Thereafter, we will scale this weight with the probability of i observing a growl from the left and no creaks, \u3008GL,S\u3009. To understand the weighting process, let\u2019s focus on a single particle. Weighting for the remaining particles is analogous.\nWe consider the particle on the top right in the sample set, b\u0303tmpi,1 , shown in Fig. 10. Agent j\u2019s level 0 belief of 0.85 in this particle is due to j hearing a growl from the left on listening. The probability of j making this observation as given by its observation function, when the tiger is on the left is 0.85. We will adjust this weight with the probability of i perceiving \u3008GL,S\u3009 when the tiger is on the left and both agents are listening. This probability as given by i\u2019s observation function is 0.765 (see Table 1). The final weight attached to this particle is 0.65. Note that the weights as shown in Fig. 10 are not normalized. After normalization i\u2019s belief that the tiger is on the left is 0.85 (obtained by adding the normalized weights for particles that have st=TL), and 0.15 for tiger on the right. We note that this conforms to our expectations.\nThe final step of the I-PF is an unbiased resampling of the particles using the weights as the distribution. To prevent an exponential growth in the number of particles 5, we resample N particles resulting in the sample set, b\u0303ti,1, that is approximately representative of the exactly updated belief.\nWhen i\u2019s belief is nested to deeper levels, the above mentioned example forms the bottom step of the recursive filtering process."}, {"heading": "8.3 Performance of the I-PF", "text": "As part of our empirical investigation of the performance of the I-PF, we show, using a standard pseudo-distance metric and visually, that it approximates the exact state estimation closely. We\n5. After t propagation steps, there will be N |\u2126j |t particles in the sample set.\nbegin by utilizing extended versions of standard test problems and proceed to demonstrate the performance on a larger problem.\n8.3.1 MULTIAGENT TIGER AND MACHINE MAINTENANCE PROBLEMS\nFor our analysis, we first utilize the two-agent tiger problem that has two physical states, as described in Section 5, and a two-agent version of the machine maintenance problem (MM) (Smallwood & Sondik, 1973) described in detail in Appendix A, that has three physical states. While these problems have few physical states, the interactive state space tends to get large as it includes models of the other agent. Due to an absence of other general approximation techniques for I-POMDPs, we use a grid based numerical integration implementation of the exact filter as the baseline approximation for comparison. We obtained the points for numerical integration by superimposing regular grids of differing resolutions on the interactive state space.\nThe lineplots in Fig. 12 show that the quality of the I-PF based approximation, as measured by KL-Divergence becomes better as the number of particles increases, for both the problem domains. This remains true for both level 1 and 2 beliefs. KL-Divergence measures the difference between the two probability distributions by giving the relative entropy of the filtered posterior with respect to the near-exact one as obtained from the numerical integration. Note that the performance of the IPF remains consistent over both the two-state tiger and the three-state MM problem. However, level 2 belief approximations require considerably more particles as compared to level 1 approximations, to achieve similar performance, indicating that the performance of the I-PF is affected by the level of nesting. Each data point in the lineplots is the average of 10 runs of the I-PF on multiple prior belief states. In the case of the tiger problem, the posterior used for comparison is the one that is obtained after agent i listens and hears a growl from the left and no creaks. For the MM problem, the posterior obtained after i manufactures and perceives no defect in the product, is used for comparison. Two of the prior level 1 beliefs of agent i when playing the tiger problem are those shown in Fig. 3. We considered a level 2 belief according to which agent i is unaware of the tiger\u2019s location and believes with equal probabilities that either of the level 1 beliefs shown in Fig. 3 are likely. We utilized analogous beliefs for the machine maintenance problem.\nA comparison of the run times of the I-POMDP belief update implemented using grid based numerical integration and the I-PF is shown in Table. 2. We varied the number of grid points and\nMultiagent Tiger Multiagent Machine Maintenance\nthe number of particles in the two implementations, respectively. As we use initial beliefs that are flat, having the same number of grid points and particles provides for a comparable approximation quality. The I-PF implementation significantly outperforms the numerical integration based implementation, while providing the comparable performance quality. Additionally, the run times of the grid based implementation increase significantly more when we move from the two-state tiger prob-\nlem to the three-state MM problem, in comparison to the increase for the I-PF for level 1 beliefs. Since the level 1 multiagent tiger model has 6 observations in comparison to 2 for the multiagent MM, the run times decrease as we move from the tiger to the MM problem for level 2 beliefs.\nDespite using an equal number of grid points and particles, the reduced run time of the I-PF in comparison to the grid based approach is due to: (i) iterating over all grid points in order to obtain the belief over each interactive state under consideration. In contrast, the I-PF iterates over all particles at each level once \u2013 propagating and weighting them to obtain the posterior; (ii) the I-PF solves the models approximately in comparison to solving them exactly over the grid points; and (iii) while the grid based belief update considers all the optimal actions for a model, the I-PF samples a single action from the distribution and propagates the corresponding particle using this action.\nLevel 1 Beliefs in the Multiagent Tiger Problem\nIn order to assess the quality of the approximations after successive belief updates, we graphed the probability density functions produced by the I-PF and the exact belief update. The densities arising after each of three filtering steps on the level 1 belief of agent i (Fig. 3(a)) in the tiger problem, are shown in Fig. 13. Each approximate p.d.f. is the average of 10 runs of the I-PF which contained 5000 particles, and is shaped using a standard Gaussian kernel. Gmytrasiewicz and Doshi (2005) provide an explanation of the exact I-POMDP belief update shown here. Briefly, as the prior belief is a flat line, the posterior becomes segmented where the segments correspond to beliefs of j that are likely based on the predicted action and anticipated observations of j. The height of a segment is proportional to the likelihood of j\u2019s possible observation. The action and observation sequence followed was \u3008L,GL, S\u3009, \u3008L,GL, S\u3009, \u3008OR,GL, S\u3009. As can be seen, the I-PF produces a good approximation of the true densities.\n8.3.2 A UAV RECONNAISSANCE PROBLEM\nUnmanned agents such as UAVs are finding important applications in tasks such as fighting forest fires (Casbeer, Beard, McLain, Sai-Ming, & Mehra, 2005; Sarris, 2001), law enforcement (Murphy & Cycon, 1998) and reconnaissance in warfare. The agents must operate in complex environments characterized by multiple parameters that affect their decisions, including, particularly in warfare, other agents who may have antagonistic preferences. The task is further complicated because the agent may possess noisy sensors and unreliable actuators.\nWe consider the task of a UAV i which performs low-altitude reconnaissance of a potentially hostile theater that may be populated by other agents with conflicting objectives that serve as ground reconnaissance targets (see Fig. 14(a)). To facilitate our analysis, we divide the UAV\u2019s operating theater into a 3\u00d73 grid of sectors and consider a ground reconnaissance target (T ), which could be located in any of the 9 sectors. For example, the target could be a terrorist who may be hiding in a safe house located in each sector. Of course, the UAV is unaware of which sector contains T , but is aware of its own location. To assist in its goal of spotting T by moving to the sector containing it, the UAV may receive a noisy communication which informs it about the rows (similar colored sectors in Fig. 14) that likely contain T , though with some uncertainty. The UAV has the option of moving in any of the four cardinal directions to the adjacent sector, or hovering in its current location and listening for more communications.\nThe target T may be informed (by its collaborators) that it could be in danger of being spotted by the UAV although with high uncertainty, in which case T may move to an adjacent or diagonal sector. Note that the actions of both, the UAV and agent T may affect the physical state of the problem. We formulate the decision problem of the UAV below: \u2022 A physical state, s={rowi, sidei or centeri, rowT , sideT or centerT }, where rowi and sidei or centeri indicate the row location of UAV i and whether the UAV is located in the side columns or the center column, respectively; \u2022 The joint action space, A = Ai\u00d7AT , where Ai = {moveN ,. . .,moveW , listen} and AT = {moveN ,. . .,moveW ,listen}. Here, movex moves the UAV or the target in the direction indicated by x, and listen denotes the act of receiving communications about the location of T or the UAV; \u2022 Observation space of the UAV is, \u2126i = {top-row (TR), center-row (CR), bottomrow (BR)}, where for example, TR indicates that the corresponding target is in one of the three sectors in the top row; \u2022 Transition function is, Ti : S \u00d7 A \u00d7 S \u2192 [0, 1]. Ti models the fact that the UAV and the target move deterministically to a surrounding sector; \u2022 Observation function, Oi : S \u00d7 A \u00d7 \u2126i \u2192 [0, 1] gives the likelihood that the UAV will be informed of the correct row in which the target is located; and \u2022 Reward function, Ri : S \u00d7 A \u2192 [0, 1] formalizes the goal of spotting the reconnaissance target. If the UAV moves to a sector containing a target , we assume that the target is spotted and the game ends.\nBecause the target may move, it is beneficial for i to anticipate T \u2019s actions. Thus, the UAV tracks some possible beliefs that T may have about the location of the UAV. We assume that the UAV is aware of T \u2019s objectives that conflict with its own, the probabilities of its observations, and therefore T \u2019s frame. We point out the size and complexity of this problem, involving 36 physical states, 5 actions and 3 observations for each agent.\nAnalogously to the previous problem sets, we measured the quality of the estimation provided by the I-PF for this larger problem. In Fig. 14(b), we show the KL-Divergence of the approximate distribution as the number of particles allocated to the I-PF are increased. The KL-Divergence decreases rapidly as we increase the number of particles for both level 1 and 2 beliefs. However,\nnotice that the magnitude of the divergence is larger for lower numbers of particles in comparison to the previous problems. This is, in part, due to the larger state space of the problem and demonstrates that the I-PF does not fully address the curse of dimensionality. Thus, many more particles are needed to reach comparable levels of divergence. We also show a comparison of the run times of the I-POMDP belief update implemented using the I-PF and grid based numerical integration in the table in Fig. 14(c). An identical number of particles and grid points were selected, which provided comparable qualities of the estimations. We were unable to run the numerical integration implementation for level 2 beliefs for this problem."}, {"heading": "9. Value Iteration on Sample Sets", "text": "Because the I-PF represents the belief of agent i, bi,l, using a set of N particles, b\u0303i,l, a value function backup operator which operates on samples is needed. Let H\u0303 denote the required backup operator,\nand U\u0303 the approximate value function, then the backup operation, U\u0303 t = H\u0303U\u0303 t\u22121, is:\nU\u0303 t(\u3008\u0303bi,l, \u03b8\u0302i\u3009) = max ai\u2208Ai\n{ 1\nN\n\u2211\nis(n)\u2208b\u0303i,l\nERi(is (n), ai)+\u03b3\n\u2211\noi\u2208\u2126i\nPr(oi|ai, b\u0303i,l)U\u0303 t\u22121(\u3008I-PF(\u0303bi,l, ai, oi), \u03b8\u0302i\u3009)\n}\n(8) where ERi(is(n), ai) = \u2211 aj Ri(s (n), ai, aj)Pr(aj |\u03b8 (n) j ), \u03b3 is the discount factor and I-PF(\u00b7) is the algorithm shown in Fig. 4. The set of optimal action(s) at a given approximate belief, OPT(\u3008\u0303bi,l, \u03b8\u0302j\u3009), is then calculated by returning the action(s) that have the maximum value:\nOPT (\u3008\u0303bi,l, \u03b8\u0302i\u3009) = argmax ai\u2208Ai\n{ 1 N \u2211\nis(n)\u2208b\u0303i,l\nERi(is (n), ai) + \u03b3 \u2211 oi\u2208\u2126i Pr(oi|ai, b\u0303i,l)\n\u00d7U\u0303 t(\u3008I-PF(\u0303bi,l, ai, oi), \u03b8\u0302i\u3009)\n} (9)\nEquations 8 and 9 are analogous to Eqs. 4 and 5 respectively, with exact integration replaced by Monte Carlo integration, and the exact belief update replaced with the I-PF. Note that H\u0303 \u2192 H as N \u2192 \u221e.\nThe algorithm for computing an approximately optimal finite horizon policy tree given an initial belief using value iteration when l > 0 is shown in Fig. 18 in Appendix B."}, {"heading": "9.1 Convergence and Error Bounds", "text": "The use of randomizing techniques such as PFs means that value iteration does not necessarily converge. This is because, unlike the exact belief update, posteriors generated by the PF with finitely many particles are not guaranteed to be identical for identical input. The non-determinism of the approximate belief update rules out isotonicity and contraction for H\u0303 as N \u2192 \u221e. 6\nOur inability to guarantee convergence of value iteration implies that we must approximate an infinite horizon policy with the approximately optimal finite horizon policy. Let U\u2217 be the value of the optimal infinite horizon policy, U\u0303 t be the value of the approximate and U t be the value of the optimal t-horizon policy tree. Then the error bound (using the supremum norm || \u00b7 ||) is, ||U\u2217\u2212 U\u0303 t|| = ||U\u2217\u2212U t+U t\u2212 U\u0303 t|| \u2264 ||U\u2217\u2212U t||+ ||U t\u2212 U\u0303 t|| (triangle inequality). Note that the first term, ||U\u2217 \u2212 U t||, is bounded by \u03b3t||U\u2217 \u2212 U0||. The bound for the second term is calculated below:\nE t = ||U\u0303 t \u2212 U t||\n= ||H\u0303U\u0303 t\u22121 \u2212HU t\u22121|| = ||H\u0303U\u0303 t\u22121 \u2212HU\u0303 t\u22121 +HU\u0303 t\u22121 \u2212HU t\u22121|| (add zero) \u2264 ||H\u0303U\u0303 t\u22121 \u2212HU\u0303 t\u22121||+ ||HU\u0303 t\u22121 \u2212HU t\u22121|| (triangle inequality) \u2264 ||H\u0303U\u0303 t\u22121 \u2212HU\u0303 t\u22121||+ \u03b3||U\u0303 t\u22121 \u2212 U t\u22121|| (contracting H) \u2264 ||H\u0303U\u0303 t\u22121 \u2212HU\u0303 t\u22121||+ \u03b3E t\u22121\nWe turn our attention to calculating ||H\u0303U\u0303 t\u22121 \u2212HU\u0303 t\u22121||. In the analysis that follows we focus on level 1 beliefs. Let U\u0307 t = HU\u0303 t\u22121, U\u0303 t = H\u0303U\u0303 t\u22121, and bi,1 be the singly nested belief where\n6. One may turn PFs into deterministic belief update operators (de-randomization) by generating several posteriors from the same input. A representative posterior is then formed by taking a convex combination of the different posteriors. For example, Thrun (2000) uses a k-nearest neighborhood approach for this purpose.\nthe worst error is made: bi,1 = argmax bi,1\u2208Bi,1\n|U\u0307 t \u2212 U\u0303 t|. Let \u03b1\u0303 be the policy tree (alpha vector) that is\noptimal at b\u0303i,1 (the sampled estimate of bi,1), and \u03b1\u0307 be the policy tree that is optimal at bi,1. We will use Chernoff-Hoeffding (C-H) upper bounds (Theorem A.1.4, pg 265 in Alon & Spencer, 2000) 7, a well-known tool for analyzing randomized algorithms, to derive a confidence threshold 1 \u2212 \u03b4 at which the observed estimate, U\u0303 t\u03b1\u0303, is within 2\u01eb of the true estimate U\u0307 t \u03b1\u0307 (= E[\u03b1\u0307]):\nPr(U\u0303 t\u03b1\u0303 > U\u0307 t \u03b1\u0307 + \u01eb) \u2264 e\n\u22122N\u01eb2/(\u03b1\u0303max\u2212\u03b1\u0303min) 2\nPr(U\u0303 t\u03b1\u0303 < U\u0307 t \u03b1\u0307 \u2212 \u01eb) \u2264 e\n\u22122N\u01eb2/(\u03b1\u0303max\u2212\u03b1\u0303min) 2\nWe may write,\nPr(U\u0303 t\u03b1\u0303 > U\u0307 t \u03b1\u0307 + \u01eb OR U\u0303 t \u03b1\u0303 < U\u0307 t \u03b1\u0307 \u2212 \u01eb) = Pr(U\u0303 t \u03b1\u0303 > U\u0307 t \u03b1\u0307 + \u01eb) + Pr(U\u0303 t \u03b1\u0303 < U\u0307 t \u03b1\u0307 \u2212 \u01eb) \u2212Pr(U\u0303 t\u03b1\u0303 > U\u0307 t \u03b1\u0307 + \u01eb AND U\u0303 t \u03b1\u0303 < U\u0307 t \u03b1\u0307 \u2212 \u01eb)\nAs the last term is zero, the equation becomes:\nPr(U\u0303 t\u03b1\u0303 > U\u0307 t \u03b1\u0307 + \u01eb OR U\u0303 t \u03b1\u0303 < U\u0307 t \u03b1\u0307 \u2212 \u01eb) \u2264 2e\n\u22122N\u01eb2/(\u03b1\u0303max\u2212\u03b1\u0303min) 2\nWe may replace Pr(U\u0303 t\u03b1\u0303 > U\u0307 t \u03b1\u0307 + \u01eb OR U\u0303 t \u03b1\u0303 < U\u0307 t \u03b1\u0307 \u2212 \u01eb) with 1\u2212Pr(U\u0307 t \u03b1\u0307 \u2212 \u01eb \u2264 U\u0303 t \u03b1\u0303 \u2264 U\u0307 t \u03b1\u0307 + \u01eb). After some simple operations, the above inequality becomes:\nPr(U\u0307 t\u03b1\u0307 \u2212 \u01eb \u2264 U\u0303 t \u03b1\u0303 \u2264 U\u0307 t \u03b1\u0307 + \u01eb) \u2265 1\u2212 2e\n\u22122N\u01eb2/(\u03b1\u0303max\u2212\u03b1\u0303min) 2\nLet the probability that U\u0303 t\u03b1\u0303 is within 2\u01eb of the true estimate U\u0307 t \u03b1\u0307, be at least 1\u2212 \u03b4. Then we have:\n1\u2212 \u03b4 = 1\u2212 2e\u22122N\u01eb 2/(\u03b1\u0303max\u2212\u03b1\u0303min) 2\nWith a confidence probability of at least 1\u2212 \u03b4, the error bound is:\n\u01eb =\n\u221a (\u03b1\u0303max \u2212 \u03b1\u0303min)2ln(2/\u03b4)\n2N (10)\nwhere \u03b1\u0303max \u2212 \u03b1\u0303min may be loosely upper bounded as Rmax\u2212Rmin\n1\u2212\u03b3 . Note that Eq. 10 can also be used to derive the number of particles, N , for some given \u03b4 and \u01eb. To get the desired bound, we note that with at least probability 1 \u2212 \u03b4 our error bound is 2\u01eb and with probability at most \u03b4 the worst possible suboptimal behavior may result: ||H\u0303U\u0303 t\u22121 \u2212 HU\u0303 t\u22121|| \u2264 (1 \u2212 \u03b4)2\u01eb + \u03b4Rmax\u2212Rmin1\u2212\u03b3 . The final error bound now obtains:\nE t \u2264 (1\u2212 \u03b4)2\u01eb+ \u03b4Rmax\u2212Rmin1\u2212\u03b3 + \u03b3E t\u22121\n= (1\u2212 \u03b4)2\u01eb(1\u2212\u03b3 t) 1\u2212\u03b3 + \u03b4 (Rmax\u2212Rmin)(1\u2212\u03b3 t) (1\u2212\u03b3)2\n(geometric series) (11)\nwhere \u01eb is as defined in Eq. 10.\n7. At horizon t, samples in b\u0303i,1 are i.i.d. However, at horizons less than t, the samples are generated by the I-PF and exhibit limited statistical independence, but independent research (Schmidt, Siegel, & Srinivasan, 1995) reveals that C-H bounds still apply.\nProposition 3 (Error Bound). For a singly nested t-horizon I-POMDPi,1, the error introduced by our approximation technique is upper bounded and is given by:\n||U\u0303 t \u2212 U t|| \u2264 (1\u2212 \u03b4) 2\u01eb(1\u2212 \u03b3t)\n1\u2212 \u03b3 + \u03b4\n(Rmax \u2212Rmin)(1\u2212 \u03b3 t)\n(1\u2212 \u03b3)2\nwhere \u01eb is as defined in Eq. 10.\nAt levels of belief nesting greater than one, j\u2019s beliefs are also approximately represented using samples. Hence the approximation error is not only due to the sampling, but also due to the possible incorrect prediction of j\u2019s actions based on its approximate beliefs. Since even a slight deviation from the exact belief may lead to an action that turns out to be the worst in value when compared to the optimal action, it seems difficult to derive bounds that are useful \u2013 tighter than the usual difference between the best and worst possible behavior (Rmax\u2212Rmin\n(1\u2212\u03b3)2 ) \u2013 for this case."}, {"heading": "9.2 Computational Savings", "text": "Since the complexity of solving I-POMDPs is dominated by the complexity of solving the models of other agents we analyze the reduction in the number of agent models that must be solved. In a K+1-agent setting with the number of particles bounded by N at each level, each particle in b\u0303t\u22121k,l of level l contains K models all of level l\u2212 1. Solution of each of these level l\u2212 1 models requires solution of the lower level models recursively. The upper bound on the number of models that are solved is O((KN)l\u22121). Given that there are K level l \u2212 1 models in a particle, and N such possibly distinct particles, we need to solve O((KN)l) models. Our upper bound on the number of models to be solved is polynomial in K for a fixed nesting level. This can be contrasted with O((K|\u0398\u2217|\nK)l) models that need to be solved in the exact case, which is exponential in K. Here, among the spaces of models of all agents, \u0398\u2217 is the largest space and is theoretically countably infinite. Typically, N \u226a |\u0398\u2217|K , resulting in a substantial reduction in computation. However, note that the total number of particles is exponential in the nesting level, l. This makes solutions for large nesting levels still intractable."}, {"heading": "10. Empirical Performance", "text": "The goal of our experimental analysis is to demonstrate empirically, (a) the reduction in error with increasing sample complexity, and (b) savings in computation time when the approximation technique is used. We again use the multiagent tiger problem introduced previously, and a multiagent version of the machine maintenance (MM) problem (see Appendix A) as test problems. While the single-agent versions of these problems are simple, their multiagent versions are sufficiently complex so as to motivate the use of approximation techniques to solve them. Additionally, we demonstrate that our approach scales to larger problems by applying it to the UAV reconnaissance problem as well."}, {"heading": "10.1 Multiagent Tiger and Machine Maintenance Problems", "text": "To demonstrate the reduction in error, we construct performance profiles showing an increase in performance as more computational resources \u2013 in this case particles \u2013 are allocated to the approximation algorithm. In Figs. 15(a) and (c) we show the performance profile curves when agent i\u2019s prior belief is the level 1 belief described previously in Fig. 3(a), and suitably modified for the MM\nMultiagent Tiger Problem\n-30\n-25\n-20\n-15\n-10\n-5\n0\n1 10 100 1000\nE xp\nec te\nd R\new ar\nd\nNo. of Particles (N)\nH=2:Exact H=2:Approx\nH=3:Exact H=3:Approx\n-30\n-25\n-20\n-15\n-10\n-5\n0\n1 10 100\nE xp\nec te\nd R\new ar\nd\nNo. of Particles (N)\nH=2:Exact H=2:Approx\nH=3:Exact H=3:Approx\n(a) (b)\nMultiagent Machine Maintenance Problem\nproblem. As expected the average rewards for both, horizon 2 and 3 approach the optimal expected reward as the number of particles increases. We show the analogous plots for a level 2 belief in Figs. 15(b) and (d). In each of these cases the average of the rewards accumulated by i over a 2 and 3 horizon policy tree (computed using the APPROXPOLICY algorithm in Fig. 18) while playing against agent j in simulated tiger and MM problems were plotted. To compensate for the randomness in sampling, we generated i\u2019s policy tree 10 times independently of each other, and averaged\nover 100 runs each time. Within each run, the location of the tiger and j\u2019s prior beliefs were sampled according to i\u2019s prior belief. j\u2019s policy was then computed using the algorithm in Fig. 18.\n(1\u2212\u03b3)2 ).\nIn Table 3, we compare the empirically determined error bound \u2013 difference between the optimal expected reward and the worst observed expected reward \u2013 with the theoretical error bound (\u03b4=0.1, \u03b3=0.9) from Section 9.1, for horizons 2 and 3. The theoretical error bounds appear loose due to the worst-case nature of our analysis but (expectedly) are much tighter than the trivial worst bounds, and become better as the number of particles increases.\nTable 4 compares the average run times of our sample-based approach (SB) with the grid based approach, for computing policy trees of different horizons starting from the level 1 belief. The values of the policy trees generated by the two approaches were similar. The run times demonstrate the impact of the curse of dimensionality on the grid based method as shown by the higher run times for the MM problem in comparison to the tiger problem. Our I-PF based implementation though not immune to this curse reduces its impact, but is affected by the curse of history, as illustrated by the higher run times for the tiger problem (branching factor of the reachability tree: |Ai||\u2126i| = 18) compared to the MM problem (branching factor:|Ai||\u2126i| = 8). We were unable to compute the solutions using the grid based implementation for both the problems for horizons beyond 3."}, {"heading": "10.2 UAV Reconnaissance Problem", "text": "We evaluate the performance of our approach on the UAV reconnaissance problem, which we introduced previously in Section 8.3. As we mentioned, this is a larger problem consisting of 36 physical states, 5 actions and 3 observations. We show the level 1 performance profile for this problem in Fig. 16(a) for horizons 2 and 3. Due to the size of the problem, we were unable to compute the exact value of the optimal policies. As before, each data point is the average reward obtained by simulating a horizon 2 or 3 policy in a two-agent setting. Agent i\u2019s initial belief is one according to which it is uncertain about the physical state and models of the other agent. We observe that the profiles tend to flatten as the number of particles increases. The corresponding expected reward is therefore close to the optimal value of the policies.\nWe also show the time taken to generate a good quality horizon 2 and 3 policy on average. They indicate that while it is indeed possible to obtain (approximate) policies for large problems, the times needed are somewhat large. Notice that these times are significantly greater than the run times for the multiagent tiger and MM problems. This is, in part, due to the larger state space and, as we show later in this article, in part due to the larger numbers of actions and observations of each agent."}, {"heading": "11. Sampling the Look Ahead Reachability Tree", "text": "Although we were able to solve I-POMDPs for large state spaces, we were unable to generate solutions for large horizons. The main reason for this is the exponential growth of the look ahead reachability tree with increasing horizons; we referred to this as the curse of history. At some time step t, there could be (|Ai||\u2126i|)t\u22121 reachable belief states of the agent i. For example, in the\nmultiagent tiger problem, at the second time step there could be 18 possible belief states, 324 of them at the third time step, and more than 0.1 million at the fifth time step.\nTo mitigate the curse of history, we reduce the branching factor of the look ahead reachability tree by sampling from the possible observations that the agent may receive. While this approach does not completely address the curse of history, it beats back the impact of this curse substantially. During the reachability tree expansion phase, the agent\u2019s actions are used to propagate its belief. Observations are then sampled at each propagated belief, oti \u223c Pr(\u2126i|a t\u22121 i , b\u0303 t\u22121 i,l ) where b\u0303 t\u22121 i,l is the propagated belief. During the value iteration phase, value is backed up the (possibly) partial reachability tree, and the agent performs the action that is optimal at the root node of the tree. For convenience, let us label this approach as reachability tree sampling (RTS).\nRTS shares its conceptual underpinnings with the belief expansion models of PBVI (Pineau et al., 2006), but differs in that our method is applicable to online policy tree generation for IPOMDPs, compared to PBVI\u2019s use in offline policy generation for POMDPs. It is also similar to the sparse sampling technique proposed for selecting actions for exploration during reinforcement learning (Kearns et al., 2002) and for online planning in POMDPs (Ross et al., 2008) by sampling the look ahead trees. A distinction is that we focus on sampling observations given the propagated multiagent beliefs, while the latter approaches focused on settings of single agent learning. Consequently, the process of computing the sampling distribution and the experimental settings differ."}, {"heading": "11.1 Computational Savings", "text": "We consider the computational savings that result from sampling observations in the look ahead reachability tree. If we are sampling N\u2126i < |\u2126i| observations at each propagated belief within the reachability tree, then at some time step t, we will obtain (|Ai||N\u2126i |)\nt\u22121 possible belief states, assuming the worst case occurs and we end up sampling N\u2126i distinct observations. This is in comparison to the (|Ai||\u2126i|)t\u22121 belief states in the complete reachability tree. Typically, as our experiments demonstrate, the number of distinct sampled observations is less than |\u2126i|, resulting in significant computational savings.\nAs an illustration of the computational savings we compare the run times of computing the policy tree for the multiagent tiger (Table 5) and the UAV reconnaissance (Table 6) problems for singly-nested beliefs. We compare value iteration in which the reachability tree is sampled (SBRTS) with value iteration that does no reachability tree sampling (SB-No-RTS), the algorithm for which is given in Fig. 18. For SB-RTS in the multiagent tiger problem, we sampled eight times from the observation distribution up to the fifth horizon and six times thereafter. For both the algorithms, we used a similar number of particles in the I-PF. As the tiger problem has a total of 6 observations,\nnot only does the SB-RTS compute the policy faster, we were able to compute it up to seven time horizons. When compared with the performance of SB-No-RTS, these results demonstrate that the approach of sampling the reachability tree could yield significant computational savings.\nHowever, as we see in Table 6, the approach does not yield significant savings in the context of the UAV problem for small horizons. Because the space of observations in the problem is small (3 distinct observations), a majority of these are often selected on sampling. Hence, for smaller horizons such as 2 and 3, we did not observe a significant decrease in the size of the look ahead reachability tree. However, the reduction in the look ahead trees for horizon 4 is enough to allow the computation of the corresponding policy, though the run time is considerable. We were unable to obtain it for the case with no RTS. Thus, the UAV problem reveals an important limitation of this technique \u2013 it may not provide significant computational savings when the space of observations is small, particularly for small horizons."}, {"heading": "11.2 Empirical Performance", "text": "We present the performance profiles in Fig. 17 for the multiagent tiger problem when partial look ahead reachability trees are built by sampling the observations. Similar to our previous experiments, the performance profiles reflect the average of the rewards accumulated by following the action prescribed by the root of the approximate policy tree that is built online. We plot the average reward accumulated by i over 10 independent trials consisting of 100 runs each, as the number of the observation samples, N\u2126i are gradually increased. Within each run, the location of the tiger and j\u2019s prior beliefs were sampled according to i\u2019s prior level 1 belief. Since we have combined RTS with the I-PF, in addition to varying N\u2126i , we also vary the number of particles, Np, employed to approximate the beliefs. As expected of performance profiles, the expected reward initially increases sharply, before flattening out as N\u2126i becomes large and the sampled observation distribution reaches the true one. Reflecting intuition, the plots for Np = 100 exhibit slightly better expected rewards on average than those for Np = 50. While the increase is not large, we note that it is consistent across all observation samples. We also obtained the average reward over a similar number of trials when a random policy (null hypothesis) is used for i. For horizon 3, the random policy gathered an average reward of -84.785 (\u00b1 37.9847), and -108.5 (\u00b1 41.56) for horizon 4. Even for a small number of observation samples, RTS does significantly better than the random policy thereby demonstrating the usefulness of partial tree expansion. However, we note that a random policy is a poor baseline for comparison and is used due to an absence of other similar approximations for I-POMDPs.\nMultiagent Tiger Problem\nDue to the small number of observations in the MM and UAV reconnaissance problems, we did not observe significant increases in the expected reward as more observations are sampled. Hence, we do not show the performance profiles for these problems.\nWe observed that the empirical expected reward is close to the optimal expected reward when only a few distinct observations were sampled while building the reachability tree. This observation when combined with the computational savings demonstrated in Section 11.1 indicate that our approximation approach is viable. Additionally, by varying the parameters Np and N\u2126i , we can flexibly, though only partially, control the effects of both the curses of dimensionality and history, respectively, on the solutions according to the application requirements. An interesting line of future work is to investigate the interplay of these parameters."}, {"heading": "12. Discussion", "text": "We described randomized methods for obtaining approximate solutions to finitely nested I-POMDPs based on a novel generalization of particle filtering to multiagent settings. The generalization is not straightforward because we are confronted with an interactive belief hierarchy in multiagent settings. We proposed the interactive particle filter which descends the levels of interactive belief hierarchies, and samples and propagates beliefs at each level. While sampling methods such as particle filters are unable to completely avoid the curse of dimensionality, they serve to focus the computational resources on those elements of the model that matter the most to the agent.\nHowever, the interactive particle filter does not address the policy space complexity. Though value iteration using sample sets is not guaranteed to converge asymptotically, we established useful error bounds for singly-nested I-POMDPs. Their generalization to multiply-nested beliefs has proved to be difficult but we continue to investigate it. We provided performance profiles for the multiagent tiger and the machine maintenance problems, and demonstrated scalability using the larger UAV reconnaissance problem.\nThe experiments show that the approach saves on computation over the space of models but it does not scale (usefully) to large values of time horizons and needs to be combined with methods\nthat deal with the curse of history. In order to reduce the impact of the curse of history, we proposed to sample observations while constructing the look ahead tree during the reachability analysis phase of policy computation. This sparse sampling technique effectively reduces the branching factor of the tree and allows computation of solutions for larger horizons as we demonstrated.\nA method to further scale the approximation technique is to pick a subset of actions in addition to sampling the observations while building the reachability tree. This further dampens the exponential growth of the reachability tree with increasing horizons, and permits solutions for larger horizons. However, this approach must be used cautiously \u2013 we do not want to leave out critical actions from the policy.\nSpecific approaches to speeding up the computation also remain to be explored. As we mentioned, the number of particles in the interactive particle filter grows exponentially with the number of nesting levels. In this regard, can we assign monotonically decreasing number of particles to represent beliefs nested at deeper levels exploiting an insight from cognitive psychology that beliefs nested at deeper levels are less likely to influence the optimal policy? Thus, if we decrease the particles sampled at the rate of r < 1, there will be no more than Np/(1\u2212r) particles in total, resulting in computational savings.\nFinally, in regards to the appropriate strategy level to use for nested models, we note the analogy with classical POMDPs, and the amount of detail and modeling information included therein. Adding more nested level of modeling is analogous to including more details in the POMDP formulation. Then, the solution to an I-POMDP is optimal given the level of detail included in the model, just like for classical POMDPs."}, {"heading": "Acknowledgments", "text": "This research is supported in part by grant #FA9550-08-1-0429 from AFOSR and in part by grants IRI-9702132 and IRI-0119270 from NSF. Versions of parts of this article have previously appeared in (Doshi & Gmytrasiewicz, 2005a) and in (Doshi & Gmytrasiewicz, 2005b). We acknowledge all the reviewers for their useful comments."}, {"heading": "Appendix A. Multiagent Machine Maintenance Problem", "text": "We extend the traditional single agent version of the machine maintenance (MM) problem (Smallwood & Sondik, 1973) to a two-agent cooperative version. The original MM problem involved a machine containing two internal components operated by a single agent. Either one or both components of the machine may fail spontaneously after each production cycle. If an internal component has failed, then there is some chance that when operating upon the product, it will cause the product to be defective. The agent may choose to manufacture the product (M) without examining it, examine the product (E), inspect the machine (I), or repair it (R) before the next production cycle. On an examination of the product, the subject may find it to be defective. Of course, if more components have failed, then the probability that the product is defective is greater.\nThe transition function, observation functions, and the reward functions for the two agents, i and j, are as shown in Table 7. Apart from including two agents that operate on the machine during the production cycle, we increased the nondeterminism of the original problem to make it more realistic. This also has the beneficial effect of producing a richer policy structure."}, {"heading": "Appendix B. Algorithm for Value Iteration on Sample Sets", "text": "We show the algorithm for computing an approximately optimal finite horizon policy tree given an initial belief using value iteration when l > 0. When l = 0, the algorithm reduces to the POMDP policy tree computation which is carried out exactly.8 The algorithm consists of the usual two steps: compute the look ahead reachability tree for horizon T as part of the reachability analysis (see Section 17.5 of Russell & Norvig, 2003) in lines 2-6 and perform value backup on the reachability\n8. For large problems, exact POMDP solutions may be replaced with approximate ones. But in doing so, our error bounds will no longer be applicable and those of the approximation technique will have to be considered.\ntree, in lines 7-28. The value of the beliefs at the leaves of the reachability tree is simply the one-step expected reward resulting from the best action."}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer,? \\Q2000\\E", "shortCiteRegEx": "Alon and Spencer", "year": 2000}, {"title": "Interactive epistemology i: Knowledge", "author": ["R.J. Aumann"], "venue": "International Journal of Game Theory,", "citeRegEx": "Aumann,? \\Q1999\\E", "shortCiteRegEx": "Aumann", "year": 1999}, {"title": "Hierarchies of conditional beliefs and interactive epistemology in dynamic games", "author": ["P. Battigalli", "M. Siniscalchi"], "venue": "Journal of Economic Theory,", "citeRegEx": "Battigalli and Siniscalchi,? \\Q1999\\E", "shortCiteRegEx": "Battigalli and Siniscalchi", "year": 1999}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Dynamic Programming and optimal control", "author": ["D. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "Bertsekas,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas", "year": 1995}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Hierarchies of beliefs and common knowledge", "author": ["A. Brandenburger", "E. Dekel"], "venue": "Journal of Economic Theory,", "citeRegEx": "Brandenburger and Dekel,? \\Q1993\\E", "shortCiteRegEx": "Brandenburger and Dekel", "year": 1993}, {"title": "Forest fire monitoring with multiple small uavs", "author": ["D. Casbeer", "R. Beard", "T. McLain", "L. Sai-Ming", "R. Mehra"], "venue": "In American Control Conference,", "citeRegEx": "Casbeer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Casbeer et al\\.", "year": 2005}, {"title": "A generalization of principal component analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta"], "venue": "R.E.Schapire", "citeRegEx": "Collins and Dasgupta,? \\Q2002\\E", "shortCiteRegEx": "Collins and Dasgupta", "year": 2002}, {"title": "A survey of convergence results on particle filtering methods for practitioners", "author": ["D. Crisan", "A. Doucet"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Crisan and Doucet,? \\Q2002\\E", "shortCiteRegEx": "Crisan and Doucet", "year": 2002}, {"title": "Mysterious computational complexity of particle filters", "author": ["F. Daum", "J. Huang"], "venue": "In Conference on Signal and Data Processing of Small Targets, SPIE Proceedings Series,", "citeRegEx": "Daum and Huang,? \\Q2002\\E", "shortCiteRegEx": "Daum and Huang", "year": 2002}, {"title": "An Introduction to Generalized Linear Models, 3rd Ed", "author": ["A. Dobson"], "venue": null, "citeRegEx": "Dobson,? \\Q2002\\E", "shortCiteRegEx": "Dobson", "year": 2002}, {"title": "Improved state estimation in multiagent settings with continuous or large dscrete state spaces", "author": ["P. Doshi"], "venue": "In Twenty Second Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Doshi,? \\Q2007\\E", "shortCiteRegEx": "Doshi", "year": 2007}, {"title": "Approximating state estimation in multiagent settings using particle filters", "author": ["P. Doshi", "P.J. Gmytrasiewicz"], "venue": "In Autonomous Agents and Multi-agent Systems Conference (AAMAS),", "citeRegEx": "Doshi and Gmytrasiewicz,? \\Q2005\\E", "shortCiteRegEx": "Doshi and Gmytrasiewicz", "year": 2005}, {"title": "A particle filtering based approach to approximating interactive pomdps", "author": ["P. Doshi", "P.J. Gmytrasiewicz"], "venue": "In Twentieth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Doshi and Gmytrasiewicz,? \\Q2005\\E", "shortCiteRegEx": "Doshi and Gmytrasiewicz", "year": 2005}, {"title": "Generalized point based value iteration for interactive pomdps", "author": ["P. Doshi", "D. Perez"], "venue": "In Twenty Third Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Doshi and Perez,? \\Q2008\\E", "shortCiteRegEx": "Doshi and Perez", "year": 2008}, {"title": "Graphical models for online solutions to interactive pomdps", "author": ["P. Doshi", "Y. Zeng", "Q. Chen"], "venue": "In Autonomous Agents and Multiagent Systems Conference (AAMAS),", "citeRegEx": "Doshi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Doshi et al\\.", "year": 2007}, {"title": "Rao-blackwellised particle filtering for dynamic bayesian networks", "author": ["A. Doucet", "N. de Freitas", "K. Murphy", "S. Russell"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Doucet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2000}, {"title": "Sequential Monte Carlo Methods in Practice", "author": ["A. Doucet", "N.D. Freitas", "N. Gordon"], "venue": null, "citeRegEx": "Doucet et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2001}, {"title": "Reasoning about Knowledge", "author": ["R. Fagin", "J. Halpern", "Y. Moses", "M. Vardi"], "venue": null, "citeRegEx": "Fagin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Fagin et al\\.", "year": 1995}, {"title": "A probabilistic approach to collaborative multi-robot localization", "author": ["D. Fox", "W. Burgard", "H. Kruppa", "S. Thrun"], "venue": "Autonomous Robots on Heterogenous Multi-Robot Systems,", "citeRegEx": "Fox et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2000}, {"title": "The Theory of Learning in Games", "author": ["D. Fudenberg", "D.K. Levine"], "venue": null, "citeRegEx": "Fudenberg and Levine,? \\Q1998\\E", "shortCiteRegEx": "Fudenberg and Levine", "year": 1998}, {"title": "Bayesian Data Analysis, Second Edition", "author": ["A. Gelman", "J. Carlin", "H. Stern", "D. Rubin"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2004}, {"title": "Bayesian inference in econometric models using monte carlo integration", "author": ["J. Geweke"], "venue": null, "citeRegEx": "Geweke,? \\Q1989\\E", "shortCiteRegEx": "Geweke", "year": 1989}, {"title": "A framework for sequential planning in multiagent settings", "author": ["P. Gmytrasiewicz", "P. Doshi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gmytrasiewicz and Doshi,? \\Q2005\\E", "shortCiteRegEx": "Gmytrasiewicz and Doshi", "year": 2005}, {"title": "Novel approach to non-linear/non-gaussian bayesian state estimation", "author": ["N. Gordon", "D. Salmond", "A. Smith"], "venue": "IEEE Proceedings-F,", "citeRegEx": "Gordon et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "Games with incomplete information played by bayesian players", "author": ["J.C. Harsanyi"], "venue": "Management Science,", "citeRegEx": "Harsanyi,? \\Q1967\\E", "shortCiteRegEx": "Harsanyi", "year": 1967}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W.K. Hastings"], "venue": null, "citeRegEx": "Hastings,? \\Q1970\\E", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Topology-free typology of beliefs", "author": ["A. Heifetz", "D. Samet"], "venue": "Journal of Economic Theory,", "citeRegEx": "Heifetz and Samet,? \\Q1998\\E", "shortCiteRegEx": "Heifetz and Samet", "year": 1998}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "A sparse sampling algorithm for near-optimal planning in large markov decision processes", "author": ["M. Kearns", "Y. Mansour", "A. Ng"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Sampling in factored dynamic systems", "author": ["D. Koller", "U. Lerner"], "venue": "Sequential Monte Carlo Methods in Practice. Springer", "citeRegEx": "Koller and Lerner,? \\Q2001\\E", "shortCiteRegEx": "Koller and Lerner", "year": 2001}, {"title": "Recursive bayesian estimation using piecewise constant approximations", "author": ["S.C. Kramer", "H. Sorenson"], "venue": null, "citeRegEx": "Kramer and Sorenson,? \\Q1988\\E", "shortCiteRegEx": "Kramer and Sorenson", "year": 1988}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P. Vitanyi"], "venue": null, "citeRegEx": "Li and Vitanyi,? \\Q1997\\E", "shortCiteRegEx": "Li and Vitanyi", "year": 1997}, {"title": "Formulation of bayesian analysis for games with incomplete information", "author": ["J. Mertens", "S. Zamir"], "venue": "International Journal of Game Theory,", "citeRegEx": "Mertens and Zamir,? \\Q1985\\E", "shortCiteRegEx": "Mertens and Zamir", "year": 1985}, {"title": "Applications for mini vtol uav for law enforcement. In SPIE 3577:Sensors, C3I, Information, and Training Technologies for Law Enforcement", "author": ["D. Murphy", "J. Cycon"], "venue": null, "citeRegEx": "Murphy and Cycon,? \\Q1998\\E", "shortCiteRegEx": "Murphy and Cycon", "year": 1998}, {"title": "Sampling methods for action selection in influence diagrams", "author": ["L. Ortiz", "L. Kaelbling"], "venue": "In Seventeenth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Ortiz and Kaelbling,? \\Q2000\\E", "shortCiteRegEx": "Ortiz and Kaelbling", "year": 2000}, {"title": "An online pomdp algorithm for complex multiagent environments", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Anytime point-based value iteration for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Value-directed compression in pomdps", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Poupart and Boutilier,? \\Q2003\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2003}, {"title": "Vdcbpi: An approximate algorithm scalable for large-scale pomdps", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Poupart and Boutilier,? \\Q2004\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2004}, {"title": "Value-directed sampling methods for belief monitoring in pomdps", "author": ["P. Poupart", "L. Ortiz", "C. Boutilier"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Poupart et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2001}, {"title": "Online planning algorithms for pomdps", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-draa"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Finding approximate pomdp solutions through belief compression", "author": ["N. Roy", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Roy et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2005}, {"title": "Artificial Intelligence: A Modern Approach (Second Edition)", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2003\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2003}, {"title": "Iterative Methods for Sparse Linear Systems", "author": ["Y. Saad"], "venue": null, "citeRegEx": "Saad,? \\Q1996\\E", "shortCiteRegEx": "Saad", "year": 1996}, {"title": "Survey of uav applications in civil markets", "author": ["Z. Sarris"], "venue": "In IEEE Mediterranean Conference on Control and Automation,", "citeRegEx": "Sarris,? \\Q2001\\E", "shortCiteRegEx": "Sarris", "year": 2001}, {"title": "Chernoff-hoeffding bounds for applications with limited independence", "author": ["J.P. Schmidt", "A. Siegel", "A. Srinivasan"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "Schmidt et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 1995}, {"title": "Improved memory bounded dynamic programming for decentralized pomdps", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Journal of Autonomous Agents and Multiagent Systems,", "citeRegEx": "Seuken and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "The optimal control of partially observable markov decision processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Recursive bayesian estimation using gaussian sums", "author": ["H.W. Sorenson", "D.L. Alspach"], "venue": null, "citeRegEx": "Sorenson and Alspach,? \\Q1971\\E", "shortCiteRegEx": "Sorenson and Alspach", "year": 1971}, {"title": "Kalman Filtering: Theory and Application", "author": ["Sorenson", "H.W. (Ed"], "venue": null, "citeRegEx": "Sorenson and .Ed...,? \\Q1985\\E", "shortCiteRegEx": "Sorenson and .Ed...", "year": 1985}, {"title": "Monte carlo pomdps", "author": ["S. Thrun"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Thrun,? \\Q2000\\E", "shortCiteRegEx": "Thrun", "year": 2000}, {"title": "Feature-based methods for large scale dynamic programming", "author": ["J. Tsitsiklis", "B.V. Roy"], "venue": "Machine Learning,", "citeRegEx": "Tsitsiklis and Roy,? \\Q1996\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1996}, {"title": "Bayesian sparse sampling for online reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 26, "context": "types in Bayesian games (Harsanyi, 1967; Mertens & Zamir, 1985).", "startOffset": 24, "endOffset": 63}, {"referenceID": 1, "context": "An agent\u2019s beliefs within I-POMDPs are called interactive beliefs, and they are nested analogously to the hierarchical belief systems considered in game theory (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Heifetz & Samet, 1998; Aumann, 1999), in theoretical computer science (Fagin, Halpern, Moses, & Vardi, 1995) and to the hyper-priors in hierarchical Bayesian models (Gelman, Carlin, Stern, & Rubin, 2004).", "startOffset": 160, "endOffset": 249}, {"referenceID": 1, "context": "An agent\u2019s beliefs within I-POMDPs are called interactive beliefs, and they are nested analogously to the hierarchical belief systems considered in game theory (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Heifetz & Samet, 1998; Aumann, 1999), in theoretical computer science (Fagin, Halpern, Moses, & Vardi, 1995) and to the hyper-priors in hierarchical Bayesian models (Gelman, Carlin, Stern, & Rubin, 2004). Since the interactive beliefs may be infinitely nested, Gmytrasiewicz and Doshi (2005) defined finitely nested I-POMDPs as computable specializations of the infinitely nested ones.", "startOffset": 236, "endOffset": 505}, {"referenceID": 25, "context": "We generalize the particle filter, and more specifically the bootstrap filter (Gordon et al., 1993), to the multiagent setting, resulting in the interactive particle filter (I-PF).", "startOffset": 78, "endOffset": 99}, {"referenceID": 27, "context": "Among the spectrum of MC techniques, two that have been particularly well-studied in sequential settings are Markov chain Monte Carlo (MCMC) (Hastings, 1970; Gelman et al., 2004), and particle filters (Gordon et al.", "startOffset": 141, "endOffset": 178}, {"referenceID": 22, "context": "Among the spectrum of MC techniques, two that have been particularly well-studied in sequential settings are Markov chain Monte Carlo (MCMC) (Hastings, 1970; Gelman et al., 2004), and particle filters (Gordon et al.", "startOffset": 141, "endOffset": 178}, {"referenceID": 25, "context": ", 2004), and particle filters (Gordon et al., 1993; Doucet et al., 2001).", "startOffset": 30, "endOffset": 72}, {"referenceID": 18, "context": ", 2004), and particle filters (Gordon et al., 1993; Doucet et al., 2001).", "startOffset": 30, "endOffset": 72}, {"referenceID": 22, "context": "Although Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, it would involve sampling from a conditional distribution of the physical state given the observation history and model of other, and from the distribution of the other\u2019s model given the physical state.", "startOffset": 24, "endOffset": 45}, {"referenceID": 53, "context": "Particle filters previously have been successfully applied to approximate the belief update in continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001).", "startOffset": 138, "endOffset": 186}, {"referenceID": 17, "context": ", 1993; Doucet et al., 2001). Approximating the I-POMDP belief update using the former technique, may turn out to be computationally exhaustive. Specifically, MCMC algorithms that utilize rejection sampling (e.g. Hastings, 1970) may cause a large number of intentional models to be sampled, solved, and rejected, before one is utilized for propagation. In addition, the complex estimation process in I-POMDPs makes the task of computing the acceptance ratio for rejection sampling computationally inefficient. Although Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, it would involve sampling from a conditional distribution of the physical state given the observation history and model of other, and from the distribution of the other\u2019s model given the physical state. However, these distributions are neither efficient to compute nor easy to derive analytically. Particle filters need not reject solved models and compute a new model in replacement, propagating all solved models over time and resampling them. They are intuitively amenable to approximating the I-POMDP belief update and produce reasonable approximations of the posterior while being computationally feasible. Particle filters previously have been successfully applied to approximate the belief update in continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001). While Thrun (2000) integrates particle filtering with Q-learning to learn the policy, Poupart et al.", "startOffset": 8, "endOffset": 1401}, {"referenceID": 17, "context": ", 1993; Doucet et al., 2001). Approximating the I-POMDP belief update using the former technique, may turn out to be computationally exhaustive. Specifically, MCMC algorithms that utilize rejection sampling (e.g. Hastings, 1970) may cause a large number of intentional models to be sampled, solved, and rejected, before one is utilized for propagation. In addition, the complex estimation process in I-POMDPs makes the task of computing the acceptance ratio for rejection sampling computationally inefficient. Although Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, it would involve sampling from a conditional distribution of the physical state given the observation history and model of other, and from the distribution of the other\u2019s model given the physical state. However, these distributions are neither efficient to compute nor easy to derive analytically. Particle filters need not reject solved models and compute a new model in replacement, propagating all solved models over time and resampling them. They are intuitively amenable to approximating the I-POMDP belief update and produce reasonable approximations of the posterior while being computationally feasible. Particle filters previously have been successfully applied to approximate the belief update in continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001). While Thrun (2000) integrates particle filtering with Q-learning to learn the policy, Poupart et al. (2001) assume the prior existence of an exact value function and present an error bound analysis of substituting the POMDP belief update with particle filters.", "startOffset": 8, "endOffset": 1490}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005).", "startOffset": 110, "endOffset": 206}, {"referenceID": 45, "context": "Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality.", "startOffset": 50, "endOffset": 62}, {"referenceID": 42, "context": "An approximate way of solving POMDPs online is the RTBSS approach (Paquet, Tobin, & Chaib-draa, 2005; Ross et al., 2008) that adopts the branch-andbound technique for pruning the look ahead reachability tree.", "startOffset": 66, "endOffset": 120}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality.", "startOffset": 111, "endOffset": 405}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.", "startOffset": 111, "endOffset": 718}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) to uncover a low dimensional belief subspace that usually encompasses a robot\u2019s potential beliefs. The method is based on the observation that beliefs along many real-world trajectories exhibit only a few degrees of freedom. The effectiveness of these methods is problem specific; indeed, it is possible to encounter problems where no substantial belief compression may occur. When applied to the I-POMDP framework, the effectiveness of the compression techniques would depend, for example, on the existence of agent models whose likelihoods within the agent\u2019s belief do not change after successive belief updates or on the existence of correlated agent models. Whether such models exist in practice is a topic of future work. Techniques that address the curse of history in POMDPs also exist. Poupart and Boutilier (2004) generate policies via policy iteration using finite state controllers with a bounded number of nodes.", "startOffset": 111, "endOffset": 1627}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) to uncover a low dimensional belief subspace that usually encompasses a robot\u2019s potential beliefs. The method is based on the observation that beliefs along many real-world trajectories exhibit only a few degrees of freedom. The effectiveness of these methods is problem specific; indeed, it is possible to encounter problems where no substantial belief compression may occur. When applied to the I-POMDP framework, the effectiveness of the compression techniques would depend, for example, on the existence of agent models whose likelihoods within the agent\u2019s belief do not change after successive belief updates or on the existence of correlated agent models. Whether such models exist in practice is a topic of future work. Techniques that address the curse of history in POMDPs also exist. Poupart and Boutilier (2004) generate policies via policy iteration using finite state controllers with a bounded number of nodes. Pineau et al. (2006) perform point-based value iteration (PBVI) by selecting a small subset of reachable belief points at each step from the belief simplex and planning only over these belief points.", "startOffset": 111, "endOffset": 1750}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) to uncover a low dimensional belief subspace that usually encompasses a robot\u2019s potential beliefs. The method is based on the observation that beliefs along many real-world trajectories exhibit only a few degrees of freedom. The effectiveness of these methods is problem specific; indeed, it is possible to encounter problems where no substantial belief compression may occur. When applied to the I-POMDP framework, the effectiveness of the compression techniques would depend, for example, on the existence of agent models whose likelihoods within the agent\u2019s belief do not change after successive belief updates or on the existence of correlated agent models. Whether such models exist in practice is a topic of future work. Techniques that address the curse of history in POMDPs also exist. Poupart and Boutilier (2004) generate policies via policy iteration using finite state controllers with a bounded number of nodes. Pineau et al. (2006) perform point-based value iteration (PBVI) by selecting a small subset of reachable belief points at each step from the belief simplex and planning only over these belief points. Doshi and Perez (2008) outline the challenges and develop PBVI for I-POMDPs.", "startOffset": 111, "endOffset": 1952}, {"referenceID": 25, "context": "Particle filters (PF) (Gordon et al., 1993; Doucet et al., 2001) are specific implementations of Bayes filters tailored toward making Bayes filters applicable to non-linear dynamic systems.", "startOffset": 22, "endOffset": 64}, {"referenceID": 18, "context": "Particle filters (PF) (Gordon et al., 1993; Doucet et al., 2001) are specific implementations of Bayes filters tailored toward making Bayes filters applicable to non-linear dynamic systems.", "startOffset": 22, "endOffset": 64}, {"referenceID": 23, "context": "Rather than sampling directly from the target distribution which is often difficult, PFs adopt the method of importance sampling (Geweke, 1989), which allows samples to be drawn from a more tractable distribution called the proposal distribution, \u03c0.", "startOffset": 129, "endOffset": 143}, {"referenceID": 25, "context": "To avoid this degeneracy, Gordon et al. (1993) suggested inserting a resampling step, which would increase the population of those particles that had high importance weights.", "startOffset": 26, "endOffset": 47}, {"referenceID": 16, "context": "The general algorithm for the particle filtering technique is given by Doucet et al. (2001). We concentrate on a specific implementation of this algorithm, that has previously been studied under various names such as MC localization, survival of the fittest, and the bootstrap filter.", "startOffset": 71, "endOffset": 92}, {"referenceID": 9, "context": "Crisan and Doucet (2002) outline a rigorous proof of the convergence of this algorithm toward the true posterior as N \u2192 \u221e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 29, "context": "Following Kaelbling et al. (1998), we assume that the value of the prize is 10, that the pain associated with encountering the tiger can be quantified as -100, and that the cost of listening is -1.", "startOffset": 10, "endOffset": 34}, {"referenceID": 26, "context": "The intentional models are analogous to types as used in Bayesian games (Harsanyi, 1967).", "startOffset": 72, "endOffset": 88}, {"referenceID": 12, "context": "As mentioned by Gmytrasiewicz and Doshi (2005), we may also ascribe the subintentional models, SMj , which constitute the remaining models in Mj,l\u22121.", "startOffset": 34, "endOffset": 47}, {"referenceID": 22, "context": "Additionally, the nested beliefs are, in general, analogous to hierarchical priors utilized for Bayesian analysis of hierarchical data (Gelman et al., 2004).", "startOffset": 135, "endOffset": 156}, {"referenceID": 12, "context": "2 For an illustration of the belief update, additional details on I-POMDPs, and how they compare with other multiagent planning frameworks, see (Gmytrasiewicz & Doshi, 2005). In a manner similar to the belief update in POMDPs, the following proposition holds for the I-POMDP belief update. The proposition results from noting that Eq. 3 expresses the belief in terms of parameters of the previous time step only. A complete proof of the belief update and this proposition is given by Gmytrasiewicz and Doshi (2005).", "startOffset": 161, "endOffset": 515}, {"referenceID": 11, "context": "We observe that the level 1 densities may be represented using any family of probability distributions such as exponential family (Dobson, 2002) or polynomials that allow approximation of any function up to arbitrary accuracy.", "startOffset": 130, "endOffset": 144}, {"referenceID": 22, "context": "At a general level, these equations represent an application of the update of hierarchical priors given observed data (Gelman et al., 2004) to the problem of state estimation in multiagent settings.", "startOffset": 118, "endOffset": 139}, {"referenceID": 18, "context": "The resulting algorithm inherits the convergence properties of the original algorithm (Doucet et al., 2001).", "startOffset": 86, "endOffset": 107}, {"referenceID": 12, "context": "Doshi (2007) presented a Rao-Blackwellised I-PF (RB-IPF), where the conditional distribution is updated using a variational Kalman filter.", "startOffset": 0, "endOffset": 13}, {"referenceID": 12, "context": "Gmytrasiewicz and Doshi (2005) provide an explanation of the exact I-POMDP belief update shown here.", "startOffset": 18, "endOffset": 31}, {"referenceID": 46, "context": "Unmanned agents such as UAVs are finding important applications in tasks such as fighting forest fires (Casbeer, Beard, McLain, Sai-Ming, & Mehra, 2005; Sarris, 2001), law enforcement (Murphy & Cycon, 1998) and reconnaissance in warfare.", "startOffset": 103, "endOffset": 166}, {"referenceID": 53, "context": "For example, Thrun (2000) uses a k-nearest neighborhood approach for this purpose.", "startOffset": 13, "endOffset": 26}, {"referenceID": 38, "context": "RTS shares its conceptual underpinnings with the belief expansion models of PBVI (Pineau et al., 2006), but differs in that our method is applicable to online policy tree generation for IPOMDPs, compared to PBVI\u2019s use in offline policy generation for POMDPs.", "startOffset": 81, "endOffset": 102}, {"referenceID": 30, "context": "It is also similar to the sparse sampling technique proposed for selecting actions for exploration during reinforcement learning (Kearns et al., 2002) and for online planning in POMDPs (Ross et al.", "startOffset": 129, "endOffset": 150}, {"referenceID": 42, "context": ", 2002) and for online planning in POMDPs (Ross et al., 2008) by sampling the look ahead trees.", "startOffset": 42, "endOffset": 61}], "year": 2009, "abstractText": "Partially observable Markov decision processes (POMDPs) provide a principled framework for sequential planning in uncertain single agent settings. An extension of POMDPs to multiagent settings, called interactive POMDPs (I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief systems which represent an agent\u2019s belief about the physical world, about beliefs of other agents, and about their beliefs about others\u2019 beliefs. This modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute. We describe a general method for obtaining approximate solutions of I-POMDPs based on particle filtering (PF). We introduce the interactive PF, which descends the levels of the interactive belief hierarchies and samples and propagates beliefs at each level. The interactive PF is able to mitigate the belief space complexity, but it does not address the policy space complexity. To mitigate the policy space complexity \u2013 sometimes also called the curse of history \u2013 we utilize a complementary method based on sampling likely observations while building the look ahead reachability tree. While this approach does not completely address the curse of history, it beats back the curse\u2019s impact substantially. We provide experimental results and chart future work.", "creator": "dvips(k) 5.95b Copyright 2005 Radical Eye Software"}}}