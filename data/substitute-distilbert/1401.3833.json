{"id": "1401.3833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Active Tuples-based Scheme for Bounding Posterior Beliefs", "abstract": "the book presents a scheme for computing lower and upper bounds approaching the posterior marginals in bayesian networks with outcome variables. its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior feedback and enhance its performance in an anytime manner. the scheme uses the cutset conditioning principle at tighten existing bounding schemes largely to facilitate anytime behavior, utilizing a fixed number of associated vertices. improves accuracy maintaining the bounds improves as what number of used cutset tuples increases and so does the computation slower. we demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant such the bound propagation algorithm as a plug - inside scheme.", "histories": [["v1", "Thu, 16 Jan 2014 04:50:19 GMT  (417kb)", "http://arxiv.org/abs/1401.3833v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bozhena bidyuk", "rina dechter", "emma rollon"], "accepted": false, "id": "1401.3833"}, "pdf": {"name": "1401.3833.pdf", "metadata": {"source": "CRF", "title": "Active Tuples-based Scheme for Bounding Posterior Beliefs", "authors": ["Bozhena Bidyuk", "Rina Dechter", "Emma Rollon"], "emails": ["bbidyuk@google.com", "dechter@ics.uci.edu", "erollon@lsi.upc.edu"], "sections": [{"heading": "1. Introduction", "text": "This paper addresses the problem of bounding the probability of evidence and posterior marginals in Bayesian networks with discrete variables. Deriving bounds on posteriors with a given accuracy is clearly an NP-hard problem (Abdelbar & Hedetniemi, 1998; Dagum & Luby, 1993; Roth, 1996) and indeed, most available approximation algorithms provide little or no guarantee on the quality of their approximations. Still, a few approaches were presented in the past few years for bounding posterior marginals (Horvitz, Suermondt, & Cooper, 1989; Poole, 1996, 1998; Mannino & Mookerjee, 2002; Mooij & Kappen, 2008) and for bounding the probability of evidence (Dechter & Rish, 2003; Larkin, 2003; Leisink & Kappen, 2003).\nIn this paper we develop a framework that can accept any bounding scheme and improve its bounds in an anytime manner using the cutset-conditioning principle (Pearl, 1988). To facilitate our scheme we develop an expression that converts bounds on the probability of evidence into bounds on posterior marginals.\nGiven a Bayesian network defined over a set of variables X , a variable X \u2208 X , and a domain value x \u2208 D(X), a posterior marginal P (x|e) (where e is a subset of assignments to the variables, called evidence) can be computed directly from two joint probabilities,\nc\u00a92010 AI Access Foundation. All rights reserved.\nP (x, e) and P (e):\nP (x|e) = P (x, e)\nP (e) (1)\nGiven a set C={C1, ..., Cp} \u2282 X of cutset variables (e.g., a loop-cutset), we can compute the probability of evidence by enumerating over all the cutset tuples ci \u2208 D(C) =\n\u220fp i=1D(Ci)\nusing the formula:\nP (e) = M \u2211\ni=1\nP (ci, e) (2)\nwhere M = |D(C)|. We can also compute the posterior marginals using the expression:\nP (x|e) = M \u2211\ni=1\nP (x|ci, e)P (ci|e) (3)\nThe computation of P (ci, e) for any assignment c = ci is linear in the network size if C is a loop-cutset and it is exponential in w if C is a w-cutset (see definition in Section 2). The limitation of the cutset-conditioning method, as defined in Eq. (2) and (3), is that the number of cutset tuples M grows exponentially with the cutset size.\nThere are two basic approaches for handling the combinatorial explosion in the cutsetconditioning scheme. One is to sample over the cutset space and subsequently approximate the distribution P (C|e) from the samples, as shown by Bidyuk and Dechter (2007). The second approach, which we use here, is to enumerate h out of M tuples and bound the rest. We shall refer to the selected tuples as \u201cactive\u201d tuples. A lower bound on P (e) can be obtained by computing exactly the quantities P (ci, e) for 1 \u2264 i \u2264 h resulting in a partial sum in Eq. (2). This approach is likely to perform well if the selected h tuples contain most of the probability mass of P (e). However, this approach cannot be applied directly to obtain the bounds on the posterior marginals in Eq. (3). Even a partial sum in Eq. (3) requires computing P (ci|e) which in turn requires a normalization constant P (e). We can obtain naive bounds on posterior marginals from Eq. (1) using PL(e) and PU (e) to denote available lower and upper bounds over joint probabilities:\nPL(x, e)\nPU (e) \u2264 P (x|e) \u2264\nPU (x, e)\nPL(e)\nHowever, those bounds usually perform very poorly and often yield an upper bound > 1. Horvitz et. al (1989) were the first to propose a scheme for bounding posterior marginals based on a subset of cutset tuples. They proposed to select h highest probability tuples from P (c) and derived lower and upper bounds on the sum in Eq. (3) from the joint probabilities P (ci, e) and priors P (ci) for 1 \u2264 i \u2264 h. Their resulting bounded conditioning algorithm was shown to compute good bounds on the posterior marginals of some variables in an Alarm network (with M = 108). However, the intervals between lower and upper bound values increase as the probability of evidence becomes smaller because the prior distribution becomes a bad predictor of the high probability tuples in P (C|e) and P (c) becomes a bad upper bound for P (c, e).\nThe expression we derive in this paper yields a significantly improved formulation which results in our Active Tuples Bounds (ATB) framework. The generated bounds facilitate\nanytime performance and are provably tighter than the bounds computed by bounded conditioning. In addition, our expression accommodates the use of any off-the-shelf scheme which bounds the probability of evidence. Namely, ATB accepts any algorithm for bounding P (e) and generates an algorithm that bounds the posterior marginals. Moreover, it can also tighten the input bounds on P (e).\nThe time complexity of ATB is linear in the number of active (explored) cutset tuples h. If the complexity of bounding P (e) is O(T ), then bounding the probability mass of the unexplored tuples is O(T \u00b7h \u00b7 (d\u2212 1) \u00b7 |C|) where |C| is the number of variables in the cutset and d is the maximum domain size.\nWe evaluate our framework experimentally, using a variant of bound propagation (BdP ) (Leisink & Kappen, 2003) as the plug-in bounding scheme. BdP computes bounds by iteratively solving a linear optimization problem for each variable where the minimum and maximum of the objective function correspond to lower and upper bounds on the posterior marginals. The performance of BdP was demonstrated on the Alarm network, the Ising grid network, and on regular bipartite graphs. Since bound propagation is exponential in the Markov boundary size, and since it requires solving linear programming problems many times, its overhead as a plug-in scheme was too high and not cost-effective. We therefore utilize a variant of bound propagation called ABdP+, introduced by Bidyuk and Dechter (2006b), that trades accuracy for speed.\nWe use Gibbs cutset sampling (Bidyuk & Dechter, 2003a, 2003b) for finding highprobability cutset tuples. Other schemes, such as stochastic local search (Kask & Dechter, 1999) can also be used. The investigation into generating high-probability cutset tuples is outside the primary scope of the paper.\nWe show empirically that ATB using bound propagation is often superior to bound propagation alone when both are given comparable time resources. More importantly, ATB\u2019s accuracy improves with time. We also demonstrate the power of ATB for improving the bounds on probability of evidence. While the latter is not the main focus of our paper, lower and upper bounds on the probability of evidence are contained in the expression for bounding posterior marginals.\nThe paper is organized as follows. Section 2 provides background on the previously proposed method of bounded conditioning. Section 3 presents and analyzes our ATB framework. Section 4 describes the implementation details of using bound propagation as an ATB plug-in and presents our empirical evaluation. Section 5 discusses related work, and Section 6 concludes."}, {"heading": "2. Background", "text": "For background, we define key concepts and describe the bounded conditioning algorithm which inspired our work."}, {"heading": "2.1 Preliminaries", "text": "In this section, we define essential terminology and provide background information on Bayesian networks.\nDefinition 2.1 (graph concepts) A directed graph is a pair G=< V , E >, where V = {X1, ..., Xn} is a set of nodes and E = {(Xi, Xj)|Xi, Xj \u2208 V} is the set of edges. Given (Xi, Xj) \u2208 E, Xi is called a parent of Xj, and Xj is called a child of Xi. The set of Xi\u2019s parents is denoted pa(Xi), or pai, while the set of Xi\u2019s children is denoted ch(Xi), or chi. The family of Xi includes Xi and its parents. The moral graph of a directed graph G is the undirected graph obtained by connecting the parents of each of the nodes in G and removing the arrows. A cycle-cutset of an undirected graph is a subset of nodes that, when removed, yields a graph without cycles. A loop in a directed graph G is a subgraph of G whose underlying graph is a cycle (undirected). A directed graph is acyclic if it has no directed loops. A directed graph is singly-connected (also called a poly-tree), if its underlying undirected graph has no cycles. Otherwise, it is called multiply-connected.\nDefinition 2.2 (loop-cutset) A vertex v is a sink with respect to a loop L if the two edges adjacent to v in L are directed into v. A vertex that is not a sink with respect to a loop L is called an allowed vertex with respect to L. A loop-cutset of a directed graph G is a set of vertices that contains at least one allowed vertex with respect to each loop in G.\nDefinition 2.3 (Bayesian network) Let X = {X1, ..., Xn} be a set of random variables over multi-valued domains D(X1), ...,D(Xn). A Bayesian network B (Pearl, 1988) is a pair <G,P> where G is a directed acyclic graph whose nodes are the variables X and P = {P (Xi|pai) | i = 1, ..., n} is the set of conditional probability tables (CPTs) associated with each Xi. B represents a joint probability distribution having the product form:\nP (x1, ...., xn) = n \u220f\ni=1\nP (xi|pa(Xi))"}, {"heading": "An evidence e is an instantiated subset of variables E \u2282 X .", "text": "Definition 2.4 (Markov blanket and Markov boundary) A Markov blanket of Xi is a subset of variables Y \u2282 X such that Xi is conditionally independent of all other variables given Y . A Markov boundary of Xi is its minimal Markov blanket (Pearl, 1988).\nIn our following discussion we will identify Markov boundaryXi with the Markov blanket consisting of Xi\u2019s parents, children, and parents of its children.\nDefinition 2.5 (Relevant Subnetwork) Given evidence e, relevant subnetwork of Xi relativde to e is a subnetwork of B obtained by removing all descendants of Xi that are not observed and do not have observed descendants.\nIf the observations change, the Markov boundary of Xi will stay the same while its relevant subnetwork may change. As most inference tasks are defined relative to a specific set of observations e, it is often convenient to restrict attention to the Markov boundary of Xi in the relevant subnetwork of Xi.\nThe most common query over Bayesian networks is belief updating which is the task of computing the posterior distribution P (Xi|e) given evidence e and a query variable Xi \u2208 X . Another query is to compute probability of evidence P (e). Both tasks are NPhard (Cooper, 1990). Finding approximate posterior marginals with a fixed accuracy is also\nNP-hard (Dagum & Luby, 1993; Abdelbar & Hedetniemi, 1998). When the network is a poly-tree, belief updating and other inference tasks can be accomplished in time linear in the size of the network. In general, exact inference is exponential in the induced width of the network\u2019s moral graph.\nDefinition 2.6 (induced width) The width of a node in an ordered undirected graph is the number of the node\u2019s neighbors that precede it in the ordering. The width of an ordering o, denoted w(o), is the width over all nodes. The induced width of an ordered graph, w\u2217(o), is the width of the ordered graph obtained by processing the nodes from last to first.\nDefinition 2.7 (w-cutset) A w-cutset of a Bayesian network B is a subset of variables C such that, when removed from the moral graph of the network, its induced width is \u2264 w.\nThroughout the paper, we will consider a Bayesian network over a set of variables X , evidence variables E \u2282 X and evidence E = e, and a cutset C = {C1, ..., Cp} \u2282 X\\E. Lower-case c = {c1, ..., cp} will denote an arbitrary instantiation of cutset C, and M = |D(C)| = \u220f\nCi\u2208C |D(Ci)| will denote the number of different cutset tuples."}, {"heading": "2.2 Bounded Conditioning", "text": "Bounded conditioning (BC) is an anytime scheme for computing posterior bounds in Bayesian networks proposed by Horvitz et. al (1989). It is derived from the loop-cutset conditioning method (see Eq. 3). Given a node X \u2208 X and a domain value x \u2208 D(X), they derive bounds from the following formula:\nP (x|e) = M \u2211\ni=1\nP (x|ci, e)P (ci|e) = h \u2211\ni=1\nP (x|ci, e)P (ci|e) + M \u2211\ni=h+1\nP (x|ci, e)P (ci|e) (4)\nThe hard-to-compute P (ci|e) is replaced for i \u2264 h with a normalization formula:\nP (x|e) =\n\u2211h i=1 P (x|c i, e)P (ci, e) \u2211h\ni=1 P (c i, e) + \u2211M i=h+1 P (c\ni, e) +\nM \u2211\ni=h+1\nP (x|ci, e)P (ci|e) (5)\nBC computes exactly P (ci, e) and P (x|ci, e) for the h cutset tuples and bounds the rest. The lower bound is obtained from Eq. (5) by replacing\n\u2211M i=h+1 P (c i, e) in the denomi-\nnator with the sum of priors \u2211M i=h+1 P (c i) and simply dropping the sum on the right:\nPLBC(x|e) ,\n\u2211h i=1 P (x, c i, e) \u2211h\ni=1 P (c i, e) + \u2211M i=h+1 P (c\ni) (6)\nThe upper bound is obtained from Eq. (5) by replacing \u2211M i=h+1 P (c i, e) in the denominator with a zero, and replacing P (x|ci, e) and P (ci|e) for i > h with the upper bounds of 1 and a derived upper bound (not provided here) respectively:\nPUBC(x|e) ,\n\u2211h i=1 P (x, c i, e) \u2211h\ni=1 P (c i, e)\n+\n\u2211M i=h+1 P (c i) \u2211h\ni=1 P L(ci|e) + 1\u2212 \u2211h i=1 P U (ci|e)\nApplying definitions for PL(ci|e) = P (c i,e)\n\u2211h i=1 P (c i,e)+ \u2211M i=h+1 P (c i) and PU (ci|e) = P (c i,e) \u2211h i=1 P (c i,e) from\nHorvitz et al. (1989), we get:\nPUBC(x|e) ,\n\u2211h i=1 P (x, c i, e) \u2211h\ni=1 P (c i, e)\n+ ( \u2211M i=h+1 P (c i))( \u2211h i=1 P (c i, e) + \u2211M i=h+1 P (c i))\n\u2211h i=1 P (c\ni, e) (7)\nThe bounds expressed in Eq. (6) and (7) converge to the exact posterior marginals as h\u2192M . However, we can show that,\nTheorem 2.1 (bounded conditioning bounds interval) The interval between lower and upper bounds computed by bounded conditioning is lower bounded by the probability mass of prior distribution P (C) of the unexplored cutset tuples:\n\u2200h, PUBC(x|e)\u2212 P L BC(x|e) \u2265\nM \u2211\ni=h+1\nP (ci)\nProof. See Appendix A."}, {"heading": "3. Architecture for Active Tuples Bounds", "text": "In this section, we describe our Active Tuples Bounds (ATB) framework. It builds on the same principles as bounded conditioning. Namely, given a cutset C and some method for generating h cutset tuples, the probabilities P (c, e) of the h tuples are evaluated exactly and the rest are upper and lower bounded. The worst bounds on P (c, e) are the lower bound of 0 and the upper bound of P (c). ATB bounds can be improved by using a plug-in algorithm that computes tighter bounds on the participating joint probabilities. It always computes tighter bounds than bounded conditioning, even when using 0 and P (c) to bound P (c, e).\nFor the rest of the section, c1:q = {c1, ..., cq} with q < |C| denotes a generic partial instantiation of the first q variables in C, while ci1:q indicates a particular partial assignment.\nGiven h cutset tuples, 0 \u2264 h \u2264 M , that we assume without loss of generality to be the first h tuples according to some enumeration order, a variable X \u2208 X\\E and x \u2208 D(X), we can rewrite Eq. (3) as:\nP (x|e) =\n\u2211M i=1 P (x, c i, e) \u2211M\ni=1 P (c i, e)\n=\n\u2211h i=1 P (x, c i, e) + \u2211M i=h+1 P (x, c i, e)\n\u2211h i=1 P (c i, e) + \u2211M i=h+1 P (c i, e)\n(8)\nThe probabilities P (x, ci, e) and P (ci, e), 1 \u2264 i \u2264 h, can be computed in polynomial time if C is a loop-cutset and in time and space exponential in w if C is a w-cutset. The question is how to compute or bound\n\u2211M i=h+1 P (x, c i, e) and \u2211M i=h+1 P (c i, e) in an efficient manner.\nOur approach first replaces the sums over the tuples ch+1,...,cM with a sum over a polynomial number (in h) of partially-instantiated tuples. From that, we develop new expressions for lower and upper bounds on the posterior marginals as a function of the lower and upper bounds on the joint probabilities P (x, c1:q, e) and P (c1:q, e). We assume in our derivation that there is an algorithm A that can compute those bounds, and refer to them as PLA(x, c1:q, e) (resp. P L A(c1:q, e)) and P U A (x, c1:q, e) (resp. P U A (c1:q, e)) respectively."}, {"heading": "3.1 Bounding the Number of Processed Tuples", "text": "We will now formally define partially-insantiated tuples and replace the sum over the exponential number of uninstantiated tuples (h+ 1 through M) with a sum over polynomial number of partially-instantiated tuples (h+ 1 through M \u2032) in Eq. 8.\nConsider a fully-expanded search tree of depth |C| over the cutset search space expanded in the order C1,...,Cp. A path from the root to the leaf at depth |C| corresponds to a full cutset tuple. We will call such a path an active path and the corresponding tuple an active tuple. We can obtain the truncated search tree by trimming all branches that are not on the active paths:\nDefinition 3.1 (truncated search tree) Given a search tree T covering the search space H over variables Y = {Y1, . . . , Ym} \u2286 X , a truncated search tree relative to a subset S = {y1, ..., yt} \u2282 D(Y1)\u00d7 ...\u00d7D(Ym) of full assignments, is obtained by marking the edges on all the paths appearing in S and removing all unmarked edges and nodes except those emanating from marked nodes.\nLet S = {c1, . . . , ch}. Clearly, the leaves at depth q < |C| in the truncated search tree relative to S correspond to the partially instantiated cutset tuples c1:q which are not extended to full cutset assignments.\nExample 3.1 Consider a Bayesian network B with cutset variables C={C1, ..., C4}, domain values D(C1)=D(C3)=D(C4)={0, 1}, D(C2)={0, 1, 2}, and four fully-instantiated tuples {0, 1, 0, 0}, {0, 1, 0, 1}, {0, 2, 1, 0}, {0, 2, 1, 1}. Figure 1 shows its truncated search tree, where the remaining partially instantiated tuples are {0, 0}, {0, 1, 1}, {0, 2, 0}, and {1}.\nProposition 3.1 Let C be a cutset, d be the maximum domain size, and h be the number of generated cutset tuples. Then the number of partially-instantiated cutset tuples in the truncated search tree is bounded by O(h \u00b7 (d\u2212 1) \u00b7 |C|).\nProof. Since every node in the path from the root C1 to a leaf Cp can not have more than (d\u2212 1) emanating leaves, the theorem clearly holds.\nLet M \u2032 be the number of truncated tuples. We can enumerate the partially instantiated tuples, denoting the j-th tuple as cj1:qj , 1 \u2264 j \u2264M \u2032, where qj is the tuple\u2019s length. Clearly, the probability mass over the cutset tuples ch+1, ..., cM can be captured by a sum over the truncated tuples. Namely:\nProposition 3.2\nM \u2211\ni=h+1\nP (ci, e) = M \u2032 \u2211\nj=1\nP (cj1:qj , e) (9)\nM \u2211\ni=h+1\nP (x, ci, e) = M \u2032 \u2211\nj=1\nP (x, cj1:qj , e) (10)\nTherefore, we can bound the sums over the tuples h+1 through M in Eq. (8) by bounding a polynomial (in h) number of partially-instantiated tuples as follows,\nP (x|e) =\n\u2211h i=1 P (x, c i, e) + \u2211M \u2032 j=1 P (x, c j 1:qj\n, e) \u2211h\ni=1 P (c i, e) +\n\u2211M \u2032 j=1 P (c j 1:qj\n, e) (11)"}, {"heading": "3.2 Bounding the Probability over the Truncated Tuples", "text": "In the following, we develop lower and upper bound expressions used by ATB."}, {"heading": "3.2.1 Lower Bounds", "text": "First, we decompose P (cj1:qj , e), 0 \u2264 j \u2264 M \u2032, as follows. Given a variable X \u2208 X and a distinguished value x \u2208 D(X):\nP (cj1:qj , e) = \u2211\nx\u2032\u2208D(X)\nP (x\u2032, cj1:qj , e) = P (x, c j 1:qj , e) + \u2211\nx\u2032 6=x\nP (x\u2032, cj1:qj , e) (12)\nReplacing P (cj1:qj , e) in Eq. (11) with the right-hand side of Eq. (12), we get:\nP (x|e) =\n\u2211h i=1 P (x, c i, e) + \u2211M \u2032 j=1 P (x, c j 1:qj\n, e) \u2211h\ni=1 P (c i, e) +\n\u2211M \u2032 j=1 P (x, c j 1:qj , e) + \u2211M \u2032 j=1 \u2211 x\u2032 6=x P (x \u2032, c j 1:qj\n, e) (13)\nWe will use the following two lemmas:\nLemma 3.1 Given positive numbers a > 0, b > 0, \u03b4 \u2265 0, if a < b, then: a b \u2264 a+\u03b4 b+\u03b4 .\nLemma 3.2 Given positive numbers a, b, \u03b4, \u03b4L, \u03b4U , if a < b and \u03b4L \u2264 \u03b4 \u2264 \u03b4U , then:\na+ \u03b4L b+ \u03b4L \u2264 a+ \u03b4 b+ \u03b4 \u2264 a+ \u03b4U b+ \u03b4U\nThe proof of both lemmas is straight forward.\nLemma 3.2 says that if the sums in the numerator and denominator have some component \u03b4 in common, then replacing \u03b4 with a larger value \u03b4U in both the numerator and the denominator yields a larger fraction. Replacing \u03b4 with a smaller value \u03b4L in both places yields a smaller fraction.\nObserve now that in Eq. (13) the sums in both the numerator and the denominator contain P (x, cj1:qj , e). Hence, we can apply Lemma 3.2. We will obtain a lower bound by replacing P (x, cj1:qj , e), 1 \u2264 j \u2264 M \u2032, in Eq. (13) with corresponding lower bounds in both numerator and denominator, yielding:\nP (x|e) \u2265\nh \u2211\ni=1\nP (x, ci, e) +\nM \u2032 \u2211\nj=1\nPLA(x, c j 1:qj , e)\nh \u2211\ni=1\nP (ci, e) + M \u2032 \u2211\nj=1\nPLA(x, c j 1:qj\n, e) + M \u2032 \u2211\nj=1\n\u2211\nx\u2032 6=x\nP (x\u2032, cj1:qj , e)\n(14)\nSubsequently, grouping PLA(x, c j 1:qj , e) and \u2211 x\u2032 6=x P (x \u2032, c j 1:qj , e) under one sum and replacing PLA(x, c j 1:qj , e)+ \u2211 x\u2032 6=x P (x \u2032, c j 1:qj , e) with its corresponding upper bound (increasing denominator), we obtain:\nP (x|e) \u2265\nh \u2211\ni=1\nP (x, ci, e) + M \u2032 \u2211\nj=1\nPLA(x, c j 1:qj , e)\nh \u2211\ni=1\nP (ci, e) + M \u2032 \u2211\nj=1\nUB[PLA(x, c j 1:qj , e) + \u2211\nx\u2032 6=x\nP (x\u2032, cj1:qj , e)]\n, PLA(x|e) (15)\nwhere upper bound UB can be obtained as follows:\nUB[PLA(x, c j 1:qj , e) + \u2211\nx\u2032 6=x\nP (x\u2032, cj1:qj , e)] , min\n{\nPLA(x, c j 1:qj , e) + \u2211 x\u2032 6=x P U A (x \u2032, c j 1:qj , e) PUA (c j 1:qj , e)\n(16)\nThe value \u2211 x\u2032 6=x P U A (x \u2032, c j 1:qj , e) is, obviously, an upper bound of \u2211 x\u2032 6=x P (x \u2032, c j 1:qj , e). The value PUA (c j 1:qj , e) is also an upper bound since PLA(x, c j 1:qj , e)+ \u2211 x\u2032 6=x P (x \u2032, c j 1:qj , e) \u2264 P (cj1:qj , e) \u2264 PUA (c j 1:qj\n, e). Neither bound expression in Eq. (16) dominates the other. Thus, we compute the minimum of the two values.\nPlease note that the numerator in Eq. (15) above also provides an anytime lower bound on the joint probability P (x, e) and can be used to compute a lower bound on the probability of evidence. In general, a lower bound denoted PLA(e) is obtained by:\nP (e) \u2265 h \u2211\ni=1\nP (ci, e) + M \u2032 \u2211\nj=1\nPLA(c j 1:qj , e) , PLA(e) (17)"}, {"heading": "3.2.2 Upper Bound", "text": "The upper bound expression can be obtained in a similar manner. Since both numerator and denominator in Eq. (13) contain addends P (x, cj1:qj , e), using Lemma 3.2 we replace each P (x, cj1:qj , e) with an upper bound P U A (x, c j 1:qj , e) yielding:\nP (x|e) \u2264\nh \u2211\ni=1\nP (x, ci, e) +\nM \u2032 \u2211\nj=1\nPUA (x, c j 1:qj , e)\nh \u2211\ni=1\nP (ci, e) + M \u2032 \u2211\nj=1\nPUA (x, c j 1:qj\n, e) + M \u2032 \u2211\nj=1\n\u2211\nx\u2032 6=x\nP (x\u2032, cj1:qj , e)\n(18)\nSubsequently, replacing each P (x\u2032, cj1:qj , e), x \u2032 6= x, with a lower bound (reducing denominator), we obtain a new upper bound expression on P (x|e):\nP (x|e) \u2264\nh \u2211\ni=1\nP (x, ci, e) + M \u2032 \u2211\nj=1\nPUA (x, c j 1:qj , e)\nh \u2211\ni=1\nP (ci, e) + M \u2032 \u2211\nj=1\nPUA (x, c j 1:qj\n, e) + M \u2032 \u2211\nj=1\n\u2211\nx\u2032 6=x\nPLA(x \u2032, c j 1:qj , e)\n, PUA (x|e) (19)\nSimilar to the lower bound, the numerator in the upper bound expression PUA (x|e) provides an anytime upper bound on the joint probability P (x, ci, e) which can be generalized to upper bound the probability of evidence:\nP (e) \u2264 h \u2211\ni=1\nP (ci, e) + M \u2032 \u2211\nj=1\nPUA (c j 1:qj , e) , PUA (e) (20)\nThe derived bounds PLA(x|e) and P U A (x|e) are never worse than those obtained by bounded conditioning, as we will show in Section 3.4."}, {"heading": "3.3 Algorithmic Description", "text": "Figure 2 summarizes the active tuples-based bounding scheme ATB. In steps 1 and 2, we generate h fully-instantiated cutset tuples and compute exactly the probabilities P (ci, e) and P (X, ci, e) for i \u2264 h, \u2200X \u2208 X\\(C\u222aE), using, for example, the bucket-elimination algorithm (Dechter, 1999). In step 3, we compute bounds on the partially instantiated tuples using algorithm A. In step 4, we compute the lower and upper bounds on the posterior marginals using expressions (15) and (19), respectively.\nExample 3.2 Consider again the Bayesian network B described in Example 3.1. Recall that B has a cutset C = {C1, ..., C4} with domains D(C1) = D(C3) = D(C4) = {0, 1} and D(C2) = {0, 1, 2}. The total number of cutset tuples is M = 24. Let X 6\u2208 C be a variable in B with domain D(X) = {x, x\u2032}. We will compute bounds on P (x|e). Assume we generated the same four cutset tuples (h = 4) as before:\nc1 = {C1 = 0, C2 = 1, C3 = 0, C4 = 0} = {0, 1, 0, 0} c2 = {C1 = 0, C2 = 1, C3 = 0, C4 = 1} = {0, 1, 0, 1} c3 = {C1 = 0, C2 = 2, C3 = 1, C4 = 0} = {0, 2, 1, 0} c4 = {C1 = 0, C2 = 2, C3 = 1, C4 = 1} = {0, 2, 1, 1}\nThe corresponding truncated search tree is shown in Figure 1. For the tuple {0, 1, 0, 0}, we compute exactly the probabilities P (x,C1=0, C2=1, C3=0, C4=0, e) and P (C1=0, C2=1, C3 = 0, C4 = 0). Similarly, we obtain exact probabilities P (x,C1 = 0, C2 = 1, C3 = 0, C4 = 1) and P (C1 = 0, C2 = 1, C3 = 0, C4 = 1) for the second cutset instance {0, 1, 0, 1}.\nSince h = 4, \u2211h i=1 P (x \u2032, ci, e) and \u2211h i=1 P (c i, e) are:\n4 \u2211\ni=1\nP (x, ci, e) = P (x, c1, e) + P (x, c2, e) + P (x, c3, e) + P (x, c4, e)\n4 \u2211\ni=1\nP (ci, e) = P (c1, e) + P (c2, e) + P (c3, e) + P (c4, e)\nThe remaining partial tuples are: c11:2 = {0, 0}, c 2 1:3 = {0, 1, 1}, c 3 1:3 = {0, 2, 0}, and c 4 1:1 = {1}. Since these 4 tuples are not full cutsets, we compute bounds on their joint probabilities. Using the same notation as in Figure 2, the sums over the partially instantiated tuples will have the form:\nUBA(x) , P U A (x, c 1 1:2, e) + P U A (x, c 2 1:3, e) + P U A (x, c 3 1:3, e) + P U A (x, c 4 1:1, e)\nLBA(x) , P L A(x, c 1 1:2, e) + P L A(x, c 2 1:3, e) + P L A(x, c 3 1:3, e) + P L A(x, c 4 1:1, e)\nFrom Eq. (19) we get:\nPUA (x|e) =\n\u22114 i=1 P (x, c i, e) + UBA(x) \u22114\ni=1 P (c i, e) + UBA(x) + LBA(x\u2032)\nFrom Eq. (15) and (16) we get:\nPLA(x|e) =\n\u22114 i=1 P (x, c i, e) + LBA(x) \u22114\ni=1 P (c i, e) + LBA(x) + UBA(x\u2032)\nThe total number of tuples processed is M \u2032 = 4 + 4 = 8 < 24.\n3.4 ATB Properties\nIn this section we analyze the time complexity of the ATB framework, evaluate its worstcase lower and upper bounds, and analyze the monotonicity properties of its bounds interval (as a function of h).\nTheorem 3.1 (complexity) Given an algorithm A that computes lower and upper bounds on joint probabilities P (c1:qi , e) and P (x, c1:qi , e) in time O(T ), and a loop-cutset C, P L A(x|e) and PUA (x|e) are computed in time O(h \u00b7N + T \u00b7 h \u00b7 (d \u2212 1) \u00b7 |C|) where d is the maximum domain size and N is the problem input size.\nProof. Since C is a loop-cutset, the exact probabilities P (ci, e) and P (x, ci, e) can be computed in time O(N). From Proposition 3.1, there are O(h \u00b7 (d \u2212 1) \u00b7 |C|) partiallyinstantiated tuples. Since algorithm A computes upper and lower bounds on P (cj1:qj , e) and P (x, cj1:qj , e) in time O(T ), the bounds on partially-instantiated tuples can be computed in time O(T \u00b7 h \u00b7 (d\u2212 1) \u00b7 |C|)).\nLet the plug-in algorithm A be a brute-force algorithm, denoted BF , that trivially instantiates PLBF (x, c j 1:qj , e) = 0, PUBF (x, c j 1:qj , e) = P (cj1:qj ), and UB[P (c j 1:qj , e)] = P (cj1:qj ). Then, from Eq. (15):\nPLBF (x|e) ,\n\u2211h i=1 P (x, c i, e) \u2211h\ni=1 P (c i, e) +\n\u2211M \u2032 j=1 P (c j 1:qj\n) =\n\u2211h i=1 P (x, c i, e) \u2211h\ni=1 P (c i, e) + \u2211M j=h+1 P (c\nj) (21)\nwhile from Eq. (19):\nPUBF (x|e) ,\n\u2211h i=1 P (x, c i, e) + \u2211M \u2032 j=1 P (c j 1:qj\n) \u2211h\ni=1 P (c i, e) +\n\u2211M \u2032 j=1 P (c j 1:qj\n) =\n\u2211h i=1 P (x, c i, e) + \u2211M j=h+1 P (c j)\n\u2211h i=1 P (c i, e) + \u2211M j=h+1 P (c j)\n(22)\nAssuming that algorithm A computes bounds at least as good as those computed by BF , PLBF (x|e) and P U BF (x|e) are the worst-case bounds computed by ATB.\nNow, we are ready to compute an upper bound on the ATB bounds interval:\nTheorem 3.2 (ATB bounds interval upper bound) ATB length of the interval between its lower and upper bounds is upper bounded by a monotonic non-increasing function of h:\nPUA (x|e)\u2212 P L A(x|e) \u2264\n\u2211M j=h+1 P (c j) \u2211h\ni=1 P (c i, e) + \u2211M j=h+1 P (c\nj) , Ih\nProof. See Appendix C.\nNext we show that ATB lower and upper bounds are as good or better than the bounds computed by BC.\nTheorem 3.3 (tighter lower bound) PLA(x|e) \u2265 P L BC(x|e).\nProof. PLBF (x|e) is the worst-case lower bound computed by ATB. Since P L BF (x|e) = PLBC(x|e), and P L A(x|e) \u2265 P L BF (x|e), then P L A(x|e) \u2265 P L BC(x|e).\nTheorem 3.4 (tighter upper bound) PUA (x|e) \u2264 P U BC(x|e).\nProof. PUBF (x|e) is the worst-case upper bound computed by ATB. Since P U BF (x|e) \u2264 PUBC(x|e) due to lemma 3.1, it follows that P U A (x|e) \u2264 P U BC(x|e)."}, {"heading": "4. Experimental Evaluation", "text": "The purpose of the experiments is to evaluate the performance of our ATB framework on the two probabilistic tasks of single-variable posterior marginals and probability of evidence. The experiments on the first task were conducted on 1.8Ghz CPU with 512 MB RAM, while the experiments on the second task were conducted on 2.66GHz CPU with 2GB RAM.\nRecall that ATB has a control parameter h that fixes the number of cutset tuples for which the algorithm computes its exact joint probability. Given a fixed h, the quality of the bounds will presumably depend on the ability to select h high probability cutset tuples. In our implementation, we use an optimized version of Gibbs sampling, that during the sampling process maintains a list of the h tuples having the highest joint probability. As noted, other schemes should be considered for this subtask as part of the future work. We obtain the loop-cutset using mga algorithm (Becker & Geiger, 1996).\nBefore we report the results, we describe bound propagation and its variants, which we use as a plug-in algorithm A and also as a stand-alone bounding scheme."}, {"heading": "4.1 Bound Propagation", "text": "Bound propagation (BdP ) (Leisink & Kappen, 2003) is an iterative algorithm that bounds the posterior marginals of a variable. The bounds are initialized to 0 and 1 and are iteratively improved by solving a linear optimization problem for each variable X \u2208 X such that the minimum and maximum of the objective function correspond to the lower and upper bound on the posterior marginal P (x|e), x \u2208 D(X).\nWe cannot directly plug BdP into ATB to bound P (c1:q, e) because it only bounds conditional probabilities. Thus, we factorize P (c1:q, e) as follows:\nP (c1:q, e) = \u220f\nej\u2208E\nP (ej |e1, . . . , ej\u22121, c1:q)P (c1:q)\nEach factor P (ej |e1, . . . , ej\u22121, c1:q) can be bounded by BdP , while P (c1:q) can be computed exactly since the relevant subnetwork over c1:q (see Def. 2.5) is singly connected. Let P L BdP and PUBdP denote the lower and upper bounds computed by BdP on some marginal. The bounds BdP computes on the joint probability are:\n\u220f\nej\u2208E\nPLBdP (ej |e1, . . . , ej\u22121, c1:q)P (c1:q) \u2264 P (c1:q, e) \u2264 \u220f\nej\u2208E\nPUBdP (ej |e1, . . . , ej\u22121, c1:q)P (c1:q)\nNote that BdP has to bound a large number of tuples when plugged into ATB, and therefore, solve a large number of linear optimization problems. The number of variables in each problem is exponential in the size of the Markov blanket of X.\nAs a baseline for comparison with ATB, we use in our experiments a variant of bound propagation called BdP+ (Bidyuk & Dechter, 2006b) that exploits the structure of the network to restrict the computation of P (x|e) to the relevant subnetwork of X (see Def. 2.5). The Markov boundary of X (see Def. 2.4) within relevant subnetwork of X does not include the children of X that are not observed and have no observed descedants; therefore, it is a subnetwork of the Markov boundary in the original network. Sometimes, the Markov boundary of X is still too big to compute under limited memory resouces. BdP+ uses a parameter k to specify the maximum size of the Markov boundary domain space. The algorithm skips the variables whose Markov boundary domain size exceeds k, and so their lower and upper bound values remain 0 and 1, respectively. When some variables are skipped, the bounds computed by BdP+ for the remaining variables may be less accurate.\nOur preliminary tests showed that plugging BdP+ into ATB is timewise infeasible (even for small k). Instead, we developed and used a different version of bound propagation called ABdP+ (Bidyuk & Dechter, 2006b) as a plug-in algorithm A, which was more costeffective in terms of accuracy and time overhead. ABdP+ includes the same enhancements as BdP+, but solves the linear optimization problem for each variable using an approximation algorithm. This implies that we obtain bounds faster but they are not as accurate. Roughly, the relaxed linear optimization problem can be described as a fractional packing and covering with multiple knapsacks and solved by a fast greedy algorithm (Bidyuk & Dechter, 2006b). ABdP+ is also parameterized by k to control the maximum size of the linear optimization problem. Thus, ATB using ABdP+ as a plug-in has two control parameters: h and k."}, {"heading": "4.2 Bounding Single-Variable Marginals", "text": "We compare the performance of the following three algorithms: ATB (with ABdP+ as a plug-in), BdP+, as described in the previous section, and BBdP+ (Bidyuk & Dechter, 2006a). The latter is a combination of ATB and BdP+. First, we run algorithm ATB with ABdP+ plug-in. Then, we use the bounds computed by ATB to initialize bounds in BdP+ (instead of 0 and 1) and run BdP+. Note that, given fixed values of h and k, BBdP+ will always compute tighter bounds than either ATB and BdP+. Our goal is to analyze its trade-off between the increase of the bounds\u2019 accuracy and the computation time overhead. We also compare with approximate decomposition (AD) (Larkin, 2003) whenever it is feasible and relevant. We did not include the results for the stand-alone ABdP+ since our objective was to compare ATB bounds with the most accurate bounds obtained by bound propagation. Bidyuk (2006) provides additional comparison with various refinements of BdP (Bidyuk & Dechter, 2006b) mentioned earlier.\n1. Times are extrapolated."}, {"heading": "4.2.1 Benchmarks", "text": "We tested our framework on four different benchmarks: Alarm, Barley, CPCS, and Munin. Alarm network is a model for monitoring patients undergoing surgery in an operating room (Beinlich, Suermondt, Chavez, & Cooper, 1989). Barley network is a part of the decisionsupport system for growing malting barley (Kristensen & Rasmussen, 2002). CPCS networks are derived from the Computer-Based Patient Care Simulation system and based on INTERNIST-1 and Quick Medical Reference Expert systems (Pradhan, Provan, Middleton, & Henrion, 1994). We experiment with cpcs54, cpcs179, cpcs360b, and cpcs422b networks. Munin networks are a part of the expert system for computer-aided electromyography (Andreassen, Jensen, Andersen, Falck, Kjaerulff, Woldbye, Srensen, Rosenfalck, & Jensen, 1990). We experiment with Munin3 and Munin4 networks. For each network, we generated 20 different sets of evidence variables picked at random. For Barley network, we select evidence variables as defined by Kristensen and Rasmussen (2002).\nTable 1 summarizes the characteristic of each network. For each one, the table specifies the number of variables N , the induced width w\u2217, the size of loop cutset |LC|, the number of loop-cutset tuples |D(LC)|, and the time needed to compute the exact posterior marginals by bucket-tree elimination (exponential in the induced width w\u2217) and by cutset conditioning (exponential in the size of loop-cutset).\nComputing the posterior marginals exactly is easy in Alarm network, cpcs54, and cpcs179 using either bucket elimination or cutset conditioning since they have small induced width and a small loop-cutset. We include those benchmarks as a proof of concept only. Several other networks, Barley, Munin3, and Munin4, also have small induced width and, hence, their exact posterior marginals can be obtained by bucket elimination. However, since ATB is linear in space, it should be compared against linear-space schemes such as cutset-conditioning. From this perspective, Barley, Munin3, and Munin4 are hard. For example, Barley network has only 48 variables, its induced width is w\u2217 = 7, and exact inference by bucket elimination takes only 30 seconds. Its loop-cutset contains only 12 variables, but the number of loop-cutset tuples exceeds 2 million because some variables have large domain sizes (up to 67 values). Enumerating and computing all cutset tuples, at a rate of about 1000 tuples per second, would take over 22 hours. Similar considerations apply in case of Munin3 and Munin4 networks."}, {"heading": "4.2.2 Measures of Performance", "text": "We measure the quality of the bounds via the average length of the interval between lower and upper bound:\nI =\n\u2211\nX\u2208X\n\u2211\nx\u2208D(X)(P U (x|e)\u2212 PL(x|e))\n\u2211\nX\u2208X |D(X)|\nWe approximate posterior marginal as the midpoint between lower and upper bound in order to show whether the bounds are well-centered around the posterior marginal P (x|e). Namely:\nP\u0302 (x|e) = PUA (x|e) + P L A(x|e)\n2\nand then measure the average absolute error \u2206 with respect to that approximation:\n\u2206 =\n\u2211\nX\u2208X\n\u2211\nx\u2208D(X) |P (x|e)\u2212 P\u0302 (x|e)| \u2211\nX\u2208X |D(X)|\nFinally, we report %P (e) = \u2211h i=1 P (x,c i,e)\nP (e) \u00d7 100% that was covered by the explored cutset tuples. Notably, in some benchmarks, a few thousand cutset tuples is enough to cover > 90% of P (e)."}, {"heading": "4.2.3 Results", "text": "We summarize the results for each benchmark in a tabular format and charts. We highlight in bold face the first ATB data point where the average bounds interval is as good or better than BdP+. The charts show the convergence of the bounds interval length as a function of h and time.\nFor ATB and BBdP+ the maximumMarkov boundary domain size was fixed at k = 210. For BdP+, we vary parameter k from 214 to 219. Note that BdP+ only depends on k, not on h. In the tables, we report the best result obtained by BdP+ and its computation time so that it appears as constant with respect to h. However, when we plot accuracy against time, we include BdP+ bounds obtained using smaller values of parameter k. In the case of Alarm network, varying k did not make any difference since the full Markov boundary domain size equals 210 < 214. The computation time of BBdP+ includes the ATB plus the BdP+ time.\nAlarm network. Figure 3 reports the results. Since the maximum Markov boundary in Alarm network is small, BdP+ runs without limitations and computes an average bounds interval of 0.61 in 4.3 seconds. Note that the enumeration of less than the 25% of the total number of cutset tuples covers 99% of the P (e). This fact suggests that schemes based on cutset conditioning should be very suitable for this benchmark. Indeed, ATB outperforms BdP+, computing more accurate bounds starting with the first data point of h = 25 where the mean interval IATB = 0.41 while the computation time is 0.038 seconds, an order of magnitude less than BdP+. The extreme efficiency of ATB in terms of time is clearly seen in the right chart. The x-axis scale is logarithmic to fit all the results. As expected, the average bounds interval generated by ATB and BBdP+ decrease as the number of cutset tuples h increases, demonstrating the anytime property of ATB with respect to h. Given a fixed h, BBdP+ has a very significant overhead in time with respect to ATB (two orders of magnitude for values of h smaller than 54) and only a minor improvement in accuracy.\nBarley network. Figure 4 reports the results. ATB and BBdP+ improve as h increases. However, the improvement is quite moderate while very time consuming due to more uniform shape of the distribution P (C|e) as reflected by the very small % of P (e) covered by explored tuples (only 1% for 562 tuples and only 52% for 12478 tuples). For example, the average ATB (resp. BBdP+) bounds interval decreases from 0.279 (resp. 0.167), obtained in 9 (resp. 10) seconds, to 0.219 (resp. 0.142) obtained in 139 (resp. 141) seconds. Given a fixed h, BBdP+ substantially improves ATB bounds with little time overhead (2 seconds in general). Namely, in this benchmark, BBdP+ computation time is dominated by ATB\ncomputation time. Note that the computation time of the stand-alone BdP+ algorithm is less than 2 seconds. Within that time, BdP+ yields an average interval length of 0.23, while ATB and BBdP+ spend 86 and 10 seconds, respectively, to obtain the same quality bounds. However, the anytime behavior of the latter algorithms allows them to improve with time, a very desirable characteristic when computing bounds. Moreover, note that its overhead in time with respect to ATB is completely negligible.\nCPCS networks. Figures 5 to 8 show the results for cpcs54, cpcs179, cpcs360b and cpcs422b, respectively. The behavior of the algorithms in all networks is very similar. As in the previous benchmarks, ATB andBBdP+ bounds interval decreases as h increases. Given a fixed h, BBdP+ computes slightly better bounds intervals than ATB in all networks but cpcs179. For all networks, BBdP+ has overhead in time with respect to ATB. This overhead is constant for all values of h and for all networks except for cpcs54, for which the overhead decreases as h increases. ATB and BBdP+ outperform BdP+. Both algorithms compute the same bound interval length as BdP+, improving the computation time in one order of magnitude. Consider for example cpcs422b, a challenging instance for any inference scheme as it has relatively large induced width and loop-cutset size. ATB outperforms BdP+ after 50 seconds starting with h = 1181, and BBdP+ outperforms BdP+ in 37\nseconds starting with h = 253 (BdP+ convergence is shown in the plot, but only the best result is reported in the table).\nLarkin (2003) reported bounds on cpcs360b and cpcs422b using AD algorithm. For the first network, AD achieved bounds interval length of 0.03 in 10 seconds. Within the same time, ATB computes an average bounds interval of \u2248 0.005. For cpcs422b, AD achieved bounds interval of 0.15, obtained in 30 seconds. Within the same time, ATB and BBdP+ obtain comparable results computing average bounds interval of 0.24 and 0.15, respectively. It is important to note that the comparison is not on the same instances since the evidence nodes are not the same. Larkin\u2019s code was not available for further experiments.\nMunin networks. Figure 9 reports the results for both Munin networks. Let us first consider Munin3 network. Given a fixed h, ATB and BBdP+ compute almost identical bound intervals with BBdP+ having a noticeable time overhead. Note that the two curves in the chart showing convergence as a function of h are very close and hard to distinguish, while the points of BBdP+ in the chart showing convergence as a function of time are shifted to the right with respect to the ones of ATB. ATB is clearly superior to BdP+ both in accuracy and time. BdP+ computes bounds interval of 0.24 within 12 seconds, while ATB computes bounds interval of 0.050 in 8 seconds. In Munin4, given a fixed\nh, BBdP+ computes tighter bounds than ATB with some time overhead. However, the improvement decreases as h increases as shown by the convergence of both curves either as a function of h and time. Since the loop-cutset size is large, the convergence of ATB is relatively slow. BdP+ computes bounds interval of 0.23 within 15 seconds, while ATB and BBdP+ compute bounds of the same quality within 54 and 21 seconds, respectively."}, {"heading": "4.3 Bounding the Probability of Evidence", "text": "We compare the performance of the following three algorithms: ATB, mini-bucket elimination (MBE) (Dechter & Rish, 2003), and variable elimination and conditioning (V EC). For ATB, we test different configurations of the control parameters (h, k). Note that when h = 0, ATB is equivalent to its plug-in algorithm A, which in our case is ABdP+."}, {"heading": "4.3.1 Algorithms and Benchmarks", "text": "MBE is a general bounding algorithm for graphical model problems. In particular, given a Bayesian network, MBE computes lower and upper bound on the probability of evidence.\nMBE has a control parameter z, that allows trading time and space for accuracy. As the value of the control parameter z increases, the algorithm computes tighter bounds using more time and space, which is exponential in z.\nV EC is an algorithm that combines conditioning and variable elimination. It is based on the w-cutset conditioning scheme. Namely, the algorithm conditions or instantiates enough variables so that the remaining problem conditioned on the instantiated variables can be solved exactly using bucket elimination (Dechter, 1999). The exact probability of evidence can be computed by summing over the exact solution output by bucket elimination for all possible instantiations of the w-cutset. When V EC is terminated before completion, it outputs a partial sum yielding a lower bound on the probability of evidence. The implementation of V EC is publicly available1.\nWe tested ATB for bounding P (e) on three different benchmarks: Two-layer Noisy-Or, grids and coding networks. All instances are included in the UAI08 evaluation2.\nIn two-layer noisy-or networks, variables are organized in two layers where the ones in the second layer have 10 parents. Each probability table represents a noisy OR-function.\n1. http://graphmod.ics.uci.edu/group/Software 2. http://graphmod.ics.uci.edu/uai08/Evaluation/Report\nEach parent variable yj has a value Pj \u2208 [0..Pnoise]. The CPT for each variable in the second layer is then defined as P (x = 0|y1, . . . , yP ) = \u220f\nyj=1 Pj and P (x = 1|y1, . . . , yP ) =\n1 \u2212 P (x = 0|y1, . . . , yP ). We experiment with a class of problems called bn2o instances in the UAI08.\nIn grid networks, variables are organized as an M \u00d7 M grid. We experiment with grids2 instances, as they were called in UAI08, which are characterized by two parameters (M,D), where D is the percentage of determinism (i.e., the percentage of values in all CPTs assigned to either 0 or 1). For each parameter configuration, 10 samples were generated by randomly assigning value 1 to one leaf node. In UAI08 competition, these instances were named D-M -I, where I is the instance number.\nCoding networks can be represented as a four layer Bayesian network having M nodes in each layer. The second and third layer correspond to input information bits and parity check bits respectively. Each parity check bit represents an XOR function of input bits. Input and parity check nodes are binary while the output nodes are real-valued. We consider the BN 126 to BN 134 instances in the UAI08 evaluation. Each one has M = 128, 4 parents\nfor each node and channel noise variance (\u03c3 = 0.40). These networks are very hard and exact results are not available.\nTable 2 summarizes the characteristics of each network. For each one, the table specifies the number of variables N , the induced width w\u2217, the size of loop cutset |LC|, the number of loop-cutset tuples |D(LC)|, and the time needed to compute the exact posterior marginals by bucket-tree elimination (exponential in the induced width w\u2217) and by cutset conditioning (exponential in the size of loop-cutset). An \u2018out\u2019 indicates that bucket-tree elimination is unfeasible in terms of memory demands. Note that the characteristics of grid networks only depend on their sizes but not on the percentage of determinism; the characteristics of all coding networks are the same.\nFor our purposes, we consider V EC as another exact algorithm to compute the exact P (e) in the first and second benchmarks and as a lower bounding technique for the third benchmark. We fix the control parameter z of MBE and the w-cutset of V EC so that the algorithms require less than 1.5GB of space.\nMUNIN3"}, {"heading": "4.3.2 Results", "text": "We summarize the results for each benchmark in a tabular format. The tables report the bounds and computation time (in seconds) for each compared algorithm. For ATB, we report results by varying the values of the control parameters (h, k). In particular, we consider values of h in the range 4 to 200, and values of k in the set {210, 212, 214}. By doing so, we analyze the impact of each control parameter on the performance of the algorithm. Grey areas in the tables correspond to (h, k) configurations that cannot be compared due to computation time.\nTwo-layer noisy-or networks. Table 3 shows the results. As expected, the quality of the bounds produced by ATB improves when the values of the control parameters (h, k) increase. We observe that the best bounds are obtained when fixing h to the highest value (i.e., 200) and k to the smallest value (i.e., 210). However, the increase in the value of h leads to higher computation times than when increasing the value of k. When taking time into account, comparing configurations with similar time (see (h = 50, k = 210) and (h = 4, k = 214), and (h = 200, k = 210) and (h = 50, k = 212), respectively), we observe that the configuration with the highest value of h and the smallest value of k outperforms the other ones.\nWhen compared with MBE, there is no clear superior approach. The accuracy of the algorithms depends on whether we look at upper or lower bounds. When considering upper bounds, ATB outperforms MBE for all instances 1b, 2b and 3b. Note that for those instances, MBE computes worse upper bounds than the trivial one (i.e., greater than 1). However, for instances 1a, 2a and 3a, MBE computes tighter upper bounds than ATB. For lower bounds, in general ATB outperforms MBE for instances with 20 and 25 evidence variables, while MBE is more accurate for instances having 15 evidence variables. Regarding computation time, ATB is definitely slower than MBE.\n1. Times are extrapolated.\nGrid networks. Table 4 reports the results. The first thing to observe is that MBE computes completely uninformative bounds. In this case, the anytime behavior of ATB is not effective either. The increase of the value of its control parameters (h, k) does not affect its accuracy. Since the Markov boundary in grid networks is relatively small, the smallest tested value of k is higher than its Markov boundary size which explains the independence on k. Another reason for its ineffectiveness may be the high percentage of determinism in these networks. It is known that sampling methods are inefficient in the presence of determinism. As a consequence, the percentage of probability mass accumulated in the h sampled tuples is not significant, which cancels the benefits of computing exact probability of evidence for that subset of tuples. Therefore, in such cases a more sophisticated sampling scheme should be used, for example (Gogate & Dechter, 2007). Consequently, for these deterministic grids, ATB\u2019s performance is controlled totally by its bound propagation plugged-in algorithm.\nCoding networks. Table 5 shows the results. We do not report the percentage of P (e) covered by the fully-instantiated cutset tuples because the exact P (e) is not available. We set the time limit of V EC to 1900 seconds (i.e., the maximum computation time required by running ATB in these instances). We only report the results for k = 210 and k = 214 because the increase in the value of k was not effective and did not result in increased accuracy. In this case, the accuracy of ATB increases as the value of h increases. In comparing ATB with the other algorithms we have to distinguish between lower and upper bounds. Regarding lower bounds, ATB clearly outperforms MBE and V EC in all instances. Indeed, the lower bound computed by MBE and V EC is very loose. Regarding\nupper bounds, ATB(h = 150, k = 210) outperforms MBE in three instances (i.e., BN 128, BN 129 and BN 131). When taking time into account ATB only outperforms MBE in instance BN 129.\nSummary of empirical evaluation. We demonstrated that ATB\u2019s bounds converge as h, the number of cutset tuples computed exactly, increases. The speed of convergence varied among benchmarks. The convergence was faster when the active cutset tuples accounted for a large percentage of the probability mass of P (C|e), as shown for the case of cpcs54, cpcs179, and cpcs360 networks. Comparing with a variant of bound propagation called BdP+, ATB was more accurate if given sufficient time and even when given the same time bound, it computed more accurate bounds on many benchmarks.\nWe showed that ATB\u2019s bounds on the posterior marginals can be further improved when used as initial bounds in BdP+. We call this hybrid of ATB followed by BdP+ the BBdP+ algorithm. Our experiments demonstrated the added power of BBdP+ in exploiting the time-accuracy trade-off.\nWe also compared the power of ATB to bound the probability of evidence against the mini-bucket elimination (MBE). We showed that neither algorithm was dominating on all benchmarks. Given the same amount of time, ATB computed more accurate bounds than\nMBE on some instances of bn2o and coding networks. ATB outperformed MBE on all instances of the grid networks on which MBE only computed bounds of 0 and 1. On this benchmark, however, ATB converged very slowly. We believe in part this is due to the grid\u2019s large loop-cutset sizes.\nWe compared ATB\u2019s ability to compute the lower bound on P (e) to V EC on coding networks. V EC obtains the bound by computing a partial sum in the cutset-conditioning formula (see Eq. 2). By comparing the lower bounds generated by ATB and V EC, we can gain insight into the trade-off between enumerating more cutset tuples and bounding the uninstantiated tuples. Since ATB\u2019s lower bound was consistently tighter, we conclude that bounding the uninstantiated tuples is cost-effective."}, {"heading": "5. Related Work", "text": "There are three early approaches which use the same principle as ATB: Poole\u2019s algorithm (1996), bounded conditioning (BC) (Horvitz et al., 1989) which we have already described, and bounded recursive decomposition (Monti & Cooper, 1996). In all these cases the computation of the bounds is composed of an exact inference over a subset of the tuples and a bounding scheme over the total probabilities over the rest of the tuples.\nSimilar to ATB, Poole\u2019s scheme is based on a partial exploration of a search tree. However, his search tree corresponds to the state space over all the variables of the whole network and hence, it is exponential in the total number of variables. In contrast, the tree structure used by our approach corresponds to the state space of the loop-cutset variables; therefore, it is exponential in the loop-cutset size only. In addition, Poole updates the bounding function when a tuple with probability 0 (i.e., a conflict) is discovered.\nAs discussed in Section 2.2, BC is also based on the cutset conditioning principle, but there are two main differences relative to ATB: (i) the probability mass of the missing tuples is bounded via prior probabilities, and consequently (ii) as we proved, the upper bound expression is looser.\nBounded recursive decomposition uses Stochastic simulation (Pearl, 1988) to generate highly probable instantiations of the variables, which is similar to ATB, and bounds the missing elements with 0 and prior values. Therefore this approach resembles Poole\u2019s algorithm and bounded conditioning. Unlike ATB, bounded recursive decomposition requires instantiation of all the variables in the network and relies on priors to guide the simulation. In contrast, our algorithm uses Gibbs sampling on a cutset only which is likely to be more accurate at selecting high probability tuples in presence of evidence. ATB subsumes all three algorithms offering a unifying approach to bounding posteriors with anytime properties, able to improve its bounds by investing more time and exploring more cutset tuples.\nThere are a number of alternative approaches for computing bounds on the marginals. Poole (1998) proposed context-specific bounds obtained from simplifying the conditional probability tables. The method performs a variant of bucket elimination where intermediate tables are collapsed by grouping some probability values together. However, since the method was validated only on a small car diagnosis network with 10 variables, it is hard to draw any conclusions. Larkin (2003) also obtains bounds by simplifying intermediate probability tables in the variable elimination order. He solves an optimization problem to\nfind a table decomposition that minimizes the error. Kearns and Saul (1999, 1998) proposed a specialized large deviation bounds approach for layered networks, while Mannino and Mookerjee (2002) suggested an elaborate bounding scheme with nonlinear objective functions. Jaakkola and Jordan (1999) proposed a variational method for computing lower and upper bounds on posterior marginals in Noisy-Or networks and evaluated its performance in the case of diagnostic QMR-DT network. More recent approaches (Tatikonda, 2003; Taga & Mase, 2006; Ihler, 2007; Mooij & Kappen, 2008) aim to bound the error of belief propagation marginals. The first two approaches are exponential in the size of the Markov boundary. The third approach is linear in the size of the network, but is formulated for pairwise interactions only. Finally, the fourth algorithm is exponential in the number of domain values. Recently, Mooij and Kappen (2008) proposed the box propagation algorithm that propagates local bounds (convex sets of probability distributions) over a subtree of the factor graph representing the problem, rooted in the variable of interest.\nIt is important to note that our approach offers an anytime framework for computing bounds where any of the above bounding algorithms can be used as a subroutine to bound joint probabilities for partially-instantiated tuples within ATB and therefore may improve the performance of any bounding scheme.\nRegarding algorithms that bound the probability of evidence, we already mentioned the mini-bucket schemes and compared against it in Section 4.3. Another recent approach is the tree-reweighted belief propagation (TRW -BP ) (Wainwright, Jaakkola, & Willsky, 2005). TRW -BP is a class of message-passing algorithms that compute an upper bound of P (e) as a convex combination of tree-structured distributions. In a recent paper, Rollon and Dechter (2010) compare TRW -BP , box propagation (adapted for computing the probability of evidence using the chain rule), MBE and ATB-ABdP+. Their empirical evaluation shows the relative strength of each scheme on the different benchmarks (Rollon & Dechter, 2010). In another recent work Wexler and Meek (2008) have proposed MAS, a bounding algorithm for computing the probability of evidence. Shekhar (2009) describes the adjustments required to produce bounds using MAS for Bayesian networks, where the potentials are less than 1. In a forthcoming paper, Wexler and Meek (2010) improve their MAS scheme to obtain tighter bounds and describe how to obtain bounds for Bayesian networks for P (e) as well as for other inferential problems such as the maximal a posteriori and most probable explanation problems. The comparison with this approach is left as future work."}, {"heading": "6. Summary and Conclusions", "text": "The paper explores a general theme of approximation and bounding algorithms for likelihood computation, a task that is known to be hard. While a few methods based on one or two principles emerge, it is clear that pooling together a variety of ideas into a single framework can yield a significant improvement. The current paper provides such a framework. It utilizes the principle of cutset conditioning harnessing the varied strengths of different methods. The framework is inherently anytime, an important characteristic for approximation schemes.\nCutset conditioning is a universal principle. It allows decomposing a problem into a collection of more tractable ones. Some of these subproblems can be solved exactly while others can be approximated. The scheme can be controlled by several parameters. In w-\ncutset we condition on a subset of variables until their treewidth is bounded by w. Each subproblem can then be solved exactly in time and space exponential in w. If the number of subproblems is too large, we can use another parameter, h, to control the number of subproblems solved exactly. The rest of the subproblems are solved using an off-the-shelf bounding scheme.\nWe developed an expression that incorporates all these aspects using the parameters: w - the induced-width of the cutset, h - the number of cutset conditioning subproblems to be solved exactly (e.g., by bucket elimination), and A - the approximation algorithm that bounds each of the bounded subproblems. We showed that the number of subproblems that are approximated is polynomial in h.\nIn our empirical evaluation of the general framework, called ATB, we used the loopcutset scheme (w = 1) and chose as a bounding algorithm a variant of bound propagation (Leisink & Kappen, 2003), yielding the integrated scheme which we call ABdP+. We experimented with several benchmarks for the computing posterior marginals and the probability of evidence, and compared against relevant state of the art algorithms.\nOur results demonstrate the value of our ATB framework across all the benchmarks we have tried. As expected, its anytime aspect is visible showing improved accuracy as a function of time. More significantly, even when provided with equal time and space resources, ATB showed remarkable superiority when compared with our variant of bound propagation and with the mini-bucket elimination algorithm (MBE) (Dechter & Rish, 2003). The latter was recently investigated further by Rollon and Dechter (2010).\nOverall, we can conclude that ATB is a competitive algorithm for both bounding posterior marginals and probability of evidence. Generally, we can expect ATB to perform well in networks whose cutset C is small relative to the total number of variables and whose distribution P (C|e) has a small number of high probability tuples.\nThe possibilities for future work are many. We can explore additional trade offs such as increasing w and therefore decreasing h and improving the selection of the h tuples. We have looked at only one possible instantiation of the plug-in algorithm A. Other approximation algorithms can be tried which may offer different time/accuracy trade-offs. In particular, we plan to investigate the effectiveness of ATB using MBE as plug-in algorithm."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the NSF under award numbers IIS-0331707, IIS-0412854 and IIS-0713118 and by the NIH grant R01-HG004175-02.\nEmma Rollon\u2019s work was done while a postdoctoral student at the Bren School of Information and Computer Sciences, University of California, Irvine.\nThe work here was presented in part in (Bidyuk & Dechter, 2006a, 2006b)."}, {"heading": "Appendix A. Analysis of Bounded Conditioning", "text": "Theorem 2.1 The interval between lower and upper bounds computed by bounded conditioning is lower bounded by the probability mass of prior distribution P (C) of the unexplored cutset tuples: \u2200h, PUBC(x|e)\u2212 P L BC(x|e) \u2265 \u2211M i=h+1 P (c i).\nProof.\nPUBC(x|e)\u2212 P L BC(x|e) =\n\u2211M i=h+1 P (c i)( \u2211h i=1 P (c i, e) + \u2211M i=h+1 P (c i)) \u2211h\ni=1 P (c i, e)\n+\n\u2211h i=1 P (x, c i, e) \u2211h\ni=1 P (c i, e)\n\u2212\n\u2211h i=1 P (x, c i, e) \u2211h\ni=1 P (c i, e) + \u2211M i=h+1 P (c i)\n\u2265\n\u2211M i=h+1 P (c i)( \u2211h i=1 P (c i, e) + \u2211M i=h+1 P (c i)) \u2211h\ni=1 P (c i, e)\n= M \u2211\ni=h+1\nP (ci) + ( \u2211M i=h+1 P (c i))2\n\u2211h i=1 P (c\ni, e) \u2265\nM \u2211\ni=h+1\nP (ci)"}, {"heading": "Appendix B. Bounding Posteriors of Cutset Nodes", "text": "So far, we only considered computation of posterior marginals for variable X \u2208 X\\(C \u222a E). Now we focus on computing bounds for a cutset node Ck \u2208 C. Let c \u2032 k \u2208 D(C) be some value in domain of Ck. Then, we can compute exact posterior marginal P (ck|e) using Bayes formula:\nP (c\u2032k|e) = P (c\u2032k, e)\nP (e) =\n\u2211M i=1 \u03b4(c \u2032 k, c i)P (ci, e) \u2211M\ni=1 P (c i, e)\n(23)\nwhere \u03b4(c\u2032k, c i) is a Dirac delta-function so that \u03b4(c\u2032k, c i) = 1 iff cik = c \u2032 k and \u03b4(c \u2032 k, c i) = 0 otherwise. To simplify notation, let Z = C\\Z. Let Mk denote the number of tuples in state-space of Z. Then we can re-write the numerator as:\nM \u2211\ni=1\n\u03b4(c\u2032k, c i)P (ci, e) =\nMk \u2211\ni=1\nP (c\u2032k, z i, e)\nand the denominator can be decomposed as:\nM \u2211\ni=1\nP (ci, e) = \u2211\nck\u2208D(Ck)\nMk \u2211\ni=1\nP (c\u2032k, z i, e)\nThen, we can re-write the expression for P (c\u2032k|e) as follows:\nP (c\u2032k|e) =\n\u2211Mk i=1 P (c \u2032 k, z i, e) \u2211\nck\u2208D(Ck)\n\u2211Mk i=1 P (ck, z\ni, e) (24)\nLet hck be the number of full cutset tuples where c i k = ck. Then, we can decompose the numerator in Eq. (24) as follows:\nMk \u2211\ni=1\nP (c\u2032k, z i, e) =\nhc\u2032 k \u2211\ni=1\nP (c\u2032k, z i, e) +\nMk \u2211\ni=hc\u2032 k +1\nP (c\u2032k, z i, e)\nSimilarly, we can decompose the sums in the denominator:\n\u2211\nck\u2208D(Ck)\nMk \u2211\ni=1\nP (ck, z i, e) =\n\u2211\nck\u2208D(Ck)\nhck \u2211\ni=1\nP (ck, z i, e) +\n\u2211\nck\u2208D(Ck)\nMk \u2211\ni=hck+1\nP (ck, z i, e)\nAfter decomposition, the Eq. (24) takes on the form:\nP (c\u2032k|e) =\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z i, e) + \u2211Mk\ni=hc\u2032 k +1 P (c\n\u2032 k, z i, e)\n\u2211\nck\u2208D(Ck)\n\u2211hck i=1 P (ck, z i, e) + \u2211\nck\u2208D(Ck) \u2211Mk i=hck+1\nP (ck, zi, e) (25)\nNow, for conciseness, we can group together all fully instantiated tuples in the denominator:\n\u2211\nck\u2208D(Ck)\nhck \u2211\ni=1\nP (ck, z i, e) =\nh \u2211\ni=1\nP (ci, e)\nThen, Eq. (25) transforms into:\nP (c\u2032k|e) =\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z i, e) + \u2211Mk\ni=hc\u2032 k +1 P (c\n\u2032 k, z i, e)\n\u2211h i=1 P (c i, e) + \u2211Mk\ni=hck+1\n\u2211\nck\u2208D(Ck) P (ck, zi, e)\n(26)\nNow, we can replace each sum \u2211Mk\ni=hc\u2032 k +1 over unexplored cutset tuples with a sum over\nthe partially-instantiated cutset tuples. Denoting as M \u2032ck = Mk \u2212 hck + 1 the number of partially instantiated cutset tuples for Ck = ck, we obtain:\nP (c\u2032k|e) =\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z i, e) + \u2211\nM \u2032 c\u2032 k\nj=1 P (c \u2032 k, z j 1:qj , e)\n\u2211h i=1 P (c\ni, e) + \u2211M \u2032ck\nj=1\n\u2211\nck\u2208D(Ck) P (ck, z j 1:qj\n, e) (27)\nIn order to obtain lower and upper bounds formulation, we will separate the sum of joint probabilities P (c\u2032k, z j 1:qj , e) where Ck = c \u2032 k from the rest:\nP (c\u2032k|e) =\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z i, e) + \u2211\nM \u2032 c\u2032 k\nj=1 P (c \u2032 k, z j 1:qj , e)\n\u2211h i=1 P (c i, e) + \u2211\nM \u2032 c\u2032 k j=1 P (c \u2032 k, z j 1:qj , e) + \u2211M \u2032ck j=1 \u2211 ck 6=c \u2032 k P (ck, z j 1:qj , e)\n(28)\nIn the expression above, probabilities P (ck, z i, e) and P (ci, e) are computed exactly since they correspond to full cutset instantiations. Probabilities P (ck, z i 1:qi\n, e), however, will be bounded since only partial cutset is observed. Observing that both the numerator and denominator have component P (c\u2032k, z i 1:qi , e) and replacing it with an upper bound PU (c\u2032k, z i 1:qi\n, e) in both the numerator and denominator, we will obtain an upper bound on P (c\u2032k|e) due to Lemma 3.2:\nP (c\u2032k|e) \u2264\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z i, e) + \u2211\nM \u2032 c\u2032 k j=1 P U A (c \u2032 k, z j 1:qj , e)\n\u2211h i=1 P (c i, e) + \u2211\nM \u2032 c\u2032 k j=1 P U A (c \u2032 k, z j 1:qj , e) + \u2211M \u2032ck j=1 \u2211 ck 6=c \u2032 k P (ck, z j 1:qj , e)\n(29)\nFinally, replacing P (ck, z j 1:qj , e), ck 6= c \u2032 k, with a lower bound (also increasing fraction value), we obtain:\nP (c\u2032k|e) \u2264\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z\ni, e) + \u2211M \u2032ck\nj=1 P U A (c \u2032 k, z j 1:qj , e)\n\u2211h i=1 P (c\ni, e) + \u2211M \u2032ck\nj=1 P U A (c \u2032 k, z j 1:qj\n, e) + \u2211M \u2032ck\nj=1\n\u2211\nck 6=c \u2032 k PLA(ck, z j 1:qj\n, e) = PUc\n(30) The lower bound derivation is similar. We start with Eq. (28) and replace P (c\u2032k, z i 1:qi\n, e) in the numerator and denominator with a lower bound. Lemma 3.2 guarantees that the resulting fraction will be a lower bound on P (c\u2032k|e):\nP (c\u2032k|e) \u2265\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z i, e) + \u2211\nM \u2032 c\u2032 k j=1 P L A(c \u2032 k, z j 1:qj , e)\n\u2211h i=1 P (c i, e) + \u2211\nM \u2032 c\u2032 k j=1 P L A(c \u2032 k, z j 1:qj , e) + \u2211M \u2032ck j=1 \u2211 ck 6=c \u2032 k P (ck, z j 1:qj , e)\n(31)\nFinally, grouping PLA(c \u2032 k, z j 1:qj , e) and \u2211 ck 6=c \u2032 k P (ck, z j 1:qj , e) under one sum and replacing PLA(c \u2032 k, z j 1:qj , e)+ \u2211 ck 6=c \u2032 k P (ck, z j 1:qj , e) with an upper bound, we obtain the lower bound PLc :\nP (c\u2032k|e) \u2265\n\u2211 hc\u2032 k\ni=1 P (c \u2032 k, z i, e) + \u2211\nM \u2032 c\u2032 k j=1 P L A(c \u2032 k, z j 1:qj , e)\n\u2211h i=1 P (c i, e) + \u2211\nM \u2032 c\u2032 k j=1 UB[P L A(c \u2032 k, z j 1:qj , e) + \u2211 ck 6=c \u2032 k P (ck, z j 1:qj , e)]\n= PLc (32)\nwhere\nUB[PLA(c \u2032 k, z j 1:qj , e) + \u2211\nck 6=c \u2032\nk\nP (ck, z j 1:qj , e)] = min\n{\nPLA(c \u2032 k, z j 1:qj , e) + \u2211 ck 6=c \u2032 k PUA (ck, z j 1:qj , e) PUA (z j 1:qj , e)\nThe lower bound PLc is a cutset equivalent of the lower bound P L obtained in Eq. (15).\nWith respect to computing bounds on P (c\u2032k, z1:q, e) in Eq. (30) and (32) in practice, we distinguish two cases. We demonstrate them on the example of upper bound.\nIn the first case, each partially instantiated tuple c1:q that includes node Ck, namely k \u2264 q, can be decomposed as c1:q = z1:q \u22c3 c\u2032k so that:\nPU (c\u2032k, z1:q, e) = P U (c1:q, e)\nThe second case concerns the partially instantiated tuples c1:q that do not include node Ck, namely k > q. In that case, we compute upper bound by decomposing:\nPU (c\u2032k, z1:q, e) = P U (ck|c1:q)P U (c1:q, e)\nAppendix C. ATB Properties\nTheorem 3.2 ATB bounds interval length is upper bounded by a monotonic nonincreasing function of h:\nPUA (x|e)\u2212 P L A(x|e) \u2264\n\u2211M j=h+1 P (c j) \u2211h\ni=1 P (c i, e) + \u2211M j=h+1 P (c\nj) , Ih\nProof. The upper bound on the bounds interval follows from the fact that, PUA (x|e)\u2212 PLA(x|e) \u2264 P U BF (x|e) \u2212 P L BF (x|e) and from the definitions of brute force lower and upper bounds given by Eq. (21) and (22). We only need to prove that the upper bound is monotonously non-increasing as a function of h.\nIh\u22121 =\n\u2211M j=h P (c j) \u2211h\u22121\ni=1 P (c i, e) + \u2211M j=h P (c\nj) =\nP (ch) + \u2211M j=h+1 P (c j)\n\u2211h\u22121 i=1 P (c i, e) + P (ch) + \u2211M j=h+1 P (c j)\nSince P (ch) \u2265 P (ch, e), then replacing P (ch) with P (ch, e) and applying Lemma 3.1, yields:\nIh\u22121 \u2265 P (ch, e) + \u2211M j=h+1 P (c j) \u2211h\u22121\ni=1 P (c i, e) + P (ch, e) + \u2211M j=h+1 P (c\nj) =\nP (ch, e) + \u2211M j=h+1 P (c j)\n\u2211h i=1 P (c i, e) + \u2211M j=h+1 P (c j)\n\u2265\n\u2211M j=h+1 P (c j) \u2211h\ni=1 P (c i, e) + \u2211M j=h+1 P (c\nj) = Ih\nThus, Ih\u22121 \u2265 Ih."}], "references": [{"title": "Approximating MAPs for belief networks is NP-hard and other theorems", "author": ["A.M. Abdelbar", "S.M. Hedetniemi"], "venue": "Artificial Intelligence,", "citeRegEx": "Abdelbar and Hedetniemi,? \\Q1998\\E", "shortCiteRegEx": "Abdelbar and Hedetniemi", "year": 1998}, {"title": "Munin - an expert EMG assistant", "author": ["S. Andreassen", "F. Jensen", "S. Andersen", "B. Falck", "U. Kjaerulff", "M. Woldbye", "A. Srensen", "A. Rosenfalck"], "venue": "Computer-Aided Electromyography and Expert Systems,", "citeRegEx": "Andreassen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Andreassen et al\\.", "year": 1990}, {"title": "A sufficiently fast algorithm for finding close to optimal junction trees", "author": ["A. Becker", "D. Geiger"], "venue": "In Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Becker and Geiger,? \\Q1996\\E", "shortCiteRegEx": "Becker and Geiger", "year": 1996}, {"title": "The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks", "author": ["I. Beinlich", "G. Suermondt", "R. Chavez", "G. Cooper"], "venue": "In Proceedings of the Second European Conference on AI and Medicine. Springer\u2013Verlag", "citeRegEx": "Beinlich et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Beinlich et al\\.", "year": 1989}, {"title": "Exploiting Graph Cutsets for Sampling-Based Approximations in Bayesian Networks", "author": ["B. Bidyuk"], "venue": "Ph.D. Thesis. Ph.D. thesis,", "citeRegEx": "Bidyuk,? \\Q2006\\E", "shortCiteRegEx": "Bidyuk", "year": 2006}, {"title": "Cycle-cutset sampling for Bayesian networks", "author": ["B. Bidyuk", "R. Dechter"], "venue": "In Proceedings of the 16th Canadian Conference on Artificial Intelligence", "citeRegEx": "Bidyuk and Dechter,? \\Q2003\\E", "shortCiteRegEx": "Bidyuk and Dechter", "year": 2003}, {"title": "Empirical study of w-cutset sampling for Bayesian networks", "author": ["B. Bidyuk", "R. Dechter"], "venue": "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Bidyuk and Dechter,? \\Q2003\\E", "shortCiteRegEx": "Bidyuk and Dechter", "year": 2003}, {"title": "An anytime scheme for bounding posterior beliefs", "author": ["B. Bidyuk", "R. Dechter"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence", "citeRegEx": "Bidyuk and Dechter,? \\Q2006\\E", "shortCiteRegEx": "Bidyuk and Dechter", "year": 2006}, {"title": "Improving bound propagation", "author": ["B. Bidyuk", "R. Dechter"], "venue": "In Proceedings of the 17th European Conference on AI (ECAI\u20192006),", "citeRegEx": "Bidyuk and Dechter,? \\Q2006\\E", "shortCiteRegEx": "Bidyuk and Dechter", "year": 2006}, {"title": "Cutset sampling for bayesian networks", "author": ["B. Bidyuk", "R. Dechter"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bidyuk and Dechter,? \\Q2007\\E", "shortCiteRegEx": "Bidyuk and Dechter", "year": 2007}, {"title": "The computational complexity of probabilistic inferences", "author": ["G. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "Cooper,? \\Q1990\\E", "shortCiteRegEx": "Cooper", "year": 1990}, {"title": "Approximating probabilistic inference in Bayesian belief networks is NP-hard", "author": ["P. Dagum", "M. Luby"], "venue": "Artificial Intelligence,", "citeRegEx": "Dagum and Luby,? \\Q1993\\E", "shortCiteRegEx": "Dagum and Luby", "year": 1993}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter,? \\Q1999\\E", "shortCiteRegEx": "Dechter", "year": 1999}, {"title": "Mini-buckets: A general scheme for bounded inference", "author": ["R. Dechter", "I. Rish"], "venue": "Journal of the ACM,", "citeRegEx": "Dechter and Rish,? \\Q2003\\E", "shortCiteRegEx": "Dechter and Rish", "year": 2003}, {"title": "Samplesearch: A scheme that searches for consistent samples", "author": ["V. Gogate", "R. Dechter"], "venue": "In Proceedings of 11th International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Gogate and Dechter,? \\Q2007\\E", "shortCiteRegEx": "Gogate and Dechter", "year": 2007}, {"title": "Bounded conditioning: Flexible inference for decisions under scarce resources", "author": ["E. Horvitz", "H. Suermondt", "G. Cooper"], "venue": "In Workshop on Uncertainty in Artificial Intelligence,", "citeRegEx": "Horvitz et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Horvitz et al\\.", "year": 1989}, {"title": "Accuracy bounds for belief propagation", "author": ["A. Ihler"], "venue": "In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI-2007),", "citeRegEx": "Ihler,? \\Q2007\\E", "shortCiteRegEx": "Ihler", "year": 2007}, {"title": "Variational probabilistic inference and the qmr-dt network", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Jaakkola and Jordan,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1999}, {"title": "Stochastic local search for Bayesian networks", "author": ["K. Kask", "R. Dechter"], "venue": "Workshop on AI and Statistics,", "citeRegEx": "Kask and Dechter,? \\Q1999\\E", "shortCiteRegEx": "Kask and Dechter", "year": 1999}, {"title": "Large deviation methods for approximate probabilistic inference, with rates of convergence", "author": ["M. Kearns", "L. Saul"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kearns and Saul,? \\Q1998\\E", "shortCiteRegEx": "Kearns and Saul", "year": 1998}, {"title": "Inference in multilayer networks via large deviation bounds", "author": ["M. Kearns", "L. Saul"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kearns and Saul,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Saul", "year": 1999}, {"title": "The use of a Bayesian network in the design of a decision support system for growing malting Barley without use of pesticides", "author": ["K. Kristensen", "I. Rasmussen"], "venue": "Computers and Electronics in Agriculture,", "citeRegEx": "Kristensen and Rasmussen,? \\Q2002\\E", "shortCiteRegEx": "Kristensen and Rasmussen", "year": 2002}, {"title": "Approximate decomposition: A method for bounding and estimating probabilistic and deterministic queries", "author": ["D. Larkin"], "venue": "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Larkin,? \\Q2003\\E", "shortCiteRegEx": "Larkin", "year": 2003}, {"title": "Probability bounds for goal directed queries in Bayesian networks", "author": ["M.V. Mannino", "V.S. Mookerjee"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Mannino and Mookerjee,? \\Q2002\\E", "shortCiteRegEx": "Mannino and Mookerjee", "year": 2002}, {"title": "Bounded recursive decomposition: a search-based method for belief network inference under limited resources", "author": ["S. Monti", "G. Cooper"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Monti and Cooper,? \\Q1996\\E", "shortCiteRegEx": "Monti and Cooper", "year": 1996}, {"title": "Bounds on marginal probability distributions", "author": ["J.M. Mooij", "H.J. Kappen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mooij and Kappen,? \\Q2008\\E", "shortCiteRegEx": "Mooij and Kappen", "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Probabilistic conflicts in a search algorithm for estimating posterior probabilities in Bayesian networks", "author": ["D. Poole"], "venue": "Artificial Intelligence,", "citeRegEx": "Poole,? \\Q1996\\E", "shortCiteRegEx": "Poole", "year": 1996}, {"title": "Context-specific approximation in probabilistic inference", "author": ["D. Poole"], "venue": "In Proceedings of 14th Uncertainty in Artificial Intelligence", "citeRegEx": "Poole,? \\Q1998\\E", "shortCiteRegEx": "Poole", "year": 1998}, {"title": "Knowledge engineering for large belief networks", "author": ["M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion"], "venue": "In Proceedings of 10th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Pradhan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 1994}, {"title": "New mini-bucket partitioning heuristics for bounding the probability of evidence", "author": ["E. Rollon", "R. Dechter"], "venue": "In Proceedings of the 24th National Conference on Artificial Intelligence", "citeRegEx": "Rollon and Dechter,? \\Q2010\\E", "shortCiteRegEx": "Rollon and Dechter", "year": 2010}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "Roth,? \\Q1996\\E", "shortCiteRegEx": "Roth", "year": 1996}, {"title": "Fixing and extending the multiplicative approximation scheme. Master\u2019s thesis, School of Information and Computer Science, University of California, Irvine", "author": ["S. Shekhar"], "venue": null, "citeRegEx": "Shekhar,? \\Q2009\\E", "shortCiteRegEx": "Shekhar", "year": 2009}, {"title": "Error bounds between marginal probabilities and beliefs of loopy belief propagation algorithm", "author": ["N. Taga", "S. Mase"], "venue": "In Advances in Artificial Intelligence, Proceedings of 5th Mexican International Conference on Artificial Intelligence", "citeRegEx": "Taga and Mase,? \\Q2006\\E", "shortCiteRegEx": "Taga and Mase", "year": 2006}, {"title": "Convergence of the sum-product algorithm", "author": ["S.C. Tatikonda"], "venue": "In Proceedings of IEEE Information Theory Workshop,", "citeRegEx": "Tatikonda,? \\Q2003\\E", "shortCiteRegEx": "Tatikonda", "year": 2003}, {"title": "A new class of upper bounds on the log partition function", "author": ["M.J. Wainwright", "T. Jaakkola", "A.S. Willsky"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "MAS: a multiplicative approximation scheme for probabilistic inference", "author": ["Y. Wexler", "C. Meek"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wexler and Meek,? \\Q2008\\E", "shortCiteRegEx": "Wexler and Meek", "year": 2008}, {"title": "Approximating max-sum-product problems using multiplicative error bounds", "author": ["Y. Wexler", "C. Meek"], "venue": "In Bayesian Statistics", "citeRegEx": "Wexler and Meek,? \\Q2010\\E", "shortCiteRegEx": "Wexler and Meek", "year": 2010}], "referenceMentions": [{"referenceID": 31, "context": "Deriving bounds on posteriors with a given accuracy is clearly an NP-hard problem (Abdelbar & Hedetniemi, 1998; Dagum & Luby, 1993; Roth, 1996) and indeed, most available approximation algorithms provide little or no guarantee on the quality of their approximations.", "startOffset": 82, "endOffset": 143}, {"referenceID": 22, "context": "Still, a few approaches were presented in the past few years for bounding posterior marginals (Horvitz, Suermondt, & Cooper, 1989; Poole, 1996, 1998; Mannino & Mookerjee, 2002; Mooij & Kappen, 2008) and for bounding the probability of evidence (Dechter & Rish, 2003; Larkin, 2003; Leisink & Kappen, 2003).", "startOffset": 244, "endOffset": 304}, {"referenceID": 26, "context": "In this paper we develop a framework that can accept any bounding scheme and improve its bounds in an anytime manner using the cutset-conditioning principle (Pearl, 1988).", "startOffset": 157, "endOffset": 170}, {"referenceID": 4, "context": "One is to sample over the cutset space and subsequently approximate the distribution P (C|e) from the samples, as shown by Bidyuk and Dechter (2007). The second approach, which we use here, is to enumerate h out of M tuples and bound the rest.", "startOffset": 123, "endOffset": 149}, {"referenceID": 4, "context": "One is to sample over the cutset space and subsequently approximate the distribution P (C|e) from the samples, as shown by Bidyuk and Dechter (2007). The second approach, which we use here, is to enumerate h out of M tuples and bound the rest. We shall refer to the selected tuples as \u201cactive\u201d tuples. A lower bound on P (e) can be obtained by computing exactly the quantities P (c, e) for 1 \u2264 i \u2264 h resulting in a partial sum in Eq. (2). This approach is likely to perform well if the selected h tuples contain most of the probability mass of P (e). However, this approach cannot be applied directly to obtain the bounds on the posterior marginals in Eq. (3). Even a partial sum in Eq. (3) requires computing P (c|e) which in turn requires a normalization constant P (e). We can obtain naive bounds on posterior marginals from Eq. (1) using P(e) and P (e) to denote available lower and upper bounds over joint probabilities: P(x, e) PU (e) \u2264 P (x|e) \u2264 P (x, e) PL(e) However, those bounds usually perform very poorly and often yield an upper bound > 1. Horvitz et. al (1989) were the first to propose a scheme for bounding posterior marginals based on a subset of cutset tuples.", "startOffset": 123, "endOffset": 1076}, {"referenceID": 4, "context": "We therefore utilize a variant of bound propagation called ABdP+, introduced by Bidyuk and Dechter (2006b), that trades accuracy for speed.", "startOffset": 80, "endOffset": 107}, {"referenceID": 26, "context": "A Bayesian network B (Pearl, 1988) is a pair <G,P> where G is a directed acyclic graph whose nodes are the variables X and P = {P (Xi|pai) | i = 1, .", "startOffset": 21, "endOffset": 34}, {"referenceID": 26, "context": "A Markov boundary of Xi is its minimal Markov blanket (Pearl, 1988).", "startOffset": 54, "endOffset": 67}, {"referenceID": 10, "context": "Both tasks are NPhard (Cooper, 1990).", "startOffset": 22, "endOffset": 36}, {"referenceID": 15, "context": "Applying definitions for P(c|e) = P (c ,e) \u2211h i=1 P (c i,e)+ \u2211M i=h+1 P (c i) and P (c|e) = P (c ,e) \u2211h i=1 P (c i,e) from Horvitz et al. (1989), we get: P BC(x|e) , \u2211h i=1 P (x, c , e) \u2211h i=1 P (c i, e) + ( \u2211M i=h+1 P (c ))( \u2211h i=1 P (c , e) + \u2211M i=h+1 P (c )) \u2211h i=1 P (c i, e) (7)", "startOffset": 123, "endOffset": 145}, {"referenceID": 12, "context": "In steps 1 and 2, we generate h fully-instantiated cutset tuples and compute exactly the probabilities P (c, e) and P (X, c, e) for i \u2264 h, \u2200X \u2208 X\\(C\u222aE), using, for example, the bucket-elimination algorithm (Dechter, 1999).", "startOffset": 206, "endOffset": 221}, {"referenceID": 22, "context": "We also compare with approximate decomposition (AD) (Larkin, 2003) whenever it is feasible and relevant.", "startOffset": 52, "endOffset": 66}, {"referenceID": 4, "context": "2 Bounding Single-Variable Marginals We compare the performance of the following three algorithms: ATB (with ABdP+ as a plug-in), BdP+, as described in the previous section, and BBdP+ (Bidyuk & Dechter, 2006a). The latter is a combination of ATB and BdP+. First, we run algorithm ATB with ABdP+ plug-in. Then, we use the bounds computed by ATB to initialize bounds in BdP+ (instead of 0 and 1) and run BdP+. Note that, given fixed values of h and k, BBdP+ will always compute tighter bounds than either ATB and BdP+. Our goal is to analyze its trade-off between the increase of the bounds\u2019 accuracy and the computation time overhead. We also compare with approximate decomposition (AD) (Larkin, 2003) whenever it is feasible and relevant. We did not include the results for the stand-alone ABdP+ since our objective was to compare ATB bounds with the most accurate bounds obtained by bound propagation. Bidyuk (2006) provides additional comparison with various refinements of BdP (Bidyuk & Dechter, 2006b) mentioned earlier.", "startOffset": 185, "endOffset": 917}, {"referenceID": 10, "context": "Alarm network is a model for monitoring patients undergoing surgery in an operating room (Beinlich, Suermondt, Chavez, & Cooper, 1989). Barley network is a part of the decisionsupport system for growing malting barley (Kristensen & Rasmussen, 2002). CPCS networks are derived from the Computer-Based Patient Care Simulation system and based on INTERNIST-1 and Quick Medical Reference Expert systems (Pradhan, Provan, Middleton, & Henrion, 1994). We experiment with cpcs54, cpcs179, cpcs360b, and cpcs422b networks. Munin networks are a part of the expert system for computer-aided electromyography (Andreassen, Jensen, Andersen, Falck, Kjaerulff, Woldbye, Srensen, Rosenfalck, & Jensen, 1990). We experiment with Munin3 and Munin4 networks. For each network, we generated 20 different sets of evidence variables picked at random. For Barley network, we select evidence variables as defined by Kristensen and Rasmussen (2002). Table 1 summarizes the characteristic of each network.", "startOffset": 121, "endOffset": 925}, {"referenceID": 22, "context": "Larkin (2003) reported bounds on cpcs360b and cpcs422b using AD algorithm.", "startOffset": 0, "endOffset": 14}, {"referenceID": 12, "context": "Namely, the algorithm conditions or instantiates enough variables so that the remaining problem conditioned on the instantiated variables can be solved exactly using bucket elimination (Dechter, 1999).", "startOffset": 185, "endOffset": 200}, {"referenceID": 15, "context": "Related Work There are three early approaches which use the same principle as ATB: Poole\u2019s algorithm (1996), bounded conditioning (BC) (Horvitz et al., 1989) which we have already described, and bounded recursive decomposition (Monti & Cooper, 1996).", "startOffset": 135, "endOffset": 157}, {"referenceID": 26, "context": "Bounded recursive decomposition uses Stochastic simulation (Pearl, 1988) to generate highly probable instantiations of the variables, which is similar to ATB, and bounds the missing elements with 0 and prior values.", "startOffset": 59, "endOffset": 72}, {"referenceID": 23, "context": "Related Work There are three early approaches which use the same principle as ATB: Poole\u2019s algorithm (1996), bounded conditioning (BC) (Horvitz et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 10, "context": ", 1989) which we have already described, and bounded recursive decomposition (Monti & Cooper, 1996). In all these cases the computation of the bounds is composed of an exact inference over a subset of the tuples and a bounding scheme over the total probabilities over the rest of the tuples. Similar to ATB, Poole\u2019s scheme is based on a partial exploration of a search tree. However, his search tree corresponds to the state space over all the variables of the whole network and hence, it is exponential in the total number of variables. In contrast, the tree structure used by our approach corresponds to the state space of the loop-cutset variables; therefore, it is exponential in the loop-cutset size only. In addition, Poole updates the bounding function when a tuple with probability 0 (i.e., a conflict) is discovered. As discussed in Section 2.2, BC is also based on the cutset conditioning principle, but there are two main differences relative to ATB: (i) the probability mass of the missing tuples is bounded via prior probabilities, and consequently (ii) as we proved, the upper bound expression is looser. Bounded recursive decomposition uses Stochastic simulation (Pearl, 1988) to generate highly probable instantiations of the variables, which is similar to ATB, and bounds the missing elements with 0 and prior values. Therefore this approach resembles Poole\u2019s algorithm and bounded conditioning. Unlike ATB, bounded recursive decomposition requires instantiation of all the variables in the network and relies on priors to guide the simulation. In contrast, our algorithm uses Gibbs sampling on a cutset only which is likely to be more accurate at selecting high probability tuples in presence of evidence. ATB subsumes all three algorithms offering a unifying approach to bounding posteriors with anytime properties, able to improve its bounds by investing more time and exploring more cutset tuples. There are a number of alternative approaches for computing bounds on the marginals. Poole (1998) proposed context-specific bounds obtained from simplifying the conditional probability tables.", "startOffset": 86, "endOffset": 2016}, {"referenceID": 10, "context": ", 1989) which we have already described, and bounded recursive decomposition (Monti & Cooper, 1996). In all these cases the computation of the bounds is composed of an exact inference over a subset of the tuples and a bounding scheme over the total probabilities over the rest of the tuples. Similar to ATB, Poole\u2019s scheme is based on a partial exploration of a search tree. However, his search tree corresponds to the state space over all the variables of the whole network and hence, it is exponential in the total number of variables. In contrast, the tree structure used by our approach corresponds to the state space of the loop-cutset variables; therefore, it is exponential in the loop-cutset size only. In addition, Poole updates the bounding function when a tuple with probability 0 (i.e., a conflict) is discovered. As discussed in Section 2.2, BC is also based on the cutset conditioning principle, but there are two main differences relative to ATB: (i) the probability mass of the missing tuples is bounded via prior probabilities, and consequently (ii) as we proved, the upper bound expression is looser. Bounded recursive decomposition uses Stochastic simulation (Pearl, 1988) to generate highly probable instantiations of the variables, which is similar to ATB, and bounds the missing elements with 0 and prior values. Therefore this approach resembles Poole\u2019s algorithm and bounded conditioning. Unlike ATB, bounded recursive decomposition requires instantiation of all the variables in the network and relies on priors to guide the simulation. In contrast, our algorithm uses Gibbs sampling on a cutset only which is likely to be more accurate at selecting high probability tuples in presence of evidence. ATB subsumes all three algorithms offering a unifying approach to bounding posteriors with anytime properties, able to improve its bounds by investing more time and exploring more cutset tuples. There are a number of alternative approaches for computing bounds on the marginals. Poole (1998) proposed context-specific bounds obtained from simplifying the conditional probability tables. The method performs a variant of bucket elimination where intermediate tables are collapsed by grouping some probability values together. However, since the method was validated only on a small car diagnosis network with 10 variables, it is hard to draw any conclusions. Larkin (2003) also obtains bounds by simplifying intermediate probability tables in the variable elimination order.", "startOffset": 86, "endOffset": 2396}, {"referenceID": 34, "context": "More recent approaches (Tatikonda, 2003; Taga & Mase, 2006; Ihler, 2007; Mooij & Kappen, 2008) aim to bound the error of belief propagation marginals.", "startOffset": 23, "endOffset": 94}, {"referenceID": 16, "context": "More recent approaches (Tatikonda, 2003; Taga & Mase, 2006; Ihler, 2007; Mooij & Kappen, 2008) aim to bound the error of belief propagation marginals.", "startOffset": 23, "endOffset": 94}, {"referenceID": 16, "context": "Kearns and Saul (1999, 1998) proposed a specialized large deviation bounds approach for layered networks, while Mannino and Mookerjee (2002) suggested an elaborate bounding scheme with nonlinear objective functions.", "startOffset": 0, "endOffset": 141}, {"referenceID": 15, "context": "Jaakkola and Jordan (1999) proposed a variational method for computing lower and upper bounds on posterior marginals in Noisy-Or networks and evaluated its performance in the case of diagnostic QMR-DT network.", "startOffset": 0, "endOffset": 27}, {"referenceID": 15, "context": "More recent approaches (Tatikonda, 2003; Taga & Mase, 2006; Ihler, 2007; Mooij & Kappen, 2008) aim to bound the error of belief propagation marginals. The first two approaches are exponential in the size of the Markov boundary. The third approach is linear in the size of the network, but is formulated for pairwise interactions only. Finally, the fourth algorithm is exponential in the number of domain values. Recently, Mooij and Kappen (2008) proposed the box propagation algorithm that propagates local bounds (convex sets of probability distributions) over a subtree of the factor graph representing the problem, rooted in the variable of interest.", "startOffset": 60, "endOffset": 446}, {"referenceID": 12, "context": "In a recent paper, Rollon and Dechter (2010) compare TRW -BP , box propagation (adapted for computing the probability of evidence using the chain rule), MBE and ATB-ABdP+.", "startOffset": 30, "endOffset": 45}, {"referenceID": 12, "context": "In a recent paper, Rollon and Dechter (2010) compare TRW -BP , box propagation (adapted for computing the probability of evidence using the chain rule), MBE and ATB-ABdP+. Their empirical evaluation shows the relative strength of each scheme on the different benchmarks (Rollon & Dechter, 2010). In another recent work Wexler and Meek (2008) have proposed MAS, a bounding algorithm for computing the probability of evidence.", "startOffset": 30, "endOffset": 342}, {"referenceID": 12, "context": "In a recent paper, Rollon and Dechter (2010) compare TRW -BP , box propagation (adapted for computing the probability of evidence using the chain rule), MBE and ATB-ABdP+. Their empirical evaluation shows the relative strength of each scheme on the different benchmarks (Rollon & Dechter, 2010). In another recent work Wexler and Meek (2008) have proposed MAS, a bounding algorithm for computing the probability of evidence. Shekhar (2009) describes the adjustments required to produce bounds using MAS for Bayesian networks, where the potentials are less than 1.", "startOffset": 30, "endOffset": 440}, {"referenceID": 12, "context": "In a recent paper, Rollon and Dechter (2010) compare TRW -BP , box propagation (adapted for computing the probability of evidence using the chain rule), MBE and ATB-ABdP+. Their empirical evaluation shows the relative strength of each scheme on the different benchmarks (Rollon & Dechter, 2010). In another recent work Wexler and Meek (2008) have proposed MAS, a bounding algorithm for computing the probability of evidence. Shekhar (2009) describes the adjustments required to produce bounds using MAS for Bayesian networks, where the potentials are less than 1. In a forthcoming paper, Wexler and Meek (2010) improve their MAS scheme to obtain tighter bounds and describe how to obtain bounds for Bayesian networks for P (e) as well as for other inferential problems such as the maximal a posteriori and most probable explanation problems.", "startOffset": 30, "endOffset": 611}, {"referenceID": 12, "context": "More significantly, even when provided with equal time and space resources, ATB showed remarkable superiority when compared with our variant of bound propagation and with the mini-bucket elimination algorithm (MBE) (Dechter & Rish, 2003). The latter was recently investigated further by Rollon and Dechter (2010). Overall, we can conclude that ATB is a competitive algorithm for both bounding posterior marginals and probability of evidence.", "startOffset": 216, "endOffset": 313}], "year": 2010, "abstractText": "The paper presents a scheme for computing lower and upper bounds on the posterior marginals in Bayesian networks with discrete variables. Its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior marginals and enhance its performance in an anytime manner. The scheme uses the cutset conditioning principle to tighten existing bounding schemes and to facilitate anytime behavior, utilizing a fixed number of cutset tuples. The accuracy of the bounds improves as the number of used cutset tuples increases and so does the computation time. We demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant of the bound propagation algorithm as a plug-in scheme.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}