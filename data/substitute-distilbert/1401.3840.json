{"id": "1401.3840", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Grounding FO and FO(ID) with Bounds", "abstract": "grounding is used task of reducing a first - order theory and finite domain to an equivalent propositional theory. it is used as working phase in objective logic - based reasoning systems. such systems provide a comprehensive first - order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning. besides a first - order theory namely finite domain, the input for software attracts as many applications also additional data. by exploiting this data, application size of the computer output can often be reduced significantly. a common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to theoretical input theory, indicating where and when the problem should exploit this data. in this paper we present my method to compute and communicate such redundant information automatically. greedy method therefore simplifies existing task of writing input theories that can be grounded efficiently by current constructions. we first present our method for classical first - order algebra ( fo ) theories. then we extend it above fo ( id ), the continuation of fo with inductive inference, which allow for more concise and comprehensive programming theories. we discuss implementation issues help experimentally validate the practical applicability of our method.", "histories": [["v1", "Thu, 16 Jan 2014 04:53:20 GMT  (430kb)", "http://arxiv.org/abs/1401.3840v1", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI", "authors": ["johan wittocx", "maarten mari\\\"en", "marc denecker"], "accepted": false, "id": "1401.3840"}, "pdf": {"name": "1401.3840.pdf", "metadata": {"source": "CRF", "title": "Grounding FO and FO(ID) with Bounds", "authors": ["Johan Wittocx", "Maarten Mari\u00ebn", "Marc Denecker"], "emails": ["johan.wittocx@cs.kuleuven.be", "maarten.marien@cs.kuleuven.be", "marc.denecker@cs.kuleuven.be"], "sections": [{"heading": null, "text": "propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning.\nBesides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounder\u2019s output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems.\nWe first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method."}, {"heading": "1. Introduction", "text": "Grounding, or propositionalization, is the task of reducing a first-order theory and finite domain to an equivalent propositional theory, called a grounding. Grounding is used as a preprocessing phase in many logic-based reasoning systems. It serves to provide the user with a rich input language, while enabling the system to rely on efficient propositional solvers to perform the actual reasoning.\nExamples of systems that rely on grounding can be found in the area of finite first-order model generation (Claessen & So\u0308rensson, 2003; McCune, 2003; East, Iakhiaev, Mikitiuk, & Truszczyn\u0301ski, 2006; Mitchell, Ternovska, Hach, & Mohebali, 2006; Torlak & Jackson, 2007; Wittocx, Marie\u0308n, & Denecker, 2008d). Such systems are in turn used as part of theorem provers (Claessen & So\u0308rensson, 2003) and for lightweight software verification (Jackson, 2006). Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrja\u0308nen, 2000; Syrja\u0308nen, 2009). Also in planning systems (Kautz & Selman, 1996) and relational data mining (Krogel, Rawles, Zelezny\u0301, Flach, Lavrac, & Wrobel, 2003) grounding is frequently used. This large number of applications indicates the importance of grounding in logic-based reasoning systems and the need to develop efficient grounders.\nA basic (naive) grounding method is by instantiating the variables in the input theory by all possible combinations of domain elements. Grounding in this way is polynomial in the size of the domain but exponential in the maximum width of a formula in the input theory, and may easily produce groundings of unwieldy size. Several techniques have been developed to efficiently produce smaller groundings. There are two main categories of such techniques. In the first, the input theory is rewritten such that the maximum width of the formulas decreases. Methods like clause splitting (Schulz, 2002) and partitioning (Ramachandran & Amir, 2005) belong to this category.\nc\u00a92010 AI Access Foundation. All rights reserved.\nThe second type of techniques is applicable when besides the finite domain, additional data is available. This is often the case in practical model generation problems, such as the ones that are typical in ASP. In a graph problem the data could be an encoding of the input graph; in the context of planning, it could be a description of the initial and goal state, etc. Sometimes the data is explicitly available, e.g., in the form of a database, sometimes it is implicit, e.g., as a set of ground facts in the input theory. The second type of techniques aims at efficiently computing small groundings by taking the data into account.\nObserve that both types of techniques can be combined in a grounder. In this paper we mainly focus on a technique of the second category. To explain the intuition underlying our method, consider the following model generation problem.\nExample 1. Let T1 the first-order logic theory over the vocabulary {Edge, Sub}, consisting of the two sentences\n\u2200u\u2200v (Sub(u, v) \u2283 Edge(u, v)) (1) \u2200x\u2200y\u2200z (Sub(x, y) \u2227 Sub(x, z) \u2283 y = z), (2)\nT1 expresses that Sub is a subgraph of Edge with at most one outgoing edge in each vertex. Computing such a subgraph of a given graph G = \u3008V,E\u3009 can be cast as a model generation problem with input theory T1 and data G. The data can be represented as a structure I\u03c3 for the subvocabulary \u03c31 = {Edge} with domain V and EdgeI\u03c3 = E. A solution can be obtained by generating a model of T1 that expands I\u03c3 with an interpretation of Sub.\nApplying the naive grounding algorithm produces |V |2 instantiations of (1) and |V |3 instantiations of (2). By taking the data into account, atoms over \u2018Edge\u2019 and \u2018=\u2019 can be substituted by their truth value in I\u03c3. Simplifying the resulting grounding then eliminates |E| instantiations of (1) and |V | instantiations of (2). Smart grounding algorithms interleave this substitution and simplification with the grounding process in order to avoid creating unnecessary parts of the grounding.\nObserve that substituting atoms over \u03c31 and then simplifying still produces a grounding of size O(|V |3). Indeed, the simplified grounding of (2) is the set of binary clauses \u00acSub(i, j) \u2228 \u00acSub(i, k) such that i, j, k \u2208 V and i 6= j. This set has size |V |3 \u2212 |V |.\nSome grounders apply reasoning on the ground theory to reduce it even further. In the example, the simplified grounding of (1) consists of the clauses \u00acSub(i, j) such that (i, j) 6\u2208 E. Since these are unit clauses, each of them is certainly true in every model of the ground theory. It follows that each binary clauses \u00acSub(i, j) \u2228 \u00acSub(i, k) such that either \u00acSub(i, j) or \u00acSub(i, k) belongs to the simplified grounding of (1) is certainly true in every model of the ground theory and thus can be omitted from the simplified grounding of (2). The result is a grounding of size |E ./1=1 E|, where ./1=1 denotes the natural join matching the first columns. For a sparse graph, |E ./1=1 E| is much smaller than |V |3. However, since reasoning on the ground theory does not avoid creating all instantiations of a formula, it does not significantly speed up the grounding process.\nOne way to avoid a large grounding without relying on reasoning on the ground theory is by adding redundant information to formulas. This method is frequently used in ASP. For example,\n\u2200x\u2200y\u2200z(Edge(x, y) \u2227 Sub(x, y) \u2227 Edge(x, z) \u2227 Sub(x, z) \u2283 y = z) (3)\nis equivalent to (2) given (1), but its grounding (without reasoning on the ground theory) is equal to the one obtained by the kind of reasoning on the ground theory illustrated above. This illustrates how adding redundant information may sometimes dramatically reduce the size of the grounding. Since current grounders are optimized to ground formulas like (3) without trying all instances, grounding may also speed up a lot.\nHowever, manually adding redundancy to formulas has its disadvantages: it leads to more complex and hence, less readable theories. Worse, it might introduce errors. It requires a good understanding of the used grounder, since it depends on the grounder what information is beneficial to add and where. Also, a human developer could easily miss useful information.\nThe above motivates a study of automated methods for deriving such redundant information and of principled ways of adding it to formulas. We develop an algorithm that, given a model generation problem with input theory T and input data I\u03c3, derives such redundant information, in the form of a pair of a symbolic upper and lower bound for each subformula of T . Each of these bounds is a formula over the vocabulary of I\u03c3. For instance, for Example 1, our algorithm will compute Edge(x, y) as upper bound for Sub(x, y), meaning that if Edge(x, y) is not true, then Sub(x, y) is not true either. We also show how to insert these bounds in the formulas of T . For example, inserting the upperbound Edge(x, y) for Sub(x, y) and the upperbound Edge(x, z) for Sub(x, z) transforms (2) into (3).\nThe rest of this paper is organized as follows. In the next section we recall some notions from first-order logic (FO) and we introduce the notations used throughout the paper. In Section 3 we formally define grounding and model generation with additional data. In Section 4 we introduce upper- and lowerbounds for formulas. We present an any-time algorithm to compute them in the context of FO input theories. We show how the bounds can be used to rewrite the input theory to an equivalent theory that has a smaller grounding.\nAlthough many search problems can be cast concisely and naturally as FO model generation problems, some problems require richer logics than FO. One such logic is FO(ID), an extension of FO with inductive definitions. Such definitions can be used to represent, e.g., the concept of reachability in a graph. In Section 5 we extend our rewriting method to FO(ID).\nIn Section 6 we discuss how to implement our algorithm to compute bounds. As a case study, we show for one particular grounding algorithm how it can be adapted to exploit bounds directly. We also present experimental results that indicate the impact of our method on grounding size and time. We end with related work and conclusions.\nThe current paper extends our previous work (Wittocx, Marie\u0308n, & Denecker, 2008c). Besides proofs for all main propositions and a more thorough experimental validation, also the following parts were added:\n\u2022 The theoretical result stating that our rewriting method certainly yields smaller groundings (Proposition 23);\n\u2022 The extension of the rewriting method to FO(ID) (Section 5);\n\u2022 The section about implementation issues (Section 6)."}, {"heading": "2. Preliminaries", "text": "In this section, we introduce the conventions and notations used in this paper. We assume the reader is familiar with FO."}, {"heading": "2.1 First-Order Logic", "text": "A vocabulary \u03a3 is a tuple \u3008\u03a3P ,\u03a3F ,\u03a3V \u3009 where \u03a3P , \u03a3F and \u03a3V are respectively sets of predicate symbols, function symbols and variables. We identify constants with zero-arity function symbols. Abusing notation, we will often leave out \u03a3V and simply write \u3008\u03a3P ,\u03a3F \u3009 to represent \u03a3. A vocabulary \u03c3 is a subvocabulary of \u03a3, denoted \u03c3 \u2286 \u03a3, if \u03c3P \u2286 \u03a3P , \u03c3F \u2286 \u03a3F and \u03c3V \u2286 \u03a3V .\nThroughout this paper variables are denoted by lowercase letters, predicate and function symbols by uppercase letters. Each predicate and function symbol has an associated arity n \u2208 N. We often denote a predicate symbol P by P/n and a function symbol F by F/n to indicate their arities.\nTuples and sets of variables are denoted by x, y, z. A term over \u03a3 is inductively defined by\n\u2022 A variable x \u2208 \u03a3 is a term;\n\u2022 If F/n is a function symbol of \u03a3 and t1, . . . , tn are terms over \u03a3, then F (t1, . . . , tn) is a term.\nTuples of terms are denoted by t, t1, t2, . . . . A first-order logic formula over \u03a3 is inductively defined by\n\u2022 If P/n is a predicate symbol and t1, . . . , tn are terms, then P (t1, . . . , tn) is a formula.\n\u2022 If t1 and t2 are two terms, then t1 = t2 is a formula.\n\u2022 If \u03d5 and \u03c8 are formulas and x is a variable, then \u00ac\u03d5, \u03d5\u2227\u03c8, \u03d5\u2228\u03c8, \u2203x \u03d5 and \u2200x \u03d5 are formulas.\nWe use \u03d5 \u2283 \u03c8, \u03d5 \u2261 \u03c8 and t1 6= t2 as a shorthands for respectively \u00ac\u03d5 \u2228 \u03c8, (\u03d5 \u2283 \u03c8) \u2227 (\u03c8 \u2283 \u03d5) and \u00ac(t1 = t2). An atom is a formula of the form P (t) or t1 = t2. A literal is an atom or the negation of an atom.\nAn occurrence of a formula \u03d5 as subformula in a formula \u03c8 is positive, respectively negative, if it occurs in the scope of an even, respectively odd, number of negations.\nFor a formula \u03d5, we often write \u03d5[x] to indicate that x are its free variables. That is, if y \u2208 x, then y occurs in \u03d5, but not in the scope of a quantifier \u2200y or \u2203y in \u03d5. For a variable x and a term t, the formula \u03d5[x/t] denotes the result of replacing all free occurrences of x in \u03d5 by t. This notation is extended to tuples of variables and terms of the same length. A sentence is a formula without free variables. A theory is a finite set of sentences.\nA \u03a3-interpretation I consists of a domain D and\n\u2022 a domain element xI \u2208 D for each variable x \u2208 \u03a3V ;\n\u2022 a relation P I \u2286 Dn for each predicate symbol P/n \u2208 \u03a3P ;\n\u2022 a function F I : Dn \u2192 D for each function symbol F/n \u2208 \u03a3F .\nA \u03a3-structure is an interpretation of only the relation and function symbols of \u03a3. The restriction of a \u03a3-interpretation I to a vocabulary \u03c3 \u2286 \u03a3 is denoted by I|\u03c3. Vice versa, I is called an expansion of I|\u03c3 to \u03a3. For a variable x and domain element d, I[x/d] is the interpretation that assigns d to x and corresponds to I on all other symbols. This notation is extended to tuples of variables and domain elements of the same length. An interpretation I is called finite if its domain is finite.\nThe value tI of a term t in an interpretation I, and the satisfaction relation |= are defined as usual (e.g., Enderton, 2001). I is called a model of a formula \u03d5 if I |= \u03d5. We denote by T1 |= T2 that every model of theory T1 is also a model of theory T2.\nA query is an expression of the form {x | \u03d5}, where the free variables of \u03d5 are among x. A tuple d of domain elements is an answer to {x | \u03d5} in a structure I if I[x/d] |= \u03d5. The set of all answers to {x | \u03d5} in I is denoted by {x | \u03d5}I ."}, {"heading": "2.2 Rewriting and Term Normal Form", "text": "In this paper we will use the following well-known equivalences to rewrite formulas to logically equivalent formulas.\n1. Moving quantifiers\n\u2200x\u2200y \u03d5 \u2261 \u2200y\u2200x \u03d5 (4) \u2203x\u2203y \u03d5 \u2261 \u2203y\u2203x \u03d5 (5) \u2200x (\u03d5 \u2227 \u03c8) \u2261 (\u2200x \u03d5) \u2227 (\u2200x \u03c8) (6) \u2203x (\u03d5 \u2228 \u03c8) \u2261 (\u2203x \u03d5) \u2228 (\u2203x \u03c8) (7) \u2200x (\u03d5 \u2228 \u03c8) \u2261 \u03d5 \u2228 (\u2200x \u03c8) if x does not occur free in \u03d5 (8) \u2203x (\u03d5 \u2227 \u03c8) \u2261 \u03d5 \u2227 (\u2203x \u03c8) if x does not occur free in \u03d5 (9)\n2. Moving negations\n\u00ac(\u03d5 \u2227 \u03c8) \u2261 (\u00ac\u03d5) \u2228 (\u00ac\u03c8) (10) \u00ac(\u03d5 \u2228 \u03c8) \u2261 (\u00ac\u03d5) \u2227 (\u00ac\u03c8) (11) \u00ac(\u2200x \u03d5) \u2261 \u2203x (\u00ac\u03d5) (12) \u00ac(\u2203x \u03d5) \u2261 \u2200x (\u00ac\u03d5) (13)\n3. Flattening terms\nP (t1, . . . , ti, . . . , tn) \u2261 \u2203x (x = ti \u2227 P (t1, . . . , ti\u22121, x, ti+1, . . . , tn)) (14)\nwhere x does not occur in P (t1, . . . , tn).\nTo facilitate the presentation, we will sometimes require that formulas are in term normal form (TNF). We say that a formula \u03d5 is in TNF, if every atomic subformula of \u03d5 is of the form P (x), F (x) = y or x = y, and all negations occur directly in front of atoms. Using (10)\u2013(14), every formula can be transformed in an equivalent formula in TNF. We say that a theory is in TNF if all its sentences are."}, {"heading": "2.3 SAT", "text": "A vocabulary \u03a3 is propositional if \u03a3F = \u2205 and every predicate symbol in \u03a3P has arity zero. A propositional theory (PC theory) is a theory over a propositional vocabulary. A propositional clause is a disjunction of propositional literals. A PC theory is in conjunctive normal form (CNF) if all its sentences are clauses. The Boolean satisfiability problem (SAT) is the NP-complete problem of deciding for a PC theory whether it is satisfiable. The NP search problem corresponding to a SAT problem is the problem of computing a witness of the decision problem in the form of a model of the theory. SAT solvers typically operate by constructing such a model.\nContemporary SAT solvers exhibit impressive performance. As such, many NP problems can be solved efficiently by translating them to SAT. For instance, this is done in the areas of model generation (Claessen & So\u0308rensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al., 2003). Most modern SAT solvers expect a CNF theory as input, instead of a general PC theory. When the input is a satisfiable theory, they return a model as a witness to their answer."}, {"heading": "3. Model Generation and Grounding", "text": "Model generation is the problem of computing a model of a logic theory T , usually in the context of a given finite domain, typically the Herbrand Universe. A model generator allows to decide the satisfiability of the theory in the context of this fixed domain. This is useful, e.g., in the context of lightweight verification (Jackson, 2006). Beyond determining satisfiability, there is a broad class of problems of which the answers are naturally given by the models of a declarative domain theory. For example, the model of a theory specifying a scheduling domain typically contains a (correct) schedule. Thus, a model generator applied to this theory will solve the scheduling problem for this domain.1 This idea of model generation as a declarative problem solving paradigm has been pioneered in the area of ASP (Marek & Truszczyn\u0301ski, 1999; Niemela\u0308, 1999). In this area, answers to a problem are given by the models of an ASP theory.\nAs mentioned in the introduction, many practical model generation problems contain additional data besides the input theory and finite domain. This data can be implicit in the input theory. For\n1. For a set of problems of this kind, see, e.g., the benchmarks of the ASP-competition (http://dtai.cs.kuleuven. be/events/ASP-competition).\nexample, ASP problems can be split into two parts: a non-ground theory and a list of ground facts. The latter part essentially represents given data. In other contexts (Mitchell & Ternovska, 2005; Torlak & Jackson, 2007; Wittocx et al., 2008d), the data is given as a (partial) structure interpreting part of the vocabulary of the input theory. In this paper we assume without loss of generality that the data is represented by a structure. In practice, it is often the case that some preprocessing, e.g., materializing a view on a database, needs to be done before the data is in this format (see also Section 5.3.2)."}, {"heading": "3.1 The Model Expansion Search Problem", "text": "Model generation with an input theory and input structure is called model expansion. Model expansion for a logic L, denoted MX(L), is defined as follows.\nDefinition 1. Let T be an L-theory over a vocabulary \u03a3, \u03c3 a subvocabulary of \u03a3 and I\u03c3 a finite \u03c3-structure. The model expansion search problem with input \u3008T, I\u03c3\u3009 is the problem of computing a \u03a3-structure M such that M |= T and M |\u03c3 = I\u03c3.\nThe vocabulary \u03c3 is called the input vocabulary of the problem, the vocabulary \u03a3\\\u03c3 the expansion vocabulary. I\u03c3 is called the input structure. We denote by M |=I\u03c3 T that M is a solution to the model expansion search problem with input \u3008T, I\u03c3\u3009. Similarly, for a formula \u03d5 over \u03a3 we denote by M |=I\u03c3 \u03d5 that M expands I\u03c3 to \u03a3 and satisfies \u03d5.\nObserve that if \u03c3 = \u03a3, model expansion reduces to model checking, while if \u03c3 = \u3008\u2205, \u2205\u3009, it reduces to model generation for T with a given finite size. Also, if T is a theory over a vocabulary \u03a3 containing no function symbols of arity greater than zero, Herbrand model generation for T can be simulated by model expansion. Indeed, let \u03c3 = \u3008\u2205,\u03a3F \u3009, and I\u03c3 the structure with the Herbrand universe of T such that CI\u03c3 = C for every constant C \u2208 \u03a3F .\nWe illustrate model expansion by two examples. In the examples in this paper, we often use many-sorted FO, since this leads to more concise and readable sentences. In many-sorted FO, the domain of an interpretation is partitioned in sorts (or types), each variable has an associated sort, each n-ary predicate symbol has an n-tuple of associated sorts and each n-ary function symbol an associated (n + 1)-tuple of sorts. If I is an interpretation and variable x has associated sort s, then xI \u2208 sI , where sI denotes the set of domain elements of sort s. Similarly, if P/n has associated sorts (s1, . . . , sn), then P I \u2286 sI1\u00d7\u00b7 \u00b7 \u00b7\u00d7sIn, if F/n has associated sorts (s1, . . . , sn+1), then F I : sI1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 sIn \u2192 sIn+1. We often denote P by P (s1, . . . , sn) and F by F (s1, . . . , sn) : sn+1 to indicate their associated sorts.\nExample 2 (Graph Colouring). The graph colouring problem is the problem of colouring a given graph with a given set of colours such that adjacent vertices have different colours. To express this problem in MX(FO), let V tx and Col be sorts and let \u03c3 = \u3008{Edge(V tx, V tx)}, \u2205\u3009. The sort Col denotes the given set of colours, the given graph is represented by V tx and Edge. Let \u03a3 be the vocabulary \u3008\u03c3P , {Colour(V tx) : Col}\u3009 and T the theory that consists of the sentence\n\u2200v1\u2200v2 (Edge(v1, v2) \u2283 Colour(v1) 6= Colour(v2)).\nThen model expansion with input theory T and input vocabulary \u03c3 expresses the graph colouring problem. Indeed, for any M |=I\u03c3 T , ColourM is a proper colouring of the graph represented by I\u03c3.\nExample 3 (SAT). To represent the SAT problem in MX(FO), let \u03c3 be a vocabulary containing the two sorts Atom and Clause, representing the atoms and the clause of the input CNF theory, and the two predicates PosIn(Atom,Clause) and NegIn(Atom,Clause), to represent the positive, respectively negative, occurrences of atoms in clauses. The theory given by\n\u2200c \u2203a ((PosIn(a, c) \u2227 True(a)) \u2228 (NegIn(a, c) \u2227 \u00acTrue(a)))\nover \u03a3 = \u3008\u03c3P \u222a {True(Atom)}, \u2205\u3009 expresses the SAT problem: for any M |=I\u03c3 T , the propositional structure represented by TrueM is a model of the CNF theory represented by I\u03c3. Indeed, the theory forces that every clause contains at least one true literal.\nAs shown by Mitchell and Ternovska (2005), it follows from Fagin\u2019s (1974) theorem that model expansion for FO captures NP, in the following sense:\n\u2022 For any fixed T and \u03c3 the problem of deciding whether there exists a model of T expanding an input structure I\u03c3 is in NP.\n\u2022 Vice versa, for any NP decision problem X on the class of finite \u03c3-structures there is a vocabulary \u03a3 \u2287 \u03c3 and a first-order \u03a3-theory T such that model expansion with input theory T expresses X, i.e., I\u03c3 belongs to X iff there exists a \u03a3-structure M such that M |=I\u03c3 T .\nThis result proves that any NP problem X can be expressed by an MX(FO) problem, and hence shows the broad applicability of MX(FO) solvers to solve NP problems.\nAs illustrated by the examples above, it is the intention that the theory T is an intuitive representation of a problem X. Not all NP problems can be represented in a natural manner in MX(FO). For instance, the problem of deciding whether a graph is connected can be expressed in MX(FO), but this requires a non-trivial encoding of a fixpoint operator in FO. Model expansion for richer logics than FO is better suited for such problems. In Section 5 we consider MX for FO(ID), an extension of FO with inductive definitions."}, {"heading": "3.2 Reducing MX(FO) to SAT", "text": "For the rest of this paper, let T be a theory over a vocabulary \u03a3, \u03c3 a subvocabulary of \u03a3 and I\u03c3 a finite \u03c3-structure with domain D.\nSince for every FO theory T , deciding whether T has a model expanding I\u03c3 is in NP, this problem can be reduced to a SAT problem Tprop in polynomial time. However, if we want to find models of T expanding I\u03c3 by using a SAT solver, we need a method to translate models of Tprop into models of T . Moreover, if we are interested in finding all models of T expanding I\u03c3, a oneto-one correspondence between these models and the models of Tprop is needed. In this paper we focus on reductions that preserve all models, which is the setting in the ASP paradigm (Marek & Truszczyn\u0301ski, 1999; Niemela\u0308, 1999).\nLet \u03c4 be the vocabulary of Tprop. To have a one-to-one correspondence between the models of T expanding I\u03c3 and the models of Tprop, it should be possible to represent \u03a3-structures expanding I\u03c3 by \u03c4 -structures. The most natural way to accomplish this is by choosing \u03c4 such that it contains a symbol Pd for every P/n \u2208 \u03a3P and d \u2208 Dn, and a symbol Fd,d\u2032 for every F/n \u2208 \u03a3F and (d, d\u2032) \u2208 Dn+1. A \u03c4 -structure making Pd, respectively Fd,d\u2032 true then corresponds to a \u03a3 structure M such that d \u2208 PM , respectively FM (d) = d\u2032. In this manner, every \u03a3-structure expanding I\u03c3 has a corresponding \u03c4 -structure. Vice versa, every \u03c4 -structure A satisfying the requirement that for every function symbol F/n and d \u2208 Dn, there is exactly one d\u2032 \u2208 D such that Fd,d\u2032 is true in A, corresponds to a \u03a3-structure with the same domains as I\u03c3. That is, there is a one-to-one correspondence between the \u03c4 -structures satisfying for every function symbol F/n and d \u2208 Dn the formula ( \u2228\nd\u2032\u2208D\nFd,d\u2032\n) \u2227  \u2227 d\u20321\u2208D  \u2227 d\u20322\u2208D\\d\u20321 \u00acFd,d\u20321 \u2228 \u00acFd,d\u20322  (15) and the \u03a3-structures with domain D.\nDenote by \u03a3dom(I\u03c3) the vocabulary \u03a3 extended with a new constant symbol d for every d \u2208 D. We call these new constants domain constants. Abusing notation, we will denote both domain elements and their corresponding domain constants by d. For a formula \u03d5[x] and a tuple d of\ndomain constants, we call \u03d5[x/d] an instance of \u03d5. For a \u03a3-interpretation M expanding I\u03c3 and a formula \u03d5 containing domain constants, we denote by M |= \u03d5 that the expansion of M to \u03a3dom(I\u03c3) defined by interpreting every domain constant by its corresponding domain element, satisfies \u03d5.\nDefinition 2. Two formulas \u03d51 and \u03d52 over \u03a3dom(I\u03c3) are I\u03c3-equivalent if M |=I\u03c3 \u03d51 iff M |=I\u03c3 \u03d52, for every \u03a3-interpretation M .\nThe following are some straightforward results about I\u03c3-equivalence.\nLemma 3. 1. Two logically equivalent formulas are I\u03c3-equivalent. 2. \u2227 d\u2208D \u03d5[x/d] is I\u03c3-equivalent to \u2200x \u03d5[x].\n3. \u2228 d\u2208D \u03d5[x/d] is I\u03c3-equivalent to \u2203x \u03d5[x].\n4. If \u03d5\u2032 and \u03c8\u2032 are I\u03c3-equivalent to respectively \u03d5 and \u03c8, then \u00ac\u03d5\u2032, \u03d5\u2032 \u2227 \u03c8\u2032, \u03d5\u2032 \u2228 \u03c8\u2032, \u2203x \u03d5\u2032 and \u2200x \u03d5\u2032 are I\u03c3-equivalent to respectively \u00ac\u03d5, \u03d5 \u2227 \u03c8, \u03d5 \u2228 \u03c8, \u2203x \u03d5 and \u2200x \u03d5.\n5. If \u03c8 is a subformula of \u03d5 and is I\u03c3-equivalent to \u03c8\u2032, then the result of replacing \u03c8 by \u03c8\u2032 in \u03d5 is I\u03c3-equivalent to \u03d5.\nA formula is in ground normal form (GNF) if it contains no quantifiers and all its atomic subformulas are of the form P (d1, . . . , dn), F (d1, . . . , dn) = d or d1 = d2, where d1, . . . , dn, d are domain constants. A theory is in GNF if all its sentences are in GNF. A GNF theory is essentially propositional: by replacing in a GNF theory T every atom P (d) by Pd, F (d) = d\n\u2032 by Fd,d\u2032 , di = dj by > or \u22a5 if, respectively, i = j or i 6= j, and adding the formula (15) for every function symbol F/n and d \u2208 Dn, we obtain a propositional theory Tprop such that the models of T and Tprop correspond. Also note the similarity between GNF and TNF theories.\nDefinition 4. A grounding for T with respect to I\u03c3 is a GNF theory Tg over \u03a3dom(I\u03c3) such that T and Tg are I\u03c3-equivalent. Tg is called reduced if it does not contain symbols of \u03c3."}, {"heading": "3.2.1 Grounding Algorithms", "text": "For the rest of this section, we assume that T is a theory in TNF. As explained in Section 2.2, we can make this assumption without loss of generality. Below we introduce, as a reference, the grounding for T with respect to I\u03c3 obtained by the naive grounding algorithm mentioned in the introduction. We call this grounding the full grounding and define it formally by induction.\nDefinition 5. The full grounding Grfull(\u03d5, I\u03c3) of a TNF sentence \u03d5 with respect to I\u03c3 is defined by\nGrfull(\u03d5) =  \u03d5 if \u03d5 is a literal Grfull(\u03c81) \u2227Grfull(\u03c82) if \u03d5 is equal to \u03c81 \u2227 \u03c82 Grfull(\u03c81) \u2228Grfull(\u03c82) if \u03d5 is equal to \u03c81 \u2228 \u03c82\u2227 d\u2208D Grfull(\u03c8[x/d]) if \u03d5 is equal to \u2200x \u03c8[x]\u2228 d\u2208D Grfull(\u03c8[x/d]) if \u03d5 is equal to \u2203x \u03c8[x]\n(16)\nThe full grounding for T with respect to I\u03c3 is the theory consisting of the full groundings of all sentences in T with respect to I\u03c3.\nWe denote the full grounding by Grfull(T, I\u03c3), or by Grfull(T ) if I\u03c3 is clear from the context. It follows directly from Lemma 3 that Grfull(T, I\u03c3) is indeed a grounding for T with respect to I\u03c3. The size of the full grounding is exponential in the maximal nesting depth of quantifiers in sentences of T , and polynomial in the domain size of I\u03c3.\nAn inductive definition like (16) can be evaluated in a top-down or bottom-up way. Both approaches are applied in current grounders. On the one hand, there are grounders that go top-down through the syntax trees of the sentences in T . When a subformula \u03d5 of the form \u2200x \u03c8[x], respectively \u2203x \u03c8[x] is reached, the grounding of \u03c8[x/d] is constructed for every domain constant d, and then \u03d5 is replaced by the conjunction, respectively disjunction, of all these groundings. The grounder of the dlv system (Perri et al., 2007) and the grounders gringo (Gebser et al., 2007) and GidL (Wittocx, Marie\u0308n, & Denecker, 2008b) take this approach.\nOther grounders go bottom-up through the syntax trees. For each subformula \u03d5[x] a table is computed consisting of tuples d and corresponding groundings of \u03d5[x/d]. These tables are computed first for atomic formulas and subsequently for compound formulas. For example, let \u03d5[x, y, z] be the formula \u03c8[x, y] \u2227 \u03c7[y, z] and assume the tables for \u03c8 and \u03c7 have been computed. Then the table for \u03d5 is computed by taking the natural join of the tables for \u03c8 and \u03c7 on the value for y, and constructing the grounding for \u03d5[x/dx, y/dy, z/dz] as the (possibly simplified) conjunction of the groundings for \u03c8[x/dx, y/dy] and \u03c7[y/dy, z/dz]. Examples of grounders with a bottom-up approach are lparse (Syrja\u0308nen, 2000; Syrja\u0308nen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al., 2006).\nTo obtain a reduced grounding for T with respect to I\u03c3 one could first construct the full grounding and then replace every subformula \u03d5 over \u03c3dom(I\u03c3) in it by > if I\u03c3 |= \u03d5 and by \u22a5 otherwise. The result can further be simplified by recursively replacing \u22a5\u2227 \u03c8 by \u22a5, >\u2227 \u03c8 by \u03c8, etc. The resulting grounding is the one computed by most current grounding algorithms and is often a lot smaller than the full grounding. We denote it by Grred(T, I\u03c3), or by Grred(T ) if I\u03c3 is clear from the context.\nSmart grounding algorithms do not use the approach outlined above, but try to avoid creating the full grounding by substituting ground formulas over the input vocabulary \u03c3 as soon as possible. For example, a grounder with a top-down approach constructs the grounding of \u2200x \u03c8[x], by grounding all instances \u03c8[x/d] one by one and then making the conjunction. During this process, all instances \u03c8[x/d] that are detected to be certainly true are omitted. As soon as an instance \u03c8[x/d] is detected to be certainly false, \u22a5 is returned as grounding for \u2200x \u03c8[x].\nA grounder using the bottom-up approach can reduce the size of the tables it computes by not storing tuples that have some default value, e.g., >, as corresponding grounding. In particular, if \u03d5[x] is a formula over \u03c3, it only stores the tuples d such that I\u03c3 6|= \u03d5[x/d]. By reducing the size of the tables in this way, the reduced grounding can be obtained much more efficiently."}, {"heading": "4. Grounding with Bounds", "text": "In this section we present our method for reducing grounding size. As mentioned in the introduction, it is based on computing bounds for subformulas of the input theory T . Each bound for a subformula \u03d5[x] is a formula over the input vocabulary \u03c3. It describes a set of tuples d for which \u03d5[x/d] is certainly true (false) in every model of T expanding any I\u03c3. The larger the set described by a bound, the more precise the bound is. Observe that the fact that bounds are formulas over \u03c3 means that they can be evaluated using the given structure I\u03c3.\nIn Section 4.1, we formally define bounds. Then we indicate how bounds can be inserted in T to obtain a new theory T \u2032. The reduced grounding of T \u2032 is often a lot smaller than the reduced grounding of T . The more precise the inserted bounds are, the smaller the grounding of T \u2032 becomes. However, we will see that T \u2032 is in general weaker than T and that additional axioms have to be added to T \u2032 to obtain equivalence with T . These additional axioms need to be grounded as well so that, if we are not careful, the total size of the grounded theory does not decrease at all. In Section 4.3, we search for sufficient conditions on the bounds to guarantee a smaller grounding.\nIn Section 4.4, we show how to derive bounds. Our method works in two stages. First, bounds for all subformulas of T are computed using an any-time algorithm. The longer the algorithm runs, the more precise bounds are derived. Often, the bounds derived at this stage do not lead to smaller groundings, for the reason explained in the previous paragraph. In the second stage, bounds that\nsatisfy the conditions to guarantee smaller groundings are derived from the ones computed in the first stage."}, {"heading": "4.1 Bounds", "text": "We distinguish between two kinds of bounds.\nDefinition 6. A certainly true bound (ct-bound) over \u03c3 with respect to T for a formula \u03d5[x] is a formula \u03d5ct[y] over \u03c3 such that y \u2286 x and T |= \u2200x (\u03d5ct[y] \u2283 \u03d5[x]). Vice versa, a certainly false bound (cf-bound) over \u03c3 with respect to T for \u03d5[x] is a formula \u03d5cf [z] over \u03c3 such that z \u2286 x and T |= \u2200x (\u03d5cf [z] \u2283 \u00ac\u03d5[x]).\nWe do not mention \u03c3 and T if they are clear from the context. Intuitively, a ct-bound \u03d5ct for \u03d5[x] provides for every structure I\u03c3 a lower bound for the set of tuples for which \u03d5 is true in every model of T expanding I\u03c3. Indeed, for every M |=I\u03c3 T we have that {x | \u03d5ct}I\u03c3 \u2286 {x | \u03d5}M . Vice versa, a cf-bound \u03d5cf provides a lower bound on the set of tuples for which \u03d5 is false: {x | \u03d5cf}I\u03c3 \u2286 {x | \u00ac\u03d5}M for every M |=I\u03c3 T . Observe that the negation of a ct-bound, respectively cf-bound, gives an upper bound on the set of tuples for which \u03d5 is false, respectively true, in at least one model of T expanding I\u03c3.\nExample 4 (Example 1 ctd.). Let \u03d51 be the subformula Sub(x, y) \u2227 Sub(x, z) of T1. Then \u00acEdge(x, y) \u2228 \u00acEdge(x, z) is a cf-bound over \u03c31 with respect to T1 for \u03d51. Indeed, one can derive from (1) that T1 entails\n\u2200x\u2200y\u2200z ((\u00acEdge(x, y) \u2228 \u00acEdge(x, z)) \u2283 \u00ac\u03d51) .\nObserve that > is a ct-bound for every sentence of T . Indeed, for every sentence \u03d5 of T , T |= \u03d5 and therefore T |= > \u2283 \u03d5. Also, \u22a5 is a ct-bound as well as a cf-bound for every formula. We call \u22a5 the trivial bound. Intuitively, the trivial bound contains no information at all: {x | \u22a5}I\u03c3 = \u2205 for every I\u03c3 and x. According to the following definition, it is the least precise bound.\nDefinition 7. Let \u03c8[y] and \u03c7[z] be two (ct- or cf-) bounds for \u03d5[x]. We say that \u03c8[y] is more precise than \u03c7[z] if \u2200x (\u03c7[z] \u2283 \u03c8[y]) is valid.\nIf \u03c8 is a more precise bound for \u03d5[x] than \u03c7, \u03c8 provides a larger lower bound because {x | \u03c7}I\u03c3 \u2286 {x | \u03c8}I\u03c3 for every I\u03c3.\nDefinition 8. A c-map C for T over \u03c3 is a mapping from all subformulas \u03d5 of T to tuples (Cct(\u03d5), Ccf(\u03d5)), where Cct(\u03d5) and Ccf(\u03d5) are respectively a ct- and cf-bound for \u03d5 over \u03c3 with respect to T .\nThe notion of precision pointwise extends to c-maps. That is, if C1 and C2 are two c-maps for T , then C1 is more precise than C2 iff for every subformula \u03d5 of T , Cct1 (\u03d5) is more precise than Cct2 (\u03d5) and Ccf1 (\u03d5) is more precise than Ccf2 (\u03d5).\nLet M be a model of T and C a c-map for T over \u03c3. From the definition of ct- and cf-bounds it follows immediately that for every subformula \u03d5[x] of T , both M |= \u2200x (Cct(\u03d5) \u2283 \u03d5) and M |= \u2200x (Ccf(\u03d5) \u2283 \u00ac\u03d5) hold. We say that a structure satisfies C if it has precisely this property.\nDefinition 9. Let C be a c-map for T over \u03c3. Then the theory C is defined by\nC ={\u2200x (Cct(\u03d5) \u2283 \u03d5) | \u03d5[x] is a subformula of T} \u222a {\u2200x (Ccf(\u03d5) \u2283 \u00ac\u03d5) | \u03d5[x] is a subformula of T}.\nA structure I satisfies C if I |= C.\nClearly, if C is a c-map for T over \u03c3 and M |=I\u03c3 T , then M |= C. We call two formulas \u03d5[x] and \u03c8[x] C-equivalent if {x | \u03d5}I = {x | \u03c8}I for each structure I that satisfies C. Equivalently, \u03d5 and \u03c8 are C-equivalent if C |= \u2200x (\u03d5 \u2261 \u03c8).\nA c-map is inconsistent if some formula \u03d5 is both certainly true and false for some tuple, according to that c-map:\nDefinition 10. A c-map C for T over \u03c3 is inconsistent if \u2203x (Cct(\u03d5) \u2227 Ccf(\u03d5)) is valid for some subformula \u03d5[x] of T . A c-map C is I\u03c3-inconsistent if I\u03c3 |= \u2203x (Cct(\u03d5)\u2227Ccf(\u03d5)) for some subformula of T .\nProposition 11. If there exists an I\u03c3-inconsistent c-map for T over \u03c3, then M 6|=I\u03c3 T for every M . If there exists an inconsistent c-map for T over \u03c3, then M 6|=I\u03c3 T for every M and I\u03c3.\nProof. Let C be an I\u03c3-inconsistent c-map for T over \u03c3 and \u03d5[x] a subformula of T such that I\u03c3 |= \u2203x (Cct(\u03d5) \u2227 Ccf(\u03d5)). Then there exists a tuple of domain elements d such that I\u03c3[x/d] |= Cct(\u03d5) and I\u03c3[x/d] |= Ccf(\u03d5). Assume towards a contradiction that M |=I\u03c3 T . Then M |= C, and hence M [x/d] |= Cct(\u03d5) \u2283 \u03d5 and M [x/d] |= Ccf(\u03d5) \u2283 \u00ac\u03d5. Since M |\u03c3 = I\u03c3, it follows that M [x/d] |= \u03d5 and M [x/d] |= \u00ac\u03d5. This is a contradiction.\nTo prove the second statement, let C be an inconsistent c-map for T over \u03c3. Then C is a also an I\u03c3-inconsistent c-map for every \u03c3-structure I\u03c3. As such, for any I\u03c3 there is no model of T expanding I\u03c3."}, {"heading": "4.2 C-Transformation", "text": "For the rest of this section, fix a c-map C for T over \u03c3. We now show how to insert the bounds of C into the sentences of T . This insertion is based on the following lemma.\nLemma 12. Let \u03d5[x] be a subformula of T . Then \u03d5 is C-equivalent to \u03d5\u2228Cct(\u03d5) and to \u03d5\u2227\u00acCcf(\u03d5).\nProof. We have to prove that C |= \u2200x (\u03d5 \u2261 (\u03d5 \u2228 Cct(\u03d5))) and C |= \u2200x (\u03d5 \u2261 (\u03d5 \u2227 \u00acCcf(\u03d5))). The former immediately follows from the fact that C |= \u2200x (Cct(\u03d5) \u2283 \u03d5), the latter from the fact that C |= \u2200x (Ccf(\u03d5) \u2283 \u00ac\u03d5) .\nAs a corollary of lemma 12 we have the following lemma.\nLemma 13. Let \u03c8 be a sentence of T and \u03d5 a subformula of \u03c8. If \u03c8\u2032 is the result of replacing the subformula \u03d5 in \u03c8 by \u03d5 \u2228 Cct(\u03d5), by \u03d5 \u2227 \u00acCcf(\u03d5) or by (\u03d5 \u2227 \u00acCcf(\u03d5)) \u2228 Cct(\u03d5), then M |= \u03c8 iff M |= \u03c8\u2032 for every M that satisfies C.\nObserve that if Cct(\u03d5) = Ccf(\u03d5) = \u22a5, then both \u03d5\u2228Cct(\u03d5) and \u03d5\u2227\u00acCcf(\u03d5) are logically equivalent to \u03d5. Hence, in this case the sentence \u03c8\u2032 in Lemma 13 is essentially the sentence \u03c8. Intuitively, adding trivial bounds to a sentence \u03c8 does not change the sentence at all.\nThe bounds assigned by C can be \u201cinserted\u201d in T by applying the transformation of Lemma 13 to all subformulas of T . The result is called a c-transformation of T , and is formally defined as follows.\nDefinition 14 (c-transformation). A c-transformation of a subformula \u03d5 of T with respect to C, denoted C\u3008\u03d5\u3009, is the formula (\u03d5\u2032 \u2227 \u00acCcf(\u03d5)) \u2228 Cct(\u03d5) where \u03d5\u2032 is defined by\n\u03d5\u2032 :=  \u03d5 if \u03d5 is an atom \u00acC\u3008\u03c8\u3009 if \u03d5 is equal to \u00ac\u03c8 C\u3008\u03c8\u3009 \u2227 C\u3008\u03c7\u3009 if \u03d5 is equal to \u03c8 \u2227 \u03c7 C\u3008\u03c8\u3009 \u2228 C\u3008\u03c7\u3009 if \u03d5 is equal to \u03c8 \u2228 \u03c7 \u2203x C\u3008\u03c8\u3009 if \u03d5 is equal to \u2203x \u03c8 \u2200x C\u3008\u03c8\u3009 if \u03d5 is equal to \u2200x \u03c8\nA c-transformation C\u3008T \u3009 of T with respect to C consists of a c-transformation with respect to C of every sentence of T .\nFrom Lemma 13, we derive the following.\nLemma 15. T and C\u3008T \u3009 are C-equivalent.\nIn general T and C\u3008T \u3009 are not logically equivalent. C\u3008T \u3009 may have models that do not satisfy C, and therefore cannot be models of T . For example, let C be the c-map that assigns (>,\u22a5) to every sentence and (\u22a5,\u22a5) to every other subformula of T . Then all sentences in C\u3008T \u3009 are of the form \u03d5\u2228> and hence C\u3008T \u3009 simplifies to >, which is in general not equivalent to T . To obtain from C\u3008T \u3009 a theory that is equivalent to T , we must add C.\nTheorem 16. If C is a c-map for T over \u03c3 and C the theory defined in Definition 9, then C\u3008T \u3009 \u222a C is equivalent to T .\nProof. Let M be a model of T . Then M |= C, and because of Lemma 13, M |= C\u3008T \u3009 \u222a C. On the other hand, if M |= C\u3008T \u3009 \u222a C, then by Lemma 13, M |= T .\nCorollary 17. If C is a c-map for T over \u03c3, then T and C\u3008T \u3009 \u222a C are I\u03c3-equivalent for any \u03c3structure I\u03c3."}, {"heading": "4.3 Atom-Based and Atom-Equal C-Maps", "text": "Corollary 17 implies that we can compute a grounding for T with respect to I\u03c3 by first computing a c-map C for T over \u03c3 and then grounding C\u3008T \u3009 \u222a C. This approach is beneficial if the reduced grounding of C\u3008T \u3009 \u222a C is smaller than the reduced grounding of T , and can be constructed at least as fast. In general these conditions are not satisfied. The more precise c-map C is, the smaller the reduced grounding of C\u3008T \u3009 becomes, but the larger the reduced grounding of C is:\nProposition 18. If C1 is more precise than C2, then Grred(C1\u3008T \u3009) is smaller than Grred(C2\u3008T \u3009). Moreover, every subformula that occurs in Grred(C1\u3008T \u3009) also occurs in Grred(C2\u3008T \u3009).\nProof. (Sketch) Let \u03d5[x] be a subformula of T and d a tuple of domain elements. It suffices to show that if C2\u3008\u03d5\u3009[x/d] is replaced by >, respectively \u22a5, when grounding, then this is also the case for C1\u3008\u03d5\u3009[x/d]. This can be proven by induction. For the base case, assume \u03d5 is an atom. Then C2\u3008\u03d5\u3009[x/d] is the formula ((\u03d5 \u2227 \u00acCcf2 (\u03d5)) \u2228 Cct2 (\u03d5))[x/d]. If this formula is replaced by > or \u22a5 when grounding, there are three possibilities: \u03d5 is a formula over \u03c3, I\u03c3[x/d] |= Cct2 (\u03d5) or I\u03c3[x/d] |= Ccf2 (\u03d5). Since C1 is more precise than C2, I\u03c3[x/d] |= Cct2 (\u03d5) implies I\u03c3[x/d] |= Cct1 (\u03d5) and I\u03c3[x/d] |= Ccf2 (\u03d5) implies I\u03c3[x/d] |= Ccf1 (\u03d5). We conclude that if C2\u3008\u03d5\u3009[x/d] is replaced by > or \u22a5 when grounding, then this is also the case for C1\u3008\u03d5\u3009[x/d]. The inductive case is similar.\nProposition 19. If C1 is more precise than C2, then Grred(C1) is larger than Grred(C2).\nProof. (Sketch) Every sentence in C1 is of the form \u2200x (Cct1 (\u03d5) \u2283 \u03d5) or \u2200x (Ccf1 (\u03d5) \u2283 \u00ac\u03d5). The number of instances of Cct1 (\u03d5) \u2283 \u03d5 in the reduced grounding of C1 is equal to the number of d such that I\u03c3[x/d] |= Cct1 (\u03d5). Similarly for Ccf1 (\u03d5) \u2283 \u00ac\u03d5. Since C2 is less precise than C1, the number of instances in Grred(C2) of the corresponding sentences \u2200x (Cct2 (\u03d5) \u2283 \u03d5) and \u2200x (Ccf2 (\u03d5) \u2283 \u00ac\u03d5) is smaller.\nA c-map that is useful to reduce grounding size should therefore not be too precise, in order to avoid a large theory Grred(C), but still be precise enough to decrease the size of Grred(C\u3008T \u3009). In this section, we present sufficient conditions to ensure these properties. We first define a class of c-maps that \u201cavoid\u201d a blow-up of Grred(C) by ensuring C can be replaced by an equivalent, smaller and easy-to-find theory CA. As such, Grred(C) can be replaced by the smaller theory Grred(CA). In the class we present, CA is a subset of C, namely the set of sentences in C that stem from the atomic subformulas of T :\nDefinition 20. Define the theory CA by\nCA ={\u2200x (Cct(\u03d5) \u2283 \u03d5) | \u03d5[x] is an atomic subformula of T} \u222a {\u2200x (Ccf(\u03d5) \u2283 \u00ac\u03d5) | \u03d5[x] is an atomic subformula of T}.\nWe call C atom-based if CA |= C.\nExample 5 (Example 1 ctd.). Let C2 be the c-map that assigns (\u22a5,\u00ac(Edge(x, y) \u2227 Edge(x, z))) to Sub(x, y) \u2227 Sub(x, z) and (\u22a5,\u22a5) to every other subformula. C2 is not atom-based, since (C2)A is equivalent to >, while C2 contains the sentence\n\u2200x\u2200y\u2200z (\u00ac(Edge(x, y) \u2227 Edge(x, z)) \u2283 \u00ac(Sub(x, y) \u2227 Sub(x, z))). (17)\nLet C3 be the c-map that assigns (\u22a5,\u00acEdge(x, y)) to Sub(x, y), (\u22a5,\u00acEdge(x, z)) to Sub(x, z) and corresponds to C2 on all other subformulas of T1. C3 is atom-based. Indeed, (C3)A consists of the (equivalent) sentences\n\u2200x\u2200y (\u00acEdge(x, y) \u2283 \u00acSub(x, y)) (18) \u2200x\u2200z (\u00acEdge(x, z) \u2283 \u00acSub(x, z)) (19)\nand C3 consists of the sentences (17), (18) and (19). Both (18) and (19) imply (17), and therefore, (C3)A |= C3.\nClearly, a c-map assigning (\u22a5,\u22a5) to every non-atomic subformula of T is an example of an atombased c-map. As such, any c-map can be transformed into an atom-based one by replacing every bound assigned to a non-atomic subformula by \u22a5. In the next section, we show how to compute more interesting atom-based c-maps.\nObserve that Grred(CA) contains only unit clauses. Combining the definition of atom-based c-map and Theorem 16 immediately gives the following result.\nProposition 21. Let C be an atom-based c-map for T over \u03c3. Then T and C\u3008T \u3009\u222aCA are equivalent, and hence I\u03c3-equivalent for every \u03c3-structure I\u03c3.\nTo obtain small groundings using bounds, it is important that the information in the bounds is exploited wherever possible. In particular, if a ct- or cf-bound \u03c8 is assigned to an atom P (x), then a similar bound should be assigned to every other atom of the form P (y). We call a c-map atom-equal if it has exactly this property for all atomic subformulas of T . That is, C is atom-equal if it assigns essentially the same bounds to atomic subformulas over the same predicate or function symbol:\nDefinition 22. A c-map C for a TNF theory T over \u03c3 is atom-equal if for every predicate symbol P/n there exist formulas \u03d5ctP [x1, . . . , xn] and \u03d5 cf P [x1, . . . , xn] such that for every atom P (y1, . . . , yn) that occurs in T , Cct(P (y1, . . . , yn)) is equal to \u03d5ctP [x1/y1, . . . , xn/yn] and Ccf(P (y1, . . . , yn)) is equal to \u03d5cfP [x1/y1, . . . , xn/yn], and similarly for function symbols.\nNote that if no predicate or function symbol occurs more than once in a theory T , then every c-map for T is atom-equal.\nExample 6 (Example 1 ctd.). Let T2 be the theory obtained by adding the sentence \u2203w Sub(w,w) to T1. The only predicate that occurs more than once in T2 is the predicate Sub. Let C4 be a c-map for T2 that assigns the following bounds to the atomic subformulas of T2 over Sub: (\u22a5,\u00acEdge(u, v)) to Sub(u, v), (\u22a5,\u00acEdge(x, y)) to Sub(x, y), (\u22a5,\u00acEdge(x, z)) to Sub(x, z) and (\u22a5,\u00acEdge(w,w)) to Sub(w,w). Then C4 is atom-equal. Indeed, if we take \u03d5ctSub = \u22a5 and \u03d5cfSub = \u00acEdge(x1, x2), then the conditions of Definition 22 are satisfied for predicate Sub.\nFor an atom-equal c-map C, CA in general contains many equivalent sentences. For example, for the c-map C4 as in Example 6, (C4)A contains amongst others, the equivalent sentences (18) and (19). It also contains \u2200w \u00acEdge(w,w) \u2283 \u00acSub(w,w), which is implied by (18). As a result, if C is an atom-equal c-map, grounding CA in a naive way yields a grounding that contains several formulas more than once. In the following proposition, we assume this redundancy is removed. In other words, we assume a grounding algorithm for CA that never adds the same GNF formula more than once to the grounding. This can be accomplished by grounding instead of CA the sentences \u2200x (\u03d5ctbP \u2283 P (x)) and \u2200x (\u03d5cfbP \u2283 \u00acP (x)) for every predicate symbol P , where \u03d5ctbP and \u03d5cfbP are as in Definition 22, and similarly for function symbols.\nProposition 23. Let C be an atom-based, atom-equal c-map for a TNF theory T . If T has a model expanding I\u03c3, then Grred(C\u3008T \u3009 \u222a CA) is at most as large as Grred(T ).\nIn the proof, we denote the size of a theory Tg by |Tg|.\nProof. The outline of this proof is as follows. First, we show that every subformula that occurs in Grred(C\u3008T \u3009), occurs in Grred(T ). Then, we prove that no atom occurring in Grred(CA) occurs in Grred(C\u3008T \u3009). Next, we show that every atom occurring in Grred(CA) occurs at least once in Grred(T ). Since we assumed Grred(CA) does not contain any formula more than once, it follows that |Grred(C\u3008T \u3009)| \u2264 |Grred(T )| \u2212 |Grred(CA)|, which concludes the proof.\nWe can directly apply Proposition 18 to show that every subformula of Grred(C\u3008T \u3009) occurs in Grred(T ): if C\u2032 is the trivial c-map, then Grred(T ) is equal to Grred(C\u2032\u3008T \u3009), and clearly C is more precise than C\u2032.\nWe now show that none of the atoms occurring in Grred(CA) occur in Grred(C\u3008T \u3009). Let P (d) be an atom occurring in Grred(C\u3008T \u3009). Then there is an atomic subformula P (x) of T such that d 6\u2208 {x | Cct(P (x))}I\u03c3 and d 6\u2208 {x | Ccf(P (x))}I\u03c3 . Because C is atom-equal, it follows that for any subformula P (y) occurring in T , neither d \u2208 {y | Cct(P (y))}I\u03c3 nor d \u2208 {y | Ccf(P (y))}I\u03c3 . Therefore P (d) does not occur in Grred(CA).\nIt remains to show that every atom that occurs in Grred(CA) also occurs in Grred(T ). Let M be a model of Grred(T ). Such a model exists because we assumed that T has a model expanding I\u03c3. Let P (d) be an atom that does not occur in Grred(T ). If P is a predicate of the input vocabulary, then P (d) does not occur in Grred(CA) either. If on the other hand, P is in the expansion vocabulary, then the structure M \u2032 obtained from M by swapping the truth value of P (d) is also a model of Grred(T ). Since Grred(C\u3008T \u3009 \u222a CA) is I\u03c3-equivalent to Grred(T ) and P 6\u2208 \u03c3, it follows that M |= Grred(CA) and M \u2032 |= Grred(CA). Because Grred(CA) only contains unit clauses, we conclude that P (d) does not occur in Grred(CA).\nWe now have the following algorithm to create a small grounding for T with respect to I\u03c3: first compute an atom-based, atom-equal c-map C for T over \u03c3 (We will present an algorithm for this in Section 4.4). If C is I\u03c3-inconsistent, output \u22a5 and stop. Else, output Grred(C\u3008T \u3009 \u222a CA).\nIt follows from Propositions 11 and 21 that the result of this algorithm is indeed a grounding for T with respect to I\u03c3. Observe that the first step of this algorithm is independent of I\u03c3. If one has to solve several model expansion problems with a fixed input theory T and input vocabulary \u03c3, but varying I\u03c3, it suffices to compute C only once.\nTo perform the last step of the algorithm, one could apply any off-the-shelf grounder on input C\u3008T \u3009 \u222a CA."}, {"heading": "4.4 Computing Bounds", "text": "We now present an algorithm to compute a (non-trivial) c-map C. It is based on our work on approximate reasoning for FO (Wittocx, Marie\u0308n, & Denecker, 2008a). In general the resulting cmap is neither atom-based nor atom-equal, but an atom-based, atom-equal c-map can be derived from it."}, {"heading": "4.4.1 Refining C-Maps", "text": "Constructing a non-trivial c-map can be done by starting from the least precise c-map, i.e., the one that assigns (\u22a5,\u22a5) to every subformula of T , and then gradually refining it. Each refinement step consists of three operations:\n1. Choose a subformula \u03d5 of T .\n2. Compute from the current c-map C a new ct-bound \u03d5rct or cf-bound \u03d5rcf for \u03d5. Below, we elaborate on this step: we present six different ways to obtain new ct- or cf-bounds, called refinement bounds, from T and C. If the sentences of T are represented by their \u201csyntax trees\u201d, each node corresponds to a subformula of T . Bottom-up refinement bounds are bounds for a node computed by considering the bounds assigned by C to its children. Vice versa, top-down refinement bounds are computed by looking at the parents and siblings of a node. Axiom refinement bounds are bounds for the roots, i.e., for the sentences of T , while input, copy and functional refinement bounds are in practice mainly bounds for atomic subformulas of T .\n3. Substitute Cct(\u03d5) by Cct(\u03d5) \u2228 \u03d5rct, respectively Ccf(\u03d5) by Ccf(\u03d5) \u2228 \u03d5rcf .\nAccording to the following lemma, a refinement step yields a new bound for \u03d5 that is more precise than the one assigned by C.\nLemma 24. If \u03c8 and \u03c7 are two ct-bounds for \u03d5 with respect to T , then \u03c8 \u2228\u03c7 is also a ct-bound for \u03d5. Moreover, \u03c8 \u2228 \u03c7 is more precise than \u03c8 and more precise than \u03c7. The same holds for cf-bounds.\nProof. Let \u03c8 and \u03c7 be two ct-bounds for \u03d5[x]. By definition, T |= \u2200x (\u03c8 \u2283 \u03d5) and T |= \u2200x (\u03c7 \u2283 \u03d5). Therefore T |= \u2200x ((\u03c8 \u2228 \u03c7) \u2283 \u03d5), which proves that \u03c8 \u2228 \u03c7 is a ct-bound for \u03d5. Since |= \u03c8 \u2283 (\u03c8 \u2228 \u03c7) and |= \u03c7 \u2283 (\u03c8 \u2228 \u03c7), \u03c8 \u2228 \u03c7 is a more precise bound than \u03c8 and \u03c7. The proof for cf-bounds is similar.\nWe conclude that repeatedly applying refinement steps leads to a more and more precise c-map. The resulting algorithm is an any-time algorithm. In Section 6 we will discuss a stop criterion for the algorithm. We will also give examples where it can reach a fixpoint, and examples where it cannot.\nWe now present the different ways to obtain refinement bounds.\nInput Refinement Let \u03d5[x] be a formula over the input vocabulary \u03c3. Since T |= \u2200x (\u03d5[x] \u2283 \u03d5[x]) and T |= \u2200x (\u00ac\u03d5[x] \u2283 \u00ac\u03d5[x]), it is clear that \u03d5[x] is a ct-bound and \u00ac\u03d5[x] a cf-bound for \u03d5[x]. We call these input refinement ct- and cf-bounds.\nAxiom Refinement If \u03d5 is a sentence of T , then > is an axiom refinement ct-bound for \u03d5. This refinement bound states that a sentence of T is true in every model of T .\nBottom-Up Refinement For a compound subformula \u03d5, depending on its structure, Table 1 gives the bottom-up refinement ct-bound \u03d5rct and cf-bound \u03d5 r cf for \u03d5 with respect to C. It is rather straightforward to obtain these formulas. For instance, the formula in the bottom-right of the table indicates that if \u03d5 is the formula \u03c8 \u2228 \u03c7, then \u03d5 is certainly false for those tuples for which both \u03c8 and \u03c7 are certainly false. Or, more formally, if both T |= Ccf(\u03c8) \u2283 \u00ac\u03c8 and T |= Ccf(\u03c7) \u2283 \u00ac\u03c7, then T |= Ccf(\u03c8) \u2227 Ccf(\u03c7) \u2283 \u00ac(\u03c8 \u2228 \u03c7).\nTop-Down Refinement In the case of top-down refinements, the bounds of a formula \u03c8 are used to construct refinement bounds for one of its direct subformulas \u03d5 (i.e., \u03d5 is one of \u03c8\u2019s children in the syntax tree). The top-down refinement ct-bounds \u03d5rct and cf-bounds \u03d5 r cf for \u03d5 are given in Table 2. In this table, the tuple y denotes the free variables of \u03c8 that do not occur in \u03d5 and x\u2032 denotes a new variable. We illustrate some of these refinement bounds. For further explanation why\nthese bounds are in a certain sense the most precise ones that can be obtained, we refer to our work on approximate reasoning (Wittocx et al., 2008a).\nLet \u03c8 be the formula \u2200x P (x, y). Recall that intuitively, the ct-bound Cct(\u03c8) indicates for which domain elements d, \u2200x P (x, d) is certainly true. For such a d and an arbitrary d\u2032 \u2208 D, P (d\u2032, d) must be true. Hence, Cct(\u03c8) is a ct-bound for \u03d5. Indeed, since x does not occur free in Cct(\u03c8), T |= \u2200x\u2200y (Cct(\u03c8) \u2283 P (x, y)) follows from T |= \u2200y (Cct(\u03c8) \u2283 \u2200x P (x, y)).\nNow let \u03c8 be the formula P (x) \u2227 Q(x, y). If we know that P (d1) \u2227 Q(d1, d2) is certainly false, but Q(d1, d2) is certainly true, then P (d1) must be certainly false. Hence, \u2203y Ccf(\u03c8) \u2227 Cct(\u03c7) is a cf-bound for P (x).\nLet \u03c8 be the formula \u2203x P (x, y) and assume that \u2203x P (x, dy) is certainly true, but for all d\u2032x, except dx, P (d\u2032x, dy) is certainly false. Then we can conclude that P (dx, dy) must be true. This is precisely what is expressed by the formula Cct(\u03c8) \u2227 \u2200x\u2032 (x 6= x\u2032 \u2283 Ccf(\u03d5)[x/x\u2032]).\nFunctional Refinement If \u03d5[x, y] is the formula F (x) = y, functional refinement bounds for \u03d5 take into account that F is a function. The functional refinement ct-bound \u03d5rct and cf-bound \u03d5 r cf are given by: \u03d5rct := \u2200y\u2032 (y\u2032 6= y \u2283 Ccf(\u03d5)[y/y\u2032])\n\u03d5rcf := \u2203y\u2032 (Cct(\u03d5)[y/y\u2032] \u2227 y 6= y\u2032)\nwhere y\u2032 is a new variable. Informally, the first of these formulas indicates that F (x) is certainly equal to y if for every y\u2032 6= y, F (x) is certainly not equal to y\u2032. The second one says that F (x) is certainly not equal to y if F (x) is certainly equal to y\u2032 for some y\u2032 6= y.\nCopy Refinement Let \u03d5[x1, . . . , xn] and \u03c8[y1, . . . , ym] be two formulas such that \u03d5[x1/z, . . . , xn/z] and \u03c8[y1/z, . . . , ym/z] are the same, modulo a renaming of their non-free variables. That is, \u03d5 and \u03c8 have exactly the same syntax tree, but their variables may differ. Denote by E(\u03d5,\u03c8) the set of all equalities xi = yj such that for some occurrence of xi in \u03d5, yj occurs in the corresponding position in \u03c8. Then the formula \u2203y1 . . . \u2203ym (Cct(\u03c8) \u2227 \u2227 E(\u03d5,\u03c8)) is a copy refinement ct-bound for \u03d5 and\nthe formula \u2203y1 . . . \u2203ym (Ccf(\u03c8) \u2227 \u2227 E(\u03d5,\u03c8)) is a copy refinement cf-bound for \u03d5. We also say that these are the copy-refinement bounds from \u03c8 to \u03d5.\nExample 7. Let \u03d5 be the formula P (x1, x1)\u2227\u2200s Q(x2, s) and \u03c8 the formula P (y1, y2)\u2227\u2200t Q(y2, t). Because \u03d5[x1/z, x2/z] is equal to \u03c8[y1/z, y2/z] modulo the renaming of s by t, these formulas satisfy the requirement for copy refinement. The set E(\u03d5,\u03c8) is given by {x1 = y1, x1 = y2, x2 = y2} and hence,\n\u2203y1\u2203y2 (Cct(\u03c8) \u2227 x1 = y1 \u2227 x1 = y2 \u2227 x2 = y2)\nis a copy refinement ct-bound for \u03d5. Observe that if Cct(\u03c8) does not contain bounded occurrences of x1 or x2, this formula is equivalent to the simpler formula Cct(\u03c8)[y1/x1, y2/x1] \u2227 x1 = x2.\nOne-Step Refinements We call \u03d5rct (\u03d5 r cf) a refinement ct-bound (cf-bound) for \u03d5 with respect to C if it is an input, axiom, bottom-up, top-down, functional or copy refinement ct-bound (cf-bound) for \u03d5 with respect to C. Lemma 25 states that a refinement ct-bound (cf-bound) is indeed a ct-bound (cf-bound).\nLemma 25. If \u03d5rct is a refinement ct-bound for \u03d5 with respect to C, then it is a ct-bound for \u03d5. Similarly for cf-bounds.\nProof. The proof consists of a simple analysis of all cases. We proved some of the cases when we introduced input, bottom-up and top-down refinement. The proof of the other cases is similar.\nDefinition 26. Let C be a c-map for T over \u03c3, \u03d5 a subformula of T , \u03d5rct a refinement ct-bound and \u03d5rcf a refinement cf-bound for \u03d5 with respect to C. An assignment Cr that corresponds to C, except that it assigns Cr(\u03d5) = (Cct(\u03d5) \u2228 \u03d5rct, Ccf(\u03d5)) or Cr(\u03d5) = (Cct(\u03d5), Ccf(\u03d5) \u2228 \u03d5rcf) is called a one-step refinement of C.\nFrom Lemma 24 and 25 we obtain the following result.\nProposition 27. Every one-step refinement of a c-map for T over \u03c3 is a c-map for T over \u03c3.\nAs already mentioned at the beginning of this section, one can compute a c-map for T over \u03c3 by first assigning (\u22a5,\u22a5) to every subformula of T and then repeatedly applying one-step refinements. We call this nondeterministic any-time algorithm the refinement algorithm.\nExample 8 (Example 1 ctd.). Figure 1 shows a possible run of the refinement algorithm for input T and \u03c3. Here, the sentences of T1 are represented by their syntax trees. The numbers indicate at which step the bounds are refined. The trivial bounds are not shown.\nIn step (1), ct-bound \u22a5 for the first sentence is replaced by \u22a5 \u2228 > using axiom refinement. Of course, this new bound can be simplified to >. For all following steps, the figure shows simplified bounds. In step (2) and (3) the bounds of subformula Edge(u, v) are refined by input refinement. Then, top-down refinement is used to set the ct-bound of \u00acSub(u, v) \u2228 Edge(u, v) to >. Next, by top-down refinement, \u00acEdge(u, v) becomes the ct-bound for \u00acSub(u, v) and then the cf-bound for Sub(u, v).\nIn a similar way, the cf-bound y 6= z is derived for subformula Sub(x, y) \u2227 Sub(x, z) (step (7) \u2013 (12)). Then, by copy refinement, the cf-bounds for Sub(x, y) becomes \u2203u\u2203v (\u00acEdge(u, v) \u2227 u = x \u2227 v = y), wich simplifies to \u00acEdge(x, y). Likewise, after simplification, \u00acEdge(x, z) is the copy refinement cf-bound for Sub(x, z). Finally, two steps of bottom-up refinement are used to set the ct-bound of \u00ac(Sub(x, y) \u2227 Sub(x, z)) to y 6= z \u2228 \u00acEdge(x, y) \u2228 \u00acEdge(x, z).\nAt this step, a fixpoint is reached: every one-step refinement that can be performed yields a bound that is logically equivalent to the one it tries to refine.\nExample 9. Consider a simplified planning problem, where actions should be scheduled such that if an action ap is a precondition of an action a0, then ap is performed at an earlier time point than a0. This problem is described by the theory T3, consisting of the sentence\n\u2200a0\u2200ap\u2200t0 Prec(ap, a0) \u2227Do(a0, t0) \u2283 (\u2203tp tp < t0 \u2227Do(ap, tp)).\nFrom this sentence, it follows that if a chain of i actions must be executed before a0 can be executed, then a0 cannot be executed before the ith timepoint. Therefore, for any i > 0, the following formula is a cf-bound for Do(a0, t0) over \u03c32 = {Prec,<}:\n\u2203a1 \u00b7 \u00b7 \u00b7 \u2203ai (Prec(a1, a0) \u2227 . . . \u2227 Prec(ai, ai\u22121)) \u2227 \u00ac\u2203t1 \u00b7 \u00b7 \u00b7 \u2203ti (t1 < t0 \u2227 . . . \u2227 ti < ti\u22121).\nDenote this formula by \u03c7i. For any n > 0 and a sufficient number of steps, the refinement algorithm can derive that \u03c8n := \u03c71 \u2228 . . . \u2228 \u03c7n is a cf-bound for Do(a0, t0). Clearly, for n1 6= n2, \u03c8n1 is not logically equivalent to \u03c8n2 . This indicates that the refinement algorithm will not reach a fixpoint for input T3 and \u03c32.\nAs shown by the examples, there are several issues concerning the practical implementation of the refinement algorithm.\n1. Due to the non-deterministic nature of the algorithm, a heuristic is needed to choose which bounds to refine and which kind of refinement to apply. A reasonable choice is to first apply all possible axiom and input refinements. Then, top-down refinement for formula \u03d5 is applied only if a bound for its parent or one of its siblings in the syntax tree has recently been refined. Similarly, bottom-up refinement is applied if a bound for one of \u03d5\u2019s children has been refined. Such a strategy was used in Example 1.\n2. The bounds should be simplified at regular time points, i.e., they should be replaced by equivalent but smaller formulas. If bounds are not simplified, they can only grow in size, rapidly leading to formulas of unwieldy size. A simplification algorithm is discussed in Section 6.\n3. To be able to detect that a fixpoint has been reached, one needs to find out that two bounds are equivalent. In general this is undecidable. To detect a fixpoint in at least some cases, one could use an FO theorem prover (and restrict its running time).\nIn case a fixpoint cannot be reached or detected, another stop criterion is needed. For example, one could restrict the number of one-step refinements, or the total time the refinement algorithm can use. Another stop criterion, and a simple fixpoint check are discussed in Section 6."}, {"heading": "4.4.2 Extracting an Atom-Based and Atom-Equal C-Map", "text": "The c-maps obtained by the refinement algorithm are in general neither atom-based nor atom-equal. To derive from an arbitrary c-map C an atom-equal c-map that is at least as precise as C, we first collect for each predicate P all bounds that are assigned to occurrences of P in the theory. Then the disjunction of these bounds is assigned as new bound to each occurrence of P . Because all bounds assigned to atoms over P are then essentially the same, we have an atom-equal c-map. We now present this method more formally:\nDefinition 28. Let C be a c-map for a TNF theory T and P/n a predicate. Let P (x11, . . . , x1n), . . . , P (xm1, . . . , xmn) be all occurrences of P in T and let y1, . . . , yn be n new variables. Denote by \u03d5ict, respectively \u03d5 i cf , the formulas\n\u2203x\u2032i1 \u00b7 \u00b7 \u00b7 \u2203x\u2032in (Cct(P (xi1, . . . , xin))[xi1/x\u2032i1, . . . , xin/x\u2032in] \u2227 y1 = x\u2032i1 \u2227 . . . \u2227 yn = x\u2032in)\nand\n\u2203x\u2032i1 \u00b7 \u00b7 \u00b7 \u2203x\u2032in (Ccf(P (xi1, . . . , xin))[xi1/x\u2032i1, . . . , xin/x\u2032in] \u2227 y1 = x\u2032i1 \u2227 . . . \u2227 yn = x\u2032in),\nwhere the variables x\u2032ij are new variables. The ct-copy closure of P (xk1, . . . , xkn) with respect to C is the disjunction \u2228 1\u2264i\u2264m \u03d5 i ct[y1/xk1, . . . , yn/xkn]. The cf-copy closure of P (xk1, . . . , xkn) is the\nformula \u2228\n1\u2264i\u2264n \u03d5 i cf [y1/xk1, . . . , yn/xkn]. The copy-closure for atoms of the form F (x) = y is defined\nsimilarly.\nWe denote the ct-copy closure of an atom \u03d5 by copyCct(\u03d5), and its cf-copy closure by copy C cf(\u03d5).\nDefinition 29. The copy-closure of C is the c-map that assigns (copyCct(\u03d5), copyCcf(\u03d5)) to every atomic subformula \u03d5 of T , and corresponds to C on all other subformulas. Example 10. Let T4 be the theory consisting of the sentences \u2200x (P (x) \u2283 R(x)) and \u2200y (Q(y) \u2283 R(y)) and let C5 be a c-map over \u03c33 = {P,R} that assigns (P (x),\u22a5) to R(x) and (Q(y),\u22a5) to R(y). The copy-closure of C5 assigns\n((\u2203x\u2032 (P (x\u2032) \u2227 x\u2032 = x)) \u2228 (\u2203x\u2032 (Q(x\u2032) \u2227 x\u2032 = x)), (\u2203x\u2032 (\u22a5 \u2227 x\u2032 = x)) \u2228 (\u2203x\u2032 (\u22a5 \u2227 x\u2032 = x)))\nto R(x). These bounds simplify to (P (x) \u2228 Q(x),\u22a5). Likewise, the copy-closure of C5 assigns to R(y) bounds that simplify to (P (y) \u2228Q(y),\u22a5). Proposition 30. The copy-closure of a c-map is an atom-equal c-map.\nProof. This follows immediately from the definition of atom-equal c-map since for every predicate symbol P (or function symbol F ), the same bounds, namely the formulas \u2228 1\u2264i\u2264n \u03d5 i ct and \u2228 1\u2264i\u2264n \u03d5 i cf mentioned in definition 28, are assigned to every atom over P (respectively F ).\nRecall that a c-map C is atom-based if C is implied by CA, i.e., by all sentences in C that stem from bounds for atomic subformulas of T . A method to derive an atom-based c-map from an arbitrary c-map is based on the following observation. Let C be a c-map for T over \u03c3 and let \u03d5[x] be the subformula \u03c7 \u2227 \u03c8 of T . If Cct(\u03d5) is the formula Cct(\u03c7) \u2227 Cct(\u03c8), i.e., it is the bottom-up refinement ct-bound for \u03d5 with respect to C, then T |= \u2200x (Cct(\u03d5) \u2283 \u03d5) is implied by T |= \u2200x (Cct(\u03c7) \u2283 \u03c7) and T |= \u2200x (Cct(\u03c8) \u2283 \u03c8). It is easy to check that the same property holds for all other bottom-up refinement bounds:\nLemma 31. Let C be a c-map for T over \u03c3 and \u03d5[x] a subformula of T , and let \u03d5rct and \u03d5rcf be the bottom-up refinement bounds for \u03d5 with respect to C. If S is the set of direct subformulas of \u03d5, i.e., its children in the syntax tree, and T \u2032 is the theory given by\nT \u2032 := {\u2200y Cct(\u03c8) \u2283 \u03c8 | \u03c8[y] \u2208 S} \u222a {\u2200y Ccf(\u03c8) \u2283 \u00ac\u03c8 | \u03c8[y] \u2208 S},\nthen T \u2032 |= \u2200x \u03d5rct \u2283 \u03d5 and T \u2032 |= \u2200x \u03d5rcf \u2283 \u00ac\u03d5. Definition 32. A c-map C for T is called a bottom-up c-map if for every non-atomic subformula \u03d5 of T , Cct(\u03d5) is the bottom-up ct-refinement bound for \u03d5 with respect to C, and Ccf(\u03d5) is the bottom-up cf-refinement bound for \u03d5 with respect to C.\nThe next proposition follows directly from Lemma 31.\nProposition 33. A bottom-up c-map C is atom-based. Observe that a bottom-up c-map C for T is completely determined by the bounds it assigns to the atomic subformulas of T . Hence, given a c-map, one can derive a bottom-up c-map from it by retaining the bounds for the atomic subformulas and then computing the corresponding bottom-up c-map. We conclude that we can derive an atom-based, atom-equal c-map from an arbitrary c-map by deriving an atom-based c-map from its copy-closure.\nExample 11 (Example 1 ctd.). Let C6 be the fixpoint shown in Figure 1. This c-map is atom-equal (and equivalent to its copy-closure). The bottom-up c-map derived from C6 is shown in Figure 2. Observe that this c-map is less precise than C6. For instance, the cf-bound assigned by C6 to the conjunction Sub(x, y) \u2227 Sub(x, z) is a disjunction of two bounds, namely bound y 6= z, obtained by top-down refinement, and bound \u00acEdge(x, y)\u2228\u00acEdge(x, z), obtained by bottom-up refinement. In the c-map of Figure 2, only the latter bound is present.\nFor the c-map in Figure 2, the c-transformation of Sub(x, y) \u2227 Sub(x, z) is given by\n((Sub(x, y) \u2227 Edge(x, y)) \u2227 (Sub(x, z) \u2227 Edge(x, z))) \u2227 (Edge(x, y) \u2227 Edge(x, z)).\nThis formula contains repeated constraints Edge(x, y) and Edge(x, z) on the variables x, y and z. In general bottom-up c-maps produce many such repetitions. These could easily be eliminated to speed up the grounding process, but it depends on the used grounding algorithm which ones are best deleted."}, {"heading": "5. Inductive Definitions", "text": "Although all NP problems can be cast as MX(FO) problems, modelling such problems using pure FO can be extremely complex. In practice, modelling is often enhanced considerably by using extensions of FO with constructs such as inductive definitions, subsorts, aggregates, partial functions and arithmetic. For this enriched language we have implemented the model generator idp (Wittocx et al., 2008b; Wittocx & Marie\u0308n, 2008).2\nIn this paper we focus on grounding of the extension of FO with inductive definitions. It is well-known that in arbitrary domains, inductively definable concepts such as \u201creachability\u201d are not FO-expressible. In finite domains however, they can be encoded (e.g., by encoding the fixpoint construction), but the process is tedious and leads to large theories. In this section we will extend the refinement algorithm to FO(ID) (Denecker, 2000; Denecker & Ternovska, 2008). This language extends FO with a construct for representing some of the most common types of inductive definitions: monotone induction and non-monotone induction such as induction over a well-founded order and iterated inductive definitions. Such definitions have many applications in real-life computational problems, e.g., in planning problems or problems involving reachability or dynamic systems (Denecker & Ternovska, 2008, 2007). At the same time, FO(ID) is also an integration of FO and logic programming.\n2. idp can be downloaded from http://dtai.cs.kuleuven.be/krr/software.html"}, {"heading": "5.1 Three-Valued Structures", "text": "While FO(ID) has a standard two-valued semantics, three-valued structures are used in the formal semantics of definitions. Indeed, an inductive definition defines a set by describing how to construct it. In the semantics, the intermediate stages of the construction are recorded by three-valued sets, representing for any object whether it belongs to the set or not, or whether this has not yet been derived. We therefore recall the basic concepts of three-valued logic.\nWe denote the truth values true, false and unknown by respectively t, f and u. A three-valued \u03a3-interpretation I\u0303 consists of a domain D and\n\u2022 a domain element xI\u0303 \u2208 D for each variable x;\n\u2022 a function P I\u0303 : Dn \u2192 {t, f,u} for each predicate symbol P/n;\n\u2022 a function F I\u0303 : Dn \u2192 D for each function symbol F/n.\nIf P I\u0303(d) 6= u for every tuple d of domain elements and predicate symbol P , then I\u0303 is two-valued: it corresponds to the interpretation I that assigns d \u2208 P I iff P I\u0303(d) = t for every predicate P and corresponds to I\u0303 on all other symbols.\nThe truth order \u2264 on the set of truth values is induced by f < u < t, the precision order \u2264p is induced by u <p f and u <p t. These orders are extended to three-valued \u03a3-structures: if I\u0303 and J\u0303 correspond on \u03a3F , then we define\n\u2022 I\u0303 \u2264 J\u0303 iff P I\u0303(d) \u2264 P J\u0303(d) for every d and P ;\n\u2022 I\u0303 \u2264p J\u0303 iff P I\u0303(d) \u2264p P J\u0303(d) for every d, P .\nObserve that two-valued structures are maximally precise three-valued structures. On the other hand, the least precise three-valued structure assigns P I\u0303(d) = u for every d and P .\nWe define the truth value I\u0303(\u03d5) of a formula \u03d5 in a three-valued interpretation I\u0303 with domain D by the standard Kleene semantics:\n\u2022 I\u0303(P (t1, . . . , tn)) := P I\u0303(tI\u03031, . . . , tI\u0303n);\n\u2022 I\u0303(\u03d51 \u2228 \u03d52) := lub\u2264{I\u0303(\u03d51), I\u0303(\u03d52)};\n\u2022 I\u0303(\u03d51 \u2227 \u03d52) := glb\u2264{I\u0303\u03d51, I\u0303(\u03d52)};\n\u2022 I\u0303(\u2203x \u03d5) := lub\u2264{I\u0303[x/d](\u03d5) | d \u2208 D};\n\u2022 I\u0303(\u2200x \u03d5) := glb\u2264{I\u0303[x/d](\u03d5) | d \u2208 D}.\nAn atom of the form P (d), where d is a tuple of domain constants, is called a domain atom. For a truth value v and a domain atom P (d), we denote by I\u0303[P (d)/v] the interpretation that assigns v to P (d) and corresponds to I\u0303 on all other symbols. This notation is extended to sets of domain atoms."}, {"heading": "5.2 Inductive Definitions", "text": "An FO(ID) theory is a set of FO sentences and definitions. A definition \u2206 is a finite set of rules of the form3\n\u2200x (P (x)\u2190 \u03d5),\n3. Usually, nested terms are allowed as arguments of P , but to facilitate the presentation, we only allow variables as arguments in this paper.\nwhere P is a predicate and \u03d5 an FO formula. The free variables of \u03d5 should be among x. P (x) is called the head of the rule, \u03d5 the body. Predicates that occur in the head of a rule of \u2206 are called defined predicates of \u2206. The set of all defined predicates of \u2206 is denoted Def(\u2206). All other symbols are called open with respect to \u2206. The set of open symbols of \u2206 is denoted by Open(\u2206).\nObserve that an FO(ID) theory has the appearance of an FO theory augmented with a collection of logic programs. As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)\u2019s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc.\nThe semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics. We borrow the presentation of this semantics from Denecker and Vennekens (2007).\nDefinition 34. Let \u2206 be a definition and I\u0303 a three-valued structure. A well-founded induction for \u2206 above I\u0303 is a sequence \u3008J\u0303\u03be\u30090\u2264\u03be\u2264\u03b1 of three-valued structures such that\n1. J\u03030 assigns P J\u03030(d) = u, if P is a defined predicate and corresponds to I\u0303 on the open symbols;\n2. For each limit ordinal \u03bb \u2264 \u03b1, J\u0303\u03bb = lub\u2264p{J\u0303\u03be | \u03be < \u03bb};\n3. For every ordinal \u03be, J\u0303\u03be+1 relates to J\u0303\u03be in one of the following ways:\n(a) J\u0303\u03be+1 = J\u0303\u03be[P (d)/t] for some domain atom P (d) such that P J\u0303\u03be(d) = u and for some rule \u2200x (P (x)\u2190 \u03d5) in \u2206, J\u0303\u03be[x/d](\u03d5) = t.\n(b) J\u0303\u03be+1 = J\u0303\u03be[U/f], where U is a set of domain atoms, such that for each P (d) \u2208 U , P J\u0303\u03be(d) = u and for all rules \u2200x (P (x)\u2190 \u03d5) in \u2206, J\u0303\u03be+1[x/d](\u03d5) = f.\nIntuitively, (a) says that a domain atom P (d) can be made true if there is a rule with P (x) as head and body \u03d5 such that \u03d5[x/d] is already true. On the other hand (b) explains that P (d) can be made false if there is no possibility of making a corresponding body true, except by circular reasoning. The set U , commonly called an unfounded set, is a witness to this: making all atoms in U false also makes all corresponding bodies false.\nA well-founded induction is called terminal if it cannot be extended anymore. The limit of a terminal well-founded induction is its last element. Denecker and Vennekens (2007) show that each terminal well-founded induction for \u2206 above I\u0303 has the same limit, which corresponds to the wellfounded model of \u2206 extending I\u0303|Open(\u2206), and is denoted by wfm\u2206(I\u0303). The well-founded model is three-valued in general.\nA two-valued structure I satisfies a definition \u2206 if I = wfm\u2206(I). An FO(ID) theory T is a finite set of FO sentences and definitions. I satisfies T if it satisfies all definitions and sentences in T . If \u2206 is a definition over \u03a3 and J a \u03a3|Open(\u2206)-structure, there exists at most one expansion I of J to \u03a3 such that I |= \u2206. A definition is called total if for any \u03a3|Open(\u2206)-structure J there is precisely one expansion I of J to \u03a3 that satisfies \u2206. Intuitively, total definitions correspond to well-formed definitions: for every defined predicate P , they define for each tuple of domain elements whether d belongs to the relation denoted by P or not. If a definition is not total, this typically indicates an error. Hence in practice, all definitions that occur in MX(FO(ID)) specifications are total. For example, this is the case for all MX(FO(ID)) specifications used in the second ASPcompetition (Denecker, Vennekens, Bond, Gebser, & Truszczyn\u0301ski, 2009). In general, checking whether a definition is total is undecidable. However, there are several broad and easily recognizable classes of total definitions. For example, all monotone and stratified definitions are total.\nWe give some examples of definitions and MX(FO(ID)) problems.\nExample 12. Definition \u22061 defines relation TC to be the transitive closure of relation R. \u22061 = {\n\u2200x\u2200y (TC(x, y)\u2190 R(x, y)). \u2200x\u2200y (TC(x, y)\u2190 \u2203z (TC(x, z) \u2227 TC(z, y))). } Example 13. To cast the problem of finding a Hamiltonian path in a given graph as an MX(FO(ID)) problem, let\n\u03c3 = \u3008{Edge/2}, \u2205\u3009 \u03a3 = {\u03c3P \u222a {Ham/2, Reached/1}, {Start/0}\u3009.\nPredicate Ham represents the edges that form the path and Reached the vertices that are in the path. The constant Start represents the first vertex of the path. Let T be the theory\n\u2200v1\u2200v2 (Ham(v1, v2) \u2283 Edge(v1, v2)). \u2200v1\u2200v2\u2200v3 (Ham(v1, v2) \u2227Ham(v1, v3) \u2283 v2 = v3). \u2200v1\u2200v2\u2200v3 (Ham(v1, v3) \u2227Ham(v2, v3) \u2283 v1 = v2). \u2200v \u00acHam(v, Start). \u2200v Reached(v).{\n\u2200v (Reached(v)\u2190 v = Start). \u2200v (Reached(v)\u2190 \u2203w (Reached(w) \u2227Ham(w, v))).\n} .\nThen model expansion for input structure T and input vocabulary \u03c3 expresses the Hamiltonian path problem: in every model M |=I\u03c3 T , the collection of edges (v1, v2) \u2208 HamM forms a Hamiltonian path in the graph represented by EdgeI\u03c3 .\nA well-known concept that we will use later on in this section is the completion of a definition. The completion of a definition \u2206 is an FO theory that is weaker than \u2206, and is defined as follows.\nDefinition 35. The completion of a definition \u2206 is the FO theory that contains for every P \u2208 Def(\u2206) the sentence \u2200x (P (x) \u2261 ((x = y1 \u2227 \u03d51) \u2228 . . . \u2228 (x = yn \u2227 \u03d5n))), where \u2200y1 (P (y1)\u2190 \u03d51), . . . , \u2200yn (P (yn)\u2190 \u03d5n) are the rules in \u2206 with P in the head.\nWe denote the completion of \u2206 by Comp(\u2206). Clearly, every body of a rule in \u2206 occurs in Comp(\u2206). If T is a theory then we denote by Comp(T ) the result of replacing in T all definitions by their completion. The following result states that the completion of T is weaker than T .\nTheorem 36 (Denecker & Ternovska, 2008). \u2206 |= Comp(\u2206) and T |= Comp(T ) for every definition \u2206 and FO(ID) theory T .\nThe SAT(ID) problem is the problem of deciding whether a given propositional FO(ID) theory is satisfiable. Currently there exist three SAT(ID) solvers. IDsat (Pelov & Ternovska, 2005) works by translating a SAT(ID) problem into an equivalent SAT problem and then calls a SAT solver. MidL (Marie\u0308n, Wittocx, & Denecker, 2007) and MiniSAT(ID) (Marie\u0308n, Wittocx, Denecker, & Bruynooghe, 2008) take a native approach. Marie\u0308n (2009) provides details on the specific form of propositional FO(ID) theories accepted by these solvers, and a method to transform arbitrary propositional FO(ID) theories into this form."}, {"heading": "5.3 Grounding Inductive Definitions", "text": "Like MX(FO) problems, MX(FO(ID)) problems can be reduced to SAT(ID) problems by grounding. In this section we extend grounding and the refinement algorithm of Section 4 to FO(ID). Without loss of generality (Marie\u0308n, Gilis, & Denecker, 2004), we assume that none of the predicates of the input vocabulary \u03c3 is defined by a definition in T , and no predicate is defined by more than one definition. Moreover, we assume that every rule body is in TNF."}, {"heading": "5.3.1 Full and Reduced Grounding", "text": "Let T be an FO(ID) theory. As for FO, a grounding Tg for T with respect to I\u03c3 is a propositional FO(ID) theory that is I\u03c3-equivalent to T . We extend the notion of full and reduced grounding to definitions.\nDefinition 37. The full grounding of a rule \u2200x P (x) \u2190 \u03d5 with respect to I\u03c3 is the set {P (d) \u2190 Grfull(\u03d5[x/d]) | d \u2208 Dn}, where n is the number of variables in x. Similarly, the reduced grounding of \u2200x (P (x) \u2190 \u03d5) is the set {P (d) \u2190 Grred(\u03d5[x/d]) | d \u2208 Dn}. The full (reduced) grounding of a definition \u2206 is the union of the full (reduced) groundings of all rules in \u2206.\nThe full (reduced) grounding of an FO(ID) theory T is the set of the full (reduced) groundings of all sentences and definitions in T ."}, {"heading": "5.3.2 Definitions Depending Only on \u03c3", "text": "We say that a definition \u2206 depends on expansion symbols if Open(\u2206) 6\u2286 \u03c3. If \u2206 does not depend on expansion symbols, then the interpretation of every predicate in Def(\u2206) is the same in every model M of T expanding I\u03c3. Indeed, for such a definition and any M |=I\u03c3 T , M |Open(\u2206) is completely determined by I\u03c3. Therefore also wfm\u2206(M) only depends on I\u03c3.\nThe deductive database literature describes several algorithms to compute wfm\u2206(M) for a definition that does not depend on expansion symbols. Most of them are only defined for definitions where every rule body is a conjunction of atoms. But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.\nAssume \u2206 is a definition that does not depend on expansion symbols. Let \u03c4 be the vocabulary \u3008\u03c3P \u222aDef(\u2206), \u03c3F \u3009 and I\u03c4 the \u03c4 -structure such that I\u03c4 |\u03c3 = I\u03c3 and I\u03c4 |= \u2206. Then clearly, M |=I\u03c3 T iff M |=I\u03c4 T for any structure M . However, a grounding for T \\\u2206 with respect to \u03c4 can be obtained more efficiently, since Grred(T \\\u2206, I\u03c4 ) is necessarily smaller than Grred(T, I\u03c3). Indeed, T \\ \u2206 is a subtheory of T , and Grred(T \\\u2206, I\u03c4 ) does not contain symbols of Def(\u2206), while Grred(T, I\u03c3) does.\nObserve also that the set of c-maps for T over \u03c4 is a superset of the set of c-maps for T over \u03c3, since the bounds assigned by the former c-maps are formulas over \u03c4 , instead of only over \u03c3. As such, c-maps computed by the refinement algorithm for T over \u03c4 might yield more efficient grounding compared to c-maps computed for T over \u03c3."}, {"heading": "5.3.3 Bounds for Definitions", "text": "We now extend the refinement algorithm to FO(ID).\nDefinition 38. A formula \u03d5 is a subformula of an FO(ID) theory T if it is a subformula of a sentence in T or a subformula of a rule body in a definition of T . A c-map for T over \u03c3 is an assignment of a ct- and cf-bound over \u03c3 to every subformula of T .\nNote that a c-map does not assign bounds to heads of rules in a definition. Our strategy to compute a c-map for an FO(ID) theory T is simple: construct the completion of T and apply the refinement algorithm on Comp(T ) to obtain a c-map C for Comp(T ). The restriction of C to the subformulas of T is a c-map for T . Indeed, every subformula \u03d5 of T occurs in Comp(T ) and since T |= Comp(T ), Comp(T ) |= \u2200x (Cct(\u03d5) \u2283 \u03d5) and Comp(T ) |= \u2200x (Ccf(\u03d5) \u2283 \u00ac\u03d5), also T |= \u2200x (Cct(\u03d5) \u2283 \u03d5) and T |= \u2200x (Ccf(\u03d5) \u2283 \u00ac\u03d5).\nIn order to use a c-map for grounding, we lift the definition of c-transformation to FO(ID) theories.\nDefinition 39. Let C be a c-map for a theory T and \u2206 a definition in T . The c-transformation of a rule \u2200x (P (t) \u2190 \u03d5) of \u2206 is given by \u2200x (P (t) \u2190 C\u3008\u03d5\u3009). The c-transformation C\u3008\u2206\u3009 of a\ndefinition \u2206 is the set of c-transformations of rules in \u2206. The c-transformation of T is the set of the c-transformations of the formulas and definitions in T .\nWe also lift the notion of C-equivalence to definitions.\nDefinition 40. Two definitions \u22061 and \u22062 are C-equivalent if for every structure I that satisfies C, I |= \u22061 iff I |= \u22062.\nHowever, Lemma 15 does not hold for FO(ID) theories: for a definition \u2206, C\u3008\u2206\u3009 is not necessarily C-equivalent to \u2206.\nExample 14. Let \u03c3 be the empty vocabulary and T the theory\nP\n{P \u2190 P}.\nThis theory is unsatisfiable because the definition {P \u2190 P} has only one model, in which P is false. This contradicts the sentence in T . Clearly, > is a ct-bound for P . If C is a c-map for T over \u03c3 assigning (>,\u22a5) to P , then C\u3008{P \u2190 P}\u3009 = {P \u2190 (P \u2227 \u00ac\u22a5) \u2228 >}, which is equivalent to {P \u2190 >}. This definition has only a model that assigns true to P . Since this model also satisfies C, we conclude that {P \u2190 P} and C\u3008{P \u2190 P}\u3009 are not C-equivalent.\nDefinition 41. Let \u2206 a definition of T . We call c-map C for T \u2206-tolerant if C\u3008\u2206\u3009 and \u2206 are C-equivalent. We call C T -tolerant if it is \u2206-tolerant for every definition \u2206 of T .\nIn the following, we say that a formula occurs positively (negatively) in a definition \u2206 if it occurs positively (negatively) in a body of a rule in \u2206.\nProposition 42. Let \u2206 be a definition of a theory T . Then a c-map C for T over \u03c3 is \u2206-tolerant if for every subformula \u03d5 of \u2206 that contains a predicate P \u2208 Def(\u2206), the following hold:\n1. If \u2206 is not total, then Cct(\u03d5) = Ccf(\u03d5) = \u22a5.\n2. If \u03d5 occurs positively in \u2206 and P occurs positively in \u03d5, then Cct(\u03d5) = \u22a5.\n3. If \u03d5 occurs negatively in \u2206 and P occurs negatively in \u03d5, then Ccf(\u03d5) = \u22a5.\nNote that the c-map of Example 14 violates the second condition. We will prove Proposition 42 by inductively constructing for any structure I that satisfies C, a sequence of three-valued structures that is a well-founded induction above I for both \u2206 and C\u3008\u2206\u3009. If I |= \u2206, we show that a terminal sequence with this property can be constructed, proving that I also satisfies C\u3008\u2206\u3009. If I 6|= \u2206, a sequence with this property can be constructed such that its last element is not less precise than I. This shows that I does not satisfy C\u3008\u2206\u3009 either. To construct a well-founded induction for both \u2206 and C\u3008\u2206\u3009, we prove that each step that extends a well-founded induction for \u2206 is also a valid step to extend it for C\u3008\u2206\u3009. Step (3a) in Definition 34 is covered by Lemma 43, step (3b) by Lemma 44.\nLemma 43. Let I be a structure that satisfies a c-map C for T over \u03c3 and let J\u0303 \u2264p I be a threevalued interpretation such that J\u0303 |\u03c3 is two-valued. Then J\u0303(\u03d5) \u2264p J\u0303(C\u3008\u03d5\u3009) for every subformula \u03d5 of T .\nProof. We prove this lemma by induction. First assume \u03d5[x] is an atom. Then C\u3008\u03d5\u3009 is the formula (\u03d5\u2227\u00acCcf(\u03d5))\u2228Cct(\u03d5). If J\u0303(\u03d5) = u, then clearly J\u0303(C\u3008\u03d5\u3009) \u2265p J\u0303(\u03d5). If J\u0303(\u03d5) = f, then J\u0303(Cct(\u03d5)) must be false, since I |= C. Therefore J\u0303(C\u3008\u03d5\u3009) = f. If on the other hand, J\u0303(\u03d5) = t, then J\u0303(Ccf(\u03d5)) = f and hence, J\u0303(C\u3008\u03d5\u3009) = t.\nThe inductive cases are all very similar to the base case. We prove one of them. Assume \u03d5 is the formula \u03c8 \u2228 \u03c7. Then C\u3008\u03d5\u3009 is the formula ((C\u3008\u03c8\u3009 \u2228 C\u3008\u03c7\u3009) \u2227 \u00acCcf(\u03d5)) \u2228 Cct(\u03d5). If J\u0303(\u03d5) = f, then J\u0303(\u03c8) = J\u0303(\u03c7) = f, and by induction J\u0303(C\u3008\u03c8\u3009) = J\u0303(C\u3008\u03c7\u3009) = f. Since also J\u0303(Cct(\u03d5)) = f, we conclude that J\u0303(C\u3008\u03d5\u3009) = f. If on the other hand J\u0303(\u03d5) = t, then J\u0303(Ccf(\u03d5)) = f. Also J\u0303(\u03c8) = t or J\u0303(\u03c7) = t, and therefore J\u0303(C\u3008\u03c8\u3009) = t or J\u0303(C\u3008\u03c7\u3009) = t. Hence J\u0303(C\u3008\u03d5\u3009) = t.\nLemma 44. Let \u2206 be a definition of T and C a c-map for T over \u03c3 that satisfies the three conditions of Proposition 42. Let I be a structure that satisfies C and J\u0303 \u2264p I a three-valued interpretation such that J\u0303 |\u03c3 is two-valued. If U is a set of domain atoms defined in \u2206 and unknown in J\u0303 , then for every subformula \u03d5 of \u2206 such that J\u0303 [U/f](\u03d5) 6= u, the following hold: \u2022 J\u0303 [U/f](\u03d5) \u2264 J\u0303 [U/f](C\u3008\u03d5\u3009) if \u03d5 occurs negatively in \u2206;\n\u2022 J\u0303 [U/f](\u03d5) \u2265 J\u0303 [U/f](C\u3008\u03d5\u3009) if \u03d5 occurs positively in \u2206; Proof. Denote H\u0303 := J\u0303 [U/f]. If J\u0303(\u03d5) 6= u, the result follows immediately from Lemma 43.\nWe prove the case where J\u0303(\u03d5) = u by induction. Assume that \u03d5 is an atom P (x). Since J\u0303(\u03d5) = u and H\u0303(\u03d5) 6= u, we know that P (xJ\u0303) \u2208 U and H\u0303(\u03d5) = f. Therefore H\u0303(C\u3008\u03d5\u3009) = H\u0303((\u03d5 \u2227 \u00acCcf(\u03d5)) \u2228 Cct(\u03d5)) = H\u0303(Cct(\u03d5)). If \u03d5 occurs negatively in \u2206, then we have to prove that H\u0303(\u03d5) \u2264 H\u0303(C\u3008\u03d5\u3009). Since H\u0303(\u03d5) = f, this inequality holds regardless the value of Cct(\u03d5) and Ccf(\u03d5) in H\u0303. If on the other hand, \u03d5 occurs positively, we have to prove that H\u0303(\u03d5) \u2265 H\u0303(C\u3008\u03d5\u3009). Since H\u0303(\u03d5) = f and H\u0303(C\u3008\u03d5\u3009) = H\u0303(Cct(\u03d5)), this inequality can only hold if H\u0303(Cct(\u03d5)) = f. Because the conditions on C ensure that Cct(\u03d5) = \u22a5, we can conclude that indeed H\u0303(Cct(\u03d5)) = f.\nWe omit the inductive cases, since they are very similar to the base case.\nProof of Proposition 42. Let I be a structure that satisfies C. We have to prove that I |= \u2206 iff I |= C\u3008\u2206\u3009. If \u2206 is not total, the proof is trivial, since then \u2206 and C\u3008\u2206\u3009 are equivalent.\nNow assume that \u2206 is total and let \u3008J\u0303\u03be\u30090\u2264\u03be\u2264\u03b1 be a well-founded induction for both \u2206 and C\u3008\u2206\u3009 above I. We will prove that if J\u0303\u03b1 is not two-valued, and J\u0303\u03b1 <p I, there exists a J\u0303\u03b1+1 such that \u3008J\u0303\u03be\u30090\u2264\u03be\u2264\u03b1+1 is again a well-founded induction for \u2206 and C\u3008\u2206\u3009. Also observe that if \u03bb is a limit ordinal and \u3008J\u0303\u03be\u30090\u2264\u03be<\u03bb is a well-founded induction for both \u2206 and C\u3008\u2206\u3009, then the same holds for \u3008J\u0303\u03be\u30090\u2264\u03be\u2264\u03bb.\nThis is sufficient to conclude the proof. Indeed, if I |= \u2206, we can keep on extending the sequence until we end up in I, and derive that I |= C\u3008\u2206\u3009. If I 6|= \u2206, then we will eventually extend the well-founded induction with a structure J\u0303\u03b1+1 6\u2264p I. But then, the well-founded model of C\u3008\u2206\u3009 will also be more precise than J\u0303\u03b1+1, which shows that I 6|= C\u3008\u2206\u3009.\nAssume that J\u0303\u03b1 is not two-valued and J\u0303\u03b1 <p I. Because \u2206 is total, there exists a J\u0303\u03b1+1 such that \u3008J\u0303\u03be\u30090\u2264\u03be\u2264\u03b1+1 is a well-founded induction for \u2206. We have to prove that it is also a well-founded induction for C\u3008\u2206\u3009. There are two possibilities: \u2022 J\u0303\u03b1+1 = J\u0303\u03b1[P (d)/t] for some domain atom P (d) and there is a rule \u2200x (P (x) \u2190 \u03d5) in \u2206\nsuch that J\u0303\u03b1[x/d](\u03d5) = t. By Lemma 43, also J\u0303\u03b1[x/d](C\u3008\u03d5\u3009) = t. Hence, \u3008J\u0303\u03be\u30090\u2264\u03be\u2264\u03b1+1 is a well-founded induction for C\u3008\u2206\u3009.\n\u2022 J\u0303\u03b1+1 = J\u0303\u03b1[U/f] and for every P (d) \u2208 U and rule \u2200x (P (x) \u2190 \u03d5) in \u2206, J\u0303\u03b1+1[x/d](\u03d5) = f. By Lemma 44, we conclude that also J\u0303\u03b1+1[x/d](C\u3008\u03d5\u3009) = f. Therefore, \u3008J\u0303\u03be\u30090\u2264\u03be\u2264\u03b1+1 is a wellfounded induction for C\u3008\u2206\u3009.\nFrom Proposition 42 we derive the following procedure to compute a T -tolerant c-map for a theory T . First compute a c-map C for T that is not necessarily T -tolerant. Then, for every definition \u2206 of T and every subformula \u03d5 of \u2206, replace Cct(\u03d5) and Ccf(\u03d5) by \u22a5, if this is required to satisfy the conditions of Proposition 42.\nWe conclude that the following algorithm produces a correct grounding for FO(ID) theory T :\n1. Compute a c-map C for T over \u03c3.\n2. If C is inconsistent with respect to I\u03c3, output \u22a5 and stop.\n3. Else, derive an atom-based, T-tolerant c-map C\u2032 from C.\n4. Output Grred(C\u2032\u3008T \u3009 \u222a C\u2032A), using any off-the-shelf grounder for FO(ID)."}, {"heading": "6. Implementation and Experiments", "text": "So far we have focussed mostly on grounding size. Proposition 23 guaranteed that grounding with bounds produces smaller groundings. In this section we are concerned with the efficiency and practical implementation of grounding with bounds. A first issue was mentioned at the end of Section 4.4.2: an atom-based c-map C computed by the refinement algorithm contains many repeated constraints on variables. To ground C\u3008T \u3009 efficiently, such repetitions should be avoided as much as possible. Secondly, an efficient grounder consults bounds as soon as possible. In particular, it should use bounds to avoid unnecessary instantiations of variables, rather than to remove these instantiations afterwards. As a case study, we will show in detail how to adapt a basic \u201ctop-down style\u201d grounding algorithm to efficiently exploit bounds. We sketch how the same principles can be applied for a \u201cbottom-up style\u201d grounder.\nIn the second part of this section we discuss some aspects of implementing the refinement algorithm. As we mentioned in Section 4.4.1, there are several issues concerning the practical implementation of this algorithm. In particular, a method to simplify bounds is needed, as well as a good stop criterion. We show how these issues can be addressed by representing bounds as first-order binary decision diagrams.\nFinally, we report on our implementation, called GidL, of the refinement and grounding algorithm. We present experimental results that show the impact of using bounds on grounding size and time."}, {"heading": "6.1 Case Study: Top-Down Grounding with Bounds", "text": "For the rest of this section, assume T is in TNF and fix an I\u03c3-consistent, atom-based c-map C for T over \u03c3. We call a formula of the form \u03d5\u2228\u03c8 or \u2203x \u03d5 a disjunctive formula. Vice versa, a conjunctive formula is a formula of the form \u03d5 \u2227 \u03c8 or \u2200x \u03d5.\nWe now present a simple \u201ctop-down style\u201d grounding algorithm that exploits bounds without constructing C\u3008T \u3009 \u222a CA explicitly. The algorithm is shown in Algorithm 1. Basically, it consults the bounds assigned by C whenever it substitutes the free variables of a formula \u03d5[x] by domain constants d. If according to the bounds, \u03d5[x/d] is certainly true, i.e., I\u03c3[x/d] |= Cct(\u03d5), then the grounding of \u03d5[x/d] is not computed. Instead, the algorithm then proceeds as if \u03d5[x/d] is equal to >. Similarly if \u03d5[x/d] is certainly false. In this way, the algorithm avoids creating unnecessary instantiations. One can check that if C is the trivial c-map, Algorithm 1 reduces to a straightforward top-down style grounding algorithm that produces Grfull(T ).\nLine 1 of Algorithm 1 checks whether one of the sentences of T is certainly false. If this is the case, then clearly T is unsatisfiable (cf. Definition 10), and this can be reported immediately. Before a sentence is grounded, line 4 checks whether this sentence is certainly true according to C. Only sentences that are not certainly true are grounded. Observe that both checks are simple syntactic checks and can be executed in constant time.\nFunction groundConj gets as input a formula \u03d5[x] and returns a grounding for \u2200x \u03d5[x]. In particular, if \u03d5 is a sentence, then the result of applying groundConj to \u03d5 is a grounding for \u03d5.\nIn groundConj, universal quantifiers are implicitly pushed inside conjunctions. That is, if \u03d5[x] is a conjunction \u03c81 \u2227 . . . \u2227 \u03c8n, then for every i \u2208 [1, n], the grounding of \u2200x \u03c8i is computed by applying groundConj to \u03c8i. The conjunction of these groundings is returned as grounding for \u2200x \u03d5. According to equivalence (6) of Section 2.2, this transformation yields an equivalent formula.\nFunction groundConj only consults the c-map when variables are substituted by domain constants or when the input formula is an atom. As such, groundConj ignores (\u201celiminates\u201d) the bounds assigned to conjunctive formulas. As we mentioned at the end of Section 4.4.2, this is important to avoid repeated constraints on a variable.\nIn groundConj(\u03d5[x]), only those substitutions \u03d5[x/d] for which I\u03c3[x/d] 6|= Cct(\u03d5) are grounded (see, e.g., line 12). Indeed, the other substitutions yield a formula that is certainly true in all models of T expanding I\u03c3, and can therefore be omitted from the ground conjunction C that is computed.\nAlgorithm 1: Ground with Bounds Input: T , \u03c3, I\u03c3 and C Output: A grounding Tg for T with respect to I\u03c3 if Ccf(\u03d5) = > for some sentence \u03d5 of T then return \u22a5;1 Tg := \u2205;2 // Ground all sentences of T for every sentence \u03d5 of T do3 if Cct(\u03d5) 6= > then Add groundConj(\u03d5) to Tg;4\n// Ground all definitions of T for every definition \u2206 of T do5 Add groundDef(\u2206) to Tg;6\n// Add the grounding of CA for every atomic subformula \u03d5[x] of T do7 for every d such that I\u03c3[x/d] |= Cct(\u03d5) do8 Add \u03d5[x/d] to Tg;9\nfor every d such that I\u03c3[x/d] |= Ccf(\u03d5) do10 Add \u00ac\u03d5[x/d] to Tg;11\nreturn Tg;12\nFunction groundConj(\u03d5[x])\nC := \u2205;1 switch \u03d5[x] do2 case \u03d5 is a literal3 for all d such that I\u03c3 6|= Cct(\u03d5)[x/d] do4 if I\u03c3 |= Ccf(\u03d5)[x/d] then return \u22a5;5 else Add \u03d5[x/d] to C;6\ncase \u03d5 = \u2200y \u03c8[x, y]7 return groundConj(\u03c8[x, y]);8\ncase \u03d5 = \u2227 i \u03c8i9\nC := \u22c3 i groundConj(\u03c8i);10\ncase \u03d5 is a disjunctive formula11 for all d such that I\u03c3 6|= Cct(\u03d5)[x/d] do12 if I\u03c3 |= Ccf(\u03d5)[x/d] then return \u22a5;13 else Add groundDisj(\u03d5[x/d]) to C;14\nreturn \u2227 C;15\nFunction groundDisj(\u03d5[x])\nD := \u2205;1 switch \u03d5[x] do2 case \u03d5 is a literal3 for all d such that I\u03c3 6|= Ccf(\u03d5)[x/d] do4 if I\u03c3 |= Cct(\u03d5)[x/d] then return >;5 else Add \u03d5[x/d] to D;6\ncase \u03d5 = \u2203y \u03c8[x, y]7 return groundDisj(\u03c8[x, y]);8\ncase \u03d5 = \u2228 i \u03c8i9\nD := \u22c3 i groundDisj(\u03c8i);10\ncase \u03d5 is a conjunctive formula11 for all d such that I\u03c3 6|= Ccf(\u03d5)[x/d] do12 if I\u03c3 |= Cct(\u03d5)[x/d] then return >;13 else Add groundConj(\u03d5[x/d]) to D;14\nreturn \u2228 D;15\nFunction groundDef(\u2206)\n\u2206g := \u2205;1 for every rule \u2200x (P (x)\u2190 \u03d5[y]) in \u2206 do2 z := x \\ y;3 for every d such that I\u03c3 6|= Ccf(\u03d5[y/d]) do4 if I\u03c3 |= Cct(\u03d5[y/d]) then \u03d5g := >;5 else \u03d5g := groundConj(\u03d5[y/d]);6 n := the number of variables in z;7 Add P (x)[y/d, z/d \u2032 ]\u2190 \u03d5g to \u2206g for every d \u2032 \u2208 Dn;8\nreturn \u2206g;9\nBefore \u03d5[x/d] is grounded, it is checked whether this substitution yields a formula that is certainly false (see, e.g., line 13). If this is the case, the whole conjunction C will certainly be false, and therefore \u22a5 is returned immediately. Observe that implicitly the formula Cct(\u03d5) \u2228 (\u00acCcf(\u03d5) \u2227 \u03d5) is grounded. Hence the correctness of groundConj follows from Lemma 13.\nFunction groundDisj is dual to groundConj. On input \u03d5[x], it returns a grounding for \u2203x \u03d5[x]. It implicitly pushes existential quantifiers through disjunctions and eliminates the bounds assigned to disjunctive formulas.\nFunction groundDef returns a grounding for its input definition \u2206. It grounds the rules of \u2206 one-by-one. For each rule \u2200x (P (x) \u2190 \u03d5[y]), only those substitutions \u03d5[y/d] that are possibly true are tried (line 4). If \u03d5[y/d] is certainly true, it is replaced by > (line 5).\nIn lines 7-11 of Algorithm 1, the theory CA is grounded. Recall that this is necessary to obtain a grounding that is I\u03c3-equivalent to T (see Proposition 21). Observe that if C is the trivial c-map, no output is produced when lines 7-11 are executed.\nThe computationally expensive steps in Algorithm 1 are the steps where the truth values in I\u03c3 of (some of the) bounds assigned by C are computed. For large bounds, these steps can become infeasible. Indeed, the expression complexity of FO is PSPACE-complete (Stockmeyer, 1974). As such, grounding with too complex bounds may take more time and space than constructing the full grounding and simplifying it afterwards. The stop criterion of Section 6.2.3 for the refinement algorithm is designed to avoid too complex bounds. Our experiments in Section 6.3 show that carefully restricting the complexity of the bounds leads to faster grounding.\nWe stress that Algorithm 1 is just one example of a grounding algorithm that exploits bounds.4 The principle of consulting bounds as soon as possible can be applied to adapt other grounding algorithms as well. For example, recall that a bottom-up style grounder starts by storing all instances of atomic subformulas of T in a table. To exploit bounds efficiently, a bottom-up grounder should consult the bounds while constructing these tables and leave out, e.g., all instances that are certainly false. As such, it avoids unnecessary large tables, which in turn improves the speed of the subsequent grounding steps."}, {"heading": "6.2 Implementing the Refinement Algorithm and Querying Bounds", "text": "In this section we discuss some aspects of implementing the refinement algorithm. As mentioned above, applying a simplification method for first-order formulas to simplify the bounds at regular time points is essential for a good implementation. One can use Goubault\u2019s (1995) method for this purpose. To this end, the bounds need to be represented by first-order binary decision diagrams. We show in this section that such a representation can be applied without too much overhead when applying one-step refinements. Moreover, using binary decision diagrams leads to extra benefits: we obtain a cheap equivalence check for bounds and an elegant algorithm to query bounds, which is needed to implement Algorithm 1. At the end of this section we discuss a stop criterion for the refinement algorithm and we discuss an implementation."}, {"heading": "6.2.1 First-Order Binary Decision Trees and Diagrams", "text": "We borrow the definition of first-order BDDs from Goubault (1995). Let \u03d5, \u03c81 and \u03c82 be three formulas. The ternary if-then-else operator is denoted by \u201c_\u201d, and defined by \u03d5 _ \u03c81;\u03c82 := (\u03d5 \u2227 \u03c81) \u2228 (\u00ac\u03d5 \u2227 \u03c82). The formula \u03d5 _ \u03c81;\u03c82 is also represented by the graph shown in Figure 3.\nDefinition 45 (Goubault, 1995). FO binary decision trees (BDTs) and kernels are defined by simultaneous induction:\n\u2022 An atom is a kernel;\n4. The question whether top-down grounders can be made more efficient than bottom-up grounders is outside the scope of this paper, and still undecided.\n\u03d5\n||y y\ny y\n\"\"E EE\nEE EE\nE\n\u03c82 \u03c81\nObserve that the graph representation of a BDT is a tree whose nodes are atoms or existentially quantified BDTs.\nGoubault (1995) showed that for every FO formula \u03d5 there exists a BDT \u03d5\u2032 such that \u03d5 and \u03d5\u2032 are equivalent. In an actual implementation, sharing, reducing and ordering are applied to obtain a simplified and compact representation of BDTs. Such representations are called reduced ordered binary decision diagrams (BDDs). Sharing means that isomorphic subtrees are stored at the same address in memory. Reducing involves exhaustively replacing subtrees of the form \u03d5 _ \u03c8;\u03c8 by \u03c8. A BDT \u03d5 is ordered if the kernels appear in some fixed order on every path in the graph representation of \u03d5.\nAs mentioned above, there are several important benefits of using BDDs to represent bounds for a formula:\n\u2022 An implementation of the refinement algorithm using BDDs allows us to use the simplification algorithm for BDDs of Goubault (1995).\n\u2022 As explained in Section 4.4, to detect that the refinement algorithm has reached a fixpoint, one needs to check the equivalence of bounds. Often, the BDDs representing two equivalent formulas will be equal.5 Hence, a cheap (but necessarily incomplete) equivalence check for two bounds consists of checking the syntactic equality of the two BDDs representing them. Since equal BDDs are stored at the same address, this check is done in constant time.\n\u2022 As we will show in Section 6.2.2, querying a bound \u03d5[x], i.e., finding all tuples d such that I\u03c3[x/d] |= \u03d5, can easily be implemented directly on a BDD representation of \u03d5. Querying a bound is one of the main operations performed by a grounding algorithm that exploits bounds directly (such as Algorithm 1).\nOn the other hand, using BDDs does not result in too much overhead when computing a c-map. If \u03d5, \u03c8 and \u03c7[x, y] are represented by BDDs, then a BDD representing \u00ac\u03d5, \u2203x \u03d5, \u2200x \u03d5, \u03d5 \u2227 \u03c8, \u03d5 \u2228 \u03c8 and \u03c7[x/x\u2032, y] can be computed efficiently (Bryant, 1986; Goubault, 1995). This implies that every one-step refinement on a c-map C can be implemented efficiently, even if the bounds assigned by C are BDDs."}, {"heading": "6.2.2 Querying a Bound", "text": "In Algorithm 1, the main operation performed on a bound \u03d5[x] is querying: finding tuples d of domain constants such that I\u03c3 |= \u03d5[x/d]. Finding a tuple d such that I\u03c3 6|= \u03d5[x/d] corresponds to querying \u00ac\u03d5. We now show that querying a bound \u03d5[x] can be done directly on the BDD representation by a simple backtracking algorithm.\n5. For propositional BDDs, this is always the case.\nThe idea is to traverse the BDD, starting from the root, and trying to end up in the leaf >. At each inner node \u03c8[y] _ \u03c81;\u03c82, the free variables in that node are replaced by domain constants dy. If I\u03c3 |= \u03c8[y/dy], the algorithm continues via \u03c81, otherwise via \u03c82. If it ends up in \u22a5, it backtracks. If on the other hand, it ends up in >, the performed substitutions constitute an answer for \u03d5.\nFunction query implements the sketched query algorithm. It gets a bound \u03d5[x] as input and returns a substitution [x/d] such that I\u03c3 |= \u03d5[x/d]. If no such substitution exists, it returns FAIL. This algorithm can easily be adapted to return all answers to \u03d5[x] instead of just one.\nFunction query(\u03d5[x])\nif \u03d5 = > then return the empty substitution;1 else if \u03d5 = \u03c8[y] _ \u03c81;\u22a5 then2 for every tuple d such that I\u03c3 |= \u03c8[y/d] do3 \u03b8 := query(\u03c81[y/d]);4 if \u03b8 6= FAIL then return \u03b8 \u222a [y/d]5\nelse if \u03d5 = \u03c8[y] _ \u22a5;\u03c82 then6 for every tuple d such that I\u03c3 6|= \u03c8[y/d] do7 \u03b8 := query(\u03c82[y/d]);8 if \u03b8 6= FAIL then return \u03b8 \u222a [y/d]9\nelse if \u03d5 is of the form \u03c8[y] _ \u03c81;\u03c82 then10 for every tuple d \u2208 D|y| do11 if I\u03c3 |= \u03c8[y/d] then \u03b8 := query(\u03c81[y/d]);12 else \u03b8 := query(\u03c82[y/d]);13 if \u03b8 6= FAIL then return \u03b8 \u222a [y/d]14\nreturn FAIL;15\nIn lines 3 and 7, the algorithm needs to find tuples d such that respectively I\u03c3 |= \u03c8[y/d] and I\u03c3 6|= \u03c8[y/d]. If \u03c8[y] is an atom P (y), this can be implemented by consulting the table P I\u03c3 . If \u03c8 is a kernel \u2203x \u03c7[x, y], function query can be applied recursively to find the tuples. Indeed, any answer (d\u2032, d) to \u03c7[x, y] provides a tuple d such that I\u03c3 |= \u03c8[y/d]. Vice versa, I\u03c3 6|= \u03c8[y/d] if \u03c7[x, y/d] has no answer.\nWe illustrate the query algorithm on an example.\nExample 15. Let \u03d5[x, y] be the BDD shown in figure 4, and let {a, b} be the domain of I\u03c3, P I\u03c3 = {b}, RI\u03c3 = {} and QI\u03c3 = {(b, b)}. To find an answer for \u03d5[x, y], the query algorithm starts at the root P (x). Since none of its children are equal to \u22a5, every domain constant is tried. Assume domain constant a is tried first. Because a 6\u2208 P I\u03c3 , the algorithm continues with node R(a) _ >;\u22a5. Because the \u201celse\u201d child of this node is \u22a5 and a 6\u2208 RI\u03c3 , the algorithm returns to the root and tries\ndomain element b. Since b \u2208 P I\u03c3 , it goes to node Q(b, y) _ >;\u22a5. Since the \u201celse\u201d child of this node is \u22a5, the algorithm tries those substitutions d for y such that (b, y/d) \u2208 QI\u03c3 . Thus, y is substituted by b. Finally, answer [x/b, y/b] is returned."}, {"heading": "6.2.3 A Stop Criterion for the Refinement Algorithm", "text": "As shown in Section 4.4, the c-map refinement algorithm does not reach a fixpoint on certain inputs. Also, even in the case a fixpoint can be found, computing it may take a long time, and the bounds assigned by the fixpoint can be so complex that querying becomes very inefficient. Using such bounds may severely slow down grounding. This indicates the need for a good stop criterion.\nSimple Stop Criteria A very simple stop criterion limits the number of one-step refinements that may be performed to a given maximum number m. This m may depend on the theory T . For instance, m can be set to C \u00d7 (number of subformulas in T ), where C is some fixed constant.\nA slightly less naive technique, which can be combined with the previous, limits the \u201ccomplexity\u201d of the bounds by putting a fixed upper bound N on the number of nodes the BDD representation of a bound may have. If a one-step refinement would lead to a new bound with more nodes than N , this refinement is not performed. As this limits the number of applicable one-step refinements, the probability of reaching a fixpoint increases.\nStop Criteria via Estimators The experiments we present in Section 6.3 indicate that there exist appropriate values for C and N that produce positive results on most of the examples. Still, on some problems, grounding slows down severely, while the size of the produced grounding does not decrease. One of these problems is the following clique problem (entry 6 in Table 4).\nExample 16. Recall that a clique is a maximally connected graph. Let\n\u03c3 = \u3008{Edge/2}, \u2205\u3009, \u03a3 = \u3008\u03c3P \u222a {Clique/1}, \u2205\u3009\nand T the theory\n\u2200x\u2200y (Clique(x) \u2227 Clique(y) \u2283 (x = y \u2228 Edge(x, y))). \u2200x ((\u2200y (Clique(y) \u2227 x 6= y \u2283 Edge(x, y))) \u2283 Clique(x)).\nIf EdgeI\u03c3 is symmetric, i.e., I\u03c3 represents an undirected graph, a model of T expanding I\u03c3 is a clique in I\u03c3 that is not contained in a strictly larger clique in I\u03c3. Within a small number of iterations, the refinement algorithm finds for Clique(x) the ct-bound \u2200x\u2032 x 6= x\u2032 \u2283 Edge(x, x\u2032). This formula expresses that Clique(x) is certainly true in every solution if x is directly connected to every other vertex in the input graph. Clearly, for most graphs, no vertex satisfies this condition. So, for most graphs, \u22a5 would be an equally precise ct-bound, but would allow much faster querying.\nThe situation is worse for the cf-bound for Clique(x). Since for an undirected graph, every single vertex is a clique, and thus occurs in at least one of the solutions, the cf-bound is necessarily unsatisfiable with respect to T . Yet, our implementation of the refinement algorithm came up with \u2203x\u2032 (\u00acEdge(x, x\u2032) \u2227 x 6= x\u2032 \u2227 (\u2200x\u2032\u2032 (x\u2032 6= x\u2032\u2032 \u2283 Edge(x\u2032, x\u2032\u2032)))) as cf-bound. The query algorithm outlined above takes cubic time in the number of vertices to find out that no x satisfies this formula.\nTo avoid the problems illustrated by the example above, one could estimate the reward of a bound versus the cost of evaluating it. Recall that more precise bounds yield smaller grounding sizes. Therefore, the reward of a bound \u03c8 is dictated by its precision. Given I\u03c3, it is possible to find a good estimate for the number of answers to \u03c8 in I\u03c3 (Demolombe, 1980), which is in turn a measure for the precision of \u03c8. For a fixed query algorithm, one can also estimate the cost cost(\u03c8) of computing an answer in I\u03c3 to a query \u03c8. In the following, we assume that the reward of a bound is a positive real number, and its cost a strictly positive real number.\nGiven the reward and the cost of bounds, the complexity of a bound \u03c8 can be limited by restricting the ratio\nr(\u03c8) := cost(\u03c8)\nreward(\u03c8) + 1 .\nIf a one-step refinement would replace a bound \u03c81 by \u03c82, but r(\u03c81) < r(\u03c82), then this refinement is not performed. Clearly, for all bounds \u03c8 assigned by a c-map C computed according to this restriction, r(\u03c8) \u2264 r(\u22a5) holds. Observe that to apply this restriction, an input structure I\u03c3 is needed. However, the obtained bounds are independent of I\u03c3.\nIt is beyond the scope of this paper to describe in detail estimators for the reward and cost of bounds. The fairly naive estimator used for the experiments in the next section assigns ratios of the order O(|DI\u03c3 |), respectively O(|DI\u03c3 |3), to the ct-bound, respectively cf-bound, mentioned in Example 16. As such, if |DI\u03c3 | is large enough, these bounds will be avoided."}, {"heading": "6.2.4 Implementation of the Refinement Algorithm", "text": "Our implementation of the refinement algorithm, including the heuristic for choosing refinement bounds (Section 4.4.1) and stop criterion, is presented by Algorithm 6. The algorithm maintains a queue Q of one-step refinements that will be applied. Each of these is represented by a tuple \u3008r, \u03d5\u3009, where r is the type of the refinement, e.g., axiom refinement, and \u03d5 the formula on which r will be applied.\nAlgorithm 6: Refinement Algorithm Q := \u2205; C := the trivial c-map for T ;1 for all sentences \u03d5 of T do Q.push(\u3008axiom, \u03d5\u3009);2 for all subformulas \u03d5 of T over \u03c3 do3 Q.push(\u3008ct-input, \u03d5\u3009); Q.push(\u3008cf-input, \u03d5\u3009);4 while Q 6= \u2205 and the maximum number of refinements is not reached do5 \u3008r, \u03d5\u3009 := Q.pop();6 if r is a ct-refinement then7\n\u03c8 := the r-refinement bound for \u03d5 with respect to C;8 \u03c8 := simplify(Cct(\u03d5) \u2228 \u03c8);9 if \u03c8 6= Cct(\u03d5) and \u03c8 is not too complex then10 Cct(\u03d5) := \u03c8;11 for all \u3008r, \u03c7\u3009 such that the r-refinement bound for \u03c7 contains Cct(\u03d5) do12 Q.push(\u3008r, \u03c7\u3009);13\nelse14 . . . // Similar code for cf-refinements15\nreturn C;16\nAs explained in Section 4.4.1, our implementation starts by scheduling all possible axiom- and input-refinements. If in a later stage a bound is changed (line 11), then all refinement bounds that contain this bound are scheduled to be applied (line 13). For example, assume that T contains the formula \u03d5 \u2227 \u03c8 and that the ct-bound of \u03d5 is refined. Then bottom-up ct-refinement for \u03d5 \u2227 \u03c8 is scheduled since the bottom-up ct-refinement bound for that formula is given by Cct(\u03d5) \u2227 Cct(\u03c8), which contains Cct(\u03d5). For the same reason also top-down cf-refinement for \u03c8 is scheduled.\nThe algorithm applies all scheduled refinements, unless the maximum number of refinement steps is reached (line 5). The other part of the discussed stop criterion is applied in line 10. If the newly\ncomputed bound \u03c8 is too complex, i.e., its BDD representation contains too many nodes or the ratio r(\u03c8) is above a certain threshold, \u03c8 is not used.\nIf BDDs are used to represent the bounds assigned by C, line 8 can be implemented in linear time in the size of C. If we use Goubault\u2019s simplification algorithm for BDDs for implementing line 9, the worst case complexity of this step is non-elementary in the size of Cct(\u03d5) \u2228 \u03c8 (Goubault, 1995). The estimators we used to implement line 10 take linear time in the size of \u03c8. It may seem that the complexity of the simplification method limits the practical applicability of Algorithm 6. However, since large BDDs usually do not pass the test in line 10, the simplification method is rarely applied on large BDDs. In the experiments of the next section, the running time of the refinement algorithm is negligible compared to the running time of the grounding algorithm."}, {"heading": "6.3 Experiments", "text": "We implemented Algorithm 1 and Algorithm 6, using BDDs to represent bounds. The resulting grounder is called GidL. In this section, we present experiments, obtained with GidL, that show the impact of using bounds on grounding size and time.\nAs input for GidL, we used 37 benchmark problems, mainly taken from Asparagus.6 The details about the experiments are available at http://dtai.cs.kuleuven.be/krr/software.html. We used four different versions of GidL:\nGidLnb: Assigns \u3008\u03d5,\u00ac\u03d5\u3009 as bound to every atomic subformula \u03d5 over the input vocabulary, and \u3008\u22a5,\u22a5\u3009 to every other subformula. As such, it creates the reduced grounding of the input theory.\nGidLbu: Assigns \u3008\u03d5,\u00ac\u03d5\u3009 as bound to every atomic subformula \u03d5 over the input vocabulary and then applies bottom-up refinements to obtain a bottom-up c-map.\nGidLmn: Limits the refinement algorithm to 4\u00d7(number of subformulas in T ) one-step refinements and allows a maximum of 4 internal nodes in each BDD used to represent the bounds. According to previous experiments (Wittocx et al., 2008b), this is the best setting when limiting the number of nodes.\nGidLr: Limits the refinement algorithm to 4\u00d7 (number of subformulas in T ) one-step refinements. It limits the complexity of the derived bounds by estimating the number of answers and the cost, as described in the previous section.\nIn Table 3, the influence of bounds on the grounding size is shown. The second and third column show the ratio of the grounding size obtained with GidLmn and GidLr compared to Grred(T ). For GidLnb and GidLbu, this ratio is always equal to 1. When interpreting Table 3, it is important to note that small reductions in grounding size are not important. The reason being that all reductions that can be obtained by the refinement algorithm are also obtained by applying unit propagation on the grounding (see Section 7 for a discussion). Since there exist very efficient implementations of unit propagation, it is not beneficial to let the refinement algorithm find small reductions at a relatively high cost. We see that both GidLmn and GidLr reduce the grounding size with more than 50% in around 30% of the benchmarks. In 7, respectively 6, of the benchmarks there is a spectacular reduction of more than 95%.\nMore important than reductions in size are reductions in grounding time. Table 4 shows the running times of the different versions of GidL, and (between brackets) the ratio of the running time to the running time of GidLnb. The running time of the refinement algorithm is included (it never took more than 0.02 seconds). A time-out (###) of 600 seconds was used.\nOn many benchmarks, the reduction in grounding time with respect to GidLnb is due to the reduction in grounding size. Yet there are also several benchmarks where time decreases a lot, while\n6. http://asp.haiti.cs.uni-potsdam.de/\nthere is almost no reduction in size. This is mostly due to the creation of a bottom-up c-map, as can be seen from the running times of GidLbu. Applying bottom-up refinements leads to the assignment of non-trivial bounds to non-atomic subformulas. This allows for earlier pruning by a top-down style grounder, and hence faster grounding.\nFrom Table 4, we can see that GidLmn performs quite well. On half of the benchmarks, it is more than 44% faster than GidLnb. It is also more than 20% faster than GidLbu on half of the benchmarks. There are some outliers however. On benchmarks 6 and 11, it is far slower than GidLbu, while not producing a significantly smaller grounding. This indicates the use of a complex bound with relatively small reward. Compared to GidLmn, GidLr is faster and more robust, indicating that using estimators for the reward and cost of bounds pays off in most cases. In only two of the benchmarks, our naive estimator makes a wrong guess. In benchmark 1, a bound with high cost and no reward is allowed, in benchmark 7, a bound with low cost and high reward is not allowed by GidLr. It is part of future work to implement improved estimators.\nWe conclude from our experiments that grounding with bounds is applicable in practice. It often leads to smaller grounding sizes on standard benchmark problems, and if the bounds are carefully restricted, it yields a significant speed up. Since the time to compute bounds is small compared to the overall grounding time, computing them is essentially for free.\nIn general, a smaller grounding does not necessarily lead to faster propositional model generation. For example, grounding size (and time) increases when symmetry breaking formulas are added, but these formulas may drastically improve the overall solving time (Torlak & Jackson, 2007). Another example are clause-learning SAT solvers: the clauses learnt by these solvers are redundant, but may improve the solving time by orders of magnitude. The question arises whether our method of grounding with bounds may lead to slower overall model generation time compared to grounding without bounds. This is not the case. The experiments above show that in general, grounding with bounds is faster than grounding without bounds. Since grounding with bounds also produces smaller groundings, the subsequent initialization phase of the SAT solver is executed faster. If T1 and T2 are two groundings obtained by grounding the same input theory and structure with, respectively without bounds, it can be shown7 that the typical simplification steps applied in this initialization phase transform T1 and T2 in exactly the same simplified theory T3. Thus, after initialization, the SAT solver is applied on exactly the same theory, whether or not the grounder used bounds. It follows that in general, the overall model generation time does not increase when bounds are applied while grounding."}, {"heading": "7. Related Work", "text": "In the previous sections we described a method to obtain fast and compact grounding. Several such methods have been described in the literature. Some of them are \u2014 like ours \u2014 preprocessing techniques that rewrite the input theory. Other techniques involve reasoning on the propositional level. In this section we provide an overview. We indicate which ones can be applied to improve GidL. We also give an overview of existing grounders."}, {"heading": "7.1 Methods to Optimize Grounding", "text": "Derivation of Bounds To our knowledge, the methods proposed in the literature to derive bounds are less general than the one we presented in this paper. This is illustrated by Table 5, where we show for several grounders the impact of manually adding redundant information. For all the grounders in this table except GidL, manually adding redundancy may have a serious impact. For some grounders, the need to add redundancy can sometimes be avoided by writing the input theory in a specific format. For example, the grounder gringo (Gebser et al., 2007) uses a syntactic check to derive bounds: it derives that predicate q of the input vocabulary is a bound for predicate p if p\n7. The exact formulation and the proof of the property are beyond the scope of this paper."}, {"heading": "37 Wire routing 0.92 0.99", "text": ""}, {"heading": "36 Weight bounded dominating set 1.00 1.00", "text": ""}, {"heading": "35 Waterbucket 0.36 0.36", "text": ""}, {"heading": "34 Train scheduling 0.25 0.25", "text": ""}, {"heading": "33 Toughnut 0.00 0.00", "text": ""}, {"heading": "32 Tarski 1.00 1.00", "text": ""}, {"heading": "31 Sudoku 0.75 0.75", "text": ""}, {"heading": "30 Spanningtree 0.06 0.06", "text": ""}, {"heading": "29 Solitaire 1.00 0.73", "text": ""}, {"heading": "28 Sokoban 0.59 0.59", "text": ""}, {"heading": "27 Social golfer 1.00 1.00", "text": ""}, {"heading": "26 Slitherlink 0.04 0.04", "text": ""}, {"heading": "25 Disjunctive scheduling 0.83 0.83", "text": ""}, {"heading": "24 Pigeonhole 1.00 1.00", "text": ""}, {"heading": "23 N-queens 1.00 1.00", "text": ""}, {"heading": "22 Missionaries 0.03 0.03", "text": ""}, {"heading": "21 Mirror puzzle 1.00 1.00", "text": ""}, {"heading": "20 Maze generation 0.90 0.90", "text": ""}, {"heading": "19 Magic series 1.00 1.00", "text": ""}, {"heading": "18 Labyrinth 0.99 0.99", "text": ""}, {"heading": "17 Knighttour 0.00 0.00", "text": ""}, {"heading": "16 Tower of Hanoi 1.00 1.00", "text": ""}, {"heading": "15 Hamiltonian circuit 0.01 0.01", "text": ""}, {"heading": "14 Algebraic groups 0.99 1.00", "text": ""}, {"heading": "13 Graph partitioning 0.94 1.00", "text": ""}, {"heading": "12 Golomb ruler 0.54 1.00", "text": ""}, {"heading": "11 FO-hamcircuit 0.94 0.99", "text": ""}, {"heading": "10 Fastfood 1.00 1.00", "text": "is defined by a choice rule of the form, e.g., {p(X)} :- q(X). However, if this rule is replaced by {p(X)} :- dom(X), where dom denotes the domain, and the constraint :- p(X),not q(X),dom(X) is added, q is still a bound for p, but this is not detected by gringo, as can be seen in Table 5.\nThe grounder of the dlv system (Perri et al., 2007) may derive bounds by reasoning on the propositional level. As we explain below, the order in which rules and constraints are grounded is of crucial importance for such a method to pay off. Since dlv grounds rules before constraints, using a constraint to state that q is a bound for p does not improve grounding time.\nPropagation on the Propositional Level One of the techniques to produce smaller groundings consists of applying a constraint propagation method on the ground theory Tg and replacing by >, respectively \u22a5, every ground literal that is derived to be true, respectively false. The resulting theory is then simplified. This technique is applied by the grounder psgrnd (East et al., 2006), which uses unit propagation (Davis & Putnam, 1960) and complete one-atom lookahead (Li & Anbulagan, 1997) as propagation methods. The latter is performed once the grounding is finished, the former is triggered each time a unit clause is added to the grounding. If an inconsistency is detected by unit propagation, the grounding process is terminated immediately. Observe that this technique yields small groundings but does not improve grounding speed, except for the (rare) case where the propagation method detects an inconsistency during grounding. Indeed, it does not avoid computing all ground instances of the formulas in the input theory.\nIf a propositional constraint propagation method is applied while the grounding is being constructed, the derived information could be used to refine bounds. For instance, if unit-propagation derives that the domain atom P (d1, . . . , dn) is true, then x1 = d1 \u2227 . . . \u2227 xn = dn is a ct-bound for P (x1, . . . , xn). These bounds could be used to speed up the construction of the rest of the grounding. For this method to be effective, however, some careful fine-tuning of the order in which sentences are grounded is required. It may even be necessary to alternatingly compute partial groundings of different sentences. To the best of our knowledge, this process has not been worked out or implemented with unit-propagation or one-atom lookahead as underlying propagation method. On the other hand, most ASP grounders apply it for the following limited propagation method: if all rules defining a predicate P are grounded, it is concluded that a domain atom P (d) is certainly true if it occurs in a ground rule of the form P (d) \u2190 >, and certainly false if it does not occur in the head of any ground rule. In this case, a good grounding order can be derived from the dependency graph of the input theory (e.g., Cadoli & Schaerf, 2005; Perri et al., 2007). In GidL, this strategy is implemented for grounding definitions.\nSharing A second technique is called sharing and consists of detecting subformulas in the ground theory Tg that occur more than once. If such a subformula \u03d5 is detected, all its occurrences in Tg are replaced by a new atom P , and the sentence P \u2261 \u03d5 is added. If \u03d5 is a large formula and occurs\noften in Tg, this may result in a significant grounding size reduction. Also, sharing improves the propagation in SAT solvers.\nShlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level. In GidL, we implemented a simple sharing technique using dynamic programming. We adapted function groundConj so that instead of returning a conjunction \u2227 C, it creates a new atom P , adds the\nsentence P \u2261 \u2227 C to the grounding, and returns P . If groundConj is applied multiple times on\nthe same input \u03d5, the same predicate P is returned each time, but P \u2261 \u2227 C is added only once. Function groundDisj is adapted in a similar fashion.\nClause splitting Clause splitting is a well-known rewriting technique applied in MACE style model generation (McCune, 2003). It consists of splitting a first-order clause\n\u2200x\u2200y\u2200z (\u03d51[x, z1] \u2228 \u03d52[y, z2]) (20)\nwhere x 6\u2208 z2, y 6\u2208 z1 and z = z1 \u222a z2 into two new clauses\n\u2200x\u2200z1 (\u03d51[x, z1] \u2228 S(z1 \u2229 z2)) (21) \u2200y\u2200z2 (\u00acS(z1 \u2229 z2) \u2228 \u03d52[y, z2]). (22)\nHere, S is a new predicate symbol. The full grounding of (20) is of the size O(|D|3), while the full grounding of (21) and (22) has only size O(|D|2).\nIf sharing is implemented by adapting the functions groundConj and groundDisj as explained above, the effect of clause splitting can be obtained by moving quantifiers according to the equivalences (4), (5), (8) and (9) of Section 2.2. For instance, we can apply equivalences (4) and (8) to replace (20) by \u2200x\u2200z (\u03d51\u2228(\u2200y \u03d52)). Grounding the latter while applying sharing has the same effect as clause splitting. Similarly, the grounding size of \u2203x\u2203y\u2203z (\u03d51[x, z1] \u2227 \u03d52[y, z2]) can be reduced by replacing this formula by \u2203x\u2203z (\u03d51 \u2227 (\u2203y \u03d52)).\nThe simple heuristic to guide clause splitting described by Claessen and So\u0308rensson (2003) can directly be applied to choose which quantifiers to move inside. We conclude that clause splitting could easily be incorporated in GidL.\nDatabase Techniques Several techniques for optimizing querying in databases can be used to optimize grounding. Examples are join-ordering strategies, backjumping and indexing techniques.\nOne of the most basic techniques to improve grounding speed consists of reordering (long) conjunctions or disjunctions of literals to speed up grounding. Which order is best depends on the grounding algorithm. Different strategies are described by, e.g, Leone, Perri, and Scarcello (2001), Syrja\u0308nen (1998, 2009) and in the database literature (Garcia-Molina, Ullman, & Widom, 2000). There is no problem implementing a similar technique in GidL. Also, reordering the nodes in the BDD representation of the bounds could optimize querying. It is part of future work to investigate such reordering strategies for BDDs.\nOne of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction \u03d51 \u2227 . . . \u2227 \u03d5n that are possibly true, given (an overestimation of) the possibly true instances of each of the conjuncts \u03d5i. In GidL, this backjumping technique is applied to implement line 12 of function groundDisj. Indeed, if \u03d5 is the formula \u03d51 \u2227 . . . \u2227 \u03d5n, then line 12 amounts to finding all possible instances of \u03d5, while the cf-bounds for \u03d51, . . . , \u03d5n provide an overestimation of the possibly true instances of these conjuncts. Similarly, the backjumping technique is applied to improve line 12 of groundConj, where all possibly false instances of a disjunction are calculated.\nCatalano, Leone, and Perri (2008) present an adaptation of indexing strategies for grounding.\nPartition-Based Reasoning Ramachandran and Amir (2005) describe a sophisticated grounding technique that can reduce the grounding size of FO theories, depending on the availability of some\ngraphical structure in these theories. This technique is not directly applicable in our case, since it produces groundings that are not necessarily I\u03c3-equivalent to the input theory. The only guarantee is that the ground theory is satisfiable iff the input problem is satisfiable."}, {"heading": "7.2 Grounders", "text": "A non-native approach to ground an MX(FO(ID)) problem consists of first translating it to an equivalent normal logic program under the well-founded semantics. This translation is described by Marie\u0308n et al. (2004). Next, a (slightly adapted) grounder for ASP is used to ground the logic program. This is the approach taken by MXidL (Marie\u0308n, Wittocx, & Denecker, 2006).\nThe first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.2.1). To construct a grounding of a sentence \u03d5, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for \u03d5 is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm.\nkodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of \u22a5. Similarly for the cf-bound. This leads to more efficient and compact groundings.\nmace (McCune, 2003) and paradox (Claessen & So\u0308rensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch.\nEast et al. (2006) developed the grounder psgrnd for MX(PSpb). PSpb is a fragment of FO(ID), extended with pseudo-boolean constraints. As explained above, psgrnd performs reasoning on the ground theory to reduce memory usage and grounding size. The experiments performed by East et al. (2006) show that carefully designed data structures are of key importance to build an efficient grounder.\nASP grounders take as input a normal logic program and transform it into an equivalent ground normal logic program. As such, these grounders do not deal with (deeply) nested formulas. Currently, there are three ASP grounders: lparse (Syrja\u0308nen, 2000; Syrja\u0308nen, 2009), gringo (Gebser et al., 2007) and the grounding component of dlv (Perri et al., 2007). All of them use techniques from database theory to perform grounding efficiently.\nFinally, we mention the grounder spec2SAT (Cadoli & Schaerf, 2005). Its input theories are in the np-spec language, a language with Datalog-like syntax and semantics based on model minimality. The grounding algorithm implemented in spec2SAT is basically a simplified version of the grounding algorithm of dlv.\nIt would be interesting to compare the efficiency of the above mentioned grounders experimentally. However, it is currently not possible to conduct such an experiment in a scientifically fair way. There are several reasons for this. First, all grounders have a different input language, making it impossible to run them on the same input. Also, there are several output languages for grounders. A richer output language leads to more compact and fast grounding. For instance, for some prob-\nlems, lparse\u2019s output size is necessarily cubic in the input domain size, while GidL\u2019s output format allows for quadratic size. Thirdly, even if the input and output languages of all grounders were the same, an expert could easily manipulate experiments by carefully choosing his modelling style. For example, if he does not manually add bounds to the input theories, GidL has an advantage. If bodies of rules are not ordered, dlv is more likely to produce good results. Etc. Finally, because of the large amount of data processed by grounders, carefully designed data structures and an optimized implementation of the core grounding algorithm is very important to achieve fast grounding (East et al., 2006). However, several of the above mentioned grounders are not yet optimized in that sense. As such, it is difficult to derive conclusions about grounding algorithms by experimentally comparing the efficiency of current implementations of these algorithms."}, {"heading": "8. Conclusions", "text": "We presented a method to compute for a given theory, upper and lower bounds for all subformulas of that theory. We showed how these bounds can be used for efficiently creating small groundings in the context of Model Expansion for FO and FO(ID). Our method frees a user from manually discovering bounds and adding them to a theory.\nWe presented a top-down style grounding algorithm that incorporates bounds. We discussed implementation issues and showed by experiments that our method works in practice: on many benchmark problems, it leads to significant reductions in grounding size and time.\nFuture work includes the extension of our algorithm to compute bounds for richer logics, such as, e.g., extensions of FO with aggregates and arithmetic. On the implementation side, we plan to use more sophisticated estimators to evaluate whether a computed bound is beneficial for grounding."}, {"heading": "Acknowledgments", "text": "Research supported by Research Foundation-Flanders (FWO-Vlaanderen) and by GOA 2003/08 \u201cInductive Knowledge Bases\u201d. Johan Wittocx is research assistant of the Research FoundationFlanders (FWO-Vlaanderen)."}], "references": [{"title": "Logic Programming and Nonmonotonic Reasoning, 9th International Conference, LPNMR 2007, Tempe", "author": ["C. Baral", "G. Brewka", "J.S. Schlipf"], "venue": "AZ, USA, May 15-17,", "citeRegEx": "Baral et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baral et al\\.", "year": 2007}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "Bryant,? \\Q1986\\E", "shortCiteRegEx": "Bryant", "year": 1986}, {"title": "Compiling problem specifications into SAT", "author": ["M. Cadoli", "A. Schaerf"], "venue": "Artificial Intelligence,", "citeRegEx": "Cadoli and Schaerf,? \\Q2005\\E", "shortCiteRegEx": "Cadoli and Schaerf", "year": 2005}, {"title": "On demand indexing for the DLV instantiator", "author": ["G. Catalano", "N. Leone", "S. Perri"], "venue": "Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP)", "citeRegEx": "Catalano et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Catalano et al\\.", "year": 2008}, {"title": "New techniques that improve MACE-style model finding", "author": ["K. Claessen", "N. S\u00f6rensson"], "venue": "In Workshop on Model Computation (MODEL)", "citeRegEx": "Claessen and S\u00f6rensson,? \\Q2003\\E", "shortCiteRegEx": "Claessen and S\u00f6rensson", "year": 2003}, {"title": "A computing procedure for quantification theory", "author": ["M. Davis", "H. Putnam"], "venue": "Journal of the ACM,", "citeRegEx": "Davis and Putnam,? \\Q1960\\E", "shortCiteRegEx": "Davis and Putnam", "year": 1960}, {"title": "Estimation of the number of tuples satisfying a query expressed in predicate calculus language", "author": ["R. Demolombe"], "venue": "In International Conference on Very Large Data Bases (VLDB),", "citeRegEx": "Demolombe,? \\Q1980\\E", "shortCiteRegEx": "Demolombe", "year": 1980}, {"title": "Extending classical logic with inductive definitions", "author": ["M. Denecker"], "venue": "J. (Eds.), International Conference on Computational Logic (CL),", "citeRegEx": "Denecker,? \\Q2000\\E", "shortCiteRegEx": "Denecker", "year": 2000}, {"title": "Inductive situation calculus", "author": ["M. Denecker", "E. Ternovska"], "venue": "Artificial Intelligence,", "citeRegEx": "Denecker and Ternovska,? \\Q2007\\E", "shortCiteRegEx": "Denecker and Ternovska", "year": 2007}, {"title": "A logic of nonmonotone inductive definitions", "author": ["M. Denecker", "E. Ternovska"], "venue": "ACM Transactions on Computational Logic (TOCL),", "citeRegEx": "Denecker and Ternovska,? \\Q2008\\E", "shortCiteRegEx": "Denecker and Ternovska", "year": 2008}, {"title": "Well-founded semantics and the algebraic theory of nonmonotone inductive definitions", "author": ["M. Denecker", "J. Vennekens"], "venue": null, "citeRegEx": "Denecker and Vennekens,? \\Q2007\\E", "shortCiteRegEx": "Denecker and Vennekens", "year": 2007}, {"title": "The second answer set programming competition", "author": ["M. Denecker", "J. Vennekens", "S. Bond", "M. Gebser", "M. Truszczy\u0144ski"], "venue": "International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR),", "citeRegEx": "Denecker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Denecker et al\\.", "year": 2009}, {"title": "Tools for modeling and solving search problems", "author": ["D. East", "M. Iakhiaev", "A. Mikitiuk", "M. Truszczy\u0144ski"], "venue": "AI Communications,", "citeRegEx": "East et al\\.,? \\Q2006\\E", "shortCiteRegEx": "East et al\\.", "year": 2006}, {"title": "A Mathematical Introduction To Logic (Second edition)", "author": ["H.B. Enderton"], "venue": null, "citeRegEx": "Enderton,? \\Q2001\\E", "shortCiteRegEx": "Enderton", "year": 2001}, {"title": "Generalized first-order spectra and polynomial-time recognizable sets", "author": ["R. Fagin"], "venue": "Complexity of Computation,", "citeRegEx": "Fagin,? \\Q1974\\E", "shortCiteRegEx": "Fagin", "year": 1974}, {"title": "Rete: A fast algorithm for the many patterns/many objects match problem", "author": ["C. Forgy"], "venue": "Artificial Intelligence,", "citeRegEx": "Forgy,? \\Q1982\\E", "shortCiteRegEx": "Forgy", "year": 1982}, {"title": "Database System Implementation. PrenticeHall", "author": ["H. Garcia-Molina", "J.D. Ullman", "J. Widom"], "venue": null, "citeRegEx": "Garcia.Molina et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Garcia.Molina et al\\.", "year": 2000}, {"title": "GrinGo : A new grounder for answer set programming", "author": ["M. Gebser", "T. Schaub", "S. Thiele"], "venue": "In Baral et al. (Baral et al.,", "citeRegEx": "Gebser et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gebser et al\\.", "year": 2007}, {"title": "A BDD-based simplification and skolemization procedure", "author": ["J. Goubault"], "venue": "Logic Journal of IGPL,", "citeRegEx": "Goubault,? \\Q1995\\E", "shortCiteRegEx": "Goubault", "year": 1995}, {"title": "Software Abstractions: Logic, Language, and Analysis", "author": ["D. Jackson"], "venue": null, "citeRegEx": "Jackson,? \\Q2006\\E", "shortCiteRegEx": "Jackson", "year": 2006}, {"title": "Pushing the envelope: Planning, propositional logic and stochastic search", "author": ["H.A. Kautz", "B. Selman"], "venue": "In National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence (AAAI/IAAI),", "citeRegEx": "Kautz and Selman,? \\Q1996\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1996}, {"title": "Comparative evaluation of approaches to propositionalization", "author": ["Krogel", "M.-A", "S. Rawles", "F. Zelezn\u00fd", "P.A. Flach", "N. Lavrac", "S. Wrobel"], "venue": "International Conference on Inductive Logic Programming (ILP),", "citeRegEx": "Krogel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Krogel et al\\.", "year": 2003}, {"title": "Improving ASP instantiators by join-ordering methods", "author": ["N. Leone", "S. Perri", "F. Scarcello"], "venue": "International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR),", "citeRegEx": "Leone et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Leone et al\\.", "year": 2001}, {"title": "Heuristics based on unit propagation for satisfiability problems", "author": ["C.M. Li"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Li,? \\Q1997\\E", "shortCiteRegEx": "Li", "year": 1997}, {"title": "Stable models and an alternative logic programming paradigm", "author": ["V.W. Marek", "M. Truszczy\u0144ski"], "venue": null, "citeRegEx": "Marek and Truszczy\u0144ski,? \\Q1999\\E", "shortCiteRegEx": "Marek and Truszczy\u0144ski", "year": 1999}, {"title": "Model Generation for ID-Logic", "author": ["M. Mari\u00ebn"], "venue": "Ph.D. thesis,", "citeRegEx": "Mari\u00ebn,? \\Q2009\\E", "shortCiteRegEx": "Mari\u00ebn", "year": 2009}, {"title": "On the relation between ID-Logic and Answer Set Programming", "author": ["M. Mari\u00ebn", "D. Gilis", "M. Denecker"], "venue": "European Conference on Logics in Artificial Intelligence (JELIA),", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2004}, {"title": "The IDP framework for declarative problem solving", "author": ["M. Mari\u00ebn", "J. Wittocx", "M. Denecker"], "venue": "In Search and Logic: Answer Set Programming and SAT,", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2006}, {"title": "MidL: a SAT(ID) solver", "author": ["M. Mari\u00ebn", "J. Wittocx", "M. Denecker"], "venue": "In 4th Workshop on Answer Set Programming: Advances in Theory and Implementation,", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2007}, {"title": "SAT(ID): Satisfiability of propositional logic extended with inductive definitions", "author": ["M. Mari\u00ebn", "J. Wittocx", "M. Denecker", "M. Bruynooghe"], "venue": "International Conference on Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2008}, {"title": "Mace4 reference manual and guide. CoRR, cs.SC/0310055", "author": ["W. McCune"], "venue": null, "citeRegEx": "McCune,? \\Q2003\\E", "shortCiteRegEx": "McCune", "year": 2003}, {"title": "A framework for representing and solving NP search problems", "author": ["D.G. Mitchell", "E. Ternovska"], "venue": null, "citeRegEx": "Mitchell and Ternovska,? \\Q2005\\E", "shortCiteRegEx": "Mitchell and Ternovska", "year": 2005}, {"title": "Model expansion as a framework for modelling and solving search problems", "author": ["D.G. Mitchell", "E. Ternovska", "F. Hach", "R. Mohebali"], "venue": "Tech. rep. TR 2006-24,", "citeRegEx": "Mitchell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2006}, {"title": "Logic programs with stable model semantics as a constraint programming paradigm", "author": ["I. Niemel\u00e4"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Niemel\u00e4,? \\Q1999\\E", "shortCiteRegEx": "Niemel\u00e4", "year": 1999}, {"title": "Grounding for model expansion in k-guarded formulas with inductive definitions", "author": ["M. Patterson", "Y. Liu", "E. Ternovska", "A. Gupta"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Patterson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2007}, {"title": "Reducing inductive definitions to propositional satisfiability", "author": ["N. Pelov", "E. Ternovska"], "venue": "International Conference on Logic Programming (ICLP),", "citeRegEx": "Pelov and Ternovska,? \\Q2005\\E", "shortCiteRegEx": "Pelov and Ternovska", "year": 2005}, {"title": "Enhancing DLV instantiator by backjumping techniques", "author": ["S. Perri", "F. Scarcello", "G. Catalano", "N. Leone"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Perri et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Perri et al\\.", "year": 2007}, {"title": "Compact propositional encodings of first-order theories", "author": ["D. Ramachandran", "E. Amir"], "venue": null, "citeRegEx": "Ramachandran and Amir,? \\Q2005\\E", "shortCiteRegEx": "Ramachandran and Amir", "year": 2005}, {"title": "A comparison of different techniques for grounding near-propositional cnf formulae", "author": ["S. Schulz"], "venue": "International Florida Artificial Intelligence Research Society Conference (FLAIRS),", "citeRegEx": "Schulz,? \\Q2002\\E", "shortCiteRegEx": "Schulz", "year": 2002}, {"title": "Exploiting subformula sharing in automatic analysis of quantified formulas", "author": ["I. Shlyakhter", "M. Sridharan", "R. Seater", "D. Jackson"], "venue": "Poster presented at Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Shlyakhter et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shlyakhter et al\\.", "year": 2003}, {"title": "The complexity of decision problems in automata and logic", "author": ["L.J. Stockmeyer"], "venue": "Ph.D. thesis,", "citeRegEx": "Stockmeyer,? \\Q1974\\E", "shortCiteRegEx": "Stockmeyer", "year": 1974}, {"title": "Implementation of local grounding for logic programs with stable model semantics", "author": ["T. Syrj\u00e4nen"], "venue": "Tech. rep", "citeRegEx": "Syrj\u00e4nen,? \\Q1998\\E", "shortCiteRegEx": "Syrj\u00e4nen", "year": 1998}, {"title": "Lparse 1.0 user\u2019s manual. http://www.tcs.hut.fi/Software/smodels/ lparse.ps.gz", "author": ["T. Syrj\u00e4nen"], "venue": null, "citeRegEx": "Syrj\u00e4nen,? \\Q2000\\E", "shortCiteRegEx": "Syrj\u00e4nen", "year": 2000}, {"title": "Logic Programs and Cardinality Constraints: Theory and Practice. Doctoral dissertation, TKK Dissertations in Information and Computer Science TKK-ICS-D12, Helsinki University of Technology, Faculty of Information and Natural Sciences, Department of Information and Computer Science, Espoo, Finland", "author": ["T. Syrj\u00e4nen"], "venue": null, "citeRegEx": "Syrj\u00e4nen,? \\Q2009\\E", "shortCiteRegEx": "Syrj\u00e4nen", "year": 2009}, {"title": "Kodkod: A relational model finder", "author": ["E. Torlak", "D. Jackson"], "venue": "International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS),", "citeRegEx": "Torlak and Jackson,? \\Q2007\\E", "shortCiteRegEx": "Torlak and Jackson", "year": 2007}, {"title": "Principles of database and knowledge-base systems, Vol. I", "author": ["J.D. Ullman"], "venue": null, "citeRegEx": "Ullman,? \\Q1988\\E", "shortCiteRegEx": "Ullman", "year": 1988}, {"title": "The well-founded semantics for general logic programs", "author": ["A. Van Gelder", "K.A. Ross", "J.S. Schlipf"], "venue": "Journal of the ACM,", "citeRegEx": "Gelder et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Gelder et al\\.", "year": 1991}, {"title": "The idp system. http://www.cs.kuleuven.be/~dtai/krr/ software/idpmanual.pdf", "author": ["J. Wittocx", "M. Mari\u00ebn"], "venue": null, "citeRegEx": "Wittocx and Mari\u00ebn,? \\Q2008\\E", "shortCiteRegEx": "Wittocx and Mari\u00ebn", "year": 2008}, {"title": "Approximate reasoning in first-order logic theories", "author": ["J. Wittocx", "M. Mari\u00ebn", "M. Denecker"], "venue": "International Conference on Knowledge Representation and Reasoning (KR),", "citeRegEx": "Wittocx et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wittocx et al\\.", "year": 2008}, {"title": "Grounding with bounds", "author": ["J. Wittocx", "M. Mari\u00ebn", "M. Denecker"], "venue": "AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wittocx et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wittocx et al\\.", "year": 2008}, {"title": "The idp system: a model expansion system for an extension of classical logic", "author": ["J. Wittocx", "M. Mari\u00ebn", "M. Denecker"], "venue": "In Workshop on Logic and Search (LaSh),", "citeRegEx": "Wittocx et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wittocx et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 30, "context": "Examples of systems that rely on grounding can be found in the area of finite first-order model generation (Claessen & S\u00f6rensson, 2003; McCune, 2003; East, Iakhiaev, Mikitiuk, & Truszczy\u0144ski, 2006; Mitchell, Ternovska, Hach, & Mohebali, 2006; Torlak & Jackson, 2007; Wittocx, Mari\u00ebn, & Denecker, 2008d).", "startOffset": 107, "endOffset": 302}, {"referenceID": 19, "context": "Such systems are in turn used as part of theorem provers (Claessen & S\u00f6rensson, 2003) and for lightweight software verification (Jackson, 2006).", "startOffset": 128, "endOffset": 143}, {"referenceID": 42, "context": "Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009).", "startOffset": 102, "endOffset": 209}, {"referenceID": 43, "context": "Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009).", "startOffset": 102, "endOffset": 209}, {"referenceID": 38, "context": "Methods like clause splitting (Schulz, 2002) and partitioning (Ramachandran & Amir, 2005) belong to this category.", "startOffset": 30, "endOffset": 44}, {"referenceID": 30, "context": "For instance, this is done in the areas of model generation (Claessen & S\u00f6rensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 21, "context": "For instance, this is done in the areas of model generation (Claessen & S\u00f6rensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al., 2003).", "startOffset": 163, "endOffset": 184}, {"referenceID": 19, "context": ", in the context of lightweight verification (Jackson, 2006).", "startOffset": 45, "endOffset": 60}, {"referenceID": 33, "context": "This idea of model generation as a declarative problem solving paradigm has been pioneered in the area of ASP (Marek & Truszczy\u0144ski, 1999; Niemel\u00e4, 1999).", "startOffset": 110, "endOffset": 153}, {"referenceID": 30, "context": "As shown by Mitchell and Ternovska (2005), it follows from Fagin\u2019s (1974) theorem that model expansion for FO captures NP, in the following sense:", "startOffset": 12, "endOffset": 42}, {"referenceID": 14, "context": "As shown by Mitchell and Ternovska (2005), it follows from Fagin\u2019s (1974) theorem that model expansion for FO captures NP, in the following sense:", "startOffset": 59, "endOffset": 74}, {"referenceID": 33, "context": "In this paper we focus on reductions that preserve all models, which is the setting in the ASP paradigm (Marek & Truszczy\u0144ski, 1999; Niemel\u00e4, 1999).", "startOffset": 104, "endOffset": 147}, {"referenceID": 36, "context": "The grounder of the dlv system (Perri et al., 2007) and the grounders gringo (Gebser et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 17, "context": ", 2007) and the grounders gringo (Gebser et al., 2007) and GidL (Wittocx, Mari\u00ebn, & Denecker, 2008b) take this approach.", "startOffset": 33, "endOffset": 54}, {"referenceID": 42, "context": "Examples of grounders with a bottom-up approach are lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al.", "startOffset": 59, "endOffset": 91}, {"referenceID": 43, "context": "Examples of grounders with a bottom-up approach are lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al.", "startOffset": 59, "endOffset": 91}, {"referenceID": 32, "context": "Examples of grounders with a bottom-up approach are lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al., 2006).", "startOffset": 133, "endOffset": 156}, {"referenceID": 7, "context": "In this section we will extend the refinement algorithm to FO(ID) (Denecker, 2000; Denecker & Ternovska, 2008).", "startOffset": 66, "endOffset": 110}, {"referenceID": 7, "context": "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)\u2019s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc.", "startOffset": 18, "endOffset": 48}, {"referenceID": 7, "context": "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)\u2019s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc. The semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics.", "startOffset": 18, "endOffset": 489}, {"referenceID": 7, "context": "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)\u2019s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc. The semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics. We borrow the presentation of this semantics from Denecker and Vennekens (2007).", "startOffset": 18, "endOffset": 709}, {"referenceID": 7, "context": "Denecker and Vennekens (2007) show that each terminal well-founded induction for \u2206 above \u0128 has the same limit, which corresponds to the wellfounded model of \u2206 extending \u0128|Open(\u2206), and is denoted by wfm\u2206(\u0128).", "startOffset": 0, "endOffset": 30}, {"referenceID": 7, "context": "MidL (Mari\u00ebn, Wittocx, & Denecker, 2007) and MiniSAT(ID) (Mari\u00ebn, Wittocx, Denecker, & Bruynooghe, 2008) take a native approach. Mari\u00ebn (2009) provides details on the specific form of propositional FO(ID) theories accepted by these solvers, and a method to transform arbitrary propositional FO(ID) theories into this form.", "startOffset": 25, "endOffset": 143}, {"referenceID": 15, "context": "But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.", "startOffset": 45, "endOffset": 58}, {"referenceID": 45, "context": "But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.", "startOffset": 99, "endOffset": 113}, {"referenceID": 40, "context": "Indeed, the expression complexity of FO is PSPACE-complete (Stockmeyer, 1974).", "startOffset": 59, "endOffset": 77}, {"referenceID": 18, "context": "One can use Goubault\u2019s (1995) method for this purpose.", "startOffset": 12, "endOffset": 30}, {"referenceID": 18, "context": "We borrow the definition of first-order BDDs from Goubault (1995). Let \u03c6, \u03c81 and \u03c82 be three formulas.", "startOffset": 50, "endOffset": 66}, {"referenceID": 18, "context": "Definition 45 (Goubault, 1995).", "startOffset": 14, "endOffset": 30}, {"referenceID": 18, "context": "Goubault (1995) showed that for every FO formula \u03c6 there exists a BDT \u03c6\u2032 such that \u03c6 and \u03c6\u2032 are equivalent.", "startOffset": 0, "endOffset": 16}, {"referenceID": 18, "context": "\u2022 An implementation of the refinement algorithm using BDDs allows us to use the simplification algorithm for BDDs of Goubault (1995).", "startOffset": 117, "endOffset": 133}, {"referenceID": 1, "context": "If \u03c6, \u03c8 and \u03c7[x, y] are represented by BDDs, then a BDD representing \u00ac\u03c6, \u2203x \u03c6, \u2200x \u03c6, \u03c6 \u2227 \u03c8, \u03c6 \u2228 \u03c8 and \u03c7[x/x\u2032, y] can be computed efficiently (Bryant, 1986; Goubault, 1995).", "startOffset": 141, "endOffset": 171}, {"referenceID": 18, "context": "If \u03c6, \u03c8 and \u03c7[x, y] are represented by BDDs, then a BDD representing \u00ac\u03c6, \u2203x \u03c6, \u2200x \u03c6, \u03c6 \u2227 \u03c8, \u03c6 \u2228 \u03c8 and \u03c7[x/x\u2032, y] can be computed efficiently (Bryant, 1986; Goubault, 1995).", "startOffset": 141, "endOffset": 171}, {"referenceID": 6, "context": "Given I\u03c3, it is possible to find a good estimate for the number of answers to \u03c8 in I\u03c3 (Demolombe, 1980), which is in turn a measure for the precision of \u03c8.", "startOffset": 86, "endOffset": 103}, {"referenceID": 18, "context": "If we use Goubault\u2019s simplification algorithm for BDDs for implementing line 9, the worst case complexity of this step is non-elementary in the size of C(\u03c6) \u2228 \u03c8 (Goubault, 1995).", "startOffset": 161, "endOffset": 177}, {"referenceID": 17, "context": "For example, the grounder gringo (Gebser et al., 2007) uses a syntactic check to derive bounds: it derives that predicate q of the input vocabulary is a bound for predicate p if p", "startOffset": 33, "endOffset": 54}, {"referenceID": 36, "context": "The grounder of the dlv system (Perri et al., 2007) may derive bounds by reasoning on the propositional level.", "startOffset": 31, "endOffset": 51}, {"referenceID": 12, "context": "This technique is applied by the grounder psgrnd (East et al., 2006), which uses unit propagation (Davis & Putnam, 1960) and complete one-atom lookahead (Li & Anbulagan, 1997) as propagation methods.", "startOffset": 49, "endOffset": 68}, {"referenceID": 36, "context": "In this case, a good grounding order can be derived from the dependency graph of the input theory (e.g., Cadoli & Schaerf, 2005; Perri et al., 2007).", "startOffset": 98, "endOffset": 148}, {"referenceID": 19, "context": "Shlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level.", "startOffset": 35, "endOffset": 50}, {"referenceID": 19, "context": "Shlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level.", "startOffset": 35, "endOffset": 155}, {"referenceID": 30, "context": "Clause splitting Clause splitting is a well-known rewriting technique applied in MACE style model generation (McCune, 2003).", "startOffset": 109, "endOffset": 123}, {"referenceID": 4, "context": "The simple heuristic to guide clause splitting described by Claessen and S\u00f6rensson (2003) can directly be applied to choose which quantifiers to move inside.", "startOffset": 60, "endOffset": 90}, {"referenceID": 36, "context": "One of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction \u03c61 \u2227 .", "startOffset": 87, "endOffset": 107}, {"referenceID": 36, "context": "One of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction \u03c61 \u2227 . . . \u2227 \u03c6n that are possibly true, given (an overestimation of) the possibly true instances of each of the conjuncts \u03c6i. In GidL, this backjumping technique is applied to implement line 12 of function groundDisj. Indeed, if \u03c6 is the formula \u03c61 \u2227 . . . \u2227 \u03c6n, then line 12 amounts to finding all possible instances of \u03c6, while the cf-bounds for \u03c61, . . . , \u03c6n provide an overestimation of the possibly true instances of these conjuncts. Similarly, the backjumping technique is applied to improve line 12 of groundConj, where all possibly false instances of a disjunction are calculated. Catalano, Leone, and Perri (2008) present an adaptation of indexing strategies for grounding.", "startOffset": 88, "endOffset": 783}, {"referenceID": 37, "context": "Partition-Based Reasoning Ramachandran and Amir (2005) describe a sophisticated grounding technique that can reduce the grounding size of FO theories, depending on the availability of some", "startOffset": 26, "endOffset": 55}, {"referenceID": 30, "context": "mace (McCune, 2003) and paradox (Claessen & S\u00f6rensson, 2003) are finite model generators for FO.", "startOffset": 5, "endOffset": 19}, {"referenceID": 42, "context": "Currently, there are three ASP grounders: lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), gringo (Gebser et al.", "startOffset": 49, "endOffset": 81}, {"referenceID": 43, "context": "Currently, there are three ASP grounders: lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), gringo (Gebser et al.", "startOffset": 49, "endOffset": 81}, {"referenceID": 17, "context": "Currently, there are three ASP grounders: lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), gringo (Gebser et al., 2007) and the grounding component of dlv (Perri et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 36, "context": ", 2007) and the grounding component of dlv (Perri et al., 2007).", "startOffset": 43, "endOffset": 63}, {"referenceID": 20, "context": "This translation is described by Mari\u00ebn et al. (2004). Next, a (slightly adapted) grounder for ASP is used to ground the logic program.", "startOffset": 33, "endOffset": 54}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.", "startOffset": 56, "endOffset": 197}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.2.1). To construct a grounding of a sentence \u03c6, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for \u03c6 is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm.", "startOffset": 56, "endOffset": 566}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.2.1). To construct a grounding of a sentence \u03c6, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for \u03c6 is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm. kodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of \u22a5. Similarly for the cf-bound. This leads to more efficient and compact groundings. mace (McCune, 2003) and paradox (Claessen & S\u00f6rensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch. East et al. (2006) developed the grounder psgrnd for MX(PS).", "startOffset": 56, "endOffset": 2077}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.2.1). To construct a grounding of a sentence \u03c6, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for \u03c6 is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm. kodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of \u22a5. Similarly for the cf-bound. This leads to more efficient and compact groundings. mace (McCune, 2003) and paradox (Claessen & S\u00f6rensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch. East et al. (2006) developed the grounder psgrnd for MX(PS). PS is a fragment of FO(ID), extended with pseudo-boolean constraints. As explained above, psgrnd performs reasoning on the ground theory to reduce memory usage and grounding size. The experiments performed by East et al. (2006) show that carefully designed data structures are of key importance to build an efficient grounder.", "startOffset": 56, "endOffset": 2347}, {"referenceID": 12, "context": "Finally, because of the large amount of data processed by grounders, carefully designed data structures and an optimized implementation of the core grounding algorithm is very important to achieve fast grounding (East et al., 2006).", "startOffset": 212, "endOffset": 231}], "year": 2010, "abstractText": "Grounding is the task of reducing a first-order theory and finite domain to an equivalent propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning. Besides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounder\u2019s output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems. We first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method.", "creator": "TeX"}}}