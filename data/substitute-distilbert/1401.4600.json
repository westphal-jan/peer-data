{"id": "1401.4600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams", "abstract": "we settled on the problem of sequential decision making in partially observable forms shared with other agents of uncertain types for similar or conflicting objectives. this problem has been originally formalized by multiple frameworks one of itself is the augmented dynamic influence diagram ( i - did ), which generalizes the well - known influence diagram to the multiagent setting. i - dids are composite models and may be used to compute the policy supporting an agent given its belief being the physical state and others models, which changes as same agent acts and observes in the subject setting.", "histories": [["v1", "Sat, 18 Jan 2014 21:09:03 GMT  (1646kb)", "http://arxiv.org/abs/1401.4600v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yifeng zeng", "prashant doshi"], "accepted": false, "id": "1401.4600"}, "pdf": {"name": "1401.4600.pdf", "metadata": {"source": "CRF", "title": "Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams", "authors": ["Yifeng Zeng", "Prashant Doshi"], "emails": ["YFZENG@CS.AAU.DK", "PDOSHI@CS.UGA.EDU"], "sections": [{"heading": null, "text": "shared with other agents of uncertain types having similar or conflicting objectives. This problem has been previously formalized by multiple frameworks one of which is the interactive dynamic influence diagram (I-DID), which generalizes the well-known influence diagram to the multiagent setting. I-DIDs are graphical models and may be used to compute the policy of an agent given its belief over the physical state and others\u2019 models, which changes as the agent acts and observes in the multiagent setting.\nAs we may expect, solving I-DIDs is computationally hard. This is predominantly due to the large space of candidate models ascribed to the other agents and its exponential growth over time. We present two methods for reducing the size of the model space and stemming its exponential growth. Both these methods involve aggregating individual models into equivalence classes. Our first method groups together behaviorally equivalent models and selects only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space. The second method further compacts the model space by focusing on portions of the behavioral predictions. Specifically, we cluster actionally equivalent models that prescribe identical actions at a single time step. Exactly identifying the equivalences would require us to solve all models in the initial set. We avoid this by selectively solving some of the models, thereby introducing an approximation. We discuss the error introduced by the approximation, and empirically demonstrate the improved efficiency in solving I-DIDs due to the equivalences."}, {"heading": "1. Introduction", "text": "Sequential decision making (planning) is a key tenet of agent autonomy. Decision making becomes complicated due to actions that are nondeterministic and a physical environment that is often only partially observable. The complexity increases exponentially in the presence of other agents who are themselves acting and observing, and whose actions impact the subject agent. Multiple related frameworks formalize the general problem of decision making in uncertain settings shared with other sophisticated agents who may have similar or conflicting objectives. One of these frameworks is the interactive partially observable Markov decision process (I-POMDP) (Gmytrasiewicz & Doshi, 2005), which generalizes POMDPs (Smallwood & Sondik, 1973; Kaelbling, Littman, & Cassandra, 1998) to multiagent settings; another framework is the interactive dynamic influ-\nc\u00a92012 AI Access Foundation. All rights reserved.\nence diagram (I-DID) (Doshi, Zeng, & Chen, 2009). In cooperative settings, the decentralized POMDP (Bernstein, Givan, Immerman, & Zilberstein, 2002) framework models multiagent decision making.\nI-DIDs are graphical models for sequential decision making in uncertain multiagent settings. They concisely represent the problem of how an agent should act in an uncertain environment shared with others who may act simultaneously in sophisticated ways. I-DIDs may be viewed as graphical counterparts of I-POMDPs which adopt an enumerative representation of the decision-making problem. I-DIDs generalize dynamic influence diagrams (DID) (Tatman & Shachter, 1990) to multiagent settings analogously to the way that I-POMDPs generalize POMDPs. Importantly, I-DIDs have the advantage of a representation that explicates the embedded domain structure by decomposing the state space into variables and relationships between the variables. Not only is this representation more intuitive to use, it translates into computational benefits when compared to the enumerative representation as used in I-POMDPs (Doshi et al., 2009).\nFollowing the paradigm of graphical models, I-DIDs compactly represent the decision problem by mapping various variables into chance, decision and utility nodes, and denoting the dependencies between variables using directed arcs between the corresponding nodes. They extend DIDs by introducing a special model node whose values are the possible models of the other agent. These models may themselves be represented using I-DIDs leading to nested modeling. Both other agents\u2019 models and the original agent\u2019s beliefs over these models are updated over time using a special model update link that connects the model nodes between time steps. Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others\u2019 models. Consequently, I-DIDs may be used to compute the policy of an agent online \u2013 given an initial belief of the agent \u2013 as the agent acts and observes in a setting that is populated by other interacting agents. We show a generic I-DID in Fig. 1 and provide more details in Section 3.\nAs we may expect, solving I-DIDs is computationally very hard. In particular, they acutely suffer from the curses of dimensionality and history (Pineau, Gordon, & Thrun, 2006). This is because the state space in I-DIDs includes the models of other agents in addition to the traditional\nphysical states. As the agents act, observe and update beliefs, I-DIDs must track the evolution of the models over time. Theoretically, the number of candidate models grows exponentially over time. Thus, I-DIDs not only suffer from the curse of history that afflicts the modeling agent, but also from that exhibited by the modeled agents. This is further complicated by the nested nature of the state space.\nConsequently, exact solutions of I-DIDs are infeasible for all but the simple problems and ways of mitigating the computational intractability are critically needed. Because the complexity is predominantly due to the candidate models, we focus on principled reductions of the model space while avoiding significant losses in the optimality of the decision maker. Our first approach builds upon the idea of grouping together behaviorally equivalent (BE) models (Rathnasabapathy, Doshi, & Gmytrasiewicz, 2006; Pynadath & Marsella, 2007). These are models whose behavioral predictions for the modeled agent(s) are identical. Because the solution of the subject agent\u2019s I-DID is affected only by the predicted behavior of the other agent regardless of the description of the ascribed model, we may consider a single representative from each BE class without affecting the optimality of the solution. Identifying BE models requires solving the individual models. We reduce the exponential growth in the model space by discriminatively updating models. Specifically, at each time step, we select only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space. In other words, models that on update would result in predictions which are identical to those of existing models are not selected for updating. For these models, we simply transfer their revised probability masses to the existing BE models. Thus, we avoid generating all possible updated models and subsequently reducing them. Rather, we generate a minimal set of models at each time step.\nRestricting the updated models to the exact minimal set would require solving all the models that are considered initially. Exploiting the notion that models whose beliefs are spatially close tend to be BE, we solve those models only whose beliefs are not \u01eb-close to a representative. We theoretically analyze the error introduced by this approach in the optimality of the solution. Importantly, we experimentally evaluate our approach on I-DIDs formulated for multiple problem domains having two agents, and show approximately an order of magnitude improvement in performance in comparison to the previous clustering approach (Zeng, Doshi, & Chen, 2007), with a comparable loss in optimality. One of these problem domains is the Georgia testbed for autonomous control of vehicles (GaTAC) (Doshi & Sonu, 2010), which facilitates scalable and realistic problem domains pertaining to autonomous control of unmanned agents such as uninhabited aerial vehicles (UAV). GaTAC provides a low-cost, open-source and flexible environment for realistically simulating the problem domains and evaluating solutions produced by multiagent decision-making algorithms.\nWe further compact the space of models in the model node by observing that behaviorally distinct models may prescribe identical actions at a single time step. We may then group together these models into a single equivalence class. In comparison to BE, the definition of our equivalence class is different: it includes those models whose prescribed action for the particular time step is the same, and we call it action equivalence (AE). Since there are typically additional models than the BE ones that prescribe identical actions at a time step, an AE class often includes many more models. Consequently, the model space is partitioned into lesser number of classes than previously and is bounded by the number of actions of the other agent.\nUnlike the update of BE classes, given the action and an observation AE classes do not update deterministically. We show how we may compute the probability with which an equivalence class is updated to another class in the next time step. Although, in general, grouping AE models introduces\nan approximation, we derive conditions under which AE model grouping preserves optimality of the solution. We demonstrate the performance of our approach on multiple two-agent problem domains including in GaTAC and show significant time savings in comparison to previous approaches.\nTo summarize, the main contributions of this article are new approaches that group equivalent models more efficiently leading to improved scalability in solving I-DIDs. Our first method reduces the exponential growth of the model space by discriminatively updating models thereby generating a behaviorally minimal set in the next time step that is characterized by the absence of BE models. The second method adopts a relaxed grouping of models that prescribe identical actions for that particular time step. Grouping AE models leads to equivalence classes that often include many models in addition to those that are BE. We augment both these methods with an approximation that avoids solving all of the initial models, and demonstrate much improved scalability in experiments.\nThe remainder of this article is structured as follows. In Section 2, we discuss previous work related to this article. In Section 3, we briefly review the graphical model of I-DID as well as its solution based on BE. In Section 4, we show how we may discriminatively update models in order to facilitate behaviorally-distinct models at subsequent time steps. We introduce an approximation, and discuss the associated computational savings and error. We introduce the approach of further grouping models based on actions, in Section 5. All approaches for solving I-DIDs are empirically evaluated along different dimensions in Section 6. We conclude this article with a discussion of the framework and the solution approaches including extensions to N > 2 agent interactions, and some limitations, in Section 7. The Appendices contain proofs of propositions mentioned elsewhere, and detailed descriptions and I-DID representations of the problem domains used in the evaluation."}, {"heading": "2. Related Work", "text": "Suryadi and Gmytrasiewicz (1999) in an early piece of related work, proposed modeling other agents using IDs. The approach proposed ways to modify the IDs to better reflect the observed behavior. However, unlike I-DIDs, other agents did not model the original agent and the distribution over the models was not updated based on the actions and observations.\nAs detailed by Doshi et al. (2009), I-DIDs contribute to an emerging and promising line of research on graphical models for multiagent decision making. This includes multiagent influence diagrams (MAID) (Koller & Milch, 2001), network of influence diagrams (NID) (Gal & Pfeffer, 2008), and more recently, limited memory influence diagram based players (Madsen & Jensen, 2008). While MAIDs adopt an external perspective of the interaction, exploiting the conditional independence between effects of actions to compute the Nash equilibrium strategy for all agents involved in the interaction, I-DIDs offer a subjective perspective to the interaction, computing the best-response policy as opposed to a policy in equilibrium. The latter may not account for other agent\u2019s behaviors outside the equilibrium and multiple equilibria may exist. Furthermore, both MAID and NID formalisms focus on a static, single-shot interaction. In contrast, I-DIDs offer solutions over extended time interactions, where agents act and update their beliefs over others\u2019 models which are themselves dynamic.\nWhile I-DIDs closely relate to the previously mentioned ID-based graphical models, another significant class of graphical models compactly represents the joint behavior as a graphical game (Kearns, Littman, & Singh, 2001). It models the agents as graph vertices and an interaction in payoff between two agents using an edge, with the objective of finding a joint distribution over agents\u2019 actions possibly in equilibrium. More recently, graphical multiagent models (Duong, Wellman,\n& Singh, 2008) enhance graphical games by allowing beliefs over agent behaviors formed from different knowledge sources, and conditioning agent behaviors on abstracted history if the game is dynamic (Duong, Wellman, Singh, & Vorobeychik, 2010).\nAs we mentioned previously, a dominating cause of the complexity of I-DIDs is the exponential growth in the candidate models over time. Using the insight that models (with identical capabilities and preferences) whose beliefs are spatially close are likely to be BE, Zeng and Doshi (2007) utilized a k-means approach to cluster models together and select K models closest to the means of the clusters in the model node at each time step. This approach facilitates the consideration of a fixed number of models at each time. However, the approach first generates all possible models before reducing the model space at each time step, thereby not reducing the memory required. Further, it utilizes an iterative and often time-consuming k-means clustering method.\nThe concept of BE of models was proposed and initially used for solving I-POMDPs (Rathnasabapathy et al., 2006), and discussed generally by Pynadath and Marsella (2007). We contextualize BE within the framework of I-DIDs and seek further extensions. A somewhat related notion is that of state equivalence introduced by Givan et al. (2003) where the equivalence concept is exploited to factorize MDPs and gain computational benefit. Along this direction, another type of equivalence in probabilistic frameworks such as MDPs and POMDPs, sometimes also called BE, is bisimulation (Milner, 1980; Givan et al., 2003; Castro, Panangaden, & Precup, 2009). Two states are bisimilar if any action from these states leads to identical immediate reward and the states transition with the same probability to equivalence classes of states. While bisimulation is a test within a model given just its definition, BE in multiagent systems is defined and used differently: as a way of comparing between models using their solutions. Interestingly, both these concepts are ultimately useful for model minimization.\nOther frameworks for modeling the multiagent decision-making problem exist. Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that exhibit probabilistic equivalence. Such histories generate an identical distribution over the histories of the other agents and lead to the same joint belief state. While we utilize BE to losslessly cluster the models of the other agent, we note that BE models when combined with the subject agent\u2019s policy induce identical distributions over the subject agent\u2019s action-observation history. More recently, Witwicki and Durfee (2010) use influence-based abstraction in order to limit an agent\u2019s belief to the other agent\u2019s relevant information by focusing on mutually-modeled features only.\nOur agent models are analogous to types in game theory (Harsanyi, 1967), which are defined as attribute vectors that encompass all of an agent\u2019s private information. In this context, Dekel et al. (2006) define a strategic topology on universal type spaces (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993) under which two types are close if their strategic behavior is similar in all strategic situations. While Dekel et al. focus on a theoretical analysis of the topology and use rationalizability as the solution concept, we focus on operationalizing BE within a computational framework. Furthermore, our solution concept is that of best response to one\u2019s beliefs."}, {"heading": "3. Background", "text": "We briefly review interactive influence diagrams (I-ID) for two-agent interactions followed by their extension to dynamic settings, I-DIDs (Doshi et al., 2009). Both these formalisms allow modeling the other agent and to use that information in the decision making of the subject agent.\nWe illustrate the formalisms and our approaches in the context of the multiagent tiger problem (Gmytrasiewicz & Doshi, 2005) \u2013 a two-agent generalization of the well-known single agent tiger problem (Kaelbling et al., 1998). In this problem, two agents, i and j, face two closed doors one of which hides a tiger while the other hides a pot of gold. An agent gets rewarded for opening the door that hides the gold but gets penalized for opening the door leading to the tiger. Each agent may open the left door (action denoted by OL), open the right door (OR), or listen (L). On listening, an agent may hear the tiger growling either from the left (observation denoted by GL) or from the right (GR). Additionally, the agent hears creaks emanating from the direction of the door that was possibly opened by the other agent \u2013 creak from the left (CL) or creak from right (CR) \u2013 or silence (S) if no door was opened. All observations are assumed to be noisy. If any door is opened by an agent, the tiger appears behind any of the two doors randomly in the next time step. While the actions of the other agent do not directly affect the reward for an agent, they may potentially change the location of the tiger. This formulation of the problem differs from that of Nair et al. (2003) in the presence of door creaks and that it is not cooperative."}, {"heading": "3.1 Interactive Dynamic Influence Diagrams", "text": "Influence diagrams (Tatman & Shachter, 1990) typically contain chance nodes which represent the random variables modeling the physical state, S, and the agent\u2019s observations, Oi, among other aspects of the problem; decision nodes that model the agent\u2019s actions, Ai; and utility nodes that model the agent\u2019s reward function, Ri. In addition to these nodes, I-IDs for an agent i include a new type of node called the model node. This is the hexagonal node, Mj,l\u22121, in Fig. 2, where j denotes the other agent and l \u2212 1 is the strategy level, which allows for a nested modeling of i by the other agent j. Agent j\u2019s level is one less than that of i, which is consistent with previous hierarchical modeling in game theory (Aumann, 1999a; Brandenburger & Dekel, 1993) and decision theory (Gmytrasiewicz & Doshi, 2005). Additionally, a level 0 model is an ID or a flat probability distribution. We note that the probability distribution over the chance node, S, and the model node together represents agent i\u2019s belief over its interactive state space. In addition to the model node, I-IDs differ from IDs by having a chance node, Aj , that represents the distribution over the other agent\u2019s actions, and a dashed link, called a policy link.\nThe model node contains as its values the alternative computational models ascribed by i to the other agent. The policy link denotes that the distribution over Aj is contingent on the models in the model node. We denote the set of these models by Mj,l\u22121, and an individual model of j as, mj,l\u22121 = \u3008bj,l\u22121, \u03b8\u0302j\u3009, where bj,l\u22121 is the level l\u22121 belief, and \u03b8\u0302j is the agent\u2019s frame encompassing the decision, observation and utility nodes. A model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or a flat probability distribution over the actions.\nWe observe that the model node and the dashed policy link that connects it to the chance node, Aj , could be represented as shown in Fig. 3(a) leading to a flat ID shown in Fig. 3(b). The decision node of each level l \u2212 1 I-ID is transformed into a chance node. Specifically, if OPT(m1j,l\u22121) is the set of optimal actions obtained by solving the I-ID (or ID) denoted by m1j,l\u22121, then Pr(aj \u2208\nscript numbers serve to distinguish the models) are mapped to the corresponding chance nodes (A1j , A 2 j ) respectively, which is indicated by the dotted arrows. Depending on the value of node, Mod[Mj], distribution of each of the chance nodes is assigned to node Aj with some probability. (b) The transformed flat ID with the model node and policy link replaced as in (a).\nA1j ) = 1\n|OPT(m1 j,l\u22121\n)| if aj \u2208 OPT(m1j,l\u22121), 0 otherwise. The different chance nodes (A 1 j , A 2 j ) \u2013\none for each model \u2013 and additionally, the chance node labeled Mod[Mj ] form the parents of the chance node, Aj . There are as many action nodes as the number of models in the support of agent i\u2019s beliefs. The conditional probability table (CPT) of the chance node, Aj , is a multiplexer that assumes the distribution of each of the action nodes (A1j , A 2 j ) depending on the value of Mod[Mj ]. In other words, when Mod[Mj ] has the value m1j,l\u22121, the chance node Aj assumes the distribution of the node A1j , and Aj assumes the distribution of A 2 j when Mod[Mj ] has the value m 2 j,l\u22121. The distribution over Mod[Mj ] is i\u2019s belief over j\u2019s models given the state.\nFor more than two agents, we add a model node and a chance node representing the distribution over an agent\u2019s action linked together using a policy link, for each other agent. Interactions among others such as coordination or team work could be considered by utilizing models, which predict the\njoint behavior of others, in a distinct model node and possibly updating such models. For example, the joint behavioral models could be graphical analogs of decentralized POMDPs. Other settings involving agents acting independently or some of them being cooperative while others adversarial may be represented as well, and is a topic of research under study. As an aside, Doshi et al. (2009) show how I-IDs relate to NIDs (Gal & Pfeffer, 2008).\nWe setup the I-ID for the multiagent tiger problem described previously, in Fig. 4. We discuss the CPTs of the various nodes in Appendix B.1. While the I-ID contains two models of j, there would be as many action nodes of j if there were more models.\nI-DIDs extend I-IDs to allow sequential decision making over multiple time steps. We depict a general, two time-slice I-DID in Fig. 5. In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 5. We briefly explain the semantics of the model update next.\nAgents in a multiagent setting may act and make observations, which changes their beliefs. Therefore, the update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t+1. Because the agents act and receive observations, their models are updated to reflect their changed beliefs. Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |\u2126j | possible observations where \u2126j is the set of j\u2019s observations, the updated set at time step t+1 will have up to |Mtj,l\u22121||Aj ||\u2126j | models. Here, |M t j,l\u22121| is the number of models at time step t, |Aj | and |\u2126j | are the largest spaces of actions and observations respectively, among all the models. The CPT of chance node Mod[M t+1j,l\u22121] encodes the indicator function, \u03c4(b t j,l\u22121, a t j , o t+1 j , b t+1 j,l\u22121), which is 1 if the belief btj,l\u22121 in a model m t j,l\u22121 using the action a t j and observation o t+1 j updates to bt+1j,l\u22121 in a model m t+1 j,l\u22121; otherwise it is 0. Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model. The dotted model update link in the I-DID may be implemented using standard dependency links and chance nodes, as shown in Fig. 6 transforming the I-DID into a flat DID.\nIn Fig. 7, we show the two time-slice flat DID with the model nodes and the model update link replaced by the chance nodes and the relationships between them. Chance nodes and dependency links not in bold are standard, usually found in single agent DIDs.\nContinuing with our illustration, we show the two time-slice I-DID for the multiagent tiger problem in Fig. 8. The model update link not only updates the number of j\u2019s candidate models due to its action and observations of growl, it also updates the probability distribution over these models. The model update link in the I-DID is implemented using standard dependency links as shown in Fig. 9. For the sake of clarity, we illustrate the update of a single model of j contained in the model node at time t."}, {"heading": "3.2 Behavioral Equivalence and Model Solution", "text": "Although the space of possible models is very large, not all models need to be considered by agent i in the model node. As we mentioned previously, models that are BE (Rathnasabapathy et al., 2006; Pynadath & Marsella, 2007) could be pruned and a single representative model considered. This is because the solution of the subject agent\u2019s I-DID is affected by the predicted behavior of the other agent; thus we need not distinguish between behaviorally equivalent models. We define BE more formally below:\nDefinition 1 (Behavioral equivalence). Two models, mj,l\u22121 and m\u2032j,l\u22121, of the other agent, j, are behaviorally equivalent if, OPT(mj,l\u22121) = OPT(m\u2032j,l\u22121), where OPT(\u00b7) denotes the solution of the model that forms the argument.\nThus, BE models are those whose behavioral predictions for the agent are identical.\nThe solution of an I-DID (and I-ID) is implemented recursively down the levels as shown in Fig. 10. In order to solve a level 1 I-DID of horizon T , we start by solving the level 0 models, which may be traditional DIDs of horizon T . Their solutions provide probability distributions over the other agents\u2019 actions, which are entered in the corresponding action nodes found in the model node of the level 1 I-DID at the corresponding time step (lines 3-5). Subsequently, the set of j\u2019s models is minimized by excluding the BE models (line 6).\nThe solution method uses the standard look-ahead technique, projecting the agent\u2019s action and observation sequences forward from the current belief state, and finding the possible beliefs that i could have in the next time step (Russell & Norvig, 2010). Because agent i has a belief over j\u2019s models as well, the look-ahead includes finding out the possible models that j could have in the future. Consequently, each of j\u2019s level 0 models represented using a standard DID must be solved in the first time step up to horizon T to obtain its optimal set of actions. These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. SE(btj , aj , oj) is an abbreviation for the belief update (lines 8-13). Beliefs over these updated set of candidate models are calculated using the standard inference methods through the dependency links between the model nodes shown in Fig. 6 (lines 15-18). Agent i\u2019s I-DID is expanded across all time steps in this manner. We point out that the algorithm in Fig. 10 may be realized with the help of standard implementations of DIDs such as HUGIN EXPERT (Andersen & Jensen, 1989). The solution is a policy tree that prescribes the optimal action(s) to perform for agent i initially given its belief, and the actions thereafter conditional on its observations."}, {"heading": "4. Discriminative Model Updates", "text": "Solving I-DIDs is computationally intractable due to not only the large space and complexity of models ascribed to j, but also due to the exponential growth in candidate models of j over time. This growth leads to a disproportionate increase in the interactive state space over time. We begin by introducing a set of models that is minimal in a sense and describe a method for generating this set. The minimal set is analogous to one of the notions of a minimal mental model space as described by\nPynadath and Marsella (2007). We assume that models of the other agent differ only in their beliefs and that the other agent\u2019s frame is known. We later discuss in Section 7 the impact of the frame being unknown as well. For clarity, we continue to focus on two-agent interactions, and discuss extensions of the techniques presented here in Section 7 as well."}, {"heading": "4.1 Behaviorally Minimal Model Set", "text": "Given the set of models, Mj,l\u22121, of the other agent, j, in a model node we define a corresponding behaviorally minimal set of models:\nDefinition 2 (Behaviorally minimal set). Define a minimal set of models, M\u0302j,l\u22121, as the largest subset of Mj,l\u22121, such that for each model, mj,l\u22121 \u2208 M\u0302j,l\u22121, there exists no other model in M\u0302j,l\u22121 that is BE to mj,l\u22121.\nHere, BE is as defined in Def. 1. We say that M\u0302j,l\u22121 (behaviorally) minimizes Mj,l\u22121. As we illustrate in Fig. 11 using the tiger problem (Kaelbling et al., 1998), the set M\u0302j,l\u22121 that minimizes Mj,l\u22121 comprises of all the behaviorally distinct representatives of the models in Mj,l\u22121 and only these models. Because any model from a group of BE models may be selected as the representative in M\u0302j,l\u22121, a minimal set corresponding to Mj,l\u22121 is not unique, although its cardinality remains fixed.\nform a behaviorally minimal set, M\u0302tj,0, we select a representative model from each BE group of models (models in differently shaded regions). Agent i\u2019s distribution over the models in M\u0302tj,0 is obtained by summing the probability mass assigned to the individual models in each region. Note that M\u0302tj,0 is not unique because any one model within a shaded region could be selected for inclusion in it.\nAgent i\u2019s probability distribution over the minimal set, M\u0302j,l\u22121, conditioned on the physical state is obtained by summing the probability mass over BE models in Mj,l\u22121 and assigning the accumulated probability to the representative model in M\u0302j,l\u22121. Formally, let m\u0302j,l\u22121 \u2208 M\u0302j,l\u22121, then:\nb\u0302i(m\u0302j,l\u22121|s) = \u2211\nmj,l\u22121\u2208Mj,l\u22121\nbi(mj,l\u22121|s) (1)\nwhere Mj,l\u22121 \u2286 Mj,l\u22121 is the set of BE models to which the representative m\u0302j,l\u22121 belongs. Thus, if M\u0302j,l\u22121 minimizes Mj,l\u22121, then Eq. 1 shows how we may obtain the probability distribution over M\u0302j,l\u22121 at some time step, given i\u2019s belief distribution over models in the model node at that step (see Fig. 11).\nThe behaviorally minimal set together with the probability distribution over it has an important property: Solution of an I-DID remains unchanged when the models in a model node and the distribution over the models are replaced by the corresponding minimal set and the distribution over it, respectively. In other words, transforming the set of models in the model node into its minimal set preserves the solution. Proposition 1 states this formally:\nProposition 1. Let X : \u2206(Mj,l\u22121) \u2192 \u2206(M\u0302j,l\u22121) be a mapping defined by Eq. 1, where Mj,l\u22121 is the space of models in a model node and M\u0302j,l\u22121 minimizes it. Then, applying X preserves the solution of the I-DID.\nProof of Proposition 1 is given in Appendix A. Proposition 1 allows us to show that M\u0302j,l\u22121 is indeed minimal given Mj,l\u22121 with respect to the solution of the I-DID.\nCorollary 1. M\u0302j,l\u22121 in conjunction with X is a sufficient solution-preserving subset of models found in Mj,l\u22121.\nProof of this corollary follows directly from Proposition 1. Notice that the subset continues to be solution preserving when we additionally augment M\u0302j,l\u22121 with models from Mj,l\u22121.\nAs the number of models in the minimal set is, of course, no more than in the original set and typically much less, solution of the I-DID is often computationally much less intensive when the model set is replaced with its behaviorally minimal counterpart."}, {"heading": "4.2 Discrimination Using Policy Graphs", "text": "A straightforward way of obtaining M\u0302j,l\u22121 exactly at any time step is to first ascertain the BE groups of models. This requires us to solve the I-DIDs or DIDs representing the models, then select a representative model from each BE group to include in M\u0302j,l\u22121, and prune all others which have the same solution as the representative."}, {"heading": "4.2.1 APPROACH", "text": "Given the set of j\u2019s models, Mj,l\u22121, at time t(=0), we present a technique for generating the minimal sets at subsequent time steps in the I-DID. We first observe that behaviorally distinct models at time t may result in updated models at t + 1 that are BE. Hence, our approach is to select at time step t only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space at t + 1. Models that will result in predictions on update which are identical to those of other existing models at t+ 1 are not selected for updating. Consequently, the resulting model set at t+ 1 is minimal.\nWe do this by solving the individual I-DIDs or DIDs in Mtj,l\u22121. Solutions to DIDs or I-DIDs are policy trees, which may be merged bottom up to obtain a policy graph, as we demonstrate in Fig. 12. Seuken and Zilberstein (2007) reuse subtrees of smaller horizon by linking to them using pointers while forming policy trees for the next horizon in the solution of decentralized POMDPs. The net effect is the formation of a policy graph similar to ours thereby providing an alternative to our approach of solving the individual models to first obtain the complete policy trees and then merge post hoc. We adopt the latter approach because the individual models, which are DIDs, when solved using available implementations produce complete policy trees. The following proposition gives the complexity of merging the policy trees to obtain the policy graph.\nProposition 2 (Complexity of tree merge). The worst-case complexity of the procedure for merging policy trees to form a policy graph is O((|\u2126j |T\u22121)2|Mj |2), where T is the horizon.\nProof. The complexity of the policy tree merge procedure is proportional to the number of comparisons that are made between parts of policy trees to ascertain their similarity. Because the procedure follows a bottom-up approach and the leaf level has the largest number of nodes, the maximum\nnumber of comparisons are made between leaf nodes. The worst case occurs when none of the leaf nodes of the different policy trees can be merged. Note that this precludes the merger of upper parts of the policy trees as well. Each policy tree may contain up to |\u2126j |T\u22121 leaf nodes, where T is the horizon. Hence, at most O((|\u2126j |T\u22121)2|Mj |2) comparisons are made, where O(|Mj |2) is the number of pairs in the model set. 1 The case when none of the leaf nodes merge must occur when the models are behaviorally distinct, and they form a minimal set, M\u0302j . In other words, Mj = M\u0302j .\n1. If we assume an ordering of the observations (edge labels) thereby ordering the tree, two policy trees may be sufficiently compared in O(|\u2126j |T\u22121) time.\nEach node in the policy graph represents an action to be performed by the agent and edges represent the agent\u2019s observations. As is common with policy graphs in POMDPs, we associate with each node at time t = 0, a range of beliefs for which the corresponding action is optimal (see Fig. 12(c)). This range may be obtained by computing the value of executing the policy tree rooted at each node at t = 0 in the graph and starting from each physical state. This results in a vector of values for each policy tree, typically called the \u03b1-vector. Intersecting the \u03b1-vectors and projecting the intersections on the belief simplex provides us with the boundaries of the needed belief ranges.\nWe utilize the policy graph to discriminate between model updates. For clarity, we formally define a policy graph next.\nDefinition 3 (Policy graph). Define a policy graph as:\nPG = \u3008V, A, E ,\u2126,Lv,Le\u3009\nwhere V is the set of vertices (nodes); A is the set of actions which form the node labels; E is the set of ordered pairs of vertices (edges); \u2126 is the set of observations which form the edge labels; Lv : V \u2192 A assigns to each vertex an action from the set of actions, A (node label); and Le : E \u2192 \u2126 assigns to each edge an observation from the set of observations, \u2126 (edge label). Le follows the property that no two edges whose first elements are identical (begin at the same vertex) are assigned the same observation.\nNotice that a policy graph augments a regular graph with meaningful node and edge labels. For a policy graph, PG, we also define the transition function, Tp : V \u00d7 \u2126 \u2192 V , for convenience. Tp(v, o) returns the vertex, v\u2032, such that {v, v\u2032} \u2208 E and Le({v, v\u2032}) = o.\nOur insight is that Tp(v, o) is the root node of a policy tree that represents the predictive behavior for the model updated using the action Lv(v) and observation o. As we iterate over j\u2019s models in the model node at time t in the expansion phase while solving the I-DID, we utilize Tp in deciding whether to update a model.\nWe first combine the policy trees obtained by solving the models in node M tj,l\u22121 to obtain the policy graph, PG, as shown in Fig. 12. Let v be the vertex in PG whose action label, Lv(v), represents the rational action for mj,l\u22121 \u2208 Mtj,l\u22121. We can ascertain this by simply checking whether the belief in mj,l\u22121 falls within the belief range associated with a node. For every observation o \u2208 Le({v, \u00b7}), we update the model, mj,l\u22121, using action Lv(v) and observation o, if v\u2032 = Tp(v, o) has not been encountered previously for this or any other model. We illustrate this below:\nExample 1 (Model update). Consider the level 0 models of j in the model node at time t, Mtj,0 = {\u30080.01, \u03b8\u0302j\u3009, \u30080.5, \u03b8\u0302j\u3009, \u30080.05, \u03b8\u0302j\u3009}, for the multiagent tiger problem. Recall that in a model of j, such as \u30080.01, \u03b8\u0302j\u3009, 0.01 is j\u2019s belief (over TL) and \u03b8\u0302j is its frame. From the PG in Fig. 12(c), the leftmost node prescribing the action L is optimal for the first and third models, while the second node also prescribing L is optimal for the second model. Beginning with model, \u30080.01, \u03b8\u0302j\u3009, Tp(v,GL) = v1 (where Lv(v1) = L) and Tp(v,GR) = v2 (Lv(v2) = OL). Since this is the first model we consider, it will be updated using L and both observations resulting in two models in Mt+1j,0 . For the model, \u30080.5, \u03b8\u0302j\u3009, if v\u2032 is the optimal node (Lv(v\u2032) = L), Tp(v\u2032, GR) = v1, which has been encountered previously. Hence, the model will not be updated using L and GR, although it will be updated using L and GL.\nIntuitively, for a model, mj,l\u22121, if node v1 = Tp(v, o) has been obtained previously for this or some other model and action-observation combination, then the update of mj,l\u22121 will be BE to the previously updated model (both will have the same policy tree rooted at v1). Hence, mj,l\u22121 need not be updated using the observation o. Because we do not permit updates that will lead to BE models, the set of models obtained at t + 1 is minimal. Applying this process analogously to models in the following time steps will lead to minimal sets at all subsequent steps and nesting levels."}, {"heading": "4.2.2 APPROXIMATION", "text": "We may gain further efficiency by avoiding the solution of all models in the model node at the first time step. One way of doing this is to randomly select K models of j, such that K \u226a |M0j,l\u22121|. Solution of the models will result in K policy trees, which could be combined as shown in Fig. 12 to form a policy graph. This policy graph is utilized to discriminate between the model updates. Notice that the approach becomes exact if the optimal solution of each model in M0j,l\u22121 is identical to that of one of the K models. Because the K models are selected randomly, this assumption is implausible and the approach is likely to result in a substantial loss of optimality that is mediated by K.\nWe propose a simple but effective refinement that mitigates the loss. Recall that models whose beliefs are spatially close are likely to be BE (Rathnasabapathy et al., 2006). Each of the remaining |M0j,l\u22121| \u2212 K models whose belief is not within \u01eb \u2265 0 of the belief of any of the K models will also be solved. This additional step makes it more likely that all the behaviorally distinct solutions will be generated and included in forming the policy graph. If \u01eb = 0, all models in the model node will be solved leading to the exact solution, while increasing \u01eb reduces the number of solved models beyond K. One measure of distance between belief points is the L1 based metric, though other metrics such as the Euclidean distance may also be used."}, {"heading": "4.3 Transfer of Probability Mass", "text": "Notice that a consequence of not updating models using some action-observation combination is that the probability mass that would have been assigned to the updated model in the model node at t+1 is lost. Disregarding this probability mass may introduce error in the optimality of the solution.\nWe did not perform the update because a model that is BE to the potentially updated model already exists in the model node at time t+1. We could avoid the error by transfering the probability mass that would have been assigned to the updated model on to the BE model.\nAs we mentioned previously, the node Mod[M t+1j,l\u22121] in the model node M t+1 j,l\u22121, has as its values the different models ascribed to agent j at time t + 1. The CPT of Mod[M t+1j,l\u22121] implements the function \u03c4(btj,l\u22121, a t j , o t+1 j , b t+1 j,l\u22121), which is 1 if b t j,l\u22121 in the model m t j,l\u22121 updates to b t+1 j,l\u22121 in model mt+1j,l\u22121 using the action-observation combination, otherwise it is 0. Let m t+1\u2032 j,l\u22121 = \u3008b t+1\u2032 j,l\u22121, \u03b8\u0302j\u3009 be the model that is BE to mt+1j,l\u22121. In order to transfer the probability mass to this model if the update is pruned, we modify the CPT of Mod[M t+1j,l\u22121] to indicate that m t+1\u2032 j,l\u22121 is the model that results from updating btj,l\u22121 with action, a t j and observation o t+1 j . This has the desired effect of transfering the probability that would have been assigned to the updated model (Fig. 6) on to mt+1 \u2032\nj,l\u22121 in the model node at time t+ 1."}, {"heading": "4.4 Algorithm", "text": "We present the discriminative update based algorithm for solving a level l \u2265 1 I-DID (as well as a level 0 DID) in Fig. 13. The algorithm differs from the exact approach (Fig. 10) in the expansion phase. In addition to a two time-slice level l I-DID and horizon T , the algorithm takes as input the number of random models to be solved initially, K, and the distance, \u01eb. Following Section 4.2, we begin by randomly selecting K models to solve (lines 2-5). For each of the remaining models, we identify one of the K solved model whose belief is spatially the closest (ties broken randomly). If the proximity is within \u01eb, the model is not solved \u2013 instead, the previously computed solution is assigned to the corresponding action node of the model in the model node, M0j,l\u22121 (lines 6-12). Subsequently, all models in the model node are associated with their respective solutions (policy trees), which are merged to obtain the policy graph (line 13), as illustrated in Fig. 12.\nIn order to populate the model node of the next time step, we identify the node v in PG that represents the optimal action for a model at time t. The model is updated using the optimal action aj (= Lv(v)) and each observation oj only if the node, v\u2032 = Tp(v, oj) has not been encountered in previous updates (lines 16-23). Given a policy graph, evaluating Tp(v, oj) is a constant time operation. Otherwise, as mentioned in Section 4.3, we modify the CPT of node, Mod[M t+1j,l\u22121], to transfer the probability mass to a BE model (line 25). Consequently, model nodes at subsequent time steps in the expanded I-DID are likely populated with minimal sets. Given the expanded I-DID, its solution may proceed in a straightforward manner as shown in Fig. 10."}, {"heading": "4.5 Computational Savings and Prediction Error Bound", "text": "The primary complexity of solving I-DIDs is due to the large number of models that must be solved over T time steps. At some time step t, there could be |M0j,l\u22121|(|Aj ||\u2126j |)\nt many models of the other agent j, where |M0j,l\u22121| is the number of models considered initially. The nested modeling further contributes to the complexity since solutions of each model at level l \u2212 1 requires solving the lower level l \u2212 2 models, and so on recursively up to level 0. In an N+1 agent setting, if the number of models considered at each level for an agent is bound by |M|, then solving an I-DID at level l requires the solutions of O((N |M|)l) many models. Discriminating between model updates reduces the number of agent models at each level to at most the size of the behaviorally minimal set, |M\u0302t|, while incurring the worst-case complexity of O((|\u2126|T\u22121)2|M|2) in forming the policy graph (Proposition 2). Consequently, we need to solve at most O((N |M\u0302\u2217|)l) number of models at each non-initial time step, where M\u0302\u2217 is the largest of the minimal sets across levels. 2 This is in comparison to O((N |M|)l), where M grows exponentially over time. In general, M\u0302 \u226a M, resulting in a substantial reduction in the computation. Additionally, a reduction in the number of models in the model node also reduces the size of the interactive state space, which makes solving the I-DID more efficient.\nIf we choose to solve all models in the initial model node, M0j,l\u22121, in order to form the policy graph, all sets of models at subsequent time steps will indeed be minimal. Consequently, there is no loss in the optimality of the solution of agent i\u2019s level l I-DID.\nFor the case where we select K < |M0j,l\u22121| models to solve, if \u01eb is infinitesimally small, we will eventually solve all models resulting in no error. With increasing values of \u01eb, larger numbers\n2. As we discuss in Section 7, we may group BE models across agents as well due to which the number of models to be solved further reduces.\nof models remain unsolved and could be erroneously associated with existing solutions. In the worst case, some of these models may be behaviorally distinct from all of the K solved models. Therefore, the policy graph is a subgraph of the one in the exact case, and leads to sets of models\nthat are subsets of the minimal sets. Additionally, lower-level models are solved approximately as well. While we seek to possibly bound the prediction error, it\u2019s impact on the optimality of agent i\u2019s level l I-DID is difficult to pinpoint. We formally define the error and discuss bounding it including some limitations on the bound, in Appendix A."}, {"heading": "5. Grouping Models Using Action Equivalence", "text": "Grouping BE models may significantly reduce the given space of other agents\u2019 models in the model node without loss in optimality. We may further compact the space of models in the model node by observing that behaviorally distinct models may prescribe identical actions at a single time step. We may then group together these models into a single equivalence class. In comparison to BE, the equivalence class includes those models whose prescribed action for the particular time step is the same, and we call it action equivalence. We define it formally next."}, {"heading": "5.1 Action Equivalence", "text": "Notice from Fig. 12(c) that the policy graph contains multiple nodes labeled with the same action at time steps t = 0 and t = 1. The associated models while prescribing actions that are identical at a particular time step, differ in the entire behavior. We call these models actionally equivalent. For a general case, we define action equivalence (AE) below:\nDefinition 4 (Action equivalence). Two models, mj,l\u22121 and m\u2032j,l\u22121, of the other agent are actionally equivalent at time step t if Pr(Atj) = Pr(A t\u2032 j ) where Pr(a t j) = 1 |OPT(mj,l\u22121)| if atj \u2208 OPT(mj,l\u22121), 0 otherwise; and Pr(at \u2032\nj ) = 1\n|OPT(m\u2032 j,l\u22121\n)| if at\n\u2032\nj \u2208 OPT(m \u2032 j,l\u22121), 0 otherwise, as defined previously.\nSince AE may include behaviorally distinct models, it partitions the model space into fewer classes.\nWe show an example aggregation of AE models in Fig. 14. From the figure, the partition of the model set, Mtj,l\u22121, induced by AE at time step 0 is {M t=0,1 j,l\u22121 ,M t=0,2 j,l\u22121 }, where M t=0,1 j,l\u22121 is the class of models in the model space whose prescribed action at t = 0 is L, and Mt=0,2j,l\u22121 is the class of models whose prescribed action at t = 0 is OR. Note that these classes include the BE models as well. Thus, all models in an AE class prescribe an identical action at that time step. Furthermore at t = 1, the partition consists of 3 AE classes and, at t = 2, the partition also consists of 3 singleton classes.\nIf Mt,pj,l\u22121 is an AE class comprising of models m t j,l\u22121 \u2208 M t,p j,l\u22121, agent i\u2019s conditional belief\nover it is obtained by summing over i\u2019s conditional belief over its member models:\nb\u0302i(M t,p j,l\u22121|s) =\n\u2211\nmt j,l\u22121 \u2208Mt,p j,l\u22121\nbi(m t j,l\u22121|s) (2)\n5.2 Revised CPT of Mod Node and its Markov Blanket\nEquation 2 changes the CPT of the node, Mod[M tj,l\u22121], due to the aggregation. Chang and Fung (1991) note that a coarsening operation of this type will not affect the distributions that do not\ndirectly involve the model space if the joint probability distribution over the Markov blanket of node, Mod[M tj,l\u22121], remains unchanged. In Fig. 15, we show the Markov blanket of Mod[M t j,l\u22121]. 3\nThe joint distribution over the Markov blanket is:\nPr(st, atj , o t+1 j ,m t+1 j,l\u22121) = \u2211 p Pr(s t,Mt,pj,l\u22121, a t j , o t+1 j ,m t+1 j,l\u22121)\n= \u2211\np Pr(s t)Pr(Mt,pj,l\u22121|s t)Pr(atj |M t,p j,l\u22121)Pr(m t+1 j,l\u22121|M t,p j,l\u22121, a t j , o t+1 j )Pr(o t+1 j |a t j)\n= Pr(st)Pr(ot+1j |a t j) \u2211 p Pr(M t,p j,l\u22121|s t)Pr(atj |M t,p j,l\u22121)Pr(m t+1 j,l\u22121|M t,p j,l\u22121, a t j , o t+1 j ) = Pr(st)Pr(ot+1j |a t j) \u2211 p \u2211 mt\nj,l\u22121 \u2208Mt,p j,l\u22121 Pr(mtj,l\u22121|s t)Pr(atj |M t,p j,l\u22121)\nPr(mt+1j,l\u22121|M t,p j,l\u22121, a t j , o t+1 j ) (from Eq. 2)\n(3)\n3. Because we assume that agents\u2019 frames do not change, we may remove the arc from Mod[M tj,l\u22121] to O t+1 j thus\nsimplifying the blanket.\nThe joint distribution prior to aggregation of Mod[M tj,l\u22121] is:\nPr(st, atj , o t+1 j ,m t+1 j,l\u22121) = Pr(s t)Pr(ot+1j |a t j) \u2211 mt\nj,l\u22121 \u2208Mt j,l\u22121 Pr(mtj,l\u22121|s t)Pr(atj |m t j,l\u22121)\nPr(mt+1j,l\u22121|m t j,l\u22121, a t j , o t+1 j )\n(4) We equate the right hand sides of Eqs. 3 and 4 to obtain the constraint that must be satisfied by the CPTs of some of the chance nodes in the Markov blanket in order for the joint distribution to remain unchanged:\n\u2211 p Pr(a t j |M t,p j,l\u22121)Pr(m t+1 j,l\u22121|M t,p j,l\u22121, a t j , o t+1 j ) \u2211 mt\nj,l\u22121 \u2208Mt,p j,l\u22121 Pr(mtj,l\u22121|s t) = \u2211\nmt j,l\u22121 \u2208Mt j,l\u22121\nPr(mtj,l\u22121|s t)Pr(atj |m t j,l\u22121)Pr(m t+1 j,l\u22121|m t j,l\u22121, a t j , o t+1 j )\n(5)\nNotice that Eq. 5 imposes a constraint on the CPTs of the successor nodes, Atj and Mod[M t+1 j,l\u22121]. If the constraint is satisfied \u2013 a setting for the CPTs of the nodes, Atj and Mod[M t+1 j,l\u22121], is found \u2013 grouping of AE models in the initial model node is exact and the optimality of the I-DID is preserved. An obvious way to satisfy Eq. 5 would be to meet the following intuitive constraint for each AE class p:\nPr(atj |M t,p j,l\u22121)Pr(m t+1 j,l\u22121|M t,p j,l\u22121, a t j , o t+1 j ) =\u2211\nmt j,l\u22121 \u2208M t,p j,l\u22121\nPr(mt j,l\u22121 |st)Pr(atj |m t j,l\u22121 )Pr(mt+1 j,l\u22121 |mt j,l\u22121 ,atj ,o t+1 j )\n\u2211 mt\nj,l\u22121 \u2208M t,p j,l\u22121\nPr(mt j,l\u22121 |st)\n(6)\nPr(atj |m t j,l\u22121) is fixed for each model, m t j,l\u22121, in AE class M t,p j,l\u22121 and equals Pr(a t j |M t,p j,l\u22121). Therefore, Eq. 6 reduces to:\nPr(mt+1j,l\u22121|M t,p j,l\u22121, a t j , o t+1 j ) =\n\u2211 mt\nj,l\u22121 \u2208M t,p j,l\u22121\nPr(mt j,l\u22121 |st)Pr(mt+1 j,l\u22121 |mt j,l\u22121 ,atj ,o t+1 j )\n\u2211 mt\nj,l\u22121 \u2208M t,p j,l\u22121\nPr(mt j,l\u22121\n|st) (7)\nObserve that Eq. 7 must hold for all values of the physical state, st. If the right hand side of the above equation remains unchanged for any value of st, we may set the CPT of Mod[M t+1j,l\u22121] using it. Typically, it is not trivial \u2013 often not possible \u2013 to find a single CPT for the chance node Mod[M t+1j,l\u22121] that will satisfy the constraint for all s\nt. Chang and Fung (1991) demonstrate that a close approximation would be to take the average of the right hand side of Eq. 7 over all possible values of st if these values are close.\nOf course, we may wish to aggregate models in node, Mod[M t+1j,l\u22121], as well (and so on). While the overall procedure is analogous, the difference is in the Markov blanket of the node that is to be aggregated. It includes the predecessor chance nodes, Mod[M tj,l\u22121], A t j , and O t+1 j in addition to its successors and parents of successors corresponding to those in Fig. 15. We illustrate the application of Eq. 7 to the example policy graph in Fig. 14(a) below:\nExample 2 (Model update). For simplicity, let the left AE class, Mt=0,1j,l\u22121 , comprise of three models, m t=0,1 j,l\u22121 , m t=0,2 j,l\u22121 and m t=0,3 j,l\u22121 , all of which prescribe action L. Let i\u2019s belief over these three models\nbe 0.16, 0.23 and 0.32 given physical state TL or TR, respectively (see Fig. 14(a)). We set the probability of updating say, Mt=0,1j,l\u22121 , using different action-observation combinations to individual models at time t=1, using Eq. 7. We show these probabilities in Fig. 14(b); these form the CPT of node Mod[M t=1j,l\u22121]. Because Pr(m t=0,1 j,l\u22121 |s) remains same given any s, constraint of Eq. 7 is met and the AE based partitioning at t=0 is exact.\nNext, we group AE models at t=1 forming 3 AE classes as shown in Fig. 14(c). Again, we may set the probabilities of updating an AE class given action-observation combinations to individual models at t=2 using the right hand side of Eq. 7. However, doing so does not meet the constraint represented by Eq. 7 because for example, Pr(mt=1,1j,l\u22121 |M t=0,p j,l\u22121 , a t=0 j , o t=1 j ) varies given different values of the conditionals. As an aside, it is not possible to meet this constraint in this example. Consequently, we adjust the CPT of the chance node, Mod[M t=2j,l\u22121], according to the average of the right hand side of Eq. 7 for different values of the conditional variables (see Fig. 14(c)). As a result, the AE based partitioning at t=1 is not exact.\nA manifestation of the approximation is that agent i may now think that j could initially open the right door, followed by listening and then open the left or right door again. Such a sequence of actions by j was not possible in the original policy graph shown in Fig. 14(a)."}, {"heading": "5.3 Algorithm", "text": "We provide an algorithm for exploiting AE in order to solve a level l \u2265 1 I-DID (as well as a level 0 DID) in Fig. 16. The algorithm starts by selectively solving lower-level I-DID or DID models at t = 0, which results in a set of policy trees (line 2). We then build the policy graph by merging the policy trees as mentioned in lines 1-13 of Fig 13. The algorithm differs from Fig. 13 in the expansion phase. In particular, we begin by grouping together AE models in the initial model node. This changes the value of the initial Mod node to the AE classes (lines 3-9). Subsequently, updated models that are AE are aggregated at all time steps, and the CPTs of the Mod nodes are revised to reflect the constraint involving AE classes (lines 13-24). As mentioned in Section 5.2, AE partitioning becomes inexact if we cannot find a CPT for the successor Mod node that satisfies Eq. 7. Given the expanded I-DID, we use the standard look-ahead and backup method to get the solution."}, {"heading": "5.4 Computational Savings", "text": "As we mentioned, the complexity of exactly solving a level l I-DID is, in part, due to solving the lower-level models of the other agent, and given the solutions, due to the exponentially growing space of models. In particular, at some time step t, there could be at most |M0j,l\u22121|(|Aj ||\u2126j |) t many models, where M0j,l\u22121 is the set of initial models of the other agent. While K \u226a |M 0 j,l\u22121| models are solved, considering AE bounds the model space to at most |Aj | distinct classes. Thus, the cardinality of the interactive state space in the I-DID is bounded by |S||Aj | elements at any time step. This is a significant reduction in the size of the state space. In doing so, we additionally incur the computational cost of merging the policy trees, which is O((|\u2126j |T\u22121)2|M0j,l\u22121|\n2) (from Proposition 2). We point out that this approach is applied recursively to solve I-DIDs at all levels down to 1, as shown in the algorithm."}, {"heading": "6. Empirical Results", "text": "We implemented the algorithms in Figs. 10, 13 and 16 and refer to the resulting techniques as Exact-BE, DMU and AE, respectively. In addition to these, we utilize the previous approximation technique of k-means clustering (Zeng et al., 2007), referred to as MC, and the exact approach without exploiting BE, referred to as Exact, as baselines. In MC, models are clustered based on the spatial closeness of their beliefs, and the clusters are refined iteratively until they stabilize. Because I-DIDs eventually transform to flat DIDs, we implemented them as a layer above the popular ID\ntool, HUGIN EXPERT V7.0. The transformed flat DIDs and level 0 DIDs were all solved using HUGIN to obtain the policy trees.\nAs benchmark problem domains, we evaluate the techniques on two well-known toy problems and a new scalable multiagent testbed with practical implications. One of the benchmarks is the two-agent generalization of the single agent tiger problem introduced previously in Section 3. As we mentioned, our formulation of this problem (|S|=2, |Ai|=|Aj |=3, |\u2126i|=6, |\u2126j |=2) follows the one introduced by Gmytrasiewicz and Doshi (2005), which differs from the formulation of Nair et al. (2003), in not being cooperative and having door creaks as additional observations. These observations are informative, though not perfectly, of j\u2019s actions. The other toy domain is a generalization of Smallwood and Sondik\u2019s machine maintenance problem (Smallwood & Sondik, 1973) to the two-agent domain. This problem (|S|=3, |Ai|=|Aj |=4, |\u2126i|=|\u2126j |=2) is fully described in Appendix B.2. I-DIDs for both these problem domains are shown in Section 3 and Appendix B.2, respectively. Decentralized POMDP solution techniques are not appropriate as baselines in cooperative problems such as machine maintenance because of the absence of a common initial belief among agents, and I-DIDs take the perspective of an agent in the interaction instead of computing the joint behavior.\nWhile the physical dimensions of these problems are small, the interactive state space that includes models of the other agent is an order of magnitude larger. Furthermore, they provide the advantage of facilitating detailed analysis of the solutions and uncovering interesting behaviors as previously demonstrated (Doshi et al., 2009). However, beyond increasing horizons, they do not allow an evaluation of the scalability of the techniques. In this context, we also evaluate the approaches within the Georgia testbed for autonomous control of vehicles (GaTAC) (Doshi & Sonu, 2010), which is a computer simulation framework for evaluating autonomous control of aerial robotic vehicles such as UAVs. Unmanned agents such as UAVs are used in fighting forest fires (Casbeer, Beard, McLain, Sai-Ming, & Mehra, 2005), law enforcement (Murphy & Cycon, 1998), and wartime reconnaissance. They operate in environments characterized by multiple parameters that affect their decisions, including other agents with common or antagonistic preferences. The task is further complicated as the vehicles may possess noisy sensors and unreliable actuators. GaTAC provides a low-cost and open-source alternative to highly complex and expensive simulation infrastructures.\nWe setup and execute experiments to evaluate the following: (a) We hypothesize that in sets of models attributed to the other agent, several are BE. This will lead to the exact approach that groups BE models (Exact-BE) being significantly more efficient than the plain approach (Exact). (b) Both approximation techniques (DMU and AE) will improve on the previous approximation technique of k-means clustering (MC). This is because MC generates all models at each time step before clustering and furthermore MC may retain BE models. (c) Finally, between DMU and AE, we hypothesize AE to be significantly more efficient because it forms as many classes as there are actions only. However, the solution quality resulting from DMU and AE will be explored."}, {"heading": "6.1 Improved Efficiency Due to BE", "text": "We report on the performance of the exact methods (Exact-BE and Exact) when used for solving both level 1 and 2 I-DIDs formulated for the small problem domains. As there are infinitely many computable models, we obtain the policy by exactly solving the I-DID given a finite set of models of the other agent initially, M0. In Fig. 17, we show the average rewards gathered by executing the\npolicy trees obtained from exactly solving both level 1 and 2 I-DIDs for the two problem domains, as a function of time allocated toward their solutions.\nEach data point is the average of 200 runs of executing the policies, where the true model of the other agent, j, is randomly selected according to i\u2019s belief distribution over j\u2019s models. The time consumed is a function of the initial number of models and the horizon of the I-DID, both of which are varied beginning with M0 = 50 at each level.\nFrom Fig. 17, we observe that Exact-BE performs significantly better than the Exact approach. Specifically, Exact-BE obtains the same amount of reward as Exact but in less time, and subsequently, for a given allocated time, it is able to obtain larger reward than Exact. This is because it is able to solve for a better quality solution in less time as it groups together BE models and retains a single representative from each class, thereby reducing the number of models held in each model node. We see significant improvement in performances in solving I-DIDs at both levels.\nWe uncover the main reason behind the improved performance of Exact-BE in Fig. 18. As we may expect, after grouping of BE models Exact-BE maintains much fewer classes of models (predicting a particular behavior for the other agent) than the number of individual models maintained by Exact. This occurs for all horizons and for both the problem domains. The number of models\nincrease as horizon reduces (but time steps increase) due to model updates; however, BE classes reduce with smaller horizon because of less distinct behaviors in the solutions.\nAs we increase the number of levels beyond two and model j more strategically, we expect Exact-BE (and Exact) to result in solutions whose average reward possibly improves but doesn\u2019t deteriorate. However, as the number of models increases exponentially with l, we expect substantially more computational resources to be consumed making it challenging to solve for deeper levels."}, {"heading": "6.2 Comparative Performance of Approximation Methods", "text": "While discriminating between model updates as described in Section 4 by itself does not lead to a loss in optimality, we combined it with the approach of solving K (which we will now call KDMU ) models out of the M0 models, and then solving those models which are not \u01eb-close to any of the KDMU models, to form the policy graph. Thus, we initially examine the behavior of the two parameters, KDMU and \u01eb, in how they regulate the performance of the DMU-based approximation technique for solving level 1 I-DIDs.\nWe show the performance of DMU for both the multiagent tiger and machine maintenance problems in Fig. 19. We also compare its performance with an implementation of MC; KMC represents the total number of models retained after clustering and pruning in the approach. The performance of MC is shown as flat lines because \u01eb does not play any role in the approach. Each data point for DMU is the average of 50 runs of executing the policies where the true model of the other agent, j, is randomly picked according to i\u2019s belief distribution over j\u2019s models, and solved exactly if possible. Otherwise, if l > 1, we solve it approximately using DMU with a large KDMU and small \u01eb. The plot is for M0 = 100, and a horizon of 10. As we increase the number of models randomly selected, KDMU , and reduce distance, \u01eb, the policies improve and converge toward the exact. Notice that DMU improves on the performance of MC as we reduce \u01eb, for KDMU = KMC . This behavior remains true for the multiagent machine maintenance problem as well.\nWe evaluate the impact of AE on solving level 1 I-DIDs for both problem domains and compare it to MC. The experiments were run analogously as before with different values for KAE and \u01eb. Both these parameters play roles that are similar to their use in DMU. We observe from Fig. 20 that\nas \u01eb reduces and more models beyond KAE are solved, the solution generated by AE improves on MC for the case where KAE = KMC . Of course, as we solve more models initially, AE produces better quality solutions because the generated policy graph includes more parts of the exact graph. Additionally, as we may expect, the solution quality approaches that of the exact and becomes significantly close to the exact (within one standard deviation for the tiger problem) for \u01eb < 0.1.\nWhile our previous experiments demonstrated that both DMU and AE are capable of improving on MC, it is not clear how many more initial models beyond KMC were solved to obtain the improvements. Furthermore, performance of DMU and AE were not compared. In Fig. 21, we directly compare the performance of DMU, AE and MC. In particular, we measure the average rewards obtained by corresponding solutions of level 2 I-DIDs as a function of time consumed by\nthe approaches. For DMU and AE, the time taken is dependent on the parameters (K and \u01eb), tree merging and the horizon of the I-DID solved. For MC, the time is due to the iterative clustering until convergence and the K models that are picked. For the problem domains considered, larger horizon with increasing K and reducing \u01eb typically leads to better average rewards. We observe that both DMU and AE significantly improve on MC \u2013 they produce identical quality solutions in less time than that by MC. Furthermore, between DMU and AE, the latter\u2019s performance is more favorable. This is because, while grouping AE models may result in an additional approximation, further efficiency is made possible by the fewer AE classes and whose number does not exceed a constant across horizons.\nWe empirically explore the reason behind the comparative performance of the approximation techniques. Because the time (and space) consumed by the approaches is predominantly due to the solution of the models in the model node, we focus on the models retained by the approaches at different horizons. Fig. 22 shows the models at different horizons for varying \u01eb and for both the problem domains. Note that when \u01eb = 0, all initial models are solved and in the case of DMU, results in the behaviorally minimal set at every horizon. Furthermore, as we mentioned previously, for non-zero \u01eb, the merged policy graph is a subgraph of the exact (\u01eb = 0) case. As we show, the resulting sets of models are subsets of the minimal set.\nAt any horizon, AE maintains no more model classes than the number of j\u2019s actions, |Aj |. As we see, this is substantially less than the number maintained by DMU. MC maintains a fixed number, KMC , of models at each horizon of the I-DID."}, {"heading": "6.3 Runtime Comparison", "text": "We show the run times of the exact and approximation techniques for solving level 1 and 2 I-DIDs while scaling in horizons, in Table 1. Notice that a plain exact approach that does not exploit model equivalences scales poorly beyond small horizons. In contrast, simply grouping BE models and reducing the exponential growth in models leads to significantly faster executions and better scaleup. The run times are reported for both these approaches solving the same I-DID exactly.\nIn obtaining the run times for the approximations, we adjusted the corresponding parameters so that the quality of the solution by each approach was similar to each other. DMU and AE reported\nsubstantially less execution times and better scaleup in comparison to MC for both the domains. However, the run times of DMU and AE are relatively similar for level 1 I-DIDs. Although, as we saw previously, there is a difference in the number of models maintained by the two techniques, solving j\u2019s level 0 DIDs is quick and the difference does not lead to a significant impact. The differences in run times are more significant for level 2 I-DIDs because solving j\u2019s level 1 I-DIDs is computationally intensive. As we may expect, AE consumes substantially less time in comparison to DMU \u2013 sometimes less than half. Both approaches scale up similarly in terms of the horizon. In particular, we are able to solve both level 1 and 2 I-DIDs for more than a horizon of 10. Further scaleup is limited predominantly due to our use of software such as HUGIN for solving the flat DIDs, which seeks to keep the entire transformed DID in main memory."}, {"heading": "6.4 Scalable Testbed: GaTAC", "text": "As we mentioned, the objective behind developing GaTAC is to provide a realistic and scalable testbed for algorithms on multiagent decision making. GaTAC facilitates this by providing an intuitive and easy to deploy architecture that makes use of powerful, open-source software components. Successful demonstrations of algorithms in GaTAC would not only represent tangible gains but have the potential for practical applications toward designing autonomous vehicles such as UAVs. 4\n4. GaTAC is available for download at http://thinc.cs.uga.edu/thinclabwiki/index.php/ GaTAC_:_Georgia_Testbed_for_Autonomous_Control_of_Vehicles\nA simplified design of the GaTAC architecture is shown in Fig. 23, where a manually controlled UAV is interacting with an autonomous one. Briefly, GaTAC employs multiple instances of an opensource flight simulator, called FlightGear (Perry, 2004), possibly on different networked platforms that communicate with each other via external servers, and an autonomous control module that interacts with the simulator instances. GaTAC can be deployed on most platforms including Linux\nand Windows with moderate hardware requirements, and the entire source code is available under GNU Affero public license version 3.\nWe utilize a relatively straightforward setting consisting of another hostile fugitive, who is the target of ground reconnaissance (Fig. 24). UAV I must track down the fugitive before he flees to the safe house. The problem is made complex by assuming that the fugitive is unaware of its own precise location though it knows the location of the safe house, and I may not be aware of the fugitive\u2019s location. The problem is further complicated if we realistically assume nondeterministic actions and observations. Our simulations in GaTAC include grids of sizes 3 \u00d7 3 and 5 \u00d7 5 with the same actors in each. GaTAC may be programmed to support more complex scenarios comprising of a team of UAVs, multiple hostile UAVs and reconnaissance targets attempting to blend in with civilians.\nWe summarize our formulation of the UAV\u2019s problem domain. We utilize the possible relative positions of the fugitive as states. Hence, possible states would be same, north, south, east, west, north-west, and so on. Our representation for a 3 \u00d7 3 theater consists of 25 physical states\nfor the UAV I . We assume that the fugitive is unaware of its own location resulting in 9 physical states for it. Extending the theater to a 5 \u00d7 5 grid leads to 81 physical states for the UAV and 25 for the fugitive. We factor the physical state into two variables in the I-DID that model the row and column positions, respectively. Both UAV I and the fugitive may move in one of the four cardinal directions, and they may additionally hover at their current positions and listen to get informative observations. Thus the actions for both I and the fugitive are {move north, move south, move west, move east, listen}. We may synchronize the actions for the two agents in GaTAC by allocating equal time duration to the performance of each action. Typically, UAVs have infrared and camera sensors whose range is limited. Accordingly, we assume that both the UAV I and the fugitive can sense whether their respective target is north of them (sense north), south of them (sense south), west or east of them in the same row (sense level) or in the same location as them (sense found). For I the target is the fugitive, while the fugitive\u2019s target is the safe house. If we assume that the fugitive is unaware of I\u2019s presence, its transition function is straightforward and simply reflects the possible nondeterministic change in grid location of the fugitive as it moves or listens. However, transitions in physical state of I are contingent on the joint actions of both agents. Furthermore, the probability distribution over the next states is not only due to the nondeterminism of the actions, but is also influenced by the current relative physical state. To provide an opportunity for the UAV I to catch the fugitive, we assume that the fugitive can sense the safe house only when it is within a distance of 1 sector (horizontally or vertically) from it. On the other hand, UAV I\u2019s observations of the fugitive are not limited by this constraint. Thus, if the fugitive is in any location that is north of I (including north-west or north-east), I receives an observation of sense north. To simulate noise in the sensors, we assume that the likelihood of the correct observation is 0.8 while all others are equiprobable. The reward function is straightforward with the fugitive receiving a reward if its location is identical to that of the safe house, and small costs for performing actions to discourage excessive\naction taking. Analogously, UAV I recieves a reward on performing an action and receiving an observation of sense found, and incurs small costs for actions that lead to other observations.\nWe modeled the problem formulation described above using a level 1 I-DID for the UAV I and level 0 models for the fugitive. We show two example policies of the fugitive obtained by solving its level 0 models, in Fig. 25(a). While we considered several models for the fugitive with differing initial beliefs, the fugitive\u2019s initial belief of likely being just below the safe house results in the left policy, while its initial belief of likely being south and east of the safe house leads to the policy on the right. We show the UAV\u2019s policy of reconnaissance in Fig. 25(b), obtained by solving its level 1 I-DID exactly while utilizing BE classes. Thirty models of the fugitive grouped into 16 BE classes were considered in the I-DID. Here, the UAV initially believes that the fugitive is likely to be in the same row or south of it.\nWe simulate the reconnaissance theaters of Fig. 24 in GaTAC. The UAV and the fugitive\u2019s behaviors are controlled by their respective policies provided as input to the autonomous control module. For each simulation run, we generated the UAV\u2019s policy by solving its level 1 I-DID using either Exact-BE, DMU or AE, and sampled one of the fugitive\u2019s 30 models based on the UAV\u2019s initial belief. The run terminates when either the fugitive reaches the safe house, or the UAV spots the fugitive by entering the same sector as the fugitive. In the case of DMU and AE, we used parameters, K = 13 and \u01eb = 0.3 for the 3 \u00d7 3 problem size, and K = 17, \u01eb = 0.15 for the 5 \u00d7 5 problem. We show the average reward gathered by the UAV across 20 simulation runs for each of the three approaches in Fig. 26(a) and the associated clock time for solving the I-DIDs in Fig. 26(b). While we considered several different beliefs for the UAV, positioning itself approximately between the fugitive and the safe house yielded a fugitive capture rate of 65% among the simulation runs and an escape rate of 25%. The remaining runs did not result in a capture or an escape.\nWhile exactly solving the I-DID using Exact-BE continues to provide the largest reward among all approaches, as shown in Fig. 26(a), it fails to scale to a longer horizon or problem size. Both DMU and AE scale, although AE performs worse than DMU in the context of reward in this domain. Note that longer horizons result in overall better quality policies in this problem domain, as we may expect. This is because the UAV is able to plan its initial action better. Finally, the improved reward obtained by AE relative to DMU\u2019s as the horizon increases from 6 to 8 for the 3 \u00d7 3 grid, and\nthe slight climb in AE\u2019s relative reward as a percentage of DMU\u2019s from about 57% to 62% for the horizon of 6 as the grid size is scaled from 3 \u00d7 3 to 5 \u00d7 5 makes us believe that AE\u2019s performance will not necessarily deteriorate compared to DMU for larger problems. Overall, we demonstrate the scalability of DMU and AE by increasing the horizon to 8 for this larger problem domain, and further scaling its size. Larger grid sizes or longer horizons resulted in DIDs that could not be solved by HUGIN given the fixed memory.\nFinally, we handpick three simulations from the numerous that we carried out and show the corresponding trajectories of the UAV and the fugitive in Fig. 27. We show two trajectories where the UAV spots the fugitive and a trajectory where the fugitive successfully escapes to the safehouse. Figure 27 shows that the trajectories of the UAV can get quite complicated while those of the fugitive are more straightforward due to its low strategic awareness."}, {"heading": "7. Discussion", "text": "Graphical models are an appealing formalism for modeling decision making due to the convenience of representation and the increased efficiency of the solution. DIDs are a key contribution in this regard. I-DIDs are founded on the normative paradigm of decision theory as formalized by DIDs and augmented with aspects of Bayesian games (Harsanyi, 1967) and interactive epistemology (Aumann, 1999a, 1999b) to make them applicable to interactions. I-DIDs generalize DIDs to multiagent settings thereby extending the advantages of DIDs to decision making in multiagent settings. I-DIDs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic formalism that takes a decision-makers perspective in an interaction which may be cooperative or non-cooperative. The broader impact is that understanding the agent\u2019s decision-making process facilitates planning and problem-solving at its own level and in the absence of centralized controllers or assumptions about agent behaviors. In a game-theoretic sense, the setting modeled by I-DIDs is a partially observable stochastic game, and solving it by computing Nash equilibria or otherwise, has received minimal attention in game theory.\nWe presented a collection of exact and approximation algorithms for scalably solving I-DIDs. These algorithms improve on early techniques by providing more effective approaches in order to reduce the exponential growth of other agents\u2019 models at each time step. Our main idea is to cluster models attributed to other agents that are BE. These models attribute identical behaviors across all time steps to the other agent. We then select representative models from each cluster without loss of optimality in the solution. Instead of generating the updated models and clustering them, we showed how we may selectively update those models that will not be BE to existing models in the next time step. Nevertheless, ascertaining BE requires solving the initial set of models. In order to approximate this, we proposed solving K randomly picked models followed by all those which are not \u01eb-close to any of the K models. We partially bounded the error due to this approximation for some cases. Despite the lack of a proper bound, our empirical results reveal that the error becomes unwieldy for large \u01eb values only. This is because many problems admit large BE regions for models, albeit which tend to reduce as horizon increases.\nIn order to further reduce the number of equivalence classes, we investigated grouping together models whose prescribed actions at a particular time step are identical. This approach is appealing because the number of AE classes is upper bounded by the number of distinct actions at any time. While AE models may be grouped without loss in optimality, we identified the conditions under which AE leads to an approximation. Our experiments indicate that considerations of BE are of significance while grouping AE models leads to most reduction in the model space among all the different approaches. However, they also show that the gap in the quality of the solutions due to grouping BE and grouping AE models can become large. This difference depends on the domain characteristics and does not necessarily worsen as the problem is scaled in horizon or size.\nDue to the expressiveness of the modeling and its ensuing complexity, our experimentation focused on settings involving two agents. However, as the number of other agents increases, po-\ntential computational savings due to exploiting BE and AE assumes greater significance. This is because the space of interactive states increases exponentially with the number of agents. Therefore, grouping agent models in less numbers of classes would substantially reduce the state space and consequently the size of the I-DID. We may group models separately for each agent in which case the computations for ascertaining BE classes grow linearly with the number of agents. On the other hand, consider the tiger problem where agent i\u2019s action is affected by somebody opening the door, without the need for knowing which particular agent opened it. In this case, we may group together models that are BE but belonging to different agents, leading to increased savings. Our preliminary experimentation in the context of the multiagent tiger problem in a setting involving two other agents (total of three agents) one of which is thought to be cooperative while the other adversarial, indicates that grouping BE models for each other agent leads to a speed up of about 7 for both a 3-horizon and 5-horizon I-DID. Specifically, the computation time reduces from 4.8s for Exact to 0.6s for Exact-BE when the horizon is 3, and from 72.6s to 10.5s for Exact-BE when the horizon is 5.\nBecause identifying exact BE is computationally intensive, we think that improved scalability may be brought by further investigating approximate BE of models. This would allow us to form larger clusters and have fewer representative models. While we are investigating multiple ways of doing this, a significant challenge is storing policy trees that grow exponentially as the horizon is further scaled. One promising approach in this regard is to compare partial policy trees of bounded depth and the distance between the belief vectors at the leaves of the trees. This allows us to define an approximate measure of BE based on the distance between the updated belief vectors given that the bounded-depth policy trees are identical. However, our preliminary investigations reveal that deriving the depth of the tree becomes challenging for certain types of problems.\nA general limitation of utilizing the spatial closeness of beliefs for approximately identifying BE models is that the error may be larger if the frames in the models differ. This is because model beliefs that are close are still less likely to result in the same behavior if say, the reward functions are different. In the absence of this approximation, our approaches for discriminatively updating models and grouping AE models continue to apply if frames are also uncertain because they operate on model solutions \u2013 policy trees and actions \u2013 and not on model specifications. Another impact of considering frame uncertainty is that the Markov blanket shown in Fig. 15 changes. A general hurdle is that further scalability of ID-based graphical models is also limited by the absence of state-ofthe-art techniques for solving DIDs within commercial implementations such as HUGIN EXPERT that predominantly rely on solving the entire DID in main memory. Although newer versions of HUGIN allow the use of limited memory IDs (Nilsson & Lauritzen, 2000), recent advances such as a branch-and-bound approach for solving multistage IDs (Yuan, Wu, & Hansen, 2010) would help drive further scalability of I-DID solutions."}, {"heading": "Acknowledgments", "text": "Yifeng Zeng acknowledges support from the Obel Family Foundation (Denmark) and NSFC (# 60974089 and # 60975052). Prashant Doshi acknowledges support from an NSF CAREER grant (# IIS-0845036) and a grant from U.S. Air Force (# FA9550-08-1-0429). The authors also thank Ekhlas Sonu and Yingke Chen for help in performing the GaTAC-based simulations, and acknowledge all the anonymous reviewers for their helpful comments."}, {"heading": "Appendix A. Proofs", "text": "Proof of Proposition 1. We prove by induction on the horizon. Let {M1j,l\u22121, . . . ,M q j,l\u22121} be the collection of behaviorally equivalent sets of models in Mj,l\u22121. We aim to show that the value of each of i\u2019s actions in the decision nodes at each time step remains unchanged on application of the transformation, X . This implies that the solution of the I-DID is preserved. Let Qn(bi,l, ai) give the action value at horizon n. Its computation in the I-DID could be modeled using the standard dynamic programming approach. Let ERi(s,mj,l\u22121, ai) be the expected immediate reward for agent i averaged over j\u2019s predicted actions. Then, \u2200mq\nj,l\u22121 \u2208Mq j,l\u22121 ERi(s,m q j,l\u22121, ai) = \u2211 aj Ri(s, ai, aj)\nPr(aj |m q j,l\u22121) = Ri(s, ai, a q j), because a q j is optimal for all m q j,l\u22121 \u2208 M q j,l\u22121.\nBasis step: Q1(bi,l, ai) = \u2211\ns,mj,l\u22121 bi,l(s,mj,l\u22121)ERi(s,mj,l\u22121, ai)\n= \u2211 s,q bi,l(s) \u2211 m q\nj,l\u22121 \u2208Mq j,l\u22121 bi,l(m\nq j,l\u22121|s)Ri(s, ai, a q j) (a q j is optimal for all behaviorally equivalent\nmodels in Mqj,l\u22121) = \u2211 s,q bi,l(s)Ri(s, ai, a q j) \u2211 m q\nj,l\u22121 \u2208Mq j,l\u22121 bi,l(m\nq j,l\u22121|s)\n= \u2211\ns,q bi,l(s)Ri(s, ai, a q j)b\u0302i,l(m\u0302 q j,l\u22121|s) (from Eq. 1)\n= \u2211\ns,q b\u0302i,l(s, m\u0302 q j,l\u22121)ERi(s, m\u0302 q j,l\u22121, ai) (a q j is optimal for representative m\u0302 q j,l\u22121)\n= Q\u03021(b\u0302i,l, ai) Inductive hypothesis: Let, \u2200ai,bi,l Q n(bi,l, ai) = Q\u0302 n(b\u0302i,l, ai), where b\u0302i,l relates to bi,l using Eq. 1. Therefore, Un(bi,l) = U\u0302n(b\u0302i,l) where Un(bi,l) is the expected utility of bi,l for horizon n. Inductive proof: Qn+1(bi,l, ai) = Q\u03021(b\u0302i,l, ai) + \u2211 oi,s,mj,l\u22121,aj Pr(oi|s, ai, aj) Pr(aj |mj,l\u22121)bi,l(s,mj,l\u22121)U n(b\u2032i,l) (basis step) = Q\u03021(b\u0302i,l, ai) + \u2211\noi,s,q Pr(oi|s, ai, a q j) bi,l(s) \u2211 m q j,l\u22121 \u2208Mq j,l\u22121 bi,l(m q j,l\u22121|s) U n(b\u2032i,l) (a q j is opti-\nmal for models in Mqj,l\u22121) = Q\u03021(b\u0302i,l, ai) + \u2211\noi,s,q Pr(oi|s, ai, a q j) bi,l(s) \u2211 m q j,l\u22121 \u2208Mq j,l\u22121 bi,l(m q j,l\u22121|s) U\u0302 n(b\u0302\u2032i,l) (using the\ninductive hypothesis) = Q\u03021(b\u0302i,l, ai) + \u2211 oi,s,q Pr(oi|s, ai, a q j) bi,l(s) b\u0302i,l(m\u0302 q j,l\u22121|s) U\u0302 n(b\u0302\u2032i,l) (from Eq. 1) = Q\u03021(b\u0302i,l, ai) + \u2211\noi,s,q Pr(oi|s, ai, a q j) b\u0302i,l(s, m\u0302 q j,l\u22121) U\u0302 n(b\u0302\u2032i,l)\n= Q\u0302n+1(b\u0302i,l, ai)\nCalculating prediction error in Section 4.5. Let mj,l\u22121 be the model associated with a solved model, m\u2032j,l\u22121, resulting in the worst error. Let \u03b1 be the exact policy tree obtained by solving mj,l\u22121 optimally and \u03b1\u2032 be the policy tree for m\u2032j,l\u22121. As m \u2032 j,l\u22121 is itself solved inexactly due to approximate solutions of lower level models, let \u03b1\u2032\u2032 be the exact policy tree that is optimal for m\u2032j,l\u22121. If bj,l\u22121 is the belief in mj,l\u22121 and b \u2032 j,l\u22121 in m \u2032 j,l\u22121, then the error is:\nE = |\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032 \u00b7 bj,l\u22121|\n= |\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032 \u00b7 bj,l\u22121 + (\u03b1 \u2032\u2032 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032\u2032 \u00b7 bj,l\u22121)| (add zero) = |(\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032\u2032 \u00b7 bj,l\u22121) + (\u03b1 \u2032\u2032 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032 \u00b7 bj,l\u22121)| \u2264 |(\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032\u2032 \u00b7 bj,l\u22121)|+ |(\u03b1 \u2032\u2032 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032 \u00b7 bj,l\u22121)| (triangle inequality)\n(8)\nFor the first term, |\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1\u2032\u2032 \u00b7 bj,l\u22121|, which we denote by \u03c1, the error is due to associating mj,l\u22121 with m\u2032j,l\u22121, both solved exactly. We analyze this error below:\n\u03c1 = |\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032\u2032 \u00b7 bj,l\u22121|\n= |\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1 \u2032\u2032 \u00b7 b\u2032j,l\u22121 + \u03b1 \u2032\u2032 \u00b7 b\u2032j,l\u22121 \u2212 \u03b1 \u2032\u2032 \u00b7 bj,l\u22121| (add zero) \u2264 |\u03b1 \u00b7 bj,l\u22121 \u2212 \u03b1 \u00b7 b \u2032 j,l\u22121 + \u03b1 \u2032\u2032 \u00b7 b\u2032j,l\u22121 \u2212 \u03b1 \u2032\u2032 \u00b7 bj,l\u22121| (\u03b1 \u2032\u2032 \u00b7 b\u2032j,l\u22121 \u2265 \u03b1 \u00b7 b \u2032 j,l\u22121) = |\u03b1 \u00b7 (bj,l\u22121 \u2212 b \u2032 j,l\u22121)\u2212 \u03b1 \u2032\u2032 \u00b7 (bj,l\u22121 \u2212 b \u2032 j,l\u22121)| = |(\u03b1\u2212 \u03b1\u2032\u2032) \u00b7 (bj,l\u22121 \u2212 b \u2032 j,l\u22121)| \u2264 ||\u03b1\u2212 \u03b1\u2032\u2032||\u221e \u00d7 ||bj,l\u22121 \u2212 b \u2032\nj,l\u22121||1 (Ho\u0308lder\u2019s inequality) \u2264 (Rmaxj \u2212R min j )T \u00d7 \u01eb\n(9)\nIn the above inequality, the largest difference between bj,l\u22121 and b\u2032j,l\u22121 is \u01eb, otherwise model, mj,l\u22121 with belief bj,l\u22121, would be solved. Notice that the error is regulated by \u01eb, and as \u01eb increases, we solve less models beyond K and the approximation error worsens.\nIn subsequent time steps, because the sets of models could be subsets of the minimal sets, the updated probabilities could be transferred to incorrect models. In the worst case, the error incurred is bounded analogously to Eq. 9. Hence, the cumulative error in j\u2019s predicted behavior over T steps is at most T \u00d7 \u03c1, which is similar to that of the previous k-means model clustering approach (Zeng et al., 2007):\n\u03c1T \u2264 (Rmaxj \u2212R min j )T 2\u01eb\nThe second term, |(\u03b1\u2032\u2032 \u00b7 bj,l\u22121 \u2212\u03b1\u2032 \u00b7 bj,l\u22121)|, in Eq. 8 represents the error due to the approximate solutions of models further down in level (for example, i\u2019s level l \u2212 2 models). Since j\u2019s behavior depends, in part, on the actions of i (and not on the value of i\u2019s solution), even a slight deviation by j from the exact prediction for i could lead to j\u2019s behavior with the worst error. Hence, it seems difficult to derive bounds for the second term that are tighter than the usual, (Rmaxj \u2212R min j )T .\nConsequently, the total error in predicting j\u2019s behavior is bounded if lower-level models are solved exactly. Otherwise, as we show in Section 6.2, the error is large only for very large \u01eb. This is because many problems admit large BE regions for the models thereby not overconstraining \u01eb, and the prediction continues to remain exact. However, we noticed that these regions do reduce in size as the horizon increases. In summary, although the error due to associating different models whose beliefs are \u01eb-close is bounded, we are unable to usefully bound the overall error in prediction due to approximate solutions of lower-level models."}, {"heading": "Appendix B. Problem Domains", "text": "We provide detailed descriptions of all the problem domains utilized in our evaluations, including their I-DID models, below.\nB.1 Multiagent Tiger Problem\nAs we mentioned previously, our multiagent tiger problem is a non-cooperative generalization of the well-known single agent tiger problem (Kaelbling et al., 1998) to the multiagent setting. It differs from other multiagent versions of the same problem (Nair et al., 2003) by assuming that the agents hear creaks as well as the growls and the reward function does not promote cooperation. Creaks are indicative of which door was opened by the other agent(s). While we described the problem in Section 3, we quantify the different uncertainties here. We assume that the accuracy of creaks is 90%, while the accuracy of growls is 85% as in the single agent problem. The tiger location is chosen randomly in the next time step if any of the agents opened any doors in the current step.\nFig. 8 shows an I-DID unrolled over two time-slices for the multiagent tiger problem. We give the CPTs for the different nodes below:\nWe assign the marginal distribution over the tiger\u2019s location from agent i\u2019s initial belief to the chance node, TigerLocationt. The CPT of TigerLocationt+1 in the next time step conditioned on TigerLocationt, Ati, and A t j is the transition function, shown in Table 2. The CPT of the observation node, Growl&Creakt+1, is shown in Table 3. CPTs of the observation nodes in level 0 DIDs are identical to the observation function in the single agent tiger problem.\nDecision nodes, Ati and A t+1 i , contain possible actions of agent i such as L, OL, and OR. Model node, M tj,l\u22121, contains the different models of agent j which are DIDs if the I-DID is at level 0, otherwise they are I-DIDs themselves. The distribution over the associated Mod[M tj ] node (see Fig. 9) is the conditional distribution over j\u2019s models given physical state from agent i\u2019s initial belief. The CPT of the chance node, Mod[M t+1j ], in the model node, M t+1 j,l\u22121, reflects which prior model, action and observation of j results in a model contained in the model node.\nFinally, the utility node, Ri, in the I-DID relies on both agents\u2019 actions, Ati and A t j , and the physical states, TigerLocationt. The utility table is shown in Table 4. These payoffs are analogous to the single agent version, which assigns a reward of 10 if the correct door is opened, a penalty of 100 if the opened door is the one behind which is a tiger, and a penalty of 1 for listening. A result of this assumption is that the other agent\u2019s actions do not impact the original agent\u2019s payoffs directly, but rather indirectly by resulting in states that matter to the original agent. The utility tables for level 0 models are exactly identical to the reward function in the single agent tiger problem.\nB.2 Multiagent Machine Maintenance Problem\nWe extend the traditional single agent based machine maintenance (MM) problem (Smallwood & Sondik, 1973) to a two-agent cooperative version. Smallwood and Sondik (1973) described an MM problem involving a machine containing two internal components. Either one or both components of the machine may fail spontaneously after each production cycle (0-fail: no component fails; 1- fail: 1 component fails; 2-fail: both components fail). If an internal component has failed, then there is some chance that when operating upon the product, it will cause the product to be defective. An agent may choose to manufacture the product (M) without examining it, examine the product (E), inspect the machine (I), or repair it (R) before the next production cycle. On an examination of the product, the subject may find it to be defective. Of course, if more components have failed, then the probability that the product is defective is greater.\nA level l I-DID for the multiagent MM problem is shown in Fig. 28. We consider M models of agent j at the lower level which differ in the probability that j assigns to the chance node Machine Failure. Agent i\u2019s initial belief over the physical state and j\u2019s models provides the marginal distribution over MachineFailuret. In the I-DID, the chance node, MachineFailuret+1, has incident\narcs from the nodes MachineFailuret, Ati, and A t j . In Table 5, we show the CPT of the chance node.\nWith the observation chance node, Defectivet+1i , we associate the CPT shown in Table 6. Note that arcs from MachineFailuret+1 and the nodes, Ati and A t j , in the previous time step are incident to this node. The observation nodes in the level 0 DIDs have CPTs that are identical to the observation function in the original MM problem.\nThe decision node, Ai, has one information arc from the observation node Defectiveti indicating that i knows the examination results before making the choice. The utility node Ri is associated with the utility table in Table 7. The utility table for a level 0 agent is identical to the one in the original MM problem.\nThe CPT of the chance node, Mod[M t+1j ], in the model node, M t+1 j,l\u22121, reflects which prior model, action and observation of j results in a model contained in the model node, analogously to the tiger problem.\nB.3 UAV Reconnaissance Problem\nWe show a level l I-DID for the multiagent UAV problem in Fig 29. Models of the fugitive (agent j) at the lower level differ in the probability that the fugitive assigns to its position in the grid. The UAV\u2019s (agent i) initial beliefs are probability distributions assigned to the relative position of the fugitive decomposed into the chance nodes, FugRelPosXt and FugRelPosY t, which represent the relative location of the fugitive along the row and column, respectively. Its CPTs assume that each action (except listen) moves the UAV in the intended direction with a probability of 0.67, while the remaining probability is equally divided among the other neighboring positions. Action listen keeps the UAV in the same position.\nThe observation node, SenFug, represents the UAV\u2019s sensing of the relative position of the fugitive in the grid. Its CPT assumes that the UAV has good sensing capability (likelihood of 0.8 for the correct relative location of the fugitive) if the action is listen, otherwise the UAV receives random observations during other actions.\nThe decision node, Ai, contains five actions of the UAV, which includes moving in the four cardinal directions and listening. The edge incident into the node indicates that the UAV ascertains the observation on the relative position of the fugitive before it takes an action.\nThe utility node, Ri, is the reward assigned to the UAV for its actions given the fugitive\u2019s relative position and its actions. The UAV gets rewarded 50 if it captures the fugitive; otherwise, it costs -5 for performing any other action.\nBecause the actual CPT tables are very large, we do not show them here. All problem domain files are available upon request."}], "references": [{"title": "Hugin: A shell for building belief universes for expert systems", "author": ["S. Andersen", "F. Jensen"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Andersen and Jensen,? \\Q1989\\E", "shortCiteRegEx": "Andersen and Jensen", "year": 1989}, {"title": "Interactive epistemology i: Knowledge", "author": ["R.J. Aumann"], "venue": "International Journal of Game Theory,", "citeRegEx": "Aumann,? \\Q1999\\E", "shortCiteRegEx": "Aumann", "year": 1999}, {"title": "Interactive epistemology ii: Probability", "author": ["R.J. Aumann"], "venue": "International Journal of Game Theory,", "citeRegEx": "Aumann,? \\Q1999\\E", "shortCiteRegEx": "Aumann", "year": 1999}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Hierarchies of beliefs and common knowledge", "author": ["A. Brandenburger", "E. Dekel"], "venue": "Journal of Economic Theory,", "citeRegEx": "Brandenburger and Dekel,? \\Q1993\\E", "shortCiteRegEx": "Brandenburger and Dekel", "year": 1993}, {"title": "Forest fire monitoring with multiple small uavs", "author": ["D. Casbeer", "R. Beard", "T. McLain", "L. Sai-Ming", "R. Mehra"], "venue": "In American Control Conference,", "citeRegEx": "Casbeer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Casbeer et al\\.", "year": 2005}, {"title": "Equivalence relations in fully and partially observable markov decision processes", "author": ["P. Castro", "P. Panangaden", "D. Precup"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Castro et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2009}, {"title": "Refinement and coarsening of bayesian networks", "author": ["Chang", "K.-C", "R. Fung"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Chang et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Chang et al\\.", "year": 1991}, {"title": "Gatac: A scalable and realistic testbed for multiagent decision making)", "author": ["P. Doshi", "E. Sonu"], "venue": "In Fifth Workshop on Multiagent Sequential Decision Making in Uncertain Domains (MSDM),", "citeRegEx": "Doshi and Sonu,? \\Q2010\\E", "shortCiteRegEx": "Doshi and Sonu", "year": 2010}, {"title": "Graphical models for interactive pomdps: Representations and solutions", "author": ["P. Doshi", "Y. Zeng", "Q. Chen"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS),", "citeRegEx": "Doshi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi et al\\.", "year": 2009}, {"title": "Knowledge combination in graphical multiagent models", "author": ["Q. Duong", "M. Wellman", "S. Singh"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Duong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2008}, {"title": "History-dependent graphical multiagent models", "author": ["Q. Duong", "M. Wellman", "S. Singh", "Y. Vorobeychik"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Duong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2010}, {"title": "Game theoretic control for robot teams", "author": ["R. Emery-Montemerlo", "G. Gordon", "J. Schneider", "S. Thrun"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Emery.Montemerlo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Emery.Montemerlo et al\\.", "year": 2005}, {"title": "Networks of influence diagrams: A formalism for representing agents\u2019 beliefs and decision-making processes", "author": ["Y. Gal", "A. Pfeffer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gal and Pfeffer,? \\Q2008\\E", "shortCiteRegEx": "Gal and Pfeffer", "year": 2008}, {"title": "Equivalence notions and model minimization in markov decision processes", "author": ["R. Givan", "T. Dean", "M. Greig"], "venue": "Artificial Intelligence,", "citeRegEx": "Givan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Givan et al\\.", "year": 2003}, {"title": "A framework for sequential planning in multiagent settings", "author": ["P. Gmytrasiewicz", "P. Doshi"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Gmytrasiewicz and Doshi,? \\Q2005\\E", "shortCiteRegEx": "Gmytrasiewicz and Doshi", "year": 2005}, {"title": "Games with incomplete information played by bayesian players", "author": ["J.C. Harsanyi"], "venue": "Management Science,", "citeRegEx": "Harsanyi,? \\Q1967\\E", "shortCiteRegEx": "Harsanyi", "year": 1967}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence Journal,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Graphical models for game theory", "author": ["M. Kearns", "M. Littman", "S. Singh"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kearns et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2001}, {"title": "Multi-agent influence diagrams for representing and solving games", "author": ["D. Koller", "B. Milch"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Koller and Milch,? \\Q2001\\E", "shortCiteRegEx": "Koller and Milch", "year": 2001}, {"title": "An influence diagram framework for acting under influence by agents with unknown goals", "author": ["N.S. Madsen", "F.V. Jensen"], "venue": "In Fourth European Workshop on Probabilistic Graphical Models (PGM),", "citeRegEx": "Madsen and Jensen,? \\Q2008\\E", "shortCiteRegEx": "Madsen and Jensen", "year": 2008}, {"title": "Formulation of bayesian analysis for games with incomplete information", "author": ["J. Mertens", "S. Zamir"], "venue": "International Journal of Game Theory,", "citeRegEx": "Mertens and Zamir,? \\Q1985\\E", "shortCiteRegEx": "Mertens and Zamir", "year": 1985}, {"title": "A Calculus of Communicating Systems", "author": ["R. Milner"], "venue": null, "citeRegEx": "Milner,? \\Q1980\\E", "shortCiteRegEx": "Milner", "year": 1980}, {"title": "Applications for mini vtol uav for law enforcement. In SPIE 3577:Sensors, C3I, Information, and Training Technologies for Law Enforcement", "author": ["D. Murphy", "J. Cycon"], "venue": null, "citeRegEx": "Murphy and Cycon,? \\Q1998\\E", "shortCiteRegEx": "Murphy and Cycon", "year": 1998}, {"title": "Taming decentralized pomdps : Towards efficient policy computation for multiagent settings", "author": ["R. Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Nair et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2003}, {"title": "Evaluating influence diagrams using limids", "author": ["D. Nilsson", "S. Lauritzen"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Nilsson and Lauritzen,? \\Q2000\\E", "shortCiteRegEx": "Nilsson and Lauritzen", "year": 2000}, {"title": "Lossless clustering of histories in decentralized pomdps", "author": ["F.A. Oliehoek", "S. Whiteson", "M.T.J. Spaan"], "venue": "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "Oliehoek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2009}, {"title": "The flightgear flight simulator. In UseLinux", "author": ["A.R. Perry"], "venue": null, "citeRegEx": "Perry,? \\Q2004\\E", "shortCiteRegEx": "Perry", "year": 2004}, {"title": "Anytime point-based approximations for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Minimal mental models", "author": ["D. Pynadath", "S. Marsella"], "venue": "In Twenty-Second Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pynadath and Marsella,? \\Q2007\\E", "shortCiteRegEx": "Pynadath and Marsella", "year": 2007}, {"title": "Exact solutions to interactive pomdps using behavioral equivalence", "author": ["B. Rathnasabapathy", "P. Doshi", "P.J. Gmytrasiewicz"], "venue": "In Autonomous Agents and Multi-Agents Systems Conference (AAMAS),", "citeRegEx": "Rathnasabapathy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rathnasabapathy et al\\.", "year": 2006}, {"title": "Artificial Intelligence: A Modern Approach (Third Edition)", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "Memory-bounded dynamic programming for dec-pomdps", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Seuken and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "The optimal control of partially observable markov decision processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research (OR),", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Learning models of other agents using influence diagrams", "author": ["D. Suryadi", "P. Gmytrasiewicz"], "venue": "In International Conference on User Modeling,", "citeRegEx": "Suryadi and Gmytrasiewicz,? \\Q1999\\E", "shortCiteRegEx": "Suryadi and Gmytrasiewicz", "year": 1999}, {"title": "Dynamic programming and influence diagrams", "author": ["J.A. Tatman", "R.D. Shachter"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Tatman and Shachter,? \\Q1990\\E", "shortCiteRegEx": "Tatman and Shachter", "year": 1990}, {"title": "Influence-based policy abstraction for weakly-coupled dec-pomdps", "author": ["S.J. Witwicki", "E.H. Durfee"], "venue": "In International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Witwicki and Durfee,? \\Q2010\\E", "shortCiteRegEx": "Witwicki and Durfee", "year": 2010}, {"title": "Solving multistage influence diagrams using branch-andbound search", "author": ["C. Yuan", "X. Wu", "E. Hansen"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Yuan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2010}, {"title": "Approximate solutions of interactive dynamic influence diagrams using model clustering", "author": ["Y. Zeng", "P. Doshi", "Q. Chen"], "venue": "In Twenty Second Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zeng et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "Not only is this representation more intuitive to use, it translates into computational benefits when compared to the enumerative representation as used in I-POMDPs (Doshi et al., 2009).", "startOffset": 165, "endOffset": 185}, {"referenceID": 34, "context": "Related Work Suryadi and Gmytrasiewicz (1999) in an early piece of related work, proposed modeling other agents using IDs.", "startOffset": 13, "endOffset": 46}, {"referenceID": 9, "context": "As detailed by Doshi et al. (2009), I-DIDs contribute to an emerging and promising line of research on graphical models for multiagent decision making.", "startOffset": 15, "endOffset": 35}, {"referenceID": 30, "context": "The concept of BE of models was proposed and initially used for solving I-POMDPs (Rathnasabapathy et al., 2006), and discussed generally by Pynadath and Marsella (2007).", "startOffset": 81, "endOffset": 111}, {"referenceID": 22, "context": "Along this direction, another type of equivalence in probabilistic frameworks such as MDPs and POMDPs, sometimes also called BE, is bisimulation (Milner, 1980; Givan et al., 2003; Castro, Panangaden, & Precup, 2009).", "startOffset": 145, "endOffset": 215}, {"referenceID": 14, "context": "Along this direction, another type of equivalence in probabilistic frameworks such as MDPs and POMDPs, sometimes also called BE, is bisimulation (Milner, 1980; Givan et al., 2003; Castro, Panangaden, & Precup, 2009).", "startOffset": 145, "endOffset": 215}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002).", "startOffset": 51, "endOffset": 75}, {"referenceID": 16, "context": "Our agent models are analogous to types in game theory (Harsanyi, 1967), which are defined as attribute vectors that encompass all of an agent\u2019s private information.", "startOffset": 55, "endOffset": 71}, {"referenceID": 23, "context": ", 2006), and discussed generally by Pynadath and Marsella (2007). We contextualize BE within the framework of I-DIDs and seek further extensions.", "startOffset": 36, "endOffset": 65}, {"referenceID": 12, "context": "A somewhat related notion is that of state equivalence introduced by Givan et al. (2003) where the equivalence concept is exploited to factorize MDPs and gain computational benefit.", "startOffset": 69, "endOffset": 89}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering.", "startOffset": 52, "endOffset": 236}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss.", "startOffset": 52, "endOffset": 389}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that exhibit probabilistic equivalence.", "startOffset": 52, "endOffset": 564}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that exhibit probabilistic equivalence. Such histories generate an identical distribution over the histories of the other agents and lead to the same joint belief state. While we utilize BE to losslessly cluster the models of the other agent, we note that BE models when combined with the subject agent\u2019s policy induce identical distributions over the subject agent\u2019s action-observation history. More recently, Witwicki and Durfee (2010) use influence-based abstraction in order to limit an agent\u2019s belief to the other agent\u2019s relevant information by focusing on mutually-modeled features only.", "startOffset": 52, "endOffset": 1031}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that exhibit probabilistic equivalence. Such histories generate an identical distribution over the histories of the other agents and lead to the same joint belief state. While we utilize BE to losslessly cluster the models of the other agent, we note that BE models when combined with the subject agent\u2019s policy induce identical distributions over the subject agent\u2019s action-observation history. More recently, Witwicki and Durfee (2010) use influence-based abstraction in order to limit an agent\u2019s belief to the other agent\u2019s relevant information by focusing on mutually-modeled features only. Our agent models are analogous to types in game theory (Harsanyi, 1967), which are defined as attribute vectors that encompass all of an agent\u2019s private information. In this context, Dekel et al. (2006) define a strategic topology on universal type spaces (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993) under which two types are close if their strategic behavior is similar in all strategic situations.", "startOffset": 52, "endOffset": 1391}, {"referenceID": 9, "context": "Background We briefly review interactive influence diagrams (I-ID) for two-agent interactions followed by their extension to dynamic settings, I-DIDs (Doshi et al., 2009).", "startOffset": 150, "endOffset": 170}, {"referenceID": 17, "context": "We illustrate the formalisms and our approaches in the context of the multiagent tiger problem (Gmytrasiewicz & Doshi, 2005) \u2013 a two-agent generalization of the well-known single agent tiger problem (Kaelbling et al., 1998).", "startOffset": 199, "endOffset": 223}, {"referenceID": 9, "context": "Background We briefly review interactive influence diagrams (I-ID) for two-agent interactions followed by their extension to dynamic settings, I-DIDs (Doshi et al., 2009). Both these formalisms allow modeling the other agent and to use that information in the decision making of the subject agent. We illustrate the formalisms and our approaches in the context of the multiagent tiger problem (Gmytrasiewicz & Doshi, 2005) \u2013 a two-agent generalization of the well-known single agent tiger problem (Kaelbling et al., 1998). In this problem, two agents, i and j, face two closed doors one of which hides a tiger while the other hides a pot of gold. An agent gets rewarded for opening the door that hides the gold but gets penalized for opening the door leading to the tiger. Each agent may open the left door (action denoted by OL), open the right door (OR), or listen (L). On listening, an agent may hear the tiger growling either from the left (observation denoted by GL) or from the right (GR). Additionally, the agent hears creaks emanating from the direction of the door that was possibly opened by the other agent \u2013 creak from the left (CL) or creak from right (CR) \u2013 or silence (S) if no door was opened. All observations are assumed to be noisy. If any door is opened by an agent, the tiger appears behind any of the two doors randomly in the next time step. While the actions of the other agent do not directly affect the reward for an agent, they may potentially change the location of the tiger. This formulation of the problem differs from that of Nair et al. (2003) in the presence of door creaks and that it is not cooperative.", "startOffset": 151, "endOffset": 1577}, {"referenceID": 9, "context": "As an aside, Doshi et al. (2009) show how I-IDs relate to NIDs (Gal & Pfeffer, 2008).", "startOffset": 13, "endOffset": 33}, {"referenceID": 30, "context": "As we mentioned previously, models that are BE (Rathnasabapathy et al., 2006; Pynadath & Marsella, 2007) could be pruned and a single representative model considered.", "startOffset": 47, "endOffset": 104}, {"referenceID": 17, "context": "11 using the tiger problem (Kaelbling et al., 1998), the set M\u0302j,l\u22121 that minimizes Mj,l\u22121 comprises of all the behaviorally distinct representatives of the models in Mj,l\u22121 and only these models.", "startOffset": 27, "endOffset": 51}, {"referenceID": 32, "context": "Seuken and Zilberstein (2007) reuse subtrees of smaller horizon by linking to them using pointers while forming policy trees for the next horizon in the solution of decentralized POMDPs.", "startOffset": 0, "endOffset": 30}, {"referenceID": 30, "context": "Recall that models whose beliefs are spatially close are likely to be BE (Rathnasabapathy et al., 2006).", "startOffset": 73, "endOffset": 103}, {"referenceID": 39, "context": "In addition to these, we utilize the previous approximation technique of k-means clustering (Zeng et al., 2007), referred to as MC, and the exact approach without exploiting BE, referred to as Exact, as baselines.", "startOffset": 92, "endOffset": 111}, {"referenceID": 9, "context": "Furthermore, they provide the advantage of facilitating detailed analysis of the solutions and uncovering interesting behaviors as previously demonstrated (Doshi et al., 2009).", "startOffset": 155, "endOffset": 175}, {"referenceID": 14, "context": "As we mentioned, our formulation of this problem (|S|=2, |Ai|=|Aj |=3, |\u03a9i|=6, |\u03a9j |=2) follows the one introduced by Gmytrasiewicz and Doshi (2005), which differs from the formulation of Nair et al.", "startOffset": 118, "endOffset": 149}, {"referenceID": 14, "context": "As we mentioned, our formulation of this problem (|S|=2, |Ai|=|Aj |=3, |\u03a9i|=6, |\u03a9j |=2) follows the one introduced by Gmytrasiewicz and Doshi (2005), which differs from the formulation of Nair et al. (2003), in not being cooperative and having door creaks as additional observations.", "startOffset": 118, "endOffset": 207}, {"referenceID": 27, "context": "Briefly, GaTAC employs multiple instances of an opensource flight simulator, called FlightGear (Perry, 2004), possibly on different networked platforms that communicate with each other via external servers, and an autonomous control module that interacts with the simulator instances.", "startOffset": 95, "endOffset": 108}, {"referenceID": 16, "context": "I-DIDs are founded on the normative paradigm of decision theory as formalized by DIDs and augmented with aspects of Bayesian games (Harsanyi, 1967) and interactive epistemology (Aumann, 1999a, 1999b) to make them applicable to interactions.", "startOffset": 131, "endOffset": 147}, {"referenceID": 39, "context": "Hence, the cumulative error in j\u2019s predicted behavior over T steps is at most T \u00d7 \u03c1, which is similar to that of the previous k-means model clustering approach (Zeng et al., 2007): \u03c1 \u2264 (R j \u2212R min j )T \u01eb The second term, |(\u03b1\u2032\u2032 \u00b7 bj,l\u22121 \u2212\u03b1\u2032 \u00b7 bj,l\u22121)|, in Eq.", "startOffset": 160, "endOffset": 179}, {"referenceID": 17, "context": "1 Multiagent Tiger Problem As we mentioned previously, our multiagent tiger problem is a non-cooperative generalization of the well-known single agent tiger problem (Kaelbling et al., 1998) to the multiagent setting.", "startOffset": 165, "endOffset": 189}, {"referenceID": 24, "context": "It differs from other multiagent versions of the same problem (Nair et al., 2003) by assuming that the agents hear creaks as well as the growls and the reward function does not promote cooperation.", "startOffset": 62, "endOffset": 81}, {"referenceID": 34, "context": "Smallwood and Sondik (1973) described an MM problem involving a machine containing two internal components.", "startOffset": 0, "endOffset": 28}], "year": 2010, "abstractText": "We focus on the problem of sequential decision making in partially observable environments shared with other agents of uncertain types having similar or conflicting objectives. This problem has been previously formalized by multiple frameworks one of which is the interactive dynamic influence diagram (I-DID), which generalizes the well-known influence diagram to the multiagent setting. I-DIDs are graphical models and may be used to compute the policy of an agent given its belief over the physical state and others\u2019 models, which changes as the agent acts and observes in the multiagent setting. As we may expect, solving I-DIDs is computationally hard. This is predominantly due to the large space of candidate models ascribed to the other agents and its exponential growth over time. We present two methods for reducing the size of the model space and stemming its exponential growth. Both these methods involve aggregating individual models into equivalence classes. Our first method groups together behaviorally equivalent models and selects only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space. The second method further compacts the model space by focusing on portions of the behavioral predictions. Specifically, we cluster actionally equivalent models that prescribe identical actions at a single time step. Exactly identifying the equivalences would require us to solve all models in the initial set. We avoid this by selectively solving some of the models, thereby introducing an approximation. We discuss the error introduced by the approximation, and empirically demonstrate the improved efficiency in solving I-DIDs due to the equivalences.", "creator": "gnuplot 4.2 patchlevel 2 "}}}