{"id": "1401.7898", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2014", "title": "Maximum Margin Multiclass Nearest Neighbors", "abstract": "or develop a general framework for margin - driven multicategory classification in metric spaces. the basic work - horse is a margin - regularized version of the nearest - neighbor classifier. we prove generalization bounds that match the state of pure art in pixel size $ u $ and significantly modify the dependence on the number processing classes $ k $. our point of departure is a nearly bayes - optimal out - sample risk bound score of $ kan $. although $ k $ - free, this bound is unregularized and non - adaptive, which motivates our main claims : rademacher and constraint - sensitive margin result with a logarithmic dependence on $ k $. as the best approximate theoretical estimates in this setting ranged of order $ \\ value k $, our bound seemed exponentially sharper. from the algorithmic standpoint, evaluating doubling metric spaces our classifier may be trained on $ n $ log in $ o ( n ^ ~ \\ log n ) $ time and evaluated on 30 points in $ o ( \\ log n ) $ time.", "histories": [["v1", "Thu, 30 Jan 2014 16:00:43 GMT  (119kb)", "http://arxiv.org/abs/1401.7898v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.TH", "authors": ["aryeh kontorovich", "roi weiss"], "accepted": true, "id": "1401.7898"}, "pdf": {"name": "1401.7898.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\n78 98\nv1 [\ncs .L\nG ]\n3 0\nJa n\n\u221a k, our bound is exponentially sharper. From the\nalgorithmic standpoint, in doubling metric spaces our classifier may be trained on n examples in O(n2 log n) time and evaluated on new points in O(log n) time."}, {"heading": "1 Introduction", "text": "Whereas the theory of supervised binary classification is by now fairly well developed, its multiclass extension continues to pose numerous novel statistical and computational challenges. On the algorithmic front, there is the basic question of how to adapt the hyperplane and kernel methods \u2014 ideally suited for two classes \u2014 to three or more. A host of new problems also arises on the statistical front. In the binary case, the VC-dimension characterizes the distribution-free sample complexity (Anthony & Bartlett, 1999) and tighter distribution-dependent bounds are available via Rademacher techniques (Bartlett & Mendelson, 2002; Koltchinskii & Panchenko, 2002). Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011).\nFollowing von Luxburg & Bousquet (2004); Gottlieb et al. (2010), we adopt a proximity-based approach to supervised multicategory classification in metric spaces. The principal motivation for this framework is two-fold:\n(i) Many natural metrics, such as L1, earthmover, and edit distance cannot be embedded in a Hilbert space without a large distortion (Enflo, 1969; Naor & Schechtman, 2007; Andoni & Krauthgamer, 2010). Any kernel\nmethod is thus a priori at a disadvantage when learning to classify nonHilbertian objects, since it cannot faithfully represent the data geometry.\n(ii) Nearest neighbor-based classification sidesteps the issue of k-to-binary reductions \u2014 which, despite voluminous research, is still the subject of vigorous debate (Rifkin & Klautau, 2004; El-Yaniv et al., 2008). In terms of time complexity, the reductions approach faces an \u2126(k) informationtheoretic lower bound (Beygelzimer et al., 2009), while nearest neighbors admit solutions whose runtime does not depend on the number of classes.\nMain results. Our contributions are both statistical and algorithmic in nature. On the statistical front, we open with the observation that the nearestneighbor classifier\u2019s expected risk is at most twice the Bayes optimal plus a term that decays with sample size at a rate not dependent on the number of classes k (and continues to hold for k = \u221e, Theorem 1). Although of interest as apparently the first \u201ck-free\u201d finite-sample result, it has the drawback of being non-adaptive in the sense of depending on properties of the unknown sampling distribution and failing to provide the learner with a usable data-dependent bound. This difficulty is overcome in our main technical contribution (Theorems 4 and 5), where we give a margin-based multiclass bound of order\nmin\n{ 1\n\u03b3\n( log k\nn\n) 1 D+1\n, 1\n\u03b3 D 2\n( log k\nn\n) 1 2 } , (1)\nwhere k is the number of classes, n is sample size, D is the doubling dimension of the metric instance space and 0 < \u03b3 \u2264 1 is the margin. This matches the state of the art asymptotics in n for metric spaces and significantly improves the dependence on k, which hitherto was of order \u221a k (Zhang, 2002, 2004) or worse. The exponential dependence on some covering dimension (such as D) is in general inevitable, as shown by a standard no-free-lunch argument (Ben-David & Shalev-Shwartz, 2014), but whether (1) is optimal remains an open question.\nOn the algorithmic front, using the above bounds, we show how to efficiently perform Structural Risk Minimization (SRM) so as to avoid overfitting. This involves deciding how many and which sample points one is allowed to err on. We reduce this problem to minimal vertex cover, which admits a greedy 2-approximation. Our algorithm admits a significantly faster \u03b5-approximate version in doubling spaces with a graceful degradation in \u03b5 of the generalization bounds, based on approximate nearest neighbor techniques developed by Gottlieb et al. (2010, 2013a). For a fixed doubling dimension and \u03b5, our runtime is O(n2 logn) for learning and O(log n) for evaluation on a test point. (Exact nearest neighbor requires \u0398(n) evaluation time.) Finally, our generalization bounds and algorithm can be made adaptive to the intrinsic dimension of the data via a recent metric dimensionality-reduction technique (Gottlieb et al., 2013b).\nRelated work. Due to space constraints, we are only able to mention the most directly relevant results \u2014 and even these, not in full generality but rather with an eye to facilitating comparison to the present work. Supervised k-category classification approaches follow two basic paradigms: (I) defining a score function on point-label pairs and classifying by choosing the label with the optimal score and (II) reducing the problem to several binary classification problems. Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < \u221e (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form O\u0303 (\nlog k \u03b3 \u221a dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al.\n(2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper.\nAs for the first paradigm, proximity is perhaps the most natural score function \u2014 and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as O\u0303(k2/n\u03b32), for the separable case with margin \u03b3. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate O\u0303(qk/2/\u03b3 \u221a n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat. Guermeur (2007, 2010) gave scale-sensitive analogues of these dimensions. He gave a risk bound decaying\nas O\u0303 (\nlog k \u03b3 \u221a d\u03b3Nat/n ) , where d\u03b3Nat is a scale-sensitive Natarajan dimension \u2014\nessentially replacing the finite VC dimension dVC in Allwein et al. (2001) by d\u03b3Nat. He further showed that for linear function classes in Hilbert spaces, d\u03b3Nat is bounded by O\u0303(k2/\u03b32), resulting in a risk bound decaying as O\u0303(k/\u03b32 \u221a n). To the best of our knowledge, the sharpest current estimate on the Natarajan dimension (for some special function classes) is dNat = O\u0303(k) with a matching lower bound of \u2126(k) (Daniely et al., 2011). A margin-based Rademacher analysis of score functions (Mohri et al., 2012) yields a bound of order O\u0303(k2/\u03b3 \u221a n), and this is also the k-dependence obtained by Cortes et al. (2013) in a recent paper proposing a multiple kernel approach to multiclass learning. Closest in spirit to our work are the results of Zhang (2002, 2004), who used the chaining technique to achieve a Rademacher complexity with asymptotics O\u0303 (\n1 \u03b3 \u221a k n ) .\nBesides the dichotomy of score functions vs. multiclass-to-binary reductions outlined above, multicategory risk bounds may also be grouped by the trichotomy of (a) combinatorial dimensions (b) Hilbert spaces (c) metric spaces (see Table 1). Category (a) is comprised of algorithm-independent results that give generalization bounds in terms of some combinatorial dimension of a fixed concept class (Allwein et al., 2001; Ben-David et al., 1995; Guermeur, 2007,\n2010; Daniely et al., 2011). Multiclass extensions of SVM and related kernel methods (Weston & Watkins, 1999; Crammer & Singer, 2002a,b; Crammer et al., 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic1 metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification. Unlike Hilbert spaces, which admit dimension-free margin bounds, we are not aware of any metric space risk bound that does not explicitly depend on some metric dimension D or covering numbers. The bounds in Ben-David & Shalev-Shwartz (2014); Gottlieb et al. (2013b) exhibit a characteristic \u201ccurse of dimensionality\u201d decay rate of O(n\u22121/(D+1)), but more optimistic asymptotics can be obtained (Guermeur, 2007, 2010; Zhang, 2002, 2004; Gottlieb et al., 2010). Although some sample lower bounds for proximity-based methods are known (Ben-David & Shalev-Shwartz, 2014), the optimal dependence on D and k is far from being fully understood."}, {"heading": "2 Preliminaries", "text": "Metric Spaces. Given two metric spaces (X , d) and (Z, \u03c1), a function f : X \u2192 Z is called L-Lipschitz if \u03c1(f(x), f(x\u2032)) \u2264 Ld(x, x\u2032) for all x, x\u2032 \u2208 X . (The real line R is always considered with its Euclidean metric |\u00b7|.) The Lipschitz constant of f , denoted \u2016f\u2016\nLip , is the smallest L for which f is L-Lipschitz. The\ndistance between two sets A,B \u2282 X is defined by d(A,B) = infx\u2208A,x\u2032\u2208B d(x, x\u2032). For a metric space (X , d), let \u03bb be the smallest value such that every ball in\n1 in the sense of not requiring an a priori fixed concept class\nX can be covered by \u03bb balls of half the radius. The doubling dimension of X is ddim(X ) := log2 \u03bb. A metric is doubling when its doubling dimension is bounded. The \u03b5-covering number of a metric space (X , d), denoted N (\u03b5,X , d), is defined as the smallest number of balls of radius \u03b5 that suffices to cover X . It can be shown (e.g., Krauthgamer & Lee (2004)) that\nN (\u03b5,X , d) \u2264 ( 2 diam(X )\n\u03b5\n)ddim(X ) , (2)\nwhere diam(X ) = sup x,x\u2032\u2208X d(x, x\u2032) is the diameter of X .\nThe multiclass learning framework. Let (X , d) be a metric instance space with diam(X ) = 1, ddim(X ) = D < \u221e, and Y \u2286 N an at most countable label set. We observe a sample S = (Xi, Yi) n i=1 \u2208 {X \u00d7 Y}n drawn iid from an unknown distribution P over X \u00d7 Y. In line with paradigm (I) outlined in the Introduction, our classification procedure consists of optimizing a score function. In hindsight, the score at a test point will be determined by its labeled neighbors, but for now, we consider an unspecified collection F of functions mapping X \u00d7Y to R. A score function f \u2208 F induces the classifier gf : X \u2192 Y via\ngf(x) = argmax y\u2208Y f(x, y), (3)\nbreaking ties arbitrarily. The margin of f \u2208 F on (x, y) is defined by\n\u03b3f (x, y) = 1\n2\n( f(x, y)\u2212 sup\ny\u2032 6=y f(x, y\u2032)\n) . (4)\nNote that gf misclassifies (x, y) precisely when \u03b3f (x, y) < 0. One of our main objectives is to upper-bound the generalization error\nP(gf (X) 6= Y ) = E[1{\u03b3f (X,Y )<0}].\nTo this end, we introduce two surrogate loss functions L : R \u2192 R+:\nLcutoff (u) = 1{u<1} Lmargin(u) = T[0,1](1\u2212 u),\nwhere\nT[a,b](z) = max {a,min {b, z}} (5)\nis the truncation operator. The empirical loss E\u0302[L(\u03b3f )] induced by any of the loss functions above is 1n \u2211n i=1 L(\u03b3f (Xi, Yi)). All probabilities P(\u00b7) and expectations E[\u00b7] are with respect to the sampling distribution P . We will write ES to indicate expectation over a sample (i.e., over Pn)."}, {"heading": "3 Risk bounds", "text": "In this section we analyze the statistical properties of nearest-neighbor multicategory classifiers in metric spaces. In Section 3.1, Theorem 1, we record the observation that the 1-nearest neighbor classifier is nearly Bayes optimal, with a risk decay that does not depend on the number of classes k. Of course, the 1-naive nearest neighbor is well-known to overfit. This is reflected in the nonadaptive nature of the analysis: the bound is stated in terms of properties of the unknown sampling distribution, and fails to provide the learner with a usable data-dependent bound.\nTo achieve the latter goal, we develop a margin analysis in Section 3.2. Our main technical result is Lemma 2, from which the logarithmic dependence on k claimed in (1) follows. Although not k-free like the Bayes excess risk bound of Theorem 1, O(log k) is exponentially sharper than the current state of the art (Zhang, 2002, 2004). Whether a k-free metric entropy bound is possible is currently left as an open problem.\nThe metric entropy bound of Lemma 2 facilitates two approaches to bounding the risk: via Rademacher complexity (Section 3.2.2) and via scale-sensitive techniques in the spirit of Guermeur (2007) (Section 3.2.3). In Section 3.2.4 we combine these two margin bounds by taking their minimum. The resulting bound will be used in Section 4 to perform efficient Structural Risk Minimization."}, {"heading": "3.1 Multiclass Bayes near-optimality", "text": "In this section, (X , d) is a metric space and Y is an at most countable (possibly infinite) label set. A sample S = (Xi, Yi) n i=1 is drawn iid from an unknown distribution P over X \u00d7Y. For x \u2208 X let (X\u03c01(x), Y\u03c01(x)) be its nearest neighbor in S:\n\u03c01(x) = argmin i\u2208[n] d(Xi, x).\nThus, the nearest-neighbor classifier gNN is given by\ngNN(x) = Y\u03c01(x). (6)\nDefine the function \u03b7 : X \u2192 RY by\n\u03b7(x) = P(Y = \u00b7 |X = x).\nThe Bayes optimal classifier g\u2217 \u2014 i.e., one that minimizes P(g(X) 6= Y ) over all measurable g \u2208 YX \u2014 is well-known to have the form\ng\u2217(x) = argmax y\u2208Y \u03b7y(x),\nwhere ties are broken arbitrarily. Our only distributional assumption is that \u03b7 is L-Lipschitz with respect to the sup-norm. Namely, for all x, x\u2032 \u2208 X , we have\n\u2016\u03b7(x)\u2212 \u03b7(x\u2032)\u2016\u221e \u2261 sup y\u2208Y |\u03b7y(x)\u2212 \u03b7y(x\u2032)| \u2264 Ld(x, x\u2032).\nThis is a direct analogue of the Lipschitz assumption for the binary case (Cover & Hart, 1967; Ben-David & Shalev-Shwartz, 2014). We make the additional standard assumption that X has a finite doubling dimension: ddim(X ) = D < \u221e. The Lipschitz and doubling assumptions are sufficient to extend the finite-sample analysis of binary nearest neighbors (Ben-David & Shalev-Shwartz, 2014) to the multiclass case:\nTheorem 1.\nES [P(gNN(X) 6= Y )] \u2264 2P(g\u2217(X) 6= Y ) + 4L\nn1/(D+1) .\nNote that the bound is independent of the number of classes k and holds even for k = \u221e. The proof is deferred to Appendix A."}, {"heading": "3.2 Multiclass margin bounds", "text": "Here again (X , d) is a metric space, but now the label set Y is assumed finite: |Y| = k < \u221e. As before, S = (Xi, Yi)ni=1 with (Xi, Yi) \u223c P iid. It will be convenient to write Sy = {Xi : Yi = y, i \u2208 [n]} for the subset of examples with label y. The metric induces the natural score function fNN(x, y) = \u2212d(x, Sy) with corresponding nearest-neighbor classifier\ngNN(x) = argmax y\u2208Y fNN(x, y), (7)\neasily seen to be identical to the one in (6). At this point we make the simple but crucial observation that the function fNN(\u00b7, y) : X \u2192 R is 1-Lipschitz. This will enable us to generalize the powerful Lipschitz extension framework of von Luxburg & Bousquet (2004) to |Y| > 2.\nWe will need a few definitions. Let FL be the collection of all L-Lipschitz functions from X to R and put FL = FL \u00d7 Y. Since each f \u2208 FL maps X \u00d7 Y to R, the margin \u03b3f (x, y) is well-defined via (4). Putting\ny\u2217f (x) = argmax y\u2208Y f(x, y), \u03b3\u2217f (x) = \u03b3f (x, y \u2217 f (x)),\nwe define the projection \u03a6f :\n\u03a6f (x, y) =\n{ \u03b3\u2217f (x), if y = y \u2217 f (x)\n\u2212\u03b3\u2217f (x), otherwise.\nFinally, we define HL as the truncated (as in (5)) projections of functions in FL:\nHL = { (x, y) 7\u2192 T[-1,1] (\u03a6f (x, y)) : f \u2208 FL } . (8)\nThus, HL is the set of functions hf : X \u00d7 Y \u2192 [\u22121, 1], where each hf (\u00b7, y) is L-Lipschitz and hf(x, y) = \u00b1T[-1,1](\u03b3\u2217f (x)), depending upon whether y = y\u2217f (x) , see Figure 1 (left)."}, {"heading": "3.2.1 Bounding the metric entropy", "text": "Our main technical result is a bound on the metric entropy of HL, which will be used to obtain error bounds (Theorems 4 and 5) for classifiers derived from this function class. The analysis differs from previous bounds (see Table 1) by explicitly taking advantage of the mutual exclusive nature of the labels, obtaining an exponential improvement in terms of the number of classes k. Endow HL with the sup-norm\n\u2016 \u00b7 \u2016\u221e = sup x\u2208X max y\u2208Y | \u00b7 | .\nLemma 2. For any \u03b5 > 0,\nlogN (\u03b5,HL, \u2016 \u00b7 \u2016\u221e) \u2264 ( 16L\n\u03b5\n)D log ( 5k\n\u03b5\n) .\nProof. By the definition of HL, for all hf \u2208 HL and x \u2208 X there is at most one y \u2208 Y such that hf (x, y) > 0. In addition, if hf (x, y) = c > 0, then hf (x, y\n\u2032) = \u2212c for all y\u2032 6= y. Since \u03b3\u2217f (x) \u2265 0, we may reparametrize hf (x, y) by (y\u2217f (x), \u03b3 \u2217 f (x)) \u2208 Y\u00d7[0, 1], see Figure 1. To complete the mapping hf 7\u2192 (y\u2217f , \u03b3\u2217f ), define the following star-like metric \u03c1 over Y \u00d7 [0, 1] (see Figure 2):\n\u03c1((y, \u03b3), (y\u2032, \u03b3\u2032)) = { |\u03b3 \u2212 \u03b3\u2032| y = y\u2032 \u03b3 + \u03b3\u2032 y 6= y\u2032 .\nLet H\u0303L be the collection of functions h\u0303 : X \u2192 Y \u00d7 [0, 1] that are L-Lipschitz: \u03c1(h\u0303(x), h\u0303(x\u2032)) \u2264 Ld(x, x\u2032), x, x\u2032 \u2208 X .\nIt is easily verified that the metric space (HL, \u2016 \u00b7 \u2016\u221e) is isometric to (H\u0303L, \u03c1\u221e) with\n\u03c1\u221e(h\u0303, h\u0303 \u2032) = sup x\u2208X \u03c1(h\u0303(x), h\u0303\u2032(x)).\nThus, N (\u03b5,HL, \u2016 \u00b7 \u2016\u221e) = N (\u03b5, H\u0303L, \u03c1\u221e), and we proceed to bound the latter.2 Fix a covering of X consisting of |N | = N (\u03b5/8L,X , d) balls {U1, . . . , U|N |} of radius \u03b5\u2032 = \u03b5/8L and choose |N | points N = {xi \u2208 Ui}|N |i=1. Construct H\u0302 \u2282 H\u03032L as follows. At every point xi \u2208 N select one of the classes y \u2208 Y and set h\u0302(xi) = (y, \u03b3(xi)) with \u03b3(xi) some multiple of 2L\u03b5\n\u2032 = \u03b5/4, while maintaining \u2016h\u0302\u2016Lip \u2264 2L. Construct a 2L-Lipschitz extension for h\u0302 from N to all over X (such an extension always exists, (McShane, 1934; Whitney, 1934)). We claim that every classifier in HL, via its twin h\u0303 \u2208 H\u0303L, is close to some h\u0302 \u2208 H\u0302, in the sense that \u03c1\u221e(h\u0303, h\u0302) \u2264 \u03b5. Indeed, every point x \u2208 X is 2\u03b5\u2032-close to some point xi \u2208 N , and since h\u0303 is L-Lipschitz and h\u0302 is 2L-Lipschitz,\n\u03c1(h\u0303(x), h\u0302(x)) \u2264 \u03c1(h\u0303(x), h\u0303(xi)) + \u03c1(h\u0303(xi), h\u0302(xi))\n+ \u03c1(h\u0302(xi), h\u0302(x))\n\u2264 Ld(x, xi) + \u03b5/4 + 2Ld(x, xi) \u2264 \u03b5.\nThus, H\u0302 provides an \u03b5-cover for H\u0303L (and hence for HL). Note that |H\u0302 | \u2264 (\u23084k/\u03b5\u2309 + 1)|N | , since by construction, functions h\u0302 are determined by their values on N , which at a given point can take one of \u23084k/\u03b5\u2309+ 1 possible values. Since by (2) we have |N | = N (\u03b5/8L,X , d) \u2264 ( 16L \u03b5 )D the bound follows.\nA tighter bound is possible when the metric space (X , d) possesses two additional properties:\n1. (X , d) is connected if for all x, x\u2032 \u2208 X and all \u03b5 > 0, there is a finite sequence of points x = x1, x2, . . . , xm = x\n\u2032 such that d(xi, xi+1) < \u03b5 for all 1 \u2264 i < m.\n2. (X , d) is centered if for all r > 0 and all A \u2282 X with diam(A) \u2264 2r, there exists a point x \u2208 X such that d(x, a) \u2264 r for all a \u2208 A.\nLemma 3. If (X , d) is connected and centered, then\nlogN (\u03b5,HL, \u2016 \u00b7 \u2016\u221e) = O (( L\n\u03b5\n)D log k + log ( 1\n\u03b5\n)) .\nProof. With the additional assumptions on X we follow the proof idea in Kolmogorov & Tikhomirov (1959) and demonstrate the tighter bound |H\u0302 | \u2264 (\u23084k/\u03b5\u2309+ 1)(2k + 1)|N |\u22121 =\n2The remainder of the proof is based on a technique communicated to us by R. Krauthgamer, a variant of the classic Kolmogorov & Tikhomirov (1959) method.\nO((2k)|N |/\u03b5). Here H\u0302 is constructed as in the proof for Lemma 2 but now each xi \u2208 N is taken to be a \u201ccenter\u201d of Ui, as furnished by Property 2 above. Let xj \u2208 N . Since X is connected, we may traverse a path from x1 to xj via the cover points x1 = xi1 , xi2 , . . . , xim = xj , such that the distance between any two successive points (xil , xil+1) is at most 2\u03b5 \u2032 = \u03b5/4L. Since h\u0302 is 2L-Lipschitz, on any two such points the value of h\u0302 can change by at most \u03b5/2. Thus, given\nthe value h\u0302(xil), the value of h\u0302(xil+1 ) can take one of at most 2k + 1 values (as Figure 2 shows, at the star\u2019s hub, h\u0302(xil+1 ) can take one of 2k + 1 values, while at one of the spokes only 5 values are possible). So we are left to choose the value of h\u0302 on the point x1 to be one from the \u23084k/\u03b5\u2309+ 1 possible values. The bounds on |H\u0302 | and the metric entropy follow."}, {"heading": "3.2.2 Rademacher analysis", "text": "The Rademacher complexity of the set of functions HL is defined by\nRn(HL) = E [ sup\nh\u2208HL\n1\nn\nn\u2211\ni=1\n\u03c3ih(Xi, Yi)\n] , (9)\nwhere the \u03c3i are n independent random variables with P(\u03c3i = +1) = P(\u03c3i = \u22121) = 1/2. In Appendix B, we invoke Lemma 2 to derive the bound\nRn(HL) \u2264 2L ( log 5k\nn\n)1/(D+1) , (10)\nwhich in turn implies \u201chalf\u201d of our main risk estimate (1):\nTheorem 4. With probability at least 1 \u2212 \u03b4, for all L > 0 and every f \u2208 FL with its projected version hf \u2208 HL,\nP(gf(X) 6= Y ) \u2264 E\u0302 [L(hf )] + \u2206Rad(n, L, \u03b4),\nwhere gf is the classifier defined in (3), L is any of the loss functions defined in Section 2 and \u2206Rad(n, L, \u03b4) is at most\n8L\n( log 5k\nn\n) 1 D+1\n+\n\u221a( log log2 2L\nn\n)\n+\n+ \u221a log 2\u03b4 2n ."}, {"heading": "3.2.3 Scale-sensitive analysis", "text": "The following Theorem , proved in Appendix C, is an adaptation of Guermeur (2007, Theorem 1), using Lemma 2.\nTheorem 5. With probability at least 1 \u2212 \u03b4, for all L > 0 and every f \u2208 FL with its induced hf \u2208 HL,\nP(gf (X) 6= Y ) \u2264 E\u0302 [Lcutoff(hf )] + \u2206fat(n, L, \u03b4),\nwhere \u2206fat(n, L, \u03b4) is at most\n\u221a 2\nn\n( 2 (16L) D log (20k) + ln ( 2L\n\u03b4\n)) + 1\nn ."}, {"heading": "3.2.4 Combined Bound", "text": "Taking L = Lcutoff in Theorem 4 we can merge the above two bounds by taking their minimum. Namely, Theorem 5 holds with \u2206(n, L, \u03b4) = min {\u2206Rad(n, L, \u03b4),\u2206fat(n, L, \u03b4)} in place of \u2206fat(n, L, \u03b4), see Figure 3. The resulting risk decay rate is of order\nmin { L ( log k\nn\n) 1 D+1\n, L D 2\n( log k\nn\n) 1 2 } ,\nas claimed in (1). In terms of the number of classes k, our bound compares favorably to those in Allwein et al. (2001); Guermeur (2007, 2010), and more recently in Daniely et al. (2011), which have a k-dependence of O(dNat log k), where dNat is the (scale-sensitive, k-dependent) Natarajan dimension of the multiclass hypothesis class. The optimal dependence of the risk on k is an intriguing open problem."}, {"heading": "4 Algorithm", "text": "Theorems 4 and 5 yield generalization bounds of the schematic form\nP(g(X) 6= Y ) \u2264 E\u0302[L] + \u2206(n, L, \u03b4). (11)\nThe free parameter L in (11) controls (roughly speaking) the bias-variance tradeoff: for larger L, we may achieve a smaller empirical loss E\u0302[L] at the expense\nof a larger hypothesis complexity \u2206(n, L, \u03b4). Our Structural Risk Minimization (SRM) consists of seeking the optimal L \u2014 i.e., one that minimizes the right-hand side of (11) \u2014 via the following high-level procedure:\n1. For each L > 0, minimize E\u0302[L(hf )] over f \u2208 FL.\n2. Choose the optimal L\u2217 and its corresponding classifier gf with f \u2208 FL\u2217 .\nMinimizing the empirical loss. Let S = (Xi, Yi) n i=1 be the training sample and L > 0 a given maximal allowed Lipschiz constant. We will say that a function h \u2208 HL is inconsistent with a sample point (x, y) if h(x, y) < 1 (i.e., if the margin of h on (x, y) is less than one). Denote by m\u0302(L) the smallest possible number of sample points on which a function h \u2208 HL may be inconsistent:\nm\u0302(L) = min h\u2208HL\nE\u0302[Lcutoff (h)].\nThus, our SRM problem consists of finding\nL\u2217 = argmin L>0 {m\u0302(L) + \u2206(n, L, \u03b4)} .\nFor k = 2, Gottlieb et al. (2010) reduced the problem of computing m\u0302(L) to one of finding a minimal vertex cover in a bipartite graph (by Ko\u0308nig\u2019s theorem, the latter is efficiently computable as a maximal matching). We will extend this technique to k > 2 as follows. Define the k-partite graph GL = ({V y}ky=1, E), where each vertex set V y corresponds to the sample points Sy with label y. Now in order for h \u2208 HL to be consistent with the points (Xi, Yi) and (Xj , Yj) for Yi 6= Yj , the following relation must hold:\nLd(Xi, Xj) \u2265 2. (12)\nHence, we define the edges of GL to consist of all point pairs violating (12):\n(Xi, Xj) \u2208 E \u21d0\u21d2 (Yi 6= Yj) \u2227 (d(Xi, Xj) < 2/L).\nSince removing either of Xi, Xj in (12) also deletes the violating edge, m\u0302(L) is by construction equivalent to the size of the minimum vertex cover for GL. Although minimum vertex cover is NP-hard to compute (and even hard to approximate within a factor of 1.3606, (Dinur & Safra, 2005)), a 2-approximation may be found in O(n2) time (Papadimitriou & Steiglitz, 1998). This yields a 2-approximation m\u0303(L) for m\u0302(L).\nOptimizing over L. Equipped with an efficient routine for computing m\u0303(L) \u2264 2m\u0302(L), we now seek an L > 0 that minimizes\nQ(L) := m\u0303(L) + \u2206(n, L, \u03b4). (13)\nSince the Lipschitz constant induced by the data is determined by the ( n 2 ) distances among the sample points, we need only consider O(n2) values of L.\nRather a brute-force searching all of these values, Theorem 7 of Gottlieb et al. (2010) shows that using an O(log n) time binary search over the values of L, one may approximately minimizeQ(L), which in turn yields an approximate solution to (11). The resulting procedure has runtime O(n2 logn) and guarantees an L\u0303 for which\nQ(L\u0303) \u2264 4 [m\u0302(L\u2217) + \u2206(n, L\u2217, \u03b4)] . (14)\nClassifying test points. Given the nearly optimal Lipschitz constant L\u0303 computed above we construct the approximate (within a factor of 4) empirical risk minimizer h\u2217 \u2208 HL\u0303. The latter partitions the sample into S = S0 \u222a S1, where S1 consists of the points on which h\n\u2217 is consistent and S0 = S \\ S1. Evaluating h\u2217 on a test point amounts to finding its nearest neighbor in S1. Although in general metric spaces, nearest-neighbors search requires \u2126(n) time, for doubling spaces, an exponential speedup is available via approximate nearest neighbors (see Section 5)."}, {"heading": "5 Extensions", "text": "In this section, we discuss two approaches that render the methods presented above considerably more efficient in terms of runtime and generalization bounds. The first is based on the fact that in doubling spaces, hypothesis evaluation time may be reduced fromO(n) to O(log n) at the expense of a very slight degradation of the generalization bounds. The second relies on a recent metric dimensionality reduction result. When the data is \u201cclose\u201d to being D\u0303-dimensional, with D\u0303 much smaller than the ambient metric space dimension D, both the evaluation runtime and the generalization bounds may be significantly improved \u2014 depending essentially on D\u0303 rather than D."}, {"heading": "5.1 Exponential speedup via approximate NN", "text": "If (X , d) is a metric space and x\u2217 \u2208 E \u2282 X is a minimizer of d(x, x\u2032) over x\u2032 \u2208 E, then x\u2217 is a nearest neighbor of x in E. A simple information-theoretic argument shows that the time complexity of computing an exact nearest neighbor in general metric spaces has \u2126(n) time complexity. However, an exponential speedup is possible if (i) X is a doubling space and (ii) one is willing to settle for approximate nearest neighbors. A (1+\u03b7) nearest neighbor oracle returns an x\u0303 \u2208 E such that\nd(x, x\u2217) \u2264 d(x, x\u0303) \u2264 (1 + \u03b7)d(x, x\u2217). (15)\nWe will use the fact that in a doubling space, one may precompute a (1+ \u03b7) nearest neighbor data structure in (2O(ddim(X )) logn+ \u03b7\u2212O(ddim(X )))n time and evaluate it on a test point in 2O(ddim(X )) logn+\u03b7\u2212O(ddim(X )) time Cole & Gottlieb (2006); Har-Peled & Mendel (2006). The approximate nearest neighbor oracle induces an \u03b7-approximate version of gNN in defined (7). After performing SRM\nas described in Section 4, we are left with a subset S1 \u2282 S of the sample, which will be used to label test points. More precisely, the predicted label of a test point will be determined by its \u03b7-nearest neighbor in S1.\nThe exponential speedup afforded by approximate nearest neighbors comes at the expense of mildly degraded generalization guarantees. The modified generalization bounds are derived in three steps, whose details are deferred to Appendix D: (i) We cast the evaluation of h \u2208 HL in (8) as a nearest neighbor calculation with a corresponding h\u0303 induced by the (1 + \u03b7) approximate nearest neighbor oracle. The nearest-neighbor formulation of h is essentially the one obtained by von Luxburg & Bousquet (2004):\nh(x, y) = 1\n2 ( min S1 {\u03be(y, y\u2032) + Ld(x, x\u2032)} (16)\n+ max S1\n{\u03be(y, y\u2032)\u2212 Ld(x, x\u2032)} ) ,\nwhere (x\u2032, y\u2032) \u2208 S1 and \u03be(y, y\u2032) = 21{y=y\u2032} \u2212 1. (ii) We observe a simple relation between h and h\u0303:\n\u2016h\u2212 h\u0303\u2016\u221e \u2261 sup x\u2208X ,y\u2208Y |h(x, y)\u2212 h\u0303(x, y)| \u2264 2\u03b7.\n(iii) Defining the 2\u03b7-perturbed function class\nHL,2\u03b7 = {T[-1,1](h\u2032) : \u2016h\u2032 \u2212 h\u2016\u221e \u2264 2\u03b7, h \u2208 HL},\nwe relate its metric entropy to that of HL:\nLemma 6. For \u03b5 > 2\u03b7 > 0, we have\nN (\u03b5,HL,2\u03b7, \u2016 \u00b7 \u2016\u221e) \u2264 N (\u03b5\u2212 2\u03b7,HL, \u2016 \u00b7 \u2016\u221e).\nThe metric entropy estimate for HL,2\u03b7 readily yields \u03b7-perturbed versions of Theorems 4 and 5. From the standpoint of generalization bounds, the effect of the \u03b7-perturbation on HL amounts, roughly speaking, to replacing L by L(1 + O(\u03b7)), which constitutes a rather benign degradation."}, {"heading": "5.2 Adaptive dimensionality reduction", "text": "The generalization bound in (1) and the runtime of our sped-up algorithm in Section 5.1 both depend exponentially on the doubling dimension of the metric space. Hence, even a modest dimensionality reduction could lead to dramatic savings in algorithmic and sample complexities. The standard Euclidean dimensionality-reduction tool, PCA, until recently had no metric analogue \u2014 at least not with rigorous performance guarantees. The technique proposed in Gottlieb et al. (2013b) may roughly be described as a metric analogue of PCA.\nA setX = {x1, . . . , xn} \u2282 X inherits the metric d of X and hence ddim(X) \u2264 ddim(X ) is well-defined. We say that X\u0303 = {x\u03031, . . . , x\u0303n} \u2282 X is an (\u03b1, \u03b2)perturbation of X if \u2211n i=1 d(xi, x\u0303i) \u2264 \u03b1 and ddim(X\u0303) \u2264 \u03b2. Intuitively, the data is \u201cessentially\u201d low-dimensional if it admits an (\u03b1, \u03b2)-perturbation with small \u03b1, \u03b2, which leads to improved Rademacher estimates. The empirical Rademacher complexity of HL on a sample S = (X,Y ) \u2208 Xn \u00d7 Yn is given by\nR\u0302n(HL;S) = E [\nsup h\u2208HL\n1\nn\nn\u2211\ni=1\n\u03c3ih(Xi, Yi) \u2223\u2223\u2223\u2223\u2223S ]\nand is related to Rn defined in (9) via\nRn(HL) = ES [ R\u0302n(HL;S) ]\nP (\u2223\u2223\u2223Rn \u2212 R\u0302n \u2223\u2223\u2223 \u2265 \u03b5 ) \u2264 2 exp(\u2212\u03b52n/2),\nwhere the identity is obvious and the inequality is a simple consequence of measure concentration (Mohri et al., 2012). Hence, up to small changes in constants, the two may be used in generalization bounds such as Theorem 4 interchangeably. The data-dependent nature of R\u0302n lets us exploit essentially low-dimensional data (see Appendix E):\nTheorem 7. Let S = (X,Y ) \u2208 Xn \u00d7 Yn be the training sample and suppose that X admits an (\u03b1, \u03b2)-perturbation X\u0303. Then\nR\u0302n(HL;S) = O ( L ( \u03b1+ ( log k\nn\n) 1 1+\u03b2 )) . (17)\nA pleasant feature of the bound above is that it does not depend on ddim(X ) (the dimension of the ambient space) or even on ddim(X) (the dimension of the data). Note the inherent tradeoff between the distortion \u03b1 and dimension \u03b2, with some non-trivial (\u03b1\u2217, \u03b2\u2217) minimizing the right-hand side of (17). Although computing the optimal (\u03b1\u2217, \u03b2\u2217) seems computationally difficult, Gottlieb et al. (2013b) were able to obtain an efficient (O(1), O(1))-bicriteria approximation. Namely, their algorithm computes an \u03b1\u0303 \u2264 c0\u03b1\u2217 and \u03b2\u0303 \u2264 c1\u03b2\u2217, with the corresponding perturbed set X\u0303, for universal constants c0, c1, with a runtime of 2O(ddim(X))n logn+O(n log5 n).\nThe optimization routine over (\u03b1, \u03b2) may then be embedded inside our SRM optimization over the Lipschitz constant L in Section 4. The end result will be a nearly optimal (in the sense of (14)) Lipschitz constant L\u0303, which induces the partition S = S0 \u222a S1, as well as (\u03b1\u0303, \u03b2\u0303), which induce the perturbed set S\u03031. To evaluate our hypothesis on a test point, we may invoke the (1+ \u03b7)-approximate nearest-neighbor routine from Section 5.1. This involves a precomputation of time complexity (2O(\u03b2\u0303) logn + \u03b7\u2212O(\u03b2\u0303))n, after which new points are classified in 2O(\u03b2\u0303) log n+ \u03b7\u2212O(\u03b2\u0303) time. Note that the evaluation time complexity depends only on the \u201cintrinsic dimension\u201d \u03b2\u0303 of the data, rather than the ambient metric space dimension."}, {"heading": "A Bayes near-optimality proof", "text": "Proof of Theorem 1. Since \u03b7 is L-Lipschitz, given x, x\u2032 \u2208 X we have\nP (Y 6= Y \u2032 |x,x\u2032) = \u2211\nj\u2208Y \u03b7j(x)(1 \u2212 \u03b7j(x\u2032)) (18)\n\u2264 \u2211\nj\n\u03b7j(x) ( 1\u2212 \u03b7j(x) + Ld(x, x\u2032) )\n= \u2211\nj\n\u03b7j(x) ( 1\u2212 \u03b7j(x) ) + Ld(x, x\u2032).\nBy the definition of the nearest neighbor classifier gNN in (6) we have ES [P (gNN(X) 6= Y )] = ES [P (Y\u03c01(X) 6= Y )], where the expectation is over the sample S determining gNN. By (18) this error is bounded above by\nES,X [ \u2211\nj\n\u03b7j(X)(1\u2212 \u03b7j(X))] + LES,X [d(X,X\u03c01(X))],\nwhere now the expectation is over S and X . Denoting k\u2032 = argmaxj \u03b7j(X) and splitting the sum , the first term (which does not depend on S) satisfies\nEX [\u03b7k\u2032(X)(1\u2212\u03b7k\u2032 (X))] + EX [ \u2211\nj 6=k\u2032 \u03b7j(X)(1\u2212 \u03b7j(X))]\n\u2264 EX [1\u2212 \u03b7k\u2032(X)] + EX [ \u2211\nj 6=k\u2032 \u03b7j(X)]\n= 2EX [1\u2212 \u03b7k\u2032 (X)] = 2P (g\u2217(X) 6= Y ).\nIt remains to bound ES,X [d(X,X\u03c01(X))] and we proceed exactly as in Ben-David & Shalev-Shwartz (2014). Let {C1, . . . , CN} be an \u03b5-cover of X of cardinality N = N (\u03b5,X , d). Given a sample S, for x \u2208 Ci such that S \u2229 Ci 6= \u2205 we have d(x,X\u03c01(x)) < \u03b5,\nwhile for x \u2208 Ci such that S\u2229Ci = \u2205 we have d(x,X\u03c01(x)) \u2264 diam(X ) = 1, thus ES,X [d(X,X\u03c01(X))] is bounded above by\n\u2264 ES [ N\u2211\ni=1\nP (Ci) ( \u03b51{S\u2229Ci 6=\u2205} + 1{S\u2229Ci=\u2205}\n) ]\n=\nN\u2211\ni=1\nP (Ci) ( \u03b5ES [ 1{S\u2229Ci 6=\u2205} ] + ES [ 1{S\u2229Ci=\u2205} ]) .\nSince P (Ci)ES [1S\u2229Ci=\u2205] = P (Ci)(1 \u2212 P (Ci))n \u2264 1/en and N = N (\u03b5,X , d) we get\nES,X [d(X,X\u03c01(X))] \u2264 \u03b5+ N (\u03b5,X , d)\nen\n\u2264 \u03b5+ 1 en\n( 2\n\u03b5\n)D .\nSetting \u03b5 = 2n\u2212 1 D+1 concludes the proof."}, {"heading": "B Rademacher analysis proofs", "text": "Proof of inequality (10). Dudley\u2019s chaining integral (Dudley, 1967) bounds from above the Rademacher complexity Rn(HL) by\ninf \u03b1>0\n( 4\u03b1+ 12 \u222b \u221e\n\u03b1\n\u221a logN (t,HL, \u2016 \u00b7 \u2016\u221e)\nn dt\n) .\nBy Lemma 2 the integral can be bounded as follows:\n\u222b \u221e\n\u03b1\n\u221a logN (t,HL, \u2016 \u00b7 \u2016\u221e)\nn dt\n\u2264 \u222b \u221e\n\u03b1\n\u221a 1\nn\n( 16L\nt\n)D log ( 5k\nt\n) dt\n\u2264 \u222b \u221e\n\u03b1\n\u221a log 5k\nn\n( 16L\nt\n)D ( 1\nt\n) dt\n=\n\u221a log 5k\nn (16L)\nD/2 \u222b \u221e\n\u03b1\n( 1\nt\n)(D+1)/2 dt\n=\n\u221a log 5k\nn (16L)\nD/2\n( 2\nD \u2212 1\n)( 1\n\u03b1(D\u22121)/2\n) ,\nwhere in the second inequality we used the fact that for x \u2208 (0, 1] and c \u2265 e we have log( cx) \u2264 log c x . Choosing\n\u03b1\u2217 = ( 9(16L)D log 5k\nn\n)1/(D+1)\nyields the bound.\nProof of Theorem 4. An adaptation3 of Mohri et al. (2012, Theorem 4.5) to HL states that with probability 1\u2212 \u03b4, for all L > 0, h \u2208 HL,\nE[Lmargin(h)] \u2264 E\u0302[Lmargin(h)] + 4Rn(HL)\n+\n\u221a( log log2 2L\nn\n)\n+\n+ \u221a log 2\u03b4 2n .\nSince 1{u<0} \u2264 Lmargin(u) we have P (gh(X) 6= Y ) \u2264 E[Lmargin(h)]. Since Lmargin(u) \u2264 Lcutoff(u) we can replace Lmargin in the empirical loss by the loss function Lcutoff . Bounding Rn(HL) using (10) concludes the proof."}, {"heading": "C Scale sensitive analysis proof", "text": "Proof of Theorem 5. An application4 of Guermeur (2010, Theorem 1) states that with probability 1\u2212 \u03b4, for all L > 0, h \u2208 HL,\nP (gh(X) 6= Y ) \u2264 1\nn\nn\u2211\ni=1\n1{h(Xi,Yi)<1}\n+\n\u221a 2\nn\n( 2 logN (1/4,HL, \u2016 \u00b7 \u2016\u221e) + ln ( 2L\n\u03b4\n)) + 1\nn .\nApplying the metric entropy bound in Lemma 2 proves the Theorem."}, {"heading": "D Approximate NN proofs", "text": "First, we will show that h\u0303 is indeed a 2\u03b7 additive perturbation of h, i.e.\n\u2016h\u2212 h\u0303\u2016\u221e \u2264 2\u03b7. (19)\nInstead of working directly with (16) we consider the following L-Lipschitz extension\nh(x, y) = 1\n2 T[-1,1] ( min S1 {\u03be(Yi, y) + Ld(Xi, x)} )\n+ 1\n2 T[-1,1] ( max S1 {\u03be(Yi, y)\u2212 Ld(Xi, x)} ) ,\neasily seen to induce the same classifier gh as (16). Consider the first term (the second term is treated similarly) and its approximate version:\nh\u0303(x, y) = T[-1,1] ( min S1 { \u03be(Yi, y) + Ld\u0303(Xi, x) }) ,\n3essentially setting \u03b1 = 1 in Mohri et al. (2012) and doing the stratification on L instead 4setting \u03b3 = 1 in Guermeur (2010, Theorem 1) and doing the stratification on L instead\nwhere d \u2264 d\u0303 \u2264 (1+\u03b7)d, given in (15), is the approximate \u201d\u2018distance\u201d\u2019 as provided by the approximate nearest neighbor. For notational convenience, denote\nh(x, y) = T[-1,1](min i qi(x, y))\nh\u0303(x, y) = T[-1,1](min i q\u0303i(x, y))\nqi(x, y) = hi(y) + ri(x)\nq\u0303i(x, y) = h\u0303i(y) + r\u0303i(x),\nwhere hi(y) = \u03be(Yi, y), ri(x) = Ld(Xi, x), and h\u0303i, r\u0303i defined analogously. Observe that if r\u0303i(x) > 2 then ri(x) > 2/(1+\u03b7) \u2265 2(1\u2212\u03b7). In this case, since h has range in [\u22121, 1], the eventual application of truncation operator T[-1,1] will force h\u0303(x, y)\u2212h(x, y) \u2264 2\u03b7. Hence, we may assume that r\u0303i(x) \u2264 2 and so ri(x) \u2264 2. It is straightforward to verify that for a, b \u2208 Rn with maxi\u2208[n] |ai \u2212 bi| \u2264 \u03b7, we have\n\u2223\u2223\u2223T[-1,1](min i ai)\u2212 T[-1,1](min i bi) \u2223\u2223\u2223 \u2264 \u03b7.\nThus, establishing |qi(x, y)\u2212 q\u0303i(x, y)| \u2264 2\u03b7 for all i \u2208 [|S1|] and y \u2208 Y with r\u0303i(x), ri(x) \u2264 2 suffices to prove the claim. Indeed, by (15) we have\n|ri(x) \u2212 r\u0303i(x)| \u2264 |ri(x)\u2212 (1 + \u03b7)ri(x)| \u2264 2\u03b7.\nProof of Lemma 6. Suppose h\u0303 \u2208 HL,\u03b7. By the definition of HL,\u03b7, there exists an h \u2208 HL such that \u2016h\u0303 \u2212 h\u2016\u221e \u2264 \u03b7. Let h\u2032 be some element in a minimal \u03b5-cover of HL so that \u2016h\u2212 h\u2032\u2016\u221e \u2264 \u03b5. Then\n\u2016h\u0303\u2212 h\u2032\u2016\u221e \u2264 \u2016h\u0303\u2212 h\u2016\u221e + \u2016h\u2212 h\u2032\u2016\u221e \u2264 \u03b5+ \u03b7.\nHence, N (\u03b5+ \u03b7,HL,\u03b7, \u2016 \u00b7 \u2016\u221e) \u2264 N (\u03b5,HL, \u2016 \u00b7 \u2016\u221e),\nwhence the claim follows."}, {"heading": "E Dimensionality reduction proof", "text": "Proof of Theorem 7. Put S\u0303 = (X\u0303, Y ). For Xi \u2208 X and X\u0303i \u2208 X\u0303 , define \u03b4i(h) = h(Xi, Yi)\u2212 h(X\u0303i, Yi). Then\nR\u0302n(HL;S) = E [\nsup h\u2208HL\n1\nn\nn\u2211\ni=1\n\u03c3ih(Xi, Yi) \u2223\u2223\u2223\u2223\u2223S ]\n= E [ sup h\u2208HL 1 n n\u2211\ni=1\n\u03c3i ( h(X\u0303i, Yi)\u2212 \u03b4i(h) ) \u2223\u2223\u2223\u2223\u2223S ]\n\u2264 R\u0302n(HL; S\u0303) + E [\nsup h\u2208HL\n1\nn\nn\u2211\ni=1\n\u03c3i\u03b4i(h) \u2223\u2223\u2223\u2223\u2223S ] .\nBy (10), we have\nRn(HL; S\u0303) \u2264 2L ( log 5k\nn\n)1/(\u03b2+1) . (20)\nSince by construction h is L-Lipschitz in its first argument, we have\n\u2223\u2223\u2223\u2223\u2223 n\u2211\ni=1\n\u03c3i\u03b4i(h) \u2223\u2223\u2223\u2223\u2223 \u2264 n\u2211\ni=1\n|\u03b4i(h)| \u2264 L n\u2211\ni=1\nd(Xi, X\u0303i) \u2264 L\u03b1. (21)\nOur claimed bound follows from (20) and (21)."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin", "author": ["Allwein", "Erin L", "Schapire", "Robert E", "Singer", "Yoram"], "venue": "classifiers. JMLR,", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "The computational hardness of estimating edit distance. SICOMP", "author": ["A. Andoni", "R. Krauthgamer"], "venue": null, "citeRegEx": "Andoni and Krauthgamer,? \\Q2010\\E", "shortCiteRegEx": "Andoni and Krauthgamer", "year": 2010}, {"title": "Neural network learning: theoretical foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett", "year": 1999}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "JMLR, 3:463-482,", "citeRegEx": "Bartlett and Mendelson,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson", "year": 2002}, {"title": "Understanding machine learning: From theory to algorithms", "author": ["S. Ben-David", "S. Shalev-Shwartz"], "venue": null, "citeRegEx": "Ben.David and Shalev.Shwartz,? \\Q2014\\E", "shortCiteRegEx": "Ben.David and Shalev.Shwartz", "year": 2014}, {"title": "Characterizations of learnability for classes", "author": ["S. Ben-David", "N. Cesa-Bianchi", "D. Haussler", "P. Long"], "venue": "n}-valued functions. J. Comput. System Sci.,", "citeRegEx": "Ben.David et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1995}, {"title": "Searching dynamic point sets in spaces with bounded doubling dimension", "author": ["R. Cole", "L. Gottlieb"], "venue": null, "citeRegEx": "Cole and Gottlieb,? \\Q2006\\E", "shortCiteRegEx": "Cole and Gottlieb", "year": 2006}, {"title": "Multi-class classification with maximum margin multiple kernel", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": null, "citeRegEx": "Cortes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2013}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "Cover and Hart,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart", "year": 1967}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "JMLR, 2:265-292,", "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "On the learnability and design of output codes for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Mach. Learn.,", "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "Multiclass learnability and the erm principle", "author": ["A. Daniely", "S. Sabato", "S. Ben-David", "S. Shalev-Shwartz"], "venue": "JMLR - Proceedings Track,", "citeRegEx": "Daniely et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2011}, {"title": "On the hardness of approximating minimum vertex cover", "author": ["I. Dinur", "S. Safra"], "venue": "Ann. Math.,", "citeRegEx": "Dinur and Safra,? \\Q2005\\E", "shortCiteRegEx": "Dinur and Safra", "year": 2005}, {"title": "The sizes of compact subsets of hilbert space and continuity of gaussian processes", "author": ["R.M. Dudley"], "venue": "J. Func. Anal.,", "citeRegEx": "Dudley,? \\Q1967\\E", "shortCiteRegEx": "Dudley", "year": 1967}, {"title": "Better multiclass classification via a margin-optimized single binary problem", "author": ["R. El-Yaniv", "D. Pechyony", "E. Yom-Tov"], "venue": null, "citeRegEx": "El.Yaniv et al\\.,? \\Q1954\\E", "shortCiteRegEx": "El.Yaniv et al\\.", "year": 1954}, {"title": "On the nonexistence of uniform homeomorphisms between Lp-spaces", "author": ["P. Enflo"], "venue": "Ark. Mat.,", "citeRegEx": "Enflo,? \\Q1969\\E", "shortCiteRegEx": "Enflo", "year": 1969}, {"title": "Efficient classification for metric data", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "COLT,", "citeRegEx": "Gottlieb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2010}, {"title": "Efficient regression in metric spaces via approximate lipschitz extension", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "SIMBAD,", "citeRegEx": "Gottlieb et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2013}, {"title": "Adaptive metric dimensionality reduction", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "ALT,", "citeRegEx": "Gottlieb et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2013}, {"title": "VC theory of large margin multi-category", "author": ["Y. Guermeur"], "venue": "classifiers. JMLR,", "citeRegEx": "Guermeur,? \\Q2007\\E", "shortCiteRegEx": "Guermeur", "year": 2007}, {"title": "Fast construction of nets in low-dimensional metrics and their applications", "author": ["S. Har-Peled", "M. Mendel"], "venue": "SIAM J. Comp.,", "citeRegEx": "Har.Peled and Mendel,? \\Q2006\\E", "shortCiteRegEx": "Har.Peled and Mendel", "year": 2006}, {"title": "\u03b5-entropy and \u03b5-capacity of sets in function spaces", "author": ["A. Kolmogorov", "V. Tikhomirov"], "venue": "Uspekhi Matematicheskikh Nauk,", "citeRegEx": "Kolmogorov and Tikhomirov,? \\Q1959\\E", "shortCiteRegEx": "Kolmogorov and Tikhomirov", "year": 1959}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Ann. Statist.,", "citeRegEx": "Koltchinskii and Panchenko,? \\Q2002\\E", "shortCiteRegEx": "Koltchinskii and Panchenko", "year": 2002}, {"title": "Navigating nets: Simple algorithms for proximity search", "author": ["R. Krauthgamer", "J. Lee"], "venue": null, "citeRegEx": "Krauthgamer and Lee,? \\Q2004\\E", "shortCiteRegEx": "Krauthgamer and Lee", "year": 2004}, {"title": "Sensitive error correcting output codes", "author": ["J. Langford", "A. Beygelzimer"], "venue": "COLT,", "citeRegEx": "Langford and Beygelzimer,? \\Q2005\\E", "shortCiteRegEx": "Langford and Beygelzimer", "year": 2005}, {"title": "Extension of range of functions", "author": ["E.J. McShane"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "McShane,? \\Q1934\\E", "shortCiteRegEx": "McShane", "year": 1934}, {"title": "Foundations Of Machine Learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": null, "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Planar earthmover is not in l1", "author": ["A. Naor", "G. Schechtman"], "venue": "SICOMP,", "citeRegEx": "Naor and Schechtman,? \\Q2007\\E", "shortCiteRegEx": "Naor and Schechtman", "year": 2007}, {"title": "Combinatorial optimization : algorithms and complexity", "author": ["C. Papadimitriou", "K. Steiglitz"], "venue": null, "citeRegEx": "Papadimitriou and Steiglitz,? \\Q1998\\E", "shortCiteRegEx": "Papadimitriou and Steiglitz", "year": 1998}, {"title": "Distance-based classification with lipschitz functions", "author": ["U. von Luxburg", "O. Bousquet"], "venue": "JMLR, 5:669-695,", "citeRegEx": "Luxburg and Bousquet,? \\Q2004\\E", "shortCiteRegEx": "Luxburg and Bousquet", "year": 2004}, {"title": "Support vector machines for multi-class pattern recognition", "author": ["J. Weston", "C. Watkins"], "venue": "ESANN 99,", "citeRegEx": "Weston and Watkins,? \\Q1999\\E", "shortCiteRegEx": "Weston and Watkins", "year": 1999}, {"title": "Analytic extensions of differentiable functions defined in closed sets", "author": ["H. Whitney"], "venue": "Trans. Amer. Math. Soc.,", "citeRegEx": "Whitney,? \\Q1934\\E", "shortCiteRegEx": "Whitney", "year": 1934}, {"title": "Covering number bounds of certain regularized linear function", "author": ["T. Zhang"], "venue": "classes. JMLR,", "citeRegEx": "Zhang,? \\Q2002\\E", "shortCiteRegEx": "Zhang", "year": 2002}, {"title": "Statistical analysis of some multi-category large margin classification methods", "author": ["T. Zhang"], "venue": "JMLR, 5:1225-1251,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Scale sensitive analysis proof Proof of Theorem 5. An application of Guermeur (2010, Theorem 1) states that with probability 1\u2212 \u03b4, for all L > 0, h \u2208 HL", "author": ["C proof"], "venue": null, "citeRegEx": "proof.,? \\Q2010\\E", "shortCiteRegEx": "proof.", "year": 2010}, {"title": "2012) and doing the stratification on L instead setting \u03b3 = 1 in Guermeur (2010, Theorem 1) and doing the stratification on L", "author": ["Mohri"], "venue": null, "citeRegEx": "Mohri,? \\Q2010\\E", "shortCiteRegEx": "Mohri", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011).", "startOffset": 145, "endOffset": 167}, {"referenceID": 11, "context": "Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011). Following von Luxburg & Bousquet (2004); Gottlieb et al.", "startOffset": 146, "endOffset": 209}, {"referenceID": 11, "context": "Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011). Following von Luxburg & Bousquet (2004); Gottlieb et al. (2010), we adopt a proximity-based approach to supervised multicategory classification in metric spaces.", "startOffset": 146, "endOffset": 233}, {"referenceID": 15, "context": "(i) Many natural metrics, such as L1, earthmover, and edit distance cannot be embedded in a Hilbert space without a large distortion (Enflo, 1969; Naor & Schechtman, 2007; Andoni & Krauthgamer, 2010).", "startOffset": 133, "endOffset": 199}, {"referenceID": 11, "context": "To the best of our knowledge, the sharpest current estimate on the Natarajan dimension (for some special function classes) is dNat = \u00d5(k) with a matching lower bound of \u03a9(k) (Daniely et al., 2011).", "startOffset": 174, "endOffset": 196}, {"referenceID": 26, "context": "A margin-based Rademacher analysis of score functions (Mohri et al., 2012) yields a bound of order \u00d5(k/\u03b3 \u221a n), and this is also the k-dependence obtained by Cortes et al.", "startOffset": 54, "endOffset": 74}, {"referenceID": 0, "context": "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework.", "startOffset": 52, "endOffset": 74}, {"referenceID": 0, "context": "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < \u221e (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form \u00d5 ( log k \u03b3 \u221a dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al.", "startOffset": 52, "endOffset": 452}, {"referenceID": 0, "context": "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < \u221e (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form \u00d5 ( log k \u03b3 \u221a dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper.", "startOffset": 52, "endOffset": 479}, {"referenceID": 0, "context": "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < \u221e (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form \u00d5 ( log k \u03b3 \u221a dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function \u2014 and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as \u00d5(k/n\u03b3), for the separable case with margin \u03b3. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate \u00d5(q/\u03b3 \u221a n).", "startOffset": 52, "endOffset": 1183}, {"referenceID": 0, "context": "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < \u221e (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form \u00d5 ( log k \u03b3 \u221a dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function \u2014 and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as \u00d5(k/n\u03b3), for the separable case with margin \u03b3. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate \u00d5(q/\u03b3 \u221a n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat.", "startOffset": 52, "endOffset": 1247}, {"referenceID": 0, "context": "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < \u221e (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form \u00d5 ( log k \u03b3 \u221a dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function \u2014 and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as \u00d5(k/n\u03b3), for the separable case with margin \u03b3. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate \u00d5(q/\u03b3 \u221a n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat. Guermeur (2007, 2010) gave scale-sensitive analogues of these dimensions. He gave a risk bound decaying as \u00d5 ( log k \u03b3 \u221a d\u03b3Nat/n ) , where d\u03b3Nat is a scale-sensitive Natarajan dimension \u2014 essentially replacing the finite VC dimension dVC in Allwein et al. (2001) by d\u03b3Nat.", "startOffset": 52, "endOffset": 1643}, {"referenceID": 0, "context": "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < \u221e (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form \u00d5 ( log k \u03b3 \u221a dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function \u2014 and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as \u00d5(k/n\u03b3), for the separable case with margin \u03b3. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate \u00d5(q/\u03b3 \u221a n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat. Guermeur (2007, 2010) gave scale-sensitive analogues of these dimensions. He gave a risk bound decaying as \u00d5 ( log k \u03b3 \u221a d\u03b3Nat/n ) , where d\u03b3Nat is a scale-sensitive Natarajan dimension \u2014 essentially replacing the finite VC dimension dVC in Allwein et al. (2001) by d\u03b3Nat. He further showed that for linear function classes in Hilbert spaces, d\u03b3Nat is bounded by \u00d5(k/\u03b3), resulting in a risk bound decaying as \u00d5(k/\u03b3 \u221a n). To the best of our knowledge, the sharpest current estimate on the Natarajan dimension (for some special function classes) is dNat = \u00d5(k) with a matching lower bound of \u03a9(k) (Daniely et al., 2011). A margin-based Rademacher analysis of score functions (Mohri et al., 2012) yields a bound of order \u00d5(k/\u03b3 \u221a n), and this is also the k-dependence obtained by Cortes et al. (2013) in a recent paper proposing a multiple kernel approach to multiclass learning.", "startOffset": 52, "endOffset": 2177}, {"referenceID": 0, "context": "Paper decay rate \u00d5(\u00b7) group Allwein et al. (2001)\u2021 log k \u03b3 \u221a dVC n (II,a) Daniely et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 0, "context": "Paper decay rate \u00d5(\u00b7) group Allwein et al. (2001)\u2021 log k \u03b3 \u221a dVC n (II,a) Daniely et al. (2011)\u2217\u2020\u2021 dNat log k n (I,a) Guermeur (2010)\u2021 log k \u03b3 \u221a d\u03b3Nat n (I,a) Crammer & Singer (2002b)\u2020 k 2 \u03b32n (I,b) Cortes et al.", "startOffset": 28, "endOffset": 96}, {"referenceID": 0, "context": "Paper decay rate \u00d5(\u00b7) group Allwein et al. (2001)\u2021 log k \u03b3 \u221a dVC n (II,a) Daniely et al. (2011)\u2217\u2020\u2021 dNat log k n (I,a) Guermeur (2010)\u2021 log k \u03b3 \u221a d\u03b3Nat n (I,a) Crammer & Singer (2002b)\u2020 k 2 \u03b32n (I,b) Cortes et al.", "startOffset": 28, "endOffset": 134}, {"referenceID": 0, "context": "Paper decay rate \u00d5(\u00b7) group Allwein et al. (2001)\u2021 log k \u03b3 \u221a dVC n (II,a) Daniely et al. (2011)\u2217\u2020\u2021 dNat log k n (I,a) Guermeur (2010)\u2021 log k \u03b3 \u221a d\u03b3Nat n (I,a) Crammer & Singer (2002b)\u2020 k 2 \u03b32n (I,b) Cortes et al.", "startOffset": 28, "endOffset": 184}, {"referenceID": 0, "context": "Paper decay rate \u00d5(\u00b7) group Allwein et al. (2001)\u2021 log k \u03b3 \u221a dVC n (II,a) Daniely et al. (2011)\u2217\u2020\u2021 dNat log k n (I,a) Guermeur (2010)\u2021 log k \u03b3 \u221a d\u03b3Nat n (I,a) Crammer & Singer (2002b)\u2020 k 2 \u03b32n (I,b) Cortes et al. (2013) k 2 \u03b3 \u221a n (I,b) Guermeur (2010) k \u03b32 \u221a n (I,b) Zhang (2004) 1 \u03b3 \u221a k n (I,b)", "startOffset": 28, "endOffset": 220}, {"referenceID": 0, "context": "Paper decay rate \u00d5(\u00b7) group Allwein et al. (2001)\u2021 log k \u03b3 \u221a dVC n (II,a) Daniely et al. (2011)\u2217\u2020\u2021 dNat log k n (I,a) Guermeur (2010)\u2021 log k \u03b3 \u221a d\u03b3Nat n (I,a) Crammer & Singer (2002b)\u2020 k 2 \u03b32n (I,b) Cortes et al. (2013) k 2 \u03b3 \u221a n (I,b) Guermeur (2010) k \u03b32 \u221a n (I,b) Zhang (2004) 1 \u03b3 \u221a k n (I,b)", "startOffset": 28, "endOffset": 252}, {"referenceID": 0, "context": "Paper decay rate \u00d5(\u00b7) group Allwein et al. (2001)\u2021 log k \u03b3 \u221a dVC n (II,a) Daniely et al. (2011)\u2217\u2020\u2021 dNat log k n (I,a) Guermeur (2010)\u2021 log k \u03b3 \u221a d\u03b3Nat n (I,a) Crammer & Singer (2002b)\u2020 k 2 \u03b32n (I,b) Cortes et al. (2013) k 2 \u03b3 \u221a n (I,b) Guermeur (2010) k \u03b32 \u221a n (I,b) Zhang (2004) 1 \u03b3 \u221a k n (I,b)", "startOffset": 28, "endOffset": 280}, {"referenceID": 7, "context": "Multiclass extensions of SVM and related kernel methods (Weston & Watkins, 1999; Crammer & Singer, 2002a,b; Crammer et al., 2002; Cortes et al., 2013) fall into category (b).", "startOffset": 56, "endOffset": 150}, {"referenceID": 16, "context": "(2013b) exhibit a characteristic \u201ccurse of dimensionality\u201d decay rate of O(n\u22121/(D+1)), but more optimistic asymptotics can be obtained (Guermeur, 2007, 2010; Zhang, 2002, 2004; Gottlieb et al., 2010).", "startOffset": 135, "endOffset": 199}, {"referenceID": 7, "context": ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification.", "startOffset": 8, "endOffset": 201}, {"referenceID": 7, "context": ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification.", "startOffset": 8, "endOffset": 282}, {"referenceID": 7, "context": ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification. Unlike Hilbert spaces, which admit dimension-free margin bounds, we are not aware of any metric space risk bound that does not explicitly depend on some metric dimension D or covering numbers. The bounds in Ben-David & Shalev-Shwartz (2014); Gottlieb et al.", "startOffset": 8, "endOffset": 560}, {"referenceID": 7, "context": ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification. Unlike Hilbert spaces, which admit dimension-free margin bounds, we are not aware of any metric space risk bound that does not explicitly depend on some metric dimension D or covering numbers. The bounds in Ben-David & Shalev-Shwartz (2014); Gottlieb et al. (2013b) exhibit a characteristic \u201ccurse of dimensionality\u201d decay rate of O(n\u22121/(D+1)), but more optimistic asymptotics can be obtained (Guermeur, 2007, 2010; Zhang, 2002, 2004; Gottlieb et al.", "startOffset": 8, "endOffset": 585}, {"referenceID": 19, "context": "2) and via scale-sensitive techniques in the spirit of Guermeur (2007) (Section 3.", "startOffset": 55, "endOffset": 71}, {"referenceID": 25, "context": "Construct a 2L-Lipschitz extension for \u0125 from N to all over X (such an extension always exists, (McShane, 1934; Whitney, 1934)).", "startOffset": 96, "endOffset": 126}, {"referenceID": 31, "context": "Construct a 2L-Lipschitz extension for \u0125 from N to all over X (such an extension always exists, (McShane, 1934; Whitney, 1934)).", "startOffset": 96, "endOffset": 126}, {"referenceID": 34, "context": "With the additional assumptions on X we follow the proof idea in Kolmogorov & Tikhomirov (1959) and demonstrate the tighter bound |\u0124 | \u2264 (\u23084k/\u03b5\u2309+ 1)(2k + 1)|N |\u22121 = The remainder of the proof is based on a technique communicated to us by R.", "startOffset": 51, "endOffset": 96}, {"referenceID": 34, "context": "With the additional assumptions on X we follow the proof idea in Kolmogorov & Tikhomirov (1959) and demonstrate the tighter bound |\u0124 | \u2264 (\u23084k/\u03b5\u2309+ 1)(2k + 1)|N |\u22121 = The remainder of the proof is based on a technique communicated to us by R. Krauthgamer, a variant of the classic Kolmogorov & Tikhomirov (1959) method.", "startOffset": 51, "endOffset": 310}, {"referenceID": 0, "context": "In terms of the number of classes k, our bound compares favorably to those in Allwein et al. (2001); Guermeur (2007, 2010), and more recently in Daniely et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 0, "context": "In terms of the number of classes k, our bound compares favorably to those in Allwein et al. (2001); Guermeur (2007, 2010), and more recently in Daniely et al. (2011), which have a k-dependence of O(dNat log k), where dNat is the (scale-sensitive, k-dependent) Natarajan dimension of the multiclass hypothesis class.", "startOffset": 78, "endOffset": 167}, {"referenceID": 16, "context": "For k = 2, Gottlieb et al. (2010) reduced the problem of computing m\u0302(L) to one of finding a minimal vertex cover in a bipartite graph (by K\u00f6nig\u2019s theorem, the latter is efficiently computable as a maximal matching).", "startOffset": 11, "endOffset": 34}, {"referenceID": 16, "context": "Rather a brute-force searching all of these values, Theorem 7 of Gottlieb et al. (2010) shows that using an O(log n) time binary search over the values of L, one may approximately minimizeQ(L), which in turn yields an approximate solution to (11).", "startOffset": 65, "endOffset": 88}, {"referenceID": 16, "context": "The technique proposed in Gottlieb et al. (2013b) may roughly be described as a metric analogue of PCA.", "startOffset": 26, "endOffset": 50}, {"referenceID": 26, "context": "where the identity is obvious and the inequality is a simple consequence of measure concentration (Mohri et al., 2012).", "startOffset": 98, "endOffset": 118}, {"referenceID": 16, "context": "Although computing the optimal (\u03b1\u2217, \u03b2\u2217) seems computationally difficult, Gottlieb et al. (2013b) were able to obtain an efficient (O(1), O(1))-bicriteria approximation.", "startOffset": 73, "endOffset": 97}], "year": 2014, "abstractText": "We develop a general framework for margin-based multicategory classification in metric spaces. The basic work-horse is a margin-regularized version of the nearest-neighbor classifier. We prove generalization bounds that match the state of the art in sample size n and significantly improve the dependence on the number of classes k. Our point of departure is a nearly Bayes-optimal finite-sample risk bound independent of k. Although k-free, this bound is unregularized and non-adaptive, which motivates our main result: Rademacher and scale-sensitive margin bounds with a logarithmic dependence on k. As the best previous risk estimates in this setting were of order \u221a k, our bound is exponentially sharper. From the algorithmic standpoint, in doubling metric spaces our classifier may be trained on n examples in O(n log n) time and evaluated on new points in O(log n) time.", "creator": "LaTeX with hyperref package"}}}