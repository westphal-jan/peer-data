{"id": "1402.0574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Learning to Predict from Textual Data", "abstract": "given probable current major event, us tackle the problem of generating plausible predictions of subsequent events some might emerge. we present a new methodology for modeling and predicting such future news events using machine learning and asset mining techniques. our pundit algorithm generalizes examples of causality pairs to infer a causality predictor. to obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing simple predefined causality patterns. for generalization, every model uses a vast number data world data ontologies. empirical evaluation on real news articles shows whenever our pundit algorithm performs as effective as non - expert humans.", "histories": [["v1", "Tue, 4 Feb 2014 01:39:12 GMT  (1391kb)", "http://arxiv.org/abs/1402.0574v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["kira radinsky", "sagie davidovich", "shaul markovitch"], "accepted": false, "id": "1402.0574"}, "pdf": {"name": "1402.0574.pdf", "metadata": {"source": "CRF", "title": "Learning to Predict from Textual Data", "authors": ["Kira Radinsky", "Sagie Davidovich", "Shaul Markovitch"], "emails": ["kirar@cs.technion.ac.il", "mesagie@gmail.com", "shaulm@cs.technion.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Causality has been studied since antiquity, e.g., by Aristotle, but modern perceptions of causality have been most influenced, perhaps, by the work of David Hume (1711\u20131776), who referred to causation as the strongest and most important associative relation, that which lies at the heart of our perception and reasoning about the world, as \u201cit is constantly supposed that there is a connection between the present fact and that which is inferred from it.\u201d\nCausation is also important for designing computerized agents. When an agent, situated in a complex environment, plans its actions, it reasons about future changes to the environment. Some of these changes are a result of its own actions, but many others are a result of various chains of events not necessarily related to the agent. The process of observing an event, and reasoning about future events that might be caused by it, is called causal reasoning.\nIn the past, computerized agents could not operate in complex environments due to their limited perceptive capabilities. The proliferation of the World Wide Web, however, changed all that. An intelligent agent can act in the virtual world of the Web, perceiving the current state of the world through extensive sources of textual information, including Web pages, tweets, news reports, and online encyclopedias, and performing various tasks such as searching, organizing, and generating information. To act intelligently in such a complex virtual environment, the agent must be able to perceive the current state and reason about future states through causal reasoning. Such reasoning ability can be extremely helpful in conducting complex tasks such as identifying political unrest, detecting and tracking\nc\u00a92012 AI Access Foundation. All rights reserved.\nsocial trends, and generally supporting decision making by politicians, businesspeople, and individual users.\nWhile many works have been devoted to extracting information from text (e.g., Banko, Cafarella, Soderl, Broadhead, & Etzioni, 2007; Carlson, Betteridge, Kisiel, Settles, Hruschka, & Mitchell, 2010), little has been done in the area of causality extraction, with the works of Khoo, Chan, and Niu (2000) and Girju and Moldovan (2002) being notable exceptions. Furthermore, the algorithms developed for causality extraction try to detect causality and cannot be used to predict it, that is, to generate new events the given event might cause.\nOur goal in this paper is to provide algorithms that perform causal reasoning, in particular causality prediction, in textually represented environments. We have developed a causality learning and prediction algorithm, Pundit, that, given an event represented in natural language, predicts future events it can cause. Our algorithm is trained on examples of causality relations. It then uses large ontologies to generalize over the causality pairs and generate a prediction model. The model is represented by an abstraction tree, that, given an input cause event, finds its most appropriate generalization, and uses learned rules to output predicted effect events.\nWe have implemented our algorithm and applied it to a large collection of news reports from the last 150 years. To extract training examples from the news corpus, we do not use correlation, by means of which causality is often misidentified. Instead, we use textual causality patterns (such as \u201cX because Y\u201d or \u201cX causes Y\u201d), applied to news headlines, to identify pairs of structured events that are supposedly related by causality. The result is a semantically-structured causality graph of 300 million fact nodes connected by more than one billion edges. To evaluate our method, we tested it on a news archive from 2010, which was not used during training. The results are judged by human evaluators.\nTo give some intuition about the type of predictions the algorithm generates, we present here two examples of actual predictions made by our system. First, given the event \u201cMagnitude 6.5 earthquake rocks the Solomon Islands,\u201d the algorithm predicted that \u201ca tsunamiwarning will be issued for the Pacific Ocean.\u201d It learned this from past examples on which it was trained, one of which was \u30087.6 earthquake strikes island near India, tsunami warning issued for Indian Ocean\u3009. Pundit was able to infer that an earthquake occurring near an island would result in a tsunami warning being issued for its ocean. Second, given the event \u201cCocaine found at Kennedy Space Center,\u201d the algorithm predicted that \u201ca few people will be arrested.\u201d This was partially based on the past example \u3008police found cocaine in lab \u2192 2 people arrested\u3009.\nThe contributions of this work are threefold: First, we present novel and scalable algorithms for generalizing causality pairs to causality rules. Second, we provide a new method for using casualty rules to predict new events. Finally, we implement the algorithms in a large scale system and perform an empirical study on realistic problems judged by human raters. We make the extracted causality information publicly available for further research in the field 1.\n1. http://www.technion.ac.il/~kirar/Datasets.html"}, {"heading": "2. Learning and Predicting Causality", "text": "In this section, we describe the Pundit algorithm for learning and predicting causality. We start with an overview of the learning and prediction process. During training, the learning algorithm receives causality event pairs, extracted from historical news archives (Section 3). The algorithm then generalizes over the given examples using world knowledge and produces an abstraction tree (AT)(Section 2.4). For each node in the AT, a prediction rule is generated from the examples in the node (Section 2.5). Then, during the prediction phase, the algorithm matches the given new event to nodes in the AT, and the associated rule is applied on it to produce possible effect events (Section 2.6). Those events are then filtered (Section 2.7) and an effect event output. The output event itself is also given in natural language, in sentence-like form. The process is illustrated in Figure 1."}, {"heading": "2.1 Event Representation", "text": "The basic element of causal reasoning is an event. The Topic Tracking and Detection (TDT) community (Allan, 2002) has defined an event as \u201ca particular thing that happens at a specific time and place, along with all necessary preconditions and unavoidable consequences.\u201d\nOther philosophical theories consider events as exemplifications of properties by objects at times (Kim, 1993). For example, Caesar\u2019s death at 44 BC is Caesar\u2019s exemplification of the property of dying at time 44 BC. Those theories impose structure on events, where a change in one of the elements yields a different event. For example, Shakespear\u2019s death is a different event from Caesar\u2019s death, as the objects exemplifying the property are different. In this section, we will discuss a way to represent events following Kim\u2019s (1993) exemplification theory that will allow us to easily compare them, generalize them, and reason about them.\nThere are three common approaches for textual event representation: The first approach describes an event at sentence level by running text or individual terms (Blanco, Castell, & Moldovan, 2008; Sil, Huang, & Yates, 2010). Event similarity is treated as a distance metric between the two events\u2019 bag of words. While such approaches can be useful, they often fail to perform fine-grained reasoning. Consider, for example, three events: \u201dUS Army bombs a warehouse in Iraq,\u201d \u201dIraq attacks US base,\u201d and \u201dTerrorist base was attacked by the US Marines in Kabul.\u201d Representing these events by individual terms alone might yield that the first two are more similar than the first and the last as they have more words in common. However, such approaches disregard the fact the actors of the first and last event are military groups and that Kabul and Iraq are the event locations. When these facts are taken into account, the first and last events are clearly more similar than the first and second.\nThe second approach describes events in a syntax-driven manner, where the event text is transformed into syntax-based components, such as noun phrases (Garcia, 1997; Khoo et al., 2000; Girju & Moldovan, 2002; Chan & Lam, 2005). In our example, this representation again erroneously finds the second and third events to be most similar due to the syntactic similarity between them. Using the first two approaches, it is hard to make practical generalizations about events or to compare them in a way that takes into account all the semantic elements that compose them.\nThe third approach is semantic (similar to the representation in Cyc; Lenat & Guha, 1990), and maps the atomic elements of each event to semantic concepts. This approach provides grounds for canonic representation of events that are both comparable and generalizable. In this work, we follow the third approach and represent events semantically.\nGiven a set of entities O that represent physical objects and abstract concepts in the real world (e.g., people, instances, and types), and a set of actions P , we define an event as an ordered set e = \u3008P,O1, . . . , O4, t\u3009, where:\n1. P is a temporal action or state that the event\u2019s objects exhibit.\n2. O1 is the actor that performed the action.\n3. O2 is the object on which the action was performed.\n4. O3 is the instrument with which the action was performed.\n5. O4 is the location of the event.\n6. t is a time-stamp.\nFor example, the event \u201cThe U.S Army destroyed a warehouse in Iraq with explosives,\u201d which occurred on October 2004, is modeled as: Destroy (Action); U.S Army (Actor); warehouse (Object); explosives (Instrument); Iraq (Location); October 2004 (Time). This approach is inspired by Kim\u2019s (1993) property-exemplification of events theory."}, {"heading": "2.2 Learning Problem Definition", "text": "We treat causality inference as a supervised learning problem. Let Ev be the universe of all possible events. Let f : Ev \u00d7 Ev \u2192 {0, 1} be the function\nf(e1, e2) =\n{ 1 if e1 causes e2,\n0 if otherwise.\nWe denote f+ = {(e1, e2)|f(e1, e2) = 1}. We assume we are given a set of possible positive examples E \u2286 f+.\nOur goal is not merely to test whether a pair of events is a plausible cause-effect pair by f , but to generate for a given event e the events it can cause. For this purpose we define g : Ev \u2192 2Ev to be g(e) = {e\u2032|f(e, e\u2032) = 1}; that is, given an event, output the set of events it can cause. We wish to build this predictor g using the examples E.\nLearning f from E could have been solved by standard techniques for concept learning from positive examples. The requirement to learn g, however, presents the challenging task of structured prediction from positive examples."}, {"heading": "2.3 Generalizing Over Objects and Actions", "text": "Our goal is to develop a learning algorithm that automatically produces a causality function based on examples of causality pairs. The inferred causality function should be able to predict the outcome of a given event, even if it was never observed before. For example, given the training examples \u3008earthquake in Turkey, destruction\u3009 and \u3008earthquake in Australia, destruction\u3009, and a current new event of \u201cearthquake in Japan,\u201d a reasonable prediction would be \u201cdestruction.\u201d To be able to handle such predictions, we must endow our learning algorithm with generalization capacity. For example, in the above scenario, the algorithm must be able to generalize Australia and Turkey to countries, and to infer that earthquakes in countries might cause destruction. This type of inference and the knowledge that Japan is also a country enables the algorithm to predict the effects of new events using patterns in the past.\nTo generalize over a set of examples, each consisting of a pair of events, we perform generalization over the components of these events. There are two types of components \u2013 objects and actions.\nTo generalize over objects, we assume the availability of a semantic network Go = (V,E), where the nodes V \u2286 O are the objects in our universe, and the labels on the edges are relations such as isA, partOf and CapitalOf. In this work, we consider one of the largest semantic networks available, the LinkedData ontology (Bizer, Heath, & Berners-Lee, 2009), which we describe in detail in Section 3.\nWe define two objects to be similar if they relate to a third object in the same way. This relation can be a label or a sequence of labels in the semantic network. For example,\nParis and London will be considered similar because their nodes are connected by the path Capital\u2212of\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 In\u2212continent\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 to the node Europe. We now formally define this idea.\nDefinition 1. Let a, b \u2208 V . A sequence of labels L = l1, . . . , lk is a generalization path of a, b, denoted by GenPath(a,b), if there exist two paths in G, (a, v1, l1), . . . (vk, vk+1, lk) and (b, w1, l1), . . . (wk, wk+1, lk), s.t. vk+1 = wk+1.\nOvergeneralization of events should be avoided \u2013 e.g., given two similar events, one occurring in Paris and one in London, we wish to produce the generalization \u201ccity in Europe\u201d ( Capital\u2212of\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 In\u2212continent\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Europe) rather than the more abstract generalization \u201ccity on a continent\u201d ( Capital\u2212of\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 In\u2212continent\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 IsA\u2212\u2212\u2192 Continent). We wish our generalization to be as specific as possible. We call this minimal generalization of objects.\nDefinition 2. The minimal generalization path, denoted by MGenPath(a, b), is defined as the set containing the shortest generalization paths. We denote distGen(a, b) as the length of the MGenPath(a, b).\nPath-based semantic distances such as the one above were shown to be successful in many NLP applications. For example, the semantic relatedness of two words was measured by means of a function that measured the distance between words in a taxonomy (Rada, Mili, Bicknell, & Blettner, 1989; Strube & Ponzetto, 2006). We build on this metric and expand it to handle events that are structured and can contain several objects from different ontologies.\nTo efficiently produce MGenPath, we designed an algorithm (described in Figure 2), based on dynamic programming, that computes the MGenPath for all object pairs in G. For simplicity, we describe an algorithm that computes a single path for each two nodes a and b, rather than the set of all shortest paths. At step 1 a queue that holds all nodes with the same generalization is initialized. At step 2, the algorithm identifies all nodes (a, b) that have a common node (c) connecting to them via the same type of edge (l). c can be thought of as a generalization of a and b. The Mgen structure maps a pair of nodes to their generalization (Mgen.Gen) and their generalization path (MGen.Pred). At step 3, in a dynamic programming manner, the algorithm iterates over all nodes (a, b) in Mgen for which we found a minimal generalization in previous iterations, and finds two nodes \u2013 one (x) connecting to a and one (y) connecting to b via the same type of edge l (step 3.4). Thus, the minimal generalization of x and y is the minimal generalization of a and b, and the path is the MGenPath of a, b with the addition of the edge type l. This update is performed in steps 3.4.1\u20133.4.4. Eventually, when no more nodes with minimal generalization can be expanded (i.e., the algorithm cannot find two nodes that connect to them via the same edge type), it stops and returns Mgen (step 4). During prediction, if several Mgen exists, we consider both during the prediction with their corresponding MGenPath.\nWe define a distance between actions using an ontology Gp, similarly to the way we defined distance between objects. Specifically, we use the VerbNet (Kipper, 2006) ontology, which is one of the largest English verb lexicons. It has mapping to many other online resources, such as Wordnet (Miller, 1995). The ontology is hierarchical and is based on a classification of the verbs to the Levin classes (Dang, Palmer, & Rosenzweig, 1998). This resource has been widely used in many natural language processing applications (Shi &\nMihalcea, 2005; Giuglea & Moschitti, 2006). Using this ontology we describe the connections between verbs. Figure 10 shows a node in this ontology that generalizes the actions \u201chit\u201d and \u201ckick.\u201d"}, {"heading": "2.4 Generalizing Events", "text": "In order to provide strong support for generalization, we wish to find similar events that can be generalized to a single abstract event. In our example, we wish to infer that both \u3008earthquake in Turkey, destruction\u3009 and \u3008earthquake in Australia, destruction\u3009 are examples of the same group of events. Therefore, we wish to cluster the events in such a way that events with similar causes and effects will be clustered together. As in all clustering methods, a distance measure between the objects should be defined. Let ei = \u3008P i, Oi1, . . . , Oi4, ti\u3009 and ej = \u3008P j , Oj1, . . . , Oj4, tj\u3009 be two events. In the previous subsection we defined a distance function between objects (and between actions). Here, we define the similarity of two events ei and ej to be a function of distances between their objects and actions:\nSIM(ei, ej) = f ( dist Gp Gen(P i, P j), distGoGen(O i 1, O j 1), . . . , dist Go Gen(O i 4, O j 4) ) , (1)\nwhere, distGGen is the distance function distGen in the graph G, and f is an aggregation function. In this work, we mainly use the average as the aggregation function, but also analyze several alternatives.\nLikewise, a similarity between two pairs of cause-effect events \u3008ci, ei\u3009 and \u3008cj , ej\u3009 is defined as:\nSIM(\u3008ci, ei\u3009, \u3008cj , ej\u3009) = f ( SIM(ci, cj), SIM(ei, ej) ) . (2)\nUsing the similarity measure suggested above, the clustering process can be thought of as a grouping of the training examples in such a way that there is a low variance in their effects and a high similarity in their cause. This is similar to information gain methods where examples are clustered by their class. We use the HAC hierarchical clustering algorithm (Eisen, Spellman, Brown, & Botstein, 1998) as our clustering method. The algorithm starts by joining the closest event pairs together into a cluster. It then keeps repeating the process by joining the closest two clusters together until all elements are linked into a hierarchical graph of events we call an abstraction tree (AT). Distance between clusters is measured by the distance of their representative events. To allow this, we assign to each node in the AT a representative cause event, which is the event closest to the centroid of the node\u2019s cause events. During the prediction phase, the input cause event will be matched to one of the created clusters, i.e., closest to the representative cause event of the cluster."}, {"heading": "2.5 Causality Prediction Rule Generation", "text": "The last phase of the learning is the creation of rules that will allow us, given a cause event, to generate a prediction about it. As the input cause event is matched against the node centroid, a naive approach would be to return the effect event of the matched centroid. This, however, would not provide us with the desired result. Assume an event ei=\u201cEarthquake hits Haiti\u201d occurred today, and that is matched to a node represented by the centroid: \u201cEarthquake hits Turkey,\u201d whose effect is \u201cRed Cross help sent to Ankara.\u201d Obviously, predicting that Red Cross help will be sent to Ankara because of an earthquake in Haiti is not reasonable. We would like to be able to abstract the relation between the past cause and past effect and learn a predicate clause that connects them, for example \u201cEarthquake hits [Country Name]\u201d yielding \u201cRed Cross help sent to [Capital of Country].\u201d During prediction, such a clause will be applied to the input event ei, producing its predicted effect. In our example, the logical predicate clause would be CapitalOf, as CapitalOf(Turkey)= Ankara. When applied on the current event ei, CapitalOf(Haiti) = Port-au-Prince, the output will now be \u201cRed Cross help sent to Port-au-Prince.\u201d Notice that the the clauses can only be applied on certain types of objects \u2013 in our case, countries. The clauses can be of any length, e.g., the pair \u3008\u201csuspect arrested in Brooklyn,\u201d \u201cBloomberg declares emergency\u201d\u3009 produces the clause Mayor(BoroughOf(x)), as Brooklyn is a borough of New York, whose mayor is Bloomberg.\nWe will now show how to learn such clauses for each node in the AT graph. Recall that the semantic network graph GO is an edge-labeled graph, where each edge is a triplet \u3008v1, v2, l\u3009, where l is a predicate (e.g., \u201cCapitalOf\u201d). The rule-learning procedure is divided into two main steps. First, we find an undirected path pi of length at most k in GO between any object of the cause event to any object of the effect event. Note that we do not necessarily look for paths between two objects with the same role. In the above example, we found a path between the location of the cause event (Brooklyn) to the actor of the effect event (Bloomberg). Second, we construct a clause using the labels of the path pi as\nthe predicates. We call this the predicate projection of size k, pred = l1, . . . , lk from an event ei to an event ej . During prediction, the projections will be applied to the new event e = \u3008P i, O1, . . . , O4, t\u3009 by finding an undirected path in GO from Oi with the sequence of labels of pred. As k is unknown, the algorithm, for each training example \u3008ct, et\u3009 in a node in the AT, finds all possible predicate paths with increasing sizes of k from the objects of ct to the objects of et in the GO graph. Each such path is weighted by the number of times it occurred in the node, the support of the rule. The full predicate generation procedure is described in Figure 3. The function LearnPredicateClause calls the inner function FindPredicatePath for different k sizes and different objects from the given cause and effect events. FindPredicatePath is a recursive function that tries to find a path between the two objects in a graph of length k. It returns the labels of such a path if found. The rule generated is a template for generating the prediction of a future event given the cause event. An example of such a rule can be seen in Figure 4. Rules that return NULL are not displayed in the figure. In this example, when we generate object O1 of the future event, we try to apply the path l1\u2212\u2192 l2\u2212\u2192 on the object O4 of the cause, thus generating possible objects that can be object O1 of the prediction (see Section 2.6). Similarly, the path l1\u2212\u2192 l2\u2212\u2192 is applied on O2, generating more possible objects. For object O2 of the prediction, a simple path of one label was generated. Therefore, during prediction, the possible objects for O2 are the ones that connect to Ocause4 with the label l8 (if any). For object O3 of the prediction, we use the Ocause3 . For O4 no special rule was generated (FindPredicatePath returned NULL for all objects), and the final prediction will have Oeffect4 ."}, {"heading": "2.6 Prediction", "text": "Given a trained model g\u0302, it can be applied to a new event e = \u3008P i, O1, . . . , O4, t\u3009 in order to produce its effects. The process is divided into two main steps \u2013 propagating the event in the AT to retrieve a set of matched nodes, and applying the rules of each matched node to produce the possible effects.\nGiven a new event, Pundit traverses the AT starting from the root. For each node in the search frontier, the algorithm computes the similarity (SIM(ei, ej)) of the input event to the centroid of each of the children on this node, and expands those children with better similarity than their parent. This idea can be stated intuitively as an attempt to find the nodes which are the least general but still similar to the new event. The full algorithm is illustrated in Figure 5. An illustration of the process can be seen in Figure 6. Here, an event of a bombing in Baghdad is received as input. The system searches for the least general cause event it has observed in the past (for simplicity we only show a short notation of the cause events in the AT). In our case, it is a generalized cluster: \u201cbombing in city.\u201d Other candidates selected are the \u201cmilitary communication\u201d cluster and the \u201cbombing\u201d cluster (as the node \u201cbombing in worship area\u201d has a lower score than \u201cbombing\u201d).\nFor each node retrieved in the previous stage, its predicate projection, pred, is applied to the new event e = \u3008P i, O1, . . . , O4, t\u3009. Informally, we can say that pred is applied by finding an undirected path in GO from Oi with the labels of pred. This rule generates a possible effect event from the retrieved node. The projection results are all the reached objects in the vertex. The formal explanation is that pred can be applied if \u2203V0 : O \u2286 V0,\u2203V1 . . . Vk : (V0, V1, l1), . . . (Vk\u22121, Vk, lk) \u2208 Edges(GO). The projection results are all the\nobjects o \u2208 Vk. The projection results of all the nodes are weighted by the similarity of the target cause to the node MGen and then ranked by the support of the rule (for tie breaking). If several MGen exists, the highest similarity is considered. See Figure 7 for a complete formal description of the algorithm. In our example (Figure 6), the candidate \u201cbombing in [city]\u201d has the following rules:\n1. P effect = happen, Oeffect1 = riot , O effect 4 = O cause 4\n2. P effect = happen, Oeffect1 = riot , O effect 4 = O cause 4\nmain\u2212street\u2212of\u2190\u2192\n3. P effect = kill, Oeffect2 = people\n4. P effect = condemn, Oeffect1 = O4 mayor\u2212of\u2190\u2192 borough\u2212of\u2190\u2192 , Oeffect2 = attack\nFor clarity, for objects where no rule can be applied (the rule for the object is NULL), we use the effect concept of the matched training example.\nFor the event Baghdad Bombing (O1 = Bombing, O4 = Baghdad), applying the rules yields the following:\n1. Baghdad Riots (P effect = happen, Oeffect1 = riot , O effect 4 = Baghdad).\n2. Caliphs Street Riots (P effect = happen, Oeffect1 = riot , O effect 4 = Caliphs Street\nmain\u2212street\u2212of\u2190\u2192 Ocause4 ).\nRule(cause, effect) =\n3. People killed (P effect = kill, Oeffect2 = people).\n4. This rule cannot be applied on the given event, as there is no outgoing edge of type borough-of for the node Baghdad."}, {"heading": "2.7 Pruning Implausible Effects", "text": "In some cases, the system generated implausible predictions. For example, for the event \u3008lightning kills 5 people\u3009, the system predicted that \u3008lightning will be arrested\u3009. This prediction was based on generalized training examples in which people who killed other people got arrested, such as: \u3008Man kills homeless man,man is arrested\u3009. But if we could determine how logical an event is, we could avoid such false predictions. In this section we discuss how we filter them out.\nThe goal of our filtering component is different from that of the predictor. While the predictor\u2019s goal is to generate predictions about future events, this component\u2019s goal is to monitor those predictions. While the predictor learns a causality relation between events, this component learns their plausibility.\nThe right way to perform such filtering is to utilize common sense knowledge for each action. Such knowledge would state the type of the actor and the object that can perform the action, the possible instruments with which the action can be preformed and the possible locations. If such knowledge would have existed, it would have identified that for the action arrest the object can be only human. However, such common sense knowledge is currently not available. Therefore, we had to resort to the common practice of using statistical methods.\nIn the information extraction literature, identifying the relationship between facts and their plausibility has been widely studied. These methods usually estimate the prior probability of a relation by examining the frequency of its pattern in a large corpus, such as the Web (Banko et al., 2007). For example, for the relation \u3008People,arrest,People\u3009 these methods return that this phrase was mentioned 188 times on the Web, and that the relation \u3008People,arrest,[Natural Disaster]\u3009 was mentioned 0 times. Similarly, we estimate the prior probability of an event to occur from its prior appearance in the New York Times, our\nprimary source of news headlines. We then filter out events that, a priori, have very low probability to occur.\nWe present the algorithm in Figure 8. We calculated how many times the semantic concepts representing the event, or their immediate generalizations, actually occurred together in the past in the same semantic roles. In our example, we check how many times lightning or other natural phenomena were arrested. Formally, we define point-wise mutual concept information (PMCI) between two entities or verbs oi, oj (e.g., lightning and arrest) in roles ri, rj (e.g., actor and action) be defined as\nPMCI(oi, oj , ri, rj) = log p(oi@ri, oj@rj)\np(oi@ri)p(oj@rj) . (3)\nGiven an event, we calculate the average PMCI of its components. The algorithm filters out predicted events that have low average PMCI. We assume that the cause and effect examples in the training are the ground truth, and should yield a high PMCI. Therefore, we evaluate the threshold for filtering from this training data. That is, we collected all the effects we observed in the training data and estimated their average PMCI on the entire NYT dataset.\nThe reader should note that applying such rules might create a problem. If in the past no earthquake occurred in Tokyo, the pruning procedure might return low plausibility. To handle these type of errors, we calculate the PMCI of the upper level categories of entities (e.g., natural disasters) rather than specific entities (e.g., earthquakes). We therefore restrict ourselves to only the two upper level categories."}, {"heading": "3. Implementation Details", "text": "In the previous section, we presented a high-level algorithm that requires training examples T , knowledge about entities GO, and event action classes P . One of the main challenges of this work was to build a scalable system to meet those requirements.\nWe present a system that mines news sources to extract events, constructs their canonical semantic model, and builds a causality graph on top of those events. The system crawled, for more than 4 months, several dynamic information sources (see Section 3.1 for details). The largest information source was the NYT archive, on which optical character recognition (OCR) was performed. The overall gathered data spans more than 150 consecutive years (1851\u2212 2009).\nFor generalization of the objects, the system automatically reads Web content and extracts world knowledge. The knowledge was mined from structured and semi-structured publicly available information repositories. The generation of the causality graph was distributed over 20 machines, using a MapReduce framework. This process efficiently unites different sources, extracts events, and disambiguates entities. The resulting causality graph is composed of over 300 million entity nodes, one billion static edges (connecting the different objects encountered in the events), and over 7 million causality edges (connecting events that were found by Pundit to cause each other). Each rule in the AT was generated using an average of 3 instances with standard deviation of 2.\nOn top of the causality graph, a search and indexing infrastructure was built to enable search over millions of documents. This highly scalable index allows a fast walk on the graph of events, enabling efficient inference capabilities during the prediction phase of the algorithm."}, {"heading": "3.1 World Knowledge Mining", "text": "The entity graph Go is composed of concepts from Wikipedia, ConceptNet (Liu & Singh, 2004), WordNet (Miller, 1995), Yago (Suchanek, Kasneci, & Weikum, 2007), and OpenCyc; the billion labeled edges of the graph Go are the predicates of those ontologies. In this section we describe the process by which this knowledge graph is created and the search system built upon it.\nOur system creates the entity graph by collecting the above content, processing feeds, and processing formatted data sets (e.g., Wikipedia). Our crawler then archives those documents in raw format, and transforms them into RDF (Resource Description Framework) format (Lassila, Swick, Wide, & Consortium, 1998). The concepts are interlinked by humans as part of the Linked Data project (Bizer et al., 2009). The goal of Bizer et al.\u2019s (2009) Linked Data project is to extend the Web by interlinking multiple datasets as RDF and by setting RDF links between data items from different data sources. Datasets include DBPedia (a structured representation of Wikipedia), WordNet, Yago, Freebase, and more. By September 2010 this had grown to 25 billion RDF triples, interlinked by around 395 million RDF links.\nWe use SPARQL queries as a way of searching over the knowledge graph. Experiments of the performance of those queries on the Berlin benchmark (Bizer & Schultz, 2009) provided evidence for the superiority of Virtuoso open source triple structures for our task."}, {"heading": "3.2 Causality Event Mining and Extraction", "text": "Our supervised learning algorithm requires many learning examples to be able to generalize well. As the amount of temporal data is extremely large, spanning over millions of articles, the goal of obtaining human annotated examples becomes impossible. We therefore provide an automatic procedure to extract labeled examples for learning causality from dynamic content. In this work, we used the NYT archives for the years 1851 \u2212 2009, WikiNews, and the BBC \u2013 over 14 million articles in total (see data statistics in Table 1). Extracting causal relations between events in text is a hard task. The state-of-the-art precision of this task is around 37% (Do, Chan, & Roth, 2011). Our hypothesis is that most of the information regarding an event can be found in the headlines. These are more structured and therefore easier to analyze. Many times the headline itself can contain both the cause and effect event. We assume that only some of the headlines are describing events and developed an extraction algorithm to identify those headlines and to extract the events from them. News headlines are quite structured, and therefore the accuracy of this stage (performed on a representative subset of the data) is 78% (see Section 4.2.1). The system mines unstructured natural language text found in those headlines, and searches for causal grammatical patterns. We construct those patterns using causality connectors (Wolff, Song, & Driscoll, 2002; Levin & Hovav, 1994). In this work we used the following connectors:\n1. Causal Connectives: the words because, as, and after as the connectors.\n2. Causal prepositions: the words due to and because of.\n3. Periphrastic causative verbs: the words cause and lead to.\nWe constructed a set of rules for extracting a causality pair. Each rule is structured as: \u3008Pattern, Constraint, Priority\u3009, where Pattern is a regular expression containing a causality connector, Constraint is a syntactic constraint on the sentence on which the pattern can be applied, and Priority is the priority of the rule if several rules can be matched. The following constraints were composed:\n1. Causal Connectives: The pattern [sentence1] after [sentence2] was used with the following constraints: [sentence1] cannot start with \u201cwhen,\u201d \u201chow,\u201d \u201cwhere,\u201d [sentence2] cannot start with \u201call,\u201d \u201chours,\u201d \u201cminutes,\u201d \u201cyears,\u201d \u201clong,\u201d \u201cdecades.\u201d In the pattern \u201cAfter [sentence1], [sentence2]\u201d we add the constraint that [sentence1] cannot start with a number. This pattern can match the sentence \u201cafter Afghan vote, complaints of fraud surface\u201d but will not match the sentence \u201cafter 10 years in Lansing, state lawmaker Tom George returns\u201d. The pattern \u201c[sentence1] as [sentence2]\u201d was used with the constraint of [sentence2] having a verb. Using the constraint, the pattern can match the sentence \u201cNokia to cut jobs as it tries to catch up to rivals\u201d, but not the sentence \u201ccivil rights photographer unmasked as informer.\u201d\n2. Causal prepositions: The pattern [sentence1][\u201cbecause of,\u201d \u201cdue to\u201d] [sentence2] only required constraints that [sentence1] does not start with \u201cwhen,\u201d \u201chow,\u201d \u201cwhere.\u201d\n3. Periphrastic causative verbs: The pattern [sentence1] [\u201cleads to,\u201d \u201cLeads to,\u201d \u201clead to,\u201d \u201cLead to,\u201d \u201cled to,\u201d \u201cLed to\u201d] [sentence2] is used, where [sentence1] cannot con-\ntain \u201cwhen,\u201d \u201chow,\u201d \u201cwhere,\u201d and the prefix cannot be \u201cstudy\u201d or \u201cstudies.\u201d Additionally, as we consider periphrastic causative verbs, we do not allow additional verbs in [sentence1] or [sentence2].\nThe result of a rule application is a pair of sentences \u2013 one tagged as a cause, and one tagged as an effect.\nGiven a natural-language sentence (extracted from an article headline), representing an event (either during learning or prediction), the following procedure transforms it into a structured event:\n1. Root forms of inflected words are extracted using a morphological analyzer derived from WordNet (Miller, 1995) stemmer. For example, in the article headline from 10/02/2010: \u201cU.S. attacks kill 17 militants in Pakistan\u201d, the words \u201cattacks,\u201d \u201ckilled\u201d and \u201cmilitants\u201d are transformed to \u201cattack,\u201d \u201ckill,\u201d and \u201cmilitant\u201d respectively.\n2. Part-Of-Speech tagging (Marneffe, MacCartney, & Manning, 2006) is performed, and the verb is identified. The class of the verb is identified using the VerbNet vocabulary (Kipper, 2006), e.g., kill belongs to P =murder class.\n3. A syntactic template matching the verb is applied to extract the semantic relations and thus the roles of the words (see example in Figure 10). Those templates are based on VerbNet, which supplies for each verb class a set of syntactic templates. These templates match the syntax to the thematic roles of the entities in the sentence. We match the templates even if they are not continuous in the sentence tree. This allows the match of a sentence even where there is an auxiliary verb between the subject and the main transitive verb. In our example, the template is \u201cNP1 V NP2,\u201d which transforms NP1 to \u201cAgent\u201d, and NP2 to \u201cPatient.\u201d Therefore, we match U.S. attacks to be the Actor, and the militant to be the Patient . If no template can be matched, the sentence is transformed into a typed-dependency graph of grammatical relations (Marneffe et al., 2006). In the example, U.S. attacks is identified as the subject of the sentence (candidate for Actor), militants as the object (candidate for Patient), and Pakistan as the preposition (using Locations lexicons). Using this analysis, we identify that the Location is Pakistan.\n4. Each word in Oi is mapped to a Wikipedia-based concept. If a word matches more than one concept, we perform disambiguation by computing the cosine similarity between the body of the news article and the body of the Wikipedia article associated with the concept. For example, U.S was matched to several concepts, such as United States, University of Salford, and Us (Brother Ali album). The most similar by content was the Wikipedia concept United States. If a word in Oi is not found in Wikipedia, it is treated as a constant, i.e., generalization will not be applied on it, but it will be used during similarity calculation. That is, distGen(const1, const2) = 0 if const1 = const2, or distGen(const1, const2) = k otherwise. In our experiments, we set k = 4, as it was the length of the longest distance found between two concepts in GO.\n5. The time of the event t is the time of the publication of the article in the news, e.g., t =10/02/2010.\nIn our example, the final result is the event e = \u3008Murder-Class, United States of America, Militant, NULL, Pakistan, 10/02/2010\u3009 . The final result of this stage is a causality graph composed of causality event pairs. Those events are structured as described in Section 2.1. We illustrate such a pair in Figure 9.\nIn certain cases, additional heuristics were needed in order to deal with the brevity of news language. We used the following heuristics:\n1. Missing Context \u2013 In \u201cMcDonald\u2019s recalls glasses due to cadmium traces,\u201d the extracted event \u201ccadmium traces\u201d needs additional context \u2013 \u201cCadmium traces [in McDonald\u2019s glasses].\u201d If an object is missing, the first sentence ([sentence1]) subject is used.\n2. Missing entities and verbs \u2013 the text \u201c22 dead\u201d should be structured to the event \u201c22 [people] [are] dead.\u201d If a number appears as the subject, the word people is added and used as the subject, and \u201cbe\u201d is added as the verb.\n3. Anaphora resolution \u2013 the text \u201cboy hangs himself after he sees reports of Hussein\u2019s execution\u201d is modeled as \u201c[boy1] sees reports of Hussein\u2019s execution\u201d causes \u201c[boy1] hangs [boy1]\u201d (Lappin & Leass, 1994).\n4. Negation \u2013 the text \u201cMatsui is still playing despite his struggles\u201d should be modeled as: \u201c[Matsui] struggles\u201d causes the event \u201cMatsui is [not] playing\u201d. Modeling preventive connectors (e.g., despite) requires negation of the modeled event."}, {"heading": "4. Experimental Evaluation", "text": "In this section, we describe a set of experiments performed to evaluate the ability of our algorithms to predict causality. We first evaluate the predictive precision of our algorithm, continue with analyzing each part of the algorithm separately, and conclude with a qualitative evaluation."}, {"heading": "4.1 Prediction Evaluation", "text": "The prediction algorithm was trained using news articles from the period 1851\u2212 2009. The world knowledge used by the algorithm was based on Web resource snapshots (Section 3) dated until 2009. The evaluation was performed on separate data \u2013 Wikinews articles from the year 2010. We refer to this data as the test data.\nAs the task tackled by our algorithm has not been addressed before, we could not find any baseline algorithm to compare against. We therefore decided to compare our algorithm\u2019s performance to that of human predictors. Our algorithm and its human competitors were assigned the basic task of predicting what event a given event might cause. We evaluate each such prediction using two metrics. The first metric is accuracy : whether the predicted\nevent actually occurred in the real world. There are two possible problems with this metric. First, a predicted event, though plausible, still might not actually have occurred in the real world. Second, the predicted event might have happened in the real world but was not caused by the given event, for example, in trivial predictions that are always true (\u201cthe sun will rise\u201d). We therefore use an additional metric, event quality, the likelihood that the predicted event was caused by the given event.\nThe experiments were conducted as follows:\n1. Event identification \u2013 our algorithm assumes that the input to the predictor h is an event. To find news headlines that represent an event, we randomly sample n = 1500 headlines from the test data. For each headline, a human evaluator is requested to decide whether the headline is an event that can cause other events. We denote the set of headlines labeled as events as E. We again randomly sample k = 50 headlines from E. We denote this group as C.\n2. Algorithm event prediction \u2013 on each headline ci \u2208 C, Pundit performs event extraction, and produces an event Pundit(ci) with the highest score of being caused by the event represented by ci. Although the system provides a ranked list of results, to simplify the human evaluation of theses results, we consider only the highest score prediction. If there is a tie for the top score, we pick one at random. The results of this stage are the pairs: {(ci, Pundit(ci))|ci \u2208 C}.\n3. Human event prediction \u2013 For each event ci \u2208 C, a human evaluator is asked to predict what that event might cause. Each evaluator is instructed to read a given headline and predict its most likely outcome, using any online resource and with no time limit. The evaluators are presented with empty structured forms with the 5 fields for the output event they need to provide. The human result is denoted as human(ci). The results of this stage are the pairs: {(ci, human(ci))|ci \u2208 C}.\n4. Human evaluation of the results \u2013\n(a) Quality: We present m = 10 people with a triplet (ci, human(ci), Pundit(ci)). The human evaluators are asked to grade (ci, human(ci)) and (ci, Pundit(ci)) on a scale of 0-4 (0 is a highly implausible prediction and 4 is a highly plausible prediction). They were allowed to use any resource and were not limited by time. The human evaluators were different from those who performed the predictions.\n(b) Accuracy: For each predicted event, we checked the news (and other Web resources), up to a year after the time of the cause event, to see whether the predicted events were reported.\nHuman evaluation was conducted using Amazon Mechanical Turk, an emerging utility for performing user study evaluations, which was shown to be very precise for certain tasks (Kittur, Chi, & Suh, 2008). During the evaluation, tasks are created by routing a question to random users and obtaining their answers. We filtered the raters using a CAPTCHA. We restricted to only US-based users, as the events used by our system are extracted from the NYT. We did not perform any other manual filtering of the results. The average times for all human tasks are reported in table 2. We observed that the most time-consuming\ntask for humans was to verify that the event indeed happened in the past. The other timeconsuming task was Human Event Prediction. This is not surprising, as both cases required more use of external resources, whereas the quality evaluation only measured whether those events make sense. Additionally, we manually investigated the human evaluations in each category, and did not find correlation between the response time and quality of the human prediction. As we used Mechanical Turk, we do not know which external resources the evaluators used. We measured inter-rater reliability using Fleiss\u2019 kappa statistical test, where \u03ba measures the consistency of the ratings. For the raters in our test, we obtained \u03ba = 0.3, which indicates fair agreement (Landis & Koch, 1977; Viera & Garrett, 2005). This result is quite significant, for the following reasons:\n1. Conservativeness of this measure.\n2. Subjectivity of the predictions \u2013 asking people whether a prediction makes sense often leads to high variance in responses.\n3. Small dataset \u2013 the tests were performed with 10 people asking to categorize into 5 different scales of plausibility over 50 examples.\n4. Lack of formal guidelines for evaluating the plausibility of a prediction \u2013 no instructions were given to the human evaluators regarding what should be considered plausible and what is not.\nAdditionally, for comparison, similar tasks in natural language processing, such as sentence formality identification (Lahiri, Mitra, & Lu, 2011), usually reach kappa values of 0.1\u2212 0.2.\nThe quality evaluation yielded that Pundit\u2019s average predictive precision is 3.08/4 (3 is a \u201cplausible prediction\u201d), as compared to 2.86/4 for the humans. For each event, we average the results of the m rankers, producing an average score for the algorithm\u2019s performance on the event, and an averaged score for the human predictors (see Table 3). We performed a paired t-test on the k paired scores. The advantage of the algorithm over the human evaluators was found to be statistically significant, with p \u2264 0.05.\nThe accuracy results are reported in Table 4. We performed a Fisher\u2019s exact test (as the results are binary) on the k paired scores. The results were found to be statistically significant, p \u2264 0.05."}, {"heading": "4.2 Component Analysis", "text": "In this section, we report the results of our empirical analysis of the different parts of the algorithm."}, {"heading": "4.2.1 Evaluation of the Extraction Process", "text": "In Section 3.1, we described a process for extracting causality pairs from the news. These pairs are used as a training set for the learning algorithm. This process consists of two main parts: causality identification and event extraction. We perform a set of experiments to provide insights on this extracted training data quality.\nCausality Extraction Experiment The first step in building a training set consists of using causality patterns to extract pairs of sentences for which the causality relation holds. To assess the quality of this process, we randomly sampled 500 such pairs from the training set and presented them to human evaluators. Each pair was evaluated by 5 humans. We filtered the raters using a CAPTCHA and filtered out outliers. The evaluators were shown two sentences the system believed to be causally related and they were asked to evaluate the plausibility of this relation on a scale of 0-4.\nThe results show that the averaged precision of the extracted causality events is 3.11 out of 4 (78%), where 3 means a plausible causality relation, and 4 means a highly plausible causality relation. For example, the causality pair: \u201cpulling over a car\u201d \u2192 \u201c2 New Jersey police officers shot,\u201d got a very high causality precision score, as this is a plausible causeeffect relation, which the system extracted from the headline \u201c2 New Jersey Police Officers Shot After Pulling Over a Car.\u201d\nFor comparison, other temporal rule extraction systems (Chambers, Wang, & Jurafsky, 2007) reach precision of about 60%. The better performance of our system can be explained by our use of specially crafted templates (we did not attempt to solve the general problem of temporal information extraction).\nMost causality pairs extracted were judged to be of high quality. The main reason for errors was that some events, although reported in the news and matching the templates we have described, are not common-sense causality knowledge. For example, \u201cAborted landing in Beirut\u201d \u2192 \u201cHijackers fly airliner to Cyprus\u201d, was rated unlikely to be causally related, although the event took place on April 09, 1988.\nEvent Extraction Experiment After a pair of sentences is determined to have a casualty relation, our algorithm extracts a structured event from each of the sentences. This event includes the following roles: action, actor, object, instrument, and time.\nTo assess the the quality of this process, we used the 500 pairs from the previous experiment and presented each of the 1000 associated sentences to 5 human evaluators. The evaluators were shown a sentence together with its extracted roles: action, actor, object, instrument, and time, and they were asked to mark each role assignment as being right or wrong.\nTable 5 shows that the precision for the extracted event components ranges from 74 \u2212 100%. In comparison, other works (Chambers & Jurafsky, 2011) for extracting entities for different types of relations reach 42\u2212 53% precision. The higher precision of our results is mainly due to the use of domain-specific templates.\nWe performed additional experiments to evaluate the matching of every entity from the above experiment to the world-knowledge ontology. The matching was based on semantic similarity. Each ranker was asked to indicate whether the extracted entity was mapped correctly to a Wikipedia URI. The results are summarized in Table 6."}, {"heading": "4.2.2 Evaluation of the Event Similarity Algorithm", "text": "Both the learning and prediction algorithms strongly rely on the event similarity function dist described in Section 2.4. To evaluate the quality of this function, we randomly sampled 30 events from the training data and found for each the most similar event from the entire past data (according to the similarity function). A human evaluator was then asked to evaluate the similarity of these events on a scale of 1\u20135. We repeated the experiment, replacing the average aggregator function f with minimum and maximum functions.\nThe results are presented in Table 7. The general precision of the average function was high (3.9). Additionally, the average function performed substantially (confirmed by a t-test) better than over the minimum and maximum. This result indicates that distance functions that aggregate over several objects of the structured event (rather than just selecting the minimum or maximum of one of the events) yield the highest performance."}, {"heading": "4.2.3 The Importance of Abstraction", "text": "Given a cause event whose effect we wish to predict, we use the algorithm described in Section 2.4 to identify similar generalized events. To evaluate the importance of this stage, we compose an alternative matching algorithm, similar to the nearest-neighbor approach (as applied by the work by Gerber, Gordon, & Sagae, 2010), that matches the cause event to the cause events of the training data. Instead of building an abstraction tree, the algorithm simply finds the closest cause in the past based on text similarity. We then rank the matched results using TF-IDF measure.\nWe applied both our original algorithm and this baseline algorithm on the 50 events used for prediction. For each event, we asked a human evaluator to compare the prediction of the original and the baseline algorithm. The results showed that in 83% of the cases the predictions with generalization were rated as more plausible than those of the nearestneighbor approach without generalization."}, {"heading": "4.2.4 Analysis of Rule Generation Application", "text": "In order to generate an appropriate prediction with respect to the given cause event, a learned rule is applied, as described in Section 2.5. We observe that in 31% of the predictions, a non-trivial rule was generated and applied (that is, a non-NULL rule that does not simply output the effect it observed in the matched past cause-effect pair example). Out of those, the application predicted correctly in more than 90% of the cases and generated a plausible object in the effect. These results indicate that generalization and rule-generation techniques are essential to the performance of the algorithm."}, {"heading": "4.2.5 Analysis of Pruning Implausible Causation", "text": "To eliminate situations in which a generated prediction is implausible, we devised an algorithm (Section 2.7) that prevents implausible predictions. We randomly selected 200 predictions from the algorithm predictions based on the human-labeled events extracted from the Wikinews articles (see Section 4.1). A human rater was requested to label predictions that are considered implausible. We then applied our filtering rules on the 200 predictions as well. The algorithm found 15% of the predictions to be implausible with 70% precision and 90% recall with respect to the human label. A qualitative example of a filtered prediction is \u201cExplosion will surrender\u201d for the cause event \u201cExplosion in Afghanistan kills two.\u201d"}, {"heading": "4.3 Qualitative Analysis", "text": "For a better understanding of the algorithm\u2019s strengths and weaknesses we now present some examples of results. Given the event \u201cLouisiana flood,\u201d the algorithm predicted that [number] people will flee. The prediction process is illustrated in Figure 11.\n1. Raw data:\nThe above prediction was based on the following raw news articles:\n(a) 150000 flee as hurricane nears North Carolina coast.\n(b) A million flee as huge storm hits Texas coast.\n(c) Thousands flee as storm whips coast of Florida.\n(d) Thousands in Dallas Flee Flood as Severe Storms Move Southwest.\n2. Causality pair extraction:\nThe \u201cas\u201d template was used to process the above headlines into the following structured events:\n(a) Cause Event : near (Action); hurricane (Actor); Coast(Object); North Carolina (Object Attribute) ; (Instrument); Carolina (Location); 31 Aug 1993 (Time). Effect Event : flee (Action); People (Actor); 150000(Actor Attributes); Carolina (Location); 31 Aug 1993 (Time).\n(b) Cause Event : hit (Action); Storm (Actor); Huge (Actor Attributes); Coast(Object); Texas (Object Attribute); Texas (Location); 13 Sep 2008 (Time). Effect Event : flee (Action); People (Actor); million(Actor Attributes); Texas (Location); 13 Sep 2008 (Time).\n(c) Cause Event : whip (Action); Storm (Actor); Coast(Object); Florida (Object Attribute); Florida (Location); March 19, 1936 (Time). Effect Event : flee (Action); People (Actor); thousands(Actor Attributes); Florida (Location); March 19, 1936 (Time).\n(d) Cause Event : move (Action); Storm (Actor); Severe (Actor Attributes); Dallas (Location); May 27, 1957 (Time). Effect Event : flee (Action); People (Actor); thousands(Actor Attributes); Flood(Object); Dallas (Location); May 27, 1957 (Time).\n3. Learning the abstraction tree:\nThe above four events were clustered together in the AT. They were clustered in the same node because the causes were found to be similar: the actors were all weather hazards and the location was a state of the United States. The effects were found to be similar as the actions and actors were similar across all events, and the actor attributes were all numbers. For this generalization, the following world knowledge was used:\n(a) Storm, hurricane and flood are \u201cweather hazards\u201d (extracted from the in-category relation in Wikipedia).\n(b) Carolina, Texas, and California are located in the \u201cUnited States\u201d (extracted from the located-in relation in Yago).\n4. Prediction:\nDuring the prediction, the event \u201cLouisiana flood\u201d (which did not occur in the training examples) was found most similar to the above node, and the node rule output was that [number] people will flee.\nAs another example, given the event \u201c6.1 magnitude aftershock earthquake hits Haiti,\u201d the highest matching predictions were: \u201c[number] people will be dead,\u201d \u201c[number] people will be missing,\u201d \u201c[number] magnitude aftershock earthquake will strike island near Haiti\u201d and \u201cearthquake will turn to United States Virgin Islands.\u201d The first three predictions seem very reasonable. For example, the third prediction came from a rule that natural disasters hitting countries next to a shore tend to affect nearby countries. In our case it predicted that the earthquake will affect the United States Virgin Islands, which are geographically close to Haiti. The fourth prediction, however, is not very realistic as an earthquake cannot change its course. It was created from a match with a past example of a tornado hitting a country on a coast. The implausible causation filters this prediction, as it has very low PMCI, and the output of the system is \u201c[number] people will be dead\u201d. This example is also interesting, as it issues a prediction using spatial locality (the United States Virgin Islands are [near] Haiti).\nAdditional examples out of the 50 in the test and their predictions can be seen in Table 8."}, {"heading": "4.4 Discussion", "text": "In our experiments we only report the precision of our algorithms. Further experiments measuring the recall of the system are necessary. However, in our experiments each validation step required human intervention. For example, validating that a prediction occurred in the future news. In order to perform a full recall experiment one should apply the algorithm on all the news headlines reported on a certain day and measure the appearance of all the\ncorresponding predictions in the future news. Unfortunately, performing human validation on such a large prediction space is hard. We leave the task of performing experiments to provide a rough estimate of recall to future work.\nIt is common practice to compare system performance to previous systems tackling the same problem. However, the ambitious task we tackled in this work had no immediate baselines to compare with. That is, there was no comparable system neither in scale nor in the ability to take an arbitrary cause event in natural language and output an effect event in natural language. Instead, we compared to the only agents we know capable of performing such a task \u2013 humans.\nAlthough the results indicate the superiority of the system over such human agents, we do no claim that the system predictions perform better than humans. We rather provide evidence that the system provides similar predictions to that of humans, and sometimes even outperforms human ability to predict, as can be supported by the superiority of the system in the accuracy evaluation.\nTo fully support the claim of superiority of the system over humans, wider experiments should be performed. Experiments larger by an order of magnitude can provide results with higher agreement between raters and shed light on the different types of events where the system\u2019s performance is better. Additionally, more experiments comparing the system performance to that of experts in the fields of each individual prediction can be valuable as well. At this point, we assume the performance of experts would be higher than that of our algorithm. The main reason for this is the causality knowledge used to train the algorithms. This knowledge is extracted from headlines that tend to have simple causality contents, which is easily understandable by the general population. This type of knowledge limits the complexity of the predictions that can be made by Pundit. Pundit predictions therefore that tend to be closer to common knowledge of the average human. In order to predict more complex events we would need to rely on better training examples than news headlines alone.\nThe evaluation presented in this section provides evidence of the quality of the predictions that the system can provide. Our results are impressive in the sense that they are comparable to that of humans, thus providing evidence to the ability of a machine to perform one of the most desirable goals of general AI."}, {"heading": "5. Related Work", "text": "We are not aware of any work that attempts to perform the task we face: receive arbitrary news events in natural language representation and predict events they can cause. Several works, however, deal with related tasks. In general, our work does not focus on better information extraction or causality extraction techniques, but rather on how this information can be leveraged for prediction. We present novel methods of combining world knowledge with event extraction methods to represent coherent events, and present novel methods for rule extraction and generalization using this knowledge."}, {"heading": "5.1 Prediction from Web Behavior, Books and Social Media", "text": "Several works have focused on using search-engine queries for prediction in both traditional media (Radinsky, Davidovich, & Markovitch, 2008) and blogs (Adar, Weld, Bershad, &\nGribble, 2007). Ginsberg et al. (2009) used queries for predicting H1N1 influenza outbreaks. In the context of causality recognition, Gordon, Bejan, and Sagae (2011) present a methodology for mining blogs to extract common-sense causality. The evaluation is done on a human-labeled dataset where each test consists of a fact and two possible effects. Applying point-mutual information to personal blog stories, the authors select the best prediction candidate. The work differs from ours in that the authors focus on personal commonsense mining and do not consider whether their predictions actually occurred. Other works focused on predicting Web content change itself. For example, Kleinberg (2002, 2006) developed general techniques for summarizing the temporal dynamics of textual content and for identifying bursts of terms within content. Similarly, Amodeo, Blanco, and Brefeld (2011) built a time series model over publication dates of documents relevant to a query in order to predict future bursts. Social media were used to predict riots (Kalev, 2011) and movie box office sales (Asur & Huberman, 2010; Joshi, Das, Gimpel, & Smith, 2010; Mishne, 2006). Other works (Jatowt & Yeung, 2011; Yeung & Jatowt, 2011; Michel, Shen, Aiden, Veres, Gray, Google Books Team, Pickett, Hoiberg, Clancy, Norvig, Orwant, Pinker, Nowak, & Aiden, 2011) have explored the use of text mining techniques over news and books to explain how culture develops, and what people\u2019s expectations and memories are.\nOur work differs from the above in several ways: First, we present a general-purpose prediction algorithm rather than a domain-specific one. Second, unlike the above works, ours combines a variety of heterogenous online sources, including world knowledge mined from the Web. Finally, we focus on generation of future event predictions represented entirely in natural language, and provide techniques to enrich and generalize historical events for the purpose of future event prediction."}, {"heading": "5.2 Textual Entailment", "text": "A related topic to our work is that of textual entailment (TE) (Glickman, Dagan, & Koppel, 2005). A text t is said to entail a textual hypothesis h if people reading it agree that the meaning of t implies the truth of h. TE can be divided into three main categories: recognition, generation, and extraction. In this section, we provide a short summary of the first two categories. For a more detailed overview we refer the reader to the survey by Androutsopoulos and Malakasiotis (2010). We then discuss the specific task of causality extraction from text in Section 5.3.4."}, {"heading": "5.2.1 Textual Entailment recognition", "text": "In this task, pairs of texts are given as input, and the output is whether TE relations hold for the pair. Some approaches map the text to logical expressions (with some semantic enrichment, using WordNet, for example) and perform logical entailment checks, usually using theorem provers (Raina, Ng, & Manning, 2005; Bos & Markert, 2005; Tatu & Moldovan, 2005). Other approaches map the two texts to a vector space model, where each word is mapped to strongly co-occurring words in the corpus (Mitchell & Lapata, 2008), and then similarity measures over those vectors are applied. Some measure syntactic similarity by applying graph similarity measure on the syntactic dependency graphs of the two texts (Zanzotto, Pennacchiotti, & Moschitti, 2009). Similarly, other methods measure the semantic distance similarity between the words in text (Haghighi, 2005), usually exploiting\nother resources such as WordNet as well. The last set of approaches represents the two texts in a single feature vector and trains a machine learning algorithm, which later, given two new texts represented via a vector, can determine whether they entail each other (Bos & Markert, 2005; Burchardt, Pennacchiotti, Thater, & Pinkal, 2009; Hickl, 2008). For example, Glickman et al. (2005) show a naive Bayes classifier trained on lexical features, i.e., the number of times that words of t appeared with words of h. Other features usually include polarity (Haghighi, 2005), whether the theorem prover managed to prove entailment (Bos & Markert, 2005), or tagging of the named entities to the categories people, organizations, or locations."}, {"heading": "5.2.2 Textual Entailment Generation", "text": "Here we discuss TE generation, where, given an expression, the system should output a set of expressions that are entailed by the input. This task is most closely related to the one presented in this work: in TE generation, a text is received and an entailed text is generated as output. Androutsopoulos and Malakasiotis (2010) mention that no benchmarks exist to evaluate this task, and the most common and costly approach is to evaluate using human judges. We also encountered this difficulty in our own task, and performed human evaluation.\nTE generation methods can be divided into two types: those that use machine translation techniques and those that use template-based techniques. Those that use machine translation techniques try to calculate the set of transformations with the highest probability, using a training corpus. Quirk, Brockett, and Dolan (2004) cluster news articles referring to the same event, select pairs of similar sentences, and apply the aforementioned techniques. Other methods use template-based approaches on large corpora, such as the Web. Some methods (Idan, Tanev, & Dagan, 2004) start with an initial seed of sentences (composed of entities), and use a search engine to find other entities for which these entailment relations hold. Those relations are used as templates. To find additional entities for which these relations hold, the relations themselves are then searched again. The TE generation system, given a text, matches it to a template and outputs all the texts that matched this template. Others (Ravichandran, Ittycheriah, & Roukos, 2003) also add additional filtering techniques on those templates.\nOur work is most closely related to the template-based approach. We have crafted a new set of templates to extract causality pairs from the news."}, {"heading": "5.3 Information Extraction", "text": "Information Extraction is the study of automatic extraction of information from unstructured sources. We categorizes the types of information extracted into three types: entities, relationships between entities, and higher-order structures such as tables and lists. The most closely related tasks to ours are those of entity extraction and relation extraction; for the rest we refer the reader to the survey by Sarawagi (2008). The former task, similar to our process of extracting concepts, deals with extracting noun phrases from text. In the latter task, given a document and a relation as input, the problem is to extract all entity pairs in the document for which this relation holds. Whereas the above works deal only with one element of our problem \u2013 extraction of information needed to understand a given\ncausality, we deal with the actual causality prediction. We do not claim to create more precise information extraction methods, but rather try to leverage all this knowledge to perform an important AI task \u2013 future event prediction."}, {"heading": "5.3.1 Entity Extraction", "text": "For entity extraction, two categories of methods exist \u2013 rule-based and statistical methods. Rule-based methods (Riloff, 1993; Riloff & Jones, 1999; Jayram, Krishnamurthy, Raghavan, Vaithyanathan, & Zhu, 2006; Shen, Doan, Naughton, & Ramakrishnan, 2007; Ciravegna, 2001; Maynard, Tablan, Ursu, Cunningham, & Wilks, 2001; Hobbs, Bear, Israel, & Tyson, 1993) define contextual patterns consisting a regular expression over features of the entities in the text (e.g., the entity word, part-of-speech tagging). Those rules are either manually coded by a domain expert or learned using bottom-up (Ciravegna, 2001; Califf & Mooney, 1999) or top-down learners (Soderland, 1999). Others follow statistical methods that define numerous features over the sentence and then treat the problem as a classification problem, applying well-known machine learning algorithms (e.g., Hidden Markov Models; Agichtein & Ganti, 2004; Borkar, Deshmukh, & Sarawagi, 2001). Our system does not deal with the many challenges in this field, as we propose a large scale domain-specific approach driven by specific extraction templates."}, {"heading": "5.3.2 Relation Extraction", "text": "Relation extraction has been developed widely in the last years from large text corpora (Schubert, 2002) and, in particular, from different Web resources, such as general Web content (Banko et al., 2007; Carlson et al., 2010; Hoffmann, Zhang, & Weld, 2010), blogs (Jayram et al., 2006), Wikipedia (Suchanek et al., 2007), and news articles (e.g., the topic detection and tracking task (Section 5.3.3)). Given two entities, the first task in this domain is to classify their relationship. Many feature-based methods (Jiang & Zhai, 2007; Kambhatla, 2004; Suchanek, 2006) and rule-based methods (Aitken, 2002; Mcdonald, Chen, Su, & Marshall, 2004; Jayram et al., 2006; Shen et al., 2007) have been developed for this task. Most methods use different features extracted from the text, such as the words, the grammar features, such as parse tree and dependency graphs, and features extra ion from external relation repositories (e.g., Wikipedia Infobox) to add additional features (Nguyen & Moschitti, 2011; Hoffmann, Zhang, Ling, Zettlemoyer, & Weld, 2011). Labeled training examples, from which those feature are extracted, are then fed into a machine learning classifier, sometimes using transformations such as kernels (Zhao & Grishman, 2005; Zhang, Zhang, Su, & Zhou, 2006; Zelenko, Aone, & Richardella, 2003; Wang, 2008; Culotta & Sorensen, 2004; Bunescu & Mooney, 2005; Nguyen, Moschitti, & Riccardi, 2009), which, given new unseen entities, will be able to classify them into those categories.\nGiven a relation, the second common task in this domain is to find entities that satisfy this relation. Out of all information extraction tasks, this task is most relevant to ours, as we try to find structured events for which the causality relation holds. Most works in this domain focus on large collections, such as the Web, where labeling all entities and relations is infeasible (Agichtein & Gravano, 2000; Banko et al., 2007; Bunescu & Mooney, 2007; Rosenfeld & Feldman, 2007; Shinyama & Sekine, 2006; Turney, 2006). Usually seed entity databases are used, along with some manual extraction templates, and then expanded and\nfiltered iteratively. Sarawagi states that \u201cin spite of the extensive research on the topic, relationship extraction is by no means a solved problem. The accuracy values still range in the neighborhood of 50%\u201370% even in closed benchmark datasets . . . In open domains like the Web, the state-of-the-art systems still involve a lot of special case handling that cannot easily be described as principled, portable approaches.\u201d (Sarawagi, 2008, p. 331). Similarly, in our task the size of our corpus does not allow us to assume any labeled sets. Instead, like the common approaches presented here, we also start with a predefined set of patterns."}, {"heading": "5.3.3 Temporal Information Extraction", "text": "The temporal information extraction task deals with extraction and ordering of events from many events over time. Temporal information extraction can be categorized into three main subtasks \u2013 predicting the temporal order of events or time expressions described in text, predicting the relation between those events, and identifying when the document was written. This task has been found to be important in many natural language processing applications, such as question answering, information extraction, machine translation and text summarization, all of which require more than mere surface understanding. Most of these approaches (Ling & Weld, 2010; Mani, Schiffman, & Zhang, 2003; Lapata & Lascarides, 2006; Chambers et al., 2007; Tatu & Srikanth, 2008; Yoshikawa, Riedel, Asahara, & Matsumoto, 2009) learn classifiers that predict a temporal order of a pair of events from predefined features of the pair.\nOther related works deal with topic detection and tracking (Cieri, Graff, Liberman, Martey, & Strassel, 2000). This area includes several tasks (Allan, 2002). In all of them, multiple, heterogenous new sources are used, including audio. The story segmentation task aims to segment data into its constituent stories. The topic tracking task \u2013 e.g., the work by Shahaf and Guestrin (2010) \u2013 aims to find all stories discussing a certain topic. A subtask of this is the link detection task which, given a pair of stories, aims to classify whether they are on the same topic. The topic detection task \u2013 e.g. the works by Ahmed, Ho, Eisenstein, Xing, Smola, and Teo (2011) and Yang, Pierce, and Carbonell (1998) \u2013 aims to detect clusters of topic-cohesive stories in a stream of topics. The first-story detection task aims to identify the first story on a topic (Jian Zhang & Yang, 2004). In this paper, we focused on short text headlines and the extraction of events from them. Our work differs from that of temporal information extraction, in that we generate predictions of future events, whereas temporal information extraction tasks focus on identifying and clustering the text corpus into topics."}, {"heading": "5.3.4 Causality Pattern Extraction and Recognition", "text": "In the first stage of our learning process we extract causality pairs from text. Causality extraction has been discussed in the literature in the past, and can be divided into the following subgroups:\n1. Use of handcrafted domain-specific patterns. Some studies deal with causality extraction using specific domain knowledge. Kaplan and Berry-Rogghe (1991) used scientific texts to create a manually designed set of propositions which were later applied on\nnew texts to extract causality. These methods require handcrafted domain knowledge, which is problematic to obtain for real-world tasks, especially in large amounts.\n2. Use of handcrafted linguistic patterns. These works use a more general approach by applying linguistic patterns. For example, Garcia (1997) manually identified 23 causative verb groups (e.g., to result in, to lead to, etc.). If a sentence contained one of those verbs, it was classified as containing a causation relation. A precision of 85% was reported. Khoo et al. (2000) used manually extracted graphical patterns based on syntactic parse trees, reporting accuracy of about 68% on an English medical database. Similarly, Girju and Moldovan (2002) defined lexicon-syntactic patterns (pairs of noun phrases with a causative verb in between) with additional semantic constraints.\n3. Semi-supervised pattern learning approaches. This set of approaches uses supervised machine learning techniques to identify causality in text. For example, Blanco et al. (2008) and Sil et al. (2010) use syntactic patterns as features that are later fed into classifiers, whose output is whether the text implies causality or the cause and effect themselves.\n4. Supervised pattern learning approaches. There have been many works on design inference rules to discover extraction patterns for a given relation using training examples (Riloff, 1996; Riloff & Jones, 1999; Agichtein & Gravano, 2000; Lin & Pantel, 2001). Specifically, Chan and Lam (2005) dealt with the problem of creating syntactic patterns for cause-effect extraction.\nIn the domain of causality pattern extraction, our work most resembles the handcrafted linguistic patterns pattern approaches. We evaluated their performance on our specific domain. Since our goal was to obtain a very precise set of examples to feed into our learning, we chose to follow such an approach as well."}, {"heading": "5.4 Learning Causality", "text": "We have drawn some of our algorithmic motivation from work in the machine learning community. In this section, we give a partial review of the main areas of machine learning that are relevant to our work."}, {"heading": "5.4.1 Bayesian Causal Inference", "text": "The functional causal model (Pearl, 2000) assumes a set of observables X1 . . . Xn, which are the vertices of a directed acyclic graph G. The semantics of the graph is that parents of a node are its directed causes. It was shown to satisfy Reichenbach\u2019s common cause principle, which states that for a node Z with children X, Y , if X and Y are statistically dependent, then there is a Z causally influencing both. This model, similar to a Bayesian network, satisfies several conditions: (1) Local Causal Markov condition: a node is statistically independent of non-descendants, given its parents; (2) Global Causal Markov condition: dseparation criterion; (3) Factorization criterion: P (X1, . . . , Xn) = \u220f i P (Xi|Parents(Xi)). The theoretical literature on the inference and learning of causality models is extensive. Those models resemble our work in the use of structural models. The literature on inference\nand learning of causality models is extensive, but to our knowledge there are no solutions that scale to the scope of tasks discussed in this paper. In contrast with Bayesian approach, the causality graph in our work contains less detailed information. Our work combines several linguistic resources that were learned from data with several heuristics to build the causality graph."}, {"heading": "5.4.2 Structured Learning", "text": "An important problem in the machine learning field is structured learning, where the input or the output of the classifier is a complex structure, such as relational domain, where each object is related to another, either in time or in its features. Our task resembles structured learning in that we also use structured input (structured events given as input) and produce a structured event as output.\nMany generative models have been developed, including hidden Markov models, Markov logic networks, and conditional random fields, among others. Other approaches use transformations, or kernels, that unite all the objects, ignoring the structure, and then feed it into a standard structured classifier, e.g., kernelized conditional random fields (Lafferty, Zhu, & Liu, 2004), maximum margin Markov networks (Taskar, Guestrin, & Koller, 2003), and others (Bakir, Hofmann, Scho\u0308lkopf, Smola, Taskar, & Vishwanathan, 2007). When dealing with complex output, such as annotated parse trees for natural language problems, most approaches define a distance metric in the label space between the objects, and they again apply standard machine learning algorithms, e.g., structured support vector machines (Joachims, 2006)."}, {"heading": "5.4.3 Learning from Positive Examples (One Class Classification)", "text": "As our system is only fed examples of the sort \u201ca causes b,\u201d and no examples of the sort \u201ca does not cause b,\u201d we must deal with the problem of learning from positive examples only. This is a challenge for most multi-class learning mechanisms, which require both negative and positive examples. Some theoretical studies of the possibility of learning from only positive unlabeled data are provided in the work by Denis (1998) (probably approximately correct (PAC) learning) and Muggleton (1996) (Bayesian learning).\nMost works (Tax, 2001; Manevitz & Yousef, 2000; Manevitz, Yousef, Cristianini, ShaweTaylor, & Williamson, 2001) in this domain develop algorithms that use one-class SVM (Vapnik, 1995) and learn the support using only positive distribution. They construct decision boundaries around the positive examples to differentiate them from all possible negative data. Tax and Duin (1991) use a hyper-sphere with some defined radius around some of the positive class points (support vector data description method). Some also use kernel tricks before finding this sphere (Tax, 2001). Scho\u0308lkopf et al. (1999, 2000) develop methods that try to separate the surface region of the positive labeled data from the region of the unlabeled data."}, {"heading": "6. Conclusions", "text": "Much research has been carried out on information extraction and ontology building. In this work, we discuss how to leverage such knowledge into a large-scale AI problem of event\nprediction. We present a system that is trained to predict future events, using a cause event as input. Each event is represented as a tuple of one predicate and 4 general semantic roles. The event pairs used for training are extracted automatically from news headlines using simple syntactic patterns. Generalization to unseen events is achieved by:\n1. Creating an abstraction tree (AT) that contains entities from observed events together with their subsuming categories extracted from available online ontologies.\n2. Finding predicate paths connecting entities from cause events to entities in the effect events, where the paths are again extracted from available ontologies.\nWe discuss the many challenges of building such a system: obtaining a large enough dataset, representing the knowledge, and developing the inference algorithms required for such a task. We perform large-scale mining and apply natural language techniques to transform the raw data of over 150 years of history archives into a structured representation of events, using a mined Web-based object hierarchy and action classes. This shows the scalability of the proposed method, which is crucial to any method that requires large amounts of data to work well. However, more engineering design and analysis should be performed to scale it to the entire knowledge of the web and provide real-time alerts. We also show that the numerous resources built by different people for different purposes (e.g., the different ontologies) can in fact be merged via a concept-graph to build a system that can work well in practice.\nWe perform large-scale learning over the large data corpus and present novel inference techniques. We consider both rule extraction and generalization. We propose novel methods for rule generalization using existing ontologies, which we believe can be useful for many other related tasks. Tasks such as entailment and topic tracking can benefit from the concepts of understanding sequences and their generalizations.\nIn this work we only scratch the surface of what can be a real-time fully functional prediction system. Due to the complexity of the problem, the size of the system and it many components, errors are unavoidable. For example, errors due to noise during event extraction, noise during the similarity calculation between events, etc. Although we perform experiments analyzing the different components of the system and their errors in addition to the overall system performance, we believe that additional training examples and better sources of knowledge and deeper ontologies can bring many improvements to our algorithms. For future work, we suggest the following directions and extensions:\n1. Better event extraction and event matching \u2013 Event extraction techniques, e.g., as proposed by Do et al. (2011) can provide higher analysis of the data from the entire text rather than just the titles. Event similarity can be enriched in many ways, e.g., in this work we compared three aggregation functions f , however, a more coherent way of learning the weights of Oi from past data can be applied.\n2. Analysis of knowledge sources \u2013 We believe that more in-depth analysis of the different types of knowledge obtained from the Web and their individual contributions should be studied. In this work, we did not explore the sensitivity of the system to the initial noise of the conceptual networks, and we believe that proper analysis of those and better networks can provide higher prediction accuracy, as already being carried on by the LinkedData community.\n3. Large scale experiments \u2013 Performance of larger experiments with humans over larger periods of times, and even comparison to experts can provide more insights on the performance and reliability of the system. Automation of such experiments without human involvement to measure accuracy of predictions will make it possible to provide richer metrics of performance, such as recall.\n4. Time effect \u2013 In this work, all events were treated similarly, even events from 100 years ago. For future directions, we wish to investigate how to give decaying weight to information about events in the system, as causality learned from an event that took place in 1851 might be less relevant to the prediction in 2010. However, much common-sense knowledge can still be used even if learned from events that happened a long time ago. For example, the headlines \u201cOrder Restored After Riots\u201d (1941) and \u201cGames Suspended After Riot\u201d (1962) are still relevant today.\nOur experimental evaluation showed that the predictions of the Pundit algorithm are at least as good as those of non-expert humans. We believe that our work is one of the first to harness the vast amount of information available on the Web to perform event prediction that is general purpose, knowledge based, and human-like."}], "references": [{"title": "Why we search: visualizing and predicting user behavior", "author": ["E. Adar", "D.S. Weld", "B.N. Bershad", "S.D. Gribble"], "venue": "In Proceedings of the International Conference on the World Wide Web (WWW)", "citeRegEx": "Adar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Adar et al\\.", "year": 2007}, {"title": "Mining reference tables for automatic text segmentation", "author": ["E. Agichtein", "V. Ganti"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)", "citeRegEx": "Agichtein and Ganti,? \\Q2004\\E", "shortCiteRegEx": "Agichtein and Ganti", "year": 2004}, {"title": "Snowball: extracting relations from large plain-text collections", "author": ["E. Agichtein", "L. Gravano"], "venue": "In Proceedings of Joint Conference on Digital Libraries (JCDL),", "citeRegEx": "Agichtein and Gravano,? \\Q2000\\E", "shortCiteRegEx": "Agichtein and Gravano", "year": 2000}, {"title": "Unified analysis of streaming news", "author": ["A. Ahmed", "Q. Ho", "J. Eisenstein", "E.P. Xing", "A.J. Smola", "C.H. Teo"], "venue": "In Proceedings of the International Conference on the World Wide Web (WWW)", "citeRegEx": "Ahmed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2011}, {"title": "Learning information extraction rules: An inductive logic programming approach", "author": ["J. Aitken"], "venue": "Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 355\u2013359.", "citeRegEx": "Aitken,? 2002", "shortCiteRegEx": "Aitken", "year": 2002}, {"title": "Topic Detection and Tracking: Event-based Information Organization, Vol. 12", "author": ["J. Allan"], "venue": null, "citeRegEx": "Allan,? \\Q2002\\E", "shortCiteRegEx": "Allan", "year": 2002}, {"title": "Hybrid models for future event prediction", "author": ["G. Amodeo", "R. Blanco", "U. Brefeld"], "venue": "In Proceedings of the ACM Conference on Information and Knowledge Management (CIKM)", "citeRegEx": "Amodeo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Amodeo et al\\.", "year": 2011}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["I. Androutsopoulos", "P. Malakasiotis"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Androutsopoulos and Malakasiotis,? \\Q2010\\E", "shortCiteRegEx": "Androutsopoulos and Malakasiotis", "year": 2010}, {"title": "Predicting the future with social media", "author": ["S. Asur", "B.A. Huberman"], "venue": "In ArxiV", "citeRegEx": "Asur and Huberman,? \\Q2010\\E", "shortCiteRegEx": "Asur and Huberman", "year": 2010}, {"title": "Predicting Structured Data", "author": ["G.H. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan"], "venue": null, "citeRegEx": "Bakir et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bakir et al\\.", "year": 2007}, {"title": "Open information extraction from the web", "author": ["M. Banko", "M.J. Cafarella", "S. Soderl", "M. Broadhead", "O. Etzioni"], "venue": "In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI)", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Linked data \u2013 the story so far. International Journal on Semantic Web and Information Systems (IJSWIS)", "author": ["C. Bizer", "T. Heath", "T. Berners-Lee"], "venue": null, "citeRegEx": "Bizer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bizer et al\\.", "year": 2009}, {"title": "The berlin sparql benchmark", "author": ["C. Bizer", "A. Schultz"], "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)", "citeRegEx": "Bizer and Schultz,? \\Q2009\\E", "shortCiteRegEx": "Bizer and Schultz", "year": 2009}, {"title": "Causal Relation Extraction", "author": ["E. Blanco", "N. Castell", "D. Moldovan"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "Blanco et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blanco et al\\.", "year": 2008}, {"title": "Automatic text segmentation for extracting structured records", "author": ["V. Borkar", "K. Deshmukh", "S. Sarawagi"], "venue": "In Proceedings of ACM SIGMOD International Conference on Management of Data (KDD)", "citeRegEx": "Borkar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Borkar et al\\.", "year": 2001}, {"title": "Recognising textual entailment with logical inference", "author": ["J. Bos", "K. Markert"], "venue": "In Proceedings of the Human Language Technology Conference Conference on Empirical Methods in Natural Language Processing (HLT EMNLP)", "citeRegEx": "Bos and Markert,? \\Q2005\\E", "shortCiteRegEx": "Bos and Markert", "year": 2005}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["R. Bunescu", "R. Mooney"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Bunescu and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Bunescu and Mooney", "year": 2007}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT EMNLP),", "citeRegEx": "Bunescu and Mooney,? \\Q2005\\E", "shortCiteRegEx": "Bunescu and Mooney", "year": 2005}, {"title": "Assessing the impact of frame semantics on textual entailment", "author": ["A. Burchardt", "M. Pennacchiotti", "S. Thater", "M. Pinkal"], "venue": "Natural Language Engineering,", "citeRegEx": "Burchardt et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Burchardt et al\\.", "year": 2009}, {"title": "Relational learning of pattern-match rules for information extraction", "author": ["M.E. Califf", "R.J. Mooney"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Califf and Mooney,? \\Q1999\\E", "shortCiteRegEx": "Califf and Mooney", "year": 1999}, {"title": "Toward an architecture for never-ending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E. Hruschka", "T. Mitchell"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Template-Based Information Extraction without the Templates", "author": ["N. Chambers", "D. Jurafsky"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Chambers and Jurafsky,? \\Q2011\\E", "shortCiteRegEx": "Chambers and Jurafsky", "year": 2011}, {"title": "Classifying temporal relations between events. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL) (Poster)", "author": ["N. Chambers", "S. Wang", "D. Jurafsky"], "venue": null, "citeRegEx": "Chambers et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2007}, {"title": "Extracting causation knowledge from natural language texts", "author": ["K. Chan", "W. Lam"], "venue": "International Journal of Information Security (IJIS),", "citeRegEx": "Chan and Lam,? \\Q2005\\E", "shortCiteRegEx": "Chan and Lam", "year": 2005}, {"title": "Large, multilingual, broadcast news corpora for cooperative research in topic detection and tracking: The tdt-2 and tdt-3 corpus efforts", "author": ["C. Cieri", "D. Graff", "M. Liberman", "N. Martey", "S. Strassel"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "Cieri et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cieri et al\\.", "year": 2000}, {"title": "Adaptive information extraction from text by rule induction and generalisation", "author": ["F. Ciravegna"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI).", "citeRegEx": "Ciravegna,? 2001", "shortCiteRegEx": "Ciravegna", "year": 2001}, {"title": "Dependency tree kernels for relation extraction", "author": ["A. Culotta", "J. Sorensen"], "venue": "In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Culotta and Sorensen,? \\Q2004\\E", "shortCiteRegEx": "Culotta and Sorensen", "year": 2004}, {"title": "Investigating regular sense extensions based on intersective levin classes", "author": ["H.T. Dang", "M. Palmer", "J. Rosenzweig"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING)", "citeRegEx": "Dang et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dang et al\\.", "year": 1998}, {"title": "PAC learning from positive statistical queries", "author": ["F. Denis"], "venue": "Proceedings of the International Conference on Algorithmic Learning Theory (ALT), pp. 112\u2013126.", "citeRegEx": "Denis,? 1998", "shortCiteRegEx": "Denis", "year": 1998}, {"title": "Minimally supervised event causality identification", "author": ["Q. Do", "Y. Chan", "D. Roth"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP)", "citeRegEx": "Do et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Do et al\\.", "year": 2011}, {"title": "Cluster analysis and display of genome-wide expression", "author": ["M.B. Eisen", "P.T. Spellman", "P.O. Brown", "D. Botstein"], "venue": "patterns. PNAS,", "citeRegEx": "Eisen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Eisen et al\\.", "year": 1998}, {"title": "Coatis, an NLP system to locate expressions of actions connected by causality links", "author": ["D. Garcia"], "venue": "Proceedings of Knowledge Engineering and Knowledge Management by the Masses (EKAW).", "citeRegEx": "Garcia,? 1997", "shortCiteRegEx": "Garcia", "year": 1997}, {"title": "Open-domain commonsense reasoning using discourse relations from a corpus of weblog stories", "author": ["M. Gerber", "A.S. Gordon", "K. Sagae"], "venue": "In Proceedings of Formalisms and Methodology", "citeRegEx": "Gerber et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gerber et al\\.", "year": 2010}, {"title": "Detecting influenza epidemics using search engine query data", "author": ["J. Ginsberg", "M.H. Mohebbi", "R.S. Patel", "L. Brammer", "M.S. Smolinski", "L. Brilliant"], "venue": null, "citeRegEx": "Ginsberg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ginsberg et al\\.", "year": 2009}, {"title": "Text mining for causal relations", "author": ["R. Girju", "D. Moldovan"], "venue": "In Proceedings of the Annual International Conference of the Florida Artificial Intelligence Research Society (FLAIRS),", "citeRegEx": "Girju and Moldovan,? \\Q2002\\E", "shortCiteRegEx": "Girju and Moldovan", "year": 2002}, {"title": "Shallow semantic parsing based on framenet, verbnet and propbank", "author": ["Giuglea", "A.-M", "A. Moschitti"], "venue": "In Proceedings of the the 17th European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Giuglea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Giuglea et al\\.", "year": 2006}, {"title": "A probabilistic classification approach for lexical textual entailment", "author": ["O. Glickman", "I. Dagan", "M. Koppel"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "Glickman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Glickman et al\\.", "year": 2005}, {"title": "Commonsense causal reasoning using millions of personal stories", "author": ["A.S. Gordon", "C.A. Bejan", "K. Sagae"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "Gordon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 2011}, {"title": "Robust textual inference via graph matching", "author": ["A.D. Haghighi"], "venue": "Proceedings of the Human Language Technology Conference Conference on Empirical Methods in Natural Language Processing (HLT EMNLP).", "citeRegEx": "Haghighi,? 2005", "shortCiteRegEx": "Haghighi", "year": 2005}, {"title": "Using discourse commitments to recognize textual entailment", "author": ["A. Hickl"], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING).", "citeRegEx": "Hickl,? 2008", "shortCiteRegEx": "Hickl", "year": 2008}, {"title": "Fastus: A finite-state processor for information extraction from real-world text", "author": ["J.R. Hobbs", "J. Bear", "D. Israel", "M. Tyson"], "venue": "In Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Hobbs et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hobbs et al\\.", "year": 1993}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT)", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": null, "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Learning 5000 relational extractors", "author": ["R. Hoffmann", "C. Zhang", "D.S. Weld"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Hoffmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2010}, {"title": "Scaling web-based acquisition of entailment relations", "author": ["I.S. Idan", "H. Tanev", "I. Dagan"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "Idan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Idan et al\\.", "year": 2004}, {"title": "Extracting collective expectations about the future from large text collections", "author": ["A. Jatowt", "C. Yeung"], "venue": "In Proceedings of the ACM Conference on Information and Knowledge Management (CIKM)", "citeRegEx": "Jatowt and Yeung,? \\Q2011\\E", "shortCiteRegEx": "Jatowt and Yeung", "year": 2011}, {"title": "Avatar information extraction system", "author": ["T.S. Jayram", "R. Krishnamurthy", "S. Raghavan", "S. Vaithyanathan", "H. Zhu"], "venue": "IEEE Data Engineering Bulletin,", "citeRegEx": "Jayram et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Jayram et al\\.", "year": 2006}, {"title": "A probabilistic model for online document clustering with application to novelty detection", "author": ["Z.G. Jian Zhang", "Y. Yang"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "Zhang and Yang,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Yang", "year": 2004}, {"title": "A systematic exploration of the feature space for relation extraction", "author": ["J. Jiang", "C. Zhai"], "venue": "In Proceedings of the Human Language Technologies and the Conference of the North American Chapter of the Association for Computational Linguistics (HLT NAACL),", "citeRegEx": "Jiang and Zhai,? \\Q2007\\E", "shortCiteRegEx": "Jiang and Zhai", "year": 2007}, {"title": "Structured output prediction with support vector machines", "author": ["T. Joachims"], "venue": "Yeung, D.-Y., Kwok, J., Fred, A., Roli, F., & de Ridder, D. (Eds.), Structural, Syntactic, and Statistical Pattern Recognition, Vol. 4109 of Lecture Notes in Computer Science, pp. 1\u20137. Springer Berlin / Heidelberg.", "citeRegEx": "Joachims,? 2006", "shortCiteRegEx": "Joachims", "year": 2006}, {"title": "Movie reviews and revenues: An experiment in text regression. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)", "author": ["M. Joshi", "D. Das", "K. Gimpel", "N.A. Smith"], "venue": null, "citeRegEx": "Joshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Combining lexical, syntactic and semantic features with maximum entropy models for information extraction", "author": ["N. Kambhatla"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pp. 178\u2013181.", "citeRegEx": "Kambhatla,? 2004", "shortCiteRegEx": "Kambhatla", "year": 2004}, {"title": "Knowledge-based acquisition of causal relationships in text", "author": ["R. Kaplan", "G. Berry-Rogghe"], "venue": "Knowledge Acquisition,", "citeRegEx": "Kaplan and Berry.Rogghe,? \\Q1991\\E", "shortCiteRegEx": "Kaplan and Berry.Rogghe", "year": 1991}, {"title": "Extracting causal knowledge from a medical database using graphical patterns", "author": ["C. Khoo", "S. Chan", "Y. Niu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Khoo et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Khoo et al\\.", "year": 2000}, {"title": "Supervenience and mind", "author": ["J. Kim"], "venue": "Selected Philosophical Essays.", "citeRegEx": "Kim,? 1993", "shortCiteRegEx": "Kim", "year": 1993}, {"title": "Extending verbnet with novel verb classes", "author": ["K. Kipper"], "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC).", "citeRegEx": "Kipper,? 2006", "shortCiteRegEx": "Kipper", "year": 2006}, {"title": "Crowdsourcing user studies with mechanical turk", "author": ["A. Kittur", "H. Chi", "B. Suh"], "venue": "In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems is the premier International Conference of human-computer interaction (CHI)", "citeRegEx": "Kittur et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kittur et al\\.", "year": 2008}, {"title": "Temporal dynamics of on-line information systems", "author": ["J. Kleinberg"], "venue": "Data Stream Management: Processing High-Speed Data Streams. Springer.", "citeRegEx": "Kleinberg,? 2006", "shortCiteRegEx": "Kleinberg", "year": 2006}, {"title": "Bursty and hierarchical structure in streams", "author": ["J. Kleinberg"], "venue": "Proceedings of the Annual ACM SIGKDD Conference (KDD).", "citeRegEx": "Kleinberg,? 2002", "shortCiteRegEx": "Kleinberg", "year": 2002}, {"title": "Kernel conditional random fields: Representation and clique selection", "author": ["J. Lafferty", "X. Zhu", "Y. Liu"], "venue": "In The 21st International Conference on Machine Learning (ICML)", "citeRegEx": "Lafferty et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2004}, {"title": "Informality judgment at sentence level and experiments with formality score", "author": ["S. Lahiri", "P. Mitra", "X. Lu"], "venue": "In Proceedings of the 12th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing)", "citeRegEx": "Lahiri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lahiri et al\\.", "year": 2011}, {"title": "The measurement of observer agreement for categorical data", "author": ["Landis"], "venue": null, "citeRegEx": "Landis,? \\Q1977\\E", "shortCiteRegEx": "Landis", "year": 1977}, {"title": "Learning sentence-internal temporal relations", "author": ["M. Lapata", "A. Lascarides"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Lapata and Lascarides,? \\Q2006\\E", "shortCiteRegEx": "Lapata and Lascarides", "year": 2006}, {"title": "An algorithm for pronominal anaphora resolution", "author": ["S. Lappin", "H. Leass"], "venue": "Computational Linguistics,", "citeRegEx": "Lappin and Leass,? \\Q1994\\E", "shortCiteRegEx": "Lappin and Leass", "year": 1994}, {"title": "Resource description framework (rdf) model and syntax specification", "author": ["O. Lassila", "R.R. Swick", "W. Wide", "W. Consortium"], "venue": null, "citeRegEx": "Lassila et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lassila et al\\.", "year": 1998}, {"title": "Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project", "author": ["D.B. Lenat", "R.V. Guha"], "venue": null, "citeRegEx": "Lenat and Guha,? \\Q1990\\E", "shortCiteRegEx": "Lenat and Guha", "year": 1990}, {"title": "A preliminary analysis of causative verbs in english", "author": ["B. Levin", "M.R. Hovav"], "venue": "Lingua,", "citeRegEx": "Levin and Hovav,? \\Q1994\\E", "shortCiteRegEx": "Levin and Hovav", "year": 1994}, {"title": "Dirt-discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": "In Proceedings of the Annual ACM SIGKDD Conference (KDD)", "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Temporal information extraction", "author": ["X. Ling", "D. Weld"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "Ling and Weld,? \\Q2010\\E", "shortCiteRegEx": "Ling and Weld", "year": 2010}, {"title": "Conceptnet: A practical commonsense reasoning toolkit", "author": ["H. Liu", "P. Singh"], "venue": "BT Technology Journal,", "citeRegEx": "Liu and Singh,? \\Q2004\\E", "shortCiteRegEx": "Liu and Singh", "year": 2004}, {"title": "Document classification on neural networks using only positive examples", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "In Proceedings of 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Manevitz and Yousef,? \\Q2000\\E", "shortCiteRegEx": "Manevitz and Yousef", "year": 2000}, {"title": "One-class svms for document classification", "author": ["L.M. Manevitz", "M. Yousef", "N. Cristianini", "J. Shawe-Taylor", "B. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Manevitz et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Manevitz et al\\.", "year": 2001}, {"title": "Inferring temporal ordering of events in news. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)", "author": ["I. Mani", "B. Schiffman", "J. Zhang"], "venue": null, "citeRegEx": "Mani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2003}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["M. Marneffe", "B. MacCartney", "C. Manning"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Named entity recognition from diverse text types", "author": ["D. Maynard", "V. Tablan", "C. Ursu", "H. Cunningham", "Y. Wilks"], "venue": "In Recent Advances in Natural Language Processing Conference (RANLP),", "citeRegEx": "Maynard et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Maynard et al\\.", "year": 2001}, {"title": "Extracting gene pathway relations using a hybrid grammar: The arizona relation", "author": ["D.M. Mcdonald", "H. Chen", "H. Su", "B.B. Marshall"], "venue": "parser. Bioinformatics,", "citeRegEx": "Mcdonald et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mcdonald et al\\.", "year": 2004}, {"title": "Quantitative analysis of culture using millions of digitized books", "author": ["J. Michel", "Y. Shen", "A. Aiden", "A. Veres", "M. Gray", "Google Books Team", "J. Pickett", "D. Hoiberg", "D. Clancy", "P. Norvig", "J. Orwant", "S. Pinker", "M. Nowak", "E. Aiden"], "venue": null, "citeRegEx": "Michel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Michel et al\\.", "year": 2011}, {"title": "Wordnet: A lexical database for english", "author": ["G. Miller"], "venue": "Journal of Communications of the ACM (CACM), 38, 39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Predicting movie sales from blogger sentiment", "author": ["G. Mishne"], "venue": "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI) Spring Symposium.", "citeRegEx": "Mishne,? 2006", "shortCiteRegEx": "Mishne", "year": 2006}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Mitchell and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Learning from positive data", "author": ["S. Muggleton"], "venue": "Proceedings of the Inductive Logic Programming Workshop, pp. 358\u2013376.", "citeRegEx": "Muggleton,? 1996", "shortCiteRegEx": "Muggleton", "year": 1996}, {"title": "Joint distant and direct supervision for relation extraction", "author": ["Nguyen", "T.-V. T", "A. Moschitti"], "venue": "In Proceedings of the The 5th International Joint Conference on Natural Language Processing (IJCNLP)", "citeRegEx": "Nguyen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2011}, {"title": "Convolution kernels on constituent, dependency and sequential structures for relation extraction", "author": ["Nguyen", "T.-V. T", "A. Moschitti", "G. Riccardi"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Nguyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2009}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": "Cambridge University Press.", "citeRegEx": "Pearl,? 2000", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["C. Quirk", "C. Brockett", "W. Dolan"], "venue": "In Proceedings the Conference on Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "Quirk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Development and application of a metric to semantic nets", "author": ["R. Rada", "H. Mili", "E. Bicknell", "M. Blettner"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Rada et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Rada et al\\.", "year": 1989}, {"title": "Predicting the news of tomorrow using patterns in web search queries", "author": ["K. Radinsky", "S. Davidovich", "S. Markovitch"], "venue": "In Proceedings of the IEEE/WIC International Conference on Web Intelligence (WI)", "citeRegEx": "Radinsky et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radinsky et al\\.", "year": 2008}, {"title": "Robust textual inference via learning and abductive reasoning", "author": ["R. Raina", "A.Y. Ng", "C.D. Manning"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "Raina et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2005}, {"title": "Automatic derivation of surface text patterns for a maximum entropy based question answering system", "author": ["D. Ravichandran", "A. Ittycheriah", "S. Roukos"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics: Short Papers (NAACL Short),", "citeRegEx": "Ravichandran et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ravichandran et al\\.", "year": 2003}, {"title": "Automatically constructing a dictionary for information extraction tasks", "author": ["E. Riloff"], "venue": "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI), pp. 811\u2013816.", "citeRegEx": "Riloff,? 1993", "shortCiteRegEx": "Riloff", "year": 1993}, {"title": "Automatically Generating Extraction Patterns from Untagged Text", "author": ["E. Riloff"], "venue": "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).", "citeRegEx": "Riloff,? 1996", "shortCiteRegEx": "Riloff", "year": 1996}, {"title": "Learning dictionaries for information extraction by multi-level bootstrapping", "author": ["E. Riloff", "R. Jones"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "Riloff and Jones,? \\Q1999\\E", "shortCiteRegEx": "Riloff and Jones", "year": 1999}, {"title": "Using corpus statistics on entities to improve semisupervised relation extraction from the web", "author": ["B. Rosenfeld", "R. Feldman"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Rosenfeld and Feldman,? \\Q2007\\E", "shortCiteRegEx": "Rosenfeld and Feldman", "year": 2007}, {"title": "Information extraction", "author": ["S. Sarawagi"], "venue": "Foundations and Trends in Databases, 1 (3), 261\u2013377.", "citeRegEx": "Sarawagi,? 2008", "shortCiteRegEx": "Sarawagi", "year": 2008}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R. Williamson", "A. Smola", "J. Shawe-Taylor", "J. Platt"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "Sv estimation of a distribution\u2019s support", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A. Smola", "J. Shawe-Taylor"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Can we derive general world knowledge from texts", "author": ["L. Schubert"], "venue": "Proceedings of the Second Conference on Human Language Technology (HLT).", "citeRegEx": "Schubert,? 2002", "shortCiteRegEx": "Schubert", "year": 2002}, {"title": "Connecting the dots between news articles", "author": ["D. Shahaf", "C. Guestrin"], "venue": "In Proceedings of the Annual ACM SIGKDD Conference (KDD)", "citeRegEx": "Shahaf and Guestrin,? \\Q2010\\E", "shortCiteRegEx": "Shahaf and Guestrin", "year": 2010}, {"title": "Declarative information extraction using datalog with embedded extraction predicates", "author": ["W. Shen", "A. Doan", "J.F. Naughton", "R. Ramakrishnan"], "venue": "In Proceedings of the Conference on Very Large Data Bases (VLDB),", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Putting pieces together: Combining framenet, verbnet and wordnet for robust semantic parsing", "author": ["L. Shi", "R. Mihalcea"], "venue": "In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),", "citeRegEx": "Shi and Mihalcea,? \\Q2005\\E", "shortCiteRegEx": "Shi and Mihalcea", "year": 2005}, {"title": "Preemptive information extraction using unrestricted relation discovery. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)", "author": ["Y. Shinyama", "S. Sekine"], "venue": null, "citeRegEx": "Shinyama and Sekine,? \\Q2006\\E", "shortCiteRegEx": "Shinyama and Sekine", "year": 2006}, {"title": "Extracting action and event semantics from web text", "author": ["A. Sil", "F. Huang", "A. Yates"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI) Fall Symposium on Commonsense Knowledge", "citeRegEx": "Sil et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sil et al\\.", "year": 2010}, {"title": "Learning information extraction rules for semi-structured and free text", "author": ["S. Soderland"], "venue": "Machine Learning, 34.", "citeRegEx": "Soderland,? 1999", "shortCiteRegEx": "Soderland", "year": 1999}, {"title": "Wikirelate! computing semantic relatedness using wikipedia", "author": ["M. Strube", "S.P. Ponzetto"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "Strube and Ponzetto,? \\Q2006\\E", "shortCiteRegEx": "Strube and Ponzetto", "year": 2006}, {"title": "Combining linguistic and statistical analysis to extract relations from web documents", "author": ["F.M. Suchanek"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 712\u2013717.", "citeRegEx": "Suchanek,? 2006", "shortCiteRegEx": "Suchanek", "year": 2006}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "In Proceedings of the International Conference on the World Wide Web (WWW)", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "A semantic approach to recognizing textual entailment", "author": ["M. Tatu", "D. Moldovan"], "venue": "In Proceedings of the Human Language Technology Conference Conference on Empirical Methods in Natural Language Processing (HLT EMNLP)", "citeRegEx": "Tatu and Moldovan,? \\Q2005\\E", "shortCiteRegEx": "Tatu and Moldovan", "year": 2005}, {"title": "Experiments with reasoning for temporal relations between events", "author": ["M. Tatu", "M. Srikanth"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING)", "citeRegEx": "Tatu and Srikanth,? \\Q2008\\E", "shortCiteRegEx": "Tatu and Srikanth", "year": 2008}, {"title": "One class classification", "author": ["D. Tax"], "venue": "PhD thesis, Delft University of Technology.", "citeRegEx": "Tax,? 2001", "shortCiteRegEx": "Tax", "year": 2001}, {"title": "Support vector domain description", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Tax and Duin,? \\Q1991\\E", "shortCiteRegEx": "Tax and Duin", "year": 1991}, {"title": "Expressing implicit semantic relations without supervision", "author": ["P.D. Turney"], "venue": "Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Turney,? 2006", "shortCiteRegEx": "Turney", "year": 2006}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Springer-Verlag, NY, USA.", "citeRegEx": "Vapnik,? 1995", "shortCiteRegEx": "Vapnik", "year": 1995}, {"title": "Understanding interobserver agreement: The kappa statistic", "author": ["A.J. Viera", "J.M. Garrett"], "venue": "Family Medicine,", "citeRegEx": "Viera and Garrett,? \\Q2005\\E", "shortCiteRegEx": "Viera and Garrett", "year": 2005}, {"title": "A re-examination of dependency path kernels for relation extraction", "author": ["M. Wang"], "venue": "Proceedings of the Third International Joint Conference on Natural Language Processing (ACL IJCNLP).", "citeRegEx": "Wang,? 2008", "shortCiteRegEx": "Wang", "year": 2008}, {"title": "Models of causation and causal verbs", "author": ["P. Wolff", "G. Song", "D. Driscoll"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Wolff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wolff et al\\.", "year": 2002}, {"title": "A study on retrospective and online event detection", "author": ["Y. Yang", "T. Pierce", "J. Carbonell"], "venue": "In Proceedings of ACM SIGIR Special Interest Group on Information Retrieval (SIGIR)", "citeRegEx": "Yang et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Yang et al\\.", "year": 1998}, {"title": "Studying how the past is remembered: Towards computational history through large scale text mining", "author": ["C. Yeung", "A. Jatowt"], "venue": "In Proceedings of the ACM Conference on Information and Knowledge Management (CIKM)", "citeRegEx": "Yeung and Jatowt,? \\Q2011\\E", "shortCiteRegEx": "Yeung and Jatowt", "year": 2011}, {"title": "Jointly identifying temporal relations with markov logic", "author": ["K. Yoshikawa", "S. Riedel", "M. Asahara", "Y. Matsumoto"], "venue": "In Proceedings of the Third International Joint Conference on Natural Language Processing (ACL IJCNLP)", "citeRegEx": "Yoshikawa et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yoshikawa et al\\.", "year": 2009}, {"title": "A machine learning approach to textual entailment recognition", "author": ["F.M. Zanzotto", "M. Pennacchiotti", "A. Moschitti"], "venue": "Natural Language Engineering,", "citeRegEx": "Zanzotto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2009}, {"title": "Kernel methods for relation extraction", "author": ["D. Zelenko", "C. Aone", "A. Richardella"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "A composite kernel to extract relations between entities with both flat and structured features", "author": ["M. Zhang", "J. Zhang", "J. Su", "G. Zhou"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Extracting relations with integrated information using kernel methods", "author": ["S. Zhao", "R. Grishman"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Zhao and Grishman,? \\Q2005\\E", "shortCiteRegEx": "Zhao and Grishman", "year": 2005}], "referenceMentions": [{"referenceID": 34, "context": ", Banko, Cafarella, Soderl, Broadhead, & Etzioni, 2007; Carlson, Betteridge, Kisiel, Settles, Hruschka, & Mitchell, 2010), little has been done in the area of causality extraction, with the works of Khoo, Chan, and Niu (2000) and Girju and Moldovan (2002) being notable exceptions.", "startOffset": 230, "endOffset": 256}, {"referenceID": 5, "context": "The Topic Tracking and Detection (TDT) community (Allan, 2002) has defined an event as \u201ca particular thing that happens at a specific time and place, along with all necessary preconditions and unavoidable consequences.", "startOffset": 49, "endOffset": 62}, {"referenceID": 53, "context": "Other philosophical theories consider events as exemplifications of properties by objects at times (Kim, 1993).", "startOffset": 99, "endOffset": 110}, {"referenceID": 31, "context": "The second approach describes events in a syntax-driven manner, where the event text is transformed into syntax-based components, such as noun phrases (Garcia, 1997; Khoo et al., 2000; Girju & Moldovan, 2002; Chan & Lam, 2005).", "startOffset": 151, "endOffset": 226}, {"referenceID": 52, "context": "The second approach describes events in a syntax-driven manner, where the event text is transformed into syntax-based components, such as noun phrases (Garcia, 1997; Khoo et al., 2000; Girju & Moldovan, 2002; Chan & Lam, 2005).", "startOffset": 151, "endOffset": 226}, {"referenceID": 51, "context": "Other philosophical theories consider events as exemplifications of properties by objects at times (Kim, 1993). For example, Caesar\u2019s death at 44 BC is Caesar\u2019s exemplification of the property of dying at time 44 BC. Those theories impose structure on events, where a change in one of the elements yields a different event. For example, Shakespear\u2019s death is a different event from Caesar\u2019s death, as the objects exemplifying the property are different. In this section, we will discuss a way to represent events following Kim\u2019s (1993) exemplification theory that will allow us to easily compare them, generalize them, and reason about them.", "startOffset": 100, "endOffset": 536}, {"referenceID": 53, "context": "This approach is inspired by Kim\u2019s (1993) property-exemplification of events theory.", "startOffset": 29, "endOffset": 42}, {"referenceID": 54, "context": "Specifically, we use the VerbNet (Kipper, 2006) ontology, which is one of the largest English verb lexicons.", "startOffset": 33, "endOffset": 47}, {"referenceID": 76, "context": "It has mapping to many other online resources, such as Wordnet (Miller, 1995).", "startOffset": 63, "endOffset": 77}, {"referenceID": 10, "context": "These methods usually estimate the prior probability of a relation by examining the frequency of its pattern in a large corpus, such as the Web (Banko et al., 2007).", "startOffset": 144, "endOffset": 164}, {"referenceID": 76, "context": "The entity graph Go is composed of concepts from Wikipedia, ConceptNet (Liu & Singh, 2004), WordNet (Miller, 1995), Yago (Suchanek, Kasneci, & Weikum, 2007), and OpenCyc; the billion labeled edges of the graph Go are the predicates of those ontologies.", "startOffset": 100, "endOffset": 114}, {"referenceID": 11, "context": "The concepts are interlinked by humans as part of the Linked Data project (Bizer et al., 2009).", "startOffset": 74, "endOffset": 94}, {"referenceID": 11, "context": "The concepts are interlinked by humans as part of the Linked Data project (Bizer et al., 2009). The goal of Bizer et al.\u2019s (2009) Linked Data project is to extend the Web by interlinking multiple datasets as RDF and by setting RDF links between data items from different data sources.", "startOffset": 75, "endOffset": 130}, {"referenceID": 76, "context": "Root forms of inflected words are extracted using a morphological analyzer derived from WordNet (Miller, 1995) stemmer.", "startOffset": 96, "endOffset": 110}, {"referenceID": 54, "context": "The class of the verb is identified using the VerbNet vocabulary (Kipper, 2006), e.", "startOffset": 65, "endOffset": 79}, {"referenceID": 72, "context": "If no template can be matched, the sentence is transformed into a typed-dependency graph of grammatical relations (Marneffe et al., 2006).", "startOffset": 114, "endOffset": 137}, {"referenceID": 77, "context": "Social media were used to predict riots (Kalev, 2011) and movie box office sales (Asur & Huberman, 2010; Joshi, Das, Gimpel, & Smith, 2010; Mishne, 2006).", "startOffset": 81, "endOffset": 153}, {"referenceID": 33, "context": "Ginsberg et al. (2009) used queries for predicting H1N1 influenza outbreaks.", "startOffset": 0, "endOffset": 23}, {"referenceID": 33, "context": "Ginsberg et al. (2009) used queries for predicting H1N1 influenza outbreaks. In the context of causality recognition, Gordon, Bejan, and Sagae (2011) present a methodology for mining blogs to extract common-sense causality.", "startOffset": 0, "endOffset": 150}, {"referenceID": 33, "context": "Ginsberg et al. (2009) used queries for predicting H1N1 influenza outbreaks. In the context of causality recognition, Gordon, Bejan, and Sagae (2011) present a methodology for mining blogs to extract common-sense causality. The evaluation is done on a human-labeled dataset where each test consists of a fact and two possible effects. Applying point-mutual information to personal blog stories, the authors select the best prediction candidate. The work differs from ours in that the authors focus on personal commonsense mining and do not consider whether their predictions actually occurred. Other works focused on predicting Web content change itself. For example, Kleinberg (2002, 2006) developed general techniques for summarizing the temporal dynamics of textual content and for identifying bursts of terms within content. Similarly, Amodeo, Blanco, and Brefeld (2011) built a time series model over publication dates of documents relevant to a query in order to predict future bursts.", "startOffset": 0, "endOffset": 875}, {"referenceID": 7, "context": "For a more detailed overview we refer the reader to the survey by Androutsopoulos and Malakasiotis (2010). We then discuss the specific task of causality extraction from text in Section 5.", "startOffset": 66, "endOffset": 106}, {"referenceID": 38, "context": "Similarly, other methods measure the semantic distance similarity between the words in text (Haghighi, 2005), usually exploiting", "startOffset": 92, "endOffset": 108}, {"referenceID": 39, "context": "The last set of approaches represents the two texts in a single feature vector and trains a machine learning algorithm, which later, given two new texts represented via a vector, can determine whether they entail each other (Bos & Markert, 2005; Burchardt, Pennacchiotti, Thater, & Pinkal, 2009; Hickl, 2008).", "startOffset": 224, "endOffset": 308}, {"referenceID": 38, "context": "Other features usually include polarity (Haghighi, 2005), whether the theorem prover managed to prove entailment (Bos & Markert, 2005), or tagging of the named entities to the categories people, organizations, or locations.", "startOffset": 40, "endOffset": 56}, {"referenceID": 36, "context": "For example, Glickman et al. (2005) show a naive Bayes classifier trained on lexical features, i.", "startOffset": 13, "endOffset": 36}, {"referenceID": 7, "context": "Androutsopoulos and Malakasiotis (2010) mention that no benchmarks exist to evaluate this task, and the most common and costly approach is to evaluate using human judges.", "startOffset": 0, "endOffset": 40}, {"referenceID": 7, "context": "Androutsopoulos and Malakasiotis (2010) mention that no benchmarks exist to evaluate this task, and the most common and costly approach is to evaluate using human judges. We also encountered this difficulty in our own task, and performed human evaluation. TE generation methods can be divided into two types: those that use machine translation techniques and those that use template-based techniques. Those that use machine translation techniques try to calculate the set of transformations with the highest probability, using a training corpus. Quirk, Brockett, and Dolan (2004) cluster news articles referring to the same event, select pairs of similar sentences, and apply the aforementioned techniques.", "startOffset": 0, "endOffset": 580}, {"referenceID": 92, "context": "The most closely related tasks to ours are those of entity extraction and relation extraction; for the rest we refer the reader to the survey by Sarawagi (2008). The former task, similar to our process of extracting concepts, deals with extracting noun phrases from text.", "startOffset": 145, "endOffset": 161}, {"referenceID": 88, "context": "Rule-based methods (Riloff, 1993; Riloff & Jones, 1999; Jayram, Krishnamurthy, Raghavan, Vaithyanathan, & Zhu, 2006; Shen, Doan, Naughton, & Ramakrishnan, 2007; Ciravegna, 2001; Maynard, Tablan, Ursu, Cunningham, & Wilks, 2001; Hobbs, Bear, Israel, & Tyson, 1993) define contextual patterns consisting a regular expression over features of the entities in the text (e.", "startOffset": 19, "endOffset": 263}, {"referenceID": 25, "context": "Rule-based methods (Riloff, 1993; Riloff & Jones, 1999; Jayram, Krishnamurthy, Raghavan, Vaithyanathan, & Zhu, 2006; Shen, Doan, Naughton, & Ramakrishnan, 2007; Ciravegna, 2001; Maynard, Tablan, Ursu, Cunningham, & Wilks, 2001; Hobbs, Bear, Israel, & Tyson, 1993) define contextual patterns consisting a regular expression over features of the entities in the text (e.", "startOffset": 19, "endOffset": 263}, {"referenceID": 25, "context": "Those rules are either manually coded by a domain expert or learned using bottom-up (Ciravegna, 2001; Califf & Mooney, 1999) or top-down learners (Soderland, 1999).", "startOffset": 84, "endOffset": 124}, {"referenceID": 101, "context": "Those rules are either manually coded by a domain expert or learned using bottom-up (Ciravegna, 2001; Califf & Mooney, 1999) or top-down learners (Soderland, 1999).", "startOffset": 146, "endOffset": 163}, {"referenceID": 95, "context": "Relation extraction has been developed widely in the last years from large text corpora (Schubert, 2002) and, in particular, from different Web resources, such as general Web content (Banko et al.", "startOffset": 88, "endOffset": 104}, {"referenceID": 10, "context": "Relation extraction has been developed widely in the last years from large text corpora (Schubert, 2002) and, in particular, from different Web resources, such as general Web content (Banko et al., 2007; Carlson et al., 2010; Hoffmann, Zhang, & Weld, 2010), blogs (Jayram et al.", "startOffset": 183, "endOffset": 256}, {"referenceID": 20, "context": "Relation extraction has been developed widely in the last years from large text corpora (Schubert, 2002) and, in particular, from different Web resources, such as general Web content (Banko et al., 2007; Carlson et al., 2010; Hoffmann, Zhang, & Weld, 2010), blogs (Jayram et al.", "startOffset": 183, "endOffset": 256}, {"referenceID": 45, "context": ", 2010; Hoffmann, Zhang, & Weld, 2010), blogs (Jayram et al., 2006), Wikipedia (Suchanek et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 104, "context": ", 2006), Wikipedia (Suchanek et al., 2007), and news articles (e.", "startOffset": 19, "endOffset": 42}, {"referenceID": 50, "context": "Many feature-based methods (Jiang & Zhai, 2007; Kambhatla, 2004; Suchanek, 2006) and rule-based methods (Aitken, 2002; Mcdonald, Chen, Su, & Marshall, 2004; Jayram et al.", "startOffset": 27, "endOffset": 80}, {"referenceID": 103, "context": "Many feature-based methods (Jiang & Zhai, 2007; Kambhatla, 2004; Suchanek, 2006) and rule-based methods (Aitken, 2002; Mcdonald, Chen, Su, & Marshall, 2004; Jayram et al.", "startOffset": 27, "endOffset": 80}, {"referenceID": 4, "context": "Many feature-based methods (Jiang & Zhai, 2007; Kambhatla, 2004; Suchanek, 2006) and rule-based methods (Aitken, 2002; Mcdonald, Chen, Su, & Marshall, 2004; Jayram et al., 2006; Shen et al., 2007) have been developed for this task.", "startOffset": 104, "endOffset": 196}, {"referenceID": 45, "context": "Many feature-based methods (Jiang & Zhai, 2007; Kambhatla, 2004; Suchanek, 2006) and rule-based methods (Aitken, 2002; Mcdonald, Chen, Su, & Marshall, 2004; Jayram et al., 2006; Shen et al., 2007) have been developed for this task.", "startOffset": 104, "endOffset": 196}, {"referenceID": 97, "context": "Many feature-based methods (Jiang & Zhai, 2007; Kambhatla, 2004; Suchanek, 2006) and rule-based methods (Aitken, 2002; Mcdonald, Chen, Su, & Marshall, 2004; Jayram et al., 2006; Shen et al., 2007) have been developed for this task.", "startOffset": 104, "endOffset": 196}, {"referenceID": 113, "context": "Labeled training examples, from which those feature are extracted, are then fed into a machine learning classifier, sometimes using transformations such as kernels (Zhao & Grishman, 2005; Zhang, Zhang, Su, & Zhou, 2006; Zelenko, Aone, & Richardella, 2003; Wang, 2008; Culotta & Sorensen, 2004; Bunescu & Mooney, 2005; Nguyen, Moschitti, & Riccardi, 2009), which, given new unseen entities, will be able to classify them into those categories.", "startOffset": 164, "endOffset": 354}, {"referenceID": 10, "context": "Most works in this domain focus on large collections, such as the Web, where labeling all entities and relations is infeasible (Agichtein & Gravano, 2000; Banko et al., 2007; Bunescu & Mooney, 2007; Rosenfeld & Feldman, 2007; Shinyama & Sekine, 2006; Turney, 2006).", "startOffset": 127, "endOffset": 264}, {"referenceID": 110, "context": "Most works in this domain focus on large collections, such as the Web, where labeling all entities and relations is infeasible (Agichtein & Gravano, 2000; Banko et al., 2007; Bunescu & Mooney, 2007; Rosenfeld & Feldman, 2007; Shinyama & Sekine, 2006; Turney, 2006).", "startOffset": 127, "endOffset": 264}, {"referenceID": 22, "context": "Most of these approaches (Ling & Weld, 2010; Mani, Schiffman, & Zhang, 2003; Lapata & Lascarides, 2006; Chambers et al., 2007; Tatu & Srikanth, 2008; Yoshikawa, Riedel, Asahara, & Matsumoto, 2009) learn classifiers that predict a temporal order of a pair of events from predefined features of the pair.", "startOffset": 25, "endOffset": 196}, {"referenceID": 5, "context": "This area includes several tasks (Allan, 2002).", "startOffset": 33, "endOffset": 46}, {"referenceID": 5, "context": "This area includes several tasks (Allan, 2002). In all of them, multiple, heterogenous new sources are used, including audio. The story segmentation task aims to segment data into its constituent stories. The topic tracking task \u2013 e.g., the work by Shahaf and Guestrin (2010) \u2013 aims to find all stories discussing a certain topic.", "startOffset": 34, "endOffset": 276}, {"referenceID": 5, "context": "This area includes several tasks (Allan, 2002). In all of them, multiple, heterogenous new sources are used, including audio. The story segmentation task aims to segment data into its constituent stories. The topic tracking task \u2013 e.g., the work by Shahaf and Guestrin (2010) \u2013 aims to find all stories discussing a certain topic. A subtask of this is the link detection task which, given a pair of stories, aims to classify whether they are on the same topic. The topic detection task \u2013 e.g. the works by Ahmed, Ho, Eisenstein, Xing, Smola, and Teo (2011) and Yang, Pierce, and Carbonell (1998) \u2013 aims to detect clusters of topic-cohesive stories in a stream of topics.", "startOffset": 34, "endOffset": 557}, {"referenceID": 5, "context": "This area includes several tasks (Allan, 2002). In all of them, multiple, heterogenous new sources are used, including audio. The story segmentation task aims to segment data into its constituent stories. The topic tracking task \u2013 e.g., the work by Shahaf and Guestrin (2010) \u2013 aims to find all stories discussing a certain topic. A subtask of this is the link detection task which, given a pair of stories, aims to classify whether they are on the same topic. The topic detection task \u2013 e.g. the works by Ahmed, Ho, Eisenstein, Xing, Smola, and Teo (2011) and Yang, Pierce, and Carbonell (1998) \u2013 aims to detect clusters of topic-cohesive stories in a stream of topics.", "startOffset": 34, "endOffset": 596}, {"referenceID": 51, "context": "Kaplan and Berry-Rogghe (1991) used scientific texts to create a manually designed set of propositions which were later applied on", "startOffset": 0, "endOffset": 31}, {"referenceID": 31, "context": "For example, Garcia (1997) manually identified 23 causative verb groups (e.", "startOffset": 13, "endOffset": 27}, {"referenceID": 31, "context": "For example, Garcia (1997) manually identified 23 causative verb groups (e.g., to result in, to lead to, etc.). If a sentence contained one of those verbs, it was classified as containing a causation relation. A precision of 85% was reported. Khoo et al. (2000) used manually extracted graphical patterns based on syntactic parse trees, reporting accuracy of about 68% on an English medical database.", "startOffset": 13, "endOffset": 262}, {"referenceID": 31, "context": "For example, Garcia (1997) manually identified 23 causative verb groups (e.g., to result in, to lead to, etc.). If a sentence contained one of those verbs, it was classified as containing a causation relation. A precision of 85% was reported. Khoo et al. (2000) used manually extracted graphical patterns based on syntactic parse trees, reporting accuracy of about 68% on an English medical database. Similarly, Girju and Moldovan (2002) defined lexicon-syntactic patterns (pairs of noun phrases with a causative verb in between) with additional semantic constraints.", "startOffset": 13, "endOffset": 438}, {"referenceID": 13, "context": "For example, Blanco et al. (2008) and Sil et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 13, "context": "For example, Blanco et al. (2008) and Sil et al. (2010) use syntactic patterns as features that are later fed into classifiers, whose output is whether the text implies causality or the cause and effect themselves.", "startOffset": 13, "endOffset": 56}, {"referenceID": 89, "context": "There have been many works on design inference rules to discover extraction patterns for a given relation using training examples (Riloff, 1996; Riloff & Jones, 1999; Agichtein & Gravano, 2000; Lin & Pantel, 2001).", "startOffset": 130, "endOffset": 213}, {"referenceID": 23, "context": "Specifically, Chan and Lam (2005) dealt with the problem of creating syntactic patterns for cause-effect extraction.", "startOffset": 14, "endOffset": 34}, {"referenceID": 82, "context": "The functional causal model (Pearl, 2000) assumes a set of observables X1 .", "startOffset": 28, "endOffset": 41}, {"referenceID": 48, "context": ", structured support vector machines (Joachims, 2006).", "startOffset": 37, "endOffset": 53}, {"referenceID": 108, "context": "Most works (Tax, 2001; Manevitz & Yousef, 2000; Manevitz, Yousef, Cristianini, ShaweTaylor, & Williamson, 2001) in this domain develop algorithms that use one-class SVM (Vapnik, 1995) and learn the support using only positive distribution.", "startOffset": 11, "endOffset": 111}, {"referenceID": 111, "context": "Most works (Tax, 2001; Manevitz & Yousef, 2000; Manevitz, Yousef, Cristianini, ShaweTaylor, & Williamson, 2001) in this domain develop algorithms that use one-class SVM (Vapnik, 1995) and learn the support using only positive distribution.", "startOffset": 169, "endOffset": 183}, {"referenceID": 108, "context": "Some also use kernel tricks before finding this sphere (Tax, 2001).", "startOffset": 55, "endOffset": 66}, {"referenceID": 28, "context": "Some theoretical studies of the possibility of learning from only positive unlabeled data are provided in the work by Denis (1998) (probably approximately correct (PAC) learning) and Muggleton (1996) (Bayesian learning).", "startOffset": 118, "endOffset": 131}, {"referenceID": 28, "context": "Some theoretical studies of the possibility of learning from only positive unlabeled data are provided in the work by Denis (1998) (probably approximately correct (PAC) learning) and Muggleton (1996) (Bayesian learning).", "startOffset": 118, "endOffset": 200}, {"referenceID": 28, "context": "Some theoretical studies of the possibility of learning from only positive unlabeled data are provided in the work by Denis (1998) (probably approximately correct (PAC) learning) and Muggleton (1996) (Bayesian learning). Most works (Tax, 2001; Manevitz & Yousef, 2000; Manevitz, Yousef, Cristianini, ShaweTaylor, & Williamson, 2001) in this domain develop algorithms that use one-class SVM (Vapnik, 1995) and learn the support using only positive distribution. They construct decision boundaries around the positive examples to differentiate them from all possible negative data. Tax and Duin (1991) use a hyper-sphere with some defined radius around some of the positive class points (support vector data description method).", "startOffset": 118, "endOffset": 600}, {"referenceID": 29, "context": ", as proposed by Do et al. (2011) can provide higher analysis of the data from the entire text rather than just the titles.", "startOffset": 17, "endOffset": 34}], "year": 2012, "abstractText": "Given a current news event, we tackle the problem of generating plausible predictions of future events it might cause. We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor. To obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing certain predefined causality patterns. For generalization, the model uses a vast number of world knowledge ontologies. Empirical evaluation on real news articles shows that our Pundit algorithm performs as well as non-expert humans.", "creator": "TeX"}}}