{"id": "1402.3371", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2014", "title": "An evaluative baseline for geo-semantic relatedness and similarity", "abstract": "in geographic information science and semantics, extensive computation of semantic similarity is widely recognised as helping to supporting a vast continuum of tasks in information integration and retrieval. by contrast, the role of geo - semantic relatedness has been largely ignored. in natural language processing, semantic relatedness is often confused at the more homogeneous semantic components. see this article, we discuss a notion of geo - semantic relatedness based on lehrer's string fields, and we compare it with geo - semantic alignment. we then describe and validate the verbal relatedness and similarity dataset ( geresid ), as new open dataset designed to evaluate multiple measures of geo - thematic relatedness and composition. your dataset is larger than existing datasets of this kind, and includes 97 category terms combined into 50 term pairs rated off 203 human subjects. geresid is available online applications can be used as an evaluation baseline to determine empirically to what degree one given computational model approximates 3 - semantic relatedness and similarity.", "histories": [["v1", "Fri, 14 Feb 2014 06:06:47 GMT  (141kb,D)", "http://arxiv.org/abs/1402.3371v1", "GeoInformatica 2014"]], "COMMENTS": "GeoInformatica 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrea ballatore", "michela bertolotto", "david c wilson"], "accepted": false, "id": "1402.3371"}, "pdf": {"name": "1402.3371.pdf", "metadata": {"source": "CRF", "title": "An Evaluative Baseline for Geo-Semantic Relatedness and Similarity", "authors": ["Andrea Ballatore", "Michela Bertolotto", "David C. Wilson"], "emails": ["andrea.ballatore@ucd.ie", "michela.bertolotto@ucd.ie", "davils@uncc.edu"], "sections": [{"heading": null, "text": "Keywords geo-semantic relatedness \u00b7 geo-semantic similarity \u00b7 gold standards \u00b7 geo-semantics \u00b7 cognitive plausibility \u00b7 GeReSiD"}, {"heading": "1 Introduction", "text": "Is lake related to river? Is road related to transportation? Are mountain and hill more related than mountain and lake? While it may seem natural to answer yes to all of these questions, the logical and computational formalisation\nAndrea Ballatore & Michela Bertolotto School of Computer Science and Informatics University College Dublin, Belfield, Dublin 4, Ireland E-mail: {andrea.ballatore,michela.bertolotto}@ucd.ie\nDavid C. Wilson Department of Software and Information Systems University of North Carolina, Charlotte, NC, USA E-mail: davils@uncc.edu\nar X\niv :1\n40 2.\n33 71\nv1 [\ncs .C\nL ]\n1 4\nFe b\n20 14\nof why this is the case has raised considerable interest in philosophy, psychology, linguistics and, more recently, in computer science. The human ability to detect semantic relatedness is essential to perform key operations in communication, such as word-sense disambiguation (e.g. interpreting bank as financial institution or as the terrain alongside the bed of a river), reducing semantic ambiguity and increasing efficiency in meaning-creation and sharing. The human cognitive apparatus possesses a remarkable ability to detect co-occurrence patterns that are not due to chance, but that indicate the existence of some semantic relation between the terms.\nSemantic similarity has been identified as a particular subset of this general notion of semantic relatedness. While semantically related terms are connected by any kind of relation, semantically similar terms are related by synonymy, hyponymy, and hypernymy, all of which involve an is a relation. In this sense, train and bus are intuitively similar (they are both means of transport), whilst bus and road are related but not similar (i.e. they often co-occur but with different roles). Semantic similarity relies on the general cognitive ability to detect similar patterns in stimuli, which attracts considerable attention in cognitive science. Notably, Goldstone and Son [15] stated that \u201cassessments of similarity are fundamental to cognition because similarities in the world are revealing. The world is an orderly enough place that similar objects and events tend to behave similarly\u201d (p. 13). Therefore, the vast applicability of semantic similarity in computer and information science should come as no surprise.\nIn geographic information science (GIScience), the theoretical and practical importance of geo-semantic similarity has been fully acknowledged, resulting in a growing body of research [2, 4, 23]. By contrast, the importance of semantic relatedness, which is widely studied in the non-geographic domain, has been almost completely ignored, with the exception of the works by Hecht and Raubal [16] and Hecht et al. [17]. Computational measures of semantic relatedness play a pivotal role in natural language processing, information retrieval (IR), and word sense disambiguation, providing access to deeper semantic connections between words and sets of words. Despite the large number of existing measures, their rigorous evaluation still constitutes an important research challenge [12].\nThis article contributes to GIScience and semantics in the following ways. First, we discuss in detail the notion of geo-semantic relatedness, drawing on Lehrer\u2019s theory of semantic fields, which consist of sets of terms covering a restricted semantic domain. Geo-semantic relatedness is defined with respect to specifiable geographic relations between terms, and is compared and contrasted with the more widely studied geo-semantic similarity. Second, we have developed and validated the Geo Relatedness and Similarity Dataset (GeReSiD), tackling the complex issue of the evaluation of computational measures of geo-semantic relatedness and similarity. In this new dataset, we have collected psychological judgements about 50 pairs of terms, covering 97 unique geographic terms, from 203 human subjects. The human judgements in GeReSiD focus explicitly on geo-semantic relatedness and similarity between geographic terms.\nThe resulting dataset provides an evaluation test bed for geo-semantic relatedness and similarity. This is compared against the existing human-generated gold standards used to assess computational measures of semantic relatedness and similarity, highlighting the limitations of such datasets. Such an evaluative baseline constitutes a valuable ground truth against which computational measures can be assessed, providing empirical evidence about the cognitive plausibility of the measures. GeReSiD can inform research in geo-semantics, indicating to what degree computational approaches match human judgements. More specifically the contribution of this evaluative baseline consists of the following aspects:\n\u2013 GeReSiD covers a sample of geographic terms larger than existing similarity datasets, including 97 natural and man-made unique terms, grouped in 50 unique pairs. Psychological judgements of geo-semantic relatedness and similarity were collected separately on the 50 pairs. \u2013 GeReSiD includes a sample of evenly distributed relatedness/similarity judgements, ranging from near-synonymity to no relationship between the terms. Our methodology is described explicitly and precisely, in order to provide practical guidelines to construct similar datasets. \u2013 Unlike existing datasets, the semantic judgements on the term pairs contained in GeReSiD are analysed with respect to interrater agreement (IRA) and interrater reliability (IRR). \u2013 The psychological judgements in GeReSiD can be observed as the mean of relatedness/similarity of the pairs, using correlation coefficients of relatedness/similarity rankings (such as Spearman\u2019s \u03c1 or Kendall\u2019s \u03c4). Alternatively, the data can be interpreted as categorical, using Cohen\u2019s kappa or Fisher\u2019s exact test [7] to evaluate the computational measure. \u2013 GeReSiD is an open dataset freely available online.1 Both raw data and the resulting dataset are available.\nThe remainder of this article is organised as follows. Section 2 discusses in depth the two key notions of geo-semantic relatedness and similarity, proposing a synthetic definition. Section 3 summarises existing datasets for semantic relatedness and similarity, with particular attention to those restricted to the geographic domain. The new evaluative baseline, GeReSiD, is outlined, analysed and discussed in Section 4. Conclusions and directions for future research are indicated in Section 5."}, {"heading": "2 Geo-semantic relatedness and similarity", "text": "This section introduces the notion of geo-semantic relatedness, comparing it and contrasting it with geo-semantic similarity. In the natural language processing literature, several terms are used inconsistently, including semantic relatedness, relational similarity, taxonomical similarity, semantic association,\n1 http://github.com/ucd-spatial/Datasets (acc. Apr 10, 2013)\nanalogy, and attributional similarity [53]. These terms are often used interchangeably [9]. A striking example of this tendency is the article title \u2018WordNet::Similarity: Measuring the relatedness of terms\u2019 [39].\nIn natural language, terms are connected by an open set of semantic relations. Common semantic relations are synonymy (A coincides with B), antonymy (A is the opposite of B), hyponymy (A is a B), hypernymy (B is a A), holonymy (A is whole of B), meronymy (A is part of B), causality (A causes B), temporal contiguity (A occurs at the same time as B), and function (A is used to perform B). Khoo and Na [28] have surveyed these semantic relations, whilst Morris and Hirst [36] have explored other non-classical semantic relations. As Khoo and Na [28] remarked, semantic relations are characterised by productivity (new relations can be easily created), uncountability (semantic relations are an open class and cannot be counted), and predictability (they follow general, recurring patterns). In the geographic domain, spatial relations such as proximity (A is near B), and containment (A is within B) have an impact on semantics [49].\nBefore providing our definition of geo-semantic relatedness and similarity, it is beneficial to review the semantics of these terms in the literature. In the context of semantic networks, Rada et al. [40] suggested that semantic relatedness is \u201cbased on an aggregate of the interconnections between the terms\u201d (p. 18). To obtain semantic similarity, the observation must be restricted to taxonomic is a relationships between terms. Resnik [41] followed this approach, and defined semantic similarity and relatedness as follows: \u201cSemantic similarity represents a special case of semantic relatedness: for example, cars and gasoline would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar\u201d (p. 448).\nMore recently, Turney [53] added a further distinction between \u2018attributional\u2019 and \u2018relational similarity.\u2019 Following the approach outlined by Medin et al. [32], \u2018attributes\u2019 are statements about a term that take only one parameter, e.g. X is red, X is long. Therefore, attributional similarity measures the correspondence between the attributes of the two terms. \u2018Relations,\u2019 on the other hand, are statements that take two or more parameters, e.g. X is a Y,X is longer than Y . Hence, relational similarity is based on the common relations between two pairs of terms [53]. On these assumptions, synonymy is seen as a high degree of attributional similarity between two terms, e.g. <river,stream>. Analogy, by contrast, is characterised as a high degree of relational similarity between two pairs of terms, e.g. <boat,river> and <car,road>. The next sections discuss geo-semantic relatedness and similarity in detail.\n2.1 Geo-semantic relatedness\nA general notion of relatedness in the geographic context was stated in Tobler\u2019s first law, which asserts that everything is related to everything else, but near things are more related than distant things [51]. While this law was formulated\nto express intuitively the high spatial autocorrelation of many geographic phenomena, it has generated several responses in GIScience. For example, in the context of information visualisation, Montello et al. [35] have proposed the first law of cognitive geography, which states that \u201cpeople believe closer things to be more similar than distant things\u201d (p. 317). Applying the same intuition to the domain of geo-semantics, we assert that two terms are geo-semantically related to the degree to which they refer to entities or phenomena connected via specifiable relations grounded in the geographic dimension.\nTo define a notion of geo-semantic relatedness, we rely on the notion of semantic field. According to Lehrer [31], a semantic field is \u201ca set of lexemes which cover a certain conceptual domain and which bear certain specifiable relation to one another\u201d (p. 283). While a \u2018domain\u2019 is an epistemological notion referring to a subset of human knowledge and experience (e.g. geography, politics, medicine, etc.), a semantic field is a more specific linguistic notion that refers to a set of lexemes utilised to describe a domain. For example, a semantic field might be formed by terms train, bus, trip, fare, delay, accident, etc., which are all connected to the underlying term of transportation, and commonly used to generate observations on the domain of mobility.\nTerms appear to be semantically related to the degree to which they belong to the same semantic field, and can indeed belong to different semantic fields. Semantic fields are neither static nor well-defined sets, but rather fuzzy configurations that shift over time, and across different agents and information communities. The condition of specifiability of relations emphasises the fact that random co-occurrence has no impact on semantic relatedness. If a relation is not specifiable, the co-occurrence of the two terms must be random. A term has a certain degree of centrality in a semantic field, i.e. the density of connectedness with other terms. For example, in the aforementioned semantic field on transportation, car is more central than delay. Similarly, in a semantic field on social life, car is likely to be less central than restaurant or pub.\nGeo-semantic relatedness can therefore be defined as a specific sub-domain of semantic relatedness, focusing on relations grounded in the geographic dimension, i.e. relations in which at least one of the terms has a spatial dimension. Examples of geo-semantically related terms are judge, trial, and tribunal, where tribunal has a strong geographic component that grounds the other terms geographically. A computational measure of geo-semantic relatedness has to aggregate and quantify the intensity of such relations between two terms, providing a useful tool for several complex tasks. For example, terms river and flood should be more geo-semantically related than vehicle and car, which possess a less prominent geographic component. Acknowledging the fact that most terms in natural language have some degree of geographic ground, we express this approach to geo-semantic relatedness following Tobler\u2019s first law of geography:\nEvery term is geo-semantically related to all other terms, but terms that co-occur with specifiable geographic relations are more related than other terms.\nIn other words, every term can in principle have some degree of geo-semantic relatedness to any other term, but terms that co-occur in observations bearing specifiable relations tend to be more geo-semantically related than those that do not. This formulation puts terms in relation to human spatial experience from which terms arise, suggesting indistinct, gradual, and shifting boundaries between geo-related and unrelated terms.\nIn this sense, geo-semantic relatedness is intrinsically fuzzy, admitting a continuous spectrum of relatedness rather than a binary classification (i.e. related or unrelated). Highly related terms belong to the same semantic field. The same terms can belong to several overlapping semantic fields. Relatedness involves all semantic relations, including synonymy, antonymy, hyponymy, hypernymy, holonymy, meronymy, causality, temporal contiguity, function, proximity, and containment. This law applies both to natural language, where geographic terms can be highly imprecise and vague, and to scientific conceptualisations, which generally aim at stricter semantics.\nSurprisingly, in GIScience semantic relatedness has been almost completely ignored, with two notable exceptions [16, 17]. In order to explore semantically and spatially related entities in Wikipedia, Hecht and Raubal [16] developed ExploSR, a graph-based relatedness measure. ExploSR computes a semantic relatedness score of two articles by assigning weights to spatially-referenced articles in the Wikipedia Article Graph. More recently, the Atlasify system generates human-readable explanations of the relationship between terms to support exploratory search [17].\nGeo-semantic relatedness can be informed by ideas developed in the area of text mining. The latent Dirichlet allocation (LDA) adopts a probabilistic approach to cluster highly semantically-related terms in a text corpus [8]. LDA was extended to include a geographic dimension into the Location Aware Topic Model (LATM) [55]. LATM quantifies the geo-semantic relatedness between keywords, topics, and geographic locations, adopting a fully distributional approach.\n2.2 Geo-semantic similarity\nWhile geo-semantic relatedness of terms can be based on co-occurrence in observations, geo-semantic similarity of terms can only be determined through the analysis of the terms\u2019 attributes and relations. Geo-semantic similarity is a subset of geo-semantic relatedness: all similar terms are also related, but related terms are not necessarily similar. The relations considered for geosemantic similarity include only synonymy, hyponymy, and hypernymy. Unlike geo-semantic relatedness, geo-semantic similarity has been deeply explored by the GIScience community, and is recognised as one of the key concepts of geo-semantics [29].\nSeveral theories of similarity have been used to conceptualise and measure geo-semantic similarity, including featural, transformational, geometric, and alignment models [47, 22, 23, 48]. Specific techniques have been devised for\nspecific knowledge-representation formalisms [20, 44]. More recently, graphbased [5] and lexical techniques [3, 4] have been investigated in the emerging area of volunteered geographic information (VGI). These works tend to focus on the conceptual level, computing the similarity of abstract geographic terms (e.g. city and river), rather than the instance level (e.g. New York and Danube).\nBeyond the specificities of such approaches, we can state that terms A and B are semantically similar with respect to C, where C is a set of attributes and relations, also known as context [26]. The context C focuses on the typical spatial organisation and appearance of the entity identified by the term (e.g. shape, size, material composition). Alternatively, the similarity of A and B can be measured with respect to their affordances, i.e. the possibilities that an entity offers to humans [19].\nAs observed in relation to geo-semantic relatedness, all terms can be geosemantically similar to some limited extent, and geo-semantic similarity is therefore best modelled as a continuous spectrum, rather than a binary classification. For example, terms restaurant and continent are similar with respect to the fact that they both refer to geographically-grounded entities. To capture this idea at the linguistic level that is relevant to this discussion, we adopt the approach outlined in [4]. Considering the terms used in lexical definitions of terms, we state recursively that:\nAll terms are geo-semantically similar, but geographic terms described using the same terms are more similar than other terms.\nA geo-semantic similarity measure has to quantify the similarity of two terms into a score, enabling a number of semantic tasks in IR and information integration. For example, terms restaurant and pub are very similar because they share similar spatial organisation and affordances. Houses and schools are geo-semantically similar with respect to their spatial organisation of parts and can be described as having walls, windows, doors, a roof, etc. Roads and rivers show similar affordances \u2013 they can be used for transportation."}, {"heading": "3 Semantic relatedness and similarity gold standards", "text": "Semantic similarity and relatedness measures can be evaluated against a humangenerated set of psychological judgements. This section gives an overview of published similarity and relatedness gold standards, mostly from psychology and computational linguistics. The term \u2018gold standard\u2019 is described by the Oxford Dictionary of English as \u201ca thing of superior quality which serves as a point of reference against which other things of its type may be compared.\u201d2 In computer science, the term is used to describe high-quality, human-generated datasets, capturing human behaviour in relation to a well-defined task. Such datasets can then be used to assess the performance of automatic approaches,\n2 http://oxforddictionaries.com/definition/gold+standard (acc. Apr 10, 2013)\nby quantifying the correlation between the machine and the human-generated data.\n3.1 Cognitive plausibility\nIn a seminal discussion on expert systems, Strube [50] argued that knowledge engineering should strive towards increasing the cognitive adequacy of computational systems, defined as their \u2018degree of nearness to human cognition\u2019 (p. 165). In the context of GIScience, geo-relatedness or geo-similarity measures need not replicate the workings of human mind in their entirety (defined as absolutely strong adequacy), but should aim at what Strube called relatively strong adequacy, i.e. the ability of the system to function like a human expert in a circumscribed domain. Following this approach, we adopt the notion of cognitive plausibility to assess to what degree a measure mimics human behaviour [27].\nIn order to quantify the cognitive plausibility of a computational semantic relatedness or similarity measure, two complementary approaches can be adopted: (1) psychological evaluations, and (2) task-based evaluations. In psychological evaluations, human subjects are asked to rank or rate term pairs. These rankings or ratings are then compared with computer-generated rankings, usually using correlation as an indicator of performance. Alternatively, human subjects can perform a task based on the assessment of relatedness or similarity, such as word sense disambiguation, and the cognitive plausibility of the measure is observed indirectly in the results of the task, using for example precision and recall measures. Such human-generated datasets are used as gold standards.\nThe usage of gold standards is common in natural language processing tasks, such as part-of-speech tagging, entity resolution, and word sense disambiguation [46, 52, 10, 38]. Adopting this approach, a technique or a model can be deemed to be more or less plausible by observing its correlation with human-generated results. Such datasets are created by combining the results from a number of human subjects who perform a given task, either under controlled conditions, or through online forms. To be considered valid by a research community, a gold standard needs to meet certain criteria, such as coverage, quality, precision, and inter-subject agreement. Disagreements about the validity of a gold standard are quite common and, when weaknesses are uncovered, a gold standard can be demoted to a golden calf [e.g. 24].\nThe intrinsic high subjectivity of relatedness and similarity rankings makes the collection and validation of gold standards complex and challenging. Although task-based evaluations might appear more \u2018objective,\u2019 they are equally affected by subjectivity: ultimately, relatedness-based or similarity-based tasks are generated, interpreted, and validated by human subjects. Acknowledging the unlikelihood of total agreement, the reliability of a similarity evaluation should be grounded in stability over time, consistency across different datasets, and reproducibility of psychological results. Ideally, both evaluation\napproaches should show convergent, cross-validating results: a strong correlation is expected between the cognitive plausibility of a measure and its performance in similarity-based tasks.\n3.2 Comparison of relatedness and similarity gold standards\nOver the past 50 years, several authors investigating semantic issues in psychology, linguistics, and computer science created datasets focused on semantic similarity and, more recently, semantic relatedness. The first similarity gold standard was published in 1965, in a article in which Rubenstein and Goodenough [45] collected a set of 65 word pairs ranked by their synonymy. Following a similar line of research, Miller and Charles [33] published a similar dataset with 30 word pairs in 1991. More recently, Finkelstein et al. [13] created the WordSimilarity-353 dataset, which contains 353 word pairs actually ranked by semantic relatedness.3 The dataset was subsequently extended to distinguish between similarity and relatedness [1].4 In a study of the retrieval mechanism of memories, Nelson et al. [37] collected associative similarity ratings for 1,016 word pairs.\nA smaller number of geo-semantic similarity datasets have been generated in the areas of GIScience and geographic information retrieval (GIR). In this area, Janowicz et al. [21] conducted a study on the cognitive plausibility of their Sim-DL similarity measure. However, the study was conducted in German on a very small set of terms, and for this reason it is difficult to reuse in different contexts. In order to evaluate their Matching-Distance Similarity Measure (MDSM), Rodr\u0301\u0131guez and Egenhofer [44] collected similarity judgements about geographic terms, including large natural entities (e.g. mountain and forest), and man-made features (e.g. bridge and house). Before GeReSiD, the MDSM evaluation dataset was the largest similarity gold standard for geographic terms. For this reason, this dataset was utilised to carry out the evaluation of network-based similarity measures [5]. In contrast, geo-semantic relatedness has been largely ignored in the geospatial domain.\nThe salient characteristics of these gold standards are summarised in Table 1, detailing their human subjects, the terms and term pairs. For each dataset, the table shows whether they focus on semantic relatedness (rel), semantic similarity (sim), and exclusively on the geographic domain (geo). The existing datasets are compared with GeReSiD, the gold standard described in Section 4, and have several limitations. First, the procedure followed to construct the datasets is usually only sketched and not described in detail. Second, the size of the datasets tends to be rather small.\nThe size of such datasets can be observed along three dimensions: number of human subjects, number of terms, and number of term pairs. A clear trade-off exists between number of human subjects and number of term pairs.\n3 http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353 (acc. Apr 10, 2013)\n4 http://alfonseca.org/eng/research/wordsim353.html (acc. Apr 10, 2013)\nR eferen ce S u b jects T erm s & term p a irs\nr e l\nsim g e o\nR u b en stein a n d G o o d en o u g h\n[4 5 ]\n5 1 p a id co lleg e u n d erg ra d s: g ro u p I (1 5 su b jects), g ro u p II (3 6 su b jects).\n4 8 term s (o rd in a ry E n g lish w o rd s); 6 5 term p a irs, ra n g in g fro m h ig h ly sy n o n y m o u s p a irs to sem a n tica lly u n rela ted p a irs.\n\u221a\nM iller a n d C h a rles\n[3 3 ]\n3 8 u n d erg ra d u a te stu d en ts. U S E n g lish n a tiv e sp ea k ers.\n4 0 term s selected fro m R u b en stein a n d G o o d en o u g h [4 5 ]; 3 2 term p a irs.\n\u221a\nF in k elstein et a l. [1 3 ], A g irre et a l. [1 ]\n1 3 ex p erts fo r fi rst set (1 5 3 p a irs), 1 6 ex p erts fo r seco\nn d set (2 0 0 p a irs). N ea r-n a tiv e E n g lish p ro fi cien cy.\n3 4 6 term s (m a n u a lly selected n o u n s a n d co m p o u n d n o u n s); 3 5 3 term p\na irs.\n\u221a \u221a\nR o d r\u0301\u0131g u ez a n d E g en h o fer [4 4 ]\n7 2 p a id u n d erg ra d stu d en ts (tw o g ro u p s o f 3 6 p eo p le). U S E n g lish n a tiv e sp ea k ers.\n3 3 g eo g ra p h ic term s fro m W o rd N et a n d S D T S ; 1 0 sets o f 1 0 o r 1 1 term p a irs.\n\u221a \u221a\nN elso n et a l. [3 7 ]\n9 4 u n d erg ra d u a te stu d en ts rew a rd ed w ith a ca d em\nic cred its 1 ,0\n1 6 term p a irs selected u n sy stem a tica lly fro m a cu ed reca ll d a ta b a se o f 2 ,0 0 0 + p a irs.\n\u221a\nJ a n o w icz et a l. [2 1 ]\n2 8 u n p a id su b jects (2 0 -3 0 y ea rs o f a g e).\nS ix g eo g ra p h ic term s rela ted to b o d ies o f w a ter.\n\u221a \u221a\nG e R e S iD (see S ectio n 4 )\n2 0 3 u n p a id E n g lish n a tiv e sp ea k ers.\n9 7 g eo g ra p h ic term s fro m O p en S treetM a p ; 5 0\nterm p a irs.\n\u221a \u221a\n\u221a\nT a b le 1 S em a n tic rela ted n ess a n d sim ila rity g o ld sta n d a rd s\nFurthermore, most datasets do not capture the distinction between semantic similarity and relatedness, and do not analyse the IRA and IRR. It is important to note that most authors did not have the explicit intention to construct gold standards, but rather to analyse specific aspects of semantic similarity or relatedness. However, in some cases, these datasets have been treated as gold standards in the subsequent literature [45, 33]. To the best of our knowledge, only WordSimilarity-353 was explicitly designed to be a generic gold standard.\nSome of these gold standards have been extensively utilised to assess general term-to-term similarity measures [45, 33, 13]. In the geographic context, only the MDSM evaluation dataset is suitable to evaluate semantic similarity of geographic terms [44]. However, no existing dataset focusing on geographic terms accounts explicitly for the difference between semantic relatedness and semantic similarity."}, {"heading": "4 Geo Relatedness and Similarity Dataset (GeReSiD)", "text": "This section presents the Geo Relatedness and Similarity Dataset (GeReSiD), a dataset of human judgements that we have developed to provide a ground truth for the assessment of computational relatedness and similarity measurements. GeReSiD captures explicitly the difference between geo-semantic relatedness and similarity on a sample of geographic terms larger than existing similarity datasets surveyed in Section 3, including both natural and man-made terms. In order to ensure its validity as a gold standard, it focuses on a sample of evenly distributed relatedness/similarity judgements, ranging from very high to very low. Section 4.1 describes our methodology precisely, in order to provide guidelines on constructing datasets to ground the evaluation of measures of geo-semantic relatedness and similarity. Subsequently, Section 4.2 outlines the results obtained from the online survey.\n4.1 Survey design\nThe psychological judgements about geo-semantic relatedness and similarity were collected via an online survey, through an interactive Web interface specifically designed for this purpose. Online surveys constitute a powerful research tool, with well-known advantages and disadvantages [57]. Given the focus of this study on generic terms found in web maps, subjects involved in projects such as OpenStreetMap represent an ideal virtual community of map users and producers to conduct a psychological evaluation. An online survey is an inexpensive and effective way to reach these online communities.\nA cross-disciplinary consensus exists on the fact that semantic judgements are affected by the context in which the terms are considered [44, 22]. Rodr\u0301\u0131guez and Egenhofer [44] asked their subjects to rank geographic terms in the following contexts: \u2018null context,\u2019 \u2018play a sport,\u2019 \u2018compare constructions,\u2019 and \u2018compare transportation systems.\u2019 The subjects\u2019 attention was therefore focused on\nspecific aspects of the terms being analysed, rather than on the terms in an unspecified setting.\nAlthough context affects the assessment of semantic similarity, in this survey we aim at capturing the overall difference between semantic relatedness and similarity of terms, without focusing on specific aspects of the conceptualisation. This comparison is an important research topic, frequently mentioned but rarely addressed directly through empirical evaluation. Introducing specific contexts into our survey would increase the complexity of the study by introducing new biases, making the direct comparison between similarity and relatedness problematic. For example, adding a specific context does not increase the inter-subject agreement: in their evaluation, Rodr\u0301\u0131guez and Egenhofer [44] report a considerably lower association between subjects in the case of context-specific questions (mean Kendall\u2019s W being .5), than with a-contextual questions (mean W = .68). Moreover, specific contexts would introduce specific biases, which are beyond the scope of Geo Relatedness and Similarity Dataset (GeReSiD).\nAs a solution to these issues, we frame the evaluation in the general context of popular web maps, in which geographic terms are most frequently visualised and utilised by users. This way, the subjects are induced to use their own conceptualisation of the geographic entities. As happens with semantic judgements, subjectivity inevitably affects the subjects\u2019 choices. In this study, subjects are free to choose what properties they consider most relevant to the comparison, and the mean of their ratings quantifies the perceived intersubject similarity and relatedness of the terms. While the study of the context is beyond the scope of this survey, it certainly represents an important direction for future work.\nThe geographic terms included in this survey are taken from the OpenStreetMap project. In our previous work, we extracted the lexicon utilised in OpenStreetMap into a machine-readable vocabulary, the OSM Semantic Network [6]. To date, the OSM Semantic Network contains a total of about 4,300 distinct terms, called \u2018tags\u2019 in the project\u2019s terminology. From this large set of geographic terms, a suitable sample had to be selected. To be included, a term had to be clearly intelligible, well defined on the OSM Semantic Network, as culturally-unspecific as possible, and present in the actual OpenStreetMap vector map. Following these criteria, we manually selected a set C of 400 terms, including a wide range of natural and man-made entities, such as \u2018sea,\u2019 \u2018lighthouse,\u2019 \u2018landfill,\u2019 \u2018valley,\u2019 and \u2018glacier.\u2019 Using the terms in C, we defined a set P containing all possible pairs of geographic terms \u3008a, b\u3009 where a, b \u2208 C, for a total of 160,000 pairs. We subsequently removed from P symmetric pairs (e.g. removing \u3008b, a\u3009 when \u3008a, b\u3009 is defined) and identities (e.g. \u3008a, a\u3009), resulting in 76,000 valid pairs.\nIn order to detect issues in the survey, a pilot study was then conducted with 12 graduate students at University College Dublin. A set Prand was constructed by selecting 100 pairs randomly from P . Each pair was associated with a 5-point Likert scale, ranging from low to high relatedness/similarity. The subjects were asked to rate each pair both for semantic relatedness and\nsimilarity, and were then interviewed informally, to obtain direct feedback about the survey. Several useful observations were obtained from this pilot survey. First, most subjects found the test too long. A smaller sample size had to be selected, considering a trade-off between number of pairs and the completion time, in order to ensure that enough subjects would complete the task without losing concentration. Based on the opinion of subjects, we identified 50 pairs as the maximum size of the task, with a completion time of around five minutes, suitable for an unpaid online questionnaire.\nIn the OpenStreetMap semantic model, tags are made of a key and a value (e.g. amenity=school). In the pilot survey, this formalism had to be explained to the subjects, who generally found it confusing. For example, the psychological comparison between amenity=school and amenity=community centre was influenced by the shared word \u2018amenity,\u2019 which is highly generic and ambiguous. To make the dataset independent from the peculiar OpenStreetMap tag structure, we extracted short labels for all the 400 terms from the terms\u2019 definitions. For example, amenity=food court was labelled as \u2018food court,\u2019 shop=music as \u2018music shop.\u2019 In order to increase their semantic clarity, the terms were manually mapped to the corresponding terms in WordNet (see Table 2).\nThe fully random set of 100 pairs Prand used in the pilot survey obtained a distribution heavily skewed towards low similarity and relatedness. To reach a more uniform distribution, we introduced a partial manual selection in the process. In order to obtain an even distribution in the resulting relatedness and similarity scores, we manually extracted from the pilot survey a set of 50 pairs rated by the 12 subjects as highly related/similar pairs (Phigh), and 50 middle relatedness/similarity pairs (Pmed). It is worth noting that while the selection of highly related/similar pairs is intuitive, middle-relatedness/similarity pairs is more challenging, and requires dealing with highly subjective conceptualisations. This aspect is reflected in the survey results (see Section 4.2). The final set of 50 pairs for the questionnaire Pq was assembled from the following elements:\n\u2013 16 high-relatedness/similarity pairs (random sample from Phigh) \u2013 18 middle-relatedness/similarity pairs (random sample from Pmed) \u2013 16 low-relatedness/similarity pairs (random sample from P )\nThe pilot survey also showed clearly that assigning both the relatedness and the similarity tasks to the same subject was impractical, and was deemed confusing by all subjects who did not possess specific expertise in linguistics. For this reason, we opted to assign randomly only one task to each subject, either on relatedness or similarity, without trying to explain to them the technicalities of this distinction. Instead, we relied on the subjects\u2019 inductive understanding of the task through correct examples. Thus, in order to collect reliable judgements on similarity and relatedness, we defined two separate questionnaires, one on relatedness (QREL), and one on similarity (QSIM ). The two questionnaires were identical, with the exception of the description of the task, and\nthe labels on the Likert scale (one with a \u2018dissimilar-similar\u2019 scale, the other with \u2018unrelated-related\u2019).\nTo avoid terminological confusion, the survey was named \u2018Survey on comparison of geographic terms,\u2019 without mentioning either \u2018similarity\u2019 or \u2018relatedness\u2019 in the introductory text. The examples used to illustrate semantic relatedness (apples - bananas, doctor - hospital, tree - shade) and similarity (apples - bananas, doctor - surgeon, car - motorcycle) were based on those by Mohammad and Hirst [34]. A random redirection to either QREL or QSIM was then implemented to ensure the random sampling of subjects into two groups, one for similarity and one for relatedness. As the similarity judgement was reported as more difficult than relatedness, we set the probability of a random redirection to QSIM at p = .65, to obtain more responders for similarity. Each subject was only exposed to one of the two questionnaires.\nSix general demographic questions about the subject were included: age group, mother tongue, gender, and continent of origin. A textbox was available to type feedback and comments about the survey. The core of each questionnaire was the seventh question, i.e. the relatedness or similarity rating task. The subject had to rate 50 pairs of geographic terms based on their relatedness or similarity, on a 1 to 5 Likert scale. Although the impact of size of the Likert scale, typical options being 5, 7 or 10, is debated in the social sciences, it has little impact on the rating means [11]. If the terms were not clear to the user, a \u2018no answer\u2019 option had to be selected.\nAnother aspect discussed in the similarity psychological literature is the counterintuitive fact that similarity judgements tend to be asymmetric (e.g. sim(building, hospital) 6= sim(hospital, building)) [54]. As this aspect is outside the scope of this study, the order in each pair \u3008a, b\u3009 was randomised to limit the symmetric bias, i.e. the potential difference between sim(a, b) and sim(b, a) from the subject\u2019s perspective. Moreover, a fixed presentation order of pairs can trigger specific semantic associations between terms, and would reduce the quality of the last pairs, rated when the subjects are more likely to be tired. To reduce this sequential-ordering bias, the presentation order of\nthe pairs was randomised automatically for each subject at the Web interface level.\nAt the end of the design process, the survey dataset contained 50 pairs of geographic terms to be rated on 5-point Likert scales, including 97 OpenStreetMap terms, with three terms being repeated twice. The pairs were selected to ensure an even distribution between low, medium and high relatedness/similarity. The rating was to be executed in two independent questionnaires, one for semantic similarity (QSIM ) and one for semantic relatedness (QREL), randomly assigned to the human subjects. In February 2012, the survey was disseminated in OpenStreetMap and geographic information system (GIS)-related forums and mailing lists.\n4.2 Survey results\nThe online questionnaires on relatedness and similarity received 305 responses, 124 for relatedness and 181 for similarity. Given the nature of online surveys, particular attention has to be paid to the agreement between the human subjects, and the detection of unreliable and random answers. In this survey, raters expressed quantitative judgements on geo-semantic relatedness and similarity on a 5-point Likert scale. Two important statistical aspects to be discussed are the interrater reliability (IRR) and the interrater agreement (IRA) [30]. IRR considers the relative similarity in ratings provided by multiple raters (i.e. the human subjects) over multiple targets (i.e. the term pairs), focusing on the order of the targets. IRA, on the other hand, captures the absolute homogeneity between the ratings, looking at the specific rating chosen by raters.\nSeveral indices have been devised to capture IRR and IRA in psychological surveys [7, 30]. Most indices range between 0 (total disagreement) and 1 (perfect agreement). For example, the ratings of two raters on three targets {1, 2, 3} and {2, 3, 4} obtain a IRR = 1 and IRA = 0: the subjects agree perfectly on the ordering of the targets, while disagreeing on all absolute ratings. LeBreton and Senter [30] recommend using several indices for IRR and IRA, to avoid the bias of any single index. We thus include the following indices of IRA and IRR: the mean Pearson\u2019s correlation coefficient [43]; Kendall\u2019s W [25]; Robinson\u2019s A [42]; Finn coefficient [14]; James, Demaree and Wolf\u2019s rWG(J) [18].\nThe 305 responders included both native (208) and non-native English speakers (97). We observed a substantially lower inter-subject agreement when including non-native speakers (rWG(J) < .5): the wider variability in these results is due to the varying knowledge of English of these subjects, who might have associated terms to \u2018false friends\u2019 in their native language, i.e. expressions in two different languages that look or sound similar, but differ considerably in meaning. For example, Italian speakers may confuse the meaning of \u2018factory\u2019 with \u2018farm\u2019 (\u2018fattoria\u2019 in Italian). Hence, they were excluded from the dataset. Furthermore, three subjects did not complete the task, and their responses were discarded. In order to detect random answers, we computed the\ncorrelation between every individual subject and the means. This way, two subjects in the similarity test showed no correlation at all with the mean ratings (Spearman\u2019s \u03c1 \u2208 [\u2212.05, .05]), and were removed from the dataset.\nOf the resulting dataset, Table 3 summarises demographic information (age group, gender, continent of origin, and self-assessed map expertise). As is possible to observe, the subjects tend be young, male, European, and frequent users of web maps.5 Table 4 focuses on the indices of IRR and IRA. Following Resnik [41], we consider upper bound on the cognitive plausibility of a computable measure to be the highest correlation obtained by a human rater with the means (e.g. \u03c1 = .92 for relatedness). The table shows these upper bounds both for Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4 . All the IRR and IRA indices indicate very similar results, falling in the range [.61, .67]. Given the highly subjective nature of semantic conceptualisations, this correlation is satisfactory, and is comparable with analogous psychological surveys [44].\nGiven the set of term pairs, and the set of human raters, we computed the relatedness/similarity scores as the mean ratings, normalised in the interval [0, 1], where 0 means no relatedness/similarity, and 1 maximum relatedness/similarity. As we have stated in the survey objectives, the distribution of such scores should be as even as possible, to ensure that a semantic measure performs well across the board, and not only in a specific region of the semantic relatedness/similarity space. Several pairs in the dataset contain related but not similar terms, and the scores confirm this difference. More specifically, <sea,island> obtained a relatedness score of .74 and a similarity of .4. Sim-\n5 Although a better gender, age, and geographic balances would be desirable, we found it difficult to obtain it in practice without drastically limiting the size of the sample.\nilarly, <mountain hut, mountaintop> obtained respectively .71 and .49 for relatedness and similarity.\nA dimension that has not been addressed in existing similarity gold standards is that of the pair agreement, i.e. the consistency of ratings expressed by all subjects on a single pair (see Section 3). For this purpose, we adopt James, Demaree and Wolf\u2019s rWG, a popular index to measure IRA on a single target, based on the rating variance [18]. Each pair in QREL and QSIM obtains an agreement measure \u2208 [0, 1], where 0 indicates a squared distribution (i.e. raters gave all ratings in equal proportion), and 1 is perfect agreement (i.e. all raters assigned exactly the same rating to the pair). Table 5 shows the content of the resulting dataset, including mean ratings and pair agreement.\nFigures 1 and 2 show several statistical characteristics of the resulting dataset, for the 50 pairs in QREL and QSIM . Plot 1(a) shows the density of the final relatedness/similarity scores, i.e. the normalised mean rankings in range [0, 1]. While the similarity is skewed towards the range [0, .4], the relatedness has slightly more scores in the range [.4, 1], resulting in symmetrical densities. This clearly reflects the fact that semantic similarity is a specific type of semantic relatedness, and semantic similarity is generally lower than relatedness. This can be also observed in the sum of the 50 relatedness scores\n(sum = 22.01,mean = .44) against the similarity scores (sum = 19.5,mean = .39). The paired Wilcoxon signed rank test [56] indicates that the relatedness scores are higher than the corresponding similarity ones, at p < .001. This trend is clearly visible in plot 1(b). Overall, these densities show that all the score range [0, 1] is satisfactorily covered, i.e. the dataset does not show large gaps.\nPlots 2(a) and 2(b) show the properties of pair agreement (index rWG), reporting the relationship between relatedness and similarity, the density of pair agreement, and the relationship between pair agreement and relatedness/similarity scores. In terms of pair agreement, relatedness and similarity follow very close patterns, with a peak \u2248 .5. This agreement might seem low, but it is largely expected, due to the subjective interpretation of the values on the Likert scale.\nAn explanation of this trend in the pair agreement lies in the fact that humans give consistently different ratings to the same objects: some subjects tend to be strict, and some lenient, resulting in different relative ratings, and therefore low absolute pair agreement [30]. In this regard, a clear pattern emerges from plot 1(b). Pair agreement tends to be high (> .7) at the extremes of the scores, when the relatedness/similarity judgement is very low ([0, .25), no relation) or very high ((.75, 1], strong relation). On the other hand, pairs with middle scores (in the interval [.25, .75]) tend to have low pair agreement. Relatedness and similarity do not show important differences with respect to pair agreement (sum = 30,mean = .6 for relatedness, sum = 29.6,mean = .59 for similarity). This detailed analysis, in particular in relation to IRR and IRA, confirms the statistical soundness of GeReSiD, which can be used to assess the cognitive plausibility of computational measures of geo-semantic relatedness and similarity."}, {"heading": "5 Conclusions", "text": "To date, despite its great potential in GIR and information integration, geosemantic relatedness has been only marginally studied. In this article, we have discussed a notion of geo-semantic relatedness based on Lehrer\u2019s theory of semantic fields, contrasting it with the widely studied geo-semantic similarity. Despite the variety and importance of computational measures devised in natural language processing, the evaluation of such measures remains a difficult and complex task [12].\nIn order to provide an evaluative baseline for geo-semantic research on relatedness and similarity, we have designed, collected, and analysed the Geo Relatedness and Similarity Dataset (GeReSiD). This dataset contains human judgements about 50 term pairs on semantic relatedness and similarity, covering 97 unique geographic terms. To increase the dataset\u2019s usability and clarity, the terms have been mapped to the corresponding terms in WordNet. The judgements were collected from 203 English native speakers, through a randomised online survey. GeReSiD is freely available online, released under an Open Knowledge license.6 The following points deserve particular consideration:\n\u2013 The human judgements have interrater agreement (IRA) and interrater reliability (IRR) in the interval [.61, .67]. Considering the type of psychological test, this is a fair agreement, indicating that the dataset can be used to evaluate computational measures of semantic relatedness and similarity for geographic terms. \u2013 Human subjects strongly agree on cases of very high and low semantic relationships, and tend to have lower agreement on the intermediate cases. \u2013 Semantic relatedness and similarity are strongly correlated (\u03c4 = .84, \u03c1 = .95). Furthermore, semantic relatedness scores are consistently higher than\n6 http://github.com/ucd-spatial/Datasets (acc. Apr 10, 2013)\nsemantic similarity, confirming the more specific nature of semantic similarity. \u2013 The contribution of GeReSiD lies both in its design and validation methodology, as well as the dataset itself. The raw data and the resulting dataset are available for analysis and re-use under a Creative Commons license. \u2013 GeReSiD constitutes an evaluative baseline to evaluate measures of semantic similarity and relatedness. Furthermore, it permits the empirical determination of whether a given measure better approximates similarity or relatedness through the direct comparison of rankings or scores. \u2013 A variety of techniques can be used to compare the rankings or scores generated by a computational measure with GeReSiD, including correlation coefficients (Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4), and categorical approaches (Cohen\u2019s kappa or Fisher\u2019s exact test).\nAlthough GeReSiD provides a novel resource to evaluate computational measures of geo-semantic relatedness and similarity, several questions remain open. GeReSiD distinguishes between geo-semantic relatedness and similarity, but not among different contexts. As context has been identified as a key aspect of semantic similarity [26], new datasets should be generated to capture explicitly the differences in geo-similarity and relatedness judgements with respect to different contexts, such as appearance and affordances. The investigation of what specific geographic aspects are used by subjects in their judgements also constitutes important future work. The dataset\u2019s IRA and IRR are comparable to similar datasets, but have a large margin of improvement.\nAs Ferrara and Tasso [12] point out, this evaluative approach has several limitations. Human subjects understand intuitively semantic relatedness and similarity, but the translation of such judgements into a number is very subjective. Different information communities can express different judgements on the same term pairs. Alternative approaches to the evaluation of computational measures should be investigated, aiming at cross-validating the findings generated by GeReSiD. A promising route might consist of evaluating humanreadable explanations of relatedness measures, and not only numeric scores or rankings [17]. Moreover, the collection of judgements was conducted through online surveys in an uncontrolled environment, which have well-known issues [57].\nUltimately, the cognitive plausibility is assessed using correlation indexes such as Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4 , which have specific limitations. For example, they tend to attribute the same weight to high and low similarity rankings, whilst computational applications normally need more precision on highly-related/similar pairs, which tend to be utilised in GIR and information integration. Using GeReSiD as input data, new techniques to assess cognitive plausibility can be developed, offering tools tailored to the study of geo-semantic relatedness and similarity. Fruitful future work, as geo-semantic similarity is a specific case of geo-semantic relatedness, will consist of the generalisation of existing geo-similarity theories to the framework of geo-semantic relatedness.\nAcknowledgements The research presented in this article was funded by a Strategic Research Cluster grant (07/SRC/I1168) by Science Foundation Ireland under the National Development Plan. The authors gratefully acknowledge this support."}], "references": [{"title": "A study on similarity and relatedness using distributional and WordNet-based approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pa\u015fca", "A. Soroa"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 19\u201327),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "SIM-NET: A View-Based Semantic Similarity Model for Ad Hoc Networks of Geospatial Databases", "author": ["M. Bakillah", "Y. B\u00e9dard", "M. Mostafavi", "J. Brodeur"], "venue": "Transactions in GIS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The Similarity Jury: Combining expert judgements on geographic concepts", "author": ["A. Ballatore", "D. Wilson", "M. Bertolotto"], "venue": "Advances in Conceptual Modeling. ER 2012 Workshops (SeCoGIS), LNCS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Computing the Semantic Similarity of Geographic Terms Using Volunteered Lexical Definitions", "author": ["A. Ballatore", "M. Bertolotto", "D. Wilson"], "venue": "International Journal of Geographical Information Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Geographic Knowledge Extraction and Semantic Similarity in OpenStreetMap", "author": ["A. Ballatore", "M. Bertolotto", "D. Wilson"], "venue": "Knowledge and Information Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A Survey of Volunteered Open Geo-Knowledge Bases in the Semantic Web", "author": ["A. Ballatore", "D. Wilson", "M. Bertolotto"], "venue": "Quality Issues in the Management of Web Information, Intelligent Systems Reference Library,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Beyond Kappa: A review of interrater agreement measures", "author": ["M. Banerjee", "M. Capozzoli", "L. McSweeney", "D. Sinha"], "venue": "Canadian Journal of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness", "author": ["A. Budanitsky", "G. Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Towards large-scale, open-domain and ontology-based named entity classification", "author": ["P. Cimiano", "J. V\u00f6lker"], "venue": "Recent Advances in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Do data characteristics change according to the number of scale points used", "author": ["J. Dawes"], "venue": "International Journal of Market Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Evaluating the Results of Methods for Computing Semantic Relatedness", "author": ["F. Ferrara", "C. Tasso"], "venue": "Computational Lin 22  Andrea Ballatore et al. guistics and Intelligent Text Processing, LNCS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "A Note on Estimating the Reliability of Categorical Data", "author": ["R. Finn"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1970}, {"title": "GeoSR: Geographically Explore Semantic Relations in World Knowledge. In: The European Information Society: Taking Geoinformation Science One Step Further, LNGC", "author": ["B. Hecht", "M. Raubal"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Explanatory semantic relatedness and explicit spatialization for exploratory search", "author": ["B. Hecht", "S.H. Carton", "M. Quaderi", "J. Sch\u00f6ning", "M. Raubal", "D. Gergle", "D. Downey"], "venue": "Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 415\u2013424),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Estimating within-group interrater reliability with and without response bias", "author": ["L. James", "R. Demaree", "G. Wolf"], "venue": "Journal of applied psychology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1984}, {"title": "Affordance-based similarity measurement for entity types", "author": ["K. Janowicz", "M. Raubal"], "venue": "Spatial Information Theory, LNCS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Algorithm, implementation and application of the SIM-DL similarity server", "author": ["K. Janowicz", "C. Ke\u00dfler", "M. Schwarz", "M. Wilkes", "I. Panov", "M. Espeter", "B. B\u00e4umer"], "venue": "GeoSpatial Semantics: Second International Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "A study on the cognitive plausibility of SIM-DL similarity rankings for geographic feature types", "author": ["K. Janowicz", "C. Ke\u00dfler", "I. Panov", "M. Wilkes", "M. Espeter", "M. Schwarz"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Semantic Similarity Measurement and Geospatial Applications", "author": ["K. Janowicz", "M. Raubal", "A. Schwering", "W. Kuhn"], "venue": "Transactions in GIS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "The semantics of similarity in geographic information retrieval", "author": ["K. Janowicz", "M. Raubal", "W. Kuhn"], "venue": "Journal of Spatial Information Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "The double-blind, randomized, placebo-controlled trial: Gold standard or golden calf", "author": ["T. Kaptchuk"], "venue": "Journal of Clinical Epidemiology,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "The problem of m rankings. The annals of mathematical statistics", "author": ["M. Kendall", "B. Smith"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1939}, {"title": "Similarity measurement in context", "author": ["C. Ke\u00dfler"], "venue": "Proceedings of the 6th International and Interdisciplinary Conference on Modeling and Using Context (pp. 277\u2013290),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "What is the difference? A cognitive dissimilarity measure for information retrieval result sets", "author": ["C. Ke\u00dfler"], "venue": "Knowledge and Information Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Semantic relations in information science", "author": ["C. Khoo", "J. Na"], "venue": "Annual Review of Information Science and Technology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Cognitive and linguistic ideas and geographic information semantics. In: Cognitive and Linguistic Aspects of Geographic Space, LNGC", "author": ["W. Kuhn"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Answers to 20 questions about interrater reliability and interrater agreement", "author": ["J. LeBreton", "J. Senter"], "venue": "Organizational Research Methods,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The influence of semantic fields on semantic change", "author": ["A. Lehrer"], "venue": "Fisiak, J. (ed.) Historical Word Formation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1985}, {"title": "Similarity involving attributes and relations: Judgments of similarity and difference are not inverses", "author": ["D. Medin", "R. Goldstone", "D. Gentner"], "venue": "Psychological Science,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1990}, {"title": "Contextual correlates of semantic similarity", "author": ["G. Miller", "W. Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1991}, {"title": "Distributional Measures of Semantic Distance: A Survey", "author": ["S. Mohammad", "G. Hirst"], "venue": "Computing Research Repository (CoRR),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Testing the first law of cognitive geography on point-display spatializations", "author": ["D.R. Montello", "S.I. Fabrikant", "M. Ruocco", "R.S. Middleton"], "venue": "Spatial Information Theory. Foundations of Geographic Information Science, LNCS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Non-classical lexical semantic relations", "author": ["J. Morris", "G. Hirst"], "venue": "Proceedings of the HLT-NAACL Workshop on Computational Lexical Semantics (pp. 46\u201351),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "What is preexisting strength? Predicting free association probabilities, similarity ratings, and cued recall probabilities", "author": ["D. Nelson", "G. Dyrdal", "L. Goodmon"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Wordnet::senserelate::allwords: a broad coverage word sense tagger that maximizes semantic relatedness. In: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Demonstration Session (pp", "author": ["T. Pedersen", "V. Kolhatkar"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "WordNet::Similarity: measuring the relatedness of concepts. In: Proceedings of Human Language Technologies: The 2004 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi"], "venue": "Andrea Ballatore et al. Demonstration Session (pp. 38\u201341),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Development and application of a metric on semantic nets", "author": ["R. Rada", "H. Mili", "E. Bicknell", "M. Blettner"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1989}, {"title": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy", "author": ["P. Resnik"], "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1995}, {"title": "The statistical measurement of agreement", "author": ["W. Robinson"], "venue": "American Sociological Review,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1957}, {"title": "Thirteen ways to look at the correlation coefficient", "author": ["J. Rodgers", "W. Nicewander"], "venue": "American Statistician,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1988}, {"title": "Comparing Geospatial Entity Classes: An Asymmetric and Context-Dependent Similarity Measure", "author": ["M. Rod\u0155\u0131guez", "M. Egenhofer"], "venue": "International Journal of Geographical Information Science,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1965}, {"title": "Automatic word sense discrimination", "author": ["H. Sch\u00fctze"], "venue": "Computational linguistics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1998}, {"title": "Approaches to Semantic Similarity Measurement for Geo-Spatial Data: A Survey", "author": ["A. Schwering"], "venue": "Transactions in GIS,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "A hybrid semantic similarity measure for spatial information retrieval", "author": ["A. Schwering", "W. Kuhn"], "venue": "Spatial Cognition & Computation,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2009}, {"title": "Spatial relations for semantic similarity measurement. In: Perspectives in Conceptual Modeling (pp. 259\u2013269)", "author": ["A. Schwering", "M. Raubal"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2005}, {"title": "The role of cognitive science in knowledge engineering", "author": ["G. Strube"], "venue": "Contemporary knowledge engineering and cognition,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1992}, {"title": "A computer movie simulating urban growth in the Detroit region", "author": ["W. Tobler"], "venue": "Economic geography. Supplement: Proceedings. International Geographical Union. Commission on Quantitative Methods,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1970}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (pp", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2003}, {"title": "Similarity of semantic relations", "author": ["P. Turney"], "venue": "Computational Linguistics,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2006}, {"title": "Features of similarity", "author": ["A. Tversky"], "venue": "Psychological review,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1977}, {"title": "Mining geographic knowledge using location aware topic model", "author": ["C. Wang", "J. Wang", "X. Xie", "W.Y. Ma"], "venue": "Proceedings of the 4th ACM Workshop on Geographical Information Retrieval (pp. 65\u201370),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2007}, {"title": "Individual comparisons by ranking methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1945}, {"title": "Researching Internet-based populations: Advantages and disadvantages of online survey research, online questionnaire authoring software packages, and web survey services", "author": ["K. Wright"], "venue": "Journal of Computer-Mediated Communication,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "In geographic information science (GIScience), the theoretical and practical importance of geo-semantic similarity has been fully acknowledged, resulting in a growing body of research [2, 4, 23].", "startOffset": 184, "endOffset": 194}, {"referenceID": 3, "context": "In geographic information science (GIScience), the theoretical and practical importance of geo-semantic similarity has been fully acknowledged, resulting in a growing body of research [2, 4, 23].", "startOffset": 184, "endOffset": 194}, {"referenceID": 21, "context": "In geographic information science (GIScience), the theoretical and practical importance of geo-semantic similarity has been fully acknowledged, resulting in a growing body of research [2, 4, 23].", "startOffset": 184, "endOffset": 194}, {"referenceID": 14, "context": "and Raubal [16] and Hecht et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Despite the large number of existing measures, their rigorous evaluation still constitutes an important research challenge [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "Alternatively, the data can be interpreted as categorical, using Cohen\u2019s kappa or Fisher\u2019s exact test [7] to evaluate the computational measure.", "startOffset": 102, "endOffset": 105}, {"referenceID": 51, "context": "analogy, and attributional similarity [53].", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "These terms are often used interchangeably [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 37, "context": "A striking example of this tendency is the article title \u2018WordNet::Similarity: Measuring the relatedness of terms\u2019 [39].", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "Khoo and Na [28] have surveyed these semantic relations, whilst Morris and Hirst [36] have explored other non-classical semantic relations.", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "Khoo and Na [28] have surveyed these semantic relations, whilst Morris and Hirst [36] have explored other non-classical semantic relations.", "startOffset": 81, "endOffset": 85}, {"referenceID": 26, "context": "As Khoo and Na [28] remarked, semantic relations are characterised by productivity (new relations can be easily created), uncountability (semantic", "startOffset": 15, "endOffset": 19}, {"referenceID": 47, "context": "In the geographic domain, spatial relations such as proximity (A is near B), and containment (A is within B) have an impact on semantics [49].", "startOffset": 137, "endOffset": 141}, {"referenceID": 38, "context": "[40] suggested that semantic related-", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "Resnik [41] followed this approach, and defined semantic similarity and relatedness as follows: \u201cSemantic similarity represents a special case of semantic relatedness: for example, cars and gasoline would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar\u201d (p.", "startOffset": 7, "endOffset": 11}, {"referenceID": 51, "context": "More recently, Turney [53] added a further distinction between \u2018attribu-", "startOffset": 22, "endOffset": 26}, {"referenceID": 30, "context": "[32], \u2018attributes\u2019 are statements about a term that take only one parameter, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Hence, relational similarity is based on the common relations between two pairs of terms [53].", "startOffset": 89, "endOffset": 93}, {"referenceID": 49, "context": "A general notion of relatedness in the geographic context was stated in Tobler\u2019s first law, which asserts that everything is related to everything else, but near things are more related than distant things [51].", "startOffset": 206, "endOffset": 210}, {"referenceID": 33, "context": "[35] have proposed the first law of cognitive geography, which states that \u201cpeople believe closer things to be more similar than distant things\u201d (p.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "According to Lehrer [31], a semantic field is \u201ca set of lexemes which cover a certain conceptual domain and which bear certain specifiable relation to one another\u201d (p.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "Surprisingly, in GIScience semantic relatedness has been almost completely ignored, with two notable exceptions [16, 17].", "startOffset": 112, "endOffset": 120}, {"referenceID": 15, "context": "Surprisingly, in GIScience semantic relatedness has been almost completely ignored, with two notable exceptions [16, 17].", "startOffset": 112, "endOffset": 120}, {"referenceID": 14, "context": "In order to explore semantically and spatially related entities in Wikipedia, Hecht and Raubal [16] developed", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "More recently, the Atlasify system generates human-readable explanations of the relationship between terms to support exploratory search [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "The latent Dirichlet allocation (LDA) adopts a probabilistic approach to cluster highly semantically-related terms in a text corpus [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 53, "context": "Model (LATM) [55].", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "Unlike geo-semantic relatedness, geo-semantic similarity has been deeply explored by the GIScience community, and is recognised as one of the key concepts of geo-semantics [29].", "startOffset": 172, "endOffset": 176}, {"referenceID": 45, "context": "Several theories of similarity have been used to conceptualise and measure geo-semantic similarity, including featural, transformational, geometric, and alignment models [47, 22, 23, 48].", "startOffset": 170, "endOffset": 186}, {"referenceID": 20, "context": "Several theories of similarity have been used to conceptualise and measure geo-semantic similarity, including featural, transformational, geometric, and alignment models [47, 22, 23, 48].", "startOffset": 170, "endOffset": 186}, {"referenceID": 21, "context": "Several theories of similarity have been used to conceptualise and measure geo-semantic similarity, including featural, transformational, geometric, and alignment models [47, 22, 23, 48].", "startOffset": 170, "endOffset": 186}, {"referenceID": 46, "context": "Several theories of similarity have been used to conceptualise and measure geo-semantic similarity, including featural, transformational, geometric, and alignment models [47, 22, 23, 48].", "startOffset": 170, "endOffset": 186}, {"referenceID": 18, "context": "specific knowledge-representation formalisms [20, 44].", "startOffset": 45, "endOffset": 53}, {"referenceID": 42, "context": "specific knowledge-representation formalisms [20, 44].", "startOffset": 45, "endOffset": 53}, {"referenceID": 4, "context": "More recently, graphbased [5] and lexical techniques [3, 4] have been investigated in the emerging area of volunteered geographic information (VGI).", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "More recently, graphbased [5] and lexical techniques [3, 4] have been investigated in the emerging area of volunteered geographic information (VGI).", "startOffset": 53, "endOffset": 59}, {"referenceID": 3, "context": "More recently, graphbased [5] and lexical techniques [3, 4] have been investigated in the emerging area of volunteered geographic information (VGI).", "startOffset": 53, "endOffset": 59}, {"referenceID": 24, "context": "and relations, also known as context [26].", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "the possibilities that an entity offers to humans [19].", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "To capture this idea at the linguistic level that is relevant to this discussion, we adopt the approach outlined in [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 48, "context": "In a seminal discussion on expert systems, Strube [50] argued that knowledge engineering should strive towards increasing the cognitive adequacy of computational systems, defined as their \u2018degree of nearness to human cognition\u2019 (p.", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "Following this approach, we adopt the notion of cognitive plausibility to assess to what degree a measure mimics human behaviour [27].", "startOffset": 129, "endOffset": 133}, {"referenceID": 44, "context": "ambiguation [46, 52, 10, 38].", "startOffset": 12, "endOffset": 28}, {"referenceID": 50, "context": "ambiguation [46, 52, 10, 38].", "startOffset": 12, "endOffset": 28}, {"referenceID": 9, "context": "ambiguation [46, 52, 10, 38].", "startOffset": 12, "endOffset": 28}, {"referenceID": 36, "context": "ambiguation [46, 52, 10, 38].", "startOffset": 12, "endOffset": 28}, {"referenceID": 43, "context": "standard was published in 1965, in a article in which Rubenstein and Goodenough [45] collected a set of 65 word pairs ranked by their synonymy.", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "Following a similar line of research, Miller and Charles [33] published a similar dataset with 30 word pairs in 1991.", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "[13] created the WordSimilarity-353 dataset, which contains 353 word pairs actually ranked by semantic relatedness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The dataset was subsequently extended to distinguish between similarity and relatedness [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 35, "context": "[37] collected associative similarity ratings for 1,016 word pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] conducted a study on the cognitive plausibility of their Sim-DL similarity measure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "In order to evaluate their Matching-Distance Similarity Measure (MDSM), Rod\u0155\u0131guez and Egenhofer [44] collected similarity judgements about geographic terms, including large natural entities (e.", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "evaluation of network-based similarity measures [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": "g e o R u b en sein a n d G o o d en o u g h [4 5 ]", "startOffset": 45, "endOffset": 51}, {"referenceID": 4, "context": "g e o R u b en sein a n d G o o d en o u g h [4 5 ]", "startOffset": 45, "endOffset": 51}, {"referenceID": 2, "context": "\u221a M iler a n d C h a rles [3 3 ]", "startOffset": 26, "endOffset": 32}, {"referenceID": 2, "context": "\u221a M iler a n d C h a rles [3 3 ]", "startOffset": 26, "endOffset": 32}, {"referenceID": 3, "context": "R u b en sein a n d G o o d en o u g h [4 5 ]; 3 2 term", "startOffset": 39, "endOffset": 45}, {"referenceID": 4, "context": "R u b en sein a n d G o o d en o u g h [4 5 ]; 3 2 term", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "[1 3 ], A g ire et a l.", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[1 3 ], A g ire et a l.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1 ]", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "\u221a R o d \u0131g u ez a n d E g en h o er [4 4 ]", "startOffset": 36, "endOffset": 42}, {"referenceID": 3, "context": "\u221a R o d \u0131g u ez a n d E g en h o er [4 4 ]", "startOffset": 36, "endOffset": 42}, {"referenceID": 2, "context": "[3 7 ]", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[3 7 ]", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 1 ]", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[2 1 ]", "startOffset": 0, "endOffset": 6}, {"referenceID": 43, "context": "However, in some cases, these datasets have been treated as gold standards in the subsequent literature [45, 33].", "startOffset": 104, "endOffset": 112}, {"referenceID": 31, "context": "However, in some cases, these datasets have been treated as gold standards in the subsequent literature [45, 33].", "startOffset": 104, "endOffset": 112}, {"referenceID": 43, "context": "eral term-to-term similarity measures [45, 33, 13].", "startOffset": 38, "endOffset": 50}, {"referenceID": 31, "context": "eral term-to-term similarity measures [45, 33, 13].", "startOffset": 38, "endOffset": 50}, {"referenceID": 12, "context": "eral term-to-term similarity measures [45, 33, 13].", "startOffset": 38, "endOffset": 50}, {"referenceID": 42, "context": "In the geographic context, only the MDSM evaluation dataset is suitable to evaluate semantic similarity of geographic terms [44].", "startOffset": 124, "endOffset": 128}, {"referenceID": 55, "context": "Online surveys constitute a powerful research tool, with well-known advantages and disadvantages [57].", "startOffset": 97, "endOffset": 101}, {"referenceID": 42, "context": "A cross-disciplinary consensus exists on the fact that semantic judgements are affected by the context in which the terms are considered [44, 22].", "startOffset": 137, "endOffset": 145}, {"referenceID": 20, "context": "A cross-disciplinary consensus exists on the fact that semantic judgements are affected by the context in which the terms are considered [44, 22].", "startOffset": 137, "endOffset": 145}, {"referenceID": 42, "context": "Rod\u0155\u0131guez and Egenhofer [44] asked their subjects to rank geographic terms in the following contexts: \u2018null context,\u2019 \u2018play a sport,\u2019 \u2018compare constructions,\u2019 and \u2018compare transportation systems.", "startOffset": 24, "endOffset": 28}, {"referenceID": 42, "context": "For example, adding a specific context does not increase the inter-subject agreement: in their evaluation, Rod\u0155\u0131guez and Egenhofer [44] report a considerably lower association between subjects in", "startOffset": 131, "endOffset": 135}, {"referenceID": 5, "context": "In our previous work, we extracted the lexicon utilised in OpenStreetMap into a machine-readable vocabulary, the OSM Semantic Network [6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 32, "context": "The examples used to illustrate semantic relatedness (apples - bananas, doctor - hospital, tree - shade) and similarity (apples - bananas, doctor - surgeon, car - motorcycle) were based on those by Mohammad and Hirst [34].", "startOffset": 217, "endOffset": 221}, {"referenceID": 10, "context": "Although the impact of size of the Likert scale, typical options being 5, 7 or 10, is debated in the social sciences, it has little impact on the rating means [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 52, "context": "sim(building, hospital) 6= sim(hospital, building)) [54].", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Several indices have been devised to capture IRR and IRA in psychological surveys [7, 30].", "startOffset": 82, "endOffset": 89}, {"referenceID": 28, "context": "Several indices have been devised to capture IRR and IRA in psychological surveys [7, 30].", "startOffset": 82, "endOffset": 89}, {"referenceID": 28, "context": "LeBreton and Senter [30] recommend using several indices for IRR and IRA, to avoid the bias of any single index.", "startOffset": 20, "endOffset": 24}, {"referenceID": 41, "context": "We thus include the following indices of IRA and IRR: the mean Pearson\u2019s correlation coefficient [43]; Kendall\u2019s W [25]; Robinson\u2019s A [42]; Finn coefficient [14]; James, Demaree and Wolf\u2019s", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "We thus include the following indices of IRA and IRR: the mean Pearson\u2019s correlation coefficient [43]; Kendall\u2019s W [25]; Robinson\u2019s A [42]; Finn coefficient [14]; James, Demaree and Wolf\u2019s", "startOffset": 115, "endOffset": 119}, {"referenceID": 40, "context": "We thus include the following indices of IRA and IRR: the mean Pearson\u2019s correlation coefficient [43]; Kendall\u2019s W [25]; Robinson\u2019s A [42]; Finn coefficient [14]; James, Demaree and Wolf\u2019s", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "We thus include the following indices of IRA and IRR: the mean Pearson\u2019s correlation coefficient [43]; Kendall\u2019s W [25]; Robinson\u2019s A [42]; Finn coefficient [14]; James, Demaree and Wolf\u2019s", "startOffset": 157, "endOffset": 161}, {"referenceID": 16, "context": "rWG(J) [18].", "startOffset": 7, "endOffset": 11}, {"referenceID": 39, "context": "Following Resnik [41], we consider upper bound on the cognitive plausibility of a computable measure to be the highest correlation obtained by a human rater with the means (e.", "startOffset": 17, "endOffset": 21}, {"referenceID": 42, "context": "nature of semantic conceptualisations, this correlation is satisfactory, and is comparable with analogous psychological surveys [44].", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "Given the set of term pairs, and the set of human raters, we computed the relatedness/similarity scores as the mean ratings, normalised in the interval [0, 1], where 0 means no relatedness/similarity, and 1 maximum related-", "startOffset": 152, "endOffset": 158}, {"referenceID": 16, "context": "For this purpose, we adopt James, Demaree and Wolf\u2019s rWG, a popular index to measure IRA on a single target, based on the rating variance [18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Each pair in QREL and QSIM obtains an agreement measure \u2208 [0, 1], where 0 indicates a squared distribution (i.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "the normalised mean rankings in range [0, 1].", "startOffset": 38, "endOffset": 44}, {"referenceID": 54, "context": "The paired Wilcoxon signed rank test [56] indicates that the relatedness scores are higher than the corresponding similarity ones, at p < .", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Overall, these densities show that all the score range [0, 1] is satisfactorily covered, i.", "startOffset": 55, "endOffset": 61}, {"referenceID": 28, "context": "An explanation of this trend in the pair agreement lies in the fact that humans give consistently different ratings to the same objects: some subjects tend to be strict, and some lenient, resulting in different relative ratings, and therefore low absolute pair agreement [30].", "startOffset": 271, "endOffset": 275}, {"referenceID": 11, "context": "Despite the variety and importance of computational measures devised in natural language processing, the evaluation of such measures remains a difficult and complex task [12].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "As context has been identified as a key aspect of semantic similarity [26], new datasets should be generated to capture explicitly the differences in geo-similarity and relatedness judgements with respect", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "As Ferrara and Tasso [12] point out, this evaluative approach has several limitations.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "A promising route might consist of evaluating humanreadable explanations of relatedness measures, and not only numeric scores or rankings [17].", "startOffset": 138, "endOffset": 142}, {"referenceID": 55, "context": "[57].", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "In geographic information science and semantics, the computation of semantic similarity is widely recognised as key to supporting a vast number of tasks in information integration and retrieval. By contrast, the role of geosemantic relatedness has been largely ignored. In natural language processing, semantic relatedness is often confused with the more specific semantic similarity. In this article, we discuss a notion of geo-semantic relatedness based on Lehrer\u2019s semantic fields, and we compare it with geo-semantic similarity. We then describe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a new open dataset designed to evaluate computational measures of geo-semantic relatedness and similarity. This dataset is larger than existing datasets of this kind, and includes 97 geographic terms combined into 50 term pairs rated by 203 human subjects. GeReSiD is available online and can be used as an evaluation baseline to determine empirically to what degree a given computational model approximates geo-semantic relatedness and similarity.", "creator": "LaTeX with hyperref package"}}}