{"id": "1402.4862", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2014", "title": "Learning the Parameters of Determinantal Point Process Kernels", "abstract": "determinantal point processes ( dpps ) appeared well - suited for modeling repulsion and have proven useful in testing applications where variation is desired. while those have many appealing properties, such as efficient sampling, learning the parameters of a dpp is still considered a difficult problem allied to the non - convex nature of the likelihood function. in this paper, we propose using smoothing methods to learn a dpp kernel parameters. these methods are applicable in large - scale simple continuous dpp, even when only exact form of the eigendecomposition is unknown. we demonstrate the utility of our dpp learning methods in studying the progression of localized neuropathy based on spatial associations given nerve fibers, and in studying human effects of sensation in images.", "histories": [["v1", "Thu, 20 Feb 2014 01:54:37 GMT  (2333kb,D)", "http://arxiv.org/abs/1402.4862v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["raja hafiz affandi", "emily b fox", "ryan p adams", "benjamin taskar"], "accepted": true, "id": "1402.4862"}, "pdf": {"name": "1402.4862.pdf", "metadata": {"source": "META", "title": "Learning the Parameters of Determinantal Point Process Kernels", "authors": ["Raja Hafiz Affandi"], "emails": ["rajara@wharton.upenn.edu", "ebfox@stat.washington.edu", "rpa@seas.harvard.edu", "taskar@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "A determinantal point process (DPP) provides a distribution over configurations of points. The defining characteristic of the DPP is that it is a repulsive point process, which makes it useful for modeling diversity. Recently, DPPs have played an increasingly important role in machine learning and statistics with applications both in the discrete setting\u2014where they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza & Taskar, 2010; 2011a; Snoek et al., 2013)\u2014 and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al., 2013a; Zou & Adams, 2012).\nFormally, given a space \u2126 \u2286 Rd, a specific point configuration A \u2286 \u2126, and a positive semi-definite kernel function L : \u2126\u00d7 \u2126\u2192 R, the probability density under a DPP with kernel L is given by\nPL(A) \u221d det(LA) , (1)\nwhere LA is the |A| \u00d7 |A| matrix with entries L(x,y) for each x,y \u2208 A. This defines a repulsive point process since point configurations that are more spread out according to the metric defined by the kernel L have higher densities. To see this, recall that the subdeterminant in Eq. (1) is proportional to the square of the volume spanned by the kernel vectors associated with the points in A.\nBuilding on work of Kulesza & Taskar (2010), it is intuitive to decompose the kernel L as\nL(x,y) = q(x)k(x,y)q(y) , (2)\nwhere q(x) can be interpreted as the quality function at point x and k(x,y) as the similarity kernel between points x and y. The ability to bias the quality in certain locations while still maintaining diversity via the similarity kernel offers great modeling flexibility.\nOne of the remarkable aspects of DPPs is that they offer efficient algorithms for inference, including computing the marginal and conditional probabilities (Kulesza & Taskar, 2012), sampling (Affandi et al., 2013a;b; Hough et al., 2006; Kulesza & Taskar, 2010), and restricting to fixed-sized point configurations (k-DPPs)(Kulesza & Taskar, 2011a). However, an important component of DPP modeling, learning the DPP kernel parameters, is still considered a difficult, open problem. Even in the discrete \u2126 setting, DPP kernel learning has been conjectured to be NP-hard (Kulesza & Taskar, 2012).\nar X\niv :1\n40 2.\n48 62\nv1 [\nst at\n.M L\n] 2\n0 Fe\nIntuitively, the issue arises from the fact that in seeking to maximize the log-likelihood of Eq. (1), the numerator yields a concave log-determinant term whereas the normalizer contributes a convex term, leading to a nonconvex objective. This non-convexity holds even under various simplifying assumptions on the form of L.\nAttempts to partially learn the kernel have been studied by, for example, learning the parametric form of the quality function q(x) for fixed similarity k(x,y) (Kulesza & Taskar, 2011b), or learning a weighting on a fixed set of kernel experts (Kulesza & Taskar, 2011a). So far, the only attempt to learn the parameters of the similarity kernel k(x,y) has used Nelder-Mead optimization (Lavancier et al., 2012), which lacks theoretical guarantees about convergence to a stationary point.\nIn this paper, we consider parametric forms for the quality function q(x) and similarity kernel k(x,y) and propose Bayesian methods to learn the DPP kernel parameters \u0398. In addition to capturing posterior uncertainty rather than a single point estimate, these methods can be easily modified to efficiently learn large-scale and continuous DPPs where the eigenstructures are either unknown or are inefficient to compute. In contrast, gradient ascent algorithms for maximum likelihood estimation (MLE) require kernels L that are differentiable with respect to \u0398 in the discrete \u2126 case. In the continuous \u2126 case, the eigenvalues must additionally have a known, differentiable functional form, which only occurs in limited scenarios.\nIn Sec. 2, we review DPPs and their fixed-sized counterpart (k-DPPs). We then explore likelihood maximization algorithms for learning DPP and k-DPP kernels. After examining the shortcomings of the MLE approach, we propose a set of techniques for Bayesian posterior inference of the kernel parameters in Sec. 3, and explore modifications to accommodate learning large-scale and continuous DPPs. In Sec. 4, we derive a set of DPP moments assuming a known kernel eigenstructure and explore using these moments as a model-checking technique. In low-dimensional settings, we can use a method of moments approach to learn the kernel parameters via numerical techniques. Finally, we test our methods on both simulated and real-world data. Specifically, in Sec. 5 we use DPP learning to study the progression of diabetic neuropathy based on spatial distribution of nerve fibers and also to study human perception of diversity of images.\n2. Background\n2.1. Discrete DPPs/k-DPPs\nFor a discrete base set \u2126 = {x1,x2, . . . ,xN}, a DPP defined by an N \u00d7N positive semi-definite kernel matrix L is a probability measure on the 2\u2126 possible subsets A of \u2126:\nPL(A) = det(LA)\ndet(L+ I) . (3)\nHere, LA \u2261 [Lij ]xi,xj\u2208A is the submatrix of L indexed by the elements in A and I is the N \u00d7N identity matrix (Borodin & Rains, 2005).\nIn many applications, we are instead interested in the probability distribution which gives positive mass only to subsets of a fixed size, k. In these cases, we consider fixed-sized DPPs (or k-DPPs) with probability distribution on sets A of cardinality k given by\nPkL(A) = det(LA)\nek(\u03bb1, . . . , \u03bbN ) , (4)\nwhere \u03bb1, . . . , \u03bbN are eigenvalues of L and ek(\u03bb1, . . . , \u03bbN ) is the kth elementary symmetric polynomial (Kulesza & Taskar, 2011a). Note that ek(\u03bb1, . . . , \u03bbN ) can be efficiently computed using recursion (Kulesza & Taskar, 2012).\n2.2. Continuous DPPs/k-DPPs\nConsider now the case where \u2126 \u2286 Rd is a continuous space. DPPs extend to this case naturally, with L now a kernel operator instead of a matrix. Again appealing to Eq. (1), the DPP probability density for point configurations A \u2282 \u2126 is given by\nPL(A) = det(LA)\u220f\u221e n=1(\u03bbn + 1) , (5)\nwhere \u03bb1, \u03bb2, . . . are eigenvalues of the operator L.\nThe k-DPP also extends to the continuous case with\nPkL(A) = det(LA)\nek(\u03bb1:\u221e) , (6)\nwhere \u03bb1:\u221e = (\u03bb1, \u03bb2, . . .).\nIn contrast to the discrete case, the eigenvalues \u03bbi for continuous DPP kernels are generally unknown; exceptions include a few kernels such as the exponentiated quadratic. However, Affandi et al. (2013a) showed that a low-rank approximation to L can be used to recover an approximation to a finite truncation of the eigenvalues representing an important part of the eigenspectrum. This enables us to approximate the normalizing constants of both DPPs and k-DPPs, and will play a crucial role in our proposed methods of Sec. 3.3."}, {"heading": "3. Learning Parametric DPPs", "text": "Assume that we are given a training set consisting of samples A1, A2, . . . , AT , and that we model these data using a DPP/k-DPP with parametric kernel\nL(x,y; \u0398) = q(x; \u0398)k(x,y; \u0398)q(y; \u0398) , (7)\nwith parameters \u0398. We denote the associated kernel matrix for a set At by LAt(\u0398) and the full kernel matrix/operator by L(\u0398). Likewise, we denote the kernel eigenvalues by \u03bbi(\u0398). In this section, we explore various methods for DPP/k-DPP learning."}, {"heading": "3.1. Learning using Optimization Methods", "text": "To learn the parameters \u0398 of a discrete DPP model, we can maximize the log-likelihood\nL(\u0398) = T\u2211 t=1 log det(LAt(\u0398))\u2212 T log det(L(\u0398) + I) .\n(8)\nLavancier et al. (2012) suggests that the Nelder-Mead simplex algorithm (Nelder & Mead, 1965) can be used to maximize L(\u0398). This method is based on evaluating the objective function at the vertices of a simplex, then iteratively shrinking the simplex towards an optimal point. While this method is convenient since it does not require explicit knowledge of derivates of L(\u0398), it is regarded as a heuristic search method and is known for its failure to necessarily converge to a stationary point (McKinnon, 1998).\nGradient ascent and stochastic gradient ascent provide more attractive approaches because of their theoretical guarantees, but require knowledge of the gradient of L(\u0398). In the discrete DPP setting, this gradient can be computed straightforwardly, and we provide examples for discrete Gaussian and polynomial kernels in the Supplementary Material. We note, however, that these methods are still susceptible to convergence to local optima due to the non-convex likelihood landscape.\nThe log likelihood of the k-DPP kernel parameter is L(\u0398) = T\u2211 t=1 log det(LAt(\u0398))\u2212 T log \u2211 |B|=k det(LB(\u0398)) , (9) which presents an addition complication due to needing a sum over ( n k ) terms in the gradient.\nFor continuous DPPs/k-DPPs, gradient ascent can only be used in cases where the exact eigendecomposition of the kernel operator is known with a differentiable form for the eigenvalues (see Eq. (5)). This restricts the applicability of gradient-based likelihood maximization to a limited set of scenarios, such as a DPP with\nGaussian quality function and similarity kernel. Furthermore, for kernel operators with infinite rank (such as the Gaussian), an explicit truncation has to be made, resulting in an approximate gradient of L(\u0398). Unfortunately, such approximate gradients are not unbiased estimates of the true gradient, so the theory associated with attractive stochastic gradient based approaches does not hold."}, {"heading": "3.2. Bayesian Learning for Discrete DPPs", "text": "Instead of optimizing the likelihood to get an MLE, here we propose a Bayesian approach to that samples from the posterior distribution over kernel parameters:\nP(\u0398|A1, . . . , AT ) \u221d P(\u0398) T\u220f t=1 det(LAt(\u0398)) det(L(\u0398) + I) (10)\nfor the DPP and, for the k-DPP,\nP(\u0398|A1, . . . , AT ) \u221d P(\u0398) T\u220f t=1\ndet(LAt(\u0398))\nek(\u03bb1(\u0398), . . . , \u03bbN (\u0398)) .\n(11) Here, P(\u0398) is the prior on \u0398. Since neither Eq. (10) nor Eq. (11) yield a closed form posterior, we resort to approximate techniques based on Markov chain Monte Carlo (MCMC). We highlight two techniques: randomwalk Metropolis-Hastings (MH) and slice sampling, although other MCMC methods can be employed without loss of generality.\nIn random-walk MH, we use a proposal distribution f(\u0398\u0302|\u0398i) to generate a candidate value \u0398\u0302 given the current parameters \u0398i, which are then accepted or rejected with probability min{r, 1} where\nr = ( P(\u0398\u0302|A1, . . . , AT ) P(\u0398i|A1, . . . , AT ) f(\u0398i|\u0398\u0302) f(\u0398\u0302|\u0398i) ) . (12)\nThe proposal distribution f(\u0398\u0302|\u0398i) is chosen to have mean \u0398i. The hyperparameters of f(\u0398\u0302|\u0398i) tune the width of the distribution, determining the average step size. See Alg. 1 of the Supplementary Material.\nWhile random-walk MH can provide a straightforward means of sampling from the posterior, its efficiency requires tuning the proposal distribution. Choosing an aggressive proposal can result in a high rejection rate, while choosing a conservative proposal can result in inefficient exploration of the parameter space. To avoid the need to tune the proposal distribution, we can instead use slice sampling (Neal, 2003), which performs a local search for an acceptable point while still satisfying detailed balance conditions. We first describe this method in the univariate case, following\nthe \u201clinear stepping-out\u201d approach described in Neal (2003). Given the current parameter \u0398i, we first sample y \u223c Uniform[0,P(\u0398i|A1, . . . , AT )]. This defines our slice with all values of \u0398 with P(\u0398|A1, . . . , AT ) greater than y included in the slice. We then define a random interval around \u0398i with width w that is linearly expanded until neither endpoint is in the slice. We propose \u0398\u0302 uniformly in the interval. If \u0398\u0302 is in the slice, it is accepted. Otherwise, \u0398\u0302 becomes the new boundary of the interval, shrinking it so as to still include the current state of the Markov chain. This procedure is repeated until a proposed \u0398\u0302 is accepted. See Alg. 2 of the Supplementary Material.\nThere are many ways to extend this algorithm to a multidimensional setting. We consider the simplest extension proposed by Neal (2003) where we use hyperrectangles instead of intervals. A hyperrectangle region is constructed around \u0398i and the edge in each dimension is expanded or shrunk depending on whether its endpoints lie inside or outside the slice. One could alternatively consider coordinate-wise or random-direction approaches to multidimensional slice sampling.\nAs an illustrative example, we consider synthetic data generated from a two-dimensional discrete DPP using a kernel where\nq(xi) = exp\n{ \u22121\n2 x>i \u0393 \u22121xi\n} (13)\nk(xi,xj) = exp\n{ \u22121\n2 (xi\u2212xj)>\u03a3\u22121(xi\u2212xj)\n} , (14)\nwhere \u0393 = diag(0.5, 0.5) and \u03a3 = diag(0.1, 0.2). We consider \u2126 to be a grid of 100 points evenly spaced in a 10\u00d7 10 unit square and simulate 100 samples from a DPP with kernel as above. We then condition on these simulated data and perform posterior inference of the kernel parameters using MCMC. Fig. 1 shows the sample autocorrelation function of the slowest mixing parameter, \u03a311, learned using random-walk MH and slice sampling. Furthermore, we ran a Gelman-Rubin test (Gelman & Rubin, 1992) on 5 chains starting from overdispersed starting positions and found that the average partial scale reduction function across the four parameters to be 1.016 for MH and 1.023 for slice sampling, indicating fast mixing of the posterior samples."}, {"heading": "3.3. Bayesian Learning for Large-Scale Discrete and Continuous DPPs", "text": "In the large-scale discrete or continuous settings, evaluating the normalizers det(L(\u0398)+I) or \u220f\u221e n=1(\u03bbn(\u0398)+1), respectively, can be inefficient or infeasible. Even in cases where an explicit form of the truncated eigenval-\nues can be computed, this will only lead to approximate MLE solutions, as discussed in Sec. 3.1.\nOn the surface, it seems that most MCMC algorithms will suffer from the same problem since they require knowledge of the likelihood as well. However, we argue that for most of these algorithms, an upper and lower bound of the posterior probability is sufficient as long as we can control the accuracy of these bounds. In particular, denote the upper and lower bounds by P+(\u0398|A1, . . . , AT ) and P\u2212(\u0398|A1, . . . , AT ), respectively. In the random-walk MH algorithm we can then compute the upper and lower bounds on the acceptance ratio,\nr+ = ( P+(\u0398\u0302|A1, . . . , AT ) P\u2212(\u0398i|A1, . . . , AT ) f(\u0398i|\u0398\u0302) f(\u0398\u0302|\u0398i) ) (15)\nr\u2212 = ( P\u2212(\u0398\u0302|A1, . . . , AT ) P+(\u0398i|A1, . . . , AT ) f(\u0398i|\u0398\u0302) f(\u0398\u0302|\u0398i) ) . (16)\nWe can precompute the threshold u \u223c Uniform[0, 1], so we can still sometimes accept or reject the proposal \u0398\u0302 even if these bounds have not completely converged. All that is necessary is for u < min{1, r\u2212} (immediately reject) or u > min{1, r+} (immediately accept). In the case that u \u2208 (r\u2212, r+), we can perform further computations to increase the accuracy of our bounds until a decision can be made. As we only sample u once in the beginning, this iterative procedure yields a Markov chain with the exact target posterior as its stationary distribution; all we have done is \u201cshort-circuit\u201d the computation once we have bounded the acceptance ratio r away from u. We show this procedure in Alg. 3 of the Supplementary Material.\nThe same idea applies to slice sampling. In the first step of generating a slice, instead of sampling y \u223c Uniform[0,P(\u0398i|A1, . . . , AT )], we use a rejection sampling scheme first propose a candidate slice as\ny\u0302 \u223c Uniform[0,P+(\u0398i|A1, . . . , AT )] . (17)\nWe then decide whether y\u0302 < P\u2212(\u0398i|A1, . . . , AT ), in which case we know y\u0302 < P(\u0398i|A1, . . . , AT ) and we accept y\u0302 as the slice and set y = y\u0302. In the case where\ny\u0302 \u2208 (P\u2212(\u0398i|A1, . . . , AT ),P+(\u0398i|A1, . . . , AT )), we keep increasing the tightness of our bounds until a decision can be made. If at any point y\u0302 exceeds the newly computed P+(\u0398i|A1, . . . , AT ), we know that y\u0302 > P(\u0398i|A1, . . . , AT ) so we reject the proposal. In this case, we generate a new y\u0302 and repeat.\nUpon accepting a slice y, the subsequent steps for proposing a parameter \u0398\u0302 proceed in a similarly modified manner. For the interval computation, the endpoints \u0398e are each examined to decide whether y < P\u2212(\u0398e|A1, . . . , AT ) (endpoint is not in slice) or y > P+(\u0398e|A1, . . . , AT ) (endpoint is in slice). The tightness of the posterior bounds is increased until a decision can be made and the interval can be adjusted, if need be. After convergence of the interval, \u0398\u0302 is generated uniformly over the interval and is likewise tested for acceptance. We illustrate this procedure in Fig. 1 of the Supplementary Material.\nThe lower and upper bounds of the posterior probability can in fact be incorporated in many MCMC-type algorithms. This provides a convenient and efficient way to garner posterior samples assuming that tightening the bounds can be done efficiently. In our case, the upper and lower bounds for the posterior probability depends on the truncation of the kernel eigenvalues and can be arbitrarily tightened by including more terms in the truncation. In the discrete DPP/k-DPP settings, the eigenvalues can be efficiently computed to a specified point using methods such as power law iterations. The corresponding bounds for a 3600\u00d73600 Gaussian kernel example are shown in Fig. 2. In the continuous setting, explicit truncation can be done when the kernel has Gaussian quality and similarity, as we show in Sec. 5.1. For other continuous DPP kernels, low-rank approximations can be used (Affandi et al., 2013a) resulting in approximate posterior samples. In contrast, a gradient ascent algorithm for MLE is not even feasible: we do not know the form of the approximated eigenvalues, so we cannot take their derivative.\nExplicit forms for the posterior probability bounds of \u0398 for DPPs and k-DPPs as a function of the eigenvalue truncations follow from Prop. C.1 and C.2 combined with Eqs. (10) and (11), respectively. Proofs are in the Supplementary Material.\nProposition 3.1 Let \u03bb1:\u221e be the eigenvalues of kernel L. Then\nM\u220f n=1 (1 + \u03bbn) \u2264 \u221e\u220f n=1 (1 + \u03bbn) (18)\nand\n\u221e\u220f n=1 (1 + \u03bbn) \u2264 exp { tr(L)\u2212 M\u2211 n=1 \u03bbn }[ M\u220f n=1 (1 + \u03bbn) ] .\n(19)\nProposition 3.2 Let \u03bb1:\u221e be the eigenvalues of kernel L. Then ek(\u03bb1:M ) \u2264 ek(\u03bb1:\u221e) (20) and\nek(\u03bb1:\u221e) \u2264 k\u2211 j=0 (tr(L)\u2212 \u2211M n=1 \u03bbn) j j! ek\u2212j(\u03bb1:M ) .\n(21)\nFinally note that the expression tr(L) in the bounds can be easily computed as either \u2211N n=1 Lii in the discrete\ncase or \u222b\n\u2126 L(x,x)dx in the continuous case."}, {"heading": "4. Method of Moments", "text": "Convergence and mixing of MCMC samplers can be challenging to assess. Although generic techniques such as Gelman-Rubin diagnostics (Gelman & Rubin, 1992) are applicable, we additionally provide a set of tools more directly tailored to the DPP by deriving a set of theoretical moments. When performing posterior inference of kernel parameters, we can check whether the moments of our data match the theoretical moments given by the posterior samples. This can be done in cases where the eigenstructure is fully known.\nIn the discrete case, we first need to compute the marginal probabilities. Borodin (2009) shows that the marginal kernel, K, can be computed directly from L:\nK = L(I + L)\u22121 . (22)\nThe mth moment can then be calculated via\nE[xm] = N\u2211 i=1 xmi K(xi,xi) . (23)\nIn the continuous case, given the eigendecomposition of the kernel operator, L(x,y) = \u2211\u221e n=1 \u03bbn\u03c6n(x) \u2217\u03c6n(y)\n(where \u03c6n(x) \u2217 denotes the complex conjugate of the nth eigenfunction), the mth moment is\nE[xm] = \u222b\n\u2126 \u221e\u2211 n=1 \u03bbn \u03bbn + 1 xm\u03c6n(x) 2dx . (24)\nNote that this generally cannot be evaluated in closed form since the eigendecompositions of most kernel operators are not known. However, in certain cases where the eigenfunctions are known analytically, the moments can be directly computed. For a kernel defined by Gaussian quality and similarity (see Sec. 5.1), the eigendecomposition can be performed using Hermite polynomials. In the Supplementary Material, we derive the mth moment in this setting.\nUnfortunately, the method of moments can be challenging to use for direct parameter learning since Eqs. (23) and (39) are not analytically available in most cases. In low dimensions, these quantities can be estimated numerically, but it remains an open question as to how these moments should be estimated for large-scale problems."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Simulations", "text": "We provide an explicit example of Bayesian learning for a continuous DPP with the kernel defined by\nq(x) = \u221a \u03b1 D\u220f d=1 1 \u221a \u03c0\u03c1d exp { \u2212 x 2 d 2\u03c1d } (25)\nk(x,y) = D\u220f d=1 exp { \u2212 (xd \u2212 yd) 2 2\u03c3d } ,x,y \u2208 RD. (26)\nHere, \u0398 = {\u03b1, \u03c1d, \u03c3d} and the eigenvalues of the operator L(\u0398) are given by (Fasshauer & McCourt, 2012),\n\u03bbm(\u0398) = \u03b1 D\u220f d=1 \u221a 1 \u03b22d+1 2 + 1\n2\u03b3d\n( 1\n\u03b3d(\u03b22d + 1) + 1\n)md\u22121 ,\n(27)\nwhere \u03b3d = \u03c3d \u03c1d , \u03b2d = (1+ 2 \u03b3d ) 1 4 , and m = (m1, . . . ,mD) is a multi-index.\nFurthermore, the trace of L(\u0398) can be easily computed as\ntr(L(\u0398)) = \u222b Rd \u03b1 D\u220f d=1 1 \u03c0\u03c1d exp { \u2212 x 2 d 2\u03c1d } dx = \u03b1 . (28)\nWe test our Bayesian learning algorithms on simulated data generated from a 2-dimensional isotropic kernel (\u03c3d = \u03c3, \u03c1d = \u03c1 for d = 1, 2) using Gibbs sampling (Affandi et al., 2013a). We then learn the parameters\nunder weakly informative inverse gamma priors on \u03c3, \u03c1 and \u03b1. Details are in the Supplementary Material. We tweak the (\u03b1, \u03c1, \u03c3) used for simulation so that we have the following three scenarios:\n(i) 10 DPP samples with average number of points=18 using (\u03b1, \u03c1, \u03c3) = (1000, 1, 1)\n(ii) 1000 DPP samples with average number of points=18 using (\u03b1, \u03c1, \u03c3) = (1000, 1, 1)\n(iii) 10 DPP samples with average number of points=77 using (\u03b1, \u03c1, \u03c3) = (100, 0.7, 0.05).\nFig. 3 shows trace plots of the posterior samples for all three scenarios. In the first scenario, the parameter estimates vary wildly whereas in the other two scenarios, the posterior estimates are more stable. In all the cases, the zeroth and second moment estimated from the posterior samples are in the neighborhood of the corresponding empirical moments. This leads us to believe that the posterior is broad in cases where we have both a small number of samples and few points in each sample. The posterior becomes more peaked as the total number of points increases. Note that using a stationary similarity kernel allows us to garner information either from few sets with many points or many sets of few points.\nDispersion Measure In many applications, we are interested in quantifying the overdispersion of point process data. In spatial statistics, one standard quantity used to measure dispersion is the Ripley Kfunction (Ripley, 1977). Here, instead, we would like to use the learned parameters of the DPP to measure overdispersion as repulsion. An important characteristic of a measure of repulsion is that it should be invariant to scaling. In the Supplementary Material, we derive results that, as the data are scaled from x to \u03b7x, the parameters scale from (\u03b1, \u03c3i, \u03c1i) to (\u03b1, \u03b7\u03c3i, \u03b7\u03c1i). This suggests that an appropriate scale-invariant measure of repulsion is \u03b3i = \u03c3i/\u03c1i."}, {"heading": "5.2. Applications", "text": ""}, {"heading": "5.2.1. Diabetic Neuropathy", "text": "Recent breakthroughs in skin tissue imaging have spurred interest in studying the spatial patterns of nerve fibers in diabetic patients. It has been observed that these nerve fibers appear to become more clustered as diabetes progresses. Waller et al. (2011) previously analyzed this phenomena based on 6 thigh nerve fiber samples. These samples were collected from 5 diabetic patients at different stages of diabetic neuropathy and one healthy subject. On average, there are 79 points in each sample (see Fig. 4). Waller et al. (2011) analyzed the Ripley K-function and concluded that the\ndifference between the healthy and severely diabetic samples is highly significant.\nWe instead study the differences between these samples by learning the kernel parameters of a DPP and quantifying the level of repulsion of the point process. Due to the small sample size, we consider a 2-class study of Normal/Mildly Diabetic versus Moderately/Severely Diabetic. We perform two analyses. In the first, we directly quantify the level of repulsion based on our scale-invariant statistic, \u03b3 = \u03c3/\u03c1 (see Sec. 5.1). In the second, we perform a leave-one-out classification by training the parameters on the two classes with one sample left out. We then evaluate the likelihood of the held-out sample under the two learned classes. We repeat this for all six samples.\nWe model our data using a 2-dimensional continuous DPP with Gaussian quality and similarity as in Eqs. (40) and (41). Since there is no observed preferred direction in the data, we use an isotropic kernel (\u03c3d = \u03c3 and \u03c1d = \u03c1 for d = 1, 2). We place weakly informative inverse gamma priors on (\u03b1, \u03c1, \u03c3), as specified in the Supplementary Material, and learn the parameters us-\ning slice sampling with eigenvalue bounds as outlined in Sec. 3.3. The results shown in Fig. 5 indicate that our \u03b3 measure clearly separates the two classes, concurring with the results of Waller et al. (2011). Furthermore, we are able to correctly classify all six samples. While the results are preliminary, being based on only six observations, they show promise for this task."}, {"heading": "5.2.2. Diversity in Images", "text": "We also examine DPP learning for quantifying how visual features relate to human perception of diversity in different image categories. This is useful in applications such as image search, where it is desirable to present users with a set of images that are not only relevant to the query, but diverse as well.\nBuilding on work by Kulesza & Taskar (2011a), three image categories\u2014cars, dogs and cities\u2014were studied. Within each category, 8-12 subcategories (such as Ford for cars, London for cities and poodle for dogs) were queried from Google Image Search and the top 64 results were retrieved. For a subcategory subcat, these images form our base set \u2126subcat. To assess human perception of diversity, human annotated sets of size six were generated from these base sets. However, it is challenging to ask a human to coherently select six diverse images from a set of 64 total. Instead, Kulesza & Taskar (2011a) generated a partial result set of five images from a 5-DPP on each \u2126subcat with a kernel based on the SIFT256 features (see Supplementary Material). Human annotators (via Amazon Mechanical Turk) were then presented with two images selected at random from the remaining subcategory images and asked to add the image they felt was least similar to the partial result set. These experiments resulted in about 500 samples spread evenly spread evenly across the different subcategories.\nWe aim to study how the human annotated sets differ from the top six Google results, Top-6. As in Kulesza & Taskar (2011a), we extracted three types of features from the images\u2014color features, SIFT descriptors (Lowe, 1999; Vedaldi & Fulkerson, 2010) and GIST descriptors (Oliva & Torralba, 2006) described in the Supplementary Material. We denote these features for image i as f colori , f SIFT i , and f GIST i , respectively. For each subcategory, we model our data as a discrete 6-DPP on \u2126subcat with kernel\nLsubcati,j = exp { \u2212 \u2211 feat \u2016f feati \u2212 f featj \u201622 \u03c3catfeat } (29)\nfor feat \u2208 {color,SIFT,GIST} and i, j indexing the 64 images in \u2126subcat. Here, we assume that each category has the same parameters across subcategories, namely, \u03c3catfeat for subcat \u2208 cat and cat \u2208 {cars, dogs, cities}.\nTo learn from the Top-6 images, we consider the samples as being generated from a 6-DPP. To emphasize the human component of the 5-DPP + human annotation sets, we examine a conditional 6-DPP (Kulesza & Taskar, 2012) that fixes the five images from the partial results set and only considers the probability of adding the human annotated image. The Supplementary Material provides details on this conditional k-DPP.\nAll subcategory samples within a category are assumed to be independent draws from a DPP defined on \u2126subcat with kernel Lsubcat parameterized by a shared set of \u03c3catfeat, for subcat \u2208 cat. As such, each of these samples equally informs the posterior of \u03c3catfeat. We perform posterior sampling of the 6-DPP or conditional 6-DPP kernel parameters using slice sampling with weakly informative inverse gamma priors on the \u03c3catfeat. Details are in the Supplementary Material.\nFig. 6 shows a comparison between \u03c3catfeat learned from the human annotated samples (conditioning on the 5- DPP partial result sets) and the Top-6 samples for different categories. The results indicate that the 5-DPP + human annotated samples differs significantly from the Top-6 samples in the features judged by human to be important in diversity in each category. For cars and dogs, human annotators deem color to be a more important feature for diversity than the Google search engine based on their Top-6 results. For cities, on the other hand, the SIFT features are deemed important for diversity by human annotators, while the Google search engine puts a much lower weight on them. Keep in mind, though, that this result only highlights the diversity components of the results while ignoring quality. In real life applications, it is desirable to combine both the quality of each image (as a measure of relevance of the image to the query) and the diversity between the top results. Regardless, we have shown that DPP kernel learning can be informative of judgements of diversity, and this information could be used (for example) to tune search engines to provide results more in accordance with human judgement."}, {"heading": "6. Conclusion", "text": "Determinantal point processes have become increasingly popular in machine learning and statistics. While many important DPP computations are efficient, learning the parameters of a DPP kernel is difficult due to the non-convexity of the likelihood. We proposed Bayesian approaches using MCMC, in particular, for inferring these parameters. In addition to being more robust and providing a characterization of the posterior uncertainty, these algorithms can be modified to deal with large-scale and continuous DPPs. We also showed how our posterior samples can be evaluated using mo-\nment matching as a model-checking method. Finally we demonstrated the utility of learning DPP parameters in studying diabetic neuropathy and evaluating human perception of diversity in images."}, {"heading": "A. Gradient for Discrete DPP", "text": "Gradient ascent and stochastic gradient ascent provide attractive approaches in learning parameters, \u0398 of DPP kernel L(\u0398) because of their theoretical guarantees, but require knowledge of the gradient of the log-likelihood L(\u0398). In the discrete DPP setting, this gradient can be computed straightforwardly and we provide examples for discrete Gaussian and polynomial kernels here.\nL(\u0398) = T\u2211 t=1 log(det(LAt(\u0398)))\u2212 T log(det(L(\u0398) + I))\n(30)\ndL(\u0398) d\u0398 = \u2211T t=1 tr ( LAt(\u0398) \u22121 dLAt (\u0398) d\u0398 ) \u2212T tr ( (L(\u0398) + I)\u22121 dL(\u0398)d\u0398\n) To find the MLE, we can perform gradient ascent\n\u0398i = \u0398i\u22121 + \u03b7 dl(\u0398)\nd\u0398 (31)\nIn the following examples, we denote xi = (x (1) i , x (2) i , . . . , x (d) i ), where d is the number of dimension.\nA.1. Example I: Gaussian Similarity with Uniform Quality\nL(\u03a3) = exp{\u2212(x\u2212 y)>\u03a3\u22121(x\u2212 y)}\nDenote G (lm) ij = Lij\n(x (l) i \u2212x (l) j )(x (m) i \u2212x (m) j )\n2\u03a32lm\nThen,\ndL(\u03a3) d\u03a3lm\n= \u2211T t=1 tr ( LAt(\u03a3) \u22121G (lm) At ) \u2212T tr ( (L(\u03a3) + I)\u22121G(lm)\n) A.2. Example II: Gaussian Similarity with\nGaussian Quality\nL(\u0393,\u03a3) = exp{\u2212x>\u0393\u22121x \u2212 (x \u2212 y)>\u03a3\u22121(x \u2212 y) \u2212 y>\u0393\u22121y}\nDenote C (lm) ij = Lij\n(x (l) i x (m) i +x (l) j x (m) j )\n2\u03932lm and G\n(lm) ij as in\nprevious example.\nThen,\ndL(\u0393,\u03a3) d\u0393lm\n= \u2211T t=1 tr ( LAt(\u03a3) \u22121C (lm) At ) \u2212T tr ( (L(\u03a3) + I)\u22121C(lm)\n) and dl(\u0393,\u03a3)d\u03a3lm is the same as the previous example.\nA.3. Example III: Polynomial Similarity with Uniform Quality\nL(p, q) = ( x>y + p )q Denote Rij = qL q\u22121 q ij and Uij = Lij log(L 1 q ij).\nThen,\ndL(p, q) dp = \u2211T t=1 tr ( LAt(p, q) \u22121RAt )\n\u2212T tr ( (L(p, q) + I)\u22121R) )\nAlgorithm 1 Random-Walk Metropolis-Hastings\nInput: Dimension: D, Starting point: \u03980, Prior distribution: P(\u0398), Proposal distribution f(\u0398\u0302|\u0398) with mean \u0398, Samples: A1, . . . , AT ]. \u0398 = \u03980 for i = 0 : (\u03c4 \u2212 1) do\n\u0398\u0302 \u223c f(\u0398\u0302|\u0398i) r = ( P(\u0398\u0302|A1,...,AT ) P(\u0398i|A1,...,AT ) f(\u0398i|\u0398\u0302) f(\u0398\u0302|\u0398i) ) u \u223c Uniform[0,1] if u < min{1, r} then\n\u0398i+1 = \u0398\u0302 Output: \u03980:\u03c4\ndL(p, q) dq = \u2211T t=1 tr ( LAt(p, q) \u22121UAt )\n\u2212T tr ( (L(p, q) + I)\u22121U) )"}, {"heading": "B. Bayesian Learning", "text": "In the main paper, we highlight two techniques: random-walk Metropolis-Hastings (MH) and slice sampling to sample from the posterior distribution. We present the pseudo algorithms here (Alg. 1 and Alg. 2).\nWe also present the pseudo random walk MH algorithm for handling large-scale and continuous DPPs using posterior bounds in Alg. 3. We also present and illustration of the slice sampling using posterior bounds in Figure 7.\nC. Proof of DPP/kDPP Bounds\nProposition C.1 Let \u03bb1:\u221e be the eigenvalues of kernel L. Then\nM\u220f n=1 (1 + \u03bbn) \u2264 \u221e\u220f n=1 (1 + \u03bbn) (32)\nand\n\u221e\u220f n=1 (1 + \u03bbn) \u2264 exp { tr(L)\u2212 M\u2211 n=1 \u03bbn }[ M\u220f n=1 (1 + \u03bbn) ] .\n(33)\nProof: The first inequality is trivial since the eigenvalues \u03bb1:\u221e are all nonnegative.\nTo proof the second inequality, we use the AMGM inequality: For any non-negative numbers, \u03b31, ..., \u03b3M , ( \u220fM n=1 \u03b3n) 1 M \u2264 \u2211M n=1 \u03b3n M .\nAlgorithm 2 Univariate Slice Sampling\nInput: Starting point: \u03980, Initial width: w, Prior distribution: P(\u0398), Samples: X = [X1, . . . , XT ]. \u0398 = \u03980 for i = 0 : (\u03c4 \u2212 1) do y \u223c Uniform[0,P(\u0398i|A1, . . . , AT )] z \u223c Uniform[0, 1] L = \u0398i \u2212 z \u2217 w2 R = L+ w2 while y > P(L|A1, . . . , AT ) do L = L\u2212 w2\nwhile y > P(R|A1, . . . , AT ) do R = R+ w2 \u0398\u0302 \u223c Uniform[L,R] if P(\u0398\u0302|A1, . . . , AT ) < y then\nwhile P(\u0398\u0302|A1, . . . , AT ) < y do if \u0398\u0302 > \u0398 then R = \u0398\u0302\nelse L = \u0398\u0302\n\u0398\u0302 \u223c Uniform[L,R] \u0398i+1 = \u0398\u0302\nOutput: \u03980:\u03c4\nLet \u039bM = \u2211\u221e n=M+1 \u03bbn and \u03b3n = 1 + \u03bbn. Then,\n\u221e\u220f n=1 (1 + \u03bbn) = \u221e\u220f n=1 \u03b3n = ( M\u220f n=1 \u03b3n)( \u221e\u220f n=M+1 \u03b3n)\n= ( M\u220f n=1 \u03b3n)( lim l\u2192\u221e M+l\u220f n=M+1 \u03b3n) \u2264 ( M\u220f n=1 \u03b3n)( lim l\u2192\u221e ( M+l\u2211 n=M+1 \u03b3n l )l)\n\u2264 ( M\u220f n=1 (1 + \u03bbn)) exp(\u039bM ) .\nProposition C.2 Let \u03bb1:\u221e be the eigenvalues of kernel L. Then\nek(\u03bb1:M ) \u2264 ek(\u03bb1:\u221e) (34)\nand\nek(\u03bb1:\u221e) \u2264 k\u2211 j=0 (tr(L)\u2212 \u2211M n=1 \u03bbn) j j! ek\u2212j(\u03bb1:M ) .\n(35)\nProof: Let ek(\u03bb1:m) be the kth elementary symmetric function: ek(\u03bb1:m) = \u2211 J\u2286{1,....,m},|J|=k \u220f j\u2208J \u03bbj .\nAlgorithm 3 Random-Walk Metropolis-Hastings with Posterior Bounds\nInput: Dimension: D, , Starting point: \u03980, Prior distribution: P(\u0398), Proposal distribution f(\u0398\u0302|\u0398) with mean \u0398, samples: X = [X1, . . . , XT ]. \u0398 = \u03980 for i = 0 : \u03c4 do\n\u0398\u0302 \u223c f(\u0398\u0302|\u0398i) r+ =\u221e, r\u2212 = \u2212\u221e u \u223c Uniform[0,1] while u \u2208 [r\u2212, r+] do r+ = ( P+(\u0398\u0302|A1,...,AT ) P\u2212(\u0398i|A1,...,AT ) f(\u0398i|\u0398\u0302) f(\u0398\u0302|\u0398i)\n) r\u2212 = ( P\u2212(\u0398\u0302|A1,...,AT ) P+(\u0398i|A1,...,AT ) f(\u0398i|\u0398\u0302) f(\u0398\u0302|\u0398i)\n) Increase tightness on P+ and P\u2212\nif u < min{1, r\u2212} then \u0398t = \u0398\u0302\nOutput: \u03980:\u03c4\nTrivially, we have a lower bound since the eigenvalues \u03bb1:\u221e are non-negative: ek(\u03bb1:m) \u2264 ek(\u03bb1:n) for m \u2264 n.\nFor the upper bound we can use the Schur-concavity of elementary symmetric functions for non-negative arguments (Guan, 2006).Thus for \u03bb\u03041:N \u227a \u03bb1:N :\nk\u2211 i=1 \u03bb\u0304n \u2264 k\u2211 n=1 \u03bbn for k = 1, . . . , N \u2212 1 (36)\nand N\u2211 n=1 \u03bb\u0304n = N\u2211 n=1 \u03bbn (37)\nwe have ek(\u03bb\u03041:N ) \u2265 ek(\u03bb1:N ). Now let \u039bM = \u2211\u221e n=M+1 and \u039b N M = \u2211N n=M+1. We consider\n\u03bb\u0304 (M) 1:N = (\u03bb1, . . . , \u03bbM , \u039bNM N \u2212M , . . . , \u039bNM N \u2212M ). (38)\nNote that \u03bb\u0304 (M) 1:N \u227a \u03bb1:N and so ek(\u03bb\u0304 (M) 1:N ) \u2265 ek(\u03bb1:N ) for M < N .\nWe now compute ek(\u03bb\u0304 (M) 1:N ). Note that for ek(\u03bb\u0304 (M) 1:N ), the terms in the sum are products of k factors, each containing some of the \u03bb1:M factors and some of the\n\u039bNM N\u2212M factors. The sum of the terms that have j factors of type \u039bNM N\u2212M is ( N\u2212M j ) ( \u039bNM N\u2212M )j ek\u2212j(\u039b(m)), so we have:\nek(\u03bb\u0304 (M) 1:N ) = k\u2211 j=0 ( N \u2212M j )( \u039bNM N \u2212M )j ek\u2212j(\u03bb1:M ) .\nUsing ( N\u2212M j ) \u2264 (N\u2212M) j j! , we get\nek(\u03bb\u0304 (M) 1:N ) = k\u2211 j=0 ( (\u039bNM ) j j! ) ek\u2212j(\u03bb1:M ) .\nLetting N \u2192\u221e, we get out upper bound\nek(\u03bb1:\u221e) \u2264 k\u2211 j=0 ( (\u039bM ) j j! ) ek\u2212j(\u03bb1:M ) for m \u2264 n."}, {"heading": "D. Moments for Continuous DPP with Gaussian Quality and Similarity", "text": "In the continuous case, given the eigendecomposition of the kernel operator, L(x,y) = \u2211\u221e n=1 \u03bbn\u03c6n(x)\n\u2217\u03c6n(y) (where \u03c6n(x)\n\u2217 denotes the complex conjugate of the nth eigenfunction), the mth moment can be evaluated as\nE[xm] = \u222b \u2126 \u221e\u2211 n=1 \u03bbn \u03bbn + 1 xm\u03c6n(x) 2dx . (39)\nHere we present the derivation for moments when\nq(x) = \u221a \u03b1 D\u220f d=1 1 \u221a \u03c0\u03c1d exp { \u2212 x 2 d 2\u03c1d } (40)\nand\nk(x,y) = D\u220f d=1 exp { \u2212 (xd \u2212 yd) 2 2\u03c3d } ,x,y \u2208 RD. (41)\nIn this case, the eigenvalues and eigenvectors of the operator L are given by Fasshauer & McCourt (2012),\n\u03bbn = \u03b1 D\u220f d=1 \u221a 1 \u03b22d+1 2 + 1\n2\u03b3d\n( 1\n\u03b3d(\u03b22d + 1) + 1\n)nd\u22121 ,\n(42) and\n\u03c6n(x) = D\u220f d=1 ( 1 \u03c0\u03c12d ) 1 4 \u221a \u03b2d 2nd\u22121\u0393(nd) exp { \u2212\u03b2 2 dx 2 2\u03c12d } Hnd\u22121 ( \u03b2dxd\u221a \u03c12d ) ,\n(43)\nwhere \u03b3d = \u03c3d \u03c1d , \u03b2d = (1 + 2 \u03b3d ) 1 4 and n = (n1, n2, . . . , nD) is a multi index.\nIn the case of DPPs (as opposed to k-DPPs), we can use the number of items as an estimate of the 0th moment. The 0th moment is given by \u2211 n=1 \u03bbn 1+\u03bbn .\nDenote x = (x1, x2, . . . , xd). The for higher moments, note that\nE[xmj ] = \u222b R \u2211\u221e n=1 \u03bbn \u03bbn+1 xmj \u03c6n(x) 2dxj\n= \u2211\u221e n=1 \u03bbn \u03bbn+1 \u222b R x m j \u03c6n(x) 2dxj\nUsing the results of moment integrals involving a product of two Hermite polynomials (Paris, 2010), we get that\nE[xmj ] = \u222b Rd \u221e\u2211 n \u03bbn \u03bbn + 1 ( \u221a \u03c1j\u221a 2\u03b2j )m\u2118m 2 (nj \u2212 1) (44)\nfor m even and 0 otherwise. The polynomial \u2118m 2 (nj\u22121) is given in Eq. (4.8) in Paris (2010). For example, the second and fourth moments are given by\n(i) E[x2j ] = \u2211\u221e n \u03bbn \u03bbn+1 ( \u221a \u03c1j\u221a 2\u03b2j )2(2nj \u2212 1)\n(ii) E[x4j ] = \u2211\u221e n \u03bbn \u03bbn+1 ( \u221a \u03c1j\u221a 2\u03b2j )43(2n2j \u2212 2nj + 1)\nFor a low dimensional setting, we can learn the parameters by using grid search such that the moments agree."}, {"heading": "E. Details on Simulation", "text": "In the main paper, we use our Bayesian learning algorithms to learn parameters from (i) simulated data generated from a 2-dimensional isotropic discrete kernel (\u03c3d = \u03c3, \u03c1d = \u03c1 for d = 1, 2), (ii) nerve fiber data using 2-dimensional isotropic continuous kernel (\u03c3d = \u03c3, \u03c1d = \u03c1 for d = 1, 2) and (ii) image diversity data using 3600-dimensional discrete kernel with Gaussian similarity. In all of these experiments, we use weakly informative inverse gamma priors on \u03c3,\u03c1 and \u03b1. In particular, for all three parameters, we used the same priors for all three parameters\nP(\u03b1) = P(\u03c1) = P(\u03c3) = Inv-Gamma(0.001, 0.001) (45) We then learn the parameters using hyperrectangle slice sampling."}, {"heading": "F. Details on Image Diversity", "text": "In studying the diversity in images, we extracted 3 different types of features from the images\u2014color features, SIFT-descriptors (Lowe, 1999; Vedaldi & Fulkerson, 2010) and GIST-descriptors (Oliva & Torralba, 2006) described in the supplementary material. We describe these features below.\nColor: Each pixel is assigned a coordinate in threedimensional Lab color space. The colors are then sorted\ninto axis-aligned bins, producing a histogram of either 8 (denoted color8) or 64 (denoted color64) dimensions.\nSIFT: The images are processed to obtain sets of 128- dimensional SIFT descriptors. These descriptors are commonly used in object recognition to identify objects in images and are invariant to scaling, orientation and minor distortions. The descriptors for a given category are combined, subsampled to set of 25,000, and then clustered using k-means into either 256 (denoted SIFT256) or 512 (denoted SIFT512) clusters. The feature vector for an image is the normalized histogram of the nearest clusters to the descriptors in the image.\nGIST: The images are processed to obtain 960- dimensional GIST feature vectors that is commonly used to describe scene structure.\nWe also extracted the features above from the center of the images, defined as the centered rectangle with dimensions half those of the original image. This yields a total of 10 different feature vectors. Since we are only concerned with the diversity of the images, we ensure that the quality across the images are uniform by normalizing each feature vector such that their L2 norm equals to 1. We then combine the feature vectors into 3 types of features- color, SIFT and GIST.\nFor the Google top 6 images, we model the samples, XtTop6 as though they are generated from a 6-DPP with kernel Lsubcat(Xt). To highlight the effect of the human annotation in the partial results sets, we model the samples as though they are generated from a conditional 6-DPP.\nIn general, given a partial set of observations A and k-DPP kernel L, we can define the conditional k-DPP probability of choosing a set B given the inclusion of set A (with |A|+ |B| = k)as\nPkL(Y = A \u222aB|A \u2208 Y ) \u221d det(LAB) (46)\nwith\nLA = ([ ((L+ IAc) \u22121 ] Ac )\u22121 \u2212 I (47)\nwhere IAc denotes the identity matrix with 0 for diagonal corresponding to elements in A. Here, following the N \u00d7N inversion, the matrix is restricted to rows and columns indexed by elements not in A, then inverted again. The normalizer is given by Kulesza & Taskar (2012). \u2211\n|Y \u2032|=k\u2212|A|\ndet(LAY \u2032) (48)\nIn our experiment, our samples can be seperated into the partial result sets and human annotations,\nXtDPP+human = (A t, bt) (49)\nwhere At is the partial result sets and bt is the human annotated result, we model the data from the conditional 6-DPP Lsubcat(bt|At). In this case, the likelihood is given by\nLi(\u0398cat) = det(Li A\nt bt (\u0398cat))\u2211N\ni=1 L i At xi (\u0398\ncat) (50)\nfor each subcategory, i. That is, for each subcategory, i, we compute Li(\u0398cat) and use Eq. 47 to compute the conditional kernel."}], "references": [{"title": "Markov determinantal point processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox"], "venue": "In Proc. UAI,", "citeRegEx": "Affandi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2012}, {"title": "Approximate inference in continuous determinantal processes", "author": ["R.H. Affandi", "E.B. Fox", "B. Taskar"], "venue": "In Proc. NIPS,", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "Nystr\u00f6m approximation for large-scale determinantal processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox", "B. Taskar"], "venue": "In Proc. AISTATS,", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "Determinantal point processes", "author": ["A. Borodin"], "venue": "arXiv preprint arXiv:0911.1153,", "citeRegEx": "Borodin,? \\Q2009\\E", "shortCiteRegEx": "Borodin", "year": 2009}, {"title": "Eynard-Mehta theorem, Schur process, and their Pfaffian analogs", "author": ["A. Borodin", "E.M. Rains"], "venue": "Journal of statistical physics,", "citeRegEx": "Borodin and Rains,? \\Q2005\\E", "shortCiteRegEx": "Borodin and Rains", "year": 2005}, {"title": "Stable evaluation of Gaussian radial basis function interpolants", "author": ["G.E. Fasshauer", "M.J. McCourt"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Fasshauer and McCourt,? \\Q2012\\E", "shortCiteRegEx": "Fasshauer and McCourt", "year": 2012}, {"title": "Inference from iterative simulation using multiple sequences", "author": ["A. Gelman", "D.B. Rubin"], "venue": "Statistical science,", "citeRegEx": "Gelman and Rubin,? \\Q1992\\E", "shortCiteRegEx": "Gelman and Rubin", "year": 1992}, {"title": "Discovering diverse and salient threads in document collections", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "In Proc. EMNLP,", "citeRegEx": "Gillenwater et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2012}, {"title": "Schur-convexity of the complete elementary symmetric function", "author": ["K. Guan"], "venue": "Journal of Inequalities and Applications,", "citeRegEx": "Guan,? \\Q2006\\E", "shortCiteRegEx": "Guan", "year": 2006}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys,", "citeRegEx": "Hough et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hough et al\\.", "year": 2006}, {"title": "Structured determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In Proc. NIPS,", "citeRegEx": "Kulesza and Taskar,? \\Q2010\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2010}, {"title": "k-DPPs: Fixed-size determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In ICML,", "citeRegEx": "Kulesza and Taskar,? \\Q2011\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2011}, {"title": "Learning determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In In Proc. UAI,", "citeRegEx": "Kulesza and Taskar,? \\Q2011\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulesza and Taskar,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2012}, {"title": "Statistical aspects of determinantal point processes", "author": ["F. Lavancier", "J. M\u00f8ller", "E. Rubak"], "venue": "arXiv preprint arXiv:1205.4818,", "citeRegEx": "Lavancier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lavancier et al\\.", "year": 2012}, {"title": "Object recognition from local scaleinvariant features", "author": ["D.G. Lowe"], "venue": "In Computer vision,", "citeRegEx": "Lowe,? \\Q1999\\E", "shortCiteRegEx": "Lowe", "year": 1999}, {"title": "Convergence of the Nelder\u2013Mead simplex method to a nonstationary point", "author": ["McKinnon", "K.I.M"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "McKinnon and I.M.,? \\Q1998\\E", "shortCiteRegEx": "McKinnon and I.M.", "year": 1998}, {"title": "A simplex method for function minimization", "author": ["J. Nelder", "R. Mead"], "venue": "Computer Journal,", "citeRegEx": "Nelder and Mead,? \\Q1965\\E", "shortCiteRegEx": "Nelder and Mead", "year": 1965}, {"title": "Building the gist of a scene: The role of global image features in recognition", "author": ["A. Oliva", "A. Torralba"], "venue": "Progress in brain research,", "citeRegEx": "Oliva and Torralba,? \\Q2006\\E", "shortCiteRegEx": "Oliva and Torralba", "year": 2006}, {"title": "Asymptotics of integrals of hermite polynomials", "author": ["R.B. Paris"], "venue": "Appl. Math. Sci", "citeRegEx": "Paris,? \\Q2010\\E", "shortCiteRegEx": "Paris", "year": 2010}, {"title": "Modelling spatial patterns", "author": ["B.D. Ripley"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Ripley,? \\Q1977\\E", "shortCiteRegEx": "Ripley", "year": 1977}, {"title": "A determinantal point process latent variable model for inhibition in neural spiking data", "author": ["J. Snoek", "R. Zemel", "R.P. Adams"], "venue": "In Proc. NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2013}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "In Proceedings of the international conference on Multimedia,", "citeRegEx": "Vedaldi and Fulkerson,? \\Q2010\\E", "shortCiteRegEx": "Vedaldi and Fulkerson", "year": 2010}, {"title": "Second-order spatial analysis of epidermal nerve fibers", "author": ["L.A. Waller", "A. S\u00e4rkk\u00e4", "V. Olsbo", "M. Myllym\u00e4ki", "I.G. Panoutsopoulou", "W.R. Kennedy", "G. Wendelschafer-Crabb"], "venue": "Statistics in Medicine,", "citeRegEx": "Waller et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Waller et al\\.", "year": 2011}, {"title": "Priors for diversity in generative latent variable models", "author": ["J. Zou", "R.P. Adams"], "venue": "In Proc. NIPS,", "citeRegEx": "Zou and Adams,? \\Q2012\\E", "shortCiteRegEx": "Zou and Adams", "year": 2012}, {"title": "In this case, the eigenvalues and eigenvectors of the operator L are given by Fasshauer", "author": ["y \u2208 R"], "venue": null, "citeRegEx": "x and R.,? \\Q2012\\E", "shortCiteRegEx": "x and R.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recently, DPPs have played an increasingly important role in machine learning and statistics with applications both in the discrete setting\u2014where they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza & Taskar, 2010; 2011a; Snoek et al., 2013)\u2014 and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al.", "startOffset": 197, "endOffset": 303}, {"referenceID": 7, "context": "Recently, DPPs have played an increasingly important role in machine learning and statistics with applications both in the discrete setting\u2014where they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza & Taskar, 2010; 2011a; Snoek et al., 2013)\u2014 and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al.", "startOffset": 197, "endOffset": 303}, {"referenceID": 21, "context": "Recently, DPPs have played an increasingly important role in machine learning and statistics with applications both in the discrete setting\u2014where they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza & Taskar, 2010; 2011a; Snoek et al., 2013)\u2014 and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al.", "startOffset": 197, "endOffset": 303}, {"referenceID": 9, "context": "One of the remarkable aspects of DPPs is that they offer efficient algorithms for inference, including computing the marginal and conditional probabilities (Kulesza & Taskar, 2012), sampling (Affandi et al., 2013a;b; Hough et al., 2006; Kulesza & Taskar, 2010), and restricting to fixed-sized point configurations (k-DPPs)(Kulesza & Taskar, 2011a).", "startOffset": 191, "endOffset": 260}, {"referenceID": 14, "context": "So far, the only attempt to learn the parameters of the similarity kernel k(x,y) has used Nelder-Mead optimization (Lavancier et al., 2012), which lacks theoretical guarantees about convergence to a stationary point.", "startOffset": 115, "endOffset": 139}, {"referenceID": 0, "context": "However, Affandi et al. (2013a) showed that a low-rank approximation to L can be used to recover an approximation to a finite truncation of the eigenvalues representing an important part of the eigenspectrum.", "startOffset": 9, "endOffset": 32}, {"referenceID": 14, "context": "(8) Lavancier et al. (2012) suggests that the Nelder-Mead simplex algorithm (Nelder & Mead, 1965) can be used to maximize L(\u0398).", "startOffset": 4, "endOffset": 28}, {"referenceID": 3, "context": "Borodin (2009) shows that the marginal kernel, K, can be computed directly from L: K = L(I + L)\u22121 .", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "In spatial statistics, one standard quantity used to measure dispersion is the Ripley Kfunction (Ripley, 1977).", "startOffset": 96, "endOffset": 110}, {"referenceID": 22, "context": "Waller et al. (2011) previously analyzed this phenomena based on 6 thigh nerve fiber samples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 22, "context": "Waller et al. (2011) previously analyzed this phenomena based on 6 thigh nerve fiber samples. These samples were collected from 5 diabetic patients at different stages of diabetic neuropathy and one healthy subject. On average, there are 79 points in each sample (see Fig. 4). Waller et al. (2011) analyzed the Ripley K-function and concluded that the", "startOffset": 0, "endOffset": 298}, {"referenceID": 23, "context": "5 indicate that our \u03b3 measure clearly separates the two classes, concurring with the results of Waller et al. (2011). Furthermore, we are able to correctly classify all six samples.", "startOffset": 96, "endOffset": 117}, {"referenceID": 15, "context": "As in Kulesza & Taskar (2011a), we extracted three types of features from the images\u2014color features, SIFT descriptors (Lowe, 1999; Vedaldi & Fulkerson, 2010) and GIST descriptors (Oliva & Torralba, 2006) described in the Supplementary Material.", "startOffset": 118, "endOffset": 157}], "year": 2014, "abstractText": "Determinantal point processes (DPPs) are well-suited for modeling repulsion and have proven useful in many applications where diversity is desired. While DPPs have many appealing properties, such as efficient sampling, learning the parameters of a DPP is still considered a difficult problem due to the non-convex nature of the likelihood function. In this paper, we propose using Bayesian methods to learn the DPP kernel parameters. These methods are applicable in large-scale and continuous DPP settings even when the exact form of the eigendecomposition is unknown. We demonstrate the utility of our DPP learning methods in studying the progression of diabetic neuropathy based on spatial distribution of nerve fibers, and in studying human perception of diversity in images.", "creator": "LaTeX with hyperref package"}}}