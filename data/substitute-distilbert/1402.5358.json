{"id": "1402.5358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2014", "title": "Extended Breadth-First Search Algorithm", "abstract": "the task of artificial intelligence is to provide representation techniques for describing problems, et well as search algorithms that can be used to answer our answer. a widespread and elaborated model is state - space representation, which, inevitably, has practical shortcomings. classical search algorithms are not applicable in practice when the state space does and only really microscopic tens of thousands of states. this can give remedy to this problem that compiling some kind of heuristic knowledge. in case of classical state - space representation, heuristic must be defined so that it qualifies an arbitrary hypothesis based on its \" goodness, \" which is obviously not trivial. in our paper, we presented an insight that gives us the tendency to handle huge state spaces and to implement a heuristic concept which is easier to embed into search algorithms.", "histories": [["v1", "Fri, 21 Feb 2014 17:21:52 GMT  (474kb)", "http://arxiv.org/abs/1402.5358v1", "5 pages, 1 figure, 1 table"]], "COMMENTS": "5 pages, 1 figure, 1 table", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tam\\'as k\\'adek", "j\\'anos p\\'anovics"], "accepted": false, "id": "1402.5358"}, "pdf": {"name": "1402.5358.pdf", "metadata": {"source": "META", "title": "Extended Breadth-First Search Algorithm", "authors": ["Tam\u00e1s K\u00e1dek", "J\u00e1nos P\u00e1novics"], "emails": ["kadek.tamas@inf.unideb.hu", "panovics.janos@inf.unideb.hu"], "sections": [{"heading": null, "text": "techniques for describing problems, as well as search algorithms that can be used to answer our questions. A widespread and elaborated model is state-space representation, which, however, has some shortcomings. Classical search algorithms are not applicable in practice when the state space contains even only a few tens of thousands of states. We can give remedy to this problem by defining some kind of heuristic knowledge. In case of classical state-space representation, heuristic must be defined so that it qualifies an arbitrary state based on its \u201cgoodness,\u201d which is obviously not trivial. In our paper, we introduce an algorithm that gives us the ability to handle huge state spaces and to use a heuristic concept which is easier to embed into search algorithms. Keywords: Artificial Intelligence, State-Space Representation, Extended Model, Breadth-First Search."}, {"heading": "1. Introduction", "text": "The most basic problem representation technique used by artificial intelligence is state-space representation. However, it can be used to describe only a certain small subset of problems conveniently. For example, even a simple chess puzzle may have too many ways to continue from a particular situation because each piece can move in quite a few directions and potentially more than one square. Of course, we do not have to deal with all the possible moves in a particular situation provided we have a means to mark the cases that are relevant regarding the solution's viewpoint. This can only be done only if we have some additional knowledge about the problem, which is usually represented by a heuristic function. In case we have this additional knowledge, however, it may still be difficult to describe it as a function. For example, in the 8-puzzle game, the heuristic function evaluates a situation as being more appealing if it has more pieces on their correct places, but this measure will fail in some cases.\nBefore we propose a new algorithm that deals with the above-mentioned problems, we first have to define an\nrepresent the additional (heuristic) knowledge about the problems. After that, we show an extended breadth-first search (EBFS) algorithm that uses the extended model and is able to handle larger state spaces. Finally, we compare this algorithm with the standard breadth-first search via a particular problem."}, {"heading": "2. An Extended State-Space Model (ESSM)", "text": "Using state-space representation, solutions to problems are obtained by executing a series of well-defined steps. During the execution of each step, newer and newer states are created, which form the state space. States are distinguished from one another based on their relevant properties. Relevant properties are defined by the sets of their possible values, so a state can be represented as an element of the Cartesian product of these sets. Let us denote this Cartesian product by S. Possible steps are then operations on the elements of S. Let us denote the set of operations by F. The state space is often illustrated as a graph, in which nodes represent states, and edges represent operations. This way, searching for a solution to a problem can be done actually using a path-finding algorithm.\nWe keep the basic idea (i.e., the concepts of states and operations on states) also in the extended state-space model (ESSM). The goal of this generalization is to provide the ability to model as many systems not conforming to the classical interpretation as possible in a uniform manner.\nA state-space representation over state space S is defined as a 5-tuple of the form\nK, initial, goal, F, B,\nwhere\n K is a nonempty set containing the initially known\nstates. Of course, K \u2286 S. The set of initially known states is usually incomplete, nevertheless, only these states can be used as a starting point to explore the\nstate space by applying the operators. Note that by applying the operators on the elements of K an arbitrary number of times, S is not necessarily covered.\n initial is a Boolean function that selects the initial\nstates from the state space:\ninitial : S  {true, false}\n goal is a Boolean function that selects the goal states\nfrom the state space:\ngoal : S  {true, false}\n F = {f1, f2, \u2026, fn} is a set of \u201cforward\u201d functions,\nwhich represent the operators in the classical sense. Operators can be used to create a new state (or even a set of new states in the extended model) from a given state.\nfi : S  2S\n B = {b1, b2, \u2026, bm} is a set of \u201cbackward\u201d functions,\nwhich usually give the states from which a given state can be obtained by applying functions in F.\nbi : S  2S\nSome notes:\n The number of initial and goal states is not necessarily\nknown initially, as we may not be able to or may not intend to generate the whole set S before or during the search.\n The n + m = 0 case is excluded because in that case,\nnothing would represent the relationship between the states.\n Although the elements of the sets F and B are formally\nsimilar functions, their semantics are quite different. The real set-valued functions in F are used to represent nondeterministic operators, while there may be real set-valued functions in set B even in case of deterministic operators.\nLet us now introduce a couple of concepts:\n Initial state: a state s for which s  S and\ninitial(s) = true.\n Goal state: a state s for which s  S and goal(s) = true.\n Known initial state: an initial state in K.\n Known goal state: a goal state in K.\n Edge: an s, s', o  S \u00d7 S \u00d7 (F  B) triple where if\no  F, then s'  o(s), and if o  B, then s  o(s').\n Path: an ordered sequence of edges in the form\ns1, s2, o1, s2, s3, o2, \u2026, sk \u2013 1, sk, ok \u2013 1,\nwhere k \u2265 2.\nGeneral objective: determine a path from s0 to s*, where s0 is an initial state, and s* is a goal state.\n2.1 A Few Properties of ESSM Representations\nFor classifying state-space representations, let us define\nsome important properties. Let p = K, initial, goal, F, B a state-space representation over S. p is said to be\n deterministic if for all s  S and f  F, |f(s)| \u2264 1. If\n|f(s)| = 0, then we say that the operator represented by the forward function f is not applicable to state s. If\nfor some s  S and f  F, |f(s)| > 1 (i.e., f is setvalued), then the representation is called nondeterministic. In this case, the operator represented by f may generate any state in the result set, even different states on different applications. In this paper, we will only focus on deterministic cases.\n symmetric if ss' (k (s'  fk(s))  l (s  bl(s'))).\nThis means that for each path P, there exists a path P\u2019 that contains the same state pairs in the same order and contains only functions in F or functions in B.\n antisymmetric if\nss' ((k (s'  fk(s))  l (s  bl(s'))) \n (l (s  bl(s'))  k (s'  fk(s)))).\nIn this case, each edge is given in one way only.\n strictly symmetric if\nF = {f1, f2, \u2026, fn}, B = {b1, b2, \u2026, bn}, and\nss'k (s'  fk(s)  s  bk(s')).\nThe definition implies that a strictly symmetric representation is also symmetric.\n one-way forward if B = .\n one-way backward if F = .\n set up with a single initial state if there exists one and\nonly one s0  S for which initial(s0) = true.\n set up with multiple initial states if there exists more\nthan one s  S for which initial(s) = true.\nIn the extended model, the classical state-space representation is a deterministic, antisymmetric representation set up with a single initial state in the following form:\n{s0}, s \u2192 (s = s0), goal, {f1, f2, \u2026, fn}, \n      otherwise, ),(dom if)}({ )( i ossiosif\nwhere oi is an operator in the traditional sense, for which oi : D \u2192 S and D \u2286 S (i = 1, 2, \u2026, n)."}, {"heading": "3. Model Restrictions for EBFS", "text": "Before describing the EBFS algorithm, we first give the model serving as an adequate representation technique for problems suffering from the above-mentioned drawbacks, i.e., the large number of states and nontrivial heuristic functions. We can now make use of the advantage of\nESSM that more than one state (set K) may be defined as the input of the search. The basic idea is that the given states should be relevant. This means that the heuristic function is replaced with the enumeration of the states that are considered (potentially) useful. In other words, the given states are predictably a part of one of the solutions. Similarly to using a heuristic function, this prediction is not necessarily perfect. There is only one limitation: at least one of the given states should be on a path representing a solution. Whenever an initial state is included in K, this condition is satisfied.\nWe can also keep the following properties of the extended model:\n it is allowed to have more than one initial and goal\nstates,\n we are able to use both forward and backward\nfunctions, so the representation can be symmetric, antisymmetric, or strictly symmetric. As mentioned above, we set aside nondeterministic representations for now.\nLet us now consider a state-space representation that suffers from the presented problems, i.e., a representation whose state space is big enough and for which it is hard to define heuristic as a function. Such a representation exists for the well-known n-queens problem. In this\nrepresentation, a state is defined by an n  n Boolean matrix, the cells of which represent the squares of a chessboard. An element of the matrix is true if there is a queen on that square and false if it is empty. We have as many operators as many squares on the chessboard. Note that this representation is far from the best choice when it is about solving this problem. We only chose this because it has the drawbacks described earlier."}, {"heading": "4. The EBFS Algorithm", "text": "The EBFS algorithm extends the BFS algorithm with the ability to run more than one breadth-first search starting from more than one state (the inititally known states). It is particularly useful if the subtrees explored reach one another as illustrated by Figure 1. The dashed line denotes the subtree that is discovered by the standard BFS algorithm starting from i1 if the nearest goal state is g1. However, in case we give also the states k1, k2, k3 besides i1 as potentially useful states, then the discovered part of the graph is smaller, even if k1 did not prove to be useful for finding the solution as the illustration shows.\nThe EBFS algorithm stores a subgraph of the representation graph during the search. For each node, it stores the state represented by the node as usual. If we have forward functions, we also need to store the forward status (open, closed, or not relevant), forward parents, forward children of the node, as well as the forward distance from each of the initially known states. Note that the forward functions represent the operators in the classical sense. The main difference from BFS at this point is that in case of EBFS, the relationship between the nodes and each initially known state is stored. Because of the ESSM model, we are able to use backward functions as well. If B is not an empty set, then we store the above information also for the backward functions. In this case, we need the status of \u201cnot relevant\u201d. For example, the forward status of a node should be not relevant if it is only discovered using backward functions (because this node is not yet relevant for forward searches). For the sake of simplicity, we now consider B an empty set and keep only the B-DISTANCE property so that we can check the termination condition as if we had some backward functions.\n4.1 The Pseudocode of the Algorithm\nfunction NEW-NODE(state) begin STATE[node]  state F-STATUS  nil F-PARENTS[node]   F-CHILDREN[node]   F-DISTANCE[node]  (\u221e,\u221e,\u2026,\u221e) B-DISTANCE[node]  (\u221e,\u221e,\u2026,\u221e) return node end function\nprocedure EBFS begin nodes   i  1 for all k in K do new  NEWNODE(k) F-STATUS[new]  open F-DISTANCE[new]i  0 B-DISTANCE[new]i  0 nodes  nodes  new i  i+1 end for\nwhile true do if { n | n  nodes \nF-STATUS[n] = open } =  then terminate unsuccessfully end if curr  SELECT(nodes) EXPAND(curr, nodes) if GOAL-CONDITION(nodes) then terminate successfully end if end while end procedure\nThe main algorithm is very similar to BFS: it is a series of expansions and termination condition checks.\nfunction SELECT(nodes) begin for all n in nodes do if n  { m | m  nodes  min(F-DISTANCE(m)) <=\nmin({ min(F-DISTANCE(o)) |\no  nodes }) } then return n end if end for return nil end function\nprocedure EXPAND(curr, nodes) begin for all f in F do newstate  f(state(curr)) node  SEARCH(nodes, newstate) if node = nil or\nF-STATUS[node] = not-relevant then if node = nil then node  NEWNODE(newstate) end if f-status[node] = open end if F-CHILDREN[curr] \nF-CHILDREN[curr]  node F-PARENT[node]  F-PARENT[node]  curr F-UPDATE(node, F-DISTANCE[curr]) end for end procedure\nDuring expansion, we apply all the operators as usual. In the general algorithm, both the forward and backward functions would need to be considered inside the SELECT and EXPAND functions. The SEARCH function checks whether the new state is already in the database.\nprocedure F-UPDATE(node, parent-distance) begin new-distance  (\u221e,\u221e,\u2026,\u221e) for all i in {1, 2, \u2026, count(K)} do new-distancei  min(F-DISTANCE[node]i, 1 + parent-distancei) end for if F-DISTANCE[node] <> new-distance then F-DISTANCE[node]  new-distance if F-STATUS[node] = closed then for all n in F-CHILDREN[node] do F-UPDATE(n, new-distance) end for end if end if end procedure\nThe F-UPDATE function recursively updates the stored information about the nodes whenever an initially known state becomes reachable from another one during the search.\nfunction GOAL-CONDITION(nodes) begin for all s in { n | n \u220a nodes  initial(STATE(n)) } do for all g in { n | n \u220a nodes  goal(STATE(n)) } do for all i in {1,2,\u2026,count(K)} do if B-DISTANCE(s)i <> \u221e and F-DISTANCE(g)i <> \u221e then return true end if end for end for end for return false end function\nThis function checks whether there is an initial state and a goal state such that the goal state can be reached from the initial state via an initially known state. Note that in the simplified case, initial states must also be initially known states."}, {"heading": "5. Results", "text": "The state-space representation described in Section 3 illustrates when the EBFS algorithm can be useful. We ran the EBFS and the classical BFS algorithms with the n-queens problem with different values of n and summarized the results in the following table:\nThe table clearly shows that even with only two initially known states, the number of states explored during the EBFS search until the successful termination is much less than that of the BFS, which is the same as in the case of EBFS with only one initially known state: the initial state (when the board is empty). When we had two initially known states, then for all values of n, one of them was the initial state of the problem (which is a sufficient condition for finding a solution if one exists), and the other was a state on a path that represents one of the solutions. The last column shows the case when we added a third state to the two described above with the intention to give a false heuristic: the two states other than the initial state were not reachable from each other. Note that even with including states that are later found to be useless during the search, the number of states explored are still much less than with BFS (of course, this figure highly depends on the selected initially known states)."}, {"heading": "6. Conclusions and Future Work", "text": "As you can see from the comparison table, EBFS outperforms the classical BFS algorithm in cases when the state space is large, but we can give a couple of states which we think to form a part of a solution. Introducing the EBFS algorithm was only enabled by creating an extended state-space model first. The EBFS algorithm itself is an extension of the classical BFS algorithm. The question that arises now is how it is possible to extend other graph search algorithms such as uniform-cost search."}, {"heading": "Acknowledgments", "text": "The publication was supported by the T\u00c1MOP-4.2.2.C11/1/KONV-2012-0001 project. The project has been supported by the European Union, co-financed by the European Social Fund."}], "references": [{"title": "Direction- Optimizing Breadth-First Search", "author": ["S. Beamer", "K. Asanovi\u0107", "D. Patterson"], "venue": "in International Conference on High Performance Computing, Networking, Storage and Analysis (SC", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Tractable Planning Under Uncertainty: Exploiting Structure", "author": ["J. Pineau"], "venue": "Ph.D. thesis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}], "referenceMentions": [], "year": 2013, "abstractText": "The task of artificial intelligence is to provide representation techniques for describing problems, as well as search algorithms that can be used to answer our questions. A widespread and elaborated model is state-space representation, which, however, has some shortcomings. Classical search algorithms are not applicable in practice when the state space contains even only a few tens of thousands of states. We can give remedy to this problem by defining some kind of heuristic knowledge. In case of classical state-space representation, heuristic must be defined so that it qualifies an arbitrary state based on its \u201cgoodness,\u201d which is obviously not trivial. In our paper, we introduce an algorithm that gives us the ability to handle huge state spaces and to use a heuristic concept which is easier to embed into search algorithms.", "creator": "Microsoft\u00ae Word 2013"}}}