{"id": "1402.6763", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2014", "title": "Linear Programming for Large-Scale Markov Decision Problems", "abstract": "we consider the problem of controlling a markov decision utility ( mdp ) with a large state space, so as to minimize average cost. since bandwidth is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing out a low - dimensional family of policies. we use the adaptive linear programming formulation of the mdp average cost problem, in which the variable fits a stationary distribution over state - action pairs, secondly we consider a neighborhood of a low - dimensional vector of the locus of stationary distributions ( defined in terms of no - action features ) as alternative comparison class. we propose two techniques, one based on stochastic linear optimization, and one based on frequency sampling. in both cases, we give bounds estimates show that the likelihood therefore our algorithms approaches the best achievable by any policy in the comparison space. most importantly, these results add towards the size of the comparison class, but not on the size of the state space. preliminary experiments showed considerable accuracy of the proposed algorithms in a queuing application.", "histories": [["v1", "Thu, 27 Feb 2014 01:43:38 GMT  (64kb)", "http://arxiv.org/abs/1402.6763v1", "27 pages, 3 figures"]], "COMMENTS": "27 pages, 3 figures", "reviews": [], "SUBJECTS": "math.OC cs.AI cs.NA", "authors": ["alan malek", "yasin abbasi-yadkori", "peter l bartlett"], "accepted": true, "id": "1402.6763"}, "pdf": {"name": "1402.6763.pdf", "metadata": {"source": "CRF", "title": "Linear Programming for Large-Scale Markov Decision Problems", "authors": ["Yasin Abbasi-Yadkori"], "emails": ["yasin.abbasiyadkori@qut.edu.au", "bartlett@eecs.berkeley.edu", "malek@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n67 63\nv1 [\nm at\nh. O\nWe consider the problem of controlling a Markov decision process (MDP) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a lowdimensional family of policies. We use the dual linear programming formulation of the MDP average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low-dimensional subset of the set of stationary distributions (defined in terms of state-action features) as the comparison class. We propose two techniques, one based on stochastic convex optimization, and one based on constraint sampling. In both cases, we give bounds that show that the performance of our algorithms approaches the best achievable by any policy in the comparison class. Most importantly, these results depend on the size of the comparison class, but not on the size of the state space. Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application."}, {"heading": "1 Introduction", "text": "We study the average loss Markov decision process problem. The problem is well-understood when the state and action spaces are small (Bertsekas, 2007). Dynamic programming (DP) algorithms, such as value iteration (Bellman, 1957) and policy iteration (Howard, 1960), are standard techniques to compute the optimal policy. In large state space problems, exact DP is not feasible as the computational complexity scales quadratically with the number of states.\nA popular approach to large-scale problems is to restrict the search to the linear span of a small number of features. The objective is to compete with the best solution within this comparison class. Two popular methods are Approximate Dynamic Programming (ADP) and Approximate Linear\nProgramming (ALP). This paper focuses on ALP. For a survey on theoretical results for ADP see (Bertsekas and Tsitsiklis, 1996, Sutton and Barto, 1998), (Bertsekas, 2007, Vol. 2, Chapter 6), and more recent papers (Sutton et al., 2009b,a, Maei et al., 2009, 2010).\nOur aim is to develop methods that find policies with performance guaranteed to be close to the best in the comparison class but with computational complexity that does not grow with the size of the state space. All prior work on ALP either scales badly or requires access to samples from a distribution that depends on the optimal policy.\nThis paper proposes a new algorithm to solve the Approximate Linear Programming problem that is computationally efficient and does not require knowledge of the optimal policy. In particular, we introduce new proof techniques and tools for average cost MDP problems and use these techniques to derive a reduction to stochastic convex optimization with accompanying error bounds. We also propose a constraint sampling technique and obtain performance guarantees under an additional assumption on the choice of features."}, {"heading": "1.1 Notation", "text": "Let X and A be positive integers. Let X = {1, 2, . . . ,X} and A = {1, 2, . . . , A} be state and action spaces, respectively. Let \u2206S denote probability distributions over set S. A policy \u03c0 is a map from the state space to \u2206A: \u03c0 : X \u2192 \u2206A. We use \u03c0(a|x) to denote the probability of choosing action a in state x under policy \u03c0. A transition probability kernel (or transition kernel) P : X \u00d7 A \u2192 \u2206X maps from the direct product of the state and action spaces to \u2206X . Let P \u03c0 denote the probability transition kernel under policy \u03c0. A loss function is a bounded real-valued function over state and action spaces, \u2113 : X \u00d7 A \u2192 [0, 1]. Let Mi,: and M:,j denote ith row and jth column of matrix M respectively. Let \u2016v\u20161,c = \u2211 i ci |vi| and \u2016v\u2016\u221e,c = maxi ci |vi| for a positive vector c. We use 1 and 0 to denote vectors with all elements equal to one and zero, respectively. We use \u2227 and \u2228 to denote the minimum and the maximum, respectively. For vectors v and w, v \u2264 w means element-wise inequality, i.e. vi \u2264 wi for all i."}, {"heading": "1.2 Linear Programming Approach to Markov Decision Problems", "text": "Under certain assumptions, there exist a scalar \u03bb\u2217 and a vector h\u2217 \u2208 RX that satisfy the Bellman optimality equations for average loss problems,\n\u03bb\u2217 + h\u2217(x) = min a\u2208A\n[ \u2113(x, a) + \u2211\nx\u2032\u2208X\nP(x,a),x\u2032h\u2217(x \u2032) ] .\nThe scalar \u03bb\u2217 is called the optimal average loss, while the vector h\u2217 is called a differential value function. The action that minimizes the right-hand side of the above equation is the optimal action in state x and is denoted by a\u2217(x). The optimal policy is defined by \u03c0\u2217(a\u2217(x)|x) = 1. Given \u2113 and\nP , the objective of the planner is to compute the optimal action in all states, or equivalently, to find the optimal policy.\nThe MDP problem can also be stated in the LP formulation (Manne, 1960),\nmax \u03bb,h \u03bb , (1) s.t. B(\u03bb1+ h) \u2264 \u2113+ Ph ,\nwhere B \u2208 {0, 1}XA\u00d7X is a binary matrix such that the ith column has A ones in rows 1+ (i\u2212 1)A to iA. Let v\u03c0 be the stationary distribution under policy \u03c0 and let \u00b5\u03c0(x, a) = v\u03c0(x)\u03c0(a|x). We can write\n\u03c0\u2217 = argmin \u03c0\n\u2211\nx\u2208X\nv\u03c0(x) \u2211\na\u2208A\n\u03c0(a|x)\u2113(x, a)\n= argmin \u03c0\n\u2211\n(x,a)\u2208X\u00d7A\n\u00b5\u03c0(x, a)\u2113(x, a) = argmin \u03c0\n\u00b5\u22a4\u03c0 \u2113 .\nIn fact, the dual of LP (1) has the form of\nmin \u00b5\u2208RXA\n\u00b5\u22a4\u2113 , (2)\ns.t. \u00b5\u22a41 = 1, \u00b5 \u2265 0, \u00b5\u22a4(P \u2212B) = 0 .\nThe objective function, \u00b5\u22a4\u2113, is the average loss under stationary distribution \u00b5. The first two constraints ensure that \u00b5 is a probability distribution over state-action space, while the last constraint ensures that \u00b5 is a stationary distribution. Given a solution \u00b5, we can obtain a policy via \u03c0(a|x) = \u00b5(x, a)/\u2211a\u2032\u2208A \u00b5(x, a\u2032)."}, {"heading": "1.3 Approximations for Large State Spaces", "text": "The LP formulations (1) and (2) are not practical for large scale problems as the number of variables and constraints grows linearly with the number of states. Schweitzer and Seidmann (1985) propose approximate linear programming (ALP) formulations. These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al. (2004), Petrik and Zilberstein (2009), Desai et al. (2012). As noted by Desai et al. (2012), the prior work on ALP either requires access to samples from a distribution that depends on optimal policy or assumes the ability to solve an LP with as many constraints as states. (See Section 2 for a more detailed discussion.) Our objective is to design algorithms for very large MDPs that do not require knowledge of the optimal policy.\nIn contrast to the aforementioned methods, which solve the primal ALPs (with value functions as variables), we work with the dual form (2) (with stationary distributions as variables). Analogous\nto ALPs, we control the complexity by limiting our search to a linear subspace defined by a small number of features. Let d be the number of features and \u03a6 be a (XA)\u00d7 d matrix with features as column vectors. By adding the constraint \u00b5 = \u03a6\u03b8, we get\nmin \u03b8\n\u03b8\u22a4\u03a6\u22a4\u2113 ,\ns.t. \u03b8\u22a4\u03a6\u22a41 = 1, \u03a6\u03b8 \u2265 0, \u03b8\u22a4\u03a6\u22a4(P \u2212B) = 0 .\nIf a stationary distribution \u00b50 is known, it can be added to the linear span to get the ALP\nmin \u03b8\n(\u00b50 +\u03a6\u03b8) \u22a4\u2113 , (3)\ns.t. (\u00b50 +\u03a6\u03b8) \u22a41 = 1, \u00b50 +\u03a6\u03b8 \u2265 0, (\u00b50 +\u03a6\u03b8)\u22a4(P \u2212B) = 0 .\nAlthough \u00b50 +\u03a6\u03b8 might not be a stationary distribution, it still defines a policy 1\n\u03c0\u03b8(a|x) = [\u00b50(x, a) + \u03a6(x,a),:\u03b8]+\u2211 a\u2032 [\u00b50(x, a \u2032) + \u03a6(x,a\u2032),:\u03b8]+ , (4)\nWe denote the stationary distribution of this policy \u00b5\u03b8 which is only equal to \u00b50 +\u03a6\u03b8 if \u03b8 is in the feasible set."}, {"heading": "1.4 Problem definition", "text": "With the above notation, we can now be explicit about the problem we are solving.\nDefinition 1 (Efficient Large-Scale Dual ALP). For an MDP specified by \u2113 and P , the efficient large-scale dual ALP problem is to produce parameters \u03b8\u0302 such\n\u00b5\u22a4 \u03b8\u0302 \u2113 \u2264 min { \u00b5\u22a4\u03b8 \u2113 : \u03b8 feasible for (3) } +O(\u01eb) (5)\nin time polynomial in d and 1/\u01eb. The model of computation allows access to arbitrary entries of \u03a6, \u2113, P , \u00b50, P \u22a4\u03a6, and 1\u22a4\u03a6 in unit time.\nImportantly, the computational complexity cannot scale with X and we do not assume any knowledge of the optimal policy. In fact, as we shall see, we solve a harder problem, which we define as follows.\nDefinition 2 (Expanded Efficient Large-Scale Dual ALP). Let V : \u211cd \u2192 \u211c+ be some \u201cviolation function\u201d that represents how far \u00b50+\u03a6\u03b8 is from a valid stationary distribution, satisfying V (\u03b8) = 0 if \u03b8 is a feasible point for the ALP (3). The expanded efficient large-scale dual ALP problem is to\n1We use the notation [v]\u2212 = v \u2227 0 and [v]+ = v \u2228 0.\nproduce parameters \u03b8\u0302 such that\n\u00b5\u22a4 \u03b8\u0302 \u2113 \u2264 min { \u00b5\u22a4\u03b8 \u2113+ 1 \u01eb V (\u03b8) : \u03b8 \u2208 \u211cd } +O(\u01eb), (6)\nin time polynomial in d and 1/\u01eb, under the same model of computation as in Definition 1.\nNote that the expanded problem is strictly more general as guarantee (6) implies guarantee (5). Also, many feature vectors \u03a6 may not admit any feasible points. In this case, the dual ALP problem is trivial, but the expanded problem is still meaningful.\nHaving access to arbitrary entries of the quantities in Definition 1 arises naturally in many situations. In many cases, entries of P\u22a4\u03a6 are easy to compute. For example, suppose that for any state x\u2032 there are a small number of state-action pairs (x, a) such that P (x\u2032|x, a) > 0. Consider Tetris; although the number of board configurations is large, each state has a small number of possible neighbors. Dynamics specified by graphical models with small connectivity also satisfy this constraint. Computing entries of P\u22a4\u03a6 is also feasible given reasonable features. If a feature \u03d5i is a stationary distribution, then P\u22a4\u03d5i = B \u22a4\u03d5i. Otherwise, it is our prerogative to design sparse feature vectors, hence making the multiplication easy. We shall see an example of this setting later."}, {"heading": "1.5 Our Contributions", "text": "In this paper, we introduce an algorithm that solves the expanded efficient large-scale dual ALP problem under a (standard) assumption that any policy converges quickly to its stationary distribution.\nOur algorithm take as input a constant S and an error tolerance \u01eb, and has access to the various products listed in Definition 1. Define \u0398 = {\u03b8 : \u03b8\u22a4\u03a6\u22a41 = 1 \u2212 \u00b5\u22a40 1, \u2016\u03b8\u2016 \u2264 S}. If no stationary distribution is known, we can simply choose \u00b50 = 0. The algorithm is based on stochastic convex optimization. We prove that for any \u03b4 \u2208 (0, 1), after O(1/\u01eb4) steps of gradient descent, the algorithm finds a vector \u03b8\u0302 \u2208 \u0398 such that, with probability at least 1\u2212 \u03b4,\n\u00b5\u22a4 \u03b8\u0302 \u2113 \u2264\u00b5\u22a4\u03b8 \u2113+\n1 \u01eb \u2016[\u00b50 +\u03a6\u03b8]\u2212\u20161 + 1 \u01eb \u2225\u2225\u2225(P \u2212B)\u22a4(\u00b50 +\u03a6\u03b8) \u2225\u2225\u2225 1 +O(\u01eb log(1/\u03b4))\nholds for all \u03b8 \u2208 \u0398; i.e., we solve the expanded problem for V (\u03b8) equal to the L1 error of the violation. The second and third terms are zero for feasible points (points in the intersection of the feasible set of LP (2) and the span of the features). For points outside the feasible set, these terms measure the extent of constraint violations for the vector \u00b50 + \u03a6\u03b8, which indicate how well stationary distributions can be represented by the chosen features.\nThe above performance bound scales with 1/\u01eb that can be large when the feasible set is empty and \u01eb is very small. We propose a second approach and show that this dependence can be eliminated if we have extra information about the MDP. Our second approach is based on the constraint\nsampling method that is already applied to large-scale linear programming and MDP problems (de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008). We obtain performance bounds, but under the condition that a suitable function that controls the size of constraint violations is available. Our proof technique is different from previous work and gives stronger performance bounds.\nOur constraint sampling method takes two extra inputs: functions v1 and v2 that specify the amount of constraint violations that we tolerate at each state-action pair. The algorithm samples O(d\u01eb log 1 \u01eb ) constraints and solves the sampled LP problem. Let \u03b8\u0303 denote the solution of the sampled ALP and \u03b8\u2217 denote the solution of the full ALP subject to constraints v1 and v2. We prove that with high probability,\n\u2113\u22a4\u00b5 \u03b8\u0303 \u2264 \u2113\u22a4\u00b5\u03b8\u2217 +O(\u01eb+ \u2016v1\u20161 + \u2016v2\u20161) ."}, {"heading": "2 Related Work", "text": "de Farias and Van Roy (2003a) study the discounted version of the primal form (1). Let c \u2208 RX be a vector with positive components and \u03b3 \u2208 (0, 1) be a discount factor. Let L : RX \u2192 RX be the Bellman operator defined by (LJ)(x) = mina\u2208A(\u2113(x, a)+\u03b3 \u2211 x\u2032\u2208X P(x,a),x\u2032J(x\n\u2032)) for x \u2208 X . Let \u03a8 \u2208 RX\u00d7d be a feature matrix. The exact and approximate LP problems are as follows:\nmax J\u2208RX c\u22a4J , max w\u2208Rd c\u22a4\u03a8w , s.t. LJ \u2265 J , s.t. L\u03a8w \u2265 \u03a8w .\nwhich can also be written as\nmax J\u2208RX c\u22a4J , max w\u2208Rd c\u22a4\u03a8w , (7) s.t. \u2200(x, a), \u2113(x, a) + \u03b3P(x,a),:J \u2265 J(x) , s.t. \u2200(x, a), \u2113(x, a) + \u03b3P(x,a),:\u03a8w \u2265 (\u03a8w)(x) .\nThe optimization problem on the RHS is an approximate LP with the choice of J = \u03a8w. Let J\u03c0(x) = E [\u2211\u221e t=0 \u03b3 t\u2113(xt, \u03c0(xt))|x0 = x ] be value of policy \u03c0, J\u2217 be the solution of LHS, and \u03c0J(x) = argmina\u2208A(\u2113(x, a)+\u03b3P(x,a),:J) be the greedy policy with respect to J . Let \u03bd \u2208 \u2206X be a probability distribution and define \u00b5\u03c0,\u03bd = (1 \u2212 \u03b3)\u03bd\u22a4(I \u2212 \u03b3P \u03c0)\u22121. de Farias and Van Roy (2003a) prove that for any J satisfying the constraints of the LHS of (7),\n\u2016J\u03c0J \u2212 J\u2217\u20161,\u03bd \u2264 1\n1\u2212 \u03b3 \u2016J \u2212 J\u2217\u20161,\u00b5\u03c0J ,\u03bd . (8)\nDefine \u03b2u = \u03b3maxx,a \u2211 x\u2032 P(x,a),x\u2032u(x \u2032)/u(x). Let U = {u \u2208 RX : u \u2265 1, u \u2208 span(\u03a8), \u03b2u < 1}. Let\nw\u2217 be the solution of ALP. de Farias and Van Roy (2003a) show that for any u \u2208 U ,\n\u2016J\u2217 \u2212\u03a8w\u2217\u20161,c \u2264 2c\u22a4u\n1\u2212 \u03b2u min w\n\u2016J\u2217 \u2212\u03a8w\u2016\u221e,1/u . (9)\nThis result has a number of limitations. First, solving ALP can be computationally expensive as the number of constraints is large. Second, it assumes that the feasible set of ALP is non-empty. Finally, Inequality (8) implies that c = \u00b5\u03c0\u03a8w\u2217 ,\u03bd is an appropriate choice to obtain performance bounds. However, w\u2217 itself is function of c and is not known before solving ALP.\nde Farias and Van Roy (2004) propose a computationally efficient algorithm that is based on a constraint sampling technique. The idea is to sample a relatively small number of constraints and solve the resulting LP. Let N \u2282 Rd be a known set that contains w\u2217 (solution of ALP). Let \u00b5V\u03c0,c(x) = \u00b5\u03c0,c(x)V (x)/(\u00b5 \u22a4 \u03c0,cV ) and define the distribution \u03c1 V \u03c0,c(x, a) = \u00b5 V \u03c0,c(x)/A. Let \u03b4 \u2208 (0, 1) and \u01eb \u2208 (0, 1). Let \u03b2u = \u03b3maxx \u2211 x\u2032 P(x,\u03c0\u2217(x)),x\u2032u(x \u2032)/u(x) and\nD = (1 + \u03b2V )\u00b5\n\u22a4 \u03c0\u2217,cV\n2c\u22a4J\u2217 sup w\u2208N\n\u2016J\u2217 \u2212\u03a8w\u2016\u221e,1/V , m \u2265 16AD\n(1\u2212 \u03b3)\u01eb\n( d log 48AD\n(1\u2212 \u03b3)\u01eb + log 2 \u03b4\n) .\nLet S be a set of m random state-action pairs sampled under \u03c1V\u03c0\u2217,c. Let w\u0302 be a solution of the following sampled LP:\nmax w\u2208Rd\nc\u22a4\u03a8w ,\ns.t. w \u2208 N , \u2200(x, a) \u2208 S, \u2113(x, a) + \u03b3P(x,a),:\u03a8w \u2265 (\u03a8w)(x) .\nde Farias and Van Roy (2004) prove that with probability at least 1\u2212 \u03b4, we have\n\u2016J\u2217 \u2212\u03a8w\u0302\u20161,c \u2264 \u2016J\u2217 \u2212\u03a8w\u2217\u20161,c + \u01eb \u2016J\u2217\u20161,c .\nThis result has a number of limitations. First, vector \u00b5\u03c0\u2217,c (that is used in the definition of D) depends on the optimal policy, but an optimal policy is what we want to compute in the first place. Second, we can no longer use Inequality (8) to obtain a performance bound (a bound on \u2016J\u03c0\u03a8w\u0302 \u2212 J\u2217\u20161,c), as \u03a8w\u0302 does not necessarily satisfy all constraints of ALP. Desai et al. (2012) study a smoothed version of ALP, in which slack variables are introduced that allow for some violation of the constraints. Let D\u2032 be a violation budget. The smoothed ALP (SALP) has the form of\nmax w,s c\u22a4\u03a8w , max w,s\nc\u22a4\u03a8w \u2212 2\u00b5 \u22a4 \u03c0\u2217,cs\n1\u2212 \u03b3 ,\ns.t. \u03a8w \u2264 L\u03a8w + s, \u00b5\u22a4\u03c0\u2217,cs \u2264 D\u2032, s \u2265 0, s.t. \u03a8w \u2264 L\u03a8w + s, s \u2265 0 .\nThe ALP on RHS is equivalent to LHS with a specific choice of D\u2032. Let U = {u \u2208 RX : u \u2265 1} be a set of weight vectors. Desai et al. (2012) prove that if w\u2217 is a solution to above problem, then\n\u2016J\u2217 \u2212\u03a8w\u2217\u20161,c \u2264 inf w,u\u2208U \u2016J\u2217 \u2212\u03a8w\u2016\u221e,1/u\n( c\u22a4u+ 2(\u00b5\u22a4\u03c0\u2217,cu)(1 + \u03b2u)\n1\u2212 \u03b3\n) .\nThe above bound improves (9) as U is larger than U and RHS in the above bound is smaller than RHS of (9). Further, they prove that if \u03b7 is a distribution and we choose c = (1\u2212\u03b3)\u03b7\u22a4(I\u2212\u03b3P \u03c0\u03a8w\u2217 ), then\n\u2225\u2225J\u00b5\u03a8w\u2217 \u2212 J\u2217 \u2225\u2225 1,\u03b7 \u2264 1 1\u2212 \u03b3\n( inf\nw,u\u2208U \u2016J\u2217 \u2212\u03a8w\u2016\u221e,1/u\n( c\u22a4u+ 2(\u00b5\u22a4\u03c0\u2217,\u03bdu)(1 + \u03b2u)\n1\u2212 \u03b3\n)) .\nSimilar methods are also proposed by Petrik and Zilberstein (2009). One problem with this result is that c is defined in terms of w\u2217, which itself depends on c. Also, the smoothed ALP formulation uses \u03c0\u2217 which is not known. Desai et al. (2012) also propose a computationally efficient algorithm. Let S be a set of S random states drawn under distribution \u00b5\u03c0\u2217,c. Let N \u2032 \u2282 Rd be a known set that contains the solution of SALP. The algorithm solves the following LP:\nmax w,s c\u22a4\u03a8w \u2212 2 (1\u2212 \u03b3)S\n\u2211\nx\u2208S\ns(x) ,\ns.t. \u2200x \u2208 S, (\u03a8w)(x) \u2264 (L\u03a8w)(x) + s(x), s \u2265 0, w \u2208 N \u2032 .\nLet w\u0302 be the solution of this problem. Desai et al. (2012) prove high probability bounds on the approximation error \u2016J\u2217 \u2212\u03a8w\u0302\u20161,c. However, it is no longer clear if a performance bound on \u2016J\u2217 \u2212 J\u03c0\u03a8w\u0302\u20161,c can be obtained from this approximation bound.\nNext, we turn our attention to average cost ALP, which is a more challenging problem and is also the focus of this paper. Let \u03bd be a distribution over states, u : X \u2192 [1,\u221e), \u03b7 > 0, \u03b3 \u2208 [0, 1], P \u03c0\u03b3 = \u03b3P\n\u03c0 + (1 \u2212 \u03b3)1\u03bd\u22a4, and L\u03b3h = min\u03c0(\u2113\u03c0 + P \u03c0\u03b3 h). de Farias and Van Roy (2006) propose the following optimization problem:\nmin w,s1,s2 s1 + \u03b7s2 , (10) s.t. L\u03b3\u03a8w \u2212\u03a8w + s11+ s2u \u2265 0, s2 \u2265 0 .\nLet (w\u2217, s1,\u2217, s2,\u2217) be the solution of this problem. Define the mixing time of policy \u03c0 by\n\u03c4\u03c0 = inf\n{ \u03c4 : \u2223\u2223\u2223\u2223\u2223 1 t t\u22121\u2211\nt\u2032=0\n\u03bd\u22a4(P \u03c0)t \u2032 \u2113\u03c0 \u2212 \u03bb\u03c0 \u2223\u2223\u2223\u2223\u2223 \u2264 \u03c4 t , \u2200t } .\nLet \u03c4\u2217 = lim inf\u03b4\u21920{\u03c4\u03c0 : \u03bb\u03c0 \u2264 \u03bb\u2217 + \u03b4}. Let \u03c0\u2217\u03b3 be the optimal policy when discount factor is\n\u03b3. Let \u03c0\u03b3,w be the greedy policy with respect to \u03a8w when discount factor is \u03b3, \u00b5 \u22a4 \u03b3,\u03c0 = (1 \u2212 \u03b3) \u2211\u221e\nt=0 \u03b3 t\u03bd\u22a4(P \u03c0)t and \u00b5\u03b3,w = \u00b5\u03b3,\u03c0\u03b3,w . de Farias and Van Roy (2006) prove that if \u03b7 \u2265 (2 \u2212\n\u03b3)\u00b5\u22a4\u03b3,\u03c0\u2217\u03b3u,\n\u03bbw\u2217 \u2212 \u03bb\u2217 \u2264 (1 + \u03b2)\u03b7max(D\u2032\u2032, 1) 1\u2212 \u03b3 minw \u2225\u2225h\u2217\u03b3 \u2212\u03a8w \u2225\u2225 \u221e,1/u + (1\u2212 \u03b3)(\u03c4\u2217 + \u03c4\u03c0w\u2217 ) ,\nwhere \u03b2 = max\u03c0 \u2016I \u2212 \u03b3P \u03c0\u2016\u221e,1/u, D\u2032\u2032 = \u00b5\u22a4\u03b3,w\u2217V/(\u03bd\u22a4V ) and V = L\u03b3\u03a8w\u2217 \u2212 \u03a8w\u2217 + s1,\u22171 + s2,\u2217u. Similar results are obtained more recently by Veatch (2013).\nAn appropriate choice for vector \u03bd is \u03bd = \u00b5\u03b3,w\u2217. Unfortunately, w\u2217 depends on \u03bd. We should also note that solving (10) can be computationally expensive. de Farias and Van Roy (2006) propose constraint sampling techniques similar to (de Farias and Van Roy, 2004), but no performance bounds are provided.\nWang et al. (2008) study ALP (3) and show that there is a dual form for standard value function based algorithms, including on-policy and off-policy updating and policy improvement. They also study the convergence of these methods, but no performance bounds are shown."}, {"heading": "3 A Reduction to Stochastic Convex Optimization", "text": "In this section, we describe our algorithm as a reduction from Markov decision problems to stochastic convex optimization. The main idea is to convert the ALP (3) into an unconstrained optimization over \u0398 by adding a function of the constraint violations to the objective, then run stochastic gradient descent with unbiased estimated of the gradient.\nFor a positive constant H, form the following convex cost function by adding a multiple of the\ntotal constraint violations to the objective of the LP (3):\nc(\u03b8) = \u2113\u22a4(\u00b50 +\u03a6\u03b8) +H \u2016[\u00b50 +\u03a6\u03b8]\u2212\u20161 +H \u2225\u2225\u2225(P \u2212B)\u22a4(\u00b50 +\u03a6\u03b8) \u2225\u2225\u2225 1\n= \u2113\u22a4(\u00b50 +\u03a6\u03b8) +H \u2016[\u00b50 +\u03a6\u03b8]\u2212\u20161 +H \u2225\u2225\u2225(P \u2212B)\u22a4\u03a6\u03b8 \u2225\u2225\u2225 1 = \u2113\u22a4(\u00b50 +\u03a6\u03b8) +H \u2211\n(x,a)\n\u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8]\u2212 \u2223\u2223+H \u2211\nx\u2032\n\u2223\u2223\u2223(P \u2212B)\u22a4:,x\u2032\u03a6\u03b8 \u2223\u2223\u2223 .\n(11)\nWe justify using this surrogate loss as follows. Suppose we find a near optimal vector \u03b8\u0302 such that c(\u03b8\u0302) \u2264 min\u03b8\u2208\u0398 c(\u03b8) +O(\u01eb). We will prove\n1. that \u2225\u2225\u2225[\u00b50 +\u03a6\u03b8\u0302]\u2212 \u2225\u2225\u2225 1 and \u2225\u2225\u2225(P \u2212B)\u22a4(\u00b50 +\u03a6\u03b8\u0302) \u2225\u2225\u2225 1 are small and \u00b50 + \u03a6\u03b8\u0302 is close to \u00b5\u03b8\u0302 (by\nLemma 2 in Section 3), and\n2. that \u2113\u22a4(\u00b50 +\u03a6\u03b8\u0302) \u2264 min\u03b8\u2208\u0398 c(\u03b8) +O(\u01eb).\nAs we will show, these two facts imply that with high probability, for any \u03b8 \u2208 \u0398,\n\u00b5\u22a4 \u03b8\u0302 \u2113 \u2264 \u00b5\u22a4\u03b8 \u2113+\n1 \u01eb \u2016[\u00b50 +\u03a6\u03b8]\u2212\u20161 + 1 \u01eb \u2225\u2225\u2225(P \u2212B)\u22a4(\u00b50 +\u03a6\u03b8) \u2225\u2225\u2225 1 +O(\u01eb) ,\nwhich is to say that minimization of c(\u03b8) solves the extended efficient large-scale ALP problem.\nUnfortunately, calculating the gradients of c(\u03b8) is O(XA). Instead, we construct unbiased estimators and use stochastic gradient descent. Let T be the number of iterations of our algorithm. Let q1 and q2 be distributions over the state-action and state space, respectively (we will later discuss how to choose them). Let ((xt, at))t=1...T be i.i.d. samples from q1 and (x \u2032 t)t=1...T be i.i.d. samples from q2. At round t, the algorithm estimates subgradient \u2207c(\u03b8) by\ngt(\u03b8) = \u2113 \u22a4\u03a6\u2212H\n\u03a6(xt,at),:\nq1(xt, at) I{\u00b50(xt,at)+\u03a6(xt,at),:\u03b8<0}\n+H (P \u2212B)\u22a4:,x\u2032t\u03a6\nq2(x\u2032t) s((P \u2212B)\u22a4:,x\u2032t\u03a6\u03b8). (12)\nThis estimate is fed to the projected subgradient method, which in turn generates a vector \u03b8t. After T rounds, we average vectors (\u03b8t)t=1...T and obtain the final solution \u03b8\u0302T = \u2211T t=1 \u03b8t/T . Vector \u00b50 + \u03a6\u03b8\u0302T defines a policy, which in turn defines a stationary distribution \u00b5\u03b8\u0302T . 2 The algorithm is shown in Figure 1.\n2Recall that \u00b5\u03b8 is the stationary distribution of policy\n\u03c0\u03b8(a|x) = [\u00b50(x, a) + \u03a6(x,a),:\u03b8]+\u2211 a\u2032 [\u00b50(x, a \u2032) + \u03a6(x,a\u2032),:\u03b8]+ .\nWith an abuse of notation, we use \u00b5\u03b8 to denote policy \u03c0\u03b8 as well."}, {"heading": "3.1 Analysis", "text": "In this section, we state and prove our main result, Theorem 1. We begin with a discussion of the assumptions we make then follow with the main theorem. We break the proof into two main ingredients. First, we demonstrate that a good approximation to the surrogate loss gives a feature vector that is almost a stationary distribution; this is Lemma 2. Second, we justify the use of unbiased gradients in Theorem 3 and Lemma 5. The section concludes with the proof.\nWe make a mixing assumption on the MDP so that any policy quickly converges to its stationary\ndistribution.\nAssumption A1 (Fast Mixing) For any policy \u03c0, there exists a constant \u03c4(\u03c0) > 0 such that for all distributions d and d\u2032 over the state space, \u2016dP \u03c0 \u2212 d\u2032P \u03c0\u20161 \u2264 e\u22121/\u03c4(\u03c0) \u2016d\u2212 d\u2032\u20161.\nDefine\nC1 = max (x,a)\u2208X\u00d7A\n\u2225\u2225\u03a6(x,a),: \u2225\u2225\nq1(x, a) , C2 = max x\u2208X\n\u2225\u2225(P \u2212B)\u22a4:,x\u03a6 \u2225\u2225\nq2(x) .\nThese constants appear in our performance bounds. So we would like to choose distributions q1 and q2 such that C1 and C2 are small. For example, if there is C \u2032 > 0 such that for any (x, a) and i, \u03a6(x,a),i \u2264 C \u2032/(XA) and each column of P has only N non-zero elements, then we can simply choose q1 and q2 to be uniform distributions. Then it is easy to see that\n\u2225\u2225\u03a6(x,a),: \u2225\u2225\nq1(x, a) \u2264 C \u2032 ,\n\u2225\u2225(P \u2212B)\u22a4:,x\u03a6 \u2225\u2225\nq2(x) \u2264 C \u2032(N +A) .\nAs another example, if \u03a6:,i are exponential distributions and feature values at neighboring states are close to each other, then we can choose q1 and q2 to be appropriate exponential distributions so that \u2225\u2225\u03a6(x,a),: \u2225\u2225 /q1(x, a) and \u2225\u2225(P \u2212B)\u22a4:,x\u03a6\n\u2225\u2225 /q2(x) are always bounded. Another example is when there exists a constant C \u2032\u2032 > 0 such that for any x, \u2225\u2225P\u22a4:,x\u03a6 \u2225\u2225 / \u2225\u2225B\u22a4:,x\u03a6 \u2225\u2225 < C \u2032\u20323 and we have access to an efficient algorithm that computes Z1 = \u2211\n(x,a)\n\u2225\u2225\u03a6(x,a),: \u2225\u2225 and Z2 = \u2211 x \u2225\u2225B\u22a4:,x\u03a6 \u2225\u2225 and can sample\nfrom q1(x, a) = \u2225\u2225\u03a6(x,a),: \u2225\u2225 /Z1 and q2(x) = \u2225\u2225B\u22a4:,x\u03a6 \u2225\u2225 /Z2. In what follows, we assume that such distributions q1 and q2 are known.\nWe now state the main theorem.\nTheorem 1. Consider an expanded efficient large-scale dual ALP problem. Suppose we apply the stochastic subgradient method (shown in Figure 1) to the problem. Let \u01eb \u2208 (0, 1). Let T = 1/\u01eb4 be the number of rounds and H = 1/\u01eb be the constraints multiplier in subgradient estimate (12). Let \u03b8\u0302T be the output of the stochastic subgradient method after T rounds and let the learning rate be \u03b71 = \u00b7 \u00b7 \u00b7 = \u03b7T = S/(G\u2032 \u221a T ), where G\u2032 = \u221a d+H(C1+C2). Define V1(\u03b8) = \u2211 (x,a) \u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8]\u2212 \u2223\u2223\n3This condition requires that columns of \u03a6 are close to their one step look-ahead.\nand V2(\u03b8) = \u2211\nx\u2032 \u2223\u2223\u2223(P \u2212B)\u22a4:,x\u2032(\u00b50 +\u03a6\u03b8) \u2223\u2223\u2223. Then, for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,\n\u00b5\u22a4 \u03b8\u0302T \u2113 \u2264 min\n\u03b8\u2208\u0398\n( \u00b5\u22a4\u03b8 \u2113+ 1\n\u01eb (V1(\u03b8) + V2(\u03b8)) +O(\u01eb)\n) , (13)\nwhere constants hidden in the big-O notation are polynomials in S, d, C1, C2, log(1/\u03b4), V1(\u03b8), V2(\u03b8), \u03c4(\u00b5\u03b8), and \u03c4(\u00b5\u03b8\u0302T ).\nFunctions V1 and V2 are bounded by small constants for any set of normalized features: for any\n\u03b8 \u2208 \u0398,\nV1(\u03b8) \u2264 \u2016\u00b50\u20161 + \u2016\u03a6\u03b8\u20161 \u2264 1 + \u2211\n(x,a)\n\u2223\u2223\u03a6(x,a),:\u03b8 \u2223\u2223 \u2264 1 + Sd ,\nV2(\u03b8) \u2264 \u2211\nx\u2032\n\u2223\u2223\u2223P\u22a4:,x\u2032(\u00b50 +\u03a6\u03b8) \u2223\u2223\u2223+ \u2211\nx\u2032\n\u2223\u2223\u2223B\u22a4:,x\u2032(\u00b50 +\u03a6\u03b8) \u2223\u2223\u2223\n\u2264 ( \u2211\nx\u2032\nP:,x\u2032 )\u22a4 [\u00b50 +\u03a6\u03b8]+ + ( \u2211\nx\u2032\nB:,x\u2032 )\u22a4 [\u00b50 +\u03a6\u03b8]+\n= (2)1\u22a4[\u00b50 +\u03a6\u03b8]+ \u2264 (2)1\u22a4 |\u00b50 +\u03a6\u03b8| = 2 + 2S .\nThus V1 and V2 can be very small given a carefully designed set of features. The output \u03b8\u0302T is a random vector as the algorithm is based on a stochastic convex optimization method. The above theorem shows that with high probability the policy implied by this output is near optimal.\nThe optimal choice for \u01eb is \u01eb = \u221a\nV1(\u03b8\u2217) + V2(\u03b8\u2217), where \u03b8\u2217 is the minimizer of RHS of (13)\nand not known in advance. Once we obtain \u03b8\u0302T , we can estimate V1(\u03b8\u0302T ) and V2(\u03b8\u0302T ) and use input \u01eb = \u221a V1(\u03b8\u0302T ) + V2(\u03b8\u0302T ) in a second run of the algorithm. This implies that the error bound scales like O( \u221a\nV1(\u03b8\u2217) + V2(\u03b8\u2217)).\nThe next lemma, providing the first ingredient of the proof, relates the amount of constraint\nviolation of a vector \u03b8 to resulting stationary distribution \u00b5\u03b8.\nLemma 2. Let u \u2208 RXA be a vector. Let N be the set of points (x, a) where u(x, a) < 0 and S be complement of N . Assume\n\u2211\nx,a\nu(x, a) = 1, \u2211\n(x,a)\u2208N\n|u(x, a)| \u2264 \u01eb\u2032, \u2225\u2225\u2225u\u22a4(P \u2212B) \u2225\u2225\u2225 1 \u2264 \u01eb\u2032\u2032.\nVector [u]+/ \u2016[u]+\u20161 defines a policy, which in turn defines a stationary distribution \u00b5u. We have that\n\u2016\u00b5u \u2212 u\u20161 \u2264 \u03c4(\u00b5u) log(1/\u01eb\u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032 .\nProof. Let f = u\u22a4(P \u2212B). From \u2225\u2225u\u22a4(P \u2212B) \u2225\u2225 1 \u2264 \u01eb\u2032\u2032, we get that for any x\u2032 \u2208 X ,\n\u2211\n(x,a)\u2208S\nu(x, a)(P \u2212B)(x,a),x\u2032 = \u2212 \u2211\n(x,a)\u2208N\nu(x, a)(P \u2212B)(x,a),x\u2032 + f(x\u2032)\nsuch that \u2211 x\u2032 |f(x\u2032)| \u2264 \u01eb\u2032\u2032. Let h = [u]+/ \u2016[u]+\u20161. Let H \u2032 = \u2225\u2225h\u22a4(B \u2212 P ) \u2225\u2225 1 . We write\nH \u2032 = \u2211\nx\u2032\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211\n(x,a)\u2208S\nh(x, a)(B \u2212 P )(x,a),x\u2032 \u2223\u2223\u2223\u2223\u2223\u2223\n= 1\n1 + \u01eb\u2032\n\u2211\nx\u2032\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211\n(x,a)\u2208S\nu(x, a)(B \u2212 P )(x,a),x\u2032 \u2223\u2223\u2223\u2223\u2223\u2223\n= 1\n1 + \u01eb\u2032\n\u2211\nx\u2032\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2212 \u2211\n(x,a)\u2208N\nu(x, a)(B \u2212 P )(x,a),x\u2032 + f(x\u2032) \u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 1 1 + \u01eb\u2032\n  \u2211\nx\u2032\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2212 \u2211\n(x,a)\u2208N\nu(x, a)(B \u2212 P )(x,a),x\u2032 \u2223\u2223\u2223\u2223\u2223\u2223 + \u2211\nx\u2032\n\u2223\u2223f(x\u2032) \u2223\u2223  \n\u2264 1 1 + \u01eb\u2032\n \u01eb\u2032\u2032 + \u2211\n(x,a)\u2208N\n\u2211\nx\u2032\n|u(x, a)| \u2223\u2223(B \u2212 P )(x,a),x\u2032\n\u2223\u2223  \n\u2264 1 1 + \u01eb\u2032\n \u01eb\u2032\u2032 + \u2211\n(x,a)\u2208N\n2 |u(x, a)|   \u2264 2\u01eb \u2032 + \u01eb\u2032\u2032\n1 + \u01eb\u2032\n\u2264 2\u01eb\u2032 + \u01eb\u2032\u2032 .\nVector h is almost a stationary distribution in the sense that\n\u2225\u2225\u2225h\u22a4(B \u2212 P ) \u2225\u2225\u2225 1 \u2264 2\u01eb\u2032 + \u01eb\u2032\u2032 . (14)\nLet \u2016w\u20161,S = \u2211 (x,a)\u2208S |w(x, a)|. First, we have that\n\u2016h\u2212 u\u20161 \u2264 \u2225\u2225\u2225\u2225h\u2212 u\n1 + \u01eb\u2032 \u2225\u2225\u2225\u2225 1 + \u2225\u2225\u2225\u2225u\u2212 u 1 + \u01eb\u2032 \u2225\u2225\u2225\u2225 1,S \u2264 2\u01eb\u2032 .\nNext we bound \u2016\u00b5h \u2212 h\u20161. Let \u03bd0 = h be the initial state distribution. We will show that as we run policy h (equivalently, policy \u00b5h), the state distribution converges to \u00b5h and this vector is close to h. From (14), we have \u00b5\u22a40 P = h \u22a4B + v0, where v0 is such that \u2016v0\u20161 \u2264 2\u01eb\u2032 + \u01eb\u2032\u2032. Let Mh be a X \u00d7 (XA) matrix that encodes policy h, Mh(i,(i\u22121)A+1)-(i,iA) = h(\u00b7|xi). Other entries of this matrix\nare zero. We get that\nh\u22a4PMh = (h\u22a4B + v0)M h = h\u22a4BMh + v0M h = h\u22a4 + v0M h ,\nwhere we used the fact that h\u22a4BMh = h\u22a4. Let \u00b5\u22a41 = h \u22a4PMh which is the state-action distribution after running policy h for one step. Let v1 = v0M hP = v0P\nh and notice that as \u2016v0\u20161 \u2264 2\u01eb\u2032 + \u01eb\u2032\u2032, we also have that \u2016v1\u20161 = \u2225\u2225P h\u22a4v\u22a40 \u2225\u2225 1 \u2264 \u2016v0\u20161 \u2264 2\u01eb\u2032 + \u01eb\u2032\u2032. Thus,\n\u00b5\u22a41 P = h \u22a4P + v1 = h \u22a4B + v0 + v1 .\nBy repeating this argument for k rounds, we get that\n\u00b5\u22a4k = h \u22a4 + (v0 + v1 + \u00b7 \u00b7 \u00b7+ vk\u22121)Mh\nand it is easy to see that\n\u2225\u2225\u2225(v0 + v1 + \u00b7 \u00b7 \u00b7+ vk\u22121)Mh \u2225\u2225\u2225 1 \u2264 k\u22121\u2211\ni=0\n\u2016vi\u20161 \u2264 k(2\u01eb\u2032 + \u01eb\u2032\u2032).\nThus, \u2016\u00b5k \u2212 h\u20161 \u2264 k(2\u01eb\u2032+ \u01eb\u2032\u2032). Now notice that \u00b5k is the state-action distribution after k rounds of policy \u00b5h. Thus, by mixing assumption, \u2016\u00b5k \u2212 \u00b5h\u20161 \u2264 e\u2212k/\u03c4(h). By the choice of k = \u03c4(h) log(1/\u01eb\u2032), we get that \u2016\u00b5h \u2212 h\u20161 \u2264 \u03c4(h) log(1/\u01eb\u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + \u01eb\u2032.\nThe second ingredient is the validity of using estimates of the subgradients. We assume access to estimates of the subgradient of a convex cost function. Error bounds can be obtained from results in the stochastic convex optimization literature; the following theorem, a high-probability version of Lemma 3.1 of Flaxman et al. (2005) for stochastic convex optimization, is sufficient.\nTheorem 3. Let Z be a positive constant and Z be a bounded subset of Rd such that for any z \u2208 Z, \u2016z\u2016 \u2264 Z. Let (ft)t=1,2,...,T be a sequence of real-valued convex cost functions defined over Z. Let z1, z2, . . . , zT \u2208 Z be defined by z1 = 0 and zt+1 = \u03a0Z(zt \u2212 \u03b7f \u2032t), where \u03a0Z is the Euclidean projection onto Z, \u03b7 > 0 is a learning rate, and f \u20321, . . . , f \u2032T are unbiased subgradient estimates such that E [f \u2032t |zt] = \u2207f(zt) and \u2016f \u2032t\u2016 \u2264 F for some F > 0. Then, for \u03b7 = Z/(F \u221a T ), for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,\nT\u2211\nt=1\nft(zt)\u2212min z\u2208Z\nT\u2211\nt=1\nft(z) \u2264 ZF \u221a T + \u221a (1 + 4Z2T ) ( 2 log 1\n\u03b4 + d log\n( 1 + Z2T\nd\n)) . (15)\nProof. Let z\u2217 = argminz\u2208Z \u2211T t=1 ft(z) and \u03b7t = f \u2032 t \u2212 \u2207ft(zt). Define function ht : Z \u2192 R by ht(z) = ft(z) + z\u03b7t. Notice that \u2207ht(zt) = \u2207ft(zt) + \u03b7t = f \u2032t. By Theorem 1 of Zinkevich (2003),\nwe get that T\u2211\nt=1\nht(zt)\u2212 T\u2211\nt=1\nht(z\u2217) \u2264 T\u2211\nt=1\nht(zt)\u2212min z\u2208Z\nT\u2211\nt=1\nht(z) \u2264 ZF \u221a T .\nThus, T\u2211\nt=1\nft(zt)\u2212 T\u2211\nt=1\nft(z\u2217) \u2264 ZF \u221a T + T\u2211\nt=1\n(z\u2217 \u2212 zt)\u03b7t .\nLet St = \u2211t\u22121\ns=1(z\u2217\u2212zs)\u03b7s, which is a self-normalized sum (de la Pen\u0303a et al., 2009). By Corollary 3.8 and Lemma E.3 of Abbasi-Yadkori (2012), we get that for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,\n|St| \u2264 \u221a\u221a\u221a\u221a ( 1 + t\u22121\u2211\ns=1\n(zt \u2212 z\u2217)2 )( 2 log 1\n\u03b4 + d log\n( 1 + Z2t\nd\n))\n\u2264 \u221a (1 + 4Z2t) ( 2 log 1\n\u03b4 + d log\n( 1 + Z2t\nd\n)) .\nThus,\nT\u2211\nt=1\nft(zt)\u2212min z\u2208Z\nT\u2211\nt=1\nft(z) \u2264 ZF \u221a T + \u221a (1 + 4Z2T ) ( 2 log 1\n\u03b4 + d log\n( 1 + Z2T\nd\n)) .\nRemark 4. Let BT denote RHS of (15). If all cost functions are equal to f , then by convexity of f and an application of Jensen\u2019s inequality, we obtain that f( \u2211T\nt=1 zt/T )\u2212minz\u2208Z f(z) \u2264 BT /T .\nAs the next lemma shows, Theorem 3 can be applied in our problem to optimize cost function\nc.\nLemma 5. Under the same conditions as in Theorem 1, we have that for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,\nc(\u03b8\u0302T )\u2212min \u03b8\u2208\u0398\nc(\u03b8) \u2264 SG \u2032\n\u221a T +\n\u221a 1 + 4S2T\nT 2\n( 2 log 1\n\u03b4 + d log\n( 1 + S2T\nd\n)) . (16)\nProof. We prove the lemma by showing that conditions of Theorem 3 are satisfied. We begin by calculating the subgradient and bounding its norm with a quantity independent of the number of states. If \u00b50(x, a) + \u03a6(x,a),:\u03b8 \u2265 0, then \u2207\u03b8 \u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8]\u2212 \u2223\u2223 = 0. Otherwise,\n\u2207\u03b8 \u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8]\u2212 \u2223\u2223 = \u2212\u03a6(x,a),:. Calculating,\n\u2207\u03b8c(\u03b8) = \u2113\u22a4\u03a6+H \u2211\n(x,a)\n\u2207\u03b8 \u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8]\u2212 \u2223\u2223+H \u2211\nx\u2032\n\u2207\u03b8 \u2223\u2223\u2223(P \u2212B)\u22a4:,x\u2032\u03a6\u03b8 \u2223\u2223\u2223\n= \u2113\u22a4\u03a6\u2212H \u2211\n(x,a)\n\u03a6(x,a),:I{\u00b50(x,a)+\u03a6(x,a),:\u03b8<0} +H \u2211\nx\u2032\n(P \u2212B)\u22a4:,x\u2032\u03a6s((P \u2212B)\u22a4:,x\u2032\u03a6\u03b8) , (17)\nwhere s(z) = I{z>0} \u2212 I{z<0} is the sign function. Let \u00b1 denote the plus or minus sign (the exact sign does not matter here). Let G = \u2016\u2207\u03b8c(\u03b8)\u2016. We have that\nG \u2264 H \u221a\u221a\u221a\u221a\u221a d\u2211\ni=1\n \u2211\nx\u2032\n \u00b1 \u2211\n(x,a)\n(P \u2212B)(x,a),x\u2032\u03a6(x,a),i\n    2 + \u2225\u2225\u2225\u2113\u22a4\u03a6 \u2225\u2225\u2225+H \u221a\u221a\u221a\u221a\u221a d\u2211\ni=1\n \u2211\n(x,a)\n\u2223\u2223\u03a6(x,a),i \u2223\u2223   2 .\nThus,\nG \u2264\n\u221a\u221a\u221a\u221a d\u2211\ni=1\n(\u2113\u22a4\u03a6:,i)2 +H \u221a d+H \u221a\u221a\u221a\u221a\u221a d\u2211\ni=1\n \u2211\n(x,a)\n( \u00b1 \u2211\nx\u2032\n(P \u2212B)(x,a),x\u2032 ) \u03a6(x,a),i   2\n\u2264 \u221a d+H \u221a d+H \u221a\u221a\u221a\u221a\u221a d\u2211\ni=1\n 2 \u2211\n(x,a)\n\u2223\u2223\u03a6(x,a),i \u2223\u2223   2 = \u221a d(1 + 3H) ,\nwhere we used \u2223\u2223\u2113\u22a4\u03a6:,i \u2223\u2223 \u2264 \u2016\u2113\u2016\u221e \u2016\u03a6:,i\u20161 \u2264 1. Next, we show that norm of the subgradient estimate is bounded by G\u2032:\n\u2016gt\u2016 \u2264 \u2225\u2225\u2225\u2113\u22a4\u03a6 \u2225\u2225\u2225+H \u2225\u2225\u03a6(xt,at),: \u2225\u2225 q1(xt, at) +H\n\u2225\u2225\u2225(P \u2212B)\u22a4:,x\u2032t\u03a6 \u2225\u2225\u2225\nq2(x\u2032t) \u2264\n\u221a d+H(C1 + C2) .\nFinally, we show that the subgradient estimate is unbiased:\nE [gt(\u03b8)] = \u2113 \u22a4\u03a6\u2212H\n\u2211\n(x,a)\nq1(x, a) \u03a6(x,a),:\nq1(x, a) I{\u00b50(x,a)+\u03a6(x,a),:\u03b8<0}\n+H \u2211\nx\u2032\nq2(x \u2032) (P \u2212B)\u22a4:,x\u2032\u03a6\nq2(x\u2032) s((P \u2212B)\u22a4:,x\u2032\u03a6\u03b8)\n= \u2113\u22a4\u03a6\u2212H \u2211\n(x,a)\n\u03a6(x,a),:I{\u00b50(x,a)+\u03a6(x,a),:\u03b8<0} +H \u2211\nx\u2032\n(P \u2212B)\u22a4:,x\u2032\u03a6s((P \u2212B)\u22a4:,x\u2032\u03a6\u03b8)\n= \u2207\u03b8c(\u03b8) .\nThe result then follows from Theorem 3 and Remark 4.\nWith both ingredients in place, we can prove our main result.\nProof of Theorem 1. Let bT be the RHS of (16). Equation (16) implies that with high probability for any \u03b8 \u2208 \u0398,\n\u2113\u22a4(\u00b50 +\u03a6\u03b8\u0302T ) +H V1(\u03b8\u0302T ) +H V2(\u03b8\u0302T ) \u2264 \u2113\u22a4(\u00b50 +\u03a6\u03b8) +H V1(\u03b8) +H V2(\u03b8) + bT . (18)\nFrom (18), we get that\nV1(\u03b8\u0302T ) \u2264 1\nH (2(1 + S) +H V1(\u03b8) +H V2(\u03b8) + bT )\ndef = \u01eb\u2032 , (19)\nV2(\u03b8\u0302T ) \u2264 1\nH (2(1 + S) +H V1(\u03b8) +H V2(\u03b8) + bT )\ndef = \u01eb\u2032\u2032 . (20)\nInequalities (19) and (20) and Lemma 2 give the following bound:\n\u2223\u2223\u2223\u00b5\u22a4 \u03b8\u0302T \u2113\u2212 (\u00b50 +\u03a6\u03b8\u0302T )\u22a4\u2113 \u2223\u2223\u2223 \u2264 \u03c4(\u00b5\u03b8\u0302T ) log(1/\u01eb \u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032 . (21)\nFrom (18) we also have\n\u2113\u22a4(\u00b50 +\u03a6\u03b8\u0302T ) \u2264 \u2113\u22a4(\u00b50 +\u03a6\u03b8) +H V1(\u03b8) +H V2(\u03b8) + bT ,\nwhich, together with (21) and Lemma 2, gives the final result:\n\u00b5\u22a4 \u03b8\u0302T \u2113 \u2264 \u2113\u22a4(\u00b50 +\u03a6\u03b8) +H V1(\u03b8) +H V2(\u03b8) + bT + \u03c4(\u00b5\u03b8\u0302T ) log(1/\u01eb \u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032\n\u2264 \u00b5\u22a4\u03b8 \u2113+H V1(\u03b8) +H V2(\u03b8) + bT + \u03c4(\u00b5\u03b8\u0302T ) log(1/\u01eb \u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032\n+ \u03c4(\u00b5\u03b8) log(1/V1(\u03b8))(2V1(\u03b8) + V2(\u03b8)) + 3V1(\u03b8) .\nRecall that bT = O(H/ \u221a T ). Because H = 1/\u01eb and T = 1/\u01eb4, we get that with high probability, for any \u03b8 \u2208 \u0398, \u00b5\u22a4 \u03b8\u0302T \u2113 \u2264 \u00b5\u22a4\u03b8 \u2113+ 1\u01eb (V1(\u03b8) + V2(\u03b8)) +O(\u01eb).\nLet\u2019s compare Theorem 1 with results of de Farias and Van Roy (2006). Their approach is to relate the original MDP to a perturbed version4 and then analyze the corresponding ALP. (See Section 2 for more details.) Let \u03a8 be a feature matrix that is used to estimate value functions. Recall that \u03bb\u2217 is the average loss of the optimal policy and \u03bbw is the average loss of the greedy policy with respect to value function \u03a8w. Let h\u2217\u03b3 be the differential value function when the restart probability in the perturbed MDP is 1 \u2212 \u03b3. For vector v and positive vector u, define the 4In a perturbed MDP, the state process restarts with a certain probability to a restart distribution. Such perturbed MDPs are closely related to discounted MDPs.\nweighted maximum norm \u2016v\u2016\u221e,u = maxx u(x) |v(x)|. de Farias and Van Roy (2006) prove that for appropriate constants C,C \u2032 > 0 and weight vector u,\n\u03bbw\u2217 \u2212 \u03bb\u2217 \u2264 C 1\u2212 \u03b3 minw \u2225\u2225h\u2217\u03b3 \u2212\u03a8w \u2225\u2225 \u221e,u + C \u2032(1\u2212 \u03b3) . (22)\nThis bound has similarities to bound (13): tightness of both bounds depends on the quality of feature vectors in representing the relevant quantities (stationary distributions in (13) and value functions in (22)). Once again, we emphasize that the algorithm proposed by de Farias and Van Roy (2006) is computationally expensive and requires access to a distribution that depends on optimal policy."}, {"heading": "4 Sampling Constraints", "text": "In this section we describe our second algorithm for average cost MDP problems. Using the results on polytope constraint sampling (de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008), we reduce approximate the solution to the dual ALP with the solution to a smaller, sampled LP. Basically, de Farias and Van Roy (2004) claim that given a set of affine constraints in Rd and some measure q over these constraints, if we sample k = O(d log(1/\u03b4)/\u01eb) constraints, then with probability at least 1 \u2212 \u03b4, any point that satisfies all of these k sampled constraints also satisfies 1 \u2212 \u01eb of the original set of constraints under measure q. This result is independent of the number of original constraints.\nLet L be a family of affine constraints indexed by i: constraint i is satisfied at point w \u2208 Rd if a\u22a4i w + bi \u2265 0. Let I be the family of constraints by selecting k random constraints in L with respect to measure q.\nTheorem 6 (de Farias and Van Roy (2004)). Assume there exists a vector that satisfies all constraints in L. For any \u03b4 and \u01eb, if we take m \u2265 4\u01eb ( d log 12\u01eb + log 2 \u03b4 ) , then, with probability 1 \u2212 \u03b4, a set I of m i.i.d. random variables drawn from L with respect to distribution q satisfies\nsup {w:\u2200i\u2208I,a\u22a4i w+bi\u22650}\nq({j : a\u22a4j w + bj < 0}) \u2264 \u01eb .\nOur algorithm takes the following inputs: a positive constant S, a stationary distribution \u00b50, a set \u0398 = {\u03b8 : \u03b8\u22a4\u03a6\u22a41 = 1 \u2212 \u00b5\u22a40 1, \u2016\u03b8\u2016 \u2264 S}, a distribution q1 over the state-action space, a distribution q2 over the state space, and constraint violation functions v1 : X \u00d7 A \u2192 [\u22121, 0] and v2 : X \u2192 [0, 1]. We will consider two families of constraints:\nL1 = {\u00b50(x, a) + \u03a6(x,a),:\u03b8 \u2265 v1(x, a) | (x, a) \u2208 X \u00d7A} , L2 = { (P \u2212B)\u22a4:,x(\u00b50 +\u03a6\u03b8) \u2264 v2(x) | x \u2208 X }\u22c3{ (P \u2212B)\u22a4:,x(\u00b50 +\u03a6\u03b8) \u2265 \u2212v2(x) | x \u2208 X } .\nLet \u03b8\u2217 be the solution of\nmin \u03b8\u2208\u0398\n\u2113\u22a4(\u00b50 +\u03a6\u03b8) , (23)\ns.t. \u03b8 \u2208 L1, \u03b8 \u2208 L2, \u03b8 \u2208 \u0398 .\nThe constraint sampling algorithm is shown in Figure 2. We refer to (24) as the sampled ALP, while we refer to (3) as the full ALP."}, {"heading": "4.1 Analysis", "text": "We require Assumption A1 as well as:\nAssumption A2 (Feasibility) There exists a vector that satisfies all constraints L1 and L2.\nValidity of this assumption depends on the choice of functions v1 and v2. Larger functions ensure that this assumption is satisfied, but as we show, this leads to larger error.\nThe next two lemmas apply theorem 6 to constraints L1 and L2, respectively.\nLemma 7. Let \u03b41 \u2208 (0, 1) and \u01eb1 \u2208 (0, 1). If we choose k1 = 4\u01eb1 ( d log 12\u01eb1 + log 2 \u03b41 ) , then with probability at least 1\u2212 \u03b41, \u2211\n(x,a) \u2223\u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8\u0303]\u2212 \u2223\u2223\u2223 \u2264 SC1\u01eb1 + \u2016v1\u20161.\nProof. Applying theorem 6, we have that w.p. 1 \u2212 \u03b41, q1(\u00b50(x, a) + \u03a6(x,a),:\u03b8\u0303 \u2265 v1(x, a)) \u2265 1 \u2212 \u01eb1, and thus \u2211\n(x,a)\nq1(x, a)I{\u00b50(x,a)+\u03a6(x,a),:\u03b8\u0303<v1(x,a)} \u2264 \u01eb1 .\nLet L = \u2211\n(x,a)\n\u2223\u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8\u0303]\u2212 \u2223\u2223\u2223. With probability 1\u2212 \u03b41,\nL = \u2211\n(x,a)\n\u2223\u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8\u0303]\u2212 \u2223\u2223\u2223 I{\u00b50(x,a)+\u03a6(x,a),:\u03b8\u0303\u2264v1(x,a)}\n+ \u2211\n(x,a)\n\u2223\u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8\u0303]\u2212 \u2223\u2223\u2223 I{\u00b50(x,a)+\u03a6(x,a),:\u03b8\u0303>v1(x,a)}\n\u2264 \u2211\n(x,a)\n\u2223\u2223\u2223\u03a6(x,a),:\u03b8\u0303 \u2223\u2223\u2223 I{\u00b50(x,a)+\u03a6(x,a),:\u03b8\u0303\u2264v1(x,a)} + \u2016v1\u20161\n\u2264 \u2211\n(x,a)\n\u2225\u2225\u03a6(x,a),: \u2225\u2225 \u2225\u2225\u2225\u03b8\u0303 \u2225\u2225\u2225 I{\u00b50(x,a)+\u03a6(x,a),:\u03b8\u0303\u2264v1(x,a)} + \u2016v1\u20161\n\u2264 \u2211\n(x,a)\nSC1q1(x, a)I{\u00b50(x,a)+\u03a6(x,a),:\u03b8\u0303\u2264v1(x,a)} + \u2016v1\u20161\n\u2264 SC1\u01eb1 + \u2016v1\u20161 .\nLemma 8. Let \u03b42 \u2208 (0, 1) and \u01eb2 \u2208 (0, 1). If we choose k2 = 4\u01eb2 ( d log 12\u01eb2 + log 2 \u03b42 ) , then with\nprobability at least 1\u2212 \u03b42, \u2225\u2225\u2225(P \u2212B)\u22a4\u03a6\u03b8\u0303 \u2225\u2225\u2225 1 \u2264 SC2\u01eb2 + \u2016v2\u20161.\nProof. Applying theorem 6, we have that q2 (\u2223\u2223\u2223(P \u2212B)\u22a4:,x\u03a6\u03b8\u0303 \u2223\u2223\u2223 \u2264 v2(x) ) \u2265 1\u2212 \u01eb2. This yields\n\u2211\nx\nq2(x)I{|(P\u2212B)\u22a4:,x\u03a6\u03b8\u0303|\u2265v2(x)} \u2264 \u01eb2 . (25)\nLet L\u2032 = \u2211\nx \u2223\u2223\u2223(P \u2212B)\u22a4:,x\u03a6\u03b8\u0303 \u2223\u2223\u2223. Thus, with probability 1\u2212 \u03b42,\nL\u2032 = \u2211\nx\n\u2223\u2223\u2223(P \u2212B)\u22a4:,x\u03a6\u03b8\u0303 \u2223\u2223\u2223 I{|(P\u2212B)\u22a4:,x\u03a6\u03b8\u0303|>v2(x)}\n+ \u2211\nx\n\u2223\u2223\u2223(P \u2212B)\u22a4:,x\u03a6\u03b8\u0303 \u2223\u2223\u2223 I{|(P\u2212B)\u22a4:,x\u03a6\u03b8\u0303|\u2264v2(x)}\n\u2264 \u2211\nx\n\u2225\u2225\u2225(P \u2212B)\u22a4:,x\u03a6 \u2225\u2225\u2225 \u2225\u2225\u2225\u03b8\u0303 \u2225\u2225\u2225 I{|(P\u2212B)\u22a4:,x\u03a6\u03b8\u0303|>v2(x)} + \u2016v2\u20161\n\u2264 \u2211\nx\nSC2q2(x)I{|(P\u2212B)\u22a4:,x\u03a6\u03b8\u0303|>v2(x)} + \u2016v2\u20161\n\u2264 SC2\u01eb2 + \u2016v2\u20161 ,\nwhere the last step follows from (25).\nWe are ready to prove the main result of this section. Let \u03b8\u0303 denote the solution of the sampled ALP, \u03b8\u2217 denote the solution of the full ALP (23), and \u00b5\u03b8\u0303 be the stationary distribution of the solution policy. Our goal is to compare \u2113\u22a4\u00b5 \u03b8\u0303 and \u2113\u22a4\u00b5\u03b8\u2217.\nTheorem 9. Let \u01eb \u2208 (0, 1) and \u03b4 \u2208 (0, 1). Let \u01eb\u2032 = SC1\u01eb + \u2016v1\u20161 and \u01eb\u2032\u2032 = SC2\u01eb + \u2016v2\u20161. If we sample constraints with k1 = 4 \u01eb ( d log 12\u01eb + log 4 \u03b4 ) and k2 = 4 \u01eb ( d log 12\u01eb + log 4 \u03b4 ) , then, with probability 1\u2212 \u03b4,\n\u2113\u22a4\u00b5 \u03b8\u0303 \u2264 \u2113\u22a4\u00b5\u03b8\u2217 + \u03c4(\u00b5\u03b8\u0303) log(1/\u01eb \u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032\n+ \u03c4(\u00b5\u2217) log(1/ \u2016v1\u2016)(2 \u2016v1\u2016+ \u2016v2\u2016) + 3 \u2016v1\u2016 .\nProof. Let \u03b41 = \u03b42 = \u03b4/2. By Lemmas 7 and 8, w.p. 1\u2212 \u03b4, \u2211\n(x,a) \u2223\u2223\u2223[\u00b50(x, a) + \u03a6(x,a),:\u03b8\u0303]\u2212 \u2223\u2223\u2223 \u2264 \u01eb\u2032 and\u2225\u2225\u2225(P \u2212B)\u22a4(\u00b50 +\u03a6\u03b8\u0303)\n\u2225\u2225\u2225 1 \u2264 \u01eb\u2032\u2032. Then by Lemma 2,\n\u2223\u2223\u2223\u2113\u22a4\u00b5\u03b8\u0303 \u2212 \u2113 \u22a4(\u00b50 +\u03a6\u03b8\u0303) \u2223\u2223\u2223 \u2264 \u03c4(\u00b5\u03b8\u0303) log(1/\u01eb \u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032 .\nWe also have that \u2113\u22a4(\u00b50 +\u03a6\u03b8\u0303) \u2264 \u2113\u22a4(\u00b50 +\u03a6\u03b8\u2217). Thus,\n\u2113\u22a4\u00b5 \u03b8\u0303 \u2264 \u2113\u22a4(\u00b50 +\u03a6\u03b8\u2217) + \u03c4(\u00b5\u03b8\u0303) log(1/\u01eb \u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032\n\u2264 \u2113\u22a4\u00b5\u03b8\u2217 + \u03c4(\u00b5\u03b8\u0303) log(1/\u01eb \u2032)(2\u01eb\u2032 + \u01eb\u2032\u2032) + 3\u01eb\u2032\n+ \u03c4(\u00b5\u03b8\u2217) log(1/ \u2016v1\u2016)(2 \u2016v1\u2016+ \u2016v2\u2016) + 3 \u2016v1\u2016 ,\nwhere the last step follows from Lemma 2."}, {"heading": "5 Experiments", "text": "In this section, we apply both algorithms to the four-dimensional discrete-time queueing network illustrated in Figure 5. This network has a relatively long history; see, e.g. Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.f. section 6.2). There are four queues, \u00b51, . . . , \u00b54, each with state 0, . . . , B. Since the cardinality of the state space is X = (1 +B) 4, even a modest B results in huge state-spaces. For time t, let Xt \u2208 X be the state and si,t \u2208 {0, 1}, i = 1, 2, 3, 3 denote whether queue i is being served. Server 1 only serves queue 1 or 4, server 2 only serves queue 2 or 3, and neither server can idle. Thus, s1,t + s4,t = 1 and s2,t + s3,t = 1. The dynamics are as follows. At each time t, the following random variables are sampled independently: A1,t \u223c Bernoulli(a1), A3,t \u223c Bernoulli(a3), and Di,t \u223c Bernoulli(di \u2217 si,t) for i = 1, 2, 3, 4. Using e1, . . . , e4 to denote the standard basis vectors, the dynamics are:\nX \u2032t+1 =Xt +A1,te1 +A3,te3\n+D1,t(e2 \u2212 e1)\u2212D2,te2 +D3,t(e4 \u2212 e3)\u2212D4,te4,\nand Xt+1 = max(0,min((B),X \u2032 t+1)) (i.e. all four states are thresholded from below by 0 and above by B). The loss function is the total queue size: \u2113(Xt) = ||Xt||1. We compared our method against two common heuristics. In the first, denoted LONGER, each server operates on the queue that is longer with ties broken uniformly at random (e.g. if queue 1 and 4 had the same size, they are equally likely to be served). In the second, denoted LBFS (last buffer first served), the downstream queues always have priority (server 1 will serve queue 4 unless it has length 0, and server 2 will serve queue 2 unless it has length 0). These heuristics are common and have been used an benchmarks for queueing networks (e.g. de Farias and Van Roy (2003a)).\nWe used a1 = a3 = .08, d1 = d2 = .12, and d3 = d4 = .28, and buffer sizes B1 = B4 = 38, B2 = B3 = 25 as the parameters of the network.. The asymmetric size was chosen because server 1 is the bottleneck and tend to have has longer queues. The first two features are the stationary distributions corresponding to two heuristics. We also included two types of non-stationary-distribution features. For every interval (0, 5], (6, 10], . . . , (45, 50] and action A, we added a feature \u03c8 with \u03d5(x, a) = 1 if \u2113(x, a) is in the interval and a = A. To define the second type, consider the three intervals I1 = [0, 10], I2 = [11, 20], and I3 = [21, 25]. For every 4-tuple of intervals (J1, J2, J3, J4) \u2208 {I1, I2, I3}4 and action A, we created a feature \u03c8 with \u03c8(x, a) = 1 only if xi \u2208 Ji and a = A. Every feature was normalized to sum to 1. In total, we had 372 features which is about a 104 reduction in dimension from the original problem."}, {"heading": "5.1 Stochastic Gradient Descent", "text": "We ran our stochastic gradient descent algorithm with I = 1000 sampled constraints and constraint gain H = 2. Our learning rate began at 10\u22124 and halved every 2000 iterations. The results of our algorithm are plotted in Figure 5.1, where \u03b8\u0302t denotes the running average of \u03b8t. The left plot is of the LP objective, \u2113\u22a4(\u00b50 + \u03a6\u03b8\u0302t). The middle plot is of the sum of the constraint violations,\u2225\u2225\u2225[\u00b50 +\u03a6\u03b8\u0302t]\u2212 \u2225\u2225\u2225 1 + \u2225\u2225\u2225(P \u2212B)\u22a4\u03a6\u03b8\u0302t \u2225\u2225\u2225 1 . Thus, c(\u03b8\u0302t) is a scaled sum of the first two plots. Finally, the right plot is of the average losses, \u2113\u22a4\u00b5 \u03b8\u0302t and the two horizontal lines correspond to the loss of the two heuristics, LONGER and LBFS. The right plot demonstrates that, as predicted by our theory, minimizing the surrogate loss c(\u03b8) does lead to lower average losses.\nAll previous algorithms (including de Farias and Van Roy (2003a)) work with value functions, while our algorithm works with stationary distributions. Due to this difference, we cannot use the same feature vectors to make a direct comparison. The solution that we find in this different approximating set is slightly worse than the solution of de Farias and Van Roy (2003a)."}, {"heading": "5.2 Constraint Sampling", "text": "For the constraint sampling algorithm, we sampled the simplex constraints uniformly with 10 different sample sizes: 508, 792, 1235, 1926, 3003, 4684, 7305, 11393, 17768, and 27712. Since XA = 4.1 \u2217 106, these sample sizes correspond to less that 1%. The stationary constraints were sampled in the same proportion (i.e. A times fewer samples). Let a1, . . . , aAN and b1, . . . , bN be the indices of the sampled simplex and stationary constraints, respectively. Explicitly, the sampled\nLP is:\nmin \u03b8\n(\u03a6\u03b8)\u22a4\u2113 , (26)\ns.t. (\u03a6\u03b8)\u22a41 = 1, \u03a6ai,;\u03b8 \u2265 0, \u2200i = 1, . . . , AN,\u2223\u2223\u2223\u03a6\u03b8\u22a4(P \u2212B):,bi \u2223\u2223\u2223 \u2264 \u01ebs, \u2200i = 1, . . . , N, \u2016\u03b8\u2016\u221e \u2264 M (27)\nwhere M and \u01eb are necessary to ensure the LP always has a feasible and bounded solution. This corresponds to setting v1 = 0 and v2 = \u01eb. In particular, we used M = 3 and \u01eb = 10 \u22123. Using differenc values of \u01eb did not have a large effect on the behavior of the algorithm.\nFor each sample size, we sample the constraints, solve the LP, then simulate the average loss of the policy. We repeated this procedure 35 times for each sample size and plotted the mean with error bars corresponding to the variance across each sample size in Figure 5.2. Note the log scale on the x-axis. The best loss corresponds to 4684 sampled simplex constraints, or roughly 1%, and is a marked improvement over the average loss found by the stochastic gradient descent method. However, changing the sample size by a factor of 4 in either direction is enough to obliterate this advantage.\nFirst, we notice that the mean average loss is not monotonic. If we use too few constraints, then the sampled ALP does not reflect our original problem and we expect that the solution will make poor policies. On the other hand, if we sample too many constraints, then the LP is too restrictive and cannot adequately explore the feature space. To explain the increasing variance, recall that we have three families of constraints: the simplex constraints, the stationarity constraints, and the box constraints (i.e. |\u03b8|\u221e \u2264 M). Only the simplex and stationarity constraints are sampled. For the small sample sizes, the majority of the active constraints are the box constraint so \u03b8\u0303 (the minimizer of the LP) is not very sensitive to the random sample. However, as the sample size grows, so does the number of active simplex and stationarity constraints; hence, the random constraints affect \u03b8\u0303 to a greater degree and the variance increases."}, {"heading": "6 Conclusions", "text": "In this paper, we defined and solved the extended large-scale efficient ALP problem. We proved that, under certain assumptions about the dynamics, the stochastic subgradient method produces a policy with average loss competitive to all \u03b8 \u2208 \u0398, not just all \u03b8 producing a stationary distribution. We demonstrated this algorithm on the Rybko-Stoylar four-dimensional queueing network and recovered a policy better than two common heuristics and comparable to previous results on ALPs de Farias and Van Roy (2003a). A future direction is to find other interesting regularity conditions under which we can handle large-scale MDP problems. We also plan to conduct more experiments with challenging large-scale problems."}, {"heading": "7 Acknowledgements", "text": "We gratefully acknowledge the support of the NSF through grant CCF-1115788 and of the ARC through an Australian Research Council Australian Laureate Fellowship (FL110100281)."}], "references": [{"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Athena Scientific,", "citeRegEx": "Bellman.,? \\Q2012\\E", "shortCiteRegEx": "Bellman.", "year": 2012}, {"title": "Uncertain convex programs: randomized solutions and confidence levels", "author": ["G. Calafiore", "M.C. Campi"], "venue": "Mathematical Programming,", "citeRegEx": "Calafiore and Campi.,? \\Q2005\\E", "shortCiteRegEx": "Calafiore and Campi.", "year": 2005}, {"title": "The exact feasibility of randomized solutions of uncertain convex programs", "author": ["M.C. Campi", "S. Garatti"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Campi and Garatti.,? \\Q2008\\E", "shortCiteRegEx": "Campi and Garatti.", "year": 2008}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2003\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2003}, {"title": "Approximate linear programming for average-cost dynamic programming", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Farias and Roy.,? \\Q2003\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2003}, {"title": "On constraint sampling in the linear programming approach to approximate dynamic programming", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2004\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2004}, {"title": "A cost-shaping linear program for average-cost approximate dynamic programming with performance guarantees", "author": ["D.P. de Farias", "B. Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2006\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2006}, {"title": "Self-normalized processes: Limit theory and Statistical Applications", "author": ["V.H. de la Pe\u00f1a", "T.L. Lai", "Q-M. Shao"], "venue": null, "citeRegEx": "Pe\u00f1a et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pe\u00f1a et al\\.", "year": 2009}, {"title": "Approximate dynamic programming via a smoothed linear program", "author": ["V.V. Desai", "V.F. Farias", "C.C. Moallemi"], "venue": "Operations Research,", "citeRegEx": "Desai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Desai et al\\.", "year": 2012}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Solving factored mdps with continuous and discrete variables", "author": ["C. Guestrin", "M. Hauskrecht", "B. Kveton"], "venue": "In Twentieth Conf. Uncertainty in Artificial Intelligence,", "citeRegEx": "Guestrin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2004}, {"title": "Linear program approximations to factored continuous-state markov decision processes", "author": ["M. Hauskrecht", "B. Kveton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hauskrecht and Kveton.,? \\Q2003\\E", "shortCiteRegEx": "Hauskrecht and Kveton.", "year": 2003}, {"title": "Convergent temporal-difference learning with arbitrary smooth function approximation", "author": ["H.R. Maei", "Cs. Szepesv\u00e1ri", "S. Bhatnagar", "D. Precup", "D. Silver", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Maei et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2009}, {"title": "Toward off-policy learning control with function approximation", "author": ["H.R. Maei", "Cs. Szepesv\u00e1ri", "S. Bhatnagar", "R.S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Maei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2010}, {"title": "Linear programming and sequential decisions", "author": ["A.S. Manne"], "venue": "Management Science,", "citeRegEx": "Manne.,? \\Q1960\\E", "shortCiteRegEx": "Manne.", "year": 1960}, {"title": "Constraint relaxation in approximate linear programs", "author": ["M. Petrik", "S. Zilberstein"], "venue": "In Proc. 26th Internat. Conf. Machine Learning (ICML),", "citeRegEx": "Petrik and Zilberstein.,? \\Q2009\\E", "shortCiteRegEx": "Petrik and Zilberstein.", "year": 2009}, {"title": "Ergodicity of stochastic processes describing the operation of open queueing networks", "author": ["A.N. Rybko", "A.L. Stolyar"], "venue": "Problemy Peredachi Informatsii,", "citeRegEx": "Rybko and Stolyar.,? \\Q1992\\E", "shortCiteRegEx": "Rybko and Stolyar.", "year": 1992}, {"title": "Generalized polynomial approximations in Markovian decision processes", "author": ["P. Schweitzer", "A. Seidmann"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Schweitzer and Seidmann.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer and Seidmann.", "year": 1985}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Cs. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation", "author": ["R.S. Sutton", "Cs. Szepesv\u00e1ri", "H.R. Maei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Approximate linear programming for average cost mdps", "author": ["M.H. Veatch"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Veatch.,? \\Q2013\\E", "shortCiteRegEx": "Veatch.", "year": 2013}, {"title": "Dual representations for dynamic programming", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 14, "context": "The MDP problem can also be stated in the LP formulation (Manne, 1960),", "startOffset": 57, "endOffset": 70}, {"referenceID": 13, "context": "Schweitzer and Seidmann (1985) propose approximate linear programming (ALP) formulations.", "startOffset": 0, "endOffset": 31}, {"referenceID": 9, "context": "These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al.", "startOffset": 70, "endOffset": 99}, {"referenceID": 9, "context": "These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al. (2004), Petrik and Zilberstein (2009), Desai et al.", "startOffset": 100, "endOffset": 123}, {"referenceID": 9, "context": "These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al. (2004), Petrik and Zilberstein (2009), Desai et al.", "startOffset": 100, "endOffset": 154}, {"referenceID": 8, "context": "(2004), Petrik and Zilberstein (2009), Desai et al. (2012). As noted by Desai et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 8, "context": "(2004), Petrik and Zilberstein (2009), Desai et al. (2012). As noted by Desai et al. (2012), the prior work on ALP either requires access to samples from a distribution that depends on optimal policy or assumes the ability to solve an LP with as many constraints as states.", "startOffset": 39, "endOffset": 92}, {"referenceID": 8, "context": "Desai et al. (2012) study a smoothed version of ALP, in which slack variables are introduced that allow for some violation of the constraints.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Desai et al. (2012) prove that if w\u2217 is a solution to above problem, then \u2016J\u2217 \u2212\u03a8w\u2217\u20161,c \u2264 inf w,u\u2208U \u2016J\u2217 \u2212\u03a8w\u2016\u221e,1/u ( cu+ 2(\u03bc\u03c0\u2217,cu)(1 + \u03b2u) 1\u2212 \u03b3 ) .", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "Similar methods are also proposed by Petrik and Zilberstein (2009). One problem with this result is that c is defined in terms of w\u2217, which itself depends on c.", "startOffset": 37, "endOffset": 67}, {"referenceID": 8, "context": "Desai et al. (2012) also propose a computationally efficient algorithm.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Desai et al. (2012) prove high probability bounds on the approximation error \u2016J\u2217 \u2212\u03a8\u0175\u20161,c.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Desai et al. (2012) prove high probability bounds on the approximation error \u2016J\u2217 \u2212\u03a8\u0175\u20161,c. However, it is no longer clear if a performance bound on \u2016J\u2217 \u2212 J\u03c0\u03a8\u0175\u20161,c can be obtained from this approximation bound. Next, we turn our attention to average cost ALP, which is a more challenging problem and is also the focus of this paper. Let \u03bd be a distribution over states, u : X \u2192 [1,\u221e), \u03b7 > 0, \u03b3 \u2208 [0, 1], P \u03c0 \u03b3 = \u03b3P \u03c0 + (1 \u2212 \u03b3)1\u03bd\u22a4, and L\u03b3h = min\u03c0(l\u03c0 + P \u03c0 \u03b3 h). de Farias and Van Roy (2006) propose the following optimization problem:", "startOffset": 0, "endOffset": 488}, {"referenceID": 21, "context": "Similar results are obtained more recently by Veatch (2013). An appropriate choice for vector \u03bd is \u03bd = \u03bc\u03b3,w\u2217.", "startOffset": 46, "endOffset": 60}, {"referenceID": 21, "context": "Similar results are obtained more recently by Veatch (2013). An appropriate choice for vector \u03bd is \u03bd = \u03bc\u03b3,w\u2217. Unfortunately, w\u2217 depends on \u03bd. We should also note that solving (10) can be computationally expensive. de Farias and Van Roy (2006) propose constraint sampling techniques similar to (de Farias and Van Roy, 2004), but no performance bounds are provided.", "startOffset": 46, "endOffset": 243}, {"referenceID": 21, "context": "Similar results are obtained more recently by Veatch (2013). An appropriate choice for vector \u03bd is \u03bd = \u03bc\u03b3,w\u2217. Unfortunately, w\u2217 depends on \u03bd. We should also note that solving (10) can be computationally expensive. de Farias and Van Roy (2006) propose constraint sampling techniques similar to (de Farias and Van Roy, 2004), but no performance bounds are provided. Wang et al. (2008) study ALP (3) and show that there is a dual form for standard value function based algorithms, including on-policy and off-policy updating and policy improvement.", "startOffset": 46, "endOffset": 383}, {"referenceID": 9, "context": "1 of Flaxman et al. (2005) for stochastic convex optimization, is sufficient.", "startOffset": 5, "endOffset": 27}, {"referenceID": 23, "context": "By Theorem 1 of Zinkevich (2003),", "startOffset": 16, "endOffset": 33}, {"referenceID": 7, "context": "Let St = \u2211t\u22121 s=1(z\u2217\u2212zs)\u03b7s, which is a self-normalized sum (de la Pe\u00f1a et al., 2009). By Corollary 3.8 and Lemma E.3 of Abbasi-Yadkori (2012), we get that for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, |St| \u2264 \u221a\u221a\u221a\u221a ( 1 + t\u22121 \u2211", "startOffset": 66, "endOffset": 142}, {"referenceID": 1, "context": "Using the results on polytope constraint sampling (de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008), we reduce approximate the solution to the dual ALP with the solution to a smaller, sampled LP. Basically, de Farias and Van Roy (2004) claim that given a set of affine constraints in Rd and some measure q over these constraints, if we sample k = O(d log(1/\u03b4)/\u01eb) constraints, then with probability at least 1 \u2212 \u03b4, any point that satisfies all of these k sampled constraints also satisfies 1 \u2212 \u01eb of the original set of constraints under measure q.", "startOffset": 80, "endOffset": 268}, {"referenceID": 1, "context": "Using the results on polytope constraint sampling (de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008), we reduce approximate the solution to the dual ALP with the solution to a smaller, sampled LP. Basically, de Farias and Van Roy (2004) claim that given a set of affine constraints in Rd and some measure q over these constraints, if we sample k = O(d log(1/\u03b4)/\u01eb) constraints, then with probability at least 1 \u2212 \u03b4, any point that satisfies all of these k sampled constraints also satisfies 1 \u2212 \u01eb of the original set of constraints under measure q. This result is independent of the number of original constraints. Let L be a family of affine constraints indexed by i: constraint i is satisfied at point w \u2208 Rd if ai w + bi \u2265 0. Let I be the family of constraints by selecting k random constraints in L with respect to measure q. Theorem 6 (de Farias and Van Roy (2004)).", "startOffset": 80, "endOffset": 900}, {"referenceID": 16, "context": "Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.", "startOffset": 0, "endOffset": 73}, {"referenceID": 16, "context": "Rybko and Stolyar (1992) and more recently de Farias and Van Roy (2003a) (c.f. section 6.2). There are four queues, \u03bc1, . . . , \u03bc4, each with state 0, . . . , B. Since the cardinality of the state space is X = (1 +B) 4, even a modest B results in huge state-spaces. For time t, let Xt \u2208 X be the state and si,t \u2208 {0, 1}, i = 1, 2, 3, 3 denote whether queue i is being served. Server 1 only serves queue 1 or 4, server 2 only serves queue 2 or 3, and neither server can idle. Thus, s1,t + s4,t = 1 and s2,t + s3,t = 1. The dynamics are as follows. At each time t, the following random variables are sampled independently: A1,t \u223c Bernoulli(a1), A3,t \u223c Bernoulli(a3), and Di,t \u223c Bernoulli(di \u2217 si,t) for i = 1, 2, 3, 4. Using e1, . . . , e4 to denote the standard basis vectors, the dynamics are: X \u2032 t+1 =Xt +A1,te1 +A3,te3 +D1,t(e2 \u2212 e1)\u2212D2,te2 +D3,t(e4 \u2212 e3)\u2212D4,te4, and Xt+1 = max(0,min((B),X \u2032 t+1)) (i.e. all four states are thresholded from below by 0 and above by B). The loss function is the total queue size: l(Xt) = ||Xt||1. We compared our method against two common heuristics. In the first, denoted LONGER, each server operates on the queue that is longer with ties broken uniformly at random (e.g. if queue 1 and 4 had the same size, they are equally likely to be served). In the second, denoted LBFS (last buffer first served), the downstream queues always have priority (server 1 will serve queue 4 unless it has length 0, and server 2 will serve queue 2 unless it has length 0). These heuristics are common and have been used an benchmarks for queueing networks (e.g. de Farias and Van Roy (2003a)).", "startOffset": 0, "endOffset": 1612}], "year": 2014, "abstractText": "We consider the problem of controlling a Markov decision process (MDP) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a lowdimensional family of policies. We use the dual linear programming formulation of the MDP average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low-dimensional subset of the set of stationary distributions (defined in terms of state-action features) as the comparison class. We propose two techniques, one based on stochastic convex optimization, and one based on constraint sampling. In both cases, we give bounds that show that the performance of our algorithms approaches the best achievable by any policy in the comparison class. Most importantly, these results depend on the size of the comparison class, but not on the size of the state space. Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application.", "creator": "LaTeX with hyperref package"}}}