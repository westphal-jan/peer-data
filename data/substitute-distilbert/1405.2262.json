{"id": "1405.2262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2014", "title": "Training Deep Fourier Neural Networks To Fit Time-Series Data", "abstract": "we present a risk toward restoring a persistent neural network inducing sinusoidal activation functions to fit to time - series data. weights are weighted using a fast fourier transform, then trained with regularization conditions improve generalization. a simple dynamic parameter tuning method is employed to adjust both the learning sequences by regularization term, such that stability and efficient training are both achieved. we show how deeper layers can be utilized to model the observed sequence using stable sparser set like sinusoid units, and how non - uniform regularization can improve generalization of promoting the shifting of weight toward simpler units. the method is demonstrated with time - scaling problems to show that it sticks to effective extrapolation of nonlinear trends.", "histories": [["v1", "Fri, 9 May 2014 15:23:06 GMT  (273kb,D)", "http://arxiv.org/abs/1405.2262v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["michael s gashler", "stephen c ashmore"], "accepted": false, "id": "1405.2262"}, "pdf": {"name": "1405.2262.pdf", "metadata": {"source": "CRF", "title": "Training Deep Fourier Neural Networks To Fit Time-Series Data", "authors": ["Michael S. Gashler", "Stephen C. Ashmore"], "emails": ["scashmor}@uark.edu,"], "sections": [{"heading": null, "text": "Keywords: neural networks, time-series, curve fitting, extrapolation, Fourier decomposition"}, {"heading": "1 Introduction", "text": "Finding an effective method for predicting nonlinear trends in time-series data is a long-standing unsolved problem with numerous potential applications, including weather prediction, market analysis, and control of dynamical systems. Fourier decompositions provide a mechanism to make neural networks with sinusoidal activation functions fit to a training sequence [25,38,44], but it is one thing to fit a curve to a training sequence, and quite another to make it extrapolate effectively to predict future nonlinear trends. We present a new method utilizing deep neural network training techniques to transform a Fourier neural network into one that can facilitate practical and effective extrapolation of future nonlinear trends.\nMuch of the work in machine learning thus far has focused on modeling static systems. These are systems with behavior that depends only on a set of inputs, and do not change behavior over time. One example of a static system is medical diagnosis. An electronic diagnostic tool could be designed to evaluate the symptoms that a patient reports and attempt to label the patient with a corresponding medical condition. Presumably, a correct diagnosis depends only\n? The final publication is available at Springer via http://dx.doi.org/[insert DOI]\nar X\niv :1\n40 5.\n22 62\nv1 [\ncs .N\nE ]\n9 M\nay 2\non the reported symptoms. Certainly, time plays a factor in the prevalence of medical conditions, but the change is often either too slow, or too unpredictable to warrant consideration in the model. Thus, time is effectively an irrelevant feature in this domain.\nWhen time is a relevant feature, such as with climate patterns, market prices, population dynamics, and control systems, one possible modeling approach is to simply treat time as yet another input feature. For example, one could select a curve and adjust its coefficients until the curve approximately fits with all the observations that have been made so far. Predictions could then be made by feeding in an arbitrary time to compute a future prediction.\nNonlinear curve-fitting approaches tend to be very effective at interpolation, predicting values among those for which it was trained, but they often struggle with extrapolation, predicting values outside those for which it was trained. Because extrapolation requires predicting in a region that is separated from all available samples, any superfluous complexity in the model tends to render predictions very poor. Thus far, only very simple models, such as linear regression, have been generally effective at extrapolating trends in time-series data. Finding an effective general method for nonlinear extrapolation remains an open challenge.\nWe use a deep artificial neural network to fit time-series data. Artificial neural networks are not typically considered to be simple models. Indeed, a neural network with only one hidden layer has been shown to be a universal function approximator [9]. Further, deep neural networks, which have multiple hidden layers, are used for their ability to fit to very complex functions. For example, they have been very effective in the domain of visual recognition [6,23,22,12,37]. It would be intuitive to assume, therefore, that deep neural networks would be a poor choice of model for extrapolation. However, our work demonstrates that a careful approach to regularization can enable complex models to extrapolate effectively, even with nonlinear trends.\nThis document is layed out as follows: Section 2 gives an intuitive-level description of our approach for fitting curves to time-series data. Section 3 summarizes related works. Section 4 gives a technical description of our approach. Section 5 reports results that validate our approach. Finally, Section 6 summarizes the contributions of this work."}, {"heading": "2 Intuitive-level Algorithm Description", "text": "Our approach uses a deep artificial neural network with a mixture of activation functions. We train the network using stochastic gradient descent [40]. Each unit in the artificial neural network uses one of three activation functions, illustrated in Figure 1, sinusoid: f(x) = sin(x), softplus: f(x) = loge(1 + e\nx), or identity: f(x) = x. Using the \u201cidentity\u201d activation function creates a linear unit, which is only capable of modeling linear components in the data. Nonlinear components in the data require a nonlinear activation function. The softplus units enable\nthe network to fit to non-repeating nonlinearities in the training sequence. The sinusoid units enable the network to fit to repeating nonlinearities in the data.\nConspicuously absent in our design are units with a logistic activation function, which are commonly used in neural networks. We do not use these in our network because they have a tendency to produce bends in the model that are not motivated by the training data. Figure 2 illustrates this undesirable behavior. Softplus units, therefore, can be expected to yield better generalizing predictions. Since softplus units are a softened variant of rectified linear units, this intuition is consistent with published empirical results showing that rectified linear units can outperform logistic units [28]. Furthermore, as visualized in Figure 3, two softplus units can be combined to approximate the behavior of a single logistic unit. Therefore, any training data that can be fitted with logistic units can also be fitted with twice as many softplus units. Perhaps, one reason the research\ncommunity did not recognize the superiority of softplus units earlier is because of this necessity to use more of them. Thus, in any pair-wise comparisons using the same number of network units, logistic units may appear to exhibit more flexibility for fitting to nonlinear data.\nAs training begins, we use the fast Fourier transform to compute weights for the sinusoidal units, such that they perfectly reconstruct the training signal. (Similar approaches have been previously proposed [25,38,44].) Unfortunately, the fast Fourier transform always results in a model that predicts the training sequence will repeat in the future, as depicted in Figure 4. Such models tend to generalize very poorly."}, {"heading": "2.1 Regularization", "text": "In order to achieve better nonlinear extrapolation, it is necessary to simplify the model. Occam\u2019s razor suggests that the simplest model capable of fitting to the training data is most likely to yield the best generalizing predictions. The simplest unit in our model is the linear unit. Therefore, it is desirable to shift as much of the total weight in the network away from the sinusoid units and onto the linear units, while still fitting the model with the training data. Where nonrecurring nonlinearities occur in the training data, linear units will not be able to fit with the training data. In such cases, we want the weight to shift toward the softplus units. Where repeating nonlinearities occur in the training data, the sinusoid units should retain some of their initial weight. We accomplish this weight-shifting during training using a combination of regularization techniques.\nA common approach for regularizing a neural network is weight decay, or more specifically L2 regularization. In stochastic gradient descent, L2 regularization is implemented by multiplying all the weights in the neural network by some factor, 1 \u2212 \u03b7\u03bb, before the presentation of each training pattern. (\u03b7 is the\nlearning rate for training, and \u03bb is a small term that controls how strongly the network is regularized.) L2 regularization prevents the network weights from growing too large during training. This tends to prevent the curve or surface represented by the neural network from exhibiting dramatic bends or folds.\nA related technique is L1 regularization. It is implemented by subtracting the term, 1\u2212 \u03b7\u03bb, from all positive weights, and adding that term to all weights that have a negative value. L1 regularization tends to promote sparsity in the neural network. It causes the network to utilize as few non-zero weights as possible while fitting the training data. It tends to be effective when a signal is comprised of a small number of components.\nIn our experimentation, we found that L2 regularization is much more effective at shifting weight away from the sinusoid units. Unfortunately, L2 regularization tends to distribute weight among all of the network units. Since it would be very difficult to select a topology a priori that contains a sufficient number of network units without containing an excessive number of network units, it is intuitive that the sparse models that result from L1 regularization may generalize better. Therefore, we begin training with L2 regularization and complete training with L1 regularization. In our initial experiments, we tried using Lp regularization, and slowly adjusting the value of p from 2 down to 1. Unfortunately, computing this exponent is a somewhat expensive operation. Therefore, we also tried the simpler approach of using L2 regularization during the first half of training, and L1 regularization during the second half of training. We found this simpler approach to work nearly as well.\nIn order to bias the network toward shifting weight onto the simpler units, we use a non-uniform regularization term. For sinusoid units, we regularize with the standard term 1\u2212 \u03b7\u03bb. For softplus units, we use 1\u2212 0.1\u03b7\u03bb. For linear units, we use 1\u2212 0.01\u03b7\u03bb. Thus, the strongest regularization is applied to the sinusoid units, while only very weak regularization is applied to the linear units. These constant factors (1, 0.1, and 0.01) were selected intuitively. We briefly attempted to optimize them, but in our experiments small variations tended to have little influence on the final results. Therefore, we proceeded with these intuitively selected factors."}, {"heading": "2.2 Training procedure", "text": "Stochastic gradient descent relies on being able to update the network weights without visiting every pattern in the training data. This works well with the logistic activation function because its derivative outputs a value close to zero everywhere except for input values close to zero. This gives it a very local region of influence, such that each pattern presentation will only make significant changes to the few weights that are relevant for predicting that pattern. The three activation functions we use in our algorithm, however, have non-local regions of influence. In other words, each pattern presentation will often significantly affect many or all of the weights in the network. This has the effect of giving the network a strong tendency to diverge unless a very small learning rate is used.\nIn our experiments we found that the values of suitable learning rates varied significantly as training progressed. Small learning rates, such as 10\u22124, would quickly lead to divergence. Even smaller learning rates, such as 10\u22126 would work for a larger number of training epochs, but would still result in divergence at some point during training. The only static learning rates that always lead to convergence were so small that training became impractical. Therefore, the crux of our training algorithm relies on dynamically adjusting the learning rate \u03b7, and regularization term \u03bb, during training.\nBefore training begins, we measure the standard deviation, \u03c3, of the training data to serve as a baseline for accuracy. The output layer of our neural network is a single linear unit. We call this layer 4. The layer that feeds into the output layer, layer 3, contains several units of each type: sinusoid, softplus, and linear. We use the Fast Fourier Transform to initialize the sinusoid units, and we initialize the other units in a manner such that they intially have little influence on the output. (More specific details are given in Section 4.)\nLayers 1 and 2 consist of only softplus and linear units. These layers are initialized to approximate the identity function, then are slightly perturbed. These layers serve the important purpose of enabling the model to essentially \u201cwarp time\u201d as needed to fit the training data with a sparse set of sinusoid units. In other words, these layers enable the model to find simple repeating patterns, even in real-world data where some of the oscillations may not occur at precisely regular intervals. Because these layers are further from the output end of the model, backpropagation will refine them more slowly than the other layers. This\nis desirable because warping time in the temporal region of the training sequence is somewhat of a \u201clast resort\u201d method for simplifying the model. Ironically, deep neural networks are often cited for their ability to produce complex models, but we are actually using them to produce a simpler model. More specifically, we use the deeper layers to \u201cexplain away\u201d superfluous complexity in the training sequence. Since the deeper layers contain no sinusoid units, they will specialize only in the temporal region of the training data, allowing the the sinusoid units that apply everywhere to fit the data with a simpler model.\nAfter initialization, but before training begins, the entire network already fits to the training data with a very small root-mean squared error (RMSE). Usually, it is much smaller smaller than 0.1\u03c3. At this point, the model simply predicts that the training data repeats itself into the future, as in Figure 4. As training proceeds, we use stochastic gradient descent to keep the RMSE near 0.1\u03c3, and regularization to improve generalization. (Smaller values result in a tighter fit, but longer training times.) As training completes, the model no longer predicts that the training data will repeat, but predicts a continuation of the nonlinear trends exhibited in the training data.\nIt is important that \u03b7 and \u03bb be dynamically adjusted using different mechanisms, so the ratio between them is free to change. After each epoch of training, we adjust \u03bb such that RMSE stays near 0.1\u03c3. When the RMSE is bigger than 0.1\u03c3, we make \u03bb smaller, and when the RMSE is smaller than 0.1\u03c3, we make \u03bb bigger. In contrast with this approach, we make \u03b7 bigger after each epoch by multiplying it by a constant factor. Eventually, this will always cause divergence, which can be detected when the RMSE score becomes very large. When that occurs, we make \u03b7 much smaller, and restore the network weights to the last point with a reasonable RMSE score. Specific implementation details about this dynamic tuning process are given in Section 4.\nThis dynamic parameter tuning process seeks to reduce training time by keeping the learning rate large while still ensuring convergence. At times, the learning rate will be too large, which may temporarily have a negative effect on the dynamic process for tuning \u03bb, but we have found that divergence tends to happen quickly. Therefore, most of the time, \u03b7 is within a reasonable range, and the process for tuning \u03bb can operate without any regard to the current value of \u03b7."}, {"heading": "3 Related Works", "text": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10]. Therefore, in this section, we will review only the works necessary to give a highlevel overview of how our method fits among the existing techniques, and defer to these other papers to complete an exhaustive survey of related techniques. At a high level, the various approaches for training neural networks to model time-series data may be broadly categorized into three major groups. Figure 5 illustrates the basic models that correspond with each of these groups.\nThe most common, and perhaps simplest, methods for time-series prediction involve feeding sample values from the past into the model to predict sample values in the future [14,1]. (See Figure 5.A.) These methods require no recurrent connections, and can be implemented without the need for any training techniques specifically designed for temporal data. Consequently, these can be easily implemented using many available machine learning toolkits, not just those specifically designed for forecasting time-series data. These convenient properties make these methods appealing for a broad range of applications. Unfortunately, they also have some significant limitations: The window size for inputs and predictions must be determined prior to training. Also, they essentially use recent observations to represent state, and they are particularly vulnerable to noise in the observed values.\nA more sophisticated group of methods involves neural networks with recurrent connections [29]. These produce their own internal representation of state. (See Figure 5.B.) This enables them to learn how much to adjust their representations of state based on observed values, and hence operate in a manner more robust against noisy observations.\nExisting methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26]. Unfortunately, in practice, evolutionary optimization tends to be extremely slow, and it is unable to yield good results with many difficult problems [35,34]. Gradient-based methods tend to converge faster than global optimization methods, but they are more susceptible to problems with local optima. With recurrent neural networks, local optima are a more significant problem than with regular feed-forward networks [8]. The recurrent feedback can create chaotic responses in their error surfaces, and can distribute local optima in poor regions of the space.\nIn early instantiations, recurrent neural networks struggled to retain internal state over long periods of time because the logistic activation functions typically used with neural networks tend to diminish the values with each time step. Long Short Term Memory architectures address this problem by using only linear units with the recurrent loops [19]. This advance has made recurrent neural networks much more capable for modeling time-series data. Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].\nThe third, and most relevant, group of methods for forecasting time-series data is regression followed by extrapolation. Extrapolation with linear regression has long been a standard method for forecasting trends. Extrapolating nonlinear trends, however, has generally been ineffective. The general model, shown in Figure 5.C, is very simple, but training it in a manner that will make generalizing predictions can be extremely challenging. For this reason, this branch of timeseries forecasting has been much less studied, and remains a relatively immature field. Our work attempts to jump-start research in this branch by presenting a practical method for extrapolating nonlinear trends in time-series data.\nThe idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44]. These studies, however, do not address the important practical issues of stability during training and regularizing the model to promote better generalization. By contrast, our work treats the matter of representing a neural network that can combine basis functions as a solved problem, and focuses on the more challenging problem of refining these networks to achieve reliable nonlinear extrapolation.\nMany other approaches, besides Fourier neural networks, have been proposed for fitting to time-series data. Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32]."}, {"heading": "4 Technical Algorithm Description", "text": "In this section we describe implementation details necessary to reproduce our results. The intuition for this algorithm is described in Section 2. To assist further research, we have integrated the major parts of our implementation into the Waffles machine learning toolkit [15]."}, {"heading": "4.1 Initializing Weights", "text": "Our neural network contains 4 layers. Layer 4 (the output layer) contains a single linear unit. Layer 3 contains k sinusoid units, where k is a power of 2. In our implementation, layer 3 also contains 12 softplus units, and 12 linear units. (These quantities can be adjusted, but due to our use of L1 regularization, there is little need to optimize them.)\nWe will consider an explanation of the fast Fourier transform to be beyond the scope of this paper, and will proceed only to describe how to use it to initialize the weights of the sinusoid units. The fast Fourier transform requires a sequence of k complex values as input. The real component is given by the values in the training sequence. The imaginary component consists of all zeros. The output of the fast Fourier transform is also a sequence of k complex values, which we denote as \u3008{r1, i1}, {r2, i2}, {r3, i3}, . . . , {rk, ik}\u3009. Let w4j refer to the weight that feeds from sinusoid unit j into the one output unit in layer 4. Let b4 refer to the input bias of the unit in layer 4. For each odd value of j, w4j is initialized to 2r(j/2+2)/k, where j/2 drops the remainder. For each even value of j, w 4 j is initialized to 2i(j/2+1)/k. The values in w 4 k and w 4 k\u22121 must then be divided by 2. b4 is initialized to r1/k.\nLet w3j refer to the weight that feeds from the first unit in layer 2 into unit j of layer 3. Let b3j refer to the input bias of unit j in layer 3. For each odd value of j, w3j is initialized to 2\u03c0(j/2 + 1), where j/2 drops the remainder, and b3j is initialized to \u03c0/2. For each even value of j, w 3 j is initialized to 2\u03c0(j/2), and b3j is initialized to \u03c0. All other weights feeding into the sigmoid units are initialized to 0. Note that this parameterization will cause the training values to be represented for input values between 0 and 1. If the user prefers to represent them from 0 to k \u2212 1, the weights in layer 3 should be further divided by k.\nThe weights of the linear units are initialized with the identity matrix and their input bias values are set to zero, such that these units approximate the identity function. The weights of the softplus units are also initialized to approximate the identity function. This is done by setting the weights with the identity matrix. The input bias for each softplus unit is set to a value, s. Then, to compensate for this bias, each weight wj that feeds out from the softplus unit into the next layer is decremented by swj . Our implementation uses the value s = 10. Larger values will cause the unit to approximate the identity function more closely, but will cause the unit to require more training to represent a nonlinear bend in the data.\nLayers 1 and 2 each consist of 12 softplus units and 12 linear units. These are initialized in the same manner as the softplus and linear units in layer 3. Only one input value (representing time) feeds into layer 1. At this point, the network should perfectly predict the training sequence, except with some small allowance for the chosen value of s, and the numerical precisions of the various computations. Before training begins, we slightly perturb all of the weights and input biases by small random values. To do this, we add random values from a Normal distribution with a mean of 0 and a standard deviation of 10\u22125.\nTo facilitate dynamic parameter tuning, the standard devation of the training sequence is computed: \u03c3 = \u221a\n1 k \u2211 i(vi \u2212 \u00b5)2, where vi refers to the ith value in\nthe training sequence, and \u00b5 is the mean value in the training sequence."}, {"heading": "4.2 Training", "text": "To begin training, we initialize the learning rate \u03b7 to 10\u22129, and the regularization term \u03bb to 1. (Note that these values are dynamically adjusted during training.) Each training epoch presents each of the k time-series values in the training sequence to the network in random order. Each presentation involves feeding a time value in, regularizing the weights, and then using regular backpropagation to update the weights by stochastic gradient descent. During the first half of the training epochs, L2 regularization is used. During the second half of the training epochs, L1 regularization is used. In each of our experiments, we performed 107 epochs of training. At the end of each training epoch, the values \u03b7 and \u03bb are dynamically adjusted. This is done by measuring the root mean squared prediction error over the training values, . If < 0.1\u03c3, then \u03bb = \u03bb \u2217 1.001 else \u03bb = \u03bb/1.001. \u03b7 is always scaled by a factor of 1.01. If < 0.2\u03c3, then a backup copy of the network weights is made, or else the weights are restored from the most recent backup copy and \u03b7 = 0.1\u03b7. We note that all of these constant values could potentially be optimized, but we anticipate that only a small amount of performance would actually be gained by doing this, so we did not actually attempt to optimize them."}, {"heading": "5 Validation", "text": "In this section, we present visual results showing that our method is able to model nonlinear trends into the future. Deliberately absent in this section are quantitative comparisons with corresponding results using recurrent neural network methods. If our intent were to establish our method as the new state of the art in time-series forcasting, then such comparisons would be essential. However, our intent is to present advances in methods for extrapolation using nonlinear regression. This branch of time-series forecasting is not yet as well-refined for this task as recurrent neural network methods. If the research community were to only focus on improving the method with the highest precision, then it would risk becoming stuck in a local optimum. By advancing this alternative approach, we intend to help open the way for more research interest in this area, which could potentially lead to different use-cases or a long-term shift in how timeseries forecasting is done.\nOur first experiment shows results with a toy problem involving a sine wave with the addition of a linear trend, f(t) = sin(t)+0.1t. For training, we used this equation to generate a sequence of 128 values. Upon initializing the weights, but before training, the model predicts that the training sequence repeats into the future, as shown in Figure 4. This model assigns significant weight to many of its sinusoid units. As training proceeds, however, the model is greatly simplified while still fitting with the training sequence. The final model assigns nearly all of its weight to just two units: a sinusoid unit and a linear unit, which matches the equation that was used to generate the training sequence. The results are shown in Figure 6. The blue dots spanning the left half of the plot represent the\ntraining sequence. The red dots spanning the right half of the plot are test values generated by continuing to sample from the same equation. These were withheld from our algorithm during training. The green curve shows the continuous predictions of the trained model.\nIt can be observed that the model does not perfectly fit the training sequence. This occurs because we trained it to fit with an RMSE of about 0.1\u03c3. One potential future improvement to our algorithm might be to decay this value as training progresses, such that a tighter fit is expected at the end of training. As expected, the model is slightly less accurate with the test sequence than with the training sequence. Although the predictions are slightly out of phase, they still model all of the nonlinear trends in the test sequence very well, which is the problem we attempted to address.\nWe also evaluated our algorithm on this problem with varying training sequence lengths. We tested with sizes of 32, 64, 128, 256, and 512. In every case, the results were very similar, so we only show results for 128 samples as a representative case. These experiments show that only a few samples are really needed to solve this simple problem, but using many samples does not cause any problems.\nNext, we tested our algorithm with a real-world dataset. We obtained the weekly temperature measurements in Anchorage Alaska from April 2009 to the present, from http://noaa.gov. We selected temperature data because it exhibits clear nonlinear trends, and we chose to use data from Anchorage Alaska because it appeared to fluctuate from a simple sine wave more than data from other locations that we considered. Our results are shown in Figure 7. The blue points\non the left half of the plot were given as the training sequence. The red points on the right half were used as the test sequence.\nAs with the previous experiment, the test results were slightly out of phase. It also failed to anticipate some of the deep temperature plunges that occurred during the last few winters. Nevertheless, our extrapolation was able to anticipate the principal nonlinear trends quite well. This is impressive considering that it was trained on data that spanned fewer than 3 full periods of the primary oscillating trend in the data. Whether the high-frequency fluctuations predicted by the model are noise, or contain some meaningful information, is not clear. However, it is simple to remove them from the model. This is done by setting the weights that feed from the high-frequency sinusoid units into the linear output unit to zero. Figure 8 shows results after zeroing out the highest-frequency half of the sinusoid nodes as a post-processing step. The non-uniform shape of the resulting model shows that our model has done more than merely fit a single sinusoid to the data, and its close fit with the future data shows that it effectively anticipated nonlinear temperature trends.\nTo demonstrate that our algorithm works well with real-world data when few training points are available, we repeated this experiment using only 64 training points. Specifically, we divided the test sequence from the Anchorage temperature problem into a training and test sequence for this problem. We also applied a low-pass filter to these results, shown in Figure 9. Note that even with so few available training points, the extrapolated trend still fits the actual trend effectively.\nTo show that our method is what leads to these good results, we repeated the experiment using regular backpropagation. We used an identical network topology, and trained with regularization, but we did not use the fast Fourier transform to initialize the weights, and we did not dynamically adjust the parameters during training. As expected, the results of this approach, shown in Figure 10 were very poor. No post-processing was performed on these results. We also performed regular backpropagation with each of the other experiments in this paper, and obtained even worse results. In most cases, it merely produced a linear trend line because it became stuck in a local optimum very early in the training process. We also empirically measured the RMSE scores for both training methods over each problem, and found that our approach was consistently better. Because the regular approach never produced reasonable extrapolation results, we do not report these empirical comparison scores.\nTo demonstrate the versatility of our method, we tested it with the MackeyGlass series. This test is interesting because it involves a chaotic series, rather than a periodic series. Results with this data are given in Figure 11. As with previous experiments, the blue points on the left half of the plot represent the training sequence, the red points on the right half represent the test sequence, and the green curve is the trained model. We note that the model begins to prematurely descend very early in the test sequence (at approximately time 1.02), which causes its predictions to be slightly out of phase for the remainder of the test sequence. Nevertheless, the model clearly exhibits similar patterns\nto those in the test set. Significantly, these patterns do not repeat those in the training sequence, nor does the model repeat its earlier predictions. This shows that our method can be effective for predicting even non-repeating trends in the near term."}, {"heading": "6 Summary of Contributions", "text": "We presented a method for fitting a neural network to time-series data for the purpose of extrapolating nonlinear trends in the data. Our method initializes the weights of sinusoid units using the fast Fourier transform. In order to promote better generalization, our method adds several units with simpler activation functions to the network, and then trains in a manner that enables the weight to shift toward these simpler units, while still fitting to the training data. We utilize a simple dynamic parameter tuning method to train efficiently while ensuring that the model always fits the training sequence well. We presented results with several experiments showing that our method is effective at predicting nonlinear trends in time-series data. This paper makes several contributions to the current knowlege, which we itemize here:\n\u2022 It proposes new theoretical intuition for why deep neural networks can actually facilitate finding simpler predictive models than can be found with shallow networks. Specifically, the deeper layers provide a mechanism to \u201cwarp time\u201d in the temporal region of the training sequence, allowing subsequent layers to fit it with a sparser set of sinusoid units. \u2022 It demonstrates that shifting weight toward simpler units can be promoted during training by regularizing the complex units more heavily. \u2022 It describes a dynamic method for simultaneously tuning both the learning rate and regularization terms. \u2022 It shows that dynamic tuning can be an effective solution to the instability problems that inherently occur with sinusoidal activation functions. \u2022 Most significantly, it unifies all of these techniques into a method for nonlinear extrapolation with time-series data, and demonstrates that it is both practical and effective."}], "references": [{"title": "The analysis of observed chaotic data in physical systems", "author": ["H.D. Abarbanel", "R. Brown", "J.J. Sidorowich", "L.S. Tsimring"], "venue": "Reviews of modern physics 65(4), 1331", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Combining neural network forecasts on wavelettransformed time series", "author": ["A. Aussem", "F. Murtagh"], "venue": "Connection Science 9(1), 113\u2013122", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "A real-coded genetic algorithm for training recurrent neural networks", "author": ["A. Blanco", "M. Delgado", "M.C. Pegalajar"], "venue": "Neural Networks 14(1), 93\u2013105", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Predicting chaotic time series with wavelet networks", "author": ["L. Cao", "Y. Hong", "H. Fang", "G. He"], "venue": "Physica D: Nonlinear Phenomena 85(1), 225\u2013238", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Time-series prediction using a local linear wavelet neural network", "author": ["Y. Chen", "B. Yang", "J. Dong"], "venue": "Neurocomputing 69(4), 449\u2013465", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "A committee of neural networks for traffic sign classification", "author": ["D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks (IJCNN), The 2011 International Joint Conference on. pp. 1918\u20131921. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "New life for neural networks", "author": ["G.W. Cottrell"], "venue": "Science 313(5786), 454\u2013455", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "An application of non-linear programming to train recurrent neural networks in time series prediction problems", "author": ["M.P. Cu\u00e9llar", "M. Delgado", "M.C. Pegalajar"], "venue": "Enterprise Information Systems VII pp. 95\u2013102", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems 2(4), 303\u2013314", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "25 years of time series forecasting", "author": ["J.G. De Gooijer", "R.J. Hyndman"], "venue": "International journal of forecasting 22(3), 443\u2013473", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural networks for time series processing", "author": ["G. Dorffner"], "venue": "Neural Network World. Citeseer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning deep face representation", "author": ["H. Fan", "Z. Cao", "Y. Jiang", "Q. Yin", "C. Doudou"], "venue": "arXiv preprint arXiv:1403.2802", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic creation of an autonomous agent: Genetic evolution of a neural-network driven robot", "author": ["D. Floreano", "F. Mondada"], "venue": "From animals to animats 3, 421\u2013430", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Time series prediction and neural networks", "author": ["R.J. Frank", "N. Davey", "S.P. Hunt"], "venue": "Journal of Intelligent and Robotic Systems 31(1-3), 91\u2013103", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Waffles: A machine learning toolkit", "author": ["M.S. Gashler"], "venue": "Journal of Machine Learning Research MLOSS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Temporal nonlinear dimensionality reduction", "author": ["M.S. Gashler", "T.R. Martinez"], "venue": "Proceedings of the International Joint Conference on Neural Networks. pp. 1959\u2013 1966. IEEE Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalenet-multiscale neural-network architecture for time series prediction", "author": ["A.B. Geva"], "venue": "Neural Networks, IEEE Transactions on 9(6), 1471\u20131482", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "Mohamed", "A.r.", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. pp. 6645\u20136649. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Forecasting stock market movement direction with support vector machine", "author": ["W. Huang", "Y. Nakamori", "S.Y. Wang"], "venue": "Computers & Operations Research 32(10), 2513\u20132522", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Designing a neural network for forecasting financial and economic time series", "author": ["I. Kaastra", "M. Boyd"], "venue": "Neurocomputing 10(3), 215\u2013236", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25. pp. 1106\u20131114", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1112.6209", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Using support vector machines for long-term discharge prediction", "author": ["J.Y. Lin", "C.T. Cheng", "K.W. Chau"], "venue": "Hydrological Sciences Journal 51(4), 599\u2013612", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Fourier neural networks: An approach with sinusoidal activation functions", "author": ["L. Mingo", "L. Aslanyan", "J. Castellanos", "M. Diaz", "V. Riazanov"], "venue": "International Journal ITA 11(1)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "A focused backpropagation algorithm for temporal pattern recognition", "author": ["M.C. Mozer"], "venue": "Backpropagation: theory, architectures, and applications pp. 137\u2013169", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "A focused back-propagation algorithm for temporal pattern recognition", "author": ["M.C. Mozer"], "venue": "Complex systems 3(4), 349\u2013381", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1989}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 807\u2013814", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Training recurrent neural networks: Why and how ? an illustration in dynamical process modeling", "author": ["O. Nerrand", "P. Roussel-Ragot", "D. Urbani", "L. Personnaz", "G. Dreyfus"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Gpu implementation of neural networks", "author": ["K.S. Oh", "K. Jung"], "venue": "Pattern Recognition 37(6), 1311\u20131314", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "The utility driven dynamic error propagation network", "author": ["A.J. Robinson", "F. Fallside"], "venue": "Tech. Rep. CUED/F-INFENG/TR.1, Cambridge University, Engineering Department", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1987}, {"title": "Time series prediction using support vector machines: a survey", "author": ["N.I. Sapankevych", "R. Sankar"], "venue": "Computational Intelligence Magazine, IEEE 4(2), 24\u201338", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Fourier neural networks", "author": ["A. Silvescu"], "venue": "Neural Networks, 1999. IJCNN\u201999. International Joint Conference on. vol. 1, pp. 488\u2013491. IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "Nonlinear black-box modeling in system identification: a unified overview", "author": ["J. Sj\u00f6berg", "Q. Zhang", "L. Ljung", "A. Benveniste", "B. Deylon", "P.Y. Glorennec", "H. Hjalmarsson", "A. Juditsky"], "venue": "Automatica 31, 1691\u20131724", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural networks for control", "author": ["E. Sontag"], "venue": "Essays on Control: Perspectives in the Theory and its Applications 14, 339\u2013380", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1993}, {"title": "Wavelet neural network classification of eeg signals by using ar model with mle preprocessing", "author": ["A. Subasi", "A. Alkan", "E. Koklukaya", "M.K. Kiymik"], "venue": "Neural Networks 18(7), 985\u2013997", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2014", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Fourier neural networks and generalized single hidden layer networks in aircraft engine fault diagnostics", "author": ["H. Tan"], "venue": "Journal of engineering for gas turbines and power 128(4), 773\u2013782", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks 1(4), 339\u2013356", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1988}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Neural Networks 16(10), 1429\u20131451", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Time series forecasting using a hybrid arima and neural network model", "author": ["G.P. Zhang"], "venue": "Neurocomputing 50, 159\u2013175", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2003}, {"title": "Forecasting with artificial neural networks:: The state of the art", "author": ["G. Zhang", "B. Eddy Patuwo", "M. Y Hu"], "venue": "International journal of forecasting 14(1), 35\u201362", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Wavelet networks", "author": ["Q. Zhang", "A. Benveniste"], "venue": "Neural Networks, IEEE Transactions on 3(6), 889\u2013898", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1992}, {"title": "Fourier-neural-network-based learning control for a class of nonlinear systems with flexible components", "author": ["W. Zuo", "Y. Zhu", "L. Cai"], "venue": "Neural Networks, IEEE Transactions on 20(1), 139\u2013151", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 24, "context": "Fourier decompositions provide a mechanism to make neural networks with sinusoidal activation functions fit to a training sequence [25,38,44], but it is one thing to fit a curve to a training sequence, and quite another to make it extrapolate effectively to predict future nonlinear trends.", "startOffset": 131, "endOffset": 141}, {"referenceID": 37, "context": "Fourier decompositions provide a mechanism to make neural networks with sinusoidal activation functions fit to a training sequence [25,38,44], but it is one thing to fit a curve to a training sequence, and quite another to make it extrapolate effectively to predict future nonlinear trends.", "startOffset": 131, "endOffset": 141}, {"referenceID": 43, "context": "Fourier decompositions provide a mechanism to make neural networks with sinusoidal activation functions fit to a training sequence [25,38,44], but it is one thing to fit a curve to a training sequence, and quite another to make it extrapolate effectively to predict future nonlinear trends.", "startOffset": 131, "endOffset": 141}, {"referenceID": 8, "context": "Indeed, a neural network with only one hidden layer has been shown to be a universal function approximator [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 22, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 21, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 11, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 36, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 39, "context": "We train the network using stochastic gradient descent [40].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Since softplus units are a softened variant of rectified linear units, this intuition is consistent with published empirical results showing that rectified linear units can outperform logistic units [28].", "startOffset": 199, "endOffset": 203}, {"referenceID": 24, "context": "(Similar approaches have been previously proposed [25,38,44].", "startOffset": 50, "endOffset": 60}, {"referenceID": 37, "context": "(Similar approaches have been previously proposed [25,38,44].", "startOffset": 50, "endOffset": 60}, {"referenceID": 43, "context": "(Similar approaches have been previously proposed [25,38,44].", "startOffset": 50, "endOffset": 60}, {"referenceID": 10, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 20, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 41, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 13, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 40, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 9, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 13, "context": "The most common, and perhaps simplest, methods for time-series prediction involve feeding sample values from the past into the model to predict sample values in the future [14,1].", "startOffset": 172, "endOffset": 178}, {"referenceID": 0, "context": "The most common, and perhaps simplest, methods for time-series prediction involve feeding sample values from the past into the model to predict sample values in the future [14,1].", "startOffset": 172, "endOffset": 178}, {"referenceID": 28, "context": "A more sophisticated group of methods involves neural networks with recurrent connections [29].", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 206, "endOffset": 215}, {"referenceID": 33, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 206, "endOffset": 215}, {"referenceID": 2, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 206, "endOffset": 215}, {"referenceID": 26, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 308, "endOffset": 315}, {"referenceID": 38, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 308, "endOffset": 315}, {"referenceID": 30, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 348, "endOffset": 355}, {"referenceID": 25, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 348, "endOffset": 355}, {"referenceID": 34, "context": "Unfortunately, in practice, evolutionary optimization tends to be extremely slow, and it is unable to yield good results with many difficult problems [35,34].", "startOffset": 150, "endOffset": 157}, {"referenceID": 33, "context": "Unfortunately, in practice, evolutionary optimization tends to be extremely slow, and it is unable to yield good results with many difficult problems [35,34].", "startOffset": 150, "endOffset": 157}, {"referenceID": 7, "context": "With recurrent neural networks, local optima are a more significant problem than with regular feed-forward networks [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 18, "context": "Long Short Term Memory architectures address this problem by using only linear units with the recurrent loops [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 6, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 15, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 17, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 32, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 177, "endOffset": 184}, {"referenceID": 24, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 177, "endOffset": 184}, {"referenceID": 37, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 251, "endOffset": 258}, {"referenceID": 43, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 251, "endOffset": 258}, {"referenceID": 42, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 3, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 1, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 16, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 35, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 4, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 19, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 95, "endOffset": 105}, {"referenceID": 23, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 95, "endOffset": 105}, {"referenceID": 31, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 95, "endOffset": 105}, {"referenceID": 14, "context": "To assist further research, we have integrated the major parts of our implementation into the Waffles machine learning toolkit [15].", "startOffset": 127, "endOffset": 131}], "year": 2014, "abstractText": "We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and regularization term, such that stability and efficient training are both achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends.", "creator": "LaTeX with hyperref package"}}}