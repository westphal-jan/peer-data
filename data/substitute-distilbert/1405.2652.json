{"id": "1405.2652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Selecting Near-Optimal Approximate State Representations in Reinforcement Learning", "abstract": "we explore a statistical learning setting introduced there ( maillard et al., nips 2011 ) where the learner does not have explicit access to the states of the underlying markov decision process ( pv ). instead, the agent only handles several models that map histories of past interactions to states. here we improve over known regret bounds in this classification and more importantly generalize to the case where linear models given to the learner don't contain a true model giving an mdp representation but only approximation of it. we thereby support improved error indices for state aggregation.", "histories": [["v1", "Mon, 12 May 2014 07:45:54 GMT  (77kb)", "https://arxiv.org/abs/1405.2652v1", null], ["v2", "Wed, 14 May 2014 12:43:36 GMT  (43kb)", "http://arxiv.org/abs/1405.2652v2", null], ["v3", "Wed, 9 Jul 2014 14:40:20 GMT  (64kb)", "http://arxiv.org/abs/1405.2652v3", null], ["v4", "Mon, 21 Jul 2014 11:52:37 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v4", null], ["v5", "Tue, 12 Aug 2014 12:19:55 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v5", null], ["v6", "Mon, 15 Sep 2014 08:32:45 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v6", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ronald ortner", "odalric-ambrym maillard", "daniil ryabko"], "accepted": false, "id": "1405.2652"}, "pdf": {"name": "1405.2652.pdf", "metadata": {"source": "CRF", "title": "Selecting Near-Optimal Approximate State Representations in Reinforcement Learning", "authors": ["Ronald Ortner", "Odalric-Ambrym Maillard", "Daniil Ryabko"], "emails": ["rortner@unileoben.ac.at,", "odalric-ambrym.maillard@ens-cachan.org,", "daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n26 52\nv6 [\ncs .L\nG ]\n1 5\nSe p"}, {"heading": "1 Introduction", "text": "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP). Instead, the learner has a set of models at her disposal that map histories (i.e., observations, chosen actions and collected rewards) to states. However, only some models give a correct MDP representation. The first regret bounds in this setting were derived in [6]. They recently have been improved in [7] and extended to infinite model sets in [8]. Here we extend and improve the results of [7] as follows. First, we do not assume anymore that the model set given to the learner contains a true model resulting in an MDP representation. Instead, models will only approximate an MDP. Second, we improve the bounds of [7] with respect to the dependence on the state space.\nFor discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138]. Here we only would like to mention the recent work [2] that considers a similar setting, however is mainly interested in the question whether the true model will be identified in the long run, a question we think is subordinate to that of minimizing the regret, which means fast learning of optimal behavior."}, {"heading": "1.1 Setting", "text": "For each time step t = 1, 2, . . ., let Ht := O \u00d7 (A \u00d7 R \u00d7 O)t\u22121 be the set of histories up to time t, where O is the set of observations, A a finite set of\nactions, and R = [0, 1] the set of possible rewards. We consider the following reinforcement learning problem: The learner receives some initial observation h1 = o1 \u2208 H1 = O. Then at any time step t > 0, the learner chooses an action at \u2208 A based on the current history ht \u2208 Ht, and receives an immediate reward rt and the next observation ot+1 from the unknown environment. Thus, ht+1 is the concatenation of ht with (at, rt, ot+1).\nState representation models.A state-representation model \u03c6 is a function from the set of histories H = \u22c3t\u22651 Ht to a finite set of states S\u03c6. A particular role will be played by state-representation models that induce a Markov decision process (MDP). An MDP is defined as a decision process in which at any discrete time t, given action at, the probability of immediate reward rt and next observation ot+1, given the past history ht, only depends on the current observation ot i.e., P (ot+1, rt|htat) = P (ot+1, rt|ot, at), and this probability is also independent of t. Observations in this process are called states of the environment. We say that a state-representation model \u03c6 is a Markov model of the environment, if the process (\u03c6(ht), at, rt), t \u2208 N is an MDP. Note that such an MDP representation needs not be unique. In particular, we assume that we obtain a Markov model when mapping each possible history to a unique state. Since these states are not visited more than once, this model is not very useful from the practical point of view, however. In general, an MDP is denoted as M(\u03c6) = (S\u03c6,A, r, p), where r(s, a) is the mean reward and p(s\u2032|s, a) the probability of a transition to state s\u2032 \u2208 S\u03c6 when choosing action a \u2208 A in state s \u2208 S\u03c6.\nWe assume that there is an underlying true Markov model \u03c6\u25e6 that gives a finite and weakly communicating MDP, that is, for each pair of states s, s\u2032 \u2208 S\u25e6 := S\u03c6\u25e6 there is a k \u2208 N and a sequence of actions a1, . . . , ak \u2208 A such that the probability of reaching state s\u2032 when starting in state s and taking actions a1, . . . , ak is positive. In such a weakly communicating MDP we can define the diameter D := D(\u03c6\u25e6) := D(M(\u03c6\u25e6)) to be the expected minimum time it takes to reach any state starting from any other state in the MDP M(\u03c6\u25e6), cf. [5]. In finite state MDPs, the Poisson equation relates the average reward \u03c1\u03c0 of any policy \u03c0 to the single step mean rewards and the transition probabilities. That is, for each policy \u03c0 that maps states to actions, it holds that\n\u03c1\u03c0 + \u03bb\u03c0(s) = r(s, \u03c0(s)) + \u2211 s\u2032\u2208S\u25e6 p(s \u2032|s, \u03c0(s)) \u00b7 \u03bb\u03c0(s\u2032), (1)\nwhere \u03bb\u03c0 is the so-called bias vector of \u03c0, which intuitively quantifies the difference in accumulated rewards when starting in different states. Accordingly, we are sometimes interested in the span of the bias vector \u03bb of an optimal policy defined as span(\u03bb) := maxs\u2208S\u25e6 \u03bb(s)\u2212mins\u2032\u2208S\u25e6 \u03bb(s\u2032). In the following we assume that rewards are bounded in [0, 1], which implies that span(\u03bb) is upper bounded by D, cf. [5, 1].\nProblem setting. Given a finite set of models \u03a6 (not necessarily containing a Markov model), we want to construct a strategy that performs as well as the algorithm that knows the underlying true Markov model \u03c6\u25e6, including its rewards and transition probabilities. For that purpose we define for the Markov\nmodel \u03c6\u25e6 the regret of any strategy at time T , cf. [5, 1, 6], as\n\u2206(\u03c6\u25e6, T ) := T\u03c1\u2217(\u03c6\u25e6)\u2212\u2211Tt=1 rt ,\nwhere rt are the rewards received when following the proposed strategy and \u03c1\u2217(\u03c6\u25e6) is the average optimal reward in \u03c6\u25e6, i.e. \u03c1\u2217(\u03c6\u25e6) := \u03c1\u2217(M(\u03c6\u25e6)) := \u03c1(M(\u03c6\u25e6), \u03c0\u2217\u03c6\u25e6) := limT\u2192\u221e 1 T E [\u2211T t=1 rt(\u03c0 \u2217 \u03c6\u25e6) ] where rt(\u03c0 \u2217 \u03c6\u25e6) are the rewards received when following the optimal policy \u03c0\u2217\u03c6\u25e6 on M(\u03c6 \u25e6). Note that for weakly communicating MDPs the average optimal reward does not depend on the initial state.\nWe consider the case when \u03a6 is finite and the learner has no knowledge of the correct approximation errors of each model in \u03a6. Thus, while for each model \u03c6 \u2208 \u03a6 there is an associated \u01eb = \u01eb(\u03c6) \u2265 0 which indicates the aggregation error (cf. Definition 1 below), this \u01eb is unknown to the learner for each model.\nWe remark that we cannot expect to perform as well as the unknown underlying Markov model, if the model set only provides approximations. Thus, if the best approximation has error \u01eb we have to be ready to accept respective error of order \u01ebD per step, cf. the lower bound provided by Theorem 2 below.\nOverview. We start with explicating our notion of approximation in Section 2, then introduce our algorithm in Section 3, present our regret bounds in Section 4, and conclude with the proofs in Section 5."}, {"heading": "2 Preliminaries: MDP Approximations", "text": "Approximations. Before we give the precise notion of approximation we are going to use, first note that in our setting the transition probabilities p(h\u2032|h, a) for any two histories h, h\u2032 \u2208 H and an action a are well-defined. Then given an arbitrary model \u03c6 and a state s\u2032 \u2208 S\u03c6, we can define the aggregated transition probabilities pagg(s\u2032|h, a) := \u2211h\u2032:\u03c6(h\u2032)=s\u2032 p(h\u2032|h, a). Note that the true transition probabilities under \u03c6\u25e6 are then given by p(s\u2032|s, a) := pagg(s\u2032|h, a) for s = \u03c6\u25e6(h) and s\u2032 \u2208 S\u25e6.\nDefinition 1. A model \u03c6 is said to be an \u01eb-approximation of the true model \u03c6\u25e6 if: (i) for all histories h, h\u2032 with \u03c6(h) = \u03c6(h\u2032) and all actions a\n\u2223\u2223r(\u03c6\u25e6(h), a)\u2212 r(\u03c6\u25e6(h\u2032), a) \u2223\u2223 < \u01eb, and \u2225\u2225p(\u00b7|\u03c6\u25e6(h), a)\u2212 p(\u00b7|\u03c6\u25e6(h\u2032), a) \u2225\u2225 1 < \u01eb2 , (2)\nand (ii) there is a surjective mapping \u03b1 : S\u25e6 \u2192 S\u03c6 such that for all histories h and all actions a it holds that\n\u2211\ns\u0307\u2032\u2208S\u03c6\n\u2223\u2223\u2223pagg(s\u0307\u2032|h, a)\u2212 \u2211\ns\u2032\u2208S\u25e6:\u03b1(s\u2032)=s\u0307\u2032 pagg(s\u2032|h, a)\n\u2223\u2223\u2223 < \u01eb2 . (3)\nIntuitively, condition (2) assures that the approximate model aggregates only histories that are mapped to similar states under the true model. Complementary, condition (3) guarantees that the state space under the approximate model\nresembles the true state space.1 Note that any model will be an \u01eb-approximation of the underlying true model \u03c6\u25e6 for sufficiently large \u01eb.\nA particular natural case are approximation models \u03c6 which also satisfy\n\u2200h, h\u2032 \u2208 H : \u03c6\u25e6(h) = \u03c6\u25e6(h\u2032) =\u21d2 \u03c6(h) = \u03c6(h\u2032).\nThat is, intuitively, states in S\u25e6 are aggregated to meta-states in S\u03c6, and (3) holds trivially.\nWe may carry over our definition of \u01eb-approximation to MDPs. This will turn out useful, since each approximate model can be interpreted as an MDP approximation, cf. Section 5.1 below.\nDefinition 2. An MDP M\u0304 = (S\u0304,A, r\u0304, p\u0304) is an \u01eb-approximation of another MDP M = (S,A, r, p) if there is a surjective function \u03b1 : S \u2192 S\u0304 such that for all s in S:\n\u2223\u2223r\u0304(\u03b1(s), a) \u2212 r(s, a) \u2223\u2223 < \u01eb, and \u2211\ns\u0307\u2032\u2208S\u0304\n\u2223\u2223\u2223p\u0304(s\u0307\u2032|\u03b1(s), a)\u2212 \u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s, a) \u2223\u2223\u2223 < \u01eb. (4)\nError Bounds for \u01eb-Approximations. The following is an error bound on the error made by an \u01eb-approximation. It generalizes bounds of [9] from ergodic to communicating MDPs. For a proof see Appendix A.\nTheorem 1. Let M be a communicating MDP and M\u0304 be an \u01eb-approximation of M . Then \u2223\u2223\u03c1\u2217(M)\u2212 \u03c1\u2217(M\u0304) \u2223\u2223 \u2264 \u01eb (D(M) + 1).\nThe following is a matching lower bound on the error by aggregation. This is an improvement over the results in [9], which only showed that the error approaches 1 when the diameter goes to infinity.\nTheorem 2. For each \u01eb > 0 and each 2 < D < 4\u01eb there are MDPs M , M\u0304 such that M\u0304 is an \u01eb-approximation of M , M has diameter D(M) = D, and\n|\u03c1\u2217(M)\u2212 \u03c1\u2217(M\u0304)| > 156\u01ebD(M).\nProof. Consider the MDP M shown in Figure 1 (left), where the (deterministic) reward in states s0, s \u2032 0 is 0 and 1 in state s1. We assume that 0 < \u03b5 := \u01eb 2 < \u03b4 := 2 D . Then the diameter D(M) is the expected transition time from s\u20320 to s0 and equal to 2\u03b4 = D. Aggregating states s0, s \u2032 0 gives the MDP M\u0304 on the right hand side of Figure 1. Obviously, M\u0304 is an \u01eb-approximation of M . It is straightforward to check that the stationary distribution \u00b5 (of the only policy) in M is (\u00b5(s0), \u00b5(s \u2032 0), \u00b5(s1)) = ( \u03b4 3\u03b5+4\u03b4 , \u03b5+\u03b4 3\u03b5+4\u03b4 , 2\u03b5+2\u03b4 3\u03b5+4\u03b4 ) , while the stationary distribution in M\u0304 is (12 , 1 2 ). Thus, the difference in average reward is\n|\u03c1\u2217(M)\u2212 \u03c1\u2217(M\u0304)| = 2\u03b5+2\u03b43\u03b5+4\u03b4 \u2212 12 = \u03b52(3\u03b5+4\u03b4) > \u03b514\u03b4 = 156\u01ebD(M). \u2293\u2294 1 The allowed error in the conditions for the transition probabilities is chosen to be \u01eb\n2\nso that the total error with respect to the transition probabilities is \u01eb. This matches the respective condition for MDP approximations in Definition 2, cf. also Section 5.1.\nTheorems 1 and 2 compare the optimal policies of two different MDPs, however it is straightforward to see from the proofs that the same error bounds hold when comparing on some MDP M the optimal average reward \u03c1\u2217(M) to the average reward when applying the optimal policy of an \u01eb-approximation M\u0304 of M . Thus, when we approximate an MDP M by an \u01eb-approximation M\u0304 , the respective error of the optimal policy of M\u0304 on M can be of order \u01ebD(M) as well. Hence, we cannot expect to perform below this error if we only have an \u01eb-approximation of the true model at our disposal."}, {"heading": "3 Algorithm", "text": "The OAMS algorithm (shown in detail as Algorithm 1) we propose for the setting introduced in Section 1 is a generalization of the OMS algorithm of [7]. Application of the original OMS algorithm to our setting would not work, since OMS compares the collected rewards of each model to the reward it would receive if the model were Markov. Models not giving sufficiently high reward are identified as nonMarkov and rejected. In our case, there may be no Markov model in the set of given models \u03a6. Thus, the main difference to OMS is that OAMS for each model estimates and takes into account the possible approximation error with respect to a closest Markov model.\nOAMS proceeds in episodes k = 1, 2, . . ., each consisting of several runs j = 1, 2, . . .. In each run j of some episode k, starting at time t = tkj , OAMS chooses a policy \u03c0kj applying the optimism in face of uncertainty principle twice.\nPlausible models. First, OAMS considers for each model \u03c6 \u2208 \u03a6 a set of plausible MDPs Mt,\u03c6 defined to contain all MDPs with state space S\u03c6 and with rewards r+ and transition probabilities p+ satisfying\n\u2223\u2223r+(s, a)\u2212 r\u0302t(s, a) \u2223\u2223 \u2264 \u01eb\u0303(\u03c6) +\n\u221a log(48S\u03c6At\n3/\u03b4) 2Nt(s,a) , (5)\n\u2225\u2225p+(\u00b7|s, a)\u2212 p\u0302t(\u00b7|s, a) \u2225\u2225 1 \u2264 \u01eb\u0303(\u03c6) +\n\u221a 2S\u03c6 log(48S\u03c6At\n3/\u03b4) Nt(s,a) , (6)\nwhere \u01eb\u0303(\u03c6) is the estimate for the approximation error of model \u03c6 (cf. below), p\u0302t(\u00b7|s, a) and r\u0302t(s, a) are respectively the empirical state-transition probabilities\nand the mean reward at time t for taking action a in state s \u2208 S\u03c6, S\u03c6 := |S\u03c6| denotes the number of states under model \u03c6, A := |A| is the number of actions, and Nt(s, a) is the number of times action a has been chosen in state s up to time t. (If a hasn\u2019t been chosen in s so far, we set Nt(s, a) to 1.) The inequalities (5) and (6) are obviously inspired by Chernov bounds that would hold with high probability in case the respective model \u03c6 is Markov, cf. also Lemma 1 below.\nOptimistic MDP for each model \u03c6. In line 4, the algorithm computes for each model \u03c6 a so-called optimistic MDP M+t (\u03c6) \u2208 Mt,\u03c6 and an associated optimal policy \u03c0+t,\u03c6 on M + t (\u03c6) such that the average reward \u03c1(M + t (\u03c6), \u03c0 + t,\u03c6) is maximized. This can be done by extended value iteration (EVI) [5]. Indeed, if r+t (s, a) and p + t (s\n\u2032|s, a) denote the optimistic rewards and transition probabilities of M+t (\u03c6), then EVI computes optimistic state values u + t,\u03c6 = (u + t,\u03c6(s))s \u2208 RS\u03c6 such that (cf. [5])\n\u03c1\u0302+t (\u03c6) := min s\u2208S\u03c6\n{ r+t (s, \u03c0 + t,\u03c6(s)) + \u2211\ns\u2032\np+t (s \u2032|s, \u03c0+t,\u03c6(s))u+t,\u03c6(s\u2032)\u2212 u+t,\u03c6(s)\n} (7)\nis an approximation of \u03c1\u2217(M+t (\u03c6)), that is,\n\u03c1\u0302+t (\u03c6) \u2265 \u03c1\u2217(M+t (\u03c6)) \u2212 2/ \u221a t. (8)\nOptimistic model selection. In line 5, OAMS chooses a model \u03c6kj \u2208 \u03a6 with corresponding MDP Mkj = M + t (\u03c6kj) and policy \u03c0kj := \u03c0 + t,\u03c6kj that maximizes the average reward penalized by the term pen(\u03c6, t) defined as\npen(\u03c6, t) := 2\u2212j/2 (( \u03bb(u+t,\u03c6) \u221a 2S\u03c6 + 3\u221a 2 )\u221a S\u03c6A log ( 48S\u03c6At3 \u03b4 ) (9)\n+\u03bb(u+t,\u03c6)\n\u221a 2 log(24t 2 \u03b4 ) ) + 2\u2212j\u03bb(u+t,\u03c6) + \u01eb\u0303(\u03c6) ( \u03bb(u+t,\u03c6) + 3 ) ,\nwhere we define \u03bb(u+t,\u03c6) := maxs\u2208S\u03c6 u + t,\u03c6(s)\u2212mins\u2208S\u03c6 u+t,\u03c6(s) to be the empirical value span of the optimistic MDP M+t (\u03c6). Intuitively, the penalization term is an upper bound on the per-step regret of the model \u03c6 in the run to follow in case \u03c6 is chosen, cf. eq. (35) in the proof of the main theorem. Similar to the REGAL algorithm of [1] this shall prefer simpler models (i.e., models having smaller state space and smaller value span) to more complex ones.\nTermination of runs and episodes. The chosen policy \u03c0kj is then executed until either (i) run j reaches the maximal length of 2j steps, (ii) episode k terminates when the number of visits in some state has been doubled (line 12), or (iii) the executed policy \u03c0kj does not give sufficiently high rewards (line 9). That is, at any time t in run j of episode k it is checked whether the total reward in the current run is at least \u2113kj\u03c1kj \u2212 lobkj(t), where \u2113kj := t \u2212 tkj + 1 is the (current) length of run j in episode k, and lobkj(t) is defined as\nlobkj(t) := ( \u03bb+kj \u221a 2Skj + 3\u221a 2 )\u2211\ns\u2208Skj\n\u2211\na\u2208A\n\u221a vkj(s, a) log ( 48SkjAt3kj \u03b4 )\n+ \u03bb+kj \u221a 2\u2113kj log ( 24t2kj \u03b4 ) + \u03bb+kj + \u01eb\u0303 ( \u03c6kj)\u2113kj(\u03bb + kj + 3 ) , (10)\nAlgorithm 1 Optimal Approximate Model Selection (OAMS)\ninput set of models \u03a6, confidence parameter \u03b4 \u2208 (0, 1), precision parameter \u01eb0 \u2208 (0, 1) 1: Let t be the current time step, and set \u01eb\u0303(\u03c6) := \u01eb0 for all \u03c6 \u2208 \u03a6. 2: for episodes k = 1, 2, . . . do 3: for runs j = 1, 2, . . . do 4: \u2200\u03c6 \u2208 \u03a6, use EVI to compute an optimistic MDP M+t (\u03c6) in Mt,\u03c6 (the set\nof plausible MDPs defined via the confidence intervals (6) and (5) for the estimates so far), a (near-)optimal policy \u03c0+t,\u03c6 on M + t (\u03c6) with approximate average reward \u03c1\u0302+t (\u03c6), and the empirical value span \u03bb(u +\nt,\u03c6). 5: Choose model \u03c6kj \u2208 \u03a6 such that\n\u03c6kj = argmax \u03c6\u2208\u03a6\n{ \u03c1\u0302 + t (\u03c6)\u2212 pen(\u03c6, t) } . (11)\n6: Set tkj := t, \u03c1kj := \u03c1\u0302 + t (\u03c6kj), \u03c0kj := \u03c0 +\nt,\u03c6kj , and Skj := S\u03c6kj .\n7: for 2j steps do 8: Choose action at := \u03c0kj(st), get reward rt, observe next state st+1 \u2208 Skj . 9: if the total reward collected so far in the current run is less than\n(t\u2212 tkj + 1)\u03c1kj \u2212 lobkj(t), (12)\nthen\n10: \u01eb\u0303(\u03c6kj) := 2\u01eb\u0303(\u03c6kj) 11: Terminate current episode. 12: else if \u2211j j\u2032=1\nvkj\u2032(st, at) = Ntk1(st, at) then 13: Terminate current episode. 14: end if 15: end for 16: end for 17: end for\nwhere \u03bb+kj := \u03bb(u + tkj ,\u03c6kj\n), Skj := S\u03c6kj , and vkj(s, a) are the (current) state-action counts of run j in episode k. That way, OAMS assumes each model to be Markov, as long as it performs well. We will see that lobkj(t) can be upper bounded by \u2113kjpen(\u03c6kj , tkj), cf. eq. (35) below.\nGuessing the approximation error. The algorithm tries to guess for each model \u03c6 the correct approximation error \u01eb(\u03c6). In the beginning the guessed value \u01eb\u0303(\u03c6) for each model \u03c6 \u2208 \u03a6 is set to the precision parameter \u01eb0, the best possible precision we aim for. Whenever the reward test fails for a particular model \u03c6, it is likely that \u01eb\u0303(\u03c6) is too small and it is therefore doubled (line 10)."}, {"heading": "4 Regret Bounds", "text": "The following upper bound on the regret of OAMS is the main result of this paper.\nTheorem 3. There are c1, c2, c3 \u2208 R such that in each learning problem where the learner is given access to a set of models \u03a6 not necessarily containing the\ntrue model \u03c6\u25e6, the regret \u2206(\u03c6\u25e6, T ) of OAMS (with parameters \u03b4, \u01eb0) with respect to the true model \u03c6\u25e6 after any T \u2265 SA steps is upper bounded by\nc1 \u00b7DSA(log( 1\u01eb0 ) logT + log 2 T ) + c2 \u00b7Dmax{\u01eb0, \u01eb(\u03c6)}T\n+c3 \u00b7 ( DS\u03c6 \u221a SA log3/2(T\u03b4 ) + \u221a |\u03a6| log( 1\u01eb0 ) logT )\u221a T\nwith probability at least 1\u2212 \u03b4, where \u03c6 \u2208 \u03a6 is an \u01eb(\u03c6)-approximation of the true underlying Markov model \u03c6\u25e6, D := D(\u03c6\u25e6), and S := \u2211 \u03c6\u2208\u03a6 S\u03c6.\nAs already mentioned, by Theorem 2 the second term in the regret bound is unavoidable when only considering models in \u03a6. Note that Theorem 3 holds for all models \u03c6 \u2208 \u03a6. For the best possible bound there is a payoff between the size S\u03c6 of the approximate model and its precision \u01eb(\u03c6).\nWhen the learner knows that \u03a6 contains a Markov model \u03c6\u25e6, the original OMS algorithm of [7] can be employed. In case when the total number S = \u2211 \u03c6 S\u03c6 of states over all models is large, i.e., S > D2|\u03a6|S\u25e6, we can improve on the state space dependence of the regret bound given in [7] as follows. The proof (found in Appendix H) is a simple modification of the analysis in [7] that exploits that by (11) the selected models cannot have arbitrarily large state space. Theorem 4. If \u03a6 contains a Markov model \u03c6\u25e6, with probability at least 1 \u2212 \u03b4 the regret of OMS is bounded by O\u0303(D2S\u25e63/2A \u221a |\u03a6|T ).\nDiscussion. Unfortunately, while the bound in Theorem 3 is optimal with respect to the dependence on the horizon T , the improvement on the state space dependence that we could achieve in Theorem 4 for OMS is not as straightforward for OAMS and remains an open question just as the optimality of the bound with respect to the other appearing parameters. We note that this is still an open question even for learning in MDPs (without additionally selecting the state representation) as well, cf. [5].\nAnother direction for future work is the extension to the case when the underlying true MDP has continuous state space. In this setting, the models have the natural interpretation of being discretizations of the original state space. This could also give improvements over current regret bounds for continuous reinforcement learning as given in [10]. Of course, the most challenging goal remains to generate suitable state representation models algorithmically instead of assuming them to be given, cf. [3]. However, at the moment it is not even clear how to deal with the case when an infinite set of models is given."}, {"heading": "5 Proof of Theorem 3", "text": "The proof is divided into three parts and follows the lines of [7], now taking into account the necessary modifications to deal with the approximation error. First, in Section 5.1 we deal with the error of \u01eb-approximations. Then in Section 5.2, we show that all state-representation models \u03c6 which are an \u01eb(\u03c6)-approximation of a Markov model pass the test in (12) on the rewards collected so far with high probability, provided that the estimate \u01eb\u0303(\u03c6) \u2265 \u01eb(\u03c6). Finally, in Section 5.3 we use this result to derive the regret bound of Theorem 3."}, {"heading": "5.1 Error Bounds for \u01eb-Approximate Models", "text": "We start with some observations about the empirical rewards and transition probabilities our algorithm calculates and employs for each model \u03c6. While the estimated rewards r\u0302 and transition probabilities p\u0302 used by the algorithm do in general not correspond to some underlying true values, the expectation values of r\u0302 and p\u0302 are still well-defined, given the history h \u2208 H so far. Indeed, consider some h \u2208 H with \u03c6(h) = s\u0307 \u2208 S\u03c6, \u03c6\u25e6(h) = s \u2208 S\u25e6, and an action a, and assume that the estimates r\u0302(s\u0307, a) and p\u0302(\u00b7|s\u0307, a) are calculated from samples when action a was chosen after histories h1, h2, . . . , hn \u2208 H that are mapped to the same state s\u0307 by \u03c6. (In the following, we will denote the states of an approximation \u03c6 by variables with dot, such as s\u0307, s\u0307\u2032, etc., and states in the state space S\u25e6 of the true Markov model \u03c6\u25e6 without a dot, such as s, s\u2032, etc.) Since rewards and transition probabilities are well-defined under \u03c6\u25e6, we have\nE[r\u0302(s\u0307, a)] = 1n\nn\u2211\ni=1\nr(\u03c6\u25e6(hi), a), and E[p\u0302(s\u0307 \u2032|s\u0307, a)] = 1n\nn\u2211\ni=1\n\u2211\nh\u2032:\u03c6(h\u2032)=s\u0307\u2032\np(h\u2032|hi, a). (13)\nSince \u03c6 maps the histories h, h1, . . . , hn to the same state s\u0307 \u2208 S\u03c6, the rewards and transition probabilities in the states \u03c6\u25e6(h), \u03c6\u25e6(h1), . . . , \u03c6\n\u25e6(hn) of the true underlying MDP are \u01eb-close, cf. (2). It follows that for s = \u03c6\u25e6(h) and s\u0307 = \u03c6(h)\n\u2223\u2223\u2223E[r\u0302(s\u0307, a)]\u2212 r(s, a) \u2223\u2223\u2223 = \u2223\u2223\u2223 1n n\u2211\ni=1\n( r(\u03c6\u25e6(hi), a)\u2212 r(\u03c6\u25e6(h), a) )\u2223\u2223\u2223 < \u01eb(\u03c6). (14)\nFor the transition probabilities we have by (3) for i = 1, . . . , n \u2211\ns\u0307\u2032\u2208S\u03c6\n\u2223\u2223\u2223pagg(s\u0307\u2032|hi, a)\u2212 \u2211\ns\u2032\u2208S\u25e6:\u03b1(s\u2032)=s\u0307\u2032 pagg(s\u2032|hi, a)\n\u2223\u2223\u2223 < \u01eb(\u03c6)2 . (15)\nFurther, all hi as well as h are mapped to s\u0307 by \u03c6 so that according to (2) and recalling that s = \u03c6\u25e6(h) we have for i = 1, . . . , n\n\u2211\ns\u0307\u2032\u2208S\u03c6\n\u2223\u2223\u2223 \u2211\ns\u2032\u2208S\u25e6:\u03b1(s\u2032)=s\u0307\u2032 pagg(s\u2032|hi, a)\u2212\n\u2211\ns\u2032\u2208S\u25e6:\u03b1(s\u2032)=s\u0307\u2032 p(s\u2032|s, a)\n\u2223\u2223\u2223\n\u2264 \u2211\ns\u2032\u2208S\u25e6\n\u2223\u2223pagg(s\u2032|hi, a)\u2212 p(s\u2032|s, a) \u2223\u2223 < \u01eb(\u03c6)2 . (16)\nBy (15) and (16) for i = 1, . . . , n \u2211\ns\u0307\u2032\u2208S\u03c6\n\u2223\u2223\u2223pagg(s\u0307\u2032|hi, a)\u2212 \u2211\ns\u2032\u2208S\u25e6:\u03b1(s\u2032)=s\u0307\u2032 p(s\u2032|s, a)\n\u2223\u2223\u2223 < \u01eb(\u03c6), (17)\nso that from (13) and (17) we can finally bound \u2211\ns\u0307\u2032\u2208S\u03c6\n\u2223\u2223\u2223E[p\u0302(s\u0307\u2032|s\u0307, a)]\u2212 \u2211\ns\u2032\u2208S\u25e6:\u03b1(s\u2032)=s\u0307\u2032 p(s\u2032|s, a)\n\u2223\u2223\u2223\n\u2264 1n n\u2211\ni=1\n\u2211\ns\u0307\u2032\u2208S\u03c6\n\u2223\u2223\u2223pagg(s\u0307\u2032|hi, a)\u2212 \u2211\ns\u2032\u2208S\u25e6:\u03b1(s\u2032)=s\u0307\u2032 p(s\u2032|s, a)\n\u2223\u2223\u2223 < \u01eb(\u03c6). (18)\nThus, according to (14) and (18) the \u01eb-approximate model \u03c6 gives rise to an MDP M\u0304 on S\u03c6 with rewards r\u0304(s\u0307, a) := E[r\u0302(s\u0307, a)] and transition probabilities p\u0304(s\u0307\u2032|s\u0307, a) := E[p\u0302(s\u0307\u2032|s\u0307, a)] that is an \u01eb-approximation of the true MDP M(\u03c6\u25e6). Note that M\u0304 actually depends on the history so far.\nThe following lemma gives some basic confidence intervals for the estimated rewards and transition probabilities. For a proof sketch see Appendix B.\nLemma 1. Let t be an arbitrary time step and \u03c6 \u2208 \u03a6 be the model employed at step t. Then the estimated rewards r\u0302 and transition probabilities p\u0302 satisfy for all s\u0307, s\u0307\u2032 \u2208 S\u03c6 and all a \u2208 A\nr\u0302(s\u0307, a)\u2212 E[r\u0302(s\u0307, a)] \u2264 \u221a log(48S\u03c6At 3/\u03b4)\nNt(s\u0307,a) ,\n\u2225\u2225\u2225p\u0302(\u00b7|s\u0307, a)\u2212 E[p\u0302(\u00b7|s\u0307, a)] \u2225\u2225\u2225 1 \u2264\n\u221a 2S\u03c6 log(48S\u03c6At\n3/\u03b4) Nt(s\u0307,a) ,\neach with probability at least 1\u2212 \u03b424t2 .\nThe following is a consequence of Theorem 1, see Appendix C for a detailed proof.\nLemma 2. Let \u03c6\u25e6 be the underlying true Markov model leading to MDP M = (S\u25e6,A, r, p), and \u03c6 be an \u01eb-approximation of \u03c6\u25e6. Assume that the confidence intervals given in Lemma 1 hold at step t for all states s\u0307, s\u0307\u2032 \u2208 S\u03c6 and all actions a. Then the optimistic average reward \u03c1\u0302+t (\u03c6) defined in (7) satisfies\n\u03c1\u0302+t (\u03c6) \u2265 \u03c1\u2217(M)\u2212 \u01eb(D(M) + 1)\u2212 2\u221at .\n5.2 Approximate Markov models pass the test in (12)\nAssume that the model \u03c6kj \u2208 \u03a6 employed in run j of episode k is an \u01ebkj := \u01eb(\u03c6kj)-approximation of the true Markov model. We are going to show that \u03c6kj will pass the test (12) on the collected rewards with high probability at any step t, provided that \u01eb\u0303kj := \u01eb\u0303(\u03c6kj) \u2265 \u01ebkj .\nLemma 3. For each step t in some run j of some episode k, given that tkj = t \u2032 the chosen model \u03c6kj passes the test in (12) at step t with probability at least 1\u2212 \u03b46t\u20322 whenever \u01eb\u0303kj(\u03c6kj) \u2265 \u01eb(\u03c6kj).\nProof. In the following, s\u0307\u03c4 := \u03c6kj(h\u03c4 ) and s\u03c4 := \u03c6 \u25e6(h\u03c4 ) are the states at time step \u03c4 under model \u03c6kj and the true Markov model \u03c6 \u25e6, respectively.\nInitial decomposition. First note that at time t when the test is performed, we have \u2211 s\u0307\u2208Skj \u2211 a\u2208A vkj(s\u0307, a) = \u2113kj = t\u2212 t\u2032 + 1, so that\n\u2113kj\u03c1kj \u2212 t\u2211\n\u03c4=t\u2032\nr\u03c4 = \u2211\ns\u0307\u2208Skj\n\u2211 a\u2208A vkj(s\u0307, a) ( \u03c1kj \u2212 r\u0302t\u2032:t(s\u0307, a) ) ,\nwhere r\u0302t\u2032:t(s\u0307, a) is the empirical average reward collected for choosing a in s\u0307 from time t\u2032 to the current time t in run j of episode k.\nLet r+kj(s\u0307, a) be the rewards and p + kj(\u00b7|s\u0307, a) the transition probabilities of the\noptimistic model M+tkj (\u03c6kj). Noting that vkj(s\u0307, a) = 0 when a 6= \u03c0kj(s\u0307), we get\n\u2113kj\u03c1kj \u2212 t\u2211\n\u03c4=t\u2032\nr\u03c4 = \u2211\ns\u0307,a\nvkj(s\u0307, a) ( \u03c1\u0302+kj(\u03c6kj)\u2212 r+kj(s\u0307, a) ) (19)\n+ \u2211\ns\u0307,a\nvkj(s\u0307, a) ( r+kj(s\u0307, a)\u2212 r\u0302t\u2032:t(s\u0307, a) ) . (20)\nWe continue bounding the two terms (19) and (20) separately.\nBounding the reward term (20). Recall that r(s, a) is the mean reward for choosing a in s in the true Markov model \u03c6\u25e6. Then we have at each time step \u03c4 = t\u2032, . . . , t\nr+kj(s\u0307\u03c4 , a)\u2212 r\u0302t\u2032:t(s\u0307\u03c4 , a) = ( r+kj(s\u0307\u03c4 , a)\u2212 r\u0302t\u2032(s\u0307\u03c4 , a) )\n+ ( r\u0302t\u2032(s\u0307\u03c4 , a)\u2212 E[r\u0302t\u2032(s\u0307\u03c4 , a)] ) + ( E[r\u0302t\u2032(s\u0307\u03c4 , a)]\u2212 r(s\u03c4 , a) ) + ( r(s\u03c4 , a)\u2212 E[r\u0302t\u2032 :t(s\u0307\u03c4 , a)] ) + ( E[r\u0302t\u2032 :t(s\u0307\u03c4 , a)]\u2212 r\u0302t\u2032:t(s\u0307\u03c4 , a) )\n\u2264 \u01eb\u0303kj + 2 \u221a\nlog(48SkjAt\u20323/\u03b4) 2Nt\u2032(s\u0307,a)\n+ 2\u01ebkj + \u221a\nlog(48SkjAt\u20323/\u03b4) 2vkj(s\u0307,a) , (21)\nwhere we bounded the first term in the decomposition by (5), the second term by Lemma 1, the third and fourth according to (14), and the fifth by an equivalent to Lemma 1 for the rewards collected so far in the current run. In summary, with probability at least 1\u2212 \u03b412t\u20322 we can bound (20) as \u2211\ns\u0307,a\nvkj(s\u0307, a) ( r+kj(s\u0307, a)\u2212 r\u0302t\u2032:t(s\u0307, a) ) \u2264 3\u01eb\u0303kj\u2113kj+ 3\u221a2 \u2211\ns\u0307,a\n\u221a vkj(s\u0307, a) log (48SkjAt\u20323 \u03b4 ) , (22)\nwhere we used the assumption that \u01eb\u0303kj \u2265 \u01ebkj as well as vkj(s\u0307, a) \u2264 Nt\u2032(s\u0307, a). Bounding the bias term (19). First, notice that we can use (7) to bound \u2211\ns\u0307,a\nvkj(s\u0307, a) ( \u03c1\u0302+kj(\u03c6kj)\u2212r+kj(s\u0307, a) ) \u2264 \u2211\ns\u0307,a\nvkj(s\u0307, a) (\u2211\ns\u0307\u2032\np+kj(s\u0307 \u2032|s\u0307, a)u+kj(s\u0307\u2032)\u2212u+kj(s\u0307)\n) ,\nwhere u+kj(s\u0307) := u + tkj ,\u03c6kj (s\u0307) are the state values given by EVI. Further, since the transition probabilities p+kj(\u00b7|s\u0307, a) sum to 1, this is invariant under a translation of the vector u+kj . In particular, defining wkj(s\u0307) := u + kj(s\u0307)\u2212 12 ( mins\u0307\u2208Skj u + kj(s\u0307)+ maxs\u0307\u2208Skj u + kj(s\u0307) ) , so that \u2016wkj\u2016\u221e = \u03bb+kj/2, we can replace u+kj with wkj , and (19) can be bounded as \u2211\ns\u0307,a\nvkj(s\u0307, a) ( \u03c1\u0302+kj(\u03c6kj)\u2212 r+kj(s\u0307, a) )\n\u2264 \u2211\ns\u0307,a\nt\u2211\n\u03c4=t\u2032\n1 { (s\u0307\u03c4 , a\u03c4 ) = (s\u0307, a)\n}( \u2211\ns\u0307\u2032\u2208Skj\np+kj(s\u0307 \u2032|s\u0307\u03c4 , a)wkj(s\u0307\u2032)\u2212 wkj(s\u0307\u03c4 )\n) . (23)\nNow we decompose for each time step \u03c4 = t\u2032, . . . , t \u2211\ns\u0307\u2032\u2208Skj\np+kj(s\u0307 \u2032|s\u0307\u03c4 , a)wkj(s\u0307\u2032)\u2212 wkj(s\u0307\u03c4 ) =\n\u2211\ns\u0307\u2032\u2208Skj\n( p+kj(s\u0307 \u2032|s\u0307\u03c4 , a)\u2212 p\u0302t\u2032(s\u0307\u2032|s\u0307\u03c4 , a) ) wkj(s\u0307 \u2032) (24)\n+ \u2211\ns\u0307\u2032\u2208Skj\n( p\u0302t\u2032(s\u0307 \u2032|s\u0307\u03c4 , a)\u2212 E[p\u0302t\u2032(s\u0307\u2032|s\u0307\u03c4 , a)] ) wkj(s\u0307 \u2032) (25)\n+ \u2211\ns\u0307\u2032\u2208Skj\n( E[p\u0302t\u2032(s\u0307 \u2032|s\u0307\u03c4 , a)]\u2212 \u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s\u03c4 , a) ) wkj(s\u0307 \u2032) (26)\n+ \u2211\ns\u0307\u2032\u2208Skj\n\u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s\u03c4 , a)wkj(s\u0307\u2032)\u2212 wkj(s\u0307\u03c4 ) (27)\nand continue bounding each of these terms individually.\nBounding (24): Using \u2016wkj\u2016\u221e = \u03bb+kj/2, (24) is bounded according to (6) as \u2211\ns\u0307\u2032\u2208Skj\n( p+kj(s\u0307 \u2032|s\u0307\u03c4 , a)\u2212 p\u0302t\u2032(s\u0307\u2032|s\u0307\u03c4 , a) ) wkj(s\u0307 \u2032) \u2264 \u2225\u2225p+kj(\u00b7|s\u0307\u03c4 , a)\u2212 p\u0302t\u2032(\u00b7|s\u0307\u03c4 , a) \u2225\u2225 1 \u2016wkj\u2016\u221e\n\u2264 \u01eb\u0303kj\u03bb + kj 2 + \u03bb+ kj 2\n\u221a 2Skj log(48SkjAt\n\u20323/\u03b4) Nt\u2032(s,a) . (28)\nBounding (25): Similarly, by Lemma 1 with probability at least 1 \u2212 \u03b424t\u20322 we can bound (25) at all time steps \u03c4 as\n\u2211\ns\u0307\u2032\u2208Skj\n( p\u0302t\u2032(s\u0307 \u2032|s\u0307\u03c4 , a)\u2212 E[p\u0302t\u2032(s\u0307\u2032|s\u0307\u03c4 , a)] ) wkj(s\u0307 \u2032) \u2264 \u03bb + kj\n2\n\u221a 2Skj log(48SkjAt\n\u20323/\u03b4) Nt\u2032 (s,a) . (29)\nBounding (26): By (18) and using that \u2016wkj\u2016\u221e = \u03bb+kj/2, we can bound (26) by \u2211\ns\u0307\u2032\u2208Skj\n( E[p\u0302t\u2032(s\u0307 \u2032|s\u0307\u03c4 , a)]\u2212 \u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s\u03c4 , a) ) wkj(s\u0307 \u2032) < \u01ebkj\u03bb + kj\n2 . (30)\nBounding (27): We set w\u2032(s) := wkj(\u03b1(s)) for s \u2208 S\u25e6 and rewrite (27) as \u2211\ns\u0307\u2032\u2208Skj\n\u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s\u03c4 , a)wkj(s\u0307\u2032)\u2212 wkj(s\u0307\u03c4 ) = \u2211\ns\u2032\u2208S\u25e6 p(s\u2032|s\u03c4 , a)w\u2032(s\u2032)\u2212 w\u2032(s\u03c4 ).(31)\nSumming this term over all steps \u03c4 = t\u2032, . . . , t, we can rewrite the sum as a martingale difference sequence, so that Azuma-Hoeffding\u2019s inequality (e.g., Lemma 10 of [5]) yields that with probability at least 1\u2212 \u03b424t\u20323\nt\u2211\n\u03c4=t\u2032\n\u2211\ns\u2032\u2208S\u25e6 p(s\u2032|s\u03c4 , a)w\u2032(s\u2032)\u2212 w\u2032(s\u03c4 ) =\nt\u2211\n\u03c4=t\u2032\n(\u2211\ns\u2032\np(s\u2032|s\u03c4 , a)w\u2032(s\u2032)\u2212 w\u2032(s\u03c4+1) )\n+w\u2032(st+1)\u2212 w\u2032(st\u2032) \u2264 \u03bb+kj \u221a 2\u2113kj log( 24t\u20323 \u03b4 ) + \u03bb + kj , (32)\nsince the sequence X\u03c4 := \u2211 s\u2032 p(s \u2032|s\u03c4 , a)w\u2032(s\u2032)\u2212w\u2032(s\u03c4+1) is a martingale difference sequence with |Xt| \u2264 \u03bb+kj . Wrap-up. Summing over the steps \u03c4 = t\u2032, . . . , t, we get from (23), (27), (28), (29), (30), (31), and (32) that with probability at least 1\u2212 \u03b412t\u20322 \u2211\ns\u0307,a\nvkj(s\u0307, a) ( \u03c1\u0302+kj(\u03c6kj)\u2212 r+kj(s\u0307, a) ) \u2264 \u01eb\u0303kj\u03bb+kj\u2113kj\n+\u03bb+kj\n\u2211\ns\u0307,a\n\u221a 2vkj(s\u0307, a)Skj log ( 48SkjAt\u20323 \u03b4 ) + \u03bb+kj \u221a 2\u2113kj log ( 24t\u20322 \u03b4 ) + \u03bb+kj , (33)\nusing that vkj(s\u0307, a) \u2264 Nt\u2032(s\u0307, a) and the assumption that \u01ebkj \u2264 \u01eb\u0303kj . Combining (20), (22), and (33) gives the claimed lemma. \u2293\u2294\nSumming Lemma 3 over all episodes gives the following lemma, for a detailed proof see Appendix D.\nLemma 4. With probability at least 1 \u2212 \u03b4, for all runs j of all episodes k the chosen model \u03c6kj passes all tests, provided that \u01eb\u0303kj(\u03c6kj) \u2265 \u01eb(\u03c6kj)."}, {"heading": "5.3 Preliminaries for the proof of Theorem 3", "text": "We start with some auxiliary results for the proof of Theorem 3. Lemma 5 bounds the bias span of the optimistic policy, Lemma 6 deals with the estimated precision of \u03c6kj , and Lemma 7 provides a bound for the number of episodes. For proofs see Appendix E, F, and G.\nLemma 5. Assume that the confidence intervals given in Lemma 1 hold at some step t for all states s\u0307 \u2208 S\u03c6 and all actions a. Then for each \u03c6, the set of plausible MDPs Mt,\u03c6 contains an MDP M\u0303 with diameter D(M\u0303) upper bounded by the true diameter D, provided that \u01eb\u0303(\u03c6) \u2265 \u01eb(\u03c6). Consequently, the respective bias span \u03bb(u+t,\u03c6) is bounded by D as well.\nLemma 6. If all chosen models \u03c6kj pass all tests in run j of episode k whenever \u01eb\u0303(\u03c6kj) \u2265 \u01eb(\u03c6kj), then \u01eb\u0303(\u03c6) \u2264 max{\u01eb0, 2\u01eb(\u03c6)} always holds for all models \u03c6.\nLemma 7. Assume that all chosen models \u03c6kj pass all tests in run j of episode k whenever \u01eb\u0303(\u03c6kj) \u2265 \u01eb(\u03c6kj). Then the number of episodes KT after any T \u2265 SA steps is upper bounded as KT \u2264 SA log2 ( 2T SA ) + \u2211 \u03c6:\u01eb(\u03c6)>\u01eb0 log2 ( \u01eb(\u03c6) \u01eb0 ) ."}, {"heading": "5.4 Bounding the regret (Proof of Theorem 3)", "text": "Now we can finally turn to showing the regret bound of Theorem 3. We will assume that all chosen models \u03c6kj pass all tests in run j of episode k whenever \u01eb\u0303(\u03c6kj) \u2265 \u01eb(\u03c6kj). According to Lemma 4 this holds with probability at least 1\u2212\u03b4.\nLet \u03c6kj \u2208 \u03a6 be the model that has been chosen at time tkj , and consider the last but one step t of run j in episode k. The regret \u2206kj of run j in episode k with respect to \u03c1\u2217 := \u03c1\u2217(\u03c6\u25e6) is bounded by\n\u2206kj := (\u2113kj + 1)\u03c1 \u2217 \u2212 \u2211t+1 \u03c4=tkj r\u03c4 \u2264 \u2113kj ( \u03c1\u2217 \u2212 \u03c1kj ) + \u03c1\u2217 + \u2113kj\u03c1kj \u2212 \u2211t \u03c4=tkj r\u03c4 ,\nwhere as before \u2113kj := t\u2212 tkj + 1 denotes the length of run j in episode k up to the considered step t. By assumption the test (12) on the collected rewards has been passed at step t, so that\n\u2206kj \u2264 \u2113kj ( \u03c1\u2217 \u2212 \u03c1kj ) + \u03c1\u2217 + lobkj(t), (34)\nand we continue bounding the terms of lobkj(t).\nBounding the regret with the penalization term. Since we have vkj(s\u0307, a) \u2264 Ntk1(s\u0307, a) for all s\u0307 \u2208 Skj , a \u2208 A and also \u2211 s\u0307,a vkj(s\u0307, a) = \u2113kj \u2264 2j , by Cauchy-Schwarz inequality \u2211\ns\u0307,a\n\u221a vkj(s\u0307, a) \u2264 2j/2 \u221a SkjA. Applying this to\nthe definition (10) of lobkj , we obtain from (34) and by the definition (9) of the penalty term that\n\u2206kj \u2264 \u2113kj ( \u03c1\u2217 \u2212 \u03c1kj ) + \u03c1\u2217 + 2j/2 ( \u03bb+kj \u221a 2Skj + 3\u221a 2 )\u221a SkjA log ( 48SkjAt3kj \u03b4 )\n+ \u03bb+kj \u221a 2\u2113kj log ( 24t2kj \u03b4 ) + \u03bb+kj + \u01eb\u0303 ( \u03c6kj)\u2113kj(\u03bb + kj + 3 )\n\u2264 \u2113kj ( \u03c1\u2217 \u2212 \u03c1kj ) + \u03c1\u2217 + 2jpen(\u03c6kj , tkj). (35)\nThe key step. Now, by definition of the algorithm and Lemma 2, for any approximate model \u03c6 we have\n\u03c1kj \u2212 pen(\u03c6kj , tkj) \u2265 \u03c1\u0302+tkj (\u03c6) \u2212 pen(\u03c6, tkj) (36)\n\u2265 \u03c1\u2217 \u2212 (D + 1)\u01eb(\u03c6)\u2212 pen(\u03c6, tkj)\u2212 2t\u22121/2kj ,\nor equivalently \u03c1\u2217 \u2212 \u03c1kj + pen(\u03c6kj , tkj) \u2264 pen(\u03c6, tkj) + (D + 1)\u01eb(\u03c6) + 2t\u22121/2kj . Multiplying this inequality with 2j and noting that \u2113kj \u2264 2j then gives\n\u2113kj ( \u03c1\u2217 \u2212 \u03c1kj ) + 2jpen(\u03c6kj , tkj) \u2264 2jpen(\u03c6, tkj) + 2j(D + 1)\u01eb(\u03c6) + 2j+1t\u22121/2kj .\nCombining this with (35), we get by application of Lemma 5, i.e., \u03bb(u+tkj ,\u03c6) \u2264 D, and the definition of the penalty term (9) that\n\u2206kj \u2264 \u03c1\u2217 + 2j/2 (( D \u221a 2S\u03c6 +\n3\u221a 2\n)\u221a S\u03c6A log ( 48S\u03c6At3kj \u03b4 ) +D \u221a 2 log( 24t2kj \u03b4 ) )\n+D + 2j \u01eb\u0303(\u03c6) ( D + 3 ) + 2j(D + 1)\u01eb(\u03c6) + 2j+1t\n\u22121/2 kj .\nBy Lemma 6 and using that 2tkj \u2265 2j (so that 2j+1t\u22121/2kj \u2264 2 \u221a 2 \u00b7 2j/2) we get\n\u2206kj \u2264 \u03c1\u2217 + 2j/2 (( D \u221a 2S\u03c6 +\n3\u221a 2\n)\u221a S\u03c6A log ( 48S\u03c6At3kj \u03b4 ) +D \u221a 2 log( 24t2kj \u03b4 ) )\n+D + 32 \u00b7 2 j max{\u01eb0, 2\u01eb(\u03c6)} ( D + 73 ) + 2 \u221a 2 \u00b7 2j/2. (37)\nSumming over runs and episodes. Let Jk be the total number of runs in episode k, and let KT be the total number of episodes up to time T . Noting that tkj \u2264 T and summing (37) over all runs and episodes gives\n\u2206(\u03c6\u25e6, T ) = KT\u2211\nk=1\nJk\u2211\nj=1\n\u2206kj \u2264 ( \u03c1\u2217 +D )KT\u2211\nk=1\nJk + 3 2 max{\u01eb0, 2\u01eb(\u03c6)} ( D + 73 ) KT\u2211\nk=1\nJk\u2211\nj=1\n2j\n+ (( D \u221a 2S\u03c6 +\n3\u221a 2\n)\u221a S\u03c6A log ( 48S\u03c6AT 3 \u03b4 ) +D \u221a 2 log(24T 2 \u03b4 ) + 2 \u221a 2 )KT\u2211\nk=1\nJk\u2211\nj=1\n2j/2.\nAs shown in Section 5.2 of [7], \u2211 k Jk \u2264 KT log2(2T/KT ), \u2211 k \u2211 j 2\nj \u2264 2(T + KT ) and \u2211 k \u2211 j 2 j/2 \u2264 \u221a 2KT log2(2T/KT )(T +KT ), and we may conclude the proof applying Lemma 7 and some minor simplifications. \u2293\u2294\nAcknowledgments. This research was funded by the Austrian Science Fund (FWF): P 26219-N15, the European Community\u2019s FP7 Program under grant agreements n\u25e6 270327 (CompLACS) and 306638 (SUPREL), the Technion, the Ministry of Higher Education and Research of France, Nord-Pas-de-Calais Regional Council, and FEDER (Contrat de Projets Etat Region CPER 2007-2013)."}, {"heading": "1. Bartlett, P.L., Tewari, A.: REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In: UAI 2009, Proc. 25th Conf. on Uncertainty in Artificial Intelligence, pp. 25\u201342. AUAI Press (2009)", "text": ""}, {"heading": "2. Hallak, A., Castro, D.D., Mannor, S.: Model selection in Markovian processes. In: 19th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining, KDD", "text": "2013, pp. 374\u2013382. ACM (2013) 3. Hutter, M.: Feature Reinforcement Learning: Part I: Unstructured MDPs. J. Artificial General Intelligence 1, 3\u201324 (2009) 4. Littman, M, Sutton, R., Singh S.: Predictive representations of state. In: Adv. Neural Inf. Process. Syst. 15, pp. 1555\u20131561 (2002) 5. Jaksch, T., Ortner, R., Auer, P.: Near-optimal regret bounds for reinforcement learning. J. Mach. Learn. Res. 11, 1563\u20131600 (2010) 6. Maillard, O.A., Munos, R., Ryabko, D.: Selecting the state-representation in reinforcement learning. In: Adv. Neural Inf. Process. Syst. 24, pp. 2627\u20132635 (2012) 7. Maillard, O.A., Nguyen, P., Ortner, R., Ryabko, D.: Optimal regret bounds for selecting the state representation in reinforcement learning. In: Proc. 30th Int\u2019l Conf. on Machine Learning, ICML 2013, JMLR Proc., vol. 28, pp. 543\u2013551 (2013) 8. Nguyen, P., Maillard, O.A., Ryabko, D., Ortner, R.: Competing with an infinite set of models in reinforcement learning. In: Proc. 16th Int\u2019l Conf. on Artificial Intelligence and Statistics, AISTATS 2013, JMLR Proc., vol. 31, pp. 463\u2013471 (2013) 9. Ortner, R.: Pseudometrics for state aggregation in average reward Markov decision processes. In: ALT 2007. LNCS (LNAI), vol. 4754, pp. 373\u2013387. Springer (2007) 10. Ortner, R., Ryabko, D.: Online Regret Bounds for Undiscounted Continuous Reinforcement Learning, In: Adv. Neural Inf. Process. Syst. 25, pp. 1772\u20131780 (2012)"}, {"heading": "A Proof of Theorem 1", "text": "We start with an error bound for approximation, assuming we compare two MDPs over the same state space.\nLemma 8. Consider a communicating MDP M = (S,A, r, p), and another MDP M\u0304 = (S,A, r\u0304, p\u0304) over the same state-action space which is an \u01ebapproximation of M (for \u03b1 = id). Assume that an optimal policy \u03c0\u2217 of M is performed on M\u0304 for \u2113 steps, and let v\u0304\u2217(s) be the number of times state s is visited state among these \u2113 steps. Then\n\u2113\u03c1\u2217(M)\u2212\u2211s\u2208S v\u0304(s) \u00b7 r\u0304(s, \u03c0\u2217(s)) < \u2113\u01eb(D + 1) +D \u221a 2\u2113 log(1/\u03b4) +D\nwith probability at least 1\u2212 \u03b4, where D = D(M) is the diameter of M .\nProof. We abbreviate r\u2217(s) := r(s, \u03c0\u2217(s)) and p\u2217(s\u2032|s) := p(s\u2032|s, \u03c0\u2217(s)), and use r\u0304\u2217(s) and p\u0304\u2217(s\u2032|s) accordingly. Then\n\u2113\u03c1\u2217(M)\u2212 \u2211\ns\u2208S v\u0304\u2217(s) \u00b7 r\u0304\u2217(s) =\n\u2211\ns\nv\u0304\u2217(s) ( \u03c1\u2217(M)\u2212 r\u0304\u2217(s) )\n= \u2211\ns\nv\u0304\u2217(s) ( \u03c1\u2217(M)\u2212 r\u2217(s) ) + \u2211\ns\nv\u0304\u2217(s) ( r\u2217(s)\u2212 r\u0304\u2217(s) ) . (38)\nNow, for the first term in (38) we can use the Poisson equation (for the optimal policy \u03c0\u2217 on M) to replace \u03c1\u2217(M)\u2212 r\u2217(s) = \u2211s\u2032 p\u2217(s\u2032|s) \u00b7\u03bb\u2217(s\u2032)\u2212\u03bb\u2217(s), writing \u03bb\u2217 := \u03bb\u03c0\u2217 for the bias of \u03c0\n\u2217 on M . Concerning the second term in (38) we can use (4) and the fact that \u2211 s v\u0304 \u2217(s) = \u2113. In summary, we get\n\u2113\u03c1\u2217(M)\u2212 \u2211\ns\u2208S v\u0304\u2217(s) \u00b7 r\u0304\u2217(s) <\n\u2211\ns\nv\u0304\u2217(s) (\u2211\ns\u2032\np\u2217(s\u2032|s)\u03bb\u2217(s\u2032)\u2212 \u03bb\u2217(s) ) + \u2113\u01eb\n= \u2113\u01eb+ \u2211\ns\nv\u0304\u2217(s) (\u2211\ns\u2032\np\u0304\u2217(s\u2032|s)\u03bb\u2217(s\u2032)\u2212 \u03bb\u2217(s) )\n+ \u2211\ns\nv\u0304\u2217(s) (\u2211\ns\u2032\np\u2217(s\u2032|s)\u03bb\u2217(s\u2032)\u2212 \u2211\ns\u2032\np\u0304\u2217(s\u2032|s)\u03bb\u2217(s\u2032) ) . (39)\nBy (4) and using that span(\u03bb\u2217) \u2264 D, the last term in (39) is bounded as \u2211\ns\nv\u0304\u2217(s) \u2211\ns\u2032\n( p\u2217(s\u2032|s, a)\u2212 p\u0304\u2217(s\u2032|s, a) ) \u03bb\u2217(s\u2032) < \u2113\u01ebD. (40)\nOn the other hand, for the second term in (39), writing s\u03c4 for the state visited at time step \u03c4 we have\n\u2211\ns\nv\u0304\u2217(s) (\u2211\ns\u2032\np\u0304\u2217(s\u2032|s)\u03bb\u2217(s\u2032)\u2212 \u03bb\u2217(s) ) = \u2113\u2211\n\u03c4=1\n(\u2211\ns\u2032\np\u0304\u2217(s\u2032|s\u03c4 )\u03bb\u2217(s\u2032)\u2212 \u03bb\u2217(s\u03c4 ) )\n= \u2113\u2211\n\u03c4=1\n(\u2211\ns\u2032\np\u0304\u2217(s\u2032|s\u03c4 )\u03bb\u2217(s\u2032)\u2212 \u03bb\u2217(s\u03c4+1) ) + \u03bb\u2217(s\u2113+1)\u2212 \u03bb\u2217(s1). (41)\nNow \u03bb\u2217(s\u2113+1)\u2212 \u03bb\u2217(s1) \u2264 span(\u03bb\u2217) \u2264 D, while the sequence\nX\u03c4 := \u2211 s\u2032 p\u0304 \u2217(s\u2032|s\u03c4 )\u03bb\u2217(s\u2032)\u2212 \u03bb\u2217(s\u03c4+1)\nis a martingale difference sequence with |Xt| \u2264 D. Thus, an application of Azuma-Hoeffding\u2019s inequality (e.g., Lemma 10 of [5]) to (41) yields that\n\u2211\ns\nv\u0304\u2217(s) (\u2211\ns\u2032\np\u0304\u2217(s\u2032|s)\u03bb\u2217(s\u2032)\u2212 \u03bb\u2217(s) ) \u2264 D \u221a 2\u2113 log(1/\u03b4) +D (42)\nwith probability higher than 1 \u2212 \u03b4. Summarizing, (39), (40), and (42) give the claimed\n\u2113\u03c1\u2217(M)\u2212 \u2211\ns\u2208S\n\u2211 a\u2208A v\u0304(s)\u2217 \u00b7 r\u0304\u2217(s) < \u2113\u01eb(D + 1) +D \u221a 2\u2113 log(1/\u03b4) +D.\n\u2293\u2294 As a corollary to Lemma 8 we can also bound the approximation error in average reward, which we will need below to deal with the error of \u01eb-approximate models.\nLemma 9. Let M be a communicating MDP with optimal policy \u03c0\u2217, and M\u0304 an \u01eb-approximation of M over the same state space. Then\n\u2223\u2223\u03c1\u2217(M)\u2212 \u03c1\u2217(M\u0304) \u2223\u2223 \u2264 \u2223\u2223\u03c1\u2217(M)\u2212 \u03c1(M\u0304, \u03c0\u2217) \u2223\u2223 \u2264 \u01eb (D(M) + 1).\nProof. Divide the result of Lemma 8 by \u2113, choose \u03b4 = 1/\u2113, and let \u2113 \u2192 \u221e. Since the average reward of a policy is no random value, the result holds surely and not just with probability 1. \u2293\u2294\nA.1 Proof of Theorem 1\nThe idea is to define a new MDP M \u2032 = (S,A, r\u2032, p\u2032) on S whose rewards r\u2032 and transition probabilities p\u2032 are \u01eb-close to M and that has the same optimal average reward as M\u0304 . Thus, for each state s \u2208 S and each action a we set r\u2032(s, a) := r\u0304(\u03b1(s), a) and\np\u2032(s\u2032|s, a) := p(s \u2032|s, a) \u00b7 p\u0304(\u03b1(s\u2032)|\u03b1(s), a)\u2211 s\u2032\u2032:\u03b1(s\u2032\u2032)=\u03b1(s\u2032) p(s \u2032\u2032|s, a) .\nNote that p\u2032(\u00b7|s, a) is indeed a probability distribution over S, that is, in particular it holds that\n\u2211 s\u2032\u2208S p\u2032(s\u2032|s, a) = \u2211 s\u2032\u2208S p(s\u2032|s, a)\u2211 s\u2032\u2032:\u03b1(s\u2032\u2032)=\u03b1(s\u2032) p(s \u2032\u2032|s, a) \u00b7 p\u0304(\u03b1(s \u2032)|\u03b1(s), a)\n= \u2211\ns\u0307\u2032\u2208S\u0304\n\u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s, a)\u2211 s\u2032\u2032:\u03b1(s\u2032\u2032)=s\u0307\u2032 p(s \u2032\u2032|s, a) \u00b7 p\u0304(s\u0307 \u2032|\u03b1(s), a)\n= \u2211\ns\u0307\u2032\u2208S\u0304\np\u0304(s\u0307\u2032|\u03b1(s), a) = 1.\nNow by definition, the rewards r\u2032(s, a) = r\u0304(\u03b1(s), a) and aggregated transition probabilities\n\u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np\u2032(s\u2032|s, a) = \u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s, a)\u2211 s\u2032\u2032:\u03b1(s\u2032\u2032)=s\u0307\u2032 p(s \u2032\u2032|s, a) \u00b7 p\u0304(s\u0307 \u2032|\u03b1(s), a) = p\u0304(s\u0307\u2032|\u03b1(s), a)\nin M \u2032 have the same values for all states s that are mapped to the same metastate by \u03b1. It follows that \u03c1\u2217(M \u2032) = \u03c1\u2217(M\u0304).\nFurther by assumption, according to (4) we have\n|r(s, a) \u2212 r\u2032(s, a)| = |r(s, a)\u2212 r\u0304(\u03b1(s), a)| < \u01eb\nand\n\u2211\ns\u2032\u2208S\n\u2223\u2223p(s\u2032|s, a)\u2212 p\u2032(s\u2032|s, a) \u2223\u2223 = \u2211\ns\u2032\u2208S p(s\u2032|s, a)\n\u2223\u2223\u2223\u22231\u2212 p\u0304(\u03b1(s\u2032)|\u03b1(s), a)\u2211\ns\u2032\u2032:\u03b1(s\u2032\u2032)=\u03b1(s\u2032) p(s \u2032\u2032|s, a)\n\u2223\u2223\u2223\u2223\n= \u2211\ns\u0307\u2032\u2208S\u0304\n\u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|s, a) \u00b7 \u2223\u2223\u2223\u2223 \u2211 s\u2032\u2032:\u03b1(s\u2032\u2032)=s\u0307\u2032 p(s \u2032\u2032|s, a)\u2212 p\u0304(s\u0307\u2032|\u03b1(s), a) \u2211\ns\u2032\u2032:\u03b1(s\u2032\u2032)=s\u0307\u2032 p(s \u2032\u2032|s, a)\n\u2223\u2223\u2223\u2223\n= \u2211\ns\u0307\u2032\u2208S\u0304\n\u2223\u2223\u2223 \u2211\ns\u2032\u2032:\u03b1(s\u2032\u2032)=s\u0307\u2032\np(s\u2032\u2032|s, a)\u2212 p\u0304(s\u0307\u2032|\u03b1(s), a) \u2223\u2223\u2223 < \u01eb.\nThus, M \u2032 is an \u01eb-approximation of M that has the same optimal average reward as M\u0304 so that application of Lemma 9 to M and M \u2032 gives the claimed result. \u2293\u2294"}, {"heading": "B Proof of Lemma 1", "text": "For any given number of observations n it holds that (cf. Appendix C.1 of [5]) for any \u03b8 > 0\nP { r\u0302(s\u0307, a)\u2212 E[r\u0302(s\u0307, a)] \u2264 \u221a log(2/\u03b8)\nn\n} < \u03b8,\nP {\u2225\u2225\u2225p\u0302(\u00b7|s\u0307, a)\u2212 E[p\u0302(\u00b7|s\u0307, a)] \u2225\u2225\u2225 1 \u2264 \u221a 2S\u03c6 log(2/\u03b8) n } < \u03b8.\nChoosing suitable values for \u03b8, a union bound over all states s\u0307, all actions a and all possible values for Nt(s\u0307, a) = 1, . . . , t shows the lemma. \u2293\u2294"}, {"heading": "C Proof of Lemma 2", "text": "Let M\u0304 be the MDP on S\u03c6 whose rewards and transition probabilities are given by the expectation values E[r\u0302(s\u0307, a)] and E[p\u0302(s\u0307\u2032|s\u0307, a)], respectively. We have already seen in (14) and (18) that M\u0304 is an \u01eb-approximation of the true MDP M , so that by Theorem 1 \u2223\u2223\u03c1\u2217(M)\u2212 \u03c1\u2217(M\u0304) \u2223\u2223 \u2264 \u01eb(D(M) + 1). (43)\nIt remains to deal with the difference between \u03c1\u2217(M\u0304) and \u03c1\u0302+t (\u03c6). By assumption, the confidence intervals of Lemma 1 hold for all state-action-pairs so that M\u0304 is contained in the set of plausible MDPs Mt,\u03c6 (defined via the empirical rewards and transition probabilities r\u0302(s\u0307, a) and p\u0302(s\u0307\u2032|s\u0307, a)). It follows together with (8) that\n\u03c1\u0302+t (\u03c6) \u2265 \u03c1\u2217(M+t (\u03c6)) \u2212 2\u221at \u2265 \u03c1 \u2217(M\u0304)\u2212 2\u221a t , (44)\nwhich together with (43) proves the claimed inequality. \u2293\u2294"}, {"heading": "D Proof of Lemma 4", "text": "By Lemma 3, at each step t of a run j in an episode k starting at step tkj = t \u2032 the test is passed with probability at least 1\u2212 \u03b46t\u20322 . Assuming that t\u2032\u2032 is the last step in that run and setting \u2113 := t\u2032\u2032 \u2212 t\u2032 + 1 to be the total number of steps in that run, the test is passed in all steps of the run with error probability bounded by (using that 2t\u2032 \u2265 2j \u2265 \u2113)\n\u2113 \u00b7 \u03b4 6t\u20322 \u2264 \u2113\u03b4 2t\u2032(t\u2032 + \u2113) = \u03b4 2t\u2032 \u2212 \u03b4 2(t\u2032 + \u2113) =\n\u222b t\u2032\u2032\nt\u2032\n\u03b4\n2\u03c42 d\u03c4 \u2264\nt\u2032\u2032\u2211\n\u03c4=t\u2032\n\u03b4\n2\u03c42 .\nSumming over all episodes and runs shows that the test is passed in all time steps with probability at least 1\u2212 \u2211\u221e \u03c4=1 \u03b4 2\u03c42 \u2265 1\u2212 \u03b4. \u2293\u2294"}, {"heading": "E Proof of Lemma 5", "text": "We define an MDP M\u0303 on state space S\u03c6 as follows. First let \u03b2 : S\u03c6 \u2192 S be an arbitrary mapping that maps states in S\u03c6 to some state in S such that \u03b1(\u03b2(s\u0307)) = s\u0307. Intuitively, \u03b2(s\u0307) is an arbitrary reference state that is mapped to s\u0307 by \u03b1. Then for s\u0307, s\u0307\u2032 in S\u03c6 we set the transition probabilities of M\u0303 as\np\u0303(s\u0307\u2032|s\u0307, a) := \u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|\u03b2(s\u0307), a). (45)\nThen by (18) and Lemma 1 we obtain\n\u2225\u2225p\u0303(\u00b7|s\u0307, a)\u2212 p\u0302t(\u00b7|s\u0307, a) \u2225\u2225 1 = \u2211\ns\u0307\u2032\n\u2223\u2223\u2223 \u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|\u03b2(s\u0307), a)\u2212 p\u0302t(\u00b7|s\u0307, a) \u2223\u2223\u2223\n\u2264 \u2211\ns\u0307\u2032\n(\u2223\u2223\u2223 \u2211\ns\u2032:\u03b1(s\u2032)=s\u0307\u2032\np(s\u2032|\u03b2(s\u0307), a)\u2212 E[p\u0302t(s\u0307\u2032|s\u0307, a)] \u2223\u2223\u2223+ \u2223\u2223\u2223E[p\u0302t(s\u0307\u2032|s\u0307, a)]\u2212 p\u0302t(s\u0307\u2032|s\u0307, a) \u2223\u2223\u2223 )\n\u2264 \u01eb(\u03c6) + \u221a 2S\u03c6 log(48S\u03c6At 3/\u03b4)\nNt(s,a) ,\nshowing that M\u0303 is contained in Mt,\u03c6. To see that D(M\u0303) \u2264 D, note that \u03b2 maps all transitions in M\u0303 to transitions of the the same or lower probability in the\ntrue MDP. That is, for any s\u0307, s\u0307\u2032 \u2208 S\u03c6 it holds that p\u0303(s\u0307\u2032|s\u0307, a) \u2265 p(\u03b2(s\u0307\u2032)|\u03b2(s\u0307), a). Thus, each trajectory in M\u0303 can be mapped to a trajectory in the true MDP that cannot have higher probability, which proves the first claim of the lemma. The second claim follows immediately along the lines of Section 4.3.1 in [5]. \u2293\u2294"}, {"heading": "F Proof of Lemma 6", "text": "By definition of the algorithm, \u01eb\u0303(\u03c6) for each model \u03c6 has initial value \u01eb0 and is doubled whenever \u03c6 fails a test. Thus, by assumption if \u01eb0 \u2264 \u01eb(\u03c6), then as soon as \u01eb(\u03c6) \u2264 \u01eb\u0303(\u03c6) < 2\u01eb(\u03c6) the value of \u01eb\u0303(\u03c6) will not change anymore, and consequently \u01eb\u0303(\u03c6) < 2\u01eb(\u03c6) always holds.\nOn the other hand, if \u01eb0 > \u01eb(\u03c6) then also \u01eb\u0303(\u03c6) \u2265 \u01eb(\u03c6) for the initial value \u01eb\u0303(\u03c6) = \u01eb0 and again by assumption \u01eb\u0303(\u03c6) = \u01eb0 remains unchanged, so that \u01eb\u0303(\u03c6) \u2264 \u01eb0 holds. \u2293\u2294"}, {"heading": "G Proof of Lemma 7", "text": "First recall that an episode is terminated when either the number of visits in some state-action pair (s, a) has been doubled (line 12 of the algorithm) or when the test on the accumulated rewards has failed (line 9). By assumption, the test is passed provided that \u01eb\u0303(\u03c6) \u2265 \u01eb(\u03c6). If \u01eb(\u03c6) \u2264 \u01eb0, then \u01eb\u0303(\u03c6) \u2265 \u01eb(\u03c6) holds trivially. Otherwise, \u03c6 will fail the test only log2 ( \u01eb(\u03c6) \u01eb0 ) times until \u01eb\u0303(\u03c6) \u2265 \u01eb(\u03c6) (after which the test is passed w.h.p. and \u01eb\u0303(\u03c6) remains unchanged by Lemma 3). Therefore, the number of episodes terminated due to failure of the test is upper bounded by \u2211\n\u03c6:\u01eb(\u03c6)>\u01eb0 log2 ( \u01eb(\u03c6) \u01eb0 ) .\nFor the number of episodes terminated since the number of visits in some state-action pair (s, a) has been doubled, one can show that it is bounded by SA log2 ( 2T SA ) , cf. Appendix C.2 of [5] or Section 5.2 of [7], and the lemma follows.\n\u2293\u2294"}, {"heading": "H Proof of Theorem 4", "text": "As the proof of the regret bound for OMS given in [7] follows the same lines as the proof of Theorem 3 given here, we only sketch the key step leading to the improvement of the bound. Note that by (36) and since average rewards are by assumption in [0, 1], for the model \u03c6kj chosen in some run j of some episode k it holds that pen(\u03c6kj , tkj) \u2264 pen(\u03c6\u25e6, tkj) \u2212 1. Hence, by definition of the penalization term (9) and since 1 \u2264 \u03bb(u+t,\u03c6\u25e6) \u2264 D, the chosen model \u03c6kj always satisfies\n( 2 \u221a 2Skj +\n3\u221a 2\n)\u221a SkjA log ( 48SkjAt3 \u03b4 ) + 2 \u221a 2 log( 24t2kj \u03b4 )\n\u2264 ( 2D \u221a 2S\u25e6 + 3\u221a\n2\n)\u221a S\u25e6A log ( 48S\u25e6At3\n\u03b4\n) + 2D \u221a 2 log(\n24t2kj \u03b4 ) + 2 \u2212j/2D.\nSome simplifications then show that Skj is O\u0303(D 2S\u25e6), so that one can replace the\ntotal number of all states S = \u2211\n\u03c6 S\u03c6 in Theorem 3 (respectively in the regret\nbound of [7]) by the total number of states of models \u03c6 with S\u03c6 = O\u0303(D 2S\u25e6) and consequently by O\u0303(|\u03a6|D2S\u25e6). \u2293\u2294"}], "references": [{"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["P.L. Bartlett", "A. Tewari"], "venue": "UAI 2009, Proc. 25th Conf. on Uncertainty in Artificial Intelligence, pp. 25\u201342. AUAI Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Model selection in Markovian processes", "author": ["A. Hallak", "D.D. Castro", "S. Mannor"], "venue": "19th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining, KDD 2013, pp. 374\u2013382. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature Reinforcement Learning: Part I: Unstructured MDPs", "author": ["M. Hutter"], "venue": "J. Artificial General Intelligence 1, 3\u201324", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive representations of state", "author": ["M Littman", "R. Sutton", "Singh S."], "venue": "Adv. Neural Inf. Process. Syst. 15, pp. 1555\u20131561", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "J. Mach. Learn. Res. 11, 1563\u20131600", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Selecting the state-representation in reinforcement learning", "author": ["O.A. Maillard", "R. Munos", "D. Ryabko"], "venue": "Adv. Neural Inf. Process. Syst. 24, pp. 2627\u20132635", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimal regret bounds for selecting the state representation in reinforcement learning", "author": ["O.A. Maillard", "P. Nguyen", "R. Ortner", "D. Ryabko"], "venue": "Proc. 30th Int\u2019l Conf. on Machine Learning, ICML 2013, JMLR Proc., vol. 28, pp. 543\u2013551", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Competing with an infinite set of models in reinforcement learning", "author": ["P. Nguyen", "O.A. Maillard", "D. Ryabko", "R. Ortner"], "venue": "Proc. 16th Int\u2019l Conf. on Artificial Intelligence and Statistics, AISTATS 2013, JMLR Proc., vol. 31, pp. 463\u2013471", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Pseudometrics for state aggregation in average reward Markov decision processes", "author": ["R. Ortner"], "venue": "ALT 2007. LNCS (LNAI), vol. 4754, pp. 373\u2013387. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning, In: Adv", "author": ["R. Ortner", "D. Ryabko"], "venue": "Neural Inf. Process. Syst. 25, pp. 1772\u20131780", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "We consider a reinforcement learning setting introduced in [6] where the learner does not have explicit access to the states of the underlying Markov decision process (MDP).", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP).", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP).", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "The first regret bounds in this setting were derived in [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "They recently have been improved in [7] and extended to infinite model sets in [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "They recently have been improved in [7] and extended to infinite model sets in [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "Here we extend and improve the results of [7] as follows.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "Second, we improve the bounds of [7] with respect to the dependence on the state space.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 159, "endOffset": 164}, {"referenceID": 6, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 159, "endOffset": 164}, {"referenceID": 7, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 159, "endOffset": 164}, {"referenceID": 1, "context": "Here we only would like to mention the recent work [2] that considers a similar setting, however is mainly interested in the question whether the true model will be identified in the long run, a question we think is subordinate to that of minimizing the regret, which means fast learning of optimal behavior.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "actions, and R = [0, 1] the set of possible rewards.", "startOffset": 17, "endOffset": 23}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In the following we assume that rewards are bounded in [0, 1], which implies that span(\u03bb) is upper bounded by D, cf.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "[5, 1].", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[5, 1].", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[5, 1, 6], as \u2206(\u03c6\u25e6, T ) := T\u03c1\u2217(\u03c6\u25e6)\u2212Tt=1 rt , where rt are the rewards received when following the proposed strategy and \u03c1\u2217(\u03c6\u25e6) is the average optimal reward in \u03c6\u25e6, i.", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[5, 1, 6], as \u2206(\u03c6\u25e6, T ) := T\u03c1\u2217(\u03c6\u25e6)\u2212Tt=1 rt , where rt are the rewards received when following the proposed strategy and \u03c1\u2217(\u03c6\u25e6) is the average optimal reward in \u03c6\u25e6, i.", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "[5, 1, 6], as \u2206(\u03c6\u25e6, T ) := T\u03c1\u2217(\u03c6\u25e6)\u2212Tt=1 rt , where rt are the rewards received when following the proposed strategy and \u03c1\u2217(\u03c6\u25e6) is the average optimal reward in \u03c6\u25e6, i.", "startOffset": 0, "endOffset": 9}, {"referenceID": 8, "context": "It generalizes bounds of [9] from ergodic to communicating MDPs.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "This is an improvement over the results in [9], which only showed that the error approaches 1 when the diameter goes to infinity.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "The OAMS algorithm (shown in detail as Algorithm 1) we propose for the setting introduced in Section 1 is a generalization of the OMS algorithm of [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "This can be done by extended value iteration (EVI) [5].", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "[5])", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Similar to the REGAL algorithm of [1] this shall prefer simpler models (i.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "When the learner knows that \u03a6 contains a Markov model \u03c6\u25e6, the original OMS algorithm of [7] can be employed.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": ", S > D2|\u03a6|S\u25e6, we can improve on the state space dependence of the regret bound given in [7] as follows.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "The proof (found in Appendix H) is a simple modification of the analysis in [7] that exploits that by (11) the selected models cannot have arbitrarily large state space.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "This could also give improvements over current regret bounds for continuous reinforcement learning as given in [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The proof is divided into three parts and follows the lines of [7], now taking into account the necessary modifications to deal with the approximation error.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": ", Lemma 10 of [5]) yields that with probability at least 1\u2212 \u03b4 24t\u20323", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "2 of [7], \u2211 k Jk \u2264 KT log2(2T/KT ), \u2211 k \u2211 j 2 j \u2264 2(T + KT ) and \u2211 k \u2211 j 2 j/2 \u2264 \u221a 2KT log2(2T/KT )(T +KT ), and we may conclude the proof applying Lemma 7 and some minor simplifications.", "startOffset": 5, "endOffset": 8}], "year": 2014, "abstractText": "We consider a reinforcement learning setting introduced in [6] where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, she has access to several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting, and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}