{"id": "1405.3410", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Efficient classification using parallel and scalable compressed model and Its application on intrusion detection", "abstract": "in order to achieve high efficiency of classification in intrusion detection, a compressed model is proposed in this procedure which combines horizontal compression with vertical compression. oner is utilized beyond horizontal com - pression for attribute extraction, and affinity propagation is employed as vertical compression to extracted small element exemplars from large training data. following regards be successful to weakly compress the larger volume 1 training data with alignment, mapreduce based parallelization methodology is formally implemented and evaluated for each step of the model compression process simulation, assessing which common but efficient classification methods can be directly used. experimental early study on two publicly available datasets of intrusion detection, kdd99 and cmdc2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at 18 to 184 times, most importantly at the threshold of a minimal accuracy difference with less than 1 % on average.", "histories": [["v1", "Wed, 14 May 2014 08:47:31 GMT  (1130kb)", "http://arxiv.org/abs/1405.3410v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["tieming chen", "xu zhang", "shichao jin", "okhee kim"], "accepted": false, "id": "1405.3410"}, "pdf": {"name": "1405.3410.pdf", "metadata": {"source": "CRF", "title": "Efficient classification using parallel and scalable compressed model and Its application on intrusion detection", "authors": ["Tieming Chen", "Xu Zhang", "Shichao Jin", "Okhee Kim"], "emails": ["*tmchen@zjut.edu.cn"], "sections": [{"heading": null, "text": "tection, a compressed model is proposed in this paper which combines horizontal compression with vertical compression. OneR is utilized as horizontal compression for attribute reduction, and affinity propagation is employed as vertical compression to select small representative exemplars from large training data. As to be able to computationally compress the larger volume of training data with scalability, MapReduce based parallelization approach is then implemented and evaluated for each step of the model compression process abovementioned, on which common but efficient classification methods can be directly used. Experimental application study on two publicly available datasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at up to 184 times, most importantly at the cost of a minimal accuracy difference with less than 1% on average.\nKeywords: Compressed model, MapReduce, Parallelization, Classification, Intrusion Detection."}, {"heading": "1. Introduction", "text": "With the larger and larger amount of network communication data generated, the design of Intrusion Detection System (IDS) with high efficiency has become much more challenging. It is very important to discover abnormal behaviors at early stage, therefore, compared to the traditional signature-based detection, research on anomaly detection has been more popular in academia, as it has the potential power to detect unknown attacks by kinds of heuristic learning on the historical training data.\nAnomaly detection generally includes two steps, building a model on training data and using the model for detection. However, training data are usually in a large scale, which can severely impede the detection since many detection models may need to scan all of them in certain cases. Intuitionally, an effective and direct way to reduce\ntime cost for detection is to minimize the volume of a model that is used in the detection process, but building a systematic and scalable solution on generating such minimizing training data model for efficient intrusion detection is still in challenge.\nTo address this problem, our work will pay attention largely to the building of data compression instead of the detection phase, striving to boost the detection efficiency based on a proposed compressed model of training data. Therefore, the solution presented in this paper is applicable for those model-based anomaly detection approaches, especially for the classification based system[1] because extracting the classification model directly from the huge volume of training data instances inevitably needs intense computation.\nAs for how to build compressed model, our proposal is made through inspecting into the following common natures of the training data, that is to say the motivation and inspiration of our works are generated from the following observations:\na. By analyzing the attributes of the training data, we can easily find that the values of\nsome attributes (features) in the whole training data only range in a small scale, which may have less impact on the detection accuracy. b. Some training instances are similar, because they are only different from each oth-\ner on several attributes and the values of these attributes are slightly different, which may have redundancy for detection model building. c. For high dimensional training data, computing the similarity or some akin metric\nbetween each pair of instance is time-consuming. That means, for some novel but promising data processing algorithms such like Affinity Propagation[2], the general computing memory would likely explode when the dimensionality of training data matrix increases to some large extent.\nFor the purpose of effectively and efficiently handling these problems, we will propose a new framework of compressed model on training data. The model compression procedure mainly includes horizontal compression and vertical compression. The overall idea is presented in Fig. 1, where the first step is to normalize the original data followed by the horizontal compression and the vertical compression sequentially. Based on the compressed model, efficient classification can be directly built to detect new data without losing accuracy.\nFig.1 The main idea of compressed model for data classification\nFurthermore, to computationally compress as large-scale as the training dataset could be, a cloud-based computing framework will be employed to parallelize the compression procedure, which realizes the scalability on training data compressing.\nTo summarize, we mainly make the following contributions in this paper:\na. We propose a compressed detection model, which is a compact version from the\noriginal training dataset with regard to reducing both feature dimension and instance volume.\nb. We implement a MapReduce based parallel computing solution for the abovemen-\ntioned model compression, which can compress larger scale of training data if only scaling up the involving distributed nodes.\nc. We finalize our compressed model-based classification approaches, and demon-\nstrate the performance of our method on detection efficiency and accuracy on two publicly available intrusion detection datasets, KDD99[3] and CDMC2012[4].\nThe remainder of this paper is organized as follows. Literature works are firstly studied in section 2. Section 3 introduces the methodology of our parallel and scalable compressed model, and explains its implementation procedure in detail. Section 4 describes the efficient classification deployments using the compressed model. Application experiments and its performance analysis on intrusion detection are presented in Section 5, while concluding remark and future work are discussed in last section."}, {"heading": "2. Related Work", "text": "During the past two decades, researchers in related fields have paid much attention to intrusion detection. Signature based detection (e.g. Snort [5]) relies on the knowledge of system vulnerabilities and known attack patterns, which hence is unable to detect unknown attacks. Correspondingly, anomaly detection is more dynamic and be able to detect novel attacks, which has attracted a lot of works worldwide. Generally, an anomaly-based intrusion detection system includes following steps, data gathering, data preprocessing[6], model building[7], and model-based detection[8]. Although some common classification methods can be well used for the model-based detection, the way of model building may affect the detection results directly and heavily. Therefore, academic research on the model-based classification approach for intrusion detection is one unceasing focused topic. At beginning, detection accuracy, usually known as detection rate(recall) and false alarm rate(false positive), is widely concerned for the real-world application purpose. Recently, detection efficiency is more considered rather than accuracy to practical significance, especially on the potential abnormal behavior detection for high-speed and real-time network traffics. Nevertheless, both"}, {"heading": "2.1 Efficiency Concerned Intrusion Detection", "text": "In 1998, Lee and Stolfo [9] published a data mining approach for the intrusion detection, where they proposed a framework for the agent-based intrusion detection, and\ndeployed data mining methods to extract detection rules. Afterwards many researchers definitely focused on the way of boosting the detection speed. For example, Sung[10] improved the detection speed by extracting the useful subset of attributes with ANN and SVM, and Srilatha[11] investigated the performance of Bayesian networks (BN) and Classification and Regression Trees (CART) to build lightweight IDS.\nActually, anomaly intrusion detection is a kind of complicated classification problem since there are usually too many attributes or features which may be redundant. So, attribute reduction or feature selection is the most popular method to improve detection efficiency by directly reducing the data attribute dimension[12]. There are several basic but still in-progressing ways of feature selection for anomaly intrusion detection. PCA is a widely used criteria to select features for intrusion detection, which can be usually incorporated with other soft computing models, such as neural networks[13], genetic algorithms[14], etc. PCA is a statistics-based method which is direct and effective, but it lacks much intelligence. Some novel methods are therefore proposed recently that employ diverse intelligent approaches to reduce features, such as fuzzy C means[15], mutual information definition[16], graph visualization technique[17], gradually removal method[18], etc. Although reducing attributes can obviously lower the time cost for detection, detection accuracy should also be guaranteed for real-world applications which will be discussed in the next subsection.\nMore importantly, as to handle the massive traffic data in network intrusion detection, two aspects of effort on efficiency improvement are further studied. One is to sample the data as a training dataset instead of considering the full dataset, for example the random data sampling method[19] was proposed to deal with the massive traffic, and the fuzzy C-means[20] was also employed for the purpose of selecting smaller number of training data. However, the biggest problem of sampling methods is that it may lose potentially useful data which are very distinguishable for detection[21]. Another important way is to parallelize the data processing procedure to achieve the computation improvement. With the development of the hardware, Giorgos firstly made efforts to improve the detection speed by applying graphics processors[22], followed by the improved works in paper [23] which focused on the scalability of analyzing large data except speed acceleration. At very current, MapReduce based cloud computing frameworks start to be studied to build efficient intrusion detection systems[24,25]. Actually, a general data mining toolkit using MapReduce has been integrated with Weka[26], and there also developed a new toolkit on Hadoop, Mahout[27],where most classical data mining algorithms are all parallelized and distributed. All these implementations show that cloud-based parallel computing is a promising solution to improve the efficiency of intrusion detection. It makes us believe that MapReduce could also be explored in our works to parallelize and distribute each step of data computing on model compression for intrusion detection."}, {"heading": "2.2 Accuracy Concerned Intrusion Detection", "text": "As discussed above, intrusion data attributes are sometimes redundant, which is one intrinsic reason for attribute reduction. Nevertheless, attributes may also contain false\ncorrelation. As to reduce the false alarm rate, alert correlation algorithms are integrated with the existing feature selection methods to automatically generate a more concise feature set to improve the overall precision of intrusion detection[28,29,30]. Recently, some other machine learning-based adaptive methods are also proposed to purify the false alarm[31,32]. Other than feature selection, clustering has been widely used as hybrid machine learning approach to improve the detection rate[33].\nGenerally speaking, a hybrid approach for intrusion detection is involved between the clustering and classification models. Clustering is usually used as the first step to filter out unrepresentative data, while classification is used later for detection[34,35]. In fact, the data which cannot be clustered accurately could be regarded as noisy data or outliers. So, the final clustered data, also called representative data or exemplars, are possibly without the noisy data and could be better used to train the classifier to improve the detection rate[36]. As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39]. Moreover, improvement researches on k-NN and SVM are still continuingly active to raise the detection precision[38,40].\nAs a new study, recent researches show that a novel clustering method, affinity propagation[2], can well be used to generate the good representative data for intrusion detection training[41,42]. Also, the AP clustering method is already employed to well apply on network stream data processing to improve the detection correctness[43]. Although it can drive high accuracy, the clustering performance of AP when processing large training datasets is very frustrating. Therefore, research on parallelization of AP is also introduced recently, for example, GPU-based parallelization of AP has been freshly developed to speed up the clustering[44]. However, like the traditional parallel algorithms, GPU-based approaches still challenge some intrinsic problems such as shared communication, load balancing, etc[45]. So as aforementioned, implementing cloud computing based AP parallelization will achieve many merits such as the fault-tolerance of failure nodes, the ability to scale-up large number of nodes on commodity hardware, etc."}, {"heading": "2.3 Both Concerned Intrusion Detection", "text": "From the related works discussed above, we can find the following two key technique trends on intrusion detection. a. Clustering is very useful to find the outliers for intrusion detection[46], especially\nAP clustering is one promising method on exemplars selection for intrusion detection[41,42,43].\nb. Cloud computing is a transparent framework to implement parallelization of data\nprocessing algorithms, especially MapReduce becomes a promising approach towards to scalable intrusion detection system[24,25,47].\nAnother observation is that most of the-state-of-art works on intrusion detection on-\nly focus on efficiency or accuracy. That means, feature selection and clustering are generally two independent research topics for efficiency and accuracy respectively, and there is few systematic and practical approach to guarantee both two.\nTherefore, our works in this paper will initially focus on the following two aspects.\na. Propose a data compact model, named compressed model, which integrates both\nfeature selection and AP clustering to be more concise and accurate. The compressed model is based on the view of systematic methodology, on which some classification model for intrusion detection can then be efficiently built.\nb. Employ MapReduce to implement a full parallel compression framework which\ncan be scalable in processing large training data. This makes the model compression both efficient and available, thus the classification-based intrusion detection using such compressed model is practical with high efficiency and accuracy."}, {"heading": "3. Parallel and Scalable Model Compression", "text": ""}, {"heading": "3.1 Using Training Data to Build Compressed Model", "text": "In this section, we will elaborate the details of our proposed compressed model. Conventionally, data collection, data analysis, model building and detection are four sequential steps for constructing the supervised or the semi-supervised anomaly detection system, where the model building step is our major concentration.\nFig.2 Compressed model building with OneR as horizontal compression and AP as vertical compression\nAs illustrated in Fig.2, to build a compressed model, firstly we horizontally abstract the useful attributes from the training data that have been normalized, saying attribute reduction. Then, we vertically compress the training data to further select representative ones, saying instance reduction.\nPrior to the discussion of the compressed model, we firstly introduce two datasets used throughout this paper. KDD99 dataset is a well-known intrusion detection evaluation dataset transformed from DARPA Intrusion Detection Evaluation dataset. Although KDD99 has been criticized for various reasons, it is still a benchmark for evaluating performance of intrusion detection. KDD99 is collected in a military network environment and it contains numerous simulated connections including normal ones and attacks. Each of the connections in the KDD99 has already been broken down into 41 attributes and well labeled as normal or a specific attack type. Here we simply classify all the instances into two types, normal and attack. Due to the huge volume of KDD99 and for experimental analysis purpose, we just use randomly selected 10,000 instances from KDD99 for training and another 90,000 instances for detection. Since KDD99 is an old dataset more than ten years ago which cannot identify the current network situation to some extent, we also use the CDMC2012 dataset in our experi-\nments 1 . The real traffic data in CDMC2012 are collected from several types of honeypots and a mail server over 5 different networks inside and outside of Kyoto University. The dataset is composed of 14 features including label information which indicates whether each session is attack or not. Similarly, we classify the dataset into two categories, normal and attack, and we use the available training data to perform our experiments, where 12,872 instances are used for training and another 115,848 instances for detection.\nNote that the whole training and testing data are always made up of a lot of instances. Each instance can be seen as a row, which consists of m attributes, and can be expressed as , where is the -\nth instance of the whole dataset which totally contains n instances. A mapping example of the attributes and their corresponding values in KDD99 dataset is listed in Table 1, and a similar mapping method is also employed in the later discussed dataset of CDMC2012.\n3.1.1 Data Normalization\nSince the training data are made up of a large number of instances and each instance has several attributes, a challenge about the training data is that the values of different attributes are distributed on disparate scales, which may cause a bias toward certain attributes over others. Here we give an example: consider two vectors with 3 attributes, {(0, 1200, 5), (1, 1000, 10)}. Taking the Euclidean distance for example, the squared distance between vectors will be (0 - 1) 2 + (1200 - 1000) 2 + (5 - 10) 2 , which is decided heavily by the second attribute. To balance the contribution of every attribute in the similarity calculation, we first normalize the data to the scale of [0, 1] through the following formula (1).\n[ ] [ ] [ ]\n[ ] [ ] . (1)\nHere [ ] denotes the value of jth attribute of ith data instance. [ ] is the smallest value of attribute among the dataset , while [ ] is the biggest one among all the data instances.\n1 CDMC2012 is the 3rd Cybersecurity Data Mining Competition satellited with ICONIP 2012, where a novel intrusion detection dataset is published as task 2. Note that two graduate students supervised by the first author have ever attended this competition and finally won the First place on task 2 with the highest detection accuracy 97.14%. You may please refer to http://www.csmining.org/cdmc2012/index.php?id=14 if interested.\n3.1.2 Horizontal Compression\nThe horizontal compression, as the first step of building a compressed model, mainly explores the relations and correlations of the features within an instance, and extracts the key features. The horizontal compression is useful because in some complex classification fields the false correlative features may impede the process of detection. Most importantly, some features may be redundant since they may not play a role to distinguish one instance from others. Moreover, it is worth mentioning that excessive features may sharply slow down the detection. As like what we will introduce below, the clustering methods utilized in our experiments will calculate the distance between any two instances to measure their similarity. Given the high dimensionality resulting from numerous attributes in each instance, it is time-consuming to calculate such distance with all the attributes one by one. For this reason, it is meaningful to horizontally compress the training data no matter in the model building phase or in the detection process.\nThere are many ways to realize our horizontal compression idea. Here we choose OneR [48] as it is very easy to understand but with very high efficiency. We briefly introduce the principle of OneR by taking the protocol_type attribute of the KDD99 dataset as an example. The illustrated process of compressing the instances horizontally using OneR is shown in Fig.3.\nOneR extracts the rules by examining the attributes one by one. The attribute in Fig.3 is protocol_type. First of all, OneR summarizes the quantitative relationship between the instance type (normal or attack) and the attribute value (TCP, UDP and ICMP). By traversing all the instances, it is concluded that there are two normal instances and one attack instance with the TCP protocol_type in Fig.3. Then OneR chooses the instance type (normal or attack) with a larger frequency as the prediction type for a certain attribute value. For example, since two normal instances (one attack instance) have the TCP value, the prediction type of the TCP is \u2018normal\u2019, which means if the instance has the protocol_type value TCP, we predict that this instance is a \u2018normal\u2019 one. Finally, OneR calculates the total error of every attribute, and chooses the attributes with relatively lower total errors from all attributes as the representative ones.\nIn our applications, as a default we finally select 12 out of 34 numerical attributes to represent an instance in KDD99 dataset, and 10 features out of total number of 14 after horizontal compression in CDMC2012.\n3.1.3 Vertical Compression\nSome training instances appear to be duplicate or similar, for example, they are probably the same packages and do the same business during a certain period of time. Therefore, vertical compression is called responsible for extracting a smaller set of representative training instances from a large scale dataset.\nFor the vertical compression purpose, we here employ the latest published but promising clustering approach, affinity propagation[2], which is actually a cornerstone through full of our idea. The reason to use the affinity propagation method instead of other traditional clustering methods, such as k-means, is straightforward that the affinity propagation can cluster the data without need to predefine a threshold of the expected number of clusters. Actually, in the case of the intrusion detection, we usually do not know how many clusters will be suitable for the training data.\nSuppose that we have the knowledge about the appropriate number of clusters beforehand, the classical clustering method k-means, however, will also be conducted in our experiments for a comparison purpose. In order to make the parameter settings in our experiments understandable, we would introduce and discuss the k-means and the affinity propagation respectively in next section.\n3.2 K-means and Affinity Propagation\n3.2.1 Brief Descriptions on Methods\nK-means. Given a set of instances , where each instance is an attribute vector with real values. k-means aims to partition the instances into sets so as to minimize the within-cluster sum of squares:\n\u2211 \u2211 \u2016 \u2016 . (2)\nHere is the mean point in . K-means firstly selects instances randomly as cluster centroids, and then it assigns each of the remaining instances to the cluster whose centroid is the most similar to this instance. After this, k-means refreshes all the clusters and makes the mean vector of the entire vectors within the cluster as the new centroid. K-means iteratively runs the procedure until the fitness function is convergent.\nAffinity Propagation. Affinity Propagation (AP) clusters instances by passing messages between data points iteratively. Define as the instances to be clustered and let ( ) denote the similarity or distance between instance and\ninstance . Finally, we should minimize the sum of the distances between instances and their exemplars. The fitness function is listed below:\n\u2211 ( ) . (3)\nHere is the exemplar of instance , and ( ) is defined as below:\n( ) {\n. (4)\nHere stands for preference which is used to indicate how much an instance is likely to be chosen as an exemplar. Please note that an exemplar in AP is just like a representative instance of a cluster in k-means. The clustering procedure of AP seeks a good clustering result that can maximize the fitness function by passing messages.\nIn summary, instead of requiring the number of clusters pre-specified in k-means, the preference in AP can affect the number of final clusters because the instance with larger preference is more likely to be chosen as an exemplar.\n3.2.2 Discussions on performance\nAlthough AP clustering is more suitable for vertical compression in our model than K-means, the time and space complexity of AP is both O(N 2 ) while the K-means is O(NK) where K is the specified initial number of clusters. That is to say, as running AP clustering on one single PC, computing the responsibility and availability matrix during iterations will easily run down when N increases, especially for the case that each data instance holds many attributes. Fig.4 shows the quick performance degeneration of AP clustering on different subsets with different number of features selected from KDD99 data aforementioned compared to that of K-means, where the running termination of both algorithms is set to obtain the nearly same number of clusters for the sake of convincing comparison. Note that the different numbers of selected attributes are resulted from OneR with 20, 30 and 40 respectively. It can be seen from Fig.4 that, whatever the number of attributes used for clustering, the computation time cost of K-means is acceptable even if the training data items reaches to 10,000, while that of AP is with lower efficiency as the number of items increases. In fact, the running of AP is finally halted out of memory when the items are larger than 5,000 for the iterative computation on data matrix (Note that AP clustering definitely runs out of memory for 6000 training data instances in Fig.4.). Therefore, the general PC cannot at all afford the computation burden of AP clustering for large training dataset such as KDD99.\nFig.4 Performances of K-means and AP clustering for different volumes of KDD99 training dataset with\ndifferent number of selected attributes running on one PC equipped with AMD Phenom(tm) II N970 QuadCore 2.20GHz CPU and 4GB RAM.\nTo solve this problem, we will propose a distributed and scalable solution for AP clustering on large training datasets using MapReduce. MapReduce is a distributed and parallel programming framework integrated in Apache Hadoop[49], which at current is a well-known popular cloud computing environment for large scale data processing. As a sequence, not only efficiency but also scalability for vertical compression conducted on large training data instances are expected when MapReduce is employed to parallelize each step of AP clustering."}, {"heading": "3.3 Compression Using MapReduce", "text": "3.3.1 Implementations of Parallelization\nA full solution of parallel computing using MapReduce for both horizontal and vertical compression is sketched as in Fig.5. OneR can be directly MapReduced by onestep, while as the procedure of AP is divided and MapReduced respectively and sequentially.\nFig.5 The main skeleton of parallelization of model compression using MapReduce\nBased on the work procedure of OneR, the parallelization could be easily realized if the error value of each attribute could be calculated on distributed nodes simultane-\nously. Therefore, as shown in Fig.5, we firstly transpose the training data matrix to let each row of data matrix denote one values vector of each attribute, which can be taken as partitioned inputs for Map nodes. When each attribute\u2019s error value in OneR is parallelly generated by Map tasks on distributed nodes, the final selection of attributes can be conducted according to the summarization by one Reduce task.\nBefore looking into the parallelization of AP, let us review again the AP algorithm into deeper. Besides the similarity s(i,k) defined between data point i and k, responsibility r(i,k) and availability a(i,k) are other two key metrics in AP. The former shows the suitability of data point k as the exemplar for data point i after the k competes all other point k\u2019, while the later shows the contribution of data point i to select k as its exemplar after the i evaluates all other point i\u2019.\nThe iterative running steps of AP for N total data points are described as below:\nStep 1. Initialize the responsibility and availability matrix with all r(i,k) and a(i,k) being 0. Step 2. Compute the similarity matrix s(i,k). Step 3. Update the responsibility r(i,k) in term of the following rule:\n)})',()',({max),((),()1(),( ' kiskiakiskirkir kk    ,where  is a damping parameter usually assigned with 0.8 according to experimental analysis[2].\nStep 4. Update the availability a(i,k) in term of the following rule:\n}))},'(,0max{),(,0(min{),()1(),( },{'    kii kirkkrkiakia \n   }{' )},'(,0max{),()1(),( ki kirkkakka \nStep 5. Select data point k as clustering exemplar where r(k,k)+a(k,k)>0, and make\ndata point k as the exemplar for data point i where s(i,k) is the maximum one.\nStep 6. Terminated if the iteration number exceeds a specific threshold or the exemplars are unchanged during a specific number of continued iterations, other-\nwise return step 3.\nObserving the above steps of AP, we will then introduce the parallelization method\nfor each step using MapReduce.\nParallelization on Similarity Matrix Computation. Euclidean distance is employed\nto compute the similarity under the following formula as for point x and point y with\ntotally n attributes:\n2 2 2\n1 1 1 1\n( , ) ( ) 2 n n n n\nk k k k k k\nk k k k s x y x y x y x y            \nIt is obvious that all the three terms for the training data matrix A can be generated from the product of the matrix A and its transposed one A T , but here we will give a more tricky approach using MapReduce.\nAssign A is composed of m instances with each instance having n attributes:\n11 12 1\n21 22 2\n1 2\n, ,...,\n, ,...,\n...\n, ,...,\nn\nn\nm m mn\na a a\na a a A\na a a             The MapReduce procedure to compute the similarity matrix of A is depicted in\nconcise as following:\n11 11 11 21 11 1\n21 21 21 1\n11 12 1 11 21 1\n1 121 22 2 12 22 2\n12\n1 2 1 2\n1, , ,...,\n2,0 , ,...,\n..., ,..., , ,...,\n,0 ,0 ,...,, ,..., , ,...,\n1,... ...\n, ,..., , ,...,\nm\nm\nn m\nm mn m\nm m mn n n mn\na a a a a a\na a a a\na a a a a a\nn a aa a a a a a\na\na a a a a a\n  \n \n                           12 12 22 12 2 22 22 22 2\n11 11 12 12 1 1 11 21 12 22 1 2 11 1 1\n21 21 22 22 2\n, ,...,\n2,0 , ,...,\n...\n,0 ,0 ,...,\n1, ... , ... ,..., ...\n2,0 , ...\nm\nm\nmn mn\nn n n n m n mn\na a a a a\na a a a\nn a a\na a a a a a a a a a a a a a a a\na a a a a\n                            \n               \n     \n2 21 1 2\n1 1\n,..., ...\n...\n,0 ,0 ,..., ...\nn n m n mn\nn m nn mn\na a a a a\nn a a a a\n                   \nAs like for OneR, the matrix A is firstly transposed to be A T in which each row is actually one vector made up of all values of one attribute. As for Map task, the respective products between each value (we call it key seed) and its backward values in each row are generated with assigning the column number of the key seed as Key and the product results list as Values. Here all the products between key seed and its forward values in each row are denoted with 0. Then, as for Reduce task, all the corresponding products in each row with the same Key value are combined to be aggregative addition terms, from which all the similarity items aforementioned can be simultaneously computed with ease.\nParallelization on Responsibility and Availability Computation. Note that if the damping parameter \u03bb is fixed, updating r(i,k) is only related to the ith row of both availability and similarity matrix while a(i,k) only to kth column of responsibility matrix. Hence, during MapReduce the Map task is only simply to assign each ith row as the Key value, and then the same ith row (with the Key value equaling i) of data point can be eventually shuffled to the same Reduce to update r(i,k) and a(i,k).\nFor efficiency purpose, we designed the following structure to present a data point:\nPoint {\nint x; //row value of matrix int y; //column value of matrix double s; //value of s(x,y)\ndouble r; //value of r(x,y) double a; //value of a(x,y)\n}\nSuch a point structure can store the similarity s(x,y), responsibility r(x,y), and avail-\nability a(x,y) of the point with xth row and yth column in training data matrix N(x,y).\nWe firstly sequentialize each training data utilizing Point to be as input for MapRe-\nduce, then the above mentioned parallel computing procedure can be automatically implemented using the Map and Reduce program. Row-based MapReduce implementation for responsibility parallelization is pictured as Fig.6, while the availability is just with the similar process except for doing column-based Map and Reduce.\nFig.6 Parallelization procedure for Responsibility updating in Point using MapReduce\nParallelization on Exemplars Selection. Because a(k,k) and r(k,k) are both included in one same Point structure, each Map node can independently compute which data points are exemplars. Therefore, computing parallelization for exemplar selection can be easily implemented by that Map task generates the corresponding exemplars and Reduce task directly combines all exemplars.\n3.3.2 Evaluations on Time Cost and Scalability\nAs to horizontal compression using parallelization, it can ignore the time cost for that it only takes less than one second to compress totally 100,000 instances from KDD99 and 128,720 instances from CDMC2012 dataset.\nThe time cost for generating the compressed model vertically with AP and k-means is described comparably in Fig.7. It can be concluded that model compression may take relatively long time, but is considerably improved compared to that from single PC as show in Fig.4. In fact, the parallel running time for clustering 6000 training data points, even with 40 attributes, using MapReduce with 8 nodes only needs about 100 seconds. As to CDMC2012 dataset, however, the average time cost using the same parallelization platform for compressing model vertically is about 30 seconds.\nFig.7 Performances of K-means and AP clustering for different volumes of KDD99 training dataset with\ndifferent number of selected attributes running on 8 nodes of MapReduce clusters with each node equipped\nwith AMD Phenom(tm) II N970 Quad-Core 2.20GHz CPU and 4GB RAM.\nNot like as single PC could not afford the computation burden of AP clustering for larger volume of training data with many attributes, the proposed MapReduce employed parallelization solution for AP is much more scalable for training data volume. As following the performance test results are presented in Fig.8 and Fig.9 respectively. Fig.8 shows the total speedup of computing power for AP-based data compressing by augmenting the number of nodes using MapReduce, while Fig.9 shows the specific scalability power for handling the increasing volume of training datasets with different number of nodes. We can clearly see that, from Fig.9, when 8 nodes employed in MapReduce, the time needed to handle 8,000 sample points is only around 3 times than that to 2,000 both for KDD99 and CDMC2012.\nFig.8 Performance of AP clustering for different number of nodes using MapReduce for the given KDD99 and CDMC2012 dataset(also each node equipped with AMD Phenom(tm) II N970 Quad-Core 2.20GHz CPU and 4GB RAM)\nFig.9 Performance of AP clustering for different volumes of KDD99 and CDMC2012 training datasets using MapReduce with different number of nodes (also each node equipped with AMD Phenom(tm) II N970 Quad-Core 2.20GHz CPU and 4GB RAM)\nAlso, the figures in the Fig.7 give us a hint that the step of vertical compression can be done off-line to meet the requirement in the real-time environment, which applies to the horizontal compression as well. To be specific, we just need to provide the original training data to the MapReducer responsible for compressing the model, from which the generated compact model will be returned to the on-line part for the later classification modeling. From Fig.7, we can also observe that the horizontal compression with OneR can significantly save time on the vertical compression in most cases."}, {"heading": "4. Classification Using Compressed Model", "text": "After the compressed model is built, two traditional classification methods are adopted in our experiments to evaluate the detection performance of the compressed model, namely KNN and SVM.\nKNN (K Nearest Neighbor) is one of the most widely used classification methods in data mining. It finds nearest neighbors of a given instance among all the training data. There exist various ways of realizing KNN algorithm, and for the purpose of comparing traditional KNN with our improved KNN by utilizing distance matrix, a linear scan is employed in our experiments.\nSVM (Support Vector Machine) is one of the recognized machine learning methods for classification, regression and other learning tasks. Here we would apply CSVC (C\u2013Support Vector Classification), one among the SVMs, to identify whether a package is abnormal or not. Unlike one-class SVM, which is a frequently used detection method in intrusion detection using the model built with normal class only, CSVC is based on the model with both normal and abnormal instances which is exactly applicable for KDD99 and CDMC2012 datasets.\nSince parameters of clustering methods and detection approaches can significantly\ninfluence the results, here we will explain the way we choose them.\nAP. In AP, the only parameter that should be set is the preference. In our experiments, for a comprehensive evaluation, we choose the preferences between the minimum and the maximum of the similarities to generate expected number of clusters that are separately distributed. The relationship between the number of exemplars generated with AP and the corresponding preference set is described in Table 2. Since the number of clusters generated by AP is decided by its preference, it is hardly possible to generate exactly the wanted number of exemplars. Thus, we endeavor to generate similar numbers of exemplars for the conditions with and without OneR respectively for a comparison purpose, but not exactly the same. Accordingly, it can be observed from Table 2 that the number of exemplars grows with the increment of the preference within expectation.\nK-means. Since we use k-means here for the comparison purpose with AP, the parameter in the k-means should be set to the same with the number of exemplars generated by AP.\nKNN. As to KNN, we set the parameter to be 1, which means that the type for each tested instance only depends on its nearest neighbor in the compressed model. Although this approach is rather simple, it has shown the high effectiveness practically.\nC-SVC. As for C-SVC, since the number of attributes is quite small compared with the amount of instances, we choose to deploy a nonlinear kernel, namely Radial Basis Function kernel (RBF kernel), to map data to higher dimensional spaces. To better use the RBF kernel, one should determine two parameters: and . Since the test data are unknown in advance, we can only find proper parameters using foregone training data with the help of cross-validation. LibSVM [50] provides an automatic implementation on grid-searching and using cross-validation, through which the best parameters for our training data can be concluded as following: for KDD99 dataset, the best\nwhen OneR is deployed while as without OneR, and for CDMC2012, dataset = for both with and without OneR."}, {"heading": "5. Application on Intrusion Detection", "text": ""}, {"heading": "5.1 Detection Efficiency", "text": "The acceleration of detection is our major contribution in this paper. The test results about it are shown in Fig.10 and Fig.11 for both KDD99 and CDMC2012 with regard to our horizontal and vertical compression. Please note that baselines in Fig.10 and Fig.11, which do not employ the vertical compression but horizontal compression may be used, are drawn as well for a comparison purpose. By observing the two graphs, we can conclude that:\na. There is an obvious improvement of speed due to our horizontal compression when\nwe compare every blue line with the dashed red line in each graph, where the maximal speed-up factor reaches to around 4 in KDD99 with KNN method. b. Given horizontal compression, there is furthermore an obvious improvement of\nspeed due to our vertical compression when we compare the red horizontal line with the red slash lines in each graph, where the maximal speed-up factor reaches to around 8 in KDD99 with KNN method. c. It is reasonable to see from figures that the smaller amount of clusters (namely\nnumber of items as shown in these two figures) is, the shorter time it will take for detection for both KNN and SVM. One may tend to use smaller number of training instances to meet the need of real time if only the detection accuracy can be satisfied. But of course, as only if the MapReduce parallelization proposed in section 3 is employed for model compression, very large number of training instances can also be conducted to generate the clusters for detection usage.\n(a) KNN (b) SVM\nFig.10 Comparative results of detection time with KNN and SVM respectively for KDD99\n0 500 1000 1500 2000 2500 3000 3500 4000\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n2900\n3000\nT i m e ( s )\nNumber of items\nWithout OneR With OneR KNN (AP) KNN (K-means) KNN (Base)\n0 500 1000 1500 2000 2500 3000 3500 4000\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\nT i m e ( s )\nNumber of items\nWithout OneR With OneR SVM (AP) SVM (K-means) SVM (Base)\n(a) KNN (b) SVM\nFig.11 Comparative results of detection time with KNN and SVM respectively for CDMC2012\nSpecifically, if we consider the blue baseline without any compression and the red resulting record with our full compressed model, the speedup ratio can reach to around 132 times (172.363 versus 1.3) for CDMC2012 and almost 184 times (2907.775 versus 15.718) for KDD99 when the simple KNN is directly used as the detection method."}, {"heading": "5.2 Detection Accuracy", "text": "Although the resulting efficiency that has been empirically proved as aforementioned is our major focus in this paper, the detection accuracy is another important factor for a real-world detection application. However, we will concentrate on the difference of detection accuracy (with and without the compressed model) instead of the direct detection accuracy, because our compressed model is not proposed to contribute to the accuracy improvement, and the detection methods which are related to the direct detection accuracy are still the common ones. We would like to survey the difference with two standard measures here, namely recall and false positive rate. Recall is the ratio between the number of correctly detected anomalies and the total number of anomalies. False positive rate is the ratio between the number of data records from normal class that are misclassified as anomalies and the total number of data records from normal class. Table 3 and Table 4 record the recall and the false positive rate (not the relative difference) respectively for KDD99, and results of CDMC2012 are shown in Table 5 and Table 6. The last row of each table below is the benchmark without vertical compression(8,000 and 5,000 instances are selected from KDD99 and CDMC2012 respectively for experimental purpose).\n0 500 1000 1500 2000 2500\n0\n10\n20\n30\n40\n50\n60\n70\n150\n160\n170\n180\nT i m e ( s )\nNumber of items\nWithout OneR With OneR KNN (AP) KNN (K-means) KNN (Base)\n0 500 1000 1500 2000 2500\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nT i m e ( s )\nNumber of items\nWithout OneR With OneR SVM (AP) SVM (K-means) SVM (Base)\n(a) With horizontal compression (b) Without horizontal compression\nFPR\nNum\nKNN SVM\nAP K-means AP K-means\n69 2.1786 2.1581 2.3042 2.2881 120 2.2735 2.0894 2.3042 2.2808 212 2.1815 2.0617 2.3042 2.2589 326 2.0967 2.0295 2.2881 2.1800 562 1.9798 1.7928 2.2750 2.2443 689 1.9842 1.8162 2.2750 2.2443\nFPR\nNum\nKNN SVM\nAP K-means AP K-means\n65 1.1338 1.1163 2.2560 2.1625 120 1.1455 0.7715 2.3042 1.9740 216 1.1163 0.8358 2.2911 1.9959 326 1.1178 0.0801 2.2677 2.1376 526 1.1192 0.8022 2.2662 2.2691 682 1.1222 0.7905 2.2691 2.2721\n876 1.9258 1.7840 2.2443 2.2443 1215 1.9535 1.5079 2.2794 2.2443 1618 1.8907 1.5108 2.2487 2.2443 2056 1.8717 1.2946 2.2443 2.2443 2512 1.8498 1.3004 2.2443 2.2443 5000 1.8279 2.2443\n886 1.1178 1.0243 2.2735 2.2428 1260 1.1178 1.0491 2.2428 2.2428 1634 1.1178 1.0184 2.2443 2.2428 2074 1.1178 0.5611 2.2458 2.2443 2342 1.1002 0.5625 2.2443 2.2443 5000 1.0929 2.2531\nAccording to these tables, we can find that both of two measures (recall and false positive rate) from the k-means have a larger difference with that of the benchmark compared to the ones resulting from the affinity propagation (averaged at less than 1% for all the cases) which is actually the cornerstone of our work."}, {"heading": "6. Conclusion and Future Work", "text": "We have proposed a compressed model for intrusion detection, including horizontal compression and vertical compression on training data. To be specific, the first endeavor in this paper is to horizontally select useful attributes of the training data through efficient method of OneR, and then vertically extract the representative data as exemplars from a larger dataset through the novel clustering method of AP. Based on the compressed model on training dataset, we finally studied KNN and SVM as two detection methods and empirically identified all parameters of AP for model compression and SVM for classification. Comprehensive experiments on intrusion detection datasets have been conducted to demonstrate the high performance resulting from the compressed model we proposed. In the best case, it runs 184 times faster than the traditional one without our model compression, and neither the recall (detection rate) nor the false positive rate sacrifices only in a very small range with less than 1% on average which can be tolerable or ignorable compared to the big efficiency improvement. Another important contribution is that we have implemented a MapReduce based parallelization framework for each step of the model compression procedure. Actually, we have made efforts to improve both the performance and scalability of compression on large volume of training data by MapReduce-based parallelization. Experimental analysis have been conducted to show that our model compression framework can handle more than 10,000 data points in minutes using 8 commodity PC nodes, while the memory of one general PC can only process not exceeding 5,000 instances using AP clustering.\nThe main merits of our works on intrusion detection are twofold. One is to initially introduce a compressed model of training data space for efficient intrusion detection which focuses on both efficiency and accuracy. The other is, to the best of our knowledge, the first effort of MapReduce based implementation to do the parallel and scalable compression on large amount of training data. However, there are also two open problems left to be solved in our approach, the ability of incremental compression processing and the performance refinement of MapReduce-based parallel computing framework.\nAs the future work, we will further develop our approach in two ways. One is to explore the study on incremental clustering ability of AP[51], as in order to efficiently handle the incremental clustering on network intrusion training data. Another, im-\nprovement on the performance of MapReduce based parallel computing is a long-term goal, especially to study on the iteration mechanism employing some latest parallelization frameworks such as SPARK[52]. We hope our methodology proposed in this paper can finally be served as a general reference model to meet kinds of security mining challenge from big data systems rather than intrusion detection[53]."}, {"heading": "Acknowledgements", "text": "This work was supported by the Natural Science Foundation of China with grant No.60113044, Zhejiang Provincial Science and Technology Project with grant No.2013C31G2020218. Also the authors would like to thank Dr.Rongsheng Gong and Prof.Samuel H. Huang from the Intelligent Systems Laboratory at University of Cincinnati for their valuable comments on this paper."}], "references": [{"title": "Anomaly Detection: A Survey", "author": ["C. Varun", "B. Arindam", "K. Vipin"], "venue": "ACM Computing Surveys,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Clustering by Passing Messages between Data Points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, 315(5814),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Snort: Lightweight Intrusion Detection for Networks", "author": ["R. Martin"], "venue": "USENIX Systems Administration Conference \u2013 LISA,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Data preprocessing for anomaly based network intrusion detection: A review[J", "author": ["J Davis J", "J. Clark A"], "venue": "Computers & Security,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "A framework for constructing features and models for intrusion detection systems[J", "author": ["W Lee", "J. Stolfo S"], "venue": "ACM transactions on Information and system security (TiSSEC),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Intrusion detection system: A comprehensive review[J", "author": ["J Liao H", "Y Tung K", "H Richard Lin C"], "venue": "Journal of Network and Computer Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Data Mining Approaches for Intrusion Detection", "author": ["L. Wenke", "J.S. Salvatore"], "venue": "USENIX Security Symposium", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Feature Selection for Intrusion Detection Using Neural Networks and Support Vector Machines", "author": ["M. Srinivas", "H.S. Andrew"], "venue": "Annual Meeting of the Transportation Research Board", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Feature Duduction and Ensemble Design of Intrusion Detection Systems. Computer & Security \u2013 COMPSEC", "author": ["C. Srilatha", "A. Ajith", "P.T. Johnson"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Feature selection and classification in multiple class datasets: An application to KDD Cup 99 dataset[J", "author": ["V Bol\u00f3n-Canedo", "N S\u00e1nchez-Maro\u00f1o", "A. Alonso-Betanzos"], "venue": "Expert Systems with Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A hierarchical intrusion detection model based on the PCA neural networks. Neurocomputing,70(7-9), 1561-1568", "author": ["G. Liu", "Z. Yi", "S. Yang"], "venue": "Advances in computational intelligence and learning in 14th European symposium on artificial neural networks,March", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Optimized intrusion detection mechanism using soft computing techniques[J", "author": ["I Ahmad", "A Abdullah", "A Alghamdi"], "venue": "Telecommunication Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Selecting Features for Anomaly Intrusion Detection: A Novel Method using Fuzzy C Means and Decision Tree Classification[M", "author": ["J Song", "Z Zhu", "P Scully"], "venue": "Cyberspace Safety and Security. Springer International Publishing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Mutual information-based feature selection for intrusion detection systems[J", "author": ["F Amiri", "M Rezaei Yousefi M", "C Lucas"], "venue": "Journal of Network and Computer Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "A novel intrusion detection system based on feature generation with visualization strategy[J", "author": ["B Luo", "J. Xia"], "venue": "Expert Systems with Applications,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "An efficient intrusion detection system based on support vector machines and gradually feature removal method[J", "author": ["Y Li", "J Xia", "S Zhang"], "venue": "Expert Systems with Applications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "A Signal Processing View on Packet Sampling and Anomaly Detection", "author": ["B. Daniela", "S. Kav\u00e9", "M. Margin"], "venue": "IEEE INFOCOM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Optimizing Network Anomaly Detection Scheme Using Instance Selection Mechanism", "author": ["L. Yang", "L. Tian-Bo", "G. Li", "T. Zhi-Hong", "Q. Lin"], "venue": "Global Telecommunications Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Improving the Intrusion Detection Systems Performance by Correlation as a Sample Selection Method[J", "author": ["R Rouhi", "F Keynia", "M. Amiri"], "venue": "Journal of Computer Sciences and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Gnort: High Performance Network Intrusion Detection Using Graphics Processors[J", "author": ["V. Giorgos", "A. Spyros", "P. Michalis", "P.M. Evangelos", "L. Sotiris"], "venue": "Recent Advances in Intrusion Detection,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Scalable high-performance parallel design for network intrusion detection systems on many-core processors[C].Proceedings of the ninth ACM/IEEE symposium on Architectures for networking and communications systems", "author": ["H Jiang", "G Zhang", "G Xie"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Building Scalable Distributed Intrusion Detection Systems Based on the MapReduce Framework[J", "author": ["D Holtz M", "M David B", "T. de Sousa J\u00fanior R"], "venue": "REVISTA Telecomunicacoes,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Design of a Hybrid Intrusion Detection System using Snort and Hadoop[J", "author": ["P. PG"], "venue": "International Journal of Computer Applications,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Toolkit-based high-performance Data Mining of large Data on MapReduce Clusters[C", "author": ["D Wegener", "M Mock", "D Adranale"], "venue": "Data Mining Workshops,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Alert correlation in collaborative intelligent intrusion detection systems\u2014A survey[J", "author": ["T Elshoush H", "M. Osman I"], "venue": "Applied Soft Computing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Using adaptive alert classification to reduce false positives in intrusion detection[C]. Recent Advances in Intrusion Detection", "author": ["T. Pietraszek"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Reducing false positives in intrusion detection", "author": ["P Spathoulas G", "K. Katsikas S"], "venue": "systems[J]. computers & security,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Adaptive False Alarm Filter Using Machine Learning in Intrusion Detection[M", "author": ["Y. Meng"], "venue": "Practical Applications of Intelligent Systems. Springer Berlin Heidelberg,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Anomaly network intrusion detection based on improved self adaptive bayesian algorithm[J", "author": ["M Farid D", "Z. Rahman M"], "venue": "Journal of computers,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Research of Intrusion Detection Based on Clustering Analysis[C].Proceedings of the 2012 International Conference on Cybernetics and Informatics", "author": ["M Wei", "L Xia", "J Jin"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "A novel intrusion detection system based on hierarchical clustering and support vector machines[J", "author": ["J Horng S", "Y Su M", "H Chen Y"], "venue": "Expert systems with Applications,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Intrusion Detection with K-Means Clustering and OneR Classification[J", "author": ["Z Muda", "W Yassin", "N Sulaiman M"], "venue": "Journal of Information Assurance & Security,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "A new intrusion detection system using support vector machines and hierarchical clustering[J", "author": ["L Khan", "M Awad", "B. Thuraisingham"], "venue": "The VLDB Journal-The International Journal on Very Large Data Bases,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Neural networks learning improvement using the K-means clustering algorithm to detect network intrusions[J", "author": ["M Faraoun K", "A. Boukelif"], "venue": "International Journal of Computational Intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "A triangle area based nearest neighbors approach to intrusion detection[J", "author": ["F Tsai C", "Y. Lin C"], "venue": "Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "A novel intrusion detection system based on hierarchical clustering and support vector machines[J", "author": ["J Horng S", "Y Su M", "H Chen Y"], "venue": "Expert systems with Applications,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "A differentiated one-class classification method with applications to intrusion detection[J", "author": ["I Kang", "K Jeong M", "D. Kong"], "venue": "Expert Systems with Applications,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "High-speed web attack detection through extracting exemplars from HTTP traffic[C", "author": ["W Wang", "X. Zhang"], "venue": "Proceedings of the 2011 ACM Symposium on Applied Computing. ACM,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Abstracting audit data for lightweight intrusion detection[M", "author": ["W Wang", "X Zhang", "G. Pitsilis"], "venue": "Information Systems Security. Springer Berlin Heidelberg,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Online stream clustering using density and affinity propagation algorithm[C", "author": ["P Zhang J", "C Chen F", "X Liu L"], "venue": "4th IEEE International Conference on Software Engineering and Service Science", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Finding exemplars in dense data with affinity propagation on clusters of GPUs[J", "author": ["M Kurdziel", "K. Boryczko"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Scalable high-performance parallel design for network intrusion detection systems on many-core processors[C]. Proceedings of the ninth ACM/IEEE symposium on Architectures for networking and communications systems", "author": ["H Jiang", "G Zhang", "G Xie"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Latent outlier detection and the low precision problem[C", "author": ["F Wang", "S Chawla", "D. Surian"], "venue": "Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Towards a scalable intrusion detection system based on parallel PSO clustering using mapreduce[C]. Proceeding of the fifteenth annual conference companion on Genetic and evolutionary computation conference companion", "author": ["I Aljarah", "A. Ludwig S"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Very Simple Classification Rules Perform Well on Most Commonly Used Datasets", "author": ["R.C. Holte"], "venue": "Machine learning", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1993}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transac-tions on Intelligent Systems and Technology (TIST)", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "Incremental Affinity Propagation Clustering Based on Message Passing[J", "author": ["L Sun", "C. Guo"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "The family of MapReduce and large-scale data processing systems[J", "author": ["S Sakr", "A Liu", "G. Fayoumi A"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Big Data Analytics for Security[J", "author": ["A Cardenas A", "K Manadhata P", "P. Rajan S"], "venue": "IEEE Security & Privacy,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, the solution presented in this paper is applicable for those model-based anomaly detection approaches, especially for the classification based system[1] because extracting the classification model directly from the huge volume of training data instances inevitably needs intense computation.", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "That means, for some novel but promising data processing algorithms such like Affinity Propagation[2], the general computing memory would likely explode when the dimensionality of training data matrix increases to some large extent.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Snort [5]) relies on the knowledge of system vulnerabilities and known attack patterns, which hence is unable to detect unknown attacks.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "Generally, an anomaly-based intrusion detection system includes following steps, data gathering, data preprocessing[6], model building[7], and model-based detection[8].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Generally, an anomaly-based intrusion detection system includes following steps, data gathering, data preprocessing[6], model building[7], and model-based detection[8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Generally, an anomaly-based intrusion detection system includes following steps, data gathering, data preprocessing[6], model building[7], and model-based detection[8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "In 1998, Lee and Stolfo [9] published a data mining approach for the intrusion detection, where they proposed a framework for the agent-based intrusion detection, and", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "For example, Sung[10] improved the detection speed by extracting the useful subset of attributes with ANN and SVM, and Srilatha[11] investigated the performance of Bayesian networks (BN) and Classification and Regression Trees (CART) to build lightweight IDS.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "For example, Sung[10] improved the detection speed by extracting the useful subset of attributes with ANN and SVM, and Srilatha[11] investigated the performance of Bayesian networks (BN) and Classification and Regression Trees (CART) to build lightweight IDS.", "startOffset": 127, "endOffset": 131}, {"referenceID": 9, "context": "So, attribute reduction or feature selection is the most popular method to improve detection efficiency by directly reducing the data attribute dimension[12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "PCA is a widely used criteria to select features for intrusion detection, which can be usually incorporated with other soft computing models, such as neural networks[13], genetic algorithms[14], etc.", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "PCA is a widely used criteria to select features for intrusion detection, which can be usually incorporated with other soft computing models, such as neural networks[13], genetic algorithms[14], etc.", "startOffset": 189, "endOffset": 193}, {"referenceID": 12, "context": "Some novel methods are therefore proposed recently that employ diverse intelligent approaches to reduce features, such as fuzzy C means[15], mutual information definition[16], graph visualization technique[17], gradually removal method[18], etc.", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "Some novel methods are therefore proposed recently that employ diverse intelligent approaches to reduce features, such as fuzzy C means[15], mutual information definition[16], graph visualization technique[17], gradually removal method[18], etc.", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "Some novel methods are therefore proposed recently that employ diverse intelligent approaches to reduce features, such as fuzzy C means[15], mutual information definition[16], graph visualization technique[17], gradually removal method[18], etc.", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "Some novel methods are therefore proposed recently that employ diverse intelligent approaches to reduce features, such as fuzzy C means[15], mutual information definition[16], graph visualization technique[17], gradually removal method[18], etc.", "startOffset": 235, "endOffset": 239}, {"referenceID": 16, "context": "One is to sample the data as a training dataset instead of considering the full dataset, for example the random data sampling method[19] was proposed to deal with the massive traffic, and the fuzzy C-means[20] was also employed for the purpose of selecting smaller number of training data.", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "One is to sample the data as a training dataset instead of considering the full dataset, for example the random data sampling method[19] was proposed to deal with the massive traffic, and the fuzzy C-means[20] was also employed for the purpose of selecting smaller number of training data.", "startOffset": 205, "endOffset": 209}, {"referenceID": 18, "context": "However, the biggest problem of sampling methods is that it may lose potentially useful data which are very distinguishable for detection[21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "With the development of the hardware, Giorgos firstly made efforts to improve the detection speed by applying graphics processors[22], followed by the improved works in paper [23] which focused on the scalability of analyzing large data except speed acceleration.", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "With the development of the hardware, Giorgos firstly made efforts to improve the detection speed by applying graphics processors[22], followed by the improved works in paper [23] which focused on the scalability of analyzing large data except speed acceleration.", "startOffset": 175, "endOffset": 179}, {"referenceID": 21, "context": "At very current, MapReduce based cloud computing frameworks start to be studied to build efficient intrusion detection systems[24,25].", "startOffset": 126, "endOffset": 133}, {"referenceID": 22, "context": "At very current, MapReduce based cloud computing frameworks start to be studied to build efficient intrusion detection systems[24,25].", "startOffset": 126, "endOffset": 133}, {"referenceID": 23, "context": "Actually, a general data mining toolkit using MapReduce has been integrated with Weka[26], and there also developed a new toolkit on Hadoop, Mahout[27],where most classical data mining algorithms are all parallelized and distributed.", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "As to reduce the false alarm rate, alert correlation algorithms are integrated with the existing feature selection methods to automatically generate a more concise feature set to improve the overall precision of intrusion detection[28,29,30].", "startOffset": 231, "endOffset": 241}, {"referenceID": 25, "context": "As to reduce the false alarm rate, alert correlation algorithms are integrated with the existing feature selection methods to automatically generate a more concise feature set to improve the overall precision of intrusion detection[28,29,30].", "startOffset": 231, "endOffset": 241}, {"referenceID": 26, "context": "As to reduce the false alarm rate, alert correlation algorithms are integrated with the existing feature selection methods to automatically generate a more concise feature set to improve the overall precision of intrusion detection[28,29,30].", "startOffset": 231, "endOffset": 241}, {"referenceID": 27, "context": "Recently, some other machine learning-based adaptive methods are also proposed to purify the false alarm[31,32].", "startOffset": 104, "endOffset": 111}, {"referenceID": 28, "context": "Recently, some other machine learning-based adaptive methods are also proposed to purify the false alarm[31,32].", "startOffset": 104, "endOffset": 111}, {"referenceID": 29, "context": "Other than feature selection, clustering has been widely used as hybrid machine learning approach to improve the detection rate[33].", "startOffset": 127, "endOffset": 131}, {"referenceID": 30, "context": "Clustering is usually used as the first step to filter out unrepresentative data, while classification is used later for detection[34,35].", "startOffset": 130, "endOffset": 137}, {"referenceID": 31, "context": "Clustering is usually used as the first step to filter out unrepresentative data, while classification is used later for detection[34,35].", "startOffset": 130, "endOffset": 137}, {"referenceID": 32, "context": "So, the final clustered data, also called representative data or exemplars, are possibly without the noisy data and could be better used to train the classifier to improve the detection rate[36].", "startOffset": 190, "endOffset": 194}, {"referenceID": 31, "context": "As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39].", "startOffset": 79, "endOffset": 89}, {"referenceID": 33, "context": "As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39].", "startOffset": 79, "endOffset": 89}, {"referenceID": 34, "context": "As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39].", "startOffset": 79, "endOffset": 89}, {"referenceID": 31, "context": "As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39].", "startOffset": 176, "endOffset": 189}, {"referenceID": 32, "context": "As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39].", "startOffset": 176, "endOffset": 189}, {"referenceID": 34, "context": "As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39].", "startOffset": 176, "endOffset": 189}, {"referenceID": 35, "context": "As for clustering, k-means is the most popular solution for intrusion detection[35,37,38], while k-NN and SVM are two commonly accepted models for classification in this domain[35,36,38,39].", "startOffset": 176, "endOffset": 189}, {"referenceID": 34, "context": "Moreover, improvement researches on k-NN and SVM are still continuingly active to raise the detection precision[38,40].", "startOffset": 111, "endOffset": 118}, {"referenceID": 36, "context": "Moreover, improvement researches on k-NN and SVM are still continuingly active to raise the detection precision[38,40].", "startOffset": 111, "endOffset": 118}, {"referenceID": 1, "context": "As a new study, recent researches show that a novel clustering method, affinity propagation[2], can well be used to generate the good representative data for intrusion detection training[41,42].", "startOffset": 91, "endOffset": 94}, {"referenceID": 37, "context": "As a new study, recent researches show that a novel clustering method, affinity propagation[2], can well be used to generate the good representative data for intrusion detection training[41,42].", "startOffset": 186, "endOffset": 193}, {"referenceID": 38, "context": "As a new study, recent researches show that a novel clustering method, affinity propagation[2], can well be used to generate the good representative data for intrusion detection training[41,42].", "startOffset": 186, "endOffset": 193}, {"referenceID": 39, "context": "Also, the AP clustering method is already employed to well apply on network stream data processing to improve the detection correctness[43].", "startOffset": 135, "endOffset": 139}, {"referenceID": 40, "context": "Therefore, research on parallelization of AP is also introduced recently, for example, GPU-based parallelization of AP has been freshly developed to speed up the clustering[44].", "startOffset": 172, "endOffset": 176}, {"referenceID": 41, "context": "However, like the traditional parallel algorithms, GPU-based approaches still challenge some intrinsic problems such as shared communication, load balancing, etc[45].", "startOffset": 161, "endOffset": 165}, {"referenceID": 42, "context": "Clustering is very useful to find the outliers for intrusion detection[46], especially AP clustering is one promising method on exemplars selection for intrusion detection[41,42,43].", "startOffset": 70, "endOffset": 74}, {"referenceID": 37, "context": "Clustering is very useful to find the outliers for intrusion detection[46], especially AP clustering is one promising method on exemplars selection for intrusion detection[41,42,43].", "startOffset": 171, "endOffset": 181}, {"referenceID": 38, "context": "Clustering is very useful to find the outliers for intrusion detection[46], especially AP clustering is one promising method on exemplars selection for intrusion detection[41,42,43].", "startOffset": 171, "endOffset": 181}, {"referenceID": 39, "context": "Clustering is very useful to find the outliers for intrusion detection[46], especially AP clustering is one promising method on exemplars selection for intrusion detection[41,42,43].", "startOffset": 171, "endOffset": 181}, {"referenceID": 21, "context": "Cloud computing is a transparent framework to implement parallelization of data processing algorithms, especially MapReduce becomes a promising approach towards to scalable intrusion detection system[24,25,47].", "startOffset": 199, "endOffset": 209}, {"referenceID": 22, "context": "Cloud computing is a transparent framework to implement parallelization of data processing algorithms, especially MapReduce becomes a promising approach towards to scalable intrusion detection system[24,25,47].", "startOffset": 199, "endOffset": 209}, {"referenceID": 43, "context": "Cloud computing is a transparent framework to implement parallelization of data processing algorithms, especially MapReduce becomes a promising approach towards to scalable intrusion detection system[24,25,47].", "startOffset": 199, "endOffset": 209}, {"referenceID": 0, "context": "To balance the contribution of every attribute in the similarity calculation, we first normalize the data to the scale of [0, 1] through the following formula (1).", "startOffset": 122, "endOffset": 128}, {"referenceID": 44, "context": "Here we choose OneR [48] as it is very easy to understand but with very high efficiency.", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "For the vertical compression purpose, we here employ the latest published but promising clustering approach, affinity propagation[2], which is actually a cornerstone through full of our idea.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "8 according to experimental analysis[2].", "startOffset": 36, "endOffset": 39}, {"referenceID": 45, "context": "LibSVM [50] provides an automatic implementation on grid-searching and using cross-validation, through which the best parameters for our training data can be concluded as following: for KDD99 dataset, the best when OneR is deployed while as without OneR, and for CDMC2012, dataset = for both with and without OneR.", "startOffset": 7, "endOffset": 11}, {"referenceID": 46, "context": "One is to explore the study on incremental clustering ability of AP[51], as in order to efficiently handle the incremental clustering on network intrusion training data.", "startOffset": 67, "endOffset": 71}, {"referenceID": 47, "context": "provement on the performance of MapReduce based parallel computing is a long-term goal, especially to study on the iteration mechanism employing some latest parallelization frameworks such as SPARK[52].", "startOffset": 197, "endOffset": 201}, {"referenceID": 48, "context": "We hope our methodology proposed in this paper can finally be served as a general reference model to meet kinds of security mining challenge from big data systems rather than intrusion detection[53].", "startOffset": 194, "endOffset": 198}], "year": 2014, "abstractText": "In order to achieve high efficiency of classification in intrusion detection, a compressed model is proposed in this paper which combines horizontal compression with vertical compression. OneR is utilized as horizontal compression for attribute reduction, and affinity propagation is employed as vertical compression to select small representative exemplars from large training data. As to be able to computationally compress the larger volume of training data with scalability, MapReduce based parallelization approach is then implemented and evaluated for each step of the model compression process abovementioned, on which common but efficient classification methods can be directly used. Experimental application study on two publicly available datasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at up to 184 times, most importantly at the cost of a minimal accuracy difference with less than 1% on average.", "creator": "Microsoft\u00ae Word 2010"}}}