{"id": "1405.5096", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2014", "title": "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms", "abstract": "we form stochastic multi - armed bandits where the expected result is a unimodal function over partially ordered arms. this important class of losers has been slightly investigated in ( cope 2009, yu 2011 ). the set of arms is either discrete, in which case arms correspond to the vertices of a total graph whose structure represents vertices in rewards, or continuous, in which case arms belong to a normal polynomial. for discrete unimodal bandits, we avoid asymptotic lower bounds for the regret achieved under any algorithm, please propose osub, an algorithm whose regret matches this lower bound. however algorithm optimally exploits the unimodal elements of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. we also provide a regret upper bound for osub in two - stationary environments where the expected rewards smoothly changes over time. the analytical results are supported by numerical experiments showing that osub properties perform better than the state - of - your - art algorithms. reviewing continuous sets of criminals, we share a brief discussion. we show that combining an appropriate discretization of the set of arms with the ucb algorithm yields its incentive - optimal regret, and in practice, outperforms recently solved algorithms designed should exploit the unimodal structure.", "histories": [["v1", "Tue, 20 May 2014 14:15:54 GMT  (63kb)", "http://arxiv.org/abs/1405.5096v1", "ICML 2014 (technical report). arXiv admin note: text overlap witharXiv:1307.7309"]], "COMMENTS": "ICML 2014 (technical report). arXiv admin note: text overlap witharXiv:1307.7309", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["richard combes", "alexandre prouti\u00e8re"], "accepted": true, "id": "1405.5096"}, "pdf": {"name": "1405.5096.pdf", "metadata": {"source": "META", "title": "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms", "authors": ["Richard Combes", "Alexandre Proutiere"], "emails": ["RCOMBES@KTH.SE", "ALEPRO@KTH.SE"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n50 96\nv1 [\ncs .L\nG ]\n2 0\nM ay\nWe consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in nonstationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure.\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s)."}, {"heading": "1. Introduction", "text": "Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs. exploitation trade-off. In such problems, the decision maker selects an arm in each round, and observes a realization of the corresponding unknown reward distribution. Each decision is based on past decisions and observed rewards. The objective is to maximize the expected cumulative reward over some time horizon by balancing exploitation (arms with higher observed rewards should be selected often) and exploration (all arms should be explored to learn their average rewards). Equivalently, the performance of a decision rule or algorithm can be measured through its expected regret, defined as the gap between the expected reward achieved by the algorithm and that achieved by an oracle algorithm always selecting the best arm. MAB problems have found many fields of application, including sequential clinical trials, communication systems, economics, see e.g. (Cesa-Bianchi & Lugosi, 2006; Bubeck & Cesa-Bianchi, 2012).\nIn their seminal paper (Lai & Robbins, 1985), Lai and Robbins solve MAB problems where the successive rewards of a given arm are i.i.d., and where the expected rewards of the various arms are not related. They derive an asymptotic (when the time horizon grows large) lower bound of the regret satisfied by any algorithm, and present an algorithm whose regret matches this lower bound. This initial algorithm was quite involved, and many researchers have tried to devise simpler and yet efficient algorithms. The most popular of these algorithms are UCB (Auer et al., 2002) and its extensions, e.g. KL-UCB (Garivier & Cappe\u0301, 2011; Cappe\u0301 et al., 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.6)). When the expected rewards of the various arms are not related (Lai & Robbins, 1985), the regret of the best algorithm is essentially of the order O(K log(T )) where K denotes the number of arms, and T is the time horizon. When\nK is very large or even infinite, MAB problems become more challenging. Fortunately, in such scenarios, the expected rewards often exhibit some structural properties that the decision maker can exploit to design efficient algorithms. Various structures have been investigated in the literature, e.g., Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al., 2008), convex (Flaxman et al., 2005).\nWe consider bandit problems where the expected reward is a unimodal function over partially ordered arms as in (Yu & Mannor, 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. This unimodal structure occurs naturally in many practical decision problems, such as sequential pricing (Yu & Mannor, 2011) and bidding in online sponsored search auctions (B., 2005).\nOur contributions. We mainly investigate unimodal bandits with finite sets of arms, and are primarily interested in cases where the time horizon T is much larger than the number of arms K .\n(a) For these problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This lower bound does not depend on the structure of the graph, nor on its size: it actually corresponds to the regret lower bound in a classical bandit problem (Lai & Robbins, 1985), where the set of arms is just a neighborhood of the best arm in the graph.\n(b) We propose OSUB (Optimal Sampling for Unimodal Bandits), a simple algorithm whose regret matches our lower bound, i.e., it optimally exploits the unimodal structure. The asymptotic regret of OSUB does not depend on the number of arms. This contrasts with LSE (Line Search Elimination), the algorithm proposed in (Yu & Mannor, 2011) whose regret scales as O(\u03b3D log(T )) where \u03b3 is the maximum degree of vertices in the graph and D is its diameter. We present a finite-time analysis of OSUB, and derive a regret upper bound that scales as O(\u03b3 log(T )+K). Hence OSUB offers better performance guarantees than LSE as soon as the time horizon satisfies T \u2265 exp(K/\u03b3D). Although this is not explicitly mentioned in (Yu & Mannor, 2011), we believe that LSE was meant to address bandits where the number of arms is not negligible compared to the time horizon.\n(c) We further investigate OSUB performance in nonstationary environments where the expected rewards smoothly evolve over time but keep their unimodal structure.\n(d) We conduct numerical experiments and show that OSUB significantly outperforms LSE and other classi-\ncal bandit algorithms when the number of arms is much smaller than the time horizon.\n(e) Finally, we briefly discuss systems with a continuous set of arms. We show that using a simple discretization of the set of arms, UCB-like algorithms are order-optimal, and actually outperform more advanced algorithms such as those proposed in (Yu & Mannor, 2011). This result suggests that in discrete unimodal bandits with a very large number of arms, it is wise to first prune the set of arms, so as to reduce its size to a number of the order of \u221a T/ log(T )."}, {"heading": "2. Related work", "text": "Unimodal bandits have received relatively little attention in the literature. They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008). In this paper, we add unimodality and show how this structure can be optimally exploited. Unimodal bandits have been specifically addressed in (Cope, 2009; Yu & Mannor, 2011). In (Cope, 2009), bandits with a continuous set of arms are studied, and the author shows that the Kiefer-Wolfowitz stochastic approximation algorithm achieves a regret of the order of O( \u221a T ) under some strong regularity assumptions on the reward function. In (Yu & Mannor, 2011), for the same problem, the authors present LSE, an algorithm whose regret scales as O( \u221a T log(T )) without the need for a strong regularity assumption. The LSE algorithm is based on Kiefer\u2019s golden section search algorithm. It iteratively eliminates subsets of arms based on PAC-bounds derived after appropriate sampling. By design, under LSE, the sequence of parameters used for the PAC bounds is pre-defined, and in particular does not depend of the observed rewards. As a consequence, LSE may explore too much sub-optimal parts of the set of arms. For bandits with a continuum set of arms, we actually show that combining an appropriate discretization of the decision space (i.e., reducing the number of arms to \u221a T/ log(T ) arms) and the UCB algorithm can outperform LSE in practice (this is due to the adaptive nature of UCB). Note that the parameters used in LSE to get a regret of the order O( \u221a T log(T )) depend on the time horizon T .\nIn (Yu & Mannor, 2011), the authors also present an extension of the LSE algorithm to problems with discrete sets of arms, and provide regret upper bounds of this algorithm. These bounds depends on the structure of the graph defining unimodal structure, and on the number of arms as mentioned previously. LSE performs better than classical bandit algorithms only when the number of arms is very large, and actually becomes comparable to the time horizon. Here we are interested in bandits with relatively small number of arms.\nNon-stationary bandits have been studied in\n(Hartland et al., 2007; Garivier & Moulines, 2008; Slivkins & Upfal, 2008; Yu & Mannor, 2011). Except for (Slivkins & Upfal, 2008), these papers deal with environments where the expected rewards and the best arm change abruptly. This ensures that arms are always well separated, and in turn, simplifies the analysis. In (Slivkins & Upfal, 2008), the expected rewards evolve according to independent brownian motions. We consider a different, but more general class of dynamic environments: here the rewards smoothly evolve over time. The challenge for such environments stems from the fact that, at some time instants, arms can have expected rewards arbitrarily close to each other.\nFinally, we should mention that bandit problems with structural properties such as those we address here can often be seen as specific instances of problems in the control of Markov chains, see (Graves & Lai, 1997). We leverage this observation to derive regret lower bounds. However, algorithms developed for the control of generic Markov chains are often too complex to implement in practice. Our algorithm, OSUB, is optimal and straightforward to implement."}, {"heading": "3. Model and Objectives", "text": "We consider a stochastic multi-armed bandit problem with K \u2265 2 arms. We discuss problems where the set of arms is continuous in Section 6. Time proceeds in rounds indexed by n = 1, 2, . . .. Let Xk(n) be the reward obtained at time n if arm k is selected. For any k, the sequence of rewards (Xk(n))n\u22651 is i.i.d. with distribution and expectation denoted by \u03bdk and \u00b5k respectively. Rewards are independent across arms. Let \u00b5 = (\u00b51, . . . , \u00b5K) represent the expected rewards of the various arms. At each round, a decision rule or algorithm selects an arm depending on the arms chosen in earlier rounds and their observed rewards. We denote by k\u03c0(n) the arm selected under \u03c0 in round n. The set \u03a0 of all possible decision rules consists of policies \u03c0 satisfying: for any n \u2265 1, if F\u03c0n is the \u03c3-algebra generated by (k\u03c0(t), Xk\u03c0(t)(t))1\u2264t\u2264n, then k\u03c0(n+1) is F\u03c0n -measurable."}, {"heading": "3.1. Unimodal Structure", "text": "The expected rewards exhibit a unimodal structure, similar to that considered in (Yu & Mannor, 2011). More precisely, there exists an undirected graph G = (V,E) whose vertices correspond to arms, i.e., V = {1, . . . ,K}, and whose edges characterize a partial order (initially unknown to the decision maker) among expected rewards. We assume that there exists a unique arm k\u22c6 with maximum expected reward \u00b5\u22c6, and that from any sub-optimal arm k 6= k\u22c6, there exists a path p = (k1 = k, . . . , km = k\u22c6) of length m (depending on k) such that for all i = 1, . . . ,m\u2212 1, (ki, ki+1) \u2208 E and \u00b5ki < \u00b5ki+1 . We denote by UG the set of vectors \u00b5 satisfying this unimodal structure.\nThis notion of unimodality is quite general, and includes, as a special case, classical unimodality (where G is just a line). Note that we assume that the decision maker knows the graph G, but ignores the best arm, and hence the partial order induced by the edges of G."}, {"heading": "3.2. Stationary and non-stationary environments", "text": "The model presented above concerns stationary environments, where the expected rewards for the various arms do not evolve over time. In this paper, we also consider non-stationary environments where these expected rewards could evolve over time according to some deterministic dynamics. In such scenarios, we denote by \u00b5k(n) the expected reward of arm k at time n, i.e., E[Xk(n)] = \u00b5k(n), and (Xk(n))n\u22651 constitutes a sequence of independent random variables with evolving mean. In non-stationary environments, the sequences of rewards are still assumed to be independent across arms. Moreover, at any time n, \u00b5(n) = (\u00b51(n), . . . \u00b5K(n)) is unimodal with respect to some fixed graph G, i.e., \u00b5(n) \u2208 UG (note however that the partial order satisfied by the expected rewards may evolve over time)."}, {"heading": "3.3. Regrets", "text": "The performance of an algorithm \u03c0 \u2208 \u03a0 is characterized by its regret up to time T (where T is typically large). The way regret is defined differs depending on the type of environment.\nStationary Environments. In such environments, the regret R\u03c0(T ) of algorithm \u03c0 \u2208 \u03a0 is simply defined through the number of times t\u03c0k (T ) = \u2211\n1\u2264n\u2264T 1{k\u03c0(n) = k} that arm k has been selected up to time T : R\u03c0(T ) = \u2211K\nk=1(\u00b5 \u22c6\u2212\u00b5k)E[t\u03c0k (T )]. Our objectives are (1) to identify an asymptotic (when T \u2192 \u221e) regret lower bound satisfied by any algorithm in \u03a0, and (2) to devise an algorithm that achieves this lower bound.\nNon-stationary Environments. In such environments, the regret of an algorithm \u03c0 \u2208 \u03a0 quantifies how well \u03c0 tracks the best arm over time. Let k\u22c6(n) denote the optimal arm with expected reward \u00b5\u22c6(n) at time n. The regret of \u03c0 up to time T is hence defined as: R\u03c0(T ) = \u2211T\nn=1\n( \u00b5\u22c6(n)\u2212 E[\u00b5k\u03c0(n)(n)] ) ."}, {"heading": "4. Stationary environments", "text": "In this section, we consider unimodal bandit problems in stationary environments. We derive an asymptotic lower bound of regret when the reward distributions belong to a parametrized family of distributions, and propose OSUB, an algorithm whose regret matches this lower bound."}, {"heading": "4.1. Lower bound on regret", "text": "To simplify the presentation, we assume here that the reward distributions belong to a parametrized family of distributions. More precisely, we define a set of distributions V = {\u03bd(\u03b8)}\u03b8\u2208[0,1] parametrized by \u03b8 \u2208 [0, 1]. The expectation of \u03bd(\u03b8) is denoted by \u00b5(\u03b8) for any \u03b8 \u2208 [0, 1]. \u03bd(\u03b8) is absolutely continuous with respect to some positive measure m on R, and we denote by p(x, \u03b8) its density. The Kullback-Leibler (KL) divergence number between \u03bd(\u03b8) and \u03bd(\u03b8\u2032) is: KL(\u03b8, \u03b8\u2032) = \u222b\nR log(p(x, \u03b8)/p(x, \u03b8\u2032))p(x, \u03b8)m(dx). We\ndenote by \u03b8\u22c6 a parameter (it might not be unique) such that \u00b5(\u03b8\u22c6) = \u00b5\u22c6, and we define the minimal divergence number between \u03bd(\u03b8) and \u03bd(\u03b8\u22c6) as: Imin(\u03b8, \u03b8\u22c6) = inf\u03b8\u2208[0,1]:\u00b5(\u03b8\u2032)\u2265\u00b5\u22c6 KL(\u03b8, \u03b8 \u2032).\nFinally, we say that arm k has parameter \u03b8k if \u03bdk = \u03bd(\u03b8k), and we denote by \u0398G the set of all parameters \u03b8 = (\u03b81, . . . , \u03b8K) \u2208 [0, 1]K such that the corresponding expected rewards are unimodal with respect to graph G: \u00b5 = (\u00b51, . . . , \u00b5K) \u2208 UG. Of particular interest is the family of Bernoulli distributions: the support of m is {0, 1}, \u00b5(\u03b8) = \u03b8, and Imin(\u03b8, \u03b8\u22c6) = I(\u03b8, \u03b8\u22c6) where I(\u03b8, \u03b8\u22c6) = \u03b8 log( \u03b8\u03b8\u22c6 ) + (1 \u2212 \u03b8) log( 1\u2212\u03b81\u2212\u03b8\u22c6 ) is KL divergence number between Bernoulli distributions of respective means \u03b8 and \u03b8\u22c6.\nWe are now ready to derive an asymptotic regret lower in parametrized unimodal bandit problems as defined above. Without loss of generality, we restrict our attention to so-called uniformly good algorithms, as defined in (Lai & Robbins, 1985) (uniformly good algorithms exist as shown later on). We say that \u03c0 \u2208 \u03a0 is uniformly good if for all \u03b8 \u2208 \u0398G, we have that R\u03c0(T ) = o(T a) for all a > 0.\nTheorem 4.1 Let \u03c0 \u2208 \u03a0 be a uniformly good algorithm, and assume that \u03bdk = \u03bd(\u03b8k) \u2208 V for all k. Then for any \u03b8 \u2208 \u0398G,\nlim inf T\u2192+\u221e\nR\u03c0(T ) log(T ) \u2265 c(\u03b8) = \u2211\n(k,k\u2217)\u2208E\n\u00b5\u22c6 \u2212 \u00b5k Imin(\u03b8k, \u03b8\u22c6) . (1)\nThe above theorem is a consequence of results in optimal control of Markov chains (Graves & Lai, 1997). All proofs are presented in appendix. As in classical discrete bandit problems, the regret scales at least logarithmically with time (the regret lower bound derived in (Lai & Robbins, 1985) is obtained from Theorem 4.1 assuming that G is the complete graph). We also observe that the unimodal structure, if optimally exploited, can bring significant performance improvements: the regret lower bound does not depend on the size K of the decision space. Indeed c(\u03b8) includes only terms corresponding to arms that are neighbors in G of the optimal arm (as if one could learn without regret that all other arms are sub-optimal).\nIn the case of Bernoulli rewards, the lower regret bound becomes log(T ) \u2211\n(k,k\u2217)\u2208E \u00b5\u22c6\u2212\u00b5k I(\u03b8k,\u03b8\u22c6)\n. Note that LSE and GLSE, the algorithms proposed in (Yu & Mannor, 2011), have performance guarantees that do not match our lower bound: when G is a line, LSE achieves a regret bounded by 41/\u22062 log(T ), whereas in the general case, GLSE incurs a regret of the order of O(\u03b3D log(T )) where \u03b3 is the maximal degree of vertices in G, and D is its diameter. The performance of LSE critically depends on the graph structure, and the number of arms. Hence there is an important gap between the performance of existing algorithms and the lower bound derived in Theorem 4.1. In the next section, we close this gap and propose an asymptotically optimal algorithm."}, {"heading": "4.2. The OSUB Algorithm", "text": "We now describe OSUB, a simple algorithm whose regret matches the lower bound derived in Theorem of 4.1 for Bernoulli rewards, i.e., OSUB is asymptotically optimal. The algorithm is based on KL-UCB proposed in (Lai, 1987; Cappe\u0301 et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm. OSUB can be readily extended to systems where reward distributions are within one-parameter exponential families by simply modifying the definition of arm indices as done in (Cappe\u0301 et al., 2013). In OSUB, each arm is attached an index that resembles the KL-UCB index, but the arm selected at a given time is the arm with maximal index within the neighborhood in G of the arm that yielded the highest empirical reward. Note that since the sequential choices of arms are restricted to some neighborhoods in the graph, OSUB is not an index policy. To formally describe OSUB, we need the following notation. For p \u2208 [0, 1], s \u2208 N, and n \u2208 N, we define:\nF (p, s, n) = sup{q \u2265 p : sI(p, q) \u2264 log(n) + c log(log(n))}, (2)\nwith the convention that F (p, 0, n) = 1, and F (1, s, n) = 1, and where c > 0 is a constant. Let k(n) be the arm selected under OSUB at time n, and let tk(n) denote the number of times arm k has been selected up to time n. The empirical reward of arm k at time n is \u00b5\u0302k(n) =\n1 tk(n) \u2211n t=1 1{k(t) = k}Xk(t), if tk(n) > 0 and \u00b5\u0302k(n) = 0 otherwise. We denote by L(n) = argmax1\u2264k\u2264K \u00b5\u0302k(n) the index of the arm with the highest empirical reward (ties are broken arbitrarily). Arm L(n) is referred to as the leader at time n. Further define lk(n) = \u2211n t=1 1{L(t) = k} the number of times arm k has been the leader up to time n. Now the index of arm k at time n is defined as:\nbk(n) = F (\u00b5\u0302k(n), tk(n), lk(L(n))).\nFinally for any k, let N(k) = {k\u2032 : (k\u2032, k) \u2208 E} \u222a {k} be the neighborhood of k in G. The pseudo-code of OSUB is\npresented below.\nAlgorithm OSUB\nInput: graph G = (V,E) For n \u2265 1, select the arm k(n) where:\nk(n) =\n\n\n\nL(n) if lL(n)(n)\u22121 \u03b3+1 \u2208 N, arg max\nk\u2208N(L(n)) bk(n) otherwise,\nwhere \u03b3 is the maximal degree of nodes in G and ties are broken arbitrarily.\nNote that OSUB forces us to select the current leader often: L(n) is chosen when lL(n)(n) \u2212 1 is a multiple of \u03b3 + 1. This ensures that the number of times an arm has been selected is at least proportional to the number of times this arm has been the leader. This property significantly simplifies the regret analysis, but it could be removed."}, {"heading": "4.3. Finite-time analysis of OSUB", "text": "Next we provide a finite time analysis of the regret achieved under OSUB. Let \u2206 denote the minimal separation between an arm and its best adjacent arm: \u2206 = min1\u2264k\u2264K maxk\u2032:(k,k\u2032)\u2208E \u00b5k\u2032 \u2212 \u00b5k. Note that \u2206 is not known a priori.\nTheorem 4.2 Assume that the rewards lie in [0,1] (i.e., the support of \u03bdk is included in [0, 1], for all k), and that (\u00b51, . . . , \u00b5K) \u2208 UG. The number of times suboptimal arm k is selected under OSUB satisfies: for all \u01eb > 0 and all T \u2265 3,\nE[tk(T )] \u2264\n\n \n \n(1 + \u01eb) log(T )+c log(log(T ))I(\u00b5k,\u00b5\u2217) if (k, k \u22c6) \u2208 E,\n+C1 log log(T ) + C2\nT\u03b2(\u01eb) C3 \u22062 otherwise,\nwhere \u03b2(\u01eb) > 0, and 0 < C1 < 7, C2 > 0, C3 > 0 are constants.\nTo prove this upper bound, we analyze the regret accumulated (i) when the best arm k\u22c6 is the leader, and (ii) when the leader is arm k 6= k\u22c6. (i) When k\u22c6 is the leader, the algorithm behaves like KL-UCB restricted to the arms around k\u22c6, and the regret at these rounds can be analyzed as in (Cappe\u0301 et al., 2013). (ii) Bounding the number of rounds where k 6= k\u22c6 is not the leader is more involved. To do this, we decompose this set of rounds into further subsets (such as the time instants where k is the leader and its mean is not well estimated), and control their expected cardinalities using concentration inequalities. Along the way, we establish Lemma 4.3, a new concentration inequality of independent interest.\nLemma 4.3 Let {Zt}t\u2208Z be a sequence of independent random variables with values in [0, B]. Define Fn the \u03c3-algebra generated by {Zt}t\u2264n and the filtration F = (Fn)n\u2208Z. Consider s \u2208 N, n0 \u2208 Z and T \u2265 n0. We define Sn =\n\u2211n t=n0 Bt(Zt \u2212 E[Zt]), where Bt \u2208 {0, 1} is a Ft\u22121-measurable random variable. Further define tn =\n\u2211n t=n0 Bt. Define \u03c6 \u2208 {n0, . . . , T+1} a F -stopping time such that either t\u03c6 \u2265 s or \u03c6 = T + 1. Then we have that: P[S\u03c6 \u2265 t\u03c6\u03b4 , \u03c6 \u2264 T ] \u2264 exp(\u22122s\u03b42B\u22122). As a consequence: P[|S\u03c6| \u2265 t\u03c6\u03b4 , \u03c6 \u2264 T ] \u2264 2 exp(\u22122s\u03b42B\u22122).\nLemma 4.3 concerns the sum of products of i.i.d. random variables and of a previsible sequence, evaluated at a stopping time (for the natural filtration). We believe that concentration results for such sums can be instrumental in bandit problems, where typically, we need information about the empirical rewards at some specific random time epochs (that often are stopping times). Refer to the appendix for a proof. A direct consequence of Theorem 4.2 is the asymptotic optimality of OSUB in the case of Bernoulli rewards:\nCorollary 4.4 Assume that rewards distributions are Bernoulli (i.e for any k, \u03bdk \u223c Bernoulli(\u03b8k)), and that \u03b8 \u2208 \u0398G. Then the regret achieved under \u03c0=OSUB satisfies: lim supT\u2192+\u221e R \u03c0(T )/ log(T ) \u2264 c(\u03b8)."}, {"heading": "5. Non-stationary environments", "text": "We now consider time-varying environments. We assume that the expected reward of each arm varies smoothly over time, i.e., it is Lipschitz continuous: for all n, n\u2032 \u2265 1 and 1 \u2264 k \u2264 K: |\u00b5k(n)\u2212 \u00b5k(n\u2032)| \u2264 \u03c3|n\u2212 n\u2032|. We further assume that the unimodal structure is preserved (with respect to the same graph G): for all n \u2265 1, \u00b5(n) \u2208 UG. Considering smoothly varying rewards is more challenging than scenarios where the environment is abruptly changing. The difficulty stems from the fact that the rewards of two or more arms may become arbitrarily close to each other (this happens each time the optimal arm changes), and in such situations, regret is difficult to control. To get a chance to design an algorithm that efficiently tracks the best arm, we need to make some assumption to limit the proportion of time when the separation of arms becomes too small. Define for T \u2208 N, and \u2206 > 0:\nH(\u2206, T ) =\nT \u2211\nn=1\n\u2211\n(k,k\u2032)\u2208E\n1{|\u00b5k(n)\u2212 \u00b5k\u2032(n)| < \u2206}.\nAssumption 1 There exists a function \u03a6 and \u22060 such that for all \u2206 < \u22060: lim supT\u2192+\u221e H(\u2206, T )/T \u2264 \u03a6(K)\u2206."}, {"heading": "5.1. OSUB with a Sliding Window", "text": "To cope with the changing environment, we modify the OSUB algorithm, so that decisions are based on past choices and observations over a time-window of fixed duration equal to \u03c4 + 1 rounds. The idea of adding a sliding window to algorithms initially designed for stationary environments is not novel (Garivier & Moulines, 2008); but here, the unimodal structure and the smooth evolution of rewards make the regret analysis more challenging.\nDefine: t\u03c4k(n) = \u2211n t=n\u2212\u03c4 1{k(t) = k}; \u00b5\u0302\u03c4k(n) = (1/t\u03c4k(n)) \u2211n t=n\u2212\u03c4 1{k(t) = k}Xk(t) if t\u03c4k(n) > 0 and \u00b5\u0302\u03c4k(n) = 0 otherwise; L \u03c4 (n) = argmax1\u2264k\u2264K \u00b5\u0302 \u03c4 k(n); l\u03c4k(n) = \u2211n\nt=n\u2212\u03c4 1{L\u03c4(t) = k}. The index of arm k at time n then becomes: b\u03c4k(n) = F (\u00b5\u0302\u03c4k(n), t \u03c4 k(n), l \u03c4 k(L\n\u03c4 (n))). The pseudo-code of SWOSUB is presented below.\nAlgorithm SW-OSUB Input: graph G = (V,E), window size \u03c4 + 1 For n \u2265 1, select the arm k(n) where:\nk(n) =\n\n\n\nL\u03c4 (n) if l\u03c4L\u03c4 (n)(n)\u22121\n\u03b3+1 \u2208 N, arg max\nk\u2208N(L\u03c4 (n)) b\u03c4k(n) otherwise."}, {"heading": "5.2. Regret Analysis", "text": "In non-stationary environments, achieving sublinear regrets is often not possible. In (Garivier & Moulines, 2008), the environment is subject to abrupt changes or breakpoints. It is shown that if the density of breakpoints is strictly positive, which typically holds in practice, then the regret of any algorithm has to scale linearly with time. We are interested in similar scenarios, and consider smoothly varying environments where the number of times the optimal arm changes has a positive density. The next theorem provides an upper bound of the regret per unit of time achieved under SW-OSUB. This bound holds for any non-stationary environment with \u03c3-Lipschitz rewards.\nTheorem 5.1 Let \u2206: 2\u03c4\u03c3 < \u2206 < \u22060. Assume that for any n \u2265 1, \u00b5(n) \u2208 UG and \u00b5\u22c6(n) \u2208 [a, 1 \u2212 a] for some a > 0. Further suppose that \u00b5k(\u00b7) is \u03c3-Lipschitz for any k. The regret per unit time under \u03c0 =SW-OSUB with a sliding window of size \u03c4 + 1 satisfies: if a > \u03c3\u03c4 , then for any T \u2265 1, R\u03c0(T )\nT \u2264 H(\u2206, T ) T (1 + \u2206) + C1K log(\u03c4) \u03c4(\u2206\u2212 4\u03c4\u03c3)2\n+ \u03b3 (\n1 + g \u22121/2 0 ) log(\u03c4) + c log(log(\u03c4)) + C2 2\u03c4(\u2206\u2212 2\u03c4\u03c3)2 ,\nwhere C1, C2 are positive constants and g0 = (a\u2212\u03c3\u03c4)(1\u2212 a+ \u03c3\u03c4)/2.\nCorollary 5.2 Assume that for any n \u2265 1, \u00b5(n) \u2208 UG and \u00b5\u22c6(n) \u2208 [a, 1 \u2212 a] for some a > 0, and that \u00b5k(\u00b7) is \u03c3Lipschitz for any k. Set \u03c4 = \u03c3\u22123/4 log(1/\u03c3)/8. The regret per unit of time of \u03c0 =SW-OSUB with window size \u03c4 + 1 satisfies:\nlim sup T\u2192\u221e\nR\u03c0(T )\nT \u2264 C\u03a6(K)\u03c3 14 log\n(\n1\n\u03c3\n)\n(1 +Kj(\u03c3)),\nfor some constant C > 0, and some function j such that lim\u03c3\u21920+ j(\u03c3) = 0.\nThese results state that the regret per unit of time achieved under SW-OSUB decreases and actually vanishes when the speed at which expected rewards evolve decreases to 0. Also observe that the dependence of this regret bound in the number of arms is typically mild (in many practical scenarios, \u03a6(K) may actually not depend on K).\nThe proof of Theorem 5.1 relies on the same types of arguments as those used in stationary environments. To establish the regret upper bound, we need to evaluate the performance of the KL-UCB algorithm in non-stationary environments (the result and the corresponding analysis are presented in appendix)."}, {"heading": "6. Continuous Set of Arms", "text": "In this section, we briefly discuss the case where the decision space is continuous. The set of arms is [0, 1], and the expected reward function \u00b5 : [0, 1] \u2192 R is assumed to be Lipschitz continuous, and unimodal: there exists x\u22c6 \u2208 [0, 1] such that \u00b5(x\u2032) \u2265 \u00b5(x) if x\u2032 \u2208 [x, x\u22c6] or x\u2032 \u2208 [x\u22c6, x]. Let \u00b5\u22c6 = \u00b5(x\u22c6) denote the highest expected reward. A decision rule selects at any round n \u2265 1 an arm x and observes the corresponding reward X(x, n). For any x \u2208 [0, 1], (X(x, n))n\u22651 is an i.i.d. sequence. We make the following additional assumption on function \u00b5.\nAssumption 2 There exists \u03b40 > 0 such that (i) for all x, y in [x\u22c6, x\u22c6+ \u03b40] (or in [x\u22c6\u2212 \u03b40, x\u22c6]), C1|x\u2212 y|\u03b1 \u2264 |\u00b5(x)\u2212 \u00b5(y)|; (ii) for \u03b4 \u2264 \u03b40, if |x\u2212x\u2217| \u2264 \u03b4, then |\u00b5(x\u2217)\u2212\u00b5(x)| \u2264 C2\u03b4 \u03b1.\nThis assumption is more general than that used in (Yu & Mannor, 2011). In particular it holds for functions with a plateau and a peak: \u00b5(x) = max(1\u2212|x\u2212x\u22c6|/\u01eb, 0). Now as for the case of a discrete set of arms, we denote by \u03a0 the set of possible decision rules, and the regret achieved under rule \u03c0 \u2208 \u03a0 up to time T is: R\u03c0(T ) = T\u00b5\u22c6\u2212\u2211Tn=1 E[\u00b5(x\u03c0(n))], where x\u03c0(n) is the arm selected under \u03c0 at time n.\nThere is no known precise asymptotic lower bound for continuous bandits. However, we know that for our problem, the regret must be at least of the order of O( \u221a T ) up to\nlogarithmic factor. In (Yu & Mannor, 2011), the authors show that the LSE algorithm achieves a regret scaling as O( \u221a T log(T )), under more restrictive assumptions. We show that combining discretization and the UCB algorithm as initially proposed in (Kleinberg, 2004) yields lower regrets than LSE in practice (see Section 7), and is orderoptimal, i.e., the regret grows as O( \u221a T log(T )).\nFor \u03b4 > 0, we define a discrete bandit problem with K = \u23081/\u03b4\u2309 arms, and where the rewards of k-th arm are distributed as X((k\u2212 1)/\u03b4, n). The expected reward of the k-th arm is \u00b5k = \u00b5((k\u2212 1)/\u03b4). Let \u03c0 be an algorithm running on this discrete bandit problem. The regret of \u03c0 for the initial continuous bandit problem is at time T : R\u03c0(T ) = T\u00b5\u22c6 \u2212 \u2211\u23081/\u03b4\u2309k=1 \u00b5kE[t\u03c0k (T )]. We denote by UCB(\u03b4) the UCB algorithm (Auer et al., 2002) applied to the discretized bandit. In the following proposition, we show that when \u03b4 = (log(T )/ \u221a T )1/\u03b1, UCB(\u03b4) is orderoptimal. In practice, one may not know the time horizon T in advance. In this case, using the \u201cdoubling trick\u201d (see e.g. (Cesa-Bianchi & Lugosi, 2006)) would incur an additional logarithmic multiplicative factor in the regret.\nProposition 1 Consider a unimodal bandit on [0, 1] with rewards in [0, 1] and satisfying Assumption 2. Set \u03b4 = (log(T )/ \u221a T )1/\u03b1. The regret under UCB(\u03b4) satisfies:\nlim sup T\u2192\u221e R\u03c0(T )\u221a T log(T ) \u2264 C23\u03b1 + 16/C1."}, {"heading": "7. Numerical experiments", "text": ""}, {"heading": "7.1. Discrete bandits", "text": "We compare the performance of our algorithm to that of KL-UCB (Cappe\u0301 et al., 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al., 2002), and UCB-U. The latter algorithm is obtained by applying UCB restricted to the arms which are adjacent to the current leader as in OSUB. We add the prefix \u201dSW\u201d to refer to Sliding Window versions of these algorithms.\nStationary environments. In our first experiment, we consider K = 17 arms with Bernoulli rewards of respective averages \u00b5 = (0.1, 0.2, ...., 0.9, 0.8, . . . , 0.1). The rewards are unimodal (the graph G is simply a line). The regret achieved under the various algorithms is presented in Figure 1 and Table 1. The parameters in LSE algorithm are chosen as suggested in Proposition 4.5 (Yu & Mannor, 2011). Regrets are calculated averaging over 50 independent runs. OSUB significantly outperforms all other algorithms. The regret achieved under LSE is not presented in Figure 1, because it is typically much larger than that of other algorithms. This poor performance can be explained by the non-adaptive nature of LSE, as already discussed earlier. LSE can beat UCB when the number of arms is\nnot negligible compared to the time horizon (e.g. in Figure 4 in (Yu & Mannor, 2011), K = 250.000 and the time horizon is less than 3K): in such scenarios, UCB-like algorithms perform poorly because of their initialization phase (all arms have to be tested once).\nIn Figure 2, the number of arms is 129, and the expected rewards form a triangular shape as in the previous example, with minimum and maximum equal to 0.1 and 0.9, respectively. Similar observations as in the case of 17 arms can be made. We deliberately restrict the plot to small time horizons: this corresponds to scenarios where LSE can perform well.\nNon-stationary environments. We now investigate the per-\nformance of SW-OSUB in a slowly varying environment. There are K = 10 arms whose expected rewards form a moving triangle: for k = 1, . . . ,K , \u00b5k(n) = (K\u22121)/K\u2212 |w(n)\u2212k|/K , where w(n) = 1+(K\u22121)(1+sin(n\u03c3))/2. Figure 3 presents the regret as a function of time under various algorithms when the speed at which the environment evolves is \u03c3 = 10\u22123. The window size are set as follows for the various algorithms: \u03c4 = \u03c3\u22124/5 for SW-UCB and SW-KL-UCB (the rationale for this choice is explained in appendix), \u03c4 = \u03c3\u22123/4 log(1/\u03c3)/8 for SW-UCB-U and OSUB. In Figure 4, we show how the speed \u03c3 impacts the regret per time unit. SW-OSUB provides the most efficient way of tracking the optimal arm."}, {"heading": "7.2. Continuous bandits", "text": "In Figure 5, we compare the performance of the LSE and UCB(\u03b4) algorithms when the set of arms is continuous. The expected rewards form a triangle: \u00b5(x) = 1/2\u2212 |x\u2212 1/2| so that \u00b5\u22c6 = 1/2 and x\u22c6 = 1/2. The parameters used in LSE are those given in (Yu & Mannor, 2011), whereas the discretization parameter \u03b4 in UCB(\u03b4) is set to \u03b4 = log(T )/ \u221a T . UCB(\u03b4) significantly outperforms LSE at any time: an appropriate discretization of continuous bandit problems might actually be more efficient than other meth-\nods based on ideas taken from classical optimization theory.\nFigure 6 compares the regret of the discrete version of LSE (with optimized parameters), and of OSUB as the number of arms K grows large, T = 50, 000. The average rewards of arms are extracted from the triangle used in the continuous bandit, and we also provide the regret achieved under UCB(\u03b4). OSUB outperforms UCB(\u03b4) even if the number of arms gets as large as 7500! OSUB also beats LSE unless the number of arms gets bigger than 0.6\u00d7 T ."}, {"heading": "8. Conclusion", "text": "In this paper, we address stochastic bandit problems with a unimodal structure, and a finite set of arms. We provide asymptotic regret lower bounds for these problems and design an algorithm that asymptotically achieves the lowest regret possible. Hence our algorithm optimally exploits the unimodal structure of the problem. Our preliminary analysis of the continuous version of this bandit problem suggests that when the number of arms become very large and comparable to the time horizon, it might be wiser to prune the set of arms before actually running any algorithm."}, {"heading": "A. Proof of Theorem 4.1", "text": "We derive here a regret lower bound for the unimodal bandit problem. To this aim, we apply the techniques used by Graves and Lai (Graves & Lai, 1997) to investigate efficient adaptive decision rules in controlled Markov chains. We recall here their general framework. Consider a controlled Markov chain (Xt)t\u22650 on a finite state space S with a control set U . The transition probabilities given control u \u2208 U are parametrized by \u03b8 taking values in a compact metric space \u0398: the probability to move from state x to state y given the control u and the parameter \u03b8 is p(x, y;u, \u03b8). The parameter \u03b8 is not known. The decision maker is provided with a finite set of stationary control laws G = {g1, . . . , gK} where each control law gj is a mapping from S to U : when control law gj is applied in state x, the applied control is u = gj(x). It is assumed that if the decision maker always selects the same control law g the Markov chain is then irreducible with stationary distribution \u03c0g\u03b8 . Now the expected reward obtained when applying control u in state x is denoted by r(x, u), so that the expected reward achieved under control law g is: \u00b5\u03b8(g) = \u2211 x r(x, g(x))\u03c0 g \u03b8 (x). There is an optimal control law given \u03b8 whose expected reward is denoted \u00b5\u22c6\u03b8 \u2208 argmaxg\u2208G \u00b5\u03b8(g). Now the objective of the decision maker is to sequentially select control laws so as to maximize the expected reward up to a given time horizon T . As for MAB problems, the performance of a decision scheme can be quantified through the notion of regret which compares the expected reward to that obtained by always applying the optimal control law.\nWe now apply the above framework to our unimodal bandit problem, and we consider \u03b8 \u2208 \u0398G. The Markov chain has values in S = R. The set of control laws is G = {1, . . . ,K}. These laws are constant, in the sense that the control applied by control law k does not depend on the state of the Markov chain, and corresponds to selecting arm k. The transition probabilities are: p(x, y; k, \u03b8) = p(y, \u03b8k). Finally, the reward r(x, k) does not depend on the state and is equal to \u00b5(\u03b8k), which is also the expected reward obtained by always using control law k.\nWe now fix \u03b8 \u2208 \u0398G. Define KLk(\u03b8, \u03bb) = KL(\u03b8k, \u03bbk) for\nany k. Further define the set B(\u03b8) consisting of all bad parameters \u03bb \u2208 \u0398G such that k\u22c6 is not optimal under parameter \u03bb, but which are statistically indistinguishable from \u03b8:\nB(\u03b8) = {\u03bb \u2208 \u0398G : \u03bbk\u22c6 = \u03b8k\u22c6 and max k \u00b5(\u03bbk) > \u00b5(\u03bbk\u22c6)},\nB(\u03b8) can be written as the union of sets Bk(\u03b8), (k, k\u22c6) \u2208 E defined as:\nBk(\u03b8) = {\u03bb \u2208 B(\u03b8) : \u00b5(\u03bbk) > \u00b5(\u03bbk\u22c6)}.\nWe have that B(\u03b8) = \u222a(k,k\u22c6)\u2208EBk(\u03b8), because if \u00b5(\u03bbk\u22c6) < maxk \u00b5(\u03bbk), then there must exist k such that (k, k\u2217) \u2208 E, and \u00b5(\u03bbk) > \u00b5(\u03bbk\u22c6 ). By applying Theorem 1 in (Graves & Lai, 1997), we know that c(\u03b8) is the minimal value of the following LP:\nmin \u2211 k ck(\u00b5(\u03b8k\u22c6)\u2212 \u00b5(\u03b8k)) (3) s.t. inf\u03bb\u2208Bk(\u03b8) \u2211 l 6=k\u22c6 clKL l(\u03b8, \u03bb) \u2265 1, (k, k\u22c6) \u2208 E(4)\nck \u2265 0, \u2200k. (5)\nNext we show that the constraints (4) on the ck\u2019s are equivalent to:\nmin (k,k\u22c6)\u2208E\nckImin(\u03b8k, \u00b5(\u03b8k\u22c6)) \u2265 1. (6)\nConsider k fixed with (k, k\u22c6) \u2208 E. We prove that:\ninf \u03bb\u2208Bk(\u03b8)\n\u2211\nl 6=k\u22c6\nclKL l(\u03b8, \u03bb) = ckImin(\u03b8k, \u00b5(\u03b8k\u22c6)). (7)\nThis is simply due to the following two observations:\n\u2022 Since \u00b5(\u03bbk) > \u00b5(\u03b8k\u22c6) and the KL divergence is positive:\n\u2211\nl 6=k\u22c6\nclKL l(\u03b8, \u03bb) \u2265 ckKLk(\u03b8, \u03bb)\n\u2265 ckImin(\u03b8k, \u00b5(\u03b8k\u22c6)).\n\u2022 For \u01eb > 0, define \u03bb\u01eb as follows: \u00b5(\u03bbk) > \u00b5(\u03b8k\u22c6) and KL(\u03b8k, \u03bbk) \u2264 Imin(\u03b8k, \u00b5(\u03b8k\u22c6)) + \u01eb and \u03bbl = \u03b8l for l 6= k. By construction, \u03bb\u01eb \u2208 Bk(\u03b8), and\nlim \u01eb\u21920\n\u2211\nl 6=k\u22c6\nclKL l(\u03b8, \u03bb\u01eb) = ckImin(\u03b8k, \u00b5(\u03b8k\u22c6)).\nFrom (7), we deduce that constraints (4) are equivalent to (6) (indeed, for (k, k\u22c6) \u2208 E, (4) is equivalent to ckImin(\u03b8k, \u00b5(\u03b8k\u22c6)) \u2265 1). With the constraints (6), the optimization problem becomes straightforward to solve, and its solution yields:\nc(\u03b8) = \u2211\n(k,k\u22c6)\u2208E\n\u00b5(\u03b8k\u22c6)\u2212 \u00b5(\u03b8k) Imin(\u03b8k, \u00b5(\u03b8k\u22c6)) ."}, {"heading": "B. Concentration inequalities and Preliminaries", "text": "B.1. Proof of Lemma 4.3\nWe prove Lemma 4.3, a new concentration inequality which extends Hoeffding\u2019s inequality, and is used for the regret analysis in subsequent sections. We believe that Lemma 4.3 could be useful in a variety of bandit problems, where an upper bound on the deviation of the empirical mean sampled at a stopping time is needed. An example would be the probability that the empirical reward of the k-th arm deviates from its expectation, when it is sampled for the s-th time.\nProof. Let \u03bb > 0, and define Gn = exp(\u03bb(Sn \u2212 \u03b4tn))1{n \u2264 T }. We have that:\nP[S\u03c6 \u2265 t\u03c6\u03b4 , \u03c6 \u2264 T ] = P[exp(\u03bb(S\u03c6 \u2212 \u03b4t\u03c6))1{\u03c6 \u2264 T } \u2265 1] = P[G\u03c6 \u2265 1] \u2264 E[G\u03c6].\nNext we provide an upper bound for E[G\u03c6]. We define the following quantities:\nYt = Bt[\u03bb(Zt \u2212 E[Zt])\u2212 \u03bb2B2/8]\nG\u0303n = exp\n(\nn \u2211\nt=n0\nYt\n)\n1{n \u2264 T }.\nSo that G can be written:\nGn = G\u0303n exp(\u2212tn(\u03bb\u03b4 \u2212 \u03bb2B2/8)).\nSetting \u03bb = 4\u03b4/B2:\nGn = G\u0303n exp(\u22122tn\u03b42/B2).\nUsing the fact that t\u03c6 \u2265 s if \u03c6 \u2264 T , we can upper bound G\u03c6 by:\nG\u03c6 = G\u0303\u03c6 exp(\u22122t\u03c6\u03b42/B2) \u2264 G\u0303\u03c6 exp(\u22122s\u03b42/B2).\nIt is noted that the above inequality holds even when \u03c6 = T + 1, since GT+1 = G\u0303T+1 = 0. Hence:\nE[G\u03c6] \u2264 E[G\u0303\u03c6] exp(\u22122s\u03b42/B2).\nWe prove that (G\u0303n)n is a super-martingale. We have that E[G\u0303T+1|FT ] = 0 \u2264 G\u0303T . For n \u2264 T \u2212 1, since Bn+1 is Fn measurable:\nE[G\u0303n+1|Fn] = G\u0303n((1\u2212Bn+1) +Bn+1E[exp(Yn+1)]).\nAs proven by Hoeffding (Hoeffding, 1963)[eq. 4.16] since Zn+1 \u2208 [0, B]:\nE[exp(\u03bb(Zn+1 \u2212 E[Zn+1]))] \u2264 exp(\u03bb2B2/8),\nso E[exp(Yn+1)] \u2264 1 and (G\u0303n)n is indeed a supermartingale: E[G\u0303n+1|Fn] \u2264 G\u0303n. Since \u03c6 \u2264 T + 1 almost surely, and (G\u0303n)n is a supermartingale, Doob\u2019s optional stopping theorem yields: E[G\u0303\u03c6] \u2264 E[G\u0303n0\u22121] = 1, and so\nP[S\u03c6 \u2265 t\u03c6\u03b4, \u03c6 \u2264 T ] \u2264 E[G\u03c6] \u2264 E[G\u0303\u03c6] exp(\u22122s\u03b42/B2) \u2264 exp(\u22122s\u03b42/B2).\nwhich concludes the proof. The second inequality is obtained by symmetry.\nB.2. Preliminary results\nLemma B.1 states that if a set of instants \u039b can be decomposed into a family of subsets (\u039b(s))s\u22651 of instants (each subset has at most one instant) where k is tried sufficiently many times (tk(n) \u2265 \u01ebs, for n \u2208 \u039b(s)), then the expected number of instants in \u039b at which the average reward of k is badly estimated is finite.\nLemma B.1 Let k \u2208 {1, . . . ,K}, and \u01eb > 0. Define Fn the \u03c3-algebra generated by (Xk(t))1\u2264t\u2264n,1\u2264k\u2264K . Let \u039b \u2282 N be a (random) set of instants. Assume that there exists a sequence of (random) sets (\u039b(s))s\u22651 such that (i) \u039b \u2282 \u222as\u22651\u039b(s), (ii) for all s \u2265 1 and all n \u2208 \u039b(s), tk(n) \u2265 \u01ebs, (iii) |\u039b(s)| \u2264 1, and (iv) the event n \u2208 \u039b(s) is Fnmeasurable. Then for all \u03b4 > 0:\nE[ \u2211\nn\u22651\n1{n \u2208 \u039b, |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4}] \u2264 1\n\u01eb\u03b42 . (8)\nProof. Let T \u2265 1. For all s \u2265 1, since \u039b(s) has at most one element, define \u03c6s = T + 1 if \u039b(s) \u2229 {1, . . . , T } is empty and {\u03c6s} = \u039b(s) otherwise. Since \u039b \u2282 \u222as\u22651\u039b(s), we have:\nT \u2211\nn=1\n1{n \u2208 \u039b, |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4}\n\u2264 \u2211\ns\u22651\n1{|\u00b5\u0302k(\u03c6s)\u2212 E[\u00b5\u0302k(\u03c6s)]| > \u03b4, \u03c6s \u2264 T }.\nTaking expectations:\nE[\nT \u2211\nn=1\n1{n \u2208 \u039b, |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4}]\n\u2264 \u2211\ns\u22651\nP[|\u00b5\u0302k(\u03c6s)\u2212 E[\u00b5\u0302k(\u03c6s)]| > \u03b4, \u03c6s \u2264 T ].\nSince \u03c6s is a stopping time upper bounded by T + 1, and that tk(\u03c6s) \u2265 \u01ebs we can apply Lemma 4.3 to obtain:\nE[\nT \u2211\nn=1\n1{n \u2208 \u039b, |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4}]\n\u2264 \u2211\ns\u22651\n2 exp ( \u22122s\u01eb\u03b42 ) \u2264 1 \u01eb\u03b42 .\nWe have used the inequality: \u2211 s\u22651 e \u2212sw \u2264 \u222b +\u221e\n0 e \u2212uwdu = 1/w. Since the above reasoning is\nvalid for all T , we obtain the claim (8).\nA useful corollary of Lemma B.1 is obtained by choosing \u03b4 = \u2206k,k\u2032/2, when arms k and k\u2032 are separated by at least \u2206k,k\u2032 .\nLemma B.2 Let k, k\u2032 \u2208 {1, . . . ,K} with k 6= k\u2032 and \u01eb > 0. Define Fn the \u03c3-algebra generated by (Xk(t))1\u2264t\u2264n,1\u2264k\u2264K . Let \u039b \u2282 N be a (random) set of instants. Assume that there exists a sequence of (random) sets (\u039b(s))s\u22651 such that (i) \u039b \u2282 \u222as\u22651\u039b(s), (ii) for all s \u2265 1 and all n \u2208 \u039b(s), tk(n) \u2265 \u01ebs and tk\u2032(n) \u2265 \u01ebs, (iii) for all s we have |\u039b(s)| \u2264 1 almost surely and (iv) for all n \u2208 \u039b, we have E[\u00b5\u0302k(n)] \u2264 E[\u00b5\u0302k\u2032(n)] \u2212\u2206k,k\u2032 (v) the event n \u2208 \u039b(s) is Fn-measurable. Then:\nE[ \u2211\nn\u22651\n1{n \u2208 \u039b, \u00b5\u0302k(n) > \u00b5\u0302k\u2032 (n)}] \u2264 8\n\u01eb\u22062k,k\u2032 . (9)\nLemma B.3 is straightforward from (Garivier & Cappe\u0301, 2011)[Theorem 10]. It should be observed that this result is not a direct application of Sanov\u2019s theorem; Lemma B.3 provides sharper bounds in certain cases, and it is also valid for non-Bernoulli distributed random variables.\nLemma B.3 For 1 \u2264 tk(n) \u2264 \u03c4 and \u03b4 > 0, if {Xk(i)}1\u2264i\u2264\u03c4 are independent random variables with mean \u00b5k, we have that:\nP\n\ntk(n)I\n\n\n1\ntk(n)\ntk(n) \u2211\ni=1\nXk(i), \u00b5k\n\n \u2265 \u03b4\n\n\n\u2264 2e\u2308\u03b4 log(\u03c4)\u2309 exp(\u2212\u03b4).\nWe present results related to the KL divergence that will be instrumental when manipulating indexes bk(n). Lemma B.4 gives an upper and a lower bound for the KL divergence. The lower bound is Pinsker\u2019s inequality. The upper bound is due to the fact that I(p, q) is convex in its second argument.\nLemma B.4 For all p, q \u2208 [0, 1]2, p \u2264 q:\n2(p\u2212 q)2 \u2264 I(p, q) \u2264 (p\u2212 q) 2\nq(1\u2212 q) . (10)\nand\nI(p, q) \u223c (p\u2212 q) 2\nq(1\u2212 q) , q \u2192 p + (11)\nProof. The lower bound is Pinsker\u2019s inequality. For the upper bound, we have:\n\u2202I \u2202q (p, q) = q \u2212 p q(1 \u2212 q) .\nSince q 7\u2192 \u2202I\u2202q (p, q) is increasing, the fundamental theorem of calculus gives the announced result:\nI(p, q) \u2264 \u222b q\np\n\u2202I \u2202u (p, u) du \u2264 (p\u2212 q) 2 q(1\u2212 q) .\nThe equivalence comes from a Taylor development of q \u2192 I(p, q) at p, since:\n\u2202I \u2202q (p, q)|q=p = 0,\n\u22022I \u2202q2 (p, q)|q=p =\n1\nq(1\u2212 q) .\nWe prove a deviation bound similar to that of Lemma B.1 for non-stationary environments.\nLemma B.5 Let k \u2208 {1, . . . ,K}, n0 \u2208 N and \u01eb > 0. Let \u039b \u2282 N be a (random) set of instants. Assume that there exists a sequence of (random) sets (\u039b(s))s\u22651 such that (i) \u039b \u2282 \u222as\u22651\u039b(s), (ii) for all s \u2265 1 and all n \u2208 \u039b(s), tk(n) \u2265 \u01ebs, and (iii) for all s \u2265 1 |\u039b(s)\u2229[n0, n0+\u03c4 ]| \u2264 1. Then for all \u03b4 > 0:\nE[\nn0+\u03c4 \u2211\nn=n0\n1{n \u2208 \u039b, |\u00b5\u0302k(n)\u2212E[\u00b5\u0302k(n)]| > \u03b4}] \u2264 log(\u03c4)\n2\u01eb\u03b42 +2.\nProof. Fix s0 \u2265 1. We use the following decomposition, depending on the value of s with respect to s0:\n{n \u2208 \u039b, |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4} \u2282 A \u222aB,\nwhere\nA = {n0, . . . , n0 + \u03c4} \u2229 (\u222a1\u2264s\u2264s0\u039b(s)), B = {n0, . . . , n0 + \u03c4}\n\u2229 {n \u2208 \u222as\u2265s0\u039b(s) : |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4}.\nSince for all s, |\u039b(s) \u2229 {n0, . . . , n0 + \u03c4}| \u2264 1, we have |A| \u2264 s0. The expected size of B is upper bounded by:\nE[|B|] \u2264 n0+\u03c4 \u2211\nn=n0\nP[n \u2208 \u222as\u2265s0\u039b(s), |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4]\n\u2264 n0+\u03c4 \u2211\nn=n0\nP[|\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4, tk(n) \u2265 \u01ebs0].\nFor a given n, we apply Lemma 4.3 with n\u2212 \u03c4 in place of n0, and \u03c6 = n if tk(n) \u2265 \u01ebs0 and \u03c6 = T + 1 otherwise. It is noted that \u03c6 is indeed a stopping time. We get:\nP[|\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4, tk(n) \u2265 \u01ebs0] \u2264 2 exp ( \u22122s0\u01eb\u03b42 ) .\nTherefore, setting s0 = log(\u03c4)/(2\u01eb\u03b42),\nE[|B|] \u2264 2\u03c4 exp ( \u22122s0\u01eb\u03b42 ) = 2.\nFinally we obtain the announced result:\nE[\nn0+\u03c4 \u2211\nn=n0\n1{n \u2208 \u039b, |\u00b5\u0302k(n)\u2212E[\u00b5\u0302k(n)]| > \u03b4}] \u2264 log(\u03c4)\n2\u01eb\u03b42 +2.\nLemma B.6 Consider k, k\u2032 \u2208 {1, . . . ,K}, n0 \u2208 N and \u01eb > 0. Let \u039b \u2282 N be a (random) set of instants. Assume that there exists a sequence of (random) sets (\u039b(s))s\u22651 such that (i) \u039b \u2282 \u222as\u22651\u039b(s), and (ii) for all s \u2265 1 and all n \u2208 \u039b(s), tk(n) \u2265 \u01ebs, tk\u2032(n) \u2265 \u01ebs and (iii) for all s \u2265 1 |\u039b(s) \u2229 [n0, n0 + \u03c4 ]| \u2264 1 and (iv) for all n \u2208 \u039b, we have E[\u00b5\u0302k(n)] \u2264 E[\u00b5\u0302k\u2032 (n)]\u2212\u2206k,k\u2032 . Then for all \u03b4 > 0:\nE[\nn0+\u03c4 \u2211\nn=n0\n1{n \u2208 \u039b, \u00b5\u0302k(n) > \u00b5\u0302k\u2032 (n)}] \u2264 4 log(\u03c4)\n\u01eb\u22062k,k\u2032 + 4."}, {"heading": "C. Proofs for stationary environments", "text": "C.1. Proof of Theorem 4.2\nNotations. Throughout the proof, by a slight abuse of notation, we omit the floor/ceiling functions when it does not create ambiguity. Consider a suboptimal arm k 6= k\u22c6. Define the difference between the average reward of k and k\u2032 : \u2206k,k\u2032 = |\u00b5k\u2032 \u2212 \u00b5k| > 0. We use the notation:\ntk,k\u2032 (n) = n \u2211\nt=1\n1{L(t) = k, k(t) = k\u2032}.\ntk,k\u2032 (n) is the number of times up time n that k\u2032 has been selected given that k was the leader.\nProof. Let T > 0. The regret ROSUB(T ) of OSUB algorithm up to time T is:\nROSUB(T ) = \u2211\nk 6=k\u22c6\n(\u00b5k\u22c6 \u2212 \u00b5k)E[ T \u2211\nn=1\n1{k(n) = k}].\nWe use the following decomposition:\n1{k(n) = k} = 1{L(n) = k\u22c6, k(n) = k} +1{L(n) 6= k\u22c6, k(n) = k}.\nNow\n\u2211\nk 6=k\u22c6\n(\u00b5k\u22c6 \u2212 \u00b5k)E[ T \u2211\nn=1\n1{L(n) 6= k\u22c6, k(n) = k}]\n\u2264 \u2211\nk 6=k\u22c6\nE[\nT \u2211\nn=1\n1{L(n) 6= k\u22c6, k(n) = k}]\n\u2264 \u2211\nk 6=k\u22c6\nE[lk(T )].\nObserving that when L(n) = k\u22c6, the algorithm selects a decision (k, k\u22c6) \u2208 E, we deduce that:\nROSUB(T ) \u2264 \u2211\nk 6=k\u22c6\nE[lk(T )]\n+ \u2211\n(k,k\u22c6)\u2208E\n(\u00b5k\u22c6 \u2212 \u00b5k)E[ T \u2211\nn=1\n1{L(n) = k\u22c6, k(n) = k}]\nThen we analyze the two terms in the r.h.s. in the above inequality. The first term corresponds to the average number of times where k\u22c6 is not the leader, while the second term represents the accumulated regret when the leader is k\u22c6. The following result states that the first term is O(log(log(T ))):\nTheorem C.1 For k 6= k\u22c6, E[lk(T )] = O(log(log(T ))).\nFrom the above theorem, we conclude that the leader is k\u22c6 except for a negligible number of instants (in expectation). When k\u22c6 is the leader, OSUB behaves as KL-UCB restricted to the set N(k\u22c6) of possible decisions. Following the same analysis as in (Garivier & Cappe\u0301, 2011) (the analysis of KL-UCB), we can show that for all \u01eb > 0 there are constants C1 \u2264 7 , C2(\u01eb) and \u03b2(\u01eb) > 0 such that:\nE[\nT \u2211\nn=1\n1{L(n) = k\u22c6, k(n) = k}]\n\u2264 E[ T \u2211\nn=1\n1{bk(n) \u2265 bk\u22c6(n)}]\n\u2264 (1 + \u01eb) log(T ) I(\u00b5k, \u00b5k\u22c6) + C1 log(log(T )) + C2(\u01eb) T \u03b2(\u01eb) .\n(12)\nCombining the above bound with Theorem C.1, we get:\nROSUB(T ) \u2264 (1 + \u01eb)c(\u03b8) log(T ) +O(log(log(T ))), (13)\nwhich concludes the proof of Theorem 4.2.\nIt remains to show that Theorem C.1 holds, which is done in the next section. The proof of Theorem C.1 is technical, and requires the concentration inequalities presented in section B. The theorem itself is proved in C.2.\nC.2. Proof of Theorem C.1\nLet k be the index of a suboptimal arm. Let \u03b4 > 0, \u01eb > 0 small enough (we provide a more precise definition later on). We define k2 = argmaxk\u2032:(k,k\u2032)\u2208E \u00b5k\u2032 the best neighbor of k. To derive an upper bound of E[lk(T )], we decompose the set of times where k is the leader into the following sets: {n \u2264 T : L(n) = k} \u2282 A\u01eb \u222aBT\u01eb , where\nA\u01eb = {n : L(n) = k, tk2(n) \u2265 \u01eblk(n)} BT\u01eb = {n \u2264 T : L(n) = k, tk2(n) \u2264 \u01eblk(n)}.\nHence we have:\nE[lk(T )] \u2264 E [ |A\u01eb|+ |BT\u01eb | ] ,\nNext we provide upper bounds of E[|A\u01eb|] and E[|BT\u01eb |].\nBound on E|A\u01eb|. Let n \u2208 A\u01eb and assume that lk(n) = s. By design of the algorithm, tk(n) \u2265 s/(\u03b3 + 1). Also tk2(n) \u2265 \u01eblk(n) = \u01ebs. We apply Lemma B.2 with \u039b(s) = {n \u2208 A\u01eb, lk(n) = s}, \u039b = \u222as\u22651\u039b(s). Of course, for any s, |\u039b(s)| \u2264 1. We have: A\u01eb = {n \u2208 \u039b : \u00b5\u0302k(n) \u2265 \u00b5\u0302k2(n)}, since when n \u2208 A\u01eb, k is the leader. Lemma B.2 can be applied with k\u2032 = k2. We get: E|A\u01eb| < \u221e.\nBound on E|BT\u01eb |. We introduce the following sets:\n\u2022 C\u03b4 is the set of instants at which the average reward of the leader k is badly estimated:\nC\u03b4 = {n : L(n) = k, |\u00b5\u0302k(n)\u2212 \u00b5k| > \u03b4}.\n\u2022 D\u03b4 = \u222ak\u2032\u2208N(k)\\{k2}D\u03b4,k\u2032 where D\u03b4,k\u2032 = {n : L(n) = k, k(n) = k\u2032, |\u00b5\u0302k\u2032 (n) \u2212 \u00b5k\u2032 | > \u03b4} is the set of instants at which k is the leader, k\u2032 is selected and the average reward of k\u2032 is badly estimated.\n\u2022 ET = {n \u2264 T : L(n) = k, bk2(n) \u2264 \u00b5k2}, is the set of instants at which k is the leader, and the upper confidence index bk2(n) underestimates the average reward \u00b5k2 .\nWe first prove that |BT\u01eb | \u2264 2\u03b3(1+\u03b3)(|C\u03b4|+|D\u03b4|+|ET |)+ O(1) as T grows large, and then provide upper bounds on E|C\u03b4|, E|D\u03b4|, and E|ET |. Let n \u2208 BT\u01eb . When k is the leader, the selected decision is in N(k):\nlk(n) = tk,k2 (n) + \u2211\nk\u2032\u2208N(k)\\{k2}\ntk,k\u2032 (n).\nWe recall that tk,k\u2032 (n) denotes the number of times up to time n when k is the leader and k\u2032 is selected. Since n \u2208 BT\u01eb , tk,k2(n) \u2264 \u01eblk(n), from which we deduce that:\n(1\u2212 \u01eb)lk(n) \u2264 \u2211\nk\u2032\u2208N(k)\\{k2}\ntk,k\u2032 (n).\nChoose \u01eb < 1/(2(\u03b3 + 1)). With this choice, from the previous inequality, we must have that either (a) there exists k1 \u2208 N(k) \\ {k, k2}, tk,k1 (n) \u2265 lk(n)/(\u03b3 + 1) or (b) tk,k(n) \u2265 (3/2)lk(n)/(\u03b3 + 1) + 1.\n(a) Assume that tk,k1(n) \u2265 lk(n)/(\u03b3 + 1). Since tk,k1 (n) is only incremented when k1 is selected and k is the leader, and since n 7\u2192 lk(n) is increasing, there exists a unique \u03c6(n) < n such that L(\u03c6(n)) = k, k(\u03c6(n)) = k1, tk,k1(\u03c6(n)) = \u230alk(n)/(2(\u03b3 + 1))\u230b. \u03c6(n) is indeed unique because tk,k1(\u03c6(n)) is incremented at time \u03c6(n).\nNext we prove by contradiction that for lk(n) \u2265 l0 large enough and \u03b4 small enough, we must have \u03c6(n) \u2208 C\u03b4 \u222a D\u03b4 \u222a ET . Assume that \u03c6(n) /\u2208 C\u03b4 \u222a D\u03b4 \u222a ET . Then bk2(\u03c6(n)) \u2265 \u00b5k2 , \u00b5\u0302k1(\u03c6(n)) \u2264 \u00b5k1 + \u03b4. Using Pinsker\u2019s inequality and the fact that tk1(\u03c6(n)) \u2265 tk,k1 (\u03c6(n)):\nbk1(\u03c6(n)) \u2264 \u00b5\u0302k1(\u03c6(n))\n+\n\u221a\nlog(lk(\u03c6(n))) + c log(log(lk(\u03c6(n))))\n2tk1(\u03c6(n))\n\u2264 \u00b5k1 + \u03b4 + \u221a log(lk(n)) + c log(log(lk(n)))\n2\u230alk(n)/(2(\u03b3 + 1))\u230b .\nNow select \u03b4 < (\u00b5k2 \u2212 \u00b5k)/2 and l0 such that \u221a\n(log(l0) + c log(log(l0)))/2\u230al0/(2(\u03b3 + 1))\u230b \u2264 \u03b4. If lk(n) \u2265 l0:\nbk1(\u03c6(n)) \u2264 \u00b5k1 + 2\u03b4 < \u00b5k2 \u2264 bk2(\u03c6(n)),\nwhich implies that k1 cannot be selected at time \u03c6(n) (because bk1(\u03c6(n)) < bk2(\u03c6(n))), a contradiction.\n(b) Assume that tk,k(n) \u2265 (3/2)lk(n)/(\u03b3 + 1) + 1 = lk(n)/(\u03b3 + 1) + lk(n)/(2(\u03b3 + 1)) + 1. There are at least lk(n)/(2(\u03b3 + 1)) + 1 instants n\u0303 such that lk(n\u0303) \u2212 1 is not a multiple of 1/(\u03b3 + 1), L(n\u0303) = k and k(n\u0303) = k. By the same reasoning as in (a) there exists a unique \u03c6(n) < n such that L(\u03c6(n)) = k, k(\u03c6(n)) = k , tk,k(\u03c6(n)) = \u230alk(n)/(2(\u03b3 + 1))\u230b and (lk(\u03c6(n)) \u2212 1) is not a multiple of 1/(\u03b3 + 1). So bk(\u03c6(n)) \u2265 bk2(\u03c6(n)). The same reasoning as that applied in (a) (replacing k1 by k) yields \u03c6(n) \u2208 C\u03b4 \u222aD\u03b4 \u222a ET .\nWe define BT\u01eb,l0 = {n : n \u2208 BT\u01eb , lk(n) \u2265 l0}, and we have that |BT\u01eb | \u2264 l0 + |BT\u01eb,l0 |. We have defined a mapping \u03c6 from BT\u01eb,l0 to C\u03b4 \u222a D\u03b4 \u222a ET . To bound the size of BT\u01eb,l0 , we use the following decomposition:\n{n : n \u2208 BT\u01eb,l0 , lk(n) \u2265 l0} \u2282 \u222an\u2032\u2208C\u03b4\u222aD\u03b4\u222aET {n : n \u2208 BT\u01eb,l0 , \u03c6(n) = n\u2032}.\nLet us fix n\u2032. If n \u2208 BT\u01eb,l0 and \u03c6(n) = n\u2032, then \u230alk(n)/(2(\u03b3 + 1))\u230b \u2208 \u222ak\u2032\u2208N(k)\\{k2}{tk,k\u2032(n\u2032)} and lk(n)\nis incremented at time n because L(n) = k. Therefore:\n|{n : n \u2208 BT\u01eb,l0 , \u03c6(n) = n\u2032}| \u2264 2\u03b3(\u03b3 + 1).\nUsing union bound, we obtain the desired result:\n|BT\u01eb | \u2264 l0+|BT\u01eb,l0 | \u2264 O(1)+2\u03b3(\u03b3+1)(|C\u03b4 |+|D\u03b4|+|ET |).\nBound on E|C\u03b4|. We apply Lemma B.1 with \u039b(s) = {n : L(n) = k, lk(n) = s}, and \u039b = \u222as\u22651\u039b(s). Then of course, |\u039b(s)| \u2264 1 for all s. Moreover by design, tk(n) \u2265 s/(\u03b3 + 1) when n \u2208 \u039b(s), so we can choose any \u01eb < 1/(\u03b3 + 1) in Lemma B.1. Now C\u03b4 = {n \u2208 \u039b : |\u00b5\u0302k(n)\u2212 \u00b5k| > \u03b4}. From (8), we get E|C\u03b4| < \u221e.\nBound on E|D\u03b4|. Let k\u2032 \u2208 N(k) \\ {k2}. Define for any s, \u039b(s) = {n : L(n) = k, k(n) = k\u2032, tk\u2032(n) = s}, and \u039b = \u222as\u22651\u039b(s). We have |\u039b(s)| \u2264 1, and for any n \u2208 \u039b(s), tk\u2032(n) = s \u2265 \u01ebs for any \u01eb < 1. We can now apply Lemma B.1 (where k is replaced by k\u2032). Note that D\u03b4,k\u2032 = {n \u2208 \u039b : |\u00b5\u0302k\u2032 (n) \u2212 \u00b5k\u2032 | > \u03b4}, and hence (8) leads to E|D\u03b4,k\u2032 | < \u221e, and thus E|D\u03b4| < \u221e.\nBound on E|ET |. We can show as in (Garivier & Cappe\u0301, 2011) (the analysis of KL-UCB) that E|ET | = O(log(log(T ))) (more precisely, this result is a simple application of Theorem 10 in (Garivier & Cappe\u0301, 2011)).\nWe have shown that E|BT\u01eb | = O(log(log(T ))), and hence E[lk(T )] = O(log(log(T ))), which concludes the proof of Theorem C.1."}, {"heading": "D. Proofs for non-stationary environments", "text": "To simplify the notation, we remove the superscript \u03c4 throughout the proofs, e.g t\u03c4k(n) and l \u03c4 k(n) are denoted by tk(n) and lk(n).\nD.1. A lemma for sums over a sliding window\nWe will use Lemma D.1 repeatedly to bound the number of times some events occur over a sliding window of size \u03c4 .\nLemma D.1 Let A \u2282 N, and \u03c4 \u2208 N fixed. Define a(n) = \u2211n\u22121\nt=n\u2212\u03c4 1{t \u2208 A}. Then for all T \u2208 N and s \u2208 N we have the inequality:\nT \u2211\nn=1\n1{n \u2208 A, a(n) \u2264 s} \u2264 s\u2308T/\u03c4\u2309. (14)\nAs a consequence, for all k \u2208 {1, . . . ,K}, we have: T \u2211\nn=1\n1{k(n) = k, tk(n) \u2264 s} \u2264 s\u2308T/\u03c4\u2309, (15)\nT \u2211\nn=1\n1{L(n) = k, lk(n) \u2264 s} \u2264 s\u2308T/\u03c4\u2309.\nThese inequalities are obtained by choosing A = {n : k(n) = k} and A = {n : L(n) = k} in (14).\nProof. We decompose {1, . . . , T } into intervals of size \u03c4 : {1, . . . , \u03c4} , {\u03c4 + 1, . . . , 2\u03c4} etc. We have:\nT \u2211\nn=1\n1{n \u2208 A, a(n) \u2264 s}\n\u2264 \u2308T/\u03c4\u2309\u22121 \u2211\ni=0\n\u03c4 \u2211\nn=1\n1{n+ i\u03c4 \u2208 A, a(n+ i\u03c4) \u2264 s}. (16)\nFix i and assume that \u2211\u03c4 n=1 1{n + i\u03c4 \u2208 A, a(n + i\u03c4) \u2264 s} > s. Then there must exist n\u2032 < \u03c4 such that n\u2032 \u2208 A and \u2211n\u2032\nn=1 1{n + i\u03c4 \u2208 A, a(n + i\u03c4) \u2264 s} = s. Since a(n\u2032+i\u03c4) \u2265 \u2211n \u2032\nn=1 1{n+i\u03c4 \u2208 A, a(n+i\u03c4) \u2264 s}, we have a(n\u2032 + i\u03c4) \u2265 s. As n\u2032 \u2208 A, we must have a(n\u2032\u2032 + i\u03c4) \u2265 (s+ 1) for all n\u2032\u2032 > n\u2032 such that n\u2032\u2032 \u2208 A. So\n\u03c4 \u2211\nn=1\n1{n+ i\u03c4 \u2208 A, a(n+ i\u03c4) \u2264 s}\n=\nn\u2032 \u2211\nn=1\n1{n+ i\u03c4 \u2208 A, a(n+ i\u03c4) \u2264 s} = s,\nwhich is a contradiction. Hence, for all i:\n\u03c4 \u2211\nn=1\n1{n+ i\u03c4 \u2208 A, a(n+ i\u03c4) \u2264 s} \u2264 s,\nand substituting in (16) gives the desired result:\nT \u2211\nn=1\n1{n \u2208 A, a(n) \u2264 s} \u2264 \u2308T/\u03c4\u2309\u22121 \u2211\ni=0\ns = s\u2308T/\u03c4\u2309.\nD.2. Regret of SW-KL-UCB\nIn order to analyze the regret of SW-OSUB , we first have to analyze the regret SW-KL-UCB on which SW-OSUB is based.\nTheorem D.2 Let \u2206: 2\u03c4\u03c3 < \u2206 < \u22060. Assume that for any n \u2265 1, \u00b5\u22c6(n) \u2208 [a, 1 \u2212 a] for some a > 0. Further suppose that \u00b5k(\u00b7) is \u03c3-Lipschitz for any k. The regret per\nunit time under \u03c0 =SW-KL-UCB with a sliding window of size \u03c4 satisfies: if a > \u03c3\u03c4 , then for any T \u2265 1,\nR\u03c0(T )\nT \u2264 H(\u2206, T ) T \u2206\n+K (\n1 + g \u22121/2 0 ) log(\u03c4) + c log(log(\u03c4)) + C1 2\u03c4(\u2206\u2212 2\u03c4\u03c3)2 ,\nwhere C1 is a positive constant and g0 = (a\u2212\u03c3\u03c4)(1\u2212a+ \u03c3\u03c4)/2.\nRecall that due to the changing environment and the use of a sliding window, the empirical reward is a biased estimator of the average reward, and that its bias is upper bounded by \u03c3\u03c4 .\nTo ease the regret analysis, we first provide bounds on the empirical reward. Unlike in the stationary case, the empirical reward \u00b5\u0302k(n) is not a sum of tk(n) i.i.d. variables. We define Xk(n\u2032, n) = Xk(n\u2032)+(\u00b5k(n)+\u03c3|n\u2032\u2212n|\u2212\u00b5k(n\u2032)) , Xk(n \u2032, n) = Xk(n \u2032)+(\u00b5k(n)\u2212\u03c3|n\u2032\u2212n|\u2212\u00b5k(n\u2032)) and:\n\u00b5\u0302 k (n) =\n1\ntk(n)\nn \u2211\nn\u2032=n\u2212\u03c4\nXk(n \u2032, n)1{k(n\u2032) = k},\n\u00b5\u0302k(n) = 1\ntk(n)\nn \u2211\nn\u2032=n\u2212\u03c4\nXk(n \u2032, n)1{k(n\u2032) = k}.\nThen of course, \u00b5\u0302 k (n) \u2264 \u00b5\u0302k(n) \u2264 \u00b5\u0302k(n).\nNow the regret under \u03c0=SW-OSUB is given by:\nR\u03c0(T ) = T \u2211\nn=1\nK \u2211\nk=1\n(\u00b5k\u22c6(n)\u2212 \u00b5k(n))P[k(n) = k].\nWe define Imin = 2(\u2206 \u2212 2\u03c4\u03c3)2. Let \u01eb > 0 and K\u03c4 = (1 + \u01eb) log(\u03c4)+c log(log(\u03c4))Imin . We introduce the following sets of events:\n(i) A = \u222aKk=1Ak, where\nAk = {1 \u2264 n \u2264 T : k(n) = k, |\u00b5k(n)\u2212 \u00b5k\u22c6(n)| < \u2206},\nAk is the set of times at which k is chosen, and k is \u201dclose\u201d to the optimal decision. Note that, by definition, |A| \u2264 H(\u2206, T ).\n(ii) B = {1 \u2264 n \u2264 T : bk\u22c6(n) \u2264 \u00b5k\u22c6(n) \u2212 \u03c4\u03c3}. B is the set of times at which the index bk\u22c6(n) underestimates the average reward of the optimal decision (with an error greater than the bias \u03c4\u03c3).\n(iii) C = \u222aKk=1Ck , Ck = {1 \u2264 n \u2264 T : k(n) = k, tk(n) \u2264 K\u03c4}. Ck is the set of times at which k is selected and it has been tried less than K\u03c4 times.\n(iv) D = \u222aKk=1Dk, Dk = {1 \u2264 n \u2264 T : k(n) = k, n /\u2208 (A\u222aB \u222aC)}. Dk is the set of times where (a) k is chosen, (b) k has been tried more than K\u03c4 times, (c) k is not close to the optimal decision, and (d) the average reward of the optimal decision is not underestimated.\nWe will show that: \u2211\nn\u2208A\n(\u00b5\u2217(n)\u2212 \u00b5k(n)(n)) \u2264 \u2206H(\u2206, T ). (17)\nand the following inequalities\nE[|B|] \u2264 O(T/\u03c4), E[|Ck|] \u2264 K\u03c4 \u2308T/\u03c4\u2309,\nE[|Dk]] \u2264 T\n(\u03c4 log(\u03c4)c)g0\u01eb2 .\nWe deduce that:\nR\u03c0(T ) \u2264 \u2206H(\u2206, T ) +O(T/\u03c4)\n+KK\u03c4 \u230aT/\u03c4\u230b+ KT (\u03c4 log(\u03c4)c)g0\u01eb2 ,\nwhich proves Theorem D.2.\nProof of (17). Let n \u2208 Ak. If n \u2208 Ak, by definition we have |\u00b5k\u22c6(n) \u2212 \u00b5k(n)| < \u2206. Then if k(n) = k, we have that \u00b5\u2217(n)\u2212 \u00b5k(n)(n) \u2264 \u2206 so that:\n\u2211\nn\u2208A\n(\u00b5\u2217(n)\u2212 \u00b5k(n)(n)) \u2264 \u2206|A| \u2264 \u2206H(\u2206, T ),\nwhich completes the proof of (17).\nBound on E[|B|]. Let n \u2208 B. Note that \u00b5\u0302 k\u22c6 (n) \u2264 \u00b5\u0302k\u22c6(n) \u2264 bk\u22c6(n). Since bk\u22c6(n) \u2264 \u00b5k\u22c6(n)\u2212\u03c3\u03c4 , we deduce that: \u00b5\u0302\nk\u22c6 (n) \u2264 \u00b5k\u22c6(n)\u2212 \u03c3\u03c4 . Now we have:\nP[n \u2208 B] = P[bk\u22c6(n) \u2264 \u00b5k\u22c6(n)\u2212 \u03c3\u03c4 ] = P[tk\u22c6(n)I (\u00b5\u0302k\u22c6(n), \u00b5k\u22c6(n)\u2212 \u03c3\u03c4)\n\u2265 log(\u03c4) + c log(log(\u03c4))] (a) \u2264 P[tk\u22c6(n)I (\n\u00b5\u0302 k\u22c6 (n), \u00b5k\u22c6(n)\u2212 \u03c3\u03c4\n)\n\u2265 log(\u03c4) + c log(log(\u03c4))] (b) \u2264 2e \u03c4(log(\u03c4))c\u22122 ,\nwhere (a) is due to the fact that \u00b5\u0302 k\u22c6 (n) \u2264 \u00b5\u0302k\u22c6(n), and (b) is obtained applying Lemma B.3. Hence: E[|B|] \u2264 O(T/\u03c4).\nBound on E[|Ck|]. Using Lemma D.1, we get |Ck| \u2264 K\u03c4 \u2308T/\u03c4\u2309, and hence |C| \u2264 KK\u03c4 \u230aT/\u03c4\u230b.\nBound on E[|Dk|]. We will prove that n \u2208 Dk implies that \u00b5\u0302k(n) deviates from its expectation by at least f(\u01eb, Imin) > 0 so that:\nP[n \u2208 Dk] \u2264 P [ \u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)] > f(\u01eb, Imin) ] .\nLet n \u2208 Dk. Since k(n) = k and bk\u22c6(n) \u2265 \u00b5k\u22c6(n) \u2212 \u03c3\u03c4 , we have bk(n) \u2265 \u00b5k\u22c6(n) \u2212 \u03c3\u03c4 . We decompose Dk as follows:\nDk = Dk,1 \u222aDk,2 Dk,1 = {n \u2208 Dk : \u00b5\u0302k(n) \u2265 \u00b5k\u22c6(n)\u2212 \u03c3\u03c4} Dk,2 = {n \u2208 Dk : \u00b5\u0302k(n) \u2264 \u00b5k\u22c6(n)\u2212 \u03c3\u03c4}\nIf n \u2208 Dk,1, \u00b5\u0302k(n)\u2212E[\u00b5\u0302k(n)] \u2265 \u00b5k\u22c6(n)\u2212\u00b5k(n)\u22122\u03c3\u03c4 > 0 so that \u00b5\u0302k(n) indeed deviates from its expectation. Now let n \u2208 Dk,2. We have:\nP[n \u2208 Dk,2] \u2264 P[bk(n) \u2265 \u00b5k\u22c6(n)\u2212 \u03c3\u03c4, n \u2208 Dk,2] = P[tk(n)I (\u00b5\u0302k(n), \u00b5k\u22c6(n)\u2212 \u03c3\u03c4)\n\u2264 log(\u03c4) + c log(log(\u03c4)), n \u2208 Dk,2] (a) \u2264 P[K\u03c4I (\n\u00b5\u0302k(n), \u00b5k\u22c6(n)\u2212 \u03c3\u03c4 )\n\u2264 log(\u03c4) + c log(log(\u03c4)), tk(n) \u2265 K\u03c4 ]\n= P\n[\nI ( \u00b5\u0302k(n), \u00b5k\u22c6(n)\u2212 \u03c3\u03c4 ) \u2264 Imin 1 + \u01eb , tk(n) \u2265 K\u03c4 ] ,\nwhere in (a), we used the facts that: \u00b5\u0302k(n) \u2264 \u00b5k\u22c6(n)\u2212\u03c3\u03c4 , \u00b5\u0302k(n) \u2265 \u00b5\u0302k(n), and tk(n) \u2265 K\u03c4 (n /\u2208 C). It is noted that since n /\u2208 Ak, by Pinkser\u2019s inequality we have that: I(\u00b5k(n) + \u03c4\u03c3, \u00b5k\u22c6 (n) \u2212 \u03c4\u03c3) \u2265 2(\u00b5k\u22c6(n) \u2212 \u00b5k(n) \u2212 2\u03c4\u03c3)2 \u2265 2(\u2206 \u2212 2\u03c4\u03c3)2 = Imin. By continuity and monotonicity of the KL divergence, there exists a unique positive function f such that:\nI (\u00b5k(n) + \u03c3\u03c4 + f(\u01eb, Imin), \u00b5k\u22c6(n)\u2212 \u03c3\u03c4) = Imin 1 + \u01eb ,\n\u00b5k(n) + \u03c3\u03c4 + f(\u01eb, Imin) \u2264 \u00b5k\u22c6(n)\u2212 \u03c3\u03c4.\nWe are interested in the asymptotic behavior of f when \u01eb , Imin both tend to 0 . Define \u00b5\u2032 , \u00b5\u2032\u2032 and \u00b50 such that\n\u00b5k(n) + \u03c3\u03c4 \u2264 \u00b5\u2032 \u2264 \u00b5\u2032\u2032 \u2264 \u00b50 = \u00b5k\u22c6(n)\u2212 \u03c3\u03c4.\nand\nI(\u00b5\u2032, \u00b50) = Imin , I(\u00b5 \u2032\u2032, \u00b50) = Imin 1 + \u01eb .\nUsing the equivalent (11) given in Lemma B.4, there exists a function a such that:\n(\u00b50 \u2212 \u00b5\u2032)2 \u00b50(1\u2212 \u00b50) (1 + a(\u00b50 \u2212 \u00b5\u2032)) = Imin,\n(\u00b50 \u2212 \u00b5\u2032\u2032)2 \u00b50(1 \u2212 \u00b50) (1 + a(\u00b50 \u2212 \u00b5\u2032\u2032)) = Imin 1 + \u01eb .\nwith a(\u03b4) \u2192 0 when \u03b4 \u2192 0+. It is noted that 0 \u2264 \u00b50 \u2212 \u00b5\u2032\u2032 \u2264 \u00b50 \u2212 \u00b5\u2032 = o(1) when Imin \u2192 0+ by continuity of the KL divergence. Hence:\n\u00b5\u2032\u2032 \u2212 \u00b5\u2032 = ( \u01eb\n2 + o(1)\n)\n\u221a\n\u00b50(1\u2212 \u00b50)Imin.\nUsing the inequality\nf(\u01eb, Imin) = \u00b5 \u2032\u2032 \u2212 (\u00b5k(n) + \u03c3\u03c4)\n\u2265 \u00b5\u2032\u2032 \u2212 \u00b5\u2032 = \u01eb 2 \u221a \u00b50(1 \u2212 \u00b50)Imin,\nwe have proved that:\n2f(\u01eb, Imin) 2 \u2265 \u01eb2g0Imin + o(\u01eb2)\nwith g0 = (a\u2212 \u03c3\u03c4)(1 \u2212 a+ \u03c3\u03c4)/2.\nTherefore, since E[\u00b5\u0302k(n)] \u2264 \u00b5k(n) + \u03c3\u03c4 , as claimed, we have\nP[n \u2208 Dk] \u2264 P [\n\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)] \u2265 f(\u01eb, Imin) , tk(n) \u2265 K\u03c4 ] .\nWe now apply Lemma 4.3 with n \u2212 \u03c4 in place of n0, K\u03c4 in place of s and \u03c6 = n if tk(n) \u2265 K\u03c4 and \u03c6 = T + 1 otherwise. We obtain, for all n:\nP[n \u2208 Dk] \u2264 P [\n\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)] \u2265 f(\u01eb, Imin), tk(n) \u2265 K\u03c4 ]\n\u2264 exp ( \u22122K\u03c4f(\u01eb, Imin)2 ) \u2264 1 (\u03c4 log(\u03c4)c)g0\u01eb2 ,\nand we get the desired bound by summing over n:\nE[|Dk|] = T \u2211\nn=1\nP[n \u2208 Dk] \u2264 T\n(\u03c4 log(\u03c4)c)g0\u01eb2 .\nD.3. Proof of Theorem 5.1\nWe first introduce some notations. For any set A of instants, we use the notation: A[n0, n] = A\u2229{n0, . . . , n0+ \u03c4}. Let n0 \u2264 n. We define tk(n0, n) the number of times k has been chosen during interval {n0, . . . , n0 + \u03c4}, lk(n0, n) the number of times k has been the leader, and tk,k\u2032 (n0, n) the number of times k\u2032 has been chosen while k was the leader:\ntk(n0, n) = n \u2211\nn\u2032=n0\n1{k(n\u2032) = k},\nlk(n0, n) = n \u2211\nn\u2032=n0\n1{L(n\u2032) = k},\ntk,k\u2032 (n0, n) = n \u2211\nn\u2032=n0\n1{L(n\u2032) = k, k(n\u2032) = k\u2032}.\nNote that lk(n \u2212 \u03c4, n) = lk(n), tk(n \u2212 \u03c4, n) = tk(n) and tk,k\u2032 (n \u2212 \u03c4, n) = tk,k\u2032 (n). Given \u2206 > 0, we define the\nset of instants at which the average reward of k is separated from the average reward of its neighbours by at least \u2206:\nNk(\u2206) = \u2229(k\u2032,k)\u2208E{n : |\u00b5k(n)\u2212 \u00b5k\u2032(n)| > \u2206}.\nWe further define the amount of time that k is suboptimal, k is the leader, and it is well separated from its neighbors:\nLk(\u2206) = {n : L(n) = k 6= k\u22c6(n), n \u2208 Nk(\u2206)}.\nBy definition of the regret under \u03c0 =SW-OSUB :\nR\u03c0(T ) =\nT \u2211\nn=1\n\u2211\nk 6=k\u22c6(n)\n(\u00b5k\u22c6(n)\u2212 \u00b5k(n))P[k(n) = k].\nTo bound the regret, as in the stationary case, we split the regret into two components: the regret accumulated when the leader is the optimal arm, and the regret generated when the leader is not the optimal arm. The regret when the leader is suboptimal satisfies:\nT \u2211\nn=1\n\u2211\nk 6=k\u22c6(n)\n(\u00b5k\u22c6(n)\u2212 \u00b5k)1{k(n) = k, L(n) 6= k\u22c6(n)}\n\u2264 T \u2211\nn=1\n1{L(n) 6= k\u22c6(n)}\n\u2264 T \u2211\nn=1\n\u2211\nk 6=k\u22c6(n)\n1{L(n) = k 6= k\u22c6(n)}\n\u2264 T \u2211\nn=1\n\u2211\nk 6=k\u22c6(n)\n1{n \u2208 Lk(\u2206)}\n+ 1{\u2203k\u2032 : (k, k\u22c6) \u2208 E : |\u00b5k(n)\u2212 \u00b5k\u2032(n)| \u2264 \u2206} \u2264 ( K \u2211\nk=1\n|Lk(\u2206)[0, T ]|+H(\u2206, T ) ) .\nTherefore the regret satisfies:\nR\u03c0(T ) \u2264 ( H(\u2206, T ) + K \u2211\nk=1\nE[|Lk(\u2206)[0, T ]|] )\n+\nT \u2211\nn=1\n\u2211\n(k,k\u22c6(n))\u2208E\n(\u00b5k\u22c6(n)\u2212 \u00b5k(n))P[k(n) = k].\n(18)\nThe second term of the r.h.s in (18) is the regret of SWOSUB when k\u22c6(n) is the leader. This term can be analyzed using the same techniques as those used for the analysis of SW-KL-UCB and is upper bounded by the regret of SWKL-UCB. It remains to bound the first term of the r.h.s in (18).\nTheorem D.3 Consider \u2206 > 4\u03c4\u03c3. Then for all k:\nE[|Lk(\u2206)[0, T ]|] \u2264 C1 \u00d7 T log(\u03c4)\n\u03c4(\u2206 \u2212 4\u03c4\u03c3)2 , (19)\nwhere C1 > 0 does not depend on T , \u03c4 , \u03c3 and \u2206.\nSubstituting (19) in (18), we obtain the announced result.\nD.4. Proof of Theorem D.3\nIt remains to prove Theorem D.3. Define \u03b4 = (\u2206\u22124\u03c4\u03c3)/2. We can decompose {1, . . . , T } into at most \u2308T/\u03c4\u2309 intervals of size \u03c4 . Therefore, to prove the theorem, it is sufficient to prove that for all n0 \u2208 Lk(\u2206) we have:\nE[|Lk(\u2206)[n0, n0 + \u03c4 ]|] \u2264 O ( log(\u03c4)\n\u03b42\n)\n.\nIn the remaining of the proof, we consider an interval {n0, . . . , n0 + \u03c4}, with n0 \u2208 Lk(\u2206) fixed. It is noted that the best neighbour of k changes with time. We define k2(n) the best neighbor of k at time n. From the Lipschitz assumption and the fact that \u2206 > 4\u03c4\u03c3, we have that for all n \u2208 {n0, . . . , n0 + \u03c4}, k2(n) = k2(n0). Indeed for all n \u2208 {n0, . . . , n0 + \u03c4}:\n\u00b5k2(n0)(n)\u2212 \u00b5k(n) \u2265 \u00b5k2(n0)(n0)\u2212 \u00b5k(n0)\u2212 2(n\u2212 n0)\u03c3 \u2265 \u2206\u2212 2\u03c4\u03c3 \u2265 2\u03c4\u03c3 > 0.\nWe write k2 = k2(n0) = k2(n) when this does not create ambiguity. We will use the fact that, for all n \u2208 {n0, . . . , n0 + \u03c4}:\nE[\u00b5\u0302k2(n)]\u2212 E[\u00b5\u0302k(n)] \u2265 \u00b5k2(n)\u2212 \u00b5k(n)\u2212 2\u03c4\u03c3, \u2265 \u00b5k2(n0)\u2212 \u00b5k(n0)\u2212 4\u03c4\u03c3, \u2265 \u2206\u2212 4\u03c4\u03c3 = 2\u03b4 > 0.\nWe decompose Lk(\u2206)[n0, n0 + \u03c4 ] = An0\u01eb \u222aBn0\u01eb , with:\nAn0\u01eb = {n \u2208 Lk(\u2206)[n0, n0 + \u03c4 ], tk2(n) \u2265 \u01eblk(n0, n)} the set of times where k is the leader, k is not the optimal arm, and its best neighbor k2 has been tried sufficiently many times during interval {n0, . . . , n0 + \u03c4},\nBn0\u01eb = {n \u2208 Lk(\u2206)[n0, n0 + \u03c4 ], tk2(n) \u2264 \u01eblk(n0, n)} the set of times where k is the leader, k is not the optimal arm, and its best neighbor k2 has been little tried during interval {n0, . . . , n0 + \u03c4}.\nBound on E[An0\u01eb ]. Let n \u2208 An0\u01eb . We recall that E[\u00b5\u0302k2(n)] \u2212 E[\u00b5\u0302k(n)] \u2265 2\u03b4, so that the reward of k or k2 must be badly estimated at time n:\nP[n \u2208 An0\u01eb ] \u2264 P[|\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4] + P[|\u00b5\u0302k2(n)\u2212 E[\u00b5\u0302k2(n)]| > \u03b4].\nWe apply Lemma B.6, with k\u2032 = k2, \u2206k,k\u2032 = 2\u03b4, \u039b(s) = {n \u2208 An0\u01eb , lk(n0, n) = s}, tk2(n) \u2265 \u01eblk(n0, n) = \u01ebs. By design of SW-OSUB : tk(n) \u2265 lk(n0, n)/(\u03b3 + 1) = s/(\u03b3+1). Using the fact that |\u039b(s)| \u2264 1 for all s, we have that:\nE[An0\u01eb ] \u2264 O ( log(\u03c4)\n\u01eb\u03b42\n)\n.\nBound on E[Bn0\u01eb ]. Define l0 such that\n\u221a\nlog(l0) + c log(log(l0))\n2\u230al0/(2(\u03b3 + 1))\u230b \u2264 \u03b4.\nIn particular we can choose l0 = 2(\u03b3 + 1)(log(1/\u03b4)/\u03b42). Indeed, with such a choice we have that\n\u221a\nlog(l0) + c log(log(l0))\n2\u230al0/(2(\u03b3 + 1))\u230b \u223c \u03b4/2 , \u03b4 \u2192 0+.\nLet \u01eb < 1/(2(\u03b3 + 1)), and define the following sets:\nCn0\u03b4 is the set of instants at which the average reward of the leader k is badly estimated:\nCn0\u03b4 = {n \u2208 {n0, . . . , n0 + \u03c4} : L(n) = k 6= k\u22c6(n), |\u00b5\u0302k(n)\u2212 E[\u00b5\u0302k(n)]| > \u03b4};\nDn0\u03b4 = \u222ak\u2032\u2208N(k)\\{k2}Dn0\u03b4,k\u2032 where Dn0\u03b4,k\u2032 = {n : L(n) = k 6= k\u22c6(n), k(n) = k\u2032, |\u00b5\u0302k\u2032 (n) \u2212 E[\u00b5\u0302k\u2032 (n)]| > \u03b4}. Dn0\u03b4 is the set of instants at which k is the leader, k\u2032 is selected and the average reward of k\u2032 is badly estimated.\nEn0 = {n \u2264 T : L(n) = k 6= k\u22c6(n), bk2(n) \u2264 E[\u00b5\u0302k2(n)]} is the set of instants at which k is the leader, and the upper confidence index bk2(n) underestimates the average reward E[\u00b5\u0302k2 (n)].\nLet n \u2208 Bn0\u01eb . Write s = lk(n0, n), and we assume that s \u2265 l0. Since tk2(n0, n) \u2264 \u01eblk(n0, n) and the fact that lk(n0, n) = tk2(n0, n) + \u2211 k\u2032\u2208N(k)\\{k2} tk\u2032(n0, n), we must have (a) there exists k1 \u2208 N(k) \\ {k, k2} such that tk1(n0, n) \u2265 s/(\u03b3 + 1) or (b) tk1(n0, n) \u2265 (3/2)s/(\u03b3 + 1) + 1. Since tk,k(n) and tk,k2(n) are incremented only at times when k(n) = k and k(n) = k2 respectively,\nthere must exist a unique index \u03c6(n) \u2208 {n0, . . . , n0 + \u03c4} such that either: (a) tk,k1(\u03c6(n)) = \u230as/(2(\u03b3 + 1))\u230b and k(\u03c6(n)) = k1; or (b) tk,k2(\u03c6(n)) = \u230a(3/2)s/(\u03b3 + 1)\u230b and k(n) = k and lk(\u03c6(n)) is not a multiple of 3. In both cases, as in the proof of theorem C.1, we must have that \u03c6(n) \u2208 Cn0\u03b4 \u222aDn0\u03b4 \u222aEn0 . We now upper bound the number of instants n which are associated to the same \u03c6(n). Let n, n\u2032 \u2208 Bn0\u01eb and s = lk(n0, n). We see that \u03c6(n\u2032) = \u03c6(n) implies either \u230alk(n0, n\u2032)/(2(\u03b3 + 1))\u230b = \u230alk(n0, n)/(2(\u03b3 + 1))\u230b or \u230a(3/2)lk(n0, n\u2032)/(\u03b3 + 1)\u230b = \u230a(3/2)lk(n0, n)/(\u03b3 + 1)\u230b. Furthermore, n\u2032 7\u2192 lk(n0, n\u2032) is incremented at time n\u2032. Hence for all n \u2208 Bn0\u01eb :\n|n\u2032 \u2208 Bn0\u01eb , \u03c6(n\u2032) = \u03c6(n)| \u2264 2\u03b3(\u03b3 + 1). We have established that:\n|Bn0\u01eb | \u2264 l0 + 2\u03b3(\u03b3 + 1)(|Cn0\u03b4 |+ |Dn0\u03b4 |+ |En0 |) = 2(\u03b3 + 1) log(1/\u03b4)/\u03b42\n+ 2\u03b3(\u03b3 + 1)(|Cn0\u03b4 |+ |Dn0\u03b4 |+ |En0 |). We complete the proof by providing bounds of the expected sizes of sets Cn0\u03b4 , D n0 \u03b4 and E n0 .\nBound of E[Cn0\u03b4 ]: Using Lemma B.5 with \u039b(s) = {n \u2208 Cn0\u03b4 , lk(n0, n) = s}, and by design of SW-OSUB : tk(n) \u2265 lk(n0, n)/(\u03b3 + 1) = s/(\u03b3 + 1). Since |\u039b(s)| \u2264 1 for all s, we have that:\nE[|Cn0\u03b4 |] \u2264 O ( log(\u03c4)\n\u03b42\n)\n.\nBound of E[Dn0\u03b4 ]: Using Lemma B.5 with \u039b(s) = {n \u2208 Dn0\u03b4 , tk,k\u2032(n0, n) = s}, and |\u039b(s)| \u2264 1 for all s, we have that:\nE[|Dn0\u03b4,k\u2032 |] \u2264 O ( log(\u03c4)\n\u03b42\n)\n.\nBound of E[En0 ]: By Lemma B.3 since lk(n) \u2264 \u03c4 :\nP[n \u2208 En0 ] \u2264 2e\u2308log(\u03c4)(log(\u03c4) + c log(log(\u03c4)))\u2309 exp(\u2212 log(\u03c4) + c log(log(\u03c4)))\n\u2264 4e \u03c4 log(\u03c4)c\u22122 .\nThus\nE[|En0 |] \u2264 4e (log \u03c4)c\u22122 .\nPutting the various bounds all together, we have:\nE[|Lk(\u2206)[n0, n0 + \u03c4 ]|] \u2264 O ( log(\u03c4)\n\u03b42\n)\n,\nfor all n0 \u2208 Lk(\u2206), uniformly in \u03b4, which concludes the proof."}, {"heading": "E. Proof of Proposition 1", "text": "The regret of UCB(\u03b4) is defined as:\nR\u03c0(T ) \u2264 \u23081/\u03b4\u2309 \u2211\nk=1\nE[tk(T )](\u00b5 \u2217 \u2212 \u00b5k).\nWe separate the arms into three different sets. {1, . . . , \u23081/\u03b4\u2309} = A\u222aB\u222aC, with: A = {k\u2217\u22121, k\u2217, k\u2217+1} the optimal arm and its neighbors, B = {k : k /\u2208 A, (k \u2212 1)\u03b4 \u2208 [x\u2217 \u2212 \u03b40, x\u2217 + \u03b40]} the arms which are not neighbors of the optimal arm, but are in [x\u2217 \u2212 \u03b40, x\u2217 + \u03b40], and C = {k : (k \u2212 1)\u03b4 /\u2208 [x\u2217 \u2212 \u03b40, x\u2217 + \u03b40]} the rest of the arms.\nWe consider \u03b4 < \u03b40/3, so that A \u2282 [x\u2217 \u2212 \u03b40, x\u2217 + \u03b40]. By our assumption on the reward function, if k \u2208 A, |x\u2217 \u2212 \u03b4(k \u2212 1)| \u2264 2\u03b4 then |\u00b5\u2217 \u2212 \u00b5k| \u2264 C2(2\u03b4)\u03b1. The regret is upper bounded by:\nR\u03c0(T ) \u2264 TC2(2\u03b4)\u03b1 + \u2211\nk\u2208B\u222aC\nE[tk(T )](\u00b5 \u2217 \u2212 \u00b5k).\nUsing the fact that \u00b5\u2217 \u2212 \u00b5k\u2217 \u2264 C2\u03b4\u03b1 and \u2211\u23081/\u03b4\u2309\nk=1 E[tk(T )] \u2264 T , the bound becomes:\nR\u03c0(T ) \u2264 TC2(3\u03b4)\u03b1 + \u2211\nk\u2208B\u222aC\nE[tk(T )](\u00b5k\u2217 \u2212 \u00b5k).\nBy (Auer et al., 2002) (the analysis of UCB), for all k, E[tk(T )] \u2264 8 log(T )/(\u00b5k\u2217 \u2212 \u00b5k)2. Replacing in the regret upper bound:\nR\u03c0(T ) \u2264 TC2(3\u03b4)\u03b1 + \u2211\nk\u2208B\u222aC\n8 log(T )/(\u00b5k\u2217 \u2212 \u00b5k).\nIf k \u2208 B, |\u03b4(k\u2217 \u2212 1) \u2212 \u03b4(k \u2212 1)| \u2265 \u03b4(|k\u2217 \u2212 k| \u2212 1), so \u00b5k\u2217 \u2212 \u00b5k \u2265 C1\u03b4\u03b1(|k\u2217 \u2212 k| \u2212 1)\u03b1. If k \u2208 C , then |\u03b4(k\u2217 \u2212 1)\u2212 \u03b4(k\u2212 1)| \u2265 \u03b40/2, so \u00b5k\u2217 \u2212\u00b5k \u2265 C1(\u03b40/2)\u03b1. So the regret for arms in B \u222a C reduces to:\nR\u03c0(T ) \u2264 TC2(3\u03b4)\u03b1 + 8 log(T )\u23081/\u03b4\u2309 C1(\u03b40/2)\u03b1 +2\n\u23081/\u03b4\u2309 \u2211\nk=1\n8 log(T ) C1(\u03b4k)\u03b1 .\nUsing a sum-integral comparison: \u2211\u23081/\u03b4\u2309 k=1 k \u2212\u03b1 \u2264 \u2211\u23081/\u03b4\u2309 k=1 k\n\u22121 \u2264 1 + log(\u23081/\u03b4\u2309), so that: R\u03c0(T ) \u2264 TC2(3\u03b4)\u03b1\n+ 8 log(T ) ( \u23081/\u03b4\u2309 C1(\u03b40/2)\u03b1 + 2(1 + log(\u23081/\u03b4\u2309)) C1\u03b4\u03b1 ) .\nSetting \u03b4 = (log(T )/ \u221a T )1/\u03b1, the regret becomes:\nR\u03c0(T ) \u2264 TC2(3\u03b1)(log(T )/ \u221a T )+\n8 log(T )\n( \u2308( \u221a T/ log(T ))1/\u03b1\u2309 C1(\u03b40/2)\u03b1 + 2(1 + log(T )) C1 log(T )/ \u221a T ) .\nwe have used the fact that \u23081/\u03b4\u2309 \u2264 T .\nR\u03c0(T ) \u2264 C2(3\u03b1) log(T ) \u221a T\n+8\n( \u221a T + 1\nC1(\u03b40/2)\u03b1 +\n2 \u221a T (1 + log(T ))\nC1\n)\nLetting T \u2192 \u221e gives the result:\nlim sup T\nR\u03c0(T )/( \u221a T log(T )) \u2264 C23\u03b1 + 16/C1."}], "references": [{"title": "The continuum-armed bandit problem", "author": ["R. Agrawal"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Agrawal,? \\Q1995\\E", "shortCiteRegEx": "Agrawal", "year": 1995}, {"title": "Finite time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Strategic bidder behavior in sponsored search auctions", "author": ["Edelman"], "venue": "In Proc. of Workshop on Sponsored Search Auctions, ACM Electronic Commerce,", "citeRegEx": "B. and Edelman.,? \\Q2005\\E", "shortCiteRegEx": "B. and Edelman.", "year": 2005}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Online optimization in x-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bubeck et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2008}, {"title": "Kullback-leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Regret and convergence bounds for a class of continuum-armed bandit problems", "author": ["E.W. Cope"], "venue": "IEEE Trans. Automat. Contr.,", "citeRegEx": "Cope,? \\Q2009\\E", "shortCiteRegEx": "Cope", "year": 2009}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA), pp", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Garivier and Capp\u00e9,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Capp\u00e9", "year": 2011}, {"title": "On upper-confidence bound policies for non-stationary bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "In Proc. of Algorithmic Learning Theory (ALT),", "citeRegEx": "Garivier and Moulines,? \\Q2008\\E", "shortCiteRegEx": "Garivier and Moulines", "year": 2008}, {"title": "Bandit Processes and Dynamic Allocation Indices", "author": ["J.C. Gittins"], "venue": "John Wiley,", "citeRegEx": "Gittins,? \\Q1989\\E", "shortCiteRegEx": "Gittins", "year": 1989}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Graves and Lai,? \\Q1997\\E", "shortCiteRegEx": "Graves and Lai", "year": 1997}, {"title": "Change point detection and meta-bandits for online learning in dynamic environments", "author": ["C. Hartland", "N. Baskiotis", "S. Gelly", "O. Teytaud", "M. Sebag"], "venue": "In Proc. of confe\u0301rence francophone sur l\u2019apprentissage automatique (CAp07),", "citeRegEx": "Hartland et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hartland et al\\.", "year": 2007}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "In Proc. of the 40th annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Nearly tight bounds for the continuumarmed bandit problem", "author": ["R.D. Kleinberg"], "venue": "In Proc. of the conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Kleinberg,? \\Q2004\\E", "shortCiteRegEx": "Kleinberg", "year": 2004}, {"title": "Adaptive treatment allocation and the multiarmed bandit problem", "author": ["T.L. Lai"], "venue": "The Annals of Statistics,", "citeRegEx": "Lai,? \\Q1987\\E", "shortCiteRegEx": "Lai", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins", "year": 1985}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Adapting to a changing environment: the brownian restless bandits", "author": ["A. Slivkins", "E. Upfal"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Slivkins and Upfal,? \\Q2008\\E", "shortCiteRegEx": "Slivkins and Upfal", "year": 2008}, {"title": "Bound on E|ET |. We can show as in (Garivier & Capp\u00e9, 2011) (the analysis of KL-UCB) that E|ET | = O(log(log(T ))) (more precisely, this result is a simple application of Theorem 10 in (Garivier", "author": ["E|D\u03b4"], "venue": null, "citeRegEx": "\u221e.,? \\Q2011\\E", "shortCiteRegEx": "\u221e.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011).", "startOffset": 67, "endOffset": 98}, {"referenceID": 20, "context": "Introduction Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs.", "startOffset": 50, "endOffset": 80}, {"referenceID": 12, "context": "Introduction Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs.", "startOffset": 50, "endOffset": 80}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB (Auer et al., 2002) and its extensions, e.", "startOffset": 45, "endOffset": 64}, {"referenceID": 5, "context": "KL-UCB (Garivier & Capp\u00e9, 2011; Capp\u00e9 et al., 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.", "startOffset": 7, "endOffset": 51}, {"referenceID": 18, "context": ", 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.", "startOffset": 62, "endOffset": 73}, {"referenceID": 0, "context": ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.", "startOffset": 12, "endOffset": 72}, {"referenceID": 16, "context": ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.", "startOffset": 12, "endOffset": 72}, {"referenceID": 4, "context": ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.", "startOffset": 12, "endOffset": 72}, {"referenceID": 8, "context": ", 2008), linear (Dani et al., 2008), convex (Flaxman et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 9, "context": ", 2008), convex (Flaxman et al., 2005).", "startOffset": 16, "endOffset": 38}, {"referenceID": 17, "context": "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).", "startOffset": 56, "endOffset": 118}, {"referenceID": 16, "context": "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).", "startOffset": 56, "endOffset": 118}, {"referenceID": 4, "context": "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).", "startOffset": 56, "endOffset": 118}, {"referenceID": 7, "context": "Unimodal bandits have been specifically addressed in (Cope, 2009; Yu & Mannor, 2011).", "startOffset": 53, "endOffset": 84}, {"referenceID": 7, "context": "In (Cope, 2009), bandits with a continuous set of arms are studied, and the author shows that the Kiefer-Wolfowitz stochastic approximation algorithm achieves a regret of the order of O( \u221a T ) under some strong regularity assumptions on the reward function.", "startOffset": 3, "endOffset": 15}, {"referenceID": 14, "context": "(Hartland et al., 2007; Garivier & Moulines, 2008; Slivkins & Upfal, 2008; Yu & Mannor, 2011).", "startOffset": 0, "endOffset": 93}, {"referenceID": 18, "context": "The algorithm is based on KL-UCB proposed in (Lai, 1987; Capp\u00e9 et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm.", "startOffset": 45, "endOffset": 76}, {"referenceID": 5, "context": "The algorithm is based on KL-UCB proposed in (Lai, 1987; Capp\u00e9 et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm.", "startOffset": 45, "endOffset": 76}, {"referenceID": 5, "context": "OSUB can be readily extended to systems where reward distributions are within one-parameter exponential families by simply modifying the definition of arm indices as done in (Capp\u00e9 et al., 2013).", "startOffset": 174, "endOffset": 194}, {"referenceID": 5, "context": "(i) When k is the leader, the algorithm behaves like KL-UCB restricted to the arms around k, and the regret at these rounds can be analyzed as in (Capp\u00e9 et al., 2013).", "startOffset": 146, "endOffset": 166}, {"referenceID": 17, "context": "We show that combining discretization and the UCB algorithm as initially proposed in (Kleinberg, 2004) yields lower regrets than LSE in practice (see Section 7), and is orderoptimal, i.", "startOffset": 85, "endOffset": 102}, {"referenceID": 1, "context": "We denote by UCB(\u03b4) the UCB algorithm (Auer et al., 2002) applied to the discretized bandit.", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Discrete bandits We compare the performance of our algorithm to that of KL-UCB (Capp\u00e9 et al., 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 1, "context": ", 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al., 2002), and UCB-U.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": "As proven by Hoeffding (Hoeffding, 1963)[eq.", "startOffset": 23, "endOffset": 40}, {"referenceID": 1, "context": "By (Auer et al., 2002) (the analysis of UCB), for all k, E[tk(T )] \u2264 8 log(T )/(\u03bck\u2217 \u2212 \u03bck).", "startOffset": 3, "endOffset": 22}], "year": 2014, "abstractText": "We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in nonstationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).", "creator": "LaTeX with hyperref package"}}}