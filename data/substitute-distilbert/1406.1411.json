{"id": "1406.1411", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2014", "title": "Advances in Learning Bayesian Networks of Bounded Treewidth", "abstract": "actual work presents novel algorithms for locating homogeneous network structures with maximum treewidth. both exact and approximate methods are developed. the simulated knowledge combines mixed - integer linear programming formulations for structure learning mixed treewidth computation. the approximate method consists in uniformly sampling $ k $ - trees ( supervised graphs and treewidth $ k $ ), sometimes subsequently selecting, exactly or approximately, the best structure estimation moral graph requires a subgraph of p $ k $ - tree. some properties of these methods are computed and evaluated. the approaches are empirically compared to each other and to a state - of - the - art method for learning bounded treewidth structures on a representation of public data sets with up to 100 variables. the experiments show that our exact algorithm outperforms the state... the art, and that the approximate approach is fairly accurate.", "histories": [["v1", "Thu, 5 Jun 2014 15:10:40 GMT  (93kb,D)", "https://arxiv.org/abs/1406.1411v1", "23 pages, 2 figures, 3 tables"], ["v2", "Fri, 6 Jun 2014 19:51:07 GMT  (93kb,D)", "http://arxiv.org/abs/1406.1411v2", "23 pages, 2 figures, 3 tables"]], "COMMENTS": "23 pages, 2 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["siqi nie", "denis deratani mau\u00e1", "cassio polpo de campos", "qiang ji"], "accepted": true, "id": "1406.1411"}, "pdf": {"name": "1406.1411.pdf", "metadata": {"source": "CRF", "title": "Advances in Learning Bayesian Networks of Bounded Treewidth", "authors": ["Siqi Nie", "Denis Deratani Mau\u00e1", "Cassio Polpo de Campos", "Qiang Ji"], "emails": ["nies@rpi.edu.", "denis.maua@usp.br.", "cassio@idsia.ch.", "jiq@rpi.edu."], "sections": [{"heading": null, "text": "with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling k-trees (maximal graphs of treewidth k), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that k-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate."}, {"heading": "1 Introduction", "text": "Bayesian networks are graphical models widely used to represent joint probability distributions on complex multivariate domains [31]. A Bayesian network comprises two parts: a directed acyclic graph (the structure) describing the relationships among variables in the model, and a collection of conditional probability tables from which the joint distribution can be reconstructed. As the number of variables in the model increases, specifying the underlying structure becomes a tedious and difficult task, and practitioners often resort to learning Bayesian networks directly from data. Here, learning a Bayesian network refers to inferring the underlying graphical structure from data, a task well-known to be NP-hard [13]. \u2217Email: nies@rpi.edu. Affiliation: Rensselaer Polytechnic Institute, USA. \u2020Email: denis.maua@usp.br. Affiliation: Universidade de Sa\u0303o Paulo, Brazil. \u2021Email: cassio@idsia.ch. Affiliation: Dalle Molle Institute for Artificial Intelligence, Switzerland. \u00a7Email: jiq@rpi.edu. Affiliation: Rensselaer Polytechnic Institute, USA.\nar X\niv :1\n40 6.\n14 11\nv2 [\ncs .A\nLearned Bayesian networks are commonly used for drawing inferences such as querying the posterior probability of some variable after evidence is entered (a task known as belief updating), finding the mode of the joint distribution (known as most probable explanation or MAP inference), or selecting a configuration of a subset of the variables that maximizes their conditional probability (known as marginal MAP inference). All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph. Polynomial-time algorithms for such inferences do exist, but they provide no guarantees on the quality of the solution they deliver, which raises doubts as to whether occasional bad results are a consequence of suboptimal structure learning or of approximate inference. In fact, under widely believed assumptions from complexity theory, exponential time complexity in the treewidth is inevitable for any algorithm that provides provably good inferences [11, 33]. Thus, learning network structures of small treewidth is essential if one wishes to perform reliable and efficient inference. This is particularly important in the presence of missing data, as learning methods usually resort to some kind of Expectation-Maximization procedure that requires performing belief updating in the network at every iteration [25]. In those cases inefficient inference leads to great computational cost of learning; unreliable inference leads to learning underfitted/overfitted structures.\nSince estimating a network\u2019s treewidth is itself an NP-hard task [2], extending current methods for learning Bayesian networks to the case of bounded treewidth while maintaining their relative efficiency and accuracy is not trivial. In comparison to unconstrained Bayesian network learning, few algorithms have been designed for the bounded treewidth case. Korhonen and Parviainen [32] showed that learning bounded treewidth Bayesian networks is NP-hard, and developed an exact algorithm based on dynamic programming that learns optimal n-node structures of treewidth at most \u03c9 in time 3nn\u03c9+O(1), which is above the 2nnO(1) time required by the best worst-case algorithms for learning optimal Bayesian networks with no constraint on treewidth [40]. Elidan and Gould [23] combined several heuristics to treewidth computation and network structure learning in order to design approximate methods. Others have addressed the similar (but not equivalent) problem of learning undirected models of bounded treewidth [3, 12, 42]. Very recently, there seems to be an increase of interest in the topic. Berg et al. [6] showed that the problem of learning bounded treewidth Bayesian networks can be reduced to a weighted maximum satisfiability problem, and subsequently solved by weighted MAX-SAT solvers. They report experimental results showing that their approach outperforms Korhonen and Parviainen\u2019s dynamic programming approach. In the same year, Parviainen et al. [36] showed that the problem can be reduced to a mixed-integer linear program (MILP), and then solved by off-the-shelf MILP optimizers (e.g. CPLEX). Their reduced MILP problem however has exponentially many constraints in the number of variables. Following the work of Cussens [16], the authors avoid creating such large programs by a cutting plane generation mechanism, which iteratively includes a new constraint while the optimum is not found. The generation of each new constraint (cutting plane) requires solving another MILP problem. The works of [6] and [36] have been developed independently and simultaneously with our work presented here; for this reason, we do not compare our methods with\ntheirs. We intend to do so in the near future. In this paper, we present two novel ideas for score-based Bayesian network structure learning with a hard constraint on treewidth. We first introduce a mixed integer linear programming formulation of the problem (Section 3) that builds on existing MILP formulations for unconstrained structure learning of Bayesian networks [16, 17] and for computing the treewidth of a graph [27]. The designed formulation is able to find a score-maximizer Bayesian network of treewidth smaller than a given constant for models containing many more variables than Korhonen and Parviainen\u2019s method, as we empirically demonstrate in Section 5. Unlike the MILP formulation of Parviainen et al. [36], the MILP problem we generate is of polynomial size in the number of variables, and does not require the use of cutting planes techniques. This makes for a clean and succinct formulation that can be solved with a single call of a MILP optimizer. A better understanding of cases where one approach is preferred to the other is yet to be achieved.\nSince linear programming relaxations are used for solving the MILP problem, any MILP formulation can be used to provide approximate solutions and error estimates in an anytime fashion (i.e., the method can be stopped at any time during the computation with a feasible solution). However, the MILP formulations (both ours and the one proposed by Parviainen et al. [36]) cannot cope with very large domains, even if we agreed on obtaining only approximate solutions. This is because the minimum size of the MILP problems is cubic in the number of variables (hence it is difficult even to start the MILP solver for large domains), and there is probably little we can do to considerably improve this situation (a further discussion on that is given in Section 3). This limitation is observed in the experiments reported in Section 5, where our MILP formulation requires a much larger amount of time to obtain much poorer solutions for networks with over 50 variables.\nIn order to deal with large domains, we devise (in Section 4) an approximate method based on a uniform sampling of k-trees (maximal triangulated graphs of treewidth k), which is achieved by using a fast computable bijection between k-trees and Dandelion codes [10]. For each sampled k-tree, we either run an exact algorithm similar to the one proposed in [32] (when computationally appealing) to learn the scoremaximizing network whose moral graph is a subgraph of that k-tree, or we resort to a much more efficient method that takes partial variable orderings uniformly at random from a (relatively small) space of orderings that are compatible with the k-tree. We discuss the time and sample complexity of both variants, and compare it to those of similar schemes for learning unconstrained networks. We show empirically (in Section 5) that the double sampling scheme (of k-trees and partial variable orderings) is very effective in learning close to optimal structures in a selected set of data sets. We conclude in Section 6 by noting that the methods we propose can be considered as state-of-the-art, and by suggesting possible improvements. To start, Section 2 presents some background knowledge on learning Bayesian networks."}, {"heading": "2 Preliminaries", "text": "A Bayesian network is a concise graphical representation of a multivariate domain, where each random variable is associated with a node of its underlying directed acyclic graph (DAG) and local conditional probability distributions are specified for the variable given its parents in the graph (we often refer to variables and nodes in the graph interchangeably).\nLet N be {1, . . . , n} and consider a finite set X = {Xi : i \u2208 N} of categorical random variables Xi taking values in finite sets Xi. Formally, a Bayesian network is a triple (X,G, \u03b8), where G = {N,A} is a DAG whose nodes are in one-to-one correspondence with variables in X , and \u03b8 = {\u03b8i(xi, x\u03c0i)} is a set of numerical parameters specifying (conditional) probability values \u03b8i(xi, x\u03c0i) = P (xi|x\u03c0i), for every node i in G, value xi of Xi and assignment x\u03c0i to the parents \u03c0i of Xi, according to G. The structure G (that is, the DAG of the network) represents a set of stochastic independence assessments among variables in X . In particular, G represents a set of graphical Markov conditions: every variable Xi is conditionally independent of its nondescendant nonparents given its parents. As a consequence, a Bayesian network (X,G, \u03b8) uniquely defines a joint probability distribution over X as the product of its parameters [31, Chapter 3.2.3]:\nP (x1, . . . , xn;G, \u03b8) = \u220f i\u2208N \u03b8i(xi, x\u03c0i). (1)\nLearning the structure G from data is a challenging problem. One approach is to identify, for each variable, the minimal set of variables that makes that variable conditionally independent of others (Markov blanket), which is usually done by means of statistical tests of stochastic independence or information theoretic measures [41]. Alternatively, structural learning can be posed as a combinatorial optimization problem in which one seeks the structure that maximizes a score function that relates to the data likelihood, while avoiding some excessive model complexity. Commonly used score functions include the Minimum Description Length (which is equivalent to the Bayesian Information Criterion) [39], and Bayesian Dirichlet (likelihood) equivalent uniform score [9, 15, 28]. These functions follow different rationale but they all satisfy two properties: (i) they can be written as a sum of local score functions that depend only on the parent set of each node and on the data, and (ii) the local score functions can be efficiently computed and stored. Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].\nIn score-based Bayesian network learning we seek a DAG structure G\u2217 such that\nG\u2217 = argmax G\u2208Gn \u2211 i\u2208N si(\u03c0i) , (2)\nwhere Gn is the class of all DAGs with n nodes, si are local score functions that depend only on the parent set \u03c0i as given by G (i.e., the computation of each si(\u03c0i) depends only on the values that Xi and X\u03c0i take in the data set). We assume (unless otherwise stated) that local scores si(\u03c0i) have been previously computed and can be retrieved at constant time. Despite the decomposability of the score functions, the optimization\ncannot be performed locally lest it almost certainly introduce directed cycles in the graph.\nWe say that a cycle in an undirected graph has a chord if there are two nodes in the cycle which are connected by an edge outside the cycle. A chordal graph is an undirected graph in which all cycles of length four or more have a chord. Any graph can be made chordal by inserting edges, a process called chordalization [2, 8]. The treewidth of a chordal graph is the size of its largest clique minus one. The treewidth of an arbitrary undirected graph is the minimum treewidth over all chordalizations of it. The moral graph of a DAG is the undirected graph obtained by connecting any two nodes with a common child and dropping arc directions. The treewidth of a DAG is the treewidth of its corresponding moral graph. The treewidth of a Bayesian network (X,G, \u03b8) is the treewidth of the DAG G.\nAn elimination order is a linear ordering of the nodes in a graph. We say that an elimination order is perfect if for every node in the order its higher-ordered neighbors form a clique (i.e., are pairwise connected). A graph admits a perfect elimination order if and only if it is chordal. Perfect elimination orders can be computed in linear time if they exist. The elimination of a node according to an elimination order is the process of pairwise connecting all of its higher-ordered neighbors. Thus, the elimination of all nodes produces a chordal graph for which the elimination order used is perfect. The edges inserted by the elimination process are called fill-in edges. Given a perfect elimination order, the treewidth of the graph can be computed as the maximum number of higher ordered neighbors in the graph.\nThe reason why most score functions penalize model complexity (as given by the number of free numerical parameters) is that data likelihood always increases by augmenting the number of parents of a variable (and hence the number of free parameters in the model), which leads to overfitting and poor generalization. The way scores penalize model complexity generally leads to structures of bounded in-degree and helps in preventing overfitting, but even bounded in-degree graphs can have large treewidth (for instance, directed square grids have treewidth equal to the square root of the number of nodes, yet have maximum in-degree equal to two), which yields a great problem to subsequent probabilistic inferences with the model.\nThere are at least two direct reasons to aim at learning Bayesian networks of bounded treewidth: (i) As discussed previously, all known exact algorithms for probabilistic inference have exponential time complexity in the treewidth, and networks with very high treewidth are usually the most challenging for approximate methods; (ii) Previous empirical results [23, 37] suggest that bounding the treewidth might improve model performance on held-out data. There is also evidence that bounding the treewidth does not impose a great burden on the expressivity of the model for real data sets [7].\nThe goal of learning Bayesian networks of bounded treewidth is to search for G\u2217\nsuch that G\u2217 = argmax\nG\u2208Gn,k \u2211 i\u2208N si(\u03c0i) , (3)\nwhere Gn,k is the class of all DAGs of treewidth not (strictly) greater than k. From a theoretical point of view, this is no easy task. Korhonen and Parviainen [32] adapted\nSrebro\u2019s complexity result for Markov networks [42] to show that learning the structure of Bayesian networks of bounded treewidth strictly greater than one is NP-hard. Dasgupta\u2019s results also prove this hardness if the score maximizes data likelihood [20] (in the case of networks of treewidth one, that is, directed trees with at most one parent per node, learning can be performed efficiently by the Chow and Liu\u2019s algorithm [14])."}, {"heading": "3 Mixed integer linear programming", "text": "The first contribution of this work is the mixed integer linear programming (MILP) formulation that we design to exactly solve the problem of structure learning with bounded treewidth. MILP formulations have shown to be very effective to learning Bayesian networks without the treewidth bound [4, 16], surpassing other attempts in a range of data sets. Moreover, the great language power of a MILP problem allows us to encode the treewidth constraint in a natural manner, which might not be easy with other structure learning approaches [22, 29, 35, 44, 45]. We note that computing the treewidth of a graph is an NP-hard problem itself [2], even if there are linear algorithms that are only exponential in the treewidth [8] (these algorithms might be seen mostly as theoretical results, since their practical use is shadowed by very large hidden constants). Hence, one should not hope to enforce a bound on the treewidth (which should work for any chosen bound) without a machinery that is not at least as powerful as NP.\nThe novel formulation is based on combining the MILP formulation for structure learning in [17] with the MILP formulation presented in [27] for computing the treewidth of an undirected graph. There are although crucial differences, which we highlight later on. We have avoided the use of sophisticated techniques for MILP in the context of structure learning, such as constraint generation [4, 16], because we are interested in providing a clean and succinct MILP formulation, which can be ran using off-the-shelf solvers without additional coding.\nSince our formulation is a combination of two previous MILP formulations of distinct problems, we will present each formulation separately, and then describe how to combine them into a concise MILP problem."}, {"heading": "3.1 A MILP formulation for bounding the treewidth", "text": "Consider a graph G = (N,E). We begin with the MILP formulation of the class of all supergraphs of a graph G that have treewidth less than or equal to a given value w:\u2211\nj\u2208N yij \u2264 w, \u2200i \u2208 N, (4a)\n(n+ 1) \u00b7 yij \u2264 n+ zj \u2212 zi, \u2200i, j \u2208 N, (4b) yij + yji = 1, \u2200(i, j) \u2208 E, (4c)\nyij + yik \u2212 (yjk + ykj) \u2264 1, \u2200i, j, k \u2208 N, (4d) zi \u2208 [0, n], \u2200i \u2208 N, (4e)\nyij \u2208 {0, 1}, \u2200i, j \u2208 N. (4f)\nThe formulation above is based on encoding all possible elimination orders of the nodes ofG. A chordalizationG\u2032 = (N,E\u2032) ofG of treewidth at mostw can be obtained from a feasible solution (if it exists) of the program by setting E\u2032 = {ij \u2208 N \u00d7 N : yij = 1 or yji = 1}. Constraint (4a) ensures G\u2032 has treewidth at most w by bounding the number of higher-ordered neighbors of every node i (which is an alternative way of defining the treewidth of chordal graphs). The variables zi, i \u2208 N , take (real) values in [0, n] (Constraint (4e)) and partially define an elimination order of the nodes: a node i is eliminated before node j if zi < zj (the specification is partial since its allows for two nodes i and j with zi = zj). This order does not need to be linear because there are cases where multiple linearizations of the partial order are equally good in building a chordalization G\u2032 of G (i.e., in minimizing the maximum clique size of G\u2032). In such cases, two nodes i and j might be assigned the same value zi = zj indicating that eliminating zi before zj and the converse results in chordal graphs with the same treewidth. The variables yij , i, j \u2208 N , are {0, 1}-valued (Constraint (4f)) and indicate whether node i precedes j in the order (i.e., whether zi < zj) and an edge exists among them in the resulting chordal graph G\u2032 (recall that an elimination process always produces a chordal graph). Although the values zi are not forced to be integers in our formulation, in practice they will most likely be so. Constraint (4b) allows yij to be 1 only if j appears after i in the order (it in fact requires that zj \u2265 zi + 1 to allow yij to be one). Constraint (4c) ensures G\u2032 is a supergraph of G. Constraint (4d) guarantees that the elimination ordering induced by zi, i \u2208 N , is perfect for G\u2032: if j and k are higher ordered neighbors of i in G\u2032, then j and k are also neighbors in G\u2032, that is, either yjk or ykj must be 1. The practical difference of this formulation with respect to the one in [27] lies in the fact that we allow partial elimination orders, and we do not need integer variables to enforce such orders. A bottleneck is the specification of Constraint (4d), as there are \u0398(n3) such constraints. The following result is an immediate conclusion of the above reasoning.\nProposition 1 The graph G has treewidth at most w if and only if the set defined by Constraints (4) is non empty.\nProposition 2 Let zi, yij , i, j \u2208 N , be variables satisfying Constraints (4a)\u2013(4f). Then the graph G\u2032 = (E\u2032, N), where E\u2032 = {ij \u2208 N \u00d7 N : yij = 1 or yji = 1}, is a chordalization of G with treewidth at most w, and any elimination order consistent with the partial order induced by zi is perfect for G\u2032."}, {"heading": "3.2 A MILP formulation for structure learning", "text": "We now turn our attention to the MILP formulation of the structure learning part. Consider a chordal (undirected) graphM = (N,E), a perfect elimination order forM , and let yij , i, j \u2208 N , be {0, 1}-valued variables such that yij = 1 if and only if E contains ij and i is eliminated before j. For each node i in N let Fi be the collection of all allowed parent sets for that node (these sets can be specified manually by the user or simply defined as the subsets of N \\ {i} with cardinality less than a given bound). We denote an element of Fi as Fit, with t = 1, . . . , |Fi| (hence Fit \u2282 N ). The following MILP formulation specifies the class of all DAGs over N that are consistent with the\nparent sets Fi and whose moral graph is a subgraph of M :\u2211 t \u03c0it = 1, \u2200i \u2208 N, (5a)\n(n+ 1)\u03c0it \u2264 n+ vj \u2212 vi, \u2200i \u2208 N, \u2200t, \u2200j \u2208 Fit, (5b) \u03c0it \u2264 yij + yji, \u2200i \u2208 N, \u2200t, \u2200j \u2208 Fit, (5c) \u03c0it \u2264 yjk + ykj , \u2200i \u2208 N, \u2200t, \u2200j, k \u2208 Fit, (5d)\nvi \u2208 [0, n], \u2200i \u2208 N, (5e) \u03c0it \u2208 {0, 1}, \u2200i \u2208 N, \u2200t, (5f)\nwhere the scope of the \u2200t in each constraint is 1, . . . , |Fi|. A DAG D = (N,A) can be obtained from a solution to the above program by setting A = {i\u2190 j : i \u2208 N, j \u2208 N, \u03c0it = 1 and j \u2208 Fit}. The variables vi, i \u2208 N , take values in [0, n] (Constraint (5e)) and partially specify a topological order of the nodes in D: if vi > vj then j is not an ancestor of i. The variables \u03c0it, i \u2208 N , t = 1, . . . , |Fi|, are {0, 1}-valued (Constraint (5f)) and indicate whether the t-th parent set in Fi was chosen for node i. Constraint (5a) enforces that exactly one parent set is chosen for each node. Constraint (5b) forces those choices to be acyclic, that is, to respect the topological order induced by the variables vi (with ties broken arbitrarily for nodes i, j with vi = vj). Here too the order does not need to be linear. In fact, only the relative ordering of nodes that are connected in M is relevant because Constraints (5c) and (5d) ensure that arcs appear in D only if the corresponding edges in the moral graph of D exist in M (Constraint (5d) is responsible for having the moralization of the graph falling inside M ).\nProposition 3 Let vi, \u03c0it, i \u2208 N, t = 1, . . . , |Fi|, be variables satisfying Constraints (5). Then the directed graph D = (N,A), where A = {i \u2190 j : i \u2208 N, j \u2208 N, \u03c0it = 1 and j \u2208 Fit} is acyclic and consistent with every set Fi. Moreover the moral graph of D is a subgraph of M .\nA corollary of the above result is that the treewidth of D is at most the treewidth of M [8]."}, {"heading": "3.3 Combining the MILP formulations", "text": "We can now put together the two previous MILP formulations to reach the following MILP formulation for the problem of learning DAGs of treewidth bounded by a constant w:\nmaximize: \u2211 it \u03c0it \u00b7 si(Fit) (6a)\nsubject to: \u2211 j\u2208N yij \u2264 w, \u2200i \u2208 N, (6b)\n(n+ 1) \u00b7 yij \u2264 n+ zj \u2212 zi, \u2200i, j \u2208 N, (6c) yij + yik \u2212 (yjk + ykj) \u2264 1, \u2200i, j, k \u2208 N, (6d)\u2211\nt\n\u03c0it = 1, \u2200i \u2208 N, (6e)\n(n+ 1)\u03c0it \u2264 n+ vj \u2212 vi, \u2200i \u2208 N, \u2200t, \u2200j \u2208 Fit, (6f) \u03c0it \u2264 yij + yji, \u2200i \u2208 N, \u2200t, \u2200j \u2208 Fit, (6g) \u03c0it \u2264 yjk + ykj , \u2200i \u2208 N, \u2200t, \u2200j, k \u2208 Fit, (6h)\nzi \u2208 [0, n],vi \u2208 [0, n], \u2200i \u2208 N, (6i) yij \u2208 {0, 1}, \u2200i, j \u2208 N, (6j) \u03c0it \u2208 {0, 1}, \u2200i \u2208 N, \u2200t. (6k)\nAs the following result shows, the MILP formulation above specifies DAGs of bounded treewidth:\nTheorem 1 Let yij , zi, vi, \u03c0it, i, j \u2208 N, t = 1, . . . , |Pi|, be variables satisfying Constraints (6b)\u2013(6k), and define a directed graph D = (N,A), where A = {i \u2190 j : i \u2208 N, j \u2208 N, \u2203t s.t. \u03c0it = 1 and j \u2208 Fit}. Then D is a acyclic, consistent with the parents sets Pi, and has treewidth at most w.\nCorollary 1 If yij , zi, vi, \u03c0it, i, j \u2208 N, t = 1, . . . , |Pi|, maximize (6a) and satisfy (6b)\u2013(6k), then the DAG D as defined above is the solution to the optimization in (3).\nThe MILP formulation (6) can be directly fed into any off-the-shelf MILP optimizer. According to Corollary (1), the outcome will always be an optimum structure if enough resources (memory and time) are given. Standard MILP optimizers (e.g. CPLEX) often employ branch-and-bound (or branch-and-cut) procedures, which are able to be halted prematurely at any time and still provide a valid solution and an outer bound for the maximum score. Hence, the MILP formulation also provides an anytime algorithm for learning Bayesian networks of bounded treewidth: the procedure can be stopped at time and still provide an approximate solutions and error bound. Moreover, the quality of the approximation solution returned increases with time, while the error bounds monotonically decrease and eventually converge to zero."}, {"heading": "3.4 Comparison with the dynamic programming approach", "text": "To validate the practical feasibility of our MILP formulation, we compare it against the the dynamic programming method proposed previously for this problem [32], which we call K&P from now on.1 Table 1 show the time performance of our MILP formulation and that of K&P on a collection of reasonably small data sets from the UCI\n1We used the freely available code provided by the authors at http://www.cs.helsinki.fi/u/jazkorho/aistats2013/.\nrepository2 (discretized over the median value, when needed) and small values of the treewidth bound. More details about these data are presented in Section 5. The experiments have been run with a limit of 64GB in memory usage and maximum number of parents per node equal to three (the latter restriction facilitates the experiments and does not impose a constraint in the possible treewidths that can be found). While one shall be careful when directly comparing the times between methods, as the implementations use different languages (we are running CPLEX 12.4, K&P uses a Cython3 compiled Python code), we note that our MILP formulation is orders of magnitude faster than K&P, and able to solve many problems which the latter could not (in Section 5 we show the results of experiments with much larger domains). A time limit of 3h was given to the MILP, in which case its own estimation of the error is reported (in fact, it found the optimal structure in all instances, but was not able to certify it to be optimal within 3h).\nThe results in the table show that our MILP formulations largely outperforms K&P, being able to handle much larger problems. Yet we see from these experiments that both algorithms scale poorly in the number of variables. In particular, K&P cannot cope with data sets containing more than a dozen of variables. The results suggest that the MILP problems become easier as the treewidth bound increases. This is likely a consequence of the increase of the space of feasible solutions, which makes the linear relaxations used for solving the MILP problem tighter, thus reducing the computational load. This is probably aggravated by the small number of variables in these data sets (hence, by increasing the treewidth we effectively approximate an unbounded learning situation).\nWe shall demonstrate empirically in Section 5 that the quality of solutions found by the MILP approach in a reasonable amount of time degrades quickly as the number of variables reaches several dozens. Indeed, the MILP formulation is unable to find\n2Obtained from http://archive.ics.uci.edu/ml/. 3http://www.cython.org.\nreasonable solutions for data sets containing 100 variables, which is not surprising given that number of Constraints (6d) and (6h) is cubic in the number of variables; thus, as n increases even the linear relaxations of the MILP problem become hard to solve. In the next section, we present a clever sampling algorithm over the space of k-trees to overcome such limitations and handle large domains. The MILP formulation just described will set a baseline for the performance of such approximate approach.\n4 Sampling k-trees using Dandelion codes In this section we develop an approximate method for learning bounded treewidth Bayesian networks that is based on sampling graphs of bounded treewidth and subsequently finding DAGs whose moral graph is a subgraph of that graph. The approach is designed aiming at data sets with large domains, which cannot be handled by the MILP formulation.\nA naive approach to designing an approximate method would be to extend one of the sampling methods for unconstrained Bayesian network learning. For instance, we could envision a rejection sampling approach, which would sample structures using some available procedure (for instance, by sampling topological orderings and then greedily finding a DAG structure consistent with that order, as in [43]), and verify their treewidth, discarding the structure when the test fails. There are two great issues with this approach: (i) the computation of treewidth is a hard problem, and even if there are linear-time algorithms (but exponential on the treewidth), they perform poorly in practice; (ii) virtually all structures would be discarded due to the fact that complex structures tend to have larger scores than simple ones, at least for the most used score functions (their penalizations reduce the local complexity of the model, but are not able to constrain a global property such as treewidth). We empirically verified these facts, but will not report further on them here.\nAnother natural approach to the problem is to consider both an elimination order for the variables (from which the treewidth can be computed) and a topological order (from which one can greedily search for parent sets without creating cycles in the graph). It is straightforward to uniformly sample from the space of orderings, but the combined overall number of such orderings is quite high: (n!)2 \u2248 e2n logn\u22122n (from the Stirling approximation). We propose an interesting way that is more efficient in terms of the size of the sampling space, and yet can be sampled uniformly (uniform sampling is a desirable property, as it ensures a good coverage of the space and is superior to other options if one has no prior information about the search space). This approach is based on the set of k-trees.\nDefinition 1 A k-tree is defined in the following recursive way: (1) A (k + 1)-clique is a k-tree. (2) If T \u2032k = (V,E) is a k-tree with nodes V and edges E, K \u2286 V is a k-clique and v \u2208 N \\ V , then Tk = (V \u222a {v}, E \u222a {(v, x)|x \u2208 K}) is a k-tree.\nWe denote by Tn,k the set of all k-trees over n nodes. In fact, a Bayesian network with treewidth bounded by k is closely related to a k-tree. Because k-trees are exactly the maximal graphs with treewidth k (graphs to which no more edges can be added without\nincreasing their treewidth), we know that the moral graph of the optimal structure has to be a subgraph of a k-tree [32].\nThe idea is to sample k-trees and then search for the best structure whose moral graph is one of the subgraphs of the k-tree. While directly sampling a k-tree might not be trivial, Caminiti et al. [10] proposed a linear time method for coding and decoding k-trees into what is called Dandelion codes (the set of such codes is denoted by An,k). Moreover, they established a bijective mapping between codes in An,k and k-trees in Tn,k. The code (Q,S) \u2208 An,k is a pair where Q \u2286 N with |Q| = k and S is a list of n \u2212 k \u2212 2 pairs of integers drawn from N \u222a { }, where is an arbitrary number not in N . For example, Q = {2, 3, 9} and S = [(0, ), (2, 1), (8, 3), (8, 2), (1, 3), (5, 3)] is a Dandelion code of a (single) 3-tree over 11 nodes (that is , n = 11, k = 3). Dandelion codes can be sampled uniformly at random by a trivial linear-time algorithm that uniformly chooses k elements out of N to build Q, and then uniformly samples n\u2212 k \u2212 2 pairs of integers in N \u222a { }.\nTheorem 2 [10] There is a bijection mapping elements of An,k and Tn,k that is computable in time linear in n and k.\nGiven Tk \u2208 Tn,k, we can use the dynamic programming algorithm proposed in [32] to find the optimal structure whose moral graph is a subgraph of Tk. Our implementation follows the ideas in [32], but can also be seen as extending the divide-and-conquer method of [26] to account for all possible divisions of nodes. This results in the following theorem.\nTheorem 3 [32] For any fixed k, given (a k-tree) G = (N,E) \u2208 Gn,k and the scoring function for each node v \u2208 N , we can find a DAG whose moralized graph is a subgraph of G maximizing the score in time and space O(n).\nWe can combine the linear-time sampling of k-trees described in Theorem 2 with the linear-time learning of bounded structures consistent with a graph in the above theorem to obtain an algorithm for learning bounded treewidth Bayesian networks. The algorithm is described in Algorithm 1 [Version 1].\nAlgorithm 1 Learning a structure of bounded treewidth by sampling Dandelion codes. There are two versions, according to the choice for step 2.c. Input a score function si, \u2200i \u2208 N . Output a DAG Gbest. 1 Initialize \u03c0besti as an empty set for all i \u2208 N 2 Repeat until a certain number of iterations is reached: 2.a Uniformly sample (Q,S) \u2208 An,k; 2.b Decode (Q,S) into Tk \u2208 Tn,k; 2.c [Version 1] Find a DAG G that maximizes the score function and is consistent\nwith Tk. 2.c [Version 2] Sample \u03c3 using Algorithm 2, and (greedily) find a DAG G that maxi-\nmizes the score function and is consistent with both \u03c3 and Tk. 2.d If \u2211 i\u2208N si(\u03c0 G i ) > \u2211 i\u2208N si(\u03c0 best i ), update \u03c0 best i , \u2200i.\nTheorem 4 The sampling space of Algorithm 1 [Version 1] is less than en log(nk). Each of its iterations runs in linear time in n (but exponential in k).\nProof. The follow equality holds [5]. |Tn,k| = ( n\nk\n) \u00b7 ( k(n\u2212 k) + 1 )n\u2212k\u22122 . (7)\nIt is not hard to see that the maximum happens for k \u2264 n/2 (because of the symmetry of ( n k ) and of k(n\u2212 k) around n/2, while n\u2212 k\u2212 2 decreases with the increase of k). By manipulating this number and applying Stirling\u2019s approximation for the factorials, we obtain:\n|Tn,k| \u2264 \u221a nen logn+1\u2212n( n\u2212k e )n\u2212k (k e )k kn\u2212k\u22122(n\u2212 k)n\u2212k\u22122 \u2264 e \u221a n\n(n\u2212 k)2 e n lognkn\u22122k\u22122 \u2264 en logn+(n\u22122k) log k,\nwhich is less than en log(nk). The decoding algorithm has complexity linear in n (Theorem 2), as well as the method to uniformly sample a Dandelion code, and the method to find the best DAG consistent with a k-tree (Theorem 3).\nWhile the running time of Algorithm 1 [Version 1] is linear in n, the computational complexity of step 2.c, which uses the method in [32], is exponential in the treewidth (more precisely, it is \u0398(k \u00b7 3k \u00b7 (k + 1)! \u00b7 n)). Hence, one cannot hope to use it with moderately high treewidth bounds (say, larger than 8). Regarding the sample space, according to the above theorem it is slightly higher than that of order-based learning of unconstrained Bayesian networks (e.g. [43]), especially if k n. However, each iteration of step 2.c needs considerable more effort than the corresponding iteration in the unbounded case (yet, as it is a method theoretically linear in n, more efficient implementations of the algorithm that searches within a given k-tree might bring an additional boost to this approach in the future).\nAs just explained, the main practical drawback of Algorithm 1 [Version 1] is step 2.c, which process each sampled k-tree. In the sequel we propose a new approach ([Version 2]) that is much faster (per iteration), at the price of a slight increase in the sampling space. We will empirically compare these approaches in the next section.\nLet \u03c3 define a partial order of the nodes. We say that a DAG G is consistent with \u03c3 if, \u2200j \u2208 pai (as defined by G), there is no directed path from i to j in \u03c3. In other words, \u03c3 constrains the valid topological orderings for the nodes in G. We do not force \u03c3 to be a linear order, because we are only interested in orderings that specify, for each edge in a k-tree Tk, which of the two ending points precedes the other (in other words, we are only interested in possible ways of orienting the edges of the k-tree). There are multiple linear orderings that achieve the very same result for Tk, and our goal is to sample from the smallest possible space of orderings (if we used a linear order, then the sampling space would be n!).\nA partial order \u03c3 can be represented as a DAGG: i is smaller than j in \u03c3 if and only if node i is an ancestor of node j inG. Given a k-tree Tk, we will sample \u03c3 by following\nthe same recursive process as in Definition 1. This is described in Algorithm 2. The procedure produces partial orders (i.e., DAGs) whose underlying graph (obtained by ignoring arc directions) is exactly the graph Tk. Note that the treewidth of the DAG corresponding to \u03c3 might exceed the treewidth of k. This does not affect the correctness of Algorithm 1, as \u03c3 is only used to specify which node preceeds which node in the order, and hence which are the possible parents; the actual parents are chosen so that the treewidth bound is respect. This can be done efficiently using Tk.\nAlgorithm 2 Sampling a partial order within a k-tree. Input a k-tree Tk with n nodes Output a partial order defined by a DAG \u03c3. 1 Initialize \u03c3 = Tk. Arbitrarily choose a (k + 1)-clique R of \u03c3 and call it the root\nclique; 2 Uniformly sample the directions of the arcs in \u03c3 linking the k+1 nodes inR, without\ncreating cycles in \u03c3, and mark these nodes as done; 3 Take a node v that is linked to k done nodes; 4 Uniformly sample the directions of the arcs in \u03c3 between v and these k done nodes,\nwithout creating cycles in \u03c3, and mark v as done; 5 Go to Step 3 unless all nodes are done.\nTheorem 5 Algorithm 2 samples DAGs \u03c3 on a sample space of size k! \u00b7 (k + 1)n\u2212k and runs in linear time in n and k.\nProof. The sampling of the k+ 1 nodes in the root clique takes time O(k) by sampling one of the (k + 1)! ways to choose the arcs without creating cycles. We assume that an appropriate structure representing Tk is known (e.g., a tree-decomposition with n\u2212 k \u2212 1 nodes), so Steps 1 and 3 can be done in O(k) time. For each iteration of Step 4, we spend time O(k) because there are only k + 1 ways to direct the edges, as this is equivalent to placing v in its relative order with respect to the already ordered k neighbors. Hence the total running time is O(kn) and the sampling space is (k + 1)! \u00b7 (k + 1)n\u2212k\u22121 = k! \u00b7 (k + 1)n\u2212k.\nThe following result shows that the sampling space of this version of the sampling algorithm remains reasonably small, especially for k n (it would be also small if k is close to n, then |Tn,k| decreases drastically, so the total sampling space would also decrease).\nTheorem 6 The sampling space of Algorithm 1 [Version 2] is less than en log(n(k+1) 2). Each of its iterations runs in linear time in n and k.\nProof. As before, the decoding algorithm (Theorem 2) and the method to uniformly sample a Dandelion code run in linear time in both n and k. Algorithm 2 samples the ordering \u03c3 in linear time too. Finally, finding the best DAG consistent with a k-tree Tk and \u03c3 is a greedy procedure over all nodes (choosing the parent set of a node each time): the treewidth cannot exceed k because we take a subgraph of Tk, and no cycles can be formed if we respect \u03c3.\nAlthough the sampling space of Version 2 is larger than the one of Version 1, Version 2 is much faster per iteration. This allows us to explore a much larger region of the space of k-tress than Version 1 can within a fixed amount of time. Moreover, one can run Version 2 without pre-computing the score function: when scores are needed, they are computed and stored into a hash table for further accesses, thus closely matching another desirable characteristic of order-based learning methods for unbounded treewidth (namely, to avoid computing all scores a priori)."}, {"heading": "5 Experiments", "text": "We empirically analyze the accuracy of Algorithm 1 by comparing its two versions with each other and with the values obtained by the MILP method. As before, we use a collection of data sets from the UCI repository of varying dimensionality, with variables discretized over the median value when needed. The number of (binary) variables and samples in each data set are described in Table 2. Some columns of the original data sets audio and community were discarded: 7 variables of audio had always a constant value, 5 variables of community have almost a different value per sample (such as personal data), and 22 variables have missing data (Table 2 shows dimensions after this pre-processing). In all experiments, we maximize the Bayesian Dirichlet likelihood equivalent uniform (BDeu) score with equivalent sample size equal to one [28].\nWe use treewidth bounds of 4 and 10, and maximum parent set size of 3 (for hill and community, it was set as 2; nevertheless, the MILP formulation is the one with a strong dependency on the maximum parent set size, as scores need to be pre-computed). To be fair among runs, we have pre-computed all scores, and have considered them as input of the problem. The MILP has been optimized by CPLEX 12.4 with a memory limit of 64GB. We have allowed it to run up to three hours, and have also collected the incumbent solution after 10 minutes. Algorithm 1 has been given only 10 minutes (in either version).\nTreewidth \u2264 4\nTreewidth \u2264 10\nTo account for the variability of the performance of the sampling methods with respect to the sampling seed, we ran each version of Algorithm 1 ten times on each data set with different seeds. We report the minimum, median and maximum obtained values over those runs for each dataset. We show the relative scores (in percentage) of the approximate methods (Versions 1 and 2 of Algorithm 1 and the best score found by the MILP formulation within 10 minutes and 3 hours) with respect to Version 2\u2019s median score, for treewidth bounds of four (Figure 1) and ten (Figure 2). The relative score is computed as the ratio of the obtained value and the median score of Version 2, so higher values are better. Moreover, a value higher than 100% shows that the method outperformed Version 2, whereas a value smaller than 100% shows the converse. The raw data used in the figures appear in Tables 3 (for Figure 1) and 4 (for Figure 2). The exponential dependence on treewidth of Version 1 made it intractable to run with treewidth bound greater than 8. We see from the plot on top that Version 2 is largely superior to Version 1, even if the former might only find suboptimal networks for a given k-tree. This is probably a consequence of the much lower running times per iteration, which allows Version 2 to explore a much larger set of k-trees. It also suggests that spending time finding good k-trees is more worthy than optimizing network structures for a given k-tree. We also see that the MILP formulation scales poorly with the number of variables, being unable to obtain satisfactory solutions for data sets with more than 50 variables. On the hill data set with treewidth \u2264 4, CPLEX running the MILP formulation was not able to output any solution within 10 minutes, and the solution obtained within 3 hours is far left of the zoomed area of the graph in Figure 1; on the community data set with treewidth \u2264 4, CPLEX did not find any solution within 3 hours. Regarding the treewidth bound of ten (Figure 2), we observe that Version 2 is very accurate and outperforms the MILP formulation in the larger data sets.\nIt is worth noting that both versions of Algorithm 1 were implemented in Matlab; hence, the comparison with the approximate solution of running the MILP formulation with the same amount of time (10 minutes) might be unfair, as we expect to produce better results by an appropriate re-coding of our sampling methods in a more efficient language (one could also try to improve the MILP formulation, although it will eventually suffer from the problems discussed in Section 3). Nevertheless, the results show that Version 2 is very competitive even in this scenario."}, {"heading": "6 Conclusions", "text": "We have created new exact and approximate procedures to learn Bayesian networks of bounded treewidth. They perform well and are of immediate practical use. The designed mixed-integer linear programming (MILP) formulation improves on MILP formulations for related tasks, especially regarding the specification of treewidth-related constraints. It solves the problem exactly and surpasses a state-of-the-art method both in size of networks and treewidth that it can handle. Even if results indicate it is better than the state of the art, MILP is not so accurate and might fail in large domains. For that purpose, we have proposed a double sampling idea that provides means to learn Bayesian networks in large domains and high treewidth limits, and is empirically shown to perform very well in a collection of public data sets. It scales well, because\nits complexity is linear both in the domain size and in the treewidth bound. There are certainly other search methods that can be integrated with our sampling approach, for instance a local search after every iteration of sampling, local permutations of orderings that are compatible with the k-trees, etc. We leave the study of these and other avenues for future work.\nDuring the making of this work, two closely related works appeared in the literature. [6] developed an exact learning procedure based on maximum satisfiability. [36] developed an alternative MILP formulation of the problem with exponentially many constraints, and used cutting plane generation techniques to improve on performance. These works have been developed independently and simultaneously with our work presented here; future work should compare their performance empirically against the methods proposed here."}, {"heading": "7 Acknowledgments", "text": "This work was partly supported by the grant N00014-12-1-0868 from the US Office of Navy Research, the Swiss NSF grant n. 200021 146606/1, and the FAPESP grant n. 2013/23197-4."}], "references": [{"title": "Approximating MAPs for belief networks is NP-hard and other theorems", "author": ["A.M. Abdelbar", "S.M. Hedetniemi"], "venue": "Artif. Intell., 102(1):21\u201338", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Complexity of finding embeddings in a k-tree", "author": ["S. Arnborg", "D. Corneil", "A. Proskurowski"], "venue": "SIAM J. on Matrix Analysis and Applications, 8(2):277\u2013284", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1987}, {"title": "Thin junction trees", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Advances in Neural Inf. Proc. Systems 14, pages 569\u2013576", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Advances in Bayesian Network Learning using Integer Programming", "author": ["M. Barlett", "J. Cussens"], "venue": "Proc. 29th Conf. on Uncertainty in AI, pages 182\u2013191", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "On the number of k-dimensional trees", "author": ["L.W. Beineke", "R.E. Pippert"], "venue": "J. of Comb. Theory, 6:200\u2013205", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1969}, {"title": "M", "author": ["J. Berg"], "venue": "J \u201darvisalo, and B. Malone. Learning optimal bounded treewidth Bayesian networks via maximum satisfiability. In Proc. 17th Int. Conf. on AI and Stat., pages 86\u201395", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximability of probability distributions", "author": ["A. Beygelzimer", "I. Rish"], "venue": "Advances in Neural Inf. Proc. Systems 16, pages 377\u2013384", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "A linear time algorithm for finding tree-decompositions of small treewidth", "author": ["H.L. Bodlaender"], "venue": "SIAM J. on Computing, 25(6):1305\u20131317", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Theory refinement on Bayesian networks", "author": ["W. Buntine"], "venue": "Proc. 7th Conf. on Uncertainty in AI, pages 52\u201360", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "Bijective linear time coding and decoding for k-trees", "author": ["S. Caminiti", "E.G. Fusco", "R. Petreschi"], "venue": "Theory of Comp. Systems, 46(2):284\u2013300", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Complexity of inference in graphical models", "author": ["V. Chandrasekaran", "N. Srebro", "P. Harsha"], "venue": "Proc. 24th Conf. on Uncertainty in AI, pages 70\u201378", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient principled learning of thin junction trees", "author": ["A. Chechetka", "C. Guestrin"], "venue": "Advances in Neural Inf. Proc. Systems, pages 273\u2013280", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning Bayesian networks is NP-complete", "author": ["D.M. Chickering"], "venue": "Learning from Data: AI and Stat. V, pages 121\u2013130. Springer-Verlag", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "Inf. Theory, IEEE Trans. on, 14(3):462\u2013467", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1968}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Mach. Learning, 9(4):309\u2013347", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Bayesian network learning with cutting planes", "author": ["J. Cussens"], "venue": "Proc. 27th Conf. on Uncertainty in AI, pages 153\u2013160", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum Likelihood Pedigree Reconstruction using Integer Linear Programming", "author": ["J. Cussens", "M. Bartlett", "E.M. Jones", "N.A. Sheehan"], "venue": "Genetic Epidemiology, 37(1):69\u201383", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximating probabilistic inference in Bayesian belief networks is NP-hard", "author": ["P. Dagum", "M. Luby"], "venue": "Artif. Intell., 60(1):141\u2013153", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Modeling and Reasoning with Bayesian Networks", "author": ["A. Darwiche"], "venue": "Cambridge University Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning polytrees", "author": ["S. Dasgupta"], "venue": "Proc. 15th Conf. on Uncertainty in AI, pages 134\u2013141", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "New Complexity Results for MAP in Bayesian Networks", "author": ["C.P. de Campos"], "venue": "In Proc. Int. Joint Conf. on AI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Structure learning of Bayesian networks using constraints", "author": ["C.P. de Campos", "Z. Zeng", "Q. Ji"], "venue": "In Proc. 26th Int. Conf. on Mach. Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Learning Bounded Treewidth Bayesian Networks", "author": ["G. Elidan", "S. Gould"], "venue": "J. of Mach. Learning Res., 9:2699\u20132731", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Taming the curse of dimensionality: Discrete integration by hashing and optimization", "author": ["S. Ermon", "C.P. Gomes", "A. Sabharwal", "B. Selman"], "venue": "Proc. 30th Int. Conf. on Mach. Learning, pages 334\u2013342", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "The Bayesian structural EM algorithm", "author": ["N. Friedman"], "venue": "Proc. 14th Conf. on Uncertainty in AI, pages 129\u2013138", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "and D", "author": ["N. Friedman", "I. Nachman"], "venue": "Pe\u2019er. Learning Bayesian network structure from massive datasets: The \u201dsparse candidate\u201d algorithm. In Proc. 15th Conf. on Uncertainty in AI, pages 206\u2013215", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Integer linear programming formulations for treewidth", "author": ["A. Grigoriev", "H. Ensinck", "N. Usotskaya"], "venue": "Technical report, Maastricht Res. School of Economics of Tech. and Organization", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Mach. Learning, 20(3):197\u2013 243", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "Characteristic imsets for learning Bayesian network structure", "author": ["R. Hemmecke", "S. Lindner", "M. Studen\u00fd"], "venue": "Int. J. of Approx. Reasoning, 53(9):1336\u20131349", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning bayesian network structure using LP relaxations", "author": ["T. Jaakkola", "D. Sontag", "A. Globerson", "M. Meila"], "venue": "Proc. 13th Int. Conf. on AI and Stat., pages 358\u2013365", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic Graphical Models", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact learning of bounded tree-width Bayesian networks", "author": ["J.H. Korhonen", "P. Parviainen"], "venue": "Proc. 16th Int. Conf. on AI and Stat., pages 370\u2013378", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "and L", "author": ["J.H.P. Kwisthout", "H.L. Bodlaender"], "venue": "C. van der Gaag. The Necessity of Bounded Treewidth for Efficient Inference in Bayesian Networks. In Proc. 19th European Conf. on AI, pages 237\u2013242", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Anytime marginal MAP inference", "author": ["D.D. Mau\u00e1", "C.P. de Campos"], "venue": "In Proc. 28th Int. Conf. on Mach. Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Exact structure discovery in Bayesian networks with less space", "author": ["P. Parviainen", "M. Koivisto"], "venue": "Proc. 25th Conf. on Uncertainty in AI, pages 436\u2013443", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning bounded tree-width Bayesian networks using integer linear programming", "author": ["P. Parviainen", "H.S. Farahani", "J. Lagergren"], "venue": "Proc. 17th Int. Conf. on AI and Stat., pages 751\u2013759", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding optimal Bayesian network given a super-structure", "author": ["E. Perrier", "S. Imoto", "S. Miyano"], "venue": "J. of Mach. Learning Res., 9(2):2251\u20132286", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artif. Intell., 82(1\u20132):273\u2013 302", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1996}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Annals of Stat., 6(2):461\u2013464", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1978}, {"title": "A simple approach for finding the globally optimal Bayesian network structure", "author": ["T. Silander", "P. Myllymaki"], "venue": "Proc. 22nd Conf. on Uncertainty in AI, pages 445\u2013452", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Bayesian networks with discrete variables from data", "author": ["P. Spirtes", "C. Meek"], "venue": "Proc. 1st Int. Conf. on Knowledge Discovery and Data Mining, pages 294\u2013299", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1995}, {"title": "Maximum likelihood bounded tree-width Markov networks", "author": ["N. Srebro"], "venue": "Artif. Intell., 143(1):123\u2013138", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}, {"title": "Ordering-based search: A simple and effective algorithm for learning Bayesian networks", "author": ["M. Teyssier", "D. Koller"], "venue": "Proc. 21st Conf. on Uncertainty in AI, pages 584\u2013590", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "An Improved Admissible Heuristic for Learning Optimal Bayesian Networks", "author": ["C. Yuan", "B. Malone"], "venue": "Proc. 28th Conf. on Uncertainty in AI, pages 924\u2013933", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning optimal Bayesian networks: A shortest path perspective", "author": ["C. Yuan", "B. Malone"], "venue": "J. of Artif. Intell. Res., 48:23\u201365", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 30, "context": "Bayesian networks are graphical models widely used to represent joint probability distributions on complex multivariate domains [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "Here, learning a Bayesian network refers to inferring the underlying graphical structure from data, a task well-known to be NP-hard [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 63, "endOffset": 82}, {"referenceID": 17, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 63, "endOffset": 82}, {"referenceID": 18, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 63, "endOffset": 82}, {"referenceID": 20, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 63, "endOffset": 82}, {"referenceID": 37, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 63, "endOffset": 82}, {"referenceID": 18, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 204, "endOffset": 220}, {"referenceID": 23, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 204, "endOffset": 220}, {"referenceID": 30, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 204, "endOffset": 220}, {"referenceID": 33, "context": "All those inferences are NP-hard to compute even approximately [1, 18, 19, 21, 38], and all known (exact and provably good) algorithms have worst-case time complexity that is exponential in the treewidth [19, 24, 31, 34], which is a measure of connectedness of the graph.", "startOffset": 204, "endOffset": 220}, {"referenceID": 10, "context": "In fact, under widely believed assumptions from complexity theory, exponential time complexity in the treewidth is inevitable for any algorithm that provides provably good inferences [11, 33].", "startOffset": 183, "endOffset": 191}, {"referenceID": 32, "context": "In fact, under widely believed assumptions from complexity theory, exponential time complexity in the treewidth is inevitable for any algorithm that provides provably good inferences [11, 33].", "startOffset": 183, "endOffset": 191}, {"referenceID": 24, "context": "This is particularly important in the presence of missing data, as learning methods usually resort to some kind of Expectation-Maximization procedure that requires performing belief updating in the network at every iteration [25].", "startOffset": 225, "endOffset": 229}, {"referenceID": 1, "context": "Since estimating a network\u2019s treewidth is itself an NP-hard task [2], extending current methods for learning Bayesian networks to the case of bounded treewidth while maintaining their relative efficiency and accuracy is not trivial.", "startOffset": 65, "endOffset": 68}, {"referenceID": 31, "context": "Korhonen and Parviainen [32] showed that learning bounded treewidth Bayesian networks is NP-hard, and developed an exact algorithm based on dynamic programming that learns optimal n-node structures of treewidth at most \u03c9 in time 3n, which is above the 2n time required by the best worst-case algorithms for learning optimal Bayesian networks with no constraint on treewidth [40].", "startOffset": 24, "endOffset": 28}, {"referenceID": 39, "context": "Korhonen and Parviainen [32] showed that learning bounded treewidth Bayesian networks is NP-hard, and developed an exact algorithm based on dynamic programming that learns optimal n-node structures of treewidth at most \u03c9 in time 3n, which is above the 2n time required by the best worst-case algorithms for learning optimal Bayesian networks with no constraint on treewidth [40].", "startOffset": 374, "endOffset": 378}, {"referenceID": 22, "context": "Elidan and Gould [23] combined several heuristics to treewidth computation and network structure learning in order to design approximate methods.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Others have addressed the similar (but not equivalent) problem of learning undirected models of bounded treewidth [3, 12, 42].", "startOffset": 114, "endOffset": 125}, {"referenceID": 11, "context": "Others have addressed the similar (but not equivalent) problem of learning undirected models of bounded treewidth [3, 12, 42].", "startOffset": 114, "endOffset": 125}, {"referenceID": 41, "context": "Others have addressed the similar (but not equivalent) problem of learning undirected models of bounded treewidth [3, 12, 42].", "startOffset": 114, "endOffset": 125}, {"referenceID": 5, "context": "[6] showed that the problem of learning bounded treewidth Bayesian networks can be reduced to a weighted maximum satisfiability problem, and subsequently solved by weighted MAX-SAT solvers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "[36] showed that the problem can be reduced to a mixed-integer linear program (MILP), and then solved by off-the-shelf MILP optimizers (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Following the work of Cussens [16], the authors avoid creating such large programs by a cutting plane generation mechanism, which iteratively includes a new constraint while the optimum is not found.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "The works of [6] and [36] have been developed independently and simultaneously with our work presented here; for this reason, we do not compare our methods with", "startOffset": 13, "endOffset": 16}, {"referenceID": 35, "context": "The works of [6] and [36] have been developed independently and simultaneously with our work presented here; for this reason, we do not compare our methods with", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "We first introduce a mixed integer linear programming formulation of the problem (Section 3) that builds on existing MILP formulations for unconstrained structure learning of Bayesian networks [16, 17] and for computing the treewidth of a graph [27].", "startOffset": 193, "endOffset": 201}, {"referenceID": 16, "context": "We first introduce a mixed integer linear programming formulation of the problem (Section 3) that builds on existing MILP formulations for unconstrained structure learning of Bayesian networks [16, 17] and for computing the treewidth of a graph [27].", "startOffset": 193, "endOffset": 201}, {"referenceID": 26, "context": "We first introduce a mixed integer linear programming formulation of the problem (Section 3) that builds on existing MILP formulations for unconstrained structure learning of Bayesian networks [16, 17] and for computing the treewidth of a graph [27].", "startOffset": 245, "endOffset": 249}, {"referenceID": 35, "context": "[36], the MILP problem we generate is of polynomial size in the number of variables, and does not require the use of cutting planes techniques.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36]) cannot cope with very large domains, even if we agreed on obtaining only approximate solutions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "In order to deal with large domains, we devise (in Section 4) an approximate method based on a uniform sampling of k-trees (maximal triangulated graphs of treewidth k), which is achieved by using a fast computable bijection between k-trees and Dandelion codes [10].", "startOffset": 260, "endOffset": 264}, {"referenceID": 31, "context": "For each sampled k-tree, we either run an exact algorithm similar to the one proposed in [32] (when computationally appealing) to learn the scoremaximizing network whose moral graph is a subgraph of that k-tree, or we resort to a much more efficient method that takes partial variable orderings uniformly at random from a (relatively small) space of orderings that are compatible with the k-tree.", "startOffset": 89, "endOffset": 93}, {"referenceID": 40, "context": "One approach is to identify, for each variable, the minimal set of variables that makes that variable conditionally independent of others (Markov blanket), which is usually done by means of statistical tests of stochastic independence or information theoretic measures [41].", "startOffset": 269, "endOffset": 273}, {"referenceID": 38, "context": "Commonly used score functions include the Minimum Description Length (which is equivalent to the Bayesian Information Criterion) [39], and Bayesian Dirichlet (likelihood) equivalent uniform score [9, 15, 28].", "startOffset": 129, "endOffset": 133}, {"referenceID": 8, "context": "Commonly used score functions include the Minimum Description Length (which is equivalent to the Bayesian Information Criterion) [39], and Bayesian Dirichlet (likelihood) equivalent uniform score [9, 15, 28].", "startOffset": 196, "endOffset": 207}, {"referenceID": 14, "context": "Commonly used score functions include the Minimum Description Length (which is equivalent to the Bayesian Information Criterion) [39], and Bayesian Dirichlet (likelihood) equivalent uniform score [9, 15, 28].", "startOffset": 196, "endOffset": 207}, {"referenceID": 27, "context": "Commonly used score functions include the Minimum Description Length (which is equivalent to the Bayesian Information Criterion) [39], and Bayesian Dirichlet (likelihood) equivalent uniform score [9, 15, 28].", "startOffset": 196, "endOffset": 207}, {"referenceID": 3, "context": "Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].", "startOffset": 100, "endOffset": 127}, {"referenceID": 16, "context": "Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].", "startOffset": 100, "endOffset": 127}, {"referenceID": 28, "context": "Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].", "startOffset": 100, "endOffset": 127}, {"referenceID": 29, "context": "Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].", "startOffset": 100, "endOffset": 127}, {"referenceID": 31, "context": "Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].", "startOffset": 100, "endOffset": 127}, {"referenceID": 43, "context": "Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].", "startOffset": 100, "endOffset": 127}, {"referenceID": 44, "context": "Score-based structure learning is a difficult task, and research on this topic has been very active [4, 17, 29, 30, 32, 44, 45].", "startOffset": 100, "endOffset": 127}, {"referenceID": 1, "context": "Any graph can be made chordal by inserting edges, a process called chordalization [2, 8].", "startOffset": 82, "endOffset": 88}, {"referenceID": 7, "context": "Any graph can be made chordal by inserting edges, a process called chordalization [2, 8].", "startOffset": 82, "endOffset": 88}, {"referenceID": 22, "context": "There are at least two direct reasons to aim at learning Bayesian networks of bounded treewidth: (i) As discussed previously, all known exact algorithms for probabilistic inference have exponential time complexity in the treewidth, and networks with very high treewidth are usually the most challenging for approximate methods; (ii) Previous empirical results [23, 37] suggest that bounding the treewidth might improve model performance on held-out data.", "startOffset": 360, "endOffset": 368}, {"referenceID": 36, "context": "There are at least two direct reasons to aim at learning Bayesian networks of bounded treewidth: (i) As discussed previously, all known exact algorithms for probabilistic inference have exponential time complexity in the treewidth, and networks with very high treewidth are usually the most challenging for approximate methods; (ii) Previous empirical results [23, 37] suggest that bounding the treewidth might improve model performance on held-out data.", "startOffset": 360, "endOffset": 368}, {"referenceID": 6, "context": "There is also evidence that bounding the treewidth does not impose a great burden on the expressivity of the model for real data sets [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 31, "context": "Korhonen and Parviainen [32] adapted", "startOffset": 24, "endOffset": 28}, {"referenceID": 41, "context": "Srebro\u2019s complexity result for Markov networks [42] to show that learning the structure of Bayesian networks of bounded treewidth strictly greater than one is NP-hard.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "Dasgupta\u2019s results also prove this hardness if the score maximizes data likelihood [20] (in the case of networks of treewidth one, that is, directed trees with at most one parent per node, learning can be performed efficiently by the Chow and Liu\u2019s algorithm [14]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Dasgupta\u2019s results also prove this hardness if the score maximizes data likelihood [20] (in the case of networks of treewidth one, that is, directed trees with at most one parent per node, learning can be performed efficiently by the Chow and Liu\u2019s algorithm [14]).", "startOffset": 259, "endOffset": 263}, {"referenceID": 3, "context": "MILP formulations have shown to be very effective to learning Bayesian networks without the treewidth bound [4, 16], surpassing other attempts in a range of data sets.", "startOffset": 108, "endOffset": 115}, {"referenceID": 15, "context": "MILP formulations have shown to be very effective to learning Bayesian networks without the treewidth bound [4, 16], surpassing other attempts in a range of data sets.", "startOffset": 108, "endOffset": 115}, {"referenceID": 21, "context": "Moreover, the great language power of a MILP problem allows us to encode the treewidth constraint in a natural manner, which might not be easy with other structure learning approaches [22, 29, 35, 44, 45].", "startOffset": 184, "endOffset": 204}, {"referenceID": 28, "context": "Moreover, the great language power of a MILP problem allows us to encode the treewidth constraint in a natural manner, which might not be easy with other structure learning approaches [22, 29, 35, 44, 45].", "startOffset": 184, "endOffset": 204}, {"referenceID": 34, "context": "Moreover, the great language power of a MILP problem allows us to encode the treewidth constraint in a natural manner, which might not be easy with other structure learning approaches [22, 29, 35, 44, 45].", "startOffset": 184, "endOffset": 204}, {"referenceID": 43, "context": "Moreover, the great language power of a MILP problem allows us to encode the treewidth constraint in a natural manner, which might not be easy with other structure learning approaches [22, 29, 35, 44, 45].", "startOffset": 184, "endOffset": 204}, {"referenceID": 44, "context": "Moreover, the great language power of a MILP problem allows us to encode the treewidth constraint in a natural manner, which might not be easy with other structure learning approaches [22, 29, 35, 44, 45].", "startOffset": 184, "endOffset": 204}, {"referenceID": 1, "context": "We note that computing the treewidth of a graph is an NP-hard problem itself [2], even if there are linear algorithms that are only exponential in the treewidth [8] (these algorithms might be seen mostly as theoretical results, since their practical use is shadowed by very large hidden constants).", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "We note that computing the treewidth of a graph is an NP-hard problem itself [2], even if there are linear algorithms that are only exponential in the treewidth [8] (these algorithms might be seen mostly as theoretical results, since their practical use is shadowed by very large hidden constants).", "startOffset": 161, "endOffset": 164}, {"referenceID": 16, "context": "The novel formulation is based on combining the MILP formulation for structure learning in [17] with the MILP formulation presented in [27] for computing the treewidth of an undirected graph.", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "The novel formulation is based on combining the MILP formulation for structure learning in [17] with the MILP formulation presented in [27] for computing the treewidth of an undirected graph.", "startOffset": 135, "endOffset": 139}, {"referenceID": 3, "context": "We have avoided the use of sophisticated techniques for MILP in the context of structure learning, such as constraint generation [4, 16], because we are interested in providing a clean and succinct MILP formulation, which can be ran using off-the-shelf solvers without additional coding.", "startOffset": 129, "endOffset": 136}, {"referenceID": 15, "context": "We have avoided the use of sophisticated techniques for MILP in the context of structure learning, such as constraint generation [4, 16], because we are interested in providing a clean and succinct MILP formulation, which can be ran using off-the-shelf solvers without additional coding.", "startOffset": 129, "endOffset": 136}, {"referenceID": 26, "context": "The practical difference of this formulation with respect to the one in [27] lies in the fact that we allow partial elimination orders, and we do not need integer variables to enforce such orders.", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "A corollary of the above result is that the treewidth of D is at most the treewidth of M [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 31, "context": "To validate the practical feasibility of our MILP formulation, we compare it against the the dynamic programming method proposed previously for this problem [32], which we call K&P from now on.", "startOffset": 157, "endOffset": 161}, {"referenceID": 42, "context": "For instance, we could envision a rejection sampling approach, which would sample structures using some available procedure (for instance, by sampling topological orderings and then greedily finding a DAG structure consistent with that order, as in [43]), and verify their treewidth, discarding the structure when the test fails.", "startOffset": 249, "endOffset": 253}, {"referenceID": 31, "context": "increasing their treewidth), we know that the moral graph of the optimal structure has to be a subgraph of a k-tree [32].", "startOffset": 116, "endOffset": 120}, {"referenceID": 9, "context": "[10] proposed a linear time method for coding and decoding k-trees into what is called Dandelion codes (the set of such codes is denoted by An,k).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Theorem 2 [10] There is a bijection mapping elements of An,k and Tn,k that is computable in time linear in n and k.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "Given Tk \u2208 Tn,k, we can use the dynamic programming algorithm proposed in [32] to find the optimal structure whose moral graph is a subgraph of Tk.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Our implementation follows the ideas in [32], but can also be seen as extending the divide-and-conquer method of [26] to account for all possible divisions of nodes.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "Our implementation follows the ideas in [32], but can also be seen as extending the divide-and-conquer method of [26] to account for all possible divisions of nodes.", "startOffset": 113, "endOffset": 117}, {"referenceID": 31, "context": "Theorem 3 [32] For any fixed k, given (a k-tree) G = (N,E) \u2208 Gn,k and the scoring function for each node v \u2208 N , we can find a DAG whose moralized graph is a subgraph of G maximizing the score in time and space O(n).", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "The follow equality holds [5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 31, "context": "c, which uses the method in [32], is exponential in the treewidth (more precisely, it is \u0398(k \u00b7 3 \u00b7 (k + 1)! \u00b7 n)).", "startOffset": 28, "endOffset": 32}, {"referenceID": 42, "context": "[43]), especially if k n.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In all experiments, we maximize the Bayesian Dirichlet likelihood equivalent uniform (BDeu) score with equivalent sample size equal to one [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "[6] developed an exact learning procedure based on maximum satisfiability.", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "[36] developed an alternative MILP formulation of the problem with exponentially many constraints, and used cutting plane generation techniques to improve on performance.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "This work presents novel algorithms for learning Bayesian network structures with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling k-trees (maximal graphs of treewidth k), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that k-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate.", "creator": "LaTeX with hyperref package"}}}