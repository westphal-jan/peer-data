{"id": "1406.3781", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2014", "title": "From Stochastic Mixability to Fast Rates", "abstract": "empirical risk minimization ( erm ) is a fundamental algorithm for statistical classification problems where the data remain generated back to some unknown distribution $ \\ mathsf { p } $ and thereby a good $ f $ chosen ; a fixed class $ \\ mathcal { f } $ with small loss $ \\ ell $. in the parametric setting, depending upon $ ( \\ ell, \\ doc { f }, \\ mathsf { p } ) $ erm can have slow $ ( 1 / \\ sqrt { n } ) $ or fast $ ( 1 / n ) $ rates of convergence of the excess risk as a function of the sample size $ n $. so exist several results that give sufficient conditions for fast rates in terms demonstrating computational properties of $ \\ ell $, $ \\ doc { v } $, and $ \\ mathsf { fl } $, such as the margin condition and the bernstein condition. after the non - finite prediction code q setting, there is an analogous slow and fast rate phenomenon, and it is successfully characterized either terms of the mixability of the loss $ \\ ne $ ( there being no role there too $ \\ mathcal { f } $ or $ \\ mathsf { p } $ ). the simplicity of stochastic mixability builds a structure between these alternative models of learning, reducing to classical mixability in a special case. the present literature presents simplified direct proof of fast levels for erm in showing \" global mixability of $ ( \\ ell, \\ mathcal { f1 }, \\ mathsf { p } ) $, and in so doing provides new insight into the fast - termination phenomenon. the proof exploits an explicit result of kemperman on the solution. perform generalized moment problem. we also show your partial converse that suggests a characterization of fast rates? erm in terms of stochastic mixability is easy.", "histories": [["v1", "Sat, 14 Jun 2014 23:25:05 GMT  (17kb,D)", "https://arxiv.org/abs/1406.3781v1", "13 pages"], ["v2", "Sat, 22 Nov 2014 11:16:28 GMT  (24kb,D)", "http://arxiv.org/abs/1406.3781v2", "21 pages, accepted to NIPS 2014"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nishant a mehta", "robert c williamson"], "accepted": true, "id": "1406.3781"}, "pdf": {"name": "1406.3781.pdf", "metadata": {"source": "CRF", "title": "From Stochastic Mixability to Fast Rates", "authors": ["Nishant A. Mehta", "Robert C. Williamson"], "emails": [], "sections": [{"heading": null, "text": "\u221a n) or fast (1/n) rates of convergence of the excess risk as a function of the sample size n. There\nexist several results that give sufficient conditions for fast rates in terms of joint properties of `, F , and P, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss ` (there being no role there for F or P). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of (`,F ,P), and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible."}, {"heading": "1 Introduction", "text": "Recent years have unveiled central contact points between the areas of statistical and online learning. These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.\u2019s (2012) injection of the concept of mixability into the statistical learning space in the form of stochastic mixability. It is this last connection that will be our departure point for this work.\nMixability is a fundamental property of a loss that characterizes when constant regret is possible in the online learning game of prediction with expert advice (Vovk, 1998). Stochastic mixability is a natural adaptation of mixability to the statistical learning setting; in fact, in the special case where the function class consists of all possible functions from the input space to the prediction space, stochastic mixability is equivalent to mixability (Van Erven et al., 2012). Just as Vovk and coworkers (see e.g. (Vovk, 2001; Kalnishkan and Vyugin, 2005)) have developed a rich convex geometric understanding of mixability, stochastic mixability can be understood as a sort of effective convexity.\nIn this work, we study the O( 1n )-fast rate phenomenon in statistical learning from the perspective of stochastic mixability. Our motivation is that stochastic mixability might characterize fast rates in statistical learning. As a first step, Theorem 5 of this paper establishes via a rather direct argument that stochastic mixability implies an exact oracle inequality (i.e. with leading constant 1) with a fast rate for finite function classes, and Theorem 7 extends this result to VC-type classes. This result can be understood as a new chapter in an evolving narrative that started with Lee et al.\u2019s (1998) seminal paper showing fast rates for agnostic learning with squared loss over convex function classes, and that was continued by Mendelson (2008b) who showed that fast rates are possible for p-losses (y, y\u0302) 7\u2192 |y \u2212 y\u0302|p over effectively convex function classes by passing through a Bernstein condition (defined in (12)).\nar X\niv :1\n40 6.\n37 81\nv2 [\ncs .L\nG ]\n2 2\nN ov\n2 01\n4\nWe also show that when stochastic mixability does not hold in a certain sense (see Section 5 for the precise statement), then the risk minimizer is not unique in a bad way. This is precisely the situation at the heart of the works of Mendelson (2008b) and Mendelson and Williamson (2002), which show that having non-unique minimizers is symptomatic of bad geometry of the learning problem. In such situations, there are certain targets (i.e. output conditional distributions) close to the original target under which empirical risk minimization learns at a slow rate, where the guilty target depends on the sample size and the target sequence approaches the original target asymptotically. Even the best known upper bounds have constants that blow up in the case of non-unique minimizers. Thus, whereas stochastic mixability implies fast rates, a sort of converse is also true, where learning is hard in a \u201cneighborhood\u201d of statistical learning problems for which stochastic mixability does not hold. In addition, since a stochastically mixable problem\u2019s function class looks convex from the perspective of risk minimization, and since when stochastic mixability fails the function class looks non-convex from the same perspective (it has multiple well-separated minimizers), stochastic mixability characterizes the effective convexity of the learning problem from the perspective of risk minimization.\nMuch of the recent work in obtaining faster learning rates in agnostic learning has taken place in settings where a Bernstein condition holds, including results based on local Rademacher complexities (Bartlett et al., 2005; Koltchinskii, 2006). The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems. Lecue\u0301 (2011) pinpoints that the difference between the two conditions is that the margin condition applies to the excess loss relative to the best predictor (not necessarily in the model class) whereas the Bernstein condition applies to the excess loss relative to the best predictor in the model class. Our approach in this work is complementary to the approaches of previous works, coming from a different assumption that forms a bridge to the online learning setting. Yet this assumption is related; the Bernstein condition implies stochastic mixability under a bounded losses assumption (Van Erven et al., 2012). Further understanding the connection between the Bernstein condition and stochastic mixability is an ongoing effort. Contributions. The core contribution of this work is to show a new path to the O\u0303 (\n1 n\n) -fast rate in\nstatistical learning. We are not aware of previous results that show fast rates from the stochastic mixability assumption. Secondly, we establish intermediate learning rates that interpolate between the fast and slow rate under a weaker notion of stochastic mixability. Finally, we show that in a certain sense stochastic mixability characterizes the effective convexity of the statistical problem.\nIn the next section we formally define the statistical problem, review stochastic mixability, and explain our high-level approach toward getting fast rates. This approach involves directly appealing to the Crame\u0301rChernoff method, from which nearly all known concentration inequalities arose in one way or another. In Section 3, we frame the problem of computing a particular moment of a certain excess loss random variable as a general moment problem. We sufficiently bound the optimal value of the moment, which allows for a direct application of the Crame\u0301r-Chernoff method. These results easily imply a fast rates bound for finite classes that can be extended to parametric (VC-type) function classes, as shown in Section 4. We describe in Section 5 how stochastic mixability characterizes a certain notion of convexity of the statistical learning problem. In Section 6, we extend the fast rates results to classes that obey a notion we call weak stochastic mixability. Finally, Section 7 concludes this work with connections to related topics in statistical learning theory and a discussion of open problems."}, {"heading": "2 Stochastic mixability, Crame\u0301r-Chernoff, and ERM", "text": ""}, {"heading": "2.1 The Setting", "text": "Let (`,F ,P) be a statistical learning problem with ` : Y \u00d7 R\u2192 R+ a nonnegative loss, F \u2282 RX a compact function class, and P a probability measure over X \u00d7 Y for input space X and output/target space Y. Let\nZ be a random variable defined as Z = (X,Y ) \u223c P. We assume for all f \u2208 F , `(Y, f(X)) \u2264 V almost surely (a.s.) for some constant V .\nA probability measure P operates on functions and loss-composed functions as: P f = E(X,Y )\u223cP f(X) P `(\u00b7, f) = E(X,Y )\u223cP ` ( Y, f(X) ) .\nSimilarly, an empirical measure Pn associated with an n-sample z, comprising n iid samples (x1, y1), . . . , (xn, yn), operates on functions and loss-composed functions as:\nPn f = 1\nn n\u2211 j=1 f(xj) Pn `(\u00b7, f) = 1 n n\u2211 j=1 ` ( yj , f(xj) ) .\nLet f\u2217 be any function for which P `(\u00b7, f\u2217) = inff\u2208F P `(\u00b7, f). For each f \u2208 F define the excess risk random variable Zf := ` ( Y, f(X) ) \u2212 ` ( Y, f\u2217(X) ) .\nWe frequently work with the following two subclasses. For any \u03b5 > 0, define the subclasses\nF \u03b5 := {f \u2208 F : PZf \u2264 \u03b5} F \u03b5 := {f \u2208 F : PZf \u2265 \u03b5} ."}, {"heading": "2.2 Stochastic mixability", "text": "For \u03b7 > 0, we say that (`,F ,P) is \u03b7-stochastically mixable if for all f \u2208 F\nlog E exp(\u2212\u03b7Zf ) \u2264 0. (1)\nIf \u03b7-stochastic mixability holds for some \u03b7 > 0, then we say that (`,F ,P) is stochastically mixable. Throughout this paper it is assumed that the stochastic mixability condition holds, and we take \u03b7\u2217 to be the largest \u03b7 such that \u03b7-stochastic mixability holds. Condition (1) has a rich history, beginning from the foundational thesis of Li (1999) who studied the special case of \u03b7\u2217 = 1 in density estimation with log loss from the perspective of information geometry. The connections that Li showed between this condition and convexity were strengthened by Gru\u0308nwald (2011, 2012) and Van Erven et al. (2012)."}, {"heading": "2.3 Crame\u0301r-Chernoff", "text": "The high-level strategy taken here is to show that with high probability the empirical risk minimization algorithm (ERM) will not select a fixed hypothesis function f with excess risk above an for some constant a > 0. For each hypothesis, this guarantee will flow from the Crame\u0301r-Chernoff method (Boucheron et al., 2013) by controlling the cumulant generating function (CGF) of \u2212Zf in a particular way to yield exponential concentration. This control will be possible because the \u03b7\u2217-stochastic mixability condition implies that the CGF of \u2212Zf takes the value 0 at some \u03b7 \u2265 \u03b7\u2217, a fact later exploited by our key tool Theorem 3.\nLet Z be a real-valued random variable. Applying Markov\u2019s inequality to an exponentially transformed random variable yields that, for any \u03b7 \u2265 0 and t \u2208 R\nPr(Z \u2265 t) \u2264 exp(\u2212\u03b7t+ log E exp(\u03b7Z)); (2)\nthe inequality is non-trivial only if t > EZ and \u03b7 > 0."}, {"heading": "2.4 Analysis of ERM", "text": "We consider the ERM estimator f\u0302z := arg minf\u2208F Pn `(\u00b7, f). That is, given an n-sample z, ERM selects any f\u0302z \u2208 F minimizing the empirical risk Pn `(\u00b7, f). We say ERM is \u03b5-good when f\u0302z \u2208 F \u03b5. In order to show that ERM is \u03b5-good it is sufficient to show that for all f \u2208 F \\ F \u03b5 we have PZf > 0. The goal is to show that with high probability ERM is \u03b5-good, and we will do this by showing that with high probability uniformly for all f \u2208 F \\ F \u03b5 we have Pn Zf > t for some slack t > 0 that will come in handy later.\nFor a real-valued random variable X, recall that the cumulant generating function of X is \u03b7 7\u2192 \u039bX(\u03b7) := log E e\u03b7X ; we allow \u039bX(\u03b7) to be infinite for some \u03b7 > 0.\nTheorem 1 (Crame\u0301r-Chernoff Control on ERM). Let a > 0 and select f such that EZf > 0. Let t < EZf . If there exists \u03b7 > 0 such that \u039b\u2212Zf (\u03b7) \u2264 \u2212 an , then\nPr { Pn `(\u00b7, f) \u2264 Pn `(\u00b7, f\u2217) + t } \u2264 exp(\u2212a+ \u03b7t).\nProof. Let Zf,1, . . . , Zf,n be iid copies of Zf , and define the sum Sf,n := \u2211n j=1\u2212Zf,j . Since (\u2212t) > E 1 nSf,n, then from (2) we have\nPr  1 n n\u2211 j=1 Zf,j \u2264 t  = Pr( 1 n Sf,n \u2265 \u2212t ) \u2264 exp (\u03b7t+ log E exp(\u03b7Sf,n))\n= exp(\u03b7t) ( E exp(\u2212\u03b7Zf ) )n .\nMaking the replacement \u039b\u2212Zf (\u03b7) = log E exp(\u2212\u03b7Zf ) yields\nlog Pr\n( 1\nn Sf,n \u2265 \u2212t\n) \u2264 \u03b7t+ n\u039b\u2212Zf (\u03b7).\nBy assumption, \u039b\u2212Zf (\u03b7) \u2264 \u2212 an , and so Pr{Pn Zf \u2264 t} \u2264 exp(\u2212a+ \u03b7t) as desired.\nThis theorem will be applied by showing that for an excess loss random variable Zf taking values in [\u22121, 1], if for some \u03b7 > 0 we have E exp(\u2212\u03b7Zf ) = 1 and if EZf = an for some constant a (that can and must depend on n), then log\u2212Zf (\u03b7/2) \u2264 \u2212 c\u03b7a n where c > 0 is a universal constant. This is the nature of the next section. We then extend this result to random variables taking values in [\u2212V, V ]."}, {"heading": "3 Semi-infinite linear programming and the general moment prob-", "text": "lem\nThe key subproblem now is to find, for each excess loss random variable Zf with mean a n and \u039b\u2212Zf (\u03b7) = 0 (for some \u03b7 \u2265 \u03b7\u2217), a pair of constants \u03b70 > 0 and c > 0 for which \u039b\u2212Zf (\u03b70) \u2264 \u2212 can . Theorem 1 would then imply that ERM will prefer f\u2217 over this particular f with high probability for ca large enough. This subproblem is in fact an instance of the general moment problem, a problem on which Kemperman (1968) has conducted a very nice geometric study. We now describe this problem.\nThe general moment problem. Let P(A) be the space of probability measures over a measurable space A = (A,S). For real-value measurable functions h and (gj)j\u2208[m] on a measurable space A = (A,S), the general moment problem is\ninf \u00b5\u2208P(A)\nEX\u223c\u00b5 h(X)\nsubject to EX\u223c\u00b5 gj(X) = yj , j \u2208 {1, . . . ,m}. (3)\nLet the vector-valued map g : A \u2192 Rm be defined in terms of coordinate functions as (g(x))j = gj(x), and let the vector y \u2208 Rm be equal to (y1, . . . , ym).\nLet D\u2217 \u2282 Rm+1 be the set\nD\u2217 := { d\u2217 = (d0, d1, . . . , dm) \u2208 Rm+1 : h(x) \u2265 d0 + m\u2211 j=1 djgj(x) for all x \u2208 A } . (4)\nTheorem 3 of (Kemperman, 1968) states that if y \u2208 int conv g(A), the optimal value of problem (3) equals\nsup { d0 + m\u2211 j=1 djyj : d \u2217 = (d0, d1, . . . , dm) \u2208 D\u2217 } . (5)\nOur instantiation. We choose A = [\u22121, 1], set m = 2 and define h, (gj)j\u2208{1,2}, and y \u2208 R2 as:\nh(x) = \u2212e(\u03b7/2)x, g1(x) = x, g2(x) = e\u03b7x, y1 = \u2212 a\nn , y2 = 1,\nfor any \u03b7 > 0, a > 0, and n \u2208 N. This yields the following instantiation of the general moment problem:\ninf \u00b5\u2208P([\u22121,1])\nEX\u223c\u00b5\u2212e(\u03b7/2)X (6a)\nsubject to EX\u223c\u00b5X = \u2212 a\nn (6b)\nEX\u223c\u00b5 e \u03b7X = 1. (6c)\nNote that equation (5) from the general moment problem now instantiates to sup { d0 \u2212 a\nn d1 + d2 : d\n\u2217 = (d0, d1, d2) \u2208 D\u2217 } , (7)\nwith D\u2217 equal to the set{ d\u2217 = (d0, d1, d2) \u2208 R3 : \u2212e(\u03b7/2)x \u2265 d0 + d1x+ d2e\u03b7x for all x \u2208 [\u22121, 1] } . (8)\nTo apply Theorem 3 of (Kemperman, 1968), we need to ensure the condition y \u2208 int conv g([\u22121, 1]) holds. We first characterize when y \u2208 conv g([\u22121, 1]) holds and handle the int conv g([\u22121, 1]) version after Theorem 3.\nLemma 2 (Feasible Moments). The point y = ( \u2212 an , 1 ) \u2208 conv g([\u22121, 1]) if and only if\na n \u2264 e \u03b7 + e\u2212\u03b7 \u2212 2 e\u03b7 \u2212 e\u2212\u03b7 = cosh(\u03b7)\u2212 1 sinh(\u03b7) . (9)\nProof. Let W denote the convex hull of g([\u22121, 1]). We need to see if ( \u2212 an , 1 ) \u2208 W . Note that W is the convex set formed by starting with the graph of x 7\u2192 e\u03b7x on the domain [\u22121, 1], including the line segment connecting this curve\u2019s endpoints (\u22121, e\u2212\u03b7) to (1, e\u03b7x), and including all of the points below this line segment but above the aforementioned graph. That is, W is precisely the set\nW := { (x, y) \u2208 R2 : e\u03b7x \u2264 y \u2264 e \u03b7 + e\u2212\u03b7\n2 + e\u03b7 \u2212 e\u2212\u03b7 2 x, \u2200x \u2208 [\u22121, 1]\n} .\nIt remains to check that 1 is sandwiched between the lower and upper bounds at x = \u2212 an . Clearly the lower bound holds. Simple algebra shows that the upper bound is equivalent to condition (9).\nNote that if (9) does not hold, then the semi-infinite linear program (6) is infeasible; infeasibility in turn implies that such an excess loss random variable cannot exist. Thus, we need not worry about whether (9) holds; it holds for any excess loss random variable satisfying constraints (6b) and (6c).\nThe following theorem is a key technical result for using stochastic mixability to control the CGF. The proof is long and can be found in Appendix A.\nTheorem 3 (Stochastic Mixability Concentration). Let f be an element of F with Zf taking values in [\u22121, 1], n \u2208 N, EZf = an for some a > 0, and \u039b\u2212Zf (\u03b7) = 0 for some \u03b7 > 0. If\na n < e\u03b7 + e\u2212\u03b7 \u2212 2 e\u03b7 \u2212 e\u2212\u03b7 , (10)\nthen\nE e(\u03b7/2)(\u2212Zf ) \u2264 { 1\u2212 0.18\u03b7an if \u03b7 \u2264 1 1\u2212 0.21an if \u03b7 > 1.\nTherefore, E e(\u03b7/2)(\u2212Zf ) \u2264 1\u2212 0.18(\u03b7 \u2227 1)an .\nNote that since log(1\u2212 x) \u2264 \u2212x when x < 1, we have \u039b\u2212Zf (\u03b7/2) \u2264 \u2212 0.18(\u03b7 \u2227 1)a\nn . In order to apply Theorem 3, we need (10) to hold, but only (9) is guaranteed to hold. The corner case is if (9) holds with equality. However, observe that one can always approximate the random variable X by a perturbed version X \u2032 which has nearly identical mean a\u2032 \u2248 a and a nearly identical \u03b7\u2032 \u2248 \u03b7 for which EX\u2032\u223c\u00b5\u2032 e\n\u03b7\u2032X\u2032 = 1, and yet the inequality in (9) is strict. Later, in the proof of Theorem 5, for any random variable that required perturbation to satisfy the interior condition (10), we implicitly apply the analysis to the perturbed version, show that ERM would not pick the (slightly different) function corresponding to the perturbed version, and use the closeness of the two functions to show that ERM also would not pick the original function.\nWe now present a necessary extension for the case of losses with range [0, V ].\nLemma 4 (Bounded Losses). Let g1(x) = x and y2 = 1 be common settings for the following two problems. The instantiation of problem (3) with A = [\u2212V, V ], h(x) = \u2212e(\u03b7/2)x, g2(x) = e\u03b7x, and y1 = \u2212 an has the same optimal value as the instantiation of problem (3) with A = [\u22121, 1], h(x) = \u2212e(V \u03b7/2)x, g2(x) = e(V \u03b7)x, and y1 = \u2212a/Vn .\nProof. Let X be a random variable taking values in [\u2212V, V ] with mean \u2212 an and E e \u03b7X = 1, and let Y be a random variable taking values in [\u22121, 1] with mean \u2212a/Vn and E e (V \u03b7)Y = 1. Consider a random variable X\u0303 that is a 1V -scaled independent copy of X; observe that E X\u0303 = \u2212 a/V n and E e (V \u03b7)X\u0303 = 1. Let the maximal possible value of E e(\u03b7/2)X be bX , and let the maximal possible value of E e (V \u03b7/2)Y be bY . We claim that bX = bY . Let X be a random variable with a distribution that maximizes E e (\u03b7/2)X subject to the previously stated constraints on X. Since X\u0303 satisfies E e(V \u03b7/2)X\u0303 = bX , setting Y = X\u0303 shows that in fact bY \u2265 bX . A symmetric argument (starting with Y and passing to some Y\u0303 = V Y ) implies that bX \u2265 bY ."}, {"heading": "4 Fast rates", "text": "We now show how the above results can be used to obtain an exact oracle inequality with a fast rate. We first present a result for finite classes and then present a result for various parametric classes, including VC-type classes (classes with logarithmic universal metric entropy), VC classes, and classes with polynomial uniform L1-bracketing numbers.\nTheorem 5 (Finite Classes Exact Oracle Inequality). Let (`,F ,P) be \u03b7\u2217-stochastically mixable, where |F| = N , ` is a nonnegative loss, and supf\u2208F ` ( Y, f(X) ) \u2264 V a.s. for a constant V . Then for all n \u2265 1, with probability at least 1\u2212 \u03b4\nP `(\u00b7, f\u0302z) \u2264 P `(\u00b7, f\u2217) + 6 max\n{ V, 1\u03b7\u2217 }( log 1\u03b4 + logN ) n .\nProof. Throughout this proof, let \u03b3n = a n where a is a constant that varies throughout the proof. For any a > 0, recall that F \u03b3n is the subclass of F for which the excess risk is at least \u03b3n. For each \u03b7 > 0, let F (\u03b7) \u03b3n \u2282 F \u03b3n correspond to those functions in F \u03b3n for which \u03b7 is the largest constant such that E exp(\u2212\u03b7Zf ) = 1. Let Fhyper \u03b3n \u2282 F \u03b3n correspond to functions f in F \u03b3n for which lim\u03b7\u2192\u221e E exp(\u2212\u03b7Zf ) < 1. Clearly, F \u03b3n = (\u22c3 \u03b7\u2208[\u03b7\u2217,\u221e) F (\u03b7) \u03b3n ) \u222a Fhyper \u03b3n . The excess loss random variables corresponding to elements f \u2208 Fhyper \u03b3n are \u201chyper-concentrated\u201d in the sense that they are infinitely stochastically mixable. However, Lemma 10 shows that for each hyper-concentrated excess loss random variable Zf , there exists another excess loss random variable Z \u2032f with mean arbitrarily close to that of Zf , with E exp(\u2212\u03b7Z \u2032f ) = 1 for some arbitrarily large but finite \u03b7, and with Z \u2032f \u2264 Zf with probability 1. The last property implies that the empirical risk of Z \u2032f is no greater than the empirical risk of Zf , and hence for each hyper-concentrated Zf it is sufficient (from the perspective of ERM\u2019s behavior) to study a corresponding Z \u2032f . From here on out, we implicitly\nmake this replacement in F \u03b3n itself, so that we now have F \u03b3n = \u22c3 \u03b7\u2208[\u03b7\u2217,\u221e) F (\u03b7) \u03b3n .\nConsider an arbitrary a > 0. For some fixed \u03b7 \u2208 [\u03b7\u2217,\u221e) for which |F (\u03b7) \u03b3n | > 0, consider the subclass F (\u03b7) \u03b3n . Individually for each such function, we will apply Theorem 1 as follows. From Lemma 4, we have \u039b\u2212Zf (\u03b7/2) = \u039b\u2212 1V Zf (V \u03b7/2). From Theorem 3, the latter is at most\n\u22120.18(V \u03b7 \u2227 1)(a/V ) n = \u2212 0.18\u03b7a (V \u03b7 \u2228 1)n .\nHence, Theorem 1 with t = 0 and the \u03b7 from the Theorem taken to be \u03b7/2 implies that the probability of the event Pn `(\u00b7, f) \u2264 Pn `(\u00b7, f\u2217) is at most\nexp ( \u22120.18 \u03b7\nV \u03b7 \u2228 1 a\n) .\nApplying the union bound over all of F \u03b3n , we conclude that Pr {\u2203f \u2208 F \u03b3n : Pn `(\u00b7, f) \u2264 Pn `(\u00b7, f\u2217)} \u2264 N exp ( \u2212\u03b7\u2217 ( 0.18a\nV \u03b7\u2217 \u2228 1\n)) .\nNow, recalling that ERM selects hypotheses purely based on their empirical risk, from inversion it holds that with probability at least 1\u2212 \u03b4, ERM will not select any hypothesis whose excess risk is at least\n6 max { V, 1\u03b7\u2217 }( log 1\u03b4 + logN ) n .\nBefore presenting the result for VC-type classes, we require some definitions. For a pseudometric space (G, d), for any \u03b5 > 0, let N (\u03b5,G, d) be the \u03b5-covering number of (G, d); that is, N (\u03b5,G, d) is the minimal number of balls of radius \u03b5 needed to cover G. We will further constrain the cover (the set of centers of the balls) to be a subset of G, so that the cover is a proper cover, thus ensuring that the stochastic mixability assumption transfers to any (proper) cover of F . Note that the \u201cproper\u201d requirement at most doubles the constant K below, as can be seen from an argument of Vidyasagar (2002, Lemma 2.1).\nWe now state a localization-based result that allows us to extend the result for finite classes to VC-type classes. Although the localization result can be obtained by combining standard techniques,1 we could not find this particular result in the literature. Below, an \u03b5-net F\u03b5 of a set F is a subset of F such that F is contained in the union of the balls of radius \u03b5 with centers in F\u03b5.\nTheorem 6. Let F be a separable function class whose functions have range bounded in [0, V ] and for which, for a constant K \u2265 1, for each u \u2208 (0,K] the L2(P) covering numbers are bounded as\nN (u,F , L2(P)) \u2264 ( K\nu\n)C . (11)\nSuppose F\u03b5 is a minimal \u03b5-net for F in the L2(P) norm, with \u03b5 = 1n . Denote by \u03c0 : F \u2192 F\u03b5 an L2(P)-metric projection from F to F\u03b5. Then, provided that \u03b4 \u2264 12 , the probability that there exists f \u2208 F such that\nPn f < Pn(\u03c0(f))\u2212 V\nn\n( 1080C log(2Kn) + 90 \u221a( log 1\n\u03b4\n) C log(2Kn) + log e\n\u03b4 ) is at most \u03b4.\nThe proof is long and distracts from the main point of this paper, and so it is presented in Appendix C. Using Theorem 6 along with much of the proof for the finite classes case, we can prove the following fast rates result for VC-type classes. The proof can be found in Appendix C. Below, we denote the loss-composed version of a function class F as ` \u25e6 F := {`(\u00b7, f) : f \u2208 F}.\n1See e.g. the techniques of Massart and Ne\u0301de\u0301lec (2006) and (for Step 3 of our proof of Theorem 11)) equation (3.17) of Koltchinskii (2011).\nTheorem 7 (VC-Type Classes Exact Oracle Inequality). Let (`,F ,P) be \u03b7\u2217-stochastically mixable with ` \u25e6 F separable, where, for a constant K \u2265 1, for each \u03b5 \u2208 (0,K] we have N (` \u25e6 F , L2(P), \u03b5) \u2264 ( K \u03b5 )C , and\nsupf\u2208F ` ( Y, f(X) ) \u2264 V a.s. for a constant V \u2265 1. Then for all n \u2265 5 and \u03b4 \u2264 12 , with probability at least 1\u2212 \u03b4\nP `(\u00b7, f\u0302z) \u2264 P `(\u00b7, f\u2217) + 1\nn max  8 max { V, 1\u03b7\u2217 }( C log(Kn) + log 2\u03b4 ) , 2V ( 1080C log(2Kn) + 90 \u221a( log 2\u03b4 ) C log(2Kn) + log 2e\u03b4 ) + 1n."}, {"heading": "5 Characterizing convexity from the perspective of risk minimiza-", "text": "tion\nIn the following, when we say (`,F ,P) has a unique minimizer we mean that any two minimizers f\u22171 , f\u22172 of P `(\u00b7, f) over F satisfy ` ( Y, f\u22171 (X) ) = ` ( Y, f\u22172 (X) ) a.s. We say the excess loss class {`(\u00b7, f)\u2212 `(\u00b7, f\u2217) : f \u2208 F} satisfies a (\u03b2,B)-Bernstein condition with respect to P for some B > 0 and 0 < \u03b2 \u2264 1 if, for all f \u2208 F :\nP ( `(\u00b7, f)\u2212 `(\u00b7, f\u2217) )2 \u2264 B (P(`(\u00b7, f)\u2212 `(\u00b7, f\u2217)))\u03b2 . (12) It already is known that the stochastic mixability condition guarantees that there is a unique minimizer (Van Erven et al., 2012); this is a simple consequence of Jensen\u2019s inequality. This leaves open the question: if stochastic mixability does not hold, are there necessarily non-unique minimizers? We show that in a certain sense this is indeed the case, in bad way: the set of minimizers will be a disconnected set.\nFor any \u03b5 > 0, define G\u03b5 as the class\nG\u03b5 := {f\u2217} \u222a { f \u2208 F : \u2016f \u2212 f\u2217\u2016L1(P) \u2265 \u03b5 } , (13)\nwhere in case there are multiple minimizers in F we arbitrarily select one of them as f\u2217. Since we assume that F is compact and G\u03b5 \\ {f\u2217} is equal to F minus an open set homeomorphic to the unit L1(P) ball, G\u03b5 \\ {f\u2217} is also compact.\nTheorem 8 (Non-Unique Minimizers). Suppose there exists some \u03b5 > 0 such that G\u03b5 is not stochastically mixable. Then there are minimizers f\u22171 , f \u2217 2 \u2208 F of P `(\u00b7, f) over F such that it is not the case that ` ( Y, f\u22171 (X) ) = ` ( Y, f\u22172 (X) ) a.s.\nProof. Select \u03b5 > 0 such that G\u03b5 is not stochastically mixable. Consider some fixed \u03b7 > 0. Since G\u03b5 is not stochastically mixable and hence not \u03b7-stochastically mixable, there exists f\u03b7 \u2208 G\u03b5 such that \u039b\u2212Zf\u03b7 (\u03b7) > 0.\nWe claim that there exists \u03b7\u2032 \u2208 (0, \u03b7) such that \u039b\u2212Zf\u03b7 (\u03b7 \u2032) = 0. If not, then lim\u03b7\u21930\n\u039b\u2212Zf\u03b7 (\u03b7)\u2212\u039b\u2212Zf\u03b7 (0)\n\u03b7 > 0\nand hence \u039b\u2032\u2212Zf\u03b7 (0) > 0, which since \u039b \u2032 \u2212Zf\u03b7 (0) = E(\u2212Zf\u03b7 ) implies that EZf\u03b7 < 0, a contradiction! Hence, indeed \u2203\u03b7\u2032 \u2208 (0, \u03b7) such that \u039b\u2212Zf\u03b7 (\u03b7 \u2032) = 0. Now, the Feasible Moments Lemma (Lemma 2) implies that EZf\u03b7 \u2264 cosh(\u03b7\u2032)\u22121 sinh(\u03b7\u2032) ; for \u03b7 \u2032 \u2265 0 the RHS has the tight upper bound \u03b7 \u2032 2 since the derivative of \u03b7\u2032 2 \u2212 cosh(\u03b7\u2032)\u22121 sinh(\u03b7\u2032) is the nonnegative function 12 tanh 2(\u03b7\u2032/2) and ( \u03b7\u2032 2 \u2212 cosh(\u03b7\u2032)\u22121 sinh(\u03b7\u2032) ) |\u03b7\u2032=0 = 0.\nThus, as \u03b7 \u2192 0 we have EZf\u03b7 \u2192 0. Since G\u03b5\\{f\u2217} is compact, we can take a positive decreasing sequence \u03b71, \u03b72, . . . approaching 0, corresponding to a sequence of (f\u03b7j )j \u2282 G\u03b5 \\ {f\u2217} with limit point g\u2217 \u2208 G\u03b5 \\ {f\u2217} for which EZg\u2217 = 0, and so there is a risk minimizer in G\u03b5 \\ {f\u2217}.\nThe implications of having non-unique risk minimizers In the case of non-unique risk minimizers, Mendelson (2008a) showed that for p-losses (y, y\u0302) 7\u2192 |y \u2212 y\u0302|p with p \u2208 [2,\u221e) there is an n-indexed sequence of probability measures (P(n))n approaching the true probability measure as n \u2192 \u221e such that, for each n, ERM learns at a slow rate under sample size n when the true distribution is P(n). This behavior is a\nconsequence of the statistical learning problem\u2019s poor geometry: there are multiple minimizers and the set of minimizers is not even connected.\nFurthermore, in this case, the best known fast rate upper bounds (see (Mendelson, 2008b) and (Mendelson and Williamson, 2002)) have a multiplicative constant that approaches \u221e as the target probability measure approaches a probability measure for which there are non-unique minimizers. The reason for the poor upper bounds in this case is that the constant B in the Bernstein condition explodes, and the upper bounds rely upon the Bernstein condition."}, {"heading": "6 Weak stochastic mixability", "text": "For some \u03ba \u2208 [0, 1], we say (`,F ,P) is (\u03ba, \u03b70)-weakly stochastically mixable if, for every \u03b5 > 0, for all f \u2208 {f\u2217} \u222a F \u03b5\nlog E exp(\u2212\u03b7\u03b5Zf ) \u2264 0, (14)\nwith \u03b7\u03b5 := \u03b70\u03b5 1\u2212\u03ba. This concept was introduced by Van Erven et al. (2012) without a name.\nSuppose that some fixed function has excess risk a = \u03b5. Then, roughly, with high probability ERM does not make a mistake provided that a\u03b7a = 1 n , i.e. when \u03b5 \u00b7 \u03b70\u03b5 1\u2212\u03ba = 1n and hence when \u03b5 = (\u03b70n) \u22121/(2\u2212\u03ba). By modifying the proof of the finite classes result (Theorem 5) to consider all functions in the subclass F \u03b3n for \u03b3n = (\u03b70n) \u22121/(2\u2212\u03ba), we have the following corollary of Theorem 5.\nCorollary 9. Let (`,F ,P) be (\u03ba, \u03b70)-weakly stochastically mixable for some \u03ba \u2208 [0, 1], where |F| = N , ` is a nonnegative loss, and supf\u2208F ` ( Y, f(X) ) \u2264 V a.s. for a constant V . Then for any n \u2265 1\u03b70V\n(1\u2212\u03ba)/(2\u2212\u03ba), with probability at least 1\u2212 \u03b4\nP `(\u00b7, f\u0302z) \u2264 P `(\u00b7, f\u2217) + 6 ( log 1\u03b4 + logN ) (\u03b70n)1/(2\u2212\u03ba) .\nIt is straightforward to achieve a similar result for VC-type classes, where the \u03b5 in the \u03b5-net can still be taken at the resolution 1n , but we need only apply the analysis to the subclass of functions with excess risk at least (\u03b70n) \u22121/(2\u2212\u03ba)."}, {"heading": "7 Discussion", "text": "We have shown that stochastic mixability implies fast rates for VC-type classes, using a direct argument based on the Crame\u0301r-Chernoff method and sufficient control of the optimal value of a certain instance of the general moment problem. The approach is amenable to localization in that the analysis separately controls the probability of large deviations for individual elements of F . It was therefore straightforward to extend the result for finite classes to VC-type classes. An important open problem is to extend the results presented here for VC-type classes to results for nonparametric classes with polynomial metric entropy, and moreover, to achieve rates similar to those obtained for these classes under the Bernstein condition.\nThere are still some unanswered questions with regards to the connection between the Bernstein condition and stochastic mixability. Van Erven et al. (2012) showed that for bounded losses the Bernstein condition implies stochastic mixability. Therefore, when starting from a Bernstein condition, Theorem 5 offers a different path to fast rates. An open problem is to settle the question of whether the Bernstein condition and stochastic mixability are equivalent. Previous results (Van Erven et al., 2012) suggest that the stochastic mixability does imply a Bernstein condition, but the proof was non-constructive, and it relied upon a bounded losses assumption. It is well known (and easy to see) that both stochastic mixability and the Bernstein condition hold only if there is a unique minimizer. Theorem 8 shows in a certain sense that if stochastic mixability does not hold, then there cannot be a unique minimizer. Is the same true when the Bernstein condition fails to hold? Regardless of whether stochastic mixability is equivalent to the Bernstein condition, the direct argument presented here and the connection to classical mixability, which\ndoes characterize constant regret in the simpler non-stochastic setting, motivates further study of stochastic mixability.\nFinally, it would be of great interest to discard the bounded losses assumption. Ignoring the dependence of the metric entropy on the maximum possible loss, the upper bound on the loss V enters the final bound through the difficulty of controlling the minimum value of u\u03b7(\u22121) when \u03b7 is large (see the proof of Theorem 3). From extensive experiments with a grid-approximation linear program, we have observed that the worst (CGF-wise) random variables for fixed negative mean and fixed optimal stochastic mixability constant are those which place very little probability mass at \u2212V and most of the probability mass at a small positive number that scales with the mean. These random variables correspond to functions that with low probability beat f\u2217 by a large (loss) margin but with high probability have slightly higher loss than f\u2217. It would be useful to understand if this exotic behavior is a real concern and, if not, find a simple, mild condition on the moments that rules it out."}, {"heading": "Acknowledgments", "text": "RCW thanks Tim van Erven for the initial discussions around the Crame\u0301r-Chernoff method during his visit to Canberra in 2013 and for his gracious permission to proceed with the present paper without him as an author, and both authors thank him for the further enormously helpful spotting of a serious error in our original proof for fast rates for VC-type classes. This work was supported by the Australian Research Council (NAM and RCW) and NICTA (RCW). NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence program."}, {"heading": "A Proof of Theorem 3", "text": "Proof of Theorem 3. From Theorem 3 of Kemperman (1968), if the moment values vector ( \u2212 an , 1 ) is in int conv g([\u22121, 1]), the optimal objective value of problem (6) is equal to\nsup { d0 \u2212 a\nn d1 + d2 : d\n\u2217 = (d0, d1, d2) \u2208 D\u2217 } . (15)\nFrom the Feasible Moments Lemma (Lemma 2 in the paper), we see that (10) corresponds to the interior point condition.\nSince we assume the interior point condition is satisfied, any d\u2217 \u2208 D\u2217 provides a lower bound on the optimal value of (6), and hence after negation provides an upper bound on the problem with same moment constraints and the objective sup E e(\u03b7/2)X ; this is precisely what we are after.\nWe therefore focus on picking a good d\u2217 = (d0, d1, d2) \u2208 R3. The inequality condition in (8) is now\n\u2212e(\u03b7/2)x \u2265 d0 + d1x+ d2e\u03b7x for all x \u2208 [\u22121, 1].\nIn particular, this inequality must hold at x = 0, yielding the constraint \u22121 \u2265 d0 + d2. We now change variables to c0 = \u2212d0, c1 = \u2212d1/\u03b7,2 and c2 = \u2212d2, yielding the inequality condition\nu\u03b7(x) := \u2212e(\u03b7/2)x + c0 + c2e\u03b7x + \u03b7c1x \u2265 0.\nNow the condition at x = 0 implies that c0 + c2 = 1, and so we make the replacement\nc0 = 1\u2212 c2, (16)\nin the definition of u\u03b7(x), yielding the inequality\nu\u03b7(x) = 1 + c2(e \u03b7x \u2212 1)\u2212 e(\u03b7/2)x + \u03b7c1x \u2265 0.\nConstraints from the local minimum at 0\nSince u\u03b7(0) = 0, we need x = 0 to be a local minimum of u, and so we require both conditions\n(a) u\u2032(0) = 0\n(b) u\u2032\u2032(0) \u2265 0\nto hold since otherwise there exists some small \u03b5 > 0 such that either u\u03b7(\u03b5) < 0 or u\u03b7(\u2212\u03b5) < 0. For (a), we compute\nu\u2032(x) = \u03b7c2e \u03b7x \u2212 \u03b7\n2 e(\u03b7/2)x + \u03b7c1.\nSince we require u\u2032(0) = 0, we pick up the constraint\n\u03b7 ( c2 \u2212 1\n2 + c1\n) = 0,\nand since \u03b7 > 0 by assumption, we have\nc1 = 1\n2 \u2212 c2. (17)\nThus, we can eliminate c1 from u\u03b7(x):\nu\u03b7(x) = 1 + c2(e \u03b7x \u2212 1)\u2212 e(\u03b7/2)x + \u03b7\n( 1\n2 \u2212 c2\n) x \u2265 0.\nFor (b), it is sufficient to have u\u2032\u2032(0) > 0. Observe that\nu\u2032\u2032(x) = \u03b72c2e \u03b7x \u2212 \u03b7\n2\n4 e(\u03b7/2)x,\nso that u\u2032\u2032(0) = \u03b72 ( c2 \u2212 14 ) , and hence for\nc2 > 1\n4 (18)\nwe have u\u2032\u2032(0) > 0. Thus far, we have picked up the constraints (16), (17), and (18).\n2We scale by \u03b7 here because we are chasing a certain \u03b7-dependent rate.\nThe other minima of u\u03b7(x)\nNow, observe that u\u2032(x) has at most two roots, because with the substitution y = e(\u03b7/2)x, we have\nu\u2032(x) = \u03b7c2y 2 \u2212 \u03b7\n2 y + \u03b7\n( 1\n2 \u2212 c2\n) ,\nwhich is a quadratic equation in y with two roots:\ny = { 1,\n1\u2212 2c2 2c2\n} \u21d2 x = { 0, 2\n\u03b7 log 1\u2212 2c2 2c2\n} .\nNow, since we take c2 > 1 4 and since the second root is negative, we know that u is increasing on [0, 1] (and we already knew that u\u03b7(0) = 0). It remains to find conditions on c2 such that u\u03b7(\u22121) \u2265 0 because that implies that u\u03b7(x) \u2265 0 for all x \u2208 [\u22121, 0]. We consider the case \u03b7 \u2264 1 and \u03b7 > 1 separately.\nIn either case, we need to check the nonnegativity of u\u03b7(\u22121) = 1 + c2(e\u2212\u03b7 \u2212 1)\u2212 e\u2212(\u03b7/2) \u2212 \u03b7 ( 1\n2 \u2212 c2 ) = (\n1\u2212 \u03b7 2\n) \u2212 e\u2212(\u03b7/2) + c2 ( e\u2212\u03b7 \u2212 (1\u2212 \u03b7) ) .\nCase \u03b7 \u2264 1: We observe that u\u03b7(\u22121) = 0 when \u03b7 = 0. Now, we will see what constraints on c2 guarantee that dd\u03b7u\u03b7(\u22121) \u2265 0 for \u03b7 \u2208 [0, 1]. We want\nd\nd\u03b7 u\u03b7(\u22121) = \u2212c2e\u2212\u03b7 +\n1 2 e\u2212\u03b7/2 \u2212 1 2 + c2 \u2265 0\nwhich is equivalent to the condition\nc2 \u2265 1\n2\n( 1\u2212 e\u2212\u03b7/2\n1\u2212 e\u2212\u03b7\n) .\nThe RHS is increasing in \u03b7, and so we need only consider \u03b7 = 1, yielding the bound\nc2 \u2265 1\n2\ne\u2212 \u221a e\ne\u2212 1 = 0.3112 . . . ,\nand so if c2 \u2265 0.32, then u\u03b7(\u22121) \u2265 0 as desired.\nCase \u03b7 > 1: Let c2 = 1 2 \u2212 \u03b1 \u03b7 for some \u03b1 \u2265 0. With this substitution, we have\nu\u03b7(\u22121) = 1 + c2(e\u2212\u03b7 \u2212 1)\u2212 e\u2212(\u03b7/2) \u2212 \u03b7 ( 1\n2 \u2212 c2 ) = 1 + ( 1\n2 \u2212 \u03b1 \u03b7\n) (e\u2212\u03b7 \u2212 1)\u2212 e\u2212(\u03b7/2) \u2212 \u03b1\n=\n( 1 + e\u2212\u03b7\n2 \u2212 e\u2212\u03b7/2\n) + \u03b1 ( \u22121 + 1\n\u03b7\n( 1\u2212 e\u2212\u03b7 )) Since we want the above to be nonnegative for all \u03b7 > 1, we arrive at the condition\n\u03b1 \u2264 inf \u03b7\u22651\n{ 1+e\u2212\u03b7\n2 \u2212 e \u2212\u03b7/2\n1\u2212 1\u03b7 (1\u2212 e\u2212\u03b7)\n} (19)\nPlotting suggests that the minimum is attained at \u03b7 = 1, with the value 12 ( \u221a e \u2212 1)2. We will fix \u03b1 to\nthis value and verify that( 1 + e\u2212\u03b7\n2 \u2212 e\u2212\u03b7/2\n) + ( 1\n2 ( \u221a e\u2212 1)2\n)( \u22121 + 1\n\u03b7\n( 1\u2212 e\u2212\u03b7 )) \u2265 0. (20)\nThis is true with equality at \u03b7 = 0. The derivative of the LHS with respect to \u03b7 is\n1 2 e\u2212\u03b7\n( e\u03b7/2 \u2212 1\u2212 ( \u221a e\u2212 1)2(e\u03b7 \u2212 \u03b7 \u2212 1)\n\u03b72\n) .\nThe derivative is positive at \u03b7 = 1, so 0 is a candidate minimum. Eventually, ( \u221a e\u22121)2(e\u03b7\u2212\u03b7\u22121)\n\u03b72 grows more\nquickly than e\u03b7/2\u22121 and surpasses the latter in value. The derivative is therefore negative for all sufficiently large \u03b7, and so we need only take the minimum of the LHS of (20) evaluated at \u03b7 = 1 and the limiting value as \u03b7 \u2192\u221e. We have\nlim \u03b7\u2192\u221e\n( 1 + e\u2212\u03b7\n2 \u2212 e\u2212\u03b7/2\n) + ( 1\n2 ( \u221a e\u2212 1)2\n)( \u22121 + 1\n\u03b7\n( 1\u2212 e\u2212\u03b7 )) = \u221a e\u2212 e\n2 \u2265 0\nHence, (20) indeed holds for \u03b1 = 12 ( \u221a e\u2212 1)2. We conclude that u\u03b7(\u22121) \u2265 0 when \u03b1 \u2264 12 ( \u221a e\u2212 1)2.\nPutting it all together In the regime \u03b7 \u2264 1, we have the constraints c2 > 14 and c2 \u2265 1 2 e\u2212 \u221a e e\u22121 (which exceeds 1 4 ), so we can choose\nc1 = 1\n2 \u2212 c2 =\n1 2 \u2212 1 2\ne\u2212 \u221a e\ne\u2212 1 =\n1\n2 \u221a e\u2212 1 e\u2212 1 = 0.1877 . . . .\nIn the regime \u03b7 > 1, we have the constraints c2 > 1 4 and \u03b1 \u2264 1 2 ( \u221a e \u2212 1)2 \u21d2 c2 \u2265 12 \u2212 1 2\u03b7 ( \u221a e \u2212 1)2 (which always exceeds 14 for \u03b7 \u2265 1), so we can choose\nc1 = 1\n2 \u2212 c2 =\n\u03b1 \u03b7 \u2264 ( \u221a e\u2212 1)2 2\u03b7 = 0.2104 . . . \u03b7 .\nThe result follows by observing that in the case of \u03b7 \u2264 1, the supremum in (15) is lower bounded by \u22121 + 0.18a\u03b7n , and hence the optimal objective value of (6) is lower bounded by the same quantity. Therefore, the problem with the same constraints and the objective sup\u00b5\u2208[\u22121,1] E e (\u03b7/2)X has its optimal objective value upper bounded by 1\u2212 0.18a\u03b7n . Repeat the same argument for the case of \u03b7 > 1."}, {"heading": "B Hyper-concentrated excess losses", "text": "Lemma 10. Let Z be a random variable with probability measure P supported on [\u2212V, V ]. Suppose that lim\u03b7\u2192\u221e E exp(\u2212\u03b7Z) < 1 and EZ = \u00b5 > 0. Then there is a suitable modification of Z \u2032 for which Z \u2032 \u2264 Z with probability 1, the mean of Z \u2032 is arbitrarily close to \u00b5, and E exp(\u2212\u03b7Z \u2032) = 1 for arbitrarily large \u03b7.\nProof. First, observe that Z \u2265 0 a.s. If not, then there must be some finite \u03b7 > 0 for which E exp(\u2212\u03b7Z) = 1. Now, consider a random variable Z \u2032 with probability measure Q , a modification of Z (with probability measure P ) constructed in the following way. Define A := [\u00b5, V ] and A\u2212 := [\u2212V,\u2212\u00b5]. Then for any > 0 we define Q as\ndQ (z) =  (1\u2212 )dP (z) if z \u2208 A dP (\u2212z) if z \u2208 A\u2212\ndP (z) otherwise.\nAdditionally, we couple P and Q\u03b5 such that the couple (Z,Z \u2032) is a coupling of (P,Q ) satisfying\nE(Z,Z\u2032)\u223c(P,Q ) 1{Z 6=Z\u2032} = min (P \u2032,Q\u2032 ) E(Z,Z\u2032)\u223c(P \u2032,Q\u2032 ) 1Z 6=Z\u2032 ,\nwhere the min is over all couplings of P and Q\u03b5. This coupling ensures that Z \u2032 \u2264 Z with probability 1; i.e. Z \u2032 is dominated by Z. Now,\nE exp(\u2212\u03b7Z \u2032) = \u222b V \u2212V e\u2212\u03b7zdQ (z)\n= \u222b A\u2212 e\u2212\u03b7zdQ (z) + \u222b A e\u2212\u03b7zdQ (z) + \u222b [0,V ]\\A e\u2212\u03b7zdQ (z)\n= \u222b A\u2212 e\u2212\u03b7zdP (\u2212z) + (1\u2212 ) \u222b A e\u2212\u03b7zdP (z) + \u222b [0,V ]\\A e\u2212\u03b7zdP (z)\n= \u222b A e\u03b7zdP (z) + (1\u2212 ) \u222b A e\u2212\u03b7zdP (z) + \u222b [0,V ]\\A e\u2212\u03b7zdP (z)\n\u2265 e\u00b5\u03b7P (A) + (1\u2212 ) \u222b A e\u2212\u03b7zdP (z) + \u222b [0,V ]\\A e\u2212\u03b7zdP (z). (21)\nNow, on the one hand, for any \u03b7 > 0, the sum of the two right-most terms in (21) is strictly less than 1 by assumption. On the other hand, \u03b7 \u2192 P (A)e\u00b5\u03b7 is exponentially increasing since > 0 and \u00b5 > 0 (and hence P (A) > 0 as well) by assumption; thus, the first term in (21) can be made arbitrarily large for large enough by increasing \u03b7. Consequently, we can choose > 0 as small as desired and then choose \u03b7 < \u221e as large as desired such that the mean of Z \u2032 is arbitrarily close to \u00b5 and E exp(\u2212\u03b7Z \u2032) = 1 respectively."}, {"heading": "C Proof of VC-type results", "text": "C.1 Proof of Theorem 6\nThe localization result is a simple consequence of the following theorem.\nTheorem 11 (Local Analysis). Let F \u2282 RX be a separable function class for which:\n\u2022 the constant zero function is an element of F ;\n\u2022 every function f \u2208 F satisfies 0 \u2264 f \u2264 1 ;\n\u2022 supf\u2208F \u2016f\u2016L2(P) \u2264 \u03b5 := 1n .\nFurther assume that for some C \u2265 1, for a constant K \u2265 1, for each u \u2208 (0,K] the L2(P) covering numbers of F are bounded as\nN (u,F , L2(P)) \u2264 ( K\nu\n)C .\nThen provided that n \u2265 4 and y > 0, with probability at least 1\u2212 e\u2212y\nsup f\u2208F\nPn f \u2264 1\nn\n( 990C log(2Kn) + \u221a 2y(1 + 3960C log(2Kn)) + 2y\n3 + 1\n) .\nRemarks\n(i) The class F is contained in an L2(P)-ball of radius \u03b5, and if interpreted as a loss class it is assumed that the losses are bounded.\n(ii) Suppose the function class F is constructed by selecting from a larger class an \u03b5-ball in the L2(P) pseudometric around some function f0 from the same larger class and taking for each function the absolute difference with f0. Then the zero function trivially is in F since |f0\u2212f0| is in the class. In this setup, the theorem states that with high probability there is no function in the class whose empirical risk will be \u201cmuch\u201d smaller/larger than the empirical risk of f0.\nProof of Theorem 11. For the proof, we introduce the random variables Z = supf\u2208F Pn f and Z\u0304 = supf\u2208F (Pn \u2212P)f . The proof is in three steps.\nStep 1: Centering approximation\nIt is easy to see that Z \u2264 Z\u0304 + \u03b5, since\nZ = sup f\u2208F Pn f = sup f\u2208F\n(Pn \u2212P)f + P f\n\u2264 sup f\u2208F (Pn \u2212P)f + sup f\u2208F P f\n\u2264 Z\u0304 + sup f\u2208F \u2016f0 \u2212 f\u2016L1(P)\n\u2264 Z\u0304 + sup f\u2208F \u2016f0 \u2212 f\u2016L2(P)\n\u2264 Z\u0304 + \u03b5,\nwhere the penultimate inequality follows from Jensen\u2019s inequality.\nStep 2: Concentration of Z\u0304 arounds its expectation\nWe will apply Bousquet\u2019s version of Talagrand\u2019s inequality, appearing as equation (18) of (Massart and Ne\u0301de\u0301lec, 2006) and reproduced below for convenience:\nIf G is a countable family of measurable functions such that, for some positive constants v and b, one has, for every g \u2208 G, P g2 \u2264 v and \u2016g\u2016\u221e \u2264 b, then, for every positive y, the following inequality holds for W = supg\u2208G(Pn \u2212P)g:\nPr { W \u2212 EW \u2265 \u221a 2(v + 4bEW )y\nn +\n2by\n3n\n} \u2264 e\u2212y.\nWe take G to be F itself; since F is separable and hence admits a countable dense subset, the countability assumption in Talagrand\u2019s inequality is not an issue. Observe that for every f \u2208 F we have\n\u2022 \u2016f\u2016\u221e \u2264 1 (from the range constraints on f)\n\u2022 P f2 \u2264 \u2016f\u2016L2(P) \u2264 \u03b5 (by the small L2(P)-ball assumption on F) .\nThus, taking b = 1 and v = \u03b5, we have\nPr { Z\u0304 \u2212 E Z\u0304 \u2265 \u221a 2(\u03b5+ 4 E Z\u0304)y\nn +\n2y\n3n\n} \u2264 e\u2212y. (22)\nIt remains to bound E Z\u0304. If it can be shown to be O\u0303( 1n ) then we will have the desired result after taking \u03b5 = O( 1n ).\nStep 3: Controlling the size of E Z\u0304\nControlling E Z\u0304 can be done through chaining after passing to a symmetrized empirical process. This control is shown in Lemma 12, stated after the current proof, yielding the result\nE Z\u0304 \u2264 990C log(2Kn) n . (23)\nPutting it all together\nThe desired result follows by plugging (23) into the concentration result (22), incorporating the \u03b5 approximation term from Step 1, and setting \u03b5 = 1n .\nLemma 12. Take the same conditions as Theorem 11 (Local Analysis Theorem), but instead allow that all f \u2208 F need only satisfy 0 \u2264 f \u2264 V for some V \u2265 1. Then provided that n \u2265 4,\nE sup f\u2208F\n(Pn \u2212P)f \u2264 990CV log(2Kn)\nn\nProof. To avoid measurability issues, we operate under the assumption that F is countable and in the final step of the proof apply an approximation argument.\nLet 1, . . . , n be iid Rademacher random variables. We write E for the expected value with respect to the random variables 1, . . . , n. That is, if A is a random variable depending only on 1, . . . , n, X1, . . . , Xn, then E A = E [A | X1, . . . , Xn]. Also, let F|X be the coordinate projection of F onto the sample X = (X1, . . . , Xn):\nF|X := { (f(X1), . . . , f(Xn) : f \u2208 F } .\nFinally, for a set G \u2282 Rn let D(G) be half of the `2-radius of G, defined as\nD(G) := 1 2 sup g\u2208G \u2016g\u20162;\nit makes sense to refer to this as a (half) radius since we will consider D(F) and the zero function is in F . Our life will be made easier if we use the lower bounded quantity D(G)\u2228\u03c3, for some deterministic \u03c3 \u2264 1 to be chosen later.\nThe first step is symmetrization. The second step is based on a chaining argument, the result of which is Corollary 13.2 of Boucheron et al. (2013), restated here3 we state the result in terms of in a specialization to Rademacher processes for convenience:\nLet (T , d) be a finite pseudometric space and (Xt)t\u2208T be a collection of sub-Gaussian random variables. Then for any t0 \u2208 T ,\nE sup t\u2208T\nXt \u2212Xt0 \u2264 12 \u222b \u03b4/2\n0\n\u221a logN (u, T , d)du,\nwhere \u03b4 = supt\u2208T d(t, t0).\nBy pushing the cardinality of T to infinity, the above result also applies to countable classes. As noted by Boucheron et al. (2013) in the paragraph concluding the statement of their Corollary 13.2, this result applies to Rademacher processes. In our case, t0 will correspond to the zero function element of F .\n3Boucheron et al. (2013) stated this result in terms of packing numbers, but careful inspection of their proof reveals that the argument works for covering numbers as well. Moreover, other proofs generally use covering numbers.\nDefine P\u0304n := Pn \u2212P. Now, from symmetrization and the above chaining-based result applied to the resulting Rademacher process, we have\nnE sup f\u2208F\nP\u0304n f \u2264 2 E E sup f\u2208F n\u2211 j=1 jf(Xj)  \u2264 24 E\n\u222b D(F|X )\u2228\u03c3 0 \u221a logN (u,F|X , \u2016 \u00b7 \u20162)du\nwhich (since if f|X is the obvious coordinate projection of f \u2208 F , then \u2016f|X\u20162 = \u221a n\u2016f\u2016L2(Pn)) is at most\n24 E \u222b D(F|X )\u2228\u03c3 0 \u221a H2 ( u\u221a n ,F|X ) du\n\u2264 24 \u221a C E \u222b D(F|X )\u2228\u03c3\n0\n\u221a log K \u221a n\nu du,\nwhere H2(u, T ) := supQ logN (u, T , L2(Q)) is the universal metric entropy of T , and in the above display we have H2 ( u\u221a n ,F|X ) rather than H2(u,F|X) because we work with the L2(Pn)-norm scaled by \u221a n.\nMaking the substitution t = u/(D(F|X)\u2228\u03c3), the above is equal to\n24 \u221a C E (( D(F|X)\u2228\u03c3 ) \u222b 1 0 \u221a log\nK \u221a n t ( D(F|X)\u2228\u03c3\n)dt)\n\u2264 24 \u221a C E (( D(F|X)\u2228\u03c3 )(\u221a log\nK \u221a n\nD(F|X)\u2228\u03c3 +\n\u221a \u03c0\n2\n))\n\u2264 24 \u221a C (\u221a log K \u221a n\n\u03c3 +\n\u221a \u03c0\n2\n)( \u03c3 + ED(F|X) ) .\nNow, we focus on ED(F|X). Observe that\nED(F|X) = \u221a n\n2 E sup f\u2208F\n\u221a Pn f2\n=\n\u221a n\n2 E sup f\u2208F\n\u221a P\u0304n f2 + P f2\n\u2264 \u221a n\n2 [ E sup f\u2208F \u221a P\u0304n f2 + sup f\u2208F \u221a P f2 ]\n\u2264 \u221a n\n2 [\u221a E sup f\u2208F P\u0304n f2 + \u03b5 ] .\nwhere the first part of the last step follows from Jensen\u2019s inequality and the second part follows from the small L2(P)-ball assumption on F . The above is at most\n\u221a V n\n2 [\u221a E sup f\u2208F P\u0304n f + \u03b5 ] .\nThus, putting everything together and making the replacement \u03b5 = 1n , we have\nE sup f\u2208F\nP\u0304n f \u2264 24 \u221a C\nn\n(\u221a log K \u221a n\n\u03c3 +\n\u221a \u03c0\n2\n)( \u03c3 + \u221a V n\n2 [\u221a E sup f\u2208F P\u0304n f + 1 n ]) .\nFinding the minimal value of E supf\u2208F P\u0304n just amounts to solving a quadratic equation, yielding the solution set\n\u221a E sup f\u2208F P\u0304n f \u2264 \u03c8 \u221a V n 2 + \u221a\u221a\u221a\u221a\u03c8(\u03c3 + \u221aV/n 2 )\nfor \u03c8 = 24 \u221a C\nn\n(\u221a log K \u221a n \u03c3 + \u221a \u03c0 2 ) .\nMaking the replacement \u03c3 = 1n , squaring, and some coarse bounding yields\nE sup f\u2208F\nP\u0304n f \u2264 990CV log(Kn)\nn\nfor n \u2265 4, V \u2265 1, and C \u2265 1. We now present the approximation argument to handle separable F . Since F is separable, it suffices to consider a countable dense subset F \u2032 \u2282 F ; however, a little more work is required as the covering numbers of F \u2032 may differ slightly from the covering numbers of F . We now control the covering numbers of F \u2032 in terms of the covering numbers of F . Observe that if there is an \u03b5-net of F of cardinality N , then there is a (2\u03b5)-net of some F \u2032 \u2282 F of cardinality N . Hence, if there is an optimal \u03b5-net of F of cardinality N , then an optimal (2\u03b5)-net of F \u2032 has cardinality at most N . That is, for any probability measure Q on X , for any u > 0 we have N (2u,F \u2032, L2(Q)) \u2264 N (u,F , L2(Q)). Thus, the result for separable F holds by replacing the constant K with 2K.\nWe now prove the localization result.\nProof of Theorem 6. First, so that we can just handle the case of functions with range [0, 1], we (crudely) apply our analysis to the function class after scaling all functions by the factor 1V , and scale the approximation term in the last step by the factor V .4\nFor any f0 \u2208 F\u03b5, observe that \u03c0\u22121(f0) is the set of those functions that are covered by f0 in the L2(P)norm. We apply the Local Analysis Theorem (Theorem 11) to each element of the set of localized absolute difference function classes\n{Gf0 : f0 \u2208 F\u03b5} ,\nfor Gf0 := { |f0 \u2212 f | : f \u2208 \u03c0\u22121(f0) } . Consider an arbitrary f0 \u2208 F\u03b5 and its corresponding class Gf0 . Since\nG\u0303f0 := { f0 \u2212 f : f \u2208 \u03c0\u22121(f0) } is isomorphic to a subset of F , and since any \u03b5-net for G\u0303f0 trivially gives rise to an \u03b5-net for Gf0 by taking the absolute value of each function from the original \u03b5-net, it follows the L2(P) covering numbers of Gf0 are bounded just as in (11).\nTaking the union bound over F\u03b5 with Theorem 11 implies that with probability at least 1\u2212 \u03b4\nmax f0\u2208F\u03b5 sup f\u2208\u03c0\u22121(f0)\nPn |f0 \u2212 f |\n\u2264 1 n\n( 990C log(2Kn) + \u221a 2 ( log 1\n\u03b4 + C log(Kn)\n) (1 + 3960C log(2Kn)) + 2 ( log 1\u03b4 + C log(Kn) ) 3 + 1 ) .\nIgnoring the 1n factor, the RHS is at most\n990C log(2Kn) + \u221a 2 ( log 1\n\u03b4 + C log(Kn)\n) (1 + 3960C log(2Kn)) + log e\n\u03b4 + C log(Kn),\n4It may be possible to get a weaker dependence on V with a more careful argument that depends on V throughout; in particular, Talagrand\u2019s inequality can handle the parameter V .\nwhich is at most\n991C log(2Kn) + \u221a 2 log 1\n\u03b4 + 2C log(Kn) + 7920\n( log 1\n\u03b4\n) C log(2Kn) + 7920(C log(2Kn))2 + log e\n\u03b4\n\u2264 1080C log(2Kn) + \u221a 2 log 1\n\u03b4 + 2C log(Kn) + 7920\n( log 1\n\u03b4\n) C log(2Kn) + log e\n\u03b4\n\u2264 1080C log(2Kn) + 90 \u221a( log 1\n\u03b4\n) C log(2Kn) + log e\n\u03b4 ,\nwhere the last inequality holds provided that \u03b4 is not too large; it suffices to assume \u03b4 \u2264 12 .\nFinally, we prove the fast rates exact oracle inequality for VC-type classes.\nC.2 Proof of Theorem 7\nProof of Theorem 7. For convenience, we begin by abusing notation and redefining F as F := `\u25e6F ; the abuse includes f\u2217 being redefined as `(\u00b7, f\u2217). With these abuses, for any f \u2208 F we redefine Zf as Zf := f \u2212 f\u2217.\nNext, we introduce a few subclasses that will be in play. Recall that for any \u03b3n > 0, F \u03b3n is the subclass of F for which the excess risk is at least \u03b3n. Also, for any \u03b3n > 0, let F \u03b3n,\u03b5 be a proper cover of F \u03b3n with respect to the L2(Pn) norm, with \u03b5 = 1n . For each \u03b7 > 0 and F \u03b3n,\u03b5, let F (\u03b7) \u03b3n,\u03b5 \u2282 F \u03b3n,\u03b5 correspond to those functions for which \u03b7 is the largest constant such that E exp(\u2212\u03b7Zf ) = 1. After making the same implicit change to F \u03b3n,\u03b5 for \u201chyper-concentrated\u201d excess loss random variables (i.e. those Zf for which lim\u03b7\u2192\u221e E exp(\u2212\u03b7Zf ) < 1) as was made to F \u03b3n in the proof of Theorem 5, we have F \u03b3n,\u03b5 =\u22c3 \u03b7\u2208[\u03b7\u2217,\u221e) F (\u03b7) \u03b3n,\u03b5.\nLet \u03b3n = a n for some constant a > 1 to be fixed later. Consider an arbitrary \u03b7 \u2208 [\u03b7 \u2217,\u221e) for which |F (\u03b7) \u03b3n,\u03b5| > 0, and recall that all functions f in this class satisfy EZf \u2265 a n . Individually for each such function f , we will apply the Crame\u0301r-Chernoff Theorem (Theorem 1) as follows. From the Bounded Losses Lemma (Lemma 4), we have \u039b\u2212Zf (\u03b7/2) = \u039b\u2212 1V Zf (V \u03b7/2). From the Stochastic Mixability Concentration Theorem (Theorem 3), the latter is at most\n\u22120.18(V \u03b7 \u2227 1)(a/V ) n = \u2212 0.18\u03b7a (V \u03b7 \u2228 1)n .\nHence, the Crame\u0301r-Chernoff Theorem (Theorem 1) with t = a2n and the \u03b7 from that Theorem taken to be \u03b7/2 implies that the probability of the event Pn f \u2264 Pn f\u2217 + a2n is at most\nexp ( \u22120.18 \u03b7\nV \u03b7 \u2228 1 a+\na\u03b7\n4n\n) = exp ( \u2212\u03b7a ( 0.18\nV \u03b7 \u2228 1 \u2212 1 4n\n)) .\nApplying the union bound over all of F \u03b3n,\u03b5, we conclude that\nPr { \u2203f \u2208 F \u03b3n,\u03b5 : Pn f \u2264 Pn f\u2217 + a\n2n\n} \u2264 ( K\n\u03b5\n)C exp ( \u2212\u03b7\u2217a ( 0.18\nV \u03b7\u2217 \u2228 1 \u2212 1 4n\n)) .\nNow, observe that if we consider some fixed failure probability \u03b42 and invert to obtain the corresponding\na, we have\na = C log K\u03b5 + log 2 \u03b4 \u03b7\u2217 (\n0.18 V \u03b7\u2217 \u2228 1 \u2212 1 4n\n) = C log K\u03b5 + log 2\u03b4 \u03b7\u2217 (\n0.18\u2212(V \u03b7\u2217 \u2228 1)/(4n) V \u03b7\u2217 \u2228 1 ) \u2264 (V \u03b7\u2217 \u2228 1) ( C log K\u03b5 + log 2 \u03b4\n) \u03b7\u2217 ( 0.18\u2212 14n\n) \u2264 8 ( V \u2228 ( 1\n\u03b7\u2217\n))( C log K\n\u03b5 + log\n2\n\u03b4\n) =: \u03bb, (24)\nfor \u03b3 (1) n := \u03bb n , where the last inequality holds since n \u2265 5. Note that by instead setting a n = \u03b3 (1) n (defined in (24)) the failure probability can only decrease. Thus, for any \u03b3n \u2265 \u03b3(1)n , we have\nPr { \u2203f \u2208 F \u03b3n,\u03b5 : Pn f \u2264 Pn f\u2217 +\n\u03b3n 2\n} \u2264 \u03b4\n2 .\nNext, we control the behavior of the subclass F \u03b3n \\ F \u03b3n,\u03b5. From Theorem 6, if \u03b4 \u2264 12\nPr { \u2203f \u2208 F \u03b3n : Pn f < Pn \u03c0(f)\u2212 \u03b3(2)n } \u2264 \u03b4\n2 .\nfor \u03b3 (2) n = V n\n( 1080C log(2Kn) + 90 \u221a( log 2\u03b4 ) C log(2Kn) + log 2e\u03b4 ) .\nNow, combining the above two high probability guarantees, with probability at least 1\u2212\u03b4 both statements below hold for all f \u2208 F \u03b3n :\nPn f \u2265 Pn \u03c0(f)\u2212 \u03b3(2)n Pn \u03c0(f) \u2265 Pn f\u2217 +\n\u03b3n 2 .\nThus, with the same probability, for all f \u2208 F \u03b3n :\nPn f \u2265 Pn f\u2217 + \u03b3n 2 \u2212 \u03b3(2)n .\nSetting \u03b3n = (\u03b3 (1) n \u2228 2\u03b3(2)n )+ 1n , and recalling that ERM selects hypotheses purely based on their empirical risk, we see that with probability at least 1\u2212 \u03b4, ERM will not select any hypothesis whose excess risk is at least\n(\u03b3(1)n \u2228(2\u03b3(2)n )) + 1\nn ."}], "references": [{"title": "A stochastic view of optimal regret through minimax duality", "author": ["Jacob Abernethy", "Alekh Agarwal", "Peter L. Bartlett", "Alexander Rakhlin"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "Fast learning rates in statistical inference through aggregation", "author": ["Jean-Yves Audibert"], "venue": "The Annals of Statistics,", "citeRegEx": "Audibert.,? \\Q2009\\E", "shortCiteRegEx": "Audibert.", "year": 2009}, {"title": "Empirical minimization", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2006}, {"title": "Local Rademacher complexities", "author": ["Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Safe learning: bridging the gap between Bayes, MDL and statistical learning theory via empirical convexity", "author": ["Peter Gr\u00fcnwald"], "venue": "In Proceedings of the 24th International Conference on Learning Theory (COLT", "citeRegEx": "Gr\u00fcnwald.,? \\Q2011\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2011}, {"title": "The safe Bayesian", "author": ["Peter Gr\u00fcnwald"], "venue": "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT", "citeRegEx": "Gr\u00fcnwald.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2012}, {"title": "The weak aggregating algorithm and weak mixability", "author": ["Yuri Kalnishkan", "Michael V. Vyugin"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory (COLT", "citeRegEx": "Kalnishkan and Vyugin.,? \\Q2005\\E", "shortCiteRegEx": "Kalnishkan and Vyugin.", "year": 2005}, {"title": "Kemperman. The general moment problem, a geometric approach", "author": ["H.B. Johannes"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Johannes,? \\Q1968\\E", "shortCiteRegEx": "Johannes", "year": 1968}, {"title": "Interplay between concentration, complexity and geometry in learning theory", "author": ["Guillaume Lecu\u00e9"], "venue": "Ecole dEte\u0301 de Probabilite\u0301s de Saint-Flour XXXVIII-2008,", "citeRegEx": "Lecu\u00e9.,? \\Q2011\\E", "shortCiteRegEx": "Lecu\u00e9.", "year": 2011}, {"title": "Estimation of mixture models", "author": ["Jonathan Qiang Li"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Li.,? \\Q1974\\E", "shortCiteRegEx": "Li.", "year": 1974}, {"title": "Agnostic learning nonconvex function classes", "author": ["2008b. Shahar Mendelson", "Robert C. Williamson"], "venue": null, "citeRegEx": "Mendelson and Williamson.,? \\Q2008\\E", "shortCiteRegEx": "Mendelson and Williamson.", "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "(Vovk, 2001; Kalnishkan and Vyugin, 2005)) have developed a rich convex geometric understanding of mixability, stochastic mixability can be understood as a sort of effective convexity.", "startOffset": 0, "endOffset": 41}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.", "startOffset": 14, "endOffset": 40}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.", "startOffset": 14, "endOffset": 378}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.\u2019s (2012) injection of the concept of mixability into the statistical learning space in the form of stochastic mixability.", "startOffset": 14, "endOffset": 423}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.\u2019s (2012) injection of the concept of mixability into the statistical learning space in the form of stochastic mixability. It is this last connection that will be our departure point for this work. Mixability is a fundamental property of a loss that characterizes when constant regret is possible in the online learning game of prediction with expert advice (Vovk, 1998). Stochastic mixability is a natural adaptation of mixability to the statistical learning setting; in fact, in the special case where the function class consists of all possible functions from the input space to the prediction space, stochastic mixability is equivalent to mixability (Van Erven et al., 2012). Just as Vovk and coworkers (see e.g. (Vovk, 2001; Kalnishkan and Vyugin, 2005)) have developed a rich convex geometric understanding of mixability, stochastic mixability can be understood as a sort of effective convexity. In this work, we study the O( 1 n )-fast rate phenomenon in statistical learning from the perspective of stochastic mixability. Our motivation is that stochastic mixability might characterize fast rates in statistical learning. As a first step, Theorem 5 of this paper establishes via a rather direct argument that stochastic mixability implies an exact oracle inequality (i.e. with leading constant 1) with a fast rate for finite function classes, and Theorem 7 extends this result to VC-type classes. This result can be understood as a new chapter in an evolving narrative that started with Lee et al.\u2019s (1998) seminal paper showing fast rates for agnostic learning with squared loss over convex function classes, and that was continued by Mendelson (2008b) who showed that fast rates are possible for p-losses (y, \u0177) 7\u2192 |y \u2212 \u0177| over effectively convex function classes by passing through a Bernstein condition (defined in (12)).", "startOffset": 14, "endOffset": 1928}, {"referenceID": 0, "context": "These include Abernethy et al.\u2019s (2009) unified Bregman-divergence based analysis of online convex optimization and statistical learning, the online-to-batch conversion of the exponentially weighted average forecaster (a special case of the aggregating algorithm for mixable losses) which yields the progressive mixture rule as can be seen e.g. from the work of Audibert (2009), and most recently Van Erven et al.\u2019s (2012) injection of the concept of mixability into the statistical learning space in the form of stochastic mixability. It is this last connection that will be our departure point for this work. Mixability is a fundamental property of a loss that characterizes when constant regret is possible in the online learning game of prediction with expert advice (Vovk, 1998). Stochastic mixability is a natural adaptation of mixability to the statistical learning setting; in fact, in the special case where the function class consists of all possible functions from the input space to the prediction space, stochastic mixability is equivalent to mixability (Van Erven et al., 2012). Just as Vovk and coworkers (see e.g. (Vovk, 2001; Kalnishkan and Vyugin, 2005)) have developed a rich convex geometric understanding of mixability, stochastic mixability can be understood as a sort of effective convexity. In this work, we study the O( 1 n )-fast rate phenomenon in statistical learning from the perspective of stochastic mixability. Our motivation is that stochastic mixability might characterize fast rates in statistical learning. As a first step, Theorem 5 of this paper establishes via a rather direct argument that stochastic mixability implies an exact oracle inequality (i.e. with leading constant 1) with a fast rate for finite function classes, and Theorem 7 extends this result to VC-type classes. This result can be understood as a new chapter in an evolving narrative that started with Lee et al.\u2019s (1998) seminal paper showing fast rates for agnostic learning with squared loss over convex function classes, and that was continued by Mendelson (2008b) who showed that fast rates are possible for p-losses (y, \u0177) 7\u2192 |y \u2212 \u0177| over effectively convex function classes by passing through a Bernstein condition (defined in (12)).", "startOffset": 14, "endOffset": 2075}, {"referenceID": 3, "context": "Much of the recent work in obtaining faster learning rates in agnostic learning has taken place in settings where a Bernstein condition holds, including results based on local Rademacher complexities (Bartlett et al., 2005; Koltchinskii, 2006).", "startOffset": 200, "endOffset": 243}, {"referenceID": 8, "context": "This is precisely the situation at the heart of the works of Mendelson (2008b) and Mendelson and Williamson (2002), which show that having non-unique minimizers is symptomatic of bad geometry of the learning problem.", "startOffset": 83, "endOffset": 115}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems.", "startOffset": 59, "endOffset": 89}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems.", "startOffset": 59, "endOffset": 230}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems.", "startOffset": 59, "endOffset": 250}, {"referenceID": 2, "context": "The Bernstein condition appears to have first been used by Bartlett and Mendelson (2006) in their analysis of empirical risk minimization; this condition is subtly different from the margin condition of Mammen and Tsybakov (1999) and Tsybakov (2004), which has been used to obtain fast rates for classification problems. Lecu\u00e9 (2011) pinpoints that the difference between the two conditions is that the margin condition applies to the excess loss relative to the best predictor (not necessarily in the model class) whereas the Bernstein condition applies to the excess loss relative to the best predictor in the model class.", "startOffset": 59, "endOffset": 334}, {"referenceID": 8, "context": "Condition (1) has a rich history, beginning from the foundational thesis of Li (1999) who studied the special case of \u03b7\u2217 = 1 in density estimation with log loss from the perspective of information geometry.", "startOffset": 76, "endOffset": 86}, {"referenceID": 5, "context": "The connections that Li showed between this condition and convexity were strengthened by Gr\u00fcnwald (2011, 2012) and Van Erven et al. (2012).", "startOffset": 89, "endOffset": 139}, {"referenceID": 4, "context": "For each hypothesis, this guarantee will flow from the Cram\u00e9r-Chernoff method (Boucheron et al., 2013) by controlling the cumulant generating function (CGF) of \u2212Zf in a particular way to yield exponential concentration.", "startOffset": 78, "endOffset": 102}], "year": 2014, "abstractText": "Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution P and returns a hypothesis f chosen from a fixed class F with small loss `. In the parametric setting, depending upon (`,F ,P) ERM can have slow (1/ \u221a n) or fast (1/n) rates of convergence of the excess risk as a function of the sample size n. There exist several results that give sufficient conditions for fast rates in terms of joint properties of `, F , and P, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss ` (there being no role there for F or P). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of (`,F ,P), and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.", "creator": "LaTeX with hyperref package"}}}