{"id": "1406.4877", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "On the Application of Generic Summarization Algorithms to Music", "abstract": "several generic summarization algorithms were developed in very past and successfully applied in fields such as music analytics speech complexity. in this paper, we review firstly apply these algorithms to music. to evaluate this summarization's performance, we approach an extrinsic approach : we compare a fado genre classifier's performance using contiguous contiguous clips against the summaries extracted with linear algorithms on 2 matching datasets. we noticed that maximal marginal relevance ( mmr ), lexrank and latent semantic analysis ( lsa ) all improve classification performance in data datasets prior for testing.", "histories": [["v1", "Wed, 18 Jun 2014 20:10:22 GMT  (12kb)", "http://arxiv.org/abs/1406.4877v1", "12 pages, 1 table; Submitted to IEEE Signal Processing Letters"]], "COMMENTS": "12 pages, 1 table; Submitted to IEEE Signal Processing Letters", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.SD", "authors": ["francisco raposo", "ricardo ribeiro", "david martins de matos"], "accepted": false, "id": "1406.4877"}, "pdf": {"name": "1406.4877.pdf", "metadata": {"source": "CRF", "title": "On the Application of Generic Summarization Algorithms to Music", "authors": ["Francisco Raposo"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n48 77\nv1 [\ncs .I\nR ]\n1 8\nJu n\n20 14\nSeveral generic summarization algorithms were developed in the past and successfully applied in fields such as text and speech summarization. In this paper, we review and apply these algorithms to music. To evaluate this summarization\u2019s performance, we adopt an extrinsic approach: we compare a Fado Genre Classifier\u2019s performance using truncated contiguous clips against the summaries extracted with those algorithms on 2 different datasets. We show that Maximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA) all improve classification performance in both datasets used for testing.\nI. INTRODUCTION\nSeveral algorithms to summarize music have been published [1]\u2013[8], mainly for popular music songs whose structure is repetitive enough. However, those algorithms were devised with the goal of producing a thumbnail of a song as its summary, the same way an image\u2019s thumbnail is that image\u2019s summary. Therefore, the goal is to output a shorter version of the original song so that people can quickly get the gist of the whole piece without listening to all of it. These algorithms usually extract continuous segments because of their human consumption-oriented purpose.\nFrancisco Raposo is with Instituto Superior Te\u0301cnico, Universidade de Lisboa, Av. Rovisco Pais, 1049-001 Lisboa, Portugal Ricardo Ribeiro is with Instituto Universita\u0301rio de Lisboa (ISCTE-IUL), Av. das Forc\u0327as Armadas, 1649-026 Lisboa, Portugal David Martins de Matos is with Instituto Superior Te\u0301cnico, Universidade de Lisboa, Av. Rovisco Pais, 1049-001 Lisboa,\nPortugal\nRicardo Ribeiro and David Martins de Matos are with L2F - INESC ID Lisboa, Rua Alves Redol, 9, 1000-029 Lisboa,\nPortugal\nThis work was supported by national funds through FCT \u2013 Fundac\u0327a\u0303o para a Cie\u0302ncia e a Tecnologia, under project PEst-\nOE/EEI/LA0021/2013.\nJune 20, 2014 DRAFT\nGeneric summarization algorithms have also been developed for and are usually applied in text summarization. Their application, in music, to extract a thumbnail is not ideal, because a \u201cgood\u201d thumbnail entails requirements such as coherence and clarity. These summaries are composed of small segments from different parts of the song which makes them unsuitable for human enjoyment and thus may help evade copyright issues. Nevertheless, most of these algorithms produce summaries that are both concise and diverse.\nWe review several summarization algorithms, in order to summarize music for automatic, instead of human, consumption. The idea is that a summary clip contains more relevant and less redundant information and, thus, may improve the performance of certain tasks that rely on processing just a portion of the whole audio signal. We evaluate the summarization\u2019s contribution by comparing the performance of a Portuguese music style Fado Genre Classifier [9] using the extracted summaries of the songs against using contiguous clips (truncated from the beginning, middle and end of the song). We summarize music using MMR, LexRank, LSA and also with a method for music summarization called Average Similarity for comparison purposes. We present results on 2 datasets showing that MMR, LexRank and LSA improve classification performance under certain parameter combinations.\nSection II reviews related work on summarization. Specifically, the following algorithms are reviewed: Average Similarity in section II-A, MMR in section II-B, LexRank in section II-C and LSA in section II-D. Section III describes the details of the experiments we performed for each algorithm and introduces the Fado Classifier. Section IV reports and discusses our classification results and section V concludes this paper with some remarks and future work."}, {"heading": "II. SUMMARIZATION", "text": "Several algorithms for both generic and music summarization have been proposed. However, music summarization algorithms were developed to extract an audible summary so that any person can listen to it coherently. Our focus is on automatic consumption, so coherence and clarity are not mandatory requirements for our summaries.\nLexRank [10] and TextRank [11] are centrality-based methods that rely on the similarity between every sentence. These are based on Google\u2019s PageRank [12] algorithm for ranking web pages and are successfully applied in text summarization. GRASSHOPPER [13] is another method applied in text summarization, as well as social network analysis, focusing on improving diversity in ranking sentences. MMR [14], [15], applied in speech summarization, is a query-specific method that selects sentences according to their similarity to the query and to the sentences previously selected. LSA [16] is another\nJune 20, 2014 DRAFT\nmethod used in text summarization based on the mathematical technique Singular Value Decomposition (SVD).\nMusic-specific summarization structurally segments songs and then selects which segments to include in the summary. This segmentation aims to extract meaningful segments (e.g. chorus, bridge). [1] presents two approaches for segmentation: using a Hidden Markov Model (HMM) to detect key changes between frames and Dynamic Time Warping (DTW) to detect repeating structure. In [2], segmentation is achieved by correlating a Gaussian-tempered \u201ccheckerboard\u201d kernel along the main diagonal of the similarity matrix of the song, outputting segment boundaries. Then, a segment-indexed similarity matrix is built, containing the similarity between every detected segment. SVD is applied to that matrix to find its rankK approximation. Segments are, then, clustered to output the song\u2019s structure. In [3], [4], songs are segmented in 3 stages. First, a similarity matrix is built and it is analyzed for fast changes, outputting segment boundaries. These segments are clustered to output the \u201cmiddle states\u201d. Finally, an HMM is applied to these states, producing the final segmentation. These algorithms then follow some strategies to select the appropriate segments. [5] groups (based on the Kullback-Leibler (KL) divergence) and labels similar segments of the song and then the summary is generated by taking the longest sequence of segments belonging to the same cluster. In [6], [7], a method called Average Similarity is used to extract a thumbnail L seconds long that is most similar to the whole piece. Another method for this task is the Maximum Filtered Correlation [8] which starts by building a similarity matrix and then a filtered time-lag matrix, which has the similarity between extended segments embedded in it. Finding the maximum value in the latter is finding the starting position of the summary.\nTo apply generic summarization algorithms to music, first we need to segment the song into musical words/terms. This fixed segmentation differs a lot from the structural segmentation used in music-specific algorithms. Fixed segmentation does not take into account the human perception of musical structure. It simply allows us to look at the variability and repetition of the signal and use them to find the most important parts. Structural segmentation aims to find meaningful segments (to people) of the song so that we can later select those segments to include in the summary. This type of segmentation often leads to audible summaries which violate copyrights of the original songs. Fixed segmentation combined with generic summarization algorithms may help evade those issues.\nIn the following sections we review the algorithms we chose to evaluate: Average Similarity, MMR,\nLexRank, and LSA.\nJune 20, 2014 DRAFT"}, {"heading": "A. Average Similarity", "text": "This approach to summarization has the purpose of finding a fixed-length continuous music segment, of duration L, most similar to the entire song. This method was introduced in [6] and later used in other research efforts such as [7].\nThe method consists of building a similarity matrix for the song and calculating an aggregated measure\nof similarity between the whole song and every L seconds long segment.\nIn [6], 45 Mel Frequency Cepstral Coefficient (MFCC)s are computed but only the 15 with highest\nvariance are kept. The cosine distance is used to calculate pairwise similarities.\nIn [7], the first 13 MFCCs and the spectral centre of gravity (sound \u201cbrightness\u201d) are used. The\nTchebychev distance was selected for building the similarity matrix.\nOnce the similarity between every frame is calculated, we build a similarity matrix S and embed the\nsimilarity values between feature vectors vi and vj in it: S (i, j) = s (vi, vj).\nThe average similarity measure can be calculated by summing up columns (or rows, since the similarity matrix is symmetric) of the similarity matrix, according to the desired summary length L, starting from different initial frames. The maximum score will correspond to the segment that is most similar to the whole song. To find the best summary of length L, we must compute the score QL (i):\nQL (i) = S\u0304 (i, i+ L) = 1\nNL\ni+L \u2211\nm=i\nN \u2211\nn=1\nS (m,n) (1)\nN is the number of frames in the entire piece. The index 1 \u2264 i \u2264 (N \u2212 L) of the best summary\nstarting frame is the one that maximizes QL (i).\nThe evaluations of this method in the literature are subjective (human) evaluations that take into account whether the generated summaries include the most memorable part(s) of the song [6]. Other evaluations are averages of scores given by test subjects, regarding specific qualities of the summary such as Clarity, Conciseness and Coherence [7]."}, {"heading": "B. Maximal Marginal Relevance", "text": "MMR [17], selects sentences from the signal according to their relevance and to their diversity against the already selected sentences in order to output low-redundancy summaries. This approach has been used in speech summarization [14], [15]. It is a query-specific summarization method, though it is possible to produce generic summaries by taking the centroid vector of all the sentences (as in [15]) as the query.\nMMR iteratively selects the sentence Si that maximizes the following mathematical model:\nJune 20, 2014 DRAFT\n\u03bb (Sim1 (Si, Q))\u2212 (1\u2212 \u03bb)max Sj Sim2 (Si, Sj) (2)\nSim1 and Sim2 are the, possibly different, similarity metrics; Si are the unselected sentences and Sj are the previously selected ones; Q is the query and \u03bb is a configurable parameter that allows the selection of the next sentence to be based on its relevance, its diversity or a linear combination of both. Usually sentences are represented as Term Frequency - Inverse Document Frequency (TF-IDF) scores vectors. The cosine similarity is frequently used as Sim1 and Sim2."}, {"heading": "C. LexRank", "text": "LexRank [10] is a centrality-based method that relies on the similarity for each sentence pair. This centrality-based method is based on Google\u2019s PageRank [12] algorithm for ranking web pages. The output is a list of ranked sentences from which we can extract the most central ones to produce a summary.\nFirst, we compare all sentences, normally represented as TF-IDF scores vectors, to each other using a similarity measure. LexRank uses the cosine similarity. After this step, we build a graph where each sentence is a vertex and edges are created between every sentence according to their pairwise similarity. Usually, the similarity score must be higher than some threshold to create an edge. LexRank can be used with both weighted and unweighted edges. Then, we perform the following calculation iteratively for each vertex until convergence is achieved (when the error rate of two successive iterations is below a certain threshold for every vertex):\nS (Vi) = (1\u2212 d)\nN + S1 (Vi) (3)\nS1 (Vi) = d\u00d7 \u2211\nVj\u2208adj[Vi]\nSim (Vi, Vj) \u2211\nVk\u2208adj[Vj ] Sim (Vj, Vk)\nS (Vj) (4)\nd is a damping factor to guarantee the convergence of the method, N is the total number of vertices and S (Vi) is the score of vertex i. This is the case where edges are weighted. When using unweighted edges, the equation is simpler:\nS (Vi) = (1\u2212 d)\nN + d\u00d7\n\u2211\nVj\u2208adj[Vi]\nS (Vj) D (Vj) (5)\nD (Vi) is the degree (i.e., number of edges) of vertex i. We can construct a summary by taking the\nhighest ranked sentences until a certain summary length is reached.\nJune 20, 2014 DRAFT\nThis method is based on the fact that sentences recommend each other. A sentence very similar to many other sentences will get a high score. Sentence score is also determined by the score of the sentences recommending it."}, {"heading": "D. Latent Semantic Analysis", "text": "LSA is based on the mathematical technique SVD that was first used for text summarization in [16]. SVD is used to reduce the dimensionality of an original matrix representation of the text. To perform LSA-based text summarization, we start by building a T terms by N sentences matrix A.\nEach element of A, aij = LijGi, has two weight components: a local weight and a global weight. The local weight is a function of the number of times a term occurs in a specific sentence and the global weight is a function of the number of sentences that contain a specific term.\nApplying SVD to matrix A will result in a decomposition formed by three matrices: U , a T \u00d7 N matrix of left singular vectors (its columns); \u03a3, a N \u00d7N diagonal matrix of singular values; and V T , a N \u00d7N matrix of right singular vectors (its rows): A = U\u03a3V T .\nSingular values are sorted by descending order in matrix \u03a3 and are used to determine topic relevance. Each latent dimension corresponds to a topic. We calculate the Rank K approximation by taking the first K columns of U , the K \u00d7 K sub-matrix of \u03a3 and the first K rows of V T . We can extract the most relevant sentences by iteratively selecting sentences corresponding to the indices of the highest values for each (most relevant) right singular vector.\nIn [18], two limitations of this approach are discussed: the fact that K is equal to the number of sentences in the summary, which, as it increases, tends to include less significant sentences; and that sentences with high values in several dimensions (topics), but never the highest, will never be included in the summary. To compensate for these problems, a sentence score was introduced and K is chosen so that the Kth singular value does not fall under half of the highest singular value: score (j) = \u221a\n\u2211k i=1 v 2 ij\u03c3 2 i ."}, {"heading": "III. EXPERIMENTS", "text": "To evaluate these algorithms on music, we tested their impact on a Fado classifier. This classifier simply classifies a song as Fado or non-Fado. Fado is a Portuguese music genre whose instrumentation usually consists solely of stringed instruments, such as the classical guitar and the Portuguese guitar. The classifier is a Support Vector Machine (SVM) [19].\nThe features used by the SVM consist of a 32-dimensional vector per song, which is a concatenation of 4 features: average vector of the first 13 MFCCs of the song; Root Mean Square (RMS) energy; high frequencies 9-dimensional rhythmic features; and low frequencies 9-dimensional rhythmic features.\nJune 20, 2014 DRAFT\nThese rhythmic features are computed based on the Fast Fourier Transform (FFT) coefficients on the 20 Hz to 100 Hz range (low frequencies) and on the 8000 Hz to 11025 Hz range (high frequencies). Assuming v is a matrix of FFT coefficients with frequency varying through columns and time through lines, each component of the 9-dimensional vector is: maxamp: max of the average v along time; minamp: min of the average v along time; number of v values above 80% of maxamp; number of v values above 15% of maxamp; number of v values above maxamp; number of v values below minamp; mean distance between peaks; standard deviation of distance between peaks; max distance between peaks.\nThese features capture rhythmic information in both low and high frequencies. Fado does not have much information in the low frequencies as it does not contain, for example, drum kicks. However, due to the string instruments used, Fado information content is higher in the high frequencies, making these features good for distinguishing it from other genres.\nWe used 2 datasets in our experiments which consist of 500 songs from which half of them are Fado songs and the other half are not. The 250 Fado songs are the same in both datasets. The datasets are encoded in mono, 16-bit, 22050 Hz Microsoft WAV files. We will make the post-summarization datasets available upon request.\nWe used 5-fold cross validation when calculating classification performance. The classification performance was calculated first for the beginning, middle and end sections (of 30s) of the songs to get a baseline and then we compared it with the classification using the summaries (also 30s) for each parameter combination and algorithm.\nFor feature extraction we used OpenSMILE\u2019s [20] implementation, namely, to extract MFCC feature vectors. We also used the Armadillo library [21] for matrix operations and the Marsyas library [22] for synthesizing the summaries.\nFor Average Similarity, we experimented with 3 different frame sizes (0.25, 0.5, and 1 s) with both\n50% and no overlap. We also experimented with MFCC vector sizes of 12 and 24.\nTo use the generic summarization algorithms, however, we need additional processing steps. We adapted those algorithms to the music domain by mapping the audio signal frames (represented as MFCC vectors) to a discrete representation of words and sentences. For each piece being summarized, we cluster all of its frames using the mlpack\u2019s [23] K-Means algorithm implementation which calculates the vocabulary for that song (i.e., each frame is now a word from that vocabulary). Then, we segment the whole piece into fixed-size sentences (e.g., 5-word sentences). This allows us to represent each sentence as a vector of word occurrences/frequencies (depending on the type of weighting chosen) which lets us compare sentences with each other using the cosine distance.\nJune 20, 2014 DRAFT\nIn our implementation of MMR, we calculate the similarity between every sentence only once and then apply the algorithm until the desired summary length is reached. We experimented using 3 different values for \u03bb (0.3, 0.5 and 0.7) and 4 different weighting types: raw (counting of the term), binary (presence of the term), TF-IDF and \u201cdampened\u201d TF-IDF (same as TF-IDF but takes logarithm of TF instead of TF itself).\nThe damping factor used in LexRank was 0.85 and the convergence threshold was set to 0.0001. We also calculated the similarity between every sentence only once, applying the iterative algorithm and picking sentences until the desired summary length is reached. We also tested LexRank using the same weighting types as for MMR.\nWe used Armadillo\u2019s [21] implementation of the SVD operation to implement LSA. After sentence/word segmentation, we apply SVD to the term by sentences matrix (column-wise concatenation of all sentence vectors). We then take the rank-K approximation of the decomposition where the Kth singular value is not smaller than half of the (K \u2212 1)th singular value. Then, we calculate the sentence score (as explained in section II-D) for each sentence and pick sentences according to that ranking until the desired summary length is reached. We tested LSA with both raw and binary weighting.\nWe tested MMR, LexRank, and LSA, with all combinations of the following parameter values: frame size of 0.5s with no overlap and with 50% (0.25s hops) overlap; vocabulary size of 25, 50, and 100 words; and sentence size of 5, 10, and 20 words. We used MFCC vectors (of size 12) as features for these experiments, they are widely used in many MIR tasks including music summarization in [2], [5]\u2013[7]."}, {"heading": "IV. RESULTS", "text": "We present only the most interesting results, since we tried many different parameter combinations for each algorithm. The Frame/Hop Size columns indicate the frame/hop sizes in seconds, which can be interpreted as overlap (e.g., the pair 0.5, 0.25 stands for frames of 0.5s duration with a hop size of 0.25s, which corresponds to a 50% overlap between frames). The classification accuracy results for the 30s contiguous segments which constitute the baseline are 95.8%, 96.2%, and 94% for the beginning, middle, and end sections, respectively, on dataset 1 and 85.2%, 92%, and 90.4%, on dataset 2.\nThe Average Similarity algorithm was successful in improving classification performance on dataset 1 (98.8% as maximum accuracy obtained with frame size of 0.5 s, no overlap, 24 MFCCs), but not on dataset 2 (90.8% maximum accuracy with frame size of 0.25 s, no overlap, 12 MFCCs).\nIn table I, we can see that although not all parameter combinations for MMR yielded an increase in classification performance on both datasets, some combinations did do that. For example, the best\nJune 20, 2014 DRAFT\ncombination on the dataset 1 yielded 100% accuracy but on dataset 2 it yielded only 90.8% which is lower than the baseline (92%). However, all other parameter combination presented in those tables yield a better result than the baseline for both datasets. We also noticed that smaller values of \u03bb would result in worse accuracy scores.\nWe can also see that the best parameter combination for LexRank on dataset 1 was also the best on dataset 2. Besides that, all other presented combinations are better when compared to the corresponding baseline, which suggests that these parameter combinations might also be good for other datasets.\nOur experiments show that LSA works best with binary weighting when applied to music. This has to do with the fact that some musical sentences, namely, at the beginning of the songs, are strings with very few repeating terms, which increases term-frequency scores. Moreover, those terms might not even appear anywhere else in the song which will, in turn, decrease the document frequency of the term, thus increasing the inverse document frequency score. These issues are detected when LSA chooses those (unwanted) sentences because they will have a high score on a certain latent topic. The binary weighting alleviates these problems because we only check for the presence of a term (not its frequency) and the document frequency of that term is not taken into account. LSA also achieved results above the baseline (table I)."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "We evaluated summarization through classification for MMR, LexRank, and LSA in the music domain. More experimenting should be done to find a set of parameter combinations that will work for most music contexts. Future work includes testing other summarization algorithms, other similarity metrics, other types of features and other types of classifiers. The use of Gaussian Mixture Models may also help in finding more \u201cnatural\u201d vocabularies and Beat Detection might be used to find better values for fixed segmentation.\nJune 20, 2014 DRAFT"}], "references": [{"title": "Semantic Segmentation and Summarization of Music: Methods Based on Tonality and Recurrent Structure", "author": ["W. Chai"], "venue": "Signal Processing Magazine, IEEE, vol. 23, no. 2, pp. 124\u2013132, March 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Summarizing Popular Music via Structural Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Applications of Signal Processing to Audio and Acoustics, 2003 IEEE Workshop on., Oct 2003, pp. 127\u2013130.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward Automatic Music Audio Summary Generation from Signal Analysis", "author": ["G. Peeters", "A.L. Burthe", "X. Rodet"], "venue": "Proc. Intl. Conf. on Music Information Retrieval, 2002, pp. 94\u2013100.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Signal-based Music Structure Discovery for Music Audio Summary Generation", "author": ["G. Peeters", "X. Rodet"], "venue": "Proc. of the Intl. Computer Music Conf. (ICMC), 2003, pp. 15\u201322.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Music Summary Using Key Phrases", "author": ["S. Chu", "B. Logan"], "venue": "Hewlett-Packard Cambridge Research Laboratory, Cambridge MA 02139, Tech. Rep. CRL 2000/1, April 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic Music Summarization via Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Proc. Int. Conf. Music Information Retrieval, 2002, M. Fingerhut, Ed., 2002, pp. 81\u201385.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Automatic Music Summarization: A \u201dThumbnail\u201d Approach", "author": ["J. Glaczynski", "E. Lukasik"], "venue": "Archives of Acoustics, vol. 36, no. 2, pp. 297\u2013309, Jan. 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio Thumbnailing of Popular Music using Chroma-based Representations", "author": ["M.A. Bartsch", "G.H. Wakefield"], "venue": "IEEE Transactions on Multimedia, vol. 7, no. 1, pp. 96\u2013104, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic Fado Music Classification", "author": ["P.G. Antunes", "D.M. de Matos", "R. Ribeiro", "I. Trancoso"], "venue": "CoRR, 2014, arXiv:1406.4447.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research, vol. 22, no. 1, pp. 457\u2013479, Dec. 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "TextRank: Bringing Order into Texts", "author": ["R. Mihalcea", "P. Tarau"], "venue": "Proc. of EMNLP 2004. Barcelona, Spain: Association for Computational Linguistics, July 2004, pp. 404\u2013411.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine", "author": ["S. Brin", "L. Page"], "venue": "Seventh Intl. World-Wide Web Conf. (WWW 1998), 1998. June 20, 2014  DRAFT  SUBMITTED TO IEEE SIGNAL PROCESSING LETTERS, VOL. X, NO. X  12", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving Diversity in Ranking using Absorbing Random Walks", "author": ["X. Zhu", "A.B. Goldberg", "J.V. Gael", "D. Andrzejewski"], "venue": "Proc. of NAACL HLT, pp. 97\u2013104, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Minimizing word error rate in textual summaries of spoken language", "author": ["K. Zechner", "A. Waibel"], "venue": "Proc. of the 1st North American chapter of the Association for Computational Linguistics conference. Stroudsburg, PA, USA: Association for Computational Linguistics, 2000, pp. 186\u2013193.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Extractive Summarization of Meeting Recordings", "author": ["G. Murray", "S. Renals", "J. Carletta"], "venue": "Proc. of the 9th European Conf. on Speech Communication and Technology, 2005, pp. 593\u2013596.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proc. of the 24th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. New York, NY, USA: ACM, 2001, pp. 19\u201325.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "Proc. of the 21st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. New York, NY, USA: ACM, 1998, pp. 335\u2013336.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Using Latent Semantic Analysis in Text Summarization and Summary Evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proc. of ISIM 2004, 2004, pp. 93\u2013100.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Recent Developments in openSMILE, the Munich Open-source Multimedia Feature Extractor", "author": ["F. Eyben", "F. Weninger", "F. Gross", "B. Schuller"], "venue": "Proc. of the 21st ACM Intl. Conf. on Multimedia, 2013, pp. 835\u2013838.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Armadillo: An open source c++ linear algebra library for fast prototyping and computationally intensive experiments", "author": ["C. Sanderson"], "venue": "NICTA, Australia, Tech. Rep., October 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "MARSYAS: A Framework for Audio Analysis", "author": ["G. Tzanetakis", "P. Cook"], "venue": "Organised Sound, vol. 4, no. 3, pp. 169\u2013175, Dec. 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "MLPACK: A Scalable C++ Machine Learning Library", "author": ["R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray"], "venue": "Journal of Machine Learning Research, vol. 14, pp. 801\u2013805, 2013. June 20, 2014  DRAFT", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Several algorithms to summarize music have been published [1]\u2013[8], mainly for popular music songs whose structure is repetitive enough.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "Several algorithms to summarize music have been published [1]\u2013[8], mainly for popular music songs whose structure is repetitive enough.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "We evaluate the summarization\u2019s contribution by comparing the performance of a Portuguese music style Fado Genre Classifier [9] using the extracted summaries of the songs against using contiguous clips (truncated from the beginning, middle and end of the song).", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "LexRank [10] and TextRank [11] are centrality-based methods that rely on the similarity between every sentence.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "LexRank [10] and TextRank [11] are centrality-based methods that rely on the similarity between every sentence.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "These are based on Google\u2019s PageRank [12] algorithm for ranking web pages and are successfully applied in text summarization.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "GRASSHOPPER [13] is another method applied in text summarization, as well as social network analysis, focusing on improving diversity in ranking sentences.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "MMR [14], [15], applied in speech summarization, is a query-specific method that selects sentences", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "MMR [14], [15], applied in speech summarization, is a query-specific method that selects sentences", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "LSA [16] is another", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "[1] presents two approaches for segmentation: using a Hidden Markov Model (HMM) to detect key changes between frames and Dynamic Time Warping (DTW) to detect repeating structure.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In [2], segmentation is achieved by correlating a Gaussian-tempered \u201ccheckerboard\u201d kernel along the main diagonal of the similarity matrix of the song, outputting segment boundaries.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [3], [4], songs are segmented in 3 stages.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [3], [4], songs are segmented in 3 stages.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "[5] groups (based on the Kullback-Leibler (KL) divergence) and labels similar segments of the song and then the summary is generated by taking the longest sequence of segments belonging to the same cluster.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In [6], [7], a method called Average Similarity is used to extract a thumbnail L seconds long that is most similar to the whole piece.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [6], [7], a method called Average Similarity is used to extract a thumbnail L seconds long that is most similar to the whole piece.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "Another method for this task is the Maximum Filtered Correlation [8] which starts by building a similarity matrix and then a filtered time-lag matrix, which has the similarity between extended segments embedded in it.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "This method was introduced in [6] and later used in other research efforts such as [7].", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "This method was introduced in [6] and later used in other research efforts such as [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "In [6], 45 Mel Frequency Cepstral Coefficient (MFCC)s are computed but only the 15 with highest variance are kept.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], the first 13 MFCCs and the spectral centre of gravity (sound \u201cbrightness\u201d) are used.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The evaluations of this method in the literature are subjective (human) evaluations that take into account whether the generated summaries include the most memorable part(s) of the song [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 6, "context": "are averages of scores given by test subjects, regarding specific qualities of the summary such as Clarity, Conciseness and Coherence [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 16, "context": "MMR [17], selects sentences from the signal according to their relevance and to their diversity against the already selected sentences in order to output low-redundancy summaries.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "This approach has been used in speech summarization [14], [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "This approach has been used in speech summarization [14], [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "It is a query-specific summarization method, though it is possible to produce generic summaries by taking the centroid vector of all the sentences (as in [15]) as the query.", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "LexRank [10] is a centrality-based method that relies on the similarity for each sentence pair.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "This centrality-based method is based on Google\u2019s PageRank [12] algorithm for ranking web pages.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "LSA is based on the mathematical technique SVD that was first used for text summarization in [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "In [18], two limitations of this approach are discussed: the fact that K is equal to the number of sentences in the summary, which, as it increases, tends to include less significant sentences; and that sentences with high values in several dimensions (topics), but never the highest, will never be included in the summary.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "The classifier is a Support Vector Machine (SVM) [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "For feature extraction we used OpenSMILE\u2019s [20] implementation, namely, to extract MFCC feature vectors.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "We also used the Armadillo library [21] for matrix operations and the Marsyas library [22] for synthesizing the summaries.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "We also used the Armadillo library [21] for matrix operations and the Marsyas library [22] for synthesizing the summaries.", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "For each piece being summarized, we cluster all of its frames using the mlpack\u2019s [23] K-Means algorithm implementation which calculates the vocabulary for that song (i.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "We used Armadillo\u2019s [21] implementation of the SVD operation to implement LSA.", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "We used MFCC vectors (of size 12) as features for these experiments, they are widely used in many MIR tasks including music summarization in [2], [5]\u2013[7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "We used MFCC vectors (of size 12) as features for these experiments, they are widely used in many MIR tasks including music summarization in [2], [5]\u2013[7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "We used MFCC vectors (of size 12) as features for these experiments, they are widely used in many MIR tasks including music summarization in [2], [5]\u2013[7].", "startOffset": 150, "endOffset": 153}], "year": 2014, "abstractText": "Several generic summarization algorithms were developed in the past and successfully applied in fields such as text and speech summarization. In this paper, we review and apply these algorithms to music. To evaluate this summarization\u2019s performance, we adopt an extrinsic approach: we compare a Fado Genre Classifier\u2019s performance using truncated contiguous clips against the summaries extracted with those algorithms on 2 different datasets. We show that Maximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA) all improve classification performance in both datasets used for testing.", "creator": "LaTeX with hyperref package"}}}