{"id": "1409.2195", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2014", "title": "Analyzing the Language of Food on Social Media", "abstract": "we investigate the predictive relationship behind intelligent diffusion of food on social media. we collect a collected of over three million taste - related posts hoping twitter would demonstrate that many latent discourse characteristics can be directly predicted from this data : overweight rate, diabetes rate, political leaning, and home geographical location of authors. for all humans, our language - based models significantly outperform the majority - class baselines. performance is further challenged with introducing complex natural language processing, such as topic modeling. we analyze since textual features have residual predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics. lastly, we design and adopt an online system for real - level query and visualization of the dataset. visualization tools, denoted as geo - referenced heatmaps, semantics - preserving wordclouds over temporal histograms, allow us some see more complex, global patterns mirrored in the language of food.", "histories": [["v1", "Mon, 8 Sep 2014 03:07:54 GMT  (8527kb,D)", "https://arxiv.org/abs/1409.2195v1", null], ["v2", "Thu, 11 Sep 2014 17:35:02 GMT  (8519kb,D)", "http://arxiv.org/abs/1409.2195v2", "An extended abstract of this paper will appear in IEEE Big Data 2014"]], "reviews": [], "SUBJECTS": "cs.CL cs.CY cs.SI", "authors": ["daniel fried", "mihai surdeanu", "stephen kobourov", "melanie hingle", "dane bell"], "accepted": false, "id": "1409.2195"}, "pdf": {"name": "1409.2195.pdf", "metadata": {"source": "CRF", "title": "Analyzing the Language of Food on Social Media", "authors": ["Daniel Fried", "Mihai Surdeanu", "Stephen Kobourov", "Melanie Hingle", "Dane Bell"], "emails": ["dane}@email.arizona.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nOur diets reflect our identities. The food we eat is influenced by our lifestyles, habits, upbringing, cultural and family heritage. In addition to reflecting our current selves, our diets also shape who we will be, by impacting our health and wellbeing. The purpose of this work is to understand if information about individuals\u2019 diets, reflected in the language they use to describe their food, can convey latent information about a community, such as its location, likelihood of diabetes, and even political preferences. This information can be used for a variety of purposes, ranging from improving public health to better targeted marketing.\nIn this work we use Twitter as a source of language about food. The informal, colloquial nature of Twitter posts, as well as the ease of data access, make it possible to assemble a large corpus describing the type of food consumed and the context of the discussion. Over eight months, we collected such a corpus of meal-related tweets together with relevant meta data, such as geographic locations and time of posting. We construct a system for aggregating, annotating, and querying these tweets as a source for predictive models and interactive visualizations (Fig. 1). Building on this dataset and system, the contributions of this work are fourfold:\n1. We analyze the predictive power of the language of food by predicting several latent population characteristics from the tweets alone (after filtering out location-related words to avoid learning trivial correlations). We demonstrate that this data can be used to predict multiple characteristics, which are conceivably connected with food: a state\u2019s percentage of overweight population, the rate of diagnosed diabetes, and even political voting history. Our results indicate that the languagebased model yields statistically-significant improvements over the majority-class baseline in all configurations, and that more complex natural language processing (NLP), such as topic modeling, further improves results.\n2. We demonstrate that the same data accurately predicts geographic home locale of the authors (from city-level, through\nstate-level, to region-level), with our model significantly outperforming the random baseline (e.g., 30 times better on the state-level).\n3. In addition to examining the effectiveness of our models on these predictive tasks, we analyze which textual features have most predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics.\n4. Lastly, we show that visualizations of the language of food over geographical or temporal dimensions can be used to infer additional information such as the importance of various daily meals in different regions, the distribution of different foods and drinks over the course of days, weeks and seasons, as well as some migration patterns in the United States and worldwide."}, {"heading": "II. DATA", "text": "Twitter provides an accessible source of data with broad demographic penetration across ethnicities, genders, and income levels1, making it well-suited for examining the dietary habits of individuals on a large scale. To identify and collect tweets about food, we queried Twitter\u2019s public streaming API2\n1http://www.pewinternet.org/2014/01/08/social-media-update-2013/ twitter-users/\n2https://dev.twitter.com/docs/api/streaming. Note: Twitter caps the number of possible tweets returned by the streaming API to a fraction of the total number of tweets available at a given moment.\nar X\niv :1\n40 9.\n21 95\nv2 [\ncs .C\nL ]\n1 1\nSe p\n20 14\nfor posts containing hashtags related to meals (Table I). We collected approximately 3.5 million tweets containing at least one of these hashtags from the period between October 2, 2013 and May 29, 2014.\nTweets are very short texts, limited to 140 characters. In our collection, the average length of a tweet is 8.7 words, after filtering out usernames, non-alphanumeric characters (hashtags excepted), and punctuation. The tweet collection contains a total of about 30 million words. Of these, there are around 1.5 million unique words.\nFig. 1 describes the system used to collect, annotate, and process the tweets for prediction and visualization. Along with the text of each tweet, we store the user\u2019s self-reported location, time zone, and geotagging information, whenever these fields are available. This meta data is used to group tweets by the home location of the author, e.g., specified as city and/or state for those users located within the United States (US). For most experiments in this paper, geolocation normalization is performed using regular expressions, matching state names or postal abbreviations of one of the 50 US states or Washington, D.C. (e.g., Texas or TX), followed by matching city names or known abbreviations (e.g., New York City or NYC) within the author\u2019s location field. In case of ambiguities (e.g., LA stands for both Los Angeles and Louisiana) we used the user\u2019s time zone to disambiguate. About 16% (562,547) of the collected tweets could be located within a state using this method (Table I). We chose to use the self-reported user location instead of the geotagging information because: (a) it is more common, (b) it tends to have a standard, easily parseable form for US addresses, and (c) to avoid potential biases introduced by travel. However, in Section VI, we extend our analysis to discover world-wide food-related patterns. In this context, because world addresses are considerably harder to parse than US addresses, we revert to geotagging information to identify the location of tweet authors.\nUsing this dataset, we can immediately see food-driven patterns. For example, Fig. 2 shows prominent food-related words that appeared in the tweets normalized to each state. Tweet text is filtered using a list of approximately 800 foodrelated words (see Sec. IV-A). Terms are ranked using term frequency\u2013inverse document frequency (tf-idf) [10] to discount words that occur frequently across all states, and give priority to those words that are highly representative of a state. Each state\u2019s food word with the highest tf-idf ranking is displayed in the map. Regional trends can be seen, for example grits, a breakfast food made from ground corn, is a common dish in the\nsouthern states, and various types of seafood (halibut, caviar, cod, clam) are popular in the eastern and western coastal states."}, {"heading": "III. TASKS", "text": "To understand the predictive power of the language of food, we implement several prediction tasks that use the tweets in the above dataset as their only input. We group these tasks into two categories: state-level characteristic prediction and locale prediction."}, {"heading": "A. Predicting State-Level Characteristics", "text": "Here we predict three aggregate characteristics for US states, using features extracted from the tweets produced by individuals in each state:\n1) Diabetes Rate: This is the percentage of adults in each state who have been told by a doctor that they have diabetes. Data in this set is taken from the Kaiser Commission on Medicaid and the Uninsured (KCMU)\u2019s analysis of the Center for Disease Control\u2019s Behavioral Risk Factor Surveillance System (BRFSS) 2012 survey.3 We convert this data into a binary dependent variable by considering whether a state\u2019s rate of diabetes is above or below the national median. The median diabetes rate is 9.7%, and the range is 6.0% (7.0% in Alaska to 13.0% in West Virginia). For example, Alabama has a diabetes rate of 12.3%, which is above the national median of 9.7%, so it is labelled as high-diabetes, while Alaska, with a rate of 7.0%, is labelled as low-diabetes.\n2) Overweight Rate: This is the percentage of adults within each state who reported having a Body Mass Index (BMI) of at least 25.0 kilograms per meter squared, placing them within the \u201coverweight\u201d or \u201cobese\u201d categories defined by the National Institutes of Health.4 As with the diabetes rate dataset, data is taken from KCMU\u2019s analysis of the BRFSS 2012 survey results5. Similarly, the corresponding binary dependent variable indicates if a state\u2019s overweight rate is above/below the national median. The median overweight rate is 64.2%,\n3The BRFSS is a random-digit-dialed telephone survey of adults age 18 and over. For more details see: http://kff.org/other/state-indicator/ adults-with-diabetes/\n4http://www.nhlbi.nih.gov/guidelines/obesity/BMI/bmi-m.htm 5http://kff.org/other/state-indicator/adult-overweightobesity-rate/\nand the range is 17.7% (51.9% in Washington, D.C. to 69.6% in Louisiana).\n3) Political Tendency: This dataset measures historical voting history over a 5-year period: whether a state is more Democratic or Republican relative to the median US state, as measured by proportion of Democratic/Republican votes in general presidential, gubernatorial, and senatorial elections, in the interval from 2008 to 2013.6. For example, Alaska cast 554,565 total votes for Democratic candidates and 748,488 for Republican candidates in these three types of elections during the six-year period, for a fraction of 42.6% Democratic votes. This is below the median fraction of 51.6%, so Alaska is labelled as Republican. Votes are compared relative to the median because of a slight bias toward Democratic votes during this time period. The median fraction of Democratic votes is 51.6%, and the range is 65.4% (27.0% in Wyoming to 92.4% in Washington D.C.).\nBecause the above dependent variables are at state level, each state is treated as a single instance for these three tasks: all of the tweets produced within the state are aggregated into a single pool for feature extraction (detailed in the next section). We used Support Vector Machines (SVM) with a linear kernel [22] for classification.\nAlthough such a prediction task has many features (from all tweets in a given state), it has a small number of data points (51, one for each state plus Washington, D.C.). For this reason, we use leave-one-out cross-validation to evaluate the accuracy of the model. For each of the three data sets (overweight, diabetes, and political), we use the following process: Each state is held out in turn. The SVM is trained on features of tweets taken from the remaining 50 states, using the labels of the current data set. The SVM is then used to predict the current dataset\u2019s label of the held-out state. The accuracy of the model on the label set is calculated as the number of correct predictions out of the total number of states. To avoid overfitting, we do not tune the classifier\u2019s hyper-parameters."}, {"heading": "B. Predicting Locales", "text": "To examine the connection between the language of food and geographic location, we seek to predict the locale of a group of tweets, using only the text of the tweets as input. We predict locales at different levels: city, state, and region. It is important to note that, to focus our analysis on the predictive power of the language of food, we remove as many state and city names as possible from the tweets to avoid learning trivial correlations (see Sec. IV-A).\n1) City: The locales in the city prediction task are the 15 most populous cities in the US.7\n2) State: Locales in the state prediction task are the 50 US states, plus Washington, D.C. As discussed in the previous section, both city and state labels are assigned to tweets by parsing the self-reported author home location in the meta data.\n3) Region: The final variant of the locale prediction task is to predict the geographic region of the US containing the user\u2019s state. We use four geographic regions, taken from the US Census Bureau: Midwest, West, Northeast, and South.8\nSimilar to the previous tasks, here we also aggregate tweets with the same locale for feature extraction, and use a linear\n6http://uselectionatlas.org/ 7http://en.wikipedia.org/wiki/List of United States cities by population 8http://www.census.gov/geo/maps-data/maps/pdfs/reference/us regdiv.pdf\nkernel SVM for classification. However, the overall setup is different. Because the goal here is to predict locale itself, we divide the tweets from each locale into training and testing sets consisting of 80% and 20% of the tweets available for that locale, respectively. Tweets are sorted chronologically so that all training tweets for a given locale were posted before all the corresponding testing tweets. The overall training/testing corpora include training/testing tweets from all locales. Using this dataset, we train a one-against-many multi-class SVM classifier for the locale as the dependent variable. For example, for the city detection task, the SVM classifies a group of tweets as belonging to one of the 15 known cities.\nThe accuracy on each locale prediction task is the number of locales correctly identified, divided by the total number of locales. Thus, a baseline classifier that randomly predicts a locale would achieve an accuracy of 1/51 = 1.96% on the state prediction task, 1/15 = 7% on the city task, and 1/4 = 25% for region prediction."}, {"heading": "IV. FEATURE DESCRIPTIONS", "text": "We use two sets of features: lexical (from tweet words) and topical (sets of words appearing in similar contexts)."}, {"heading": "A. Lexical Features", "text": "We take the simple approach of representing each locale as a bag of words assembled from all the tweets in that group. Each word becomes a feature with value equal to the number of times it occurrs across all tweets for that locale. We tokenize the tweets using the Stanford CoreNLP software.9 An additional pre-processing step removes the following tokens: (a) tokens that do not contain alpha-numeric characters or punctuation (to reduce noise); (b) stopwords and words that occur a single time (to reduce data size); and, most importantly, (c) URLs, usernames (preceded by an @ symbol), and words and hashtags naming state and city locations10 (to avoid learning trivial correlations, such as #TX indicating a tweet from Texas).\nWe also experiment with open versus closed vocabularies. For open vocabularies, we use two configurations: all words produced by the above pre-processing step, or only hashtags. For a closed vocabulary experiment, we use a set of 809 words related to food, meals, and eating, obtained from the English portion of a Spanish-English food glossary11 and an online food vocabulary list12. These experiments will help us understand how much predictive power is contained in food words alone versus the full text (or hashtags) of the tweets, which capture a much broader context."}, {"heading": "B. Topic Model Features", "text": "Topic models provide a method to infer the themes present in tweets, represented as clusters of words that tend to appear in similar contexts (e.g., a topic learned by the model, which we refer to as the American diet topic, contains chicken, baked, beans, and fried, among other terms). Using topics as features is beneficial for a couple of reasons: (1) topics provide a\n9http://nlp.stanford.edu/software/corenlp.shtml 10In addition of known state names and abbreviations we used a list of the 250 most populous cities in the US from http://en.wikipedia.org/wiki/List of United States cities by population, together with common nicknames, such as \u201c#sanfran\u201d for San Francisco.\n11http://www.lingolex.com/spanishfood/a-b.htm 12http://www.enchantedlearning.com/wordlist/food.shtml\nmethod to address the sparsity resulting from having very short documents (tweets are limited to 140 characters) by treating groups of related words as a single feature; (2) topical features aid in post-hoc analysis by allowing us to detect correlations that go beyond individual words.\nWe use Latent Dirichlet Allocation (LDA) [3] to learn a set of topics from the food tweets in an unsupervised fashion. LDA treats each tweet as a mixture of latent topics. Each topic is itself a probability distribution over words, and the words in the tweet are viewed as being sampled from this mixture of distributions. The LDA model (topic distributions and mixtures) is trained from all available tweets in the corpus, using the MALLET software package.13 We chose 200 as the number of topics for the model to learn. This number produced topics that seemed fine-grained enough to capture specific patterns in diet, language, or lifestyle \u2013 clusters of foods of various nationalities, or specific diets such as vegetarian. For clarity in our analysis, we have manually assigned subject labels, such as American diet, to some of these topics based on the words contained in the topic.14 We use these assigned labels to refer to the topics in the remainder of this paper. Once the LDA topic model is trained, we use it to infer the mixture of topics for each tweet in the prediction tasks. The topic most strongly associated with the tweet (the topic with highest probability given the model and the tweet) is used as an additional feature for the tweet, similarly to the lexical features generated from the words of the tweet. Topics are counted across all tweets in a state in the same manner as the lexical features.\nWhen applied in combination with the configuration containing solely food word or hashtag vocabularies, the LDA topics are constructed using the corresponding filtered versions of the tweets, i.e., with all non-food words or non-hashtag words removed.\nTo account for the large differences in the number of tweets available for each state (for example, the state with the most normalized tweets, New York has 83,670 tweets, while the state with the fewest, Wyoming, has 339), we scale all the features collected for each state. Each feature\u2019s value within a state\u2019s feature set is divided by the number of tweets collected for the state."}, {"heading": "V. RESULTS", "text": "We present empirical results for both categories of tasks introduced in the previous section: predicting state-level characteristics and predicting locales. We also analyze the effectiveness of the language of food for these prediction tasks by examining the most important textual features in the classification models, and investigating the importance of open versus closed vocabularies."}, {"heading": "A. State-Level Characteristics", "text": "Table II shows classification results on the state-level statistics prediction task (Section III-A) for varying feature sets. Since all three datasets are nearly evenly split between the binary classes (each dataset has either 25 or 26 states out of 51 in each of the two classes), a baseline that predicts the majority label achieves approximately 51% accuracy. We compare the performance of the tweet-based predictive models to this majority baseline, and evaluate how filtering the lexical content\n13http://mallet.cs.umass.edu/ 14But these topic labels are not visible to the classifier.\nof the tweets and adding topical features affects accuracy on these prediction tasks. We draw several observations from this experiment:\n(a) First and foremost, the language of food can indeed infer all the latent characteristics investigated: all configurations investigated statistically outperform the majority-class baseline. The best performance is obtained when the entire text of the tweets is used (All Words), which captures not only direct references to food, but also the context in which it is discussed. However, the performance of the closed vocabulary of food words (Food) is within 5% of the best performance, demonstrating that most of the predictive signal is captured by direct references to food.\n(b) The classifiers achieve the highest accuracy on the overweight dataset. This is an intuitive result, which confirms that there is a strong correlation between food and likelihood of obesity. However, the fact that this correlation can be detected solely from social media posts is, to our knowledge, novel and suggests potential avenues for better and personalized public health. A similar correlation with political preferences is also interesting, indicating potential marketing applications in the political domain.\n(c) More complex NLP (topic modeling in our case) is beneficial: the performance of the models that include LDA topics is, on average, better than that of the configurations without topics.15 We plan to use more informative representations of text, e.g., based on deep learning [21], in future work.\nTable III shows the words and topical features assigned the greatest importance, i.e., largest magnitude weights, by the SVM training process, for each dataset and class. It is interesting to note that a dietary topic we have labeled as American Diet, containing terms such as chicken, baked, beans and fried, is an important feature for predicting both\n15The improvement is not statistically significant for most experiments, but this can be attributed to the small size of the dataset (51 data points).\nthat a state has higher rates of overweight and diabetes than normal, whereas other diets, such as #vegan and Paleo Diet are important predictors for the opposite. Note that pronouns have high weights in the overweight prediction task: the firstperson singular I and my are valuable for predicting that a state is overweight, while collective words such as the You, We topic cluster are valuable for predicting that a state is below the median. This is less surprising in view of prior work, such as Ranganath et al. [17], showing that the types of pronouns used by an individual are associated with a host of traits such as gender and intention. For the political affiliation task, we observe that features correlated with Republican states include those centered around work (the Airport topic) and home (the After Work topic, including words such as home, after, work). The most predictive feature for Democratic states is #vegan, and we also see topics associated with urban life and eating out, such as Deli, #brunch, promotions such as Restaurant\nAdvertising, and Eating Out."}, {"heading": "B. City Prediction", "text": "For the first locale prediction task we focus on city identification. Table IV shows the accuracies of the various feature sets for this task. The input for this task is 15 cities, so the random-prediction baseline accuracy is 6.67%. As in the previous task, every set of features improves significantly upon this baseline, ranging from 40% accuracy using only Food words to 86.67% accuracy using Food words, Hashtags, and LDA topics, demonstrating once again the predictive power of the language of food. The significant improvement of the closed food vocabulary alone (Food) over the baseline indicates that the diets in each of these 15 cities are distinct enough to have some predictive power. However, diets alone are not enough to completely identify the cities, and we see that for this task more context is beneficial: adding hashtags helps considerably (53.33% accuracy), and adding topical features to the food and hashtag filtered set of lexical features improves performance even further (86.67%).\nTable V, which measures performance as the size of training/testing sets varies, indicates that accuracy is greatly affected by the size of the data available for both training and testing. Indeed, when only 20% of the training set is used, the models achieve the same score as the baseline classifier (6.67%). Performance continues to increase as we add more data, suggesting that we have not reached a performance ceiling yet.\nTable VI lists the top five features for each city in this task. The table shows that variations in diet are clear: tacos are significant in Austin, #vegetarian food is indicative of San Francisco, #brunch is representative of New York, etc. Using the context around food is clearly important. We see that several cities in California are associated with #foodie (Los\nAngeles and San Francisco) or eating while on Vacation (San Diego and San Francisco). First-person pronouns are highly weighted in cities in Texas (we in Austin, I in Houston, and my and I in San Antonio)."}, {"heading": "C. State Prediction", "text": "Table VII lists the results for the state prediction task. There are 51 possible locales in this task, from the 50 US states plus Washington D.C., so the random-prediction baseline achieves 1.96% accuracy. As in all previous experiments, the stateprediction model improves significantly upon this baseline with every set of features that we tried. The model achieves its lowest accuracy, 33.33%, using the set of food words without topical features (Food). Unlike in the city prediction task but similar to the state-level characteristic prediction task, the model is most accurate when using the unfiltered tweets\nwith topical features (All Words + LDA), reaching 66.67% accuracy. This indicates that the closed food vocabulary is not sufficient for optimal performance on this task, and the larger food-related context is required for optimal performance.\nTable VIII analyzes the effect of the number of available tweets on prediction accuracy. Performance varies from 11.76% when using 20% of tweets in the training set and 20% in the testing set, to 64.7% when using all available tweets. Increasing the number of tweets in the training set has a larger positive effect on accuracy than increasing the number of tweets in the testing set. As in the city prediction task, performance continues to increase as we add more data, suggesting that the performance ceiling has not been reached.\nDue to space consideration, we include the top features per state and the corresponding discussion in the supplemental material.16"}, {"heading": "D. Region Prediction", "text": "The final locale prediction task predicts the four major US geographic regions: Midwest, Northeast, South, and West (Section III-B3) using tweets from each region. The high level of geographic granularity (each region contains about a dozen states, on average) simplifies the tweet-based task in one sense, since there are now fewer possible classification labels, but also makes the task more difficult because of the variation in diet and tweet lexical content within these broad geographic regions. The random-prediction baseline in this task achieves 25% accuracy. Three of the feature sets, however, achieve 75% accuracy, only misclassifying a single region.17 We also see that for all of the feature sets except the closed food vocabulary (Food), lexical features give one more correct region classification over the baseline, and adding\n16https://sites.google.com/site/twitter4food/ 17Since there are only four data points in the testing set, measuring if improvements over the baseline are statistically significant cannot be reliable, so it is skipped here.\ntopical features yields an additional correct classification. The feature set consisting of all words and the LDA topics classifies three out of four regions correctly, only misidentifying testing tweets from the Midwest as being from the Northeast.\nTable X shows the most predictive features for each region. We see a clear preference for certain meal term hashtags in the table: #breakfast is a strong predictor for the Midwest, #brunch for the Northeast, #lunch for the South, and #dinner for the West; see Fig. 3. Brunch and mixed drinks are important features in the Northeast, likely because they were also highly weighted features for New York City, and New York City produced many of the tweets in this region. The West is a mixture of features that were important for California, such as #foodporn and #vegan, and the diet of other states in the region, such as the Mexican food topical feature. The Northeast and South are similar in that both have the Mixed Drink topic and a meat-related topic, but differ in other features, such as the Northeast\u2019s #brunch and Group Dining, and the South\u2019s #lunch and After Work topic.\nVI. VISUALIZATION TOOLS\nWhile the machine learning models described above are well-suited for prediction on predefined tasks, we also constructed several visualization tools to discover previously unknown trends in the Twitter dataset. These tools aim to\nallow aggregate analysis of tweet content in the context of geographic and temporal location."}, {"heading": "A. Top Terms by State", "text": "The first of these tools, the term visualizer (Fig. 2), does a simple keyword analysis of the tweets available for each US state. We extract all terms that are contained within a list of around 800 food-related words (see Sec. IV-A) and rank them using tf-idf, treating all tweets normalized to a given state as a single document: each term\u2019s score is the number of times it occurred within a state, multiplied by the logarithm of the inverse proportion of the number of states it occurred in [10]. Ranking by tf-idf emphasizes words that are common in a particular state, but ensures that words used frequently in all states, such as food and eat, are not highly ranked. The term(s) with the highest ranking in each state are displayed on the state in the map. As discussed previously, this tool immediately highlights dietary patterns: grits in the Southern states, etc."}, {"heading": "B. Temporal Histograms", "text": "Temporal histograms allow us to visualize the changing popularity of terms over the course of a day, week, or year. About 71% of the collected tweets (2,503,351) are from users who have listed their time zone. For these tweets, we compute the time local to the user when the tweet was posted. The temporal visualization tool (Fig. 4) allows querying these time-localized tweets by phrase and constructing histograms at varying time granularities: hour of day, day of the week, or month of the year. On the weekly scale, it is easy to see that while breakfast is more or less uniform all week long, brunch occurs much more frequently on weekends, particularly Sunday. On the daily scale, wine peaks around 8pm, while beer follows a bi-modal distribution, with two roughly equal-sized peaks around 1pm and 8pm."}, {"heading": "C. Tweet Location Maps", "text": "About 10% of the collected tweets (362,978) have associated geolocation information \u2013 the user\u2019s longitude and\nlatitude at the moment the tweet was posted. We use this meta-information to build a system for querying and plotting worldwide geographic maps of tweets. The interface allows searching by phrase or LDA topic and displays geographic plots or heatmaps showing the locations of all tweets matching the query.\nThis system allows the discovery of broad geographic trends in the data. For example, Fig. 5 shows heatmaps made from queries for several LDA topics for foods of various geographic origins. These topics are perhaps reflective of immigration patterns to the US or worldwide, e.g., the Italian food topic has high intensity in Italy and New York City (Fig. 5a) and the Vietnamese food topic has high intensity in Vietnam and in Southern California (Fig. 5b). The Full Breakfast topic, reflecting a traditional British and American breakfast of bacon, eggs, and sausage, is pronounced throughout all of English-speaking United Kingdom and the heavily populated regions of both the western and eastern United States (Fig. 5c). Similarly, Fig. 6 shows the prevalence of Spanish and Latin-American influenced food throughout the Spanishspeaking world, including portions of the United States and the Philippines."}, {"heading": "D. Parallel Word Clouds", "text": "Word clouds offer a space-efficient way to summarize text by highlighting important words. Semantics-preserving word clouds also add the feature that related words (e.g., those that frequently co-occur) are placed close to each other [2]. Our parallel word clouds further focus on comparing and contrasting two or more groups of texts: words are scaled by importance, related words are close to each other, and important words that occur in both groups are in the same locations in all clouds (citation hidden for review). Fig. 7 shows such parallel word clouds for weekday vs. weekend tweets, highlighting a different set of trends: family, brunch, and even breakfast are more prominent on weekends; work and tonight are common on weekdays (in red); and, finally, restaurant, and fun are present on weekends (in blue)."}, {"heading": "VII. RELATED WORK", "text": "Previous work has used textual analysis of Twitter posts to study diverse and global populations, including investigating temporal changes in mood [7] and correlations between religious expression and sentiment [19]. Several other works predict latent characteristics of individuals and communities\nusing social media posts and metadata. Rao et al. [18] predict gender, age, regional origin, and political orientation for individual Twitter users, using tweets and a set of hand-constructed linguistic features. Burger et al. [4] and Bamman et al. [1] predict users\u2019 gender using their tweets and additional meta information, such as name, self description, and their social network. Jurafsky et al. [9] analyze a corpus of restaurant reviews and predict restaurant ratings using linguistic features such as sentiment, narrative, and self-portrayal.\nPaul and Dredze [16] apply the Ailment Topic Aspect Model to 1.5 million health-related tweets and discover mentions of over a dozen ailments, including allergies and insomnia. Schwartz et al. [20] use Twitter to predict public health and well-being statistics on a state-wide level. The language used in 82 million geo-located tweets from 1,300 different US counties is used to predict the subjective well-being of people living in those counties. As in our study, LDA topics improved accuracy. Hingle et al. [8] use Twitter together with analytical software to capture real-time food consumption and diet-related behavior. While this study identifies relationships between dietary and behavioral patterns the results were based on a small dataset (50 participants and 773 tweets). Nascimento et al. [13] evaluate self-reported migraine headache suffering using over 20,000 migrane-related tweets over a seven-day period, finding different peaking hours on weekdays and weekends. Yom-Tov et al. [23] show how Twitter can be used to discover possible outbreaks of communicable diseases at large public gatherings. Mysl\u0131\u0301n et al. [12] use machine classification of tobacco-related Twitter posts to detect tobacco-relevant posts and sentiment towards tobacco products.\nPrevious work has modelled linguistic variation on Twitter in terms of demographic and geographic variables. O\u2019Connor et al. [15] create a generative model of word use from demographic traits, and show clusters of Twitter users with common lexicons. Eisenstein et al. [5], [6] show that despite the global diffusion of social media, geographic regions have distinct word and topic use on Twitter.\nPrevious work has also developed systems for aggregating, processing, and visualizing tweets. McCreadie et al. [11] develop a system for detecting newsworthy events and clustering tweets in real-time. Nguyen et al. [14] produce geographic visualizations of tweet sentiment using machine learning classifiers and the tweets\u2019 location metadata.\nOur work builds upon these previous results, and is, to our knowledge, the first to provide a large-scale, empirical analysis of the predictive power of the language of food."}, {"heading": "VIII. CONCLUSION AND FUTURE WORK", "text": "This work empirically demonstrates that food and food discussion are a major part of who we are. We develop a system for collecting a large corpus of food-related tweets and using these tweets to predict many latent population characteristics: overweight and diabetes rates, political learning, and geographic location of authors. Furthermore, we integrate several visualization tools that summarize and query this data, allowing us to discover more complex geographical/temporal trends that are driven by the language of food, such as potential migration patterns. Our analysis indicates that the language of food alone is extremely powerful. For example, on most predictive tasks, a closed vocabulary of only 800 food words approaches the peak performance obtained when using the entire tweets. Perhaps most importantly, our analysis of the learned predictive models provides big-data-driven insights\ninto connections between the language of food and the investigated population characteristics.\nWe note that our choice of populations (e.g., cities, states) for these tasks is purely practical (driven by the size of Twitter data at this granularity, and availability of dependent variables for the predictive tasks) and not a limitation of the proposed approach. In the future we would like to use our system to predict characteristics of individuals (e.g., propensity for diabetes), using the individuals\u2019 food information. Given sufficient amounts of available data, this can lead to non-trivial public health applications and, in a commercial and/or political space, to improved targeted marketing.\nThis paper is accompanied by a supplemental website, https://sites.google.com/site/twitter4food/, which includes a live version of all visualization tools presented."}], "references": [{"title": "Gender identity and lexical variation in social media", "author": ["D. Bamman", "J. Eisenstein", "T. Schnoebelen"], "venue": "Journal of Sociolinguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Experimental comparison of semantic word clouds", "author": ["L. Barth", "S.G. Kobourov", "S. Pupyrev"], "venue": "In SEA,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Discriminating gender on twitter. In EMNLP, pages 1301\u20131309", "author": ["J.D. Burger", "J. Henderson", "G. Kim", "G. Zarrella"], "venue": "Association for Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A latent variable model for geographic lexical variation", "author": ["J. Eisenstein", "B. O\u2019Connor", "N.A. Smith", "E.P. Xing"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Mapping the geographical diffusion of new words", "author": ["J. Eisenstein", "B. O\u2019Connor", "N.A. Smith", "E.P. Xing"], "venue": "arXiv preprint arXiv:1210.5268,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures", "author": ["S.A. Golder", "M.W. Macy"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Collection and visualization of dietary behavior and reasons for eating using a popular and free social media software application", "author": ["M. Hingle", "D. Yoon", "J.F.S.G. Kobourov", "M. Schneider", "D. Falk", "R. Burd"], "venue": "Journal of Medical Internet Research (JMIR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Narrative framing of consumer sentiment in online restaurant reviews", "author": ["D. Jurafsky", "V. Chahuneau", "B. Routledge", "N. Smith"], "venue": "First Monday,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Scalable distributed event detection for twitter", "author": ["R. McCreadie", "C. Macdonald", "I. Ounis", "M. Osborne", "S. Petrovic"], "venue": "In Int. Conf. on Big Data,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Using twitter to examine smoking behavior and perceptions of emerging tobacco products", "author": ["M. Mysl\u0131\u0301n", "S.-H. Zhu", "W. Chapman", "M. Conway"], "venue": "J Med Internet Res,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Real-time sharing and expression of migraine headache suffering on Twitter: A cross-sectional infodemiology study", "author": ["D.T. Nascimento", "F.M. DosSantos", "T. Danciu", "M. DeBoer", "H. van Holsbeeck", "R.S. Lucas", "C. Aiello", "L. Khatib", "A.M. Bender", "J.-K. Zubieta", "F.A. DaSilva"], "venue": "J Med Internet Res,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The royal birth of 2013: Analysing and visualising public sentiment in the uk using twitter", "author": ["V.D. Nguyen", "B. Varghese", "A. Barker"], "venue": "In Int. Conf. on Big Data,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "A mixture model of demographic lexical variation", "author": ["B. OConnor", "J. Eisenstein", "E.P. Xing", "N.A. Smith"], "venue": "In Proc. of NIPS workshop on machine learning in computational social science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "You are what you tweet: Analyzing Twitter for public health", "author": ["M.J. Paul", "M. Dredze"], "venue": "In ICWSM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "It\u2019s not you, it\u2019s me: Detecting flirting and its misperception in speed-dates", "author": ["R. Ranganath", "D. Jurafsky", "D. McFarland"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Classifying latent user attributes in Twitter", "author": ["D. Rao", "D. Yarowsky", "A. Shreevats", "M. Gupta"], "venue": "In 2nd Intl. Workshop on Search and mining user-generated contents,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Happy tweets: Christians are happier, more socially connected, and less analytical than atheists on Twitter", "author": ["R.S. Ritter", "J.L. Preston", "I. Hernandez"], "venue": "Social Psychological and Personality Science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Characterizing geographic variation in well-being using tweets", "author": ["H. Schwartz", "J. Eichstaedt", "M. Kern", "L. Dziurzynski", "M. Agrawal", "G. Park", "S. Lakshmikanth", "S. Jha", "M. Seligman", "L. Ungar"], "venue": "In 7th Intl. AAAI ICWSM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C.D. Manning", "A. Ng", "C. Potts"], "venue": "In EMNLP. Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Detecting disease outbreaks in mass gatherings using internet data", "author": ["E. Yom-Tov", "D. Borsa", "J.I. Cox", "A.R. McKendry"], "venue": "J Med Internet Res,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Terms are ranked using term frequency\u2013inverse document frequency (tf-idf) [10] to discount words that occur frequently across all states, and give priority to those words that are highly representative of a state.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "We used Support Vector Machines (SVM) with a linear kernel [22] for classification.", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "We use Latent Dirichlet Allocation (LDA) [3] to learn a set of topics from the food tweets in an unsupervised fashion.", "startOffset": 41, "endOffset": 44}, {"referenceID": 20, "context": ", based on deep learning [21], in future work.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "[17], showing that the types of pronouns used by an individual are associated with a host of traits such as gender and intention.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "IV-A) and rank them using tf-idf, treating all tweets normalized to a given state as a single document: each term\u2019s score is the number of times it occurred within a state, multiplied by the logarithm of the inverse proportion of the number of states it occurred in [10].", "startOffset": 266, "endOffset": 270}, {"referenceID": 1, "context": ", those that frequently co-occur) are placed close to each other [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "Previous work has used textual analysis of Twitter posts to study diverse and global populations, including investigating temporal changes in mood [7] and correlations between religious expression and sentiment [19].", "startOffset": 147, "endOffset": 150}, {"referenceID": 18, "context": "Previous work has used textual analysis of Twitter posts to study diverse and global populations, including investigating temporal changes in mood [7] and correlations between religious expression and sentiment [19].", "startOffset": 211, "endOffset": 215}, {"referenceID": 17, "context": "[18] predict gender, age, regional origin, and political orientation for individual Twitter users, using tweets and a set of hand-constructed linguistic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] and Bamman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] predict users\u2019 gender using their tweets and additional meta information, such as name, self description, and their social network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] analyze a corpus of restaurant reviews and predict restaurant ratings using linguistic features such as sentiment, narrative, and self-portrayal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Paul and Dredze [16] apply the Ailment Topic Aspect Model to 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "[20] use Twitter to predict public health and well-being statistics on a state-wide level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] use Twitter together with analytical software to capture real-time food consumption and diet-related behavior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] evaluate self-reported migraine headache suffering using over 20,000 migrane-related tweets over a seven-day period, finding different peaking hours on weekdays and weekends.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] show how Twitter can be used to discover possible outbreaks of communicable diseases at large public gatherings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] use machine classification of tobacco-related Twitter posts to detect tobacco-relevant posts and sentiment towards tobacco products.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] create a generative model of word use from demographic traits, and show clusters of Twitter users with common lexicons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5], [6] show that despite the global diffusion of social media, geographic regions have distinct word and topic use on Twitter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], [6] show that despite the global diffusion of social media, geographic regions have distinct word and topic use on Twitter.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "[11] develop a system for detecting newsworthy events and clustering tweets in real-time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] produce geographic visualizations of tweet sentiment using machine learning classifiers and the tweets\u2019 location metadata.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "We investigate the predictive power behind the language of food on social media. We collect a corpus of over three million food-related posts from Twitter and demonstrate that many latent population characteristics can be directly predicted from this data: overweight rate, diabetes rate, political leaning, and home geographical location of authors. For all tasks, our language-based models significantly outperform the majorityclass baselines. Performance is further improved with more complex natural language processing, such as topic modeling. We analyze which textual features have most predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics. Lastly, we design and implement an online system for real-time query and visualization of the dataset. Visualization tools, such as geo-referenced heatmaps, semantics-preserving wordclouds and temporal histograms, allow us to discover more complex, global patterns mirrored in the language of food.", "creator": "LaTeX with hyperref package"}}}