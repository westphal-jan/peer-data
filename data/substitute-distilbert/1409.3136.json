{"id": "1409.3136", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2014", "title": "Metric Learning for Temporal Sequence Alignment", "abstract": "in this paper, we propose to learn a regression notation to perform alignment of multivariate time series. the learning examples for this task are time series for which the true alignment is known. we model mathematical alignment problem as such structured prediction task, and propose realistic losses between alignments for which the accuracy is tractable. we provide experiments implementing real data in the audio quality audio context, whilst scientists show that the learning of a similarity measure leads to improvements in the performance of the alignment task. we also propose to use explicit metric learning framework to perform feature selection and, from basic modeling features, build a combination of these with better performance for the implementation.", "histories": [["v1", "Wed, 10 Sep 2014 16:10:33 GMT  (465kb,D)", "http://arxiv.org/abs/1409.3136v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\u00e9mi lajugie", "damien garreau", "francis r bach", "sylvain arlot"], "accepted": true, "id": "1409.3136"}, "pdf": {"name": "1409.3136.pdf", "metadata": {"source": "CRF", "title": "Metric Learning for Temporal Sequence Alignment", "authors": ["Damien Garreau", "R\u00e9mi Lajugie", "Sylvain Arlot"], "emails": ["damien.garreau@ens.fr1,2,", "remi.lajugie@inria.fr1,2,", "sylvain.arlot@ens.fr1,2,", "francis.bach@inria.fr1,2"], "sections": [{"heading": "1 Introduction", "text": "The problem of aligning temporal sequences is ubiquitous in applications ranging from bioinformatics [6, 1, 23] to audio processing [5, 7]. The idea is to align two similar time series that have the same global structure but local temporal differences. Most alignments algorithms rely on similarity measures, and having a good metric is crucial, especially in the high-dimensional setting where some features of the signals can be\n\u2217Both authors contributed equally.\nar X\niv :1\n40 9.\n31 36\nirrelevant to the alignment task. The goal of this paper is to show how to learn this similarity measure from annotated examples in order to improve the precision of the alignments. For example, in the context of music information retrieval, alignment is used in two different cases: (1) audio-to-audio alignment and (2) audio-to-score alignment. In the first case, the goal is to match two audio interpretations of the same piece that are potentially different in rythm, whereas audio-to-score alignment focuses on matching an audio signal to a symbolic representation of the score. In the second case, there are some attempts to learn from annotated data a measure for performing the alignment. Joder et al. [13] propose to fit a generative model in that context, and Keshet et al. [14] learn this measure in a discriminative setting. Similarly to Keshet et al. [14], we use a discriminative loss to learn the measure, but our work focuses on audio-to-audio alignment. In that context, the set of authorized alignments is much larger, and we explicitly cast the problem as a structured prediction task, that we solve using off-the-shelf stochastic optimization techniques [16] but with proper and significant adjustements, in particular in terms of losses. The need for metric learning goes far beyond unsupervised partitioning problems. Weinberger and Saul [26] proposed a large-margin framework for learning a metric in nearest-neighbour algorithms based on sets of must-link/must-not-link constraints. Lajugie et al. [17] proposed to use a large margin framework to learn a Mahalanobis metric in the context of partitioning problems. Since structured SVM have been proposed by Tsochantaridis et al. [25], Taskar et al. [22], they have successfully been used to solve many learning problems, for instance to learn weights for graph matching [4] or a metric for ranking tasks [18]. They have also been used to learn graph structures using graph cuts [21].\nContributions. We make the following five contributions: (1) we cast the learning of a Mahalanobis metric in the context of alignment as a structured prediction problem, (2) we show that on real musical datasets this metric improves the performance of alignment algorithms using high-level features, (3) we propose to use the metric learning framework to learn combinations of basic audio features and get good alignment performances, (4) we show experimentally that the standard Hamming loss, although tractable computationnally does not permit to learn a relevant similarity measure in some real world settings, (5) we propose a new loss, closer to the true evaluation loss for alignments, leading to a tractable learning task, and derive an efficient Frank-Wolfe based algorithm to deal with this new loss."}, {"heading": "2 Matricial formulation of alignment problems", "text": ""}, {"heading": "2.1 Notations", "text": "In this paper, we consider the alignment problem between two multivariate time series sharing the same dimension p, but possibly of different lengths TA and TB , namely A \u2208 RTA\u00d7p and B \u2208 RTB\u00d7p. We refer to the rows of A as a1, . . . , aTA \u2208 Rp and\nthose of B as b1, . . . , bTB \u2208 Rp. From now on, we denote by X the pair of signals (A,B). Let C(X) \u2208 RTA\u00d7TB be an arbitrary pairwise affinity matrix associated to the pair X , that is, C(X)i,j encodes the affinity between ai and bj . Note our framework can be extended to the case where A and B are multivariate signals of different dimensions, as long as C(X) is well-defined. The goal of the alignment task is to find two nondecreasing sequences of indices \u03b1 and \u03b2 of same length u \u2265 max(TA, TB) and to match each time index \u03b1(i) in time series A to the time index \u03b2(i) in the time series B, in such a way that \u2211u i=1 C(X)\u03b1(i),\u03b2(i) is maximal, and that (\u03b1, \u03b2) satisfies:  \u03b1(1) = \u03b2(1) = 1 (matching beginning)\n\u03b1(u) = TA, \u03b2(u) = TB (matching ending) \u2200i, (\u03b1(i+ 1), \u03b2(i+ 1))\u2212 (\u03b1(i), \u03b2(i)) \u2208 {(1, 0), (0, 1), (1, 1)} (three type of moves)\n(1) For a given (\u03b1, \u03b2), we define the binary matrix Y \u2208 {0, 1}TA\u00d7TB such that Y\u03b1(i),\u03b2(i) = 1 for every i \u2208 {1, . . . , u} and 0 otherwise. We denote by Y(X) the set of such matrices, which is uniquely determined by TA and TB . An example is given in Fig. 1. A vertical move in the Y matrix means that the signal B is waiting for A and an horizontal one that A is waiting for B. In this sense we can say that the time reference is \u201cwarped\u201d. When C(X) is known, the alignment task can be cast as the following linear program (LP) over the set Y(X):\nmax Y \u2208Y(X)\nTr(C(X)>Y ). (2)\nOur goal is to learn how to form the affinity matrix: once we have learned C(X), the alignment is obtained from Eq. (2). The optimization problem in Eq. (2) will be referred to as the decoding of our model.\nDynamic time warping. Given the affinity matrix C(X) associated with the pair of signals X = (A,B), finding the alignment that solves the LP of Eq. (2) can be done efficiently in O(TATB) using a dynamic programming algorithm. It is often referred to as dynamic time warping [6, 19]. This algorithm is described in Alg. 1 of the supplementary material. Various additional constraints may be used in the dynamic time warping algorithm [19], which we could easily add. The cardinality of the set Y(X) is huge: it corresponds to the number of paths on a rectangular grid from the southwest (1, 1) to the northeast corner (TA, TB) with only vertical, horizontal and diagonal moves allowed. This is the definition of the Delannoy numbers [2]. As noted in [24], when t = TA = TB is big, one can show that #Yt,s \u223c\n(3+2 \u221a 2)t\n\u221a \u03c0t \u221a 3 \u221a 2\u22124 ."}, {"heading": "2.2 The Mahalanobis metric", "text": "In many applications, for a pair X = (A,B), the affinity matrix is computed by C(A,B)i,j = \u2212\u2016ai,k\u2212bj,k\u20162. In this paper we propose to learn the metric to compare\nai and bj instead of using the plain Euclidean metric. That is, C(X) is parametrized by a matrix W \u2208 W \u2282 Rp\u00d7p, where W \u2282 Rp\u00d7p is the set of semi-definite positive matrices, and we use the corresponding Mahalanobis metric to compute the pairwise affinity between ai and bj : C(X;W )i,j = \u2212(ai \u2212 bj)>W (ai \u2212 bj). Note that the decoding of Eq. (2) is the maximization of a linear function in the parameter W :\nmax Y \u2208Y(X) Tr(C(X;W )>Y )\u21d4 max Y \u2208Y(X) Tr(W>\u03c6(X,Y )), (3)\nif we define the joint feature map \u03c6(X,Y ) = \u2212 \u2211 i,j Yi,j(ai\u2212 bj)(ai\u2212 bj)> \u2208 Rp\u00d7p."}, {"heading": "3 Learning the metric", "text": "From now on, we assume that we are given n pairs of training instances1 (Xi, Y i) = ((Ai, Bi), Y i) \u2208 RT iA\u00d7p\u00d7RT iB\u00d7p\u00d7RT iA\u00d7T iB , i = 1, . . . , n. Our goal is to find a matrix W such that the predicted alignments are close to the groundtruth on these examples, as well as on unseen examples. We first define a loss between alignments, in order to quantify this proximity between alignments.\n1We will see that it is necessary to have fully labelled instances, which means that for each pair Xi we need an exact alignment Y i between Ai and Bi. Partial alignment might be dealt with by alternating between metric learning and constrained alignment."}, {"heading": "3.1 Losses between alignments", "text": "In our framework, the alignments are encoded by matrices in Y(X), thus we are interested in functions ` : Y(X) \u00d7 Y(X) \u2192 R+. Let us define the Frobenius norm by \u2016M\u20162F = \u2211 i,jM 2 i,j .\nHamming loss. A simple loss between matrices is the Frobenius norm of their difference, which turns out to be the unnormalized Hamming loss [10] for 0/1 valued matrices. For two matrices Y1, Y2 \u2208 Y(X), it is defined as:\n`H(Y1, Y2) = \u2016Y1 \u2212 Y2\u20162F (4) = Tr(Y >1 Y1) + Tr(Y > 2 Y2)\u2212 2 Tr(Y >1 Y2)\n= Tr(Y11TB1 > TA) + Tr(Y21TB1 > TA)\u2212 2 Tr(Y > 1 Y2),\nwhere 1T is the vector of RT with all coordinates equal to 1. The last line of Eq. (4) comes from the fact that the Yi have 0\u2212 1 values; that makes the Hamming loss linear in Y1 and Y2. This loss is often used in other structured prediction tasks [16]. In the audio-to-score setting, Keshet et al. [14] use a modified version of this loss, which is the average number of times the difference between the two alignments is greater than a fixed threshold. This loss is easy to optimize since it is linear in our parametrization of the alignement problem, but not optimal for audio-to-audio alignment. Indeed, a major drawback of the Hamming loss is, for alignments of fixed length, it depends only on the number of \u201ccrossings\u201d between alignment paths: one can easily find Y1, Y2, Y3 such that `H(Y2, Y1) = `H(Y3, Y1) but Y2 is intuitively much closer to Y1 than Y3 (see Fig. 2). It is important to notice this is often the case as the length of the signals grows.\nArea loss. A more natural loss can be computed as the mean distance beween the paths depicted by two matrices Y 1, Y 2 \u2208 Y(X). This loss is represented by the grey zone on Fig. 1 and corresponds to the area between the paths of two matrices Y . Formally, as in Fig. 1, for each t \u2208 {1, . . . , TB} we define \u03b4t as the minimum between |min{k, Y 1k,t = 1} \u2212 max{k, Y 2k,t = 1}| and |max{k, Y 1k,t = 1} \u2212 min{k, Y 2k,t = 1}|. Then the area loss is the mean of the \u03b4t. In the audio literature [15], this loss is sometimes called the \u201cmean absolute deviation\u201d loss and is noted \u03b4abs(Y 1, Y 2). Unfortunately, in the general case of alignment problem \u03b4abs is not linear in the matrices Y . But in the context of alignment of sequences of two different nature, one of the signal is a reference and thus the index sequence \u03b1 defined in Eq. (1) is increasing, e.g. for the audio to partition alignment problem [13]. This loss is then linear in each of its arguments. More precisely, if we introduce the matrices L \u2208 RTA\u00d7TA which is lower triangular with ones, we can write the loss as\n`O = \u2016L(Y1 \u2212 Y2)\u20162F (5) = Tr(LY11TB1 > TA) + Tr(LY21TB1 > TA)\u2212 2 Tr(LY1Y > 2 L >).\nWe now prove that this loss corresponds to the area loss in this special case. Let Y be an alignment matrix and L \u2208 RTA\u00d7TA be the matrix such that Lr,s = 1 if and only if\nr \u2265 s. Then it is easy see that (LY )i,j = \u2211 k Li,kYk,j = \u2211i k=0 Yk,j . If Y does not have vertical moves, i.e. for each j there is an unique kj such that Yk,j = 1, we have that (LY )i,j = 1 if and only if i \u2265 kj . So \u2211 i,j(LY )i,j = #{(i, j), i \u2265 kj}, which is exactly the area under the curve determined by the path of Y . In all our experiments, we use \u03b4abs for evaluation but not for training.\nApproximation of the area loss: the symmetrized area loss. In many real world applications [15], the best loss to assess the quality of an alignment is the area loss. As shown by our experiments, if the Hamming loss is sufficient in some simple situations and allows to learn a metric that leads to good alignment performance in terms of area loss, on more challenging datasets it does not work at all (see Sec. 5). This is due to the fact that two alignments that are very close in terms of area loss can suffer a big Hamming loss. In Fig. 2, we provide examples where the Hamming loss is not sufficient to assess performance. Thus it is natural to extend the formulation of Eq. (5) to matrices in Y(X). We first start by symmetrizing the formulation of Eq. (5) to overcome problems of overpenalization of vertical vs. horizontal moves. Let L1 \u2208 RTB\u00d7TB be the matrix such that Lr,s = 1 if and only if r \u2265 s. We define, for any binary matrices Y 1 and Y 2,\n`S(Y1, Y2) = 1\n2\n( \u2016L(Y1 \u2212 Y2)\u2016F + \u2016(Y1 \u2212 Y2)L1)\u20162F ) (6)\n= 1\n2\n[ Tr(Y >1 L >LY1) + Tr(LY21TB1 > TA)\u2212 2 Tr(Y > 2 L >LY1)\n+ Tr(Y1L1L > 1 Y ) + Tr(Y > 2 1TA1TBL1L > 1 Y2)\u2212 2 Tr(Y2L1L>1 Y >1\n)] .\nWe propose to use the following trick to obtain a concave loss over Y(X), the convex hull ofY(X). Let us introduceD = \u03bbmax(L>L)ITA\u00d7TA andD1 = \u03bbmax(L1L>1 )ITB\u00d7TB with \u03bbmax(U) the largest eigenvalue of U . For any binary matrices Y1, Y2, we have that:\n`S(Y1, Y2) = 1\n2\n[ Tr(Y >1 (L >L\u2212D)Y1) + Tr(DY11TB1>TA)\n+ Tr(LY21TB1 > TA)\u2212 2 Tr(Y > 2 (L >L\u2212D)Y1) + Tr(Y1(L1L > 1 \u2212D)Y ) + Tr(Y1D11TB1>TA)\nTr(Y >2 L1L > 1 Y2)\u2212 2 Tr(Y2L1L>1 Y >1 ) ] and we get a concave function over Y(X) that coincides with `S on Y(X)."}, {"heading": "3.2 Empirical loss minimization", "text": "Recall that we are given n alignment examples (Xi, Y i)1\u2264i\u2264n. For a fixed loss `, our goal is now to solve the following minimization problem in W :\nmin W\u2208W { 1 n n\u2211 i=1 ` ( Y i, argmax Y \u2208Y Ti A ,Ti B Tr(C(Xi;W )>Y ) ) + \u03bb\u2126(W ) } , (7)\nwhere \u2126 = \u03bb2 \u2016W\u2016 2 F is a convex regularizer preventing from overfitting, with \u03bb \u2265 0."}, {"heading": "4 Large margin approach", "text": "In this section we describe a large margin approach to solve a surrogate to the problem in Eq. (7), which is untractable. As shown by Eq. (3), the decoding task is the maximum of a linear function in the parameter W and aims at predicting an output over a large and discrete space (the space of potential alignments with respect to the constraints in Eq. (1)). Learning W thus falls in the structured prediction framework [25, 22]. We define the hinge-loss, a convex surrogate to `, by\nL(X,Y ;W ) = max Y \u2032\u2208Y(X)\n{ `(Y, Y \u2032)\u2212 Tr(W> [\u03c6(X,Y )\u2212 \u03c6(X,Y \u2032)]) } . (8)\nThe evaluation of L is usually referred to as the \u201closs-augmented decoding\u201d, see [25]. Among the aforementionned losses, the Hamming loss `H is the only one leading directly to a tractable loss-augmented decoding problem and thus that falls directly into the structured prediction framework. Indeed, plugging Eq. (4) into (8) leads to a lossaugmented decoding that is a LP over the set Y(X) and that can therefore be solved using the dynamic time warping algorithm. If we define Y\u0302 i as the argmax in Eq. (8) when (X,Y ) = (Xi, Y i), then elementary computations show that\nY\u0302 i = argmin Y \u2208YTA,TB Tr((U> \u2212 2Y i> \u2212 C(Xi;W )>)Y ),\nwhere U = 1TB1 > TB \u2208 RTA\u00d7TB . We now aim at solving the following problem, sometimes called the margin-rescaled problem:\nmin W\u2208W\n\u03bb 2 \u2016W\u20162F + 1 n n\u2211 i=1 max Y \u2208Y Ti A ,Ti B { `(Y, Y i)\u2212 Tr(W> [ \u03c6(Xi, Y i)\u2212 \u03c6(Xi, Y ) ] ) } .\n(9)\nHamming loss case. From Eq. (3), one can notice that our joint feature map is linear in Y . Thus, if we take a loss that is linear in the first argument of `, for instance the Hamming loss, the loss-augmented decoding is the maximization of a linear function over the spaces Y(X) that we can do efficiently using dynamic programming algorithms (see Sec. 2.1 and supplementary material). That way, plugging the Hamming loss (Eq. (4)) in Eq. (9) leads to a convex structured prediction problem. This problem can be solved using standard techniques such that cutting plane methods [12], stochastic gradient descent [20], or block-coordinate Frank-Wolfe in the dual [16]. Note that we adapted the standard unconstrained optimization methods to our setting, where W 0.\nOptimization using the symmetrized area loss. In this section we propose to show that it is possible to deal The symmetrized area loss is concave in its first argument, thus the problem of Eq. 9 using it is in a min/max form and thus deriving a dual is straightforward. Details can be found in the supplementary material. If we plug the symmetrized area loss `S (SAL) defined in Eq. (6) into our problem (9), we can show that the dual of (9) has the following form:\nmin (Z1,...,Zn)\u2208Y\n1 2\u03bbn2 \u2016 \u2211n i=1\u2212 \u2211 j,k(Yi \u2212 Zi)j,k(aj \u2212 bk)(aj \u2212 bk)T \u20162F\n\u2212 1n \u2211n i=1 `S(Z,Z i), (10)\nif we denote by Y(Xi) the convex hull of the sets Y(Xi), and by Y the cartesian product over all the training examples i of such sets. Note that we recover a similar result as [16]. Since the SAL loss is concave, the aforrementionned problem is convex. The problem (10) is a quadratic program over the compact set Z . Thus we can use a Frank-Wolfe [8] algorithm. Note that it is similar to the one proposed by Lacoste-Julien et al. [16] but with supplementary term due to the concavity of the loss."}, {"heading": "5 Experiments", "text": "We applied our method to the task of learning a good similarity measure for aligning audio signals. In this field researchers have spent a lot of efforts in designing wellsuited and meaningful features [13, 5]. But the problem of combining these features for aligning temporal sequences is still challenging."}, {"heading": "5.1 Dataset of Kirchhoff and Lerch [15]", "text": "Dataset description. First, we applied our method on the dataset of Kirchhoff and Lerch [15]. In this dataset, pairs of aligned examples (Ai, Bi) are artificially created by stretching an original audio signal. That way the groundtruth alignment Y i is known and thus the data falls into our setting A more precise description of the dataset can be found in [15]. The N = 60 pairs are stretched along two different tempo curves. Each signal is made of 30s of music that are divided in frames of 46ms with a hopsize of 23ms, thus leading to a typical length of the signals of T \u2248 1300 in our setting. We keep p = 11 features simple to implement and that are known to perform well for alignment tasks [15]. Those were: five MFCC [9] (labeled M1,. . . ,M5 in Fig. 3), the spectral flatness (SF), the spectral centroid (SC), the spectral spread (SS), the maximum of the envelope (Max), and the power level of each frame (Pow), see [15] for more details on the computation of the features. We normalize each feature by subtracting the median value and dividing by the standard deviation to the median, as audio data are subject to outliers.\nExperiments. We conducted the following experiment: for each individual feature, we perform alignment using dynamic time warping algorithm and evaluate the performance of this single feature in terms of losses typically used to asses performance in this setting [15]. In Fig. 3, we report the results of these experiments. Then, we plug these data into our method, using the Hamming loss to learn a linear positive combination of these features. The result is reported in Fig 3. Thus, combining these features on this dataset yields to better performances than only considering a single feature."}, {"heading": "5.2 Chorales dataset", "text": "Dataset. The Bach 10 dataset2 consists in ten J. S. Bach\u2019s Chorales (small quadriphonic pieces). For each Chorale, a MIDI reference file corresponding to the \u201cscore\u201d, or basically a representation of the partition. The alignments between the MIDI files and the audio file are given, thus we have converted these MIDI files into audio following what is classically done for alignment (see e.g, [11]). That way we fall into the audio-to-audio framework in which our technique apply. Each piece of music is approximately 25s long, leading to similar signal length.\nExperiments. We use the same features as in Sec. 5.1. As depicted in Fig. 4, the optimization with Hamming loss performs poorly on this dataset. In fact, the best individual feature performance is far better than the performance of the learned W . Thus metric learning with the \u201cpractical\u201d Hamming loss performs much worse than the best single feature. Then, we conducted the same learning experiment with the symetrized area loss `S . The resulting learned parameter is far better than the one learned using the Hamming\n2http://music.cs.northwestern.edu/data/Bach10.html.\nloss. We get a performance that is similar to the one of the best feature. Note that these features were handcrafted and reaching their performance on this hard task with only a few training instances is already challenging. In Fig. 2, we have depicted the result, for a learned parameter W , of the loss augmented decoding performed either using the area. As it is known for structured SVM, this represents the most violated constraint [25]. We can see that the most violated constraint for the Hamming loss leads to an alignment which is totally unrelated to the groundtruth alignment whereas the one for the symmetrized area loss is far closer and much more discriminative."}, {"heading": "5.3 Feature selection", "text": "Last, we conducted experiments over the same datasets. Starting from low level features, namely the 13 leading MFCCs coefficients and their first two derivatives, we learn a linear combination of these that achieves good alignment performance in terms of the area loss.Note that very little musical prior knowledge is put into these. Moreover we either improve on the best handcrafted feature on the dataset of [15] or perform similarly. On both datasets, the performance of learned combination of handcrafted features performed similarly to the combination of these 39 MFCCs coefficients"}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a structured prediction framework for learning the metric in temporal alignment problems. We were able to combine hand-crafted features, as well as building automatically new state-of-the-art features from basic low-level information with very little expert knowledge. Technically, this is made possible by considering a loss beyond the usual Hamming loss which is typically used because it is \u201cpractical\u201d within a structured prediction framework (linear in the output representation). The present work may be extended in several ways, the main one being to consider cases where only partial information about the alignments is available. This is often the case in music [5] or bioinformatics applications. Note a simple alternating optimization between metric learning and constrained alignment provide a simple first solution, which could probably be improved upon."}, {"heading": "Acknowledgements", "text": "We acknowledge the support of the GARGANTUA project (Mastodons program of CNRS), the grant SIERRA-23999 from the European Research Council and a PhD fellowship from the EADS Foundation."}, {"heading": "A Derivation of the BCFW-like algorithm for the quadratic loss.", "text": "A.1 Relaxing the set for loss augmented inference. Let us start from the global structured objective equation of the paper. Recall that we dispose of the training examples ((X1, Y 1), . . . , (Xn, Y n)). In order to make the derivation easier, and following Lacoste-Julien et al. [16], we denote the difference between the feature map associated to any Y \u2208 Y(Xi) and the one associated to the true training example label Yi by: Tr(W\u03c6(Xi, Y i)) = Tr(W \u2211 j,k(Y i j,k \u2212 Yj,k)(aij \u2212 bik)(a i j \u2212 bik)T ) = \u3008W,\u03c8i(Y )\u3009. The objective of structured prediction is thus:\nmin W\u2208W\n\u03bb 2 \u2016W\u201622 + 1 n n\u2211 i=1 max Y \u2208Y(Xi) { `i(Y, Y i)\u2212 \u3008W,\u03c8i(Y )\u3009 } . (11)\nThe term maxY \u2208Y(Xi) { `i(Y, Y i)\u2212 \u3008W,\u03c8i(Y )\u3009 }\ncorresponds to the structural hinge loss for our problem. Let us introduce Y(Xi) the convex hull of the sets Y(Xi). We will also use Y = Y(X1) \u00d7 . . . \u00d7 Y(Xn). From now on, we will perform the loss augmented decoding on this relaxed set. This problem has potentially non integral\nsolutions. We call the maximization of the hinge loss over Y the loss augmented inference. Now we can write a new optimization objective:\nmin W\u2208W\n\u03bb 2 \u2016W\u201622 + max\n(Z1,...,Zn)\u2208Y\n{ 1\nn n\u2211 i=1 [ `i(Zi, Y\ni)\u2212 \u3008W,\u03c8i(Zi)\u3009 ]} . (12)\nNote that since our joint feature map \u03c6(Xi, Y ) is linear in Y , if ` is linear as well (for instance if ` is the Hamming loss), this problem is strictly equivalent to (11) since in that case, the loss-augmented inference is a LP over Y(Xi), which has necessary a solution in Y(Xi) (see, e.g, [Prop. B.21] of [3]. In general, in order to be convex and thus tractable, the aforrementioned problem requires a loss which is concave over the convex sets Y(Xi).\nA.2 Dual of the structured SVM Since Prob. (11) is in saddle point form, we get the dual by switching the max and the min:\nmax (Z1,...,Zn)\u2208Y(X1)\u00d7...\u00d7Y(Xn) min W\u2208W\n\u03bb 2 \u2016W\u201622 + { 1 n n\u2211 i=1 [ `i(Y, Y i)\u2212 \u3008W,\u03c8i(Zi)\u3009 ] }.\n(13) From the above equation, we deduce the following relation linking primal variable W and dual variables (Z1, . . . , Zn) \u2208 Y(X1)\u00d7 . . .\u00d7 Y(Xn): In the specific case whenW is unconstrained and simply equals to Rp\u00d7p, this reduces to:\nW = 1\n\u03bb n\u2211 i=1 \u03c8i(Zi). (14)\nIfW is the set of symmetric semidefinite positive matrices we get:\nW = 1\n\u03bb n\u2211 i=1 (\u03c8i(Zi))+, (15)\nwith (\u03c8i(Zi))+ the projection of (\u03c8i(Zi)) overW . Eventually if we consider W the set of diagonal matrices, and denote by Diag the operator associating to a matrix the matrix composed of its diagonal:\nW = 1\n\u03bb n\u2211 i=1 Diag(\u03c8i(Zi)). (16)\nThese relations are also known as the \u201crepresenter theorems\u201d. For what follows we consider the case ofW = Rp\u00d7p but dealing with the other cases is similar. In that case the dual can be written simply as:\nmax (Z1,...,Zn)\u2208Y(X1)\u00d7...\u00d7Y(Xn) \u2212 1 2\u03bbn2 \u2016 n\u2211 i=1 \u03c8i(Zi)\u20162F + 1 n n\u2211 i=1 `(Yi, Zi). (17)\nWe recover a result similar to the ones of Lacoste-Julien et al. [16].\nA.3 A Frank-Wolfe algorithm for solving Prob. (17) Now, we can derivate a Frank-Wolfe algorithm for solving the dual problem of 17. As noted in the paper, we are able to maximize or minimize any linear form over the sets Y(Xi), thus we are able to solve LPs over the convex hulls Y(Xi) of such sets. Plugging back the specific form of our joint feature map directly into Eq. (17) we get that \u03c8i(Zi) = \u2212 \u2211 j,k(Yi\u2212Zi)j,k(aj \u2212 bk)(aj \u2212 bk)T and thus we can write the dual problem as:\nmin (Z1,...,Zn)\u2208\nY(X1)\u00d7...\u00d7Y(Xn)\n1\n2\u03bbn2 \u2016 n\u2211 i=1 \u2212 \u2211 j,k (Yi \u2212 Zi)j,k(aj \u2212 bk)(aj \u2212 bk)T \u20162F \u2212 1 n n\u2211 i=1 `(Y i, Zi) (18)\nNow, as in the paper, let us introduce L \u2208 RTA\u00d7TA and L1 \u2208 RTB\u00d7TB . If Ui is the matrix of ones of the same size as Zi, we consider the following loss:\n`(Y i, Zi) = 1\n2\n[ Tr(ZiT (LTL\u2212D)Zi) + Tr(DZiU i) + Tr(Y iTLTLi)\u2212 2 Tr(ZiTLTLY i)\n+ Tr(Zi(LT1 L1 \u2212D1)Zi) + Tr(D1ZiU i) + Tr(Y iLT1 Li1)\u2212 2 Tr(ZLT1 L1Y i) ] .(19)\nThis loss is sound for alignments problems since, when Yi and Zi are in Y , this is simply the `S loss \u2016LYi \u2212 LZi\u20162F + \u2016YiL1 \u2212 ZiL1\u20162F . Thus we get the following overall dual objective:\nmin (Z1,...,Zn)\u2208Y\n1\n2\u03bbn2 \u2016 n\u2211 i=1 \u2212 \u2211 j,k (Yi \u2212 Zi)j,k(aj \u2212 bk)(aj \u2212 bk)T \u20162F\n\u2212 1 n ( n\u2211 i=1 [Tr(ZiT (LTL\u2212D)Zi) + Tr(ZiTDU i) +\nTr(Y TLTLi)\u2212 2 Tr(ZiTLTLY i) + Tr(Zi(LT1 L1 \u2212D1)ZiT ) + Tr(UiD1Z i) + Tr(Y LT1 L i 1)\u2212 2 Tr(ZLT1 L1Y iT ) ] ). (20)\nWe recall that D is a diagonal matrix such that A>A\u2212D 0 and thus our objective is convex. Our dynamic programming algorithm (DTW) is able to maximize any linear function over the sets. Thus we can use a Frank-Wolfe [8] technique. At iteration t, this algorithm iteratively computes a linearization of the function at the current point (Z1, . . . Zn)k, computes a linearization of the function, optimize it, get a new point (Z1, . . . Zn)\u2217kand then make a convex combination using a stepsize \u03b3. Note that we have directly a stochastic version of such an algorithm. As noted in Lacoste-Julien et al. [16] instead of computing a gradient for each block of variable Zi, we simply need to choose randomly one block at each timestep and make an update on these variables. The linearization simply consists in computing the matrix gradient for each of the matrix variables Zi which turns out to be:\n\u2207Zi(g) = 1n [ C \u2212 12 ( 2(LTL\u2212D)Zi +DUi \u2212 2LTLY i\n+ 2Zi(LT1 L1 \u2212D) + UiD \u2212 2Y iLTL )]\n(21)\nwhere C is simply the affinity matrix of dynamic time warping."}, {"heading": "B The dynamic time warping algorithm", "text": "Let us give the pseudocode of the dynamic time warping that maximize the LP (2) of the article.In opposition to M\u00fcller [19], we give a version of the algorithm for the affinity matrix C. Intuitively, the cost matrix is the opposite of a cost matrix, thus we aim to maximize the cumulated affinity instead of minimizing the cumulated cost. This algorithm is O(TATB), making it very costly to compute for large time series."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>In this paper, we propose to learn a Mahalanobis distance to perform alignment<lb>of multivariate time series. The learning examples for this task are time series for<lb>which the true alignment is known. We cast the alignment problem as a struc-<lb>tured prediction task, and propose realistic losses between alignments for which<lb>the optimization is tractable. We provide experiments on real data in the audio to<lb>audio context, where we show that the learning of a similarity measure leads to<lb>improvements in the performance of the alignment task. We also propose to use<lb>this metric learning framework to perform feature selection and, from basic audio<lb>features, build a combination of these with better performance for the alignment.", "creator": "LaTeX with hyperref package"}}}