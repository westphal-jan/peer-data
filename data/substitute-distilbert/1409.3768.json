{"id": "1409.3768", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection", "abstract": "sparse high dimensional graphical mode selection is a popular topic in contemporary parameter learn. to this understanding, various useful approaches have been proposed in the context of $ \\ ell _ 1 $ - penalized analysis in the gaussian framework. so many of these statistical covariance estimation approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend heavily the gaussian functional form. to address this gap, an simpler pseudo - likelihood based discrete correlation graph decision method ( concord ) has been recently proposed. this method suggests coordinate - wise minimization of matching vertex based pseudo - likelihood, who has been shown to have robust model selection properties in comparison with simplified gaussian approach. in direct contrast to the parallel work extending the gaussian setting however, having new convex pseudo - likelihood framework includes not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. in this tool, we address this crucial gap by proposing two proximal gradient methods ( alpha - ista and concord - fista ) for performing $ \\ ell _ m $ - regularized inverse covariance matrix measures in the pseudo - likelihood framework. we present timing comparisons with coordinate - wise ratios and demonstrate that our approach yields tremendous payoffs for $ \\ ell _ 1 $ - penalized partial correlation variance estimation outside constrained gaussian setting, thus yielding the fastest and most scalable approach for such problems. we undertake careful theoretical analysis of their approach and rigorously approximate convergence, and also derive functions thereof.", "histories": [["v1", "Fri, 12 Sep 2014 15:25:07 GMT  (76kb,D)", "http://arxiv.org/abs/1409.3768v1", "NIPS accepted version"]], "COMMENTS": "NIPS accepted version", "reviews": [], "SUBJECTS": "stat.CO cs.LG stat.ML", "authors": ["sang-yun oh", "onkar dalal", "kshitij khare", "bala rajaratnam"], "accepted": true, "id": "1409.3768"}, "pdf": {"name": "1409.3768.pdf", "metadata": {"source": "CRF", "title": "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection", "authors": ["Sang-Yun Oh", "Onkar Dalal", "Kshitij Khare"], "emails": ["syoh@lbl.gov", "onkar@alumni.stanford.edu", "kdkhare@stat.ufl.edu", "brajarat@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Background", "text": "Sparse inverse covariance estimation has received tremendous attention in the machine learning, statistics and optimization communities. These sparse models, popularly known as graphical models, have widespread use in various applications, especially in high dimensional settings. The most popular inverse covariance estimation framework is arguably the `1-penalized Gaussian likelihood optimization framework as given by\nminimize \u2126\u2208Sp++ \u2212 log det \u2126 + tr(S\u2126) + \u03bb\u2016\u2126\u20161\nwhere Sp++ denotes the space of p-dimensional positive definite matrices, and `1-penalty is imposed on the elements of \u2126 = (\u03c9ij)1\u2264i\u2264j\u2264p by the term \u2016\u2126\u20161 = \u2211 i,j |\u03c9ij | along with the scaling factor \u03bb > 0. The matrix S denotes the\nar X\niv :1\n40 9.\n37 68\nv1 [\nst at\n.C O\n] 1\nsample covariance matrix of the data Y \u2208 IRn\u00d7p. As the `1-penalized log likelihood is convex, the problem becomes more tractable and has benefited from advances in convex optimization. Recent efforts in the literature on Gaussian graphical models therefore have focused on developing principled methods which are increasingly more and more scalable. The literature on this topic is simply enormous and for the sake of brevity, space constraints and the topic of this paper, we avoid an extensive literature review by referring to the references in the seminal work of Banerjee et al. [2008] and the very recent work of Dalal and Rajaratnam [2014]. These two papers contain references to recent work, including past NIPS conference proceedings."}, {"heading": "1.2 The CONCORD method", "text": "Despite their tremendous contributions, one shortcoming of the traditional approaches to `1-penalized likelihood maximization is the restriction to the Gaussian assumption. To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al. [2010]. These approaches are either not convex, and/or convergence of corresponding maximization algorithms are not established. In this sense, non-Gaussian partial correlation graph estimation methods have lagged severely behind, despite the tremendous need to move beyond the Gaussian framework for obvious practical reasons. In very recent work, a convex pseudo-likelihood approach with good model selection properties called CONCORD Khare et al. [2014] was proposed. The CONCORD algorithm minimizes\nQcon(\u2126) = \u2212 p\u2211 i=1 n log\u03c9ii + 1 2 p\u2211 i=1 \u2016\u03c9iiYi + \u2211 j 6=i \u03c9ijYj\u201622 + n\u03bb \u2211 1\u2264i<j\u2264p |\u03c9ij | (1)\nvia cyclic coordinate-wise descent that alternates between updating off-diagonal elements and diagonal elements. It is straightforward to show that operators Tij for updating (\u03c9ij)1\u2264i<j\u2264p (holding (\u03c9ii)1\u2264i\u2264p constant) and Tii for updating (\u03c9ii)1\u2264i\u2264p (holding (\u03c9ij)1\u2264i<j\u2264p constant) are given by\n(Tij(\u2126))ij = S\u03bb\n( \u2212 (\u2211 j\u2032 6=j \u03c9ij\u2032sjj\u2032 + \u2211 i\u2032 6=i \u03c9i\u2032jsii\u2032 )) sii + sjj\n(2)\n(Tii(\u2126))ii = \u2212 \u2211 j 6=i \u03c9ijsij +\n\u221a(\u2211 j 6=i \u03c9ijsij )2 + 4sii\n2sii . (3)\nThis coordinate-wise algorithm is shown to converge to a global minima though no rate is given [Khare et al., 2014]. Note that the equivalent problem assuming a Gaussian likelihood has seen much development in the last ten years, but a parallel development for the recently introduced CONCORD framework is lacking for obvious reasons. We address this important gap by proposing state-of-the-art proximal gradient techniques to minimizeQcon. A rigorous theoretical analysis of the pseudo-likelihood framework and the associated proximal gradient methods which are proposed is undertaken. We establish rates of convergence and also demonstrate that our approach can lead to massive computational speed-ups, thus yielding extremely fast and principled solvers for the sparse inverse covariance estimation problem outside the Gaussian setting."}, {"heading": "2 CONCORD using proximal gradient methods", "text": "The penalized matrix version the CONCORD objective function in (1) is given by\nQcon(\u2126) = n\n2\n[ \u2212 log |\u21262D|+ tr(S\u21262) + \u03bb\u2016\u2126X\u20161 ] . (4)\nwhere \u2126D and \u2126X denote the diagonal and off-diagonal elements of \u2126. We will use the notation A = AD +AX to split any matrix A into its diagonal and off-diagonal terms.\nThis section proposes a scalable and thorough approach to solving the CONCORD objective function using recent advances in convex optimization and derives rates of convergence for such algorithms. In particular, we use proximal gradient-based methods to achieve this goal and demonstrate the efficacy of such methods for the non-Gaussian graphical modeling problem. First, we propose CONCORD-ISTA and CONCORD-FISTA in section 2.1: methods which are inspired by the iterative soft-thresholding algorithms in Beck and Teboulle [2009]. We undertake a comprehensive treatment of the CONCORD optimization problem by also investigating the dual of the CONCORD problem. Other popular methods in the literature, including the potential use of alternating minimization algorithm and the second order proximal Newtons method CONCORD-PNOPT, are considered in Supplemental section A.5.\nAlgorithm 1 CONCORD-ISTA Input: sample covariance matrix S, penalty matrix \u039b Initialize: \u2126(0) \u2208 Sp+, \u03c4(0,0) \u2264 1,\nc < 1, \u2206subg = 2 subg. while \u2206subg > subg or \u2206func > func do\nCompute \u2207h1: G(k) = \u2212 ( \u2126\n(k) D )\u22121 + 12 ( S \u2126(k) + \u2126(k)S ) Compute \u03c4k:\nLargest \u03c4k \u2208 {cj\u03c4(k,0)}j=0,1,... such that, \u2126(k+1) = S\u03c4k\u039b ( \u2126(k) \u2212 \u03c4kG(k) ) satisfies (8).\nUpdate: \u2126(k+1) using the appropriate step size. Compute next initial step size: \u03c4(k+1,0) Compute convergence criteria:\n\u2206subg = \u2016\u2207h1(\u2126(k)) + \u2202h2(\u2126(k))\u2016\n\u2016\u2126(k)\u2016 .\nend while\nAlgorithm 2 CONCORD-FISTA Input: sample covariance matrix S, penalty matrix \u039b Initialize: (\u0398(1) =)\u2126(0) \u2208 Sp+, \u03b11 = 1, \u03c4(0,0) \u2264 1,\nc < 1, \u2206subg = 2 subg. while \u2206subg > subg or \u2206func > func do\nCompute\u2207h1: G(k) = \u2212 ( \u0398\n(k) D )\u22121 + 12 ( S\u0398(k) + \u0398(k)S ) Compute \u03c4k:\nLargest \u03c4k \u2208 {cj\u03c4(k,0)}j=0,1,... such that, \u2126(k) = S\u03c4k\u039b ( \u0398(k) \u2212 \u03c4kG(k) ) satisfies (8)\nUpdate: \u03b1k+1 = (1 + \u221a 1 + 4\u03b1k2)/2\nUpdate: \u0398(k+1) = \u2126(k) + ( \u03b1k\u22121 \u03b1k+1 ) ( \u2126(k) \u2212 \u2126(k\u22121) ) Compute next initial step size: \u03c4(k+1,0) Compute convergence criteria:\n\u2206subg = \u2016\u2207h1(\u2126(k)) + \u2202h2(\u2126(k))\u2016\n\u2016\u2126(k)\u2016 .\nend while"}, {"heading": "2.1 Iterative Soft Thresholding Algorithms: CONCORD-ISTA, CONCORD-FISTA", "text": "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov\u2019s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm. The essence of the proximal gradient algorithms is to divide the objective function into a smooth part and a non-smooth part, then take a proximal step (w.r.t. the non-smooth part) in the negative gradient direction of the smooth part. Nesterov\u2019s accelerated gradient extension Nesterov [1983] uses a combination of gradient and momentum steps to achieve accelerated rates of convergence. In this section, we apply these methods in the context of CONCORD which also has a composite objective function.\nThe matrix CONCORD objective function (4) can be split into a smooth part h1(\u2126) and a non-smooth part h2(\u2126):\nh1(\u2126) = \u2212 log det \u2126D + 1\n2 tr(\u2126S\u2126), h2(\u2126) = \u03bb\u2016\u2126X\u20161. (5)\nThe gradient and hessian of the smooth function h1 are given by\n\u2207h1(\u2126) = \u2126D\u22121 + 1\n2\n( S\u2126T + \u2126S ) ,\n\u22072h1(\u2126) = i=p\u2211 i=1 \u03c9\u22122ii [ eiei T \u2297 eieiT ] + 1 2 (S \u2297 I + I \u2297 S) , (6)\nwhere ei is a column vector of zeros except for a one in the i-th position. The proximal operator for the non-smooth function h2 is given by element-wise soft-thresholding operator S\u03bb as\nproxh2(\u2126) = arg min \u0398\n{ h2(\u0398) + 1\n2 \u2016\u2126\u2212\u0398\u20162F } = S\u039b(\u2126) = sign(\u2126) max{|\u2126| \u2212 \u039b, 0}, (7)\nwhere \u039b is a matrix with 0 diagonal and \u03bb for each off-diagonal entry. The details of the proximal gradient algorithm CONCORD-ISTA are given in Algorithm 1, and the details of the accelerated proximal gradient algorithm CONCORD-FISTA are given in Algorithm 2."}, {"heading": "2.2 Choice of step size", "text": "In the absence of a good estimate of the Lipschitz constant L, the step size for each iteration of CONCORD-ISTA and CONCORD-FISTA is chosen using backtracking line search. The line search for iteration k starts with an initial step size \u03c4(k,0) and reduces the step with a constant factor c until the new iterate satisfies the sufficient descent condition:\nh1(\u2126 (k+1)) \u2264 Q(\u2126(k+1),\u2126(k)) (8)\nwhere,\nQ(\u2126,\u0398) = h1(\u0398) + tr ( (\u2126\u2212\u0398)T\u2207h1(\u0398) ) + 1\n2\u03c4\n\u2225\u2225\u2126\u2212\u0398\u2225\u22252 F .\nIn section 4, we have implemented algorithms choosing the initial step size in three different ways: (a) a constant starting step size (=1), (b) the feasible step size from the previous iteration \u03c4k\u22121, (c) the step size heuristic of BarzilaiBorwein. The Barzilai-Borwein heuristic step size is given by\n\u03c4k+1,0 = tr ( (\u2126(k+1) \u2212 \u2126(k))T (\u2126(k+1) \u2212 \u2126(k)) ) tr ( (\u2126(k+1) \u2212 \u2126(k))T (G(k+1) \u2212G(k))\n) . (9) This is an approximation of the secant equation which works as a proxy for second order information using successive gradients (see Barzilai and Borwein [1988] for details)."}, {"heading": "2.3 Computational complexity", "text": "After the one time calculation of S, the most significant computation for each iteration in CONCORD-ISTA and CONCORD-FISTA algorithms is the matrix-matrix multiplication W = S\u2126 in the gradient term. If s is the number of non-zeros in \u2126, then W can be computed using O(sp2) operations if we exploit the extreme sparsity in \u2126. The second matrix-matrix multiplication for the term tr(\u2126(S\u2126)) can be computed efficiently using tr(\u2126W ) = \u2211 \u03c9ijwij over the set of non-zero \u03c9ij\u2019s. This computation only requires O(s) operations. The remaining computations are all at the element level which can be completed in O(p2) operations. Therefore, the overall computational complexity for each iteration reduces toO(sp2). On the other hand, the proximal gradient algorithms for the Gaussian framework require inversion of a full p \u00d7 p matrix which is non-parallelizable and requires O(p3) operations. The coordinatewise method for optimizing CONCORD in Khare et al. [2014] also requires cycling through the p2 entries of \u2126 in specified order and thus does not allow parallelization. In contrast, CONCORD-ISTA and CONCORD-FISTA can use \u2018perfectly parallel\u2019 implementations to distribute the above matrix-matrix multiplications. At no step do we need to keep all of the dense matrices S, S\u2126,\u2207h1 on a single machine. Therefore, CONCORD-ISTA and CONCORD-FISTA are scalable to any high dimensions restricted only by the number of machines."}, {"heading": "3 Convergence Analysis", "text": "In this section, we prove convergence of CONCORD-ISTA and CONCORD-FISTA methods along with their respective convergence rates of O(1/k) and O(1/k2). We would like to point out that, although the authors in Khare et al. [2014] provide a proof of convergence for their coordinate-wise minimization algorithm for CONCORD, they do not provide any rates of convergence. The arguments for convergence leverage the results in Beck and Teboulle [2009] but require some essential ingredients. We begin with proving lower and upper bounds on the diagonal entries \u03c9kk for \u2126 belonging to a level set of Qcon(\u2126). The lower bound on the diagonal entries of \u2126 establishes Lipschitz continuity of the gradient \u2207h1(\u2126) based on the hessian of the smooth function as stated in (6). The proof for the lower bound uses the existence of an upper bound on the diagonal entries. Hence, we prove both bounds on the diagonal entries. We begin by defining a level set C0 of the objective function starting with an arbitrary initial point \u2126(0) with a finite function value as\nC0 = { \u2126 | Qcon(\u2126) \u2264 Qcon(\u2126(0)) = M } . (10)\nFor the positive semidefinite matrix S, let U denote 1\u221a 2 times the upper triangular matrix from the LU decomposition of S, such that S = 2UTU (the factor 2 simplifies further arithmetic). Assuming the diagonal entries of S to be strictly nonzero (if skk = 0, then the kth component can be ignored upfront since it has zero variance and is equal to a constant for every data point), we have at least one k such that uki 6= 0 for every i. Using this, we prove the following theorem bounding the diagonal entries of \u2126.\nTheorem 3.1. For any symmetric matrix \u2126 satisfying \u2126 \u2208 C0, the diagonal elements of \u2126 are bounded above and below by constants which depend only on M , \u03bb and S. In other words,\n0 < aM,\u03bb,S \u2264 |\u03c9kk| \u2264 bM,\u03bb,S , \u2200 k = 1, 2, . . . , p, for some constants aM,\u03bb,S and bM,\u03bb,S .\nProof. (a) Upper bound: Suppose |\u03c9ii| = max{|\u03c9kk|, for k = 1, 2, . . . , p}. Then, we have M = Qcon(\u2126\n(0)) \u2265 Qcon(\u2126) = h1(\u2126) + h2(\u2126) \u2265 \u2212 log det \u2126D + tr ( (U\u2126)T (U\u2126) ) + \u03bb\u2016\u2126X\u20161\n= \u2212 log det \u2126D + \u2016U\u2126\u20162F + \u03bb\u2016\u2126X\u20161. (11) Considering only the kith entry in the Frobenious norm term and the ith column penalty in the third term we get\nM \u2265 \u2212p log |\u03c9ii|+ j=p\u2211 j=k ukj\u03c9ji 2 + \u03bb j=p\u2211 j=k,j 6=i |\u03c9ji|. (12)\nNow, suppose |uki\u03c9ii| = z and \u2211j=p j=k,j 6=i ukj\u03c9ji = x. Then\n|x| \u2264 j=p\u2211\nj=k,j 6=i\n|ukj ||\u03c9ji| \u2264 u\u0304 j=p\u2211\nj=k,j 6=i\n|\u03c9ji|,\nwhere u\u0304 = max{|ukj |, for j = k, k + 1, . . . , p and j 6= i}. Going back to the inequality (12), for \u03bb\u0304 = \u03bb2u\u0304 , we have\nM\u0304 = M + \u03bb\u03042 \u2212 p log |uki| \u2265 \u2212p log z + (z + x)2 + 2\u03bb\u0304|x|+ \u03bb\u03042 (13) = \u2212p log z + ( z + x+ \u03bb\u0304sign(x) )2 \u2212 2\u03bb\u0304z sign(x) (14) Here, if x \u2265 0, then M\u0304 \u2265 \u2212p log z + z2 using the first inequality (13), and if x < 0, then M\u0304 \u2265 \u2212p log z + 2\u03bb\u0304z using the second inequality (14). In either cases, the functions \u2212p log z + z2 and \u2212p log z + 2\u03bb\u0304z are unbounded as z \u2192\u221e. Hence, the upper bound of M\u0304 on these functions guarantee an upper bound bM,\u03bb,S such that |\u03c9ii| \u2264 bM,\u03bb,S . Therefore, |\u03c9kk| \u2264 bM,\u03bb,S for all k = 1, 2, . . . , p. (b) Lower bound: By positivity of the trace term and the `1 term (for off-diagonals), we have\nM \u2265 \u2212 log det \u2126D = i=p\u2211 i=1 \u2212 log |\u03c9ii|. (15)\nThe negative log function g(z) = \u2212 log(z) is a convex function with a lower bound at z\u2217 = bM,\u03bb,S with g(z\u2217) = \u2212 log bM,\u03bb,S . Therefore, for any k = 1, 2, . . . , p, we have\nM \u2265 i=p\u2211 i=1 \u2212 log |\u03c9ii| \u2265 \u2212(p\u2212 1) log bM,\u03bb,S \u2212 log |\u03c9kk|. (16)\nSimplifying the above equation, we get the lower bound aM,\u03bb,S on the diagonal entries \u03c9kk. More specifically, log |\u03c9kk| \u2265 \u2212M \u2212 (p\u2212 1) log bM,\u03bb,S .\nTherefore, |\u03c9kk| \u2265 aM,\u03bb,S = e\u2212M\u2212(p\u22121) log bM,\u03bb,S > 0 serves as a lower bound for all k = 1, 2, . . . , p.\nGiven that the function values are non-increasing along the iterates of Algorithms 1, 2 and 3, the sequence of \u2126(k) satisfy \u2126(k) \u2208 C0 for k = 1, 2, ..... The lower bounds on the diagonal elements of \u2126(k) provides the Lipschitz continuity using\n\u22072h1(\u2126(k)) ( a\u22122M,\u03bb,S + \u2016S\u20162 ) (I \u2297 I) . (17)\nTherefore, using the mean-value theorem, the gradient\u2207h1 satisfies \u2016\u2207h1(\u2126)\u2212\u2207h1(\u0398)\u2016F \u2264 L\u2016\u2126\u2212\u0398\u2016F , (18)\nwith the Lipschitz continuity constant L = a\u22122M,\u03bb,S + \u2016S\u20162. The remaining argument for convergence follows from the theorems in Beck and Teboulle [2009].\nTheorem 3.2. ([Beck and Teboulle, 2009, Theorem 3.1]). Let {\u2126(k)} be the sequence generated by either Algorithm 1 with constant step size or with backtracking line-search. Then for any k \u2265 1,\nQcon(\u2126 (k))\u2212Qcon(\u2126\u2217) \u2264 \u03b1L\u2016\u2126(0) \u2212 \u2126\u2217\u20162F 2k\n(19)\nfor the solution \u2126\u2217, where \u03b1 = 1 for the constant step size setting and \u03b1 = c for the backtracking step size setting. Theorem 3.3. ([Beck and Teboulle, 2009, Theorem 4.4]). For the sequences {\u2126(k)}, {\u0398(k)} generated by Algorithm 2, for any k \u2265 1,\nQcon(\u2126 (k))\u2212Qcon(\u2126\u2217) \u2264 2\u03b1L\u2016\u2126(0) \u2212 \u2126\u2217\u20162F (k + 1)2\n(20)\nfor the solution \u2126\u2217, where \u03b1 = 1 for the constant step size setting and \u03b1 = c for the backtracking step size setting.\nHence, CONCORD-ISTA and CONCORD-FISTA converge at the rates of O(1/k) and O(1/k2) for the kth iteration."}, {"heading": "4 Implementation & Numerical Experiments", "text": "In this section, we outline algorithm implementation details and present results of our comprehensive numerical evaluation. Section 4.1 gives performance comparisons from using synthetic multivariate Gaussian datasets. These datasets are generated from a wide range of sample sizes (n) and dimensionality (p). Additionally, convergence of CONCORDISTA and CONCORD-FISTA will be illustrated. Section 4.2 has timing results from analyzing a real breast cancer dataset with outliers. Comparisons are made to the coordinate-wise CONCORD implementation in gconcord package for R available at http://cran.r-project.org/web/packages/gconcord/.\nFor implementing the proposed algorithms, we can take advantage of existing linear algebra libraries. Most of the numerical computations in Algorithms 1 and 2 are linear algebra operations, and, unlike the sequential coordinatewise CONCORD algorithm, CONCORD-ISTA and CONCORD-FISTA implementations can solve increasingly larger problems as more and more scalable and efficient linear algebra libraries are made available. For this work, we opted to using Eigen library [Guennebaud, Jacob, et al., 2010] for its sparse linear algebra routines written in C++. Algorithms 1 and 2 were also written in C++ then interfaced to R for testing. Table 1 gives names for various CONCORD-ISTA and CONCORD-FISTA versions using different initial step size choices."}, {"heading": "4.1 Synthetic Datasets", "text": "Synthetic datasets were generated from true sparse positive random \u2126 matrices of three sizes: p = {1000, 3000, 5000}. Instances of random matrices used here consist of 4995, 14985 and 24975 non-zeros, corresponding to 1%, 0.33% and 0.20% edge densities, respectively. For each p, three random samples of sizes n = {0.25p, 0.75p, 1.25p} were used as inputs. The initial guess, \u2126(0), and the convergence criteria was matched to those of coordinate-wise CONCORD implementation. Highlights of the results are summarized below, and the complete set of comparisons are given in Supplementary materials Section A.\nFor synthetic datasets, our experiments indicate that two variations of the CONCORD-ISTA method show little performance difference. However, ccista 0 was marginally faster in our tests. On the other hand, ccfista 1 variation of CONCORD-FISTA that uses \u03c4(k+1,0) = \u03c4k as initial step size was significantly faster than ccfista 0. Table 2 gives actual running times for the two best performing algorithms, ccista 0 and ccfista 1, against the coordinate-wise concord. As p and n increase ccista 0 performs very well. For smaller n and \u03bb, coordinate-wise concord performs well (more in Supplemental section A). This can be attributed to min(O(np2),O(p3)) computational complexity of coordinate-wise CONCORD [Khare et al., 2014], and the sparse linear algebra routines used in CONCORD-ISTA and CONCORD-FISTA implementations slowing down as the number of non-zero elements in \u2126 increases. On the other hand, for large n fraction (n = 1.25p), the proposed methods ccista 0 and ccfista 1 are significantly faster than coordinate-wise concord. In particular, when p = 5000 and n = 6250, the speed-up of ccista 0 can be as much as 150 times over coordinate-wise concord.\nConvergence behavior of CONCORD-ISTA and CONCORD-FISTA methods is shown in Figure 1. The best performing algorithms ccista 0 and ccfista 1 are shown. The vertical axis is the subgradient \u2206subg (See Algorithms 1, 2). Plots show that ccista 0 seems to converge at a constant rate much faster than ccfista 1 that appears to slow\ndown after a few initial iterations. While the theoretical convergence results from section 3 prove convergence rates of O(1/k) and O(1/k2) for CONCORD-ISTA and CONCORD-FISTA, in practice, ccista 0 with constant step size performed the fastest for the tests in this section."}, {"heading": "4.2 Real Data", "text": "Real datasets arising from various physical and biological sciences often are not multivariate Gaussian and can have outliers. Hence, convergence characteristic may be different on such datasets. In this section, the performance of proposed methods are assessed on a breast cancer dataset [Chang et al., 2005]. This dataset contains expression levels of 24481 genes on 266 patients with breast cancer. Following the approach in Khare et al. Khare et al. [2014], the number of genes are reduced by utilizing clinical information that is provided together with the microarray expression dataset. In particular, survival analysis via univariate Cox regression with patient survival times is used to select a subset of genes closely associated with breast cancer. A choice of p-value < 0.03 yields a reduced dataset with p = 4433 genes.\nOften times, graphical model selection algorithms are applied in a non-Gaussian and n p setting such as the case here. In this n p setting, coordinate-wise CONCORD algorithm is especially fast due to its computational complexityO(np2). However, even in this setting, the newly proposed methods ccista 0, ccista 1, and ccfista 1 perform competitively to, or often better than, concord as illustrated in Table 3. On this real dataset, ccista 1 performed the fastest whereas ccista 0 was the fastest on synthetic datasets."}, {"heading": "5 Conclusion", "text": "The Gaussian graphical model estimation or inverse covariance estimation has seen tremendous advances in the past few years. In this paper we propose using proximal gradient methods to solve the general non-Gaussian sparse inverse covariance estimation problem. Rates of convergence were established for the CONCORD-ISTA and CONCORDFISTA algorithms. Coordinate-wise minimization has been the standard approach to this problem thus far, and we provide numerical results comparing CONCORD-ISTA/FISTA and coordinate-wise minimization. We demonstrate that CONCORD-ISTA outperforms coordinate-wise in general, and in high dimensional settings CONCORD-ISTA can outperform coordinate-wise optimization by orders of magnitude. The methodology is also tested on real data sets. We undertake a comprehensive treatment of the problem by also examining the dual formulation and consider methods to maximize the dual objective. We note that efforts similar to ours for the Gaussian case has appeared in not one, but several NIPS and other publications. Our approach on the other hand gives a complete and thorough treatment of the non-Gaussian partial correlation graph estimation problem, all in this one self-contained paper."}, {"heading": "A Timing comparison", "text": "A.1 Median Speed-up"}, {"heading": "1000 250 0.6 ( 0.7) 0.4 ( 0.3)", "text": ""}, {"heading": "1000 750 3.4 ( 1.8) 1.9 ( 0.9)", "text": ""}, {"heading": "1000 1250 23.1 ( 5.7) 12.0 ( 3.5)", "text": ""}, {"heading": "3000 750 2.7 ( 2.1) 1.9 ( 1.6)", "text": ""}, {"heading": "3000 2250 12.8 ( 1.6) 8.8 ( 2.2)", "text": ""}, {"heading": "3000 3750 81.9 ( 6.6) 58.2 ( 8.7)", "text": ""}, {"heading": "5000 1250 5.6 ( 3.2) 3.0 ( 1.8)", "text": ""}, {"heading": "5000 3750 21.1 ( 2.6) 13.5 ( 2.6)", "text": ""}, {"heading": "5000 6250 145.8 ( 6.6) 110.1 (16.4)", "text": "A.2 Comparison among CONCORD-ISTA and CONCORD-FISTA variations\nA.3 Comparison with CONCORD algorithm\nA.4 Running times"}, {"heading": "23 1000 1250 0.163 0.23 9 43.84 13 1.25 23 2.75", "text": ""}, {"heading": "22 1000 1250 0.103 0.44 9 44.16 15 1.93 24 3.02", "text": ""}, {"heading": "21 1000 1250 0.077 0.97 9 40.50 15 1.65 24 3.34", "text": ""}, {"heading": "20 1000 1250 0.066 2.03 9 44.15 14 1.79 24 4.09", "text": ""}, {"heading": "19 1000 1250 0.061 2.91 9 43.84 16 2.36 24 5.38", "text": ""}, {"heading": "18 1000 1250 0.059 3.43 9 44.25 16 2.49 24 5.00", "text": ""}, {"heading": "17 1000 1250 0.058 3.69 9 44.21 15 2.54 24 5.29", "text": ""}, {"heading": "16 1000 750 0.300 0.04 8 6.96 13 1.13 18 2.20", "text": ""}, {"heading": "15 1000 750 0.163 0.23 9 8.00 15 1.57 24 2.80", "text": ""}, {"heading": "14 1000 750 0.103 0.76 9 8.40 15 1.58 24 3.26", "text": ""}, {"heading": "13 1000 750 0.077 3.09 9 8.37 16 3.53 25 4.84", "text": ""}, {"heading": "12 1000 750 0.066 5.86 10 10.45 20 4.01 27 6.96", "text": ""}, {"heading": "11 1000 750 0.061 7.64 10 9.97 20 5.41 28 7.96", "text": ""}, {"heading": "10 1000 750 0.059 8.56 10 9.86 20 5.19 28 9.86", "text": ""}, {"heading": "9 1000 750 0.058 8.99 10 9.96 20 4.56 28 12.44", "text": ""}, {"heading": "8 1000 250 0.300 0.05 9 2.58 15 1.23 23 2.67", "text": ""}, {"heading": "7 1000 250 0.163 0.99 9 2.61 18 1.98 26 3.31", "text": ""}, {"heading": "71 5000 6250 0.163 0.04 16 14787.78 26 97.33 34 173.66", "text": ""}, {"heading": "70 5000 6250 0.103 0.08 17 15600.83 26 112.48 33 144.42", "text": ""}, {"heading": "69 5000 6250 0.077 0.10 17 15671.14 27 101.03 25 123.92", "text": ""}, {"heading": "68 5000 6250 0.066 0.11 17 16220.33 27 111.75 25 129.70", "text": ""}, {"heading": "67 5000 6250 0.061 0.11 17 15698.53 27 103.06 25 132.57", "text": ""}, {"heading": "66 5000 6250 0.059 0.12 17 16221.44 27 115.35 25 130.19", "text": ""}, {"heading": "65 5000 6250 0.058 0.12 17 15698.02 27 113.65 25 150.95", "text": ""}, {"heading": "64 5000 3750 0.300 0.01 14 1767.63 24 78.77 30 117.03", "text": ""}, {"heading": "63 5000 3750 0.163 0.04 16 2021.49 25 82.88 33 133.36", "text": ""}, {"heading": "62 5000 3750 0.103 0.08 16 1780.97 26 88.29 32 141.14", "text": ""}, {"heading": "61 5000 3750 0.077 0.10 17 2094.73 29 95.84 33 178.13", "text": ""}, {"heading": "60 5000 3750 0.066 0.11 17 2183.90 29 98.54 25 121.39", "text": ""}, {"heading": "59 5000 3750 0.061 0.13 17 1967.39 29 114.72 35 186.34", "text": ""}, {"heading": "58 5000 3750 0.059 0.13 17 1965.36 29 111.53 35 189.05", "text": ""}, {"heading": "57 5000 3750 0.058 0.14 17 2324.54 29 99.50 35 165.12", "text": ""}, {"heading": "56 5000 1250 0.300 0.01 14 626.20 25 69.71 30 105.65", "text": ""}, {"heading": "55 5000 1250 0.163 0.05 16 719.81 25 71.23 34 147.53", "text": ""}, {"heading": "54 5000 1250 0.103 0.10 17 667.62 27 81.21 33 163.00", "text": ""}, {"heading": "53 5000 1250 0.077 0.53 17 674.71 30 121.39 35 265.84", "text": ""}, {"heading": "52 5000 1250 0.066 1.42 17 832.68 32 193.88 37 379.23", "text": ""}, {"heading": "51 5000 1250 0.061 2.13 18 892.30 36 272.03 40 604.35", "text": ""}, {"heading": "50 5000 1250 0.059 2.52 18 903.05 37 393.77 40 681.49", "text": ""}, {"heading": "49 5000 1250 0.058 2.71 18 757.67 38 408.49 40 547.93", "text": "A.5 Other Methods\nA.5.1 Dual problem of CONCORD\nFormulating the dual using the matrix form is challenging since the KKT conditions involving the gradient term S\u2126 + \u2126S do not have a closed form solution as in the case of Gaussian problem in Dalal and Rajaratnam [2014]. Therefore, we consider a vector form of the CONCORD problem by defining two new variables x1 \u2208 Rp and x2 \u2208 Rp(p\u22121)/2 as\nx1 = (\u03c911, \u03c922, . . . , \u03c9pp) T\nx2 = (\u03c912, \u03c913, . . . , \u03c91p, \u03c923, . . . , \u03c92p, . . . , \u03c9p\u22121p) T . (21)\nWe define two coefficient matrices A1, A2 as\nA1 =  Y1 Y2 . . .\nYp\n , A2 =  Y2 Y3 \u00b7 \u00b7 \u00b7 Yp Y1 Y1 . . .\nY1\nY3 \u00b7 \u00b7 \u00b7 Yp Y2\n. . . Y2\n. . . Yp\u22121 Yp Yp\u22122\nYp\u22122 Yp Yp\u22121  , (22)\nwhere A1np\u00d7p and A2np\u00d7p(p\u22121)/2 dimensional matrices. Using these definitions, the CONCORD problem (4) can be rewritten as\nminimize x1,x2\n\u2212 n log x1 + 1\n2 \u2225\u2225A1x1 +A2x2\u2225\u22252 + \u03bb\u2016x2\u20161. (23) where, log(x1) = \u2211i=p i=1 log(x1i). We will use x = [ x1 x2 ] for simplicity of notation where ever possible.\nThe transformed CONCORD problem in (23) can be written in composite form using a new variable z = A1x1+A2x2 as\nminimize x1,x2,z\n\u2212 n log x1 + 1\n2 \u2225\u2225z\u2225\u22252 + \u03bb\u2016x2\u20161 subject to A1x1 +A2x2 = z (24)\nThe Lagrangian for this problem is given by\nL(x1, x2, z, y) = \u2212n log x1 + 1\n2 \u2225\u2225z\u2225\u22252 + \u03bb\u2016x2\u20161 + yT (A1x1 +A2x2 \u2212 z) . (25) Maximizing with respect to the three primal variables yields following optimality conditions (the . notation is adapted from MATLAB to denote element-wise operations),\nz \u2212 y = 0 \u2212n./x1 +A1T y = 0\n\u03bbsign(x2) +A2T y 3 0. (26)\nSubstituting these the dual problem can be written as\nmaximize y\n\u2212 n log n./A1T y + 1\n2 \u2225\u2225y\u2225\u22252 + yT (A1(n./A1T y)\u2212 y) subject to \u2016A2T y\u2016\u221e \u2264 \u03bb,\nor equivalently\nmaximize y\n1\n2 \u2225\u2225y\u2225\u22252 \u2212 n log (A1T y) + c subject to \u2016A2T y\u2016\u221e \u2264 \u03bb, (27)\nwhere, c = n log n\u2212 n2 is a constant. This problem can also be written in composite form as\nmaximize y\n1\n2 \u2225\u2225y\u2225\u22252 \u2212 n log (A1T y) + 1\u2016w\u2016\u221e\u2264\u03bb subject to A2T y \u2212 w = 0. (28)\nThe gradient and hessian of the smooth function h(y) = 12 \u2225\u2225y\u2225\u22252 \u2212 n log (A1T y) is given by\n\u2207h(y) = y \u2212A1(n./A1T y), \u22072h(y) = I +A1diag ( n./(A1 T y)2 ) A1\nT . (29) Here, the hessian is bounded away from the semi-definite boundary. Hence the function h is strongly convex with parameter 1. Moreover, on lines of Theorem 3.1, we can show that if y is restricted to a convex level set C = {y|h(y) \u2264M} for some constant M , then the function h has a Lipschitz continuous gradient. Note that\n\u2212n log (A1T y) \u2264 h(y) \u2264M\ne\u2212 M n \u2264 A1T y. (30)\nTherefore, the hessian satisfies \u22072h(y) = I +A1diag ( n./(A1 T y)2 ) A1 T (1 + n\u03c1(A1TA1)e 2M n )I. (31)\nTo conclude, the dual problem provides an alternate method to prove the O( 1k ) and O( 1 k2 ) rates of convergence for CONCORD problem.\nA.5.2 Proximal Newton\u2019s Algorithm for CONCORD\nRecall that the hessian of the smooth function h1 as given in 6 is\n\u22072h1(\u2126) = i=p\u2211 i=1 \u03c9\u22122ii [ eiei T \u2297 eieiT ] + 1 2 (S \u2297 I + I \u2297 S) .\nThe subproblem solved for the direction of descent for the second order PNOPT algorithm is given by\n\u2206\u2126(k) = arg min W \u3008G(k),W \u3009+ 1 2 i=p\u2211 i=1 \u03c9\u22122ii tr ( Weiei TWeiei T ) + tr (WSW ) + \u03bb\u2016\u2126X(k) +W\u20161. (32)\nUsing these, the matrix version of the second order algorithm is given in Algorithm 3. Here, the subproblem for the descent step is as a huge Lasso problem. This can be solved by standard Lasso packages which uses coordinate descent methods.\nAlgorithm 3 CONCORD - Proximal Newton Optimization Matrix form (CONCORD-PNOPT)\nInitialize: \u2126(0) \u2208 Sp+, \u03c4(0,0) = 1,\u2206opt = 2 opt and \u2206term = 2 term while \u2206subg > subg or \u2206term > term do\nCompute \u2207h1: G(k) = \u2126D \u22121 + 12 ( S \u2126(k) T + \u2126(k)S ) Compute Newton step:\n\u2206\u2126(k) = arg min W \u3008G(k),W \u3009+ 1 2 i=p\u2211 i=1 \u03c9\u22122ii tr ( Weiei TWeiei T ) + tr (WSW ) + \u03bb\u2016\u2126X(k) +W\u20161\nCompute sufficient descent \u2206(k): \u2206(k) = \u3008G(k),\u2206\u2126(k)\u3009+ \u03bb ( \u2016\u2126X(k) + \u2206\u2126X(k)\u20161 \u2212 \u2016\u2126X(k)\u20161 ) Compute \u03c4k, such that Qcon(\u2126(k+1)) \u2264 Qcon(\u2126(k)) + \u03b1\u03c4k\u2206(k). Update: \u2126(k+1) = \u2126(k) + \u03c4k\u2206\u2126(k) Compute convergence criteria:\n\u2206subg = \u2016\u2207h(\u2126(k)) + \u2202g(\u2126(k))\u2016\n\u2016\u2126(k)\u2016 , \u2206term = \u2016f(\u2126(k+1))\u2212 f(\u2126(k))\u2016 \u2016f(\u2126(k))\u2016\nend while"}], "references": [{"title": "DAspremont. Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or", "author": ["Onureena Banerjee", "Laurent El Ghaoui", "Alexandre"], "venue": "Binary Data. JMLR,", "citeRegEx": "Banerjee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2008}, {"title": "G-ama: Sparse gaussian graphical model estimation via alternating minimization", "author": ["Onkar Anant Dalal", "Bala Rajaratnam"], "venue": "arXiv preprint arXiv:1405.3034,", "citeRegEx": "Dalal and Rajaratnam.,? \\Q2014\\E", "shortCiteRegEx": "Dalal and Rajaratnam.", "year": 2014}, {"title": "Partial Correlation Estimation by Joint Sparse Regression Models", "author": ["Jie Peng", "Pei Wang", "Nengfeng Zhou", "Ji Zhu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "A path following algorithm for Sparse Pseudo-Likelihood Inverse Covariance Estimation (SPLICE)", "author": ["Guilherme V Rocha", "Peng Zhao", "Bin Yu"], "venue": "Technical Report 60628102,", "citeRegEx": "Rocha et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rocha et al\\.", "year": 2008}, {"title": "Applications of the lasso and grouped lasso to the estimation of sparse graphical models", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": "Technical report,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "A convex pseudo-likelihood framework for high dimensional partial correlation estimation with convergence guarantees", "author": ["Kshitij Khare", "Sang-Yun Oh", "Bala Rajaratnam"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Khare et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Khare et al\\.", "year": 2014}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Monotone operators and the proximal point algorithm", "author": ["R.T. Rockafellar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Rockafellar.,? \\Q1976\\E", "shortCiteRegEx": "Rockafellar.", "year": 1976}, {"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J.M. Borwein"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Barzilai and Borwein.,? \\Q1988\\E", "shortCiteRegEx": "Barzilai and Borwein.", "year": 1988}, {"title": "Robustness, scalability, and integration of a wound-response gene expression signature in predicting breast cancer survival", "author": ["Howard Y Chang", "Dimitry S A Nuyten", "Julie B Sneddon", "Trevor Hastie", "Robert Tibshirani", "Therese S\u00f8 rlie", "Hongyue Dai", "Yudong D He", "Laura J van\u2019t Veer", "Harry Bartelink", "Matt van de Rijn", "Patrick O Brown", "Marc J van de Vijver"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Chang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "The literature on this topic is simply enormous and for the sake of brevity, space constraints and the topic of this paper, we avoid an extensive literature review by referring to the references in the seminal work of Banerjee et al. [2008] and the very recent work of Dalal and Rajaratnam [2014].", "startOffset": 218, "endOffset": 241}, {"referenceID": 0, "context": "The literature on this topic is simply enormous and for the sake of brevity, space constraints and the topic of this paper, we avoid an extensive literature review by referring to the references in the seminal work of Banerjee et al. [2008] and the very recent work of Dalal and Rajaratnam [2014]. These two papers contain references to recent work, including past NIPS conference proceedings.", "startOffset": 218, "endOffset": 297}, {"referenceID": 2, "context": "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al.", "startOffset": 101, "endOffset": 120}, {"referenceID": 2, "context": "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al.", "startOffset": 101, "endOffset": 151}, {"referenceID": 2, "context": "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al. [2010]. These approaches are either not convex, and/or convergence of corresponding maximization algorithms are not established.", "startOffset": 101, "endOffset": 184}, {"referenceID": 2, "context": "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al. [2010]. These approaches are either not convex, and/or convergence of corresponding maximization algorithms are not established. In this sense, non-Gaussian partial correlation graph estimation methods have lagged severely behind, despite the tremendous need to move beyond the Gaussian framework for obvious practical reasons. In very recent work, a convex pseudo-likelihood approach with good model selection properties called CONCORD Khare et al. [2014] was proposed.", "startOffset": 101, "endOffset": 634}, {"referenceID": 5, "context": "(3) This coordinate-wise algorithm is shown to converge to a global minima though no rate is given [Khare et al., 2014].", "startOffset": 99, "endOffset": 119}, {"referenceID": 6, "context": "1: methods which are inspired by the iterative soft-thresholding algorithms in Beck and Teboulle [2009]. We undertake a comprehensive treatment of the CONCORD optimization problem by also investigating the dual of the CONCORD problem.", "startOffset": 79, "endOffset": 104}, {"referenceID": 6, "context": "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov\u2019s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm.", "startOffset": 109, "endOffset": 152}, {"referenceID": 6, "context": "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov\u2019s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm.", "startOffset": 109, "endOffset": 245}, {"referenceID": 6, "context": "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov\u2019s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm.", "startOffset": 109, "endOffset": 305}, {"referenceID": 6, "context": "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov\u2019s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm. The essence of the proximal gradient algorithms is to divide the objective function into a smooth part and a non-smooth part, then take a proximal step (w.r.t. the non-smooth part) in the negative gradient direction of the smooth part. Nesterov\u2019s accelerated gradient extension Nesterov [1983] uses a combination of gradient and momentum steps to achieve accelerated rates of convergence.", "startOffset": 109, "endOffset": 665}, {"referenceID": 8, "context": "This is an approximation of the secant equation which works as a proxy for second order information using successive gradients (see Barzilai and Borwein [1988] for details).", "startOffset": 132, "endOffset": 160}, {"referenceID": 5, "context": "The coordinatewise method for optimizing CONCORD in Khare et al. [2014] also requires cycling through the p entries of \u03a9 in specified order and thus does not allow parallelization.", "startOffset": 52, "endOffset": 72}, {"referenceID": 5, "context": "We would like to point out that, although the authors in Khare et al. [2014] provide a proof of convergence for their coordinate-wise minimization algorithm for CONCORD, they do not provide any rates of convergence.", "startOffset": 57, "endOffset": 77}, {"referenceID": 5, "context": "We would like to point out that, although the authors in Khare et al. [2014] provide a proof of convergence for their coordinate-wise minimization algorithm for CONCORD, they do not provide any rates of convergence. The arguments for convergence leverage the results in Beck and Teboulle [2009] but require some essential ingredients.", "startOffset": 57, "endOffset": 295}, {"referenceID": 6, "context": "The remaining argument for convergence follows from the theorems in Beck and Teboulle [2009].", "startOffset": 68, "endOffset": 93}, {"referenceID": 5, "context": "This can be attributed to min(O(np),O(p)) computational complexity of coordinate-wise CONCORD [Khare et al., 2014], and the sparse linear algebra routines used in CONCORD-ISTA and CONCORD-FISTA implementations slowing down as the number of non-zero elements in \u03a9 increases.", "startOffset": 94, "endOffset": 114}, {"referenceID": 9, "context": "In this section, the performance of proposed methods are assessed on a breast cancer dataset [Chang et al., 2005].", "startOffset": 93, "endOffset": 113}, {"referenceID": 5, "context": "Following the approach in Khare et al. Khare et al. [2014], the number of genes are reduced by utilizing clinical information that is provided together with the microarray expression dataset.", "startOffset": 26, "endOffset": 59}], "year": 2014, "abstractText": "Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of `1-penalized estimation in the Gaussian framework. Though many of these inverse covariance estimation approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing `1-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous payoffs for `1-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.", "creator": "LaTeX with hyperref package"}}}