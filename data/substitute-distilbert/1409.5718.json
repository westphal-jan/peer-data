{"id": "1409.5718", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2014", "title": "Convolutional Neural Networks over Tree Structures for Programming Language Processing", "abstract": "deep neural networks have made unprecedented breakthroughs in parallel fields of artificial intelligence. however, it has not been applied in the field approaching programming language processing. in this paper, specialists propose the tree - based artificial neural network ( tbcnn ) to model programming languages, which contain rich and explicit program interface information. in our model, program vector representations are closely underlying the \" coding \" pretraining criterion based on abstract syntax trees ( asts ) ; mathematical convolutional layer explicitly captures neighboring constituents on the tree ; with the \" binary continuous tree \" representing \" 3 - way pooling, \" our model can deal with asts at different shapes and sizes. designers evaluate the program vector representations empirically, showing that the additive criterion successfully captures underlying features of ast nodes, and that program numerical representations significantly speed up supervised learning. we also compare tbcnn to encoding methods ; our model achieves better fidelity in the task of program classification. to our best knowledge, this unit is the first to analyze programs with deep neural networks ; results mention the scope of deep learning to the field of programming language processing. the experimental experiments validate its feasibility ; hence also show a promising future of this new research area.", "histories": [["v1", "Thu, 18 Sep 2014 06:50:52 GMT  (220kb,D)", "http://arxiv.org/abs/1409.5718v1", null], ["v2", "Tue, 8 Dec 2015 12:31:51 GMT  (310kb,D)", "http://arxiv.org/abs/1409.5718v2", "Accepted at AAAI-16"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.SE", "authors": ["lili mou", "ge li", "lu zhang 0023", "tao wang", "zhi jin"], "accepted": true, "id": "1409.5718"}, "pdf": {"name": "1409.5718.pdf", "metadata": {"source": "CRF", "title": "TBCNN: A Tree-Based Convolutional Neural Network for Programming Language Processing", "authors": ["Lili Mou", "Ge Li", "Zhi Jin", "Lu Zhang", "Tao Wang"], "emails": ["zhanglu}@sei.pku.edu.cn", "twangcat@stanford.edu"], "sections": [{"heading": "Introduction", "text": "The deep neural network, also known as deep learning, is a highly automated learning algorithm; it has become a rapid developing research area recent years. By exploring multiple layers of non-linear transformation, the deep architecture can extract underlying abstract features of data, which is crucial to artificial intelligence (AI) (Bengio 2009). Although deep learning has been successful in various fields\u2014like natural language processing (NLP) (Collobert and Weston 2008), computer vision (Krizhevsky, Sutskever, and Hinton 2012), and speech recognition (Dahl, Mohamed, and Hinton 2010)\u2014its advantages are not exploited in the field of programming language processing.\nProgramming languages, similar to natural languages, are the reflection of human thought. Programs are complex, flexible and powerful; they also contain rich statistical properties (Hindle et al. 2012). Analyzing programs with learningbased approaches is an important research topic in both\nAI and software engineering (Lu, Cukic, and Culp 2012; Canavera, Esfahani, and Malek 2012; Lee, Jung, and Pande 2014).\nDespite some similarities, there are also obvious differences between programming languages and natural languages (Pane, Ratanamahatana, and Myers 2001). Based on a formal language, programs contain rich and explicit structural information. Even though structures also exist in natural languages, they are not as stringent as programs due to the limitation of human intuition capacity. (Pinker 1994) illustrates an interesting example, \u201cThe dog the stick the fire burned beat bit the cat.\u201d This sentence complies with all grammar rules, but too many attributive clauses are nested. Hence, it can hardly be understood by human intuition due to the over-complicated structure. On the contrary, 3 nested loops are pretty common in programs. Further, the grammar rules \u201calias\u201d the neighboring relationships among program components. For example, the statements inside and outside a loop do not form one semantic group, and thus they are not semantically neighboring. Therefore, due to the different characteristics of natural languages and programming languages, new neural models should be proposed for programs to capture such structural information.\nIn this paper, we propose the novel Tree-Based Convolutional Neural Network (TBCNN) to model programming languages based on abstract syntax trees (ASTs). Our contributions include: (1) proposing the \u201ccoding criterion\u201d to learn vector representations for each node in ASTs, serving as the pretraining phase; (2) proposing tree-based convolution approach to extract local structural features of programs; (3) introducing the notions of \u201ccontinuous binary tree\u201d and \u201c3- way pooling\u201d so that our model is suitable for trees with different structures and sizes.\nIn our experiments, program vector representations are evaluated empirically. The results are consistent with human intuition, showing that our pretraining process successfully captures meaningful features of different program symbols (nodes in ASTs). TBCNN is evaluated in the task of program classification; it achieves higher accuracy compared with baseline methods. Our study validates the feasibility of neural programming language processing.\nTo our best knowledge, we are the first to analyze programs by deep neural networks. We extend the scope of deep learning to the field of programming language processing.\nar X\niv :1\n40 9.\n57 18\nv1 [\ncs .L\nG ]\n1 8\nSe p\n20 14\nBased on current evidence, we believe it will become an outstanding technique in this new field in the near future."}, {"heading": "Tree-based Convolutional Neural Network", "text": ""}, {"heading": "The Architecture", "text": "Programming languages have the natural tree representation\u2014the abstract syntax tree (AST). Figure 2 shows the AST corresponding to the following C code snippet (parsed by pycparser1):\nint a = b + 3;\nEach node in the AST is an abstract component in the program source code. A node p with children c1, \u00b7 \u00b7 \u00b7 , cn represents the constructing process of the component p \u2192 c1 \u00b7 \u00b7 \u00b7 cn.\nThe overall architecture of TBCNN is shown in Figure 1. In our model, each node in ASTs is first represented as a distributed, real-valued vector so that the (anonymous) features of the symbols are captured. The vector representations are learned by the proposed \u201ccoding criterion.\u201d\nThen we convolve a set of feature detectors on the AST and apply 3-way pooling, after which, we add a hidden layer and an output layer. For supervised classification tasks, the activation function of the output layer is softmax.\n1https://pypi.python.org/pypi/pycparser/"}, {"heading": "Representation Learning for AST Nodes", "text": "As all nodes in ASTs are discrete, they should be represented as real-valued, distributed vectors. A generic criterion for representation learning is that similar symbols have similar feature vectors (Bengio, Courville, and Vincent 2013). For example, the symbols While and For are similar because both of them are related to control flow, particularly loops. But they are different from ID, since ID probably represents some data.\nIn our scenario, we would like the children\u2019s representations to \u201ccode\u201d its parent\u2019s via a single neural layer. Formally, let vec(\u00b7) \u2208 RNf be the feature representation of a symbol. For each non-leaf node p and its direct children c1, \u00b7 \u00b7 \u00b7 , cn, we would like\nvec(p) \u2248 tanh (\u2211\ni liWcode,i \u00b7 vec(ci) + bcode\n) (1)\nwhere Wcode,i \u2208 RNf\u00d7Nf is the weight matrix corresponding to symbol ci; bcode \u2208 RNf is the bias. li = #leaves under ci #leaves under p is the coefficient of the weight. (Weights Wcode,i are weighted by leaf number.)\nBecause different nodes may have different numbers of children, the number of Wcode,i\u2019s is not fixed. To overcome the problem, we introduce the \u201ccontinuous binary tree,\u201d where only two weight matrices W lcode and W r code serve as model parameters. Wi is a linear combination of the two parameter matrices. The details will be explained in the last part of this section.\nThe closeness between vec(p) and its coded vector is measured by Euclidean distance square, i.e.,\nd = \u2225\u2225\u2225vec(p)\u2212 tanh(\u2211\ni liWcode,i \u00b7 vec(ci) + bcode )\u2225\u2225\u22252 2\nTo prevent the pretraining algorithm from learning trivial representations (e.g., 0\u2019s will give 0 distance but is meaningless), negative sampling is applied like (Collobert et al. 2011). For each pretraining data sample p, c1, \u00b7 \u00b7 \u00b7 , cn, we substitute one symbol (either p or one of c\u2019s) with a random symbol. The distance of the negative sample is denoted as dc. We would like dc to be at least larger than that of the\npositive training sample plus a margin \u2206 (set to 1 in our experimental setting). Then the pretraining objective is to\nminimize W lcode,W r code,bcode\nmax {0,\u2206 + d\u2212 dc}"}, {"heading": "Coding Layer", "text": "Having pretrained the feature vectors for every symbol, we feed them forwardly to the tree-based convolutional layer. For leaves, they are just the vector representations learned in the pretraining phase. For a non-leaf node p, it has two representations: the one learned in the pretraining phase (lefthand side of Formula 1), and the coded one (right-hand side of Formula 1). They are linearly combined before being fed to the convolutional layer. Let c1, \u00b7 \u00b7 \u00b7 , cn be the children of node p and we denote the combined vector as p. We have\np = Wcomb1 \u00b7 vec(p) +Wcomb2 \u00b7 tanh (\u2211\ni liWcode,i \u00b7 vec(xi) + bcode ) where Wcomb1,Wcomb2 \u2208 RNf\u00d7Nf are the parameters for combination. They are initialized as diagonal matrices and then fine-tuned during supervised training."}, {"heading": "Tree-based Convolution Layer", "text": "Now that each symbol in ASTs is represented as a distributed, real-valued vector x \u2208 RNf , we apply a set of fixed-depth local feature detectors sliding over the entire tree, as is illustrated in Figure 3. This can be viewed as convolution with a set of finite support kernels. We call this \u201ctree-based convolution.\u201d\nFormally, in a fixed-depth window, if there are n nodes with vector representations x1, \u00b7 \u00b7 \u00b7 ,xn, then the output of the set of feature detectors is\ny = tanh\n( n\u2211\ni=1\nWconv,i \u00b7 xi + bconv ) where y, bconv \u2208 RNc and Wconv,i \u2208 RNc\u00d7Nf . (Nc is the number of feature detectors/convolution kernels.) 0\u2019s are\npadded for nodes at the bottom that do not have as many layers as the feature detectors. In our experiments, the depth of convolution kernel is set to 2.\nNote that, to deal with variable numbers of children, we also adopt the \u201ccontinuous binary tree.\u201d In this scenario, there are three weight matrices serving as model parameters, namely W tconv, W l conv and W r conv. Wconv,i is a linear combination of these three weight matrices (explained in the last part of this section).\n3-Way Pooling Layer After convolution, local neighboring features in the AST are extracted, and a new tree is generated. The new tree has exactly the same shape and size as the original one, which is also varying among different programs. Therefore, they cannot directly be fed to a fixed-size hidden layer.\nWe propose 3-way max pooling, which pools the features into 3 parts: TOP, LOWER LEFT and LOWER RIGHT according to the position in AST (Figure 4). The following strategies are used during the pooling process.\n\u2022 The nodes with depth less than k * average depth of the leaves are pooled to TOP. (k is set to 0.6 in our setting.)\n\u2022 Lower nodes are pooled to the LOWER LEFT and LOWER RIGHT according to their relative position regarding the root node.\nWith such 3-way pooling, local structural features along the entire AST can reach the output layer with short pathes. Hence, these structural features can be trained effectively by back propagation.\nAfter pooling, the features are fully connected to a hidden layer and then fed to the output layer (softmax) for supervised classification."}, {"heading": "The \u201cContinuous Binary Tree\u201d Model", "text": "As stated in previous subsections, during coding or convolving, one problem is that we cannot determine the number of weight matrices because AST nodes have variable numbers of children.\nOne possible solution is the \u201ccontinuous bag of words\u201d model (Mikolov et al. 2013)2, but position information will be lost completely. Similar approach is used in (Hermann and Blunsom 2014). In (Socher et al. 2013a), a different weight matrix is allocated as parameters for each position. This method fails to scale up since there will be a huge number of different positions in our scenario.\nIn our model, we view any subtree as a \u201cbinary\u201d tree, regardless of its size and shape. That is, we have only 3 weight matrices for convolution and 2 for coding. We call it a \u201ccontinuous binary tree.\u201d\nTake convolution as an example. The three parameter matrices are W tconv, W l conv and W r conv. (Superscripts t, l, r refers to \u201ctop,\u201d \u201cleft\u201d and \u201cright.\u201d) For node xi in the window, its weights for convolution Wconv,i is a linear combination of W tconv,W l conv, andW r conv with coefficients \u03b7 t i , \u03b7 l i, and \u03b7 r i . The coefficients are computed according to the relative position of a node in the sliding window. Figure 5 is an analogy to the continuous binary tree model. The formulas for computing \u03b7\u2019s are listed as follows.\n\u2022 \u03b7ti = di \u2212 1 d\u2212 1 , where di is the depth of the node i in the\nsliding window and d is the depth of the window.\n\u2022 \u03b7ri = (1\u2212\u03b7ti) pi \u2212 1 n\u2212 1 , where pi is the position of the node,\nand n is the total number of p\u2019s children.\n\u2022 \u03b7li = (1\u2212 \u03b7ti)(1\u2212 \u03b7ri ) Likewise, the continuous binary tree for coding has two weight matrices W lcode and W r code. The details are not repeated here.\nTo sum up, the entire parameter set for TBCNN is \u0398 =( W lcode,W r code,Wcomb1,Wcomb2,W t conv,W l conv,W r conv,Whid,\nWout, bcode, bconv, bhid, bout, vec(\u00b7) )\n, where Whid, Wout, bhid, bout are the weights and biases for the hidden and output layer. The number of hidden layer neurons is denoted asNh. To set up supervised training, W lcode,W r code, bcode, vec(\u00b7) are derived from the pretraining phase; Wcomb1 and Wcomb2 are initialized as diagonal matrices; other parameters are initialized randomly. We add `2 regularization toW \u2019s. The parameters are trained supervisedly in the program classification task using standard backpropagation algorithm by stochastic gradient descent with momentum.\n2In their original paper, they do not deal with variable length data, but their method extends naturally to this scenario. Their method is also mathematically equivalent to dynamic pooling."}, {"heading": "Experimental Results", "text": "In this section, we present two experimental results. First, we evaluate the program vector representations empirically by hierarchical clustering to show the vector representations successfully capture meaningful features of AST nodes. Second, we apply TBCNN to classify programs based on functionality to verify the feasibility and effectiveness of neural programming language processing."}, {"heading": "The Dataset", "text": "We use the dataset of a pedagogical programming open judge (OJ) system3. There are a large number of programming problems on the OJ system. Students submit their source codes as the solution to certain problems; the OJ system judges the validity of submitted source codes automatically. We download the source codes and the corresponding problem IDs (labels) as our dataset. The representation learning is performed over all C codes. In the task of program classification, two groups of programming problems are selected for classification. Each group contains 4 problems, which are similar in functionality4. We split the dataset into 3 parts: 60% for training, 20% for cross-validating (CV) and 20% for testing.\nThe results, source codes, and dataset are available from our project website5."}, {"heading": "Evaluation of Vector Representations", "text": "We perform hierarchical clustering to evaluate the learned representations for AST nodes. The result confirms that similar symbols tend to have similar feature vectors. Figure 6 illustrates a subset of AST nodes. (The entire result and the original vector representations can be downloaded at our project website.)\nAs demonstrated in Figure 6, the symbols mainly fall into three categories: (1) BinaryOp, ArrayRef, ID, Constant are grouped together since they are related\n3The URL of the OJ system is anonymized for peer reviewing. The dataset can be downloaded at our project URL.\n4This selected dataset prevents our classification task from being trivial. For randomly chosen problems, it is likely to achieve very high accuracy with simple approaches.\n5https://sites.google.com/site/treebasedcnn/"}, {"heading": "Training cost (random initialization)", "text": "to data reference/manipulating; (2) For, If, While, DoWhile are similar since they are related to control flow; (3) ArrayDecl, FuncDecl, PtrDecl are similar since they are related to declarations. The result is quite meaningful because it is consistent with human understanding of programs.\nAnother evaluation for representation learning is to see whether it improves supervised learning of interest. We perform program classification, and plot in Figure 7 the training and cross-validating (CV) learning curves of first 40 epochs (iterations over all training examples) for both random initialized and pretrained weights. For the sake of simplicity and fairness, the hyperparameters are set as Nf = Nc = Nh = 30; `2 penalty = 0; momentum= 0; learning rate is set to 0.03 and fixed. These settings are selected manually in advance, different from the settings for final classification (they are chosen by CV).\nAs we can see from Figure 7, after a plateaux of 15 epochs, the cost functions are going down significantly in the setting with pretrained parameters. However, if we randomly initialize the weights, training becomes extremely more slow and ineffective\u2014the decrease of cost function in first 40 epochs is less than 4 digits after the decimal point. The experiment shows that pretraining speeds up the training process to a large extents. The details of the supervised learning results will be presented in the next subsection."}, {"heading": "Evaluation of the TBCNN model", "text": "To evaluate the feasibility and effectiveness of neural programming language processing, we apply TBCNN to classify programs. The results are presented in Table 16.\nWe compare our approach to baseline methods, namely logistic regression (LR) and SVM with linear and radial basis function (RBF) kernels. These methods adopt the bag-ofwords model and use counting features, i.e., the feature vector of a program is the numbers of symbol occurrences. Linear classifiers (logistic regression and linear SVM) achieve\n6Part of the result is first reported in (Mou et al. 2014).\napproximately the same accuracy. SVM with RBF kernel is better than linear classifiers. We also apply the existing recursive neural networks (RNN, Socher et al. 2013b) to the program classification task. But it is not trained effectively in our scenario. The cost function is roughly\u2212 log(0.25) \u2248 1.4 during our whole training process, where 0.25 is the probability of each (similar to the blue/cyan dashed lines in Figure 7). RNN merely learns a distribution over labels, and no useful information is effectively captured by RNN. We compare the models and analyze the results in the next section.\nBy exploring the program features automatically with TBCNN, we achieve better accuracy than baseline methods in both two groups. According to model design, TBCNN captures local structural information by convolution, but it loses counting information because of max pooling. On the other hand, bag-of-words (BOW) model contains counting information, but the structural information is lost. When we combine these two models, we achieve highest accuracy in both groups. This confirms that, TBCNN and BOW capture different aspects of programs\u2014local structural features and counting features, both of which are beneficial for program classification."}, {"heading": "Related Work in Deep Learning", "text": "Deep neural networks have made significant breakthroughs in many fields of artificial intelligence, for example, computer vision (Krizhevsky, Sutskever, and Hinton 2012), speech recognition (Dahl, Mohamed, and Hinton 2010), natural language processing (Collobert et al. 2011), etc. Stacked restricted Boltzmann machines and autoencoders are successful pretraining methods (Hinton, Osindero, and Teh 2006; Bengio et al. 2007). They explore the underlying features of data unsupervisedly, and give a more meaningful initialization of weights for later supervised learning. These approaches work well with generic data, but they are not suitable for programming language processing, because programs contain rich structural information. Further, AST structures also vary largely among different programs, and hence they cannot be fed directly to a fixed-size network.\nTo capture explicit structures of data, it may be important and beneficial to integrate human priors to the net-\nworks (Bengio, Courville, and Vincent 2013). One example is the convolutional neural network (CNN, LeCun et al. 1995), which specifies spatial neighboring information in data. CNN works with data in k-dimensional space; but it fails to capture tree-structural information as in programs. Another example is the recurrent neural network, which can be regarded as a time-decaying network (Mikolov et al. 2010). Hence, it is typically suitable for one-dimensional data, but structural information is also lost.\nA model similar to ours is the recursive neural network (RNN) proposed in (Socher et al. 2011; 2013b) for NLP. Although structural information may be coded to some extent in RNN, the major drawback is that only root features are used for supervised learning. As we have seen, RNN is not trained effectively in our setting for the program classification task. We analyze the results and consider the following as main causes: (1) Different pretraining criteria. By the coding criterion, useful local structures are buried under uninformative high level program components (e.g. root, function call). (2) Weak information interaction. The root features are the bottleneck of RNN. Lower layer features have long-path dependency to the output layer through the bottleneck, which adds to the difficulty of training RNN in our scenario.\nDifferent from the above models, TBCNN explores local structural features by tree-based convolution; these features are divided into three regions and pooled for supervised learning. Based on the experiments, we think RNN and TBCNN are complementary to each other, each suitable for different scenarios."}, {"heading": "Conclusion", "text": "In this paper, we extent the scope of deep learning to programming language processing. Due to the rich and explicit tree structures of programs, we propose the novel Tree-Based Convolutional Neural Network (TBCNN). In this model, program vector representations are learned by the coding criterion; local structural features are detected by the convolution layer; the notions of continuous binary tree and 3-way pooling are introduced to model trees with varying sizes and shapes.\nEmpirical experiments show that meaningful features of AST nodes are successfully captured by the coding criterion. The TBCNN model is evaluated in the task of program classification; it achieves higher accuracy than baseline methods.\nThe experiments validate the feasibility of neural program processing; they also show a bright future of this new field. Based on current evidence, we believe deep learning will make great progress in the field of programming language processing."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning 2(1):1\u2013127.", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Mining the execution history of a software system to infer the best time for its adaptation", "author": ["K. Canavera", "N. Esfahani", "S. Malek"], "venue": "Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering.", "citeRegEx": "Canavera et al\\.,? 2012", "shortCiteRegEx": "Canavera et al\\.", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine learning.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Phone recognition with the mean-covariance restricted Boltzmann machine", "author": ["G. Dahl", "A. Mohamed", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Dahl et al\\.,? 2010", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K. Hermann", "P. Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hermann and Blunsom,? 2014", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "On the naturalness of software", "author": ["A. Hindle", "E. Barr", "Z. Su", "M. Gabel", "P. Devanbu"], "venue": "Proceedings of 34th International Conference on Software Engineering.", "citeRegEx": "Hindle et al\\.,? 2012", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger"], "venue": "Proceedings of International Conference on Artificial Neural Networks.", "citeRegEx": "LeCun et al\\.,? 1995", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Detecting memory leaks through introspective dynamic behavior modelling using machine learning", "author": ["S. Lee", "C. Jung", "S. Pande"], "venue": "Proceedings of 36th International Conference on Software Engineering.", "citeRegEx": "Lee et al\\.,? 2014", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Software defect prediction using semi-supervised learning with dimension reduction", "author": ["H. Lu", "B. Cukic", "M. Culp"], "venue": "Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering.", "citeRegEx": "Lu et al\\.,? 2012", "shortCiteRegEx": "Lu et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Building program vector representations for deep learning", "author": ["L. Mou", "G. Li", "Y. Liu", "H. Peng", "Z. Jin", "Y. Xu", "L. Zhang"], "venue": "arXiv preprint arXiv:1409.3358.", "citeRegEx": "Mou et al\\.,? 2014", "shortCiteRegEx": "Mou et al\\.", "year": 2014}, {"title": "Studying the language and structure in non-programmers\u2019 solutions to programming problems", "author": ["J. Pane", "C. Ratanamahatana", "B. Myers"], "venue": "International Journal of HumanComputer Studies 54(2):237\u2013264.", "citeRegEx": "Pane et al\\.,? 2001", "shortCiteRegEx": "Pane et al\\.", "year": 2001}, {"title": "The Language Instinct: The New Science of Language and Mind", "author": ["S. Pinker"], "venue": "Pengiun Press.", "citeRegEx": "Pinker,? 1994", "shortCiteRegEx": "Pinker", "year": 1994}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q. Le", "C. Manning", "A. Ng"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "By exploring multiple layers of non-linear transformation, the deep architecture can extract underlying abstract features of data, which is crucial to artificial intelligence (AI) (Bengio 2009).", "startOffset": 180, "endOffset": 193}, {"referenceID": 4, "context": "Although deep learning has been successful in various fields\u2014like natural language processing (NLP) (Collobert and Weston 2008), computer vision (Krizhevsky, Sutskever, and Hinton 2012), and speech recognition (Dahl, Mohamed, and Hinton 2010)\u2014its advantages are not exploited in the field of programming language processing.", "startOffset": 100, "endOffset": 127}, {"referenceID": 8, "context": "Programs are complex, flexible and powerful; they also contain rich statistical properties (Hindle et al. 2012).", "startOffset": 91, "endOffset": 111}, {"referenceID": 18, "context": "(Pinker 1994) illustrates an interesting example, \u201cThe dog the stick the fire burned beat bit the cat.", "startOffset": 0, "endOffset": 13}, {"referenceID": 5, "context": ", 0\u2019s will give 0 distance but is meaningless), negative sampling is applied like (Collobert et al. 2011).", "startOffset": 82, "endOffset": 105}, {"referenceID": 15, "context": "One possible solution is the \u201ccontinuous bag of words\u201d model (Mikolov et al. 2013)2, but position information will be lost completely.", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": "Similar approach is used in (Hermann and Blunsom 2014).", "startOffset": 28, "endOffset": 54}, {"referenceID": 20, "context": "In (Socher et al. 2013a), a different weight matrix is allocated as parameters for each position.", "startOffset": 3, "endOffset": 24}, {"referenceID": 16, "context": "Part of the result is first reported in (Mou et al. 2014).", "startOffset": 40, "endOffset": 57}, {"referenceID": 5, "context": "Deep neural networks have made significant breakthroughs in many fields of artificial intelligence, for example, computer vision (Krizhevsky, Sutskever, and Hinton 2012), speech recognition (Dahl, Mohamed, and Hinton 2010), natural language processing (Collobert et al. 2011), etc.", "startOffset": 252, "endOffset": 275}, {"referenceID": 0, "context": "Stacked restricted Boltzmann machines and autoencoders are successful pretraining methods (Hinton, Osindero, and Teh 2006; Bengio et al. 2007).", "startOffset": 90, "endOffset": 142}, {"referenceID": 14, "context": "Another example is the recurrent neural network, which can be regarded as a time-decaying network (Mikolov et al. 2010).", "startOffset": 98, "endOffset": 119}, {"referenceID": 19, "context": "A model similar to ours is the recursive neural network (RNN) proposed in (Socher et al. 2011; 2013b) for NLP.", "startOffset": 74, "endOffset": 101}], "year": 2014, "abstractText": "Deep neural networks have made significant breakthroughs in many fields of artificial intelligence. However, it has not been applied in the field of programming language processing. In this paper, we propose the treebased convolutional neural network (TBCNN) to model programming languages, which contain rich and explicit tree structural information. In our model, program vector representations are learned by the \u201ccoding\u201d pretraining criterion based on abstract syntax trees (ASTs); the convolutional layer explicitly captures neighboring features on the tree; with the \u201cbinary continuous tree\u201d and \u201c3-way pooling,\u201d our model can deal with ASTs of different shapes and sizes. We evaluate the program vector representations empirically, showing that the coding criterion successfully captures underlying features of AST nodes, and that program vector representations significantly speed up supervised learning. We also compare TBCNN to baseline methods; our model achieves better accuracy in the task of program classification. To our best knowledge, this paper is the first to analyze programs with deep neural networks; we extend the scope of deep learning to the field of programming language processing. The experimental results validate its feasibility; they also show a promising future of this new research area.", "creator": "TeX"}}}