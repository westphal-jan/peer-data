{"id": "1410.4176", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2014", "title": "Learning Distributed Word Representations for Natural Logic Reasoning", "abstract": "natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic networks, which have proven valuable in a wide range of sophisticated language tasks. however, knowledge remains an open variable whether it is like anyone train formal representations to support the rich, diverse logical reasoning illustrated by natural logic. we address complicated question using sophisticated neural network - based models for common embeddings : plain neural networks and neural tensor networks. our experiments evaluate existing models'intention to learn the basic algebra of natural logic relations from simulated data types from the wordnet noun graph. the overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics.", "histories": [["v1", "Wed, 15 Oct 2014 19:27:10 GMT  (15kb)", "http://arxiv.org/abs/1410.4176v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel r bowman", "christopher potts", "christopher d manning"], "accepted": false, "id": "1410.4176"}, "pdf": {"name": "1410.4176.pdf", "metadata": {"source": "CRF", "title": "Learning Distributed Word Representations for Natural Logic Reasoning", "authors": ["Samuel R. Bowman", "Christopher D. Manning"], "emails": ["sbowman@stanford.edu", "cgpotts@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n41 76\nv1 [\ncs .C\nL ]\n1 5\nO ct\nNatural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic. We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks. Our experiments evaluate the models\u2019 ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics."}, {"heading": "1 Introduction", "text": "Natural logic offers a powerful relational conception of semantics: the meanings for expressions are given, at least in part, by their inferential connections with other expressions [1, 2]. For instance, turtle is analyzed, not primarily by its extension in the world, but rather by its lexical network: it entails reptile, excludes chair, is entailed by sea turtle, and so forth. With generalized notions of entailment and contradiction, these relationships can be defined for all lexical categories as well as complex phrases, sentences, and even texts. The resulting theories of meaning offer valuable new analytic tools for tasks involving database inference, relation extraction, and textual entailment.\nNatural logic aligns well with distributed (e.g., vector) representations, which also naturally model meaning relationally. Distributed representations have been used successfully in a wide array of sophisticated language tasks (e.g., [3]). However, it remains an open question whether it is possible to train such representations to support the rich, diverse logical reasoning captured by natural logic; while they excel at synonymy (similarity), the results are more mixed for entailment, contradiction, and mutual consistency. Using the natural logic of [2] as our formal model, we address this open question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks (NTNs). The natural logic is built from the seven relations defined in Table 1. Its formal properties are now well-understood [4], so it provides a rigorous set of goals for our neural models. To keep the discussion manageable, we limit attention to experiments involving the lexicon; for a more extended treatment of complex expressions involving logical connectives and quantifiers, see Bowman et al. [5].\nIn our experiments, we evaluate these models\u2019 ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The simulated data help us to achieve analytic insights into what the models learn, and the WordNet data show how they fare with a real natural language vocabulary. We find that only the NTN is able to fully learn the underlying algebra, but that both models excel in the WordNet experiment."}, {"heading": "2 Neural network models for relation classification", "text": "We build embedding-based models using the method of [5], which is centered on the task of labeling a pair of words or sentences with one of a small set of logical relations. The architecture that we use, which is limited to only pairs of single terms (such as words), is depicted in Figure 1. The model represents the two input terms as embeddings, which are fed into a comparison function based on one of two types of neural network layer functions to produce a representation for the relationship between the two terms. This representation is then fed into a softmax classifier, which outputs a distribution over possible labels. The entire network, including the embeddings, is trained using backpropagation.\nThe simpler version of the comparison concatenates the two input vectors before feeding them into a standard neural network (NN) layer. The more powerful neural tensor network (NTN) version uses an additional third-order tensor parameter to allow for multiplicative interactions between the two inputs [6]. For more details on the implementation and training of the layer functions, see [5].\nThis model used here differs from the one described in that work in two ways. First, because the inputs are single terms, we do not use a composition function here. Second, for our experiment on WordNet data, we introduce an additional neural network layer between the embedding input and the comparison function, which facilitates initializing the embeddings themselves from pretrained vectors and was found to help performance in that setting."}, {"heading": "3 Reasoning about semantic relations", "text": "In this experiment, we test our model\u2019s ability to learn and use the natural logic inference rules schematized in Figure 2a. For instance, given that a \u2290 b and b \u2290 c, one can conclude that a \u2290 c, by basic set-theoretic reasoning (transitivity of \u2290). Similarly, from a \u228f b and b \u2227 c, it follows that a | c. Cells in the table containing a dot correspond to pairs of relations for which no valid inference can be drawn in our logic.\nExperiments To test our models\u2019 ability to learn these inferential patterns, we create artificial boolean structures in which terms denote sets of entities from a small domain (e.g., Figure 2b), employ logical deduction to identify the valid statements, divide those into train and test sets, and remove from the test set those statements which cannot be proven from the training statements (Figure 2c). In our experiments, we create 80 randomly generated sets drawn from a domain of\nseven elements. This yields 6400 statements about pairs of formulae, of which 3200 are chosen as a test set, and that test set is further reduced to the 2960 examples that can be provably derived from the training data. We trained both the NN model and the NTN model on these data sets.\nResults We found that the NTN model worked best with 11-dimensional vector representations for the 80 sets and a 90-dimensional feature vector for the classifier, though the performance of the model was not highly dependant on either dimensionality setting. Averaging over five randomly generated data sets, the model was able to correctly label 98.1% (standard error 0.67%) of the provably derivable test examples, and 87.7% (SE = 3.59%) of the remaining test examples. The simpler NN worked best with 11 and 75 dimensions, respectively, but was able to achieve accuracies of only 84.8% (SE = 0.84%) and 68.7% (SE = 1.58%), respectively. Training accuracy was 99.8% (SE = 0.04%) for the NTN and 94.7% (SE = 0.89%) for the NN.\nThese results are fairly straightforward to interpret. The NTN model was able to accurately encode the relations between the terms in the geometric relations between their vectors, and was able to then use that information to recover relations that were not overtly included in the training data. In contrast, the NN was able to achieve this behavior only incompletely. It is possible but not likely that it could be made to find a good solution with further optimization on different learning algorithms, or that it would do better on a larger universe of sets, for which there would be a larger set of training data to learn from, but the NTN is readily able to achieve these effects in the setting discussed here."}, {"heading": "4 Reasoning about lexical relations in WordNet", "text": "Using simulated data as above is reassuring about what the models learn and why, but we also want to know how they perform with a real natural language vocabulary. Unfortunately, as far as we are aware, there are no available resources labeling such a vocabulary with the relations from Table 1. However, the relations in WordNet [7] come close and pose the same substantive challenges within a somewhat easier classification problem.\nWe extract three types of relation from WordNet. Hypernym and hyponym can be represented directly in the WordNet graph structure, and correspond closely to the \u2290 and \u228f relations from natural logic. As in natural logic, these relations are mirror images of one another: if dog is a hyponym of animal (perhaps indirectly by way of canid, mammal, etc.), then animal is a hypernym of dog. We also extract coordinate terms, which share a direct hypernym, like dalmatian, pug, and puppy, which are all direct hyponyms of dog. Coordinate terms tend to exclude one another, thereby providing a loose approximation of the natural logic exclusion relation |. WordNet defines hypernymy and hyponymy over sets of synonyms, rather than over individual terms, so we do not include a synonym or equivalent relation, but rather consider only one member of each set of synonyms. Word pairs which do not fall into these three relations are not included in the data set.\nTo limit the size of the vocabulary without otherwise simplifying the learning problem, we extract all of the instances of these three relations for single word nouns in WordNet that are hyponyms of the node organism.n.01. In order to balance the distribution of the classes, we slightly downsample instances of the coordinate relation, yielding a total of 36,772 relations among 3,217 terms. We report results below using crossvalidation, choosing a disjoint 10% test sample for each of five runs. Unlike in the previous experiment, it is not straightforward here to determine in advance how much data should be required to train an accurate model, so we performed training runs with various fractions of the remaining data. Embeddings were fixed at 25 dimensions and were initialized randomly or using distributional vectors from GloVe [8]. The feature vector produced by the comparison layer was fixed at 80 dimensions.\nResults We find that the NTN performs perfectly with random initialization, and that the plain NN performs almost as well, a point of contrast with the results of Section 3. We also find that initialization with GloVe is helpful in allowing the models to maintain fair performance with smaller amounts of training data. Some of the randomly initialized model runs failed to learn usable representations at all and labeled all examples with the most frequent labels. We excluded these runs from the statistics, but marked settings for which this occurred with the symbol \u2020. For all of the remaining runs, training accuracy was consistently above 99%."}, {"heading": "5 Conclusion", "text": "This paper evaluated two neural models on the task of learning natural logic relations between distributed word representations. The results suggest that at least the neural tensor network has the capacity to meet this challenge with reasonably-sized training sets, learning both to embed a vocabulary in a way that encodes a diverse set of relations, and to subsequently use those embeddings to infer new relations. In [5], we extend these results to include complex expressions involving logical connectives and quantifiers, with similar conclusions about (recursive versions of) these models. These findings are promising for the future of learned distributed representations in the applied modeling of logical semantics."}], "references": [{"title": "A brief history of natural logic", "author": ["J. van Benthem"], "venue": "Logic, Navya-Nyaya and Applications: Homage to Bimal Matilal,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "An extended model of natural logic", "author": ["B. MacCartney", "C.D. Manning"], "venue": "Proc. IWCS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JLMR, 12", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Recent progress on monotonicity", "author": ["T.F. Icard", "L.S. Moss"], "venue": "Linguistic Issues in Language Technology, 9(7)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Recursive neural networks for learning logical semantics", "author": ["S.R. Bowman", "C. Potts", "C.D. Manning"], "venue": "arXiv manuscript 1406.1827", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["D. Chen", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proc. ICLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "WordNet", "author": ["C. Fellbaum"], "venue": "Theory and Applications of Ontology: Computer Applications. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. EMNLP", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Natural logic offers a powerful relational conception of semantics: the meanings for expressions are given, at least in part, by their inferential connections with other expressions [1, 2].", "startOffset": 182, "endOffset": 188}, {"referenceID": 1, "context": "Natural logic offers a powerful relational conception of semantics: the meanings for expressions are given, at least in part, by their inferential connections with other expressions [1, 2].", "startOffset": 182, "endOffset": 188}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Using the natural logic of [2] as our formal model, we address this open question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks (NTNs).", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "Its formal properties are now well-understood [4], so it provides a rigorous set of goals for our neural models.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Table 1: The seven natural logic relations of [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "We build embedding-based models using the method of [5], which is centered on the task of labeling a pair of words or sentences with one of a small set of logical relations.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "The more powerful neural tensor network (NTN) version uses an additional third-order tensor parameter to allow for multiplicative interactions between the two inputs [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "For more details on the implementation and training of the layer functions, see [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "However, the relations in WordNet [7] come close and pose the same substantive challenges within a somewhat easier classification problem.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Embeddings were fixed at 25 dimensions and were initialized randomly or using distributional vectors from GloVe [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "In [5], we extend these results to include complex expressions involving logical connectives and quantifiers, with similar conclusions about (recursive versions of) these models.", "startOffset": 3, "endOffset": 6}], "year": 2014, "abstractText": "Natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic. We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks. Our experiments evaluate the models\u2019 ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics.", "creator": "LaTeX with hyperref package"}}}