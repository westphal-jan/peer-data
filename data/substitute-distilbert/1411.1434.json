{"id": "1411.1434", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "On the Information Theoretic Limits of Learning Ising Models", "abstract": "researchers provide a general framework whereby computing lower - bounds on the sample complexity of recovering the underlying function of constrained models, given i. iii. d limitations. while there have been recent results for specific equivalence classes, these involve fairly extensive technical arguments that remains specialized to preserve specific graph class. in contrast, we isolate two key graph - design ingredients that can then be used to specify sample complexity in - bounds. presence of these structural properties makes the graph class hard to resolve. we derive corollaries since our main result that not only recover existing recent results, but also provide other bounds for novel graph classes not considered previously. we also extend our framework to the random mode setting and derive corollaries for erd \\ h { o } s - r \\'{ io } nyi graphs in a certain dense setting.", "histories": [["v1", "Wed, 5 Nov 2014 22:28:00 GMT  (39kb)", "https://arxiv.org/abs/1411.1434v1", "21 pages; to appear in NIPS 2014"], ["v2", "Fri, 5 Dec 2014 22:02:55 GMT  (39kb)", "http://arxiv.org/abs/1411.1434v2", "21 pages; to appear in NIPS 2014"]], "COMMENTS": "21 pages; to appear in NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rashish tandon", "karthikeyan shanmugam", "pradeep ravikumar", "alexandros g dimakis"], "accepted": true, "id": "1411.1434"}, "pdf": {"name": "1411.1434.pdf", "metadata": {"source": "CRF", "title": "On the Information Theoretic Limits of Learning Ising Models", "authors": ["Karthikeyan Shanmugam", "Rashish Tandon", "Alexandros G. Dimakis", "Pradeep Ravikumar"], "emails": ["karthiksh@utexas.edu,", "rashish@cs.utexas.edu", "dimakis@austin.utexas.edu,", "pradeepr@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n14 34\nv2 [\ncs .L\nG ]\n5 D\nWe provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erdo\u030bs-R\u00e9nyi graphs in a certain dense setting."}, {"heading": "1 Introduction", "text": "Graphical models provide compact representations of multivariate distributions using graphs that represent Markov conditional independencies in the distribution. They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15]. In many of these domains, a key problem of interest is to recover the underlying dependencies, represented by the graph, given samples i.e. to estimate the graph of dependencies given instances drawn from the distribution. A common regime where this graph selection problem is of interest is the high-dimensional setting, where the number of samples n is potentially smaller than the number of variables p. Given the importance of this problem, it is instructive to have lower bounds on the sample complexity of any estimator: it clarifies the statistical difficulty of the underlying problem, and moreover it could serve as a certificate of optimality in terms of sample complexity for any estimator that actually achieves this lower bound. We are particularly interested in such lower bounds under the structural constraint that the graph lies within a given class of graphs (such as degree-bounded graphs, bounded-girth graphs, and so on).\nThe simplest approach to obtaining such bounds involves graph counting arguments, and an application of Fano\u2019s lemma. [2, 17] for instance derive such bounds for the case of degree-bounded and power-law graph classes respectively. This approach however is purely graph-theoretic, and thus fails to capture the interaction of the graphical model parameters with the graph structural constraints, and thus typically provides suboptimal lower bounds (as also observed in [16]). The other standard approach requires a more complicated argument through Fano\u2019s lemma that requires finding a subset of graphs such that (a) the subset is large enough in number, and (b) the graphs in the subset are close enough in a suitable metric, typically the KL-divergence of the corresponding distributions. This approach is however much more technically intensive, and even for the simple\nclasses of bounded degree and bounded edge graphs for Ising models, [16] required fairly extensive arguments in using the above approach to provide lower bounds.\nIn modern high-dimensional settings, it is becoming increasingly important to incorporate structural constraints in statistical estimation, and graph classes are a key interpretable structural constraint. But a new graph class would entail an entirely new (and technically intensive) derivation of the corresponding sample complexity lower bounds. In this paper, we are thus interested in isolating the key ingredients required in computing such lower bounds. This key ingredient involves one the following structural characterizations: (1) connectivity by short paths between pairs of nodes, or (2) existence of many graphs that only differ by an edge. As corollaries of this framework, we not only recover the results in [16] for the simple cases of degree and edge bounded graphs, but to several more classes of graphs, for which achievability results have already been proposed[1]. Moreover, using structural arguments allows us to bring out the dependence of the edge-weights, \u03bb, on the sample complexity. We are able to show same sample complexity requirements for d-regular graphs, as is for degree d-bounded graphs, whilst the former class is much smaller. We also extend our framework to the random graph setting, and as a corollary, establish lower bound requirements for the class of Erdo\u030bs-R\u00e9nyi graphs in a dense setting. Here, we show that under a certain scaling of the edge-weights \u03bb, Gp,c/p requires exponentially many samples, as opposed to a polynomial requirement suggested from earlier bounds[1]."}, {"heading": "2 Preliminaries and Definitions", "text": "Notation: R represents the real line. [p] denotes the set of integers from 1 to p. Let 1S denote the vector of ones and zeros where S is the set of coordinates containing 1. Let A \u2212 B denote A\u22c2Bc and A\u2206B denote the symmetric difference for two sets A and B.\nIn this work, we consider the problem of learning the graph structure of an Ising model. Ising models are a class of graphical model distributions over binary vectors, characterized by the pair (G(V,E), \u03b8\u0304), where G(V,E) is an undirected graph on p vertices and \u03b8\u0304 \u2208 R(p2) : \u03b8\u0304i,j = 0 \u2200(i, j) /\u2208 E, \u03b8\u0304i,j 6= 0 \u2200 (i, j) \u2208 E. Let X = {+1,\u22121}. Then, for the pair (G, \u03b8\u0304), the distribution on X p is\ngiven as: fG,\u03b8\u0304(x) = 1 Z exp\n(\n\u2211\ni,j\n\u03b8\u0304i,j xixj\n)\nwhere x \u2208 X p and Z is the normalization factor, also\nknown as the partition function.\nThus, we obtain a family of distributions by considering a set of edge-weighted graphs G\u03b8 , where each element of G\u03b8 is a pair (G, \u03b8\u0304). In other words, every member of the class G\u03b8 is a weighted undirected graph. Let G denote the set of distinct unweighted graphs in the class G\u03b8 . A learning algorithm that learns the graph G (and not the weights \u03b8\u0304) from n independent samples (each sample is a p-dimensional binary vector) drawn from the distribution fG,\u03b8\u0304(\u00b7), is an efficiently computable map \u03c6 : \u03c7np \u2192 G which maps the input samples {x1, . . .xn} to an undirected graph G\u0302 \u2208 G i.e. G\u0302 = \u03c6(x1, . . . ,xn). We now discuss two metrics of reliability for such an estimator \u03c6. For a given (G, \u03b8\u0304), the probability of error (over the samples drawn) is given by p(G, \u03b8\u0304) = Pr ( G\u0302 6= G )\n. Given a graph class G\u03b8, one may consider the maximum probability of error for the map \u03c6, given as:\npmax = max (G,\u03b8)\u2208G\u03b8\nPr ( G\u0302 6= G ) . (1)\nThe goal of any estimator \u03c6 would be to achieve as low a pmax as possible. Alternatively, there are random graph classes that come naturally endowed with a probability measure \u00b5(G, \u03b8) of choosing the graphical model. In this case, the quantity we would want to minimize would be the average probability of error of the map \u03c6, given as:\npavg = E\u00b5\n[ Pr ( G\u0302 6= G )]\n(2)\nIn this work, we are interested in answering the following question: For any estimator \u03c6, what is the minimum number of samples n, needed to guarantee an asymptotically small pmax or pavg ? The answer depends on G\u03b8 and \u00b5(when applicable).\nFor the sake of simplicity, we impose the following restrictions1: We restrict to the set of zero-field ferromagnetic Ising models, where zero-field refers to a lack of node weights, and ferromagnetic refers to all positive edge weights. Further, we will restrict all the non-zero edge weights (\u03b8\u0304i,j) in the graph classes to be the same, set equal to \u03bb > 0. Therefore, for a given G(V,E), we have \u03b8\u0304 = \u03bb1E for some \u03bb > 0. A deterministic graph class is described by a scalar \u03bb > 0 and the family of graphs G. In the case of a random graph class, we describe it by a scalar \u03bb > 0 and a probability measure \u00b5, the measure being solely on the structure of the graph G (on G). Since we have the same weight \u03bb(> 0) on all edges, henceforth we will skip the reference to it, i.e. the graph class will simply be denoted G and for a given G \u2208 G, the distribution will be denoted by fG(\u00b7), with the dependence on \u03bb being implicit. Before proceeding further, we summarize the following additional notation. For any two distributions fG and fG\u2032 , corresponding to the graphs G and G\u2032 respectively, we denote the Kullback-Liebler divergence (KL-divergence) between them\nas D (fG\u2016fG\u2032) = \u2211 x\u2208X p fG(x) log ( fG(x) fG\u2032 (x) )\n. For any subset T \u2286 G, we let CT (\u01eb) denote an \u01eb-covering w.r.t. the KL-divergence (of the corresponding distributions) i.e. CT (\u01eb)(\u2286 G) is a set of graphs such that for any G \u2208 T , there exists a G\u2032 \u2208 CT (\u01eb) satisfying D (fG\u2016fG\u2032) \u2264 \u01eb. We denote the entropy of any r.v. X by H(X), and the mutual information between any two r.v.s X and Y , by I(X ;Y ). The rest of the paper is organized as follows. Section 3 describes Fano\u2019s lemma, a basic tool employed in computing information-theoretic lower bounds. Section 4 identifies key structural properties that lead to large sample requirements. Section 5 applies the results of Sections 3 and 4 on a number of different deterministic graph classes to obtain lower bound estimates. Section 6 obtains lower bound estimates for Erdo\u030bs-R\u00e9nyi random graphs in a dense regime. All proofs can be found in the Appendix (see supplementary material)."}, {"heading": "3 Fano\u2019s Lemma and Variants", "text": "Fano\u2019s lemma [5] is a primary tool for obtaining bounds on the average probability of error, pavg . It provides a lower bound on the probability of error of any estimator \u03c6 in terms of the entropy H(\u00b7) of the output space, the cardinality of the output space, and the mutual information I(\u00b7 , \u00b7) between the input and the output. The case of pmax is interesting only when we have a deterministic graph class G, and can be handled through Fano\u2019s lemma again by considering a uniform distribution on the graph class.\nLemma 1 (Fano\u2019s Lemma). Consider a graph class G with measure \u00b5. Let, G \u223c \u00b5, and let Xn = {x1, . . . ,xn} be n independent samples such that xi \u223c fG, i \u2208 [n]. Then, for pmax and pavg as defined in (1) and (2) respectively,\npmax \u2265 pavg \u2265 H(G)\u2212 I(G;Xn)\u2212 log 2\nlog|G| (3)\nThus in order to use this Lemma, we need to bound two quantities: the entropyH(G), and the mutual information I(G;Xn). The entropy can typically be obtained or bounded very simply; for instance, with a uniform distribution over the set of graphs G, H(G) = log |G|. The mutual information is a much trickier object to bound however, and is where the technical complexity largely arises. We can however simply obtain the following loose bound: I(G;Xn) \u2264 H(Xn) \u2264 np. We thus arrive at the following corollary:\nCorollary 1. Consider a graph class G. Then, pmax \u2265 1\u2212 np+log 2log|G| .\nRemark 1. From Corollary 1, we get: If n \u2264 log|G|p ( (1\u2212 \u03b4)\u2212 log 2log|G| )\n, then pmax \u2265 \u03b4. Note that this bound on n is only in terms of the cardinality of the graph class G, and therefore, would not involve any dependence on \u03bb (and consequently, be very loose).\nTo obtain sharper lower bound guarantees that depends on graphical model parameters, it is useful to consider instead a conditional form of Fano\u2019s lemma[1, Lemma 9], which allows us to obtain lower bounds on pavg in terms conditional analogs of the quantities in Lemma 1. For the case of pmax, these conditional analogs correspond to uniform measures on subsets of the original class G.\n1Note that a lower bound for a restricted subset of a class of Ising models will also serve as a lower bound for the class without that restriction.\nThe conditional version allows us to focus on potentially harder to learn subsets of the graph class, leading to sharper lower bound guarantees. Also, for a random graph class, the entropy H(G) may be asymptotically much smaller than the log cardinality of the graph class, log|G| (e.g. Erdo\u030bs-R\u00e9nyi random graphs; see Section 6), rendering the bound in Lemma 1 useless. The conditional version allows us to circumvent this issue by focusing on a high-probability subset of the graph class.\nLemma 2 (Conditional Fano\u2019s Lemma). Consider a graph class G with measure \u00b5. Let, G \u223c \u00b5, and let Xn = {x1, . . . ,xn} be n independent samples such that xi \u223c fG, i \u2208 [n]. Consider any T \u2286 G and let \u00b5 (T ) be the measure of this subset i.e. \u00b5 (T ) = Pr\u00b5 (G \u2208 T ). Then, we have\npavg \u2265 \u00b5 (T ) H(G|G \u2208 T )\u2212 I(G;Xn|G \u2208 T )\u2212 log 2\nlog|T | and,\npmax \u2265 H(G|G \u2208 T )\u2212 I(G;Xn|G \u2208 T )\u2212 log 2\nlog|T |\nGiven Lemma 2, or even Lemma 1, it is the sharpness of an upper bound on the mutual information that governs the sharpness of lower bounds on the probability of error (and effectively, the number of samples n). In contrast to the trivial upper bound used in the corollary above, we next use a tighter bound from [20], which relates the mutual information to coverings in terms of the KL-divergence, applied to Lemma 2. Note that, as stated earlier, we simply impose a uniform distribution on G when dealing with pmax. Analogous bounds can be obtained for pavg .\nCorollary 2. Consider a graph class G, and any T \u2286 G. Recall the definition of CT (\u01eb) from Section 2. For any \u01eb > 0, we have pmax \u2265 ( 1\u2212 log|CT (\u01eb)|+n\u01eb+log 2log|T | ) .\nRemark 2. From Corollary 2, we get: If n \u2264 log|T |\u01eb ( (1\u2212 \u03b4)\u2212 log 2log|T | \u2212 log|CT (\u01eb)| log|T | )\n, then pmax \u2265 \u03b4. \u01eb is an upper bound on the radius of the KL-balls in the covering, and usually varies with \u03bb.\nBut this corollary cannot be immediately used given a graph class: it requires us to specify a subset T of the overall graph class, the term \u01eb, and the KL-covering CT (\u01eb). We can simplify the bound above by setting \u01eb to be the radius of a single KL-ball w.r.t. some center, covering the whole set T . Suppose this radius is \u03c1, then the size of the covering set is just 1. In this case, from Remark 2, we get: If n \u2264 log|T |\u03c1 ( (1\u2212 \u03b4)\u2212 log 2log|T | )\n, then pmax \u2265 \u03b4. Thus, our goal in the sequel would be to provide a general mechanism to derive such a subset T : that is large in number and yet has small diameter with respect to KL-divergence.\nWe note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21]."}, {"heading": "4 Structural conditions governing Correlation", "text": "As discussed in the previous section, we want to find subsets T that are large in size, and yet have a small KL-diameter. In this section, we summarize certain structural properties that result in small KL-diameter. Thereafter, finding a large set T would amount to finding a large number of graphs in the graph class G that satisfy these structural properties. As a first step, we need to get a sense of when two graphs would have corresponding distributions with a small KL-divergence. To do so, we need a general upper bound on the KL-divergence between the corresponding distributions. A simple strategy is to simply bound it by its symmetric divergence[16]. In this case, a little calculation shows :\nD (fG\u2016fG\u2032) \u2264 D (fG\u2016fG\u2032) +D (fG\u2032\u2016fG) = \u2211\n(s,t)\u2208E\\E\u2032 \u03bb (EG [xsxt]\u2212 EG\u2032 [xsxt]) +\n\u2211\n(s,t)\u2208E\u2032\\E \u03bb (EG\u2032 [xsxt]\u2212 EG [xsxt])\n(4)\nwhere E and E\u2032 are the edges in the graphs G and G\u2032 respectively, and EG[\u00b7] denotes the expectation under fG. Also note that the correlation between xs and xt, EG[xsxt] = 2PG(xsxt = +1)\u2212 1.\nFrom Eq. (4), we observe that the only pairs, (s, t), contributing to the KL-divergence are the ones that lie in the symmetric difference,E\u2206E\u2032. If the number of such pairs is small, and the difference of correlations in G and G\u2032 (i.e. EG [xsxt]\u2212EG\u2032 [xsxt]) for such pairs is small, then the KL-divergence would be small.\nTo summarize the setting so far, to obtain a tight lower bound on sample complexity for a class of graphs, we need to find a subset of graphs T with small KL diameter. The key to this is to identify when KL divergence between (distributions corresponding to) two graphs would be small. And the key to this in turn is to identify when there would be only a small difference in the correlations between a pair of variables across the two graphs G and G\u2032. In the subsequent subsections, we provide two simple and general structural characterizations that achieve such a small difference of correlations across G and G\u2032."}, {"heading": "4.1 Structural Characterization with Large Correlation", "text": "One scenario when there might be a small difference in correlations is when one of the correlations is very large, specifically arbitrarily close to 1, say EG\u2032 [xsxt] \u2265 1 \u2212 \u01eb, for some \u01eb > 0. Then, EG[xsxt] \u2212 EG\u2032 [xsxt] \u2264 \u01eb, since EG[xsxt] \u2264 1. Indeed, when s, t are part of a clique[16], this is achieved since the large number of connections between them force a higher probability of agreement i.e. PG(xsxt = +1) is large.\nIn this work we provide a more general characterization of when this might happen by relying on the following key lemma that connects the presence of \u201cmany\u201d node disjoint \u201cshort\u201d paths between a pair of nodes in the graph to high correlation between them. We define the property formally below. Definition 1. Two nodes a and b in an undirected graph G are said to be (\u2113, d) connected if they have d node disjoint paths of length at most \u2113. Lemma 3. Consider a graph G and a scalar \u03bb > 0. Consider the distribution fG(x) induced by the graph. If a pair of nodes a and b are (\u2113, d) connected, then EG [xaxb] \u2265 1\u2212 2\n1+ (1+(tanh(\u03bb))\u2113)d\n(1\u2212(tanh(\u03bb))\u2113)d\n.\nFrom the above lemma, we can observe that as \u2113 gets smaller and d gets larger,EG [xaxb] approaches its maximum value of 1. As an example, in a k-clique, any two vertices, s and t, are (2, k \u2212 1) connected. In this case, the bound from Lemma 3 gives us: EG [xaxb] \u2265 1 \u2212 21+(cosh\u03bb)k\u22121 . Of course, a clique enjoys a lot more connectivity (i.e. also (\n3, k\u221212 ) connected etc., albeit with node\noverlaps) which allows for a stronger bound of \u223c 1\u2212 \u03bbke\u03bb e\u03bbk (see [16])2\nNow, as discussed earlier, a high correlation between a pair of nodes contributes a small term to the KL-divergence. This is stated in the following corollary. Corollary 3. Consider two graphs G(V,E) and G\u2032(V,E\u2032) and scalar weight \u03bb > 0 such that E \u2212 E\u2032 and E\u2032 \u2212 E only contain pairs of nodes that are (\u2113, d) connected in graphs G\u2032 and G respectively, then the KL-divergence between fG and fG\u2032 , D (fG\u2016fG\u2032) \u2264 2\u03bb|E\u2206E\n\u2032| 1+ (1+(tanh(\u03bb))\u2113)d\n(1\u2212(tanh(\u03bb))\u2113)d\n."}, {"heading": "4.2 Structural Characterization with Low Correlation", "text": "Another scenario where there might be a small difference in correlations between an edge pair across two graphs is when the graphs themselves are close in Hamming distance i.e. they differ by only a few edges. This is formalized below for the situation when they differ by only one edge. Definition 2 (Hamming Distance). Consider two graphs G(V,E) and G\u2032(V,E\u2032). The hamming distance between the graphs, denoted by H(G,G\u2032), is the number of edges where the two graphs differ i.e. H(G,G\u2032) = |{(s, t) | (s, t) \u2208 E\u2206E\u2032}| (5) Lemma 4. Consider two graphs G(V,E) and G\u2032(V,E\u2032) such that H(G,G\u2032) = 1, and (a, b) \u2208 E is the single edge in E\u2206E\u2032. Then, EfG [xaxb] \u2212 EfG\u2032 [xaxb] \u2264 tanh(\u03bb). Also, the KL-divergence between the distributions, D (fG\u2016f \u2032G) \u2264 \u03bb tanh(\u03bb).\n2Both the bound from [16] and the bound from Lemma 3 have exponential asymptotic behaviour (i.e. as k grows) for constant \u03bb. For smaller \u03bb, the bound from [16] is strictly better. However, not all graph classes allow for the presence of a large enough clique, for e.g., girth bounded graphs, path restricted graphs, Erdo\u030bs-R\u00e9nyi graphs.\nThe above bound is useful in low \u03bb settings. In this regime \u03bb tanh\u03bb roughly behaves as \u03bb2. So, a smaller \u03bb would correspond to a smaller KL-divergence."}, {"heading": "4.3 Influence of Structure on Sample Complexity", "text": "Now, we provide some high-level intuition behind why the structural characterizations above would be useful for lower bounds that go beyond the technical reasons underlying Fano\u2019s Lemma that we have specified so far. Let us assume that \u03bb > 0 is a positive real constant. In a graph even when the edge (s, t) is removed, (s, t) being (\u2113, d) connected ensures that the correlation between s and t is still very high (exponentially close to 1). Therefore, resolving the question of the presence/absence of the edge (s, t) would be difficult \u2013 requiring lots of samples. This is analogous in principle to the argument in [16] used for establishing hardness of learning of a set of graphs each of which is obtained by removing a single edge from a clique, still ensuring many short paths between any two vertices. Similarly, if the graphs, G and G\u2032, are close in Hamming distance, then their corresponding distributions, fG and fG\u2032 , also tend to be similar. Again, it becomes difficult to tease apart which distribution the samples observed may have originated from."}, {"heading": "5 Application to Deterministic Graph Classes", "text": "In this section, we provide lower bound estimates for a number of deterministic graph families. This is done by explicitly finding a subset T of the graph class G, based on the structural properties of the previous section. See the supplementary material for details of these constructions. A common underlying theme to all is the following: We try to find a graph in G containing many edge pairs (u, v) such that their end vertices, u and v, have many paths between them (possibly, node disjoint). Once we have such a graph, we construct a subset T by removing one of the edges for these wellconnected edge pairs. This ensures that the new graphs differ from the original in only the wellconnected pairs. Alternatively, by removing any edge (and not just well-connected pairs) we can get another larger family T which is 1-hamming away from the original graph."}, {"heading": "5.1 Path Restricted Graphs", "text": "Let Gp,\u03b7 be the class of all graphs on p vertices with have at most \u03b7 paths (\u03b7 = o(p)) between any two vertices. We have the following theorem :\nTheorem 1. For the class Gp,\u03b7, if n \u2264 (1 \u2212 \u03b4)max { log(p/2) \u03bb tanh\u03bb , 1+cosh(2\u03bb)\u03b7\u22121 2\u03bb log ( p 2(\u03b7+1) )} , then pmax \u2265 \u03b4.\nTo understand the scaling, it is useful to think of cosh(2\u03bb) to be roughly exponential in \u03bb2 i.e. cosh(2\u03bb) \u223c e\u0398(\u03bb2)3. In this case, from the second term, we need n \u223c \u2126 ( e\u03bb 2\u03b7\n\u03bb log ( p \u03b7 )) samples.\nIf \u03b7 is scaling with p, this can be prohibitively large (exponential in \u03bb2\u03b7). Thus, to have low sample complexity, we must enforce \u03bb = O(1/ \u221a \u03b7). In this case, the first term gives n = \u2126(\u03b7 log p), since \u03bb tanh(\u03bb) \u223c \u03bb2, for small \u03bb. We may also consider a generalization of Gp,\u03b7. Let Gp,\u03b7,\u03b3 be the set of all graphs on p vertices such that there are at most \u03b7 paths of length at most \u03b3 between any two nodes (with \u03b7 + \u03b3 = o(p)). Note that there may be more paths of length > \u03b3.\nTheorem 2. Consider the graph class Gp,\u03b7,\u03b3 . For any \u03bd \u2208 (0, 1), let t\u03bd = p 1\u2212\u03bd\u2212(\u03b7+1) \u03b3 . If n \u2264\n(1\u2212 \u03b4)max\n\n\n\nlog(p/2) \u03bb tanh\u03bb ,\n1+\n[ cosh(2\u03bb)\u03b7\u22121 ( 1+tanh(\u03bb)\u03b3+1\n1\u2212tanh(\u03bb)\u03b3+1\n)t\u03bd ]\n2\u03bb \u03bd log(p)\n\n\n\n, then pmax \u2265 \u03b4.\nThe parameter \u03bd \u2208 (0, 1) in the bound above may be adjusted based on the scaling of \u03b7 and \u03b3. Also, an approximate way to think of the scaling of ( 1+tanh(\u03bb)\u03b3+1\n1\u2212tanh(\u03bb)\u03b3+1 ) is \u223c e\u03bb\u03b3+1 . As an example, for constant \u03b7 and \u03b3, we may choose v = 12 . In this case, for some constant c, our bound imposes n \u223c \u2126 (\nlog p \u03bb tanh\u03bb ,\nec\u03bb \u03b3+1\u221ap\n\u03bb log p ) . Now, same as earlier, to have low sample complexity, we must\n3In fact, for \u03bb \u2264 3, we have e\u03bb 2/2 \u2264 cosh(2\u03bb) \u2264 e2\u03bb 2 . For \u03bb > 3, cosh(2\u03bb) > 200\nhave \u03bb = O(1/p1/2(\u03b3+1)), in which case, we get a n \u223c \u2126(p1/(\u03b3+1) log p) sample requirement from the first term.\nWe note that the family Gp,\u03b7,\u03b3 is also studied in [1], and for which, an algorithm is proposed. Under certain assumptions in [1], and the restrictions: \u03b7 = O(1), and \u03b3 is large enough, the algorithm in [1] requires log p\u03bb2 samples, which is matched by the first term in our lower bound. Therefore, the algorithm in [1] is optimal, for the setting considered."}, {"heading": "5.2 Girth Bounded Graphs", "text": "The girth of a graph is defined as the length of its shortest cycle. Let Gp,g,d be the set of all graphs with girth atleast g, and maximum degree d. Note that as girth increases the learning problem becomes easier, with the extreme case of g = \u221e (i.e. trees) being solved by the well known ChowLiu algorithm[3] in O(log p) samples. We have the following theorem:\nTheorem 3. Consider the graph class Gp,g,d. For any \u03bd \u2208 (0, 1), let d\u03bd = min ( d, p 1\u2212\u03bd\ng\n)\n. If\nn \u2264 (1\u2212 \u03b4)max\n\n\n\nlog(p/2) \u03bb tanh\u03bb ,\n1+\n(\n1+tanh(\u03bb)g\u22121 1\u2212tanh(\u03bb)g\u22121\n)d\u03bd\n2\u03bb \u03bd log(p)\n\n\n\n, then pmax \u2265 \u03b4.\n5.3 Approximate d-Regular Graphs\nLet Gapproxp,d be the set of all graphs whose vertices have degree d or degree d\u2212 1. Note that this class is subset of the class of graphs with degree at most d. We have:\nTheorem 4. Consider the class Gapproxp,d . If n \u2264 (1 \u2212 \u03b4)max { log( pd4 ) \u03bb tanh\u03bb , e\u03bbd 2\u03bbde\u03bb log ( pd 4 ) } then\npmax \u2265 \u03b4.\nNote that the second term in the bound above is from [16]. Now, restricting \u03bb to prevent exponential growth in the number of samples, we get a sample requirement of n = \u2126(d2 log p). This matches the lower bound for degree d bounded graphs in [16]. However, note that Theorem 4 is stronger in the sense that the bound holds for a smaller class of graphs i.e. only approximately d-regular, and not d-bounded."}, {"heading": "5.4 Approximate Edge Bounded Graphs", "text": "Let Gapproxp,k be the set of all graphs with number of edges \u2208 [ k 2 , k ]\n. This class is a subset of the class of graphs with edges at most k. Here, we have:\nTheorem 5. Consider the class Gapproxp,k , and let k \u2265 9. If we have number of samples n \u2264 (1 \u2212\n\u03b4)max\n{\nlog( k2 ) \u03bb tanh\u03bb ,\ne\u03bb( \u221a 2k\u22121)\n2\u03bbe\u03bb( \u221a 2k+1)\nlog ( k 2 )\n}\n, then pmax \u2265 \u03b4.\nNote that the second term in the bound above is from [16]. If we restrict \u03bb to prevent exponential growth in the number of samples, we get a sample requirement of n = \u2126(k log k). Again, we match the lower bound for the edge bounded class in [16], but through a smaller class.\n6 Erdo\u030bs-R\u00e9nyi graphs G(p, c/p)\nIn this section, we relate the number of samples required to learn G \u223c G(p, c/p) for the dense case, for guaranteeing a constant average probability of error pavg. We have the following main result whose proof can be found in the Appendix.\nTheorem 6. Let G \u223c G(p, c/p), c = \u2126(p3/4 + \u01eb\u2032), \u01eb\u2032 > 0. For this class of random graphs, if pavg \u2264 1/90, then n \u2265 max (n1, n2) where:\nn1 = H(c/p)(3/80) (1\u2212 80pavg \u2212O(1/p)) \n 4\u03bbp 3 exp(\u2212 p 36 ) + 4 exp(\u2212\np 3 2 144 ) + 4\u03bb\n9\n( 1+(cosh(2\u03bb)) c2 6p )\n\n\n, n2 = p\n4 H(c/p)(1\u2212 3pavg)\u2212O(1/p)\n(6)\nRemark 3. In the denominator of the first expression, the dominating term is 4\u03bb 9 ( 1+(cosh(2\u03bb)) c2 6p ) .\nTherefore, we have the following corollary. Corollary 4. Let G \u223c G(p, c/p), c = \u2126(p3/4+\u01eb\u2032) for any \u01eb\u2032 > 0. Let pavg \u2264 1/90, then\n1. \u03bb = \u2126( \u221a p/c) : \u2126 ( \u03bbH(c/p)(cosh(2\u03bb)) c2 6p ) samples are needed.\n2. \u03bb < O( \u221a p/c) : \u2126(c log p) samples are needed. (This bound is from [1] )\nRemark 4. This means that when \u03bb = \u2126( \u221a p/c), a huge number (exponential for constant \u03bb) of samples are required. Hence, for any efficient algorithm, we require \u03bb = O (\u221a p/c )\nand in this regime O (c log p) samples are required to learn."}, {"heading": "6.1 Proof Outline", "text": "The proof skeleton is based on Lemma 2. The essence of the proof is to cover a set of graphs T , with large measure, by an exponentially small set where the KL-divergence between any covered and the covering graph is also very small. For this we use Corollary 3. The key steps in the proof are outlined below:\n1. We identify a subclass of graphs T , as in Lemma 2, whose measure is close to 1, i.e. \u00b5(T ) = 1\u2212 o(1). A natural candidate is the \u2019typical\u2019 set T p\u01eb which is defined to be a set of graphs each with ( cp2 \u2212 cp\u01eb 2 , cp 2 + cp\u01eb 2 ) edges in the graph. 2. (Path property) We show that most graphs in T have property R: there are O(p2) pairs of nodes such that every pair is well connected by O( c 2\np ) node disjoint paths of length 2 with high probability. The measure \u00b5(R|T ) = 1\u2212 \u03b41. 3. (Covering with low diameter) Every graph G in R\u22c2 T is covered by a graph G\u2032 from a covering set CR(\u03b42) such that their edge set differs only in the O(p2) nodes that are well connected. Therefore, by Corollary 3, KL-divergence between G and G\u2032 is very small (\u03b42 = O(\u03bbp2 cosh(\u03bb)\u2212c 2/p)). 4. (Efficient covering in Size) Further, the covering set CR is exponentially smaller than T . 5. (Uncovered graphs have exponentially low measure) Then we show that the uncovered\ngraphs have large KL-divergence ( O(p2\u03bb) ) but their measure \u00b5(Rc |T ) is exponentially small.\n6. Using a similar (but more involved) expression for probability of error as in Corollary 2, roughly we need O( log|T |\u03b41+\u03b42 ) samples.\nThe above technique is very general. Potentially this could be applied to other random graph classes."}, {"heading": "7 Summary", "text": "In this paper, we have explored new approaches for computing sample complexity lower bounds for Ising models. By explicitly bringing out the dependence on the weights of the model, we have shown that unless the weights are restricted, the model may be hard to learn. For example, it is hard to learn a graph which has many paths between many pairs of vertices, unless \u03bb is controlled. For the random graph setting, Gp,c/p, while achievability is possible in the c = poly log p case[1], we have shown lower bounds for c > p0.75. Closing this gap remains a problem for future consideration. The application of our approaches to other deterministic/random graph classes such as the ChungLu model[4] (a generalization of Erdo\u030bs-R\u00e9nyi graphs), or small-world graphs[18] would also be interesting."}, {"heading": "Acknowledgments", "text": "R.T. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033. K.S. and A.D. acknowledge the support of NSF via CCF 1422549, 1344364, 1344179 and DARPA STTR and a ARO YIP award."}, {"heading": "8 Appendix A - Proofs for Section 3 and Section 4", "text": ""}, {"heading": "8.1 Proof of Lemma 1", "text": "Proof. Starting with the original statement of Fano\u2019s lemma (see [5, Theorem 2.10.1]), we get:\npavg \u2265 H(G) \u2212 I(G;\u03c6(Xn))\u2212 log 2\nlog|G| a \u2265 H(G) \u2212 I(G;X\nn)\u2212 log 2 log|G| (7)\nHere we have: (a) by the Data Processing Inequality (see [5, Theorem 2.8.1])\nNow, note that:\npavg = \u2211\nG\u2208G Pr\u00b5(G).Pr\n( G\u0302 6= G )\n\u2264 max G\u2208G\nPr ( G\u0302 6= G )\n= pmax (8)"}, {"heading": "8.2 Proof of Corollary 1", "text": "Proof. We get the stated bound by picking \u00b5 to be a uniform measure on G in Lemma 1, and then using: H(G) = log|G| and I(G;Xn) \u2264 H(Xn) \u2264 np."}, {"heading": "8.3 Proof of Lemma 2", "text": "Proof. The conditional version of Fano\u2019s lemma (see [1, Lemma 9]) yields:\nE\u00b5\n[ Pr ( G\u0302 6= G ) \u2223 \u2223 \u2223G \u2208 T ] \u2265 H(G|G \u2208 T )\u2212 I(G;X n|G \u2208 T )\u2212 log 2\nlog|T | (9)\nNow,\npavg = E\u00b5\n[ Pr ( G\u0302 6= G )]\n= Pr\u00b5 (G \u2208 T )E\u00b5 [ Pr ( G\u0302 6= G ) \u2223 \u2223 \u2223G \u2208 T ] + Pr\u00b5 (G /\u2208 T )E\u00b5 [ Pr ( G\u0302 6= G ) \u2223 \u2223 \u2223G /\u2208 T ]\na \u2265 Pr\u00b5 (G \u2208 T )E\u00b5 [ Pr ( G\u0302 6= G ) \u2223 \u2223 \u2223 G \u2208 T ]\nb \u2265 \u00b5 (T ) H(G|G \u2208 T )\u2212 I(G;X n|G \u2208 T )\u2212 log 2 log|T | (10)\nHere we have: (a) since both terms in the equation before are positive. (b) by using the conditional Fano\u2019s lemma.\nAlso, note that:\nE\u00b5\n[ Pr ( G\u0302 6= G ) \u2223 \u2223 \u2223G \u2208 T ] = \u2211\nG\u2208T Pr\u00b5 (G|G \u2208 T ) .Pr\n( G\u0302 6= G )\n\u2264 max G\u2208T\nPr ( G\u0302 6= G )\n\u2264 max G\u2208G\nPr ( G\u0302 6= G )\n= pmax (11)"}, {"heading": "8.4 Proof of Corollary 2", "text": "Proof. We pick \u00b5 to be a uniform measure and use H(G) = log|G|. In addition, we upper bound the mutual information through an approach in [20] which relates it to coverings in terms of the\nKL-divergence as follows:\nI(G;Xn|G \u2208 T ) a= \u2211\nG\u2208T P\u00b5(G|G \u2208 T )D (fG(xn)\u2016fX(xn))\nb \u2264 \u2211\nG\u2208T P\u00b5(G|G \u2208 T )D (fG(xn)\u2016Q(xn)))\nc =\n\u2211 G\u2208T P\u00b5(G|G \u2208 T )D\n\nfG(x n)\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nG\u2032\u2208CT (\u01eb)\n1\n|CT (\u01eb)| fG\u2032(x\nn)\n\n\n= \u2211\nG\u2208T P\u00b5(G|G \u2208 T )\n\u2211\nxn\nfG(x n) log\n\n\n\nfG(x n)\n\u2211\nG\u2032\u2208CT (\u01eb) 1 |CT (\u01eb)|fG\u2032(x n))\n\n\n\nd \u2264 log|CT (\u01eb)|+ n\u01eb (12)\nHere we have: (a) fX(\u00b7) = \u2211 G\u2208T P\u00b5(G|G \u2208 T )fG(\u00b7) . (b) Q(\u00b7) is any distribution on {\u22121, 1}np (see [20, Section 2.1]). (c) by picking Q(\u00b7) to be the average of the set of distributions {fG(\u00b7), G \u2208 CT (\u01eb)}. (d) by lower bounding the denominator sum inside the log by only the covering element term for each G \u2208 T . Also using D (fG(xn)\u2016fG\u2032(xn)) = nD (fG\u2016f \u2032G) (\u2264 n\u01eb), since the samples are drawn i.i.d.\nPlugging these estimates in Lemma 2 gives the corollary."}, {"heading": "8.5 Proof of Lemma 3", "text": "Proof. Consider a graph G(V,E) with two nodes a and b such that there are at least d node disjoint paths of length at most \u2113 between a and b. Consider another graph G\u2032(V,E\u2032) with edge set E\u2032 \u2286 E such that E\u2032 contains only edges belonging to the d node disjoint paths of length \u2113 between a and b. All other edges are absent in E\u2032. Let P denote the set of node disjoint paths. By Griffith\u2019s inequality (see [7, Theorem 3.1] ),\nEfG [xaxb] \u2265 EfG\u2032 [xaxb] = 2PG\u2032 (xaxb = +1)\u2212 1 (13)\nHere, PG\u2032(.) denotes the probability of an event under the distribution fG\u2032 .\nWe will calculate the ratio PG\u2032 (xaxb = +1) /PG\u2032 (xaxb = \u22121). Since we have a zero-field ising model (i.e. no weight on the nodes), fG\u2032(x) = fG\u2032(\u2212x). Therefore, we have:\nPG\u2032 (xaxb = +1) PG\u2032 (xaxb = \u22121) = 2PG\u2032 (xa = +1, xb = +1) 2PG\u2032 (xa = \u22121, xb = +1) (14)\nNow consider a path p \u2208 P of length \u2113p whose end points are a and b. Consider an edge (i, j) in the path p. We say i, j disagree if xi and xj are of opposite signs. Otherwise, we say they agree. When xb = +1, xa is +1 iff there are even number of disagreements in the path p. Odd number of disagreements would correspond to xa = \u22121, when xb = +1. The location of the disagreements exactly specifies the signs on the remaining variables, when xb = +1. Let d(p) denote the number of disagreements in path p. Every agreement contributes a term exp(\u03bb) and every disagreement\ncontributes a term exp(\u2212\u03bb). Now, we use this to bound (14) as follows:\nPG\u2032 (xaxb = +1) PG\u2032 (xaxb = \u22121) a =\n\u220f\np\u2208P\n(\n\u2211\nd(p) even\ne\u03bb\u2113pe\u22122\u03bbd(p)\n)\n\u220f\np\u2208P\n(\n\u2211\nd(p) odd\ne\u03bb\u2113pe\u22122\u03bbd(p)\n)\nb =\n\u220f\np\u2208P\n( (1 + e\u22122\u03bb)\u2113p + (1\u2212 e\u22122\u03bb)\u2113p )\n\u220f p\u2208P ((1 + e\u22122\u03bb)\u2113p \u2212 (1\u2212 e\u22122\u03bb)\u2113p)\nc =\n\u220f\np\u2208P\n( 1 + (tanh(\u03bb))\u2113p )\n\u220f p\u2208P (1\u2212 (tanh(\u03bb))\u2113p) (15)\nd \u2265\n( 1 + (tanh(\u03bb))\u2113 )d (1\u2212 (tanh(\u03bb))\u2113)d (16)\nHere we have: (a) by the discussion above regarding even and odd disagreements. Further, the partition function Z (of fG\u2032) cancels in the ratio and since the paths are disjoint, the marginal splits as a product of marginals over each path. (b) using the binomial theorem to add up the even and odd terms separately. (c) \u2113p \u2264 \u2113, \u2200p \u2208 P . (d) there are d paths in P . Substituting in (13), we get:\nEfG [xaxb] \u2265 1\u2212 2\n1 + (1+(tanh(\u03bb)) \u2113)d (1\u2212(tanh(\u03bb))\u2113)d . (17)"}, {"heading": "8.6 Proof of Corollary 3", "text": "Proof. From Eq. (4), we get:\nD (fG\u2016fG\u2032) \u2264 \u2211 (s,t)\u2208E\u2212E\u2032 \u03bb (EG [xsxt]\u2212 EG\u2032 [xsxt]) +\n\u2211\n(s,t)\u2208E\u2032\u2212E \u03bb (EG\u2032 [xsxt]\u2212 EG [xsxt])\na \u2264\n\u2211\n(s,t)\u2208E\u2212E\u2032 \u03bb (1\u2212 EG\u2032 [xsxt]) +\n\u2211\n(s,t)\u2208E\u2032\u2212E \u03bb (1\u2212 EG [xsxt])\nb \u2264 2\u03bb|E \u2212 E \u2032| 1 + (1+(tanh(\u03bb)) \u2113)d\n(1\u2212(tanh(\u03bb))\u2113)d +\n2\u03bb|E\u2032 \u2212 E| 1 + (1+(tanh(\u03bb)) \u2113)d\n(1\u2212(tanh(\u03bb))\u2113)d (18)\nHere we have: (a) EG [xsxt] \u2264 1 and EG\u2032 [xsxt] \u2264 1 (b) for any (s, t) \u2208 E \u2212 E\u2032, the pair of nodes are (\u2113, d) connected. Therefore, bound on EG\u2032 [xsxt] from Lemma 3 applies. Similar bound holds for EG [xsxt] for (s, t) \u2208 E\u2032 \u2212 E."}, {"heading": "8.7 Proof of Lemma 4", "text": "Proof. Since the graphs G(V,E) and G\u2032(V,E\u2032) differ by only the edge (a, b) \u2208 E, we have: PG(xaxb = +1)\nPG(xaxb = \u22121) = e2\u03bb\nPG\u2032(xaxb = +1) PG\u2032(xaxb = \u22121) (19)\nHere, PG(\u00b7) corresponds to the probability of an event under fG. Let q = PG\u2032(xaxb = +1). Now, writing the difference of the correlations,\nEG [xaxb]\u2212 EG\u2032 [xaxb] = 2 (PG(xaxb = +1)\u2212 PG\u2032(xaxb = \u22121)) a = 2 ( e2\u03bbq\n1\u2212 q + e2\u03bbq \u2212 q )\n= 2 ( e2\u03bb \u2212 1 )\n( q \u2212 q2 1\u2212 q + e2\u03bbq )\n(20)\nHere we have: (a) by substituting from (19)\nLet h(q) = ( q\u2212q2 1\u2212q+e2\u03bbq ) . Since we have \u03bb > 0 i.e. ferromagnetic ising model, we know that q \u2208 [ 12 , 1]. Also, differentiating h(q), we get:\nh\u2032(q) = 1\u2212 2q \u2212\n( e2\u03bb \u2212 1 ) q2\n(1 \u2212 q + e2\u03bbq)2 (21)\nIt is easy to check that h\u2032(q) \u2264 0 for q \u2208 [ 12 , 1]. Thus, h(q) is a decreasing function, and so, substituting q = 1/2 in (20),\nEG [xaxb]\u2212 EG\u2032 [xaxb] \u2264 e2\u03bb \u2212 1 e2\u03bb + 1 = tanh(\u03bb) (22)\nAlso, from Eq. (4),\nD (fG\u2016fG\u2032) \u2264 \u03bb (EG [xaxb]\u2212 EG\u2032 [xaxb]) \u2264 \u03bb tanh(\u03bb) (23)"}, {"heading": "9 Appendix B - Proofs for Section 5", "text": "For the proofs in this section, we will be using the estimate of the number of samples presented in Remark 2. To recapitulate, we had the following generic statement:\nFor any graph class G and its subset T \u2282 G, suppose we can cover T with a single point (denoted by G0) with KL-radius \u03c1, i.e. for any other G \u2208 T , D (fG\u2016fG0) \u2264 \u03c1. Now, if\nn \u2264 log|T | \u03c1 (1 \u2212 \u03b4) (24)\nthen pmax \u2265 \u03b4. Note that, assuming T is growing with p, we have ignored the lower order term. So, for each of the graph classes under consideration, we shall show how to construct G0, T and compute \u03c1."}, {"heading": "9.1 Proof of Theorem 1", "text": "Proof. The graph class is Gp,\u03b7 , the set of all graphs on p vertices with at most \u03b7 (\u03b7 = o(p)) paths between any two vertices.\nConstructing G0: We consider the following basic building block. Take two vertices (s, t) and connect them. In addition, take \u03b7 \u2212 1 more vertices, and connect them to both s and t. Now, there are exactly \u03b7 paths between (s, t). There are (\u03b7 + 1) total nodes and (2\u03b7 \u2212 1) total edges. Now, take \u03b1 disjoint copies of these blocks. We note that we must have \u03b1(\u03b7 + 1) \u2264 p. We choose \u03b1 = \u230a\np \u03b7+1\n\u230b\n\u2265 p2(\u03b7+1) suffices.\nConstructing T - Ensemble 1: Starting with G0, we consider the family of graphs T obtained by removing the main (s, t) edge from one of the blocks. So, we get \u03b1 different graphs. Let Gi, i \u2208 [\u03b1], be the graph obtained by removing this edge from the ith block. Then, note that G0 and Gi only differ by a single pair (si, ti), which is (2, \u03b7) connected in Gi. From Corollary 3 we have, D (fG0\u2016fGi) \u2264 2\u03bb1+cosh(2\u03bb)\u03b7\u22121 = \u03c1. Plugging |T | = \u03b1, and \u03c1 into Eq. (24) gives us the second term for the bound in the theorem.\nConstructing T - Ensemble 2: Starting with G0, we consider the family of graphs T obtained by removing any edge from one of the blocks. So, we get \u03b1(2\u03b7 \u2212 1) \u2265 p2 different graphs. Let Gi be any such graph. Then, note that G0 and Gi only differ by a single edge. From Lemma 4 we have, D (fG0\u2016fGi) \u2264 \u03bb tanh(\u03bb) = \u03c1. Plugging |T | \u2265 p/2, and \u03c1 into Eq. (24) gives us the first term for the bound in the theorem."}, {"heading": "9.2 Proof of Theorem 2", "text": "Proof. The graph class is Gp,\u03b7,\u03b3 , the set of all graphs on p vertices with at most \u03b7 paths of length at most \u03b3 between any two vertices.\nConstructing G0: We consider the following basic building block. Take two vertices (s, t) and connect them. In addition, take \u03b7 \u2212 1 more vertices, and connect them to both s and t. Also, take another k vertex disjoint paths, each of length \u03b3 + 1, between (s, t). Now, there are exactly \u03b7 + k paths between (s, t), but at most \u03b7 paths of length at most \u03b3. There are (k\u03b3+ \u03b7+1) total nodes and (k(\u03b3 + 1) + 2\u03b7 \u2212 1) total edges. Now, take \u03b1 disjoint copies of these blocks. Note that we must choose \u03b1 and k such that \u03b1(k\u03b3 + \u03b7 + 1) \u2264 p. For some \u03bd \u2208 (0, 1), we choose \u03b1 = p\u03bd . In this case, k = t\u03bd = p 1\u2212\u03bd\u2212(\u03b7+1)\n\u03b3 suffices.\nConstructing T - Ensemble 1: Starting with G0, we consider the family of graphs T obtained by removing the main (s, t) edge from one of the blocks. So, we get \u03b1 different graphs. Let Gi, i \u2208 [\u03b1], be the graph obtained by removing this edge from the ith block. Then, note that G0 and Gi only differ by a single pair (si, ti), which is (2, \u03b7\u22121) connected and also (t\u03bd , \u03b3+1) connected, in Gi. Based on the proof of Lemma 3, the estimate of D (fGi\u2016fG0) can be recomputed by handling the two different sets of correlation contributions from the two sets of node disjoint paths, and then combining them based on the probabilities. We get, D (fG0\u2016fGi) \u2264 2\u03bb\n1+\n[\ncosh(2\u03bb)\u03b7\u22121 ( 1+tanh(\u03bb)\u03b3+1\n1\u2212tanh(\u03bb)\u03b3+1\n)t\u03bd ] = \u03c1.\nPlugging |T | = \u03b1, and \u03c1 into Eq. (24) gives us the second term for the bound in the theorem. Constructing T - Ensemble 2: Starting with G0, we consider the family of graphs T obtained by removing any edge from one of the blocks. So, we get \u03b1(k(\u03b3 + 1) + 2\u03b7 \u2212 1) \u2265 p2 different graphs. Let Gi be any such graph. Then, note that G0 and Gi only differ by a single edge. From Lemma 4 we have, D (fG0\u2016fGi) \u2264 \u03bb tanh(\u03bb) = \u03c1. Plugging |T | and \u03c1 into Eq. (24) gives us the second term for the bound in the theorem."}, {"heading": "9.3 Proof of Theorem 3", "text": "Proof. The graph class is Gp,g,d, the set of all graphs on p vertices with girth atleast g and degree at most d.\nConstructing G0: We consider the following basic building block. Take two vertices (s, t) and connect them. In addition, take k vertex disjoint paths, each of length g \u2212 1 between (s, t). Now, there are exactly k paths between (s, t). There are (k(g \u2212 2) + 2) total nodes and (k(g \u2212 1) + 1) total edges.\nNow, take \u03b1 disjoint copies of these blocks. Note that we must choose \u03b1 and k such that \u03b1(k(g \u2212 2) + 2) \u2264 p. For some \u03bd \u2208 (0, 1), we choose \u03b1 = p\u03bd . In this case, k = d\u03bd = min ( d, p 1\u2212\u03bd\ng\n)\nsuffices.\nConstructing T - Ensemble 1: Starting with G0, we consider the family of graphs T obtained by removing the main (s, t) edge from one of the blocks. So, we get \u03b1 different graphs. Let Gi, i \u2208 [\u03b1], be the graph obtained by removing this edge from the ith block. Then, note that G0 and Gi only differ by a single pair (si, ti), which is (d\u03bd , g \u2212 1) connected in Gi. From Corollary 3 we have, D (fG0\u2016fGi) \u2264 2\u03bb\n1+ ( 1+tanh(\u03bb)g\u22121 1\u2212tanh(\u03bb)g\u22121\n)d\u03bd = \u03c1. Plugging |T | = \u03b1, and \u03c1 into Eq. (24) gives us the\nsecond term for the bound in the theorem.\nConstructing T - Ensemble 2: Starting with G0, we consider the family of graphs T obtained by removing any edge from one of the blocks. So, we get \u03b1(k(g \u2212 1) + 1) \u2265 p2 different graphs. Let Gi be any such graph. Then, note that G0 and Gi only differ by a single edge. From Lemma 4 we have, D (fG0\u2016fGi) \u2264 \u03bb tanh(\u03bb) = \u03c1. Plugging |T | and \u03c1 into Eq. (24) gives us the second term for the bound in the theorem."}, {"heading": "9.4 Proof of Theorem 4", "text": "Proof. The graph class is Gapproxp,d , the set of all graphs on p vertices with degree either d or d \u2212 1 (we assume that p is a multiple of d+ 1 - if not, we can instead look at a smaller class by ignoring at most d vertices). The construction here is the same as in [16].\nConstructing G0: We divide the vertices into p/(d+ 1) groups, each of size d+ 1, and then form cliques in each group.\nConstructing T : Starting with G0, we consider the family of graphs T obtained by removing any one edge. Thus, we get pd+1 ( d+1 2 )\n\u2265 pd4 such graphs. Also, any such graph, Gi, differs from G0 by a single edge, and also, differs only in a pair that is part of a clique minus one edge. So, combining the estimates from [16] and Lemma 4, we have, D (fG0\u2016fGi) \u2264 min ( 2\u03bbde\u03bb e\u03bbd , \u03bb tanh(\u03bb) ) = \u03c1. Plugging |T | and \u03c1 into Eq. (24) gives us the theorem."}, {"heading": "9.5 Proof of Theorem 5", "text": "Proof. The graph class is Gapproxp,k , the set of all graphs on p vertices with at most k edges. The construction here is the same as in [16]\nConstructing G0: We choose a largest possible number of vertices m such that we can have a clique on them i.e. (\nm 2\n) \u2264 k. Then, \u221a 2k + 1 \u2265 m \u2265 \u221a 2k \u2212 1. We ignore any unused vertices.\nConstructing T : Starting with G0, we consider the family of graphs T obtained by removing any one edge. Thus, we get (\nm 2\n)\n\u2265 k2 such graphs. Also, any such graph, Gi, differs from G0 by a single edge, and also, differs only in a pair that is part of a clique minus one edge. So, combining the estimates from [16] and Lemma 4, we have, D (fG0\u2016fGi) \u2264 min ( 2\u03bbe\u03bb( \u221a 2k+1)\ne\u03bb( \u221a 2k\u22121) , \u03bb tanh(\u03bb) ) = \u03c1.\nPlugging |T | and \u03c1 into Eq. (24) gives us the theorem."}, {"heading": "10 Appendix C: Proof of Theorem 6", "text": "In this section, we outline the covering arguments in detail along with a Fano\u2019s Lemma variant to prove Theorem 6.\nWe recall some definitions and results from [1].\nDefinition 3. Let T n\u01eb = {G : |d\u0304(G)\u2212 c| \u2264 c\u01eb} denote the \u01eb-typical set of graphs where d\u0304(G) is the ratio of sum of degree of nodes to the total number of nodes.\nA graph G on p nodes is drawn according to the distribution characterizing the Erdo\u030bs-R\u00e9nyi ensemble G(p, c/p) (also denoted GER without the parameter c). Then n i.i.d samples Xn = X\n(1), . . .X(n) are drawn according to fG(x) with the scalar weight \u03bb > 0. Let H(\u00b7) denote the binary entropy function.\nLemma 5. (Lemma 8, 9 and Proof of Theorem 4 in [1] ) The \u01eb- typical set satisfies:\n1. PG\u223cG(p,c/p) (G \u2208 T p\u01eb ) = 1\u2212 ap where ap \u2192 0 as p \u2192 \u221e.\n2. 2\u2212( p 2)H(c/p)(1+\u01eb) \u2264 PG\u223cG(p,c/p) (G) \u2264 2\u2212( p 2)H(c/p).\n3. (1\u2212 \u01eb)2(p2)H(c/p) \u2264 |T p\u01eb | \u2264 2( p 2)H(c/p)(1+\u01eb) for sufficiently large p.\n4. H(G|G \u2208 T p\u01eb ) \u2265 ( p 2 ) H(c/p).\n5. (Conditional Fano\u2019s Inequality:)\nP (G\u0302(Xn) 6= G|G \u2208 T p\u01eb ) \u2265 H(G|G \u2208 T p\u01eb )\u2212 I(G;Xn|G \u2208 T p\u01eb )\u2212 1\nlog2|T p\u01eb | (25)"}, {"heading": "10.1 Covering Argument through Fano\u2019s Inequality", "text": "Now, we consider the random graph class G(p, c/p). Consider a learning algorithm \u03c6. Given a graph G \u223c G(p, c/p), and n samples Xn drawn according to distribution fG(x) (with weight \u03bb > 0), let G\u0302 = \u03c6 (Xn) be the output of the learning algorithm. Let fX(.) be the marginal distribution of Xn sampled as described above. Then the following holds for pavg :\npavg = EG(p,c/p)\n[\nEXn\u223cfG\n[\n1G\u03026=G\n]]\n\u2265 PrG(p,c/p) (G \u2208 T p\u01eb )E [ EXn\u223cfG [ 1G\u0302 6=G ] |G \u2208 T p\u01eb ]\na = (1\u2212 ap)E\n[\nEXn\u223cfG\n[\n1G\u03026=G\n] |G \u2208 T p\u01eb ]\n= (1\u2212 ap)p\u2032avg (26) Here, (a) is due to Lemma 5. Here, p\u2032avg is the average probability of error under the conditional distribution obtained by conditioning G(p, c/p) on the event G \u2208 T p\u01eb . Now, consider G sampled according to the conditional distribution G(p, c/p)|G \u2208 T p\u01eb . Then, n samples Xn are drawn i.i.d according to fG(x). G\u0302 = \u03c6(xn) is the output of the learning algorithm. Applying conditional Fano\u2019s inequality from (25) and using estimates from Lemma 5, we have:\np\u2032avg = PG\u223cG(p,c/p)|G\u2208T p\u01eb ,Xn\u223cfG(x) ( G\u0302 6= G )\na \u2265\n(\np 2\n)\nH(c/p)\u2212 I(G;Xn|G \u2208 T p\u01eb )\u2212 1 log2|T p\u01eb |\nb \u2265\n(\np 2\n)\nH(c/p)\u2212 I(G;Xn|G \u2208 T p\u01eb )\u2212 1 (\np 2\n)\nH(c/p)(1 + \u01eb)\n= 1 1 + \u01eb \u2212 I(G;X n|G \u2208 T p\u01eb ) (\np 2\n) H(c/p)(1 + \u01eb) \u2212 1(p\n2\n) H(c/p)(1 + \u01eb) (27)\nNow, we upper bound I(G;Xn|G \u2208 T p\u01eb ). Now, use a result by Yang and Barron [20] to bound this term.\nI(G;Xn|G \u2208 T p\u01eb ) = \u2211\nG\nPG(p,c/p)|G\u2208T p\u01eb (G)D (fG(x n)\u2016fX(xn))\n\u2264 \u2211\nG\nPG(p,c/p)|G\u2208T p\u01eb (G)D (fG(x n)\u2016Q(xn)) (28)\nwhere Q(\u00b7) is any distribution on {\u22121, 1}np. Now, we choose this distribution to be the average of {fG(.), G \u2208 S} where the set S \u2286 T p\u01eb is a set of graphs that is used to \u2019cover\u2019 all the graphs in T p\u01eb . Now, we describe the set S together with the covering rules when c = \u2126(p3/4 + \u01eb\u2032), \u01eb\u2032 > 0."}, {"heading": "10.2 The covering set S: dense case", "text": "First, we discuss certain properties that most graphs in T p\u01eb possess building on Lemma 3. Using these properties, we describe the covering set S.\nConsider a graph G on p nodes. Divide the node set into three equal parts A, B and C of equal size (p/3). Two nodes a \u2208 A and c \u2208 C are (2, \u03b3) connected through B if there are at least \u03b3 nodes in B which are connected to both a and c (with parameter \u03b3 as defined in Section 4.3). Let D(G) \u2286 A\u00d7 C be the set of all pairs (a, c) : a \u2208 A, c \u2208 C such that nodes a and c are (2, \u03b3) connected. Let |D(G)| = mA,C . Let E(G) denote the edge in graph G.\n10.2.1 Technical results on D(G) Nodes a \u2208 A and c \u2208 C are clearly (2, d)-connected if there are d nodes in B which are connected to both a and b as it will mean d disjoint paths connecting a and b through the partition B. Now if G \u223c G(p, c/p), then expected number of disjoint paths between a and c through B is p3 c 2 p2 since\nthe probability of a path existing through a node b \u2208 B is c2p2 . Let na,c be the number of such paths between a and c. The event that there is a path through b1 \u2208 B is independent of the event that there is a path through b2 \u2208 B, applying chernoff bounds (see [12]) for p/3 independent bernoulli variables we have: Lemma 6. Pr ( na,c \u2264 c 2 3p \u2212 \u221a 4p log p )\n\u2264 1p2 for any two nodes a \u2208 A and c \u2208 C when G \u223c G(p, c/p). The bound is useful for c = \u2126(p 3 4+\u01eb \u2032 ), \u01eb\u2032 > 0.\nTherefore, in this regime of dense graphs, any two nodes in partitions A and C are (2, \u03b3 = c2/6p) connected with probability 1\u2212 1p2 .\nGiven a \u2208 A and c \u2208 C, the probability that a and c are (2, \u03b3) connected is 1 \u2212 1p2 . The expected number of pairs in A \u00d7 C that are (2, \u03b3) connected is (p/3)2 (1 \u2212 1p2 ). Let D(G) \u2286 A\u00d7 C be the set of all pairs (a, c) : a \u2208 A, c \u2208 C such that nodes a and c are (2, \u03b3) connected. Let mA,C = |D|. Then we have the following concentration result on mA,C : Lemma 7. Pr (\nmA,C \u2264 12 (p/3) 2 )\n\u2264 bp = p/3 exp(\u2212(p/36)) when G \u223c G(p, c/p), c = \u2126(p 3 4+\u01eb \u2032 ), \u01eb\u2032 > 0.\nProof. The event that the pair (a1, c1) \u2208 A \u00d7 C is (2, \u03b3) connected and the event that the pair (a2, c2) \u2208 A\u00d7C are dependent if a1 = a2 or c1 = c2. Therefore, we need to obtain a concentration result for the case when you have (p/3)2 Bernoulli variables (each corresponding to a pair in A\u00d7C being (2, \u03b3) connected ) which are dependent.\nConsider a complete bipartite graph between A and C. Since, |A| = |C| = p/3. Edges of every complete bipartite graph Kp/3,p/3 can be decomposed into a disjoint union of p/3 perfect matchings between the partitions (this is due to Hall\u2019s Theorem repeatedly applied on graphs obtained by\nremoving perfect matchings. See [9] ). Therefore, the set of pairs A \u00d7 C = p/3 \u22c3\n1=1 Mi where Mi =\n{(ai1 , ci1), . . . (aip/3 , cip/3)} where all for any j 6= k, aik 6= aij and cik 6= cij . Let us focus on the number of pairs which are (2, \u03b3) connected between A and C in a random graph G \u223c G(p, c/p). If mA,C \u2264 12 (p/3)\n2, then at least for one i, the number of pairs in G among the pairs in Mi that are (2, \u03b3) is at most 12 (p/3). This is because (p/3)2 = \u2211\ni |Mi|. Let Eci denote the event that number of edges in G among pairs in Mi is at most 12 (p/3).\nPr\n(\nmA,C \u2264 1\n2 (p/3)2\n) \u2264 Pr ( \u22c3\ni\nEci\n)\n\u2264 \u2211\ni\nPr (Eci ) . (29)\nThe last inequality is due to union bound. A pair in Mi being (2, \u03b3) connected happens with probability 1\u2212 1/p2 from Lemma 6. Since it is a perfect matching, all these events are independent. Let cG(Mi) be the number of pairs in Mi which are (2, \u03b3) connected. Therefore, applying a chernoff bound (see [12] Theorem 18.22) for independent Bernoulli variables, we have:\nPr(Eci ) = Pr (cG(Mi) \u2264 E[(p/3)(1/2)) = Pr ( cG(Mi) \u2264 E[cG(Mi)]\u2212 (p/3)(1/2\u2212 1/p2) )\n(chernoff)\n\u2264 exp ( \u2212(p/3)2(1/2\u2212 1/p2)2/2(p/3) )\na \u2264 exp (\u2212(p/36))\n(a) holds for large p, i.e. for p \u2265 p0 such that (1/2\u2212 1/p2o)2 \u2265 1/6. Simple calculation shows that p0 can be taken to be greater than or equal to 10.\nNow, applying this to (29), we have \u2200 p \u2265 10:\nPr\n(\nmA,C \u2264 1\n2 (p/3)\n2\n)\n\u2264 bp = p/3 exp(\u2212(p/36)). (30)\nLet E(G) be the set of edges in G.\nLemma 8. Pr (\u2223 \u2223\n\u2223 ||E(G)\u22c2D(G)| |D(G)| \u2212 cp\n\u2223 \u2223 \u2223 \u2265 cp\u01eb \u2223 \u2223 \u2223mA,C \u2265 12 (p/3) 2 ) \u2264 2 exp(\u2212 c2\u01eb236 ) = rc when G \u223c G(p, c/p), c = \u2126(p 3 4+\u01eb \u2032 ), \u01eb\u2032 > 0.\nProof. The presence of an edge between a pair on nodes in A \u00d7 C is independent of the value of mA,C or whether the pair belongs to D. This is because a pair of nodes being (2, \u03b3) connected depends on the rest of the graph and not on the edges in D(G). Given |D| \u2265 12 (p/3)2, |E(G)\u22c2D(G)| = \u2211\n(i,j)\u2208D 1(i,j)\u2208E(G) is the sum of least 1 2 (p/3) 2 bernoulli variables each with suc-\ncess probability c/p. Therefore, applying chernoff bounds we have:\nPr\n(\u2223\n\u2223 \u2223 \u2223 ||E(G)\u22c2D(G)| |D(G)| \u2212 c p\n\u2223 \u2223 \u2223 \u2223 \u2264 c p \u01eb \u2223 \u2223 \u2223 \u2223 mA,C \u2265 1 2 (p/3) 2 ) \u2264 2 exp ( \u2212 c 2\u01eb2 p22|D| |D| 2 )\na \u2264 2 exp\n(\n\u2212c 2\u01eb2\n4 (1/3)2\n)\n(31)\n(32)\n(a)- This is because |D| \u2265 12 (p/3)2.\nLemma 9. Pr ((\u2223 \u2223\n\u2223 ||E(G)\u22c2D(G)| |D(G)| \u2212 cp\n\u2223 \u2223 \u2223 \u2265 cp\u01eb ) \u22c3 ( mA,C \u2264 12 (p/3) 2 ))\n\u2264 bp + rc, c = \u2126(p 3 4+\u01eb \u2032 ), \u01eb\u2032 > 0.\nProof.\nPr\n((\u2223\n\u2223 \u2223 \u2223 ||E(G)\u22c2D(G)| |D(G)| \u2212 c p\n\u2223 \u2223 \u2223 \u2223 \u2265 c p \u01eb ) \u22c3 ( mA,C \u2264 1 2 (p/3) 2 )) a \u2264 Pr ( mA,C \u2264 1 2 (p/3) 2 )\n+Pr\n(\u2223\n\u2223 \u2223 \u2223 ||E(G)\u22c2D(G)| |D(G)| \u2212 c p\n\u2223 \u2223 \u2223 \u2223 \u2265 c p \u01eb \u2223 \u2223 \u2223 \u2223 mA,C \u2265 1 2 (p/3) 2 )\n\u2264 bp + rc (33)\n(a)- is because Pr(A \u22c3 B) \u2264 Pr(A) + Pr(Ac)Pr (B|Ac) \u2264 Pr(A) + Pr (B|Ac) ."}, {"heading": "10.2.2 Covering set S and its properties", "text": "For any graph G, let GD=\u2205 be the graph obtained by removing any edge (if present) between the pairs of nodes in D(G). Let V be the set of graphs on p nodes such that |D| = mA,C \u2265 12 (p/3) 2 and | |E(G) \u22c2D(G)|\n|D(G)| \u2212 cp | \u2264 c\u01ebp . Define Rp\u01eb = T p\u01eb \u22c2V to be the set of graphs that are in the \u01eb typical\nset and also belongs to V . We have seen high probability estimates on mA,C when G \u223c G(p, c/p). Now, we state an estimate for Pr(Rp\u01eb ) when G \u223c G(p, c/p)|T p\u01eb . Lemma 10. PrG(p,c/p) ((Rp\u01eb )c |G \u2208 T p\u01eb ) \u2264 bp+rc1\u2212ap \u2264 2(bp+rc) for large p, c = \u2126(p 3 4+\u01eb \u2032 ), \u01eb\u2032 > 0.\nProof. Expanding the probability expression in Lemma 9 through conditioning on the events G \u2208 T p\u01eb and G \u2208 (T p\u01eb )c, we have:\nPr\n((\u2223\n\u2223 \u2223 \u2223 |E(G)\u22c2D(G)| |D(G)| \u2212 c p\n\u2223 \u2223 \u2223 \u2223 \u2265 c p \u01eb ) \u22c3 ( mA,C \u2264 1 2 (p/3) 2 ) |G \u2208 T p\u01eb ) Pr(G \u2208 T p\u01eb ) \u2264 bp + rc\n(34)\nThis implies:\nPr\n((\u2223\n\u2223 \u2223 \u2223 ||E(G)\u22c2D(G)| |D(G)| \u2212 c p\n\u2223 \u2223 \u2223 \u2223 \u2265 c p \u01eb ) \u22c3 ( mA,C \u2264 1 2 (p/3)2 ) |G \u2208 T p\u01eb ) a \u2264 bp + rc\n1\u2212 ap b \u2264 2(bp + rc) (35)\n(a) is because of estimate 1 in Lemma 5. (b)- For large p, ap can be made smaller than 1/2.\nLemma 11. [8](Size of a Typical set) For any 0 \u2264 p \u2264 1, m \u2208 Z+ and a small \u01eb > 0, let Nm,p\u01eb = {x \u2208 {0, 1}m : \u2223 \u2223 \u2223 |{i:xi=1}| m \u2212 p \u2223 \u2223 \u2223 \u2264 p\u01eb}. Then, |Nm,p\u01eb | = \u2211\nmp(1\u2212\u01eb)\u2264q\u2264mp(1+\u01eb)\n(\nm q\n)\n. Further,\n|Nm,p\u01eb | \u2265 (1\u2212 \u01eb)2mH(p)(1\u2212\u01eb). Definition 4. (Covering set) S = {GD=\u2205|G \u2208 Rp\u01eb}.\nNow, we describe the covering rule for the set Rp\u01eb . For any G \u2208 Rp\u01eb , we cover G by GD=\u2205. Note that, given G, by definition, GD=\u2205 is unique. Therefore, there is no ambiguity and no necessity to break ties. Since, the set D(G) is dependent only on the edges outside the set of pairs A \u00d7 C, D (GD=\u2205) = D(G). Therefore, from a given G\u2032 \u2208 Rp\u01eb , by adding different sets of edges in D(G\u2032), it is possible to obtain elements in Rp covered by G\u2032. We now estimate the size of the covering set S relative to the size of T p\u01eb . We show that it is small.\nLemma 12. log|S| log|T p\u01eb | \u2264 9 10\n(\n1+ 119 \u01eb\n1+\u01eb\n)\n\u2212 O(1/p) for large p.\nProof. By definition of Rp\u01eb , for every G \u2208 Rp\u01eb , |D| \u2265 12 (p/3)2 and the number of edges is in D is at least 12 (p/3)\n2(c/p)(1 + \u01eb). And the graph that covers G is GD=\u2205 where all edges from D are removed if present in G. Let any set of q edges be added to GD=\u2205 among the pairs of nodes in D to form G\u2032 such that |D|(c/p)(1 \u2212 \u01eb) \u2264 q \u2264 |D|(c/p)(1 + \u01eb). Then, any such G\u2032 belongs to Rp\u01eb . This follows from the definition of the Rp\u01eb . And G\u2032 is still uniquely covered by GD=\u2205. Uniqueness follows from the discussion that precedes this Lemma. For every covering graph Gc \u2208 S, there are at least \u2211\n|D(Gc)|(c/p)(1\u2212\u01eb)\u2264q\u2264|D(Gc)|(c/p)(1+\u01eb)\n(|D(Gc)| q ) distinct graphs G \u2208 Rp\u01eb uniquely covered by\nGc. Using these observations, we upper bound |S| as follows:\nlog|T p\u01eb | \u2265 log ( \u2211\nGc\u2208S |{G \u2208 Rp\u01eb : G is covered by Gc}|\n)\n\u2265 log\n\n\n\u2211\nGc\u2208S\n\u2211\n|D(Gc)|(c/p)(1\u2212\u01eb)\u2264q\u2264|D(Gc)|(c/p)(1+\u01eb)\n(|D(Gc)| q )\n\n\na \u2265 log\n(\n\u2211\nGc\u2208S |N |D(Gc)|,(c/p)\u01eb |\n)\na \u2265 log|S|+ log ( (1\u2212 \u01eb)2 12 (p/3)2H(c/p)(1\u2212\u01eb) )\n(36)\n(a)- This is due to Lemma 11 and the fact that |D| \u2265 12 (p/3)2. Using (36), we have the following chain of inequalities:\nlog|S| log|T p\u01eb | a \u2264 1\u2212 log(1\u2212 \u01eb) + 1 2 (1\u2212 \u01eb)(p/3)2H(c/p) (\np 2\n)\nH(c/p)(1 + \u01eb)\n= 1\u2212O(1/p)\u2212 (1\u2212 \u01eb) 9(1 + \u01eb)\n(p/p\u2212 1) b \u2264 1\u2212O(1/p)\u2212 (1\u2212 \u01eb)\n10(1 + \u01eb)\n= 9\n10\n( 1 + 119 \u01eb\n1 + \u01eb\n)\n\u2212O(1/p) (37)\n(a)- Upper bound is used from Lemma 5. (b)- This is valid for p \u2265 10."}, {"heading": "10.3 Completing the covering argument:dense case", "text": "We now resume the covering argument from Section 10.1. Having specified the covering set S, let the distribution Q(xn) = 1|S| \u2211\nG\u2208S fG(x\nn). Let G1 \u2208 S be some arbitrary graph. Recalling the\nupper bound on I(G;Xn|G \u2208 T p\u01eb ) from (28), we have: I(G;Xn|G \u2208 T p\u01eb ) \u2264 \u2211\nG\u2208T p\u01eb\nPG(p,c/p)|G\u2208T p\u01eb (G)D (fG(x n)\u2016Q(xn))\n= \u2211\nG\u2208T p\u01eb\nPG(p,c/p)|G\u2208T p\u01eb (G) \u2211\nxn\nfG(x n) log\nfG(x n\n1 |S|\n\u2211\nG\u2208S fG(xn)\n\u2264 log|S|+ \u2211\nG\u2208(Rp\u01eb )c PG(p,c/p)|G\u2208T p\u01eb (G)D (fG(x\nn)\u2016fG1(xn))\n+ \u2211\nG\u2208Rp\u01eb\nPG(p,c/p)|G\u2208T p\u01eb (G)D ( fG(x n)\u2016fGD=\u2205(xn) )\na \u2264 log|S|+ 2n\u03bb\n(\np\n2\n)\n(2bp + 2rc) + n2\u03bb(p/3) 2 1\n1 + (1+(tanh(\u03bb)) 2)\u03b3 (1\u2212(tanh(\u03bb))2)\u03b3 (38)\n\u2264 log|S|+ n ( 2\u03bb ( p\n2\n)\n(2bp + 2rc) + 2\u03bb(p/3)2\n1 + (cosh(2\u03bb))\u03b3\n)\n(39)\n(40)\nJustifications are:\n(a) D (fG(xn)\u2016fG1(xn)) = nD (fG(x)\u2016fG1(x)) (due to independence of the n samples) and D (fG(x)\u2016fG1(x)) \u2264 \u2211\ns,t\u2208V,s6=t\n( \u03b8s,t \u2212 \u03b8\u2032s,t ) (EG [xsxt]\u2212 EG\u2032 [xsxt]) \u2264 \u03bb(2) ( p 2 ) . This is\nbecause there are ( p 2 ) edges and correlation is at most 1. Upper bound for P ((Rp\u01eb)c) is from Lemma 10. G and GD=\u2205 differ only in the edges present in D and irrespective of the edges in D, all node pairs in D are (2, \u03b3) connected by definition of D. Therefore, the second set of terms in (38) is bounded using Lemma 3.\nSubstituting the upper bound (39) in (27) and rearranging terms, we have the following lower bound for the number of samples needed when c = \u2126(p3/4+\u01eb \u2032 ), \u01eb\u2032 > 0:\nn \u2265 ( p 2 ) H(c/p)(1 + \u01eb) (\n2\u03bb ( p 2 ) (2bp + 2rc) + 2\u03bb(p/3)2 1+(cosh(2\u03bb))\u03b3\n)\n(\n1 1 + \u01eb \u2212 pavg 1\u2212 ap \u2212 log|S|(p\n2\n) H(c/p)(1 + \u01eb) \u2212 1(p\n2\n)\nH(c/p)(1 + \u01eb)\n)\na \u2265 H(c/p)(1 + \u01eb)(\n(4\u03bbp/3) exp(\u2212(p/36) + 4 exp(\u2212 c2\u01eb236 )) + (4/9)\u03bb 1+(cosh(2\u03bb))\u03b3\n)\n(\n1\n10\n( 1\u2212 119 \u01eb 1 + \u01eb ) \u2212 pavg 1\u2212 ap \u2212O(1/p) )\n\u01eb=1/2\n\u2265 H(c/p)(3/2)( (4\u03bbp/3) exp(\u2212(p/36) + 4 exp(\u2212 c2144 )) + (4/9)\u03bb 1+(cosh(2\u03bb))\u03b3 )\n(\n1 40 \u2212 pavg 1\u2212 ap \u2212O(1/p)\n)\nlarge p \u2265 H(c/p)(3/2)( (4\u03bbp/3) exp(\u2212(p/36) + 4 exp(\u2212 c2144 )) + (4/9)\u03bb 1+(cosh(2\u03bb))\u03b3 )\n(\n1\n40 \u2212 2pavg \u2212O(1/p)\n)\nc=\u2126(p3/4),\u03b3= c 2\n6p \u2265 H(c/p)(3/2)( (4\u03bbp/3) exp(\u2212(p/36)) + 4 exp(\u2212 p 3 2\n144 ) + (4/9)\u03bb\n1+(cosh(2\u03bb)) c2 6p\n)\n1\n40 (1\u2212 80pavg \u2212O(1/p))\n(41)\n(42)\n(a)- This is obtained by substituting all the bounds for bp and rc and log|S| from Section 10.2. From counting arguments in [1], we have the following lower bound for G(p, c/p). Lemma 13. [1] Let G \u223c G(p, c/p). Then the average error pavg and the number of samples for this random graph class must satisfy:\nn \u2265 ( p 2 )\np H(c/p) (1\u2212 \u01eb\u2212 pavg(1 + \u01eb))\u2212O(1/p) (43)\nfor any constant \u01eb > 0.\nCombining Lemma 13 with \u01eb = 1/2 and (41), we have the result in Theorem 6."}], "references": [{"title": "Highdimensional structure estimation in ising models: Local separation criterion", "author": ["Animashree Anandkumar", "Vincent YF Tan", "Furong Huang", "Alan S Willsky"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Complex Graphs and Networks", "author": ["Fan Chung", "Linyuan Lu"], "venue": "American Mathematical Society,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Markov random field texture models", "author": ["G. Cross", "A. Jain"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1983}, {"title": "Ising models on locally tree-like graphs", "author": ["Amir Dembo", "Andrea Montanari"], "venue": "The Annals of Applied Probability, 20(2):565\u2013592,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Network information theory", "author": ["Abbas El Gamal", "Young-Han Kim"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Perfect matchings in o(n\\logn) time in regular bipartite graphs", "author": ["Ashish Goel", "Michael Kapralov", "Sanjeev Khanna"], "venue": "SIAM Journal on Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Markov random field models of digitized image texture", "author": ["M. Hassner", "J. Sklansky"], "venue": "In ICPR78,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1978}, {"title": "Beitrag zur theorie der ferromagnetismus", "author": ["E. Ising"], "venue": "Zeitschrift fu\u0308r Physik,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1925}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Schutze"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Minimax rates of estimation for highdimensional linear regression over lq-balls", "author": ["Garvesh Raskutti", "Martin J. Wainwright", "Bin Yu"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Spatial statistics", "author": ["B.D. Ripley"], "venue": "Wiley, New York,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1981}, {"title": "Information-theoretic limits of selecting binary graphical models in high dimensions", "author": ["Narayana P Santhanam", "Martin J Wainwright"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "On the difficulty of learning power law graphical models", "author": ["R. Tandon", "P. Ravikumar"], "venue": "IEEE International Symposium on Information Theory (ISIT),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Collective dynamics of \u2019small-world", "author": ["Duncan J. Watts", "Steven H. Strogatz"], "venue": "networks. Nature,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Markov image modeling", "author": ["J.W. Woods"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1978}, {"title": "Information-theoretic determination of minimax rates of convergence", "author": ["Yuhong Yang", "Andrew Barron"], "venue": "Annals of Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}], "referenceMentions": [{"referenceID": 10, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 156, "endOffset": 160}, {"referenceID": 4, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 179, "endOffset": 190}, {"referenceID": 8, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 179, "endOffset": 190}, {"referenceID": 16, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 179, "endOffset": 190}, {"referenceID": 9, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 212, "endOffset": 216}, {"referenceID": 12, "context": "They are thus widely used in a number of machine learning domains where there are a large number of random variables, including natural language processing [13], image processing [6, 10, 19], statistical physics [11], and spatial statistics [15].", "startOffset": 241, "endOffset": 245}, {"referenceID": 14, "context": "[2, 17] for instance derive such bounds for the case of degree-bounded and power-law graph classes respectively.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "This approach however is purely graph-theoretic, and thus fails to capture the interaction of the graphical model parameters with the graph structural constraints, and thus typically provides suboptimal lower bounds (as also observed in [16]).", "startOffset": 237, "endOffset": 241}, {"referenceID": 13, "context": "classes of bounded degree and bounded edge graphs for Ising models, [16] required fairly extensive arguments in using the above approach to provide lower bounds.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "As corollaries of this framework, we not only recover the results in [16] for the simple cases of degree and edge bounded graphs, but to several more classes of graphs, for which achievability results have already been proposed[1].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "As corollaries of this framework, we not only recover the results in [16] for the simple cases of degree and edge bounded graphs, but to several more classes of graphs, for which achievability results have already been proposed[1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 0, "context": "Here, we show that under a certain scaling of the edge-weights \u03bb, Gp,c/p requires exponentially many samples, as opposed to a polynomial requirement suggested from earlier bounds[1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "3 Fano\u2019s Lemma and Variants Fano\u2019s lemma [5] is a primary tool for obtaining bounds on the average probability of error, pavg .", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "In contrast to the trivial upper bound used in the corollary above, we next use a tighter bound from [20], which relates the mutual information to coverings in terms of the KL-divergence, applied to Lemma 2.", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 11, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 13, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 17, "context": "We note that Fano\u2019s lemma and variants described in this section are standard, and have been applied to a number of problems in statistical estimation [1, 14, 16, 20, 21].", "startOffset": 151, "endOffset": 170}, {"referenceID": 13, "context": "A simple strategy is to simply bound it by its symmetric divergence[16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "Indeed, when s, t are part of a clique[16], this is achieved since the large number of connections between them force a higher probability of agreement i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": ", albeit with node overlaps) which allows for a stronger bound of \u223c 1\u2212 \u03bbke e\u03bbk (see [16])2 Now, as discussed earlier, a high correlation between a pair of nodes contributes a small term to the KL-divergence.", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "Both the bound from [16] and the bound from Lemma 3 have exponential asymptotic behaviour (i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "For smaller \u03bb, the bound from [16] is strictly better.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "This is analogous in principle to the argument in [16] used for establishing hardness of learning of a set of graphs each of which is obtained by removing a single edge from a clique, still ensuring many short paths between any two vertices.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We note that the family Gp,\u03b7,\u03b3 is also studied in [1], and for which, an algorithm is proposed.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "Under certain assumptions in [1], and the restrictions: \u03b7 = O(1), and \u03b3 is large enough, the algorithm in [1] requires log p \u03bb2 samples, which is matched by the first term in our lower bound.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "Under certain assumptions in [1], and the restrictions: \u03b7 = O(1), and \u03b3 is large enough, the algorithm in [1] requires log p \u03bb2 samples, which is matched by the first term in our lower bound.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Therefore, the algorithm in [1] is optimal, for the setting considered.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "trees) being solved by the well known ChowLiu algorithm[3] in O(log p) samples.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "Note that the second term in the bound above is from [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "This matches the lower bound for degree d bounded graphs in [16].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Note that the second term in the bound above is from [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "Again, we match the lower bound for the edge bounded class in [16], but through a smaller class.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "(This bound is from [1] ) Remark 4.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "For the random graph setting, Gp,c/p, while achievability is possible in the c = poly log p case[1], we have shown lower bounds for c > p.", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "The application of our approaches to other deterministic/random graph classes such as the ChungLu model[4] (a generalization of Erd\u0151s-R\u00e9nyi graphs), or small-world graphs[18] would also be interesting.", "startOffset": 103, "endOffset": 106}, {"referenceID": 15, "context": "The application of our approaches to other deterministic/random graph classes such as the ChungLu model[4] (a generalization of Erd\u0151s-R\u00e9nyi graphs), or small-world graphs[18] would also be interesting.", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "References [1] Animashree Anandkumar, Vincent YF Tan, Furong Huang, Alan S Willsky, et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[3] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Fan Chung and Linyuan Lu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Thomas M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Amir Dembo and Andrea Montanari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Abbas El Gamal and Young-Han Kim.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Ashish Goel, Michael Kapralov, and Sanjeev Khanna.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Garvesh Raskutti, Martin J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Narayana P Santhanam and Martin J Wainwright.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Duncan J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Yuhong Yang and Andrew Barron.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In addition, we upper bound the mutual information through an approach in [20] which relates it to coverings in terms of the 10", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "ferromagnetic ising model, we know that q \u2208 [ 1 2 , 1].", "startOffset": 44, "endOffset": 54}, {"referenceID": 0, "context": "ferromagnetic ising model, we know that q \u2208 [ 1 2 , 1].", "startOffset": 44, "endOffset": 54}, {"referenceID": 0, "context": "Also, differentiating h(q), we get: h\u2032(q) = 1\u2212 2q \u2212 ( e \u2212 1 ) q (1 \u2212 q + e2\u03bbq)2 (21) It is easy to check that h\u2032(q) \u2264 0 for q \u2208 [ 1 2 , 1].", "startOffset": 128, "endOffset": 138}, {"referenceID": 0, "context": "Also, differentiating h(q), we get: h\u2032(q) = 1\u2212 2q \u2212 ( e \u2212 1 ) q (1 \u2212 q + e2\u03bbq)2 (21) It is easy to check that h\u2032(q) \u2264 0 for q \u2208 [ 1 2 , 1].", "startOffset": 128, "endOffset": 138}, {"referenceID": 13, "context": "The construction here is the same as in [16].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "So, combining the estimates from [16] and Lemma 4, we have, D (fG0\u2016fGi) \u2264 min ( 2\u03bbde e\u03bbd , \u03bb tanh(\u03bb) )", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "The construction here is the same as in [16] Constructing G0: We choose a largest possible number of vertices m such that we can have a clique on them i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "So, combining the estimates from [16] and Lemma 4, we have, D (fG0\u2016fGi) \u2264 min ( 2\u03bbe( \u221a 2k+1) e\u03bb( \u221a 2k\u22121) , \u03bb tanh(\u03bb) )", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "We recall some definitions and results from [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "(Lemma 8, 9 and Proof of Theorem 4 in [1] ) The \u01eb- typical set satisfies: 1.", "startOffset": 38, "endOffset": 41}, {"referenceID": 17, "context": "Now, use a result by Yang and Barron [20] to bound this term.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "See [9] ).", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "[8](Size of a Typical set) For any 0 \u2264 p \u2264 1, m \u2208 Z and a small \u01eb > 0, let Nm,p \u01eb = {x \u2208 {0, 1}m : \u2223", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "From counting arguments in [1], we have the following lower bound for G(p, c/p).", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "[1] Let G \u223c G(p, c/p).", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erd\u0151s-R\u00e9nyi graphs in a certain dense setting.", "creator": "LaTeX with hyperref package"}}}