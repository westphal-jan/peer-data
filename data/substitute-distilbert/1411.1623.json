{"id": "1411.1623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "A Hybrid Recurrent Neural Network For Music Transcription", "abstract": "we investigate the consequences of incorporating higher - level symbolic score - like information into automatic music transcription ( amt ) systems yet improve their inference. we use recurrent neural networks ( rnns ) and their variants as music language models ( mlms ) and present a generative architecture for combining these models with predictions from finite frame level acoustic device. we also compare specific neural input architectures for acoustic modeling. the proposed model computes a distribution over sample output sequences given the acoustic synthesis signal and hence present an algorithm candidate performing a global search for good candidate feedback. the performance of the proposed model is evaluated on piano music from the maps dataset and we observe that the comparison chart consistently outperforms existing transcription methods.", "histories": [["v1", "Thu, 6 Nov 2014 14:18:39 GMT  (54kb)", "http://arxiv.org/abs/1411.1623v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siddharth sigtia", "emmanouil benetos", "nicolas boulanger-lewandowski", "tillman weyde", "artur s d'avila garcez", "simon dixon"], "accepted": false, "id": "1411.1623"}, "pdf": {"name": "1411.1623.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n16 23\nv1 [\ncs .L\nG ]\n6 N\nov 2\nIndex Terms\u2014 Recurrent Neural Networks, Polyphonic Music Transcription, Music Language Models"}, {"heading": "1. INTRODUCTION", "text": "Automatic Music Transcription (AMT) involves identifying the pitches present in a given polyphonic acoustic signal and generating a corresponding symbolic, score-like transcription [10]. Most AMT systems focus primarily on modeling the acoustic signal to identify the pitches present as a function of time. Music exhibits structural regularity much like language, and therefore symbolic music prediction systems or Music Language Models (MLMs) can provide accurate symbolic priors and have the potential to significantly improve AMT systems. However, MLMs have not been extensively applied to AMT because polyphonic symbolic music prediction is quite a difficult problem and simple models such as n-grams which are used in speech are insufficient for modeling sequences of polyphonic music [3].\nRecurrent neural networks (RNNs) are powerful temporal models that can, in theory, capture long-term dependencies between inputs because of their powerful hidden representation. RNNs and their more complex variants [3], have recently been applied successfully to the problem of symbolic music prediction. This has led to a revival of interest in the problem of incorporating prior symbolic knowledge to improve AMT systems. Although RNNs achieve reasonable accuracy at symbolic music prediction tasks, it is not obvious how these priors can be incorporated into music transcription systems. The obvious strategy of multiplying the predictions of the\nSS is supported by a City University London Pump-Priming Grant; EB is supported by a City University London Research Followship;NB is currently working at Google Inc, Mountain View, California, USA\nacoustic and language models and then renormalizing, like in a product of experts, suffers from the label bias problem for low entropy sequences [11].\nRecently, there have been a few studies that try to incorporate symbolic priors into AMT systems. The model proposed in [4], is an input-output variant of the RNN-RBM model for music transcription. Although the model performs well on several datasets, it suffers from the problem of teacher forcing, where the acoustic and symbolic information are incorrectly weighted. The system in [15], uses a family of Dynamic Bayesian Network (DBN) language models to complement the acoustic model, though the search space of possible transcriptions must be constrained in order for the method to be tractable. In [19], the authors propose a novel dynamical system for incorporating symbolic information into a non-negative factorisation based transcription model. The method proposed in [18], incorporates symbolic information into a PLCA based transcription system using Dirichlet priors. Although the model performs well, it can only be used when the acoustic model is based on spectrogram factorisation techniques. Another shortcoming of the model in [18] is that the acoustic and language models are trained independently by optimising different objectives.\nThe popular technique of superposing a Hidden Markov Model (HMM) to the outputs of a frame-level classifier, like in state-of-theart speech recognition systems [9] is intractable for AMT tasks. This is because the outputs of the acoustic classifier at any time are highdimensional binary vectors. Consequently, the number of hidden HMM states is exponential in the number of output variables. This makes the parameter estimation problem for the HMM intractable. HMMs can be applied to polyphonic AMT systems under the assumption that each pitch is independent of all the other pitches [14]. However this assumption is violated by polyphonic music and therefore the method is unsatisfactory.\nIn this paper we employ the architecture in [5], which was originally proposed for modelling sequences of phonemes in speech recognition. The architecture provides a principled way for superposing an RNN to the predictions of an arbitrary frame level classifier and combines the two models under a common training objective. It is advantageous to use RNNs for high-dimensional problems like AMT, since the outputs of the RNN form a distributed representation, which makes the parameter estimation problem more tractable compared to an HMM. Additionally, the predictions of an RNN are conditioned on the entire sequence history which is a generalisation over the HMM transitions which are conditioned only on the previous time-step. We also compare performance between using Deep Neural Network (DNN) and RNN acoustic models. We present an efficient high-dimensional beam-search algorithm for decoding and compare the performance of this hybrid architecture\nto existing AMT systems. The rest of the paper is organised as follows. Section 2 introduces RNNs. Section 3 describes the hybrid architecture and Section 4 discusses the inference algorithm that is used for testing. Section 5 describes the experimental setup and details of training. Section 6 discusses the results and the paper is concluded in Section 7."}, {"heading": "2. RECURRENT NEURAL NETWORKS", "text": "An RNN is a powerful discrete-time dynamical system that can in principle, capture complex long term dependencies between its inputs. An RNN, when used as a generative model, defines a distribution over a sequence z in the following manner:\nP (z) =\nT\u220f\nt=1\nP (zt|At) (1)\nwhere At \u2261 {z\u03c4 |\u03c4 < t} is the sequence history at time t. The hidden state of an RNN with a single layer of hidden units is defined by the following recurrence relation:\nht = \u03c3(Wzhzt\u22121 +Whhht\u22121 + bh) (2)\nwhere Wzh are the weights from the inputs at t \u2212 1 to the hidden units at t, Whh are the recurrent weights between hidden units at t\u2212 1 and t and bh are the hidden biases.\nThe output vector at time zt is obtained in the following way:\nzt = f(Whzht + bz) (3)\nwhere f is some function applied to each element. The choice of f depends on the outputs that are being modeled. If the output variables form a one-of-K representation, then f is a softmax function that yields a multinomial distribution at the outputs. When f is a sigmoid function, then the outputs represent the independent probabilities of occurrence of each output variable.\nThe fact that the output variables are independent of each other is a very restrictive assumption when used for modeling polyphonic music. This is because musical notes appear in highly correlated patterns where the presence or absence of a note influences the likelihood of occurrence of all other notes. Therefore, instead of using the RNN to predict the probabilities of pitches directly, we can use the RNN to predict the parameters of a high-dimensional distribution estimator like the Restricted Boltzmann Machine (RBM) or the Neural Autoregressive Density Estimator (NADE) [3]. The RNN-NADE is a natural choice for a language model since it is tractable to obtain probabilities from the conditional NADEs at each step, which is necessary during inference. Another advantage of using the RNNNADE is that the gradients of the objective function can be calculated exactly and therefore we can make use of more powerful optimisers like Hessian Free (HF) [12]."}, {"heading": "3. HYBRID ARCHITECTURE", "text": "In this section we describe the architecture used to combine an RNNbased MLM with an arbitrary frame level classifier. The architecture is a generative graphical model that generalises the HMM architecture by conditioning predictions at some time t, on all previous predictions \u03c4 < t, as opposed to the HMM, where \u03c4 = t\u2212 1. Figure 1 is a graphical representation of the architecture.\nThe hybrid architecture factorises the joint probability of the sequence of acoustic vectors x and their corresponding labels z in the following way:\nP (z, x) = P (z1 . . . zT , x1 . . . xT ) (4)\n= P (z1)P (x1|z1) T\u220f\nt=2\nP (zt|At)P (xt|zt). (5)\nIn the above factorisation, the symbolic prediction termsP (zt|At) can be obtained from an RNN, while the P (xt|zt) terms are emission probabilities of observing the acoustic vector xt given a state zt. The above factorisation makes the following independence assumption for an emitted acoustic vector xt:\nP (xt|z, {x\u03c4 , \u03c4 < t}) = P (xt|zt). (6)\nUsing Bayes\u2019 rule, the joint probability can be reformulated in terms of the scaled likelihood:\nP (z, x) \u221d P (z1; \u0398l) P (z1|x1)\nP (z1)\nT\u220f\nt=2\nP (zt|At) P (zt|xt)\nP (zt) (7)\nwhere \u0398l are the parameters of the language model. The term P (zt|xt) can be obtained from the output of an arbitrary frame-level classifier, P (zt) is the marginal distribution of target vectors which can be easily calculated from the training set and constant terms involving xt have been removed by introducing the proportionality symbol.\nWe train the model by maximising the log-likelihood of occurrence of pairs of training examples x, z. The model can be easily trained with gradient descent because the gradient of the loglikelihood splits up into terms associated with the acoustic and language models in the following way:\n\u2202 logP (x, z)\n\u2202\u0398a =\n\u2202\n\u2202\u0398a\nT\u2211\nt=1\nlogP (zt|xt) (8)\n\u2202 logP (x, z)\n\u2202\u0398l =\n\u2202\n\u2202\u0398l\nT\u2211\nt=1\nlogP (zt|At) (9)\nwhere \u0398a,\u0398l are the parameters of the acoustic and language models respectively."}, {"heading": "4. INFERENCE", "text": "In the hybrid architecture, the prediction zt at time t is conditioned upon the entire sequence historyAt due to the RNN language model. This enforces successive frames to be coherent and thus performs temporal smoothing. In addition to temporal smoothing, an accurate language model can impose musicological rules and restrictions on the output transcriptions. While decoding, proceeding in a greedy chronological manner yields sub-optimal results because the sequence history At has not been optimally determined. At the same time, exhaustively searching for the globally optimal sequence is intractable since each non-leaf node in the search graph has 2N descendants. Instead, we perform a global search for the most likely sequence using beam search, a breadth-first tree search algorithm that keeps track of only the w most promising paths at any depth t [8, 4, 5]. In the search graph, a node at depth t corresponds to a subsequence of length t and the log-likelihood of each sub-sequence is the heuristic that guides search.\nIn addition to the beam width w, the high-dimensional variant of the beam-search algorithm outlined in [4] requires an additional parameter, the branching factor K. When using complex distribution estimators like the NADE, deterministically enumerating all possible configurations in order of decreasing probability is intractable. In such situations, the algorithm proceeds by making a pool of the top K candidate solutions by sampling. Random sampling from the conditional distribution of the language model is slow and inefficient and limits the size of the beam width during search.\nAlgorithm 1 High Dimensional Beam Search [4] Find the most likely sequence z given x with a beam width w. q \u2190 min-priority queue q.insert(0, {} ,mlm, mam) for t = 1 to T do\nq\u2032 \u2190 min-priority queue of capacity w \u2217 while q\u2032.len() < w do\nfor l, s,mlm,mam in q do z\u2032 = mam.next most probable() l\u2032 = logPlm(z \u2032|s)Pam(z \u2032|x)\u2212 logP (z\u2032)\nm\u2032lm \u2190 mlm with zt := z \u2032 m\u2032am \u2190 mam with x := xt+1 q.insert(l+ l\u2032, {s, z\u2032} ,m\u2032lm,m \u2032 am)\nq \u2190 q\u2032\nreturn q.pop() \u2217 A min-priority queue of capacity w maintains the w highest values at all times.\nInstead of pooling the top K configurations by drawing samples from the language model at each time step, we propose using the acoustic model to enumerate the most likely predictions. The motivation for doing so is twofold. Firstly, using the most likely solutions from the acoustic model to direct search avoids cases where the language model makes mistakes early on in a sequence and can never recover from them. Secondly, the outputs of the acoustic classifier are independent of each other. Enumerating the most likely solutions with a DP algorithm is more efficient than stochastic sampling [4]. Unlike [4], the high-dimensional beam search algorithm outlined in algorithm 1 does not require the branching factor K to be specified in advance and allows the use of much larger beam widths."}, {"heading": "5. EXPERIMENTS", "text": ""}, {"heading": "5.1. Acoustic Modelling", "text": "We experiment with using 3 different neural network architectures for learning relevant features from spectrogram inputs. Firstly, we use a deep, feed-forward neural network (DNN) as the acoustic classifier. DNNs currently form the state of the art for acoustic modelling in speech [9] and have been successfully applied to music transcription in the past [13, 3]. The ability of DNNs to learn a hierarchy of increasingly complex features makes them an ideal choice for acoustic modelling.\nDespite being powerful frame-level classifiers, DNN outputs are often noisy because they do not account for dependencies between input frames. In order to avoid this issue, we also experiment with using an RNN acoustic model. DNNs base their predictions upon a single frame of input, while the predictions of an RNN at time t are conditioned on all frames for time \u03c4 < t. Previous work on using RNNs as acoustic models for transcription demonstrates that RNNs are very good at predicting note-onsets [2]. We use the stacked RNN architecture, where several recurrent hidden layers are stacked in order to encourage each recurrent layer to operate at a different timescale [17]. One limitation of using the RNN as the acoustic model is that it violates the independence assumption made in Equation 6. The RNN predictions at t are conditioned on all past inputs for \u03c4 < t through the hidden layers. Since the language model and the acoustic model are trained separately, combining their predictions leads to certain factors being counted twice. Although in theory, this makes it hard to use RNN acoustic models, in our experiments we discovered that this difficulty does not affect performance.\nFinally, we experiment with using the features learnt by a DNN as inputs to an RNN. The motivation for doing this is that the features learnt by the DNN are believed to disentangle the factors of variation present in the inputs [7]. It is easier for the RNN to discover relationships between frames of disentangled features as compared to the original spectrogram inputs. We use the activations of the hidden units of all the layers of a DNN as input features to a stacked RNN."}, {"heading": "5.2. Language Modelling", "text": "As mentioned in Section 2, the RNN can be used as a generative model to define distributions over sequences. Unlike speech recognition, where the language model computes a multinomial distribution over a discrete set of phoneme labels, the MLM has to compute distributions over high-dimensional binary vectors. In order to capture the interactions between the output variables at each time-step, we prefer to use the RNN-NADE over the RNN as the MLM. At each step, the conditional NADE defines a joint distribution over the space of high-dimensional binary output vectors. At test time, the conditional NADE at time t provides the likelihood of observing the vectors predicted by the acoustic model, conditioned on all the predictions so far."}, {"heading": "5.3. Experimental Setup", "text": "We perform experiments on the MAPS dataset [6] to test the performance of the hybrid architecture and compare its performance to other models. The MAPS dataset consists of 270 pieces of piano music along with their ground truth MIDI transcriptions. 210 of these are rendered by software synthesisers, while 60 are played on real pianos. For our experiments, we randomly select 200 tracks\nfor training, 20 for validation and 50 for testing1. We use the entire length of the training and validation tracks and use the first 30 seconds of the tracks for testing. Pre-processing the data consisted of downsampling the tracks to 16 kHz and calculating the magnitude spectrogram. Spectrograms were computed with a window size of 64 ms and a hop size of 32 ms for the training and validation tracks. For the test tracks, spectrograms were computed every 10 ms [1]. The spectrograms were further preprocessed by subtracting the mean and dividing by the standard deviation of each frequency bin, calculated over the training set."}, {"heading": "5.4. Training", "text": "The acoustic and language models were trained by gradient descent, according to Equations 8 and 9. The output layers of both the DNN and RNN acoustic models consisted of sigmoid units. Each output of the acoustic model can be interpreted as the independent probability of a pitch being present in that frame. The acoustic classifiers were trained by minimising a cross entropy cost, since the target vectors for all frames are high-dimensional binary vectors. For both DNN and RNN models, weights were randomly initialised by sampling values from a Gaussian distribution with 0 mean and 0.01 standard deviation. We also used a momentum of 0.9 while updating the weights. The DNN models were trained on independent frames of spectrograms extracted from the training set. For training the stacked RNN models, the training tracks were further divided into sub-sequences of length 200 and the models were trained by Back-Propagation Through Time (BPTT) [16]. The RNN-NADE language models were trained on the ground truth MIDI data associated with the training data. The RNN-NADE models were optimised with Hessian Free (HF) optimisation."}, {"heading": "5.5. Evaluation Metrics", "text": "We evaluate the performance of our system using the evaluation metrics used in MIREX [1]. We present F-measures for both framebased and onset-only note-based tracking evaluation metrics. Additionally, we report precision, recall and accuracy measures for the 3 best performing models."}, {"heading": "6. RESULTS", "text": "In Table 1, we present F-measures for the different systems evaluated using different combinations of acoustic models and post-\n1Training/testing data info at: www.eecs.qmul.ac.uk/~sss31/\nprocessing. We report F-measures for both frame-based and note onset based evaluation metrics [1]. The best DNN acoustic model consists of 3 layers with 100 units each. The RNN acoustic models have two stacked hidden layers with 250 hidden units each. For language modelling, the conditional NADEs have 150 hidden units and the RNN has 100 hidden units. Four types of post-processing are considered in the experiments. No post processing, where the most likely outputs from the classifiers are chosen; learning independent thresholds for each classifier output based on the training set; HMM post processing assuming each pitch-class is independent; and finally the proposed hybrid architecture with a beam width w = 100. The post processing also includes minimum duration pruning (70 ms) to improve the model\u2019s accuracy at detecting note-onsets.\nFrom Table 1, we observe that the hybrid architecture consistently outperforms other methods. The best F-measure on both frame-based and note-onset based metrics is achieved by the hybrid architecture. The note-onset based F-measure is comparable to the frame-based F-measure which demonstrates the ability of the model to accurately identify note onsets. Beam search post-processing leads to a 3% increase in frame-based F-measure and a 6% increase in note-onset F-measure over greedy search (w = 1) for the DNN acoustic model. The RNN acoustic models are better at accurately predicting note-onsets because they implicitly perform temporal smoothing. In our experiments we discovered that the noisy DNN outputs when smoothed with a median filter, performed equally well as the RNN acoustic models on the note-based metrics. The relative improvement in performance when using the hybrid architecture is maximum for the DNN acoustic models, which is probably due to the fact that they do not violate the independence assumption in Equation 6. Table 2 shows additional metrics for the 3 hybrid models that perform best. It is clear that most of the errors are due to false alarms, which can be attributed to the error in accurately modelling note durations. However this error is not unique to this particular system and persists even in the ground truth transcriptions. The beam search takes 20 hours on a CPU to decode the first 30 seconds of all the test tracks."}, {"heading": "7. CONCLUSION", "text": "We present a hybrid RNN-based architecture for including symbolic priors in an automatic music transcription system. The architecture combines acoustic and high-level symbolic predictions in a principled manner and we propose an efficient algorithm for inference. The model generalises the popular technique of using independent HMMs to smooth the predictions of acoustic classifiers. Evaluation on the MAPS dataset suggests that the model outperforms related music transcription systems. In the future, we plan to work on improving the individual components of the architecture, namely the acoustic and language modeling. We would also like to investigate ways to improve beam search to make it feasible for real-time applications. Finally, we would like to expand our evaluations to datasets with multiple instruments."}, {"heading": "8. REFERENCES", "text": "[1] Mert Bay, Andreas F Ehmann, and J Stephen Downie. Evaluation of multiple-F0 estimation and tracking systems. In International Society for Music Information Retrieval Conference, pages 315\u2013320, 2009.\n[2] Sebastian Bock and Markus Schedl. Polyphonic piano note transcription with recurrent neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 121\u2013124. IEEE, 2012.\n[3] Nicolas Boulanger-lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 1159\u20131166, 2012.\n[4] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. High-dimensional sequence transduction. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3178\u20133182. IEEE, 2013.\n[5] Nicolas Boulanger-Lewandowski, Jasha Droppo, Mike Seltzer, and Dong Yu. Phone sequence modeling with recurrent neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5417\u20135421, May 2014.\n[6] Valentin Emiya, Roland Badeau, and Bertrand David. Multipitch estimation of piano sounds using a new probabilistic spectral smoothness principle. IEEE Transactions on Audio, Speech, and Language Processing, 18(6):1643\u20131654, 2010.\n[7] Ian Goodfellow, Honglak Lee, Quoc V Le, Andrew Saxe, and Andrew Y Ng. Measuring invariances in deep networks. In Advances in neural information processing systems, pages 646\u2013 654, 2009.\n[8] Alex Graves. Sequence transduction with recurrent neural networks. In Representation Learning Workshop, ICML, 2012.\n[9] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012.\n[10] Anssi Klapuri and Manuel Davy, editors. Signal Processing Methods for Music Transcription. Springer-Verlag, New York, 2006.\n[11] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages 282\u2013289, 2001.\n[12] J. Martens and I. Sutskever. Learning recurrent neural networks with Hessian-free optimization. In Proceedings of the 28th International Conference Machine on Learning (ICML), pages 1033\u20131040, 2011.\n[13] Juhan Nam, Jiquan Ngiam, Honglak Lee, and Malcolm Slaney. A classification-based polyphonic piano transcription approach using learned feature representations. In International Society for Music Information Retrieval Conference (ISMIR), pages 175\u2013180, 2011.\n[14] Graham E Poliner and Daniel PW Ellis. A discriminative model for polyphonic piano transcription. EURASIP Journal on Advances in Signal Processing, 2007, 2007.\n[15] S.A Raczynski, E. Vincent, and S. Sagayama. Dynamic bayesian networks for symbolic polyphonic pitch modeling. IEEE Transactions on Audio, Speech, and Language Processing, 21(9):1830\u20131840, Sept 2013.\n[16] D.E. Rumelhart, G.E. Hintont, and R.J. Williams. Learning representations by back-propagating errors. Nature, 323:533\u2013 536, 1986.\n[17] Ju\u0308rgen Schmidhuber. Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234\u2013242, 1992.\n[18] Siddharth Sigtia, Emmanouil Benetos, Srikanth Cherla, Tillman Weyde, Artur S. dAvila Garcez, and Simon Dixon. An RNN-based music language model for improving automatic music transcription. In International Society for Music Information Retrieval Conference (ISMIR), 2014.\n[19] Umut Simsekli, Jonathan Le Roux, and John R. Hershey. Hierarchical and coupled non-negative dynamical systems with application to audio modeling. In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), Oct 2013."}], "references": [{"title": "Evaluation of multiple-F0 estimation and tracking systems", "author": ["Mert Bay", "Andreas F Ehmann", "J Stephen Downie"], "venue": "In International Society for Music Information Retrieval Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Polyphonic piano note transcription with recurrent neural networks", "author": ["Sebastian Bock", "Markus Schedl"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Nicolas Boulanger-lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "High-dimensional sequence transduction", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Phone sequence modeling with recurrent neural networks", "author": ["Nicolas Boulanger-Lewandowski", "Jasha Droppo", "Mike Seltzer", "Dong Yu"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Multipitch estimation of piano sounds using a new probabilistic spectral smoothness principle", "author": ["Valentin Emiya", "Roland Badeau", "Bertrand David"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Measuring invariances in deep networks", "author": ["Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "In Representation Learning Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Signal Processing Methods for Music Transcription", "author": ["Anssi Klapuri", "Manuel Davy", "editors"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Learning recurrent neural networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Proceedings of the 28th International Conference Machine on Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A classification-based polyphonic piano transcription approach using learned feature representations", "author": ["Juhan Nam", "Jiquan Ngiam", "Honglak Lee", "Malcolm Slaney"], "venue": "In International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A discriminative model for polyphonic piano transcription", "author": ["Graham E Poliner", "Daniel PW Ellis"], "venue": "EURASIP Journal on Advances in Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Dynamic bayesian networks for symbolic polyphonic pitch modeling", "author": ["S.A Raczynski", "E. Vincent", "S. Sagayama"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "An RNN-based music language model for improving automatic music transcription", "author": ["Siddharth Sigtia", "Emmanouil Benetos", "Srikanth Cherla", "Tillman Weyde", "Artur S. dAvila Garcez", "Simon Dixon"], "venue": "In International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Hierarchical and coupled non-negative dynamical systems with application to audio modeling", "author": ["Umut Simsekli", "Jonathan Le Roux", "John R. Hershey"], "venue": "In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "Automatic Music Transcription (AMT) involves identifying the pitches present in a given polyphonic acoustic signal and generating a corresponding symbolic, score-like transcription [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 2, "context": "However, MLMs have not been extensively applied to AMT because polyphonic symbolic music prediction is quite a difficult problem and simple models such as n-grams which are used in speech are insufficient for modeling sequences of polyphonic music [3].", "startOffset": 248, "endOffset": 251}, {"referenceID": 2, "context": "RNNs and their more complex variants [3], have recently been applied successfully to the problem of symbolic music prediction.", "startOffset": 37, "endOffset": 40}, {"referenceID": 10, "context": "SS is supported by a City University London Pump-Priming Grant; EB is supported by a City University London Research Followship;NB is currently working at Google Inc, Mountain View, California, USA acoustic and language models and then renormalizing, like in a product of experts, suffers from the label bias problem for low entropy sequences [11].", "startOffset": 343, "endOffset": 347}, {"referenceID": 3, "context": "The model proposed in [4], is an input-output variant of the RNN-RBM model for music transcription.", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "The system in [15], uses a family of Dynamic Bayesian Network (DBN) language models to complement the acoustic model, though the search space of possible transcriptions must be constrained in order for the method to be tractable.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "In [19], the authors propose a novel dynamical system for incorporating symbolic information into a non-negative factorisation based transcription model.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The method proposed in [18], incorporates symbolic information into a PLCA based transcription system using Dirichlet priors.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Another shortcoming of the model in [18] is that the acoustic and language models are trained independently by optimising different objectives.", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "The popular technique of superposing a Hidden Markov Model (HMM) to the outputs of a frame-level classifier, like in state-of-theart speech recognition systems [9] is intractable for AMT tasks.", "startOffset": 160, "endOffset": 163}, {"referenceID": 13, "context": "HMMs can be applied to polyphonic AMT systems under the assumption that each pitch is independent of all the other pitches [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 4, "context": "In this paper we employ the architecture in [5], which was originally proposed for modelling sequences of phonemes in speech recognition.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "Therefore, instead of using the RNN to predict the probabilities of pitches directly, we can use the RNN to predict the parameters of a high-dimensional distribution estimator like the Restricted Boltzmann Machine (RBM) or the Neural Autoregressive Density Estimator (NADE) [3].", "startOffset": 274, "endOffset": 277}, {"referenceID": 11, "context": "Another advantage of using the RNNNADE is that the gradients of the objective function can be calculated exactly and therefore we can make use of more powerful optimisers like Hessian Free (HF) [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 7, "context": "Instead, we perform a global search for the most likely sequence using beam search, a breadth-first tree search algorithm that keeps track of only the w most promising paths at any depth t [8, 4, 5].", "startOffset": 189, "endOffset": 198}, {"referenceID": 3, "context": "Instead, we perform a global search for the most likely sequence using beam search, a breadth-first tree search algorithm that keeps track of only the w most promising paths at any depth t [8, 4, 5].", "startOffset": 189, "endOffset": 198}, {"referenceID": 4, "context": "Instead, we perform a global search for the most likely sequence using beam search, a breadth-first tree search algorithm that keeps track of only the w most promising paths at any depth t [8, 4, 5].", "startOffset": 189, "endOffset": 198}, {"referenceID": 3, "context": "In addition to the beam width w, the high-dimensional variant of the beam-search algorithm outlined in [4] requires an additional parameter, the branching factor K.", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Algorithm 1 High Dimensional Beam Search [4]", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "Enumerating the most likely solutions with a DP algorithm is more efficient than stochastic sampling [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "Unlike [4], the high-dimensional beam search algorithm outlined in algorithm 1 does not require the branching factor K to be specified in advance and allows the use of much larger beam widths.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "DNNs currently form the state of the art for acoustic modelling in speech [9] and have been successfully applied to music transcription in the past [13, 3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "DNNs currently form the state of the art for acoustic modelling in speech [9] and have been successfully applied to music transcription in the past [13, 3].", "startOffset": 148, "endOffset": 155}, {"referenceID": 2, "context": "DNNs currently form the state of the art for acoustic modelling in speech [9] and have been successfully applied to music transcription in the past [13, 3].", "startOffset": 148, "endOffset": 155}, {"referenceID": 1, "context": "Previous work on using RNNs as acoustic models for transcription demonstrates that RNNs are very good at predicting note-onsets [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 16, "context": "We use the stacked RNN architecture, where several recurrent hidden layers are stacked in order to encourage each recurrent layer to operate at a different timescale [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "The motivation for doing this is that the features learnt by the DNN are believed to disentangle the factors of variation present in the inputs [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "We perform experiments on the MAPS dataset [6] to test the performance of the hybrid architecture and compare its performance to other models.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "For the test tracks, spectrograms were computed every 10 ms [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 15, "context": "For training the stacked RNN models, the training tracks were further divided into sub-sequences of length 200 and the models were trained by Back-Propagation Through Time (BPTT) [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "We evaluate the performance of our system using the evaluation metrics used in MIREX [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "We report F-measures for both frame-based and note onset based evaluation metrics [1].", "startOffset": 82, "endOffset": 85}], "year": 2014, "abstractText": "We investigate the problem of incorporating higher-level symbolic score-like information into Automatic Music Transcription (AMT) systems to improve their performance. We use recurrent neural networks (RNNs) and their variants as music language models (MLMs) and present a generative architecture for combining these models with predictions from a frame level acoustic classifier. We also compare different neural network architectures for acoustic modeling. The proposed model computes a distribution over possible output sequences given the acoustic input signal and we present an algorithm for performing a global search for good candidate transcriptions. The performance of the proposed model is evaluated on piano music from the MAPS dataset and we observe that the proposed model consistently outperforms existing transcription methods.", "creator": "LaTeX with hyperref package"}}}