{"id": "1411.2328", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2014", "title": "Modeling Word Relatedness in Latent Dirichlet Allocation", "abstract": "standard lda model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. to prevent this problem, approaching current paper, we propose a model called word related latent dirichlet allocation ( wr - lda ) by incorporating word correlation into updated conceptual models. this leads to new capabilities that super lda model does not have such methodology estimating infrequently occurring occurrences or many - language topic modeling. experimental results demonstrate the effectiveness of our scenarios compared with standard lda.", "histories": [["v1", "Mon, 10 Nov 2014 05:24:41 GMT  (1135kb,D)", "http://arxiv.org/abs/1411.2328v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["xun wang"], "accepted": false, "id": "1411.2328"}, "pdf": {"name": "1411.2328.pdf", "metadata": {"source": "CRF", "title": "Modeling Word Relatedness in Latent Dirichlet Allocation", "authors": ["Xun Wang"], "emails": ["xunwang45@gmail.com", "Permissions@acm.org."], "sections": [{"heading": "1. INTRODUCTION", "text": "Latent Dirichlet Allocation(LDA) (Blei et al., 2003) has been widely used in many different fields for its ability in capturing latent semantics in textual and image data. In standard LDA, the generating probability is estimated from a document-word matrix using word frequency information. One main disadvantage of LDA is that topic assignment of words is conditionally independent from each other and the relevance between vocabularies (i.e semantic similarity) is totally neglected. This inability makes LDA fall short in many aspects. For example, it is quite plausible that synonymous words such as \"use\" and \"utilize\", \"Chinese\" and \"China\" or \"politics\" and \"politicians\" should be generated from the same topic. However, standard LDA models treat them as independent units and if they do not co-appear in the same context, they can hardly be included in the same topic. An extreme case is cross-lingual topic modeling where words in different languages never co-occur with each other, though talking about same topics as they are.\nRecently, a number of attempts have been made to address the limitation due to conditional-independence. In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12]. Zhao and Xing [18] incorporated word \u2212 pair, sentence \u2212 pair and document \u2212 pair knowledge information into statistical models. One disadvantage is that these approaches usually require aligned text corpus based on machine translation techniques. Boyd-Graber and Blei [4] developed the unaligned topic approach called MUTO where topics are distributions over\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. CIKM \u201914, November 03 - 07 2014, Shanghai, China Copyright 2014 ACM 978-1-4503-2598-1/14/11 ...$15.00 http://dx.doi.org/10.1145/2661829.2662090 .\nword pairs instead of just being distributions over terms. However, these methods focus on cross-lingual techniques where word correlation is only partly considered.\nTo the best of our knowledge, the closest existing works are the approach developed by Andrzejewski et al. [2] and the approach developed by Petterson et al. [14] where word relations are considered. In Andrzejewski et al.\u2019s work [2], domain knowledge is used to model vocabulary relations such as Must-Link or Cannot-Link. By applying a novel Dirichlet Tree prior, Must-Link words are more likely to be generated from the same topic and Cannot-Link are less likely. However, Andrzejewski et al.\u2019s approach requires specific domain knowledge and can not be extended to general word correlation. In Petterson et al. \u2019s work[14], they use a sophisticated biased prior in Dirichlet distribution to consider the word correlations rather than the uniform one used in Standard LDA. One disadvantage is that their work considers word correlations only in the prior, but not in the algorithm.\nIn this paper, we propose an approach called Word Related Latent Dirichlet Allocation (WR-LDA) that incorporates word correlation into topic model. Given a collection of documents, standard LDA topic model wishes finding parameters by maximizing the marginal log likelihood of data. In our model, we sacrifice part of the maximization of log likelihood by incorporating vocabulary correlations based on the assumption that similar words tend to be generated from similar topics. We experiment our model on the different datasets and results indicate the effectiveness of model.\nSection 2 presents some related work and Section 3 presents our model. The remaining is the experiments and the conclusion."}, {"heading": "2. RELATED WORK", "text": "Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17]. for their ability in discovering topics latent in the document collection. Standard topic models suffer the disadvantage that researchers only use word co-appearance frequency for topic modeling without considering word correlations. However similar or synonymous two terms are, if they do not co-appear in the document, they can hardly be classified into the same topic. This disadvantage is largely magnified in multi-lingual topic modeling where different languages never co-occur with each other.\nPrevious work on multilingual topic models mostly require parallelism at either the sentence level or document level [2] and the approach developed by Petterson et al. [14]. Boyd-Graber and Blei (2009) proposed a multilingual topic approach which requires a list of word pairs. Topic is defined as distribution over word pairs. However, word correlation is only partly considered in multilingual\nar X\niv :1\n41 1.\n23 28\nv1 [\ncs .C\nL ]\n1 0\nN ov\n2 01\n4\ntopic techniques. Andrzejewski et al. [2] proposed the approach by considering domain knowledge to model vocabulary relations such as MustLink or Cannot-Link by using a Dirichlet Tree prior in topic models. However, their model can not be extended to general word correlation. Petterson et al. [14]uses a biased Dirichlet prior by using a Logistic word smoother which takes accounts word relations. The disadvantage of their model is that their work take account of word correlations only in the prior, but not in the algorithm."}, {"heading": "3. LDA TOPIC MODEL", "text": "The Latent Dirichlet allocation (LDA) model (Blei et al., 2003) defines the document likelihood using a hierarchical Bayesian scheme. Specifically, each document is presented as a mixture of topics which is drawn from a Dirichlet distribution and each topic is presented as a mixture of words. Document words are sampled from a topic-specific word distribution specified by a drawn of the wordtopic-assignment from the topic proportion vector. Let K be the number of topics; V be the number of the terms in a vocabulary. \u03b2 is a K \u00d7 V matrix and \u03b2k is the distribution vector over V terms where \u03b2ij = P (w = j|zw = i) denotes the probability word w is generated from topic k. The generation for LDA is shown as follows: 1. For each document m \u2208 [1,M ]: Draw topic proportion \u03b8m|\u03b1 \u223c Dir(\u03b1) 2. For each word w 2.1 draw topic zw \u223cMultinomial(\u03b8m) 2.2 draw w \u223c p(w|zw, \u03b2)\nLDA can be inferred from a collapsed Gibbes sampling (Heinrich 2005) or variational inference (Blei et al., 2003). Variational inference tries to find parameter\u03b1 and \u03b2 that maximize the (marginal) log likelihood of the data:\nL(\u03b1, \u03b2) = M\u2211 d=1 log(wd|\u03b1, \u03b2) (1)\nSince the posterior distribution of latent variables can not be computed efficiently, in variational inference of LDA, researchers use variational distribution q(\u03b8, z|\u03b3, \u03c6) to approximate posterior distribution in each document.\nq(\u03b8, z|\u03b3, \u03c6) = q(\u03b8m|\u03b3m) \u220f n q(zn|\u03c6n) (2)\n\u03c6n = {\u03c6n1, \u03c6n2, ..., \u03c6nK}. \u03c6nk can be interpreted as the probability that word at nth position in current document is generated from topic k. The next step is to set up an optimizing problem to determine the value of \u03b3 and \u03c6 where the desideratum of finding a tight lower bound on the log likelihood is transformed to the following optimization problem:\n(\u03b3, \u03c6) = argmin \u03b3,\u03c6\nKL(q(\u03b8, z|\u03b3, \u03c6)||p(\u03b8, z|w,\u03b1, \u03b2)) (3)\nwhere KL(p||q) denotes the KullbackLeibler (KL) divergence be-\ntween two distributions.\nL(\u03b3, \u03c6;\u03b1, \u03b2) =\nEq[log p(w|z, \u03b2)] + Eq[log p(z|\u03b8)] + Eq[log p(\u03b8|\u03b1)] \u2212 Eq[log q(\u03b8)]\u2212 Eq[log q(z)]\u2212 Eq[log q(\u03b2)] (4)\n{\u03b1, \u03b2} can be iteratively inferred from variational EM algorithm (Blei et al., 2003)."}, {"heading": "4. WR-LDA", "text": ""}, {"heading": "4.1 Description", "text": "In this section, we present our WR-LDA model and show how it can incorporate word correlation into topic models. Let G = (V,E) denote the graph where V = {wi}i=Vi=1 denotes word collection and E = {ew,w\u2032 : w \u2208 V,w\u2032 \u2208 V }. e(w,w\u2032) denotes the edge between nodew andw\u2032. \u03baw,w\u2032 denotes the edge weight which is the similarity between word w and w\u2032. Based on the assumption that similar words should have similar probability generated by different topics, we introduce the Loss function R(\u03b2), inspired by the idea of graph harmonic function (Zhu et al., 2003; Mei et al., 2008).\nR(\u03b2) = 1\n2 \u2211 w \u2211 w\u2032 \u2211 k \u03baw,w\u2032(\u03b2kw \u2212 \u03b2kw\u2032)2 (5)\nIntuitively,R(\u03b2) measures the difference between p(w|\u03b2) and p(w\u2032|\u03b2) for each pair (w,w\u2032). The more similar two words are, the larger penalty there would be for distribution difference. Clearly, we prefer the topic distribution with smaller R(\u03b2).\nIn WR-LDA, we try to maximize function O(\u03b1, \u03b2) which is the combination of L(\u03b1, \u03b2) and \u2212R(\u03b2) instead of just optimizing log likelihood L(\u03b1, \u03b2)1 in standard LDA.\nO(\u03b1, \u03b2) = \u03bbL(\u03b1, \u03b2)\u2212 (1\u2212 \u03bb)R(\u03b2) (6)\nwhere \u03bb is the parameter that balances the likelihood and loss function. When \u03bb = 1, our model degenerates into standard LDA model. Similar idea can also been found in Zhang et al. (2010) for bilingual topic extraction and Mei et al.(2008) for topic network construction."}, {"heading": "4.2 Inference", "text": "In this subsection, we describe the variational inference for WRLDA. We emphasize on the part that is different from LDA and briefly describe the part that is similar. We use the variational distribution q(\u03b8, z|\u03b3, \u03c6) to approximate posterior distribution p(\u03b8, z|w,\u03b1, \u03b2) and use variational EM algorithm for inference.\nE-step: The E-step of WR-LDA tries to find the optimizing values of the variational parameters {\u03b3, \u03c6}. Since the loss function R(\u03b2) does not involve \u03b3 and \u03c6, E-step of WR-LDA is the same as that of standard LDA. We update {\u03b3, \u03c6} according to the following equations.\n\u03c6nk = \u03b2kwnexp{Eq[log(\u03b8k)|\u03b3]} (7)\n\u03b3k = \u03b1k + \u2211 n \u03c6nk (8)\nwhere Eq[log(\u03b8k)|\u03b3] = \u03a8(\u03b3k) \u2212 \u03a8( \u2211 k \u03b3k) and \u03a8(\u00b7) is the first\nderivative of log \u0393 function. The E-step of WR-LDA is shown in Figure 2 (Blei et al., 2003).\n1The details of L(\u03b1, \u03b2) is shown in Appendix A.\nM-step: We try to maximizes the resulting lower bound ofO(\u03b1, \u03b2) shown in Equ.6 with respect to the model parameters \u03b1 and \u03b2.\nUpdate \u03b2: By considering the Lagrange multipliers corresponding to the constraints that \u2211 w \u03b2kw = 1 and \u2211 k \u03b1 m k = 1, we define T (\u03b1, \u03b2) as follows:\nT (\u03b1, \u03b2) = O(\u03b1, \u03b2) + \u2211 k tk( \u2211 w \u03b2kw \u2212 1) + \u2211 d td( \u2211 k \u03b1mk \u2212 1)\nSo given {\u03b3, \u03c6, \u03bb} we have\n\u2202O(\u03b1, \u03b2)\n\u2202\u03b1k = 0\n\u2202T (\u03b1, \u03b2)\n\u2202\u03b2kw = 0 (9)\nIn LDA, \u03b2 is updated in the M-step with \u03b2kw \u221d \u2211 m \u2211 n\u2208m \u03c6dnk1(wdn =\nw). However in WR-LDA, because of the incorporation of loss function R(\u03b2), the derivative of T (\u03b1, \u03b2) with respect to \u03b2kw depends on \u03b2kw\u2032 , where w\u2032 6= w, as shown in Equ.10.\n\u2202T (\u03b1, \u03b2)\n\u2202\u03b2kw = \u03bb\n1\n\u03b2kw \u2211 m \u2211 n\u2208m \u03c6dnk1(wdn = w)\n\u2212 2(1\u2212 \u03bb) \u2211 w\u2032 \u03baw,w\u2032(\u03b2kw \u2212 \u03b2kw\u2032) + tk (10)\nTo achieve the optimal value of \u03b2, we use Newton-Raphson method for optimization. \u03b2k is a 1 \u00d7 V vector. The update for \u03b2k in Newton-Raphson is as follows:\n\u03b2t+1k = \u03b2 t k \u2212H(\u03b2k)\u22121 5 T (\u03b2k) (11)\nwhere 5L(\u03b2k) is the gradient of function T (\u03b1, \u03b2) with regard to \u03b2kwi , i \u2208 [1, V ]. H(\u03b2k) is a Hessian matrix.\nH(\u03b2k)(i, j) = \u22022T (\u03b1, \u03b2)\n\u2202\u03b2kwi\u2202\u03b2kwj = \u22122(1\u2212 \u03bb)\u03bawi,wj\n\u2212 \u03bb\u03b4(i, j) \u03b22kwi \u2211 m \u2211 n\u2208m \u03c6nk1(wn = wi)\n(12)\nIf the size of vocabulary V is 104,H(\u03b2k) would be a 104\u00d7104 matrix, the inverse of which would be too expensive to be calculated directly. To solve this problem, we adopt the strategy proposed in Mei et al.(2008)\u2019s work, where we only need to find the value of \u03b2n that makes T (\u03b1n, \u03b2n|\u03bbn, \u03c6n, \u03b3n) > T (\u03b1n\u22121, \u03b2n\u22121|\u03bbn, \u03c6n, \u03b3n) instead of getting the local maximum of T (\u03b1n, \u03b2n|\u03bbn, \u03c6n, \u03b3n) at each M-step. In each iteration of M-step, we firstly set the value of \u03b2n,0 to the value which maximizes L(\u03b1, \u03b2), just as in Standard LDA.\n\u03b2n0kw \u221d \u2211 m \u2211 n\u2208m \u03c6dnk1(wdn = w) (13)\nThen we iteratively obtain \u03b2n,1, \u03b2n,2, ..., \u03b2n,t according to Equ.14 until the value of T (\u03b1, \u03b2) drops.\n\u03b2n,tkw = \u03c1\u03b2 n,t\u22121 kw + (1\u2212 \u03c1)\n\u2211\u2032 w \u03baww\u2032\u03b2\nn,t\u22121 kw\u2032\u2211\u2032\nw \u03baww\u2032 (14)\nClearly, \u2211 w \u03b2kw = 1 and \u03b2kw \u2265 0 always hold in Equ.14. Equ.14 can be interpreted as follows: when \u03c1 is set to 0, it means that the updating value of \u03b2kw is totally decided by its neighbors. In graph harmonic algorithm (zhu2003semi), Equ.14 is just the optimization of Lossfunction R(\u03b2) when \u03c1 is set to 0. So we try to optimize O(\u03b1, \u03b2) by firstly decreasing the value of Lossfunction R(\u03b2) until O(\u03b1, \u03b2) decreases.\nupdate \u03b1: The first derivative of T (\u03b1, \u03b2) in regard with \u03b1k also depends on \u03b1k\u2032 , k 6= k\u2032, we also use Newton-Raphson method for optimization. Since the Hessian matrix H(\u03b1) is the form of:\nH(\u03b1) = diag(h) + 1z1T where hk = \u2212M\u03a8\u2032(\u03b1k) and z = M\u03a8\u2032( \u2211 k \u03b1k)the inverse of H(\u03b1) can be easily calculated (Blei et al., 2003):\nH\u22121(\u03b1) = diag(h)\u22121 \u2212 diag(h) \u2212111Tdiag(h)\u22121 z\u22121 + \u2211 k h \u22121 k\n\u03b1 can be updated as follows:\n\u03b1t+1 = \u03b1t \u2212H(\u03b1)\u22121 5 T (\u03b1) (15)\nMultiplying by the gradient, we easily obtain the ith component of matrix H(\u03b1)\u22121 5 T (\u03b1) (Blei et al., 2003).\n[H(\u03b1)\u22121 5 T (\u03b1)]i = [5T (\u03b1)]i \u2212 c\nhi (16)\nwhere c = \u2211\nk[5T (\u03b1)]k/hk z\u22121+ \u2211 k h \u22121 k"}, {"heading": "5. EXPERIMENTS", "text": "In this section, we compare WR-LDA with standard LDA and other baselines in multiple applications."}, {"heading": "5.1 Text Modeling", "text": "We firstly compare text modeling of the WR-LDA with standard LDA on the 20 Newsgroups data set2 with a standard list of 598 stop words removed.\nWe fit the dataset to a 110 topics. In WR-LDA, we need to tune \u03bb. To evaluate the performances of WR-LDA with different \u03bb, we have the following function.\nM(\u03b3\u0302) = 1\nC1 20\u2211 i=1 \u2211 d\u2208Gi \u2211 d \u2032\u2208Gi \u03b62(KL(\u03b3\u0302 d||\u03b3\u0302d \u2032 ))\n+ 1\nC2 20\u2211 i=1 \u2211 j 6=i \u2211 d\u2208Gi \u2211 d \u2032\u2208Gj \u03b61(KL(\u03b3\u0302 d||\u03b3\u0302d \u2032 )) (17)\n2http://qwone.com/ jason/20Newsgroups/\nGi denotes the collection of document with the label i; \u03b3\u0302dk = \u03b3dk/Md. KL(p||q) represents the KL distance between two distributions. \u03b61(x) = ex/(1 + ex) and \u03b62(x) = 1 \u2212 \u03b61(x). C1 and C2 are normalization factor. \u03bb varies from 0 to 106 with an interval of 105. The results are shown in Fig 4. In the following experiments, we set \u03bb as 3.6 \u2217 105.\nFig 5 shows the 2D embedding of the expected topic proportions of WR-LDA(\u03bb = 3.6 \u2217 105) and LDA using the t-SNE stochastic neighborhood embedding (van der Maaten and Hinton, 2008). Fig 6 shows some of the results."}, {"heading": "5.2 Regression", "text": "We evaluate supervised version of WR-LDA on the hotel reviews dataset from TripAdvisor3 . As in Zhu and Xing (2010), we take logs of the response values to make them approximately normal. Each review is associated with an overall rating score and five aspect rating scores for the aspects: Value, Rooms, Location, Cleanliness, and Service. In this paper, we focus on predicting the overall rating scores for reviews.\nWe compare the results from the following approaches: LDA+SVR, sLDA, WR-LDA+SVR and sWR-LDA. For LDA+SVR and WCLDA+SVR, we use their low dimensional representation of documents as input features to a linear SVR and denote this method. The evaluation criterion is predictive R2 (pR2) as defined in the work of Blei and McAuliffe (2007).\nFigure 7 shows the predictive R2 scores of different models. First, since supervised topic models can leverage the side information (e.g., rating scores) to discover latent topic representations, they generally outperform the decoupled two-step procedure as adopted in unsupervised topic models. Secondly, we can see that sWR-LDA outperforms sLDA and WR-LDA+SVR outperforms LDA+SVR.\nFrom the top words of different topics shown at Fig8, we can easily explain why sWR-LDA is better than sLDA. For sWR-LDA, part of topics show a regular positiveness/negativeness pattern. Topic 3 is on the negative aspects of a hotel while Topic 7 and Topic 9 describe positive aspects. This is because word correlation is considered in WR-LDA. For example, positive vocabularies such as great, good and wonderful, they have similar semantic meanings and thus high edge weight. So they are very likely to be generated from the 3http://www.tripadvisor.com/\nsame topic. The topics discovered by LDA do not show a regular pattern on positiveness or negativeness, resulting in low predictive R2. Moreover, we can see that topics are more coherent in WRLDA. Topic 2 talks about sea sceneries, Topic 5 talks about food, Topic 6 talks about conditions of bathroom or swimming pool and Topic 10 talks about room objects. However, topics in LDA do not show such clear pattern."}, {"heading": "6. CROSS-LINGUAL TOPIC EXTRACTION", "text": ""}, {"heading": "6.1 Dataset Construction", "text": "To qualitatively compare our approach with the baseline method, we test different models on Cross-Lingual Topic Extraction task in different respects. The data set we used in this experiment is selected from a Chinese news website Sina(\u65b0\u6d6a)4 and Google News5. Selected news talks about 5 topics categorized by news websites such as \"Sports(\u4f53\u80b2)\", \"Entertainments(\u5a31\u4e50)\", \"World(\u56fd \u9645)\", \"Science(\u79d1\u5b66)\" and \"Business(\u8d22\u7ecf)\". Each Chinese news passage is associated with an English news passage which talks about the same event (i.e. Passage entitled \"\u571f\u8033\u5176\u6c11\u4f17\u5bf9\u653f\u5e9c\u6539 \u9020\u5e7f\u573a\u6297\u8bae\u8513\u5ef6\u81f3\u5168\u56fd\" vs Passage entitled \"Protests no Turkish Spring, says PM Erdogan\"). We select 1000 English passages and 1000 Chinese passages during the period from March. 8th, 2001 to Jun. 23th, 2013, including {Sports:360, WordNews:296, Bussiness:92, Science:68, and Entertainments:184}."}, {"heading": "6.2 Details", "text": "LetCc denote Chinese news collection, whereCc = {d1c , d2c , ..., dNc }, N = 1000. Ce is the English news collectionCe = {d1e, d2e, ..., dNe }, and dic is the correspondent passage of die, i \u2208 [1, N ]. We firstly removed infrequent words6 and frequent words 7. A bilingual ChineseEnglish dictionary would give us a many-to-many mapping between the vocabularies of the two languages. If one word can be potentially translated into another word, the two words would be connected with an edge. Specifically, let we denote an English word and wc denotes a Chinese word. Cwe denotes the set of Chinese words that are translated from we, Cwe = {wc|e(wc, we) 6= 0}\n\u03ba(we, wc) = { 1 if wc \u2208 Cwe 0 if wc 6\u2208 Cwe\n(18)"}, {"heading": "6.3 Evaluation", "text": "Part of the results are presented in Table 1. Since commonly used measures for LDA topic model such as perplexity can not capture whether topics are coherent or not, we use the measures in Petterson el al. (2011)\u2019s work.\nWe compare the topic distributions of each Chinese news passage with its correspondent English pair based on the following measures:\n\u2022 Mean l2 Distance (L2-D): 1 N \u2211N i=1(( \u2211N k=1 \u03b3\u0302 dic k \u2212 \u03b3\u0302 die k ) 2)1/2\n\u2022 Mean Hellinger Distance (H-D): 1 N \u2211N i=1 \u2211N k=1((\u03b3\u0302 dic k ) 1/2 \u2212 (\u03b3\u0302d i e k ) 1/2)2\n4http://www.sina.com.cn 5https://news.google.com 6Words that occurred less than 3 times in the corpus. 7Words that occurred more than M/10 times in the corpus, where M is the total number of documents.\n\u2022 Agreements on first topic: (A-1) 1 N \u2211N i=1 I(argmaxk\u03b3\u0302 dic k = argmaxk\u03b3\u0302 die k )\n\u2022 Mean number of agreements in top 5 topics (A- 5): 1 N \u2211N i=1 argeement(d i c, d i e), where agreements argeement(dic, die)\nis the cardinality of the intersection of the 5 most likely topics of dic, d i e.\nClearly, we prefer smaller values of L2-D and H-D and larger values of A-1 and A- 5. We compare the performances from following approaches:\n\u2022 WR-LDA1: Only Chinese-English correlations are considered and the weights of edges between English-English words, ChineseChinese words are 0.\n\u2022 WR-LDA2: All word correlations are considered.\n\u2022 DC model: Approach proposed by Petterson et al.(2011), where word correlations are incorporated into a Beta prior using a logistic smooth function.\n\u2022 LDA: the standard LDA topic model.\nFrom Figure 9, LDA achieves the worst results as expected because it can hardly detect the topics shared by bilingual documents due to the reason that vocabularies from different languages hardly co-appear in the same passages. WR-LDA1 is better than the DC model, which considers word correlations only in the prior construction, rather than in the algorithm. WR-LDA2, which consider word correlations fully, achieves better results than WR-LDA1, which partly considers word correlations.\nFor further illustration, we randomly choose two pairs of words, \u201dMessi\u201dand \u201d\u6885\u897f\u201d(Messi in Chinese), which is the name of a famous Argentine soccer player, and \u201dChina\u201dand \u201d\u4e2d\u56fd(China in Chinese)\u201d. Since each pair of words have the same meaning, we prefer that they have the similar probabilities generated from the same topic. Figure 8 shows the probability that \u201dMessi\u201dand \u201d\u6885 \u897f\u201dgenerated from different topics and Figure 9 shows \u201dChina\u201dand \u201d\u4e2d\u56fd\u201d. The x-axis is the index to the topic and y-axis is the loglikelihood of probability. We can see that in LDA, words within\nFigure 9: Comparison of topic distributions\neach pair are always negative correlated. This can be easily explained by the fact that Chinese words and English words never co-appear in a document. So if a Chinese word has large probability generated by one topic, it means this top is a Chinese dominated topic and most English words would be excluded. Word pairs in DC model are correlated but not as strong as that in WR-LDA. Since DC model models word correlation only in the prior, such influence can be diluted while Gibbs sampling goes on. In WR-LDA, word correlations are considered in the algorithm and during each iteration, algorithm will fix the gap between correlated words. That is why WR-LDA outperforms DC model."}, {"heading": "7. CONCLUSION", "text": "In this paper, we present WR-LDA, a revised version of LDA topic model that incorporates word correlations. Experiments on text modeling, review rating prediction and cross-lingual topic modeling demonstrate the effectiveness of our model. There are two disadvantages of WR-LDA when compared with LDA. (1) The value of parameter lambda involved in the model is hard to tune while LDA there is no additional parameter involved. (2) Due to the introduction of Penalty function in WR-LDA, we have to keep record of parameter \u03c6 in varational inference for all documents, which largely increase the cost of both memory and time"}, {"heading": "8. REFERENCES", "text": "[1] C. C. Aggarwal and C. Zhai. Mining text data. Springer,\n2012. [2] D. Andrzejewski, X. Zhu, and M. Craven. Incorporating\ndomain knowledge into topic modeling via dirichlet forest priors. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 25\u201332. ACM, 2009.\n[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022, 2003.\n[4] J. Boyd-Graber and D. M. Blei. Multilingual topic models for unaligned text. In Proceedings of the Twenty-Fifth\nConference on Uncertainty in Artificial Intelligence, pages 75\u201382. AUAI Press, 2009.\n[5] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357. ACM, 1999.\n[6] J. Li and C. Cardie. Timeline generation: tracking individuals on twitter. In Proceedings of the 23rd international conference on World wide web, pages 643\u2013652. International World Wide Web Conferences Steering Committee, 2014.\n[7] J. Li, M. Ott, and C. Cardie. Identifying manipulated offerings on review portals. In EMNLP, pages 1933\u20131942, 2013.\n[8] J. Li, A. Ritter, C. Cardie, and E. Hovy. Major life event extraction from twitter based on congratulations/condolences speech acts. 2014.\n[9] J. Li, A. Ritter, and E. Hovy. Weakly supervised user profile extraction from twitter. ACL, 2014.\n[10] J. D. Mcauliffe and D. M. Blei. Supervised topic models. In Advances in neural information processing systems, pages 121\u2013128, 2008.\n[11] D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and A. McCallum. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 880\u2013889. Association for Computational Linguistics, 2009.\n[12] X. Ni, J.-T. Sun, J. Hu, and Z. Chen. Mining multilingual topics from wikipedia. In Proceedings of the 18th international conference on World wide web, pages 1155\u20131156. ACM, 2009.\n[13] J. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised learning of human action categories using spatial-temporal words. International journal of computer vision, 79(3):299\u2013318, 2008.\n[14] J. Petterson, W. Buntine, S. M. Narayanamurthy, T. S. Caetano, and A. J. Smola. Word features for latent dirichlet\nallocation. In Advances in Neural Information Processing Systems, pages 1921\u20131929, 2010.\n[15] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 248\u2013256. Association for Computational Linguistics, 2009.\n[16] Y.-C. Tam and T. Schultz. Bilingual lsa-based translation lexicon adaptation for spoken language translation. In INTERSPEECH, pages 2461\u20132464, 2007.\n[17] I. Titov and R. McDonald. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th international conference on World Wide Web, pages 111\u2013120. ACM, 2008.\n[18] B. Zhao and E. P. Xing. Bitam: Bilingual topic admixture models for word alignment. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 969\u2013976. Association for Computational Linguistics, 2006."}], "references": [{"title": "Mining text data", "author": ["C.C. Aggarwal", "C. Zhai"], "venue": "Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating domain knowledge into topic modeling via dirichlet forest priors", "author": ["D. Andrzejewski", "X. Zhu", "M. Craven"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 25\u201332. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Multilingual topic models for unaligned text", "author": ["J. Boyd-Graber", "D.M. Blei"], "venue": "Proceedings of the Twenty-Fifth  Conference on Uncertainty in Artificial Intelligence, pages 75\u201382. AUAI Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Timeline generation: tracking individuals on twitter", "author": ["J. Li", "C. Cardie"], "venue": "Proceedings of the 23rd international conference on World wide web, pages 643\u2013652. International World Wide Web Conferences Steering Committee", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Identifying manipulated offerings on review portals", "author": ["J. Li", "M. Ott", "C. Cardie"], "venue": "EMNLP, pages 1933\u20131942", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Major life event extraction from twitter based on congratulations/condolences speech acts", "author": ["J. Li", "A. Ritter", "C. Cardie", "E. Hovy"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Weakly supervised user profile extraction from twitter", "author": ["J. Li", "A. Ritter", "E. Hovy"], "venue": "ACL", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised topic models", "author": ["J.D. Mcauliffe", "D.M. Blei"], "venue": "Advances in neural information processing systems, pages 121\u2013128", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Polylingual topic models", "author": ["D. Mimno", "H.M. Wallach", "J. Naradowsky", "D.A. Smith", "A. McCallum"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 880\u2013889. Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Mining multilingual topics from wikipedia", "author": ["X. Ni", "J.-T. Sun", "J. Hu", "Z. Chen"], "venue": "Proceedings of the 18th international conference on World wide web, pages 1155\u20131156. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised learning of human action categories using spatial-temporal words", "author": ["J.C. Niebles", "H. Wang", "L. Fei-Fei"], "venue": "International journal of computer vision, 79(3):299\u2013318", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Word features for latent dirichlet  allocation", "author": ["J. Petterson", "W. Buntine", "S.M. Narayanamurthy", "T.S. Caetano", "A.J. Smola"], "venue": "Advances in Neural Information Processing Systems, pages 1921\u20131929", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 248\u2013256. Association for Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Bilingual lsa-based translation lexicon adaptation for spoken language translation", "author": ["Y.-C. Tam", "T. Schultz"], "venue": "INTERSPEECH, pages 2461\u20132464", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["I. Titov", "R. McDonald"], "venue": "Proceedings of the 17th international conference on World Wide Web, pages 111\u2013120. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Bitam: Bilingual topic admixture models for word alignment", "author": ["B. Zhao", "E.P. Xing"], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions, pages 969\u2013976. Association for Computational Linguistics", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 17, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 15, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 10, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 11, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 17, "context": "Zhao and Xing [18] incorporated word \u2212 pair, sentence \u2212 pair and document \u2212 pair knowledge information into statistical models.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Boyd-Graber and Blei [4] developed the unaligned topic approach called MUTO where topics are distributions over", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "[2] and the approach developed by Petterson et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] where word relations are considered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "\u2019s work [2], domain knowledge is used to model vocabulary relations such as Must-Link or Cannot-Link.", "startOffset": 8, "endOffset": 11}, {"referenceID": 13, "context": "\u2019s work[14], they use a sophisticated biased prior in Dirichlet distribution to consider the word correlations rather than the uniform one used in Standard LDA.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 197, "endOffset": 206}, {"referenceID": 7, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 197, "endOffset": 206}, {"referenceID": 8, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 197, "endOffset": 206}, {"referenceID": 0, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 224, "endOffset": 231}, {"referenceID": 12, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 224, "endOffset": 231}, {"referenceID": 9, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 253, "endOffset": 261}, {"referenceID": 14, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 253, "endOffset": 261}, {"referenceID": 6, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 278, "endOffset": 285}, {"referenceID": 16, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 278, "endOffset": 285}, {"referenceID": 1, "context": "Previous work on multilingual topic models mostly require parallelism at either the sentence level or document level [2] and the approach developed by Petterson et al.", "startOffset": 117, "endOffset": 120}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proposed the approach by considering domain knowledge to model vocabulary relations such as MustLink or Cannot-Link by using a Dirichlet Tree prior in topic models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14]uses a biased Dirichlet prior by using a Logistic word smoother which takes accounts word relations.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA.", "creator": "LaTeX with hyperref package"}}}