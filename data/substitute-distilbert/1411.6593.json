{"id": "1411.6593", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2014", "title": "Rational Deployment of Multiple Heuristics in IDA*", "abstract": "recent advances in metareasoning for search has shown its usefulness in improving numerous search algorithms. this paper applies rational metareasoning to ida * when several admissible heuristics are included. the obvious basic approach of taking the maximum of the heuristics is improved upon abandoning lazy evaluation of the method, resulting : a variant known as lazy ida *. we introduce a rational version of basic ida * that decides whether to skip the more convenient heuristics or to bypass estimates, based on a myopic disease regret estimate. empirical evaluation in several domains supports the various results, and shows that implements lazy ida * utilizing a state - but - them - art heuristic combination method.", "histories": [["v1", "Mon, 24 Nov 2014 20:04:20 GMT  (37kb)", "http://arxiv.org/abs/1411.6593v1", "7 pages, 6 tables, 20 references"]], "COMMENTS": "7 pages, 6 tables, 20 references", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["david tolpin", "oded betzalel", "ariel felner", "solomon eyal shimony"], "accepted": false, "id": "1411.6593"}, "pdf": {"name": "1411.6593.pdf", "metadata": {"source": "CRF", "title": "Rational Deployment of Multiple Heuristics in IDA*", "authors": ["David Tolpin", "Ariel Felner", "Solomon Eyal Shimony"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n65 93\nv1 [\ncs .A\nI] 2\n4 N\nov 2\n01 4"}, {"heading": "1 Introduction", "text": "Introducing meta reasoning techniques into search is a research direction that has recently proved useful for many search algorithms. All search algorithms have decision points on how to continue search. Traditionally, tailored rules are hard-coded into the algorithms. However, applying meta reasoning techniques based on value of information or other ideas can significantly speed up the search. This was shown for depth-first search in CSPs [17], for Monte-Carlo tree search [6], and recently for A* [16]. In this paper we apply meta reasoning techniques to speed up IDA* when several admissible heuristics are available.\nIDA* [8] is a linear-space simulation of A*. Thus it makes sense to examine how such speed-up was done for A* in a similar context, as was done in Lazy A* (or LA\u2217, for short) [16] by reducing the time spent on computing heuristics. A* is a best-first heuristic search algorithm guided by the cost function f(n) = g(n) + h(n). A* uses OPEN and CLOSED lists and always expands the minimal cost node from OPEN, generates its children and moves it to CLOSED. When more than one admissible heuristic is available, one can clearly evaluate all these heuristics, and use their maximum as an admissible heuristic. The problem with naive maximization is that all the heuristics are computed for all the generated nodes, resulting in increased overhead, which can be overcome as follows.\nWith two (or more) admissible heuristics, when a node n is generated, Lazy A* only computes one heuristic, h1(n), and adds n to OPEN. Only when n re-emerges as the top of OPEN is another heuristic, h2(n), evaluated; and if h2(n) > h1(n) then n is re-inserted into OPEN. If the goal is reached before node n\u2019s re-emergence, the computation of h2(n) is never performed, thereby saving time, especially if h2 is computationally heavy. In rational lazy A* (RLA\u2217) [16], the ideas of lazy heuristic evaluation and trading additional node expansions for decreased time for computing heuristics were combined. RLA\u2217 is based on rational meta-reasoning, and uses a myopic regret\n1 CS Department, Ben-Gurion University. E-mail:shimony@cs.bgu.ac.il 2 CS Department, Ben-Gurion University. E-mail:odedbetz@cs.bgu.ac.il 3 ISE Department, Ben-Gurion University. E-mail:felner@bgu.ac.il 4 CS Department, Ben-Gurion University. E-mail:shimony@cs.bgu.ac.il\ncriterion to decide whether to compute h2(n) or to bypass the computation of h2 and expand n immediately when n re-emerges from OPEN. RLA\u2217 aims at reduced search time, even at the expense of more node expansions than LA\u2217.\nThe memory consumption of A* is linear in the number of generated nodes, which is typically exponential in the problem description size, which may be unacceptable. In contrast to A*, IDA* is a linear-space algorithm which emulates A* by performing a series of depth-first searches from the root, each with increasing costs, thus reexpanding nodes multiple times. IDA* is typically used in domains and problem instances where A* requires more than the available memory and thus cannot be run to completion. If the heuristic h(n) is admissible (never overestimates the real cost to the goal) then the set of nodes expanded by A* is both necessary and sufficient to find the optimal path to the goal [2]. Similar guarantees holds for IDA* under some additional reasonable assumptions. Thus, techniques used to develop RLA\u2217, can in principle be applied to IDA*, the focus of this paper. However, IDA* has a different logical structure and needs a completely different treatment. In particular, in A* one needs to assign an f -value to each generated node n while in IDA*, one only needs to know whether the f -value is below or above the current threshold.\nThe first thing to consider for IDA* is lazy evaluation of the heuristics. In order to reduce the time spent on heuristic computations, Lazy IDA* evaluates the heuristics one at a time, lazily. When h1 causes a cutoff there is no need to evaluate h2. Unlike Lazy A*, where lazy evaluation must pay an overhead (re-inserting into the OPEN list) [16], Lazy IDA* (LIDA*) is straightforward and has no immediate overhead.\nThe main contribution of this paper is Rational lazy IDA* (RLIDA*) which uses meta reasoning at runtime5. We analyze IDA* and provide a criterion, based on a myopic expected regret, which decides whether to evaluate a heuristic or to bypass that evaluation and expand the node right away. We then provide experimental results on sliding tile puzzles and on the container relocation problem [20], showing that RLIDA* outperforms both IDA* and LIDA*."}, {"heading": "2 Lazy IDA*", "text": "We begin by describing IDA*, and the minor change needed to make it use the heuristics lazily, thus implementing lazy IDA*."}, {"heading": "2.1 Definitions", "text": "Throughout this paper we assume for clarity that we have two available admissible heuristics, h1 and h2. Unless stated otherwise, we\n5 This paper is an extended version of a short (2-page) paper to appear in the ECAI 2014 proceedings. In addition to containing all the analysis that could not fit into the short version, there are some additional experimental results and a comparison to additional related work.\nAlgorithm 1: Lazy IDA*\n1 Lazy-IDA* (root) { 2 Let Thresh = max(h1(root), h2(root)) 3 Let solution = null 4 while solution == null and Thresh < \u221e do 5 solution, Thresh = Lazy-DFS(root, Thresh)\n6 return solution\n7 Lazy-DFS(n, Thresh) { 8 if g(n) > Thresh then 9 return null, g(n)\n10 if goal-test(n) then 11 return n, Thresh\n12 if g(n)+h1(n) > Thresh then 13 return null, g(n)+h1(n)\n14 if opt-cond and g(n)+h2(n) > Thresh then 15 return null, g(n)+h2(n)\n16 Let next-Thresh = \u221e 17 for n\u2019 in successors(n) do 18 Let solution, temp-Thresh = Lazy-DFS-lim(n\u2019, Thresh) 19 if solution \u00ac = null then 20 return solution, temp-Thresh\n21 else 22 Let next-Thresh = min(temp-Thresh, next-Thresh)\n23 return null, next-Thresh\nassume that h1 is faster to compute than h2 but that h2 is weakly more informed, i.e., h1(n) \u2264 h2(n) for the majority of the nodes n, although counter cases where h1(n) > h2(n) are possible. We say that h2 dominates h1, if such counter cases do not exist and h2(n) \u2265 h1(n) for all nodes n. We use f1(n) to denote g(n) + h1(n), and f2(n) to denote g(n) + h2(n). We denote the cost of the optimal solution by C\u2217. Additionally, we denote the computation time of h1 and of h2 by t1 and t2, respectively. Unless stated otherwise we assume that t2 is much greater than t1. We thus mainly aim to reduce the number of times h2 is computed."}, {"heading": "2.2 Why use lazy IDA*?", "text": "Let T be the IDA* threshold. After h(n) is evaluated, if f(n) = g(n) + h(n) > T , then n is pruned and IDA* backtracks to n\u2019s parent. Given both h1 and h2, a naive implementation of IDA* will evaluate them both and use their maximum in comparing against T . Lazy IDA* (LIDA*) is based on the simple fact that when you have an or condition in the form of cond1 or cond2 then if cond1 = True then cond2 becomes irrelevant (don\u2019t-care) and need not be computed, as the entire or condition is surely true. In the context of IDA*, if f1(n) > T then the search can backtrack without the need to compute h2. This simple observation is probably recognized by most implementers of IDA*. Thus, it is likely that LIDA* is a way to implement IDA* when more than one heuristic is present.\nThe pseudo-code for LIDA* is depicted as Algorithm 1. In lines 13-14 we check whether f1 is already above the threshold in which case, the search backtracks. h2 is only calculated (in lines 15-16) if f1(n) \u2264 T . The \u201coptional condition\u201d in line 14 is needed for the rational lazy A* algorithm, described below, which entails adding appropriate conditions that aim at h2 only if its usefulness outweights its computational overhead on average. In the standard version of\nlazy IDA*, the \u201coptional condition\u201d in line 14 is always true, and the respective heuristics are always evaluated at this juncture. We also note that lines 9-10 are needed to ensure that the goal test at lines 11-12 will only return the optimal solution. This check is particulary needed for Rational Lazy IDA* as described below."}, {"heading": "2.3 Issues in Lazy IDA*", "text": "Several additional obvious improvements to LIDA* are possible. Here we examine some such potential enhancements, as well as possible pitfalls."}, {"heading": "2.3.1 Heuristic bypassing", "text": "Heuristic bypassing (HBP) is a technique that in many cases allows bypassing the computation of a given heuristic without causing any other change in the course of the algorithm. In A* one needs to compute an f -value, while Applied to IDA*, one only needs to know whether the f -value is below or above the threshold. First, it is important to note that Lazy IDA* as described above, is a special case of HBP. When f1(n) > T there is no need to consult h2(n) and we bypass the computation of h2. Another variant of HBP for LIDA* is applicable for a node n under the following two preconditions: (1) the operator between n and its parent p is bidirectional, and (2) both heuristics are consistent [4]. Suppose that node n was generated and that p is the parent of n; that the cost of the edge is c and that f1(p) + c < f2(p). Since p was expanded, we know that f2(p) \u2264 T . Since the heuristics are consistent, we know that f1(n) \u2264 f1(p) + c \u2264 T . Thus, in such cases, one can skip the computation of h1(n) and go directly to h2. Nevertheless, the savings here are negligible as we assumed that t1 << t2 and our aim is thus to decrease the number of times h2 is computed. We also note that HBP needs additional effort for book keeping.\nWhen the heuristic is inconsistent then a mechanism called bidirectional pathmax (BPMX) can be used to propagate heuristic values from parents to children and vice versa [4]. Using exhaustive evaluations of all heuristics, even if h1(n) already exceeded the threshold, can potentially help in propagating larger heuristic values to the neighborhood of n. Nevertheless, experiments showed that even in this context, lazy evaluation of heuristics is faster in time than exhaustive evaluation [4]."}, {"heading": "2.3.2 Extra iterations of Lazy IDA*", "text": "In rare cases, LIDA* can cause extra DFS iterations. Suppose that the current threshold is T and the current value of the next threshold (NT) is T +3 as some node m was seen in the current iteration with f(m) = T + 3. Now we generate node n with f1(n) = T + 1 and thus set NT = T + 1 and bypass h2. However, if f2(n) = T + 2 then consulting h2 would have caused NT = 2. With LIDA*, we may now start a new and redundant DFS iteration with T + 1.\nWhile Lazy A*, was always as informative as A* using the maximum of the heuristics, this is not the case for Lazy IDA*. Nevertheless, since there is potentially an exponential number of nodes in the frontier of a DFS iteration, such scenarios are quite rare and Lazy IDA* outperforms regular IDA* despite this worst-case scenario."}, {"heading": "3 Rational Lazy IDA*", "text": "A general theory for applying rational meta-reasoning for search algorithms was presented in [13]. Using principles of rational metareasoning theoretically every algorithm action (heuristic function\nevaluation, node expansion, open list operation) should be treated as an action in a sequential decision-making meta-level problem: actions should be chosen so as to achieve the minimal expected search time. However, the appropriate general meta-reasoning problem is extremely hard to define precisely and to solve optimally. In order to apply it practically, specific assumptions and simplifications should be added.\nIn this paper we focus on just one decision type, made in the context of IDA* - that of deciding whether to evaluate or to bypass the computation of h2. In order to choose rationally, we define a criterion based on the regret for bypassing h2(n) in this context. We define regret here as the value lost (in terms of increased run time) due bypassing the computation of h2(n), i.e. how much runtime is increased due to bypassing the computation. We wish to compute h2(n) only if this regret is positive on the average. Some of the ideas behind Rational Lazy IDA* are borrowed from those of [16] and Rational Lazy A* (RLA*). However, the assumptions of RLA* are different, and cannot be used for IDA* as they were made under the assumption that there exists an OPEN list and that an f -value of a node should be stored within the node. In contrast, in IDA* there is no OPEN list and we only need to know whether f(n) is below or above the threshold T . Therefore IDA* needs a different treatment.\nIn IDA*, each iteration is a depth-first search up to a gradually increasing threshold T , until a solution is found. For each node n, we say that evaluating h(n) is helpful if g(n) + h(n) > T . That is, the heuristic helped in the sense that node n is pruned, rather than expanded, in this iteration.\nThe only addition of Rational Lazy IDA* to Lazy IDA* is the option to bypass h2(n) computations (line 14). In this case, n is expanded right away.6 Suppose that we choose to compute h2 \u2014 this results in one of the following outcomes:\n1. h2(n) is not helpful and n is immediately expanded. 2. h2(n) is helpful (because g(n) + h2(n) > T ), pruning n, which\nis not expanded in the current IDA* iteration.\nObserve that computing h2 can be beneficial only in outcome 2 plus the additional condition that the time saved due to pruning a search subtree outweighs the time to compute h2, i.e., t2(n). However, whether outcome 2 takes place after a given state is not known to the algorithm until after h2 is computed. The algorithm must decide whether to evaluate h2 according to what it believes to be the probability of each of the outcomes. The time wasted by being suboptimal in deciding whether to evaluate h2 is called the regret of the decision. We derive a rational policy for deciding when to evaluate h2, under the following assumptions:\n1. The decision is made myopically: we work under the belief that the algorithm continues to behave like Lazy IDA* starting with the children of n. 2. h2 is consistent: if evaluating h2 is beneficial on n, it is also beneficial on any successor of n. 3. As a first approximation, we also assume that h1 will not cause pruning in any of the children.\nIf Rational Lazy IDA* is indeed better than Lazy IDA*, the first assumption results in an upper bound on the regret. Note that these meta-reasoning assumptions are made in order to derive decisions, and as is common in research on meta-reasoning, the assumptions\n6 It is important to note that in such cases, f2(n) might be greater than T . For this reason we added lines 9-10 in the pseudo code above, to ensure that the solution returned is always optimal.\ndo not actually hold in practice [13]. Nevertheless, if the violation of the assumptions is not \u201ctoo severe\u201d, the resulting algorithms still show significant improvement. Without such assumptions the model becomes far too complicated and one cannot move ahead at all. For example, the myopic assumption trivially does not hold by design, as applying it strictly at runtime means that we only use the rational decision rule at the root, which does not make sense in practice. Violating this assumption results in an actual expected runtime that is lower than that computed under this assumption. The other two simplifying assumptions do not have this nice property as far as we know, however, and one would prefer to drop them. This non-trivial issue remains for future research.\nIf h2(n) is not helpful and we decide to compute it, the effort invested in evaluating h2(n) turns out to be wasted. On the other hand, if h2(n) is helpful but we decide to bypass it, we needlessly expand n. Due to the myopic and other assumptions, Rational Lazy IDA* would evaluate both h1 and h2 for all children of n. Due to consistency of h2, the children of n will not be expanded in this IDA* iteration.\nTable 1 summarizes the regret of each possible decision, for each possible future outcome; each column in the table represents a decision, while each row represents a future outcome. In the table, te is the time to evaluate h1 and expand n, and b(n) is the local branching factor at node n (taking into account parent pruning). Computing h2 needlessly \u201cwastes\u201d t2 time. Bypassing h2 computation when h2 would have been helpful \u201cwastes\u201d te + b(n)t1 + b(n)t2 time, but because computing h2 would have cost t2, the regret is te + b(n)t1 + (b(n)\u2212 1)t2.\nLet us denote the probability that h2(n) is helpful by ph. The expected regret of computing h2(n) is thus (1\u2212 ph)t2. On other hand, the expected regret of bypassing h2(n) is ph(te + b(n)t1 + (b(n)\u2212 1)t2). As we wish to minimize the expected regret, we should thus evaluate h2 just when:\n(1\u2212 ph)t2 < ph(te + b(n)t1 + (b(n)\u2212 1)t2) (1)\nor equivalently:\n(1\u2212 phb(n))t2 < ph(te + b(n)t1) (2)\nIf phb(n) \u2265 1 (the left side of the equations is negative), then the expected regret is minimized by always evaluating h2, regardless of the values of t1, t2 and te. A simple decision rule would be to evaluate h2 exactly in these cases.\nFor phb(n) < 1, the decision of whether to evaluate h2 depends on the values of t1, t2 and te:\nevaluate h2 if t2 < ph\n1\u2212 phb(n) (te + b(n)t1) (3)\nThe factor ph 1\u2212phb(n) depends on the potentially unknown probability ph, making it difficult to reach the optimum decision. However, if our goal is just to do better than Lazy IDA*, then it is safe to replace ph by an upper bound on ph. We discuss this next."}, {"heading": "3.1 Bounding the probability that h2 is helpful", "text": "Search time can be saved by evaluating h2 selectively, only in the nodes where the probability that the evaluation is helpful is \u201chigh enough\u201d. In particular, in the case of two heuristics, h1 and h2, the decision whether to evaluate h2(n) can be made based on h1(n) and prior history of evaluations of h1 and h2 on the same or \u201csimilar\u201d nodes. One can try to estimate ph, either online or offline in order to use the decision boundaries such as Equation 3 based on these empirical frequencies directly.\nNevertheless, we examine another possibility here, based on the rationale that our goal in RLIDA* is to do better than simple LIDA*, and wish to trade off computation times \u201csafely\u201d, i.e. with little risk of being worse than LIDA*. One way to estimate the probability ph that the evaluation is helpful \u201csafely\u201d is to bound this probability using concentration inequalities.\nConcentration inequalities bound probabilities of certain events for a bound random variable, that is, such a variable x for which Pr[x \u2208 [0, 1]] = 1, and we need to construct such a variable. Let x be:\nx = 1\u2212 h1(s) max(h1(s), h2(s))\n(4)\nIt is easy to see that x \u2208 [0, 1] and increases with h2(s). The condition h2(n) > T \u2212 g(n) (i.e., h2(n) is helpful) is equivalent to condition x > l where:\nl = 1\u2212 h1(n) T \u2212 g(n) (5)\nWe need to bound the probability that Pr(XN+1 > l) given the prior history of evaluations of x (that is, of h1 and h2). Denote by xN the average of N samples:\nxN = 1\nN\nN \u2211\ni=1\nXi (6)\nThe probability Pr(XN+1 > l) is less than the probability that the mean E[x] of the random variable x is at least \u00b5, xN \u2264 \u00b5 \u2264 l, plus the probability that XNn+1 > l given E[x] = \u00b5 (the union bound).\nph = Pr(XN+1 > l) \u2264 Pr(E[x] > \u00b5)+Pr(XN+1 > l E[x] = \u00b5) (7) Denote \u00b5 = (1 \u2212 \u03b1)xN + \u03b1l, \u03b1 \u2208 [0, 1] \u2014 we will obtain the bound as a function of \u03b1 and then select \u03b1 that minimizes the bound. According to the Hoeffding inequality:\nPr(E[x] > (1\u2212 \u03b1)xN + \u03b1l) < e\u22122N(\u03b1(l\u2212xN )) 2\n(8)\nand to the Markov inequality:\nPr(XN+1 > l E[x] = (1\u2212\u03b1)xN +\u03b1l) < (1\u2212 \u03b1)xN + \u03b1l l (9)\nAn upper bound for the probability Pr(XN+1 > l) is a function of \u03b1:\nph = Pr(XN+1 > l) \u2264 B(\u03b1) = e\u22122N(\u03b1(l\u2212xN )) 2 + (1\u2212 \u03b1)xN + \u03b1l\nl (10)\nThe bound B(\u03b1) can be minimized for \u03b1 \u2208 [0, 1] by solving dB(\u03b1) d\u03b1\n= 0, but a closed-form solution does not generally exist. However, a reasonable value for \u03b1 can be easily found. Choosing\n\u03b1 \u2217 =\n\u221a log \u221a\n2Nl 2N\nl \u2212 xN (11)\nand substituting into (10), obtain\nPr(XN+1 > l) \u2264 B\u2217 = B(\u03b1\u2217) = 1 +\n\u221a log \u221a 2Nl\n\u221a 2Nl\n+ xN\nl (12)\nIn the bound (12) the second term B2 = xN l is tantamount to the Markov inequality when the sample average xN coincides with the\nmean E[x]. The first term B1 = 1+\n\u221a log \u221a 2Nl\n\u221a 2Nl does not depend on x and for l > 0 approaches zero as N approaches infinity. Although the concentration inequalities are correct for iid samples, a state that does not necessarily hold for samples of heuristic values during the search, nevertheless it is a usable first-order approximation. We use B\u2217 as defined in Equation 12 as an estimate of ph."}, {"heading": "4 Empirical evaluation", "text": "The greatest advantage of IDA* over A* is storage complexity. However, IDA* has a number of limitations. First, the number of nodes expanded by IDA* is typically much greater than that of A* because IDA* is unable to detect transpositions and because in every iteration, IDA* repeats the former iterations. In addition, IDA* preforms very poorly if there is a large number of different f -costs below C\u2217 encountered during the search (leading to a large number of iterations), which occurs in domains such as TSP.\nTherefore we selected for empirical evaluation domains that are known to be IDA*-friendly (such as the 15-puzzle), or where recent work has shown IDA* to perform well, such as the container relocation problem [20]. Regretfully, most planning problems (from the planning competitions) used in [16], are inappropriate for IDA* due to multiple transpositions in the search space. Another requirement we had is the availability of known informative admissible heuristics for the domain (otherwise it does not pay to compute them), that are costly to compute (if they are very cheap, we might as well always compute them). In domains where the latter requirements do not hold, elaborate meta-reasoning on whether to compute a heuristic will thus obviously not achieve any significant improvement.\nThe above restrictions are obvious limitations to the applicability of the scheme proposed in this paper, and should be considered when trying to apply our methods. Nevertheless, as stated in Section 5.1, our scheme should be extensible to other IDA*-like algorithms where the large number of f-costs is not a problem."}, {"heading": "4.1 Sliding tile puzzles", "text": "We first provide evaluations on the 15-puzzle and its weighted variant, where the cost of moving each tile is equal to the number on the tile. Note that there is another version of the weighted 15-puzzle where the cost of a tile move is the reciprocal of the number on the tile [15]. However, the number of possible f-costs under f* in this version is typically very large, thus in this reciprocal variant, IDA* is expected to perform abysmally, making IDA* inapplicable to this domain. Indeed, some preliminary runs confirmed this expectation, and we therefore dropped this reciprocal weights version from our evaluation.\nFor consistency of comparison, we used as test cases for the 15 puzzle 98 out of Korf\u2019s 100 tests [8]: all the tests that were solved in less than 20 minutes with standard IDA* using the Manhattan Distance (MD) heursitic. (All experiments were performed using Java, on a 3.3GHz AMD Phenom II X6 1100T Processor, with 64 bit Ubuntu 12.04, and with sufficient memory to avoid paging.) As the\nmore informative heuristic h2 we used the linear-conflict heuristic (LC) [10] which adds a value of 2 to MD for pairs of tiles that are in the same row (or the same column) as their respective goals but in a reversed order. One of these tiles will need to move away from the row (or column) to let the other pass.\nSince the runtime of both heuristics is nearly constant across the states, (i.e., t1(n) \u2248 c1 and t2(n) \u2248 c2 for some constants c1, c2) it turns out that the decision of whether to compute h2 is stable across a wide range of ph values, and thus a constant value of ph performs well for this domain. Results are presented for an assumed constant ph = 0.3, estimated offline from trial runs of RLIDA* on a few problem instances. Average results for IDA* with only MD, IDA* with LC, Lazy IDA* using both heuristics, and Rational Lazy IDA*, are shown in Table 2. The advantage of Rational Lazy IDA* is evident: even though it expands many more nodes than Lazy IDA*, its runtime is significantly lower as it saves even more time on evaluations of LC. LIDA* evaluated LC 21,886,093 times, out of which only 6,561,972 were helpful. Much time was wasted on evaluating nonhelpful heuristics. In contrast, RLIDA* only chose to evaluate LC 8,106,832 times, out of which 4,413,050 were helpful. The bottom Clairvoyant row is an unrealizable scheme that uses an oracle, not achivable in practice, which has a runtime better than any achievable optimal decision on whether to evaluate h2. Its numbers were estimated by using the LIDA* results, assuming that h2 was computed only in the 6,561,972 helpful nodes, and bypassed otherwise. As can be seen, the runtime of our version of RLIDA* is closer to Clairvoyant than to LIDA*. It shows that much of the potential of RLIDA* was indeed exploited by our version.\nTable 3 shows similar results for 82 of the previous initial positions on weighted 15 puzzle that were solved in 20 minutes by IDA* (the weighted 15 puzzle is harder). In this domain, Rational Lazy A* also achieves a significant speedup and was much closer to Clairvoyant than to LIDA*.\nFor the heuristics we used in our tests and for ph = 0.3, it turns out that the decision on whether to evaluate h2 depends just on the branching factor: evaluate h2 only for b(n) = 3 (excluding the parent), i.e. for cases where the blank was in the middle. Applying the bounds from Section 3.1 to estimate ph did not achieve significant further improvement over RLIDA* with a constant ph (not shown in the tables), due to the fact that the simple decision rule was rather stable across a relatively wide range of ph. We thus expect this same rule to work for sliding tile puzzles of other dimensions, and tried the same scheme in rectangular tile puzzles: 3*5 (numbers from 1 to 14) and 3*6 (numbers from 1 to 17). Since the fraction of nodes with\n3 children in these puzzles is lower than the 4*4 puzzle, we expect RLIDA* to do better than in the 4*4 puzzle. As we did not have access to standard benchmark instances, we generated instances using random walks of 45 to 80 steps from the goal state.\nTables 4, 5 show that the improvement factor in both domains due to rational lazy IDA* is similar to that obtained in the (4*4) 15 puzzle. However the gap between RLIDA* and the unrealizable clairvoyant scheme is smaller than for the 4*4 puzzle, so RLIDA* seems to be making better decisions in these latter variants, as expected. Though indicative, one caveat is that the way instances were generated in the rectangular versions is different from the 4*4 puzzle, and the general shape of the search space may also differ."}, {"heading": "4.2 Container relocation problem", "text": "The container relocation problem is an abstraction of a planning problem encountered in retrieving stacked containers for loading onto a ship in sea-ports [20]. We are given S stacks of containers, where each stack consists of up to T containers. In each stack, containers are stacked on top of one another. In the initial state there are N \u2264 S \u00d7 T containers, arbitrarily numbered from 1 to N . The rules of stacking and of moving containers is the same as for blocks in the well-known blocks world domain, i.e., a container can be moved if there is no container on top of it. However, unlike blocks-world planning, the objective function is different, as follows.\nThe goal is to retrieve all containers in order of number, from 1 to N , where \u201cretrieve\u201d can be seen as placing a container on an additional, special and always empty, stack where the container disappears (in the application domain this \u201cspecial stack\u201d is actually a freight truck that takes the container away to be loaded onto a ship). The objective function to minimize is the number of container moves until all containers are gone (\u201cloaded onto the truck\u201d). The complication comes from the fact that we can only \u201cretrieve\u201d a container if it is at the top of one of the stacks. Thus, containers on top of it should be moved away. Optimally solving this problem is NP-hard [20].\nAlthough there are various variants of this problem, we assume here the version where each container (\u201cblock\u201d in blocks-world terminology) is uniquely numbered. Another assumption typically made is that a stack s that currently has T containers is \u201cfull\u201d and no additional containers can be placed on s until some container is moved away from s. We also address only the \u201crestricted\u201d version of the problem [20], where the only relocations allowed are of containers currently on top of the smallest numbered container. Finally, since\na solution always involves removing all N containers, and each container can be moved to the truck only once, it is customary to count only moves from stack to stack (called \u201crelocations\u201d), ignoring the final move of containers to the truck.\nThe heuristics we used for the experiments are as follows. Every container numbered X which is above at least one container Y with with a number smaller than X must be moved from its stack in order to allow Y to be retrieved. The number of such containers in a state can be computed quickly, and forms an admissible heuristic denoted LB1 in [20]. A more complicated heuristic adds one relocation for each container that must be relocated a second time as any place to which it is moved will block some other container. Following [20], we denote this heuristic by LB37. This heuristic requires much more computation time than LB1, and additionally its runtime depends heavily on the state.\nIn the experiments, we used as instances the 49 hardest tests out of those that were solved in less than 20 minutes with the LB1 heuristic, from the CVS test suite described in [1, 7], retrived from http://iwi.econ.uni-hamburg.de/IWIWeb/Default.aspx?tabId=1083&tabindex=4. The instances actually used had either 5 or 6 stacks, and from 6 to 10 tiers. Results are shown in table 6. In this domain Rational Lazy A* shows some performance improvement even when ph was assumed constant (Ph = 0.3). However, in this problem the branching factor is almost constant, and equal to the number of stacks minus 1, during much of the search. As a result, there is room for improvement by better estimating ph. Indeed using the bounds developed in Section 3.1 to estimate ph dynamically achieves significant additional speedup, as shown by the line RLIDA*, ph \u2264 0.5. Due to the fact that the runtimes of the heuristics have a large variance and are hard to predict precisely, using Eq. 3 did not achieve good results, so the results reported in the table are actually for the simplified decision rule that computes h2 only when phb(n) \u2265 1, as mentioned after Equation 2."}, {"heading": "5 DISCUSSION", "text": ""}, {"heading": "5.1 Related work", "text": "Other elaborate schemes for deciding on heuristics at runtime appear in the research literature. Domshlak st al. [3] also noted that although theoretically taking the maximum of admissible heuristics is best within the context of A*, the overhead may not be worth it. Instead, their idea is to select which heuristic to compute at runtime. Based on this idea, they formulated selective max (Sel-MAX) for A*, an online learning scheme which chooses one heuristic to compute at each state. In principle, Sel-MAX could be adapted to run in IDA*. However, the domains we used in experiments had a heuristic h1 which has negligible computation time, and should thus always be computed. Sel-Max is aimed at cases where there is a need for selection, i.e., if the time for computing each heuristic is not negligible.\n7 To guarantee admissibility we made some minor notation changes from how this heuristic is formally stated in the original paper\nAutomatically selecting combinations of heuristics for A* and IDA* from a large set of available heuristics was examined in [5]. Selecting a combination of heuristics is in some sense orthogonal to the work presented in this paper, as once such a selection is made, one might still further optimize the actual scheme for computing the selected heuristics. The heuristics can be evaluated lazily, and rationally omitting some of them conditional on the results of previously computed heuristics in the same node can also be done. Generalizing both methods, one could try to optimize a policy for computing heuristics at the nodes, rather than just find the best combination, but how to do so is non-trivial. That is because the number of policies is at least doubly exponential in the number of heuristics under consideration, whearas the number of combinations is \u201conly\u201d exponential in the number of heuristics.\nA related line of research of performing meta reasoning for IDA*like algorithms is on choosing the threshold for the next iteration. In basic IDA*, the next threshold is strictly defined as the smallest value among nodes that were pruned. Learning and decision making techniques are applied to choose a different threshold such that time is saved but optimality of the algorithm is still maintained [14, 12, 18]. This issue is orthogonal to the problem addressed in this paper. In fact, our method for trading off time spent on computing heuristics with time spent on expanding additional nodes should be extensible to other IDA*-like algorithms. As in some of these algorithms the f-limit is not the next f-cost, such an extension should overcome one of the major stumbling blocks to further applicability of our method stated in Section 4.\nIn addition, the notion of type system was recently introduced to divide the state space into different types [9, 19, 11]. This was done usually for predicting the number of nodes expanded. Our work here can be seen as using a simple type system for deciding whether to evaluate the h2 heuristic."}, {"heading": "5.2 Summary and future work", "text": "Rational Lazy IDA* and its analysis can be seen as an instance of the rational meta-reasoning framework [13]. While this framework is very general, it is extremely hard to apply in practice. Recent work exists on meta-reasoning in DFS algorithms for CSP) [17] and in Monte-Carlo tree search [6]. This paper applies these methods successfully to a variant of IDA*.\nWe discussed two schemes for decreasing the time spent on computing heuristics during search. Lazy IDA* is very simple and a natural implementation of IDA* in the presence of 2 or more heuristics, especially if one is dominant but more costly. Rational Lazy IDA* allows additional cuts in the number of h2 computations, at the expense of being less informed and thereby generating more nodes. However, due to a rational tradeoff, this allows for an additional speedup, and Rational Lazy IDA* achieves the best overall performance in our domains.\nExperimental results on several domains show the advantage of RLIDA*. The non-realizable clairvoyant scheme discussed in Section 4 serves as a bound of the potential gain from RLIDA*. We note that the most important term in some of the domains is ph, the probability that h2 will indeed cause a cutoff. In this paper we provided a rudimentary method to bound ph based on previous samples. Future work might find better ways to estimate ph, hopefully getting closer to the clairvoyant ideal. One such direction can be to use any of the newly introduced type-systems, e.g., those that measure the correlation of a given heuristic between neighbors [19, 11].\nAnother direction is to relax some of the meta-reasoning assump-\ntions, especially those frequently violated in practice, and develop appropriate decision rules. In particular, consider the assumption that h1 does not prune any of the children. Preliminary runs on the tile puzzles showed that this assumption is violated in about 40% of the nodes, which seems to be a significant violation. Despite this violation, RLIDA* achieved most of the potential gain, so even though relaxing this assumption may further improve the runtime, the extra effort (and possible runtime overhead) may not be worth it. However, for the container relocation problem, this assumption was violated in about 60% of the nodes and there is also a considerable gap between RLIDA* and clairvoyant, so for this domain relaxing the assumption may be worth the effort.\nAlthough the techniques used in this paper may be applicable to other IDA*-like algorithms (e.g., RBFS, or DFBnB) the assumptions used in this paper are rather delicate, necessitating a different set of assumptions and thus different resulting meta-level decision schemes for such algorithm, another interesting item for future work."}], "references": [{"title": "Generalized best-first search strategies and the optimality of A*", "author": ["R. Dechter", "J. Pearl"], "venue": "Journal of the ACM, 32(3), 505\u2013536, ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1985}, {"title": "Online speedup learning for optimal planning", "author": ["Carmel Domshlak", "Erez Karpas", "Shaul Markovitch"], "venue": "JAIR, 44,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Inconsistent heuristics in theory and practice", "author": ["A. Felner", "U. Zahavi", "R. Holte", "J. Schaeffer", "N. Sturtevant", "Z. Zhang"], "venue": "Artificial Intelligence, 175(9-10), 1570\u20131603, ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A new efficient in situ sampling model for heuristic selection in optimal search", "author": ["Santiago Franco", "Michael W. Barley", "Patricia J. Riddle"], "venue": "Australasian Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Selecting computations: Theory and applications", "author": ["Nicholas Hay", "Stuart Russell", "David Tolpin", "Solomon Eyal Shimony"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A greedy look-ahead heuristic for the container relocation problem", "author": ["Bo Jin", "Andrew Lim", "Wenbin Zhu"], "venue": "in IEA/AIE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Depth-first iterative-deepening: An optimal admissible tree search", "author": ["R.E. Korf"], "venue": "Artificial Intelligence, 27(1), 97\u2013109, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1985}, {"title": "Time complexity of iterative-deepening-A* ", "author": ["Richard E. Korf", "Michael Reid", "Stefan Edelkamp"], "venue": "Artif. Intell.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Finding optimal solutions to the twenty-four puzzle", "author": ["Richard E. Korf", "Larry A. Taylor"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Predicting the size of IDA*\u2019s search tree", "author": ["Levi H.S. Lelis", "Sandra Zilles", "Robert C. Holte"], "venue": "Artif. Intell.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Enhanced iterativedeepening search", "author": ["Alexander Reinefeld", "Tony A. Marsland"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Reducing reexpansions in iterative-deepening search by controlling cutoff bounds", "author": ["Uttam K. Sarkar", "Partha P. Chakrabarti", "Sujoy Ghose", "S.C. De Sarkar"], "venue": "Artif. Intell.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "Bounded suboptimal search: A direct approach using inadmissible estimates", "author": ["Jordan T. Thayer", "Wheeler Ruml"], "venue": "Proceedings of the Twenty-second International Joint Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Toward rational deployment of multiple heuristics in a", "author": ["D. Tolpin", "T. Beja", "S.E. Shimony", "A. Felner", "E. Karpas"], "venue": "IJCAI, ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Rational deployment of CSP heuristics", "author": ["David Tolpin", "Solomon Eyal Shimony"], "venue": "in IJCAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A comparative study of ida*-style searches", "author": ["Benjamin W. Wah", "Yi Shang"], "venue": "in ICTAI, pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "Predicting the performance of IDA* using conditional distributions", "author": ["Uzi Zahavi", "Ariel Felner", "Neil Burch", "Robert C. Holte"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "An investigation of IDA* algorithms for the container relocation problem", "author": ["Huidong Zhang", "Songshan Guo", "Wenbin Zhu", "Andrew Lim", "Brenda Cheang"], "venue": "Proceedings of the 23rd International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems - Volume Part I,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "This was shown for depth-first search in CSPs [17], for Monte-Carlo tree search [6], and recently for A* [16].", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "This was shown for depth-first search in CSPs [17], for Monte-Carlo tree search [6], and recently for A* [16].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "This was shown for depth-first search in CSPs [17], for Monte-Carlo tree search [6], and recently for A* [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 6, "context": "IDA* [8] is a linear-space simulation of A*.", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "Thus it makes sense to examine how such speed-up was done for A* in a similar context, as was done in Lazy A* (or LA, for short) [16] by reducing the time spent on computing heuristics.", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "In rational lazy A* (RLA) [16], the ideas of lazy heuristic evaluation and trading additional node expan-", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "If the heuristic h(n) is admissible (never overestimates the real cost to the goal) then the set of nodes expanded by A* is both necessary and sufficient to find the optimal path to the goal [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 13, "context": "Unlike Lazy A*, where lazy evaluation must pay an overhead (re-inserting into the OPEN list) [16], Lazy IDA* (LIDA*) is straightforward and has no immediate", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "We then provide experimental results on sliding tile puzzles and on the container relocation problem [20], showing that RLIDA* outperforms both IDA* and LIDA*.", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "Another variant of HBP for LIDA* is applicable for a node n under the following two preconditions: (1) the operator between n and its parent p is bidirectional, and (2) both heuristics are consistent [4].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "When the heuristic is inconsistent then a mechanism called bidirectional pathmax (BPMX) can be used to propagate heuristic values from parents to children and vice versa [4].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "this context, lazy evaluation of heuristics is faster in time than exhaustive evaluation [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 13, "context": "Some of the ideas behind Rational Lazy IDA* are borrowed from those of [16] and", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "known to be IDA*-friendly (such as the 15-puzzle), or where recent work has shown IDA* to perform well, such as the container relocation problem [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "Regretfully, most planning problems (from the planning competitions) used in [16], are inappropriate for IDA* due to multiple transpositions in the search space.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "Note that there is another version of the weighted 15-puzzle where the cost of a tile move is the reciprocal of the number on the tile [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "For consistency of comparison, we used as test cases for the 15 puzzle 98 out of Korf\u2019s 100 tests [8]: all the tests that were solved in less than 20 minutes with standard IDA* using the Manhattan Dis-", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "more informative heuristic h2 we used the linear-conflict heuristic (LC) [10] which adds a value of 2 to MD for pairs of tiles that are in the same row (or the same column) as their respective goals but in a", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "The container relocation problem is an abstraction of a planning problem encountered in retrieving stacked containers for loading onto a ship in sea-ports [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 17, "context": "Optimally solving this problem is NP-hard [20].", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "We also address only the \u201crestricted\u201d version of the problem [20], where the only relocations allowed are of containers currently on top of the smallest numbered container.", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "The number of such containers in a state can be computed quickly, and forms an admissible heuristic denoted LB1 in [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "Following [20], we denote this heuristic by LB3.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "In the experiments, we used as instances the 49 hardest tests out of those that were solved in less than 20 minutes with the LB1 heuristic, from the CVS test suite described in [1, 7], retrived from http://iwi.", "startOffset": 177, "endOffset": 183}, {"referenceID": 1, "context": "[3] also noted that although theoretically taking the maximum of admissible heuristics is best within the context of A*, the overhead may not be worth it.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "7 To guarantee admissibility we made some minor notation changes from how this heuristic is formally stated in the original paper Automatically selecting combinations of heuristics for A* and IDA* from a large set of available heuristics was examined in [5].", "startOffset": 254, "endOffset": 257}, {"referenceID": 11, "context": "is saved but optimality of the algorithm is still maintained [14, 12, 18].", "startOffset": 61, "endOffset": 73}, {"referenceID": 10, "context": "is saved but optimality of the algorithm is still maintained [14, 12, 18].", "startOffset": 61, "endOffset": 73}, {"referenceID": 15, "context": "is saved but optimality of the algorithm is still maintained [14, 12, 18].", "startOffset": 61, "endOffset": 73}, {"referenceID": 7, "context": "In addition, the notion of type system was recently introduced to divide the state space into different types [9, 19, 11].", "startOffset": 110, "endOffset": 121}, {"referenceID": 16, "context": "In addition, the notion of type system was recently introduced to divide the state space into different types [9, 19, 11].", "startOffset": 110, "endOffset": 121}, {"referenceID": 9, "context": "In addition, the notion of type system was recently introduced to divide the state space into different types [9, 19, 11].", "startOffset": 110, "endOffset": 121}, {"referenceID": 14, "context": "Recent work exists on meta-reasoning in DFS algorithms for CSP) [17] and in", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Monte-Carlo tree search [6].", "startOffset": 24, "endOffset": 27}, {"referenceID": 16, "context": ", those that measure the correlation of a given heuristic between neighbors [19, 11].", "startOffset": 76, "endOffset": 84}, {"referenceID": 9, "context": ", those that measure the correlation of a given heuristic between neighbors [19, 11].", "startOffset": 76, "endOffset": 84}], "year": 2014, "abstractText": "Recent advances in metareasoning for search has shown its usefulness in improving numerous search algorithms. This paper applies rational metareasoning to IDA* when several admissible heuristics are available. The obvious basic approach of taking the maximum of the heuristics is improved upon by lazy evaluation of the heuristics, resulting in a variant known as Lazy IDA*. We introduce a rational version of lazy IDA* that decides whether to compute the more expensive heuristics or to bypass it, based on a myopic expected regret estimate. Empirical evaluation in several domains supports the theoretical results, and shows that rational lazy IDA* is a state-of-the-art heuristic combination method.", "creator": "LaTeX with hyperref package"}}}