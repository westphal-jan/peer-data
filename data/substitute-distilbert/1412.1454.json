{"id": "1412.1454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2014", "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation", "abstract": "we present a novel family of language model ( lm ) parameter techniques yielding sparse non - negative matrix ( snm ) estimation. a first set methodology experiments empirically evaluating it on the entire billion word benchmark shows that snm $ n $ - gram lists perform almost as successfully as the well - established kneser - ney ( kn ) predict. when using skip - gram features the plots are calculated to match the state - of - we - art recurrent graph network ( rnn ) lms ; combining the two modeling techniques yields the best known result on the benchmark. major computational advantages seeking snm over both maximum entropy and rnn lm estimation are probably its main strength, promising an approach that has the unprecedented flexibility in combining arbitrary features estimated and yet should bring to very large tables of data as gracefully as $ n $ - gram lms do.", "histories": [["v1", "Wed, 3 Dec 2014 19:42:12 GMT  (13kb)", "http://arxiv.org/abs/1412.1454v1", null], ["v2", "Fri, 26 Jun 2015 20:35:52 GMT  (13kb)", "http://arxiv.org/abs/1412.1454v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["noam shazeer", "joris pelemans", "ciprian chelba"], "accepted": false, "id": "1412.1454"}, "pdf": {"name": "1412.1454.pdf", "metadata": {"source": "CRF", "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation", "authors": ["Noam Shazeer", "Joris Pelemans", "Ciprian Chelba"], "emails": ["noam@google.com", "jpeleman@google.com", "ciprianchelba@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n14 54\nv1 [\ncs .L\nA first set of experiments empirically evaluating it on the One Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark.\nThe computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do."}, {"heading": "1 Introduction", "text": "A statistical language model estimates the prior probability values P (W ) for strings of words W in a vocabulary V whose size is in the tens, hundreds of thousands and sometimes even millions. Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition, which are assumed to be conditionally independent; we will assume that W is such a segment, or sentence.\nEstimating full sentence language models is computationally hard if one seeks a properly normalized probability model1 over strings of words of finite length in\n1We note that in some practical systems the constraint on using a properly normalized language\nV\u2217. A simple and sufficient way to ensure proper normalization of the model is to decompose the sentence probability according to the chain rule and make sure that the end-of-sentence symbol </s> is predicted with non-zero probability in any context. With W = w1, w2, . . . , wn we get:\nP (W ) = n\u220f\ni=1\nP (wi|w1, w2, . . . , wi\u22121) (1)\nSince the parameter space of P (wk|w1, w2, . . . , wk\u22121) is too large, the language model is forced to put the context Wk\u22121 = w1, w2, . . . , wk\u22121 into an equivalence class determined by a function \u03a6(Wk\u22121). As a result,\nP (W ) \u223c=\nn\u220f\nk=1\nP (wk|\u03a6(Wk\u22121)) (2)\nThe word strings encountered in a practical application are of finite length. The probability distribution P (W ) should assign probability 0.0 to strings of words of infinite length, and thus sum up to 1.0 over the set of strings of finite length\u2014 the support of P (W ). From a modeling point of view in a practical situation, the text gets broken into sentences, and the language model needs to predict the distinguished end-of-sentence symbol </s>. It can be easily shown that if the language model is smooth, i.e. P (wk|\u03a6(Wk\u22121)) > \u01eb > 0,\u2200wk,Wk\u22121, then we also have P (</s>|\u03a6(Wk\u22121)) > \u01eb > 0,\u2200Wk\u22121 which in turn ensures that the model assigns probability 1.0 to the set strings of words of finite length.\nResearch in language modeling consists of finding appropriate equivalence classifiers \u03a6 and methods to estimate P (wk|\u03a6(Wk\u22121)). The most successful paradigm in language modeling uses the (n\u2212 1)-gram equivalence classification, that is, defines\n\u03a6(Wk\u22121) . = wk\u2212n+1, wk\u2212n+2, . . . , wk\u22121\nOnce the form \u03a6(Wk\u22121) is specified, only the problem of estimating P (wk|\u03a6(Wk\u22121)) from training data remains.\nPerplexity as a Measure of Language Model Quality\nA statistical language model can be evaluated by how well it predicts a string of symbols Wt\u2014commonly referred to as test data\u2014generated by the source to be modeled.\nmodel is side-stepped at a gain in modeling power and simplicity.\nA commonly used quality measure for a given model M is related to the entropy of the underlying source and was introduced under the name of perplexity (PPL) [Jelinek, 1997]:\nPPL(M) = exp(\u2212 1\nN\nN\u2211\nk=1\nln [PM (wk|Wk\u22121)]) (3)\nTo give intuitive meaning to perplexity, it represents the number of guesses the model needs to make in order to ascertain the identity of the next word, when running over the test word string from left to right. It can be easily shown that the perplexity of a language model that uses the uniform probability distribution over words in the vocabulary V equals the size of the vocabulary; a good language model should of course have lower perplexity, and thus the vocabulary size is an upper bound on the perplexity of any sensible language model.\nVery likely, not all words in the test string Wt are part of the language model vocabulary. It is common practice to map all words that are out-of-vocabulary to a distinguished unknown word symbol, and report the out-of-vocabulary (OOV) rate on test data\u2014the rate at which one encounters OOV words in the test string Wt\u2014 as yet another language model performance metric besides perplexity. Usually the unknown word is assumed to be part of the language model vocabulary\u2014open vocabulary language models\u2014and its occurrences are counted in the language model perplexity calculation, Eq. (3). A situation less common in practice is that of closed vocabulary language models where all words in the test data will always be part of the vocabulary V ."}, {"heading": "2 Skip-gram Language Modeling", "text": "Recently, neural network (NN) smoothing [Bengio et al., 2003], [Emami, 2006], [Schwenk, 2007], and in particular recurrent neural networks [Mikolov, 2012] (RNN) have shown excellent performance in language modeling [Chelba et al., 2013]. Their excellent performance is attributed to a combination of leveraging long-distance context, and training a vector representation for words.\nAnother simple way of leveraging long distance context is to use skip-grams. In our approach, a skip-gram feature extracted from the context Wk\u22121 is characterized by the tuple (r, s, a) where:\n\u2022 r denotes number of remote context words\n\u2022 s denotes the number of skipped words\n\u2022 a denotes the number of adjacent context words\nrelative to the target word wk being predicted. For example, in the sentence, <S> The quick brown fox jumps over the lazy dog </S> a (1, 2, 3) skip-gram feature for the target word dog is: [brown skip-2 over the lazy]\nFor performance reasons, it is recommended to limit s and to limit either (r+a) or limit both r and s; not setting any limits will result in events containing a set of skip-gram features whose total representation size is quintic in the length of the sentence.\nWe configure the skip-gram feature extractor to produce all features f , defined by the equivalence class \u03a6(Wk\u22121), that meet constraints on the minimum and maximum values for:\n\u2022 the number of context words used r + a;\n\u2022 the number of remote words r;\n\u2022 the number of adjacent words a;\n\u2022 the skip length s.\nWe also allow the option of not including the exact value of s in the feature representation; this may help with smoothing by sharing counts for various skip features. Tied skip-gram features will look like: [curiousity skip-* the cat]\nIn order to build a good probability estimate for the target word wk in a context Wk\u22121 we need a way of combining an arbitrary number of skip-gram features fk\u22121, which do not fall into a simple hierarchy like regular n-gram features. The following section describes a simple, yet novel approach for combining such predictors in a way that is computationally easy, scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view."}, {"heading": "3 Sparse Non-negative Matrix Modeling", "text": ""}, {"heading": "3.1 Model definition", "text": "In the Sparse Non-negative Matrix (SNM) paradigm, we represent the training data as a sequence of events E = e1, e2, ... where each event e \u2208 E consists of a sparse non-negative feature vector f and a sparse non-negative target word vector t. Both vectors are binary-valued, indicating the presence or absence of a feature or target words, respectively. Hence, the training data consists of |E||Pos(f)| positive and |E||Pos(f)|(|V| \u2212 1) negative training examples, where Pos(f) denotes the number of positive elements in the vector f .\nA language model is represented by a non-negative matrix M that, when applied to a given feature vector f , produces a dense prediction vector y:\ny = Mf \u2248 t (4)\nUpon evaluation, we normalize y such that we end up with a conditional probability distribution PM(t|f) for a model M. For each word w \u2208 V that corresponds to index j in t, and its feature vector f that is defined by the equivalence class \u03a6 applied to the history h(w) of that word in a text, the conditional probability PM(w|\u03a6(h(w))) then becomes:\nPM(w|\u03a6(h(w))) = PM(tj |f) = yj\n\u2211|V| u=1 yu =\n\u2211 i\u2208Pos(f)Mij\n\u2211 i\u2208Pos(f) \u2211|V| u=1Miu\n(5)\nFor convenience, we will write P (tj |f) instead of PM(tj|f) in the rest of the paper. As required by the denominator in Eq. (5), this computation involves summing over all of the present features for the entire vocabulary. However, if we precompute the row sums \u2211|V| u=1Miu and store them together with the model, the evaluation can be done very efficiently in only |Pos(f)| time. Moreover, only the positive entries in Mi need to be considered, making the range of the sum sparse."}, {"heading": "3.2 Adjustment function and metafeatures", "text": "We let the entries of M be a slightly modified version of the relative frequencies:\nMij = e A(i,j)Cij\nCi\u2217 (6)\nwhere C is a feature-target count matrix, computed over the entire training corpus and A(i, j) is a real-valued function, dubbed adjustment function. For each featuretarget pair (fi, tj), the adjustment function extracts k new features \u03b1k, called metafeatures, which are hashed as keys to store corresponding weights \u03b8(hash(\u03b1k)) in a huge hash table. To limit memory usage, we use a flat hash table and allow collisions, although this has the potentially undesirable effect of tying together the weights of different metafeatures. Computing the adjustment function for any (fi, tj) then amounts to summing the weights that correspond to its metafeatures:\nA(i, j) = \u2211\nk\n\u03b8(hash[\u03b1k(i, j)]) (7)\nFrom the given input features, such as regular n-grams and skip n-grams, we construct our metafeatures as conjunctions of any or all of the following elementary metafeatures:\n\u2022 feature identity, e.g. [brown skip-2 over the lazy]\n\u2022 feature type, e.g. (1, 2, 3) skip-grams\n\u2022 feature count Ci\u2217\n\u2022 target identity, e.g. dog\n\u2022 feature-target count Cij\nwhere we reused the example from Section 2. Note that the seemingly absent feature-target identity is represented by the conjunction of the feature identity and the target identity. Since the metafeatures may involve the feature count and feature-target count, in the rest of the paper we will write \u03b1k(i, j, Ci\u2217, Cij). This will become important later when we discuss leave-one-out training.\nEach elementary metafeature is joined with the others to form more complex metafeatures which in turn are joined with all the other elementary and complex metafeatures, ultimately ending up with all 25 \u2212 1 possible combinations of metafeatures.\nBefore they are joined, count metafeatures are bucketed together according to their (floored) log2 value. As this effectively puts the lowest count values, of which there are many, into a different bucket, we optionally introduce a second (ceiled) bucket to assure smoother transitions. Both buckets are then weighted according to the log2 fraction lost by the corresponding rounding operation. Note that if we apply double bucketing to both the feature and feature-target count, the amount of metafeatures per input feature becomes 27 \u2212 1.\nWe will come back to these metafeatures in Section 4.4 where we examine their individual effect on the model."}, {"heading": "3.3 Loss function", "text": "Estimating a model M corresponds to finding optimal weights \u03b8k for all the metafeatures for all events in such a way that the average loss over all events between the target vector t and the prediction vector y is minimized, according to some loss function L. The most natural choice of loss function is one that is based on the multinomial distribution. That is, we consider t to be multinomially distributed with |V| possible outcomes. The loss function Lmulti then is:\nLmulti(y, t) = \u2212log(Pmulti(t|f)) = \u2212log( yj\n\u2211|V| u=1 yu ) = log(\n|V|\u2211\nu=1\nyu)\u2212 log(yj)\n(8)\nAnother possibility is the loss function based on the Poisson distribution2 : we consider each tj in t to be Poisson distributed with parameter yj . The conditional probability of PPoisson(t|f) then is:\nPPoisson(t|f) = \u220f\nj\u2208t\ny tj j e \u2212yj\ntj! (9)\nand the corresponding Poisson loss function is:\nLPoisson(y, t) = \u2212log(PPoisson(t|f)) = \u2212 \u2211\nj\u2208t\n[tj log(yj)\u2212 yj \u2212 log(tj !)]\n= \u2211\nj\u2208t\nyj \u2212 \u2211\nj\u2208t\ntj log(yj) (10)\nwhere we dropped the last term, since tj is binary-valued3 . Although this choice is not obvious in the context of language modeling, it is well suited to gradient-based optimization and, as we will see, the experimental results are in fact excellent."}, {"heading": "3.4 Model Estimation", "text": "The adjustment function is learned by applying stochastic gradient descent on the loss function. That is, for each feature-target pair (fi, tj) in each event we need to update the parameters of the metafeatures by calculating the gradient with respect to the adjustment function.\nFor the multinomial loss, this gradient is:\n\u2202(Lmulti(Mf , t))\n\u2202(A(i, j)) =\n\u2202(log( \u2211|V|\nu=1(Mf)u)\u2212 log(Mf )j)\n\u2202(Mij)\n\u2202(Mij) \u2202(Aij)\n= [ \u2202(log(\n\u2211|V| u=1(Mf)u))\n\u2202(Mij) \u2212\n\u2202(log(Mf )j)\n\u2202(Mij) ]Mij\n= [ \u2202( \u2211|V|\nu=1(Mf )u)\u2211|V| u=1(Mf )u\u2202(Mij) \u2212 \u2202(Mf)j (Mf)j\u2202(Mij) ]Mij\n= ( fi\n\u2211|V| u=1(Mf )u\n\u2212 fi\nyj )Mij\n= fiMij( 1\n\u2211|V| u=1 yu\n\u2212 1\nyj ) (11)\n2Although we do not use it at this point, the Poisson loss also lends itself nicely for multiple target prediction which might be useful in e.g. subword modeling.\n3In fact, even in the general case where tk can take any non-negative value, this term will disappear in the gradient, as it is independent of M.\nThe problem with this update rule is that we need to sum over the entire vocabulary V in the denominator. For most features fi, this is not a big deal as Ciu = 0, but some features occur with many if not all targets e.g. the empty feature for unigrams. Although we might be able to get away with this by re-using these sums and applying them to many/all events in a mini batch, we chose to work with the Poisson loss in our first implementation.\nIf we calculate the gradient of the Poisson loss, we get the following:\n\u2202(LPoisson(Mf , t))\n\u2202(A(i, j)) =\n\u2202( \u2211|V| u=1 (Mf)u \u2212 \u2211|V| u=1 tu log(Mf )u)\n\u2202(Mij)\n\u2202(Mij)\n\u2202(A(i, j))\n= [ \u2202( \u2211|V| u=1 (Mf)u)\n\u2202(Mij) \u2212\n\u2202( \u2211|V|\nu=1 tu log(Mf )u)\n\u2202(Mij) ]Mij\n= [fi \u2212 tj\n(Mf)j \u2202(Mf )j \u2202(Mij) ]Mij\n= [fi \u2212 tjfi\n(Mf)j ]Mij\n= fiMij(1\u2212 tj\nyj ) (12)\nIf we were to apply this gradient to each (positive and negative) training example, it would be computationally too expensive, because even though the second term is zero for all the negative training examples, the first term needs to be computed for all |E||Pos(f)||V| training examples.\nHowever, since the first term does not depend on yj , we are able to distribute the updates for the negative examples over the positive ones by adding in gradients for a fraction of the events where fi = 1, but tj = 0. In particular, instead of adding the term fiMij , we add fitj\nCi\u2217 Cij Mij:\nCi\u2217 Cij Mij\n\u2211\ne=(fi,tj)\u2208E\nfitj = Ci\u2217\nCij MijCij = Mij\n\u2211\ne=(fi,tj)\u2208E\nfi (13)\nwhich lets us update the gradient only on positive examples. We note that this update is only strictly correct for batch training, and not for online training since Mij changes after each update. Nonetheless, we found this to yield good results as well as seriously reducing the computational cost. The online gradient applied to each training example then becomes:\n\u2202(LPoisson(Mf , t))\n\u2202(A(i, j)) = fitjMij(\nCi\u2217 Cij \u2212 1 yj ) (14)\nwhich is non-zero only for positive training examples, hence speeding up computation by a factor of |V|.\nThese aggregated gradients however do not allow us to use additional data to train the adjustment function, since they tie the update computation to the relative frequencies Ci\u2217\nCij . Instead, we have to resort to leave-one-out training to prevent\nthe model from overfitting the training data. We do this by excluding the event, generating the gradients, from the counts used to compute those gradients. So, for each positive example (fi, tj) of each event e = (f , t), we compute the gradient, excluding fi from Ci\u2217 and fitj from Cij . For the gradients of the negative examples on the other hand we only exclude fi from Ci\u2217 and we leave Cij untouched, since here we did not observe tj . In order to keep the aggregate computation of the gradients for the negative examples, we distribute them uniformly over all the positive examples with the same feature; each of the Cij positive examples will then compute the gradient of Ci\u2217\u2212Cij Cij\nnegative examples. To summarize, when we do leave-one-out training we apply the following gra-\ndient update rule on all positive training examples:\n\u2202(LPoisson(Mf , t))\n\u2202(A(i, j)) = fitj Ci\u2217 \u2212 Cij Cij Cij Ci\u2217 \u2212 1 e \u2211 k \u03b8(hash[\u03b1k(i,j,Ci\u2217\u22121,Cij)])\n+ fitj Cij \u2212 1\nCi\u2217 \u2212 1\ny\u2032j \u2212 1\ny\u2032j e \u2211 k \u03b8(hash[\u03b1k(i,j,Ci\u2217\u22121,Cij\u22121)]) (15)\nwhere y\u2032j is the product of leaving one out for all the relevant features i.e. y \u2032 j = (M\u2032f)j and M\u2032ij = e \u2211 k \u03b8(hash[\u03b1k(i,j,Ci\u2217\u22121,Cij\u22121)]) Cij\u22121 Ci\u2217\u22121 ."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Corpus: One Billion Benchmark", "text": "Our experimental setup used the One Billion Word Benchmark corpus4 made available by [Chelba et al., 2013].\nFor completeness, here is a short description of the corpus, containing only monolingual English data:\n\u2022 Total number of training tokens is about 0.8 billion\n\u2022 The vocabulary provided consists of 793471 words including sentence boundary markers <S>, <\\S>, and was constructed by discarding all words with count below 3\n4http://www.statmt.org/lm-benchmark\n\u2022 Words outside of the vocabulary were mapped to <UNK> token, also part of the vocabulary\n\u2022 Sentence order was randomized\n\u2022 The test data consisted of 159658 words (without counting the sentence beginning marker <S> which is never predicted by the language model)\n\u2022 The out-of-vocabulary (OoV) rate on the test set was 0.28%."}, {"heading": "4.2 SNM for n-gram LMs", "text": "When trained using solely n-gram features, SNM comes very close to the stateof-the-art Kneser-Ney [Kneser and Ney, 1995] (KN) models. Table 1 shows that Katz [Katz, 1995] performs considerably worse than both SNM and KN which only differ by about 5%. When we interpolate these two models linearly, the added gain is only about 1%, suggesting that they are approximately modeling the same things. The difference between KN and SNM becomes smaller when we increase the size of the context, going from 5% for 5-grams to 3% for 8-grams, which indicates that SNM is better suited to a large number of features.\n4.3 Sparse Non-negative Modeling for Skip n-grams\nWhen we incorporate skip-gram features, we can either build a \u2018pure\u2019 skip-gram SNM that contains no regular n-gram features, except for unigrams, and interpolate this model with KN, or we can build a single SNM that has both the regular ngram features and the skip-gram features. We compared the two approaches by choosing skip-gram features that can be considered the skip-equivalent of 5-grams i.e. they contain at most 4 words. In particular, we used skip-gram features where the remote span is limited to at most 3 words for skips of length between 1 and 3 (r = [1..3], s = [1..3], r + a = [1..4]) and where all skips longer than 4 are tied\nand limited by a remote span length of at most 2 words (r = [1..2], s = [4..\u2217], r + a = [1..4]). We then built a model that uses both these features and regular 5-grams (SNM5-skip), as well as one that only uses the skip-gram features (SNM5skip (no n-grams)).\nAs it turns out and as can be seen from Table 2, it is better to incorporate all the features into one single SNM model than to interpolate with a KN 5-gram model (KN5). Interpolating the all-in-one SNM5-skip with KN5 yields almost no additional gain.\nThe best SNM results so far (SNM10-skip) were achieved using 10-grams, together with untied skip features of at most 5 words with a skip of exactly 1 word (s = 1, r + a = [1..5]) as well as tied skip features of at most 4 words where only 1 word is remote, but up to 10 words can be skipped (r = 1, s = [1..10], r + a = [1..4]).\nThis mixture of rich short-distance and shallow long-distance features enables the model to achieve state-of-the-art results, as can be seen in Table 3. When we compare the perplexity of this model with the state-of-the art RNN results in [Chelba et al., 2013], the difference is only 3%. Moreover, although our model has more parameters than the RNN (33 vs 20 billion), training takes about a tenth of the time (24 hours vs 240 hours). Interestingly, when we interpolate the two models, we have an additional gain of 20%, and as far as we know, the perplexity of 41.3 is already the best ever reported on this database, beating the previous best by 6% [Chelba et al., 2013].\nFinally, when we optimize interpolation weights over all models in [Chelba et al., 2013], including SNM5-skip and SNM10-skip, the contribution of the other models as well as the perplexity reduction is negligible, as can be seen in Table 3, which also summarizes the perplexity results for each of the individual models."}, {"heading": "4.4 Ablation Experiments", "text": "To find out how much, if anything at all, each metafeature contributes to the adjustment function, we ran a series of ablation experiments in which we ablated one metafeature at a time. When we experimented on SNM5, we found, unsurprisingly, that the most important metafeature is the feature-target count. At first glance, it does not seem to matter much whether the counts are stored in 1 or 2 buckets, but the second bucket really starts to pay off for models with a large number of singleton features e.g. SNM10-skip5. This is not the case for the feature counts, where having a single bucket is always better, although in general the feature counts do not contribute much. In any case, feature counts are definitely the least important for the model. The remaining metafeatures all contribute more or less equally, all of which can be seen in Table 4."}, {"heading": "5 Related Work", "text": "SNM estimation is closely related to all n-gram LM smoothing techniques that rely on mixing relative frequencies at various orders. Unlike most of those, it combines the predictors at various orders without relying on a hierarchical nesting of the contexts, setting it closer to the family of maximum entropy (ME) [Rosenfeld, 1994],\n5Ideally we want to have the SNM10-skip ablation results as well, but this takes up a lot of time, during which other development is hindered.\nor exponential models. We are not the first ones to highlight the effectiveness of skip n-grams at capturing dependencies across longer contexts, similar to RNN LMs; previous such results were reported in [Singh and Klakow, 2013].\nThe speed-ups to ME, and RNN LM training provided by hierarchically predicting words at the output layer [Goodman, 2001b], and subsampling [Xu et al., 2011] still require updates that are linear in the vocabulary size times the number of words in the training data, whereas the SNM updates in Eq. (15) for the much smaller adjustment function eliminate the dependency on the vocabulary size.\nThe computational advantages of SNM over both Maximum Entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do."}, {"heading": "6 Conclusions and Future Work", "text": "We have presented SNM, a new family of LM estimation techniques. A first empirical evaluation on the One Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs perform almost as well as the well-established KN models.\nWhen using skip-gram features the models are able to match the stat-of-the-art RNN LMs; combining the two modeling techniques yields the best known result on the benchmark.\nFuture work items include model pruning, exploring richer features similar to [Goodman, 2001a], as well as richer metafeatures in the adjustment model, mixing SNM models trained on various data sources such that they perform best on a given development set, and estimation techniques that are more flexible in this respect."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al", "2003] Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Class-Based N-gram Models of Natural Language", "author": ["Brown et al", "1992] P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J. Della Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}, {"title": "Improved Backing-Off For M-Gram Language Modeling", "author": ["Kneser", "Ney", "1995] R. Kneser", "H. Ney"], "venue": "In Proceedings of ICASSP", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio", "2005] F. Morin", "Y. Bengio"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["Sundermeyer et al", "2012] M. Sundermeyer", "R. Schluter", "H. Ney"], "venue": "In Proceedings of Interspeech", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Chelba et al", "2013] Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "Google Tech Report 41880", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Comparing RNNs and log-linear interpolation of improved skip-model on four Babel languages: Cantonese, Pashto, Tagalog, Turkish", "author": ["Singh", "Klakow", "2013] M. Singh", "D. Klakow"], "venue": "Proceedings of ICASSP", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Efficient Subsampling for Training Complex Language Models", "author": ["Xu et al", "2011] Puyang Xu", "A. Gunawardana", "S. Khudanpur"], "venue": "In Proceedings of EMNLP", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}], "referenceMentions": [], "year": 2017, "abstractText": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.", "creator": "LaTeX with hyperref package"}}}