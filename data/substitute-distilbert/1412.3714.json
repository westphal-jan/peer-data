{"id": "1412.3714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Feature Weight Tuning for Recursive Neural Networks", "abstract": "this paper addresses how a recursive neural network model can automatically leave out useless information or emphasize linguistic evidence, in other words, to perform \" weight tuning \" for macro - level representation acquisition. we propose such models, weighted neural network ( wnn ) and binary - expectation neural network ( benn ), which automatically control how much one specific unit falls to every higher - than representation. the proposed model can then performed as incorporating a more powerful compositional function for efficient acquisition in recursive neural networks. experimental results proved the significant improvement over standard neural models.", "histories": [["v1", "Thu, 11 Dec 2014 16:35:27 GMT  (207kb,D)", "http://arxiv.org/abs/1412.3714v1", null], ["v2", "Sat, 13 Dec 2014 00:57:57 GMT  (207kb,D)", "http://arxiv.org/abs/1412.3714v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG", "authors": ["jiwei li"], "accepted": false, "id": "1412.3714"}, "pdf": {"name": "1412.3714.pdf", "metadata": {"source": "CRF", "title": "Feature Weight Tuning for Recursive Neural Networks", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Recursive neural network models [1] constitute one type of neural structure for obtaining higherlevel representations beyond word-level such as phrases or sentences. It operates on a tree structure (e.g., parse tree) in which long-term dependency can be to some extent captured and works in a bottom-up fashion until the root is reached. Figure 1 gives a brief illustration about how recursive neural network works to obtain the distributed representation for the short sentence \u201cThe movie is wonderful\u201d. Suppose his and hwonderful are the embeddings for word is and wonderful. Specifically, the representation for their parent node VP at second layer is given by:\nhVP = f(W \u00b7 [his,hwonderful] + b) (1) whereW and b denote the parameters involved in the convolution. f(\u00b7) is the convolutional function, usually tanh or sigmod function, which exhibit non-linear property.\nFor different NLP tasks, the obtained embeddings could be further fed into task-specific machine learning models1, through which parameters could be optimized. Take sentiment analysis as an example, we could feed the aforementioned sentence into a logistic regression model to classify it as either positive or negative. These embeddings are sometimes more capable of capturing the latent semantic meanings or syntactic rules with the text than manually developed features, from which many NLP tasks would benefit (e.g., [2, 3]).\nUnfortunately, this type of structure suffers some sorts of intrinsic drawbacks. Revisit Figure 1, common sense tells us that tokens like \u201cthe\u201d, \u201cmovie\u201d and \u201cis\u201d do not contribute much to the sentiment decision but word \u201cwonderful\u201d is the key part (and a good machine learning model should have the ability of learning these rules). Unfortunately, the intrinsic structure of recursive neural network makes it less flexible to get rid of the influence from these less sentiment-related words. If the keyword \u201cwonderful\u201d hides too deep in the parse tree, for example, as in sentence \u201cI studied Russia in Moscow, where all my family think the winter is wonderful\u201d, it will takes quite a few\n1Of course, embeddings could also be optimized through the task-specific objective functions.\nar X\niv :1\n41 2.\n37 14\nv1 [\ncs .N\nE ]\n1 1\nD ec\nconvolution steps before it comes up to the surface, with the consequence that its influence on the final sentence representation could be very trivial. Such problem, which could also be referred to as gradient vanishing [9]. is not specific for recursive models, but for most deep learning architectures.\nWhen we compare neural models with SVM, one notable weakness of big-of-word based SVM is its inability of considering how words are combined to form meanings (or order information in other words) [10]. But interestingly, such downside of SVM comes with the advantage of resilience in feature managing as the optimization is \u201cflat-expanded\u201d. Low weights will be assigned to lessinformative evidence, which may further be pushed to zero by regularization. Table 1 gives a brief comparison between unigram based SVM and neural network models for sentence-level sentiment prediction on Pang et al.\u2019s dataset [4], and as can be seen, in this specific task, standard neural network models underperform SVM2.\nRevisit the form of Equ.1, there are two straws we can grasp at to deal with the aforementioned problem: (1) expecting the learned feature embeddings for less useful words such as the3 exert very little influence (for example, a zero vector for the best) (2) expecting the compositional parameters W and b are extremely powerful. For the former, it is sometimes hard, as mostly we borrow (or initialize) word embeddings from those trained from large corpus (e.g., word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data4.\nRegarding the latter issue, several alternative compositional functions have been proposed to enable more varieties in composition to cater. Recent proposed approaches include, for example, MatrixVector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which\n2To note, results here are not comparable with Socher et al.\u2019s work [2] which obtains state-of-art performance for sentiment classification, as here labels at sentence-level constitute only sort of supervision for both SVM and neural network models (for details, see footnote 7).\n3We just use this example for illustration. Practically, the might be a good sentiment indicator as it usually co-appears with superlatives.\n4There are cases, for example, [2], where task-specific word embeddings are learned. But it requires sufficient training data to avoid over fitting. For example, Socher et al.\u2019s work labels every single node as positive/negative/neutral along parse trees (with a total number of more than 200,000 phrases).\nassociates different labels (e.g., POS tags, relation tags) with different sets of compositional parameters. These approaches to some extent enlarge the power of compositional functions.\nIn this paper, we borrow the idea of \u201cweight tuning\u201d from feature based SVM and try to incorporate such idea into neural architectures. To achieve this goal, we propose two recursive neural architectures, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN). The proposed approaches associate each node in the recursive network with additional parameters, indicating how important current stage is for final decision. Parameters associated with proposed models are automatically optimized through the objective function manifested by the data. Our model combines the capability of neural models to capture the local compositional meanings with weight tuning approach to reduce the influence of undesirable information at the same time. Experimental results show the effectiveness of the proposed models in a range of different NLP tasks.\nThe rest of this paper is organized as follows: Section 2 briefly describes the related work. The details of WNN and BENN are illustrated in Section 4 and experimental results are presented in Section 5, followed by a brief conclusion."}, {"heading": "2 Related Work", "text": "Distributed representations, calculated based on neural frameworks, are extended beyond tokenlevel, to represent N-grams [15], phrases [2], sentences (e.g., [3, 16]), discourse [17, 13], paragraphs [18] or documents [19]. Recursive and recurrent [20, 21] models constitute two types of commonly used frameworks for sentence-level embedding acquisition. Different variations of recurrent/recursive models are proposed to cater for different scenarios (e.g., [3, 2]). Other recently proposed approaches included sentence compositional approach proposed in [22], or paragraph/sentence vector [18] where representations are optimized through predicting words within the sentence.\nNeural network architecture sometimes requires a vector representation of each input token. Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.\nBoth of the proposed architectures are in this work inspired by the long short-term memory (LSTM) model, first proposed by Hochreiter and Schmidhuber back in 1990s [27, 28] to process time sequence data where there are very long time lags of unknown size between important events5. LSTM associates each time with a series of \u201cgates\u201d to determine whether the information from early timesequence should be forgotten [28] and when current information should be allowed to flow into or out of the memory. LSTM could partially address gradient vanishing problem in recurrent neural models and have been widely used in machine translation [29, 30]"}, {"heading": "3 \u201cWeight Tuning\u201d for Neural Network", "text": "Let s denote a sequence of token s = {w1,w2, ...,wns }. It could be phrases, sentences etc. Each word w is associated with a specific vector embedding ew = {e1w, e2w, ..., eKw}, where K denotes the dimension of the word embedding. We wish to compute the vector representation for sentence s, denoted as hs = {h1s,h 2 s, ...,h K s }. Parse tree for each sentence is obtained from Stanford Parser [31]."}, {"heading": "3.1 WNN for Recursive Neural Network", "text": "For any node C in the parse tree, it is associated with representation hC. The basic idea of WNN is to associate each node C with an additional weight variable MC, which is in range (0,1), to denote the importance of current node. Technically, MC is used to pushing the output representation of not-useful node towards the direction of 0 and retain relatively important information.\nWe expect that information regarding the importance of current node (e.g., whether it is relevant to positive/negative sentiment) is embedded in its representation hC. So we use a convolution func-\n5http://en.wikipedia.org/wiki/Long_short_term_memory\ntion to enable this type of information to emerge to the surface from the following compositional functions:\nRC = f(WM \u00b7 hC + bM) (2)\nMC = sigmod(U T M \u00b7 RC) (3)\nwhere WM is a D \u00d7 K dimensional matrix and bB is the 1 \u00d7 K bias vector. RC is a K dimensional intermediate vector. Such implementation can be viewed as using a three-layer neural model with D latent neurons for an output projected to a [0,1] space.\nLet output(C) denote the output from node C to its parent. In WNN, output(C) would consider both current information, which is embedded in the embedding hC and its related importance MC. output(C) is therefore given by\noutput(C) =MC \u2217 hC (4)\nRecall the example in Figure 1, we have:\noutput(the) =Mthe \u00b7 hthe output(movie) =Mmovie \u00b7 hmovie\n(5)\nIf the model thinks not too much relevant information embedded in hC, the value of MC would be small, pushing the output vector towards 0. The representations for parents, for example VP and NP in Figure 1, are therefore computed as follows:\nhVP = tanh(WB \u00b7 [output(is),output(wonderful)]) hNP = tanh(WB \u00b7 [output(the),output(movie)])\n(6)\nwhereWB denotes a K\u00d7 2K dimensional matrix and [output(is),output(wonderful)] denotes the concatenation of the two vectors. In an optimum situation, Mthe and Mmovie will take the values around 0, leading to the representation of node NP to an around-zero vector.\nTraining WNN For illustration purpose, we use a binary classification task to show how to train WNN. To note, the described training approach applies to other situations (e.g., multi-class classification, regression) with minor adjustments.\nIn binary classification task, each sequence is associated with a gold-standard label ys. ys takes value of 1 if positive and 0 otherwise. Standardly, to determine the value of ys, we feed the representation hs into a logistic regression model:\np(ys = 1) = sigmod(UThs + b) (7)\nwhere UT is a 1 \u2217 K vector and b denotes the bias. Then by adding the regularization part parameterized by Q, the loss function J(\u0398) for the training dataset is given by:\nJ(\u0398) = \u2212 log[p(ys = 1)ys \u00b7 (1 \u2212 p(ys = 1))1\u2212ys ] +Q \u2211 \u03b8\u2208\u0398 \u03b82 (8)\nRevisit the example in Figure 1, for any parameter \u03b8 to optimize, the calculation for gradient \u2202J/\u2202\u03b8 is trivial once \u2202[MVP \u00b7 hVP]/\u2202\u03b8 and \u2202[MNP \u00b7 hNP]/\u2202\u03b8 are obtained, which are given by:\n\u2202MVP \u00b7 hVP \u2202\u03b8 =MVP \u2202hVP \u2202\u03b8 + \u2202MVP \u2202\u03b8 hVP (9)\nTo note, hVP is embraced in MVP. As all components in Equ 9 are continuous, the gradient can be efficiently obtained from standard backpropagation [32, 33]."}, {"heading": "3.2 BENN for Recursive Neural Network", "text": "BENN associates each node with a binary variable BC, which is sampled from a binary distribution parameterized by LC. LC is a scalar fixed to the range of [0,1], indicating the possibility that current node should pass information to its ancestors. LC is obtained in the similar ways as in WNN by using a convolution to project the current representation hC to a scalar lying within [0,1].\nRC = f(WB \u00b7 hC + bB) (10)\nLC = sigmod(U T B \u00b7 RC) (11)\nBC \u223c binary(LC) (12) For smoothing purpose, in BENN, current node C outputs the expectation of embedding hC to its parent, as given by: output(C) = E[hC] (13) Take the case in Figure 1 as an example again, vector hNP will therefore follow the following distribution:\np(hNP = tanh(WB[hthe,hmovie])) = Lthe \u00b7 Lmovie p(hNP = tanh(WB[0,hmovie])) = (1 \u2212 Lthe) \u00b7 Lmovie p(hNP = tanh(WB[hthe, 0])) = Lthe \u00b7 (1 \u2212 Lmovie) p(hNP = tanh(WB[0, 0])) = (1 \u2212 Lthe) \u00b7 (1 \u2212 Lmovie)\n(14)\nE[hNP] can be further obtained based on such distribution E[hNP] = \u2211 h P(hNP = h) \u00b7 h (15)\nTo note, for leaf nodes, E[hC] = hC.\nTraining BENN For training, we again use binary sentiment classification for illustration. For any sentence s with label ys, we have\np(ys = 1) = sigmod(UTE[hs] + b) (16) With respect to any given parameter \u03b8, the derivative of E[hs] is further given by:\n\u2202E(hs)\n\u2202\u03b8\n= \u2202LNP \u00b7 LVP \u00b7 tanh(WB[E(hNP),E(hVP)])\n\u2202\u03b8\n+ \u2202(1 \u2212 LNP) \u00b7 LVP \u00b7 tanh(WB[0,E(hVP)])\n\u2202\u03b8\n+ \u2202LNP \u00b7 (1 \u2212 LVP) \u00b7 tanh(WB[E(hNP), 0]\n\u2202\u03b8\n+ \u2202(1 \u2212 LNP) \u00b7 (1 \u2212 LVP) \u00b7 tanh(WB[0, 0])\n\u2202\u03b8\n(17)\nWith all components being continuous, the gradient can be efficiently obtained from standard backpropagation."}, {"heading": "4 Experiment", "text": "We perform experiments to better understand the behavior of the proposed models compared with standard neural models (and other variations). To achieve this, we implement our model on problems that require fixed-length vector representations for phrases or sentences."}, {"heading": "4.1 Sentiment Analysis", "text": "Sentence-level Labels We first perform experiments on dateset from [4]. In this setting, binary labels at the top of sentence constitute the only resource of supervision (to note, it is different from setting in [2]). All neural models adopt the same settings for fair comparison: L2 regularization, gradient decent based on AdaGrad with mini batch size of 25, tuned parameters for regularization on 5-fold cross validation.\nFor standard neural models, we implement two settings: standard (GLOVE) where word embeddings are directly fixed to GLOVE and standard (learned) where word embeddings are treated as parameters to optimize in the framework. Additionally, we implemented some recent popular variations of neural models with more sophisticatedly designed compositional functions, including:\n\u2022 MV-RNN (Matrix-Vector RNN): which was proposed in [12] which represents every node in a parse tree as both a vector and a matrix. Given the vector representation hC1 , matrix representation VC1 for child node C1, hC2 and VC2 for child node C2, the vector representation hp and matrix representation Vp for parent p are given by:\nhp = f(W1[VC1 \u00b7 hC2 ,VC2 \u00b7 hC1 ]) Vp = f(W1[VC1 ,VC2 ])\n(18)\nWe fix word vector embeddings using SENNA and treat matrix representations as parameters to optimize.\n\u2022 RNTN (Recursive Neural Tensor Network): proposed in [2]. Given hC1 and hC2 for children nodes, RNTN computes parent vector hp in the following way:\nhp = f([hC1 ,hC2 ] TV[hC1 ,hC2 ] +W[hC1 ,hC2 ]) (19)\n\u2022 Label-specific: associate each of the sentence roles (i.e., VP, NP or NN) with a specific composition matrix.\nWe report results in Table 2. As discussed earlier, standard neural models underperform the bag of word models. To note, for derivations of standard neural models such as Standard (learned) and MVRNN with many more parameters to learn, the performance is even worse due to over-fitting. Our revised versions of neural models, although not significantly output bag of words SVM, generates better results, yielding significant improvement over standard neural models and existing revised versions. Figure 3 illustrates the automatic learned muted factor MC regarding different nodes in the parse tree based on recursive network. As we can observe , the model is capable of learning the proper weight of vocabularies, assigning larger weight values to important sentiment indicators (e.g., wonderful, silly and tedious) and suppressing the influence of less important ones. We attribute\nthe better performance of proposed models over standard neural models to such automatic weighttuning ability.\nTo note, in this scenario, we are not claiming that we generate state-of-art results using the proposed model. More sophisticated bag-of-word models, for example, (e.g., [34]) can generate better performance what the proposed models achieve. The point we wish to illustrate here is that the proposed models provide a promising perspective over standard neural models due to the \u201cweight tuning\u201d property. And in the cases where more detailed data is available to capture the compositionally, the proposed models hold promise to generate more compelling results, as we will illustrate in Socher et al\u2019s setting for sentiment analysis.\nSocher et al\u2019s setting We now consider Socher et al\u2019s setting [2] for sentiment analysis, where contains gold-standard labels at every phrase node in the parse tree. The task could be considered either as a 5-way fine-grained classification task where the labels are verynegative/negative/neutral/positive/very-positive or a 2-way coarse-way as positive/negative based on labeled dataset. We follow the experimental protocols described in [2] (word embeddings are treated as parameters to learn rather than fixed to externally borrowed embeddings). In this work we only consider labeling the full sentences.\nIn addition to varieties of neural models mentioned in Socher et al\u2019s work, we also report the performance of recently proposed paragraph vector model [18], which first obtains sentence embeddings in an unsupervised manner by predicting words within the context and then feeds the pre-obtained embeddings into a logistic regression model. paragraph vector achieves current the state-of-art performance regarding Socher et al\u2019s dataset.\nPerformances are reported in Table 3. As can be seen, the proposed approach slightly underperforms current state-of-art performance achieved by paragraph vector but outperforms all the other versions of recursive neural models, indicating the adding \u201cweight tuning\u201d parameters indeed leads to better compositionally.\nTo note, when there is more comprehensive dataset which we can rely on to obtain the favorable task-specific word embeddings, compositionally plays an important role in deciding whether the review is positive or negative by harnessing local word order information. In that case, neural models exhibit its power in capturing local evidence from the composition, leading to significantly better performance than all bag-of-words based models (i.e., SVM and Bigram Naives Bayes)."}, {"heading": "4.2 Document-level Sentiment Analysis on IMDB dataset", "text": "We move on to sentiment analysis at document level. We use the IMDB dataset proposed by Maas et al. [34]. The dataset consists of 100,000 movie reviews taken from IMDB and each movie review contains several sentences. We follow the experimental protocols described in [34].\nWe first train word vectors from word2vect using the 75,000 training documents. Next we train the compositional functions using the 25,000 labeled documents by keeping the word embedding fixed. We first obtain sentence-level representations using WNN/BENN (recursive). As each review contains multiple sentences, we convolute sentence representations to one single vector using WNN/BENN recurrent network. We cross validate parameters using the labeled documents and test the models on the 25,000 testing reviews.\nThe results of our approach and other baselines are reported in Table 5. As can be seen, for long documents, bag-of-words (both unigram and diagram) perform quite well and it is difficult to beat. Standard neural models again do not generate competent results compared with bag of word models in this task. But by incorporating weighted tuning mechanism, we got much better performance, roughly 5% when compared against standard neural models. Although WNN and BENN still underperform current state-of-art model Paragraph Vector [18], they produces better performance than bag-of-word models."}, {"heading": "4.3 Sentence Representations for Coherence Evaluation", "text": "Sentiment analysis forces more on the semantic perspective of meaning. Next we turn to a more syntactic oriented task, where we obtain sentence-level representations based on the proposed model to decide the coherence of a given sequence of sentences.\nWe use corpora widely employed for coherence prediction [35, 36]. One contains reports on airplane accidents from the National Transportation Safety Board and the other contains reports about earthquakes from the Associated Press. Standardly, we use pairs of articles, one containing the original document order which is assumed to be coherent and used as positive examples, and the other a random permutation of the sentences from the same document, which are treated as not-coherent examples. We follow the protocols introduced in [35, 37, 14] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent. In test time, we assume that the model makes a right decision if the original document gets a score higher than the one with random permutations. Current state-of-art performance regarding this task is obtained by using standard recursive network as described in [14].\nTable 5 illustrates the performance of different models. Entity-grid model [35] generates state-of-art performance among all non-neural network models. As can be seen, neural models perform pretty well in this task when compared against existing feature based algorithm. From the reported results,\nbetter sentence representations are obtained by incorporating \u201cweighted tuning\u201d properties, pushing the state of art of this task to the accuracy of 0.936."}, {"heading": "5 Conclusion", "text": "In this paper, we propose two revised versions of neural models, WNN and BENN for obtaining higher level feature representations for a sequence of tokens. The proposed framework automatically incorporates the concept of \u201cweight tuning\u201d of SVM into the DL architectures which lead to better higher-level representations and generate significantly better performance against standard neural models in multiple tasks. While it still underperforms bag-of-word models in some cases, and the newly proposed paragraph vector approach, it provides as an alternative to existing recursive neural models for representation learning.\nTo note, while we limit our attentions to recursive models in this work, the idea of weight tuning in WNN and BENN, that associates nodes in neural models with additional weighed variables is a general one and can be extended to many other deep learning models with minor adjustment."}], "references": [{"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Bidirectional recursive neural networks for token-level labeling with structure", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "arXiv preprint arXiv:1312.0493,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Rnnlmrecurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "J Cernocky"], "venue": "In Proc. of the 2011 ASRU Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Subsequence kernels for relation extraction", "author": ["Raymond J Mooney", "Razvan C Bunescu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy"], "venue": "In Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Empirical Methods in Natural Language Processing, A model of coherence based on distributed sentence", "author": ["Jiwei Li", "Eduard Hovy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "A convolutional neural network for modelling sentences", "author": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network", "author": ["Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1406.3830,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Translation modeling with bidirectional recurrent neural networks", "author": ["Martin Sundermeyer", "Tamer Alkhouli", "Joern Wuebker", "Hermann Ney"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Regina Barzilay", "Lillian Lee"], "venue": "In HLT-NAACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "A coherence model based on syntactic patterns", "author": ["Annie Louis", "Ani Nenkova"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recursive neural network models [1] constitute one type of neural structure for obtaining higherlevel representations beyond word-level such as phrases or sentences.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": ", [2, 3]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": ", [2, 3]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 3, "context": "Table 1: A brief comparison between SVM and standard neural network models for sentence-level sentiment classification using date set from [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "Neural network models are trained with L2 regularization, using AdaGrad [5] with minibatches (for details about implementations of recursive networks, please see Section 2).", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Word embeddings are borrowed from Glove [6] with dimensionality of 300, which generates better performance than word2vect, SENNA [7] and RNNLM [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "Word embeddings are borrowed from Glove [6] with dimensionality of 300, which generates better performance than word2vect, SENNA [7] and RNNLM [8].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Such problem, which could also be referred to as gradient vanishing [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "When we compare neural models with SVM, one notable weakness of big-of-word based SVM is its inability of considering how words are combined to form meanings (or order information in other words) [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "\u2019s dataset [4], and as can be seen, in this specific task, standard neural network models underperform SVM2.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": ", word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data4.", "startOffset": 18, "endOffset": 25}, {"referenceID": 9, "context": ", word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data4.", "startOffset": 18, "endOffset": 25}, {"referenceID": 5, "context": ", word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data4.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "Recent proposed approaches include, for example, MatrixVector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "Recent proposed approaches include, for example, MatrixVector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which", "startOffset": 136, "endOffset": 139}, {"referenceID": 11, "context": "Recent proposed approaches include, for example, MatrixVector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which", "startOffset": 232, "endOffset": 236}, {"referenceID": 1, "context": "\u2019s work [2] which obtains state-of-art performance for sentiment classification, as here labels at sentence-level constitute only sort of supervision for both SVM and neural network models (for details, see footnote 7).", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "4There are cases, for example, [2], where task-specific word embeddings are learned.", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "Distributed representations, calculated based on neural frameworks, are extended beyond tokenlevel, to represent N-grams [15], phrases [2], sentences (e.", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Distributed representations, calculated based on neural frameworks, are extended beyond tokenlevel, to represent N-grams [15], phrases [2], sentences (e.", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 2, "endOffset": 9}, {"referenceID": 15, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 22, "endOffset": 30}, {"referenceID": 11, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 22, "endOffset": 30}, {"referenceID": 16, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Recursive and recurrent [20, 21] models constitute two types of commonly used frameworks for sentence-level embedding acquisition.", "startOffset": 24, "endOffset": 32}, {"referenceID": 19, "context": "Recursive and recurrent [20, 21] models constitute two types of commonly used frameworks for sentence-level embedding acquisition.", "startOffset": 24, "endOffset": 32}, {"referenceID": 2, "context": ", [3, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": ", [3, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 20, "context": "Other recently proposed approaches included sentence compositional approach proposed in [22], or paragraph/sentence vector [18] where representations are optimized through predicting words within the sentence.", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "Other recently proposed approaches included sentence compositional approach proposed in [22], or paragraph/sentence vector [18] where representations are optimized through predicting words within the sentence.", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 22, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 23, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 24, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 25, "context": "Both of the proposed architectures are in this work inspired by the long short-term memory (LSTM) model, first proposed by Hochreiter and Schmidhuber back in 1990s [27, 28] to process time sequence data where there are very long time lags of unknown size between important events5.", "startOffset": 164, "endOffset": 172}, {"referenceID": 26, "context": "Both of the proposed architectures are in this work inspired by the long short-term memory (LSTM) model, first proposed by Hochreiter and Schmidhuber back in 1990s [27, 28] to process time sequence data where there are very long time lags of unknown size between important events5.", "startOffset": 164, "endOffset": 172}, {"referenceID": 26, "context": "LSTM associates each time with a series of \u201cgates\u201d to determine whether the information from early timesequence should be forgotten [28] and when current information should be allowed to flow into or out of the memory.", "startOffset": 132, "endOffset": 136}, {"referenceID": 27, "context": "LSTM could partially address gradient vanishing problem in recurrent neural models and have been widely used in machine translation [29, 30]", "startOffset": 132, "endOffset": 140}, {"referenceID": 28, "context": "LSTM could partially address gradient vanishing problem in recurrent neural models and have been widely used in machine translation [29, 30]", "startOffset": 132, "endOffset": 140}, {"referenceID": 29, "context": "Parse tree for each sentence is obtained from Stanford Parser [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Such implementation can be viewed as using a three-layer neural model with D latent neurons for an output projected to a [0,1] space.", "startOffset": 121, "endOffset": 126}, {"referenceID": 30, "context": "As all components in Equ 9 are continuous, the gradient can be efficiently obtained from standard backpropagation [32, 33].", "startOffset": 114, "endOffset": 122}, {"referenceID": 31, "context": "As all components in Equ 9 are continuous, the gradient can be efficiently obtained from standard backpropagation [32, 33].", "startOffset": 114, "endOffset": 122}, {"referenceID": 0, "context": "LC is a scalar fixed to the range of [0,1], indicating the possibility that current node should pass information to its ancestors.", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "LC is obtained in the similar ways as in WNN by using a convolution to project the current representation hC to a scalar lying within [0,1].", "startOffset": 134, "endOffset": 139}, {"referenceID": 3, "context": "Sentence-level Labels We first perform experiments on dateset from [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "In this setting, binary labels at the top of sentence constitute the only resource of supervision (to note, it is different from setting in [2]).", "startOffset": 140, "endOffset": 143}, {"referenceID": 10, "context": "\u2022 MV-RNN (Matrix-Vector RNN): which was proposed in [12] which represents every node in a parse tree as both a vector and a matrix.", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "\u2022 RNTN (Recursive Neural Tensor Network): proposed in [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 32, "context": ", [34]) can generate better performance what the proposed models achieve.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "Socher et al\u2019s setting We now consider Socher et al\u2019s setting [2] for sentiment analysis, where contains gold-standard labels at every phrase node in the parse tree.", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "We follow the experimental protocols described in [2] (word embeddings are treated as parameters to learn rather than fixed to externally borrowed embeddings).", "startOffset": 50, "endOffset": 53}, {"referenceID": 16, "context": "In addition to varieties of neural models mentioned in Socher et al\u2019s work, we also report the performance of recently proposed paragraph vector model [18], which first obtains sentence embeddings in an unsupervised manner by predicting words within the context and then feeds the pre-obtained embeddings into a logistic regression model.", "startOffset": 151, "endOffset": 155}, {"referenceID": 32, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "We follow the experimental protocols described in [34].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "Baseline performances are reported from [2, 18].", "startOffset": 40, "endOffset": 47}, {"referenceID": 16, "context": "Baseline performances are reported from [2, 18].", "startOffset": 40, "endOffset": 47}, {"referenceID": 32, "context": "The results are reported from [34]", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Although WNN and BENN still underperform current state-of-art model Paragraph Vector [18], they produces better performance than bag-of-word models.", "startOffset": 85, "endOffset": 89}, {"referenceID": 33, "context": "We use corpora widely employed for coherence prediction [35, 36].", "startOffset": 56, "endOffset": 64}, {"referenceID": 34, "context": "We use corpora widely employed for coherence prediction [35, 36].", "startOffset": 56, "endOffset": 64}, {"referenceID": 33, "context": "We follow the protocols introduced in [35, 37, 14] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent.", "startOffset": 38, "endOffset": 50}, {"referenceID": 35, "context": "We follow the protocols introduced in [35, 37, 14] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent.", "startOffset": 38, "endOffset": 50}, {"referenceID": 12, "context": "We follow the protocols introduced in [35, 37, 14] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent.", "startOffset": 38, "endOffset": 50}, {"referenceID": 12, "context": "Current state-of-art performance regarding this task is obtained by using standard recursive network as described in [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 33, "context": "Entity-grid model [35] generates state-of-art performance among all non-neural network models.", "startOffset": 18, "endOffset": 22}, {"referenceID": 33, "context": "Reported baseline results are reprinted from [35].", "startOffset": 45, "endOffset": 49}], "year": 2014, "abstractText": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \u201cweight tuning\u201d for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.", "creator": "LaTeX with hyperref package"}}}