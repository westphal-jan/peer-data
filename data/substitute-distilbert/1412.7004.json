{"id": "1412.7004", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison", "abstract": "we investigate the problem of inducing word embeddings that are tailored alongside a particular communication relation. our learning algorithm takes an existing lexical vector space and compresses it such that subsequent resulting word sets generate good designs for a target bilexical relation. in physics we show that task - specific embeddings can benefit both production quality and efficiency in lexical prediction tasks.", "histories": [["v1", "Mon, 22 Dec 2014 14:49:19 GMT  (900kb,D)", "https://arxiv.org/abs/1412.7004v1", "Under review as a workshop contribution at ICLR 2015"], ["v2", "Fri, 10 Apr 2015 17:33:57 GMT  (901kb,D)", "http://arxiv.org/abs/1412.7004v2", "Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "Under review as a workshop contribution at ICLR 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pranava swaroop madhyastha", "xavier carreras", "ariadna quattoni"], "accepted": true, "id": "1412.7004"}, "pdf": {"name": "1412.7004.pdf", "metadata": {"source": "CRF", "title": "TAILORING WORD EMBEDDINGS FOR BILEXICAL PREDICTIONS: AN EXPERIMENTAL COMPARISON", "authors": ["Pranava Swaroop Madhyastha"], "emails": ["pranava@cs.upc.edu", "xavier.carreras@xrce.xerox.com", "ariadna.quattoni@xrce.xerox.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "There has been a large body of work that focuses on learning word representations, either in the form of word clusters (Brown et al., 1992) or vectors (Sahlgren, 2006; Turney & Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Baroni et al., 2014; Bansal et al., 2014) and these have proven useful in many NLP applications (Koo et al., 2008; Turian et al., 2010).\nAn ideal lexical representation should compress the space of lexical words while retaining the essential properties of words in order to make predictions that correctly generalize across words. The typical approach is to first induce a lexical representation in a task-agnostic setting and then use it in different tasks as features. A different approach is to learn a lexical representation tailored for a certain task. In this work we explore the second approach, and employ the formulation by Madhyastha et al. (2014) to induce task-specific word embeddings. This method departs from a given lexical vector space, and compresses it such that the resulting word embeddings are good predictors for a given lexical relation.\nSpecifically we learn functions that compute compatibility scores between pairs of lexical items under some linguistic relation. In our work, we refer to these functions as bilexical operators. As an instance of this problem, consider learning a model that predicts the probability that an adjective modifies a noun in a sentence. In this case, we would like the bilexical operator to capture the fact that some adjectives are more compatible with some nouns than others.\nGiven the complexity of lexical relations, one expects that the properties of words that are relevant for some relation are different for another relation. This might affect the quality of an embedding, both in terms of its predictive power and the compression it obtains. If we employ a task-agnostic low-dimensional embedding, will it retain all important lexical properties for any relation? And, given a fixed relation, can we further compress an existing word representation? In this work we present experiments along these lines that confirm that task-specific embeddings can benefit both the quality and the efficiency of lexicalized predictive models.\nar X\niv :1\n41 2.\n70 04\nv2 [\ncs .C\nL ]"}, {"heading": "2 FORMULATION", "text": "Let V be a vocabulary, and let x \u2208 V denote a word. We are interested in modeling a target bilexical relation, that is, a relation between pairs of words without context. For example, in a noun-adjective relation we model what nouns can be assigned to what adjectives. We will denote as Q \u2286 V the set of query words, or words that appear in the left side of the bilexical relation. And we will use C \u2286 V to denote candidate words, appearing in the right side of the relation.\nIn this paper we experiment with the log-linear models by Madhyastha et al. (2014) that given a query word q compute a conditional distribution over candidate words c. The models take the following form:\nPr(c | q;W ) = exp{\u03c6(q) >W\u03c6(c)}\u2211\nc\u2032 exp{\u03c6(q)>W\u03c6(c\u2032)} (1)\nwhere \u03c6 : V \u2192 Rn is a distributional representation of words, and W \u2208 Rn\u00d7n is a bilinear form. The learning problem is to obtain \u03c6 and W from data, and we approach it in a semi-supervised fashion. There exist many approaches to learn \u03c6 from unlabeled data, and in this paper we experiment with two approaches: (a) a simple distributional approach where we represent words with a bag-ofwords of contextual words; and (b) the skip-gram model by Mikolov et al. (2013). To learnW we assume access to labeled data in the form pairs of compatible examples, i.e. D = {(q, c)1, . . . , (q, c)l}, where q \u2208 Q and c \u2208 C. The goal is to be able to predict query-candidate pairs that are unseen during training. Recall that we model relations between words without context. Thus the lexical representation \u03c6 is essential to generalize to pairs involving unseen words.\nWith \u03c6 fixed, we learn W by minimizing the negative log-likelihood of the labeled data using a regularized objective, L(W ) = \u2212 \u2211 (q,c)\u2208D log Pr(c | q;W )+\u03c4\u03c1(W ), where \u03c1(W ) is a regularization penalty and \u03c4 is a constant controlling the trade-off.\nWe are interested in regularizers that induce low-rank parameters W , since they lead to task-specific embeddings. Assume that W has rank k, such that W = UV > with U, V \u2208 Rn\u00d7k. If we consider the product \u03c6(q)>UV >\u03c6(c), we can now interpret \u03c6>U as a k-dimensional embedding of query words, and \u03c6(c)>V as a k-dimensional embedding of candidate words. Thus, if we obtain a lowrank W that is highly predictive, we can interpret U and V as task-specific compressions of the original embedding \u03c6 tailored for the target bilexical relation, from n to k dimensions.\nSince minimizing the rank of a matrix is hard, we employ a convex relaxation based on the nuclear norm of the matrix `? (that is, the `1 norm of the singular values, see Srebro et al. (2005)). In our experiments we compare the low-rank approach to `1 and `2 regularization penalties, which are common in linear prediction tasks. For all settings we use the forward-backward splitting (FOBOS) optimization algorithm by Duchi & Singer (2009).\nWe note that if we set W to be the identity matrix our model scores are inner products between the query-candidate embeddings, a common approach to evaluate semantic similarity in unsupervised distributional approaches. In general, we can compute a low-dimensional projection of \u03c6 down to k dimensions, using SVD, and perform the inner product in the projected space. We refer to this as the unsupervised approach, since the projected embeddings do not use the labeled dataset specifying the target relation."}, {"heading": "3 EXPERIMENTS WITH SYNTACTIC RELATIONS", "text": "We conducted a set of experiments to test the performance of the learning algorithm with respect to the initial lexical representation \u03c6, for different configurations of the representation and the learner. We experiment with six bilexical syntactic relations using the Penn Treebank corpus (Marcus et al., 1993), following the experimental setting by Madhyastha et al. (2014). For a relation between queries and candidate words, such as noun-adjective, we partition the query words into train, development and test queries, thus test pairs are always unseen pairs.\nTo report performance, we measure pairwise accuracy with respect to the efficiency of the model in terms of number of active parameters. To measure the efficiency of a model we consider the number of double operations that are needed to compute, given a query word, the scores for all candidates in the vocabulary. See (Madhyastha et al., 2014) for details.\nWe experiment with two types of initial representations \u03c6. The first is a simple high-dimensional distributional representation based on contextual bag-of-words (BoW): each word is represented by the bag of words that appear in contextual windows. In our experiments these were sparse 2,000- dimensional vectors. The second representation are the low-dimensional skip-gram embeddings (SKG) by Mikolov et al. (2013), where we used 300 dimensions. In both cases we induced such representations using the BLIPP corpus (Charniak et al., 2000) and using a context window of size 10 for both. Thus the main difference is that the bag-of-words is an uncompressed representation, while the skip-gram embeddings are a neural-net-style compression of the same contextual windows.\nAs for the bilexical model, we test it under three regularization schemes, namely `2, `1, and `?. For the first two, the efficiency of computing predictions is a function of the non-zero entries inW , while for the latter it is the rank k of W , which defines the dimension of the task-specific embeddings. We also test a baseline unsupervised approach (UNS)."}, {"heading": "4 RESULTS AND DISCUSSION", "text": "Figure 1 shows the performance of models for noun-adjective, verb-object and verb-subject relations (in both directions). In line with the results by Madhyastha et al. (2014) we observe that the supervised approach in all cases outperforms the unsupervised case, and that the nuclear norm scheme provides the best performance in terms of accuracy and speed: other regularizers can obtain similar accuracies, but low-rank constraints during learning favor very-low dimensional embeddings that are highly predictive.\nIn terms of starting with bag-of-words vectors or skip-gram embeddings, in three relations the former is clearly better, while in the other three relations the latter is clearly better. We conclude that taskagnostic embeddings do identify useful relevant properties of words, but at the same time not all necessary properties are retained. In all cases, the nuclear norm regularizer successfully compresses the initial representation, even for the embeddings which are already low-dimensional.\nTable 1 presents the best result for each relation, initial representation and regularization scheme. Plus, for the `? regularizer we present results at three different ranks, namely 5, 10 or the rank that obtains the best result for each relation. These highly compressed embeddings perform nearly as good as the best performing model for each relation.\nTable 2 shows a set of query nouns, and two sets of neighbor query nouns, using the embeddings for two different relations to compute the two sets. We can see that, by changing the target relation, the set of close words changes. This suggests that words have a wide range of different behaviors, and different relations might exploit lexical properties that are specific to the relation."}, {"heading": "5 CONCLUSION", "text": "We have presented a set of experiments where we compute word embeddings specific to target linguistic relations. We observe that low-rank penalties favor embeddings that are good both in terms of predictive accuracy and efficiency. For example, in certain cases, models using very lowdimensional embeddings perform nearly as good as the best models.\nIn certain tasks, we have shown that we can refine low-dimensional skip-gram embeddings, making them more compressed while retaining their predictive properties. In other tasks, we have shown that our method can improve over skip-gram models when starting from uncompressed distributional representations. This suggests that skip-gram embeddings do not retain all the necessary information of the original words. This motivates future research that aims at general-purpose embeddings that do retain all necessary properties, and can be further compressed in light of specific lexical relations."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank the reviewers for their helpful comments. This work has been partially funded by the Spanish Government through the SKATER project (TIN2012-38584-C06-01)."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In Proceedings of ACL,", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germ\u00e1n"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Class-based n-gram models of natural language", "author": ["Brown", "Peter F", "deSouza", "Peter V", "Mercer", "Robert L", "Pietra", "Vincent J. Della", "Lai", "Jenifer C"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "BLLIP 1987\u201389 WSJ Corpus Release 1, LDC No. LDC2000T43", "author": ["Charniak", "Eugene", "Blaheta", "Don", "Ge", "Niyu", "Hall", "Keith", "Johnson", "Mark"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Charniak et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2000}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["Duchi", "John", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2009}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo", "Terry", "Carreras", "Xavier", "Collins", "Michael"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Learning task-specific bilexical embeddings", "author": ["Madhyastha", "Swaroop Pranava", "Carreras", "Xavier", "Quattoni", "Ariadna"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Madhyastha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Madhyastha et al\\.", "year": 2014}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Marcus", "Mitchell P", "Santorini", "Beatrice", "Marcinkiewicz", "Mary A"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces", "author": ["Sahlgren", "Magnus"], "venue": "PhD thesis, Stockholm University,", "citeRegEx": "Sahlgren and Magnus.,? \\Q2006\\E", "shortCiteRegEx": "Sahlgren and Magnus.", "year": 2006}, {"title": "Maximum-margin matrix factorization", "author": ["Srebro", "Nathan", "Rennie", "Jason D. M", "Jaakola", "Tommi S"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Peter D", "Pantel", "Patrick"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "There has been a large body of work that focuses on learning word representations, either in the form of word clusters (Brown et al., 1992) or vectors (Sahlgren, 2006; Turney & Pantel, 2010; Mikolov et al.", "startOffset": 119, "endOffset": 139}, {"referenceID": 8, "context": ", 1992) or vectors (Sahlgren, 2006; Turney & Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Baroni et al., 2014; Bansal et al., 2014) and these have proven useful in many NLP applications (Koo et al.", "startOffset": 19, "endOffset": 147}, {"referenceID": 9, "context": ", 1992) or vectors (Sahlgren, 2006; Turney & Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Baroni et al., 2014; Bansal et al., 2014) and these have proven useful in many NLP applications (Koo et al.", "startOffset": 19, "endOffset": 147}, {"referenceID": 1, "context": ", 1992) or vectors (Sahlgren, 2006; Turney & Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Baroni et al., 2014; Bansal et al., 2014) and these have proven useful in many NLP applications (Koo et al.", "startOffset": 19, "endOffset": 147}, {"referenceID": 0, "context": ", 1992) or vectors (Sahlgren, 2006; Turney & Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Baroni et al., 2014; Bansal et al., 2014) and these have proven useful in many NLP applications (Koo et al.", "startOffset": 19, "endOffset": 147}, {"referenceID": 5, "context": ", 2014) and these have proven useful in many NLP applications (Koo et al., 2008; Turian et al., 2010).", "startOffset": 62, "endOffset": 101}, {"referenceID": 12, "context": ", 2014) and these have proven useful in many NLP applications (Koo et al., 2008; Turian et al., 2010).", "startOffset": 62, "endOffset": 101}, {"referenceID": 0, "context": ", 2014; Bansal et al., 2014) and these have proven useful in many NLP applications (Koo et al., 2008; Turian et al., 2010). An ideal lexical representation should compress the space of lexical words while retaining the essential properties of words in order to make predictions that correctly generalize across words. The typical approach is to first induce a lexical representation in a task-agnostic setting and then use it in different tasks as features. A different approach is to learn a lexical representation tailored for a certain task. In this work we explore the second approach, and employ the formulation by Madhyastha et al. (2014) to induce task-specific word embeddings.", "startOffset": 8, "endOffset": 645}, {"referenceID": 6, "context": "In this paper we experiment with the log-linear models by Madhyastha et al. (2014) that given a query word q compute a conditional distribution over candidate words c.", "startOffset": 58, "endOffset": 83}, {"referenceID": 6, "context": "In this paper we experiment with the log-linear models by Madhyastha et al. (2014) that given a query word q compute a conditional distribution over candidate words c. The models take the following form: Pr(c | q;W ) = exp{\u03c6(q) >W\u03c6(c)} \u2211 c\u2032 exp{\u03c6(q)>W\u03c6(c\u2032)} (1) where \u03c6 : V \u2192 R is a distributional representation of words, and W \u2208 Rn\u00d7n is a bilinear form. The learning problem is to obtain \u03c6 and W from data, and we approach it in a semi-supervised fashion. There exist many approaches to learn \u03c6 from unlabeled data, and in this paper we experiment with two approaches: (a) a simple distributional approach where we represent words with a bag-ofwords of contextual words; and (b) the skip-gram model by Mikolov et al. (2013). To learnW we assume access to labeled data in the form pairs of compatible examples, i.", "startOffset": 58, "endOffset": 726}, {"referenceID": 6, "context": "In this paper we experiment with the log-linear models by Madhyastha et al. (2014) that given a query word q compute a conditional distribution over candidate words c. The models take the following form: Pr(c | q;W ) = exp{\u03c6(q) >W\u03c6(c)} \u2211 c\u2032 exp{\u03c6(q)>W\u03c6(c\u2032)} (1) where \u03c6 : V \u2192 R is a distributional representation of words, and W \u2208 Rn\u00d7n is a bilinear form. The learning problem is to obtain \u03c6 and W from data, and we approach it in a semi-supervised fashion. There exist many approaches to learn \u03c6 from unlabeled data, and in this paper we experiment with two approaches: (a) a simple distributional approach where we represent words with a bag-ofwords of contextual words; and (b) the skip-gram model by Mikolov et al. (2013). To learnW we assume access to labeled data in the form pairs of compatible examples, i.e. D = {(q, c), . . . , (q, c)}, where q \u2208 Q and c \u2208 C. The goal is to be able to predict query-candidate pairs that are unseen during training. Recall that we model relations between words without context. Thus the lexical representation \u03c6 is essential to generalize to pairs involving unseen words. With \u03c6 fixed, we learn W by minimizing the negative log-likelihood of the labeled data using a regularized objective, L(W ) = \u2212 \u2211 (q,c)\u2208D log Pr(c | q;W )+\u03c4\u03c1(W ), where \u03c1(W ) is a regularization penalty and \u03c4 is a constant controlling the trade-off. We are interested in regularizers that induce low-rank parameters W , since they lead to task-specific embeddings. Assume that W has rank k, such that W = UV > with U, V \u2208 Rn\u00d7k. If we consider the product \u03c6(q)>UV >\u03c6(c), we can now interpret \u03c6>U as a k-dimensional embedding of query words, and \u03c6(c)>V as a k-dimensional embedding of candidate words. Thus, if we obtain a lowrank W that is highly predictive, we can interpret U and V as task-specific compressions of the original embedding \u03c6 tailored for the target bilexical relation, from n to k dimensions. Since minimizing the rank of a matrix is hard, we employ a convex relaxation based on the nuclear norm of the matrix `? (that is, the `1 norm of the singular values, see Srebro et al. (2005)).", "startOffset": 58, "endOffset": 2115}, {"referenceID": 6, "context": "In this paper we experiment with the log-linear models by Madhyastha et al. (2014) that given a query word q compute a conditional distribution over candidate words c. The models take the following form: Pr(c | q;W ) = exp{\u03c6(q) >W\u03c6(c)} \u2211 c\u2032 exp{\u03c6(q)>W\u03c6(c\u2032)} (1) where \u03c6 : V \u2192 R is a distributional representation of words, and W \u2208 Rn\u00d7n is a bilinear form. The learning problem is to obtain \u03c6 and W from data, and we approach it in a semi-supervised fashion. There exist many approaches to learn \u03c6 from unlabeled data, and in this paper we experiment with two approaches: (a) a simple distributional approach where we represent words with a bag-ofwords of contextual words; and (b) the skip-gram model by Mikolov et al. (2013). To learnW we assume access to labeled data in the form pairs of compatible examples, i.e. D = {(q, c), . . . , (q, c)}, where q \u2208 Q and c \u2208 C. The goal is to be able to predict query-candidate pairs that are unseen during training. Recall that we model relations between words without context. Thus the lexical representation \u03c6 is essential to generalize to pairs involving unseen words. With \u03c6 fixed, we learn W by minimizing the negative log-likelihood of the labeled data using a regularized objective, L(W ) = \u2212 \u2211 (q,c)\u2208D log Pr(c | q;W )+\u03c4\u03c1(W ), where \u03c1(W ) is a regularization penalty and \u03c4 is a constant controlling the trade-off. We are interested in regularizers that induce low-rank parameters W , since they lead to task-specific embeddings. Assume that W has rank k, such that W = UV > with U, V \u2208 Rn\u00d7k. If we consider the product \u03c6(q)>UV >\u03c6(c), we can now interpret \u03c6>U as a k-dimensional embedding of query words, and \u03c6(c)>V as a k-dimensional embedding of candidate words. Thus, if we obtain a lowrank W that is highly predictive, we can interpret U and V as task-specific compressions of the original embedding \u03c6 tailored for the target bilexical relation, from n to k dimensions. Since minimizing the rank of a matrix is hard, we employ a convex relaxation based on the nuclear norm of the matrix `? (that is, the `1 norm of the singular values, see Srebro et al. (2005)). In our experiments we compare the low-rank approach to `1 and `2 regularization penalties, which are common in linear prediction tasks. For all settings we use the forward-backward splitting (FOBOS) optimization algorithm by Duchi & Singer (2009). We note that if we set W to be the identity matrix our model scores are inner products between the query-candidate embeddings, a common approach to evaluate semantic similarity in unsupervised distributional approaches.", "startOffset": 58, "endOffset": 2364}, {"referenceID": 7, "context": "We experiment with six bilexical syntactic relations using the Penn Treebank corpus (Marcus et al., 1993), following the experimental setting by Madhyastha et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 6, "context": "See (Madhyastha et al., 2014) for details.", "startOffset": 4, "endOffset": 29}, {"referenceID": 6, "context": ", 1993), following the experimental setting by Madhyastha et al. (2014). For a relation between queries and candidate words, such as noun-adjective, we partition the query words into train, development and test queries, thus test pairs are always unseen pairs.", "startOffset": 47, "endOffset": 72}, {"referenceID": 3, "context": "In both cases we induced such representations using the BLIPP corpus (Charniak et al., 2000) and using a context window of size 10 for both.", "startOffset": 69, "endOffset": 92}, {"referenceID": 7, "context": "The second representation are the low-dimensional skip-gram embeddings (SKG) by Mikolov et al. (2013), where we used 300 dimensions.", "startOffset": 80, "endOffset": 102}, {"referenceID": 6, "context": "In line with the results by Madhyastha et al. (2014) we observe that the supervised approach in all cases outperforms the unsupervised case, and that the nuclear norm scheme provides the best performance in terms of accuracy and speed: other regularizers can obtain similar accuracies, but low-rank constraints during learning favor very-low dimensional embeddings that are highly predictive.", "startOffset": 28, "endOffset": 53}], "year": 2015, "abstractText": "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks.", "creator": "LaTeX with hyperref package"}}}