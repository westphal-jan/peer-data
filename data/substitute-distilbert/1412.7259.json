{"id": "1412.7259", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Unsupervised Feature Learning with C-SVDDNet", "abstract": "in older paper, we investigate that problem linking learning feature representation from unlabeled displays using a single - layer k - means network. a cube - means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers'great attention recently due to its simplicity, effectiveness, and scalability. however, one drawback of this feature understanding is that it tends to be unreliable when the training data contains noise. to address this equation, we consider a pattern based feature learning algorithm that describes the density and distribution of each cluster while k - means with an svdd ball for better robust feature representation. for this purpose, we present a new svdd algorithm called c - svdd that centers the neural ball beyond the mode of local density of each pixel, and later provide proof the objective - c - svdd can be solved very efficiently as a linear programming problem. additionally, previous single - layer networks favor a large : large centroids but a shorter pooling size, resulting in a representation that highlights the global aspects of little object. here we explore an alternative network mechanism with much preferred number of nodes but with much finer pooling size, hence emphasizing the local details of the object. the design eventually also extended with less receptive field scales and multiple pooling sizes. extensive experiments from several popular object recognition benchmarks, such as minst, norb, cifar - vu and stl - 10, observed that the overall c - svddnet technique yields comparable or better performance than that of the previous state of the art methods.", "histories": [["v1", "Tue, 23 Dec 2014 05:56:50 GMT  (1212kb,D)", "http://arxiv.org/abs/1412.7259v1", null], ["v2", "Mon, 12 Jan 2015 05:31:03 GMT  (1212kb,D)", "http://arxiv.org/abs/1412.7259v2", null], ["v3", "Fri, 29 May 2015 09:50:54 GMT  (2852kb,D)", "http://arxiv.org/abs/1412.7259v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["dong wang", "xiaoyang tan"], "accepted": false, "id": "1412.7259"}, "pdf": {"name": "1412.7259.pdf", "metadata": {"source": "CRF", "title": "C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning", "authors": ["Dong Wang", "Xiaoyang Tan"], "emails": ["(x.tan@nuaa.edu.cn)."], "sections": [{"heading": null, "text": "I. INTRODUCTION\nLearning good feature representation from unlabeled data is the key to make progress in recognition and classification tasks, and has attracted great attention and interest from both academia and industry recently. A representative method for this is the deep learning (DL) approach [1] with its goal to learn multiple layers of abstract representations from data. Among others, one typical DL method is the so called convolutional neural network (ConvNet), which consists of multiple trainable stages stacked on top of each other, followed by a supervised classifier [2] [3]. Many variations of ConvNet network have been proposed as well for different vision tasks [4] [5] [6] with great success.\nIn these methods layers of representation are usually obtained by greedily training one layer at a time on the lower level [5] [7] [3], using an unsupervised learning algorithm. Hence the performance of single-layer learning has a big effect on the final representation. Neural network based singlelayer methods, such as autoencoder [8] and RBM (Restricted Boltzmann Machine, [9]), are widely used for this but they\nDong Wang and Xiaoyang Tan are with the Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, P.R. China. Corresponding author: Xiaoyang Tan (x.tan@nuaa.edu.cn).\nusually have many parameters to adjust, which is very timeconsuming in practice.\nThat motivates more simple and more efficient methods for single-layer feature learning. Among others K-means clustering algorithm is a commonly used unsupervised learning method, which maps the input data into a feature representation simply by associating each data point to its nearest cluster center. There is only one parameter involved in the K-means based method, i.e., the number of clusters, hence the model is very easy to use in practice. Coates et al. [10] shows that the K-means based feature learning network is capable to achieve superior performance compared to sparse autoencoder, sparse RBM and GMM (Guassian Mixture Model). However, the K-means based feature representation may be too terse, and does not take the non-uniform distribution of cluster size into account - Intuitively, clusters containing more data are likely to be part of the features with higher influential power, compared to the smaller ones.\nIn this paper, we proposed a SVDD (Support Vector Data Description, [11], [12]) based method to address these issues. The key idea of our method is to use SVDD to measure the density of each cluster resulted from K-means clustering, based on which more robust feature representation is built. Actually the K-means algorithm lacks a robust definition of the size of its clusters, since the nearest center principle is not robust against the noise or outliers commonly encountered in real world applications. We advocate that the SVDD could be a good way to address this issue. Actually SVDD is a widely used tool to find a minimal closed spherical boundary to describe the data belonging to the target class and therefore, given a cluster of data, we are expecting SVDD to generate a ball containing the normal data except outliers. Performing this procedure on all the clusters of K-means, we will finally obtain K SVDD balls on which our representation can be built. In addition, to take the cluster size into account, we use the distance from the data to each ball\u2019s surface instead of the center as the feature.\nOne possible problem of this method, however, may come from the instability of SVDD\u2019s center, due to the fact that its position is mainly determined by the support vectors on the boundary and the noise in the data may deviate the center far from the mode (c.f., Fig. 5 (left)). Hence the resulting SVDD ball may not be consistent with the data\u2019s distribution when used for feature representation. To address this issue, we add a new constraint to the original SVDD objective function to make the model align better with the data. In addition, we show that our modified SVDD can be solved very efficiently as a linear programming problem, instead of as a quadratic\nar X\niv :1\n41 2.\n72 59\nv1 [\ncs .C\nV ]\n2 3\nD ec\n2 01\n4"}, {"heading": "2 TNN-SUBMISSION", "text": "one. Usually we need to compute hundreds of clusters, and a linear programming solution can thus save us large amounts of time. The proposed method is further extended by adopting a set of receptive fields with different sizes to capture multiscale information ranging from detailed edge-like features to part-level features.\nOne of the major conclusions of Coates et al.\u2019s series of controlled experiments on single layer unsupervised feature learning networks [10] is that compared to the choice of particular learning algorithm, the parameters that define the feature extraction pipeline, especially the number of features, have much more deep impact on the performance. Using a K-means network with 4000 features, for example, they are able to achieve surprisingly good performance on several benchmark datasets - even better than those with much deeper architectures (e.g., Convolutional Deep Belief Nets (CDBN) [13], Deep Boltzmann Machine [14] and Sparse Auto-encoder [10]). One major reason for this is that the existence of large number of basis vectors increases the chances that an input can be encoded properly by the network [10]. However, one drawback accompanying this large network is that a very crude pooling size has to be adopted (e.g., 46 \u00d7 46 over 92\u00d7 92 feature maps) to condense the resulting feature maps, otherwise the dimensionality of the final feature representation could be prohibitively high. For example, a 3\u00d73 pooling over 4000 feature maps with 92\u00d7 92 in size would lead to a total number of features over 3.8M.\nWhile the Coates et al.\u2019s large single-layer network can be thought of as highlighting the benefits of learning a representation that encodes global information of an object using large number of centroids, here we explore alternative architecture of single-layer networks with much smaller number of nodes and with much finer pooling size, hence emphasizing the local details of the object in the representation. To compensate for loss of information due to the use of a smaller number of basis vectors, it is crucial to encoding the pooling response effectively. Here we use a variant of SIFT-based encoder [15], which essentially projects the responses of a pooling operation into a low dimensional space while suppressing the noise and improving the invariant properties of the final feature representation. we further extend the network with multiple receptive field scales and multiple pooling sizes for better feature learning. The feasibility and effectiveness of the proposed C-SVDD-based small single-layer network (called C-SVDDNet) is verified extensively on several object recognition benchmarks with competitive performance. A preliminary version of this work appeared in [16].\nThe remaining parts of this paper are organized as follows: In Section II, preliminaries are provided regarding the Kmeans based feature representation, then we detail our improved feature learning network in Section III. In Section IV, we investigate the performance of our network empirically over one face recognition datasets and several popular object classification datasets. We conclude this paper in Section V."}, {"heading": "II. PRELIMINARIES", "text": ""}, {"heading": "A. Unsupervised Feature Learning", "text": "The goal of unsupervised feature learning is to automatically discover useful hidden patterns/features in large datasets without relying on a supervisory signal, and those learnt patterns can be utilized to create representations that facilitate subsequent supervised learning (e.g., object classification). A typical pipeline for unsupervised feature learning includes four steps. The first step is to train a set of local filters from the unlabeled training data. Then given an input image, we construct a set of feature maps for it using the learnt filters (e.g., through an image convolution) in the second step. After applying a pooling operation on them in the third step, these feature maps are finally combined into a vector as the feature representation for the input image.\nThe local filters are a set of latent patterns found in the data and for this there exist many basic algorithms (e.g., K-means, Gaussian Mixture Models (GMM), autoencoder [8], RBM [9], sparse coding) and their more sophisticated variants, such as Sparse Auto-encoder [10], Sparse RBM [10], Locally-connected Neural Pyramid (LCNP) [17], Conv. Sparse Factor Analysis (CSFA) [18], Hierarchical Matching Pursuit (HMP) [19], Hierarchical Sparse Coding (HSC) [20], Fisher vector [21], and so on. The local filters learnt by these algorithms essentially contain important prior knowledge about the distribution of the data, which plays a critical role in the subsequent feature encoding. Interestingly, Coates et al. [10] shows through a series of controlled experiments that compared to how to obtain these filters, how many of them (the number of features) and how to use them (feature encoding) has more fundamental impact on the final performance of a network. In this work, we focus on the single layer K-means network, due to its simplicity, effectiveness, scalability, and its potential to serve as a good prototype to study the feature learning approaches."}, {"heading": "B. K-means for Feature Mapping", "text": "K-means is a data clustering algorithm that divides data into a set of K clusters, with Euclidean distance as similarity measure. It aims to minimize the sum of distance between all data to their corresponding centers. Let X = {xi}, i = 1, ..., n be the set of n d-dimensional points, C = {Ck}, k = 1, ...,K be the set of K clusters, with ck the mean of cluster Ck. The K-means algorithm finds the clusters by solving the following objective function, J(C) = \u2211K k=1 \u2211 xi\u2208Ck \u2016xi \u2212 ck\u2016\n2. After learning the clusters, they would be used to produce a feature mapping. So if we have K clusters, the dimension of the resulting feature representation will be K. The simplest way for feature mapping is the so-called \u201chard coding\u201d method, i.e., setting the winner cluster center on while all the others off, as follows,\nfk(x) = { 1 if k = argminj\u2016cj \u2212 x\u201622 0 otherwise (1)\nThe resulting K-dimensional vector f can be interpreted as the MAP estimate of the input point x given the K-means model. However it is too sparse and is often not representative\nof the full posterior mass. A better summary is the following \u201ctriangle\u201d encoding [10]:\nfk(x) = max{0, \u00b5(z)\u2212 zk(x)} (2)\nwhere zk(x) = \u2016x\u2212ck\u20162, and \u00b5(z) is the mean of the elements of z. This activation function outputs 0 for the feature fk that has an above average distance to the centroid ck. This model leads to a less sparse representation (roughly half of the features could be set to be 0). Note that this \u201ctriangle\u201d encoding strategy essentially allows us to learn a distributed representation using the simple K-means method instead of more complicated network-based methods (e.g., autoencoder and RBM), hence saving much time in training. Coastes et al. [10] shows that this strategy actually leads to comparable performance to, if not better than, those based on network methods.\nHowever, this method does not take the characteristics of each cluster into consideration. Actually, the number of data point in each cluster is usually different, so is the distribution of data points in each cluster. We believe that these differences would make a difference in feature representation as well. Unfortunately the aforementioned K-means feature mapping scheme completely ignores these and only uses the position of center for feature encoding. As shown in Fig. 1, although the data point x has the same distance to the centers C1 and C2 of two clusters, it should be assigned a different score on C1 than on C2 since the former cluster C1 is much bigger than the latter. In practice such unequal clusters are not uncommon and the K-means method by itself can not reliably grasp the size of its clusters due to the existence of outliers. To this end, we propose an SVDD based method to describe the density and distribution of each cluster and use this for more robust feature representation."}, {"heading": "III. THE PROPOSED METHOD", "text": "In this section, after presenting the proposed architecture of single-layer network, we give the details of the proposed Centered-SVDD method for feature encoding, and compare it with the K-means \u201ctriangle\u201d encoding method. Then we describe our SIFT-based post-pooling layer and discuss how to extend the method to extract multi-scale information."}, {"heading": "A. The architecture of the Single-layer Network", "text": "A typical single-layer network contains several components: an input image is first mapped into a set of feature maps using filter banks (or dictionary), which are then subjected to a pooling/subsampling operation to condense the information contained in the feature maps. Finally, the pooled feature maps are concatenated to a feature vector, which serves as the representation for the subsequent classification/cluster tasks. There are several design options in this procedure, where the size of filter bank and that of the pooling grids are the major tradeoff one has to make. Fig.2 gives two typical schemes of network architectures, where scheme A uses a big filter bank but a crude pooling size, while scheme B has a smaller filter bank but a fine-grided pooling size.\nGenerally speaking, bigger filter banks help each sample find its nearby representative points more accurately but at the cost of yielding a high-dimensional representation, hence a crude pooling/subsampling is needed to reduce the dimensionality. Overall this type of architecture emphasizes more on the global aspects of the samples than on the local ones (e.g., local texture, local shape, etc.). Actually, Coates et al. show that this kind of network is able to yield state of the art results on several challenging datasets [10]. On the other hand, other works use smaller filter banks but highlight the importance of detailed local information in constructing the representation, usually based on some complicated feature encoding strategy, as done in PCANet [22] or Fisher Vector [23].\nIn this work, we follow the second design choice, based on the consideration that for the task of object classification, the learned representation should preserve enough local spatial information for the subsequent processing. Fig. 3 gives the architecture of our single layer network. Compared to [10], we use an improved feature encoding method named CSVDD (detailed in the next section) and adopt the architecture of relatively small dictionary. Different to [22] or [23], we learn filter banks for feature encoding but add a SIFT-based post-pooling processing procedure onto the network, which essentially projects the responses of a pooling operation into a more compact and robust representation space."}, {"heading": "4 TNN-SUBMISSION", "text": ""}, {"heading": "B. Using SVDD Ball to Cover Unequal Clusters", "text": "Assume that a dataset contains N data objects, {xi}, i = 1, ..., n and a ball is described by its center a and the radius R. The goal of SVDD (Support Vector Data Description, [11]) is to find a closed spherical boundary around the given data points. In order to avoid the influence of outliers, SVDD actually faces the tradeoff between two conflicting goals, i.e., minimizing the radius while covering as many data points as possible. This can be formulated as the following objective,\nmina,R,\u03bei R 2 + \u03bb N\u2211 i=1 \u03bei s.t. \u2016xi \u2212 a\u20162 \u2264 R2 + \u03bei \u03bei \u2265 0,\n(3)\nwhere the slack variable \u03be represents the penalty related with the deviation of the i-th training data point outside the ball, and \u03bb is a user defined parameter controlling the degree of regularization imposed on the objective. With the KKT conditions, we have a = \u2211N i=1 xi, i.e., the center a of the ball is a linear combination of the data xi. The dual function of Eq.( 3) is\nmax \u2211 i \u03b1i\u3008xi, xi\u3009 \u2212 \u2211 i \u2211 j \u03b1i\u03b1j\u3008xi, xj\u3009\ns.t. \u2211 i \u03b1i = 1 , \u03b1i \u2208 [0, \u03bb] , i = 1, ..., N, (4)\nwhere \u03b1i and \u03b1j are Lagrangian multipliers. By solving the quadratic programming problem we can get the center a and the radius R.\nThe SVDD method can be understood as a type of one-class SVM and its boundary is solely determined by support vectors points. SVDD allows us to summarize a group of data points in a nice and robust way. Hence it is natural to use SVDD ball to model each cluster from K-means, thereby combining the strength of both models. In particular, for a given data point we first compute its distance hk to the surface of each SVDD ball Ck, and then use the following modified \u201ctriangle\u201d encoding method for feature representation (c.f., E.q.( 2)),\nfk(x) = max{0, g(h)\u2212 hk(x)}, (5)\nwhere hk(x) = ||x\u2212Rk||2 is the distance from the point x to the surface of the k-th SVDD ball, while g(h) is the average of the values hk.\nShown in Fig. 4 for a data point x, Ci, i = 1, 2 respectively are the centroids of two SVDD balls with Ri, i = 1, 2 being the radius. Since the distances from x to C1 and C2 are equal, x will be assigned the same scores on the two ball with the Kmeans scheme (c.f., E.q.( 2)). However, if we take the density and size of the clusters into accounts, the score from C2 should be higher in our method."}, {"heading": "C. The C-SVDD Model", "text": "Although SVDD ball provides a robust way to describe the cluster of data, one unwelcome property of the ball is that it may not align well with the distribution of data points in that cluster. As illustrated in Fig. 5 (left), although the SVDD ball\n5 covers the cluster C1 well, its center is biased to the region with low density. This should be avoided since it actually gives suboptimal estimates on the distribution of the cluster of data.\nTo address this issue, inspired by the observation that the centers of K-means are always located at the corresponding mode of their local density, we propose to shift the SVDD ball to the centroid of the data such that it may fit better with the distribution of the data in a cluster. Our new objective function is then formulated as\nminR,\u03bei R 2 + \u03bb N\u2211 i=1 \u03bei s.t. \u2016xi \u2212 a\u20162 \u2264 R2 + \u03bei\na = 1\nN N\u2211 i=1 xi\n\u03bei \u2265 0,\n(6)\nand its Lagrange function is as follows,\nL(R, \u03be, \u03b1, \u03b2) = R2 + \u03bb N\u2211 i=1 \u03bei + N\u2211 i=1 \u03b1i{\u2016xi \u2212 a\u20162 \u2212R2 \u2212 \u03bei}\n\u2212 N\u2211 i=1 \u03b2i\u03bei,\n(7)\nwhere \u03b1i \u2265 0 and \u03b2i \u2265 0 are the corresponding Lagrange multipliers. According to KKT Conditions, we have,\n\u2202L \u2202R = 2R\u2212 2R N\u2211 i=1 \u03b1i = 0 , N\u2211 i=1 \u03b1i = 1 (8)\n\u2202L \u2202\u03bei = \u03bb\u2212 \u03b1i \u2212 \u03b2i = 0 (9)\nTaking Eq.(8) and Eq.(9) into the Lagrange function (7) we get that\nL(R, \u03be, \u03b1, \u03b2) = N\u2211 i=1 \u03b1i{\u2016xi \u2212 a\u20162}.\nRecalling that a = 1N \u2211N i=1 xi, one has the following dual function,\nmax \u2211 i \u03b1i\u3008xi, xi\u3009 \u2212 2 N \u2211 i \u2211 j \u03b1i\u3008xi, xj\u3009\ns.t. \u2211 i \u03b1i = 1 , \u03b1i \u2208 [0, \u03bb] , i = 1, ..., N. (10)\nThis can be reformulated as\nmin 2\nN \u03b1THe\u2212 \u03b1TF\ns.t. \u03b1T e = 1 , \u03b1i \u2208 [0, \u03bb] , i = 1, ..., N, (11)\nwhere H = (\u3008xi, xj\u3009))N\u00d7N , F = (\u3008xi, xi\u3009)N\u00d71 , e = (1, 1, ..., 1)T . This objective function is linear to \u03b1, and thus can be solved efficiently with a linear programming algorithm.\nSince the model is centered towards the mode of the distribution of the data points in a cluster, we named our method\nas C-SVDD (centered-SVDD). Fig. 5 shows the difference between SVDD and C-SVDD, where the left is from SVDD and the right from C-SVDD. We can see that our new model aligns better with the density of the data points, as expected. It is also worth mentioning that the normalization parameter \u03bb plays an important role in our model - a larger \u03bb value would allow more noise to enter the ball, while \u03bb = 0, the C-SVDD model actually reduces to the naive single-cluster K-means. More discussions on setting this value empirically will be given in Section IV.\nAfter the model is trained, we use the modified \u201ctriangle\u201d encoding (E.q. 5) for feature encoding, with almost the same computational complexity with its K-means counterpart."}, {"heading": "D. K-means Encoding vs. C-SVDD Encoding", "text": "To this end, it will be useful to take a brief discussion on the difference of two kinds of feature maps, i.e., K-meansbased \u201ctriangle\u201d encoding (E.q. 2) and our C-SVDD-based one 1. For this a pilot experiment is conducted. Particularly, we learn a very small dictionary containing only five atoms using five face images, by clustering ZCA-whitened patches randomly sampled from the faces, and then take these for feature encoding. Fig.6 illustrates the face images used for dictionary learning (top) and the five learnt atoms (leftmost). The feature maps of face images encoded by the K-means encoding method and those by the C-SVDD encoding method are respectively shown in Fig.6 (a) and Fig.6 (b), where each row are corresponding to one dictionary atom next to it and each column corresponding to one face.\nBy comparing the feature maps shown in Fig.6 (a) and Fig.6 (b), one can see that the C-SVDD-based ones contain more detailed information than the K-means feature maps for the first three atoms, while the responses of the last two atoms are largely suppressed by our method (c.f., last two rows of Fig.6 (b)). To further understand this phenomenon, we plot the entropy of each atom (by treating them as a small image patch) in Fig.7 (c). The figure shows that the entropy of the last two atoms is much smaller than that of the first three ones, which indicates that the local appearance patterns captured by these last two atoms are much simpler than those by the first three. Hence these two atoms will tend to be widely used by many faces, resulting in reduced discriminative capability in distinguishing different subjects. In this sense, it will be useful to suppress their responses (c.f., the last two rows of Fig.6 (b)).\nIt is also useful to inspect the distribution of local facial patches attracted by these atoms. Fig.7 (a) gives the results. It can be seen that this distribution is not uniform and the number of local patches attracted by the fourth atom is significantly larger than those by other atoms. As a result, for K-means encoding method, the feature maps yielded by this atom show much more rich details than others (see the fourth row of Fig.6 (a)), potentially indicating that it could play more important roles than others in the subsequent classification task. However, as explained above, since this atom actually contain much less information than the first three atoms (low\n1Hereinafter we will call them respectively \u201cK-means encoding\u201d and \u201cCSVDD encoding\u201d for short without confusion."}, {"heading": "6 TNN-SUBMISSION", "text": "entropy and being a \u201ccommon word\u201d), it is really not good to over-emphasize its importance in feature encoding.\nThis drawback of K-means feature mapping is largely bypassed by our C-SVDD-based scheme. As shown Fig.7 (b), the fourth atom actually represents a very small cluster. In fact, the radius of C-SVDD ball corresponding to the more informative atom tends to be large, and one major advantage of our C-SVDD-based strategy is that it is capable to exploit this characteristic of dictionary atoms for more effective feature encoding, as shown in the first three rows of Fig.6 (b). This partially explains the superior performance of the proposed C-SVDD method compared to its K-means counterpart (c.f., experimental results in Section IV)."}, {"heading": "E. Encoding Feature Maps with SIFT Representation", "text": "To encode the feature maps we use a variant of SIFTrepresentation. SIFT is a widely used descriptor in computer vision and is helpful to suppress the noise and improve\nthe invariant properties of the final feature representation. However, one problem of SIFT-based representation is due to its high dimensionality. For example, if we extract 128 dimensional SIFT-descriptors densely in 250 feature maps with the size of 23 \u00d7 23 in pixel, the dimension of the obtained representation vector will be as high as over 16M (250 \u00d7 23 \u00d7 23 \u00d7 128 = 16, 928, 000). To address this issue, we first divide each feature map into m\u00d7m blocks and then extract an 8-bit gradient histogram from each block in the same way as SIFT does. This results in a feature representation with dimension of m \u00d7 m \u00d7 8 for each map, hence significantly reducing the dimensionality while preserving rich information for the subsequent task of pattern classification."}, {"heading": "F. Multi-scale Receptive Field Voting", "text": "Next we extend our method to exploit multi-scale information for better feature learning. A multi-scale method is a way to describe the objects of interest in different sizes of\ncontext. This would be useful since patches of a fixed size can seldom characterize an object well - actually they can only capture local appearance information limited in that size. For example, if the size is very small, information about edges could be captured but the information on how to combine these into more meaningful patterns such as motifs, parts, poselets, and object, is lost, while information about these entities at different levels is valuable in that they are not only discriminative by itself but complementary to each other as well. Most popular manually designed feature descriptors, such as SIFT or HoG, address this problem to some extend by pooling image gradients into edglets-like features, but it is still unclear, for example, how to assemble edglets into motifs using these methods. Convolutional neural network provides a simple and comprehensive solution to this issue by automatically learn hierarchies of features ranging from edglets to objects. However, during this procedure, information on where those high-level patterns are found becomes more and more ambiguous.\nIn our work only a single-layer network is used, hence it is difficult to learn multi-scale information in a hierarchical way. Instead, we take a naive way to obtain multi-scale information by using receptive fields of different sizes. In particular, we fetch patches with Si \u00d7 Si, i = 1, 2, 3 squares in size from training images and use these to train dictionary atoms with corresponding size through K-means. Fig.8 shows some examples of atoms we learnt on a face dataset. One can see that these feature extractors are similar to those learnt using a typical ConvNet. Specifically, with the increasing window size, the learnt features become more understandable - for example, as shown in Fig.8 (c), using a receptive field with size of 20 \u00d7 20 on face images of 64 \u00d7 64, we successfully learned facial parts such as the eyes, the mouth, and so on, while a smaller receptive field gives us some oriented filters, as shown in Fig.8 (a). At each scale we train several networks with different pooling size. One advantage of this method is that it is very efficient to learn and is effective in capture salient features in a multi-scale context. However, it will not tell us how the bigger patterns are explained by smaller ones - such information would be useful from a generative angle.\nTo use the learnt multi-scale information for classification, we train a separate classifier on the output layer of the corresponding network (view) according to different receptive sizes and different pooling sizes, then combine them under a boosting framework. Particularly, assume that the total number of categories is C, and we have M scales (with K different number of pooling sizes for each scale), then we have to learn M \u00d7K \u00d7 C output nodes. These nodes are corresponding to M \u00d7 K multi-class classifiers. Let us denote the parameter of the t\u2212 th classifier \u03b8t \u2208 RD\u00d7C (D is the dimension of feature representation) as \u03b8t = [wt1, wt2, ..., wtC ], where wtk is the weight vector for the k-th category. We first train these parameters using a series of one-versus-rest L2-SVM classifiers, and then normalize the outputs of each classifier using a soft max function,\nftk(xi) = exp(wTtkxi)\u2211C c=1 exp(w T tcxi).\n(12)\nFinally, the normalized predictions ftk are combined to make the final decision,\ng(xi) = argmaxc \u2211 t aTtcft(xi). (13)\nwhere ft = {ft1, ft2, ..., ftC} is the output vector of the t-th classifier, and the corresponding combination coefficients atc are trained using the following objective,\nminac \u2211 i max(0, 1\u2212 \u2211 t aTtcft(xi)) 2 + \u03bb||ac||2 (14)\nThis is the same type of one-versus-rest L2-SVM mentioned before."}, {"heading": "IV. EXPERIMENTS AND ANALYSIS", "text": "To evaluate the performance of the improved K-means network, we conduct extensive experiments on four object datasets (MINST [2], NORB [24], CIFAR-10 [10], STL-10 [10]). Details about these datasets are given below."}, {"heading": "8 TNN-SUBMISSION", "text": ""}, {"heading": "A. Experiment Settings", "text": "All the images undergo whitening preprocessing before feeding them into the network. The whitening operation linearly transforms the data such that their covariance matrix becomes unit sphere, hence justifying the Euclidean distance we use in the K-means clustering procedure.\nUnless otherwise noted, the parameter settings listed in Table.I apply to all experiments. The influence of some important parameters, such as the number of filters, will be investigated in more detail in the subsequent sections. For single scale network the receptive field is set to be 5\u00d7 5 by default across all the datasets, as recommended in [10], while in multi-scale version, we use receptive fields in three scales, as shown in Table.I.\nFor C-SVDD ball there is a regularization parameter \u03bb to set. This parameter allows us to control the amount of noise we are willing to tolerant to. As can be seen from E.q.1, a small \u03bb value encourages a tight ball. We set \u03bb = 1 by default for most datasets except for those with too noisy background are set to 0.005.\nThroughout the experiments, we use Coates\u2019 K-means \u201ctriangle\u201d encoding method [10] (c.f., Section II-B) as baseline (denoted as \u2018K-means\u2019), while its direct counterpart method by simply replacing \u201ctriangle\u201d encoding with C-SVDD encoding is denoted as \u2018C-SVDD\u2019. Furthermore, we denote the proposed single layer network (c.f., Fig. 3) as \u2018C-SVDDNet\u2019, and its multi-scale version as \u2018MSRV + C-SVDDNet\u2019. In addition, we re-evaluate the baseline method [10] within the proposed network by replacing its component of C-SVDD with the Kmeans-based encoding, denoted as \u2018K-meansNet\u2019."}, {"heading": "B. Experimental results", "text": "In this section we report our experimental results and compare them with some state-of-the-art methods on each dataset. Fig.9 gives sample images of all the datasets used in this work. For each dataset we either use k-fold cross validation or strictly follow the specific evaluation protocol (if available) in order to facilitate comparison with previous work. Note that in all the experiments conducted here, we do not use data augment or data of other datasets.\n9\nMINST The MNIST is one of the most popular datasets in pattern recognition. It consists of grey valued images of handwritten digits between 0 and 9 (Fig.9(a)). It has a training set of 60,000 examples, and a test set of 10,000 examples, all of which have been size-normalized and centered in a fixed-size image with 28 \u00d7 28 in pixel. In training we use a dictionary with 400 atoms for feature mapping, and after pooling/subsampling we break each feature map into 9 blocks to extract SIFT features. For multi-scale receptive voting, we use 3 types of receptive fields: 5\u00d75, 7\u00d77 and 9\u00d79. Combined these with two settings for pooling sizes (i.e., 1\u00d71 and 2\u00d72, respectively), 6 different views/representations can be obtained for each image in this dataset.\nTable.II gives our experimental results on the MINST dataset. It is well-known that deep learning has achieved great success on this task of digit recognition. For example, only 95 among 10,000 test digits are misclassified by the Deep Boltzmann Machines [14], while Convolutional Deep Belief Networks [13] and Maxout Networks [27] respectively reduce this number to 82 and 45. Our simple single layer network (MSRV+C-SVDDnet) achieves an error as low as 0.35%, which is highly competitive to other complex methods using deep architecture. Fig.10 shows all the 35 misclassified digits by our method, and one can see that these misclassified digits are very confusing even for human beings. Compared to the original K-means network [10], the proposed method reduces the error rate by 65%, with much smaller number of filters.\nThis reveals that at least on this dataset with clean background, it is very beneficial to focus more on the representation of the details of the image, rather than emphasizing too much on its global aspects using a large number of filters and a large pooling size (c.f., Fig. 2 (A)).\nNORB The dataset contains large amounts of object images in five classes including animals, humans, planes, trucks, and cars (c.f., Fig.9(b)). The dataset comes in two different versions, normalized-uniform and jittered-cluttered. In this paper we use the normalized-uniform version. The image size is 96\u00d7 96 in pixel and objects are centred in the images with a uniform background. There are 10 instances of each object class, imaged under 6 illuminations and 162 viewpoints (18 azimuths 9 elevations). According to the evaluation protocol, these instances are split into two equal-size disjoint sets containing 29,160 images each, used for training and testing respectively. At test time a trained model has to recognize unseen instances of the same object classes. We resize all images to 64\u00d7 64 in pixel and use average pooling with a block size of 4 \u00d7 4. Finally, a 72-dim SIFT representation is extracted (from 9 blocks) for each feature map. For multi-scale receptive voting, we use 2 types of receptive fields (5\u00d75 and 7\u00d77) and 3 types of pooling sizes (2 \u00d7 2, 3 \u00d7 3 and 4 \u00d7 4). This results in 6 different views/representations for each input image.\nTable.III gives the results. One can see from the table that our method achieves the best result among the compared methods with an accuracy of 98.64%. Note that this is a huge dataset containing nearly 60 thousands data with both viewpoint and lighting variations (c.f., Fig.9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10]. It is previously shown that a fine-tuned K-means single-layer network [10] yields better performance than those aforementioned methods."}, {"heading": "10 TNN-SUBMISSION", "text": "However, we achieve the state of the art performance with only 400 atoms, about 10 times smaller than that used in [10].\nCIFAR-10 The CIFAR-10 dataset consists of 60,000 32\u00d7 32 colour images from 10 classes, with 6,000 images per class. According to the evaluation protocol, 50,000 images are used for training and the remaining 10,000 images are used for testing. Note that images in CIFAR-10 vary significantly not only in object positions and object scales within each class, but also in colors and textures (c.f., Fig.9(c)). Furthermore, many images are with cluttered background and in low resolution. All these factors impose great challenges to our algorithm. To avoid bring too much noise into the representation, we focus more on the global aspects of the image when feature learning, by using a large dictionary with 1,200 atoms, two small pooling sizes (1\u00d71 and 2\u00d72 ), and 3 different receptive scales (5\u00d75, 7\u00d77, and 9\u00d79). The feature map is divided into 4 blocks, yielding 32-bit SIFT representation for each map. We also set a small \u03bb value in the C-SVDD training (c.f., E.q. 6) to encourage larger SVDD balls.\nTable.IV gives the results. It shows that a single scale CSVDDnet achieves an accuracy of 82.64%, which is comparable or superior to the performance of other multi-layer architecture systems, such as PCANet [22], Convolutional Kernel Networks [35] and [10]. One possible explanation is that our compact SIFT encoding leads to more robust representation than others, such as Learning smooth pooling [32] and Sum-Product Networks [31]. By integrating multiscale information we improve the accuracy to 85.30%, close to the state of the art performance on this benchmark.\nSTL-10 The STL-10 is a large image dataset popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning (c.f., Fig.9(d)). Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes, among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96\u00d796 pixels in size. There are 10 predefined overlapped folds of training images, with 1000 images in each fold. In each fold, a classifier is trained on a set of 1000 training images, and tested on all 8000 testing images. In consistence with [10], we report the average accuracy across 10 folds. For unsupervised feature learning we randomly select 20,000 unlabeled data. The size of spatial pooling is 4 \u00d7 4, hence the size of feature maps fed for SIFT representation is 23\u00d723. For multi-scale receptive voting we use 2 scale (5\u00d75 and 7\u00d7 7), on each of which we perform spatial pooling in 5 sizes ranging from 2\u00d7 2 to 6\u00d7 6.\nTable.V gives our results on the STL-10 dataset. The major challenges of this dataset lie in that its images are captured in the wild with cluttered background, objects in various scales and poses (c.f., Fig.9(d)). As before, we compared our method with several feature learning methods with state of the art performance. One can see that our one scale C-SVDD network obtains 65.62% accuracy, using a filtering dictionary of 500 atoms, better than 58.6% of Trans. Invariant RBM (TIRBM) [37], 60.10% of Selective Receptive Fields (SRF) [36], and 62.30% of Discriminative Sum-Product Networks (DSPN) [31]. Also note that replacing the proposed C-SVDD encoding\nwith K-means encoding leads to over 3.0% performance loss, while fusing the multi-scale information gives us about 2.6% improvement in accuracy, exceeding the current best performer [39] on this challenging dataset."}, {"heading": "C. The Influence of Parameters and Discussions", "text": "In this section, we investigate the influence of several key parameters of our method on the performance, and empirically illustrate the contributions of the individual stage of our unsupervised feature learning chain. This series of experiments are performed on the STL-10 dataset.\nImpact of the number of features By the number of features, we mean the number of filters K used for feature extraction, which is equal to the number of dictionary atoms. Fig.11 gives the performance curves according to varying number of features with different methods on the STL-10 dataset. It can be seen that with the increasing number of features, the performance of both K-means and C-SVDD methods rises, which is consistent with the results by Coates et al. [10]. One possible explanation is that since both K-means encoding and C-SVDD encoding use the learnt dictionary to extract non-linear features, more dictionary atoms help to disentangle factors of variations in images. In our opinion the capability to learn a large number of atoms at relatively low computational cost is one of the major advantages of K-means based methods for unsupervised feature learning over other algorithms such as Gaussian Mixture Model (GMM), sparse coding, and RBM. For example, it is difficult for a GMM to learn a dictionary with over 800 atoms [10].\nOn the other hand, a too large dictionary can increase the redundancy and decrease the efficiency. Hence it is desirable to reduce the number of features while not hurting the performance too much. Fig.11 shows that our C-SVDD encoding method consistently works better than the K-means encoding at different number of features, and combining CSVDD encoding and SIFT-based representation dramatically reduces the needs for large dictionary without scarifying the\n11\nC-SVDD+SIFT K-means+SIFT C-SVDD K-means\nperformance. Actually, Table.V and Fig.11 show that using our C-SVDD encoding and the SIFT feature representation, the dictionary size reduces by 10 times (from 4,800 [36] to 500) while the performance improves by nearly 12% (from 53.80% to 65.42%).\nEffect of the pooling size To investigate the effect of different pooling sizes on the performance using the proposed method, we conduct a series of experiments on the STL-10 dataset. Particularly, for a 96 \u00d7 96 original image, we use a receptive field of 5 \u00d7 5 in pixel for feature extraction and obtain a layer of feature maps with 92 \u00d7 92. The pooling blocks are set to be m\u00d7m such that the size of final feature maps after pooling is 92m \u00d7 92 m . We vary m\u00d7m from 1 \u00d7 1 to 31 \u00d7 31 and record the yielded accuracy. Fig.12 gives the results under different settings. We can see from the figure that generally for the one layer K-means-based network we need bigger block sizes for improved translation invariance, but adding a robust SIFT encoding layer after pooling effectively reduces the needs for large pooling size while obtaining better performance. One possible reason is that this tends to characterize more detailed information of the objects to be represented.\nEffect of the multi-scale receptive field voting Fig.13 gives the detailed accuracy of 10 representations using 2 sizes of receptive fields and 5 sizes of pooling blocks. One can see that different representation leads to different prediction accuracy but combining them leads to better performance. This shows that the representations captured with different receptive fields and pooling sizes are complementary to each other.\nContribution of components To illustrate the contributions of the individual stages of the proposed method (i.e., C-SVDDbased encoding, SIFT-representation and multi-scale voting), we conduct a series of experiments on the STL-10 dataset by removing each of the three main stages in turn while leaving the remaining stages in place (the comparison is thus against our full method). Fig.14 gives the results. In general each stage is beneficial and (not shown) the results are cumulative over\nthe stages, but the SIFT stage seems to contribute most to the performance improvement."}, {"heading": "V. CONCLUSION", "text": "In this paper, we propose a simple one-layer neural network termed C-SVDDNet for unsupervised feature learning. The key idea of the proposed method is to describe the density and distribution of each cluster from K-means with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called C-SVDD that centers the SVDD ball towards the mode of local density of each cluster. Furthermore we show that the objective of CSVDD can be solved very efficiently as a linear programming problem. Another key component of the C-SVDDNet is a postpooling layer, which effectively encodes the pooling responses with a variant SIFT descriptors. With the help of this, we can characterize the details of the input without needing to learn a large number of filters, hence allowing a \u201clight\u201d design strategy for network architecture. Last but not least, we extend"}, {"heading": "12 TNN-SUBMISSION", "text": "the network with multiple receptive field scales and multiple pooling sizes for better feature learning. Extensive experiments on several popular object recognition benchmarks demonstrate that the proposed C-SVDDNet yields comparable or better performance than that of the previous state of the art methods."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the National Science Foundation of China (61073112, 61035003, 61373060), Jiangsu Science Foundation (BK2012793), Qing Lan Project, Research Fund for the Doctoral Program (RFDP) (20123218110033)."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "arXiv preprint arXiv:1206.5538, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "What is the best multi-stage architecture for object recognition?", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "in Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1872\u20131886, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1872}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1112.6209, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning convolutional feature hierarchies for visual recognition.", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "in NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Hyperfeatures\u2013multilevel local coding for visual recognition", "author": ["A. Agarwal", "B. Triggs"], "venue": "Proc. Ninth European Conf. Computer Vision, 2006. Springer, 2006, pp. 30\u201343.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometry of the restricted boltzmann machine", "author": ["M.A. Cueto", "J. Morton", "B. Sturmfels"], "venue": "Algebraic Methods in Statistics and Probability,(eds. M. Viana and H. Wynn), AMS, Contemporary Mathematics, vol. 516, pp. 135\u2013153, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "Ann Arbor, vol. 1001, p. 48109, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Support vector data description", "author": ["D.M. Tax", "R.P. Duin"], "venue": "Machine learning, vol. 54, no. 1, pp. 45\u201366, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Fault detection based on svdd and cluster algorithm", "author": ["J. Xu", "J. Yao", "L. Ni"], "venue": "Electronics, Communications and Control (ICECC), 2011 International Conference on. IEEE, 2011, pp. 2050\u20132052.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 609\u2013616.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Centering svdd for unsupervised feature representation in object classification", "author": ["D. Wang", "X. Tan"], "venue": "Neural Information Processing. Springer, 2013, pp. 376\u2013383.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale object recognition with cudaaccelerated hierarchical neural networks", "author": ["R. Uetz", "S. Behnke"], "venue": "Intelligent Computing and Intelligent Systems, 2009. ICIS 2009. IEEE International Conference on, vol. 1. IEEE, 2009, pp. 536\u2013541.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "The hierarchical beta process for convolutional factor analysis and deep learning", "author": ["B. Chen", "G. Polatkan", "G. Sapiro", "L. Carin", "D.B. Dunson"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 361\u2013368.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised feature learning for rgb-d based object recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "Experimental Robotics. Springer, 2013, pp. 387\u2013402.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "Proc. IEEE Computer Vision and Pattern Recognition (CVPR), 2011, pp. 1713\u20131720.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "International journal of computer vision, vol. 105, no. 3, pp. 222\u2013245, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Pcanet: A simple deep learning baseline for image classification?", "author": ["T.-H. Chan", "K. Jia", "S. Gao", "J. Lu", "Z. Zeng", "Y. Ma"], "venue": "arXiv preprint arXiv:1404.3606,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "Proc. IEEE Computer Vision and Pattern Recognition (CVPR), vol. 2, 2004, pp. II\u201397.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Proc. IEEE Computer Vision and Pattern Recognition (CVPR), 2012, pp. 3642\u20133649.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, vol. abs/1312.4400, 2013. [Online]. Available: http://arxiv.org/abs/1312.4400", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1302.4389, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 1058\u20131066.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Deeply-supervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "arXiv preprint arXiv:1409.5185, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "3d object recognition with deep belief nets.", "author": ["V. Nair", "G.E. Hinton"], "venue": "in NIPS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Discriminative learning of sum-product networks.", "author": ["R. Gens", "P. Domingos"], "venue": "in NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Learning smooth pooling regions for visual recognition", "author": ["M. Malinowski", "M. Fritz"], "venue": "the British Machine Vision Conference, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving deep neural networks with probabilistic maxout units", "author": ["J.T. Springenberg", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.6116, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "arXiv preprint arXiv:1406.3332, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Selecting receptive fields in deep networks.", "author": ["A. Coates", "A.Y. Ng"], "venue": "in NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "arXiv preprint arXiv:1206.6418, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning of invariant features via simulated fixations in video.", "author": ["W.Y. Zou", "A.Y. Ng", "S. Zhu", "K. Yu"], "venue": "in NIPS,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Committees of deep feedforward networks trained with few data", "author": ["B. Miclut"], "venue": "Pattern Recognition. Springer, 2014, pp. 736\u2013742.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "A representative method for this is the deep learning (DL) approach [1] with its goal to learn multiple layers of abstract representations from data.", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "Among others, one typical DL method is the so called convolutional neural network (ConvNet), which consists of multiple trainable stages stacked on top of each other, followed by a supervised classifier [2] [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 2, "context": "Among others, one typical DL method is the so called convolutional neural network (ConvNet), which consists of multiple trainable stages stacked on top of each other, followed by a supervised classifier [2] [3].", "startOffset": 207, "endOffset": 210}, {"referenceID": 3, "context": "Many variations of ConvNet network have been proposed as well for different vision tasks [4] [5] [6] with great success.", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "Many variations of ConvNet network have been proposed as well for different vision tasks [4] [5] [6] with great success.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Many variations of ConvNet network have been proposed as well for different vision tasks [4] [5] [6] with great success.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "In these methods layers of representation are usually obtained by greedily training one layer at a time on the lower level [5] [7] [3], using an unsupervised learning algorithm.", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "In these methods layers of representation are usually obtained by greedily training one layer at a time on the lower level [5] [7] [3], using an unsupervised learning algorithm.", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "In these methods layers of representation are usually obtained by greedily training one layer at a time on the lower level [5] [7] [3], using an unsupervised learning algorithm.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Neural network based singlelayer methods, such as autoencoder [8] and RBM (Restricted Boltzmann Machine, [9]), are widely used for this but they", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "Neural network based singlelayer methods, such as autoencoder [8] and RBM (Restricted Boltzmann Machine, [9]), are widely used for this but they", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "[10] shows that the K-means based feature learning network is capable to achieve superior performance compared to sparse autoencoder, sparse RBM and GMM (Guassian Mixture Model).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In this paper, we proposed a SVDD (Support Vector Data Description, [11], [12]) based method to address these issues.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "In this paper, we proposed a SVDD (Support Vector Data Description, [11], [12]) based method to address these issues.", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "\u2019s series of controlled experiments on single layer unsupervised feature learning networks [10] is that compared to the choice of particular learning algorithm, the parameters that define the feature extraction pipeline, especially the number of features, have much more deep impact on the performance.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": ", Convolutional Deep Belief Nets (CDBN) [13], Deep Boltzmann Machine [14] and Sparse Auto-encoder [10]).", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": ", Convolutional Deep Belief Nets (CDBN) [13], Deep Boltzmann Machine [14] and Sparse Auto-encoder [10]).", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": ", Convolutional Deep Belief Nets (CDBN) [13], Deep Boltzmann Machine [14] and Sparse Auto-encoder [10]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "One major reason for this is that the existence of large number of basis vectors increases the chances that an input can be encoded properly by the network [10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "Here we use a variant of SIFT-based encoder [15], which essentially projects the responses of a pooling operation into a low dimensional space while suppressing the noise and improving the invariant properties of the final feature representation.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "A preliminary version of this work appeared in [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": ", K-means, Gaussian Mixture Models (GMM), autoencoder [8], RBM [9], sparse coding) and their more sophisticated variants, such as Sparse Auto-encoder [10], Sparse RBM [10], Locally-connected Neural Pyramid (LCNP) [17], Conv.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": ", K-means, Gaussian Mixture Models (GMM), autoencoder [8], RBM [9], sparse coding) and their more sophisticated variants, such as Sparse Auto-encoder [10], Sparse RBM [10], Locally-connected Neural Pyramid (LCNP) [17], Conv.", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": ", K-means, Gaussian Mixture Models (GMM), autoencoder [8], RBM [9], sparse coding) and their more sophisticated variants, such as Sparse Auto-encoder [10], Sparse RBM [10], Locally-connected Neural Pyramid (LCNP) [17], Conv.", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": ", K-means, Gaussian Mixture Models (GMM), autoencoder [8], RBM [9], sparse coding) and their more sophisticated variants, such as Sparse Auto-encoder [10], Sparse RBM [10], Locally-connected Neural Pyramid (LCNP) [17], Conv.", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": ", K-means, Gaussian Mixture Models (GMM), autoencoder [8], RBM [9], sparse coding) and their more sophisticated variants, such as Sparse Auto-encoder [10], Sparse RBM [10], Locally-connected Neural Pyramid (LCNP) [17], Conv.", "startOffset": 213, "endOffset": 217}, {"referenceID": 17, "context": "Sparse Factor Analysis (CSFA) [18], Hierarchical Matching Pursuit (HMP) [19], Hierarchical Sparse Coding (HSC) [20], Fisher vector [21], and so on.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "Sparse Factor Analysis (CSFA) [18], Hierarchical Matching Pursuit (HMP) [19], Hierarchical Sparse Coding (HSC) [20], Fisher vector [21], and so on.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Sparse Factor Analysis (CSFA) [18], Hierarchical Matching Pursuit (HMP) [19], Hierarchical Sparse Coding (HSC) [20], Fisher vector [21], and so on.", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "Sparse Factor Analysis (CSFA) [18], Hierarchical Matching Pursuit (HMP) [19], Hierarchical Sparse Coding (HSC) [20], Fisher vector [21], and so on.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "[10] shows through a series of controlled experiments that compared to how to obtain these filters, how many of them (the number of features) and how to use them (feature encoding) has more fundamental impact on the final performance of a network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "A better summary is the following \u201ctriangle\u201d encoding [10]:", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "[10] shows that this strategy actually leads to comparable performance to, if not better than, those based on network methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "show that this kind of network is able to yield state of the art results on several challenging datasets [10].", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "On the other hand, other works use smaller filter banks but highlight the importance of detailed local information in constructing the representation, usually based on some complicated feature encoding strategy, as done in PCANet [22] or Fisher Vector [23].", "startOffset": 230, "endOffset": 234}, {"referenceID": 22, "context": "On the other hand, other works use smaller filter banks but highlight the importance of detailed local information in constructing the representation, usually based on some complicated feature encoding strategy, as done in PCANet [22] or Fisher Vector [23].", "startOffset": 252, "endOffset": 256}, {"referenceID": 9, "context": "Compared to [10], we use an improved feature encoding method named CSVDD (detailed in the next section) and adopt the architecture of relatively small dictionary.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "Different to [22] or [23], we learn filter banks for feature encoding but add a SIFT-based post-pooling processing procedure onto the network, which essentially projects the responses of a pooling operation into a more compact and robust representation space.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "Different to [22] or [23], we learn filter banks for feature encoding but add a SIFT-based post-pooling processing procedure onto the network, which essentially projects the responses of a pooling operation into a more compact and robust representation space.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "The goal of SVDD (Support Vector Data Description, [11]) is to find a closed spherical boundary around the given data points.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": "To evaluate the performance of the improved K-means network, we conduct extensive experiments on four object datasets (MINST [2], NORB [24], CIFAR-10 [10], STL-10 [10]).", "startOffset": 125, "endOffset": 128}, {"referenceID": 23, "context": "To evaluate the performance of the improved K-means network, we conduct extensive experiments on four object datasets (MINST [2], NORB [24], CIFAR-10 [10], STL-10 [10]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": "To evaluate the performance of the improved K-means network, we conduct extensive experiments on four object datasets (MINST [2], NORB [24], CIFAR-10 [10], STL-10 [10]).", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "To evaluate the performance of the improved K-means network, we conduct extensive experiments on four object datasets (MINST [2], NORB [24], CIFAR-10 [10], STL-10 [10]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "For single scale network the receptive field is set to be 5\u00d7 5 by default across all the datasets, as recommended in [10], while in multi-scale version, we use receptive fields in three scales, as shown in Table.", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "Throughout the experiments, we use Coates\u2019 K-means \u201ctriangle\u201d encoding method [10] (c.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "In addition, we re-evaluate the baseline method [10] within the proposed network by replacing its component of C-SVDD with the Kmeans-based encoding, denoted as \u2018K-meansNet\u2019.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "Deep Boltzmann Machines [14] (2009) 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "95 Convolutional Deep Belief Networks [13] (2009) 0.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "82 Multi-column deep neural networks [25] (2012) 0.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "23 Network in Network [26] (2013) 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "47 Maxout Networks [27] (2013) 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 27, "context": "45 Regularization of neural networks [28] (2013) 0.", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "21 PCANet [22] (2014) 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "62 Deeply-Supervised Nets [29] (2014) 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "ConvNet [24] (2004) 93.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "4 Deep Boltzmann Machine [14] (2009) 92.", "startOffset": 25, "endOffset": 29}, {"referenceID": 29, "context": "8 Deep Belief Network [30] (2009) 95.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "0 optimized ConvNet [3] (2009) 94.", "startOffset": 20, "endOffset": 23}, {"referenceID": 16, "context": "4 Locally-connected Neural Pyramid (LCNP) [17] (2009) 97.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "13 Sparse Auto-encoder [10] (2011) 96.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "9 Sparse RBM [10] (2011) 96.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "2 Regularization of neural networks [28] (2013) 96.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "K-means (4000 features) [10] (2011) 97.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "Sum-Product Networks [31] (2012) 83.", "startOffset": 21, "endOffset": 25}, {"referenceID": 31, "context": "96 Learning smooth pooling [32] (2013) 80.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "02 Maxout Networks [27] (2013) 88.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "32 Stochastic Pooling [33] (2013) 84.", "startOffset": 22, "endOffset": 26}, {"referenceID": 33, "context": "87 Probabilistic Maxout Units [34] (2013) 88.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "65 PCANet [22] (2014) 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "67 Convolutional Kernel Networks [35] (2014) 82.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "K-means (4000 features) [10] (2011) 79.", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "Selective Receptive Fields (3 Layers) [36] (2011) 60.", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "Invariant RBM (TIRBM) [37] (2012) 58.", "startOffset": 22, "endOffset": 26}, {"referenceID": 37, "context": "70 Simulated visual fixation ConvNet [38] (2012) 61.", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "Net (DSPN) [31] (2012) 62.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "0 Hierarchical Matching Pursuit (HMP) [19] (2013) 64.", "startOffset": 38, "endOffset": 42}, {"referenceID": 38, "context": "0 Deep Feedforward Networks [39] (2014) 68.", "startOffset": 28, "endOffset": 32}, {"referenceID": 35, "context": "K-means (4800 features) [36] (2011) 53.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "For example, only 95 among 10,000 test digits are misclassified by the Deep Boltzmann Machines [14], while Convolutional Deep Belief Networks [13] and Maxout Networks [27] respectively reduce this number to 82 and 45.", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "For example, only 95 among 10,000 test digits are misclassified by the Deep Boltzmann Machines [14], while Convolutional Deep Belief Networks [13] and Maxout Networks [27] respectively reduce this number to 82 and 45.", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": "For example, only 95 among 10,000 test digits are misclassified by the Deep Boltzmann Machines [14], while Convolutional Deep Belief Networks [13] and Maxout Networks [27] respectively reduce this number to 82 and 45.", "startOffset": 167, "endOffset": 171}, {"referenceID": 9, "context": "Compared to the original K-means network [10], the proposed method reduces the error rate by 65%, with much smaller number of filters.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10].", "startOffset": 125, "endOffset": 128}, {"referenceID": 13, "context": "9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10].", "startOffset": 190, "endOffset": 194}, {"referenceID": 9, "context": "9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10].", "startOffset": 226, "endOffset": 230}, {"referenceID": 9, "context": "9(b)) and many feature learning algorithms use this database as a benchmark to evaluate their scalability, such ConvNet [24] [3], DBM [14], DBN [30], Locally-connected Neural Pyramid (LCNP) [17] and many sparse coding methods [10] [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 9, "context": "It is previously shown that a fine-tuned K-means single-layer network [10] yields better performance than those aforementioned methods.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "However, we achieve the state of the art performance with only 400 atoms, about 10 times smaller than that used in [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "64%, which is comparable or superior to the performance of other multi-layer architecture systems, such as PCANet [22], Convolutional Kernel Networks [35] and [10].", "startOffset": 114, "endOffset": 118}, {"referenceID": 34, "context": "64%, which is comparable or superior to the performance of other multi-layer architecture systems, such as PCANet [22], Convolutional Kernel Networks [35] and [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "64%, which is comparable or superior to the performance of other multi-layer architecture systems, such as PCANet [22], Convolutional Kernel Networks [35] and [10].", "startOffset": 159, "endOffset": 163}, {"referenceID": 31, "context": "One possible explanation is that our compact SIFT encoding leads to more robust representation than others, such as Learning smooth pooling [32] and Sum-Product Networks [31].", "startOffset": 140, "endOffset": 144}, {"referenceID": 30, "context": "One possible explanation is that our compact SIFT encoding leads to more robust representation than others, such as Learning smooth pooling [32] and Sum-Product Networks [31].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "In consistence with [10], we report the average accuracy across 10 folds.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "Invariant RBM (TIRBM) [37], 60.", "startOffset": 22, "endOffset": 26}, {"referenceID": 35, "context": "10% of Selective Receptive Fields (SRF) [36], and 62.", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "30% of Discriminative Sum-Product Networks (DSPN) [31].", "startOffset": 50, "endOffset": 54}, {"referenceID": 38, "context": "6% improvement in accuracy, exceeding the current best performer [39] on this challenging dataset.", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "For example, it is difficult for a GMM to learn a dictionary with over 800 atoms [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 35, "context": "11 show that using our C-SVDD encoding and the SIFT feature representation, the dictionary size reduces by 10 times (from 4,800 [36] to 500) while the performance improves by nearly 12% (from 53.", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "In this paper, we investigate the problem of learning feature representation from unlabeled data using a single-layer K-means network. A K-means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers\u2019 great attention recently due to its simplicity, effectiveness, and scalability. However, one drawback of this feature mapping is that it tends to be unreliable when the training data contains noise. To address this issue, we propose a SVDD based feature learning algorithm that describes the density and distribution of each cluster from Kmeans with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called CSVDD that centers the SVDD ball towards the mode of local density of each cluster, and we show that the objective of CSVDD can be solved very efficiently as a linear programming problem. Additionally, previous single-layer networks favor a large number of centroids but a crude pooling size, resulting in a representation that highlights the global aspects of the object. Here we explore an alternative network architecture with much smaller number of nodes but with much finer pooling size, hence emphasizing the local details of the object. The architecture is also extended with multiple receptive field scales and multiple pooling sizes. Extensive experiments on several popular object recognition benchmarks, such as MINST, NORB, CIFAR-10 and STL-10, shows that the proposed C-SVDDNet method yields comparable or better performance than that of the previous state of the art methods.", "creator": "LaTeX with hyperref package"}}}