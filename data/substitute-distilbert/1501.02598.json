{"id": "1501.02598", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "abstract": "we extend the effective skip - gram model of mikolov et,. ( 2013 ) by taking visual information into account. like s kip - gram, our multimodal models ( ki - gram ) acquire vector - based word representations by learning to display linguistic contexts in text corpora. however, for a restricted set of words, the models are also exposed innate neural characteristics of the colors they denote ( extracted from natural images ), and must predict linguistic and visual features jointly. the standard kip - gram models achieve excellent performance on a variety of developmental benchmarks. moreover, since they propagate visual information to all words, systems also use neurons to improve image labeling and retrieval in the hierarchical zero - shot setup, where the elusive concepts typically not present in training. finally, the mms iq - gram models discover intriguing comprehension - related properties of abstract words, paving the way to realistic implementations of embodied symbols of meaning.", "histories": [["v1", "Mon, 12 Jan 2015 10:48:32 GMT  (2994kb,D)", "https://arxiv.org/abs/1501.02598v1", "10 pages"], ["v2", "Tue, 13 Jan 2015 09:37:08 GMT  (2994kb,D)", "http://arxiv.org/abs/1501.02598v2", "10 pages"], ["v3", "Thu, 12 Mar 2015 09:47:33 GMT  (2537kb,D)", "http://arxiv.org/abs/1501.02598v3", "accepted at NAACL 2015, camera ready version, 11 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["angeliki lazaridou", "nghia the pham", "marco baroni"], "accepted": true, "id": "1501.02598"}, "pdf": {"name": "1501.02598.pdf", "metadata": {"source": "CRF", "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "authors": ["Angeliki Lazaridou"], "emails": ["angeliki.lazaridou@unitn.it", "thenghia.pham@unitn.it", "marco.baroni@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "Distributional semantic models (DSMs) derive vector-based representations of meaning from patterns of word co-occurrence in corpora. DSMs have been very effectively applied to a variety of semantic tasks (Clark, 2015; Mikolov et al., 2013b; Turney and Pantel, 2010). However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000). This observa-\ntion has led to the development of multimodal distributional semantic models (MDSMs) (Bruni et al., 2014; Feng and Lapata, 2010; Silberer and Lapata, 2014), that enrich linguistic vectors with perceptual information, most often in the form of visual features automatically induced from image collections.\nMDSMs outperform state-of-the-art text-based approaches, not only in tasks that directly require access to visual knowledge (Bruni et al., 2012), but also on general semantic benchmarks (Bruni et al., 2014; Silberer and Lapata, 2014). However, current MDSMs still have a number of drawbacks. First, they are generally constructed by first separately building linguistic and visual representations of the same concepts, and then merging them. This is obviously very different from how humans learn about concepts, by hearing words in a situated perceptual context. Second, MDSMs assume that both linguistic and visual information is available for all words, with no generalization of knowledge across modalities. Third, because of this latter assumption of full linguistic and visual coverage, current MDSMs, paradoxically, cannot be applied to computer vision tasks such as image labeling or retrieval, since they do not generalize to images or words beyond their training set.\nWe introduce the multimodal skip-gram models, two new MDSMs that address all the issues above. The models build upon the very effective skip-gram approach of Mikolov et al. (2013a), that constructs vector representations by learning, incrementally, to predict the linguistic contexts in which target words occur in a corpus. In our extension, for a subset of the target words, relevant visual evidence from\nar X\niv :1\n50 1.\n02 59\n8v 3\n[ cs\n.C L\n] 1\n2 M\nar 2\nnatural images is presented together with the corpus contexts (just like humans hear words accompanied by concurrent perceptual stimuli). The model must learn to predict these visual representations jointly with the linguistic features. The joint objective encourages the propagation of visual information to representations of words for which no direct visual evidence was available in training. The resulting multimodally-enhanced vectors achieve remarkably good performance both on traditional semantic benchmarks, and in their new application to the \u201czero-shot\u201d image labeling and retrieval scenario. Very interestingly, indirect visual evidence also affects the representation of abstract words, paving the way to ground-breaking cognitive studies and novel applications in computer vision."}, {"heading": "2 Related Work", "text": "There is by now a large literature on multimodal distributional semantic models. We focus here on a few representative systems. Bruni et al. (2014) propose a straightforward approach to MDSM induction, where text- and image-based vectors for the same words are constructed independently, and then \u201cmixed\u201d by applying the Singular Value Decomposition to their concatenation. An empirically superior model has been proposed by Silberer and Lapata (2014), who use more advanced visual representations relying on images annotated with highlevel \u201cvisual attributes\u201d, and a multimodal fusion strategy based on stacked autoencoders. Kiela and Bottou (2014) adopt instead a simple concatenation strategy, but obtain empirical improvements by using state-of-the-art convolutional neural networks to extract visual features, and the skip-gram model for text. These and related systems take a twostage approach to derive multimodal spaces (unimodal induction followed by fusion), and they are only tested on concepts for which both textual and visual labeled training data are available (the pioneering model of Feng and Lapata (2010) did learn from text and images jointly using Topic Models, but was shown to be empirically weak by Bruni et al. (2014)).\nHowell et al. (2005) propose an incremental multimodal model based on simple recurrent networks (Elman, 1990), focusing on grounding propagation\nfrom early-acquired concrete words to a larger vocabulary. However, they use subject-generated features as surrogate for realistic perceptual information, and only test the model in small-scale simulations of word learning. Hill and Korhonen (2014), whose evaluation focuses on how perceptual information affects different word classes more or less effectively, similarly to Howell et al., integrate perceptual information in the form of subject-generated features and text from image annotations into a skipgram model. They inject perceptual information by merging words expressing perceptual features with corpus contexts, which amounts to linguisticcontext re-weighting, thus making it impossible to separate linguistic and perceptual aspects of the induced representation, and to extend the model with non-linguistic features. We use instead authentic image analysis as proxy to perceptual information, and we design a robust way to incorporate it, easily extendible to other signals, such as feature norm or brain signal vectors (Fyshe et al., 2014).\nThe recent work on so-called zero-shot learning to address the annotation bottleneck in image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective. Instead of combining visual and linguistic information in a common space, it aims at learning a mapping from image- to text-based vectors. The mapping, induced from annotated data, is then used to project images of objects that were not seen during training onto linguistic space, in order to retrieve the nearest word vectors as labels. Multimodal word vectors should be better-suited than purely text-based vectors for the task, as their similarity structure should be closer to that of images. However, traditional MDSMs cannot be used in this setting, because they do not cover words for which no manually annotated training images are available, thus defeating the generalizing purpose of zero-shot learning. We will show below that our multimodal vectors, that are not hampered by this restriction, do indeed bring a significant improvement over purely text-based linguistic representations in the zero-shot setup.\nMultimodal language-vision spaces have also been developed with the goal of improving caption generation/retrieval and caption-based image retrieval (Karpathy et al., 2014; Kiros et al., 2014;\nMao et al., 2014; Socher et al., 2014). These methods rely on necessarily limited collections of captioned images as sources of multimodal evidence, whereas we automatically enrich a very large corpus with images to induce general-purpose multimodal word representations, that could be used as input embeddings in systems specifically tuned to caption processing. Thus, our work is complementary to this line of research."}, {"heading": "3 Multimodal Skip-gram Architecture", "text": ""}, {"heading": "3.1 Skip-gram Model", "text": "We start by reviewing the standard SKIP-GRAM model of Mikolov et al. (2013a), in the version we use. Given a text corpus, SKIP-GRAM aims at inducing word representations that are good at predicting the context words surrounding a target word. Mathematically, it maximizes the objective function:\n1\nT T\u2211 t=1  \u2211 \u2212c\u2264j\u2264c,j 6=0 log p(wt+j |wt)  (1) where w1, w2, ..., wT are words in the training corpus and c is the size of the window around target wt, determining the set of context words to be predicted by the induced representation of wt. Following Mikolov et al., we implement a subsampling option randomly discarding context words as an inverse function of their frequency, controlled by hyperparameter t. The probability p(wt+j |wt), the core part of the objective in Equation 1, is given by softmax:\np(wt+j |wt) = e u\u2032wt+j Tuwt\u2211W w\u2032=1 e u\u2032 w\u2032 Tuwt (2)\nwhere uw and u\u2032w are the context and target vector representations of word w respectively, and W is the size of the vocabulary. Due to the normalization term, Equation 2 requires O(|W |) time complexity. A considerable speedup to O(log |W |), is achieved by using the hierarchical version of Equation 2 (Morin and Bengio, 2005), adopted here."}, {"heading": "3.2 Injecting visual knowledge", "text": "We now assume that word learning takes place in a situated context, in which, for a subset of the target words, the corpus contexts are accompanied by a\nthe cute\nsat on the matlittle CAT +=\nvisual representation of the concepts they denote (just like in a conversation, where a linguistic utterance will often be produced in a visual scene including some of the word referents). The visual representation is also encoded in a vector (we describe in Section 4 below how we construct it). We thus make the skip-gram \u201cmultimodal\u201d by adding a second, visual term to the original linguistic objective, that is, we extend Equation 1 as follow:\n1\nT T\u2211 t=1 (Lling(wt) + Lvision(wt)) (3)\nwhere Lling(wt) is the text-based skip-gram objective \u2211 \u2212c\u2264j\u2264c,j 6=0 log p(wt+j |wt), whereas the Lvision(wt) term forces word representations to take visual information into account. Note that if a word wt is not associated to visual information, as is systematically the case, e.g., for determiners and non-imageable nouns, but also more generally for any word for which no visual data are available, Lvision(wt) is set to 0.\nWe now propose two variants of the visual objective, resulting in two distinguished multi-modal versions of the skip-gram model."}, {"heading": "3.3 Multi-modal Skip-gram Model A", "text": "One way to force word embeddings to take visual representations into account is to try to directly increase the similarity (expressed, for example, by the cosine) between linguistic and visual rep-\nresentations, thus aligning the dimensions of the linguistic vector with those of the visual one (recall that we are inducing the first, while the second is fixed), and making the linguistic representation of a concept \u201cmove\u201d closer to its visual representation. We maximize similarity through a max-margin framework commonly used in models connecting language and vision (Weston et al., 2010; Frome et al., 2013). More precisely, we formulate the visual objective Lvision(wt) as: \u2212\n\u2211 w\u2032\u223cPn(w) max(0, \u03b3\u2212 cos(uwt , vwt)+ cos(uwt , vw\u2032)) (4) where the minus sign turns a loss into a cost, \u03b3 is the margin, uwt is the target multimodally-enhanced word representation we aim to learn, vwt is the corresponding visual vector (fixed in advance) and vw\u2032 ranges over visual representations of words (featured in our image dictionary) randomly sampled from distribution Pn(wt). These random visual representations act as \u201cnegative\u201d samples, encouraging uwt to be more similar to its own visual representation than to that of other words. The sampling distribution is currently set to uniform, and the number of negative samples controlled by hyperparameter k."}, {"heading": "3.4 Multi-modal Skip-gram Model B", "text": "The visual objective in MMSKIP-GRAM-A has the drawback of assuming a direct comparison of linguistic and visual representations, constraining them to be of equal size. MMSKIP-GRAM-B lifts this constraint by including an extra layer mediating between linguistic and visual representations (see Figure 1 for a sketch of MMSKIP-GRAM-B). Learning this layer is equivalent to estimating a cross-modal mapping matrix from linguistic onto visual representations, jointly induced with linguistic word embeddings. The extension is straightforwardly implemented by substituting, into Equation 4, the word representation uwt with zwt = M\nu\u2192vuwt , where Mu\u2192v is the cross-modal mapping matrix to be induced. To avoid overfitting, we also add an L2 regularization term for Mu\u2192v to the overall objective (Equation 3), with its relative importance controlled by hyperparamer \u03bb."}, {"heading": "4 Experimental Setup", "text": "The parameters of all models are estimated by backpropagation of error via stochastic gradient descent.\nOur text corpus is a Wikipedia 2009 dump comprising approximately 800M tokens.1 To train the multimodal models, we add visual information for 5,100 words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus and have concreteness score \u2265 0.5 according to Turney et al. (2011). On average, about 5% tokens in the text corpus are associated to a visual representation. To construct the visual representation of a word, we sample 100 pictures from its ImageNet entry, and extract a 4096-dimensional vector from each picture using the Caffe toolkit (Jia et al., 2014), together with the pre-trained convolutional neural network of Krizhevsky et al. (2012). The vector corresponds to activation in the top (FC7) layer of the network. Finally, we average the vectors of the 100 pictures associated to each word, deriving 5,100 aggregated visual representations.\nHyperparameters For both SKIP-GRAM and the MMSKIP-GRAM models, we fix hidden layer size to 300. To facilitate comparison between MMSKIPGRAM-A and MMSKIP-GRAM-B, and since the former requires equal linguistic and visual dimensionality, we keep the first 300 dimensions of the visual vectors. For the linguistic objective, we use hierarchical softmax with a Huffman frequency-based encoding tree, setting frequency subsampling option t= 0.001 and window size c= 5, without tuning. The following hyperparameters were tuned on the text9 corpus:2 MMSKIP-GRAM-A: k=20, \u03b3=0.5; MMSKIP-GRAM-B: k=5, \u03b3=0.5, \u03bb=0.0001."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Approximating human judgments", "text": "Benchmarks A widely adopted way to test DSMs and their multimodal extensions is to measure how well model-generated scores approximate human similarity judgments about pairs of words. We put together various benchmarks covering diverse aspects of meaning, to gain insights on the effect of perceptual information on different similarity facets. Specifically, we test on general relatedness (MEN, Bruni et al. (2014), 3K pairs), e.g., pickles are related to hamburgers, semantic (\u2248 taxonomic) simi-\n1http://wacky.sslmit.unibo.it 2http://mattmahoney.net/dc/textdata.html\nlarity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.5K pairs), e.g., pickles are similar to onions, as well as visual similarity (VisSim, Silberer and Lapata (2014), same pairs as SemSim with different human ratings), e.g., pickles look like zucchinis.\nAlternative Multimodal Models We compare our models against several recent alternatives. We test the vectors made available by Kiela and Bottou (2014). Similarly to us, they derive textual features with the skip-gram model (from a portion of the Wikipedia and the British National Corpus) and use visual representations extracted from the ESP dataset (von Ahn and Dabbish, 2004) through a convolutional neural network (Oquab et al., 2014). They concatenate textual and visual features after normalizing to unit length and centering to zero mean. We also test the vectors that performed best in the evaluation of Bruni et al. (2014), based on textual features extracted from a 3B-token corpus and SIFT-based Bag-of-Visual-Words visual features (Sivic and Zisserman, 2003) extracted from the ESP collection. Bruni and colleagues fuse a weighted concatenation of the two components through SVD. We further reimplement both methods with our own textual and visual embeddings as CONCATENATION and SVD (with target dimensionality 300, picked without tuning). Finally, we present for comparison the results on SemSim and VisSim reported by Silberer and Lapata (2014), obtained with a stacked-autoencoders architecture run on textual features extracted from Wikipedia with the Strudel algorithm (Baroni et al., 2010) and attribute-based visual features (Farhadi et al., 2009) extracted from ImageNet.\nAll benchmarks contain a fair amount of words for which we did not use direct visual evidence. We are interested in assessing the models both in terms of how they fuse linguistic and visual evidence when they are both available, and for their robustness in lack of full visual coverage. We thus evaluate them in two settings. The visual-coverage columns of Table 1 (those on the right) report results on the subsets for which all compared models have access to direct visual information for both words. We further report results on the full sets (\u201c100%\u201d columns of Table 1) for models that can propagate visual information and that, consequently, can meaningfully be tested\non words without direct visual representations.\nResults The state-of-the-art visual CNN FEATURES alone perform remarkably well, outperforming the purely textual model (SKIP-GRAM) in two tasks, and achieving the best absolute performance on the visual-coverage subset of Simlex-999. Regarding multimodal fusion (that is, focusing on the visual-coverage subsets), both MMSKIP-GRAM models perform very well, at the top or just below it on all tasks, with comparable results for the two variants. Their performance is also good on the full data sets, where they consistently outperform SKIP-GRAM and SVD (that is much more strongly affected by lack of complete visual information). They\u2019re just a few points below the state-of-the-art MEN correlation (0.8), achieved by Baroni et al. (2014) with a corpus 3 larger than ours and extensive tuning. MMSKIP-GRAM-B is close to the state of the art for Simlex-999, reported by the resource creators to be at 0.41 (Hill et al., 2014). Most impressively, MMSKIP-GRAM-A reaches the performance level of the Silberer and Lapata (2014) model on their SemSim and VisSim data sets, despite the fact that the latter has full visual-data coverage and uses attribute-based image representations, requiring supervised learning of attribute classifiers, that achieve performance in the semantic tasks comparable or higher than that of our CNN features (see Table 3 in Silberer and Lapata (2014)). Finally, if the multimodal models (unsurprisingly) bring about a large performance gain over the purely linguistic model on visual similarity, the improvement is consistently large also for the other benchmarks, confirming that multimodality leads to better semantic models in general, that can help in capturing different types of similarity (general relatedness, strictly taxonomic, perceptual).\nWhile we defer to further work a better understanding of the relation between multimodal grounding and different similarity relations, Table 2 provides qualitative insights on how injecting visual information changes the structure of semantic space. The top SKIP-GRAM neighbours of donuts are places where you might encounter them, whereas the multimodal models relate them to other take-away food, ranking visually-similar pizzas at the top. The owl example shows how multimodal\nmodels pick taxonomically closer neighbours of concrete objects, since often closely related things also look similar (Bruni et al., 2014). In particular, both multimodal models get rid of squirrels and offer other birds of prey as nearest neighbours. No direct visual evidence was used to induce the embeddings of the remaining words in the table, that are thus influenced by vision only by propagation. The subtler but systematic changes we observe in such cases suggest that this indirect propagation is not only non-damaging with respect to purely linguistic representations, but actually beneficial. For the concrete mural concept, both multimodal models rank paintings and portraits above less closely related sculptures (they are not a form of painting). For tobacco, both models rank cigarettes and cigar over coffee, and MMSKIP-GRAM-B avoids the arguably less common \u201ccrop\u201d sense cued by corn. The last two examples show how the multimodal models turn up the embodiment level in their representation of abstract words. For depth, their neighbours suggest a concrete marine setup over the more abstract measurement sense picked by the MMSKIP-GRAM neighbours. For chaos, they rank a demon, that is, a concrete agent of chaos at the top, and replace the more abstract notion of despair with equally gloomy but more imageable shadows and destruction (more on abstract words below)."}, {"heading": "5.2 Zero-shot image labeling and retrieval", "text": "The multimodal representations induced by our models should be better suited than purely textbased vectors to label or retrieve images. In particular, given that the quantitative and qualitative results collected so far suggest that the models propagate visual information across words, we apply them to image labeling and retrieval in the challenging zeroshot setup (see Section 2 above).3 3We will refer here, for conciseness\u2019 sake, to image labeling/retrieval, but, as our visual vectors are aggregated representations of images, the tasks we\u2019re modeling consist, more precisely, in labeling a set of pictures denoting the same object and retrieving the corresponding set given the name of the object.\nSetup We take out as test set 25% of the 5.1K words we have visual vectors for. The multimodal models are re-trained without visual vectors for these words, using the same hyperparameters as above. For both tasks, the search for the correct word label/image is conducted on the whole set of 5.1K word/visual vectors.\nIn the image labeling task, given a visual vector representing an image, we map it onto word space, and label the image with the word corresponding to the nearest vector. To perform the vision-tolanguage mapping, we train a Ridge regression by 5- fold cross-validation on the test set (for SKIP-GRAM only, we also add the remaining 75% of word-image vector pairs used in estimating the multimodal models to the Ridge training data).4\nIn the image retrieval task, given a linguistic/multimodal vector, we map it onto visual space, and retrieve the nearest image. For SKIP-GRAM, we use Ridge regression with the same training regime as for the labeling task. For the multimodal models, since maximizing similarity to visual representations is already part of their training objective, we do not fit an extra mapping function. For MMSKIPGRAM-A, we directly look for nearest neighbours of the learned embeddings in visual space. For MMSKIP-GRAM-B, we use the Mu\u2192v mapping function induced while learning word embeddings.\nResults In image labeling (Table 3) SKIP-GRAM is outperformed by both multimodal models, confirming that these models produce vectors that are directly applicable to vision tasks thanks to visual propagation. The most interesting results however are achieved in image retrieval (Table 4), which is essentially the task the multimodal models have been implicitly optimized for, so that they could be applied to it without any specific training. The strategy of directly querying for the nearest visual vectors of the MMSKIP-GRAM-A word embeddings works remarkably well, outperforming on the higher ranks SKIP-GRAM, which requires an ad-hoc mapping function. This suggests that the multimodal\n4We use one fold to tune Ridge \u03bb, three to estimate the mapping matrix and test in the last fold. To enforce strict zero-shot conditions, we exclude from the test fold labels occurring in the LSVRC2012 set that was employed to train the CNN of Krizhevsky et al. (2012), that we use to extract visual features.\nembeddings we are inducing, while general enough to achieve good performance in the semantic tasks discussed above, encode sufficient visual information for direct application to image analysis tasks. This is especially remarkable because the word vectors we are testing were not matched with visual representations at model training time, and are thus multimodal only by propagation. The best performance is achieved by MMSKIP-GRAM-B, confirming our claim that its Mu\u2192v matrix acts as a multimodal mapping function."}, {"heading": "5.3 Abstract words", "text": "We have already seen, through the depth and chaos examples of Table 2, that the indirect influence of visual information has interesting effects on the representation of abstract terms. The latter have received little attention in multimodal semantics, with Hill and Korhonen (2014) concluding that abstract nouns, in particular, do not benefit from propagated perceptual information, and their representation is even harmed when such information is forced on them (see Figure 4 of their paper). Still, embodied theories of cognition have provided considerable evidence that abstract concepts are also grounded in the senses (Barsalou, 2008; Lakoff and Johnson, 1999). Since the word representations produced by MMSKIP-GRAM-A, including those pertaining to abstract concepts, can be directly used to search for near images in visual space, we decided to verify, experimentally, if these near images (of concrete things) are relevant not only for concrete words, as\nexpected, but also for abstract ones, as predicted by embodied views of meaning.\nMore precisely, we focused on the set of 200 words that were sampled across the USF norms concreteness spectrum by Kiela et al. (2014) (2 words had to be excluded for technical reasons). This set includes not only concrete (meat) and abstract (thought) nouns, but also adjectives (boring), verbs (teach), and even grammatical terms (how). Some words in the set have relatively high concreteness ratings, but are not particularly imageable, e.g.: hot, smell, pain, sweet. For each word in the set, we extracted the nearest neighbour picture of its MMSKIP-GRAM-A representation, and matched it with a random picture. The pictures were selected from a set of 5,100, all labeled with distinct words (the picture set includes, for each of the words associated to visual information as described in Section 4, the nearest picture to its aggregated visual representation). Since it is much more common for concrete than abstract words to be directly represented by an image in the picture set, when searching for the nearest neighbour we excluded the picture labeled with the word of interest, if present (e.g., we excluded the picture labeled tree when picking the nearest neighbour of the word tree). We ran a CrowdFlower5 survey in which we presented each test word with the two associated images (randomizing presentation order of nearest and random picture), and asked subjects which of the two pictures they found more closely related to the word. We collected minimally 20 judgments per word. Subjects showed large agreement (median proportion of majority choice at 90%), confirming that they understood the task and behaved consistently.\nWe quantify performance in terms of proportion of words for which the number of votes for the nearest neighbour picture is significantly above chance according to a two-tailed binomial test. We set significance at p<0.05 after adjusting all p-values with the Holm correction for running 198 statistical tests. The results in Table 5 indicate that, in about half the cases, the nearest picture to a word MMSKIPGRAM-A representation is meaningfully related to the word. As expected, this is more often the case for concrete than abstract words. Still, we also observe a\n5http://www.crowdflower.com\nsignificant preference for the model-predicted nearest picture for about one fourth of the abstract terms. Whether a word was exposed to direct visual evidence during training is of course making a big difference, and this factor interacts with concreteness, as only two abstract words were matched with images during training.6 When we limit evaluation to word representations that were not exposed to pictures during training, the difference between concrete and abstract terms, while still large, becomes less dramatic than if all words are considered.\nFigure 2 shows four cases in which subjects expressed a strong preference for the nearest visual neighbour of a word. Freedom, god and theory are strikingly in agreement with the view, from embodied theories, that abstract words are grounded in rel-\n6In both cases, the images actually depict concrete senses of the words: a memory board for memory and a stop sign for stop.\nevant concrete scenes and situations. The together example illustrates how visual data might ground abstract notions in surprising ways. For all these cases, we can borrow what Howell et al. (2005) say about visual propagation to abstract words (p. 260):\nIntuitively, this is something like trying to explain an abstract concept like love to a child by using concrete examples of scenes or situations that are associated with love. The abstract concept is never fully grounded in external reality, but it does inherit some meaning from the more concrete concepts to which it is related.\nOf course, not all examples are good: the last column of Figure 2 shows cases with no obvious relation between words and visual neighbours (subjects preferred the random images by a large margin).\nThe multimodal vectors we induce also display an interesting intrinsic property related to the hypothesis that grounded representations of abstract words are more complex than for concrete ones, since abstract concepts relate to varied and composite situations (Barsalou and Wiemer-Hastings, 2005). A natural corollary of this idea is that visually-grounded representations of abstract concepts should be more diverse: If you think of dogs, very similar images of specific dogs will come to mind. You can also imagine the abstract notion of freedom, but the nature of the related imagery will be much more varied. Recently, Kiela et al. (2014) have proposed to measure abstractness by exploiting this very same intuition. However, they rely on manual annotation of pictures via Google Images and define an ad-hoc measure of image dispersion. We conjecture that the representations naturally induced by our models display a similar property. In particular, the entropy of our multimodal vectors, being an expression of how varied the information they encode is, should correlate with the degree of abstractness of the corresponding words. As Figure 3(a) shows, there is indeed a difference in entropy between the most concrete (meat) and most abstract (hope) words in the Kiela et al. set.\nTo test the hypothesis quantitatively, we measure the correlation of entropy and concreteness on the 200 words in the Kiela et al. (2014) set.7 Figure 3(b) shows that the entropies of both the 7Since the vector dimensions range over the real number line, we calculate entropy on vectors that are unit-normed after adding a small constant insuring all values are positive.\nMMSKIP-GRAM-A representations and those generated by mapping MMSKIP-GRAM-B vectors onto visual space (MMSKIP-GRAM-B*) achieve very high correlation (but, interestingly, not MMSKIPGRAM-B). This is further evidence that multimodal learning is grounding the representations of both concrete and abstract words in meaningful ways."}, {"heading": "6 Conclusion", "text": "We introduced two multimodal extensions of SKIPGRAM. MMSKIP-GRAM-A is trained by directly optimizing the similarity of words with their visual representations, thus forcing maximum interaction between the two modalities. MMSKIP-GRAM-B includes an extra mediating layer, acting as a crossmodal mapping component. The ability of the models to integrate and propagate visual information resulted in word representations that performed well in both semantic and vision tasks, and could be used as input in systems benefiting from prior visual knowledge (e.g., caption generation). Our results with abstract words suggest the models might also help in tasks such as metaphor detection, or even retrieving/generating pictures of abstract concepts. Their incremental nature makes them well-suited for cognitive simulations of grounded language acquisition, an avenue of research we plan to explore further."}, {"heading": "Acknowledgments", "text": "We thank Adam Liska, Tomas Mikolov, the reviewers and the NIPS 2014 Learning Semantics audience. We were supported by ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES)."}], "references": [{"title": "Strudel: A distributional semantic model based on properties and types", "author": ["Baroni et al.2010] Marco Baroni", "Eduard Barbu", "Brian Murphy", "Massimo Poesio"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Situating abstract concepts", "author": ["Barsalou", "Katja Wiemer-Hastings"], "venue": "Grounding Cognition: The Role of Perception and Action in Memory,", "citeRegEx": "Barsalou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barsalou et al\\.", "year": 2005}, {"title": "Distributional semantics in Technicolor", "author": ["Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam Khanh Tran"], "venue": "In Proceedings of ACL,", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Vector space models of lexical meaning", "author": ["Stephen Clark"], "venue": "Handbook of Contemporary Semantics,", "citeRegEx": "Clark.,? \\Q2015\\E", "shortCiteRegEx": "Clark.", "year": 2015}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Lia-Ji Li", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Describing objects by their attributes", "author": ["Farhadi et al.2009] Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Farhadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Visual information in semantic representation", "author": ["Feng", "Lapata2010] Yansong Feng", "Mirella Lapata"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Feng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2010}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Interpretable semantic vectors from a joint model of brain-and textbased meaning", "author": ["Fyshe et al.2014] Alona Fyshe", "Partha P Talukdar", "Brian Murphy", "Tom M Mitchell"], "venue": "Proceedings of ACL,", "citeRegEx": "Fyshe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2014}, {"title": "Symbol grounding and meaning: A comparison of high-dimensional and embodied theories of meaning", "author": ["Glenberg", "Robertson2000] Arthur Glenberg", "David Robertson"], "venue": "Journal of Memory and Language,", "citeRegEx": "Glenberg et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Glenberg et al\\.", "year": 2000}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what I mean", "author": ["Hill", "Korhonen2014] Felix Hill", "Anna Korhonen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation. http://arxiv.org/abs/arXiv:1408.3456", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "A model of grounded language acquisition: Sensorimotor features improve lexical and grammatical learning", "author": ["Howell et al.2005] Steve Howell", "Damian Jankowicz", "Suzanna Becker"], "venue": "Journal of Memory and Language,", "citeRegEx": "Howell et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Howell et al\\.", "year": 2005}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia et al.2014] Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Armand Joulin", "Li Fei-Fei"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Kiela", "Bottou2014] Douwe Kiela", "L\u00e9on Bottou"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Improving multimodal representations using image dispersion: Why less is sometimes more", "author": ["Kiela et al.2014] Douwe Kiela", "Felix Hill", "Anna Korhonen", "Stephen Clark"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel"], "venue": "In Proceedings of the NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Thought", "author": ["Lakoff", "Johnson1999] George Lakoff", "Mark Johnson"], "venue": null, "citeRegEx": "Lakoff et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lakoff et al\\.", "year": 1999}, {"title": "Is this a wampimuk? crossmodal mapping between distributional semantics and the visual world", "author": ["Elia Bruni", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Mao et al.2014] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan Yuille"], "venue": "In Proceedings of the NIPS Deep Learning and Representation", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Oquab et al.2014] Maxime Oquab", "Leon Bottou", "Ivan Laptev", "Josef Sivic"], "venue": "In Proceedings of CVPR", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "Minds, Brains and Science", "author": ["John Searle"], "venue": null, "citeRegEx": "Searle.,? \\Q1984\\E", "shortCiteRegEx": "Searle.", "year": 1984}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["Sivic", "Zisserman2003] Josef Sivic", "Andrew Zisserman"], "venue": "In Proceedings of ICCV,", "citeRegEx": "Sivic et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sivic et al\\.", "year": 2003}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Milind Ganjoo", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["Turney et al.2011] Peter Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Turney et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "Labeling images with a computer game", "author": ["von Ahn", "Dabbish2004] Luis von Ahn", "Laura Dabbish"], "venue": "In Proceedings of CHI,", "citeRegEx": "Ahn et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2004}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Weston et al.2010] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 26, "context": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account.", "startOffset": 33, "endOffset": 56}, {"referenceID": 5, "context": "DSMs have been very effectively applied to a variety of semantic tasks (Clark, 2015; Mikolov et al., 2013b; Turney and Pantel, 2010).", "startOffset": 71, "endOffset": 132}, {"referenceID": 13, "context": "However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000).", "startOffset": 118, "endOffset": 146}, {"referenceID": 30, "context": "However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000).", "startOffset": 118, "endOffset": 146}, {"referenceID": 3, "context": "MDSMs outperform state-of-the-art text-based approaches, not only in tasks that directly require access to visual knowledge (Bruni et al., 2012), but", "startOffset": 124, "endOffset": 144}, {"referenceID": 4, "context": "also on general semantic benchmarks (Bruni et al., 2014; Silberer and Lapata, 2014).", "startOffset": 36, "endOffset": 83}, {"referenceID": 26, "context": "The models build upon the very effective skip-gram approach of Mikolov et al. (2013a), that constructs vector representations by learning, incrementally, to predict the linguistic contexts in which target words occur in a corpus.", "startOffset": 63, "endOffset": 86}, {"referenceID": 3, "context": "Bruni et al. (2014) propose a straightforward approach to MDSM induction, where text- and image-based vectors for the same words are constructed independently, and then", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "These and related systems take a twostage approach to derive multimodal spaces (unimodal induction followed by fusion), and they are only tested on concepts for which both textual and visual labeled training data are available (the pioneering model of Feng and Lapata (2010) did learn from text and images jointly using Topic Models, but was shown to be empirically weak by Bruni et al. (2014)).", "startOffset": 374, "endOffset": 394}, {"referenceID": 7, "context": "timodal model based on simple recurrent networks (Elman, 1990), focusing on grounding propagation from early-acquired concrete words to a larger vocabulary.", "startOffset": 49, "endOffset": 62}, {"referenceID": 11, "context": "brain signal vectors (Fyshe et al., 2014).", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "beling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective.", "startOffset": 7, "endOffset": 72}, {"referenceID": 24, "context": "beling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective.", "startOffset": 7, "endOffset": 72}, {"referenceID": 33, "context": "beling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective.", "startOffset": 7, "endOffset": 72}, {"referenceID": 26, "context": "We start by reviewing the standard SKIP-GRAM model of Mikolov et al. (2013a), in the version", "startOffset": 54, "endOffset": 77}, {"referenceID": 37, "context": "We maximize similarity through a max-margin framework commonly used in models connecting language and vision (Weston et al., 2010; Frome et al., 2013).", "startOffset": 109, "endOffset": 150}, {"referenceID": 10, "context": "We maximize similarity through a max-margin framework commonly used in models connecting language and vision (Weston et al., 2010; Frome et al., 2013).", "startOffset": 109, "endOffset": 150}, {"referenceID": 6, "context": "words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus and have concreteness score \u2265 0.", "startOffset": 37, "endOffset": 56}, {"referenceID": 17, "context": "To construct the visual representation of a word, we sample 100 pictures from its ImageNet entry, and extract a 4096-dimensional vector from each picture using the Caffe toolkit (Jia et al., 2014), together with the pre-trained convolutional neural network of Krizhevsky et al.", "startOffset": 178, "endOffset": 196}, {"referenceID": 6, "context": "words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus and have concreteness score \u2265 0.5 according to Turney et al. (2011). On average, about 5% tokens in the text corpus are associated to a visual representation.", "startOffset": 38, "endOffset": 165}, {"referenceID": 6, "context": "words that have an entry in ImageNet (Deng et al., 2009), occur at least 500 times in the corpus and have concreteness score \u2265 0.5 according to Turney et al. (2011). On average, about 5% tokens in the text corpus are associated to a visual representation. To construct the visual representation of a word, we sample 100 pictures from its ImageNet entry, and extract a 4096-dimensional vector from each picture using the Caffe toolkit (Jia et al., 2014), together with the pre-trained convolutional neural network of Krizhevsky et al. (2012). The vector corresponds", "startOffset": 38, "endOffset": 541}, {"referenceID": 3, "context": "Specifically, we test on general relatedness (MEN, Bruni et al. (2014), 3K pairs), e.", "startOffset": 51, "endOffset": 71}, {"referenceID": 14, "context": "larity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.", "startOffset": 20, "endOffset": 39}, {"referenceID": 14, "context": "larity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.", "startOffset": 20, "endOffset": 85}, {"referenceID": 14, "context": "larity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.5K pairs), e.g., pickles are similar to onions, as well as visual similarity (VisSim, Silberer and Lapata (2014), same", "startOffset": 20, "endOffset": 201}, {"referenceID": 29, "context": "lutional neural network (Oquab et al., 2014).", "startOffset": 24, "endOffset": 44}, {"referenceID": 3, "context": "We also test the vectors that performed best in the evaluation of Bruni et al. (2014), based on textual features", "startOffset": 66, "endOffset": 86}, {"referenceID": 0, "context": "pata (2014), obtained with a stacked-autoencoders architecture run on textual features extracted from Wikipedia with the Strudel algorithm (Baroni et al., 2010) and attribute-based visual features (Farhadi et al.", "startOffset": 139, "endOffset": 160}, {"referenceID": 8, "context": ", 2010) and attribute-based visual features (Farhadi et al., 2009) extracted from ImageNet.", "startOffset": 44, "endOffset": 66}, {"referenceID": 14, "context": "41 (Hill et al., 2014).", "startOffset": 3, "endOffset": 22}, {"referenceID": 0, "context": "8), achieved by Baroni et al. (2014) with a corpus 3 larger than ours and extensive tuning.", "startOffset": 16, "endOffset": 37}, {"referenceID": 4, "context": "also look similar (Bruni et al., 2014).", "startOffset": 18, "endOffset": 38}, {"referenceID": 22, "context": "To enforce strict zero-shot conditions, we exclude from the test fold labels occurring in the LSVRC2012 set that was employed to train the CNN of Krizhevsky et al. (2012), that we use to extract visual features.", "startOffset": 146, "endOffset": 171}, {"referenceID": 19, "context": "More precisely, we focused on the set of 200 words that were sampled across the USF norms concreteness spectrum by Kiela et al. (2014) (2 words had to be excluded for technical reasons).", "startOffset": 115, "endOffset": 135}, {"referenceID": 19, "context": "Table 5: Subjects\u2019 preference for nearest visual neighbour of words in Kiela et al. (2014) vs.", "startOffset": 71, "endOffset": 91}, {"referenceID": 16, "context": "For all these cases, we can borrow what Howell et al. (2005) say about", "startOffset": 40, "endOffset": 61}, {"referenceID": 19, "context": "Recently, Kiela et al. (2014) have proposed to measure abstractness by exploiting this very same intuition.", "startOffset": 10, "endOffset": 30}, {"referenceID": 19, "context": "sure the correlation of entropy and concreteness on the 200 words in the Kiela et al. (2014) set.", "startOffset": 73, "endOffset": 93}, {"referenceID": 19, "context": "(b) Spearman \u03c1 between concreteness and various measures on the Kiela et al. (2014) set.", "startOffset": 64, "endOffset": 84}], "year": 2015, "abstractText": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.", "creator": "LaTeX with hyperref package"}}}