{"id": "1501.02702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Max-Cost Discrete Function Evaluation Problem under a Budget", "abstract": "we propose novel methods emphasizing max - cost discrete function evaluation problem ( dfep ) under budget constraints. we get motivated by applications such as clinical diagnosis where a patient is treated to a sequence costly ( possibly expensive ) tests before a commitment when made. our goal is often develop strategies for minimizing max - costs. the problem is known to be not hard and greedy methods based the specialized indicator functions have been proposed. we write a broad class of \\ emph { admissible } impurity functions that admit monomials, classes of polynomials, and hinge - loss functions that allow easily flexible impurity families with provably optimal approximation bounds. this flexibility is important for datasets when max - cost can be overly sensitive to \" outliers. \" outliers bias max - cost to a few examples that require a large number of tests for classification. we expect admissible functions that allow for accuracy - cost trade - overs reductions result in $ o ( \\ compute n ) $ guarantees of the optimal cost among trees including increased conditional accuracy ranges.", "histories": [["v1", "Mon, 12 Jan 2015 16:33:47 GMT  (491kb,D)", "http://arxiv.org/abs/1501.02702v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["feng nan", "joseph wang", "venkatesh saligrama"], "accepted": false, "id": "1501.02702"}, "pdf": {"name": "1501.02702.pdf", "metadata": {"source": "CRF", "title": "Max-Cost Discrete Function Evaluation Problem under a Budget", "authors": ["Feng Nan", "Joseph Wang", "Venkatesh Saligrama"], "emails": ["fnan@bu.edu", "joewang@bu.edu", "srv@bu.edu"], "sections": [{"heading": null, "text": "We propose novel methods for maxcost Discrete Function Evaluation Problem (DFEP) under budget constraints. We are motivated by applications such as clinical diagnosis where a patient is subjected to a sequence of (possibly expensive) tests before a decision is made. Our goal is to develop strategies for minimizing max-costs. The problem is known to be NP hard and greedy methods based on specialized impurity functions have been proposed. We develop a broad class of admissible impurity functions that admit monomials, classes of polynomials, and hinge-loss functions that allow for flexible impurity design with provably optimal approximation bounds. This flexibility is important for datasets when max-cost can be overly sensitive to \u201coutliers.\u201d Outliers bias max-cost to a few examples that require a large number of tests for classification. We design admissible functions that allow for accuracy-cost trade-off and result in O(log n) guarantees of the optimal cost among trees with corresponding classification accuracy levels."}, {"heading": "1 Introduction", "text": "In many applications such as clinical diagnosis, monitoring, and web search, a patient, entity or query is subjected to a sequence of tests before a decision or prediction is made. Tests can be expensive and often complementary, namely, the outcome of one test may render another redundant. The goal in these scenarios is to minimize total test costs with negligible loss in diagnostic performance.\nWe propose to formulate this problem as an instance of the Discrete Function Evaluation Problem (DFEP). Under this framework, we seek to learn a decision tree which correctly classifies data while minimizing the cost of testing. We then propose methods to trade-off accuracy and costs.\nAn instance of the problem is defined as I = (S,C, T, c); Here S = {s1, . . . , sn} is the set of n objects; C = {C1, . . . , Cm} is a partition of S into m classes; T is a set of tests; c is a cost function that assigns a cost c(t) \u2265 0 for each test t \u2208 T . Applying test t \u2208 T on object s \u2208 S will output a discrete value t(s) in a finite set of possible outcomes {1, . . . , lt}. T is assumed to be complete in the sense that for any distinct si, sj \u2208 S there exists a t \u2208 T such that t(si) 6= t(sj) so they can be distinguished by t. Given an instance of the DFEP, the goal is to build a testing procedure that uses tests in T to determine the class of an unknown object. Formally, any testing procedure can be represented by a decision tree, where every internal node is associated with a test and objects are directed from the root to the corresponding leaves based on the test outcomes at each node. Given instance I and decision tree D, the testing cost of s \u2208 S, denoted as cost(D, s), is the sum of all costs incurred along the root-to-leaf path in D traced by s. We define the total cost as\nCostW (D) = max s\u2208S cost(D, s)\nThis is known as the max-cost testing problem in the DFEP literature and has independently received significant attention [Cicalese et al., 2014, Saettler et al., 2014, Moshkov, 2010, Bellala et al., 2012] due to the fact that in real world problems, the prior probability used to compute the expected testing cost is either unavailable or inaccurate. Another motivation stems from time-critical applications, such as emergency response [Bellala et al., 2012], where violation of a time-constraint may lead to unacceptable consequences.\nar X\niv :1\n50 1.\n02 70\n2v 1\n[ cs\n.L G\n] 1\n2 Ja\nn 20\n15\nIn this paper we propose novel approaches and themes for the max-cost DFEP problem. It is now well-known [Cicalese et al., 2014] that O(log n) is the best approximation factor for DFEP unless P = NP . Greedy methods that achieve O(log n) approximation guarantee have been proposed [Cicalese et al., 2014, Saettler et al., 2014, Moshkov, 2010]. These methods often rely on judiciously engineering so called impurity functions that are surprisingly effective in realizing \u201coptimal\u201d O(log n) guarantees. Authors in [Cicalese et al., 2014, Moshkov, 2010, Saettler et al., 2014] describe impurity functions based on the notion of Pairs, while the authors in [Bellala et al., 2012] describe more complex impurity functions but require distributional assumptions.\nIn contrast, we propose a broad class of admissible functions such that any function from this class can be chosen as an impurity function with an O(log n) approximation guarantee. Our admissible functions are in essence positive, monotone supermodular functions and admit not only pairs, monomials, classes of polynomials, but also hingeloss functions.\nWe propose new directions for the max-cost DFEP problem. In contrast to the current emphasis on correct classification, we propose to deliberately tradeoff cost with accuracy. This perspective can be justified under various scenarios. First, max-cost is overly sensitive to \u201coutliers,\u201d namely, a few instances require prohibitively many tests for correct classification. In these situations max-cost is not representative of most of the data and is biased towards a small subset of objects. Consequently, censoring those few \u201coutliers\u201d is meaningful from the perspective that max-cost applies to all but few examples. Second many applications have hard cost constraints that supersede correct classification of the entire data set and the goal is a tree that guarantees these cost constraints while minimizing errors.\nOur proposed admissible functions are sufficiently general and allows for trading accuracy for cost. In particular we develop methods with O(log n) guarantees of the optimal cost among trees with a corresponding classification accuracy level. Moreover, we show empirically on a number of examples that selection of impurity functions plays an important role in this trade-off. In particular some admissible functions, such as hinge-loss are particularly wellsuited for low-budgets while others are preferable in high-budget scenarios.\nApart from the related approaches already described above, our work is also related to those that generally deal with expected costs [Golovin and Krause, 2011, Golovin et al., 2010, Bellala et al., 2012] or related problems such as sub-modular set coverage problem [Guillory and Bilmes, 2010]. At a conceptual level the main difference in [Guillory and Bilmes, 2010, Golovin and Krause, 2011, Golovin et al., 2010, Bellala et al., 2012] is in the way tests are chosen. Unlike our approach these methods employ utility functions in the policy space that acts on a sequence of observations. [Golovin and Krause, 2011] develops the notion of adaptive submodularity and has applied it for automated diagnosis. The proposed adaptive greedy algorithm can handle multiple classes/ test outcomes and arbitrary test costs but the approximation factor for the max-cost depends on the prior probability and can be very large in adversarial situations. A popular class of related approximation algorithms is generalized binary search (GBS) [Dasgupta, 2004, Kosaraju et al., 1999, Nowak, 2008]. A special case of this problem is where each object belongs to a distinct class and is known as object identification problem [Chakaravarthy et al., 2011] or pool-based active learning [Dasgupta, 2004]. When tests are restricted to binary outcomes and uniform test costs, O(log(1/pmin)) approximation, where pmin is the minimum probability of any single object [Dasgupta, 2004] can be obtained. Alternatively [Gupta et al., 2010] provides an algorithm which leads to an O(log n) approximation factor for the optimal expected cost with arbitrary test costs and binary test outcomes. With respect to the max-cost, [Hanneke, 2006] gave a O(log n) approximation for multiway tests and arbitrary test costs.\nOrganization: We present a greedy algorithm in Section 2 which we show under general assumptions on the impurity function leads to an O(log n) approximation of the optimal tree. We examine the assumptions on impurity functions and use them to define a class of admissible impurity functions in Section 3. Following this, we generalize from the errorfree case to trade-off between max-cost and error in Section 4. Finally, we demonstrate performance of the greedy algorithm on real world data sets in Section 5 and show the advantage of different impurity functions along with the trade-off between error and max-cost."}, {"heading": "2 Greedy Algorithm and Analysis", "text": "In this section, we present an analysis of the greedy algorithm GreedyTree. We first show that GreedyTree yields a tree whose max-cost is within O(log n) of the optimal max-cost for any DFEP. This bound on max-cost holds for any impurity function that satisfies a very general criteria as opposed to a fixed impurity function. In Section 3 we examine the assumptions on the impurity functions and present multiple examples of impurity functions for which this approximation bound holds.\nBefore beginning the analysis, we first define the following terms: for a given impurity function F , F (G) is the impurity function on the set of objects G; DF is the family of decision trees with F (L) = 0 for any of its leaf L; OPT (S) is the minimum maxcost among all trees in DF for the given input set of objects S; CostF (S) is the max-cost of the tree constructed by GreedyTree based on impurity function F .\nAlgorithm 1 GreedyTree\n1: procedure GreedyTree(G,T) 2: if F (G) = 0 then return\n3: for each test t \u2208 T do 4: Compute R(t) := max\ni\u2208outcomes c(t) F (G)\u2212F (Git) ,\n5: where Git is the set of objects in G 6: that has outcome i for test t. 7: t\u0302\u2190 argmintR(t) 8: T \u2190 T\\{t\u0302} 9: for each outcome i of t\u0302 do\n10: GreedyTree(Git, T )\nFor simplicity, we assume the impurity function takes on integer values and outcome-independent test costs. Note that integer valued impurity functions is not a limitation because of the discrete (finite) nature of the problem - one can always scale any rational-valued impurity function to make it integer-valued. Similarly, it can be easily shown that our result extends to the outcome-dependent cost setting considered in [Saettler et al., 2014] as well.\nGiven a DFEP, GreedyTree greedily chooses the test with the largest worst-case impurity reduction until all leaves are pure, i.e. impurity equals zero. Let \u03c4 be the first test selected by GreedyTree. By definition of the max-cost,\nCostF (S) OPT (S) = c(\u03c4) + max i CostF (S\ni \u03c4 )\nOPT (S) ,\nwhere Si\u03c4 is the set of objects in S that has out-\ncome i for test \u03c4 . Let q be such that CostF (S q \u03c4 ) = max i CostF (S i \u03c4 ). We first provide a lemma to lower bound the optimal cost, which will later be used to prove a bound on the cost of the tree.\nLemma 2.1. Let F be monotone and supermodular, and \u03c4 is the first test chosen by GreedyTree on the set of objects S, then\nc(\u03c4)F (S)/(F (S)\u2212 F (Sq\u03c4 )) \u2264 OPT (S). Proof. Let D\u2217 \u2208 DF be a tree with optimal maxcost. Let v be an arbitrarily chosen internal node in D\u2217, let \u03b3 be the test associated with v and let R \u2286 S be the set of objects associated with the leaves of the subtree rooted at v. Let i be such that c(\u03c4)/(F (S) \u2212 F (Si\u03c4 )) is maximized and j be such that c(\u03b3)/(F (S) \u2212 F (Si\u03b3)) is maximized. We then have:\nc(\u03c4) F (S)\u2212 F (Sq\u03c4 ) \u2264 c(\u03c4) F (S)\u2212 F (Si\u03c4 ) \u2264 c(\u03b3) F (S)\u2212 F (Sj\u03b3) \u2264 c(\u03b3) F (R)\u2212 F (Rj\u03b3) . (1)\nThe first inequality follows from the definition of i. The second inequality follows from the greedy choice at the root. To show the last inequality, we have to show F (S) \u2212 F (Sj\u03b3) \u2265 F (R) \u2212 F (Rj\u03b3). This follows from the fact that Sj\u03b3 \u222a R \u2286 S and Rj\u03b3 = S j \u03b3 \u2229 R and therefore F (S) \u2265 F (Sj\u03b3 \u222a R) \u2265 F (Sj\u03b3)+F (R)\u2212F (Rj\u03b3), where the first inequality follows from monotonicity and the second follows from the definition of supermodularity.\nFor a node v, let S(v) be the set of objects associated with the leaves of the subtree rooted at v. Let v1, v2, . . . , vp be a root-to-leaf path on D\n\u2217 as follows: v1 is the root of the tree, and for each i = 1, . . . , p\u22121 the node vi+1 is a child of vi associated with the branch of j that maximizes c(ti)/(F (S) \u2212 F (Sjti)), where ti is the test associated with vi. If follows from (1) that\n[F (S(vi))\u2212 F (S(vi+1))]c(\u03c4) F (S)\u2212 F (Sq\u03c4 ) \u2264 cti . (2)\nSince the cost of the path from v1 to vp is no larger than the max-cost of the D\u2217, we have that\nOPT (S) \u2265 p\u22121\u2211 i=1 cti\n\u2265 c(\u03c4) F (S)\u2212 F (Sq\u03c4 ) p\u22121\u2211 i=1 (F (S(vi))\u2212 F (S(vi+1)) = c(\u03c4)(F (S)\u2212 F (S(vp))\nF (S)\u2212 F (Sq\u03c4 ) =\nc(\u03c4)F (S)\nF (S)\u2212 F (Sq\u03c4 ) .\nUsing Lemma 2.1, we can now state the main theorem of this section which bounds the cost of the greedily constructed tree.\nTheorem 2.2. GreedyTree constructs a decision tree achieving O(log n)-factor approximation of the optimal max-cost in DF on the set S of n objects if F is non-negative, monotone, supermodular with log(F (S)) = O(log n).\nProof.\nCostF (S) OPT (S) = c(\u03c4) + CostF (S\nq \u03c4 )\nOPT (S) (3)\n\u2264 c(\u03c4) OPT (S) + CostF (S\nq \u03c4 )\nOPT (Sq\u03c4 ) (4)\n\u2264 F (S)\u2212 F (S q \u03c4 ) F (S) + CostF (S q \u03c4 ) OPT (Sq\u03c4 ) (5)\n\u2264 log( F (S) F (Sq\u03c4 ) ) + log(F (Sq\u03c4 )) + 1 (6)\n= log(F (S)) + 1 = O(log n). (7)\nThe inequality in (4) follows from the fact that OPT (S) \u2265 OPT (Sq\u03c4 ). (5) follows from Lemma 2.1. The first term in (6) follows from the inequality xx+1 \u2264 log(1 + x) for x > \u22121 and the second term follows from the induction hypothesis that for each G \u2282 S, CostF (G)/OPT (G) \u2264 log(F (G)) + 1. If F (G) = 0 for some set of objects G, we define CostF (G)/OPT (G) = 1.\nWe can verify the base case of the induction as follows. if F (G) = \u03b2, which is the smallest non-zero impurity of F on subsets of objects S, we claim that the optimal decision tree chooses the test with the smallest cost among those that can reduce the impurity function F :\nOPT (G) = min t|F (Git)=0,\u2200i\u2208outcomes c(t).\nSuppose otherwise, the optimal tree chooses first a test t with a child node G\u2032 such that F (G\u2032) = \u03b2 and later chooses another test t\u2032 such that all the child nodes of G\u2032 by t\u2032 has zero impurity, then t\u2032 could have been chosen in the first place to reduce all child nodes of G to zero impurity by supermodularity of F and therefore this cannot be the optimal ordering of tests. On the other hand, R(t) = \u221e in GreedyTree for those test t that cannot reduce impurity and R(t) = c(t) for those tests that can. So the algorithm would pick the test among those that can reduce impurity and have the smallest cost. Thus, we have shown that CostF (G)/OPT (G) = log(F (G)) + 1 = 1 for the base case.\nGiven that P 6= NP , the optimal order approximation for the DFEP problem is O(log n), which is achieved by GreedyTree. This approximation is not dependent on a particular impurity function, but instead holds for any function which satisfies the assumptions. In Section 3, we define a family of impurity functions that satisfy these assumptions.\n3 Admissible Functions\nA fundamental element of constructing decision trees is the impurity function, which measures the disagreement of labels between a set of objects. Many impurity functions have been proposed for constructing decision trees, and the choice of impurity function can have a significant impact on the performance of the tree. In this section we examine the assumptions placed on the impurity function by Lemma 2.1 and Theorem 2.2 which we use to define a class of functions we call admissible impurity functions and provide examples of admissible impurity functions.\nDefinition A function F of a set of objects is admissible if it satisfies the following five properties: (1) Non-negativity: F (G) \u2265 0 for any set of objects G; (2) Purity: F (G) = 0 if G consists of objects of the same class; (3) Monotonicity: F (G) \u2265 F (R),\u2200R \u2286 G; (4) Supermodularty: F (G\u222a j)\u2212F (G) \u2265 F (R\u222a j)\u2212F (R) for any R \u2286 G and object j /\u2208 R; (5) log(F (S)) = O(log n).\nA wide range of functions falls into the class of admissible impurity functions. We propose a general family of polynomial functions which we show is admissible. Given a set of objects G, niG denotes the number of objects in G that belong to class i.\nLemma 3.1. Suppose there are k classes in G. Any polynomial function of n1G, . . . , n k G with non-negative terms such that n1G, . . . , n k G do not appear as singleton terms is admissible. Formally, if\nF (G) = M\u2211 i=1 \u03b3i(n 1 G) pi1(n2G) pi2 . . . (nkG) pik , (8)\nwhere \u03b3i\u2019s are non-negative, pij\u2019s are non-negative integers and for each i there exists at least 2 nonzero pij\u2019s, then F is admissible.\nProof. Properties (1),(2),(3) and (5) are obviously true. To show F is supermodular, suppose R \u2282 G\nand object j\u0302 /\u2208 R and j\u0302 belongs to class j, we have\nF (R \u222a j\u0302)\u2212 F (R) = \u2211 i\u2208Ij \u03b3i[(n 1 R) pi1 . . . (njR + 1) pij . . . (nkR) pik\u2212\n(n1R) pi1 . . . (njR) pij . . . (nkR) pik ] \u2264 \u2211 i\u2208Ij \u03b3i[(n 1 G) pi1 . . . (njG + 1) pij . . . (nkG) pik\u2212\n(n1G) pi1 . . . (njG) pij . . . (nkG) pik ]\n= F (G \u222a j\u0302)\u2212 F (G),\nwhere the first summation index set Ij is the set of terms that involve njR. The inequality follows because (njR + 1) pij can be expanded so the negative term can be canceled, leaving a sum-of-products form for R, which is term-by-term dominated by that of G.\nA special case of polynomial impurity function is the previously proposed Pairs function P (G) [Saettler et al., 2014, Cicalese et al., 2014, Moshkov, 2010]. Two objects (s1, s2) are defined as a pair if they are of different classes, with the Pairs function P (G) equal to the total number of pairs in the set G:\nP (G) = k\u22121\u2211 i=1 k\u2211 j=i+1 niGn j G,\nwhere k is the number of distinct classes in set G.\nCorollary 3.2. The Pairs impurity function is admissible.\nAs a corollary of Theorem 2.2 and Corollary 3.2, we see that O(log n) approximation for Pairs and outcome-dependent cost holds for multiple test outcomes as well, extending the binary outcome setting shown in [Saettler et al., 2014].\nAnother family of admissible impurity functions is the Powers function.\nCorollary 3.3. Powers function\nF (G) = ( k\u2211 i=1 niG) l \u2212 k\u2211 i=1 (niG) l (9)\nis admissible for l = 2, 3, . . . .\nNote Pairs can be viewed as a special case of Powers function when l = 2. An important property of the Powers impurity functions is the fact that for any power l, the function is zero only if the set of objects all belong to the same class. As a result, using any\nof these Powers impurity function in GreedyTree results in an error-free tree with near optimal cost.\nAnother interesting admissible impurity used in Section 4 is the hinged-Pairs function defined:\nP\u03b1(G) = \u2211 i 6=j [[niG \u2212 \u03b1]+[n j G \u2212 \u03b1]+ \u2212 \u03b1 2]+, (10)\nwhere [x]+ = max(x, 0). This function differs from the Powers impurity function due to the fact that for a \u03b1 > 0, the function P\u03b1(G) = 0 need not imply that all objects in G belong to the same class. In the next section, we will discuss how this allows for trees to be constructed incorporating classification error. We include the proof of the following lemma in the Appendix.\nLemma 3.4. In the multi-class setting, P\u03b1(G) is admissible.\nImpurity Function Selection: While all admissible impurity functions enjoy the O(log n) approximation of the optimal max-cost, they lead to different trees depending on the problems. To illustrate this point, consider the toy example in Figure 1. A set G has 30 objects in class 1 (circles) and 30 objects in Class 2 (triangles). Two tests t1 and t2 are available to the algorithm. Test t1 separates 20 objects of Class 2 from the rest of the objects while t2 evenly divides the objects into halves with equal number of objects from Class 1 and Class 2 in either half. Intuitively, t2 is not a useful test from a classification point of view because it does not separate objects based on class at all. This is reflected in the right plot of Figure 1: choosing t2 increases cost but does not reduce classification error while choosing t1 reduces the error to 1 6 . If the impurity function chosen is the Pairs function, test t2 will be chosen due to the fact that Pairs biases towards tests with balanced test outcomes. In contrast, the hingedPairs function leads to test t1, and therefore may be preferable in this case (for more details on this example see the Appendix). Although both impurity functions are admissible and return trees with near optimal guarantees, empirical performance can differ greatly and is strongly dependent on the structure of the data. In practice, we find that choosing the tree with the lowest classification error across a variety of impurity functions yields improved performance compared to a single impurity function strategy."}, {"heading": "4 Trade-off Bounds", "text": "Up to this point, we have focused on constructing error-free trees. Unfortunately, the max-cost criteria is highly sensitive to outliers, and therefore often\nyields trees with unnecessarily large maximum depth to accommodate a small subset of outliers in the data set. Refer to the synthetic experiment in Section 5 for such an example. To overcome the sensitivity to outliers, we present an approach to constructing near optimal trees with non-zero error rates.\nEarly-stopping: Instead of requiring all leaves to have zero impurity (F (L) = 0) in Algorithm 1, we can stop the recursion as soon as all leaves have impurity below a threshold \u03b4 (F (L) \u2264 \u03b4). This will allow error and cost trade-off. Let DF :\u03b4 denote the set of trees with F (L) \u2264 \u03b4 for all leaves L and let OPTF :\u03b4(S) denote the optimal max-cost among all trees in DF :\u03b4.\nSimilar to the error-free setting, the O(log n) approximation of the optimal max-cost still holds for early stopping as shown next. The proofs of Lemma 4.1 and Theorem 4.2 are similar to that of Lemma 2.1 and Theorem 2.2 and we include them in the Appendix.\nLemma 4.1. Let F be an admissible function and \u03c4 is the first test chosen by GreedyTree on the set of objects S, then\nc(\u03c4)(F (S)\u2212 \u03b4)/(F (S)\u2212 F (Sq\u03c4 )) \u2264 OPTF :\u03b4(S).\nTheorem 4.2. GreedyTree constructs a decision tree achieving O(log n)-factor approximation of the optimal max-cost in DF :\u03b4 on the set S of n objects if F is admissible.\nHinged-Pairs: Similar to early-stopping, we can also use the hinged-Pairs P\u03b1 (10) with \u03b1 > 0 in GreedyTree to allow error-cost trade-off. We first establish an error upper bound for trees in DP\u03b1:0.\nLemma 4.3. For a multi-class input set S with k classes, the classification error of any tree in DP\u03b1:0 with l leaves is bounded by k(k \u2212 1)l , where we set \u03b1 = n.\nProof. Suppose j is the largest class in leaf L. For i 6= j, if niL > \u03b1, we have max(niLn j L \u2212 \u03b1(niL + njL), 0) = 0, which implies n i Ln j L \u2264 \u03b1nL. So\nniL \u2264 kniLn j L\nnL \u2264 k\u03b1 = k n.\nIf niL \u2264 \u03b1, we have niL \u2264 n \u2264 k n. So for any leaf L we have \u2211 i6=j n i L\nn \u2264 k(k \u2212 1) . The overall error bound thus follows.\nOften in practice a tree may contain a relatively large number of leaves but only a small fraction of them contain most of the objects. A more refined upper bound on the error is given by the following lemma, which we prove in the Appendix.\nLemma 4.4. Consider a multi-class input set S with k classes and \u03b1 = n. For any tree T \u2208 DP\u03b1:0 with l leaves, given any \u03b7 \u2208 [0, 1], let l\u03b7 be the smallest integer such that the largest l\u03b7 leaves of T have more than 1 \u2212 \u03b7 of the total number of objects n. Then the classification error is bounded by k(k \u2212 1)l\u03b7 + k\u22121k \u03b7.\nDenote DE: as the class of trees with classification error less than or equal to on the set of input S. We can further derive a useful relation between DE: and DP n:0.\nLemma 4.5. For any multi-class input set S with k classes, DE: \u2286 DP n:0 \u2286 DE:k(k\u22121) l.\nProof. To show DE: \u2286 DP n , for any tree T \u2208 DE: , we have \u2211l i=1 n\u0303Li \u2264 n, where l is the number of leaves and n\u0303Li is the number of objects in leaf Li that are not from the majority class: n\u0303L = nL \u2212 nmaxL . This implies n\u0303L \u2264 n for all leaves of T . Suppose j is the class with most number of objects in leaf L: njL = n max L . It is not hard to see for any class i 6= j\nniLn j L\nniL + n j L\n\u2264 niL \u2264 n,\nwhich implies [[niL\u2212\u03b1]+[n j L\u2212\u03b1]+\u2212\u03b12]+ = 0. Thus we have F (L) = \u2211 p 6=q[[n p L\u2212\u03b1]+[n q L\u2212\u03b1]+\u2212\u03b12]+ = 0. Thus DE: \u2286 DP n:0. DP n:0 \u2286 DE:k(k\u22121) l follows from Lemma 4.3.\nThe main theorem of this section is the following.\nTheorem 4.6. In multi-class classification with k classes, if T is the decision tree returned by GreedyTree using hinged-Pairs (setting \u03b1 = n) applied on the set S of n objects, then we have the following:\nCostP\u03b1(S) \u2264 O(log n)OPTP\u03b1:0(S) \u2264 O(log n)OPTE: (S).\nProof. The first inequality follows from Theorem 2.2 and the second inequality follows from Lemma 4.5.\nThe above theorem states that for a given error parameter , a greedy tree can be constructed using hinged-Pairs P\u03b1 by setting \u03b1 = n, with the maxcost guaranteed to be within an O(log n) factor of the best possible max-cost among all decision trees that have classification error less than or equal to . To our knowledge this is the first bound relating classification error to cost, which provides a theoretical basis for accuracy-cost trade-off."}, {"heading": "5 Experimental Results", "text": "We first demonstrate the effect of outliers using a simple synthetic example, where a small set of outliers dramatically increases the max-cost of the tree. We show that allowing a small number of errors in the tree drastically reduces the cost of the tree, allowing for efficient trees to be constructed in the presence of outliers. Next, we demonstrate the ability to construct decision trees on real world data sets. We observe a similar behavior to the synthetic data set on many of these data sets, where allowing a small amount of error results in trees with significantly lower cost. Additionally, we see the effect of impurity function choice on performance of the trees. For all real datasets, we present performance of the Powers impurity function presented in Eq. (9) with\nl = 2, 3, 4, 5 and error introduced by early stopping as well as the hinged-Pairs impurity function presented in Eq. (10) with error introduced by varying the parameter \u03b1.\nSynthetic Example: Here we consider a multiclass classification example to demonstrate the effect a small set of objects can have on the max-cost of the tree. Consider a data set composed of 1024 objects belonging to 4 classes with 10 binary tests available. Assume that the set of tests is complete, that is no two objects have the same set of test outcomes. Note that by fixing the order of the tests, the set of test outcomes maps each object to an integer in the range [0, 1023]. From this mapping, we give the objects in the ranges [1, 255] , [257, 511] , [513, 767], and [769, 1023] the labels 1, 2, 3, and 4, respectively, and the objects 0, 256, 512, and 768 the labels 2, 3, 4, and 1, respectively (Figure 2 shows the data projected to the first two tests). Suppose each test carries a unit cost. By Kraft\u2019s Inequality [Cover and Thomas, 1991], the optimal max-cost in order to correctly classify every object is 10, however, using only t1 and t2 as selected by the greedy algorithm, leads to a correct classification of all but 4 objects, as shown in Figure 3. For this type of data set, a constant sized set of costs can change from a tree with a constant max-cost to a tree with a log n max-cost.\nData Sets: We compare performance using 9 data sets from the UCI Repository [Frank and Asuncion, 2010]. We assume that all tests (features) have a uniform cost. For each data set, we replace non-unique objects with a single in-\nstance using the most common label for the objects, allowing every data set to be complete (perfectly classified by the decision trees). Additionally, continuous features are transformed to discrete features by quantizing to 10 uniformly spaced levels. More details on the data sets used can be found in the Appendix.\nError vs. Cost Trade-Off: Fig. 4 shows the trade-off between classification error and max-cost, which suggest two key trends. First, it appears that many data sets, such as house votes, Statlog DNA, Wisconsin breast cancer, and mammography, can be classified with minimal error using few tests. Intuitively, this small error appears to correspond to a small subset of outlier objects which require a\nlarge number of tests to correctly classify while the majority of the data can be classified with a small number of tests. Second, empirical evidence suggests that the optimal choice of impurity function is dependent on the desired max-cost of the tree. For trees with a smaller budget (and therefore lower depth), the hinged-Pairs impurity function outperforms the Powers impurity function with early stopping, whereas for larger budget (and greater depth), the Powers impurity function outperforms hingedPairs. This matches our intuitive understanding of the impurity functions, as the Powers impurity function biases towards tests which evenly divide the data whereas hinged-Pairs puts more emphasis on classification performance."}, {"heading": "6 Conclusion", "text": "We characterize a broad class of admissible impurity functions that can be used in a greedy algorithm to yield O(log n) guarantees of the optimal max-cost. We give examples of such admissible functions and demonstrate that they have different empirical properties even though they all enjoy the O(log n) guarantee. We further design admissible functions to allow for accuracy-cost trade-off and provide a bound relating classification error to cost. Finally, through real world datasets we demonstrate that our algorithm can indeed censor the outliers and achieve high classification accuracy using low max-cost. To visualize such outliers we construct a 2-D synthetic experiment and show our algorithm successfully identifies these as outliers."}], "references": [{"title": "Group-based active query selection for rapid diagnosis in time-critical situations", "author": ["Bellala et al", "G. 2012] Bellala", "S. Bhavnani", "C. Scott"], "venue": "Information Theory, IEEE Transactions", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Decision trees for entity identification: Approximation algorithms and hardness results", "author": ["Chakaravarthy et al", "V.T. 2011] Chakaravarthy", "V. Pandit", "S. Roy", "P. Awasthi", "M.K. Mohania"], "venue": "ACM Trans. Algorithms,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost", "author": ["Cicalese et al", "F. 2014] Cicalese", "E.S. Laber", "A.M. Saettler"], "venue": "In Proceedings of the 31th International Conference on Machine", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Adaptive submodularity: Theory and applications in active learning", "author": ["Golovin", "Krause", "D. 2011] Golovin", "A. Krause"], "venue": null, "citeRegEx": "Golovin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2011}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["Golovin et al", "D. 2010] Golovin", "A. Krause", "D. Ray"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Interactive submodular set cover", "author": ["Guillory", "Bilmes", "A. 2010] Guillory", "J.A. Bilmes"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Guillory et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Guillory et al\\.", "year": 2010}, {"title": "Approximation algorithms for optimal decision trees and adaptive tsp problems", "author": ["Gupta et al", "A. 2010] Gupta", "V. Nagarajan", "R. Ravi"], "venue": "In Proceedings of the 37th International Colloquium Conference on Automata,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "On an optimal split tree problem", "author": ["Kosaraju et al", "S.R. 1999] Kosaraju", "T.M. Przytycka", "R.S. Borgstrom"], "venue": "In Proceedings of the 6th International Workshop on Algorithms and Data Structures,", "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Trading off worst and expected cost in decision tree problems and a value dependent model", "author": ["Saettler et al", "A. 2014] Saettler", "E. Laber", "F. Cicalese"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "We propose novel methods for maxcost Discrete Function Evaluation Problem (DFEP) under budget constraints. We are motivated by applications such as clinical diagnosis where a patient is subjected to a sequence of (possibly expensive) tests before a decision is made. Our goal is to develop strategies for minimizing max-costs. The problem is known to be NP hard and greedy methods based on specialized impurity functions have been proposed. We develop a broad class of admissible impurity functions that admit monomials, classes of polynomials, and hinge-loss functions that allow for flexible impurity design with provably optimal approximation bounds. This flexibility is important for datasets when max-cost can be overly sensitive to \u201coutliers.\u201d Outliers bias max-cost to a few examples that require a large number of tests for classification. We design admissible functions that allow for accuracy-cost trade-off and result in O(log n) guarantees of the optimal cost among trees with corresponding classification accuracy levels.", "creator": "LaTeX with hyperref package"}}}