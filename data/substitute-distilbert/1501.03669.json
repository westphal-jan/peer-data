{"id": "1501.03669", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2015", "title": "A Proximal Approach for Sparse Multiclass SVM", "abstract": "sparsity - inducing penalties remains useful tools to design multiclass support vector machines ( svms ). in this way, we propose a convex optimization approach for efficiently and exactly integrating the multiclass svm learning problem involving a sparse regularization and the multiclass hinge integral formulated by crammer and singer. we assume elementary algorithms : the first way dealing with the hinge loss as finite penalty term, and the other one addressing the case when approximate hinge loss is enforced through error constraint. the related turbulence optimization problems can economically efficiently solved thanks to the dynamics facilitated by advanced self - dual proximal algorithms and epigraphical splitting techniques. experiments carried out on several datasets demonstrate the interest of considering the exact expression of the hinge loss rather than providing smooth approximation. the efficiency of the proposed algorithms w. r. t. several state - of - the - art issues is also assessed through comparisons of execution times.", "histories": [["v1", "Thu, 15 Jan 2015 13:23:14 GMT  (280kb,D)", "https://arxiv.org/abs/1501.03669v1", null], ["v2", "Fri, 16 Jan 2015 09:26:32 GMT  (280kb,D)", "http://arxiv.org/abs/1501.03669v2", "Corrected the name of an author"], ["v3", "Fri, 6 Feb 2015 23:36:03 GMT  (303kb,D)", "http://arxiv.org/abs/1501.03669v3", null], ["v4", "Sun, 26 Apr 2015 15:33:36 GMT  (303kb,D)", "http://arxiv.org/abs/1501.03669v4", null], ["v5", "Mon, 14 Dec 2015 09:49:32 GMT  (304kb,D)", "http://arxiv.org/abs/1501.03669v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["g chierchia", "nelly pustelnik", "jean-christophe pesquet", "b pesquet-popescu"], "accepted": false, "id": "1501.03669"}, "pdf": {"name": "1501.03669.pdf", "metadata": {"source": "CRF", "title": "A Proximal Approach for Sparse Multiclass SVM\u2217", "authors": ["G. Chierchia", "Nelly Pustelnik", "Jean-Christophe Pesquet", "B. Pesquet-Popescu"], "emails": ["first.last@telecom-paristech.fr).", "nelly.pustelnik@ens-lyon.fr.", "jean-christophe.pesquet@univ-paris-est.fr."], "sections": [{"heading": null, "text": "Sparsity-inducing penalties are useful tools to design multiclass support vector machines (SVMs). In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by [1]. We provide two algorithms: the first one dealing with the hinge loss as a penalty term, and the other one addressing the case when the hinge loss is enforced through a constraint. The related convex optimization problems can be efficiently solved thanks to the flexibility offered by recent primal-dual proximal algorithms and epigraphical splitting techniques. Experiments carried out on several datasets demonstrate the interest of considering the exact expression of the hinge loss rather than a smooth approximation. The efficiency of the proposed algorithms w.r.t. several state-of-the-art methods is also assessed through comparisons of execution times."}, {"heading": "1 Introduction", "text": "Support vector machines (SVMs) have gained much popularity in solving large-scale classification problems. As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6]. Consequently, the major difficulty encountered in this kind of applications stems from the computational cost. The SVM learning problem is classically solved by using standard Lagrangian duality techniques [7, 1]. This approach brings in several advantages, such as the kernel trick [8], or the possibility to break the problem down into a sequence of smaller ones [9, 10]. Some works also proposed to approximate the dual problem using cutting plane approaches, in order to address scenarios with thousands or even an infinite number of classes [3, 6].\n\u2217This work was supported by the CNRS IMAG\u2019in OPTIMISME project \u2020G. Chierchia (Corresponding author) and B. Pesquet-Popescu are with Te\u0301le\u0301com ParisTech/Institut Te\u0301le\u0301com, LTCI, UMR CNRS 5141, 75014 Paris, France (e-mail: first.last@telecom-paristech.fr). \u2021N. Pustelnik is with the Laboratoire de Physique de l\u2019ENS Lyon, CNRS UMR 5672, F69007 Lyon, France. Phone: +33 4 72 72 86 49, E-mail: nelly.pustelnik@ens-lyon.fr. \u00a7J.-C. Pesquet is with the Universite\u0301 Paris-Est, LIGM, CNRS-UMR 8049, 77454 Marne-la-Valle\u0301e Cedex 2, France. Phone: +33 1 60 95 77 39, E-mail: jean-christophe.pesquet@univ-paris-est.fr.\nar X\niv :1\n50 1.\n03 66\n9v 5\n[ cs\n.L G\n] 1\n4 D\nIn some applications, however, only a small number of training data is available. This is undoubtedly true in medical contexts, where the goal is to classify a patient as being \u201chealthy\u201d, \u201ccontaminated\u201d, or \u201cinfected\u201d, but the verified cases of infected patients might be just a few. In such applications, the lack of training data may lead to the so-called overfitting problem, eventually leading to a prediction which is too strongly tailored to the particularities of the training set and poorly generalizes to new data.\nA common solution to prevent overfitting consists of introducing a sparsity-inducing regularization in order to perform an implicit feature selection that gets rid of irrelevant or noisy features. In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18]. However, when a sparse regularization is introduced, the dual approach does no longer yield a simple formulation. Therefore, SVMs with sparse regularization lead to a nonsmooth convex optimization problem which is challenging. The main objective of this paper is to exactly and efficiently solve the multiclass SVM learning problem for convex regularizations. To this end, we propose two algorithms based on a primal-dual proximal method [19, 20] and a novel epigraphical splitting technique [21]. In addition to more detailed theoretical developments, this paper extends our preliminary work [22] by providing a new algorithm, and a larger number of experiments including comparisons with state-of-the-art methods for different types of database."}, {"heading": "1.1 Related work", "text": "The use of sparse regularization in SVMs was firstly proposed in the context of binary classification. The idea traces back to the work by [23], who demonstrated that the `1-norm regularization can effectively perform \u201cfeature selection\u201d by shrinking small coefficients to zero. Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28]. A different solution was proposed by [29], who reformulated the SVM learning problem by using an indicator vector (its components being either equal to 0 or 1) to model the active features, and solved the resulting combinatorial problem by convex relaxation using a cutting-plane algorithm. More recently, [30] proposed an accelerated algorithm for `1-regularized SVMs involving the square hinge loss. They also proposed a procedure for handling nonconvex regularization (using the reweighted `1-minimization scheme by [31]), showing that nonconvex penalties lead to similar prediction quality while using less features than convex ones.\nBinary SVMs can be turned into multiclass classifiers by a variety of strategies, such as the one-vs-all approach [7, 32]. While these techniques provide a simple and powerful framework, they cannot capture the correlations between different classes, since they break a multiclass problem into multiple independent binary problems. [1] therefore proposed a direct formulation of multiclass SVMs by generalizing the notion of margins used in the binary case. A natural idea thus consists of equipping muticlass SVMs with sparse regularization. A simple example is the `1-regularized multiclass SVM, which can be addressed by linear programming techniques [33]. In multiclass problems however, feature selection becomes more complex than in the binary case, since multiple discriminating functions need to be estimated, each one with its own set of important features. For this reason, mixed-norm regularization has recently attracted much interest due to its ability\nto impose group sparsity [34, 35, 12, 36]. In the context of multiclass SVMs, [37] proposed to deal with the `1,\u221e-norm regularization by reformulating the SVM learning problem in terms of linear programming. However, they validated their method on small-size problems, indicating that the linear reformulation may be inefficient for larger-size ones. More recently, [38] proposed an algorithm to handle `1,2-regularized SVMs involving a smooth loss function. While their method is efficient and can handle other convex regularizations, it does not solve rigorously the multiclass SVM learning problem, possibly leading to performance limitations."}, {"heading": "1.2 Contributions", "text": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37]. However, both cutting-plane methods and proximal algorithms have been employed to find an approximate solution, while linear programming techniques may not scale well to large datasets. In this paper, we propose a novel approach based on proximal tools and recent epigraphical splitting techniques [21], which allow us to exactly solve the sparse multiclass SVM learning problem through an efficient primal-dual proximal method [19, 20]."}, {"heading": "1.3 Outline", "text": "The paper is organized as follows. In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11]."}, {"heading": "1.4 Notation", "text": "\u03930(RX) denotes the set of proper, lower semicontinuous, convex functions from the Euclidean space RX to ]\u2212\u221e,+\u221e]. The epigraph of \u03c8 \u2208 \u03930(RX) is the nonempty closed convex subset of RX \u00d7 R defined as epi\u03c8 = { (y, \u03b6) \u2208 RX \u00d7 R \u2223\u2223 \u03c8(y) \u2264 \u03b6}. For every x \u2208 RX , the subdifferential of \u03c8 at x is \u2202\u03c8(x) = { u \u2208 RX\n\u2223\u2223 (\u2200y \u2208 RX) \u3008y \u2212 x | u\u3009+ \u03c8(x) \u2264 \u03c8(y)}. Let C be a nonempty closed convex subset of RX , then \u03b9C is the indicator function of C, equal to 0 on C and +\u221e otherwise."}, {"heading": "2 Sparse Multiclass SVM", "text": "A multiclass classifier can be modeled as a function d : RN \u2192 {1, . . . ,K} that predicts the class k \u2208 {1, . . . ,K} associated to a given observation u \u2208 RN (e.g. a signal, an image or a graph). This predictor relies on K different discriminating functions Dk : RN 7\u2192 R which, for every k \u2208 {1, . . . ,K}, measure the likelihood that an observation belongs to the class k. Consequently,\nthe predictor selects the class that best matches an observation, i.e.\nd(u) \u2208 argmax k\u2208{1,...,K} Dk(u).\nIn supervised learning, the discriminating functions are built from a set of L input-output pairs S = { (u`, z`) \u2208 RN \u00d7 {1, . . . ,K} | ` = {1, . . . , L} } ,\nand they are assumed to be linear in some feature representation of inputs [39]. The latter assumption leads to the following form of the discriminating functions:\nDk(u) = \u03c6(u) >x(k) + b(k), (1)\nwhere \u03c6 : RN 7\u2192 RM denotes a mapping from the input space onto an arbitrary feature space, and (x(k), b(k))1\u2264k\u2264K denote the parameters to be estimated. For convenience, we concatenate the latter ones into a single vector x \u2208 R(M+1)K\nx =  x(1) b(1) ... x(K)\nb(K)\n } x(1) } x(K)\nand we define the function \u03d5 : RN 7\u2192 RM+1 as\n\u03d5(u) = [ \u03c6(u)> 1 ]> ,\nso that (1) can be shortened to Dk(u) = \u03d5(u) >x(k)."}, {"heading": "2.1 Background", "text": "The objective of learning consists of finding the vector x such that, for every ` \u2208 {1, . . . , L}, the input-output pair (u`, z`) \u2208 S is correctly predicted by the classifier, i.e.,\nz` = argmax k\u2208{1,...,K}\n\u03d5(u`) >x(k).\nBy the definition of argmax, the above equality holds if1\n(\u2200` \u2208 {1, ..., L}) max k 6=z`\n\u03d5(u`) >(x(k) \u2212 x(z`)) < 0,\nor, equivalently, (\u2200` \u2208 {1, ..., L}) max\nk 6=z` \u03d5(u`)\n>(x(k) \u2212 x(z`)) \u2264 \u2212\u00b5`, (2)\n1To simplify the notation, we shorten k \u2208 {1, . . . ,K} \\ {z`} to k 6= z`.\nwhere, for every ` \u2208 {1, . . . , L}, \u00b5` is a positive scalar. Unfortunately, this constraint has no practical interest for learning purposes, as it becomes infeasible when the training set is not fully separable. Multiclass SVMs overcome this issue by introducing the notion of soft margins, which consists of adding a vector of slack variables \u03be = (\u03be(`))1\u2264`\u2264L into (2):(\u2200` \u2208 {1, ..., L}) maxk 6=z` \u03d5(u`) >(x(k) \u2212 x(z`)) \u2264 \u03be(`) \u2212 \u00b5`,\n(\u2200` \u2208 {1, ..., L}) \u03be(`) \u2265 0, (3)\nThe multiclass SVM learning problem is thus obtained by adding a quadratic regularization [1], yielding2\nminimize (x,\u03be)\u2208R(M+1)K\u00d7RL K\u2211 k=1 \u2016x(k)\u201622 + \u03bb L\u2211 `=1\n\u03be(`) s. t.(\u2200` \u2208 {1, ..., L}) maxk 6=z` \u03d5(u`) >(x(k) \u2212 x(z`)) \u2264 \u03be(`) \u2212 \u00b5`,\n(\u2200` \u2208 {1, ..., L}) \u03be(`) \u2265 0, (4)\nwhere \u03bb \u2208 ]0,+\u221e[. Note that the linear penalty on the slack variables allows us to minimize the violation of constraint (2). By using standard convex analysis [40], the above problem can be equivalently rewritten without slack variables as\nminimize x\u2208R(M+1)K K\u2211 k=1 \u2016x(k)\u201622 + \u03bb L\u2211 `=1 max { 0, \u00b5` + max k 6=z` \u03d5(u`) >(x(k) \u2212 x(z`)) } . (5)\nHereabove, the second term is called hinge loss when \u00b5` \u2261 1."}, {"heading": "2.2 Proposed approach", "text": "We extend Problem (5) by replacing the squared `2-norm regularization with a generic function g \u2208 \u03930(R(M+1)K). Moreover, we rewrite the hinge loss in an equivalent form by introducing, for every ` \u2208 {1, . . . , L}, the linear operator T` : R(M+1)K 7\u2192 RK defined as(\n\u2200x \u2208 R(M+1)K ) T` x = [ \u03d5(u`) >(x(k) \u2212 x(z`)) ] 1\u2264k\u2264K ,\nthe vector r` = (r (k) ` )1\u2264k\u2264K \u2208 R K defined as\n(\u2200k \u2208 {1, . . . ,K}) r(k)` =\n{ 0, if k = z`,\n\u00b5`, otherwise,\nand the function h` : RK 7\u2192 R defined, for every y(`) = (y(`,k))1\u2264k\u2264K \u2208 RK , as\nh`(y (`)) = max 1\u2264k\u2264K y(`,k) + r (k) ` , (6)\n2Note that the regularization does not involve the offsets (b(k))1\u2264k\u2264K .\nso that the following holds h`(T`x) = max {\n0, \u00b5` + max k 6=z`\n\u03d5(u`) >(x(k) \u2212 x(z`)) } .\nWe aim at solving the following convex optimization problems:\n\u2022 regularized formulation\nminimize x\u2208R(M+1)K g(x) + \u03bb L\u2211 `=1 h`(T` x), (7)\n\u2022 constrained formulation\nminimize x\u2208R(M+1)K g(x) s. t. L\u2211 `=1 h`(T` x) \u2264 \u03b7, (8)\nwhere \u03bb and \u03b7 are positive constants. Note that, by Lagrangian duality, the above formulations are equivalent for some specific values of \u03b7 and \u03bb. The interest of considering the constrained formulation lies in the fact that \u03b7 may be easier to set, since it is directly related to the properties of the training data.\nAs mentioned in the introduction, the regularization term g is chosen so as to promote some form of sparsity. A popular example is the `1-norm, as it ensures that the solution will have a number of coefficients exactly equal to zero, depending on the strength of the regularization [15]. Another example is given by the mixed `1,p-norm. For every x \u2208 R(M+1)K , let us assume that, for each k \u2208 {1, . . . ,K}, the vector x(k) \u2208 RM+1 is block-decomposed as follows:\nx(k) = [ ( x(k,1) )> \ufe38 \ufe37\ufe37 \ufe38\nsizeM1\n. . . ( x(k,B) )> \ufe38 \ufe37\ufe37 \ufe38\nsizeMB\nb(k) ]> ,\nwith M1 + \u00b7 \u00b7 \u00b7+MB = M . We define the `1,p-norm as\ng(x) = K\u2211 k=1 B\u2211 b=1 \u2016x(k,b)\u2016p.\nThe mixed-norm regularization is known to induce block-sparsity : the solution is partitioned into groups and the components of each group are ideally either all zeros or all non-zeros. In this context, the exponent values p = 2 or p = +\u221e are the most popular choices. In particular, the `1,\u221e-norm tends to favor solutions with few nonzero groups having components of similar magnitude."}, {"heading": "3 Optimization method", "text": "The resolution of Problems (7) and (8) requires an efficient algorithm for dealing with nonsmooth functions and hard constraints. In the convex optimization literature, proximal algorithms constitute\none of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44]. The key tool in these methods is the proximity operator [45], defined for a function \u03c8 \u2208 \u03930(H) as\n(\u2200u \u2208 H) prox\u03c8(u) = argmin v\u2208H\n1 2 \u2016v \u2212 u\u20162 + \u03c8(v).\nThe proximity operator can be interpreted as a sort of subgradient step for the function \u03c8, as p = prox\u03c8(y) is uniquely defined through the inclusion y \u2212 p \u2208 \u2202\u03c8(p). In addition, it reverts to the projection onto a closed convex set C \u2282 H in the case when \u03c8 = \u03b9C , in the sense that\n(\u2200u \u2208 H) prox\u03b9C (u) = PC(u) = argmin v\u2208C\n1 2 \u2016v \u2212 u\u20162. (9)\nProximal algorithms work by iterating a sequence of steps in which the proximity operators of the functions involved in the minimization are evaluated at each iteration. An efficient computation of these operators is thus essential to design fast algorithms for solving Problems (7)-(8). In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators."}, {"heading": "3.1 Regularized formulation", "text": "Problem (7) fits nicely into the framework provided by FBPD algorithm, since the proximity operators of both g and (h`)1\u2264`\u2264L can be efficiently computed. Indeed, proxg has a closed form for several norms and mixed norms [43, 41], while (proxh`)1\u2264`\u2264L can be computed through the projection onto the standard simplex, as described in Proposition 3.1. The projection onto the simplex can be efficiently computed with the method proposed by [49].\nProposition 3.1. For every ` \u2208 {1, . . . , L},\n(\u2200y(`) \u2208 RK) prox\u03bbh` ( y(`) ) = y(`) \u2212 PS\u03bb ( y(`) + r` ) , (10)\nwith\nS\u03bb = { u = ( u(k) ) 1\u2264k\u2264K \u2208 [0,+\u221e[ K \u2223\u2223 K\u2211 k=1 u(k) = \u03bb } .\nProof. Note that u \u2208 RK 7\u2192\u03bbmax1\u2264k\u2264K u(k) is the support function of S\u03bb, defined as (\u2200u \u2208 RK) \u03c3S\u03bb(u) = supv\u2208S\u03bb v >u. Hence, for every y(`) \u2208 RK , \u03bbh`(y(`)) = \u03c3S\u03bb(y(`) + r`) and\nprox\u03bbh` ( y(`) ) = prox\u03c3S\u03bb (y(`) + r`)\u2212 r`,\nSince \u03c3S\u03bb is the conjugate function of \u03b9S\u03bb , (10) is deduced by applying Moreau\u2019s decomposition formula [50, Theorem 14.3(ii)] and (9).\nThe iterations associated with Problem (7) are summarized in Algorithm 1, where the sequence (x[i])i\u2208N is guaranteed to converge to a solution to Problem (7), provided that such a solution exists [19, 20]. In Algorithm 1, we use the notation\nT = [T>1 . . . T > L ] >, r = [r>1 . . . r > L ] >.\nRemark 3.2. Algorithm 1 allows us to solve Problem (7) together with its (Fenchel-Rockafellar) dual formulation\nminimize y\u2208RLK g\u2217(\u2212T>y)\u2212 L\u2211 `=1 r>` y (`) s. t. y \u2208 (S\u03bb)L, (11)\nwhere g\u2217 is the convex conjugate of g. In the case when g = (1/2)\u2016\u00b7\u201622, the primal and dual solutions are linked by x = \u2212T>y, and thus Problem (11) reduces to the (Lagrangian) dual formulation of Problem (4) used in standard SVMs [1].\nAlgorithm 1 FBPD for solving Problem (7) Initialization  choose x [0] \u2208 R(M+1)K choose y[0] \u2208 RLK\nset \u03c4 > 0 and \u03c3 > 0 such that \u03c4\u03c3\u2016T\u20162 \u2264 1.\nFor i = 0, 1, . . .  x[i+1] = prox\u03c4g ( x[i] \u2212 \u03c4 T>y[i] ) y\u0302[i+1] = y[i] + \u03c3T ( 2x[i+1] \u2212 x[i]\n) y[i+1] = P(S\u03bb)L ( y\u0302[i+1] + \u03c3 r ) ."}, {"heading": "3.2 Constrained formulation", "text": "Problem (8) presents a more challenging computational issue, as the projection onto the hinge-loss constraint set cannot be evaluated in closed form, and it would require to solve a constrained quadratic problem at each iteration. In order to manage this constraint, we propose to introduce a vector of auxiliary variables \u03b6 = ( \u03b6(`) )\n1\u2264`\u2264L in the minimization process, so that Problem (8) can be equivalently rewritten as\nminimize (x,\u03b6)\u2208R(M+1)K\u00d7RL g(x) s. t.  L\u2211 `=1 \u03b6(`) \u2264 \u03b7,\n(\u2200` \u2208 {1, . . . , L}) h`(T` x) \u2264 \u03b6(`).\n(12)\nInterestingly, our approach is conceptually similar to adding the slack variables in (3), even though our reformulation specifically aims at simplifying the way of solving the problem. Indeed, a possible\ninterpretation of Problem (12) is the following:\nminimize (x,\u03b6)\u2208R(M+1)K\u00d7RL g(x) s. t.\n{ (Tx, \u03b6) \u2208 E,\n\u03b6 \u2208 V\u03b7, (13)\nwhere E denotes the collection of epigraphs of h1, . . . , hL E = { (y, \u03b6) \u2208 RLK \u00d7 RL \u2223\u2223 (\u2200` \u2208 {1, . . . , L}) (y(`), \u03b6(`)) \u2208 epih`},\nand V\u03b7 denotes a closed half-space\nV\u03b7 = { \u03b6 \u2208 RL \u2223\u2223 L\u2211 `=1 \u03b6(`) \u2264 \u03b7 } .\nThe iterations related to Problem (13) are listed in Algorithm 2, where the sequence (x[i], \u03b6 [i])i\u2208N is guaranteed to converge to a solution to (13), provided that such a solution exists [19, 20].\nAlgorithm 2 FBPD for solving Problem (8) Initialization  choose (x [0], \u03b6 [0]) \u2208 R(M+1)K \u00d7 RL choose (y[0], \u03be[0]) \u2208 RL(K\u22121) \u00d7 RL\nset \u03c4 > 0 and \u03c3 > 0 such that \u03c4\u03c3max{\u2016T\u20162, 1} \u2264 1.\nFor i = 0, 1, . . .  x[i+1] = prox\u03c4g ( x[i] \u2212 \u03c4 T>y[i] ) \u03b6 [i+1] = PV\u03b7 ( \u03b6 [i] \u2212 \u03c4 \u03be[i] ) y\u0302[i] = y[i] + \u03c3T ( 2x[i+1] \u2212 x[i] ) \u03be\u0302[i] = \u03be[i] + \u03c3 ( 2\u03b6 [i+1] \u2212 \u03b6 [i] )( y\u0303[i], \u03be\u0303[i] ) = PE ( y\u0302[i]/\u03c3, \u03be\u0302[i]/\u03c3 ) y[i+1] = y\u0302[i] \u2212 \u03c3y\u0303[i]\n\u03be[i+1] = \u03be\u0302[i] \u2212 \u03c3\u03be\u0303[i].\nThe advantage of our approach lies in the fact that the projections onto E and V\u03b7 employed in Algorithm 2 have closed form expressions. Indeed, the projection onto V\u03b7 is straightforward [43, Section 6.2.3], while the projection onto E can be block-decomposed as\nPE(y, \u03b6) = ( Pepih`(y (`), \u03b6(`)) )\n1\u2264`\u2264L (14)\nwhere a closed-form expression of Pepih` with ` \u2208 {1, . . . , L} is given in Proposition 3.3. The proof of this new result follows the same line as the proof by [21, Proposition 5], where we derived the epigraphical projection associated to the `\u221e-norm.\nThe decomposition in (14) yields two potential benefits. Firstly, the projection Pepih` is computed onto the lower-dimensional convex subset epih` of RK \u00d7R, whose dimensionality is only\nfixed by the number K of classes. Secondly, these projections can be computed in parallel, since they are defined over disjoint blocks whose number is given by the cardinality L of the training set (we refer to [51] for an example of parallel implementation on GP-GPUs).\nProposition 3.3. For every ` \u2208 {1, . . . , L}, let h` be the function defined in (6) and, for every (y(`), \u03b6(`)) \u2208 RK \u00d7R, let ( \u03bd(`,k) ) 1\u2264k\u2264K be the sequence ( y(`,k) + r (k) ` ) 1\u2264k\u2264K sorted in ascending order, 3 and set \u03bd(`,0) = \u2212\u221e and \u03bd(`,K+1) = +\u221e. Then, Pepih`(y (`), \u03b6(`)) = (p(`), \u03b8(`)) with\np(`) = [ min{y(`,k), \u03b8(`) \u2212 r(k)` } ]\n1\u2264k\u2264K\nand\n\u03b8(`) = 1\nK \u2212 k(`) + 2 \u03b6(`) + K\u2211 k=k (`) \u03bd(`,k)  , (15) where k (`) is the unique integer in {1, . . . ,K + 1} such that\n\u03bd(`,k (`)\u22121) < \u03b8(`) \u2264 \u03bd(`,k (`) ), (16)\nwith the convention \u2211K\nk=K+1 \u00b7 = 0.\nProof. For every (y(`), \u03b6(`)) \u2208 RK \u00d7 R, Pepih`(y(`), \u03b6(`)) denotes the unique solution to\nmin (p(`),\u03b8(`))\u2208epih`\n\u2016p(`) \u2212 y(`)\u20162 + (\u03b8(`) \u2212 \u03b6(`))2,\nwhich is equivalent to find the minimizer of\nmin \u03b8(`)\u2208R\n{ (\u03b8(`) \u2212 \u03b6(`))2 + min\np(`,1)\u2264\u03b8(`)\u2212r(1)` ... p(`,K)\u2264\u03b8(`)\u2212r(K)`\n\u2016p(`) \u2212 y(`)\u20162 } . (17)\nFor every \u03b8(`) \u2208 R, the inner minimization is achieved when p(`,k) =min{y(`,k), \u03b8(`) \u2212 r(k)` } for each k \u2208 {1, . . . ,K}, reducing (17) to\nmin \u03b8(`)\u2208R\n{ (\u03b8(`) \u2212 \u03b6(`))2 + K\u2211 k=1 (max{y(`,k) + r(k)` \u2212 \u03b8 (`), 0})2 } ,\nwhich achieves its minimum when \u03b8(`) = prox\u03d5`(\u03b6 (`)), with\n(\u2200v \u2208 R) \u03d5`(v) = 1\n2 K\u2211 k=1 (max{y(`,k) + r(k)` \u2212 v, 0}) 2. (18)\n3Note that the expensive sorting operation can be avoided by using a heap data structure [52], which keeps a partially-sorted sequence such that the first element is the largest. This approach was used, e.g., by van den Berg et al. [53, Algorithm 2] for implementing the projection onto the `1-ball.\nThe closed-form expression of this proximity operator, as well as the projection onto epih`, are derived in the following. In order to prove (15), we need to compute the proximity operator of \u03d5` defined in (18). Such a function belongs to \u03930(R) since, for each k\u2208{1, . . . ,K}, max{(\u03bd(`,k)\u2212 \u00b7), 0} is finite convex and (\u00b7)2 is finite convex and increasing on [0,+\u221e[. In addition, \u03d5` is differentiable and such that, for every v \u2208 R and k \u2208 {1, . . . ,K + 1},\n\u03bd(`,k\u22121) < v \u2264 \u03bd(`,k) \u21d2 \u03d5`(v) = 1\n2 K\u2211 j=k ( v \u2212 \u03bd(`,j) )2 .\nTherefore, there exists a k\u0304(`) \u2208 {1, . . . ,K + 1} such that \u03bd(`,k\u0304(`)\u22121) < \u03b8(`) \u2264 \u03bd(`,k\u0304(`)), which yields (16). Moreover, by the definition of proximity operator, \u03b8(`) = prox\u03d5`(\u03b6 (`)) is uniquely defined by \u03b6(`) \u2212 \u03b8(`) = \u03d5\u2032`(\u03b8(`)), yielding\n\u03b6(`) \u2212 \u03b8(`) = K\u2211\nk=k\u0304(`)\n(\u03b8(`) \u2212 \u03bd(`,k)),\nwhich is equivalent to (15). The uniqueness of k\u0304(`) follows from that of prox\u03d5`(\u03b6 (`))."}, {"heading": "4 Numerical results", "text": "In this section, we numerically evaluate the performance of sparse multiclass SVM w.r.t. the three following databases.\n\u2022 Leukemia database. The first experiment concerns the classification of microarray data. The considered database contains 72 samples of N = M = 7129 gene expression levels (so that \u03c6(u) = u) measured from patients having K = 3 types of leukemia disease [54]. The database is usually organized in L = 38 training samples and 34 test samples.4 In our experiments, we used blocks of 5 genes for the mixed-norm regularization.\n\u2022 MNIST dataset. The second experiment concerns the classification of handwritten digits. More precisely, we consider the MNIST database [55], which contains a number of 28\u00d7 28 grayscale images (N = 784) displaying digits from 0 to 9 (K = 10). The database is organized in 60000 training images and 10000 test images.5 In our experiments, we defined the mapping \u03c6 by resorting to the scattering convolution network [56] with m = 2 wavelet layers scaled up to 2J = 4, which transforms an input image of size 28\u00d7 28 in 81 images of size 14\u00d7 14 (thus M = 15876). For the regularization, we used the `1,\u221e-norm by dividing each vector (x(k))1\u2264k\u2264K in 14\n2 blocks of size 81. Moreover, in order to evaluate the performance, we trained a classifier on 25 different training subsets of size L \u2208 {3K, 5K, 10K}, we computed the classification errors by evaluating the 25 trained classifiers on the whole test set, and we averaged the resulting errors.\n4Data available at www.broadinstitute.org/cancer/software/genepattern/datasets 5Data available at http://yann.lecun.com/exdb/mnist\n\u2022 News20 database. The third experiment concerns the classification of text documents into a fixed number of predefined categories. More precisely, we consider the News20 database [57], which contains a number of documents partitioned across K = 20 different newsgroups. The database is organized in 11314 training documents and 7532 test documents.6 In our experiments, we defined the mapping \u03c6 by resorting to the term frequency \u2013 inverse document frequency transformation [58], yielding M = 26214. For the regularization, we used `1,2-norm in the same way as [38]. Moreover, in order to evaluate the performance, we trained a classifier on 10 different training subsets of size L \u2208 {5K, 10K, 50K}, we computed the classification errors by evaluating the 10 trained classifiers on the whole test set, and we averaged the resulting errors."}, {"heading": "4.1 Assessment of classification accuracy", "text": "In this section, we evaluate the classification errors obtained with the sparse multiclass SVM formulated in Problems (7)-(8). Our objective here is to show that the exact hinge loss allows us to achieve better performance than its approximated smooth versions, especially with a few training data. Hence, we compare the proposed method with the following approaches:\n\u2022 the multiclass SVM proposed by [38]\nminimize x\u2208R(M+1)K g(x) + \u03bb L\u2211 `=1 \u2211 k 6=z` ( max { 0, \u00b5` + \u03d5(u`) >(x(k) \u2212 x(z`)) })2 , (19)\n\u2022 the multinomial logistic regression (e.g., see [11])\nminimize x\u2208R(M+1)K g(x) + \u03bb L\u2211 `=1 log\n( 1 +\n\u2211 k 6=z` exp { \u00b5` + \u03d5(u`) >(x(k) \u2212 x(z`)) }) . (20)\n\u2022 the binary SVM by [30] based on the \u201cone-vs-all\u201d strategy, which aims, for every k \u2208 {1, . . . ,K}, to\nminimize x(k)\u2208R(M+1) g(x) + \u03bb L\u2211 `=1 ( max { 0, \u00b5` + z\u0303` \u03d5(u`) >x(k) })2 , (21)\nwith z\u0303` being equal to 1 if z` = k, and \u22121 otherwise. Note that (21) may be seen as a special case of (19).\nIn the following, we refer to Problems (7)-(8) as hinge, and to Problems (19)-(21), respectively, as square, logit, and one-vs-all. Since the parameters \u03bb and \u03b7 need to be estimated (e.g., through cross validation), it is important to evaluate the impact of their choice on the performance, although it is out of the scope of this paper to devise an optimal strategy to set this bound. To compare the above methods for different choices of these parameters, we set \u03bb = \u03b1\u22121 or \u03b7 = \u03b1L, by varying \u03b1 inside a fixed set of predefined values. We also follow the usual convention of setting \u00b5` \u2261 1.\n6Data available at www.cad.zju.edu.cn/home/dengcai/Data/TextData.html"}, {"heading": "4.2 Assessment of execution times", "text": "In this section, we compare the execution times of Algorithms 1 and 2 with7\n\u2022 a FISTA implementation of Problem (19),\n\u2022 a forward-backward implementation of Problem (20),\n\u2022 a FBPD implementation of Problem (12) reformulated with linear constraints\nminimize (x,\u03b6)\u2208R(M+1)K\u00d7RLK g(x) s. t.  L\u2211 `=1 K\u2211 k=1 \u03b6(`,k) \u2264 K\u03b7,\n(\u2200` \u2208 {1, ..., L}) \u03b6(`,1) = \u00b7 \u00b7 \u00b7 = \u03b6(`,K), (\u2200` \u2208 {1, ..., L}) \u03b6(`,1) \u2265 0, . . . , \u03b6(`,K) \u2265 0, (\u2200` \u2208 {1, ..., L}) T` x + r` \u2212 (\u03b6(`,k))1\u2264k\u2264K \u2264 0.\nThis approach is conceptually similar to the linear programming methods proposed by [33] and [37] for `1- or `1,+\u221e-regularized SVMs.\nFigures 3a, 3c and 3e show the execution times (averaged among 10 training sets) obtained by the above algorithms for various values of \u03bb and \u03b7 on the MNIST database with L \u2208 {3K, 5K, 10K}. In this experiment, the execution times refer to a stopping criterion of 10\u22125 on the relative error between two consecutive iterates. Conversely, Figures 3b, 3d and 3f show the relative distance to \u2016x[i] \u2212 x[\u221e]\u2016/\u2016x[\u221e]\u2016 (as a function of time) for the values of \u03bb and \u03b7 yielding the best accuracy (as reported in Figure 1), where x[\u221e] denotes the solution computed with a stopping criterion of 10\u22125. These results demonstrate that the proposed algorithms are faster than the approaches based on linear constraints and logistic regression, while being comparable in terms of execution times to approaches based on the square hinge loss. In addition, Algorithm 2 turns out to converge faster than Algorithm 1. This can be explained by the higher computational cost of the projection onto the standard simplex."}, {"heading": "4.3 Quadratic regularization", "text": "Although our emphasis is on sparse learning, we propose to complete our analysis by evaluating the efficiency of the proposed algorithms in the case when g is a quadratic regularization function. To this end, we compare the execution times of Algorithms 1 and 2 with the SVM-struct algorithm proposed by [6], which provides a numerical approach for solving Problem (4) through a cuttingplane technique. Figure 4 reports the execution times (averaged on 10 training sets) obtained by the above methods on the MNIST database with L \u2208 {3K, 5K, 10K, 50K, 100K, 500K} and different values of \u03b1. In this experiment, we set the stopping criterion to 10\u22123 in all methods, and the regularization parameter of SVM-struct to L/\u03b1. The results show that the proposed algorithms are competitive with state-of-the-art solutions in scenarios with a limited number of training data. The same cannot be claimed for larger databases, as SVM-struct scales particularly well w.r.t. the\n7The codes were implemented in MATLAB and executed on a Intel CPU at 3.33 GHz and 24 GB of RAM.\nnumber M of features and the size L of the training set. Note however that, when L/K = 500, the number of significant features for the SVM classifier designed with a quadratic regularization is equal to M \u2212 546 = 158214 (by setting a threshold to 10\u22125), while a sparse approach using an `1,\u221e-norm regularization yields only 42795 nonzero features."}, {"heading": "5 Conclusions", "text": "We have proposed two efficient algorithms for learning a sparse multiclass SVM. Our approach makes it possible to minimize a criterion involving the multiclass hinge loss and a sparsity-inducing regularization. In the literature, such a criterion is typically approximated by replacing the hinge loss with a smooth penalty, such as the quadratic hinge loss or the logistic loss. In this paper, we have provided two solutions that directly deal with the hinge loss: one addressing the regularized formulation and the other one adapted to the constrained formulation. The performance of the proposed solutions have been evaluated over three databases in scenarios with a few training data. The results show that the use of the hinge loss, rather than an approximation, leads to a slightly better classification accuracy and tends to make the method more robust w.r.t. the choice of the regularization parameter, while the proposed algorithms are often faster than state-of-the-art solutions."}], "references": [{"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 265\u2013392, Jan. 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "A speech recognizer based on multiclass SVMs with HMM-guided segmentation", "author": ["D. Mart\u0301\u0131n-Iglesias", "J. Bernal-Chaves", "C. Pel\u00e1ez-Moreno", "A. Gallardo-Anto\u013a\u0131n", "F. D\u0131\u0301az-de Ma\u0155\u0131a"], "venue": "Nonlinear Analyses and Algorithms for Speech Processing, vol. 3817, pp. 257\u2013266, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 1453\u20131484, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Large-scale learning with SVM and convolutional for generic object categorization", "author": ["F.J. Huang", "Y. LeCun"], "venue": "Conference on Computer Vision and Pattern Recognition, New York, USA, 17-22 Jun. 2006, pp. 284\u2013291.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "Conference on Computer Vision and Pattern Recognition, Anchorage, AK, 23-28 June 2008, pp. 1\u20138.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning, vol. 77, no. 1, pp. 27\u201359, Oct. 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["M. Aizerman", "E. Braverman", "L. Rozonoer"], "venue": "Automation and Remote Control, vol. 25, pp. 821\u2013837, 1964.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1964}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods - Support Vector Learning, B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, Eds., pp. 185\u2013208. MIT Press, Cambridge, USA, Jan. 1998.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Large-scale multiclass support vector machine training via euclidean projection onto the simplex", "author": ["M. Blondel", "A. Fujino", "N. Ueda"], "venue": "International Conference on Pattern Recognition, Stockholm, Sweden, 24-28 August 2014, pp. 1289\u20131294.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse multinomial logistic regression: Fast algorithms and generalization bounds", "author": ["B. Krishnapuram", "L. Carin", "M.A.T. Figueiredo", "A.J. Hartemink"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 27, no. 6, June 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Boosting with structural sparsity", "author": ["J. Duchi", "Y. Singer"], "venue": "International Conference on Machine Learning, Montreal, Canada, 14-18 June 2009, pp. 297\u2013304.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "A comparison of optimization methods and software for large-scale L1-regularized linear classification", "author": ["G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "Machine Learning, vol. 11, pp. 3183\u20133234, Dec. 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "`p \u2212 `q penalty for sparse linear and sparse multiple kernel multi-task learning", "author": ["A. Rakotomamonjy", "R. Flamary", "G. Gasso", "S. Canu"], "venue": "IEEE Trans. on Neural Networks, vol. 22, no. 8, pp. 1307\u20131320, Aug. 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 1, pp. 1\u2013106, Jan. 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric sparsity and regularization", "author": ["L. Rosasco", "S. Villa", "S. Mosci", "M. Santoro", "A. Verri"], "venue": "Journal of Machine Learning Research, vol. 14, pp. 1665\u20131714, July 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic feature learning for spatio-spectral image classification with sparse SVM", "author": ["D. Tuia", "M. Volpi", "M. Dalla Mura", "A. Rakotomamonjy", "R. Flamary"], "venue": "IEEE Trans. on Geoscience and Remote Sensing, vol. 52, no. 10, pp. 6062\u20136074, Oct. 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximal methods for the latent group lasso penalty", "author": ["S. Villa", "L. Rosasco", "S. Mosci", "A. Verri"], "venue": "Computational Optimization and Applications, vol. 58, no. 2, pp. 381\u2013407, Dec. 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A splitting algorithm for dual monotone inclusions involving cocoercive operators", "author": ["B.C. V\u0169"], "venue": "Advances in Computational Mathematics, vol. 38, no. 3, pp. 667\u2013681, Apr. 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms", "author": ["L. Condat"], "venue": "Journal of Optimization Theory and Applications, vol. 158, no. 2, pp. 460\u2013479, Aug. 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Epigraphical projection and proximal tools for solving constrained convex optimization problems", "author": ["G. Chierchia", "N. Pustelnik", "J.-C. Pesquet", "B. Pesquet-Popescu"], "venue": "Signal, Image and Video Processing, July 2014. 20", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Epigraphical proximal projection for sparse multiclass SVM", "author": ["G. Chierchia", "N. Pustelnik", "J.-C. Pesquet", "B. Pesquet-Popescu"], "venue": "International Conference on Acoustics, Speech and Signal Processing, Florence, Italy, 4-9 May 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection via concave minimization and support vector machines", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "International Conference on Machine Learning, Madison, USA, 1998, pp. 82\u201390.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Use of the zero-norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Sch\u00f6lkopf", "M. Tipping"], "venue": "Machine Learning, vol. 3, pp. 1439\u20131461, 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Support vector machines with adaptive Lq penalty", "author": ["Y. Liu", "H. Helen Zhang", "C. Park", "J. Ahn"], "venue": "Computational Statistics and Data Analysis, vol. 51, no. 12, pp. 6380\u20136394, Aug. 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "The f\u221e-norm support vector machine", "author": ["H. Zou", "M. Yuan"], "venue": "Statistica Sinica, vol. 18, pp. 379\u2013398, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Variable selection via a combination of the L0 and L1 penalties", "author": ["Y. Liu", "Y. Wu"], "venue": "Journal of Computational and Graphical Statistics, vol. 14, no. 4, pp. 782\u2013798, Dec. 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "The doubly regularized support vector machine", "author": ["L. Wang", "J. Zhu", "H. Zou"], "venue": "Statistica Sinica, vol. 16, pp. 589\u2013616, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "International Conference on Machine Learning, Haifa, Israel, 21-24 June 2010, pp. 1047\u20131054.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Non-convex regularizations for feature selection in ranking with sparse SVM", "author": ["L. Laporte", "R. Flamary", "S. Canu", "S. D\u00e9jean", "J. Mothe"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, vol. 25, no. 6, pp. 1118 \u2013 1130, June 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E.J. Cand\u00e9s", "M.B. Wakin", "S. Boyd"], "venue": "Journal of Fourier Analysis and Applications, vol. 14, no. 5, pp. 877\u2013905, Dec. 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "In defense of one-vs-all classification", "author": ["R. Rifkin", "A. Klautau"], "venue": "Machine Learning, vol. 5, pp. 101\u2013141, 2004.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "On l1-norm multi-class support vector machines: methodology and theory", "author": ["L. Wang", "X. Shen"], "venue": "Journal of the American Statistical Association, vol. 102, pp. 583\u2013594, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B, vol. 68, pp. 49\u201367, 2006.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "The group Lasso for logistic regression", "author": ["L. Meier", "S. Van De Geer", "P. B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B, vol. 70, no. 1, pp. 53\u201371, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Statistics and Computing, vol. 20, no. 2, pp. 231\u2013252, 2010. 21", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Variable selection for multicategory SVM via sup-norm regularization", "author": ["H.H. Zhang", "Y. Liu", "Y. Wu", "J. Zhu"], "venue": "Electronic Journal of Statistics, vol. 2, pp. 149\u2013167, 2008.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Block coordinate descent algorithms for large-scale sparse multiclass classification", "author": ["M. Blondel", "K. Seki", "K. Uehara"], "venue": "Machine Learning, vol. 93, no. 1, pp. 31\u201352, Oct. 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition", "author": ["T.M. Cover"], "venue": "IEEE Trans. on Electronic Computers, vol. EC-14, no. 3, pp. 326\u2013334, June 1965.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1965}, {"title": "Convex Optimization, Cambrige", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2004}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Fixed-Point Algorithms for Inverse Problems in Science and Engineering, H. H. Bauschke, R. S. Burachik, P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, Eds., pp. 185\u2013212. Springer-Verlag, New York, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Primal-dual splitting algorithm for solving inclusions with mixtures of composite, Lipschitzian, and parallel-sum type monotone operators", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Set-Valued and Variational Analysis, vol. 20, no. 2, pp. 307\u2013330, June 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in Optimization, vol. 1, no. 3, pp. 123231, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Playing with duality: An overview of recent primal-dual approaches for solving large-scale optimization problems", "author": ["N. Komodakis", "J.-C. Pesquet"], "venue": "IEEE Signal Processing Magazine, 2014, accepted for publication.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien", "author": ["J.J. Moreau"], "venue": "Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France, vol. 93, pp. 273\u2013299, 1965.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1965}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["A. Chambolle", "T. Pock"], "venue": "Journal of Mathematical Imaging and Vision, vol. 40, no. 1, May 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "A forward-backward view of some primal-dual optimization methods in image recovery", "author": ["P.L. Combettes", "L. Condat", "J.-C. Pesquet", "B.C. V\u0169"], "venue": "International Conference on Image Processing, Paris, France, 27-30 October 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Variable metric forward-backward splitting with applications to monotone inclusions in duality", "author": ["P.L. Combettes", "B.C. V\u0169"], "venue": "Optimization, vol. 63, no. 9, pp. 1289\u20131318, Sept. 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast projection onto the simplex and the l1 ball", "author": ["L. Condat"], "venue": "2014, Available online at http://hal.archives-ouvertes.fr/hal-01056171.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "Parallel implementations of a disparity estimation algorithm based on a proximal splitting method", "author": ["R. Gaetano", "G. Chierchia", "B. Pesquet-Popescu"], "venue": "Visual Communication and Image Processing, San Diego, USA, 27-30 November 2012, pp. 1\u20136.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1990}, {"title": "Probing the Pareto frontier for basis pursuit solutions", "author": ["E. Van Den Berg", "M.P. Friedlander"], "venue": "SIAM Journal on Scientific Computing, vol. 31, no. 2, pp. 890\u2013912, Nov. 2008.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring", "author": ["T.R. Golub", "D.K. Slonim", "P. Tamayo", "C. Huard", "M. Gaasenbeek", "J.P. Mesirov", "H. Coller", "M.L. Loh", "J.R. Downing", "M.A. Caligiuri", "C.D. Bloomfield", "E.S. Lander"], "venue": "Science, vol. 286, no. 5439, pp. 531\u2013537, 1999.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1999}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov. 1998.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1998}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1872\u20131886, Aug. 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1872}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "International Conference on Machine Learning, Tahoe City, USA, 9-12 July 1995, pp. 331\u2013339.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1995}, {"title": "Text categorization with suport vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "European Conference on Machine Learning, Chemnitz, Germany, 21-24 April 1998, pp. 137\u2013142. 23", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 1, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 2, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 3, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 4, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 5, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 0, "context": "The SVM learning problem is classically solved by using standard Lagrangian duality techniques [7, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 6, "context": "This approach brings in several advantages, such as the kernel trick [8], or the possibility to break the problem down into a sequence of smaller ones [9, 10].", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "This approach brings in several advantages, such as the kernel trick [8], or the possibility to break the problem down into a sequence of smaller ones [9, 10].", "startOffset": 151, "endOffset": 158}, {"referenceID": 8, "context": "This approach brings in several advantages, such as the kernel trick [8], or the possibility to break the problem down into a sequence of smaller ones [9, 10].", "startOffset": 151, "endOffset": 158}, {"referenceID": 2, "context": "Some works also proposed to approximate the dual problem using cutting plane approaches, in order to address scenarios with thousands or even an infinite number of classes [3, 6].", "startOffset": 172, "endOffset": 178}, {"referenceID": 5, "context": "Some works also proposed to approximate the dual problem using cutting plane approaches, in order to address scenarios with thousands or even an infinite number of classes [3, 6].", "startOffset": 172, "endOffset": 178}, {"referenceID": 9, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 10, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 11, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 12, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 13, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 14, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 15, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 16, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 17, "context": "To this end, we propose two algorithms based on a primal-dual proximal method [19, 20] and a novel epigraphical splitting technique [21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 18, "context": "To this end, we propose two algorithms based on a primal-dual proximal method [19, 20] and a novel epigraphical splitting technique [21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 19, "context": "To this end, we propose two algorithms based on a primal-dual proximal method [19, 20] and a novel epigraphical splitting technique [21].", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "In addition to more detailed theoretical developments, this paper extends our preliminary work [22] by providing a new algorithm, and a larger number of experiments including comparisons with state-of-the-art methods for different types of database.", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "The idea traces back to the work by [23], who demonstrated that the `1-norm regularization can effectively perform \u201cfeature selection\u201d by shrinking small coefficients to zero.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 162, "endOffset": 166}, {"referenceID": 26, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 182, "endOffset": 186}, {"referenceID": 27, "context": "A different solution was proposed by [29], who reformulated the SVM learning problem by using an indicator vector (its components being either equal to 0 or 1) to model the active features, and solved the resulting combinatorial problem by convex relaxation using a cutting-plane algorithm.", "startOffset": 37, "endOffset": 41}, {"referenceID": 28, "context": "More recently, [30] proposed an accelerated algorithm for `1-regularized SVMs involving the square hinge loss.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "They also proposed a procedure for handling nonconvex regularization (using the reweighted `1-minimization scheme by [31]), showing that nonconvex penalties lead to similar prediction quality while using less features than convex ones.", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "Binary SVMs can be turned into multiclass classifiers by a variety of strategies, such as the one-vs-all approach [7, 32].", "startOffset": 114, "endOffset": 121}, {"referenceID": 0, "context": "[1] therefore proposed a direct formulation of multiclass SVMs by generalizing the notion of margins used in the binary case.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "A simple example is the `1-regularized multiclass SVM, which can be addressed by linear programming techniques [33].", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 33, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 10, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 34, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 35, "context": "In the context of multiclass SVMs, [37] proposed to deal with the `1,\u221e-norm regularization by reformulating the SVM learning problem in terms of linear programming.", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "More recently, [38] proposed an algorithm to handle `1,2-regularized SVMs involving a smooth loss function.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 148, "endOffset": 156}, {"referenceID": 36, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 148, "endOffset": 156}, {"referenceID": 31, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 191, "endOffset": 199}, {"referenceID": 35, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 191, "endOffset": 199}, {"referenceID": 19, "context": "In this paper, we propose a novel approach based on proximal tools and recent epigraphical splitting techniques [21], which allow us to exactly solve the sparse multiclass SVM learning problem through an efficient primal-dual proximal method [19, 20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 17, "context": "In this paper, we propose a novel approach based on proximal tools and recent epigraphical splitting techniques [21], which allow us to exactly solve the sparse multiclass SVM learning problem through an efficient primal-dual proximal method [19, 20].", "startOffset": 242, "endOffset": 250}, {"referenceID": 18, "context": "In this paper, we propose a novel approach based on proximal tools and recent epigraphical splitting techniques [21], which allow us to exactly solve the sparse multiclass SVM learning problem through an efficient primal-dual proximal method [19, 20].", "startOffset": 242, "endOffset": 250}, {"referenceID": 36, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 274, "endOffset": 278}, {"referenceID": 28, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 280, "endOffset": 284}, {"referenceID": 35, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 286, "endOffset": 290}, {"referenceID": 9, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 296, "endOffset": 300}, {"referenceID": 37, "context": ", L} } , and they are assumed to be linear in some feature representation of inputs [39].", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "The multiclass SVM learning problem is thus obtained by adding a quadratic regularization [1], yielding2", "startOffset": 90, "endOffset": 93}, {"referenceID": 38, "context": "By using standard convex analysis [40], the above problem can be equivalently rewritten without slack variables as", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "A popular example is the `1-norm, as it ensures that the solution will have a number of coefficients exactly equal to zero, depending on the strength of the regularization [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 39, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 40, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 13, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 41, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 42, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 43, "context": "The key tool in these methods is the proximity operator [45], defined for a function \u03c8 \u2208 \u03930(H) as (\u2200u \u2208 H) prox\u03c8(u) = argmin v\u2208H 1 2 \u2016v \u2212 u\u2016 + \u03c8(v).", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 18, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 44, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 45, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 46, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 41, "context": "Indeed, proxg has a closed form for several norms and mixed norms [43, 41], while (proxh`)1\u2264`\u2264L can be computed through the projection onto the standard simplex, as described in Proposition 3.", "startOffset": 66, "endOffset": 74}, {"referenceID": 39, "context": "Indeed, proxg has a closed form for several norms and mixed norms [43, 41], while (proxh`)1\u2264`\u2264L can be computed through the projection onto the standard simplex, as described in Proposition 3.", "startOffset": 66, "endOffset": 74}, {"referenceID": 47, "context": "The projection onto the simplex can be efficiently computed with the method proposed by [49].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "The iterations associated with Problem (7) are summarized in Algorithm 1, where the sequence (x)i\u2208N is guaranteed to converge to a solution to Problem (7), provided that such a solution exists [19, 20].", "startOffset": 193, "endOffset": 201}, {"referenceID": 18, "context": "The iterations associated with Problem (7) are summarized in Algorithm 1, where the sequence (x)i\u2208N is guaranteed to converge to a solution to Problem (7), provided that such a solution exists [19, 20].", "startOffset": 193, "endOffset": 201}, {"referenceID": 0, "context": "In the case when g = (1/2)\u2016\u00b7\u20162, the primal and dual solutions are linked by x = \u2212T>y, and thus Problem (11) reduces to the (Lagrangian) dual formulation of Problem (4) used in standard SVMs [1].", "startOffset": 190, "endOffset": 193}, {"referenceID": 17, "context": "The iterations related to Problem (13) are listed in Algorithm 2, where the sequence (x[i], \u03b6 )i\u2208N is guaranteed to converge to a solution to (13), provided that such a solution exists [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 18, "context": "The iterations related to Problem (13) are listed in Algorithm 2, where the sequence (x[i], \u03b6 )i\u2208N is guaranteed to converge to a solution to (13), provided that such a solution exists [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 49, "context": "Secondly, these projections can be computed in parallel, since they are defined over disjoint blocks whose number is given by the cardinality L of the training set (we refer to [51] for an example of parallel implementation on GP-GPUs).", "startOffset": 177, "endOffset": 181}, {"referenceID": 50, "context": "Note that the expensive sorting operation can be avoided by using a heap data structure [52], which keeps a partially-sorted sequence such that the first element is the largest.", "startOffset": 88, "endOffset": 92}, {"referenceID": 52, "context": "The considered database contains 72 samples of N = M = 7129 gene expression levels (so that \u03c6(u) = u) measured from patients having K = 3 types of leukemia disease [54].", "startOffset": 164, "endOffset": 168}, {"referenceID": 53, "context": "More precisely, we consider the MNIST database [55], which contains a number of 28\u00d7 28 grayscale images (N = 784) displaying digits from 0 to 9 (K = 10).", "startOffset": 47, "endOffset": 51}, {"referenceID": 54, "context": "5 In our experiments, we defined the mapping \u03c6 by resorting to the scattering convolution network [56] with m = 2 wavelet layers scaled up to 2J = 4, which transforms an input image of size 28\u00d7 28 in 81 images of size 14\u00d7 14 (thus M = 15876).", "startOffset": 98, "endOffset": 102}, {"referenceID": 55, "context": "More precisely, we consider the News20 database [57], which contains a number of documents partitioned across K = 20 different newsgroups.", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "6 In our experiments, we defined the mapping \u03c6 by resorting to the term frequency \u2013 inverse document frequency transformation [58], yielding M = 26214.", "startOffset": 126, "endOffset": 130}, {"referenceID": 36, "context": "For the regularization, we used `1,2-norm in the same way as [38].", "startOffset": 61, "endOffset": 65}, {"referenceID": 36, "context": "\u2022 the multiclass SVM proposed by [38]", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": ", see [11])", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "\u2022 the binary SVM by [30] based on the \u201cone-vs-all\u201d strategy, which aims, for every k \u2208 {1, .", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "This approach is conceptually similar to the linear programming methods proposed by [33] and [37] for `1- or `1,+\u221e-regularized SVMs.", "startOffset": 84, "endOffset": 88}, {"referenceID": 35, "context": "This approach is conceptually similar to the linear programming methods proposed by [33] and [37] for `1- or `1,+\u221e-regularized SVMs.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "To this end, we compare the execution times of Algorithms 1 and 2 with the SVM-struct algorithm proposed by [6], which provides a numerical approach for solving Problem (4) through a cuttingplane technique.", "startOffset": 108, "endOffset": 111}], "year": 2015, "abstractText": "Sparsity-inducing penalties are useful tools to design multiclass support vector machines (SVMs). In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by [1]. We provide two algorithms: the first one dealing with the hinge loss as a penalty term, and the other one addressing the case when the hinge loss is enforced through a constraint. The related convex optimization problems can be efficiently solved thanks to the flexibility offered by recent primal-dual proximal algorithms and epigraphical splitting techniques. Experiments carried out on several datasets demonstrate the interest of considering the exact expression of the hinge loss rather than a smooth approximation. The efficiency of the proposed algorithms w.r.t. several state-of-the-art methods is also assessed through comparisons of execution times.", "creator": "LaTeX with hyperref package"}}}