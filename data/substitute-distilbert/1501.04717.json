{"id": "1501.04717", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2015", "title": "Robust Face Recognition by Constrained Part-based Alignment", "abstract": "developing a reliable and practical look recognition system is a long - standing goal in computer vision research. existing literature suggests that pixel - wise face alignment is the key drivers achieve high - accuracy face recognition. by assuming a human face into piece - wise planar surfaces, where each surface corresponds to one facial part, we develop in this pattern a constrained part - based visibility ( cpa ) algorithm for face fit across design and / or expression. our proposed algorithm is based on a trainable cpa model, which learns appearance differences of individual parts and a tree - structured shape plot among different samples. constructing such probe face, robots simultaneously aligns all its parts by observing them to the appearance evidence with consideration of the constraint from the tree - structured shape configuration. this objective is formulated as a norm minimization problem regularized following graph likelihoods. cpa could be easily integrated with many design classifiers to perform part - based sight recognition. extensive experiments on benchmark face datasets confirm that cpa outperforms or is on par with existing methods for understanding face recognition across pose, aperture, and / or illumination changes.", "histories": [["v1", "Tue, 20 Jan 2015 06:05:01 GMT  (2948kb,D)", "http://arxiv.org/abs/1501.04717v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yuting zhang", "kui jia", "yueming wang", "gang pan", "tsung-han chan", "yi ma"], "accepted": false, "id": "1501.04717"}, "pdf": {"name": "1501.04717.pdf", "metadata": {"source": "CRF", "title": "Robust Face Recognition by Constrained Part-based Alignment", "authors": ["Yuting Zhang", "Kui Jia", "Yueming Wang", "Gang Pan", "Tsung-Han Chan", "Yi Ma"], "emails": ["zyt@zju.edu.cn;", "gpan@zju.edu.cn)", "kuijia@gmail.com)", "ymingwang@gmail.com)", "thchan@ieee.org)", "mayi@shanghaitech.edu.cn)", "yima@uiuc.edu)"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nDeveloping a reliable and practical face recognition system is a long-standing goal in computer vision research. A tremendous amount of works have been done in the past three decades, however, most of them can only work well in controlled scenarios. In more practical scenarios, performance of these existing methods degrades drastically due to face variations caused by illumination, pose, and/or expression changes [1].\nTo handle the variation caused by illumination change, the methods of Lee et al. [2], Wagner et al. [3], motivated by the illumination cone model [4, 5], used multiple carefully chosen face images of varying illuminations per subject as gallery. Given a probe face of unknown subject under arbitrary illumination, face images under this specific illumination can be generated in the gallery to match with the probe face. When multiple gallery images of such kind are not available,\nY. Zhang and G. Pan are with the Department of Computer Science, Zhejiang University, Hangzhou 310027, P. R. China (e-mail: zyt@zju.edu.cn; gpan@zju.edu.cn)\nK. Jia is with the Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau, China. (email: kuijia@gmail.com)\nY. Wang is with the Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou 310027, P. R. China (e-mail: ymingwang@gmail.com)\nT. Chan is with the Advanced Digital Sciensces Center (ADSC), Singapore 138632. (email: thchan@ieee.org)\nY. Ma is with the School of Informatin Science and Technology, ShanghaiTech Universitiy, Shanghai 200031, P. R.China. (email: mayi@shanghaitech.edu.cn) He is also with the Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, IL 61801, USA. (email: yima@uiuc.edu)\nalternative methods [6, 7] considered extracting illumination invariant features for face recognition.\nTo address pose or expression variations, earlier approaches extend classic subspace or template based face recognition methods [8, 9]. The use of these methods is rather restricted due to their dependence on the availability of gallery images of multiple facial poses or expressions. Recent approaches consider more practical scenarios where only face images under the normal condition (frontal view and neutral expression) are assumed to be available in the gallery. To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17]. However, most of these methods are still far from practice since they assume both gallery and probe face images have been manually aligned into some canonical form, and they cannot generally cope with illumination variation either.\nIn literature, the most popular approaches for automatic face alignment across pose or expression are based on facial landmark localization, e.g., the Active Appearance Models (AAMs) [18, 19] and elastic graph matching (EGM) [20]. AAMs and EGM used densely-connected elastic graphs, which however, are difficult to optimize in that the solutions are likely to be trapped into undesirable local minima. Consequently, localized landmarks using these methods are often not accurate enough, especially when applied to unseen face images. To improve the localization accuracy, an explicit shape constraint for graph nodes was considered in the constrained local models (CLMs) [21]. CLMs [21, 22, 23] are still based on densely-connected graph models, and their shape constraints are over-simplified so that the dependency among different graph nodes is ignored. Recently, deformable part-based models (DPMs) show their promise in many applications such as object detection [24] and facial landmark localization [25]. In particular, Zhu and Ramanan [25] adopted a tree-structured part model, which encodes node dependency while admitting efficient solutions. Such a tree model was used by Zhu and Ramanan [25] for facial landmark localization. Nevertheless, it cannot be readily extended for pixel-wise face alignment, and consequently for face recognition across pose or expression.\nWith the aim of developing a system that can simultaneously handle illumination variation and minor changes of pose and expression, Wagner et al. [3] recently leveraged sparsity optimization and a carefully prepared gallery set (multiple images of varying illuminations per subject), to align probe\nar X\niv :1\n50 1.\n04 71\n7v 1\n[ cs\n.C V\n] 2\n0 Ja\nn 20\n15\n2 face images into a canonical form for better recognition. In particular, they assumed human face as a planar surface and chose a global similarity transformation for face alignment. This may be valid when probe faces are close to the normal condition. It is however a rather simplified assumption when there exist pose and/or expression changes. Indeed, a human face has non-planar geometry and non-rigid deformation. Some portions of the face (e.g., the nose region) undergo significant appearance changes as the pose varies, while other portions (e.g. the mouth region) deform significantly as expression changes. It is thus more appropriate to approximate a human face as a piece-wise planar surface, and correspondingly use piece-wise geometric transformation for face alignment.\nImproving performance using enriched models is not an easy task in computer vision research. In fact, opposite effects often happen. As to the problems considered in this paper, we will show that by partitioning a human face as a collection of parts, and carefully characterizing the deformation relations among different parts, performance of face recognition across pose or expression can be significantly improved. In particular, we propose a Constrained Part-based Alignment (CPA) method for this task. Our method is partially motivated by the promise of piece-wise planar formulation and the explicit shape constraints used in CLMs [21, 22, 23] and DPMs [24, 25].\nCPA partitions the object of interest, e.g., a human face, as a constellation of parts, and uses a similarity transformation to model the deformation of each part. A tree-structured shape model is used in CPA to constrain the relations among the deformations of different parts. A CPA model also has a batch of registered face images serving as the appearance evidence of each part. For a probe face image, all its parts are simultaneously aligned towards the registered model, by fitting them to the appearance evidence and penalizing the cost of violating the constraint from the tree-structured shape model. We formulate this objective as a norm minimization problem regularized by graph likelihoods, which is solved by an alternating method composed of two steps: one for aligning the parts, and the other for adjusting the configuration of the tree-structured model. The former can be reduced into a sequence of convex problems, while the latter admits efficient solutions by gradient decent methods. In this paper, we use the proposed CPA model for face recognition, where registered gallery images are taken as the appearance evidence, and a probe image is aligned using the CPA model. After alignment, most of the off-the-shelf face recognition methods can be readily used for part recognition.The overall decision is made by aggregating predictions from different parts by a plurality voting scheme. Robustness against illumination variation can also be achieved by choosing specific face recognition methods, e.g., [3, 26].\nRicher models are often more difficult to train. For the proposed CPA model, both the appearance model and the treestructured shape model need to be automatically learned. On the one hand, the appearance model is obtained by aligning the gallery images in batch with the constraint from a given shape model. In particular, we use low-rank and sparse matrix decomposition as the criteria to optimize batch alignment of each part, and globally regularize these individual problems of\npart alignment by graph likelihoods. On the other hand, the shape model is learned by the probabilistic inference based on given part constellations, where the maximum a posteriori (MAP) estimation is performed with the graph likelihoods and the given conjugate priors of the likelihoods. As the two problems are coupled, we solve them in a joint way. Moreover, we generalize our learning method to train a mixture of CPA models (mCPA) for better handling pose and expression variations.\nPart-based face recognition methods [27, 28, 29, 30, 31] have shown improved performance over more standard approaches using holistic faces. However, existing part-based methods can only be applied when gallery and probe face images have been manually registered to each other. Our proposed CPA method enables this promising part-based strategy to be applicable in more practical scenarios, by automatically registering the gallery and probe face images in terms of deformable parts. In this paper, we present experiments on the Multi-PIE [32] and MUCT [33] datasets and show that our proposed CPA method can simultaneously and effectively handle illumination, pose, and/or expression variations.\n\u2022 Comparing with the natural alternative methods that holistically align face images, followed by either holistic face recognition or part-based recognition, our method gives significantly improved performance. \u2022 State-of-the-art pose-invariant face recognition method [34] relies on model learning using a large number of 3D face shapes, while training of our proposed CPA only requires 2D face images of a few subjects. Nevertheless, our method outperforms that of Li et al. [34]. when the degrees of pose change are within \u00b115\u25e6. This range of pose change is often encountered in practical access control scenarios, where test subjects would be cooperative with face recognition systems. \u2022 Notably, while CPA is motivated to address the challenges of face recognition across pose/expression, it performs surprisingly well when probe face images are at frontal view and with neural expression. Our results in the frontalview, neutral-expression, varying-illumination, and acrosssession setting of Multi-PIE dataset are better than all existing methods, and as high as 99.6%. This confirms that considering human face as a piece-wise planar surface and aligning it part-wisely are very effective for high-accuracy face recognition. \u2022 We also empirically investigate the discriminative power of individual facial parts, and the robustness of our method against partial occlusion.\nDetails of these investigations are presented in Section VI. The rest of this paper are organized as follows. Section II reviews more related work in addition to those discussed in Section I. Section III presents our proposed CPA model and how to use it for alignment of probe face images. Section IV combines CPA with existing methods for part-based face recognition. Details of CPA model learning are presented in Section V, where we also extend CPA to a mixture of CPA for better handling pose or expression variations. Intensive experiments are finally reported in Section VI to show the\nefficacy of our proposed method.\nII. RELATED WORK\nAs discussed in Section I, most of existing methods for face recognition across pose or expressions assume gallery and probe face images have been manually registered, and they can only be applied to controlled scenarios. In particular, Chai et al. [13] learned local patch based mapping relations from nonfrontal pose to frontal pose by locally linear regression. Jorstad et al. [15] estimated pixel-wise registration from non-neutral expression to neutral expression by optical flow. Arashloo and Kittler [16] considered a Markov random field (MRF) model to regularize 2D displacements of local patches across different poses. For automatic face recognition across pose, given a probe face image, AAMs [18, 19] optimized localization of a set of facial landmarks to realize face alignment of pixel-level accuracy. However, the landmarks localized by AAMs are usually not accurate enough, and also the pixelwise correspondence induced by matched landmarks are not consistent enough across different poses and expressions [35]. Among existing methods, maybe the most successful ones across pose and/or expression are based on 3D models [36, 35, 34]. In spite of their promise, their relying on 3D data makes them less relevant to the 2D techniques considered in this paper. However, we will show that our proposed CPA method compares favorably with them when the pose variations are in a reasonably confined range.\nDeformable part models (DPM) have shown their success in object detection [24], facial landmark localization [25], and human pose estimation [37]. In particular, these methods optimize an objective function that scores both the appearance evidence and spatial constellation of the parts, where the former is scored by part detectors, and the latter is scored by a staror tree-structured shape model, which constrain the pair-wise offsets of different parts. Our use of part-based models is different from the DPM based methods [24, 25, 37]. Our\nCPA method aims at pixel-wise image alignment rather than finding the bounding box of an object/part or locating a small amount of landmarks. We measure the appearance evidence of parts by their similarity to aligned galleries, and constrain the part deformation relations by a tree-structured shape model. Compared with shape constraints in DPMs [25, 37], our proposed one models more complex relations with the part constellation and holds strict probabilistic properties that are beneficial to the model learning.\nOur method is also closely related to the work of Wagner et al. [3]. To cope with illumination variation, they used multiple gallery images of varying illuminations per subject. Our method also follows this strategy. However, we take a human face as a collection of parts rather than assuming it as a planar surface as in the method of Wagner et al. [3]. The piece-wise planar assumption used in our CPA model can handle much more complex facial appearance variations such as large pose and/or facial expression changes. Furthermore, we regularize the deformations of individual parts by a shape constraint in order to prevent the alignment from degenerated solutions. Such regularization term is absent in the method of Wagner et al. [3].\nIII. CPA: CONSTRAINED PART-BASED ALIGNMENT\nAssume we have face images of multiple subjects in a database, and these images have been registered into some canonical form, i.e., aligned to some template. The template we consider in this paper is composed of a collection of parts. These parts of varying sizes are spatially arranged in a proper manner so that overall they define our face template (Fig. 1-i). Suppose there are m parts in the template. Each of the m parts has its associated face region in each image in the database. We call these face regions associated with every ith part a part dictionary, which is denoted as D\u3008i\u3009 (Fig. 1-v). Let y be a test face image that is not generally in the canonical form (e.g, due to pose change or misalignment). As the name of CPA suggests,\nconstrained part-based alignment aims to align y to the face template so that different face regions of y are respectively registered with the corresponding parts of the face template. This part-based alignment can be realized by pursuing a set of transformations \u03bd = (\u03bd\u30081\u3009, \u03bd\u30082\u3009, . . . , \u03bd\u3008m\u3009) that act on the image domain of y by y \u25e6 \u03bd\u3008i\u3009, i = 1, . . . ,m. 1\nTo pursue \u03bd, we consider techniques used in [26, 3]. In particular, Wright et al. [26], Wagner et al. [3] assumed there exist multiple registered face images of varying illuminations per subject in the database. A well-aligned test image could be represented by a linear combination of registered training images, plus a sparse error term to compensate for data corruption or various intra-subject variations. By leveraging the error sparsity assumption, Wagner et al. [3] optimized a holistic similarity transformation by solving a `1-norm minimization problem. Extending techniques in [3] directly to part-based alignment gives the following objective\nmin x\u3008i\u3009,e\u3008i\u3009,\u03bd i=1,2,...,m m\u2211 i \u03bb\u3008i\u3009\u2016e\u3008i\u3009\u20161\ns.t. y \u25e6 \u03bd\u3008i\u3009 = D\u3008i\u3009x\u3008i\u3009 + e\u3008i\u3009, (2)\nwhere \u2016\u00b7\u20161 is the `1-norm that encourages the sparsity of errors {e\u3008i\u3009}mi=1. y \u25e6\u03bd\u3008i\u3009 aligns the ith part of y, and {\u03bb\u3008i\u3009 \u2208 R+}mi=1 balances the alignment errors of the m parts. The matrix D\u3008i\u3009 is the ith part dictionary, whose columns represent the aligned regions of face images in the database, and x\u3008i\u3009 denotes the reconstruction coefficient.\nThe above direct extension of [3] essentially aligns the m parts independently. Unfortunately, it empirically appears to be\n1Let {(u, v)} denote image coordinates of y, and \u03bd\u3008i\u3009 be a 2-D coordinate transformation function. Then, image transformation is realized by y \u25e6 \u03bd\u3008i\u3009(u, v) = y(\u03bd\u3008i\u3009(u, v)), and we also use y \u25e6 \u03bd\u3008i\u3009 denotes the transformed image (the ith part). {\u03bd\u3008i\u3009}mi=1 belong to a d-dimensional transformation group G, e.g. the similarity group or affine group. In this paper, we assume G to be the 2-D similarity group and parameterize the similarity transformation mapping (u, v) to (u\u2032, v\u2032) as (tu, tv , s, \u03b8)T \u2208 R4 that satisfies(\nu\u2032 v\u2032\n) = exp(s) ( cos \u03b8 \u2212 sin \u03b8 sin \u03b8 cos \u03b8 )( u v ) + ( tu tv ) . (1)\nWith a little abuse of notation, we simultaneously take \u03bd\u3008i\u3009 as a transform function and a column vector composed of the d parameters of G (d = 4). Accordingly, \u03bd \u2208 Rd\u00d7m.\nvery unstable for facial part alignment. As shown in Fig. 2b, the parts may often drift away from their initialization to some more \u201cflat\u201d face regions. Indeed, alignment by optimizing \u03bd\u3008i\u3009 in y\u25e6\u03bd\u3008i\u3009 is a non-convex procedure (as explained in more details in Section III-B1). Compared with an entire face, individual parts contain less visual structure and thus alignment of them is prone to meaningless local minima. To overcome this problem, we consider in this paper incorporating some sort of global information of facial structure to regularize the deformations of individual parts. In particular, we are motivated by [25] to use a tree-structured shape model to constrain the difference of transformation parameters of different parts. We write g (\u03bd,Z) for the regularization term determined by the tree-structured shape model, where Z denotes model parameters in the form of a tuple. The tree-structured shape model is illustrated in Fig. 1 i - iii and will be elaborated in Section III-A. With consideration of the tree-structured shape constraint, we revise (2) to formulate the alignment objective of our proposed CPA model as\nmin x\u3008i\u3009,e\u3008i\u3009,\u03bd i=1,2,...,m m\u2211 i \u03bb\u3008i\u3009\u2016e\u3008i\u3009\u20161 + \u03b7g (\u03bd,Z)\ns.t. y \u25e6 \u03bd\u3008i\u3009 = D\u3008i\u3009x\u3008i\u3009 + e\u3008i\u3009, (3)\nwhere \u03b7 \u2208 R+ weights the regularization term. The choice of {\u03bb\u3008i\u3009}mi=1 will be addressed when we learn the CPA model in Section V. As shown in Fig. 2c, our proposed CPA method can produce very stable alignment results using the same part dictionaries (Fig. 2a) as independent part alignment does. In Fig. 3, we give all the part instances of the CPA model that are fitted to real face images. We note that CPA is not designed to produce a seamless face constituted by deformed individual parts, as AAMs [18, 19] can do. Instead, it aims to establish correspondence of facial parts across pose or expression so that face images with intra-subject variations can be matched at the part level to improve face recognition."}, {"heading": "A. Tree-structured Shape Model", "text": "Let (V, E) denote the tree, where V = {0, 1, 2, . . . ,m} is the vertex set, and E is the set of directed edges. V is composed of the nodes of the m facial parts and a root node (the node \u201c0\u201d)\n5\ncorresponding to the holistic face. Every ith node takes \u03bd\u3008i\u3009 as its variables corresponding to the transformation parameters of the facial part. For the root node, we use \u03bd\u30080\u3009 = (0, 0, 0, 0)T when assuming the face at the global level has been registered into the canonical form, which suggests (u, v) = \u03bd\u30080\u3009(u, v). We consider this simplified case in the following derivation of the tree-structured shape model.\nIn order to build a model on the tree, we first associate with each edge the difference of transformation parameters of its two end nodes. We then concretize g (\u03bd,Z) as follows. In particular, for the edge (j, i) \u2208 E with parent node j and child node i, we use a multivariate Gaussian distribution with mean \u00b5\u3008i\u3009 \u2208 Rd and precision \u039b\u3008i\u3009 \u2208 Rd\u00d7d to model the transformation difference \u03bd\u3008i\u3009\u03b4 = \u03bd\n\u3008i\u3009 \u2212 \u03bd\u3008j\u3009 \u2208 Rd, i.e., p(\u03bd \u3008i\u3009 \u03b4 |z\u3008i\u3009) = N (\u03bd \u3008i\u3009 \u03b4 ;\u00b5 \u3008i\u3009,\u039b\u3008i\u3009 \u22121\n) with z\u3008i\u3009 = (\u00b5\u3008i\u3009,\u039b\u3008i\u3009), where the Gaussian probability distribution function (PDF) is\nN (\u03bd\u03b4;\u00b5,\u039b\u22121) = |\u039b|1/2\n(2\u03c0)d/2 exp\n{ \u22121\n2 (\u03bd\u03b4 \u2212 \u00b5)T\u039b(\u03bd\u03b4 \u2212 \u00b5)\n} .\n(4) Thus, let Z = (z\u30081\u3009, z\u30082\u3009, . . . , z\u3008m\u3009) be a tuple of Gaussian parameters.\nSince the definition of p(\u03bd\u3008i\u3009\u03b4 |z\u3008i\u3009) is free of \u03bd\u3008j\u3009 for (j, i) \u2208 E , we have essentially assumed the independence between \u03bd \u3008i\u3009 \u03b4 and \u03bd\n\u3008j\u3009, which suggests p(\u03bd\u3008i\u3009|\u03bd\u3008j\u3009, z\u3008i\u3009) = p(\u03bd\u3008i\u3009\u03b4 |z\u3008i\u3009). Thus, we take our tree-structured shape model as a Bayesian network, whose joint probability is\np({\u03bd\u3008i\u3009}mi=0|Z) = p(\u03bd\u30080\u3009) \u00b7 \u220f\n(j,i)\u2208E\np(\u03bd\u3008i\u3009|\u03bd\u3008j\u3009, z\u3008i\u3009)\n= m\u220f i=1 p(\u03bd \u3008i\u3009 \u03b4 |z \u3008i\u3009). (5)\nwhere p(\u03bd\u30080\u3009) \u2261 1. Finally, taking the negative logarithm of the joint probability gives the regularization term g (\u03bd,Z) in (3) as\ng (\u03bd,Z) =\u2212 ln p({\u03bd\u3008i\u3009}mi=0|Z)\n= 1\n2 m\u2211 i=1 (\u03bd \u3008i\u3009 \u03b4 \u2212 \u00b5 \u3008i\u3009)T\u039b\u3008i\u3009(\u03bd \u3008i\u3009 \u03b4 \u2212 \u00b5 \u3008i\u3009) + b, (6)\nwhere b = dm2 ln(2\u03c0)\u2212 1 2 \u2211m i=1 ln |\u039b\u3008i\u3009| is the term independent of {\u03bd\u3008i\u3009\u03b4 }mi=1.\nSimilar tree-structured shape model and quadratic regularization term were also used in [25] for face detection and facial landmark localization. However, they ignored the dependency among variables of the difference of transformation parameters associated with any pair of facial parts connected by an edge in the tree, which, in our case, equals to constrain \u039b\u3008i\u3009 to be diagonal. Compared to [25], our shape constraint is derived from a Bayesian network formulation, which not only interprets the underlying probabilistic properties of treestructured shape models, but also enables maximum a posteriori (MAP) estimation of the model parameters Z in the training stage of CPA. We will show later that the probabilistic prior introduced for MAP estimation prevents the CPA training from degenerate solutions when image alignment and tree-model learning are jointly formulated in an unsupervised way. This scenario is different from [25], as their tree-structured shape model is integrated in a classification problem, which is strongly supervised.\nIn fact, parametrization of AAMs and CLMs [21, 23] is also based on a joint Gaussian model, which seems to be similar to (6) derived from a tree model. To see the difference, if we concatenate the variables {\u03bd\u3008i\u3009}mi=1 as vec(\u03bd) and model it as a Gaussian distribution, the corresponding precision matrix, denoted as \u039b \u2208 Rdm\u00d7dm, will have a block sparse structure with at most d2(m+ |{(j, i) \u2208 E : j 6= 0}|) non-zero entries. The assumption on the independence between \u03bd\u3008i\u3009\u03b4 and \u03bd\n\u3008j\u3009 for (i, j) \u2208 E , which constrains \u039b\u2019s degree of freedom to be d2m, distinguishes the tree-structured shape model from a general joint Gaussian model with block diagonal precision.\nUp to now we have assumed the face at the global level has been registered into the canonical form. In practice, however, observed face images are not generally in this canonical form. Consequently, the learned tree-structured shape model for a globally canonical face will not apply directly. To mitigate this problem, we consider optimizing an additional holistic transformation \u03c3, so that the learned tree-structured shape model can still be useful to constrain the optimization of \u03bd. The CPA objective (3) is then rewritten as\nmin x\u3008i\u3009,e\u3008i\u3009,\u03bd,\u03c3 i=1,2,...,m m\u2211 i \u03bb\u3008i\u3009\u2016e\u3008i\u3009\u20161 + \u03b7g (\u03bd,Z)\ns.t. y \u25e6 \u03c3 \u25e6 \u03bd\u3008i\u3009 = D\u3008i\u3009x\u3008i\u3009 + e\u3008i\u3009. (7)\nWe call \u03c3 the holistic deformation, and keep calling \u03bd\u3008i\u3009 the ith part deformation."}, {"heading": "B. Optimization", "text": "Given a learned tree-structured shape model, we present algorithms to solve our CPA objective (7) in this subsection. The main difficulty of solving (7) comes from the non-convexity of its constraints y \u25e6 \u03c3 \u25e6 \u03bd\u3008i\u3009 = D\u3008i\u3009x\u3008i\u3009 + e\u3008i\u3009, i = 1, . . . ,m, which couples the nonlinear operations of holistic deformation \u03c3 and part deformations {\u03bd\u3008i\u3009}mi=1 on the image domain. We choose the strategy of alternating optimization: we first update {x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1, and \u03bd together while fixing \u03c3, and we\n6 then update \u03c3 2. These two steps are alternately applied until the algorithm converges. Note that this alternating strategy requires relatively good initialization of \u03c3 and \u03bd, so that the learned tree-structured shape model can be effective to constrain the optimization of \u03bd. We defer the discussion of this issue in Section III-B3 while assuming at this moment that the initial \u03c3 and \u03bd are good enough to start the alternating process.\n1) Solving Part Deformations : Given fixed \u03c3, we update each \u03bd\u3008i\u3009 by a generalization of the Gauss-Newton method. More specifically, for a linear update from \u03bd\u3008i\u3009 to \u03bd\u3008i\u3009+\u2206\u03bd\u3008i\u3009, the left-hand side of the equality constraint in (7) can be approximated by its first-order Taylor expansion at \u03bd\u3008i\u3009, i.e., y \u25e6 \u03c3 \u25e6 (\u03bd\u3008i\u3009 + \u2206\u03bd\u3008i\u3009) \u2248 y \u25e6 \u03c3 \u25e6 \u03bd\u3008i\u3009 + J\u3008i\u3009\u2206\u03bd\u3008i\u3009, where J\u3008i\u3009 . = \u2202(y \u25e6 \u03c3 \u25e6 \u03bd\u3008i\u3009)/\u2202\u03bd\u3008i\u3009 is the Jacobian with respect to (w.r.t.) \u03bd\u3008i\u3009. Let \u2206\u03bd = [\u2206\u03bd\u30081\u3009,\u2206\u03bd\u30082\u3009, . . . ,\u2206\u03bd\u3008m\u3009]. The above linearization leads to the following problem to optimize {x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1,\u2206\u03bd\nmin x\u3008i\u3009,e\u3008i\u3009,\u2206\u03bd i=1,2,...,m m\u2211 i \u03bb\u3008i\u3009\u2016e\u3008i\u3009\u20161 + \u03b7g (\u03bd + \u2206\u03bd,Z)\ns.t. y \u25e6 \u03c3 \u25e6 \u03bd\u3008i\u3009 + J\u3008i\u3009\u2206\u03bd\u3008i\u3009 = D\u3008i\u3009x\u3008i\u3009 + e\u3008i\u3009. (8)\nWe repeatedly solve the problem (8) to linearly update \u03bd, until converging to a local minimum, which gives the solution to the original problem (7). Similar iterative techniques have also been used in related works [38, 3], and showed good behaviors of convergence.\nWe solve the convex problem (8) by adapting the Augmented Lagrange Multiplier (ALM) method [39]. Let\nh(x\u3008i\u3009, e\u3008i\u3009,\u2206\u03bd\u3008i\u3009) =\ny \u25e6 \u03c3 \u25e6 \u03bd\u3008i\u3009 + J\u3008i\u3009\u2206\u03bd\u3008i\u3009 \u2212D\u3008i\u3009x\u3008i\u3009 \u2212 e\u3008i\u3009. (9)\nThe augmented Lagrangian function for (8) can be written as\nL\u03b2({x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1,\u2206\u03bd, {\u03b3\u3008i\u3009}mi=1) = m\u2211 i { \u03bb\u3008i\u3009\u2016e\u3008i\u3009\u20161 + \u2329 \u03b3\u3008i\u3009, h(x\u3008i\u3009, e\u3008i\u3009,\u2206\u03bd\u3008i\u3009) \u232a (10)\n+ \u03b2\n2 \u2225\u2225\u2225h(x\u3008i\u3009, e\u3008i\u3009,\u2206\u03bd\u3008i\u3009)\u2225\u2225\u22252 F } +g (\u03bd + \u2206\u03bd,Z) ,\nwhere {\u03b3\u3008i\u3009}mi=1 are the Lagrange multiplier vectors, \u2016\u00b7\u2016F denotes matrix Frobenius norm, and \u3008\u00b7, \u00b7\u3009 denotes inner product of vectors or matrices. Instead of directly solving the constrained problem (8), ALM searches for a saddle point of (10). Given initial {\u03b3\u3008i\u3009}mi=1, it iteratively and alternately updates {x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1,\u2206\u03bd and {\u03b3\u3008i\u3009}mi=1 by\n1) {x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1,\u2206\u03bd \u2190\narg min x\u3008i\u3009,e\u3008i\u3009,\u2206\u03bd i=1,2,...,m\nL\u03b2l({x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1,\u2206\u03bd, {\u03b3\u3008i\u3009}mi=1); (11)\n2) \u03b3\u3008i\u3009 \u2190 \u03b3\u3008i\u3009 + \u03b2l \u00b7 h(x\u3008i\u3009, e\u3008i\u3009,\u2206\u03bd\u3008i\u3009), for i = 1, 2. . . . ,m;\n2Instead of updating \u03c3, {x\u3008i\u3009}mi=1, and {e\u3008i\u3009}mi=1 while fixing \u03bd in the second step, we propose a more efficient approach that jointly updates \u03c3 and \u03bd while keeping {x\u3008i\u3009}mi=1 and {e\u3008i\u3009}mi=1 fixed. Details of the approach will be presented in Section III-B2.\nwhere l is the iteration number, and {\u03b2l}l=1,2,... is an increasing sequence. ALM is proven to converge to the optimum of the original problem as \u03b2l becomes sufficient large [40].\nIt is still difficult to directly solve (11) w.r.t. all the three groups of variables: {x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1, and \u2206\u03bd. Instead, we consider updating them in an alternating manner. And it turns out that each subproblem associated with any one group of variables has a closed form solution. More specifically, let\nS\u03b1(x) = sign(x) \u00b7max{|x| \u2212 \u03b1, 0} (12)\nbe the soft thresholding operator. For i = 1, 2, . . . ,m, we update x\u3008i\u3009 and e\u3008i\u3009 sequentially by\nr\u3008i\u3009 \u2190 y\u3008i\u3009 \u25e6 \u03c3 \u25e6 \u03bd\u3008i\u3009 + (1/\u03b2l)\u03b3\u3008i\u3009, x\u3008i\u3009 \u2190 ( D\u3008i\u3009 T D\u3008i\u3009 )\u22121 D\u3008i\u3009 T (r\u3008i\u3009 + J\u3008i\u3009\u2206\u03bd\u3008i\u3009 \u2212 e\u3008i\u3009),\ne\u3008i\u3009 \u2190 S\u03bb\u3008i\u3009/\u03b2l(r \u3008i\u3009 + J\u3008i\u3009\u2206\u03bd\u3008i\u3009 \u2212D\u3008i\u3009x\u3008i\u3009),\nwhere r\u3008i\u3009 is an auxiliary variable used for notation convenience only. Then, the optimum \u2206\u03bd can be found by solving\n0 = \u2202L\u00b5({x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1,\u2206\u03bd, {\u03b3\u3008i\u3009}mi=1)\n\u2202\u2206\u03bd\u3008i\u3009 (13)\n=\u03b2l\n( r\u3008i\u3009 + J\u3008i\u3009\u2206\u03bd\u3008i\u3009 \u2212D\u3008i\u3009x\u3008i\u3009 \u2212 e\u3008i\u3009 ) J\u3008i\u3009\n+ \u03b7 \u2202g(\u03bd + \u2206\u03bd,Z)\n\u2202\u2206\u03bd\u3008i\u3009 . (14)\nfor i = 1, 2, . . . ,m. Let {\u03c2i}mi=1 denote the standard basis of Rm. The m equations together form a system about \u2206\u03bd with the following form:\nm\u2211 i=1 G\u3008i\u3009\u2206\u03bd\u03c2i\u03c2 T i + \u03b7 \u2202g(\u03bd + \u2206\u03bd,Z) \u2202\u2206\u03bdT = Q, (15)\nwhere G\u3008i\u3009 \u2208 Rd\u00d7d, and Q \u2208 Rd\u00d7m. Here, G\u3008i\u3009 = \u03b2lJ \u3008i\u3009TJ\u3008i\u3009, and the ith column of Q is \u03b2lJ\u3008i\u3009 T (D\u3008i\u3009x\u3008i\u3009 + e\u3008i\u3009 \u2212 r\u3008i\u3009). As g(\u03bd + \u2206\u03bd,Z) has the form of the summation of the quadratics of {\u2206\u03bd\u3008i\u3009}mi=1, (15) is a linear system. It is usually sparse due to the tree structure of the shape model. Hence, even a large number of facial parts are present, we can still simultaneously and efficiently solve {\u2206\u03bd\u3008i\u3009}mi=1 with high precision. We describe the expanded form of (15) in Appendix A.\nInstead of solving (11) exactly by converged iterations of alternating optimization of {x\u3008i\u3009}mi=1, {e\u3008i\u3009}mi=1 and \u2206\u03bd, we use inexact ALM that updates the three groups of variables alternately for only once in each iteration of the ALM method. Compared to exact ALM, inexact ALM shows better practical performance in terms of optimization efficiency [39]. In summary, we solve (7) by repeatedly solving the linearized problem (8), which itself can be efficiently solved by inexact ALM.\n2) Solving Holistic Deformation : Given fixed \u03bd in (7), the most straightforward way to optimize \u03c3, together with {x\u3008i\u3009}mi=1 and {e\u3008i\u3009}mi=1, is to use similar techniques as in Section III-B1, i.e., linearizing the constraint in (7) w.r.t. \u03c3 and using ALM to solve the linearized problem. In this paper, we propose an alternative approach that involves the variables \u03c3 and \u03bd only, without performing the actual image\n7 Algorithm 1 Outer loop for CPA \u2013 Default Option 1: Initialize \u03bd and \u03c3 2: while NOT converged do 3: \u03bd\u2190 arg min\n\u03bd min\nx\u3008i\u3009,e\u3008i\u3009 i=1,2,...,m\n\u2211m i \u2016e\u3008i\u3009\u20161 + \u03b7g (\u03bd,Z) s.t. (7)\n4: \u03b6\u3008i\u3009 \u2190 \u03c3 \u25e6 \u03bd\u3008i\u3009 5: \u03bd, \u03c3\u2190 arg min\n\u03bd,\u03c3 g (\u03bd,Z) s.t. \u03c3 \u25e6 \u03bd\u3008i\u3009 = \u03b6\u3008i\u3009\n6: end while\ndeformation, and is thus much more efficient compared to the aforementioned standard approach. More specifically, given fixed {x\u3008i\u3009}mi=1 and {e\u3008i\u3009}mi=1 obtained in the previous iteration of part deformations, the right-hand side of the equality constraint in (7) is also fixed. If we keep it unchanged in the update of \u03c3, it indicates that \u03c3 \u25e6 \u03bd\u3008i\u3009 for a combined deformation will not change. Denote \u03b6\u3008i\u3009 = \u03c3 \u25e6 \u03bd\u3008i\u3009 for this fixed combined deformation, we propose to update \u03c3 and \u03bd jointly by optimizing\nmin \u03bd,\u03c3\ng (\u03bd,Z) s.t. \u03c3 \u25e6 \u03bd\u3008i\u3009 = \u03b6\u3008i\u3009, (16)\nwhich does not involve any actual image deformation and can thus be efficiently solved. For similarity transformation considered in this paper, we present in Appendix B the associated parametrization of (16) and the optimization algorithm.\nAlgorithm 1 gives the outer loop of the algorithms presented in Sections III-B1 and III-B2 for the proposed constrained part-based alignment.\n3) Initializing Holistic and Part Deformations: Algorithms in Sections III-B1 and III-B2 require relatively good initialization of \u03c3 and \u03bd. In practice, we rely on off-the-shelf face detectors [41, 25], which provide a rough bounding box of the face or the holistic pose and locations of facial parts [25]. We initialize \u03c3 using the bounding box provided by face detectors. If locations of facial parts are available, we also use them to initialize \u03bd. Otherwise, we choose the initial \u03bd as the part deformations that maximize the likelihood of the tree-structured shape model. This initialization acts as the average template of the shape constraint, which is independent of any probe image.\nThe initialization based on face detectors is generally not accurate enough for face recognition. Fortunately, in most cases they are good enough for automatic alignment. Fig. 3 present examples that illustrate the converged solutions of our CPA algorithm.\nIn some cases when face detectors give worse outputs, our CPA algorithm might take a longer time to converge or might converge to an inconvenient solution. To handle this situation, we use the holistic alignment method of Wagner et al. [3] to initialize the holistic transformation \u03c3. Their method is not able to align non-frontal faces well due to its holistic planar surface assumption, however, it is good enough to be used as our initialization.\nIV. CPA BASED FACE RECOGNITION\nIn this section, we propose a CPA based face recognition method. Suppose there are N subjects in the gallery set, and each of them has multiple face images. Suppose these face\nimages have been aligned at the part level to form the part dictionaries, which are denoted as D\u3008i\u3009 = [D\u3008i\u30091 , . . . , D \u3008i\u3009 N ] for i = 1, . . . ,m. Given a probe face y, CPA suggests aligning facial parts of y to {D\u3008i\u3009}mi=1 by solving (7) for a part-based face recognition. However, similar to the situation of holistic face alignment in [3], the presence of facial parts of multiple subjects in {D\u3008i\u3009}mi=1 makes (7) have many local minima, corresponding to aligning y to the facial parts of different subjects. Instead, one can perform CPA in a subject-wise manner by optimizing\nmin x \u3008i\u3009 s ,e \u3008i\u3009 s ,\u03bds,\u03c3s\ni=1,2,...,m\nm\u2211 i \u03bb\u3008i\u3009\u2016e\u3008i\u3009s \u20161 + \u03b7g (\u03bds,Z)\ns.t. y \u25e6 \u03c3s \u25e6 \u03bd\u3008i\u3009s = D\u3008i\u3009s x\u3008i\u3009s + e\u3008i\u3009s , (17)\nwhere \u03bds = [\u03bd \u30081\u3009 s , . . . , \u03bd \u3008m\u3009 s ], \u03c3s, and {e\u3008i\u3009s }mi=1 are variables of part deformations, holistic deformation, and the alignment residuals w.r.t. the sth subject respectively. After solving (17) for all the N subjects, for every ith part we sort the subjectwise alignment residuals {\u2016e\u3008i\u3009s \u20161}Ns=1 and select the top C subjects with the smallest alignment residuals. Part dictionaries of these C selected subjects are then put together to form a pruned gallery of facial parts. To make the facial parts in the pruned gallery all aligned to the ith part of y, we transform D \u3008i\u3009 s by (\u03c3s \u25e6\u03bd\u3008i\u3009s )\u22121 instead of transforming y by \u03c3s \u25e6\u03bd\u3008i\u3009s for each selected subject s. For part-based face recognition, many existing methods such as SRC [26], LBP [42] and LDA [43] can be used at the part level, based on the pruned gallery. Final recognition can be performed by aggregating the part-level decisions using the basic plurality voting scheme.\nFor obtaining better performance, we may adapt advanced aggregating schemes, such as the kernelized plurality voting [29], and joint recognition method for multiple observations, such as the joint dynamic sparse representation [30]. Nonetheless, we consider only the basic choice to keep our work from obesity.\nAlgorithm 2 gives a summary of our proposed CPA based face recognition method. An illustrative procedure with 6 sequential modules is also shown in Fig. 4. Note that there are a few technical details in Algorithm 2 that can make a difference to recognition performance. In particular, as the selected subjects are probably inconsistent for different parts, we combine them together to substitute the selected subject set for each part (Line 13) so that the groundtruth subject has a higher possibility to be included for part recognition. Further, to control the number of overall selected subjects, we set C to the smallest integer making the pruned gallery size no less than a given parameter P (Line 13). We also adjust the ith part of y by the averaged transform of the ith part dictionaries of the selected subjects before aligning them to it (Line 15,16).\nV. MODEL LEARNING FOR CPA\nAlgorithms in the preceding sections assume that the CPA model, i.e., the tree-structured shape model and its associated part dictionaries, have been given. In this section, we present how to learn them from a training gallery of face images. Note that each part dictionary is composed of a set of registered\nAlgorithm 2 CPA based face recognition Require: Subject-wise part dictionaries { {D\u3008i\u3009s }mi=1 }N s=1\nfor the m parts and N subjects; the transformation group G; the parameter tuple Z of tree-structured shape model ; the probe image y; and, the chosen subject number P Ensure: identity(y) . Subject-wise alignment\n1: for each subject s do 2: Align y by CPA using {D\u3008i\u3009s }mi=1,\nand get {e\u3008i\u3009s }mi=1, {\u03bd \u3008i\u3009 s }mi=1, \u03c3s\n3: for each part i do 4: \u03b6 \u3008i\u3009 s \u2190 \u03bd\u3008i\u3009s \u25e6 \u03c3s 5: end for 6: end for . Pruning subjects 7: for each part i do 8: Sort subjects in ascending order according to {\u2016e\u3008i\u3009s \u20161}Ns=1,\nand get the orders s\u3008i\u30091 , s \u3008i\u3009 2 , . . . , s \u3008i\u3009 N\n9: end for 10: for j = 1, 2, . . . , N do 11: Bj \u2190 \u22c3m i=1{s \u3008i\u3009 l : l = 1, 2 . . . , j} 12: end for 13: C \u2190 j s.t. |Bj\u22121| < P \u2264 |Bj | . Part-wise recognition 14: for i = 1, 2, . . . ,m do 15: \u03b6\u0304\u3008i\u3009 \u2190 mean{\u03b6\u3008i\u3009s : s = s\u3008i\u30091 , s \u3008i\u3009 2 , . . . , s \u3008i\u3009 C } 16: y\u0302\u3008i\u3009 \u2190 y \u25e6 \u03b6\u0304\u3008i\u3009 17: for each s \u2208 BC do 18: \u03b6\u0302 \u3008i\u3009 s \u2190 \u03b6\u3008i\u3009s \u25e6 (\u03b6\u0304\u3008i\u3009) \u22121 19: D\u0302 \u3008i\u3009 s \u2190 D\u3008i\u3009s \u25e6 \u03b6\u3008i\u3009s 20: end for 21: Use an existing recognition method to recognize y\u0302\u3008i\u3009\nwith the pruned gallery {D\u0302\u3008i\u3009s , s}s\u2208BC , and get the predicted label id\u3008i\u3009\n22: end for . Decision by voting 23: identity(y)\u2190the label occurs the most times in {id\u3008i\u3009}mi=1\nfacial parts of gallery images. For some facial parts, e.g., those around the chin position, it is even difficult to manually annotate facial landmarks in order to align them with high precision. To realize the automatic alignment of facial parts to form the part dictionaries, we propose an approach that couples the learning of a tree-structured shape model and that of part dictionaries. Indeed, the relational constraints among deformations of different parts provided by the tree-structured shape model make alignment of part dictionaries become stable, and the deformation parameters of facial parts of the training images also give statistical evidence to learn the parameters Z in the CPA model. Learning them in a coupled way is thus a natural choice. In the following, before presenting algorithmic details of this coupled CPA model learning, we first present how to align and form the part dictionaries given the constraint from a known tree-structured shape model, which will serve as one of the alternating steps in the coupled CPA model learning algorithm."}, {"heading": "A. Learning Part Dictionaries with Constraint from Known", "text": "Tree-structured Shape Model\nWe learn the part dictionaries from a training gallery of n face images, denoted as D = [d1, d2, . . . , dn]. Every kth column dk in D represents a stacked vector form of face image in the gallery. Learning part dictionaries is concerned with optimizing the d-dimensional {\u03c3k \u2208 G}nk=1 for holistic deformations and {\u03bd\u3008i\u3009k \u2208 G}nk=1, i = 1, . . . ,m, for part deformations, so that after part-based alignment every ith part dictionary, denoted as D\u3008i\u3009 = [d\u3008i\u30091 , d \u3008i\u3009 2 , . . . , d \u3008i\u3009 n ] with d \u3008i\u3009 k . = dk \u25e6 \u03c3k \u25e6 \u03bd\u3008i\u3009k , contains facial parts that have been registered into some canonical form, as shown in Fig. 5b. We observe that the registered facial parts in each part dictionary correspond to appearance of different subjects at the same facial region. These registered facial parts will ideally resemble each other if nuisances due to inter-subject, illumination3, expression, and/or pose variations can be decomposed out. In other words, the part dictionary D\u3008i\u3009 would be low-rank after decomposing out the aforementioned nuisances, which can be modeled as a sparse error matrix. This sparse and low-rank decomposition was used by Peng et al. [38] to align a batch of linearly correlated images, such as frames in a video sequence or face images of a same subject. Motivated\n3If sufficient number of illumination conditions exist, illumination variations may not be classified as a nuisance in that they can be linearly modeled for low-rankness.\n9 Left eye Left ear Right corner of mouth Jaw D E A\n(a) Independent learning by Peng et al. [38]\u2019s method at 20 iterations\nLeft eye Left ear Right corner of mouth\nJaw\nD\nE\nA\n(b) Learning with a known shape constraint at the converged stage.\nFigure 5: Part dictionary learning. Parts are initialized at dashed red boxes, and aligned in batch. The solid green boxes denote the obtained domains of the parts. D is a learned part dictionary (images cropped from the obtained domains), and A+E is its best low-rank and sparse decomposition, where A is the lowrank component, and E is the sparse error. The cropped images are normalized in terms of intensity for display convenience.\nby [38], in this paper, we leverage the very similar low-rank (matrices of part dictionaries without nuisances) and sparsity (error matrices modeling various variations) properties to align and form the part dictionaries. However, if we directly apply techniques of Peng et al. [38] to perform independent partbased alignment, the alignment process often converges to less meaningful solutions as shown in Fig. 5a. The reason is similar to that causing failure of applying the method of Wagner et al. [3] in the case of aligning parts of a probe face to the gallery set, as we explained in Section III. Instead, we constrain part-based alignment for learning part dictionaries by the tree-structured shape model. .\nTo simplify the notations, we write \u03c3 = [\u03c31, . . . , \u03c3m] \u2208 Rd\u00d7n, and combine {\u03bd\u3008i\u3009k } i=1,2,...,m k=1,2,...,n into a third-order tensor\n\u03bd\u030c \u2208 Rd\u00d7m\u00d7n. We define two operators that apply on \u03bd\u030c as T \u3008i\u3009(\u03bd\u030c) = [ \u03bd \u3008i\u3009 1 , \u03bd \u3008i\u3009 2 , . . . , \u03bd \u3008i\u3009 n ] \u2208 Rd\u00d7n,\nTk(\u03bd\u030c) = [ \u03bd \u30081\u3009 k , \u03bd \u30082\u3009 k , . . . , \u03bd \u3008m\u3009 k ] \u2208 Rd\u00d7m,\nwhere T \u3008i\u3009(\u00b7) extracts deformation parameters of the ith part of the n face images, and Tk(\u00b7) extracts those of m facial parts of the kth image. We then write D\u3008i\u3009 .= D \u25e6\u03c3 \u25e6T \u3008i\u3009(\u03bd\u030c), where \u201c\u25e6\u201d applies column-wisely. With these definitions our objective for learning part dictionaries can be written as\nmin A\u3008i\u3009,E\u3008i\u3009,\u03bd\u030c,\u03c3 i=1,2,...,m { m\u2211 i=1 ( \u2016A\u3008i\u3009\u2016\u2217 + \u03bb\u3008i\u3009\u2016E\u3008i\u3009\u20161 )\n+ \u03b7 n\u2211 k=1 g (Tk(\u03bd\u030c),Z)\n}\ns.t. D \u25e6 \u03c3 \u25e6 T \u3008i\u3009(\u03bd\u030c) = A\u3008i\u3009 + E\u3008i\u3009, (18)\nwhere \u2016 \u00b7 \u2016\u2217 is the nuclear norm, which is a convex surrogate function of matrix rank. As shown by Peng et al. [38], Cand\u00e8s et al. [44], the penalty parameters {\u03bb\u3008i\u3009}mi=1 can be set as the reciprocal of the square root of E\u3008i\u3009\u2019s row number, denoted as \u03c9\u3008i\u3009. In this paper, we set \u03bb\u3008i\u3009 = \u03bb\u0302/ \u221a \u03c9\u3008i\u3009 with the constant\n\u03bb\u0302 = 1. We accordingly set \u03b7 = \u03b7\u0302 \u00b7 \u2211m i=1(\u03c9\n\u3008i\u3009)\u22121/2 with \u03b7\u0302 chosen as 0.02. To solve (18), we use a similar alternating strategy as in Section III-B. Appendix C gives details of the algorithmic procedure. It requires the initialization of \u03bd\u030c, which can be obtained using the same method as in Session III-B3. Fig. 5b shows example images of learned part dictionaries by solving (18). Compared with independent part-based alignment (example results shown in Fig. 5a), the proposed alignment method with the constraint from a tree-structured model gives much more meaningful results."}, {"heading": "B. Learning the Tree-structured Shape Model Jointly with Part Dictionaries", "text": "To jointly learn the tree-structured shape model and part dictionaries, it is natural to take Z in (18) as the additional variables to optimize. To solve this revised problem, one can alternately update part dictionaries using the algorithms in Appendix C, which involves the variables {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1, \u03bd\u030c, and \u03c3, and estimate Z by maximum likelihood (ML) estimation, which fits parameters {z\u3008i\u3009 = (\u00b5\u3008i\u3009,\u039b\u3008i\u3009)}mi=1 of Gaussian distributions with the updated {Tk(\u03bd\u030c)}nk=1. Unfortunately, this approach empirically appears to give degenerate solutions corresponding to a \u201cnon-elastic\u201d shape model, where the Gaussian distributions modeling the deformation differences of connected nodes in the tree have variances of almost zero magnitude. Fig. 6a illustrates this phenomenon, where we measure the determinant and nuclear norm4 of the covariance matrices {\u039b\u3008i\u3009\u22121}mi=1, which are iteratively updated in an alternating optimization process.\n4For a Gaussian distribution, the determinant of its covariance matrix is the product of the standard deviations in its principle directions; and, the nuclear norm is the sum of the variances in those directions. Both of the two indicates the overall strength of its variances.\n10\nTo remedy this problem, we consider imposing a prior on Z to regularize the learning of tree-structured shape model. Denote h(Z,\u03a6) = \u2212 ln p(Z|\u03a6) with \u03a6 as the hyperparameters. Incorporating h(Z,\u03a6) into (18) results in the following new objective for joint learning of Z and part dictionaries\nmin A\u3008i\u3009,E\u3008i\u3009,\u03bd\u030c,\u03c3,Z\ni=1,2,...,m\n{ m\u2211 i=1 ( \u2016A\u3008i\u3009\u2016\u2217 + \u03bb\u3008i\u3009\u2016E\u3008i\u3009\u20161 )\n+\u03b7 ( n\u2211 k=1 g (Tk(\u03bd\u030c),Z) + h(Z,\u03a6) )}\ns.t. D \u25e6 \u03c3 \u25e6 T \u3008i\u3009(\u03bd\u030c) = A\u3008i\u3009 + E\u3008i\u3009. (19)\nLet \u03bd\u3008i\u3009\u03b4,k = \u03bd \u3008i\u3009 k \u2212 \u03bd \u3008j\u3009 k . Based on (5) we have\nn\u2211 k=1 g (Tk(\u03bd\u030c),Z) = \u2212 m\u2211 i=1 ln p({\u03bd\u3008i\u3009\u03b4,k} n k=1|z\u3008i\u3009), (20)\nwhere z\u3008i\u3009 = (\u00b5\u3008i\u3009,\u039b\u3008i\u3009), and \u03bd\u3008i\u3009\u03b4,k is drawn from the Gaussian distribution with mean \u00b5\u3008i\u3009 and precision \u039b\u3008i\u3009. We impose prior on {\u00b5\u3008i\u3009,\u039b\u3008i\u3009} using Gaussian-Wishart distribution, which is the conjugate prior of the Gaussian distribution. Denote \u03c6\u3008i\u3009 as the parameters of the Gaussian-Wishart prior for z\u3008i\u3009. We\nhave\nh(Z,\u03a6) = \u2212 m\u2211 i=1 ln p(z\u3008i\u3009|\u03c6\u3008i\u3009). (21)\nwhere \u03a6 = (\u03c6\u30081\u3009, \u03c6\u30082\u3009, . . . , \u03c6\u3008m\u3009) is the tuple of the hyperparameters.\nGiven \u03a6, we solve (19) by alternately updating the following two steps:\n1) Update Z by solving minZ \u2211n k=1 g (Tk(\u03bd\u030c),Z) +\nh(Z,\u03a6); 2) Update {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1, \u03bd\u030c,\u03c3 by solving (18).\nTo update the first step, note that by Bayes\u2019 rule, n\u2211 k=1 g (Tk(\u03bd\u030c),Z) + h(Z,\u03a6) (22)\n=\u2212 m\u2211 i=1 { ln p(z\u3008i\u3009|{\u03bd\u3008i\u3009\u03b4,k} n k=1, \u03c6 \u3008i\u3009) + ln p({\u03bd\u3008i\u3009\u03b4,k} n k=1) } ,\nwhere p({\u03bd\u3008i\u3009\u03b4,k}nk=1) does not change with z\u3008i\u3009, and the terms of the summation have no overlap variables. Thus the above first step can be updated by independently solving the m subproblems\nmax z\u3008i\u3009\np(z\u3008i\u3009|{\u03bd\u3008i\u3009\u03b4,k} n k=1, \u03c6 \u3008i\u3009), (23)\nfor i = 1, . . . ,m. It is a standard maximum a posterior (MAP) inference for the Gaussian distribution, where the posterior\n11\nAlgorithm 3 CPA model learning Require: Training images D, initial \u03bd\u030c,\u03c3, and prior weight \u03d1 Ensure: learned \u03bd\u030c,\u03c3,Z\n1: Z \u2190 minZ \u2211n k=1 g (Tk(\u03bd\u030c),Z) 2: Set \u03a6 in consistency to Z and \u03d1 3: while not converged do 4: Update {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1, \u03bd\u030c,\u03c3 by solving (18). 5: Z \u2190 minZ \u2211n k=1 g (Tk(\u03bd\u030c),Z) + h(Z,\u03a6) 6: end while\nstill follows a Gaussian-Wishart distribution, whose parameters are different from those of the prior. The maximum point of a Gaussian-Wishart PDF can be derived in closed form in terms of its parameters. Please refer to Appendix D for details of the Gaussian-Wishart prior and inference problems relevant to it. For the above second step, it is the problem of learning part dictionaries given a fixed Z , which is addressed in the previous subsection.\nNow, it comes the problem: how to set hyper-parameters {\u03c6\u3008i\u3009}mi=1 properly? Since the initialization of \u03bd\u030c gives rough evidence on how the tree-structured shape model should be, we set \u03c6\u3008i\u3009 to be consistent with the ML estimation on z\u3008i\u3009 with the initial \u03bd\u030c for i = 1, 2, . . . ,m. Thus, the trained CPA model will not deviate much from its rough estimation. Meanwhile, we also set \u03c6\u3008i\u3009 properly to control the prior\u2019s weight, i.e., to make the prior contribute to the MAP estimation as much as \u03d1n training samples, where \u03d1 > 0. Still, please refer to Appendix D for how to set \u03c6 with explicit physical meaning. The above learning procedure for a CPA model is summarized in Algorithm 3.\nThe above algorithms for CPA model learning require a careful initialization on \u03c3 and \u03bd\u030c, which can be done either by detectors [41, 25] or by manual annotations. In particular, for every kth training image, we localize the two eye corners to determine the similarity transform \u03c3k. We then localize the facial landmarks at the centers of the facial parts5, and initialize \u03bd\u3008i\u3009k to satisfy that the i\nth part has the pre-defined orientation and size relative to the canonical form of the entire face that is determined by \u03c3k. Although this initialization scheme is very exercisable, but it makes the part transformation differences between linked parts identical for all the n training images. Consequently, we cannot get statistical evidence to learn Z . To overcome this difficulty, we propose a heuristic initialization scheme for learning the CPA model. In particular, we start with the part dictionary learning (18) by setting \u03b7 = 0, i.e., without any shape constraint, and run it for only a few (here, 5) iterations of the generalized Gaussian method composed of the repeats of the linearized problem (32) (in the Appendix C). The updated part transformations are used for the actual initialization of the CPA model learning.\nFig. 6b shows the effect of our proposed objective (19) for joint learning of the CPA model, where we investigate the covariance matrices {\u039b\u3008i\u3009\u22121}mi=1 of the Gaussian distributions obtained in each iteration of our algorithm. Compared with\n5We always design the parts to center at the facial landmarks that are easy to annotate.\nFig. 6a, the stability of their determinant and nuclear norm values suggests that a better tree-structured shape model is obtained.\nUp to now we have assumed that the configuration of the tree, i.e., how the nodes (facial parts) are connected to form the tree, is given. To learn the configuration of a tree, Zhu and Ramanan [25] used Chow and Liu [45] algorithm for the application of facial landmark localization. Chow-Liu algorithm finds the configuration of a Bayesian network that best approximates the joint distribution of all the variables. However, for a node with children, our tree-structured shape model (as well as Zhu and Ramanan [25]\u2019s shape model) assumes the independence between its part transformation and its part transformation differences with its children (refer to Section III-A), which is not an assumption for Chow-Liu algorithm.\nInstead, we take the tree configuration that minimizes the objective function for learning the CPA model, which can be reduced to \u2211n k=1 g (Tk(\u03bd\u030c),Z) + h(Z,\u03a6) as (22). According to (20) and (21), we can decomposed it into the summation of part-wise scores, where each term associates only with the edge linking one node and its parent. In view of this, we first link the (m+1) nodes (recall the node of \u201c0\u201d) into a complete directed graph, compute the score associating with each of the (m+ 1)m edges, and take the minimum spanning tree rooted at \u201c0\u201d to be the optimal configuration of the tree."}, {"heading": "C. Learning Mixture of CPA Models", "text": "The presented CPA model can be extended as a mixture of CPA models (mCPA) to cope with a larger range of pose and/or expression variations. Each component of the mCPA is parameterized the same as that of a standard CPA model, but it may have a different number of facial parts, since some facial parts may be occluded under pose changes.\nFor an mCPA model with c components, I\u03b9 \u2286 {1, 2, . . . ,m} denotes the index set of available parts for the \u03b9th component. Correspondingly, we also use {D\u3008i\u3009\u03b9 }mi=1 to denote the part dictionaries and Z\u03b9 = {z\u30081\u3009\u03b9 , z\u30082\u3009\u03b9 , . . . , z\u3008m\u3009\u03b9 } to denote the parameters of tree-structured shape models. If the \u03b9th component does not has the ith part, i.e. i /\u2208 I\u03b9, D\u3008i\u3009\u03b9 and z\u3008i\u3009\u03b9 would just be treated as void notations and would not be used.\nLearning an mCPA model is based on the formulation (19) for the standard CPA model learning. The difference is that we consider learning models of the c components altogether, so that the low-rank and sparse properties of corresponding part dictionaries belonging to different components can be overall leveraged, resulting in consistent part dictionaries across the c components. Given n training face images, we write D\u03b9 for the n\u03b9 face images assigned to the \u03b9th components, \u03c3\u03b9 \u2208 Rd\u00d7n\u03b9 for their holistic deformations, and \u03bd\u030c\u03b9 \u2208 Rd\u00d7m\u00d7n\u03b9 for their part deformations. Our objective for learning the mCPA model can be written as\nmin A\u3008i\u3009,E\u3008i\u3009,\u03bd\u030c\u03b9,\u03c3\u03b9,Z\u03b9 i=1,2,...,m; \u03b9=1,2,...,c { m\u2211 i=1 ( \u2016A\u3008i\u3009\u2016\u2217 + \u03bb\u3008i\u3009\u2016E\u3008i\u3009\u20161 ) (24)\n+\u03b7 c\u2211 \u03b9=1 ( n\u03b9\u2211 k=1 g (Tk(\u03bd\u030c\u03b9),Z\u03b9) + h(Z\u03b9,\u03a6\u03b9) )} ,\n12\ns.t."}, {"heading": "D\u03b9 \u25e6 \u03c3\u03b9 \u25e6 T \u3008i\u3009(\u03bd\u030c\u03b9) = A\u3008i\u3009\u03b9 + E\u3008i\u3009\u03b9 for i \u2208 I\u03b9.", "text": "A\u3008i\u3009\u03b9 , E\n\u3008i\u3009 \u03b9 are empty matrices for i /\u2208 I\u03b9, A\u3008i\u3009 = [A\n\u3008i\u3009 1 , A \u3008i\u3009 2 , . . . , A \u3008i\u3009 c ],\nE\u3008i\u3009 = [E \u3008i\u3009 1 , E \u3008i\u3009 2 , . . . , E \u3008i\u3009 c ].\nThe above objective can be solved similarly as for (19). Namely, we alternately update the part dictionaries and the tree-structured shape models. After solving (24), we have the part dictionaries D\u3008i\u3009\u03b9 . = D\u03b9\u25e6\u03c3\u03b9\u25e6T \u3008i\u3009(\u03bd\u030c\u03b9) and the shape model parameters {Z\u03b9}c\u03b9=1. Given a gallery set, ideally an mCPA model should be learned from face images of this gallery. However, in some cases the gallery does not contain non-frontal/non-neural face images to learn the corresponding component models of the mCPA. To remedy this problem, we first learn an mCPA model using a separate training set that contains face images of all the interested poses and expressions of a few subjects, and then learn the part dictionaries on the gallery set of interest while fixing the learned tree-structured shape model from that separate training set.\nWith a learned mCPA model, we use the same method as that of the standard CPA model to recognize a probe face, which requires us to select the correct component corresponding to its pose and expression before hand. In a fully automatic face recognition system, we may figure out the pose and expression by off-the-shelf methods, which, we will discuss in Section VI-D.\nVI. EXPERIMENTS\nIn this section, we present experiments to evaluate the proposed CPA method in the context of face recognition across illuminations, poses, and expressions. We used images of frontal face with neutral expression as the gallery, and face images with illumination, pose, and expression variations as the probes. We used the CMU Multi-PIE [32] and MUCT [33] datasets to conduct our experiments. The CMU Multi-PIE dataset contains face images with well controlled illumination, pose, and expression variations, and is thus intensively used for controlled experiments.\nWe designed an mCPA model whose component for frontal view and neutral expression consists of 21 facial parts of varying sizes. The basic constellation of the 21 parts is listed in Table I. For the other components, the availability of a part is determined by its visibility. The parameters used for the mCPA learning are set as \u03bb\u0302 = 1, \u03b7\u0302 = 0.02, and \u03d1 = 0.25 for all the experiments reported in this section.\nWith learned mCPA models, we first evaluated our method in Section VI-A in the scenario of face recognition across pose and expression with illumination variation. In particular, we show the advantages of our CPA method over other alternatives, such as the methods of a holistic face alignment followed by a holistic or part-based face recognition. We then demonstrate in Section VI-B the effectiveness of our CPA method when using part-based face recognition strategy. In Section VI-C, we test on face images with synthesized occlusions to demonstrate\nRemark: The relative sizes are in terms of the conventional facial region aligned with the two eyes, i.e., 60\u00d780 window with two outer eye corners at (5,22) and (56,22); and, the absolute sizes are measured in pixels for the part dictionaries.\nthe robustness of our method. Finally, we compared with the state-of-the-art across-pose face recognition methods in Section VI-D.\nFor controlled experiments reported in Sections VI-A, VI-B, and VI-C, we initialized face locations by manually annotating eye corner points, and assumed that the pose and expression of each probe face are given. For practical experiments in Section VI-D that conduct fully automatic face recognition across pose, we used off-the-shelf face detector [41] and pose estimator [25] to initialize our method."}, {"heading": "A. Face Recognition Across Pose and Expression with Different Illumination", "text": "Our choice of off-the-shelf methods for face recognition across illumination is based on SRC [26], for which we used multiple gallery images of varying illuminations for each subject. Under this setting, we compare CPA with the following three baseline alternatives.\n1) The first one manually aligns probe face images using labeled eye-corner points. After manual alignment, face recognition is conducted in a holistic manner. This alternative method is termed as \u201cmanual+holistic\u201d. 2) The second one automatically aligns probe face images using the holistic alignment algorithm of Wagner et al. [3]. After alignment, face recognition is again conducted in a holistic manner. This alternative is termed as \u201cholistic+holistic\u201d. If using SRC [26] as a classifier, this alternative is essentially the same as in [3], which is\n13\nsimilar to our CPA method in the way that the gallery is pruned to a subset before being used for recognition. For a fair comparison, we also made the subset consist of P subjects for the alignment algorithm of Wagner et al. [3]. 3) The third one also automatically aligns probe face images using the holistic alignment algorithm of Wagner et al. [3]. The same pruning scheme was used as for the second alternative. However, after alignment, a part-based face recognition strategy is used where positions of local parts are pre-defined and fixed relative to the global face. This alternative is termed as \u201cholistic+parted\u201d.\nOur CPA method will be occasionally referred to as \u201cmCPA+parted\u201d in accordance with the names of these baselines. The chosen subject number for pruning the gallery was set to P = 20.\n1) Evaluation on the Multi-PIE Dataset : The CMU MultiPIE [32] is the largest publicly available dataset suitable for test of our CPA method. It contains face images of 337 subjects captured over the time span of about 5 months. These face images are organized into 4 sessions according to their capture time. There are 200 ~ 250 subjects present in each session, where every subject is imaged from 15 different viewpoints with the flashlight varying in 18 different directions . Face images in Multi-PIE are with 6 well controlled facial expressions, including the neutral one and 5 non-neutral ones.\nIn our experiments, we used face images of 7 different illuminations (illumination id: {0, 1, 7, 13, 14, 16, 18}) in the training and gallery sets, and those of a novel illumination (illumination id: 10) in the probe set. We used 9 subjects (subject id: {267, 272, 273, 274, 276, 277, 278, 286, 289}), who appear in all the 2nd, 3rd, 4th sessions, for training of the tree-structured shape model. The 249 subjects that appear in Session 1 (subject id: 1 ~ 249) were used for testing. Their face images in Session 1 constituted the gallery set, and those in the other sessions were the probes. Table IIa gives more specifications on the probes.\nFor experiments of face recognition across pose on the MultiPIE dataset, we used the neutral-expression face images of 5 viewpoints of yaw angles at 0\u25e6, \u00b115\u25e6, and \u00b130\u25e6. For those across expression, we used face images of all the 5 available non-neutral expressions under the frontal viewpoint. We built a 10-component mCPA model as shown in Fig. 7 and Fig. 8. Unless otherwise mentioned, these experiment settings were also used in Section VI-B, VI-C, and VI-D.\nTable IIb reports recognition results of different alternative methods on face images with varying degrees of pose changes. Compared with \u201cmanual+holistic\u201d, \u201cholistic+holistic\u201d gives improved performance, which suggests that automatic alignment can improve face registration accuracy and consequently help for face recognition across pose, even in a holistic alignment manner. The performance of \u201cholistic+parted\u201d is unstable, which tells that simple part-based recognition without part alignment is not a feasible approach. Our CPA method significantly outperforms all these baselines. For the 4 nonfrontal viewpoints, CPA improves over the best alternative method \u201cholistic+holistic\u201d approximately by 15% ~ 40% in terms of recognition rate.\nAn interesting observation from Table IIb is that our CPA method can almost perfectly perform face recognition under the across-session setting of Multi-PIE, where probe face images of frontal viewpoint and neutral expression are from Sessions 2, 3 ,4 of Multi-PIE and gallery face images are from Session 1. Under this setting, our method gives 99.60% recognition rate, about 5% higher than that of \u201cholistic+holistic\u201d, i.e., the method in [3]. To the best of our knowledge, this is the best publicly known result under this setting. Note that the challenges of face recognition across sessions are mostly due to appearance change of human faces after a certain period of time, since illumination variation should have ideally been compensated by the multiple images of varying illuminations in the gallery. Nevertheless, our method suggests that by assuming human face as a piece-wise planar surface and using part-wise alignment, the problem of face recognition across sessions can to a large extent be overcome.\nTable IIc reports recognition results on frontal-view face images of varying expressions. Comparative performance of different alternative methods is very similar to that reported in Table IIb for face recognition across pose. Our CPA method outperforms the second best \u201cholistic+holistic\u201d method by 12% ~ 21% in terms of recognition rate.\n2) Evaluation on the MUCT Dataset: In practical face recognition scenarios, illumination usually show various levels of strength, and people often present near-neutral expressions. The MUCT dataset is a good simulation of such scenarios, where face images of natural expressions like frowns and minor smiles are captured under frontal lighting of 3 strength levels. We used the first 10 subjects (subject id: 0 ~ 9) of MUCT for\n14\ntraining the tree-structured shape model, and the rest 265 for testing. Frontal-view face images (labeled as \u201ca\u201d) were used as gallery, and those of the 4 available non-frontal viewpoints (labeled as \u201cb,c,d,e\u201d) were used as probes.\nTable III reports recognition results of different alternative methods. Consistent to the results reported in Section VI-A1, our CPA method outperforms all the other 3 alternatives. It in fact performs almost perfectly for all the 4 degrees of pose change."}, {"heading": "B. Effectiveness of Part-based Recognition in CPA method", "text": "The CPA method integrates part alignment with part-based recognition in a cohesive way. Part-based face recognition fuses weaker predictions from individual parts to make a stronger final decision, where a variety of methods can be used for recognition of individual parts. In this section, we investigate the varying discriminative power of individual parts, and also\nhow existing representative face recognition methods perform for part recognition in CPA. We also present experiments to show the efficacy of the proposed pruning scheme in Algorithm 2. These investigations were conducted on the MultiPIE dataset under the setting of face recognition across pose with different illumination for the gallery and probe (the setting in Section VI-A1 for producing Table IIb).\n1) Part Discriminativeness: Fig. 9 reports results obtained by recognizing each of the 21 parts that are jointly aligned by our CPA method. The recognition rates on most of the 21 parts are around 60% ~ 90% for the frontal pose and 40% ~ 80% for the \u00b115\u25e6 poses. These results suggest that the discriminativeness of individual parts is good, but not strong enough for a high-accuracy recognition performance. By fusing the predictions from individual parts, a high-accuracy final recognition (\u201cALL\u201d in Fig. 9) can be achieved for pose change of within \u221215\u25e6 ~ +15\u25e6 yaw angles. Even for a larger pose change of \u00b130\u25e6 yaw angles, our CPA method by fusing predictions from aligned individual parts performs fairly well, while recognition rates for most of individual parts are below 20%. This again demonstrates the efficacy of our proposed CPA method that integrates part-based recognition with part alignment.\n2) Integrated with Different Recognition Methods: Recognition of aligned individual parts in CPA is realized by off-the-shelf face recognition methods. Representatives of these methods include Nearest Subspace (NS) [2], Linear Discriminate Analysis (LDA) [43], and Local Binary Pattern\n15\n(LBP) [42]. In this section, we investigate how these different choices of recognition methods perform when integrated into our CPA method. Here we compared the CPA method (\u201cmCPA+parted\u201d) with only \u201cholistic+holistic\u201d.\nFor LDA, we learned its projection matrix after pruning candidate subjects and aligning gallery images, both of which are necessary steps for \u201cholistic+holistic\u201d and our CPA method. For LBP, we also extracted LBP features in the same stage, i.e., after the preparation of aligned gallery subsets. All the experimental settings were the same as those used in preceding sections, expect replacing the previously used SRC with NS, LDA, or LBP. Considering the promise of LBP for one-shot face recognition where only a single image per subject is available in gallery, we also conducted experiments under this one-shot setting, where probe and gallery face images are of the same illumination. Table IVa summarizes the illumination settings of experiments reported in this section.\nRecognition rates of CPA with integration of different methods are reported in Table IVb. Table IVb tells that with any choice of NS, LDA, or LBP, our proposed CPA for part-based alignment is superior to holistic face alignment. This confirms that our proposed CPA helps face recognition by effectively aligning individual facial parts.\n3) Pruning Efficacy: Pruning scheme in Algorithm 2 certainly leads to more efficient algorithm. In this section, we are interested in investigating how it impacts the recognition performance. To this end, we modified Algorithm 2 by simply removing the pruning scheme, i.e., setting P to the subject number in the gallery. Bottom half of Table V lists recognition rates of the non-pruning CPA method together with their difference with those of the original CPA. For the non-frontal poses, noticeable performance drops occur when the pruning scheme is removed from CPA. Top half of Table V gives more evidence on the impact of the pruning scheme. Taking the non-pruning version as the baseline, we find that, for the non-frontal poses, the original CPA method corrects one time more recognition errors than it introduces, where \u201ccorrecting\u201d a recognition error means correctly recognizing an image that is falsely recognized by the baseline, and \u201cintroducing\u201d means the opposite. Experimental results reported in Table V were obtained by using SRC for part recognition. All other settings were the same as those used in preceding sections."}, {"heading": "C. Recognition with Synthetic Random Block Occlusion", "text": "We report experiments in this section to demonstrate the robustness of CPA against partial occlusion. As shown in Fig. 10, we synthesized partially occluded probe face images by adding block occlusion at random positions of a probe face. The size of occluded blocks varied from 10% to 60% of\n16"}, {"heading": "10% 20% 30% 40% 50% 60%", "text": "the holistic face. Experiment settings were set the same as in Section VI-A1. The method [3], i.e. \u201cholistic+holistic\u201d, was taken as the baseline.\nFig. 11a reports recognition rates on frontal-view face images at different occlusion ratios, and Fig. 11b reports those of varying viewpoints at the occlusion ratio of 30%. Compared with [3], Fig. 11a and Fig. 11b show that our CPA method is more robust against partial occlusion. When a small portion (10%) of face images is occluded, recognition rate of [3] drops below 90%. In contrast, recognition rates of CPA remain above 90% when the occlusion ratios increase from 0% to 30%. For the 30% occlusion ratio, recognition rate of CPA is more than 45% higher than that of [3] for pose change of within \u00b115\u25e6."}, {"heading": "D. Comparison with the State-of-the-art", "text": "In this section, we used the Multi-PIE dataset to compare CPA with the state-of-the-art Morphable Displacement Field (MDF) method [34] for face recognition across pose. MDF\nachieves good performance by learning an MDF model from a large number of 3D face shapes, while training of CPA only requires 2D face images of a few subjects.\nLike our CPA method, MDF can also be equipped with different features and classifiers for recognition. In order fairly compare the two methods, in this paper, we used the same LDA classifier for both the two methods 6. While LDA models were learned from pruned gallery in the CPA method, a single LDA model was learned from the entire gallery set in advance for the MDF method, as it does not prune gallery during recognition.\nWe compare the CPA and MDF methods using probe face images that are under different pose and illumination conditions from those of gallery images. The experiment settings were largely the same as those of face recognition across pose with different illumination used in Session VI-A1, except that:\n1) In order to be consistent with the experimental protocol in the work of Li et al. [34], we used the last 137 subjects instead of the first 229 ones in Multi-PIE for testing. More specifically, for each subject, face images in the session where he/she first appears were included in the gallery set, and those in the other sessions were in the probe set. 2) We learned the tree-structured shape model in CPA using face images of 9 other subjects (subject id: {038, 040, 041, 042, 043, 044, 046, 047, 048}), who appear in all the four sessions of Multi-PIE. 3) We set P = 10 for the pruning scheme used in Algorithm 2.\nTable VIa reports recognition rates of CPA and MDF on neutral-expression face images of varying degrees of pose change, where both fully automatic and semi-automatic (manual initialization with known pose) experiments are reported. For the case of semi-automatic experiments using LDA as the classifier, our CPA method outperforms MDF when the degrees of pose change are within \u00b115\u25e6. When the degrees of pose change increase to \u00b130\u25e6, CPA performs worse than MDF. The performance inconsistency between smaller and larger degrees of pose change is in fact determined by the algorithm nature of the two methods. CPA is based on 2D similarity transform of individual facial parts, whose training only requires face images of a few subjects, while a large number of 3D face shapes are necessary for learning an MDF model. By learning more generic knowledge of 3D face shapes, MDF is able to better cope with face recognition across a larger degree of pose change. However, for the range between \u221215 and +15 degrees, which are more often encountered in practical scenarios such as access control, and face recognition in which is also more reliable, CPA gives more accurate recognition results than MDF does. In addition, our CPA method with SRC gives similarly better results in this range of pose change.\nWe realize a fully automatic CPA method by initializing our algorithm using Viola and Jones [41]\u2019s face detector, followed by a coarse and holistic alignment using the method of Wagner et al. [3]. For knowledge of face pose used in the mCPA model,\n6Experimental results of MDF were produced by Li et al. [34]. As SRC was not implemented in their experiment pipeline, we thank them for their kind help to run MDF with the alternative LDA classifier.\n17\nwe consider two situations where poses of probe faces are either known in advance or estimated by Zhu and Ramanan [25]\u2019s method. The accuracy rate of Zhu et al.\u2019s pose estimator is 96.56% under our experiment settings 7.\nTable VIb reports recognition results of fully automatic CPA. Fully automatic CPA (the last two rows) performs comparably with the semi-automatic one (the first row) when pose changes of probe face images are within \u00b115\u25e6. Note that the fully automatic alignment works from coarse to fine granularity, say, sequentially uses Viola and Jones [41]\u2019s face detector, the method of Wagner et al. [3] for a holistic alignment, and partbased alignment by CPA. The experimental results show that this coarse-to-fine strategy works well for reasonable degrees of pose change (e.g., within \u00b115\u25e6). For larger degrees of pose change, performance of holistic alignment by [3] drops. Consequently, our method loses the chance of correcting its alignment failure. In addition, the last two rows of Table VIb also tells that the mCPA model performs equally well when poses of probe faces are either given or automatically estimated. This confirms the effectiveness of our proposed method for automatic part-based face alignment."}, {"heading": "VII. CONCLUSION", "text": "In this paper we propose a method termed CPA for acrosspose and -expression face recognition, which can be benefited by pixel-wisely accurate alignment. The CPA model consists of appearance evidence of each part and a tree-structured shape model for constraining part deformation, both of which can be automatically learned from training images. To align a probe image, we fit its parts to the appearance evidence with consideration of constraint from the learned tree-structured\n7It seems that Zhu et al.\u2019s method [25] can work very well only when the training and test images are with the same illumination. To handle the illumination variation in our experiments, we normalized image illumination by the non-local means (NLM) based method [46] before pose estimation.\nshape model. This objective is formulated as a norm minimization problem regularized by the graph likelihoods, which can be efficiently solved by an alternating optimization method. CPA can easily incorporate many existing face recognition method for part-based recognition. Intensive experiments show the efficacy of CPA in handling illumination, pose, and/or expression changes when integrated with an recognition method robust to illumination changes. In further research, we are interested in applying applying/adapting CPA to other computer vision applications."}, {"heading": "ACKNOWLEDGEMENT", "text": "Thanks to [34] for kindly producing the experimental results\non the MDF model that are reported in Table VI."}, {"heading": "APPENDIX A LINEAR SYSTEM IN (15)", "text": "Let Q = [q1, q2, . . . , qm] \u2208 Rd\u00d7m, (15) is the same as the linear equation array:\nG\u3008i\u3009\u2206\u03bd\u3008i\u3009 + \u03b7\n( \u2202g(\u03bd + \u2206\u03bd,Z)\n\u2202\u2206\u03bd\u3008i\u3009\n)T = qi, (25)\nfor i = 1, 2, . . . ,m, which can be written in a more standard form W11 W12 \u00b7 \u00b7 \u00b7 W1m W21 W22 \u00b7 \u00b7 \u00b7 W2m ... ... . . . ...\nWm1 Wm2 \u00b7 \u00b7 \u00b7 Wmm\n  \u2206\u03bd\u30081\u3009 \u2206\u03bd\u30082\u3009\n... \u2206\u03bd\u3008m\u3009  =  c1 c2 ... cm  , (26) where ci \u2208 Rd is a column vector, and Wij \u2208 Rd\u00d7d. Recall that E is the edge set of the tree-structured model. In particular, if i is the parent of j, it holds (i, j) \u2208 E . Now, we can find that\nci = qi+\u03b7 \u039b\u3008i\u3009(\u00b5\u3008i\u3009 \u2212 \u03bd\u3008i\u3009\u03b4 )\u2212 \u2211 j\u2208{(i,j)\u2208E} \u039b\u3008j\u3009(\u00b5\u3008j\u3009 \u2212 \u03bd\u3008j\u3009\u03b4 )  , for i = 1, 2, . . . ,m, and\nWij =  G\u3008i\u3009 + \u03b7 \u2211 l\u2208{i}\u222a{l:(i,l)\u2208E} \u039b \u3008l\u3009, j = i, \u2212\u03b7\u039b\u3008j\u3009, (i, j) \u2208 E , \u2212\u03b7\u039b\u3008i\u3009, (j, i) \u2208 E , 0, otherwise,\nfor for i, j = 1, 2, . . . ,m."}, {"heading": "APPENDIX B SOLVING TRANSFORMATIONS IN (16) WITH FIXED PARTS", "text": "G is the 2-D similarity group parametrized in Rd (d = 4) as it is defined in (1). Given initial values on \u03c3 \u2208 G and {\u03bd\u3008i\u3009}mi=1 \u2208 G, (16) seeks the minimum of g (\u03bd,Z) while keeping the value of \u03c3 \u25e6 \u03bd\u3008i\u3009 unchanged. Let \u03bd = [\u03bd\u30081\u3009, \u03bd\u30082\u3009, . . . , \u03bd\u3008m\u3009] \u2208 Rd\u00d7m, and \u2206\u03bd = [\u2206\u03bd\u30081\u3009,\u2206\u03bd\u30082\u3009, . . . ,\u2206\u03bd\u3008m\u3009] \u2208 Rd\u00d7m. We reformulate (16) as\nmin \u2206\u03bd,\u2206\u03c3\ng (\u03bd + \u2206\u03bd,Z) , (27)\ns.t. (\u03c3 + \u2206\u03c3) \u25e6 (\u03bd\u3008i\u3009 + \u2206\u03bd\u3008i\u3009) = \u03c3 \u25e6 \u03bd\u3008i\u3009. (28)\n18\nAfter optimizing it, \u03c3 is updated to \u03c3 + \u2206\u03c3, and \u03bd is updated to \u03bd + \u2206\u03bd.\nLet\n\u03bd\u3008i\u3009= (s\u3008i\u3009, \u03b8\u3008i\u3009, t\u3008i\u3009u , t \u3008i\u3009 v ), \u2206\u03c3 = (\u03b4s\u2217, \u03b4\u03b8\u2217, \u03b4t\u2217u, \u03b4t \u2217 v),\nwhere s\u00b7, \u03b8\u00b7, t\u00b7u, t \u00b7 v (\u201c\u00b7\u201d for \u201c\u3008i\u3009\u201d and \u201c\u2217\u201d) respectively denote the scale, rotation, horizontal translation, and vertical translation of a 2-D similarity transformation. (28) is equivalent to\n\u2206\u03bd\u3008i\u3009 =  \u2212\u03b4s\u2217 \u2212\u03b4\u03b8\u2217\n\u2212t\u3008i\u3009u + f\u2217u + f \u3008i\u3009 u \u2212t\u3008i\u3009v + f\u2217v + f \u3008i\u3009 v  , (29) where,\nf\u2217u = (t\u2217u, t \u2217 v)\nexp(\u03b4s\u2217 + s\u2217) ( cos(\u03b4\u03b8\u2217 + \u03b8\u2217) sin(\u03b4\u03b8\u2217 + \u03b8\u2217) ) ,\nf\u2217v = (t\u2217v,\u2212t\u2217u)\nexp(\u03b4s\u2217 + s\u2217) ( cos(\u03b4\u03b8\u2217 + \u03b8\u2217) sin(\u03b4\u03b8\u2217 + \u03b8\u2217) ) ,\nand,\nf \u3008i\u3009u = (t \u3008i\u3009 u , t \u3008i\u3009 v )\nexp(\u03b4s\u2217)\n( cos \u03b4\u03b8\u2217\nsin \u03b4\u03b8\u2217\n) ,\nf \u3008i\u3009v = (t \u3008i\u3009 v ,\u2212t\u3008i\u3009u ) exp(\u03b4s\u2217)\n( cos \u03b4\u03b8\u2217\nsin \u03b4\u03b8\u2217\n) .\nSubstituting (29) into the objective function in (27), we obtain the unconstrained equivalence of the original problem, say, min\u2206\u03c3 g (\u03bd + \u2206\u03bd,Z), where \u2206\u03bd is expended as (29). We solve this problem by gradient descent. Now, we need to find g (\u03bd + \u2206\u03bd,Z)\u2019s gradient in \u2206\u03c3.\nRecall that E is the tree edge set. For i = 1, 2, . . . ,m, let j satisfies (j, i) \u2208 E (j is the parent of i), and\n\u03d5\u3008i\u3009 = ( (\u03bd\u3008i\u3009 + \u2206\u03bd\u3008i\u3009)\u2212 (\u03bd\u3008i\u3009 + \u2206\u03bd\u3008j\u3009)\u2212 \u00b5\u3008i\u3009 ) ,\nWe then have\ng (\u03bd + \u2206\u03bd,Z) = 1 2 m\u2211 i=1 \u03d5\u3008i\u3009 T \u039b\u3008i\u3009\u03d5\u3008i\u3009 + b, (30)\nand, \u2207(\u2206\u03c3)g (\u03bd + \u2206\u03bd,Z) = m\u2211 i=1 \u03d5\u3008i\u3009 T \u039b\u3008i\u3009 \u2202\u03d5\u3008i\u3009 \u2202(\u2206\u03c3) . (31)\nFor i satisfying (0, i) \u2208 E (i is directly linked with the root), it holds\n\u03d5\u3008i\u3009 =  s\u3008i\u3009 \u2212 \u03b4s\u2217 \u03b8\u3008i\u3009 \u2212 \u03b4\u03b8\u2217\nf \u3008i\u3009 u \u2212 f\u2217u f \u3008i\u3009 v \u2212 f\u2217v \u2212 \u00b5\u3008i\u3009, \u2202\u03d5\u3008i\u3009\n\u2202(\u2206\u03c3) = \u22121 0 0 0 0 \u22121 0 0 f\u2217u \u2212 f \u3008i\u3009 u f \u3008i\u3009 v \u2212 f\u2217v \u2212 cos(\u03b4\u03b8\u2217+\u03b8\u2217) exp(\u03b4s\u2217+s\u2217) \u2212 sin(\u03b4\u03b8\u2217+\u03b8\u2217) exp(\u03b4s\u2217+s\u2217)\nf\u2217v \u2212 f \u3008i\u3009 v f\u2217u \u2212 f \u3008i\u3009 u\nsin(\u03b4\u03b8\u2217+\u03b8\u2217) exp(\u03b4s\u2217+s\u2217) \u2212 cos(\u03b4\u03b8\u2217+\u03b8\u2217) exp(\u03b4s\u2217+s\u2217)\n .\nFor i satisfying (j, i) \u2208 E , j 6= 0 (i is not directly linked with the root), it holds\n\u03d5\u3008i\u3009 =  s\u3008i\u3009 \u2212 s\u3008j\u3009 \u03b8\u3008i\u3009 \u2212 \u03b8\u3008j\u3009\nf \u3008i\u3009 u \u2212 f \u3008j\u3009u f \u3008i\u3009 v \u2212 f \u3008j\u3009v\n\u2212 \u00b5\u3008i\u3009,\n\u2202\u03d5\u3008i\u3009\n\u2202(\u2206\u03c3) =\n 0 0 0 0 0 0 0 0\nf \u3008j\u3009 u \u2212 f \u3008i\u3009u f \u3008i\u3009v \u2212 f \u3008j\u3009v 0 0 f \u3008j\u3009 v \u2212 f \u3008i\u3009v f \u3008j\u3009u \u2212 f \u3008i\u3009u 0 0  ."}, {"heading": "APPENDIX C SOLUTION TO (19)", "text": "We solve (19) by the alternately conducting the following two steps:\n1) Fix \u03c3 and solve \u03bd\u030c, {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1. 2) Fix {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1 and solve \u03c3, \u03bd\u030c. Step 1: Given fixed \u03c3, we update \u03bd\u030c by a generalization of the Gauss-Newton method. To be specific, for a linear update from \u03bd\u030c to \u03bd\u030c + \u2206\u03bd\u030c (\u2206\u03bd\u030c \u2208 Rd\u00d7m\u00d7n), we approximate the equality constraint in (19) by its first-order Taylor expansion at T \u3008i\u3009(\u03bd\u030c), i.e., D \u25e6 \u03c3 \u25e6 T \u3008i\u3009(\u03bd\u030c) \u2248 D \u25e6 \u03c3 \u25e6 T \u3008i\u3009(\u03bd\u030c) +\u2211n k=1 J \u3008i\u3009 k T \u3008i\u3009(\u2206\u03bd\u030c) k T k , where J \u3008i\u3009 k . = \u2202(dk\u25e6\u03c3k\u25e6\u03bd\u3008i\u3009k )/\u2202\u03bd \u3008i\u3009 k is the Jacobian w.r.t. \u03bd\u3008i\u3009k , and { k}nk=1 denotes the standard basis of Rn. The above linearization leads to the following problem to optimize {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1,\u2206\u03bd\u030c\nmin A\u3008i\u3009,E\u3008i\u3009,\u2206\u03bd\u030c i=1,2,...,m { m\u2211 i=1 ( \u2016A\u3008i\u3009\u2016\u2217 + \u03bb\u3008i\u3009\u2016E\u3008i\u3009\u20161 )\n+ \u03b7 n\u2211 k=1 g (Tk(\u03bd\u030c + \u2206\u03bd\u030c),Z)\n} (32)\ns.t. D \u25e6 \u03c3 \u25e6 T \u3008i\u3009(\u03bd\u030c) + n\u2211 k=1 J \u3008i\u3009 k T \u3008i\u3009(\u2206\u03bd\u030c) k T k = A \u3008i\u3009 + E\u3008i\u3009.\nWe repeatedly solve (32) to update \u03bd\u030c, until it converges to a local minimum, which gives the solution to the original problem(19) .\nWe solve the convex problem (32) by adapting the ALM. Let\nh(A\u3008i\u3009, E\u3008i\u3009, T \u3008i\u3009(\u2206\u03bd\u030c)) =\nD \u25e6 \u03c3 \u25e6 T \u3008i\u3009(\u03bd\u030c) + n\u2211 k=1 J \u3008i\u3009 k T \u3008i\u3009(\u2206\u03bd\u030c) k T k \u2212A\u3008i\u3009 \u2212 E\u3008i\u3009.\nThe augmented Lagrange function is written as\nL\u03b2({A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1,\u2206\u03bd\u030c, {\u0393\u3008i\u3009}mi=1) = (33) m\u2211 i { \u2016A\u3008i\u3009\u2016\u2217 + \u03bb\u3008i\u3009\u2016E\u3008i\u3009\u20161 + \u2329 \u0393\u3008i\u3009, h(A\u3008i\u3009, E\u3008i\u3009, T \u3008i\u3009(\u2206\u03bd\u030c)) \u232a + \u00b5\n2 \u2225\u2225\u2225h(A\u3008i\u3009, E\u3008i\u3009, T \u3008i\u3009(\u2206\u03bd\u030c))\u2225\u2225\u22252 F } + n\u2211 k=1 g (Tk(\u03bd\u030c + \u2206\u03bd\u030c),Z) ,\n19\nwhere {\u0393\u3008i\u3009}mi=1 are the Lagrange multiplier matrices, and the matrix inner product \u3008\u00b7, \u00b7\u3009 is defined as \u3008A,B\u3009 = trace(ATB). Given initial {\u0393\u3008i\u3009}mi=1, ALM iteratively and alternately updates {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1,\u2206\u03bd\u030c, and {\u0393\u3008i\u3009}mi=1 by\n1) {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1,\u2206\u03bd\u030c \u2190\narg min A\u3008i\u3009,E\u3008i\u3009,\u2206\u03bd\u030c i=1,2,...,m\nL\u03b2({A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1,\u2206\u03bd\u030c, {\u0393\u3008i\u3009}mi=1); (34)\n2) \u0393\u3008i\u3009 \u2190 \u0393\u3008i\u3009 + \u03b2l \u00b7 h(A\u3008i\u3009, E\u3008i\u3009, T \u3008i\u3009(\u2206\u03bd\u030c)), for i = 1, 2, . . . ,m;\nwhere l is the iteration number, and {\u03b2l}l=1,2,... is an sequence increasing to sufficient large.\nDirectly solving (34) w.r.t. all the unknown variables is still a difficult problem. Instead, we alternately update them in three groups, i.e., {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1, and \u2206\u03bd\u030c, so that each subproblem associated with any single group of variables has a closed form solution. More specifically, for i = 1, 2, . . . ,m, we update A\u3008i\u3009 and E\u3008i\u3009 sequentially by\nR\u3008i\u3009 \u2190 D \u25e6 \u03c3 \u25e6 T \u3008i\u3009(\u03bd\u030c) + (1/\u03b2l)\u0393\u3008i\u3009,\n(U\u0302 ,\u03a3\u0302, V\u0302 )\u2190 svd ( R\u3008i\u3009 +\nn\u2211 k J \u3008i\u3009 k T \u3008i\u3009(\u2206\u03bd\u030c) k T k \u2212 E\u3008i\u3009\n) ,\nA\u3008i\u3009 \u2190 U\u0302S1/\u03b2l(\u03a3\u0302)V\u0302 T ,\nE\u3008i\u3009 \u2190 S\u03bb\u3008i\u3009/\u03b2l\n( R\u3008i\u3009 +\nn\u2211 k J \u3008i\u3009 k T \u3008i\u3009(\u2206\u03bd\u030c) k T k \u2212A\u3008i\u3009\n) ,\nwhere R\u3008i\u3009 is an auxiliary variable used for notation convenience, S\u03b1(\u00b7) is the soft-thresholding function defined in (12). Then, for k = 1, 2, . . . , n, we find the optimum Tk(\u2206\u03bd\u030c) by solving\n0 = \u2202L\u00b5({A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1,\u2206\u03bd\u030c)\n\u2202\u2206\u03bd \u3008i\u3009 k\n(35)\n=\u03b2l ( (R\u3008i\u3009 \u2212A\u3008i\u3009 \u2212 E\u3008i\u3009) k + J\u3008i\u3009k \u2206\u03bd \u3008i\u3009 k ) J \u3008i\u3009 k\n+ \u03b7 \u2202g(Tk(\u03bd\u030c),Z)\n\u2202\u2206\u03bd \u3008i\u3009 k\n,\nfor i = 1, 2, . . . ,m. The m equations forms a sparse linear system of the form in (15) (expended form in Appendix A), whose parameters are {G\u3008i\u3009}mi=1 and Q. Here, G\u3008i\u3009 = \u03b2lJ \u3008i\u3009 k T J \u3008i\u3009 k , the ith column of Q is \u03b2l(A\u3008i\u3009 +E\u3008i\u3009 \u2212R\u3008i\u3009) k. Still, we use the inexact scheme for ALM, in which {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1, and \u2206\u03bd\u030c are alternately updated for only once in each iteration of ALM.\nStep 2: Given fixed {A\u3008i\u3009}mi=1, {E\u3008i\u3009}mi=1, \u03c3k \u25e6 \u03bd \u3008i\u3009 k can be taken as constant for any possible i and k. For every k = 1, 2, . . . , n, we use the same technique as in Session III-B2 to solve \u03c3k and T \u3008i\u3009(\u03bd\u030c). More specifically, denote \u03b6 \u3008i\u3009 k = \u03c3k\u25e6\u03bd \u3008i\u3009 k , which should not be changed, and update \u03c3k and Tk(\u03bd\u030c) by\nmin Tk(\u03bd\u030c),\u03c3k\ng (Tk(\u03bd\u030c),Z) s.t. \u03c3k \u25e6 \u03bd\u3008i\u3009k = \u03b6 \u3008i\u3009 k . (36)\nThe solution to this type of problem is present in Appendix B By the way, \u03c3 might be initialized by perfect manual annotations in practice for part dictionary learning. In this\ncase, we may fairly take it as a known variable the original problem (18) so that the algorithm efficiency can be improved by reducing an alternating loop."}, {"heading": "APPENDIX D MAXIMUM A POSTERIORI WITH GAUSSIAN-WISHART PRIOR", "text": "The PDF of the Wishart distribution is\nW(\u039b|V, r) = 1 \u03c1 |\u039b|(r\u2212d\u22121)/2 exp\n( \u22121\n2 Tr(\u039bV \u22121)\n) ,\ns.t. \u03c1 = 2rd/2|V |r/2\u0393d(r/2),\nwhere \u0393d(\u00b7) is the d-D Gamma function. The PDF of the Gaussian-Wishart distribution is\n\u0398(\u00b5,\u039b|u, \u03ba, V, r) = N (\u00b5|u, (\u03ba\u039b)\u22121) \u00b7 W(\u039b|V, r).\nwhere N (\u00b7|\u00b7, \u00b7) denotes the Gaussian PDF defined in (4). The Gaussian-Wishart distribution is the conjugate prior of the Gaussian distribution, where \u201cconjugate\u201d means that the prior and the corresponding posterior follow the same type of distribution only with different parameters.\nGiven n observations {\u03bdk}nk=1 drawn from N (\u00b5,\u039b\u22121), let \u00b5ML,\u039bML denote the ML estimations for \u00b5,\u039b. Now, taking the Gaussian-Wishart distribution \u0398(u0, \u03ba0, V0, r0) as the prior for \u00b5,\u039b, their posterior is\np(\u00b5,\u039b\u22121|{\u03bdk}nk=1, u0, \u03ba0, V0, r0) = \u0398(\u00b5,\u039b|un, \u03ban, Vn, rn),\nwhere,\nrn = r0 + n,\n\u03ban = \u03ba0 + n, un = \u03ba0u0 + n\u00b5ML\n\u03ba \u3008i\u3009 0 + n\n,\nVn = ( V \u3008i\u3009 0 \u22121 + n\u039b\u22121ML + \u03ba0n\n\u03ba0 + n H\n)\u22121 ,\nH = (\u00b5ML \u2212 u0)(\u00b5ML \u2212 u0)T .\nBy finding the maximum of \u0398(\u00b5,\u039b\u22121|un, \u03ban, Vn, rn), we obtained the MAP estimations of \u00b5,\u039b:\n\u039bMAP = (r0 + n\u2212 d)Vn, \u00b5MAP = un.\nInversely, taking n = 0, we can set \u0398(u0, \u03ba0, V0, r0) to be consistent with specific Gaussian distribution N (\u00b50,\u039b\u221210 ), say,\nu0 = \u00b50, V0 = \u039b \u22121 0 /(r0 \u2212 d).\nIn addition, note that using the prior \u0398(u0, \u03ba0, V0, r0) equals to incorporating r0 (or \u03ba0) additional samples drawn from the Gaussian distributionN ( u0, ((r0 \u2212 d)V0)\u22121 ) into the existing observations, where r0 and \u03ba0 should be normally set to the same value. In view of this, we set r0 = \u03ba0 = \u03d1n, where \u03d1 > 0 determines the weight of the prior w.r.t. the observations.\n20"}], "references": [{"title": "Face recognition: A literature survey", "author": ["W. Zhao", "R. Chellappa", "P.J. Phillips", "A. Rosenfeld"], "venue": "ACM Comput. Surv., vol. 35, no. 4, pp. 399\u2013458, Dec. 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.-C. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 5, pp. 684 \u2013698, may 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Toward a practical face recognition system: Robust alignment and illumination by sparse representation", "author": ["A. Wagner", "J. Wright", "A. Ganesh", "Z. Zhou", "H. Mobahi", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 2, pp. 372 \u2013386, feb. 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "What is the set of images of an object under all possible illumination conditions?", "author": ["P. Belhumeur", "D. Kriegman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "From few to many: illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 643 \u2013660, jun 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Total variation models for variable lighting face recognition", "author": ["T. Chen", "W. Yin", "X.S. Zhou", "D. Comaniciu", "T. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 9, pp. 1519 \u20131524, sept. 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Appearance characterization of linear lambertian objects, generalized photometric stereo, and illumination-invariant face recognition", "author": ["S. Zhou", "G. Aggarwal", "R. Chellappa", "D. Jacobs"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 2, pp. 230 \u2013245, feb. 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "View-based and modular eigenspaces for face recognition", "author": ["A. Pentland", "B. Moghaddam", "T. Starner"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1994. Proceedings CVPR \u201994., jun 1994, pp. 84 \u201391.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Face recognition under varying pose", "author": ["D. Beymer"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1994. Proceedings CVPR \u201994., jun 1994, pp. 756 \u2013761.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Tied factor analysis for face recognition across large pose differences", "author": ["S. Prince", "J. Warrell", "J. Elder", "F. Felisberti"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 6, pp. 970 \u2013984, june 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A compressive sensing approach for expression-invariant face recognition", "author": ["P. Nagesh", "B. Li"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009., june 2009, pp. 1518 \u20131525.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Eigen lightfields and face recognition across pose", "author": ["R. Gross", "I. Matthews", "S. Baker"], "venue": "Fifth IEEE International Conference on Automatic Face and Gesture Recognition, 2002. Proceedings., may 2002, pp. 1 \u20137.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Locally linear regression for pose-invariant face recognition", "author": ["X. Chai", "S. Shan", "X. Chen", "W. Gao"], "venue": "IEEE  Transactions on Image Processing, vol. 16, no. 7, pp. 1716 \u20131725, july 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning patch correspondences for improved viewpoint invariant face recognition", "author": ["A. Ashraf", "S. Lucey", "T. Chen"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2008. CVPR 2008., june 2008, pp. 1 \u20138.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "A deformation and lighting insensitive metric for face recognition based on dense correspondences", "author": ["A. Jorstad", "D. Jacobs", "A. Trouve"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, june 2011, pp. 2353 \u20132360.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Energy normalization for poseinvariant face recognition based on MRF model image matching", "author": ["S. Arashloo", "J. Kittler"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 6, pp. 1274 \u20131280, june 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Wide-baseline stereo for face recognition with large pose variation", "author": ["C. Castillo", "D. Jacobs"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, june 2011, pp. 537 \u2013544.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Active appearance models", "author": ["T. Cootes", "G. Edwards", "C. Taylor"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 681 \u2013685, jun 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Active appearance models revisited", "author": ["I. Matthews", "S. Baker"], "venue": "International Journal of Computer Vision, vol. 60, no. 2, pp. 135\u2013164, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Face recognition by elastic bunch graph matching", "author": ["L. Wiskott", "J.-M. Fellous", "N. Kruger", "C. von der Malsburg"], "venue": "Proceedings., International Conference on Image Processing, 1997., vol. 1, oct 1997, pp. 129 \u2013132 vol.1.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Feature detection and tracking with constrained local models", "author": ["D. Cristinacce", "T. Cootes"], "venue": "Proc. British Machine Vision Conference, vol. 3, 2006, pp. 929\u2013938.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Localizing parts of faces using a consensus of exemplars", "author": ["P. Belhumeur", "D. Jacobs", "D. Kriegman", "N. Kumar"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, june 2011, pp. 545 \u2013552.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Deformable model fitting by regularized landmark mean-shift", "author": ["J. Saragih", "S. Lucey", "J. Cohn"], "venue": "International Journal of Computer Vision, vol. 91, pp. 200\u2013215, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 9, pp. 1627 \u20131645, sept. 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["X. Zhu", "D. Ramanan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, june 2012, pp. 2879 \u20132886.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 210 \u2013227, feb. 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical ensemble of gabor fisher classifier for face recognition", "author": ["Y. Su", "S. Shan", "X. Chen", "W. Gao"], "venue": " 21 in Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th International Conference on, 2006, pp. 6 pp.\u2013 96.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Partbased face recognition using near infrared images", "author": ["K. Pan", "S. Liao", "Z. Zhang", "S. Li", "P. Zhang"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR \u201907. IEEE Conference on, 2007, pp. 1\u20136.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Maximizing all margins: Pushing face recognition with kernel plurality", "author": ["R. Kumar", "A. Banerjee", "B. Vemuri", "H. Pfister"], "venue": "2011 IEEE International Conference on Computer Vision (ICCV), 2011, pp. 2375\u20132382.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiobservation visual recognition via joint dynamic sparse representation", "author": ["H. Zhang", "N. Nasrabadi", "Y. Zhang", "T. Huang"], "venue": "2011 IEEE International Conference on Computer Vision (ICCV), 2011, pp. 595\u2013602.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Which parts of the face give out your identity?", "author": ["O. Ocegueda", "S. Shah", "I. Kakadiaris"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Multi-PIE", "author": ["R. Gross", "I. Matthews", "J. Cohn", "T. Kanade", "S. Baker"], "venue": "Image and Vision Computing, vol. 28, no. 5, pp. 807 \u2013 813, 2010, best of Automatic Face and Gesture Recognition 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "The MUCT Landmarked Face Database", "author": ["S. Milborrow", "J. Morkel", "F. Nicolls"], "venue": "Pattern Recognition Association of South Africa, 2010, http://www.milbo.org/muct/.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Morphable displacement field based image matching for face recognition across pose", "author": ["S. Li", "X. Liu", "X. Chai", "H. Zhang", "S. Lao", "S. Shan"], "venue": "European Conference on Computer Vision (ECCV) 2012, ser. Lecture Notes in Computer Science, vol. 7572. Springer Berlin Heidelberg, 2012, pp. 102\u2013115.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Fully automatic pose-invariant face recognition via 3D pose normalization", "author": ["A. Asthana", "T. Marks", "M. Jones", "K. Tieu", "M. Rohith"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2011, nov. 2011, pp. 937 \u2013944.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Face recognition based on fitting a 3D morphable model", "author": ["V. Blanz", "T. Vetter"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 9, pp. 1063 \u2013 1074, sept. 2003.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Layered object models for image segmentation", "author": ["Y. Yang", "S. Hallman", "D. Ramanan", "C. Fowlkes"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1731 \u20131743, sept. 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images", "author": ["Y. Peng", "A. Ganesh", "J. Wright", "W. Xu", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PP, no. 99, p. 1, 2012.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "Y. Ma"], "venue": "University of Illinois at Urbana- Champaign, Tech. Rep. UILU-ENG-09-2215, 2009.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "Proceedings of the  2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2001. CVPR 2001., vol. 1, 2001, pp. I\u2013511 \u2013 I\u2013518 vol.1.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2001}, {"title": "Face description with local binary patterns: Application to face recognition", "author": ["T. Ahonen", "A. Hadid", "M. Pietikainen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 12, pp. 2037 \u20132041, dec. 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Eigenfaces vs. Fisherfaces: recognition using class specific linear projection", "author": ["P. Belhumeur", "J. Hespanha", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 711 \u2013720, jul 1997.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1997}, {"title": "Robust principal component analysis?", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "J. ACM,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE Transactions on Information Theory, vol. 14, no. 3, pp. 462 \u2013 467, may 1968.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1968}, {"title": "Illumination invariant face recognition by non-local smoothing", "author": ["V. \u0160truc", "N. Pave\u0161ic"], "venue": "Biometric ID Management and Multimodal Communication, ser. Lecture Notes in Computer Science, J. Fierrez, J. Ortega- Garcia, A. Esposito, A. Drygajlo, and M. Faundez-Zanuy, Eds. Springer Berlin Heidelberg, 2009, vol. 5707, pp. 1\u20138.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In more practical scenarios, performance of these existing methods degrades drastically due to face variations caused by illumination, pose, and/or expression changes [1].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "[2], Wagner et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3], motivated by the illumination cone model [4, 5], used multiple carefully chosen face images of varying illuminations per subject as gallery.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[3], motivated by the illumination cone model [4, 5], used multiple carefully chosen face images of varying illuminations per subject as gallery.", "startOffset": 46, "endOffset": 52}, {"referenceID": 4, "context": "[3], motivated by the illumination cone model [4, 5], used multiple carefully chosen face images of varying illuminations per subject as gallery.", "startOffset": 46, "endOffset": 52}, {"referenceID": 5, "context": "edu) alternative methods [6, 7] considered extracting illumination invariant features for face recognition.", "startOffset": 25, "endOffset": 31}, {"referenceID": 6, "context": "edu) alternative methods [6, 7] considered extracting illumination invariant features for face recognition.", "startOffset": 25, "endOffset": 31}, {"referenceID": 7, "context": "To address pose or expression variations, earlier approaches extend classic subspace or template based face recognition methods [8, 9].", "startOffset": 128, "endOffset": 134}, {"referenceID": 8, "context": "To address pose or expression variations, earlier approaches extend classic subspace or template based face recognition methods [8, 9].", "startOffset": 128, "endOffset": 134}, {"referenceID": 9, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 144, "endOffset": 156}, {"referenceID": 10, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 144, "endOffset": 156}, {"referenceID": 11, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 144, "endOffset": 156}, {"referenceID": 12, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 353, "endOffset": 373}, {"referenceID": 13, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 353, "endOffset": 373}, {"referenceID": 14, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 353, "endOffset": 373}, {"referenceID": 15, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 353, "endOffset": 373}, {"referenceID": 16, "context": "To recognize a probe face with pose or expression changes, they either identified an implicit identity feature/representation of the probe face [10, 11, 12], which is pose- and expression-invariant, or explicitly estimate global or local mappings of facial appearance so that a virtual face under the normal condition can be synthesized for recognition [13, 14, 15, 16, 17].", "startOffset": 353, "endOffset": 373}, {"referenceID": 17, "context": ", the Active Appearance Models (AAMs) [18, 19] and elastic graph matching (EGM) [20].", "startOffset": 38, "endOffset": 46}, {"referenceID": 18, "context": ", the Active Appearance Models (AAMs) [18, 19] and elastic graph matching (EGM) [20].", "startOffset": 38, "endOffset": 46}, {"referenceID": 19, "context": ", the Active Appearance Models (AAMs) [18, 19] and elastic graph matching (EGM) [20].", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "To improve the localization accuracy, an explicit shape constraint for graph nodes was considered in the constrained local models (CLMs) [21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "CLMs [21, 22, 23] are still based on densely-connected graph models, and their shape constraints are over-simplified so that the dependency among different graph nodes is ignored.", "startOffset": 5, "endOffset": 17}, {"referenceID": 21, "context": "CLMs [21, 22, 23] are still based on densely-connected graph models, and their shape constraints are over-simplified so that the dependency among different graph nodes is ignored.", "startOffset": 5, "endOffset": 17}, {"referenceID": 22, "context": "CLMs [21, 22, 23] are still based on densely-connected graph models, and their shape constraints are over-simplified so that the dependency among different graph nodes is ignored.", "startOffset": 5, "endOffset": 17}, {"referenceID": 23, "context": "Recently, deformable part-based models (DPMs) show their promise in many applications such as object detection [24] and facial landmark localization [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "Recently, deformable part-based models (DPMs) show their promise in many applications such as object detection [24] and facial landmark localization [25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": "In particular, Zhu and Ramanan [25] adopted a tree-structured part model, which encodes node dependency while admitting efficient solutions.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "Such a tree model was used by Zhu and Ramanan [25] for facial landmark localization.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "[3] recently leveraged sparsity optimization and a carefully prepared gallery set (multiple images of varying illuminations per subject), to align probe ar X iv :1 50 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Our method is partially motivated by the promise of piece-wise planar formulation and the explicit shape constraints used in CLMs [21, 22, 23] and DPMs [24, 25].", "startOffset": 130, "endOffset": 142}, {"referenceID": 21, "context": "Our method is partially motivated by the promise of piece-wise planar formulation and the explicit shape constraints used in CLMs [21, 22, 23] and DPMs [24, 25].", "startOffset": 130, "endOffset": 142}, {"referenceID": 22, "context": "Our method is partially motivated by the promise of piece-wise planar formulation and the explicit shape constraints used in CLMs [21, 22, 23] and DPMs [24, 25].", "startOffset": 130, "endOffset": 142}, {"referenceID": 23, "context": "Our method is partially motivated by the promise of piece-wise planar formulation and the explicit shape constraints used in CLMs [21, 22, 23] and DPMs [24, 25].", "startOffset": 152, "endOffset": 160}, {"referenceID": 24, "context": "Our method is partially motivated by the promise of piece-wise planar formulation and the explicit shape constraints used in CLMs [21, 22, 23] and DPMs [24, 25].", "startOffset": 152, "endOffset": 160}, {"referenceID": 2, "context": ", [3, 26].", "startOffset": 2, "endOffset": 9}, {"referenceID": 25, "context": ", [3, 26].", "startOffset": 2, "endOffset": 9}, {"referenceID": 26, "context": "Part-based face recognition methods [27, 28, 29, 30, 31] have shown improved performance over more standard approaches using holistic faces.", "startOffset": 36, "endOffset": 56}, {"referenceID": 27, "context": "Part-based face recognition methods [27, 28, 29, 30, 31] have shown improved performance over more standard approaches using holistic faces.", "startOffset": 36, "endOffset": 56}, {"referenceID": 28, "context": "Part-based face recognition methods [27, 28, 29, 30, 31] have shown improved performance over more standard approaches using holistic faces.", "startOffset": 36, "endOffset": 56}, {"referenceID": 29, "context": "Part-based face recognition methods [27, 28, 29, 30, 31] have shown improved performance over more standard approaches using holistic faces.", "startOffset": 36, "endOffset": 56}, {"referenceID": 30, "context": "Part-based face recognition methods [27, 28, 29, 30, 31] have shown improved performance over more standard approaches using holistic faces.", "startOffset": 36, "endOffset": 56}, {"referenceID": 31, "context": "In this paper, we present experiments on the Multi-PIE [32] and MUCT [33] datasets and show that our proposed CPA method can simultaneously and effectively handle illumination, pose, and/or expression variations.", "startOffset": 55, "endOffset": 59}, {"referenceID": 32, "context": "In this paper, we present experiments on the Multi-PIE [32] and MUCT [33] datasets and show that our proposed CPA method can simultaneously and effectively handle illumination, pose, and/or expression variations.", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "\u2022 State-of-the-art pose-invariant face recognition method [34] relies on model learning using a large number of 3D face shapes, while training of our proposed CPA only requires 2D face images of a few subjects.", "startOffset": 58, "endOffset": 62}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] learned local patch based mapping relations from nonfrontal pose to frontal pose by locally linear regression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] estimated pixel-wise registration from non-neutral expression to neutral expression by optical flow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Arashloo and Kittler [16] considered a Markov random field (MRF) model to regularize 2D displacements of local patches across different poses.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "For automatic face recognition across pose, given a probe face image, AAMs [18, 19] optimized localization of a set of facial landmarks to realize face alignment of pixel-level accuracy.", "startOffset": 75, "endOffset": 83}, {"referenceID": 18, "context": "For automatic face recognition across pose, given a probe face image, AAMs [18, 19] optimized localization of a set of facial landmarks to realize face alignment of pixel-level accuracy.", "startOffset": 75, "endOffset": 83}, {"referenceID": 34, "context": "However, the landmarks localized by AAMs are usually not accurate enough, and also the pixelwise correspondence induced by matched landmarks are not consistent enough across different poses and expressions [35].", "startOffset": 206, "endOffset": 210}, {"referenceID": 35, "context": "Among existing methods, maybe the most successful ones across pose and/or expression are based on 3D models [36, 35, 34].", "startOffset": 108, "endOffset": 120}, {"referenceID": 34, "context": "Among existing methods, maybe the most successful ones across pose and/or expression are based on 3D models [36, 35, 34].", "startOffset": 108, "endOffset": 120}, {"referenceID": 33, "context": "Among existing methods, maybe the most successful ones across pose and/or expression are based on 3D models [36, 35, 34].", "startOffset": 108, "endOffset": 120}, {"referenceID": 23, "context": "Deformable part models (DPM) have shown their success in object detection [24], facial landmark localization [25], and human pose estimation [37].", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "Deformable part models (DPM) have shown their success in object detection [24], facial landmark localization [25], and human pose estimation [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 36, "context": "Deformable part models (DPM) have shown their success in object detection [24], facial landmark localization [25], and human pose estimation [37].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "Our use of part-based models is different from the DPM based methods [24, 25, 37].", "startOffset": 69, "endOffset": 81}, {"referenceID": 24, "context": "Our use of part-based models is different from the DPM based methods [24, 25, 37].", "startOffset": 69, "endOffset": 81}, {"referenceID": 36, "context": "Our use of part-based models is different from the DPM based methods [24, 25, 37].", "startOffset": 69, "endOffset": 81}, {"referenceID": 24, "context": "Compared with shape constraints in DPMs [25, 37], our proposed one models more complex relations with the part constellation and holds strict probabilistic properties that are beneficial to the model learning.", "startOffset": 40, "endOffset": 48}, {"referenceID": 36, "context": "Compared with shape constraints in DPMs [25, 37], our proposed one models more complex relations with the part constellation and holds strict probabilistic properties that are beneficial to the model learning.", "startOffset": 40, "endOffset": 48}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3]\u2019s method at Iteration 17 y D\u00b7x e", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "1 To pursue \u03bd, we consider techniques used in [26, 3].", "startOffset": 46, "endOffset": 53}, {"referenceID": 2, "context": "1 To pursue \u03bd, we consider techniques used in [26, 3].", "startOffset": 46, "endOffset": 53}, {"referenceID": 25, "context": "[26], Wagner et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] assumed there exist multiple registered face images of varying illuminations per subject in the database.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] optimized a holistic similarity transformation by solving a `-norm minimization problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Extending techniques in [3] directly to part-based alignment gives the following objective", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "The above direct extension of [3] essentially aligns the m parts independently.", "startOffset": 30, "endOffset": 33}, {"referenceID": 24, "context": "In particular, we are motivated by [25] to use a tree-structured shape model to constrain the difference of transformation parameters of different parts.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "We note that CPA is not designed to produce a seamless face constituted by deformed individual parts, as AAMs [18, 19] can do.", "startOffset": 110, "endOffset": 118}, {"referenceID": 18, "context": "We note that CPA is not designed to produce a seamless face constituted by deformed individual parts, as AAMs [18, 19] can do.", "startOffset": 110, "endOffset": 118}, {"referenceID": 24, "context": "Similar tree-structured shape model and quadratic regularization term were also used in [25] for face detection and facial landmark localization.", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "Compared to [25], our shape constraint is derived from a Bayesian network formulation, which not only interprets the underlying probabilistic properties of treestructured shape models, but also enables maximum a posteriori (MAP) estimation of the model parameters Z in the training stage of CPA.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "This scenario is different from [25], as their tree-structured shape model is integrated in a classification problem, which is strongly supervised.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "In fact, parametrization of AAMs and CLMs [21, 23] is also based on a joint Gaussian model, which seems to be similar to (6) derived from a tree model.", "startOffset": 42, "endOffset": 50}, {"referenceID": 22, "context": "In fact, parametrization of AAMs and CLMs [21, 23] is also based on a joint Gaussian model, which seems to be similar to (6) derived from a tree model.", "startOffset": 42, "endOffset": 50}, {"referenceID": 37, "context": "Similar iterative techniques have also been used in related works [38, 3], and showed good behaviors of convergence.", "startOffset": 66, "endOffset": 73}, {"referenceID": 2, "context": "Similar iterative techniques have also been used in related works [38, 3], and showed good behaviors of convergence.", "startOffset": 66, "endOffset": 73}, {"referenceID": 38, "context": "We solve the convex problem (8) by adapting the Augmented Lagrange Multiplier (ALM) method [39].", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "Compared to exact ALM, inexact ALM shows better practical performance in terms of optimization efficiency [39].", "startOffset": 106, "endOffset": 110}, {"referenceID": 39, "context": "In practice, we rely on off-the-shelf face detectors [41, 25], which provide a rough bounding box of the face or the holistic pose and locations of facial parts [25].", "startOffset": 53, "endOffset": 61}, {"referenceID": 24, "context": "In practice, we rely on off-the-shelf face detectors [41, 25], which provide a rough bounding box of the face or the holistic pose and locations of facial parts [25].", "startOffset": 53, "endOffset": 61}, {"referenceID": 24, "context": "In practice, we rely on off-the-shelf face detectors [41, 25], which provide a rough bounding box of the face or the holistic pose and locations of facial parts [25].", "startOffset": 161, "endOffset": 165}, {"referenceID": 2, "context": "[3] to initialize the holistic transformation \u03c3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "However, similar to the situation of holistic face alignment in [3], the presence of facial parts of multiple subjects in {D}i=1 makes (7) have many local minima, corresponding to aligning y to the facial parts of different subjects.", "startOffset": 64, "endOffset": 67}, {"referenceID": 25, "context": "For part-based face recognition, many existing methods such as SRC [26], LBP [42] and LDA [43] can be used at the part level, based on the pruned gallery.", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "For part-based face recognition, many existing methods such as SRC [26], LBP [42] and LDA [43] can be used at the part level, based on the pruned gallery.", "startOffset": 77, "endOffset": 81}, {"referenceID": 41, "context": "For part-based face recognition, many existing methods such as SRC [26], LBP [42] and LDA [43] can be used at the part level, based on the pruned gallery.", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "For obtaining better performance, we may adapt advanced aggregating schemes, such as the kernelized plurality voting [29], and joint recognition method for multiple observations, such as the joint dynamic sparse representation [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 29, "context": "For obtaining better performance, we may adapt advanced aggregating schemes, such as the kernelized plurality voting [29], and joint recognition method for multiple observations, such as the joint dynamic sparse representation [30].", "startOffset": 227, "endOffset": 231}, {"referenceID": 37, "context": "[38] to align a batch of linearly correlated images, such as frames in a video sequence or face images of a same subject.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38]\u2019s method at 20 iterations", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "by [38], in this paper, we leverage the very similar low-rank (matrices of part dictionaries without nuisances) and sparsity (error matrices modeling various variations) properties to align and form the part dictionaries.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "[38] to perform independent partbased alignment, the alignment process often converges to less meaningful solutions as shown in Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] in the case of aligning parts of a probe face to the gallery set, as we explained in Section III.", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "[38], Cand\u00e8s et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[44], the penalty parameters {\u03bb}i=1 can be set as the reciprocal of the square root of E\u3008i\u3009\u2019s row number, denoted as \u03c9\u3008i\u3009.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "In this illustration, we use 60 images from the MUCT dataset [33] to learn the CPA model define in Fig.", "startOffset": 61, "endOffset": 65}, {"referenceID": 39, "context": "The above algorithms for CPA model learning require a careful initialization on \u03c3 and \u03bd\u030c, which can be done either by detectors [41, 25] or by manual annotations.", "startOffset": 128, "endOffset": 136}, {"referenceID": 24, "context": "The above algorithms for CPA model learning require a careful initialization on \u03c3 and \u03bd\u030c, which can be done either by detectors [41, 25] or by manual annotations.", "startOffset": 128, "endOffset": 136}, {"referenceID": 24, "context": "To learn the configuration of a tree, Zhu and Ramanan [25] used Chow and Liu [45] algorithm for the application of facial landmark localization.", "startOffset": 54, "endOffset": 58}, {"referenceID": 43, "context": "To learn the configuration of a tree, Zhu and Ramanan [25] used Chow and Liu [45] algorithm for the application of facial landmark localization.", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "However, for a node with children, our tree-structured shape model (as well as Zhu and Ramanan [25]\u2019s shape model) assumes the independence between its part transformation and its part transformation differences with its children (refer to Section III-A), which is not an assumption for Chow-Liu algorithm.", "startOffset": 95, "endOffset": 99}, {"referenceID": 31, "context": "We used the CMU Multi-PIE [32] and MUCT [33] datasets to conduct our experiments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "We used the CMU Multi-PIE [32] and MUCT [33] datasets to conduct our experiments.", "startOffset": 40, "endOffset": 44}, {"referenceID": 39, "context": "For practical experiments in Section VI-D that conduct fully automatic face recognition across pose, we used off-the-shelf face detector [41] and pose estimator [25] to initialize our method.", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "For practical experiments in Section VI-D that conduct fully automatic face recognition across pose, we used off-the-shelf face detector [41] and pose estimator [25] to initialize our method.", "startOffset": 161, "endOffset": 165}, {"referenceID": 25, "context": "Our choice of off-the-shelf methods for face recognition across illumination is based on SRC [26], for which we used multiple gallery images of varying illuminations for each subject.", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "If using SRC [26] as a classifier, this alternative is essentially the same as in [3], which is", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "If using SRC [26] as a classifier, this alternative is essentially the same as in [3], which is", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "1) Evaluation on the Multi-PIE Dataset : The CMU MultiPIE [32] is the largest publicly available dataset suitable for test of our CPA method.", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": ", the method in [3].", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "Representatives of these methods include Nearest Subspace (NS) [2], Linear Discriminate Analysis (LDA) [43], and Local Binary Pattern", "startOffset": 63, "endOffset": 66}, {"referenceID": 41, "context": "Representatives of these methods include Nearest Subspace (NS) [2], Linear Discriminate Analysis (LDA) [43], and Local Binary Pattern", "startOffset": 103, "endOffset": 107}, {"referenceID": 40, "context": "(LBP) [42].", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "The method [3], i.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Compared with [3], Fig.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "When a small portion (10%) of face images is occluded, recognition rate of [3] drops below 90%.", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "For the 30% occlusion ratio, recognition rate of CPA is more than 45% higher than that of [3] for pose change of within \u00b115\u25e6.", "startOffset": 90, "endOffset": 93}, {"referenceID": 33, "context": "In this section, we used the Multi-PIE dataset to compare CPA with the state-of-the-art Morphable Displacement Field (MDF) method [34] for face recognition across pose.", "startOffset": 130, "endOffset": 134}, {"referenceID": 33, "context": "[34], we used the last 137 subjects instead of the first 229 ones in Multi-PIE for testing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "We realize a fully automatic CPA method by initializing our algorithm using Viola and Jones [41]\u2019s face detector, followed by a coarse and holistic alignment using the method of Wagner et al.", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "we consider two situations where poses of probe faces are either known in advance or estimated by Zhu and Ramanan [25]\u2019s method.", "startOffset": 114, "endOffset": 118}, {"referenceID": 39, "context": "Note that the fully automatic alignment works from coarse to fine granularity, say, sequentially uses Viola and Jones [41]\u2019s face detector, the method of Wagner et al.", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "[3] for a holistic alignment, and partbased alignment by CPA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "For larger degrees of pose change, performance of holistic alignment by [3] drops.", "startOffset": 72, "endOffset": 75}, {"referenceID": 24, "context": "\u2019s method [25] can work very well only when the training and test images are with the same illumination.", "startOffset": 10, "endOffset": 14}, {"referenceID": 44, "context": "To handle the illumination variation in our experiments, we normalized image illumination by the non-local means (NLM) based method [46] before pose estimation.", "startOffset": 132, "endOffset": 136}, {"referenceID": 33, "context": "Thanks to [34] for kindly producing the experimental results on the MDF model that are reported in Table VI.", "startOffset": 10, "endOffset": 14}], "year": 2015, "abstractText": "Developing a reliable and practical face recognition system is a long-standing goal in computer vision research. Existing literature suggests that pixel-wise face alignment is the key to achieve high-accuracy face recognition. By assuming a human face as piece-wise planar surfaces, where each surface corresponds to a facial part, we develop in this paper a Constrained Part-based Alignment (CPA) algorithm for face recognition across pose and/or expression. Our proposed algorithm is based on a trainable CPA model, which learns appearance evidence of individual parts and a tree-structured shape configuration among different parts. Given a probe face, CPA simultaneously aligns all its parts by fitting them to the appearance evidence with consideration of the constraint from the tree-structured shape configuration. This objective is formulated as a norm minimization problem regularized by graph likelihoods. CPA can be easily integrated with many existing classifiers to perform partbased face recognition. Extensive experiments on benchmark face datasets show that CPA outperforms or is on par with existing methods for robust face recognition across pose, expression, and/or illumination changes.", "creator": "LaTeX with hyperref package"}}}