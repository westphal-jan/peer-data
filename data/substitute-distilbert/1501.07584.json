{"id": "1501.07584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2015", "title": "Efficient Divide-And-Conquer Classification Based on Feature-Space Decomposition", "abstract": "this study presents random divide - through - conquer ( dc ) function based on feature space decomposition for simulations. when large - structure datasets are present, typical approaches usually employed truncated kernel methods on the feature space to dc approaches on the sample space. however, this did not guarantee separability between classes, led to overfitting. to assess such problems, this work proposes a novel dc approach where feature spaces consisting of three steps. firstly, we divide the feature space into eight ranks using the decomposition method proposed in this paper. subsequently, these feature subspaces have sent into individual local classifiers until training. finally, the outcomes of local classifiers are fused together to generate the final classification equation. experiments analyzing large - scale datasets are carried out for performance management. the differences show that the error rates of the proposed dc method decreased comparing with the state - of - the - art fast svm filter, e. g., reducing crash rates by 10. 53 % and 7. 53 % on rcv1 hardware covtype datasets alone.", "histories": [["v1", "Thu, 29 Jan 2015 20:41:29 GMT  (13kb)", "http://arxiv.org/abs/1501.07584v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi guo", "bo-wei chen", "feng jiang", "xiangyang ji", "sun-yuan kung"], "accepted": false, "id": "1501.07584"}, "pdf": {"name": "1501.07584.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Qi Guo", "Bo-Wei Chen", "Feng Jiang", "Xiangyang Ji", "Sun-Yuan Kung"], "emails": ["dennisbwc@gmail.com)"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n07 58\n4v 1\n[ cs\n.L G\n] 2\n9 Ja\nn 20\nIndex Terms\u2014 Feature space decomposition, feature space division, fusion, divide-and-conquer, classification"}, {"heading": "1. INTRODUCTION", "text": "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models. Furthermore, RBFs can map the input features into the intrinsic space [3] that is spanned by infinite-dimensional vectors. This correspondingly increases the opportunity of creating a discriminant hyperplane in the empirical space [3], subsequently enhancing discriminability. However, when input dimensions are sufficiently large, calculation of a kernel matrix becomes a burden. Moreover, RBFs may lead to overfitting due to infinite dimensions. To deal with such problems, rather than using conventional RBFs, Wu et al. [4] proposed using Truncated Radical Basis Functions (TRBFs) to avoid generating\ninfinite dimensions in the intrinsic space. Furthermore, they also devised an intrinsic data matrix, which was derived from a finite-decomposable kernel, to replace calculation of kernel matrices in the empirical space. Therefore, the time complexity was saved from original O(N3) to min(N3, J2N + J3) for KRR, where N is the number of instances, and J is the number of feature dimension expanded by TRBFs. Moreover, avoiding direct calculation of kernel matrices effectively resolved the need for matrix expansion.\nThe success of TRBF-based method relies on dimensional reduction in the intrinsic space and the conversion from empirical space to intrinsic space. Although computational load is relieved without losing too much accuracy, however, that method [4] did not improve discriminability and separability between features. Furthermore, the algorithmic architecture of that method did not support distributed processing, especially when mainstream toolboxes like Apach Hadoop (hadoop.apache.org) and Spark (spark.apache.org) adopt divide-and-conquer strategy in their implementation. Proposing a new architecture that supports divide-and-conquer computation correspondingly becomes necessary.\nIn response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far. Zhang et al. used divide-and-conquer KRR [5] to support computation of large-scale data. Firstly, their method randomly partitioned a dataset into subsets of equal size. Local solutions were subsequently computed by using KRR based on each subset. By averaging the local solutions, a global predictor was therefore obtained. Instead of using randomized data selection as Zhang et al. did, Hsieh et al. [6] focused on systematic data division before applying divideand-conquer classifiers to the data. In their approach, kernel K-means clustering was performed to select the representatives of the entire input data. Next, the members of a subset were selected based on one representative. Their experimental result showed a favorable accuracy when systematic data division was used.\nAlthough the above-mentioned approaches realized divideand-conquer concept in their algorithms, overfitting of kernel space was not fully addressed and resolved. To deal with the\naforementioned problems, this study proposes 1) A novel approach for feature-space decomposition, where the original feature space is converted to subspaces. Besides, the bases of each subspace are reranked according to their importance.\n2) A divide-and-conquer structure that allows independent local classifiers to create discriminant hyperplanes based subspaces rather than the entire empirical space. This lowers computational complexity while avoiding overfitting problems.\nThe rest of this paper is organized as follows. Section 2 introduces the overview of the proposed method. Section 3 then describes details of the proposed feature-space decomposition and fusion method. Next, Section 4 summarizes the performance of the proposed method and the analysis results. Conclusions are finally drawn in Section 5."}, {"heading": "2. SYSTEM OVERVIEW", "text": "Given an M\u00d7N data matrix X with N instances and M features and a 1\u00d7N label vector y, denote the feature space as \u2126, and X are the projection of the N instances on \u2126. We first define the feature-space decomposition method D = {T, I}, where T is a feature-space transform function, and I is a set of feature index groups.\nThe decomposition method D contains five sub-methods which are discussed in Section 3.1, namely, Random Decomposition (RD), Principle Component Analysis (PCA), Discriminant Component Analysis (DCA), Block Cholesky Decomposition (BCD) and Approximate Block Decomposition (ABD). Furthermore, each have an M\u00d7M sub-transform matrix, denoted as WRD, WPCA, WDCA, WBCD and WABD. Also, each contains a subset of feature index groups, e.g., IRD = {IRDi |I RD i \u2282 {1, 2, \u00b7 \u00b7 \u00b7 ,M}, i = 1, 2, \u00b7 \u00b7 \u00b7 , h\nRD}, where hRD is the number of feature subspaces decomposed by RD sub-method, respectively. As for T , we have\n\u2126\u2217 = T (\u2126), X\u2217 = T (X) = WX =\n\n  \nWRD\nWPCA WDCA WBCD WABD\n\n   X (1)\nwhere W and \u2126\u2217 are respectively the transform matrix and the new feature space. As for I , we have I = {IRD, IPCA, \u00b7 \u00b7 \u00b7 , IABD} , and the total number of subspaces is h = hRD+ hPCA+ \u00b7 \u00b7 \u00b7+hABD. Not all the sub-methods need to be used in real practice. If some are not applied, the corresponding WMethod and IMethod can just be empty.\nThe original feature space \u2126 is first transformed to \u2126\u2217 by T and then decomposed into subspaces \u2126\u22171,\u2126 \u2217 2, ...,\u2126 \u2217\nh by I; all the instances are first projected X\u2217 and subsequently decomposed into X\u22171 , X \u2217 2 , ..., X \u2217\nh.Then, a local classifier fi(i = 1, 2, \u00b7 \u00b7 \u00b7 , h), e.g., SVMs, KRRs, etc. is trained using data matrix X\u2217i . Let row vectors Ri = fi(X \u2217 i ) denote the results of\nfi on X\u2217i and R =\n\n \nR1 R2\n... Rh\n\n  . The elements of Ri can be\ndiscrete labels or continuous prediction values. The system generates the output based on R using fusion methods which are discussed in Section 3.2."}, {"heading": "3. PROPOSED DIVISION AND FUSION METHODS", "text": ""}, {"heading": "3.1. Feature-Space Decomposition", "text": "Section 2 shows that the merit of the proposed method is, it can perform classification within the subspaces and ignores the dependance among subspaces. Theoretically, decomposition method should be able to reduce the dependance as much as possible between any two feature subspaces while remaining dependance within the subspaces. This is the reason for conducting transformation on the feature space before division.\nAmong all the sub-methods in this study, the simplest idea is RD which directly decompose the feature space based on I . Its WRD is an M \u00d7M identity matrix.\nAs for PCA, we conduct PCA on the data matrix X and split up the features according to I . Since PCA diagonizes the feature covariance matrix S, this method eliminates the relevance of different features among and within subspaces. If the data obey Gaussian distribution, the PCA also eliminate the dependance of features among and within feature subspaces.\nDCA also conducts orthogonal transformation like PCA, while its discriminant matrix is [Sw + \u03c1I]\u22121S , where Sw is the within-class scatter matrix, and \u03c1 is the ridge parameter [3]. We have\nSw = \u03a3 L l=1\u03a3 Nl j=1[x (l) j \u2212 \u00b5 (l)][x (l) j \u2212 \u00b5 (l)]T (2)\nwhere l is the number of classes, Nl represents the number of samples in class l, and \u00b5(l) specifies the average point of class. We conduct generalized eigenvector decomposition [7] to obtain the eigenvectors \u03bd1, \u03bd2, ..., \u03bdMand eigenvalue matrix \u03bb1, \u03bb2, ..., \u03bbM , such that\nS\u03bdk = \u03bbk[Sw + \u03c1I]\u03bdk, k = 1, 2, ...,M (3)\nand the transform matrix is defined as WDCA = [\u03bd1, \u03bd2, \u00b7 \u00b7 \u00b7 , \u03bdM ]. Computing S and Sw both enjoys O(M2N) complexity. As [Sw + \u03c1I] can hardly be singular, the complexity of generalized eigenvalue decomposition equals that of \u03bbk[Sw + \u03c1I]\n\u22121S\u03bdk = \u03bbk\u03bdk, k = 1, 2, ...,M , which is of O(M3) time complexity. Therefore, the total complexity of DCA is O(2M2N +M3).\nBCD exploits a blocked Doolittle Algorithm, which is a form of Gaussian transformation rather than the orthogonal transformation, to eliminate the relevance among subspaces while remaining relevance within subspaces. For a symmetrical block matrix A , we eliminate the first row and column of\nblocks, as shown in Equation 4.\n\n  \nA\u2032 11 O12 \u00b7 \u00b7 \u00b7 OTk1 O21 A\u203222 \u00b7 \u00b7 \u00b7 A \u2032 k2 T\n... ...\n. . . ...\nOk1 A \u2032 k2 \u00b7 \u00b7 \u00b7 A\u2032 kk\n\n   = B1\n\n A11 \u00b7 \u00b7 \u00b7 ATk1 ... . . . ...\nAk1 \u00b7 \u00b7 \u00b7 Akk\n\n (B1)T\n(4)\nwhere B1 =\n\n  \nI11 O12 \u00b7 \u00b7 \u00b7 O1k \u2212A21A \u22121\n11 I22 \u00b7 \u00b7 \u00b7 O2k\n... ...\n. . . ...\n\u2212Ak1A \u22121 11 Ok2 \u00b7 \u00b7 \u00b7 Ikk\n\n   , and the divi-\nsion of blocks remains the same. The subscript of B indicates the row and column it eliminates. Iteratively, we subsequently generate B2,\u00b7 \u00b7 \u00b7 ,Bk to sequentially eliminate the rest rows and columns of blocks. The main goal of BCD is sequentially block-diagonizing the discriminant matrix based on the blocked Doolittle Algorithm. As shown in Algorithm 1, X is firstly rearranged to generate X\u0303 according to IBCD, in which MatrixSplit(X, IBCD) splits X into X\u03031, X\u03032, \u00b7 \u00b7 \u00b7 , X\u0303hBCD according to IBCD. The discriminant matrix of BCD is S. Function BlockedDoolittle(X\u0303, I) generates Bi(i = 1, 2, \u00b7 \u00b7 \u00b7 , h\nBCD) based on the idea of Equation 4 to eliminate the ith row and column of S. The BCD transform matrix is WBCD = BhBCDBhBCD\u22121 \u00b7 \u00b7 \u00b7B1. Comparing to BCD, PCA needs to do an M \u00d7 M matrix inversion, whose complexity is O(M3) on non-sparse matrix, whereas BCD only uses an M\nm \u00d7 M m matrix for m(m+1)2 times if divided\nequally, which only costs 1 m of the time of PCA.\nAlgorithm 1 [X\u2217,WBCD] = BCD(X, I) {X\u03031, X\u03032, ..., X\u0303n} =MatrixSplit(X, I) X\u0303 = [X\u03031, X\u03032, ..., X\u0303m]\nS = X\u0303X\u0303T =\n\n   S11 \u00b7 \u00b7 \u00b7 S1m ... . . . ...\nSm1 \u00b7 \u00b7 \u00b7 Smm\n\n   , where Sij = X\u0303iX\u0303j T\nfor i = 1 to hBCD do Bi =BlockedDoolittle(S, i) S = BiS(Bi)T end for WBCD = BhBCDBhBCD\u22121 \u00b7 \u00b7 \u00b7B1 X\u2217 = WBCDX\u0303\nBesides the aforementioned orthogonal transformation of PCA and DCA, as well as the Gaussian transformation of BCD, we also propose an approximate orthogonal transformation on which the ABD method is based. First we define a new operator \u2297 as Definition 1.\nDefinition 1 For two blocked matrix A = {Aij} and B = {Bij} with the same size and division of blocks, define operator \u2297, s.t.\nA\u2297B =\n\n x11 \u00b7 \u00b7 \u00b7 x1m ... . . . ...\nxm1 \u00b7 \u00b7 \u00b7 xmm\n\n (5)\nTable 1. Time complexity of different transformation methods.\nComplexity Detail RD O(N) Unsupervised, identity transform PCA O(M2N +M3) Unsupervised, orthogonal transform (OT) DCA O(2M2N +M3) Supervised, OT BCD O(M2N +M3/m) Unsupervised, Gaussian transform ABD O(MmN +m3) Unsupervised, approximate OT\nwhere xij equals the sum of all the elements of Aij elementwise multiply Bij .\nWe rewrite X as\n\n X11 \u00b7 \u00b7 \u00b7 X1N ... . . . ...\nXhABD1 \u00b7 \u00b7 \u00b7 XhABDN\n\n where\nwe divide each instance into m vectors according to I . The discriminant matrix is X\u2297X using this division. By conducting eigenvector decomposition on the discriminant matrix, we have\nX \u2297X = V T\u039bV (6)\nwhere V = {vij}, and each column of V is an eigenvector. The transform matrix is\nWABD =\n\n v11I11 \u00b7 \u00b7 \u00b7 v1N I1N ... . . . ...\nvm1Im1 \u00b7 \u00b7 \u00b7 vmN ImN\n\n . (7)\nIf there are approximately equal number of features in each subset, computing X \u2297X yields O(MmN) complexity and the eigenvalue decomposition costs O(m3). Therefore, the total complexity of ABD is O(MmN +m3).\nTable 1 shows the time complexity and details of the aforementioned sub-methods. By combining the five methods together, D includes both supervision (i.e., DCA) and unsupervision (i.e., RD,PCA,BCD and ABD) in transformation as well as four transformation methods."}, {"heading": "3.2. Feature Subspace Fusion", "text": "After obtaining the classification result matrix R from local classifier, we weight the outcome of each subspace by training a global classifier fn+1 by using R as a data matrix and y as labels. The output of fn+1 is the final prediction result. Observations show that m < 50 << N and TRBFKRR[4] generates favorable results for fn+1. As the training complexity of TRBFKRR is min(N3, J2N + J3),\nwhere J =\n(\nm+ p p\n)\n, and p is TRBF order. It is efficient\nto train on data matrix with a large number of instances and a small number of features like R."}, {"heading": "4. EXPERIMENTAL RESULT", "text": "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,\u00b7 \u00b7 \u00b7 ,fh in our system respectively and\ntested the results on large scale datasets (i.e., either M or N is larger than 104). Our methods are notated as \u201cDCclassifier1-classifier2\u201d, where \u201cclassifier1\u201d indicates the classifier used for f1,f1,\u00b7 \u00b7 \u00b7 ,fm, and \u201cclassifier2\u201d is for fn+1. All the experiments are conducted on an Intel Core i7 2.1GHz CPU and 8G RAM machine. The datasets tested in this paper are shown in Table 3 and can be downloaded from http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/ or UCI Machine Learning Repository.\nFeature-Space Decomposition Setting: Table 2 shows the decomposition setting in our experiments. For data matrices with high feature dimensions, e.g., news20, RCV1, we just use RD and ABD with relatively low computational complexity. For data matrices with low feature dimensions, all the transformation methods can be combined together to achieve a lower error rate. We use LibLinear as the global classifier when dealing with census dataset, as its proportion of positive and negative instances are 0.06/0.94, which can cause bias when TRBFKRR is applied.\nLinear Classification: Linear classification is conducted on news20 and RCV1 datasets. LibLinear is exploited as local classifiers f1,f2,\u00b7 \u00b7 \u00b7 ,fm, and TRBFKRR is used as global classifier fn+1 in our system. We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11]. As shown in Table 4, our methods either have advantages on training times or error rates. .\nNon-Linear Classification: DCSVM is set as local classifiers for nonlinear classification. Interestingly, in DCDCSVM-TRBFKRR, divide-and-conquer process is conducted on both instance dimension and feature dimension in the method. We evaluate it on covtype dataset and compare with the results of the other SVM methods by Hsieh et al. [6], as is shown in Table 5. The proposed method achieve the lowest error rate with relatively low time complexity in both covtype and census datasets.\nMoreover, comparing to directly training a TRBFKRR classifier using the whole data matrix, DC-TRBFKRRTRBFKRR greatly reduces the training complexity from min(N3, J2N+J3) to min(N3, J\n2N m + J 3 m2 ), which enables\nTRBFKRR training on data matrix with large N and M ."}, {"heading": "5. CONCLUSION", "text": "This paper presents a feature-space decomposition classification method including five sub-methods. The experimental results show that our divide-and-conquer classification scheme can reduce error rates (e.g., reduce error rates by 10.53% and 7.53% in covtype and RCV1 datasets), comparing to training directly using the whole datasets, and outperform stateof-the-art fast SVM solver by reducing overfitting problem. The future work will focus on providing theoretical analysis for feature-space decomposition and its effects on divide-andconquer classification.\n1Results are cited from Keerthi et al. [12] 2Results are cited from Hsieh et al. [6]"}, {"heading": "6. REFERENCES", "text": "[1] V. N. Vapnik and V. Vapnik, Statistical Learning Theory, vol. 2, New York, NY: Wiley, 1998.\n[2] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-based Learning Methods, Cambridge: Cambridge University Press, 2000.\n[3] S.-Y. Kung, Kernel Methods and Machine Learning, Cambridge: Cambridge University Press, 2014.\n[4] S.-Y. Kung and P.-Y. Wu, \u201cOn efficient learning and classification kernel methods,\u201d in Proc. 2012 IEEE Int. Conf. Acoustics, Speech and Signal Processing, Kyoto, Japan, Mar. 25-30, 2012, pp. 2065\u20132068.\n[5] Y. Zhang, J. Duchi, and M. Wainwright, \u201cDivide and conquer kernel ridge regression,\u201d in Proc. 2013 Conf. on Learning Theory, Princeton, NJ, USA, Jun. 12-14, 2013, pp. 592\u2013617.\n[6] C.-J. Hsieh, S. Si, and I. S. Dhillon, \u201cA divide-andconquer solver for kernel support vector machines,\u201d in Proc. 31st Int. Conf. Machine Learning, Beijing, China, 2014, pp. 566\u2013574.\n[7] C. B. Moler and G. W. Stewart, \u201cAn algorithm for generalized matrix eigenvalue problems,\u201d SIAM Journal on Numerical Analysis, vol. 10, no. 2, pp. 241\u2013256, 1973.\n[8] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, \u201cLiblinear: A library for large linear classification,\u201d J. Mach. Learn. Res., vol. 9, pp. 1871\u20131874, June 2008.\n[9] T. Joachims, \u201cMaking large-scale SVM learning practical,\u201d in Advances in Kernel Methods \u2013 Support Vector Learning, B. Scho\u0308lkopf, C. Burges, and A. Smola, Eds. Cambridge, MA: MIT Press, 1999.\n[10] W. Kao, K. Chung, C. Sun, and C. Lin, \u201cDecomposition methods for linear support vector machines,\u201d Neural Computation, vol. 16, no. 8, pp. 1689\u20131704, Aug 2004.\n[11] V. Sindhwani and S. S. Keerthi, \u201cLarge scale semisupervised linear svms,\u201d in Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, New York, NY, USA, 2006, pp. 477\u2013484.\n[12] S. S. Keerthi and D. DeCoste, \u201cA modified finite newton method for fast solution of large scale linear svms,\u201d Journal of Machine Learning Research, vol. 6, no. 3, pp. 341\u2013361, 2005."}], "references": [{"title": "Statistical Learning Theory", "author": ["V.N. Vapnik", "V. Vapnik"], "venue": "vol. 2, New York, NY: Wiley", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge: Cambridge University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Kernel Methods and Machine Learning", "author": ["S.-Y. Kung"], "venue": "Cambridge: Cambridge University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "On efficient learning and classification kernel methods,", "author": ["S.-Y. Kung", "P.-Y. Wu"], "venue": "IEEE Int. Conf. Acoustics, Speech and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "and M", "author": ["Y. Zhang", "J. Duchi"], "venue": "Wainwright, \u201cDivide and conquer kernel ridge regression,\u201d in Proc. 2013 Conf. on Learning Theory, Princeton, NJ, USA, Jun. 12-14", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "and I", "author": ["C.-J. Hsieh", "S. Si"], "venue": "S. Dhillon, \u201cA divide-andconquer solver for kernel support vector machines,\u201d in Proc. 31st Int. Conf. Machine Learning, Beijing, China", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "An algorithm for generalized matrix eigenvalue problems,", "author": ["C.B. Moler", "G.W. Stewart"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1973}, {"title": "Liblinear: A library for large linear classification,", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Making large-scale SVM learning practical,", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods \u2013 Support Vector Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Decomposition methods for linear support vector machines,", "author": ["W. Kao", "K. Chung", "C. Sun", "C. Lin"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Large scale semisupervised linear svms,", "author": ["V. Sindhwani", "S.S. Keerthi"], "venue": "Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A modified finite newton method for fast solution of large scale linear svms,", "author": ["S.S. Keerthi", "D. DeCoste"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models.", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models.", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "Furthermore, RBFs can map the input features into the intrinsic space [3] that is spanned by infinite-dimensional vectors.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "This correspondingly increases the opportunity of creating a discriminant hyperplane in the empirical space [3], subsequently enhancing discriminability.", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "[4] proposed using Truncated Radical Basis Functions (TRBFs) to avoid generating infinite dimensions in the intrinsic space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Although computational load is relieved without losing too much accuracy, however, that method [4] did not improve discriminability and separability between features.", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "In response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "In response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "used divide-and-conquer KRR [5] to support computation of large-scale data.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "[6] focused on systematic data division before applying divideand-conquer classifiers to the data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "DCA also conducts orthogonal transformation like PCA, while its discriminant matrix is [Sw + \u03c1I]S , where Sw is the within-class scatter matrix, and \u03c1 is the ridge parameter [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 6, "context": "We conduct generalized eigenvector decomposition [7] to obtain the eigenvectors \u03bd1, \u03bd2, .", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Observations show that m < 50 << N and TRBFKRR[4] generates favorable results for fn+1.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,\u00b7 \u00b7 \u00b7 ,fh in our system respectively and", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,\u00b7 \u00b7 \u00b7 ,fh in our system respectively and", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "[6], as is shown in Table 5.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] 2Results are cited from Hsieh et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6]", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively.", "creator": "LaTeX with hyperref package"}}}