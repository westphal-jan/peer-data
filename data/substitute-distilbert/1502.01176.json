{"id": "1502.01176", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2015", "title": "Learning Local Invariant Mahalanobis Distances", "abstract": "for many analytic and tensor types, there are natural transformations to which objective data should be invariant or insensitive. for instance, in visual imaging, certain images should involve shifted to orientation and translation. this requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. in this paper we propose a novel and computationally efficient way to learn a local mahalanobis metric per transformed, and show how we to learn a local invariant metric to any transformation in order to change performance.", "histories": [["v1", "Wed, 4 Feb 2015 12:27:04 GMT  (112kb,D)", "http://arxiv.org/abs/1502.01176v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ethan fetaya", "shimon ullman"], "accepted": true, "id": "1502.01176"}, "pdf": {"name": "1502.01176.pdf", "metadata": {"source": "CRF", "title": "Learning Local Invariant Mahalanobis Distances", "authors": ["Ethan Fetaya", "Shimon Ullman"], "emails": [], "sections": [{"heading": null, "text": "data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance.\nMetric learning is a machine learning task which learns a distance metric d(x, y) between data points, based on data instances. As distances play an important role in many machine learning algorithms, e.g. k-Nearest Neighbor and k-Means clustering, finding an appropriate metric for the task can improve performance considerably. This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.\nA standard approach to metric learning is to learn a global Mahalanobis metric\nd(x, y)2M = (x\u2212 y)TM(x\u2212 y) (1)\nWhere M is a positive semi-definite matrix (PSD). The PSD constraint only assures this is a pseudometric , but for simplicity we will not make this distinction. Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data. As M is a PSD matrix, it can be written as M = LTL and therefore\nd(x, y)2M = (x\u2212 y)TM(x\u2212 y) = ||x\u0303\u2212 y\u0303||22 x\u0303 = Lx, y\u0303 = Ly.\nThis means that finding an optimal Mahalanobis distance is equivalent to finding the optimal linear transformation on the data, and then using L2 distance on the transformed data. This approach has two limitations, first it is limited to linear transformation. Second, it requires a large amount of labeled data.\nOne approach that can be used to overcome the first limitation is to use local distances [10] where we learn a unique distance function per training datum. Local approaches do not produce, in general, a global metric (as they are usually not symmetrical) but are commonly considered metric learning nonetheless. These methods, in\nar X\niv :1\n50 2.\n01 17\n6v 1\n[ cs\n.L G\n] 4\nF eb\n2 01\ngeneral, need similar and dissimilar training data for each local metric.\nIn our current work we use a local approach inspired by the work on exemplar-SVM [18], that showed that using only negative examples can suffice for good performance. The intuition behind this is that objects of the same class do not necessarily have to be similar, but objects from different classes must be dissimilar. We will show how to learn a local Mahalanobis distance that for each datum tries to keep the non-class as far away as possible. This approach can use a large amount of weakly supervised data, as in many cases negative examples are easier then positive examples to acquire. For example, if we are interested in face identification, we can learn a local metric around a query face image given a bank of train face images, which we only assume do not belong to the queried person. Unlike other metric learning methods, we will not need any labels on which image belongs to which person in the negative set.\nThe intuition why Mahalanobis distances are the natural model for local metrics is simple. Assume we have some metric d(x, y) on the dataset and assume that it is smooth (at least continuously twice differentiable). From the metric properties we know that if we fix x and look at f(y) = d(y, x) then f has a global minimum at y = x. Applying second order Tylor approximation to f around x we get\nd(y, x) = f(y) \u2248 f(x) + (y \u2212 x)T\u2207f(x)+ (y \u2212 x)T\u22072f(x)(y \u2212 x) = (y \u2212 x)T\u22072f(x)(y \u2212 x)\n(2)\nThe equality holds since x is the global minimum with value f(x) = d(x, x) = 0, and this also implies that \u22072f(x) is positive semidefinite. While the Taylor approximation only holds for values of y close to x, as metric methods such as k-NN focus on similar objects the approximation should be good at the points of interest. This observation leads us to look for local matrices that are of the form of a Mahalanobis distance.\nWe will first define our local Mahalanobis distance learning method as a semidefinite programming problem. We will then show how this problem can be solved efficiently without any costly matrix decompositions. This allows us to solve high dimensional problems that regular semidefinite solvers cannot handle.\nThe second major contribution of this paper will be to show how invariant local matrices can be learned. In many cases we know there are simple transformations that our metric should not be sensitive to. For example, small translation and rotation on natural images. We know a priori that if x\u2032 = T (x), where T is the said transformation, then d(x, x\u2032) \u2248 0. We will show how this prior knowledge about our data can by incorporated by learning a local invariant metric. This also can be done in an efficient manner, and we will show that this improves performance in our experiments."}, {"heading": "1 Related work", "text": "Metric learning is an active research field with many algorithms, generally divided into linear [21] which learn a Mahalanobis distance, non-linear [14] that learn a nonlinear\ntransformation and use L2 distance on the transformed space, and local which learn a metric per datum. The LMNN and MLMM [21] algorithm are considered the leading metric learning method. For a recent comprehensive survey that covers linear, nonlinear and local methods see [2].\nThe exemplar-SVM algorithm [18] can be seen as a local similarity measure. This is obtained by maximizing margins, with a linear model, and is weakly supervised as our work. Unlike exemplar-SVM, we learn a Mahalanobis matrix and can learn an invariant metric. Another related work is PMLM [20], which also finds a local Mahalanobis metric for each data point. However, this method uses global constraints, and therefore cannot work with weakly supervised data, i.e. a single positive example. All the techniques above do not learn local invariant metrics.\nThe most common way to achieve invariance, or at least insensitivity, to a transformation in computer vision applications is by using hand-crafted descriptors such as SIFT [17] or HOG [8]. Another way, used in convolutional networks [15], is by adding pooling and subsampling forcing the net to be insensitive to small translations. It is important to note that transformations such as rotations have a global behaviour, i.e. there is a global consistency between the pixel movement. This global consistency is not totally captured by the pooling and subsampling. As we will see in our experiments, using an invariant metric can be useful even when working with robust features such as HOG."}, {"heading": "2 Local Mahalanobis", "text": "In this section we will show how a local Mahalanobis distance with maximal margin can be learned in a fast and simple way.\nWe will assume that we are given a single query image x0 that belong to some class, e.g. a face of a person. We will also be given a set of negative data x1, ..., xN that do not belong to that class, e.g. a set of face images of various other people. We will learn a local Mahalanobis metric for x0, M(x0) 0, where M 0 means M is positive semi-definite. For matricesM,N , we will denote by ||M || the Frobinous norm ||M ||2 = \u2211 ij M 2 ij and by \u3008M,N\u3009 the standard inner product \u3008M,N\u3009 = \u2211 ij MijNij .\nWe wish to find a Mahalanobis matrix M given the positive datum x0 and the negative data x1, ..., nn. Large margin methods have been very successful in metric learning [21], and more generally in machine learning, therefore, our algorithm will look for the PSD matrixM that maximizes the distance to the closest negative example\nM = argmax M 0 ( min 1\u2264i\u2264n\n(xi \u2212 x0)TM(xi \u2212 x0)) (3)\nThe optimization cannot be solved as it is not bounded, since multiplying M by a scalar multiplies the minimum distance by the same scalar. This can be solved by normalizing M to have ||M || = 1. As normally done with margin methods, we can\nminimize the norm under fixed margin constrained instead of maximizing the margin under fixed norm constraint. The resulting objective is\nM(x0) = argmin M\n1 2 ||M ||2\nsubject to : (xi \u2212 x0)TM(xi \u2212 x0) \u2265 2 \u2200i \u2208 {1, ..., n} M 0\n(4)\nWhere the constant 2 is arbitrary and will be convenient later on. While this is a convex semidefinite programming task, it is very slow for reasonable dimensional data (in the thousands) even for state of the art solvers. This is because PSD solvers apply a projection to the semidefinite cone, performing an expensive singular value or eigen decomposition at each iteration.\nTo solve this optimization in a fast manner we will first relax the PSD constraint and look at the following objective\nM(x0) = argmin M\n1 2 ||M ||2\nsubject to : (xi \u2212 x0)TM(xi \u2212 x0) \u2265 2 \u2200i \u2208 {1, ..., n} (5)\nWe will then see how this is equivalent to a kernel SVM problem with a quadratic kernel, and therefore can be solved easily with off-the-shelf SVM solvers such as LIBSVM [5]. Finally we will show how the solution of objective 5 is in fact the solution of objective 4 resulting in a fast solution to objective 4 without any matrix decomposition.\nTheorem 1. The solution of objective 5 is given by running kernel SVM with kernel k(x, y) = \u3008x, y\u30092 on inputs x\u03030, x\u03031, ..., x\u0303n where x\u0303i = xi \u2212 x0\nProof. Define \u03d5(x) = x \u00b7 xT , a function that maps a column vector to a matrix. This function has the following simple properties:\n\u2022 k(x, y) = \u3008x, y\u30092 = \u3008\u03d5(x), \u03d5(y)\u3009, i.e. the function \u03d5 is the mapping associated with the quadratic kernel.\n\u2022 For any matrix W , we have \u3008W,\u03d5(x)\u3009 = xTWx.\nwhich can be easily verified using \u03d5(x)ij = xixj . We can define auxiliary labelling y0 = \u22121 and yi = 1 for 1 \u2264 i \u2264 n. Combining everything objective 5 can be rewritten as\nM(x0) = argmin M\n1 2 ||M ||2\nsubject to : yi \u00b7 (\u3008M,\u03d5(x\u0303i)\u3009 \u2212 1) \u2265 1 \u2200i \u2208 {0, 1, ..., n} (6)\nwhere we can include i = 0 as \u3008M,\u03d5(x\u03030)\u3009 = 0 for any matrix.\nObjective 6 is exactly an SVM problem with quadratic kernel, with bias fixed to one, given inputs x\u03030, ..., x\u0303n, proving the theorem. Notice also that for the identity matrix M = I we have \u3008M,\u03d5(x\u0303i)\u3009 > 0 for i \u2265 1 and \u3008M,\u03d5(x\u0303i)\u3009 = 0 for i = 0, therefore the data is separable by M = I and the optimization is feasible.\nNow that we have shown how objective 5 can be converted into a standard SVM form, for which efficient solvers exists, we will show how it is the solution to objective 4.\nTheorem 2. The solution to objective 5 is the solution to objective 4.\nProof. To prove the theorem it suffices to show that the solution is indeed positive semidefinite. A well known observation arrising from the dual formulation of the SVM objective [4] is that the optimal solution M has the form\nM = n\u2211 i=0 \u03b1iyi\u03d5(x\u0303i), \u03b1i \u2265 0. (7)\nSince \u03d5(x) 0 for any x, as its only nonzero eigenvalue is ||x||, \u03d5(x\u03030) = \u03d5(0) = 0, and yi = 1 for i \u2265 1 we get\nM = n\u2211 i=0 \u03b1iyi\u03d5(x\u0303i) = n\u2211 i=1 \u03b1i\u03d5(x\u0303i) 0, (8)\nwhere the positive semidefiniteness in eq. 8 is assured due to the set of PSD matrices being a convex cone.\nCombining theorem 1 with theorem 2 we get that in order to solve objective 4 it is enough to run an SVM solver with a quadratic kernel function, thus avoiding any matrix decomposition.\nLooking at this as a SVM problem has further benefits. The SVM solvers do not compute M directly, but return the set of support vectors x\u0303i1 , ..., x\u0303ik and coefficients \u03b1i1 , ..., \u03b1ik such that M = \u2211 k \u03b1ik\u03d5(x\u0303ik). This allows us to work in high dimension d, where theO(d2) memory needed to store the matrix can be a problem, and can slow computations further. As the rank of the matrix is bounded by the number of support vectors, one can see that in many applications we get a relatively low rank matrix. This bound on the rank can be improved by using sparse-SVM algorithms [7]. In practice we got low rank matrices without resorting to sparse SVM solvers."}, {"heading": "3 Local Invariant Mahalanobis", "text": "For some applications, we know a priori that certain transformations should have a small effect on the metric. We will show how to include this knowledge into the local metric we learn, learning locally invariant metrices. In section 4 we will see this has a major effect on performance.\nAssume we know a set of functions T1, ..., Tk that the desired metric should be insensitive to, i.e. d(x, Ti(x)) should be small for all x and i. A canonical example is small rotations and translations on natural images. One of the major issues in computer vision arises from the instability of the pixel representation to these transformations. Various descriptors such as SIFT [17] and HOG [8] offer a more robust representation, and have been highly successful in many computer vision applications. We will show in section 4 that even when using a relatively robust representation such as HOG, learning an invariant metric has a significant impact.\nA natural way to mathematically formulate the idea of being insensitive to a transformation, is to require the leading term of the approximation to vanish in that direction. In our case this means\n(T (x0)\u2212 x0)T\u22072yd(x0, y)(T (x0)\u2212 x0) = 0. (9) If we return to our basic intuition of the local Mahalanobis matrix as the Hessian\nmatrix \u22072yd(x0, y), we can now state the new local invariant Mahalanobis objective\nM(x0) = argmin M\n1 2 ||M ||2\nsubject to :\n(xi \u2212 x0)TM(xi \u2212 x0) \u2265 2 \u2200i \u2208 {1, ..., n} (Tj(x0)\u2212 x0)TM(Tj(x0)\u2212 x0) = 0 \u2200j \u2208 {1, ..., k} M 0\n(10)\nWe will show how by applying a small transformation to the data, we can reduce this to objective 4 which we can solved easily.\nTheorem 3. Define V = span{T1(x0) \u2212 x0, ..., Tk(x0) \u2212 x0}, then the minimizer of objective 4 with xi \u2212 x0 replaced by zi, its projection to V \u22a5 is the minimizer of objective 10.\nProof. For PSD matrices, the constraint that (Tj(x0) \u2212 x0)TM(Tj(x0) \u2212 x0) = 0 is equivalent to M(Tj(x0) \u2212 x0) = 0. This can be seen if we write the vector in the basis of M eigenvectors, and notice that components with positive eigenvalues have a positive contribution to the quadratic form. This means that Mv = 0 for all v \u2208 V . Each vector xi\u2212x0 can be split into two orthogonal elements, xi\u2212x0 = zi+vi where vi is its projection onto V and zi is its projection onto V \u22a5. Our equality constraints (Tj(x0)\u2212 x0)TM(Tj(x0)\u2212 x0) = 0 now imply\n(xi \u2212 x0)TM(xi \u2212 x0) = (zi + vi)TM(zi + vi) = zTi Mzi (11) since all the other terms vanish. We can now rewrite objective 10 as\nM(x0) = argmin M\n1 2 ||M ||2\nsubject to : zTi Mzi \u2265 2 \u2200i \u2208 {1, ..., n} Mv = 0 \u2200v \u2208 V M 0\n(12)\nIf we forget the equality constrains we get objective 4 with xi \u2212 x0 replaced by zi. To finish the proof we need to show that the solution to the optimization without the equality constraints, does indeed satisfy them.\nAs we have already seen in the proof of theorem 2 the optimal solution is of the form M = \u2211 \u03b1i\u03d5(zi) = \u2211 \u03b1izi \u00b7 zTi . The vector zi is a member of V T so for v \u2208 V\nMv = (\u2211 \u03b1izi \u00b7 zTi ) v = \u2211 \u03b1izi \u00b7 (zTi v) = 0 (13)\nProving that the solution satisfies the equality constraints.\nA few comments are worth noting about this formulation. First the problem may not be linearly separable, although in our experiments with real data we did not encounter any unseperable case. This can be easily solved, if needed, by the standard method of adding slack variables. Second, the algorithm just adds a simple preprocessing step to the previous algorithm and runs in approximately the same time."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Running time", "text": "We compared running the optimization with an SVM solver [5], to solving it as a semidefinite problem and as a quadratic problem (relaxing the semidefinite constraint). The main limitation when running off-the-shelf solvers is memory. Quadratic or semidefinite solvers need a constraint matrix, which in our case is a full matrix of size n\u00d7 d2 where n is the number of samples and d is the data dimension. We tested all three approaches on the MNIST dataset of dimension 784 using only 5000 negative examples, as this already resulted in a matrix of size 24.6Gb.\nCurrently first order methods, such as ADMM [3], are the leading approaches to solving problems such as quadratic and semidefinite programming for large matrices. We used YALMIP for modeling and solved using SCS [19]. The time to run this as an semidefinite program was 1152 \u00b1 417sec. The time it took to run this as a quadratic program was 545\u00b174sec. In comparison, when we run this as an SVM problem it took at most 0.36sec. We excluded the time needed to build the n\u00d7d2 constraint matrix for the quadratic and semidefinite solvers.\nThis order of magnitude improvement should not be a surprise. It is a well known that while SVM can be solved as a quadratic program, generic quadratic solvers perform much slower then solver designed specifically for SVM."}, {"heading": "4.2 MNIST", "text": "The MNIST dataset is a well known digit recognition dataset, comprising of 28 \u00d7 28 grayscale images on which we perform deskewing preprocessing. For each of the\n60, 000 training images we computed a local Mahalanobis distance and local invariant Mahalanobis(using only negative examples). On test time we performed knn classification with k = 3 using the local metrics. We show some examples of nearest neighbours in Figure 4.2. We compared this with exemplar-SVM, as the leading technique most similar to ours. We also compared our scores to exemplar-SVM where we add the tansformed images as positive training data. To show the importance of the invariance objective, we compare also to SVM with quadratic kernel to which we add the transformed data as positive training data (unlike the way we use the shifted data). Finally, we compared our results to the state-of-the-art metric learning LMNN method (linear metric), and to MLMNN, a local version of LMNN, which learns multiple metrics (but not one per datum).\nAs can be seen in table 1, we perform much better then exemplar SVM and are comparable with MLMNN. It is important to note that unlike MLMNN, we compare each datum only to negatives, so our methods is applicable in scenarios where MLMNN is not.\nAnother key observation is the difference between the invariant-Mahalanobis and the quadratic-SVM with shifts. While very similar functionally, we see that looking at\nthe problem as a local Mahalanobis matrix gives important intuition, i.e. the way to use the shifted images, that leads to better performance."}, {"heading": "4.3 Labeling faces in the wild (LFW)", "text": "LFW is a challenging dataset containing 13,233 face images of 5749 different individuals with a high level of variability. The LFW dataset is divided into 10 subsets, when the task is to classify 600 pairs of images from one subset to same/not-same using the other 9 subsets as training data. We perform the unsupervised LFW task, where we do not use any labelling inside the training images we get, besides the fact that they are different than both test images.\nWe used the aligned images [13] and represented using HOG features [8]. We compared our results to a cosine similarity baseline, to exemplar-SVM and exemplarSVM with shifts. We note that we cannot use LMNN or MLMNN on this data, as we only have negative images with a single positive image.\nAs we can see from table 2, the local Mahalanobis greatly out-performs the exemplarSVM. We also see that even when using robust features such as HOG, learning an invariant metric improves performance, albeit to a lesser degree."}, {"heading": "5 Summary", "text": "We showed an efficient way to learn a local Mahalanobis metric given a query datum and a set of negative data points. We have also shown how to incorporate prior knowledge about our data, in particular the transformations to which it should be robust, and use it to learn locally invariant metrics. We have shown that our methods are competitive with leading methods while being applicable to other scenarios where methods such as LMNN and MLMNN cannot be used."}], "references": [{"title": "Learning a mahalanobis metric from equivalence constraints. JMLR", "author": ["A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "Chu", "B. Eric Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery", "author": ["C. Burges"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Learning optimally sparse support vector machines", "author": ["A. Cotter", "S. Shalev-Shwartz", "N. Srebro"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Information-theoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Image retrieval and classification using local distance functions", "author": ["A. Frome", "Y. Singer", "J. Malik"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["S. Hoi", "W. Liu", "Chang", "S.-F"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Learning to align from scratch", "author": ["G. Huang", "M.A. Mattar", "H. Lee", "E. Learned-Miller"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Non-linear metric learning", "author": ["D. Kedem", "S. Tyree", "K. Weinberger", "F. Sha"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Efficient learning of mahalanobis metrics for ranking", "author": ["D. Lim", "G. Lanckriet"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints. IJCV", "author": ["D. Lowe"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Ensemble of exemplar-svms for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A. Efros"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Operator splitting for conic optimization via homogeneous self-dual embedding", "author": ["B. O\u2019Donoghue", "E. Chu", "N. Parikh", "S. Boyd"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Parametric local metric learning for nearest neighbor classification", "author": ["J. Wang", "A. Woznica", "A. Kalousis"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Learning a mahalanobis distance metric for data clustering and classification", "author": ["S. Xiang", "F. Nie", "C. Zhang"], "venue": "Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 111, "endOffset": 118}, {"referenceID": 5, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 111, "endOffset": 118}, {"referenceID": 15, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few.", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data.", "startOffset": 19, "endOffset": 29}, {"referenceID": 0, "context": "Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data.", "startOffset": 19, "endOffset": 29}, {"referenceID": 8, "context": "Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data.", "startOffset": 19, "endOffset": 29}, {"referenceID": 9, "context": "One approach that can be used to overcome the first limitation is to use local distances [10] where we learn a unique distance function per training datum.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "In our current work we use a local approach inspired by the work on exemplar-SVM [18], that showed that using only negative examples can suffice for good performance.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "Metric learning is an active research field with many algorithms, generally divided into linear [21] which learn a Mahalanobis distance, non-linear [14] that learn a nonlinear", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "Metric learning is an active research field with many algorithms, generally divided into linear [21] which learn a Mahalanobis distance, non-linear [14] that learn a nonlinear", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "The LMNN and MLMM [21] algorithm are considered the leading metric learning method.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "For a recent comprehensive survey that covers linear, nonlinear and local methods see [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 17, "context": "The exemplar-SVM algorithm [18] can be seen as a local similarity measure.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Another related work is PMLM [20], which also finds a local Mahalanobis metric for each data point.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "The most common way to achieve invariance, or at least insensitivity, to a transformation in computer vision applications is by using hand-crafted descriptors such as SIFT [17] or HOG [8].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "The most common way to achieve invariance, or at least insensitivity, to a transformation in computer vision applications is by using hand-crafted descriptors such as SIFT [17] or HOG [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 14, "context": "Another way, used in convolutional networks [15], is by adding pooling and subsampling forcing the net to be insensitive to small translations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "Large margin methods have been very successful in metric learning [21], and more generally in machine learning, therefore, our algorithm will look for the PSD matrixM that maximizes the distance to the closest negative example", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "We will then see how this is equivalent to a kernel SVM problem with a quadratic kernel, and therefore can be solved easily with off-the-shelf SVM solvers such as LIBSVM [5].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "A well known observation arrising from the dual formulation of the SVM objective [4] is that the optimal solution M has the form", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "This bound on the rank can be improved by using sparse-SVM algorithms [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 16, "context": "Various descriptors such as SIFT [17] and HOG [8] offer a more robust representation, and have been highly successful in many computer vision applications.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "Various descriptors such as SIFT [17] and HOG [8] offer a more robust representation, and have been highly successful in many computer vision applications.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "1 Running time We compared running the optimization with an SVM solver [5], to solving it as a semidefinite problem and as a quadratic problem (relaxing the semidefinite constraint).", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Currently first order methods, such as ADMM [3], are the leading approaches to solving problems such as quadratic and semidefinite programming for large matrices.", "startOffset": 44, "endOffset": 47}, {"referenceID": 18, "context": "We used YALMIP for modeling and solved using SCS [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "We used the aligned images [13] and represented using HOG features [8].", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "We used the aligned images [13] and represented using HOG features [8].", "startOffset": 67, "endOffset": 70}], "year": 2015, "abstractText": "For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance. Metric learning is a machine learning task which learns a distance metric d(x, y) between data points, based on data instances. As distances play an important role in many machine learning algorithms, e.g. k-Nearest Neighbor and k-Means clustering, finding an appropriate metric for the task can improve performance considerably. This approach has been applied successfully to many problems such as face identification [11], image retrieval [12, 6], ranking [16] and clustering [22] to name just a few. A standard approach to metric learning is to learn a global Mahalanobis metric d(x, y)M = (x\u2212 y)M(x\u2212 y) (1) Where M is a positive semi-definite matrix (PSD). The PSD constraint only assures this is a pseudometric , but for simplicity we will not make this distinction. Various algorithms [21, 1, 9] differ by the objective through which they learn the matrix M from the data. As M is a PSD matrix, it can be written as M = LL and therefore d(x, y)M = (x\u2212 y)M(x\u2212 y) = ||x\u0303\u2212 \u1ef9||2 x\u0303 = Lx, \u1ef9 = Ly. This means that finding an optimal Mahalanobis distance is equivalent to finding the optimal linear transformation on the data, and then using L2 distance on the transformed data. This approach has two limitations, first it is limited to linear transformation. Second, it requires a large amount of labeled data. One approach that can be used to overcome the first limitation is to use local distances [10] where we learn a unique distance function per training datum. Local approaches do not produce, in general, a global metric (as they are usually not symmetrical) but are commonly considered metric learning nonetheless. These methods, in 1 ar X iv :1 50 2. 01 17 6v 1 [ cs .L G ] 4 F eb 2 01 5 general, need similar and dissimilar training data for each local metric. In our current work we use a local approach inspired by the work on exemplar-SVM [18], that showed that using only negative examples can suffice for good performance. The intuition behind this is that objects of the same class do not necessarily have to be similar, but objects from different classes must be dissimilar. We will show how to learn a local Mahalanobis distance that for each datum tries to keep the non-class as far away as possible. This approach can use a large amount of weakly supervised data, as in many cases negative examples are easier then positive examples to acquire. For example, if we are interested in face identification, we can learn a local metric around a query face image given a bank of train face images, which we only assume do not belong to the queried person. Unlike other metric learning methods, we will not need any labels on which image belongs to which person in the negative set. The intuition why Mahalanobis distances are the natural model for local metrics is simple. Assume we have some metric d(x, y) on the dataset and assume that it is smooth (at least continuously twice differentiable). From the metric properties we know that if we fix x and look at f(y) = d(y, x) then f has a global minimum at y = x. Applying second order Tylor approximation to f around x we get d(y, x) = f(y) \u2248 f(x) + (y \u2212 x)\u2207f(x)+ (y \u2212 x)\u2207f(x)(y \u2212 x) = (y \u2212 x)\u2207f(x)(y \u2212 x) (2) The equality holds since x is the global minimum with value f(x) = d(x, x) = 0, and this also implies that \u2207f(x) is positive semidefinite. While the Taylor approximation only holds for values of y close to x, as metric methods such as k-NN focus on similar objects the approximation should be good at the points of interest. This observation leads us to look for local matrices that are of the form of a Mahalanobis distance. We will first define our local Mahalanobis distance learning method as a semidefinite programming problem. We will then show how this problem can be solved efficiently without any costly matrix decompositions. This allows us to solve high dimensional problems that regular semidefinite solvers cannot handle. The second major contribution of this paper will be to show how invariant local matrices can be learned. In many cases we know there are simple transformations that our metric should not be sensitive to. For example, small translation and rotation on natural images. We know a priori that if x\u2032 = T (x), where T is the said transformation, then d(x, x\u2032) \u2248 0. We will show how this prior knowledge about our data can by incorporated by learning a local invariant metric. This also can be done in an efficient manner, and we will show that this improves performance in our experiments.", "creator": "LaTeX with hyperref package"}}}