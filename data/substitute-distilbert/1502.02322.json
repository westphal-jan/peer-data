{"id": "1502.02322", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Rademacher Observations, Private Data, and Boosting", "abstract": "induced minimization of the logistic loss is second popular approach to batch supervised learning. our paper starts from the surprising information that, when fitting linear ( or kernelized ) classifiers, the minimization of the event loss is \\ item { matching } to the description of an algorithm \\ textit { rado } - loss computed ( i ) over transformed data pieces we call rademacher observations ( rados ), and ( ii ) over the \\ query { same } classifier as the one of the logistic loss. thus, a classifier learnt from rados can be \\ textit { expected } equipped to classify \\ textit { observations }. we exhibit a learning algorithm over instances with machine - compliant convergence rates minus the \\ textit { logistic loss } ( computed over examples ). experiments on domains with up few millions distinct examples, backed up by theoretical calculations, display that learning over a small set of random rados can challenge the state of the individuals that operate over larger \\ textit { complete } set of examples. we show that rados comply with various privacy requirements that offer them good fits for machine learning in a privacy framework. we give several algebraic, geometric and computational hardness results on reconstructing examples from rados. we similarly show how it is possible students make, and efficiently learn from, rados in a differential privacy framework. tests resembling faster learning from differentially private attributes can compete with learning from random rados, and hence smooth batch learning from examples, achieving non - trivial results vs accuracy tradeoffs.", "histories": [["v1", "Mon, 9 Feb 2015 01:12:11 GMT  (317kb,D)", "https://arxiv.org/abs/1502.02322v1", null], ["v2", "Thu, 2 Apr 2015 03:55:51 GMT  (317kb,D)", "http://arxiv.org/abs/1502.02322v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard nock", "giorgio patrini", "arik friedman"], "accepted": true, "id": "1502.02322"}, "pdf": {"name": "1502.02322.pdf", "metadata": {"source": "CRF", "title": "Rademacher Observations, Private Data, and Boosting", "authors": ["Richard Nock", "Giorgio Patrini"], "emails": ["richard.nock@nicta.com.au", "giorgio.patrini@anu.edu.au", "arik.friedman@nicta.com.au"], "sections": [{"heading": "1 Introduction", "text": "This paper deals with the following fundamental question:\nWhat information is sufficient for learning, and what guarantees can it bring that regular data cannot ?\nBy \u201cregular\u201d, we mean the usual inputs provided to a learner. In our context of batch supervised learning, this is a training set of examples, each of which is an observation with a class, and learning means inducing in reduced time an accurate function from observations to classes, a classifier. It turns out that we do not need the detail of classes to learn a classifier (linear or kernelized): an aggregate, whose size is the dimension of the observation space, is minimally sufficient, the mean operator [24].\nar X\niv :1\n50 2.\n02 32\n2v 2\n[ cs\n.L G\nBut do we need examples ? This perhaps surprising and non-trivial question is becoming crucial now that the nature of stored and processed signals intelligence data is heavily debated in the public sphere [19, 28]. In the context of machine learning (ML), the objective of being accurate is more and more frequently subsumed by more complex goals, sometimes involving challenging tradeoffs in which accuracy does not ultimately appear in the topmost requirements. Privacy is one such crucial goal [10, 14, 15]. There are various models to capture the privacy requirement, such as secure multi-party computation and differential privacy (DP, [12]). The former usually relies on cryptographic protocols, which can be heavy even for bare classification and simple algorithms [4]. The latter usually relies on the power of randomization to ensure that any \u201clocal\u201d change cannot be spotted from the output delivered [13, 12]. In a ML setting, randomization can be performed at various stages, from the examples to the output of a classifier. We focus on the upstream stage of the process, i.e. the input to the learner, which grants the benefits that all subsequent stages also comply with differential privacy. Randomization has its power: it also has its limits in this case, as it may significantly degrade the performance of learners.\nThe way we address this problem starts from a surprising observation, whose relevance to supervised ML goes beyond learning with private data: learning a linear (or kernelized) classifier over examples throughout the minimization of the expected logistic loss is equivalent to learning the same classifier by minimizing an exponential loss over a complete set of transformed data that we call Rademacher observations, rados. Each rado is the sum of edge vectors over examples (edge = observation \u00d7 label). We also show that efficient learning from all rados may also be achieved when carried out over subsets of all possible rados.\nThis is our first contribution, and we expect it to be useful in several other areas of supervised learning. In the context of learning with private data, our other contributions can be summarized as showing how rados may yield new privacy guarantees \u2014 not limited to differential privacy \u2014 while authorising boosting-compliant rates for learning. More precisely, our second contribution is to propose a rado-based learning algorithm, which has boosting-compliant convergence rates over the logistic loss computed over the examples. Thus, we learn an accurate classifier over rados, and the same classifier is accurate over examples as well.\nThe fact that efficient learning may be achieved through subset of rados is interesting because it opens the problem of designing this particular subset to address domain-specific requirements that add to the ML accuracy requirement. Among our other contributions, we provide one important design example, showing how to build differentially private mechanisms for rado delivery, such as when protecting specific sensitive features in data. Experiments confirm in this case that learning from differentially private rados may still be competitive with learning from examples. We provide another design which pairs to our rado-based boosting algorithm, with the crucial property that when examples have been DP-protected by the popular Gaussian mechanism [12], the joint pair (rado delivery design, boosting algorithm) may achieve convergence rates comparable to the noisefree setting with high probability, even over strong DP protection regimes. Our last contribution is to show that rados may protect the privacy of the original examples not only in the DP framework, but also from several algebraic, geometric and even computational-complexity theoretic standpoints.\nThe remainder of this paper is organized as follows. Section \u00a72 presents Rademacher observations, shows the equivalence between learning from examples and learning from rados, and how learning from subsets of rados may be sufficient for efficient learning; \u00a73 presents our rado-based boosting algorithm, and \u00a74 presents experiments with this algorithm; \u00a75 presents our results in DP models, \u00a76 presents related experiments; \u00a77 provides results on the hardness of reconstructing examples from rados from algebraic, geometric and computational standpoints. To keep a readable paper, proofs and additional experiments are given in two separate appendices available in Section\n10 (proofs) and Section 11 (experiments)."}, {"heading": "2 Rados and supervised learning", "text": "Let [n] = {1, 2, ..., n}. We are given a set of m examples S .= {(xi, yi), i \u2208 [m]}, where xi \u2208 X \u2286 Rd is an observation and yi \u2208 {\u22121, 1} is a label, or class. X is the domain. A linear classifier \u03b8 \u2208 \u0398 for some fixed \u0398 \u2286 Rd gives a label to x \u2208 X equal to the sign of \u03b8>x \u2208 R. Our results can be lifted to kernels (at least with finite dimension feature maps) following standard arguments [26]. We let \u03a3m . = {\u22121, 1}m.\nDefinition 1 For any \u03c3 \u2208 \u03a3m, the Rademacher observation \u03c0\u03c3 with signature \u03c3 is \u03c0\u03c3 .= (1/2) \u00b7\u2211 i(\u03c3i + yi)xi.\nThe simplest way to randomly sample rados is to pick \u03c3 as i.i.d. Rademacher variables, hence the name. Reference to S is implicit in the definition of \u03c0\u03c3. A Rademacher observation sums edge vectors (the terms yixi), over the subset of examples for which yi = \u03c3i. When \u03c3 = y is the vector of classes, \u03c0\u03c3 = m\u00b5S is m times the mean operator [26, 24]. When \u03c3 = \u2212y, we get the null vector \u03c0\u03c3 = 0. A popular approach to learn \u03b8 over S is to minimize the surrogate risk Flog (S,\u03b8) built from the logistic loss (logloss):\nFlog (S,\u03b8) . = 1\nm \u2211 i log ( 1 + exp ( \u2212yi\u03b8>xi )) . (1)\nWe define the exponential rado-risk F rexp(S,\u03b8,U), computed on any U \u2286 \u03a3m with cardinal |U| = n, as:\nF rexp(S,\u03b8,U) .\n= 1\nn \u2211 \u03c3\u2208U exp ( \u2212\u03b8>\u03c0\u03c3 ) . (2)\nIt turns out that Flog = g(F r exp) for some continuous strictly increasing g; hence, minimizing one criterion is equivalent to minimizing the other and vice versa. This is stated formally in the following Lemma.\nLemma 2 The following holds true, for any \u03b8 and S:\nFlog(S,\u03b8) = log(2) + 1\nm logF rexp(S,\u03b8,\u03a3m) . (3)\n(Proof in the Appendix, Subsection 10.1). Lemma 2 shows that learning with examples via the minimization of Flog (S,\u03b8), and learning with all rados via the minimization of F r exp(S,\u03b8,\u03a3m), are essentially equivalent tasks. Since the cardinal |\u03a3m| = 2m is exponential, it is unrealistic, even on moderate-size samples, to pick that latter option. This raises however a very interesting question: if we replace \u03a3m by subset U of size 2m,what does the relationship between examples and rados in eq. (3) become? We answer this question under the setting that:\n(i) instead of \u03a3m, we consider a predefined \u03a3r \u2286 \u03a3m;\n(ii) instead of considering U = \u03a3r, we sample uniformly i.i.d. U \u223c \u03a3r for n \u2265 1 rados.\nWhile (ii) is directly targeted at reducing the number of rados, (i) is an upper-level strategic design to tackle additional constraints, such as differential privacy. We now need following definition of the logistic rado-risk :\nF rlog (S,\u03b8,U) .\n= log(2) + 1\nm logF rexp(S,\u03b8,U) , (4)\nfor any U \u2286 \u03a3m, so that Flog (S,\u03b8) = F rlog (S,\u03b8,\u03a3m). We also define the open ball B(0, r) . = {x \u2208 Rd : \u2016x\u20162 < r}.\nTheorem 3 Assume \u0398 \u2286 B(0, r\u03b8), for some r\u03b8 > 0. Let:\n% . = sup\u03b8\u2032\u2208\u0398 max\u03c0\u03c3\u2208\u03a3r exp(\u2212\u03b8\u2032>\u03c0\u03c3)\nF rexp(S,\u03b8,\u03a3r) ,\n%\u2032 .= F rexp(S,\u03b8,\u03a3r)\nF rexp(S,\u03b8,\u03a3m) ,\nwhere \u03a3r follows (i) above. Then \u2200\u03b7 > 0, there is probability \u2265 1\u2212\u03b7 over the sampling of U in (ii) above that:\nFlog (S,\u03b8) \u2264 F rlog(S,\u03b8,U) +Q\u2212 1\nm \u00b7 log\n( 1\u2212 q\u221a\nn\n) , (5)\nwith\nq = \u2126 ( % \u00b7 \u221a r\u03b8 max\n\u03a3r \u2016\u03c0\u03c3\u20162 + d log\n2en\nd + log\n1\n\u03b7\n) (6)\nand Q . = \u2212(1/m) \u00b7 log %\u2032 satisfies Q = 0 if \u03a3r = \u03a3m and\nQ \u2264 r\u03b8 ( \u2016\u2207\u03b8F rlog (S,\u03b8,\u03a3m) \u20162 + \u03c0r ) (7)\notherwise, letting \u03c0r . = \u2016E\u03c3\u223c\u03a3r(1/m) \u00b7 \u03c0\u03c3\u20162. Furthermore, \u22000 \u2264 \u03b2 < 1/2, if m is sufficiently large, then letting \u03c0\u2217r . = max\u03a3r \u2016(1/m) \u00b7 \u03c0\u03c3\u20162, ineq. (5) becomes:\nFlog (S,\u03b8) \u2264 F rlog(S,\u03b8,U) +Q\n+O\n( % m\u03b2 \u00b7 \u221a r\u03b8\u03c0\u2217r n + d nm log 2en d\u03b7 ) . (8)\n(Proof in the Appendix, Subsection 10.2) Theorem 3 does not depend on the algorithm that learns \u03b8. The right-hand side of ineq. (5) shows two penalties. Q arises from the choice of \u03a3r and is therefore structural. Regardless of \u03a3r, when the classifier is reasonably accurate over all rados and expected examples edges in \u03a3r average to a ball of reduced radius, the upperbound on Q in ineq. (7) can be very small. The other penalty, which depends on q, is statistical and comes from the sampling in \u03a3r. Theorem 3 shows that when \u03a3r = \u03a3m, even when n m, the minimization of F rlog (S,\u03b8,U) may still bring, with high probability, guarantees on the minimization of Flog (S,\u03b8). Thus, a lightweight optimization procedure over a small number of rados may bring guarantees on the minimization of the expected logloss over examples for the same classifier. The following Section exhibits one such algorithm.\nAlgorithm 1 Rado boosting (RadoBoost)\nInput set of rados Sr . = {\u03c01,\u03c02, ...,\u03c0n}; T \u2208 N\u2217; Step 1 : let \u03b80 \u2190 0, w0 \u2190 (1/n)1 ; Step 2 : for t = 1, 2, ..., T\nStep 2.1 : [d] 3 \u03b9(t)\u2190 wfi(Sr,wt); Step 2.2 : let\nrt \u2190 1\n\u03c0\u2217\u03b9(t) n\u2211 j=1 wtj\u03c0j\u03b9(t) ; (9)\n\u03b1t \u2190 1\n2\u03c0\u2217\u03b9(t) log 1 + rt 1\u2212 rt ; (10)\nStep 2.3 : for j = 1, 2, ..., n\nw(t+1)j \u2190 wtj \u00b7 1\u2212 rt\u03c0j\u03b9(t)\u03c0\u2217\u03b9(t) 1\u2212 r2t  ; (11) Return \u03b8T defined by \u03b8Tk . = \u2211 t:\u03b9(t)=k \u03b1t , \u2200k \u2208 [d];"}, {"heading": "3 Boosting using rados", "text": "Algorithm 1 provides a boosting algorithm, RadoBoost, that learns from a set of Rademacher observations Sr . = {\u03c01,\u03c02, ...,\u03c0n}. Their (unknown) Rademacher assignments are denoted U .= {\u03c31,\u03c32, ...,\u03c3n} \u2286 \u03a3m. These rados have been computed from some sample S, unknown to RadoBoost. In the statement of the algorithm, \u03c0jk denotes coordinate k of \u03c0j , and \u03c0\u2217k . = maxj |\u03c0jk|. More generally, the coordinates of some vector z \u2208 Rd are denoted z1, z2, ..., zd. Step 2.1 gets a feature index \u03b9(t) from a weak feature index oracle, wfi. In its general form, wfi returns a feature index maximizing |rt| in (9). The weight update was preferred to AdaBoost\u2019s because rados can have large feature values and the weight update prevents numerical precision errors that could otherwise occur using AdaBoost\u2019s exponential weight update. We now prove a key Lemma on RadoBoost, namely the fast convergence of the exponential rado-risk F rexp(S,\u03b8,U) under a weak learning assumption (WLA). We shall then obtain the convergence of the logistic rado-risk (4), and, via Theorem 3, the convergence with high probability of Flog (S,\u03b8).\n(WLA) \u2203\u03b3 > 0 such that \u2200t \u2265 1, the feature returned by wfi in Step 2.2 (9) satisfies |rt| \u2265 \u03b3.\nLemma 4 Suppose the (WLA) holds. Then after T rounds of boosting in RadoBoost, the following upperbound holds on the exponential rado-loss of \u03b8T :\nF rexp(S,\u03b8T ,U) \u2264 exp ( \u2212T\u03b32/2 ) . (12)\n(Proof in the Appendix, Subsection 10.3) We now consider Theorem 3 with \u03a3r = \u03a3m, and therefore Q = 0. Blending Lemma 4 and Theorem 3 using (4) yields that, under the (WLA), we may observe with high probability (again, fixing \u03a3r = \u03a3m, so Q = 0 in Theorem 3):\nFlog (S,\u03b8T ) \u2264 log(2)\u2212 T\u03b32\n2m +Q\u2032 , (13)\nwhere Q\u2032 is the rightmost term in ineq. (5) or ineq. (8). So provided n 2m is sufficiently large, minimizing the exponential rado-risk over a subset of rados brings a classifier whose average logloss on the whole set of examples may decrease at rate \u2126(\u03b32/m) under a weak learning assumption made over rados only. This rate competes with those for direct approaches to boosting the logloss [23], and we now show that our weak learning assumption is also essentially equivalent to the one done in boosting over examples [27]. Let us rewrite rt(w) as the normalized edge in (9), making explicit the dependence in the current rado weights. Let\nrext (w\u0303) .\n= 1\nx\u2217\u03b9(t) m\u2211 i=1 wixi\u03b9(t) (14)\nbe the normalized edges for the same feature \u03b9(t) as the one picked in step 2.1 of RadoBoost, but computed over examples using some weight vector w\u0303 \u2208 Pm; here, Pm is the m-dim probability simplex and x\u2217\u03b9(t) . = maxi |xik|.\nLemma 5 \u2200wt \u2208 Pn, \u2200\u03b3 > 0, there exists w\u0303 \u2208 Pm and \u03b3ex > 0 such that |rt(wt)| \u2265 \u03b3 iff |rext (w\u0303)| \u2265 \u03b3ex.\n(Proof in the Appendix, Subsection 10.4) The proof of the Lemma gives clues to explain why the presence of outlier feature values may favor RadoBoost."}, {"heading": "4 Basic experiments with RadoBoost", "text": "We have compared RadoBoost to its main contender, AdaBoost [27], using the same weak learner; in AdaBoost, it returns a feature maximizing |rt| as in eq. (14). In these basic experi-\nments, we have deliberately not optimized the set of rados in which we sample U for RadoBoost; hence, we have \u03a3r = \u03a3m.\nWe have performed comparisons with 10 folds stratified cross-validation (CV) on 16 domains of the UCI repository [2] of varying size. For space considerations, Table 1 presents the results. Each algorithm was ran for a total number of T = 1000 iterations; furthermore, the classifier kept for testing is the one minimizing the empirical risk throughout the T iterations; in doing so, we also assessed the early convergence of algorithms. We fixed n = min{1000, train fold size/2}. Table 1 displays that RadoBoost compares favourably to AdaBoost, and furthermore it tends to be all the better as m and d increase. On some domains like Hardware and Twitter, the difference is impressive and clearly in favor of RadoBoost. As discussed for Lemma 5, we could interpret these comparatively very poor performances of AdaBoost as the consequence of outlier features that can trick AdaBoost in picking the wrong sign in the leveraging coefficient \u03b1t for a large number of iterations if we use real-valued classifiers (see column 100\u03c3 in Table 1). This drawback can be easily corrected (Cf Appendix, Subsection 11.1) by enforcing minimal |rt| values. This significantly improves AdaBoost on Hardware and Twitter. The improvements observed on RadoBoost are even more favorable."}, {"heading": "5 Rados and differential privacy", "text": "We now discuss the delivery of rados to comply with several DP constraints and their eventual impact on boosting. We thus adress both levels (i+ii) of rado delivery in \u00a72. Our general model is the standard DP model [12]. Intuitively, an algorithm is DP compliant if for any two neighboring datasets, it assigns similar probability to any possible output O. In other words, any particular record has only limited influence on the probability of any given output of the algorithm, and therefore the output discloses very little information about any particular record in the input. Formally, a randomized algorithm A is ( , \u03b4)-differentially-private [11] for some , \u03b4 > 0 iff:\nPA[O|S] \u2264 exp( ) \u00b7 PA[O|S\u2032] + \u03b4,\u2200S \u2248 S\u2032, O, (15)\nwhere the probability is over the coin tosses of A. This model is very strong, especially when \u03b4 = 0, and in the context of ML, maintaining high accuracy in strong DP regimes is generally\nAlgorithm 2 Feature-wise DP-compliant rados (DP-Feat)\nInput set of examples S, sensitive feature j\u2217 \u2208 [d], number of rados n, differential privacy parameter > 0; Step 1 : let \u03b2 \u2190 1/(1 + exp( /2)) \u2208 [0, 1/2); Step 2 : sample \u03c31,\u03c32, ...,\u03c3n i.i.d. (uniform) in \u03a3 \u03b2,j\u2217 m ; Return set of rados {\u03c0\u03c3 : \u03c3 sampled in Step 2};\na tricky tradeoff [10]. Because rados are an intermediate step between training sample S and a rado-based learner, there are two ways to design rados with respect to the DP framework: crafting DP-compliant rados from unprotected examples, or crafting rados from DP-compliant examples with the aim to improve the performance of the rado-based learner (Figure 5.2). These scenarii can be reduced to the design of \u03a3r."}, {"heading": "5.1 A feature-wise DP mechanism for rados", "text": "In this Subsection, we consider a relaxation of differential-privacy, namely feature-wise differential privacy, where the differential privacy requirement applies to j\u2217-neighboring datasets: we say that two samples S, S\u2032 are j\u2217-neighbors, noted S \u2248j\u2217 S\u2032, if they are the same except for the value of the jth\u2217 \u2208 [d] observation feature of some example. We further assume that the feature is boolean. For example, we may have a medical database containing a column representing the HIV status of a doctor\u2019s patients (1 row = a patient), and we do not wish that changing a single patient HIV status significantly changes the density of that feature\u2019s values in rados. This setting would also be very useful in genetic applications to hide in rados gene disorders that affect one or few genes. Feature-wise DP is analogous to the concept of \u03b1-label privacy [7], where differential privacy is guaranteed with respect to the label. Algorithm A in ineq. (15) is given in Algorithm 2. It relies on the following subset \u03a3r . = \u03a3\u03b2,j\u2217m \u2286 \u03a3m:\n\u03a3\u03b2,j\u2217m . = { \u03c3 \u2208 \u03a3m : \u03c0\u03c3j\u2217 \u2208 [ |{i : yixij\u2217 = +1}| \u2212 m\n2 \u00b1\u2206\u03b2\n]} , (16)\nwith \u2206\u03b2 . = (m/2) \u2212 \u03b2(m + 1). The key feature of this mechanism is that it does not alter the examples in the sense that DP-compliant rados belong to the set of cardinal 2m that can be generated from S. Usual data-centered DP mechanisms would rather alter data, e.g. via noise injection [15]. Algorithm 2 exploits the fact that it is the tails of feature j\u2217 that leak sensitive information about the feature in rados (see Figure 2). The following Theorem is stated so as we can pick small \u03b4, typically \u03b4 1/m. Other variants are possible that bring different tradeoffs between and \u03b4.\nTheorem 6 Assume is chosen so that = o(1) but = \u2126(1/m). In this case, DP-Feat maintains (n \u00b7 , n \u00b7 \u03b4)-differential privacy on feature j\u2217 for some \u03b4 > 0 such that \u00b7 \u03b4 = O(m\u22125/2).\n(Proof in the Appendix, Subsection 10.5) We have implemented Step 2 in Algorithm DP-Feat in the simplest way, using a simple Rademacher rejection sampling where each \u03c3j is picked i.i.d. as \u03c3j \u223c \u03a3m until \u03c3j \u2208 \u03a3\u03b2,j\u2217m . The following Theorem shows its algorithmic efficiency.\nTheorem 7 For any \u03b7 > 0, let n\u2217\u03b7 . = \u03b7(1\u2212 exp(2\u03b2 \u2212 1))/(4\u03b2), and let nR denote the total number of rados sampled in \u03a3m until n rados are found in \u03a3 \u03b2,j\u2217 m . Then for any \u03b7 > 0, there is probability \u2265 1\u2212 \u03b7 that\nnR \u2264 n \u00b7 { 1 if n \u2264 n\u2217\u03b7\u2308 1\nmDBE(1\u2212\u03b2\u20161/2) log n n\u2217\u03b7\n\u2309 otherwise ,\nwhere DBE is the bit-entropy divergence: DBE(p\u2016q) = p log(p/q) + (1\u2212 p) log((1\u2212 p)/(1\u2212 q)), for p, q \u2208 (0, 1).\n(Proof in the Appendix, Subsection 10.6) Remark that replacing \u03a3m by \u03a3r = \u03a3 \u03b2,j\u2217 m would not necessarily impair the boosting convergence of RadoBoost trained from rados samples from DPFeat (Lemma 4). The only systematic change would be in ineq. (13) where we would have to integrate the structural penalty Q from Theorem 3 to further upperbound Flog (S,\u03b8T ). In this case, the upperbound in (7) reveals that at least when the mean operator in \u03a3\u03b2,j\u2217m has small norm \u2014 which may be the case even when some examples in S have large norm \u2014 and the gradient penalty is small, then Q may be small as well.\nWe end up with several important remarks, whose formal statements and proofs are left out due to space constraints. First, the tail truncation design exploited in DP-Feat can be fairly simply generalized in two directions, to handle (a) real-valued features, and/or (b) several sensitive features instead of one. Second, we can do DP-compliant design of rado delivery beyond featurewise privacy, e.g. to protect \u201crado-wide\u201d quantities like norms."}, {"heading": "5.2 Boosting from DP-compliant examples via rados", "text": "We now show how to craft rados from DP-compliant examples so as to approximately keep the convergence rates of RadoBoost. More precisely, since edge vectors are sufficient to learn (eq. 1), we assume that edge vectors are DP-compliant (neighbor samples, S \u2248 S\u2032, would differ on one edge vector). A gold standard to protect data in the DP framework is to convolute data with noise. One popular mechanism is the Gaussian mechanism [12, 16], which convolutes data with independent Gaussian random variables N(0, \u03c22I), whose standard deviation \u03c2 depends on the DP requirement ( , \u03b4). Strong DP regimes are tricky to handle for learning algorithms. For example, the approximation factor \u03c1 of the singular vectors under DP noise of the noisy power method roughly behaves as \u03c1 = \u2126(\u03c2/\u2206) [16] (Corollary 1.1) where \u2206 = O(d) is a difference between two\nsingular values. When \u03c2 is small, this is a very good bound. When the DP requirement blows up, the bound remains relevant if d increases, which may be hard to achieve in practice \u2014 it is easier in general to increase m than d, which requires to compute new features for past examples.\nWe consider ineq. (15) with neighbors I and I \u2032 being two sets of m edge vectors differing by one edge vector, and O is a noisified set of m edge vectors generated through the Gaussian mechanism [12] (Appendix A). We show the following non-trivial result: provided we design another particular \u03a3r, the convergence rate of RadoBoost, as measured over non-noisy rados, essentially survives noise injection in the edge vectors through the Gaussian mechanism, even under strong noise regimes, as long as m is large enough. The intuition is straightforward: we build rados summing a large number of edge vectors only (this is the design of \u03a3r), so that the i.i.d. noise component gets sufficiently concentrated for the algorithm to be able to learn almost as fast as in the noise-free setting. We emphasize the non-trivial fact that convergence rate is measured over the non-noisy rados, which of course RadoBoost does not see. The result is of independent interest in the boosting framework, since it makes use of a particular weak learner (wfi), which we call prudential, which picks features with |rt| (9) upperbounded.\nWe start by renormalizing coefficients \u03b1t (eq. (10)) in RadoBoost by a parameter \u03ba \u2265 1 given as input, so that we now have \u03b1t \u2190 (1/(2\u03ba\u03c0\u2217\u03b9(t))) log((1 + rt)/(1\u2212 rt)) in Step 2.2. It is not hard\nto check that the convergence rate of RadoBoost now becomes, prior to applying the (WLA)\nF rlog(S,\u03b8T ,U) \u2264 log(2)\u2212 1\n2\u03bam \u2211 t r2t . (17)\nWe say that wfi is \u03bbp-prudential for \u03bbp > 0 iff it selects at each iteration a feature such that |rt| \u2264 \u03bbp. Edges vectors have been DP-protected as yi(xi + xri ), with xri \u223c N(0, \u03c22I) (for i \u2208 [m]). Let m\u03c3 . = |{i : \u03c3i = yi}| denote the support of a rado, and (m\u2217 > 0 fixed):\n\u03a3r = \u03a3 m\u2217 m . = {\u03c3 \u2208 \u03a3m : m\u03c3 = m\u2217} . (18)\nTheorem 8 \u2200U \u2286 \u03a3r, \u2200\u03c4 > 0, if \u221a m\u2217 = \u2126 (\u03c2 ln(1/\u03c4)), then \u2203\u03bbp > 0 such that RadoBoost having access to a \u03bbp-prudential weak learner returns after T iteration a classifier \u03b8T which meets with probability \u2265 1\u2212 \u03c4:\nF rlog(S,\u03b8T ,U) \u2264 log(2)\u2212 1\n4\u03bam \u2211 t r2t . (19)\nThe proof, in the Appendix (Subsection 10.7), details parameters and dependencies hidden in the statement. The use of a prudential weak learner is rather intuitive in a noisy setting since \u03b1t blows up when |rt| is close to 1. Theorem 8 essentially yield that a sufficiently large support for rados is enough to keep with high probability the convergence rate of RadoBoost within noise-free regime. Of course, the weak learner is prudential, which implies bounded |rt| < 1, and furthermore the leveraging coefficients \u03b1t are normalized, which implies smaller margins. Still, Theorem 8 is a good theoretical argument to rely on rados when learning from DP-compliant edge vectors."}, {"heading": "6 Experiments on differential privacy", "text": "Table 2 presents a subset of the experiments carried out with RadoBoost and AdaBoost in the contexts of Subsections 5.1 and 5.2 (see Section 11 for all additional experiments). Unless otherwise stated, experimental settings (cross validation, number of rados for learning, etc.) are the same as in Section 4.\nIn a first set of experiments, we have assessed the impact on learning of the feature-wise DP mechanism: on each tested domain, we have selected at random a binary feature, and then used Algorithm DP-Feat to protect the feature for different values of DP parameter , in a range that covers usual DP experiments [18] (Table 1). The main conclusion that can be drawn from the experiments is that learning from DP-compliant rados can compete with learning from random rados, and even learning from examples (AdaBoost), even for rather small .\nWe then have assessed the impact on learning of examples that have been protected using the Gaussian mechanism [12], with or without rados, with or without a prudential weak learner for boosting, and with or without using a fixed support for rado computation. The Appendix provides extensive results for all domains but the largest ones (Twitter, SuSy, Higgs). In the central column (and Tables 4 through 7 in the Appendix), computing the differences between RadoBoost\u2019s error and AdaBoost\u2019s reveals that, on domains where it is beaten by AdaBoost when there is no noise, RadoBoost almost always rapidly become competitive with AdaBoost as noise increases. Hence, RadoBoost is a good contender from the boosting family to learn from differentially private (or noisy) data. Second, using a prudential weak learner which picks the median feature (instead of the\nmore efficient weak learner that picks the best as in Section 4) can have RadoBoost with fixed support rados compete or beat RadoBoost with plain random rados, at least for small noise levels (see Transfusion and Magic in the right column of Table 2). Replacing the median-prudential weak learner by a strong learner can actually degrade RadoBoost\u2019s results (see the Appendix, Tables 10 and 11). These two observations advocate in favor of the theory developed in Subsection 5.2. Finally, using rados with fixed support instead of plain random rados (Section 4) can significantly improve the performances of RadoBoost (see the Appendix, Tables 10 and 11)."}, {"heading": "7 From rados to examples: hardness results", "text": "The problem we address here is how we can recover examples from rados, and when we cannot recover examples from rados. This last setting is particularly useful from the privacy standpoint, as this may save us costly obfuscation techniques that impede ML tasks [4]."}, {"heading": "7.1 Algebraic and geometric hardness", "text": "For any m \u2208 N\u2217, we define matrix Gm \u2208 {0, 1}m\u00d72m as:\nGm . =\n[ 0>2m\u22121 1 > 2m\u22121\nGm\u22121 Gm\u22121\n] (20)\nif m > 1, and G1 . = [0 1] otherwise (zd denotes a vector in Rd). Each column of Gm is the binary indicator vector for the edge vectors considered in a rado. Hereafter, we let E \u2208 Rd\u00d7m the matrix of columnwise edge vectors from S, \u03a0 \u2208 Rd\u00d7n the columnwise rado matrix and U \u2208 {0, 1}2m\u00d7n in which each column gives the index of a rado computed in Sr. By construction, we have:\n\u03a0 = EGmU , (21)\nand so we have the following elementary results for the (non) reconstruction of E (proof omitted).\nLemma 9 (a) when recoverable, edge-vectors satisfy: E = \u03a0U>G>m(GmUU >G>m) \u22121; (b) when U, \u03a0, m are known but n < m, there is not a single solution to eq. (21) in general.\nLemma 9 states that even when U, \u03a0 and m are known, elementary constraints on rados can make the recovery of edge vectors hard \u2014 notice that such constraints are met in our experiments with RadoBoost in Sections 4 and 6.\nBut this represents a lot of unnecessary knowledge to learn from rados: RadoBoost just needs \u03a0 to learn. We now explore the guarantees that providing this sole information brings in terms of (not) reconstructing E. \u2200M \u2208 Ra\u00d7b, we let C(M) denote the set of column vectors, and for any C \u2286 Rd, we let C \u2295 .= \u222az\u2208CB(z, ). We define the Hausdorff distance, DH(E,E\u2032), between E and E\u2032:\nDH(E,E \u2032)\n. = inf{ : C(E) \u2286 C(E\u2032)\u2295 \u2227 C(E\u2032) \u2286 C(E)\u2295 } .\nThe following Lemma shows that if the only information known is \u03a0, then there exist samples that bring the same set of rados C(\u03a0) as the unknown E but who are at distance proportional to the \u201cwidth\u201d of the domain at hand.\nLemma 10 For any \u03a0 \u2208 Rd\u00d7n, suppose eq. (21) holds, for some unknowns m > 0, E \u2208 Rd\u00d7m, U \u2208 {0, 1}2m\u00d7n. Suppose C(E) \u2282 B(0, R) for some R > 0. Then there exists E\u2032 \u2208 Rd\u00d7(m+1), U\u2032 \u2208 {0, 1}2m+1\u00d7n such that\nC(E\u2032) \u2282 B(0, R) and \u03a0 = E\u2032Gm+1U\u2032 , (22)\nbut\nDH(E,E \u2032) = \u2126 ( R log d\u221a d logm ) (23)\nif m \u2265 2d, and DH(E,E\u2032) = \u2126(R/ \u221a d) otherwise.\n(Proof in the Appendix, Subsection 10.8) Hence, without any more knowledge, leaks, approximations or assumptions on the domain at hand, the recovery of E pays in the worst case a price proportional to the radius of the smallest enclosing B(0, .) ball for the unknown set of examples. We emphasize that this inapproximability result does not rely on the computational power at hand."}, {"heading": "7.2 Computational hardness", "text": "In this Subsection, we investigate two important problems in the recovery of examples. The first problem addresses whether we can approximately recover sparse examples from a given set of rados, that is, roughly, solve (21) with a sparsity constraint on examples. The first Lemma we give is related to the hardness of solving underdetermined linear systems for sparse solutions [9]. The sparsity constraint can be embedded in the compressed sensing framework [8] to yield finer hardness and approximability results, which is beyond the scope of our paper. We define problem \u201cSparse-Approximation\u201d as:\n(Instance) : set of rados Sr = {\u03c01,\u03c02, ...,\u03c0n}, m \u2208 N\u2217, r, ` \u2208 R+, \u2016.\u2016p, Lp-norm for p \u2208 R+;\n(Question) : Does there exist set S . = {(xi, yi), i \u2208 [m]} and set U .= {\u03c31,\u03c32, ...,\u03c3n} \u2208 {\u22121, 1}m such that:\n\u2016xi\u2016p \u2264 ` , \u2200i \u2208 [m] , (Sparse examples) \u2016\u03c0j \u2212 \u03c0\u03c3j\u2016p \u2264 r , \u2200j \u2208 [n] . (Rado approximation)\nLemma 11 Sparse-Approximation is NP-Hard.\n(Proof in the Appendix, Subsection 10.9) In the context of rados, the second problem we address has very large privacy applications. Suppose entity A\u00a9 has a huge database of people (e.g. clients), and obtains a set of rados emitted by another entity B\u00a9. An important question that A\u00a9 may ask is whether the rados observed can be approximately constructed by its database, for example to figure out which of its clients are also its competitors\u2019. We define this as problem \u201cProbe-SampleSubsumption\u201d:\n(Instance) : set of examples S, set of rados Sr = {\u03c01,\u03c02, ...,\u03c0n}, m \u2208 N\u2217, p, r \u2208 R+.\n(Question) : Does there exist S\u2032 .= {(xi, yi), i \u2208 [m]} \u2286 S and set U .= {\u03c31,\u03c32, ...,\u03c3n} \u2208 {\u22121, 1}m such that:\n\u2016\u03c0j \u2212 \u03c0\u03c3j\u2016p \u2264 r , \u2200j \u2208 [n] . (Rado approximation)\nLemma 12 Probe-Sample-Subsumption is NP-Hard.\n(Proof in the Appendix, Subsection 10.10) This worst-case result calls for interesting domain-specific qualifications, such as in genetics where the privacy of raw data, i.e. individual genomes, can be compromised by genome-wise statistics [17, 21]."}, {"heading": "8 Conclusion", "text": "We have introduced novel quantities that are sufficient for efficient learning, Rademacher observations. The fact that a subset of these can replace traditional examples for efficient learning opens interesting problems on how to craft these subsets to cope with additional constraints. We have illustrated these constraints in the field of efficient learning from privacy-compliant data, from various standpoints that include differential privacy as well as algebaric, geometric and computational considerations. In that last case, results rely on NP-Hardness, and thus go beyond the \u201chardness\u201d of factoring integers on which rely some popular cryptographic techniques [4]. Finally, rados are cryptography-compliant: homomorphic encryption schemes can be used to compute rados in the encrypted domain from encrypted edge vectors or examples \u2014 rado computation can thus be easily distributed in secure multiparty computation applications."}, {"heading": "9 Acknowledgments", "text": "The authors are indebted to Tiberio Cae\u0301tano for early discussions that brought the idea of Rademacher observations and their use in privacy related applications. Thanks are also due to Stephen Hardy and Hugh Durrant-Whyte for many stimulating discussions and feedback on the subject. NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "10 Appendix \u2014 Proofs", "text": "To simplify the proofs, we define the following quantity:\n\u03c0\u0303\u03c3 . = \u2211 i \u03c3ixi , \u2200\u03c3 \u2208 \u03a3m . (24)\nso that each rado can be defined as: \u03c0\u03c3 = (1/2) \u00b7 (\u03c0\u0303\u03c3 + \u03c0\u0303y). We recall that y is the label vector."}, {"heading": "10.1 Proof of Lemma 2", "text": "We have\nFlog (S,\u03b8) . = 1\nm \u2211 i log ( 1 + exp ( \u2212yi\u03b8>xi ))\n= 1\nm \u2211 i log  \u2211 y\u2208{\u22121,1} exp ( 1 2 \u00b7 y\u03b8>xi )\u2212 1 m \u00b7 1 2 \u00b7 \u03b8>\u03c0\u0303y (25)\n= 1\nm log \u2211 \u03c3\u2208\u03a3m exp ( 1 2 \u00b7 \u03b8>\u03c0\u0303\u03c3 ) \u2212 1 m \u00b7 1 2 \u00b7 \u03b8>\u03c0\u0303y\n= 1\nm log \u2211 \u03c3\u2208\u03a3m exp ( 1 2 \u00b7 \u03b8>\u03c0\u0303\u03c3 ) + 1 m \u00b7 log exp ( \u22121 2 \u00b7 \u03b8>\u03c0\u0303y ) = 1\nm log \u2211 \u03c3\u2208\u03a3m exp ( 1 2 \u00b7 \u03b8>(\u03c0\u0303\u03c3 \u2212 \u03c0\u0303y) ) = 1\nm log \u2211 \u03c3\u2208\u03a3m exp ( \u22121 2 \u00b7 \u03b8>(\u03c0\u0303\u03c3 + \u03c0\u0303y) ) (26)\n= log(2) + 1\nm log\n1\n2m \u2211 \u03c3\u2208\u03a3m exp ( \u22121 2 \u00b7 \u03b8>(\u03c0\u0303\u03c3 + \u03c0\u0303y) ) = log(2) + 1\nm log\n1\n2m \u2211 \u03c3\u2208\u03a3m exp ( \u2212\u03b8>\u03c0\u03c3 ) = log(2) + 1\nm logF rexp(S,\u03b8,\u03a3m) .\nWe refer to ([24]) (Lemma 1) for the proof of 25. Eq. (26) holds because \u03a3m is closed by negation."}, {"heading": "10.2 Proof of Theorem 3", "text": "Let us suppose that our set of rados U satisfies:\nU \u2286 \u03a3r \u2286 \u03a3m , (27)\nwhere \u03a3r is a fixed reference subset of \u03a3m. We shall use the shorthand EU [f(U)] to denote uniform i.i.d. sampling of U in \u03a3r. Furthermore, we also let for short\n` .\n= sup \u03b8\u2208\u0398 max \u03c0\u03c3\u2208\u03a3r\nexp(\u2212\u03b8>\u03c0\u03c3) . (28)\nThe proof relies on basic knowledge of VC theory and the \u201csymmetrization trick\u201d, which can be found e.g. in ([6]). Plugging eq. (28) into the proof of the symmetrization Lemma (Lemma 2 in ([6])) yields the following symmetrization Lemma for the exponential rado-loss. Notice that the assumption is the same as in Lemma 2 in ([6]).\nLemma 13 For any fixed sample S, for any t such that nt2 \u2265 2, the following holds over the Rademacher sampling of \u03c3 in \u03a3m:\nP [\nsup \u03b8\u2208\u0398\n(EU [ F rexp(S,\u03b8, U) ] \u2212 F rexp(S,\u03b8,U)) \u2265 t ] \u2264 2`2 \u00b7 P\n[ sup \u03b8\u2208\u0398 (F rexp(S,\u03b8,U)\u2212 F rexp(S,\u03b8,U\u2032)) \u2265 t 2 ] ,\nwhere U,U\u2032 are two size-n i.i.d. samples.\nConsider U,U\u2032 \u2286 \u03a3r, each of cardinal n and differing from one assignment only. Then it follows, for any \u03b8 \u2208 \u0398 and from ineq. (29):\n|F rexp(S,\u03b8,U)\u2212 F rexp(S,\u03b8,U\u2032)| \u2264 2`\nn . (29)\nApplying the independent bounded differences inequality ([20]), we get, for any \u03b8 \u2208 \u0398 and t > 0:\nP [ EU [ F rexp(S,\u03b8, U) ] \u2212 F rexp(S,\u03b8,U) \u2265 t\n4\n] \u2264 exp ( \u2212 nt 2\n16`2\n) . (30)\nLetting \u03a0(n) denote the growth function for linear separators computed over rados, we still have the upperbound\n\u03a0(n) \u2264 ( en\nd+ 1\n)d+1 . (31)\nWe thus get, for any \u03b8 \u2208 \u0398: P [\nsup \u03b8\u2208\u0398\n(EU [ F rexp(S,\u03b8, U) ] \u2212 F rexp(S,\u03b8,U)) \u2265 t ] \u2264 2`2 \u00b7 P\n[ sup \u03b8\u2208\u0398 (F rexp(S,\u03b8,U)\u2212 F rexp(S,\u03b8,U\u2032)) \u2265 t 2 ] (32)\n\u2264 2\u03a0(2n)`2 \u00b7 P [ F rexp(S,\u03b8,U)\u2212 F rexp(S,\u03b8,U\u2032) \u2265 t\n2\n] (33)\n\u2264 4\u03a0(2n)`2 \u00b7 P [ EU [ F rexp(S,\u03b8, U) ] \u2212 F rexp(S,\u03b8,U) \u2265 t\n4\n] (34)\n\u2264 4\u03a0(2n)`2 \u00b7 exp ( \u2212 nt 2\n16`2\n) (35)\n\u2264 4 ( 2en\nd+ 1\n)d+1 `2 \u00b7 exp ( \u2212 nt 2\n16`2\n) . (36)\nIneq. (32) follows from Lemma 13, ineq. (33) follows from standard VC arguments (see e.g. ([6]), Section 4), ineq. (34) follows from the observation that event a\u2212b \u2265 u implies (a\u2212c \u2265 u/2)\u2228(b\u2212c \u2265 u/2), ineq. (35) follows from (30), and finally ineq (36) follows from ineq. (31). Picking\nt = t\u2217 . = 16` \u00b7 \u221a 1\nn log `+\nd n log 2en d + 1 n log 1 \u03b7 (37)\nyields that the right hand-side of ineq. (36) is not more than \u03b7, for any \u03b7 > 0. So with probability \u2265 1\u2212\u03b7, any classifier \u03b8 \u2208 \u0398 will enjoy EU [ F rexp(S,\u03b8, U) ] \u2264 F rexp(S,\u03b8,U) + t\u2217, and so we shall have:\nF rlog(S,\u03b8,U)\n. = log(2) +\n1\nm \u00b7 logF rexp(S,\u03b8,U)\n\u2265 log(2) + 1 m \u00b7 log\n( EU [ F rexp(S,\u03b8, U) ] \u2212 t\u2217 ) = log(2) + 1\nm \u00b7 log\n( EU [ F rexp(S,\u03b8, U) ]) + 1\nm \u00b7 log\n( 1\u2212 16% \u00b7 \u221a 1\nn log `+\nd n log 2en d + 1 n log 1 \u03b7\n) (38)\n= log(2) + 1\nm \u00b7 logF rexp(S,\u03b8,\u03a3m) +\n1\nm \u00b7 log\nF rexp(S,\u03b8,\u03a3r) F rexp(S,\u03b8,\u03a3m)\n+ 1\nm \u00b7 log\n( 1\u2212 16% \u00b7 \u221a 1\nn log `+\nd n log 2en d + 1 n log 1 \u03b7\n)\n= Flog (S,\u03b8) + 1\nm \u00b7 log\nF rexp(S,\u03b8,\u03a3r) F rexp(S,\u03b8,\u03a3m)\n+ 1\nm \u00b7 log\n( 1\u2212 16% \u00b7 \u221a 1\nn log `+\nd n log 2en d + 1 n log 1 \u03b7\n) . (39)\nIn eq. (38), we use the fact that % = `/EU [ F rexp(S,\u03b8, U) ] and EU [ F rexp(S,\u03b8, U) ] = F rexp(S,\u03b8,\u03a3r). Hence, reordering the expression yields that with probability \u2265 1 \u2212 \u03b7, the final classifier \u03b8 will\nsatisfy:\nFlog (S,\u03b8) \u2264 F rlog(S,\u03b8,U)\u2212 1\nm \u00b7 log\nF rexp(S,\u03b8,\u03a3r) F rexp(S,\u03b8,\u03a3m)\n\u2212 log ( 1\u2212 16 \u00b7 %\u221a n \u00b7 \u221a log `+ d log 2en d + log 1 \u03b7 ) . (40)\nThere remains to use the fact that ` \u2264 exp(r\u03b8 max\u03a3r \u2016\u03c0\u03c3\u20162) to complete the proof of ineq. (5) in Theorem 3. To prove ineq. (8), let us call 1 \u2212 z the quantity inside the log in ineq. (40). We clearly have to have 0 \u2264 z < 1, and so for any value of z and for any 0 \u2264 \u03b1 < 1, there exists a value m\u2217 > 0 such that\nm1\u2212\u03b1 \u2265 1 z log 1 1\u2212 z (\u2265 0) , (41)\nfor any m \u2265 m\u2217. In this case, we get after reordering, since 1\u2212 z\u2032 \u2264 exp z\u2032,\n1\u2212 z m\u03b1 \u2264 exp ( \u2212 z m\u03b1 ) \u2264 exp ( 1\nm log(1\u2212 z)\n) , (42)\nand so, taking logs and using ineq. (39), we obtain that for any 0 \u2264 \u03b2 < 1/2, there exists m\u2217 > 0 such that for any m \u2265 m\u2217:\nFlog (S,\u03b8) \u2264 F rlog(S,\u03b8,U)\u2212 1\nm \u00b7 log\nF rexp(S,\u03b8,\u03a3r) F rexp(S,\u03b8,\u03a3m)\n\u2212 log ( 1\u2212 16 \u00b7 % m\u03b2 \u00b7 \u221a r\u03b8 n \u00b7max \u03a3r \u2225\u2225\u2225\u2225 1m \u00b7 \u03c0\u03c3 \u2225\u2225\u2225\u2225\n2\n+ d\nnm log\n2en\nd +\n1\nnm log\n1\n\u03b7\n) . (43)\nCalling 1\u2212 z\u2032 the quantity inside the log, there remains to use log(1\u2212 z\u2032) \u2265 \u2212Kz\u2032 for some K > 0 when z\u2032 is sufficiently close to 0 (hence, m sufficiently large again). This proves ineq. (8) and completes the proof of Theorem 3. Remark that provided n is sufficiently large, the right hand-side of ineq (41) admits the following equivalent:\n1 z log 1 1\u2212 z \u223c 1 + z 2 , (44)\nwith z = \u2126(1/ \u221a n) (omitting the dependences in the other parameters). Hence, ineq (41) can be ensured as long as m is large enough with respect to r\u03b8, max\u03a3r \u2016(1/m) \u00b7 \u03c0\u03c3\u20162 (which cannot exceed the maximum norm of an observation in S), d and log(1/\u03b7).\nSo, when we apply this last result to RadoBoost, it says that for a large enough sample, we can indeed pick an n sufficiently large but small compared to m so that we shall observe with high probability a decay rate of the expected logistic loss computed over S, E[Flog (S,\u03b8T )], of order \u2126(\u03b32/m) (expectation is measured with respect to the sampling of U).\nWe are now left with proving ineq. (7), and so we study:\n\u2212Q = 1 m \u00b7 log\n( 1 |\u03a3r| \u2211 \u03c3\u2032\u2208\u03a3r exp(\u2212\u03b8>\u03c0\u03c3\u2032)\n1 |\u03a3m| \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3)\n)\n= 1\nm \u00b7 log\n( |\u03a3m| \u2211 \u03c3\u2032\u2208\u03a3r exp(\u2212\u03b8>\u03c0\u03c3\u2032)\n|\u03a3r| \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3)\n)\n= 1\nm \u00b7 log\n(\u2211 \u03c3\u2208\u03a3m \u2211 \u03c3\u2032\u2208\u03a3r exp(\u2212\u03b8>\u03c0\u03c3\u2032)\u2211\n\u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3)\n)\n= 1\nm \u00b7 log\n(\u2211 \u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) \u00b7 exp(\u2212\u03b8>(\u03c0\u03c3\u2032 \u2212 \u03c0\u03c3))\u2211\n\u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) ) = 1\nm \u00b7 log\n( E(\u03c3,\u03c3\u2032)\u223cD [ exp(\u2212\u03b8>(\u03c0\u03c3\u2032 \u2212 \u03c0\u03c3)) ]) ,\nwith D(\u03c3,\u03c3\u2032) \u221d exp(\u2212\u03b8>\u03c0\u03c3). Jensen\u2019s inequality yields:\n\u2212Q \u2265 1 m \u00b7 E(\u03c3,\u03c3\u2032)\u223cD\n[ \u2212\u03b8>(\u03c0\u03c3\u2032 \u2212 \u03c0\u03c3) ] = 1\nm \u00b7 E(\u03c3,\u03c3\u2032)\u223cD\n[ \u03b8>\u03c0\u03c3 ] \u2212 1 m \u00b7 E(\u03c3,\u03c3\u2032)\u223cD [ \u03b8>\u03c0\u03c3\u2032 ] . (45)\nWe now remark that E(\u03c3,\u03c3\u2032)\u223cD [ \u03b8>\u03c0\u03c3\u2032 ] = \u2211 \u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) \u00b7 \u03b8>\u03c0\u03c3\u2032\u2211\n\u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3)\n= \u03b8> ((\u2211 \u03c3\u2032\u2208\u03a3r \u03c0\u03c3\u2032 ) \u00b7 (\u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) )\u2211\n\u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) ) = \u03b8>E\u03c3\u223c\u03a3r [\u03c0\u03c3] , (46)\nand furthermore\n1\nm \u00b7 E(\u03c3,\u03c3\u2032)\u223cD\n[ \u03b8>\u03c0\u03c3 ] = 1 m \u00b7 \u2211 \u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) \u00b7 \u03b8>\u03c0\u03c3\u2211 \u03c3\u2032\u2208\u03a3r \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3)\n= 1 m \u00b7 \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) \u00b7 \u03b8>\u03c0\u03c3\u2211\n\u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) = \u03b8> ( 1 m \u00b7 \u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) \u00b7 \u03c0\u03c3\u2211 \u03c3\u2208\u03a3m exp(\u2212\u03b8>\u03c0\u03c3) ) = \u03b8>\u2207\u03b8 1\nm \u00b7 logF rexp (S,\u03b8,\u03a3m)\n= \u03b8>\u2207\u03b8F rlog (S,\u03b8,\u03a3m) . (47)\nAssembling eqs (46) and (47), we get from ineq. (45): Q \u2264 r\u03b8 \u2225\u2225\u2225\u2225\u2207\u03b8F rlog (S,\u03b8,\u03a3m)\u2212 E\u03c3\u223c\u03a3r [ 1m \u00b7 \u03c0\u03c3 ]\u2225\u2225\u2225\u2225 2\n\u2264 r\u03b8 ( \u2016\u2207\u03b8F rlog (S,\u03b8,\u03a3m) \u20162 + \u2225\u2225\u2225\u2225E\u03c3\u223c\u03a3r [ 1m \u00b7 \u03c0\u03c3 ]\u2225\u2225\u2225\u2225\n2\n) ,\nas claimed."}, {"heading": "10.3 Proof of Lemma 4", "text": "Theorem 1 in ([22]) immediately yields\n1 n exp\n( \u2212\u03b8>T \u03c0j ) \u2264 T\u220f t=1 \u221a 1\u2212 r2t \u00b7 w(T+1)j ,\u2200j \u2208 [n] . (48)\nSince 1>wT+1 = 1, summing over j \u2208 [n] yields:\nF rexp(S,\u03b8T ,U) \u2264 T\u220f t=1 \u221a 1\u2212 r2t\n\u2264 exp ( \u22121\n2 \u2211 t r2t\n) .\nUsing the (WLA), this yields ineq. (12)."}, {"heading": "10.4 Proof of Lemma 5", "text": "Fix for short k = \u03b9(t). We rewrite rt(wt) as a function of the examples:\nrt(wt) = 1\n\u03c0\u2217k n\u2211 j=1 wtj\u03c0jk\n= 1\n\u03c0\u2217k n\u2211 j=1 \u2211 i:\u03c3ji=yi wtjyixik\n= 1\nx\u2217k m\u2211 i=1 x\u2217k \u03c0\u2217k \u00b7 \u2211\nj:\u03c3ji=yi\nwtj  yixik . (49) Define w\u0303 \u2208 Pm such that\nw\u0303i . = 1 W\u0303 \u00b7 x\u2217k \u03c0\u2217k \u00b7 \u2211\nj:\u03c3ji=yi\nwtj ,\u2200i \u2208 [m] , (50)\nwith\nW\u0303 . = x\u2217k \u03c0\u2217k \u00b7 m\u2211 i=1 \u2211 j:\u03c3ji=yi wtj\n= x\u2217k \u03c0\u2217k \u00b7 n\u2211 j=1 wtj |{i : \u03c3ji = yi}| (51)\nthe normalization coefficient. Because wt \u2208 Pn, x\u2217k > 0 and \u03c0\u2217k > 0, it comes that indeed w\u0303 \u2208 Pm, and W\u0303 > 0 (unless Sr is reduced to the null rado). We thus have |rt(wt)| \u2265 \u03b3 iff\n|rext (w\u0303)| \u2265 \u03b3\nW\u0303 . (52)\nThis proves the statement of the Lemma. Remark that\nx\u2217k \u03c0\u2217k \u2264 W\u0303 \u2264 x\u2217k(\n\u03c0\u2217k maxj |{i:\u03c3ji=yi}|\n) , (53)\nso if we assume the weak learning assumption holds for the examples, |rext (w\u0303)| \u2265 \u03b3ex > 0, then the weak learning assumption over rados always holds for\n\u03b3 = x\u2217k \u03c0\u2217k \u00b7 \u03b3ex , (54)\nand may holds for a value \u03b3 which can be as large as\n\u03b3 = x\u2217k( \u03c0\u2217k\nmaxj |{i:\u03c3ji=yi}| ) \u00b7 \u03b3ex . (55) These two bounds are data dependent (but they depend on data only), and whenever they are significant outlier values for feature k, i.e. x\u2217k is achieved by few examples and all others have feature value of significantly smaller order, then the available \u03b3 can be significantly larger than \u03b3ex. Compared to the cases where no such outliers would exist, we thus may expect significantly better results for RadoBoost."}, {"heading": "10.5 Proof of Theorem 6", "text": "To ease notations hereafter, we consider wlog that d = 1 and so j\u2217 = 1. We also drop index notation j\u2217 in related notations (so \u03a3 \u03b2,j\u2217 m becomes \u03a3 \u03b2 m).\nWe let S and S\u2032 denote two j-neighbors, so that S \u2248j S\u2032 holds and they differ by the value of one (boolean) feature. Algorithm DP-Feat selects uniformly at random the rados in sets\n\u03a3\u03b2m(S) . = {\u03c3 \u2208 \u03a3m : \u03c0\u03c3 \u2208 I(S)} , (56) \u03a3\u03b2m(S \u2032) .= { \u03c3 \u2208 \u03a3m : \u03c0\u03c3 \u2208 I(S\u2032) } , (57)\nwith\nI(S) .= {\u2212(m\u2212m(+)) + \u03b2(m+ 1) \u2264 z \u2264 m(+)\u2212 \u03b2(m+ 1)} , (58) I(S\u2032) .= {\u2212(m\u2212m(+)) + \u03b2(m+ 1) + \u03b6 \u2264 z \u2264 m(+)\u2212 \u03b2(m+ 1) + \u03b6} , (59)\nsince m\u2032(+) = m(+) + \u03b6 for some \u03b6 \u2208 {\u22121, 0, 1}. To relate the sizes of these two sets, we first compute the size of {\u03c3 : \u03c0\u03c3 = r|S}, for r \u2208 Z. Assuming first r \u2265 0, we have:\n|{\u03c3 : \u03c0\u03c3 = r|S}| = min{m(+)\u2212r,m\u2212m(+)}\u2211\ni=0\n( m(+)\ni+ r\n)( m\u2212m(+)\ni\n) . (60)\nIf r < 0, then similarly:\n|{\u03c3 : \u03c0\u03c3 = r|S}| = min{m(+),m\u2212m(+)+r}\u2211\ni=0\n( m(+)\ni )( m\u2212m(+) i\u2212 r ) , (61)\nwhich is the same expression as (60) with the substitutions r 7\u2192 \u2212r, m(+) 7\u2192 m\u2212m(+), m\u2212m(+) 7\u2192 m(+), so we have only to analyse the case r \u2265 0. If m(+)\u2212r > m\u2212m(+), we have by Vandermonde identity:\n|{\u03c3 : \u03c0\u03c3 = r|S}| = m\u2212m(+)\u2211 i=0 ( m(+) m(+)\u2212 i\u2212 r )( m\u2212m(+) i ) = ( m\nm(+)\u2212 r\n) . (62)\nIf m(+)\u2212 r \u2264 m\u2212m(+), then it is not hard to show that Vandermonde identity still brings (62). We thus have\n|\u03a3\u03b2m(S)| = m(+)\u2212\u03b2(m+1)\u2211\nr=\u2212(m\u2212m(+))+\u03b2(m+1)\n( m\nm(+)\u2212 r\n)\n=\n( m\n\u03b2(m+ 1)\n) + m(+)\u2212\u03b2(m+1)\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1)+1 ( m m(+)\u2212 r )\n= m\u2212 \u03b2(m+ 1) + 1 \u03b2(m+ 1) \u00b7 (\nm\n\u03b2(m+ 1)\u2212 1\n) + m(+)\u2212\u03b2(m+1)\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1)+1 ( m m(+)\u2212 r )\n=\n( 1 \u03b2 \u2212 1 ) \u00b7 (\nm\n\u03b2(m+ 1)\u2212 1\n) + m(+)\u2212\u03b2(m+1)\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1)+1 ( m m(+)\u2212 r )\n\u2265 (\nm\n\u03b2(m+ 1)\u2212 1\n) + m(+)\u2212\u03b2(m+1)\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1)+1 ( m m(+)\u2212 r )\n= m(+)\u2212\u03b2(m+1)+1\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1)+1 ( m m(+)\u2212 r )\n= m(+)\u2212\u03b2(m+1)+1\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1)+1 m(+) + 1\u2212 r m\u2212m(+) + r \u00b7 ( m (m(+) + 1)\u2212 r ) (63)\n\u2265 m(+)\u2212\u03b2(m+1)+1\u2211\nr=\u2212(m\u2212m(+))+\u03b2(m+1)+1\n\u03b2(m+ 1) m\u2212 \u03b2(m+ 1) + 1 \u00b7 (\nm\n(m(+) + 1)\u2212 r\n) (64)\n=\n( 1 \u03b2 \u2212 1 )\u22121 m(+)\u2212\u03b2(m+1)+1\u2211\nr=\u2212(m\u2212m(+))+\u03b2(m+1)+1\n( m\n(m(+) + 1)\u2212 r\n) (65)\n=\n( 1 \u03b2 \u2212 1 )\u22121 \u00b7 |\u03a3\u03b2m(S\u2032)| (66)\nif \u03b6 = 1, and\n|\u03a3\u03b2m(S)| = m(+)\u2212\u03b2(m+1)\u2211\nr=\u2212(m\u2212m(+))+\u03b2(m+1)\n( m\nm(+)\u2212 r\n)\n=\n( m\n\u03b2(m+ 1)\n) + m(+)\u2212\u03b2(m+1)\u22121\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1) ( m m(+)\u2212 r )\n= m\u2212 \u03b2(m+ 1) + 1 \u03b2(m+ 1) \u00b7 (\nm\n\u03b2(m+ 1)\u2212 1\n) + m(+)\u2212\u03b2(m+1)\u22121\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1) ( m m(+)\u2212 r )\n=\n( 1 \u03b2 \u2212 1 ) \u00b7 (\nm\n\u03b2(m+ 1)\u2212 1\n) + m(+)\u2212\u03b2(m+1)\u22121\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1) ( m m(+)\u2212 r )\n\u2265 (\nm\n\u03b2(m+ 1)\u2212 1\n) + m(+)\u2212\u03b2(m+1)\u22121\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1) ( m m(+)\u2212 r )\n= m(+)\u2212\u03b2(m+1)\u22121\u2211 r=\u2212(m\u2212m(+))+\u03b2(m+1)\u22121 ( m m(+)\u2212 r )\n\u2265 ( 1 \u03b2 \u2212 1 )\u22121 \u00b7 |\u03a3\u03b2m(S\u2032)| (67)\nif \u03b6 = \u22121. The last inequality follows from the same chain of inequalities as in eqs. (63 \u2013 66). We now bound the ratio of probabilities for the rado being equal to r, for both sets:\nP \u03c3\u223c\u03a3\u03b2m(S) [\u03c0\u03c3 = r|S]\nP \u03c3\u223c\u03a3\u03b2m(S\u2032) [\u03c0\u03c3 = r|S \u2032] = |\u03a3\u03b2m(S\u2032)| |\u03a3\u03b2m(S)|\n\u00b7 ( m m(+)\u2212r )( m\nm(+)+\u03b6\u2212r )\n\u2264 ( 1 \u03b2 \u2212 1 ) \u00b7 ( m m(+)\u2212r )( m m(+)+\u03b6\u2212r )\n=\n( 1 \u03b2 \u2212 1 ) \u00b7 (m(+) + \u03b6 \u2212 r)!(m\u2212m(+)\u2212 \u03b6 + r)! (m(+)\u2212 r)!(m\u2212m(+) + r)! (68)\n=\n( 1 \u03b2 \u2212 1 ) \u00b7  m(+)+1\u2212r m\u2212m(+)+r if \u03b6 = 1 1 if \u03b6 = 0 m\u2212m(+)+1+r\nm(+)\u2212r if \u03b6 = \u22121 \u2264 ( 1 \u03b2 \u2212 1 )2 . (69)\nThe last inequality comes from eq. (58) which guarantees r \u2265 \u2212(m\u2212m(+)) + \u03b2(m+ 1), and so m(+) + 1\u2212 r m\u2212m(+) + r \u2264 1 \u03b2 \u2212 1 , (70)\nand furthermore eq. (58) also guarantees r \u2264 m(+)\u2212 \u03b2(m+ 1), and so m\u2212m(+) + 1 + r\nm(+)\u2212 r \u2264 1 \u03b2 \u2212 1 (71)\nas well. We finally get from ineq. (69):\nP \u03c3\u223c\u03a3\u03b2m(S) [\u03c0\u03c3 = r|S]\nP \u03c3\u223c\u03a3\u03b2m(S\u2032) [\u03c0\u03c3 = r|S\n\u2032] \u2264 exp( ) , (72)\nwhich holds for any r \u2208 \u03a3\u03b2m(S) \u2229 \u03a3\u03b2m(S\u2032). Notice however that the symmetric difference of these two sets is not empty. To finish the proof, we need to take into account this symmetric difference. This is the data-dependent step in DP-Feat which may leak information about one feature and disclose its content, through the use of eq. (58). To see this, if we assume that one possesses all the data but the unknown feature value for one person, and knows how rados are computed using DP-Feat, then by observing the output \u03c0\u03c3,j\u2217 , he may guess the unknown value, as depicted by Figure 3. Let us denote A this event. When returning one rado from \u03a3\u03b2m(.), if we consider without loss of generality a uniform distribution over examples, then, referring to the notations of Figure 3, we have:\nP[A] = P[A|S]P[S] + P[A|S\u2032]P[S\u2032] (73) < P[A|S] + P[A|S\u2032] . (74)\nIf A occurs in S, then it is for r = m(+)\u2212 (m\u2212 \u03b2(m+ 1)) in Figure 3. We get from eq. (62): P[A|S] = ( m m\u2212\u03b2(m+1) ) \u2211m\u2212\u03b2(m+1)\nr=\u03b2(m+1) ( m r ) = ( m \u03b2(m+1) ) \u2211m\u2212\u03b2(m+1)\nr=\u03b2(m+1) ( m r ) , (75) and we obtain following the same reasoning, using the fact that m(+) increases by one in S\u2032,\nP[A|S\u2032] = ( m \u03b2(m+1) ) \u2211m\u2212\u03b2(m+1)\nr=\u03b2(m+1) ( m r ) . (76) The probability of hitting the symmetric difference of \u03a3\u03b2m(S) \u2229 \u03a3\u03b2m(S\u2032) is taken into account considering \u03b4 = P[A] in the ( , \u03b4)-differentially private release of one rado. We get:\n\u03b4 < 2 ( m \u03b2(m+1) ) \u2211m\u2212\u03b2(m+1)\nr=\u03b2(m+1) ( m r ) . (77) The interplay between and \u03b4 can be appreciated throughout the use of the following properties:\n\u03b2(m+1)\u22121\u2211 r=0 ( m r ) \u2264 2m\u00b7H(u) , (78)(\nm\nm/2\n) <\n1\u221a m \u00b7 2m , (79)\nwe have used\nH(z) . = \u2212z log2 z \u2212 (1\u2212 z) log2(1\u2212 z) ,\nu . = \u03b2 \u2212 1\u2212 \u03b2 m .\nWe get\n\u03b4 < 2\u221a m \u00b7 1 1\u2212 2m\u00b7(H(u)\u22121) (80)\nBecause H(u) is concave, it satisfies (fixing \u2032 .= /2 for short):\nH(u) \u2264 H(\u03b2) + (u\u2212 \u03b2)H \u2032(\u03b2)\n= H(\u03b2)\u2212 1\u2212 \u03b2 m log2 1\u2212 \u03b2 \u03b2\n= H(\u03b2)\u2212 (1\u2212 \u03b2) \u2032\nm\n= 1 log 2 \u00b7 ( log(1 + exp( \u2032))\u2212 ( 1 + 1 m ) \u00b7 \u2032 exp \u2032 1 + exp \u2032 ) . = f( \u2032) . (81)\nWe have:\n1 1\u2212 2m\u00b7(f( \u2032)\u22121) \u223c0 1 2m2 log2(2) \u2032 +\n( 1\n2 \u2212 1 4m2 log3(2)\n) +O( \u2032) . (82)\nSo, assuming \u2032 = o(1), there exists m\u2032 > 0 and a constant K > 0 such that for any m > m\u2032,\n\u03b4 < K \u00b7 1 m 5 2 . (83)\nFinally, we get that when = \u2126(1/m), ( , \u03b4)-differential privacy can be ensured on the delivery of n = 1 rado as long as \u00b7 \u03b4 = O(m\u22125/2). Taking into account the fact that rados are generated independently and using Theorem 3.16 in [12] concludes the proof of Theorem 6 for arbitrary n.\nTo finish the proof, we remark that \u03a3\u03b2m(.) 6= \u2205. Indeed, since m \u2265 1, \u03b2 < m/(m + 1); furthermore, as long as m > 2, provided we also have\n1 + 2\u03b2 1\u2212 2\u03b2 = O(m) ,\nwe shall have I(S) \u2229 Z 6= \u2205. This can easily be ensured if\n1\n+ = O(m) , (84)\ni.e., provided = o(1), = \u2126(1/m)."}, {"heading": "10.6 Proof of Theorem 7", "text": "We keep the same notations as in the proof of Theorem 6. The Rademacher rejection sampling of \u03c3 has a probability to reject a single rado bounded by (a fraction of) the tail of the Binomial, as\nindeed\nP\u03c3\u223c\u03a3m [\u03c3 6\u2208 \u03a3\u03b2m|S] = 1\n2m \u00b7 \u2211 r<\u2212(m\u2212mk(+))+\u03b2(m+1)\u2228r>mk(+)\u2212\u03b2(m+1) ( m m(+)\u2212 r )\n= 1 2m \u00b7 \u2212(m\u2212mk(+))+\u03b2(m+1)\u22121\u2211\nr=\u2212(m\u2212mk(+))\n( m\nm(+)\u2212 r\n) + 1\n2m \u00b7 m(+)\u2211 r=mk(+)\u2212\u03b2(m+1)+1 ( m m(+)\u2212 r )\n= 1\n2m \u00b7 m\u2211 r=m\u2212\u03b2(m+1)+1 ( m r ) + 1 2m \u00b7 \u03b2(m+1)\u22121\u2211 r=0 ( m r )\n= 2 \u00b7 1 2m \u00b7 m\u2211 r=m\u2212\u03b2(m+1)+1 ( m r )\n= 2 \u00b7 1 2m \u00b7 m\u2211 r=(1\u2212\u03b2)(m+1) m+ 1\u2212 r m+ 1 \u00b7 ( m+ 1 r )\n\u2264 2\u03b2 \u00b7 1 2m \u00b7 m\u2211 r=(1\u2212\u03b2)(m+1) ( m+ 1 r )\n\u2264 2\u03b2 \u00b7 1 2m \u00b7 m+1\u2211 r=(1\u2212\u03b2)(m+1) ( m+ 1 r )\n= 4\u03b2 \u00b7 m+1\u2211\nr=(1\u2212\u03b2)(m+1)\n( m+ 1\nr\n) \u00b7 ( 1\n2\n)m+1\u2212r \u00b7 ( 1\n2 )r \u2264 4\u03b2 exp (\u2212(m+ 1) \u00b7DBE(1\u2212 \u03b2\u20161/2)) , (85)27\nwhere DBE is the bit-entropy divergence ([3]):\nDBE(p\u2016q) = p log p q + (1\u2212 p) log 1\u2212 p 1\u2212 q . (86)\nThe last equation follows e.g. from Theorem 2 in ([1]). So the probability p that there exists a rado, among the n generated, that was rejected at least Tr times for some Tr \u2265 1 satisfies\np \u2264 4n\u03b2 \u221e\u2211 t=Tr exp (\u2212(m+ 1) \u00b7 t \u00b7DBE(1\u2212 \u03b2\u20161/2))\n= 4n\u03b2 \u00b7 exp (\u2212(m+ 1) \u00b7 Tr \u00b7DBE(1\u2212 \u03b2\u20161/2)) \u00b7 \u221e\u2211 t=0 exp (\u2212(m+ 1) \u00b7 t \u00b7DBE(1\u2212 \u03b2\u20161/2))(87)\nWe now use the facts that (i) m \u2265 (1 + 2\u03b2)/(1 \u2212 2\u03b2) (Step 2 in Algorithm DP-Feat), and (ii) function\nf(z) . = 2\n1\u2212 2z \u00b7 (log(2) + (1\u2212 z) log(1\u2212 z) + z log z) (88)\nis convex over [0, 1/2) and has limit tangent 1\u2212 2z in z = 1/2, so\nexp (\u2212(m+ 1) \u00b7DBE(1\u2212 \u03b2\u20161/2)) \u2264 exp ( \u2212 2 1\u2212 2\u03b2 \u00b7 (log(2) + (1\u2212 \u03b2) log(1\u2212 \u03b2) + \u03b2 log \u03b2) )\n\u2264 exp(2\u03b2 \u2212 1) (< 1) ,\nand it comes\n\u221e\u2211 t=0 exp (\u2212(m+ 1) \u00b7 t \u00b7DBE(1\u2212 \u03b2\u20161/2)) \u2264 1 1\u2212 exp(2\u03b2 \u2212 1) , (89)\nand so\np \u2264 4n\u03b2 1\u2212 exp(2\u03b2 \u2212 1) \u00b7 exp (\u2212(m+ 1) \u00b7 Tr \u00b7DBE(1\u2212 \u03b2\u20161/2)) (90)\nSo, if n, \u03b2,\u03b7 are such that\nn \u2264 \u03b7(1\u2212 exp(2\u03b2 \u2212 1)) 4\u03b2 , (91)\nthen there is probability \u2265 1 \u2212 \u03b7 that no rado was rejected. Otherwise, with probability \u2265 1 \u2212 \u03b7, each rado among the n was rejected no more than\nT \u2217r = \u2308\n1\nmDBE(1\u2212 \u03b2\u20161/2) log\n4\u03b2n\n\u03b7(1\u2212 exp(2\u03b2 \u2212 1))\n\u2309 (92)\ntimes. There remains to multiply this bound by the number of rados to get an upperbound on the number of iterations of Rademacher rejection sampling, and we obtain eq. (17). This finishes the proof of Theorem 7.\nRemarks: the actual dependence of eq. (92) on \u03b2 is such that unless is extremely close to 01, in which case the requirement on differential privacy is the strongest, T \u2217r does not actually blow up. To see this, let us define\nf(\u03b2) . = 1\nDBE(1\u2212 \u03b2\u20161/2) log\n4\u03b2\n1\u2212 exp(2\u03b2 \u2212 1) . (93)\nFigure 4 displays f(\u03b2) over different ranges. One sees that when = 0.1, provided m/ log n is in the order of thousands and n e, then T \u2217r is in fact of the order log(1/\u03b7), which may be quite small indeed."}, {"heading": "10.7 Proof of Theorem 8", "text": "Let us first remark that the DP-protection of vector edges by computing noisified example set\nS+ . = {(x+i , yi) . = (xi + x r i , yi), i \u2208 [m]} , (94)\nwhere xri \u223c N(0, \u03c22I), is equivalent to noisifying edges because label y \u2208 {\u22121, 1} and the pdf of the Gaussian mechanism is invariant by multiplication by y.\nThe key quantity to prove the Theorem is, for any noisified rado \u03c0+j . = (1/2) \u00b7\u2211i (\u03c3ji + yi)x+i , the support mj . = |{i : \u03c3ji = yi}| of the rado. We also renormalize the leveraging coefficient in RadoBoost, replacing eq. (10) in RadoBoost pseudocode by:\n\u03b1t \u2190 1\n2\u03ba\u03c0\u2217\u03b9(t) log 1 + rt 1\u2212 rt , (95)\nfor some fixed \u03ba \u2265 1.\n1Recall that \u03b2 = 1/(1 + exp( /2)) in Step 1 of Algorithm DP-Feat.\nWe now embark in the proof of Theorem 8. Lemma 2 in ([22]) yields\nexp ( \u2212\u03b8>T \u03c0j ) = exp ( \u2212\u03b8>T \u03c0+j ) \u00b7 exp\n( 1\n2 \u00b7 \u03b8>T \u2211 i (\u03c3ji + yi)x r i\n)\n\u2264 (\nT\u220f t=1 \u221a 1\u2212 r2t \u00b7 nw(T+1)j\n) 1 \u03ba \u00b7 exp ( 1\n2 \u00b7 \u03b8>T \u2211 i (\u03c3ji + yi)x r i\n) ,\u2200j \u2208 [n] .(96)\nAveraging over j \u2208 [n] yields:\nF rexp(S,\u03b8T ,U) \u2264 ( T\u220f t=1 \u221a 1\u2212 r2t ) 1 \u03ba \u00b7 n\u2211 j=1 n 1 \u03ba \u22121w 1 \u03ba (T+1)j \u00b7 exp ( 1 2 \u00b7 \u03b8>T \u2211 i (\u03c3ji + yi)x r i )\n\u2264 exp ( \u2212 1\n2\u03ba \u2211 t r2t ) \ufe38 \ufe37\ufe37 \ufe38\nA\n\u00b7 n\u2211 j=1\nw\u0303(T+1)j \u00b7 exp ( 1\n2 \u00b7 \u03b8>T \u2211 i (\u03c3ji + yi)x r i ) \ufe38 \ufe37\ufe37 \ufe38\nB\n, (97)\nwith w\u0303(T+1)j . = n 1 \u03ba \u22121w\n1 \u03ba\n(T+1)j . The right-hand side of ineq. (97) multiplies two separate quantities, A which quantifies the performances of \u03b8T in RadoBoost on the set of noisy rados on which it was trained, and B which is an expectation, computed over wT , of the agreements between \u03b8T and the noisy part of the rados. When rados are noise-free and \u03ba \u2265 1, we have xri = 0, \u2200i and\nn\u2211 j=1 w\u0303(T+1)j = n 1 \u03ba \u00b7 1 n n\u2211 j=1 w 1 \u03ba (T+1)j\n\u2264 n 1\u03ba \u00b7  1 n n\u2211 j=1 w(T+1)j  1\u03ba = n 1 \u03ba \u00b7 n\u2212 1k = 1 (98)\nbecause of the concavity of x1/\u03ba, and so we return to the noise-free rado boosting bound with \u201cpenalty 1/\u03ba\u201d for renormalizing the leveraging coefficients in RadoBoost (this proves ineq. (17)). Assuming \u03b8T output by RadoBoost, we obtain, \u2200S,U such that support of all n rados is of the same size, i.e. mj = m\u2217,\u2200j \u2208 [n],\nF rlog(S,\u03b8T ,U)\n= log(2) + 1\nm logF rexp(S,\u03b8T ,U)\n\u2264 log(2)\u2212 1 2\u03bam \u2211 t r2t + 1 m \u00b7 log n\u2211 j=1\nw\u0303(T+1)j \u00b7 exp ( 1\n2 \u00b7 \u03b8>T \u2211 i (\u03c3ji + yi)x r i\n)\n\u2264 log(2)\u2212 1 2\u03bam \u2211 t r2t + m\u2217 m \u00b7 log n\u2211 j=1\nw\u0303(T+1)j \u00b7 exp ( 1\n2m\u2217 \u00b7 \u03b8>T \u2211 i (\u03c3ji + yi)x r i\n)\n= log(2)\u2212 1 2\u03bam \u2211 t\nr2t\ufe38 \ufe37\ufe37 \ufe38 . =C\n+ m\u2217 m \u00b7 log n\u2211 j=1\nw\u0303(T+1)j \u00b7 exp (\n\u03c2\u221a m\u2217 \u00b7 \u03b8>T \u2211 i \u03c3ji + yi 2\u03c2 \u221a m\u2217 xri ) \ufe38 \ufe37\ufe37 \ufe38\n. =D\n. (99)\nWe now study a sufficient condition for C \u2212 D to be \u2126((1/m)\u2211t r2t ) with high probability over the noise mechanism, thereby ensuring a convergence rate over non-noisy rados that shall comply with the noise-free bounds of ineq. (13), up to the hidden factors. This shall be achieved through several Lemmata.\nLemma 14 With probability \u2265 1\u2212 \u03c4 over the noise mechanism we shall have:\u2225\u2225\u2225\u2225\u2225\u2211 i \u03c3ji + yi 2\u03c2 \u221a m\u2217 xri \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u221a 2 log (n \u03c4 ) , \u2200j \u2208 [n] . (100)\nProof The Sudakov-Tsirelson inequality ([5], Theorem 5.6) states that if x \u223c N(0, Id) and f(x) : Rd \u2192 R is L-Lipschitz, then\nP [f(x)\u2212 E[f(x)] \u2265 t] \u2264 exp ( \u2212 t 2\n2L2\n) . (101)\nSince function f(x) . = \u2016x\u20162 is 1-Lipschitz by the triangle inequality and \u2211\ni \u03c3ji+yi 2\u03c2 \u221a m\u2217 xri is a standard\nGaussian random because the xri are sampled independently, ineq. (101) yields that we shall have simultaneously over the randomized part of the rados, with probability \u2265 1\u2212 \u03c4,\u2225\u2225\u2225\u2225\u2225\u2211\ni\n\u03c3ji + yi 2\u03c2 \u221a m\u2217 xri \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u221a 2 log (n \u03c4 ) ,\u2200j \u2208 [n],\nwhich proves the Lemma.\nLemma 15 Assume \u03b8T \u2208 B(0, r\u03b8) for some r\u03b8 > 0. Then with probability \u2265 1\u2212 \u03c4 over the noise mechanism we shall have\nD \u2264 \u03c2r\u03b8 m\n\u221a 2m\u2217 log (n \u03c4 ) . (102)\nProof We use Lemma 14. Cauchy-Schwartz inequality implies\n\u03b8>T \u2211 i \u03c3ji + yi 2\u03c2 \u221a m\u2217 xri \u2264 \u2016\u03b8T \u20162 \u00b7 \u2225\u2225\u2225\u2225\u2225\u2211 i \u03c3ji + yi 2\u03c2 \u221a m\u2217 xri \u2225\u2225\u2225\u2225\u2225 2\n\u2264 r\u03b8 \u221a 2 log (n \u03c4 ) ,\u2200j \u2208 [n] . (103)\nWe thus get in this case\nD \u2264 \u03c2r\u03b8 m\n\u221a 2m\u2217 log (n \u03c4 ) + m\u2217 m \u00b7 log n\u2211 j=1 w\u0303(T+1)j\n\u2264 \u03c2r\u03b8 m\n\u221a 2m\u2217 log (n \u03c4 ) . (104)\nbecause of ineq. (98).\nWe now prove a specific r\u03b8 > 0 which makes use of the concentration of the randomized part of rados in Lemma 14.\nLemma 16 Suppose there exists \u00b5 > \u00b5\u2032 > 0 such that it simultaneously holds:\n\u00b5 \u2264 mink maxj |\u03c0jk| m\u2217 , (105)\n\u00b5\u2032 \u2264 \u00b5\u2212 \u03c2 \u221a 1 m\u2217 log (n \u03c4 ) , (106)\nwhere \u03c0jk = (1/2) \u2211 i(\u03c3ji + yi)xik is the non-noisy part of rado \u03c0 + j . Assume the existence of \u03c1 > 0 such that the weak learner wfi in RadoBoost is \u03bbp-prudential for\n\u03bbp = 1\u2212 2\u221a\n1\u2212 \u03c1\u03ba\u00b5\u2032m\u2217 . (107)\nThen probability \u2265 1\u2212 \u03c4 over the noise mechanism we shall have \u2016\u03b8T \u20162 \u2264 (1\u2212 \u03c1) \u2211 t r2t . (108)\nRemarks: notice that ineq. (105) is equivalent to saying that each coordinate k has at east one non-zero entry in the noise-free part of the rados. Unless coordinate k is zero for all examples \u2014 in which case we can just discard this feature \u2014, this assumption is easy to satisfy. Proof We have\n\u2016\u03b8T \u20162 \u2212 \u2211 t r2t = \u2211 t\n1\n4\u03ba2\u03c02\u2217\u03b9(t) log2 1 + rt 1\u2212 rt \u2212 r2t . (109)\nAssuming the existence of z > 0 such that 2\u03ba\u03c0\u2217\u03b9(t) \u2265 z,\u2200t, and using the fact that\nlog2 1 + x 1\u2212 x \u2264 4x2 (1\u2212 |x|)2 , \u2200x \u2208 (0, 1) , (110)\nwe shall have \u2211 t\n1\n4\u03ba2\u03c02\u2217\u03b9(t) log2 1 + rt 1\u2212 rt \u2212 r2t \u2264 \u2211 t 1 z2 log2 1 + rt 1\u2212 rt \u2212 r2t\n\u2264 \u2211 t 1 z2 \u00b7 4r 2 t (1\u2212 |rt|)2 \u2212 r2t\n= \u2211 t r2t ( 4\u2212 z2(1\u2212 |rt|)2 z2(1\u2212 |rt|)2 ) \u2264 \u2212\u03c1\n\u2211 t r2t , (111)\nas long as\n|rt| \u2264 1\u2212 2\u221a\n1\u2212 \u03c1z , \u2200t , (112)\nwhere \u03c1 \u2208 (0, 1). Since \u03c0\u2217\u03b9(t) \u2265 mink maxj |\u03c0+jk|, we can fix z\u2217 = 2\u03bamink maxk |\u03c0+jk|, but recall that \u03c0+jk sums a random Gaussian part and a non random part. Ineq. (100) tells us that with high probability, the magnitude of the random part will satisfy\u2211\ni\n(\u03c3ji + yi)x r i \u2264 \u03c2 \u221a 2m\u2217 log (n \u03c4 ) ,\u2200j \u2208 [n] . (113)\nThus, we shall have in this case, using ineqs. (105, 106) and given Lemma 14:\nmin k\nmax |\u03c0+jk| \u2265 ( \u00b5\u2212 \u03c2 \u221a 1 m\u2217 log (n \u03c4 )) \u00b7m\u2217\n\u2265 \u00b5\u2032m\u2217 ,\nand we get the statement of the Lemma.\nWe now return to ineq. (99), and use Lemmata 14, 15 and 16, and obtain that with probability \u2265 1\u2212 \u03c4, a sufficiently prudential weak learner shall imply:\nF rlog(S,\u03b8T ,U)\n\u2264 log(2)\u2212 1 2\u03bam \u2211 t r2t + m\u2217 m \u00b7 log n\u2211 j=1\nw\u0303(T+1)j \u00b7 exp (\n\u03c2\u221a m\u2217 \u00b7 \u03b8>T \u2211 i \u03c3ji + yi 2\u03c2 \u221a m\u2217 xri\n)\n\u2264 log(2)\u2212 1 m \u00b7 ( 1 2\u03ba \u2212 (1\u2212 \u03c1) \u00b7 \u03c2 \u221a 2m\u2217 log (n \u03c4 )) \ufe38 \ufe37\ufe37 \ufe38\n. =E\n\u2211 t r2t . (114)\nWe want E \u2265 1/(4\u03ba). Equivalently, we want\n1\u2212 \u03c1 \u2264 1 4\u03ba\u03c2 \u221a 2m\u2217 log ( n \u03c4 ) , (115) and for the prudential weak learner to exist, we also need\n1\u2212 \u03c1 > 4 \u03ba2\u00b5\u20322m2\u2217 . (116)\nAssuming ineqs (105) and (106), we thus get that if\n\u03ba \u2265 4\u03c2 \u00b5\u20322m 3 2\u2217\n\u221a 2 log (n \u03c4 ) , (117)\nthen there exists a prudential weak learner for which, with probability \u2265 1 \u2212 \u03c4 over the noise mechanism, we shall have after T rounds of boosting of RadoBoost, using the prudential weak learner and renormalizing the leveraging coefficients by \u03ba as in (95),\nF rlog(S,\u03b8T ,U) \u2264 log(2)\u2212 1\n4\u03bam \u2211 t r2t , (118)\nwhich proves Theorem 8. Notice that the constraint \u03ba \u2265 1 can easily be enforced by picking \u00b5\u2032 sufficiently small.\nRemarks: we finish by emphasizing the fact that ineq. (19) is computed over non-noisy rados. It is not hard to see that ineqs (105) and (106) shall be all the easier to meet as m\u2217 is large compared to log n, log(1/\u03c4) and \u03c2. So, provided rados have a sufficiently large support, the convergence rate of the logistic rado-risk of RadoBoost over the non noisy rados may compete, up to a small constant factor, with the one that would be achieved by training RadoBoost over non-noisy rados."}, {"heading": "10.8 Proof of Lemma 10", "text": "Consider first that m \u2265 2d. A simple proof of the Lemma consists in considering the largest d-dim square, of edge length ` = 2R/ \u221a d, shown with thick dashed line in Figure 5. We then pack this square with m+1 spheres, as shown. Since the edge length is covered by dlog(m)/ log(d)e diameters of these spheres, we obtain that the radius r of each such sphere satisfies:\nr = 2R\u221a\nd \u00b7 d log(m+1)log d e\n\u2265 R log d 2 \u221a d log(m+ 1) , (119)\nbecause m \u2265 2d > d. Because of the construction, at least one of these spheres does not contain an edge vector from C(E) and is thus empty. Consider one such empty sphere whose center e\u2217 is the closest to 0, as shown in Figure 5, and consider one adjacent sphere, located no farther2, with one\n2If no such sphere exists, we can pick e\u2217 = 0, the center of a sphere B(0, r) which contains no example from S. In this case, there is no need to remove any example from S: the proof still holds by adding example (0, y) to S, to\nedge vector e = yx from C(E) inside, with (x, y) \u2208 S, where S generates \u03a0. We create S\u2032 out of S by replacing (x, y) by two examples, (ye\u2217, y) and (e\u2212 ye\u2217, y). It is worthwhile remarking that\nC(E\u2032) \u2282 B(0, R) (120)\nby construction, and furthermore any rado that can be created from S can also be created from S\u2032. Hence, any \u03a0 defined over S can also be obtained from S\u2032. There remains to remark that, by construction, e\u2217 is distant from every edge vector of S from at least r, and so:\nDH(E,E \u2032) = \u2126 ( R log d\u221a d logm ) ; (121)\nthis proves Lemma 10 when m \u2265 2d. When m < 2d, the construction of Figure 5 can still be done but with larger balls, for which\nr = R\n2 \u221a d . (122)\nPicking as e\u2217 the center of any of these empty balls, we obtain\nDH(E,E \u2032) \u2265 R\n2 \u221a d , (123)\nas claimed."}, {"heading": "10.9 Proof of Lemma 11", "text": "We make a reduction from the X3C3 ([25]) problem whose instance is a set S . = {s1, s2, ..., sn} and a set of 3-subsets of S, C . = {c1, c2, ..., cd}, and an integer m. Each element of S belongs to exactly three subsets of C. The question is whether there exists a cover of S using at most m elements from C. The reduction is the following:\n\u2022 to each feature corresponds an element of C;\n\u2022 to each element sj of S we associate a boolean rado \u03c0j which is 1 in coordinate k iff sj \u2208 ck, and zero otherwise:\n\u03c0j = 1{k:sj\u2208ck} . (124)\n(1I is \u201c1\u201d in coordinate ik for k \u2208 I, and zero everywhere else)\n\u2022 The number of examples is m;\n\u2022 Parameters r and ` are fixed as follows:\n\u2013 if p 6= 0, the value of r is 21/p. We also fix ` = -machine, where -machine is the smallest such that 1\u2212 < 1 in machine encoding;\n\u2013 else if p = 0, then r = 2 and ` = 1;\ncreate S\u2032.\nLet us number the constraints of Sparse-Approximation, so that we want:\n\u2016xi\u2016p \u2264 ` , \u2200i \u2208 [m] , (Sparse examples) (125) \u2016\u03c0j \u2212 \u03c0\u03c3j\u2016p \u2264 r , \u2200j \u2208 [n] . (Rado approximation) (126)\nSuppose there exists a solution to X3C3 with m subsets of C, C\u2217 .= {c\u2217k1 , c\u2217k2 , ..., c\u2217km}. Create m positive examples (yi = 1) whose observation is xi . = 1{ki} (the all-0 vector with only one \u201c1\u201d in coordinate ki). Clearly, the sparsity constraint on examples (125) is satisfied. We craft the rados following n Rademacher assignations, where \u03c3i is +1 only for xki , and \u22121 otherwise. Notice that\n\u03c0j \u2212 \u03c0\u03c3j = 1{k:sj\u2208ck} \u2212 1{ki,sj\u2208c\u2217ki} (127) = 1{k:sj\u2208ck\u2227ck 6\u2208C\u2217} . (128)\nIt comes\n\u2016\u03c0j \u2212 \u03c0\u03c3j\u2016p \u2264 21/p . = r , \u2200j \u2208 [n] , (129)\nif p 6= 0, and\n\u2016\u03c0j \u2212 \u03c0\u03c3j\u20160 \u2264 2 . = r , \u2200j \u2208 [n] (130)\notherwise, since each element of S belongs to three sets in C. Therefore, there exists a solution to Sparse-Approximation.\nNow, suppose there exists a solution to Sparse-Approximation. Remark that we can remove wlog any example having null observation as this does not change the feasibility of the solution. Consider the case where p 6= 0. The Rado approximation constraint (126) of Sparse-Approximation makes that the following property (P) is satisfied:\n(P) for each j \u2208 [n], there exists i \u2208 [m] and feature k \u2208 [d] such that \u03c0\u03c3j and example xi have their coordinate k non-zero, and furthermore the coordinate in xi has magnitude exactly : it cannot be less otherwise (126) is violated, and it cannot be more otherwise (125) is violated. Hence, each of these xi have exactly one non-zero coordinate.\nBecause property (P) holds for all rados, we see that the corresponding indexes in the xi (the corresponding non-zero coordinates for features for which (P) holds; there cannot be more than m) define a solution to X3C3. The case p = 0 is easier as (125) enforces the number of non-zero coordinates in each observation to be at most one, and therefore exactly one since there is no null observation.\nWe finally note that Sparse-Approximation trivially belongs to NP, so it is actually NP-Complete."}, {"heading": "10.10 Proof of Lemma 12", "text": "We make the same reduction as for Sparse-Approximation. The set of examples S consists of all canonical basis vectors, associated to positive class."}, {"heading": "11 Appendix \u2014 Experiments", "text": ""}, {"heading": "11.1 Supplementary experiments to Table 1", "text": "Table 3 is obtained under the same experimental setting as that of Table 1, with an important modification in how the normalized edge is computed. More specifically, the computation of rt in\nStep 2.2 of RadoBoost (see (9)) is completed by the following step:\nrt \u2190 sign(rt) \u00b7max{0.1, |rt|} (131)\nThe same modification is also carried out in AdaBoost ([27]) (Corollary 1). This aims to prevent the fact that domains with outlier feature values could trick AdaBoost in picking the wrong sign for \u03b1t for a large number of iterations, due to values of rt with a very small magnitude (but with the wrong sign). Experiments display that this corrects AdaBoost\u2019s bad results on Twitter, but on other domains like Fertility, Haberman, Sonar, Abalone, the change happens to give worse results for AdaBoost and/or AdaBoost(n). RadoBoost\u2019s results, on the other hand, tend to improve with sparse exceptions."}, {"heading": "11.2 Supplementary experiments to Section 5 \u2014 I / III", "text": "Tables 4, 5, 6, 7 present results comparing AdaBoost, RadoBoost with random rados and RadoBoost with fixed support size rados (m\u2217). Unless otherwise stated in Tables, the following experimental setup holds:\n\u2022 RadoBoost is trained with n = min{1000, train fold size/2} rados;\n\u2022 AdaBoost is trained using the complete training fold;\n\u2022 for each standard deviation \u03c3, we generate 10 noisy domains; each is then processed following 10 folds stratified cross-validation. Thus, each dot on the colored curves is the average of ten experiments;\n\u2022 RadoBoost is trained with two types of rados: random rados as in Section 4 \u2014 this gives the grey dashed curves \u2014, or rados with fixed support m\u2217 (noted s on the plots) as in Subsection 5.2 \u2014 this gives the colored curves \u2014;"}, {"heading": "11.3 Supplementary experiments to Section 5 \u2014 II / III", "text": "Tables 8 and 9 compare RadoBoost trained with rados of fixed support and using a \u201cprudential\u201d weak learner (which picks the median feature according to |rt|), to RadoBoost trained with plain random rados and using the \u201cstrongest\u201d possible weak learner which picks the best feature according to |rt|."}, {"heading": "11.4 Supplementary experiments to Section 5 \u2014 III / III", "text": "Tables 10 and 11 compare two different rado generation mechanisms with respect to RadoBoost: the random generation of arbitrary rados (Section 4), and the random generation of rados with fixed support (Subsection 5.2). In both Tables, the weak learner is always the same (contrary to Tables 8 and 9), i.e. the \u201cstrong\u201d weak learner that picks the best feature according to |rt|, at each iteration."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>The minimization of the logistic loss is a popular approach to batch supervised learning. Our<lb>paper starts from the surprising observation that, when fitting linear (or kernelized) classifiers,<lb>the minimization of the logistic loss is equivalent to the minimization of an exponential rado-loss<lb>computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over<lb>the same classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be<lb>directly used to classify observations. We provide a learning algorithm over rados with boosting-<lb>compliant convergence rates on the logistic loss (computed over examples). Experiments on<lb>domains with up to millions of examples, backed up by theoretical arguments, display that<lb>learning over a small set of random rados can challenge the state of the art that learns over<lb>the complete set of examples. We show that rados comply with various privacy requirements<lb>that make them good candidates for machine learning in a privacy framework. We give several<lb>algebraic, geometric and computational hardness results on reconstructing examples from rados.<lb>We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy<lb>framework. Tests reveal that learning from differentially private rados can compete with learning<lb>from random rados, and hence with batch learning from examples, achieving non-trivial privacy<lb>vs accuracy tradeoffs.", "creator": "LaTeX with hyperref package"}}}