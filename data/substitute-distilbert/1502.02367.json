{"id": "1502.02367", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Gated Feedback Recurrent Neural Networks", "abstract": "in this work, we propose creating parallel recurrent neural network ( rnn ) i. 1 second rnn, gated - stabilized rnn ( gf - rnn ), extends the existing approach of stacking multiple recurrent layers by allowing and controlling behaviors flowing from larger recurrent layers to successive layers using coordinated global gating unit for each pair of layers. the recurrent signals exchanged between layers are adaptive adaptively based on the smallest hidden state accompanying the current input. we evaluated the proposed gf - rnn with different types of recurrent units, available as tanh, long short - term memory and slow recurrent units, on the tasks of character - level language modeling and python program evaluation. our empirical evaluation of different modular units, revealed that in both tasks, merging gf - rnn outperforms the conventional interfaces and build deep stacked rnns. we suggest that the limitation arises because the gf - rnn can adaptively assign different layers to different timescales and layer - to - layer interactions ( including the top - down ones which are too usually present in a stacked rnn ) by learning to moderate boundary interactions.", "histories": [["v1", "Mon, 9 Feb 2015 05:25:54 GMT  (1592kb,D)", "https://arxiv.org/abs/1502.02367v1", null], ["v2", "Thu, 12 Feb 2015 19:18:07 GMT  (1592kb,D)", "http://arxiv.org/abs/1502.02367v2", "11 pages, corrected typos"], ["v3", "Wed, 18 Feb 2015 11:34:38 GMT  (1592kb,D)", "http://arxiv.org/abs/1502.02367v3", "11 pages, corrected typos"], ["v4", "Wed, 17 Jun 2015 06:26:21 GMT  (2181kb,D)", "http://arxiv.org/abs/1502.02367v4", "9 pages, removed appendix"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["junyoung chung", "\u00e7aglar g\u00fcl\u00e7ehre", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1502.02367"}, "pdf": {"name": "1502.02367.pdf", "metadata": {"source": "META", "title": "Gated Feedback Recurrent Neural Networks", "authors": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["JUNYOUNG.CHUNG@UMONTREAL.CA", "CAGLAR.GULCEHRE@UMONTREAL.CA", "KYUNGHYUN.CHO@UMONTREAL.CA", "FIND-ME@THE.WEB"], "sections": [{"heading": "1. Introduction", "text": "Recurrent neural networks (RNNs) have been widely studied and used for various machine learning tasks which involve sequence modeling, especially when the input and output have variable lengths. Recent studies have revealed that RNNs using gating units can achieve promising results in both classification and generation tasks (see, e.g., Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).\nAlthough RNNs can theoretically capture any long-term dependency in an input sequence, it is well-known to be difficult to train an RNN to actually do so (Hochreiter,\n1991; Bengio et al., 1994; Hochreiter, 1998). One of the most successful and promising approaches to solve this issue is by modifying the RNN architecture e.g., by using a gated activation function, instead of the usual state-to-state transition function composing an affine transformation and a point-wise nonlinearity. A gated activation function, such as the long short-term memory (LSTM, Hochreiter & Schmidhuber, 1997) and the gated recurrent unit (GRU, Cho et al., 2014), is designed to have more persistent memory so that it can capture long-term dependencies more easily.\nSequences modeled by an RNN can contain both fast changing and slow changing components, and these underlying components are often structured in a hierarchical manner, which, as first pointed out by El Hihi & Bengio (1995) can help to extend the ability of the RNN to learn to model longer-term dependencies. A conventional way to encode this hierarchy in an RNN has been to stack multiple levels of recurrent layers (Schmidhuber, 1992; El Hihi & Bengio, 1995; Graves, 2013; Hermans & Schrauwen, 2013). More recently, Koutn\u0131\u0301k et al. (2014) proposed a more explicit approach to partition the hidden units in an RNN into groups such that each group receives the signal from the input and the other groups at a separate, predefined rate, which allows feedback information between these partitions to be propagated at multiple timescales. Stollenga et al. (2014) recently showed the importance of feedback information across multiple levels of feature hierarchy, however, with feedforward neural networks.\nIn this paper, we propose a novel design for RNNs, called a gated-feedback RNN (GF-RNN), to deal with the issue of learning multiple adaptive timescales. The proposed RNN has multiple levels of recurrent layers like stacked RNNs do. However, it uses gated-feedback connections from upper recurrent layers to the lower ones. This makes the hidden states across a pair of consecutive timesteps fully connected. To encourage each recurrent layer to work at different timescales, the proposed GF-RNN controls the strength of the temporal (recurrent) connection adaptively. This ef-\nar X\niv :1\n50 2.\n02 36\n7v 4\n[ cs\n.N E\n] 1\n7 Ju\nn 20\n15\nfectively lets the model to adapt its structure based on the input sequence.\nWe empirically evaluated the proposed model against the conventional stacked RNN and the usual, single-layer RNN on the task of language modeling and Python program evaluation (Zaremba & Sutskever, 2014). Our experiments reveal that the proposed model significantly outperforms the conventional approaches on two different datasets."}, {"heading": "2. Recurrent Neural Network", "text": "An RNN is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden states for each symbol of the input sequence. The activation of the hidden states at timestep t is computed as a function f of the current input symbol xt and the previous hidden states ht\u22121:\nht =f (xt,ht\u22121) .\nIt is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht\u22121:\nht =\u03c6 (Wxt + Uht\u22121) , (1)\nwhere W is the input-to-hidden weight matrix, U is the state-to-state recurrent weight matrix, and \u03c6 is usually a logistic sigmoid function or a hyperbolic tangent function.\nWe can factorize the probability of a sequence of arbitrary length into\np(x1, \u00b7 \u00b7 \u00b7 , xT ) = p(x1)p(x2 | x1) \u00b7 \u00b7 \u00b7 p(xT | x1, \u00b7 \u00b7 \u00b7 , xT\u22121).\nThen, we can train an RNN to model this distribution by letting it predict the probability of the next symbol xt+1 given hidden states ht which is a function of all the previous symbols x1, \u00b7 \u00b7 \u00b7 , xt\u22121 and current symbol xt:\np(xt+1 | x1, \u00b7 \u00b7 \u00b7 , xt) = g (ht) .\nThis approach of using a neural network to model a probability distribution over sequences is widely used, for instance, in language modeling (see, e.g., Bengio et al., 2001; Mikolov, 2012)."}, {"heading": "2.1. Gated Recurrent Neural Network", "text": "The difficulty of training an RNN to capture long-term dependencies has been known for long (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998). A previously successful approaches to this fundamental challenge has been to modify the state-to-state transition function to encourage some hidden units to adaptively maintain long-term memory, creating paths in the time-unfolded RNN, such that gradients can flow over many timesteps.\nLong short-term memory (LSTM) was proposed by Hochreiter & Schmidhuber (1997) to specifically address this issue of learning long-term dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. More recently, Cho et al. (2014) proposed a gated recurrent unit (GRU) which adaptively remembers and forgets its state based on the input signal to the unit. Both of these units are central to our proposed model, and we will describe them in more details in the remainder of this section."}, {"heading": "2.1.1. LONG SHORT-TERM MEMORY", "text": "Since the initial 1997 proposal, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014). Here we follow the implementation provided by Zaremba et al. (2014).\nSuch an LSTM unit consists of a memory cell ct, an input gate it, a forget gate ft, and an output gate ot. The memory cell carries the memory content of an LSTM unit, while the gates control the amount of changes to and exposure of the memory content. The content of the memory cell cjt of the j-th LSTM unit at timestep t is updated similar to the form of a gated leaky neuron, i.e., as the weighted sum of the new content c\u0303jt and the previous memory content cjt\u22121 modulated by the input and forget gates, i j t and f j t , respectively:\ncjt = f j t c j t\u22121 + i j t c\u0303 j t , (2)\nwhere\nc\u0303t = tanh (Wcxt + Ucht\u22121) . (3)\nThe input and forget gates control how much new content should be memorized and how much old content should be forgotten, respectively. These gates are computed from the previous hidden states and the current input:\nit =\u03c3 (Wixt + Uiht\u22121) , (4) ft =\u03c3 (Wfxt + Ufht\u22121) , (5)\nwhere it = [ ikt ]p k=1 and ft = [ fkt ]p k=1\nare respectively the vectors of the input and forget gates in a recurrent layer composed of p LSTM units. \u03c3(\u00b7) is an element-wise logistic sigmoid function. xt and ht\u22121 are the input vector and previous hidden states of the LSTM units, respectively.\nOnce the memory content of the LSTM unit is updated, the hidden state hjt of the j-th LSTM unit is computed as:\nhjt = o j t tanh ( cjt ) .\nThe output gate ojt controls to which degree the memory content is exposed. Similarly to the other gates, the output gate also depends on the current input and the previous\nhidden states such that\not = \u03c3 (Woxt + Uoht\u22121) . (6)\nIn other words, these gates and the memory cell allow an LSTM unit to adaptively forget, memorize and expose the memory content. If the detected feature, i.e., the memory content, is deemed important, the forget gate will be closed and carry the memory content across many timesteps, which is equivalent to capturing a long-term dependency. On the other hand, the unit may decide to reset the memory content by opening the forget gate. Since these two modes of operations can happen simultaneously across different LSTM units, an RNN with multiple LSTM units may capture both fast-moving and slow-moving components."}, {"heading": "2.1.2. GATED RECURRENT UNIT", "text": "The GRU was recently proposed by Cho et al. (2014). Like the LSTM, it was designed to adaptively reset or update its memory content. Each GRU thus has a reset gate rjt and an update gate zjt which are reminiscent of the forget and input gates of the LSTM. However, unlike the LSTM, the GRU fully exposes its memory content each timestep and balances between the previous memory content and the new memory content strictly using leaky integration, albeit with its adaptive time constant controlled by update gate zjt .\nAt timestep t, the state hjt of the j-th GRU is computed by\nhjt = (1\u2212 z j t )h j t\u22121 + z j t h\u0303 j t , (7)\nwhere hjt\u22121 and h\u0303 j t respectively correspond to the previous memory content and the new candidate memory content. The update gate zjt controls how much of the previous memory content is to be forgotten and how much of the new memory content is to be added. The update gate is computed based on the previous hidden states ht\u22121 and the current input xt:\nzt =\u03c3 (Wzxt + Uzht\u22121) , (8)\nThe new memory content h\u0303jt is computed similarly to the conventional transition function in Eq. (1):\nh\u0303t = tanh (Wxt + rt Uht\u22121) , (9)\nwhere is an element-wise multiplication.\nOne major difference from the traditional transition function (Eq. (1)) is that the states of the previous step ht\u22121 is modulated by the reset gates rt. This behavior allows a GRU to ignore the previous hidden states whenever it is deemed necessary considering the previous hidden states and the current input:\nrt =\u03c3 (Wrxt + Urht\u22121) . (10)\nThe update mechanism helps the GRU to capture longterm dependencies. Whenever a previously detected feature, or the memory content is considered to be important for later use, the update gate will be closed to carry the current memory content across multiple timesteps. The reset mechanism helps the GRU to use the model capacity efficiently by allowing it to reset whenever the detected feature is not necessary anymore."}, {"heading": "3. Gated Feedback Recurrent Neural Network", "text": "Although capturing long-term dependencies in a sequence is an important and difficult goal of RNNs, it is worthwhile to notice that a sequence often consists of both slowmoving and fast-moving components, of which only the former corresponds to long-term dependencies. Ideally, an RNN needs to capture both long-term and short-term dependencies.\nEl Hihi & Bengio (1995) first showed that an RNN can capture these dependencies of different timescales more easily and efficiently when the hidden units of the RNN is explicitly partitioned into groups that correspond to different timescales. The clockwork RNN (CW-RNN) (Koutn\u0131\u0301k et al., 2014) implemented this by allowing the i-th module to operate at the rate of 2i\u22121, where i is a positive integer, meaning that the module is updated only when t mod 2i\u22121 = 0. This makes each module to operate at different rates. In addition, they precisely defined the connectivity pattern between modules by allowing the i-th module to be affected by j-th module when j > i.\nHere, we propose to generalize the CW-RNN by allowing the model to adaptively adjust the connectivity pattern between the hidden layers in the consecutive timesteps. Similar to the CW-RNN, we partition the hidden units into multiple modules in which each module corresponds to a different layer in a stack of recurrent layers.\nUnlike the CW-RNN, however, we do not set an explicit rate for each module. Instead, we let each module operate at different timescales by hierarchically stacking them. Each module is fully connected to all the other modules across the stack and itself. In other words, we do not define the connectivity pattern across a pair of consecutive timesteps. This is contrary to the design of CW-RNN and the conventional stacked RNN. The recurrent connection between two modules, instead, is gated by a logistic unit ([0, 1]) which is computed based on the current input and the previous states of the hidden layers. We call this gating unit a global reset gate, as opposed to a unit-wise reset gate which applies only to a single unit (See Eqs. (2) and (9)).\nThe global reset gate is computed as:\ngi\u2192j = \u03c3 ( wi\u2192jg h j\u22121 t + u i\u2192j g h \u2217 t\u22121 ) ,\nwhere h\u2217t\u22121 is the concatenation of all the hidden states from the previous timestep t \u2212 1. The superscript i\u2192j is an index of associated set of parameters for the transition from layer i in timestep t\u22121 to layer j in timestep t. wi\u2192jg and ui\u2192jg are respectively the weight vectors for the current input and the previous hidden states. When j = 1, hj\u22121t is xt.\nIn other words, the signal from hit\u22121 to h j t is controlled by a single scalar gi\u2192j which depends on the input xt and all the previous hidden states h\u2217t\u22121.\nWe call this RNN with a fully-connected recurrent transitions and global reset gates, a gated-feedback RNN (GFRNN). Fig. 1 illustrates the difference between the conventional stacked RNN and our proposed GF-RNN. In both models, information flows from lower recurrent layers to upper recurrent layers. The GF-RNN, however, further allows information from the upper recurrent layer, corresponding to coarser timescale, flows back into the lower recurrent layers, corresponding to finer timescales.\nIn the remainder of this section, we describe how to use the previously described LSTM unit, GRU, and more traditional tanh unit in the GF-RNN."}, {"heading": "3.1. Practical Implementation of GF-RNN", "text": "tanh Unit. For a stacked tanh-RNN, the signal from the previous timestep is gated. The hidden state of the j-th\nlayer is computed by\nhjt =tanh ( W j\u22121\u2192jhj\u22121t + L\u2211 i=1 gi\u2192jU i\u2192jhit\u22121 ) ,\nwhere L is the number of hidden layers, W j\u22121\u2192j and U i\u2192j are the weight matrices of the current input and the previous hidden states of the i-th module, respectively. Compared to Eq. (1), the only difference is that the previous hidden states are from multiple layers and controlled by the global reset gates.\nLong Short-Term Memory and Gated Recurrent Unit. In the cases of LSTM and GRU, we do not use the global reset gates when computing the unit-wise gates. In other words, Eqs. (4)\u2013(6) for LSTM, and Eqs. (8) and (10) for GRU are not modified. We only use the global reset gates when computing the new state (see Eq. (3) for LSTM, and Eq. (9) for GRU).\nThe new memory content of an LSTM at the j-th layer is computed by\nc\u0303jt = tanh ( W j\u22121\u2192jc h j\u22121 t + L\u2211 i=1 gi\u2192jU i\u2192jc h i t\u22121 ) .\nIn the case of a GRU, similarly,\nh\u0303jt = tanh ( W j\u22121\u2192jhj\u22121t + r j t L\u2211 i=1 gi\u2192jU i\u2192jhit\u22121 ) ."}, {"heading": "4. Experiment Settings", "text": ""}, {"heading": "4.1. Tasks", "text": "We evaluated the proposed GF-RNN on character-level language modeling and Python program evaluation. Both\ntasks are representative examples of discrete sequence modeling, where a model is trained to minimize the negative log-likelihood of training sequences:\nmin \u03b8\n1\nN N\u2211 n=1 Tn\u2211 t=1 \u2212 log p ( xnt | xn1 , . . . , xnt\u22121;\u03b8 ) ,\nwhere \u03b8 is a set of model parameters."}, {"heading": "4.1.1. LANGUAGE MODELING", "text": "We used the dataset made available as a part of the human knowledge compression contest (Hutter, 2012). We refer to this dataset as the Hutter dataset. The dataset, which was built from English Wikipedia, contains 100 MBytes of characters which include Latin alphabets, non-Latin alphabets, XML markups and special characters. Closely following the protocols in (Mikolov et al., 2012; Graves, 2013), we used the first 90 MBytes of characters to train a model, the next 5 MBytes as a validation set, and the remaining as a test set, with the vocabulary of 205 characters including a token for an unknown character. We used the average number of bits-per-character (BPC,E[\u2212 log2 P (xt+1|ht)]) to measure the performance of each model on the Hutter dataset."}, {"heading": "4.1.2. PYTHON PROGRAM EVALUATION", "text": "Zaremba & Sutskever (2014) recently showed that an RNN, more specifically a stacked LSTM, is able to execute a short Python script. Here, we compared the proposed architecture against the conventional stacking approach model on this task, to which refer as Python program evaluation.\nScripts used in this task include addition, multiplication, subtraction, for-loop, variable assignment, logical comparison and if-else statement. The goal is to generate, or predict, a correct return value of a given Python script. The input is a program while the output is the result of a print statement: every input script ends with a print statement. Both the input script and the output are sequences of characters, where the input and output vocabularies respectively consist of 41 and 13 symbols.\nThe advantage of evaluating the models with this task is that we can artificially control the difficulty of each sample (input-output pair). The difficulty is determined by the number of nesting levels in the input sequence and the length of the target sequence. We can do a finer-grained analysis of each model by observing its behavior on examples of different difficulty levels.\nIn Python program evaluation, we closely follow (Zaremba & Sutskever, 2014) and compute the test accuracy as the next step symbol prediction given a sequence of correct preceding symbols."}, {"heading": "4.2. Models", "text": "We compared three different RNN architectures: a singlelayer RNN, a stacked RNN and the proposed GF-RNN. For each architecture, we evaluated three different transition functions: tanh + affine, long short-term memory (LSTM) and gated recurrent unit (GRU). For fair comparison, we constrained the number of parameters of each model to be roughly similar to each other.\nFor each task, in addition to these capacity-controlled experiments, we conducted a few extra experiments to further test and better understand the properties of the GF-RNN."}, {"heading": "4.2.1. LANGUAGE MODELING", "text": "For the task of character-level language modeling, we constrained the number of parameters of each model to correspond to that of a single-layer RNN with 1000 tanh units (see Table 1 for more details). Each model is trained for at most 100 epochs.\nWe used RMSProp (Hinton, 2012) and momentum to tune the model parameters (Graves, 2013). According to the preliminary experiments and their results on the validation set, we used a learning rate of 0.001 and momentum coefficient of 0.9 when training the models having either GRU or LSTM units. It was necessary to choose a much smaller learning rate of 5\u00d710\u22125 in the case of tanh units to ensure the stability of learning. Whenever the norm of the gradient explodes, we halve the learning rate.\nEach update is done using a minibatch of 100 subsequences of length 100 each, to avoid memory overflow problems when unfolding in time for backprop. We approximate full back-propagation by carrying the hidden states computed at the previous update to initialize the hidden units in the next update. After every 100-th update, the hidden states\nwere reset to all zeros."}, {"heading": "4.2.2. PYTHON PROGRAM EVALUATION", "text": "For the task of Python program evaluation, we used an RNN encoder-decoder based approach to learn the mapping from Python scripts to the corresponding outputs as done by Cho et al. (2014); Sutskever et al. (2014) for machine translation. When training the models, Python scripts are fed into the encoder RNN, and the hidden state of the encoder RNN is unfolded for 50 timesteps. Prediction is performed by the decoder RNN whose initial hidden state is initialized with the last hidden state of the encoder RNN. The first hidden state of encoder RNN h0 is always initialized to a zero vector.\nFor this task, we used GRU and LSTM units either with or without the gated-feedback connections. Each encoder or decoder RNN has three hidden layers. For GRU, each hidden layer contains 230 units, and for LSTM each hidden layer contains 200 units.\nFollowing Zaremba & Sutskever (2014), we used the mixed\ncurriculum strategy for training each model, where each training example has a random difficulty sampled uniformly. We generated 320, 000 examples using the script provided by Zaremba & Sutskever (2014), with the nesting randomly sampled from [1, 5] and the target length from[ 1, 1010 ] .\nWe used Adam (Kingma & Ba, 2014) to train our models, and each update was using a minibatch with 128 sequences. We used a learning rate of 0.001 and \u03b21 and \u03b22 were both set to 0.99. We trained each model for 30 epochs, with early stopping based on the validation set performance to prevent over-fitting.\nAt test time, we evaluated each model on multiple sets of test examples where each set is generated using a fixed target length and number of nesting levels. Each test set contains 2, 000 examples which are ensured not to overlap with the training set."}, {"heading": "5. Results and Analysis", "text": ""}, {"heading": "5.1. Language Modeling", "text": "It is clear from Table 2 that the proposed gated-feedback architecture outperforms the other baseline architectures that we have tried when used together with widely used gated units such as LSTM and GRU. However, the proposed architecture failed to improve the performance of a vanillaRNN with tanh units. In addition to the final modeling performance, in Fig. 2, we plotted the learning curves of some models against wall-clock time (measured in seconds). RNNs that are trained with the proposed gatedfeedback architecture tends to make much faster progress over time. This behavior is observed both when the number of parameters is constrained and when the number of hid-\nden units is constrained. This suggests that the proposed GF-RNN significantly facilitates optimization/learning.\nEffect of Global Reset Gates\nAfter observing the superiority of the proposed gatedfeedback architecture over the single-layer or conventional stacked ones, we further trained another GF-RNN with LSTM units, but this time, after fixing the global reset gates to 1 to validate the need for the global reset gates. Without the global reset gates, feedback signals from the upper recurrent layers influence the lower recurrent layer fully without any control. The test set BPC of GF-LSTM without global reset gates was 1.854 which is in between the results of conventional stacked LSTM and GF-LSTM with global reset gates (see the last row of Table 2) which confirms the importance of adaptively gating the feedback connections.\nQualitative Analysis: Text Generation\nHere we qualitatively evaluate the stacked LSTM and GFLSTM trained earlier by generating text. We choose a subsequence of characters from the test set and use it as an initial seed. Once the model finishes reading the seed text, we let the model generate the following characters by sampling a symbol from softmax probabilities of a timestep and then provide the symbol as next input.\nGiven two seed snippets selected randomly from the test set, we generated the sequence of characters ten times for each model (stacked LSTM and GF-LSTM). We show one of those ten generated samples per model and per seed snippet in Table 3. We observe that the stacked LSTM failed to close the tags with </username> and </contributor> in both trials. However, the GF-LSTM succeeded to close\nboth of them, which shows that it learned about the structure of XML tags. This type of behavior could be seen throughout all ten random generations.\nLarge GF-RNN\nWe trained a larger GF-RNN that has five recurrent layers, each of which has 700 LSTM units. This makes it possible for us to compare the performance of the proposed architecture against the previously reported results using other types of RNNs. In Table 4, we present the test set BPC by a multiplicative RNN (Sutskever et al., 2011), a stacked LSTM (Graves, 2013) and the GF-RNN with LSTM units. The performance of the proposed GF-RNN is comparable to, or better than, the previously reported best results. Note that Sutskever et al. (2011) used the vocabulary of 86 characters (removed XML tags and the Wikipedia markups), and their result is not directly comparable with ours. In this experiment, we used Adam instead of RMSProp to optimize the RNN. We used learning rate of 0.001 and \u03b21 and \u03b22 were set to 0.9 and 0.99, respectively."}, {"heading": "5.2. Python Program Evaluation", "text": "Fig. 3 presents the test results of each model represented in heatmaps. The accuracy tends to decrease by the growth\nof the length of target sequences or the number of nesting levels, where the difficulty or complexity of the Python program increases. We observed that in most of the test sets, GF-RNNs are outperforming stacked RNNs, regardless of the type of units. Fig. 3 (c) represents the gaps between the test accuracies of stacked RNNs and GF-RNNs which are computed by subtracting (a) from (b). In Fig. 3 (c), the red and yellow colors, indicating large gains, are concentrated on top or right regions (either the number of nesting levels or the length of target sequences increases). From this we can more easily see that the GF-RNN outperforms the stacked RNN, especially as the number of nesting levels grows or the length of target sequences increases."}, {"heading": "6. Conclusion", "text": "We proposed a novel architecture for deep stacked RNNs which uses gated-feedback connections between different layers. Our experiments focused on challenging sequence modeling tasks of character-level language modeling and Python program evaluation. The results were consistent over different datasets, and clearly demonstrated that gated-feedback architecture is helpful when the models are trained on complicated sequences that involve longterm dependencies. We also showed that gated-feedback architecture was faster in wall-clock time over the training and achieved better performance compared to standard\nstacked RNN with a same amount of capacity. Large GFLSTM was able to outperform the previously reported best results on character-level language modeling. This suggests that GF-RNNs are also scalable. GF-RNNs were able to outperform standard stacked RNNs and the best previous records on Python program evaluation task with varying difficulties.\nWe noticed a deterioration in performance when the proposed gated-feedback architecture was used together with a tanh activation function, unlike when it was used with more sophisticated gated activation functions. More thorough investigation into the interaction between the gatedfeedback connections and the role of recurrent activation function is required in the future."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013). Also, the authors thank Yann N. Dauphin and Laurent Dinh for insightful comments and discussion. We acknowledge the support of the following agencies for research funding and computing support: NSERC, Samsung, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "Technical report, arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal"], "venue": "In Adv. Neural Inf. Proc. Sys", "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["El Hihi", "Salah", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hihi et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hihi et al\\.", "year": 1995}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred A"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Michiel", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Neural networks for machine learning", "author": ["Hinton", "Geoffrey"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton and Geoffrey.,? \\Q2012\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma thesis, Institut fu\u0308r Informatik, Lehrstuhl Prof. Brauer, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Hochreiter", "Sepp"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1998}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The human knowledge compression contest", "author": ["Hutter", "Marcus"], "venue": null, "citeRegEx": "Hutter and Marcus.,? \\Q2012\\E", "shortCiteRegEx": "Hutter and Marcus.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Koutn\u0131\u0301k", "Jan", "Greff", "Klaus", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Koutn\u0131\u0301k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutn\u0131\u0301k et al\\.", "year": 2014}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov", "Tomas"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov and Tomas.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tomas.", "year": 2012}, {"title": "Subword language modeling with neural networks", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "HaiSon", "Kombrink", "Stefan", "J. Cernocky"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Stollenga", "Marijn F", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Stollenga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recent studies have revealed that RNNs using gating units can achieve promising results in both classification and generation tasks (see, e.g., Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 132, "endOffset": 204}, {"referenceID": 22, "context": "Recent studies have revealed that RNNs using gating units can achieve promising results in both classification and generation tasks (see, e.g., Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 132, "endOffset": 204}, {"referenceID": 2, "context": "Although RNNs can theoretically capture any long-term dependency in an input sequence, it is well-known to be difficult to train an RNN to actually do so (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 154, "endOffset": 211}, {"referenceID": 16, "context": "More recently, Koutn\u0131\u0301k et al. (2014) proposed a more explicit approach to partition the hidden units in an RNN into groups such that each group receives the signal from the input and the other groups at a separate, predefined rate, which allows feedback information between these partitions to be propagated at multiple timescales.", "startOffset": 15, "endOffset": 38}, {"referenceID": 16, "context": "More recently, Koutn\u0131\u0301k et al. (2014) proposed a more explicit approach to partition the hidden units in an RNN into groups such that each group receives the signal from the input and the other groups at a separate, predefined rate, which allows feedback information between these partitions to be propagated at multiple timescales. Stollenga et al. (2014) recently showed the importance of feedback information across multiple levels of feature hierarchy, however, with feedforward neural networks.", "startOffset": 15, "endOffset": 357}, {"referenceID": 2, "context": "The difficulty of training an RNN to capture long-term dependencies has been known for long (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 92, "endOffset": 149}, {"referenceID": 2, "context": "The difficulty of training an RNN to capture long-term dependencies has been known for long (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998). A previously successful approaches to this fundamental challenge has been to modify the state-to-state transition function to encourage some hidden units to adaptively maintain long-term memory, creating paths in the time-unfolded RNN, such that gradients can flow over many timesteps. Long short-term memory (LSTM) was proposed by Hochreiter & Schmidhuber (1997) to specifically address this issue of learning long-term dependencies.", "startOffset": 111, "endOffset": 515}, {"referenceID": 2, "context": "The difficulty of training an RNN to capture long-term dependencies has been known for long (Hochreiter, 1991; Bengio et al., 1994; Hochreiter, 1998). A previously successful approaches to this fundamental challenge has been to modify the state-to-state transition function to encourage some hidden units to adaptively maintain long-term memory, creating paths in the time-unfolded RNN, such that gradients can flow over many timesteps. Long short-term memory (LSTM) was proposed by Hochreiter & Schmidhuber (1997) to specifically address this issue of learning long-term dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. More recently, Cho et al. (2014) proposed a gated recurrent unit (GRU) which adaptively remembers and forgets its state based on the input signal to the unit.", "startOffset": 111, "endOffset": 736}, {"referenceID": 6, "context": "Since the initial 1997 proposal, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014).", "startOffset": 83, "endOffset": 124}, {"referenceID": 23, "context": "Since the initial 1997 proposal, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014).", "startOffset": 83, "endOffset": 124}, {"referenceID": 6, "context": "Since the initial 1997 proposal, several variants of the LSTM have been introduced (Gers et al., 2000; Zaremba et al., 2014). Here we follow the implementation provided by Zaremba et al. (2014).", "startOffset": 84, "endOffset": 194}, {"referenceID": 4, "context": "The GRU was recently proposed by Cho et al. (2014). Like the LSTM, it was designed to adaptively reset or update its memory content.", "startOffset": 33, "endOffset": 51}, {"referenceID": 16, "context": "The clockwork RNN (CW-RNN) (Koutn\u0131\u0301k et al., 2014) implemented this by allowing the i-th module to operate at the rate of 2i\u22121, where i is a positive integer, meaning that the module is updated only when t mod 2i\u22121 = 0.", "startOffset": 27, "endOffset": 50}, {"referenceID": 18, "context": "Closely following the protocols in (Mikolov et al., 2012; Graves, 2013), we used the first 90 MBytes of characters to train a model, the next 5 MBytes as a validation set, and the remaining as a test set, with the vocabulary of 205 characters including a token for an unknown character.", "startOffset": 35, "endOffset": 71}, {"referenceID": 4, "context": "For the task of Python program evaluation, we used an RNN encoder-decoder based approach to learn the mapping from Python scripts to the corresponding outputs as done by Cho et al. (2014); Sutskever et al.", "startOffset": 170, "endOffset": 188}, {"referenceID": 4, "context": "For the task of Python program evaluation, we used an RNN encoder-decoder based approach to learn the mapping from Python scripts to the corresponding outputs as done by Cho et al. (2014); Sutskever et al. (2014) for machine translation.", "startOffset": 170, "endOffset": 213}, {"referenceID": 21, "context": "Test set BPC of neural language models trained on the Hutter dataset, MRNN = multiplicative RNN results from Sutskever et al. (2011) and Stacked LSTM results from Graves (2013).", "startOffset": 109, "endOffset": 133}, {"referenceID": 21, "context": "Test set BPC of neural language models trained on the Hutter dataset, MRNN = multiplicative RNN results from Sutskever et al. (2011) and Stacked LSTM results from Graves (2013).", "startOffset": 109, "endOffset": 177}, {"referenceID": 21, "context": "In Table 4, we present the test set BPC by a multiplicative RNN (Sutskever et al., 2011), a stacked LSTM (Graves, 2013) and the GF-RNN with LSTM units.", "startOffset": 64, "endOffset": 88}, {"referenceID": 21, "context": "In Table 4, we present the test set BPC by a multiplicative RNN (Sutskever et al., 2011), a stacked LSTM (Graves, 2013) and the GF-RNN with LSTM units. The performance of the proposed GF-RNN is comparable to, or better than, the previously reported best results. Note that Sutskever et al. (2011) used the vocabulary of 86 characters (removed XML tags and the Wikipedia markups), and their result is not directly comparable with ours.", "startOffset": 65, "endOffset": 297}, {"referenceID": 1, "context": "The authors would like to thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 7, "context": ", 2012) and Pylearn2 (Goodfellow et al., 2013).", "startOffset": 21, "endOffset": 46}], "year": 2015, "abstractText": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.", "creator": "LaTeX with hyperref package"}}}