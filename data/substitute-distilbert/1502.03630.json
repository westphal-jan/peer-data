{"id": "1502.03630", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Ordering-Sensitive and Semantic-Aware Topic Modeling", "abstract": "topic modeling of textual corpora exists continually emerging and challenging problem. in most previous work, the \" bag - over - words \" assumption is successfully computed which ignores the ordering of words. this assumption simplifies the computation, but it merely loses consistent ordering information and the semantic ambiguous words describes the context. for modern paper, we present a gaussian mixture neural data model ( gmntm ) which incorporates both the ordering of words and the semantic meaning of sentences into word modeling. specifically, we represent each topic as a cluster of multi - dimensional items and embed the corpus into a collection of vectors generated by the gaussian mixture model. each word is related not only by what topic, but also by constantly embedding vector of its surrounding words regarding the context. the gaussian mixture components and the topic of documents, sentences and words can be learnt jointly. extensive applications show that our model can assign better topics and more accurate word distributions for each topic. quantitatively, comparing to state - at - the - art topic modeling approaches, gmntm allows significantly better performance in terms of perplexity, retrieval accuracy and classification accuracy.", "histories": [["v1", "Thu, 12 Feb 2015 12:32:39 GMT  (44kb,D)", "http://arxiv.org/abs/1502.03630v1", "To appear in proceedings of AAAI 2015"]], "COMMENTS": "To appear in proceedings of AAAI 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["min yang", "tianyi cui", "wenting tu"], "accepted": true, "id": "1502.03630"}, "pdf": {"name": "1502.03630.pdf", "metadata": {"source": "META", "title": "Ordering-sensitive and Semantic-aware Topic Modeling", "authors": ["Min Yang", "Tianyi Cui", "Wenting Tu"], "emails": ["myang@cs.hku.hk", "tianyicui@gmail.com", "wttu@cs.hku.hk"], "sections": [{"heading": null, "text": "Introduction With the growing of large collection of electronic texts, much attention has been given to topic modeling of textual corpora, designed to identify representations of the data and learn thematic structure from large document collections without human supervision. Topic models have been applied to a variety of applications, including information retrieval (Wei and Croft 2006), collaborative filtering (Marlin 2003), authorship identification (Rosen-Zvi et al. 2004) and opinion extraction (Lin et al. 2012), etc. Existing topic models (Griffiths and Tenenbaum 2004; Mcauliffe and Blei 2008; Blei 2012) are built based on the assumption that each document is represented by a mixture of topics, where each topic defines a probability distribution over words. These models, including the probabilistic latent semantic analysis (PLSA) (Hofmann 1999) model and latent Dirichlet allocation (LDA) (Blei, Ng, and Jordan 2003) model, can be viewed as graphical models with latent variables. Some nonparametric extensions to these models have also been quite successful (Teh et al. 2006; Steyvers and Griffiths 2007).\nNevertheless, exact inference for these model is computationally hard, so one has to resort to slow or inaccurate approximations to compute the posterior distribution over topics. New undirected graphical model approaches, including the Replicated softmax model (Hinton and Salakhutdinov 2009), are also successfully applied to exploring the topics of the text, and in particular cases they outperform LDA (Srivastava, Salakhutdinov, and Hinton 2013).\nA major limitation of these topic models and many of their extensions is the bag-of-word assumption, which assumes that document can be fully characterized by bag-ofword features. This assumption is favorable in the computational point of view, but loses the ordering of the words and cannot properly capture the semantics of the context. For example, the phrases \u201cthe department chair couches offers\u201d and \u201cthe chair department offers couches\u201d have the same unigram statistics, but are about quite different topics. When deciding which topic generated the word \u201cchair\u201d in the first sentence, knowing that it was immediately preceded by the word \u201cdepartment\u201d makes it much more likely to have been generated by a topic that assigns high probability to words related to university administration (Wallach 2006).\nThere has been little work on developing topic models where the order of words is taken into consideration. To remove the assumption that the order of words is negligible, Gruber, Weiss, and Rosen-Zvi (2007) propose modeling the topics of words in the document via a Markov chain. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. Even though they consider the order of words to some extent, their model is still not capable of characterizing the semantics of words. For example, the integer representation of the words \u201cteacher\u201d and \u201cteach\u201d are completely unrelated, even if we know they have strong semantic connections and are very likely belonging to the same topic. To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014). Nevertheless, the dense word embeddings learned by previous NPLMs cannot be directly interpreted as topics. This is because that word embeddings are usually considered opaque, in the sense that it is difficult to assign meanings to the the vector ar X iv :1 50 2.\n03 63\n0v 1\n[ cs\n.L G\n] 1\n2 Fe\nb 20\n15\nrepresentation. In this paper, we proposed a novel topic model called the Gaussian Mixture Neural Topic Model (GMNTM). The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014). We represent the topic model as a Gaussian mixture model of vectors which encode words, sentences and documents. Each mixture component is associated with a specific topic. We present a method that jointly learns the topic model and the vector representation. As in NPLM methods, the word embeddings are learnt to optimize the predictability of a word using its surrounding words, with an important constraint that the vector representations are sampled from the Gaussian mixture which represents topics. Because the semantic meaning of sentences and documents are incorporated to infer the topic of a specific word, in our model, words with similar semantics are more likely to be clustered into the same topic, and topics of sentences and documents are more accurately learned. It potentially overcomes the weaknesses of the bag-of-word method and the bag-of-n-grams method, both of which don\u2019t use the order of words or the semantic of the context. We conduct experiments to verify the effectiveness of the proposed model on two widely used publicly available datasets. The experiment results show that our model substantially outperforms the state-of-the-art models in terms of perplexity, document retrieval quality and document classification accuracy."}, {"heading": "Related works", "text": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009). Among these approaches, LDA (Blei, Ng, and Jordan 2003) and its variants are the most popular models for topic modeling. The mixture of topics per document in the LDA model is generated from a Dirichlet prior mutual to all documents in the corpus. Different extensions of the LDA model have been proposed. For example, Teh et al. (2006) assumes that the number of mixture components is unknown a prior and is to be inferred from the data. Mcauliffe and Blei (2008) develops a supervised latent Dirichlet allocation model (sLDA) for document-response pairs. Recent work incorporates context information into the topic modeling, such as time (Wang and McCallum 2006), geographic location (Mei et al. 2006), authorship (Steyvers et al. 2004), and sentiment (Yang et al. 2014b; 2014a), to make topic models fit expectations better.\nRecently, there are several undirected graphical models being proposed, which typically outperform LDA. Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden\nunits on top of the first with bipartite undirected connections. Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model.\nHowever, all of these these topic models employ the bagof-words assumption, which is rarely true in practice. The bag-of-word assumption loses the ordering of the words and ignore the semantics of the context. There are several previous literature taking the order of words into account. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. They extend a unigram topic model so that it can reflect properties of a hierarchical Dirichlet bigram model. Gruber, Weiss, and Rosen-Zvi (2007) propose modeling the topic of words a Markov chain. Florez and Nachman (2014) exploits the semantics regularities captured by a Recurrent Neural Network (RNN) in text documents to build a recommender system. Although these methods captures the ordering of words, none of them them consider the semantics, thus they cannot capture the semantic similarities between words such as \u201cteach\u201d and \u201cteacher\u201d. In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014). Our topic model captures both the ordering of words and the semantics of the context. As a consequence, semantically similar words are more likely having similar topic distribution (e.g., \u201cJesus\u201d and \u201cChrist\u201d )."}, {"heading": "The GMNTM Model", "text": "In this section, we first describe the GMNTM model as a probabilistic generative model. Then we illustrate the inference algorithm for estimating the model parameters."}, {"heading": "Generative model", "text": "We assume there are W different words in the vocabulary and there are D documents in corpus. For each word w \u2208 {1, . . . ,W} in vocabulary, there is an associated V - dimensional vector representation vec(w) \u2208 RV for the word. Each document in corpus with index d \u2208 {1, . . . , D} also has a vector representation vec(d) \u2208 RV . If all the documents contain S sentences, then these sentences are indexed by s \u2208 {1, . . . , S}. The sentence with index s is associated with a vector representation vec(s) \u2208 RV .\nThere are T topics in the GMNTM model, where T is designated by the user. Each topic corresponds to a Gaussian mixture component. The k-th topic is represented by a V -dimensional Gaussian distribution N (\u00b5k,\u03a3k) with mixture weight \u03c0k \u2208 R, where \u00b5k \u2208 RV , \u03a3k \u2208 RV\u00d7V , and\u2211T\nk=1 \u03c0k = 1. The parameters of the Gaussian mixture model are collectively represented by\n\u03bb = {\u03c0k, \u00b5k,\u03a3k} k = 1, . . . , T (1)\nGiven the collection of parameters, we use\np(x|\u03bb) = T\u2211\ni=1\n\u03c0iN (x|\u00b5i,\u03a3i) (2)\nto represent the probability distribution for sampling a vector x from the Gaussian mixture model.\nWe describe the procedure that the corpus is generated. Given the Gaussian mixture model \u03bb, the generative process is described as follow: for each word w in the vocabulary, we sample its topic z(w) from the multinomial distribution \u03c0 := (\u03c01, \u03c02, . . . , \u03c0T ) and sample its vector representation vec(w) from distribution N (\u00b5z(w),\u03a3z(w)). Equivalently, the vector vec(w) is sampled from the Gaussian mixture model parameterized by \u03bb. For each document d and each sentence s in the document, we sample their topics z(d), z(s) from distribution \u03c0 and sample their vector representations, namely vec(d) and vec(s), also from the Gaussian mixture model. Let \u03a8 be the collection of latent vectors associated with all the words, sentences and documents in the corpus,\n\u03a8 := {vec(w)} \u222a {vec(d)} \u222a {vec(s)} (3)\nFor each word slot in the sentence, its word realization is generated according to the document\u2019s vector vec(d), the current sentence\u2019s vector vec(s) as well as at most m previous words in the same sentence. Formally, for the i-th location in the sentence, we represent its word realization by wi. The probability distribution of wi is defined by:\np (wi = w|d, s, wi\u2212m, . . . , wi\u22121)\n\u221d exp(awdoc + awsen + m\u2211 t=1 awt + b) (4)\nwhere adoc, asen and at are influences from the document, the sentence and the previous word, respectively. They are defined by\nawdoc = \u3008uwdoc, vec(d)\u3009 (5) awsen = \u3008uwsen, vec(s)\u3009 (6) awt = \u3008uwt , vec(wi\u2212t)\u3009 (7)\nHere, uwdoc, u w sen, u w t \u2208 RV are parameters of the model, and they are shared across all slots in the corpus. We use U to represent this collection of vectors,\nU := {udoc, usen} \u222a {ut|t \u2208 1, 2, . . . ,m}} (8)\nCombining the equations above, the probability distribution of wi is defined by a multi-class logistic model, where the features come from the vectors associated with the document, the sentence and the m previous words. By estimating the model parameters, we learn the word representations that make one word predictable from its previous words and the context. Jointly, we learn the distribution of topics that words, sentences and documents belong to.\nGiven the model parameters and the vectors for documents, sentences and words, we can infer the posterior probability distribution of topics. In particular, for a document d with vector representation vec(d), the posterior distribution\nof its topic, namely q(z(d)), is easy to calculate. For any z \u2208 1, 2, . . . , T , we have\nq(z(d) = z) = \u03c0zN (vec(d)|\u00b5z,\u03a3z)\u2211T\nk=1 \u03c0kN (vec(d)|\u00b5k,\u03a3k) . (9)\nSimilarly, for each sentence s in the document d, the posterior distribution of its topic is\nq(z(s) = z) = \u03c0zN (vec(s)|\u00b5z,\u03a3z)\u2211T\nk=1 \u03c0kN (vec(s)|\u00b5k,\u03a3k) . (10)\nFor each word w in the vocabulary, the posterior distribution of its topic is similarly calculated as\nq(z(w) = z) = \u03c0zN (vec(w)|\u00b5z,\u03a3z)\u2211T\nk=1 \u03c0kN (vec(w)|\u00b5k,\u03a3k) (11)\nFinally, for each word slot in the document, we also want to explore its topic. Since the topic of a particular location in the document is affected by its word realization and the sentence/document it belongs to, we define the probability of it belonging to topic z proportional to the product of q(z(w) = z), q(z(s) = z) and q(z(d) = z), where w, s, and d are the word, the sentence and the document that this word slot associates with."}, {"heading": "Estimating model parameters", "text": "We estimate the model parameters \u03bb, U and \u03a8 by maximizing the likelihood of the generative model. The parameter estimation consists of two stages. In Stage I, we maximize the likelihood of the model with respect to \u03bb. Since \u03bb characterizes a Gaussian mixture model, this procedure can be implemented by the Expectation Maximization (EM) algorithm. In Stage II, we maximize the model likelihood with respect to U and \u03a8 , this procedure can be implemented by stochastic gradient descent. We alternatively execute Stage I and Stage II until the parameters converge. The algorithm in this section is summarized in Algorithm 1.\nStage I: Estimating \u03bb In this stage, the latent vector of words, sentences and documents are fixed. We estimate the parameters of the Gaussian mixture model \u03bb = {\u03c0k, \u00b5k,\u03a3k}. This is a classical statistical estimation problem which can be solved by running the EM algorithm. The reader can refer to the book (Bishop 2006) for the implementation details.\nStage II: estimatingU and \u03a8 When \u03bb is known and fixed, we estimate the model parameters U and the latent vectors \u03a8 by maximizing the log-likelihood of the generative model. In particular, we iteratively sample a location in the corpus, and consider the log-likelihood of the observed word at this location. Let the word realization at location i be represented by wi. The log-likelihood of this location is equal to\nJi(U, \u03a8) = log(p(\u03a8 |\u03bb)) + awidoc + a wi sen + m\u2211 t=1 awit + b\n\u2212 log (\u2211\nw\nexp(awdoc + a w sen + m\u2211 t=1 awt + b) ) (12)\nAlgorithm 1 Inference Algorithm \u2022 Inputs: A corpus containing D documents, S sentences,\nand a vocabulary containing W distinct words \u2022 Initialize parameters\n\u2013 Randomly initialize the vectors \u03a8 . \u2013 Initialize parameters U with all-zero vectors. \u2013 Initialize Gaussian mixture model parameters with the\nstandard normal distribution N (0,diag(1)). \u2022 Repeat until converge\n\u2013 Fixing parameters U and \u03a8 , run the EM algorithm to estimate the Gaussian mixture model parameters \u03bb.\n\u2013 Fixing the Gaussian mixture model \u03bb, run stochastic gradient descent to maximize the log-likelihood of the model with respect to parameters U and \u03a8 .\nwhere p(\u03a8 |\u03bb) is the prior distribution of parameter \u03a8 in the Gaussian mixture model, defined by equation (2). The quantities awdoc, a w sen and a w t are defined in equations (5), (6), and (7). The objective function Ji(U, \u03a8) involves all parameters in the collections (U, \u03a8). Taking the computation efficiency into consideration, we only update the parameters associated with the word wi. Concretely, we update\nvec(wi)\u2190 vec(wi) + \u03b1 \u2202Ji(U, \u03a8)\n\u2202vec(wi) (13)\nuwit \u2190 u wi t + \u03b1\n\u2202Ji(U, \u03a8)\n\u2202uwit (14)\nwith \u03b1 as the learning rate. Similarly, we update vec(s), vec(d) and uwdoc, u w sen using the same gradient step, as they are parameters associated with the current sentence and the current document. Once the gradient update is accomplished, we sample another location to continue the update. The procedure terminates when there are sufficient number of updates performed, so that bothU and \u03a8 converge to fixed values."}, {"heading": "Experiments", "text": "In this section, we evaluate our model on the 20 Newsgroups and the Reuters Corpus Volume 1 (RCV1-v2) data sets. Followed the evaluation in (Srivastava, Salakhutdinov, and Hinton 2013), we compare our GMNTM model with the stateof-the-art topic models in perplexity, retrieval quality and classification accuracy."}, {"heading": "Datasets description", "text": "We adopt two widely used datasets, the 20 Newsgroups data and the RCV1-v2 data, in our evaluations. Data preprocessing is performed on both datasets. We first remove nonalphabet characters, numbers, pronoun, punctuation and stop words from the text. Then, stemming is applied so as to reduce the vocabulary size and settle the issue of data spareness. The detailed properties of the datasets are described as follow.\n20 Newsgroups dataset: This dataset is a collection of 18,845 newsgroup documents1. The corpus is partitioned into 20 different newsgroups, each corresponding to a separate topic. Following the preprocessing in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the dataset is partitioned chronologically into 11,314 training documents and 7,531 testing documents. Reuters Corpus Volume 1 (RCV1-v2): This dataset is an archive of 804,414 newswire stories produced by Reuters journalists between August 20, 1996, and August 19, 1997 (Lewis et al. 2004)2. RCV1-v2 has been manually categorized into 103 topics, and the topic classes form a tree which is typically of depth 3. As in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the data was randomly split into 794,414 training documents and 10,000 testing documents."}, {"heading": "Baseline methods", "text": "In the experiments, the proposed topic modeling approach is compared with several baseline methods, which we describe below: Latent Dirichlet Allocation (LDA): In the LDA model (Blei, Ng, and Jordan 2003), we used the online variational inference implementation of the gensim toolkit 3. We used the recommended parameter setting \u03b1 = 1/T . Hidden Topic Markov Models (HMM): This model is proposed by (Gruber, Weiss, and Rosen-Zvi 2007), which models the topics of words in the document as a Markov chain. The HMM model is run using the publicly available code4. We use default settings for all hyper parameters. Over-Replicated Softmax (ORS): This model is proposed by (Srivastava, Salakhutdinov, and Hinton 2013). It is a two hidden layer DBM model, which has been shown to obtain a state-of-the-art performance in terms of classification and retrieval tasks compared with Replicated Softmax model (Hinton and Salakhutdinov 2009) and Cannonade model (Larochelle and Lauly 2012)."}, {"heading": "Implementation details", "text": "In our GMNTM model, the learning rate \u03b1 is set to 0.025 and gradually reduced to 0.0001. For each word, at most m = 6 previous words in the same sentence is used as the context. For easy comparison with other models, the word vector size is set to the same as the number of topics V = T = 128. Increasing the word vector size further could improve the quality of the topics that are generated by the GMNTM model.\nDocuments are split into sentences and words using the NLTK toolkit (Bird 2006)5. The Gaussian mixture model is inferred using the variational inference algorithm in scikitlearn toolkit (Pedregosa et al. 2011)6. To perform comparable experiments with restricted vocabulary, words outside\n1Available at http://qwone.com/~jason/20Newsgroups 2Available at http://trec.nist.gov/data/reuters/reuters.html 3http://radimrehurek.com/gensim/models/remodel.html 4http://code.google.com/p/Oppenheimer/downloads/list 5http://www.nltk.org/ 6http://scikit-learn.org/\nof the vocabulary is replaced as a special token and doesn\u2019t count into the word perplexity calculation."}, {"heading": "Generative model evaluation", "text": "We first evaluate our model\u2019s performance as a generative model for documents. We perform our evaluation on the 20 Newsgroups dataset and the RCV1-v2 dataset. For each of the datasets, we extract the words from raw data and preserve the ordering of words. We follow the same evaluation as in (Srivastava, Salakhutdinov, and Hinton 2013), comparing our model with the other models in terms of perplexity.\nWe estimate the log-probability for 1000 held-out documents that are randomly sampled from the test sets. After running the algorithm to infer the vector representations of words, sentences, and documents in held-out test documents, the average test perplexity per word is then estimated as exp ( \u2212 1N \u2211 w log p(w) ) , where N are the total number of words in the held-out test documents, and p(w) is calculated according to equation (4).\nTable 1 shows the perplexity for each dataset. The perplexity for Over-Replicated Softmax is taken from (Srivastava, Salakhutdinov, and Hinton 2013). As shown by Table 1, our model performs significantly better than the other models on both datasets in terms of perplexity. More specifically, for 20 Newsgroups data set, the perplexity decreases from 949 to 933, and for RCV1-v2 data set, it decreases from 982 to 826. This verifies the effectiveness of the proposed topic modeling approach in fitting the dataset. The GMNTM model works particularly well on large-scale datasets such as RCV1-v2."}, {"heading": "Document retrieval evaluation", "text": "To evaluate the quality of the documents representations that are learnt by our model, we perform an information retrieval task. Following the setting in (Srivastava, Salakhutdinov, and Hinton 2013), documents in the training set are used as a database, while the test set is used as queries. For each query, documents in the database are ranked using cosine distance as the similarity metric. The retrieval task is performed separately for each label and the results are averaged. Figure 1 compares the precision-recall curves with 128 topics. The curves for LDA and Over-Replicated are taken from (Srivastava, Salakhutdinov, and Hinton 2013). We see that for the 20 Newsgroups dataset, our model performs on par or slightly better than the other models. While for the RCV1-v2 dataset, our model achieves a significant improvement. Since RCV1-v2 contains a greater amount of texts, the GMNTM model considering the ordering of words is more powerful in mining the semantics of the text."}, {"heading": "Document classification evaluation", "text": "Following the evaluation of (Srivastava, Salakhutdinov, and Hinton 2013), we also perform document classification with the learnt topic representation from our model. The same as in (Srivastava, Salakhutdinov, and Hinton 2013), multinomial logistic regression with a cross entropy loss function is used for the 20 Newsgroups data set, and the evaluation metric is the classification accuracy. For the RCV1-v2 data set, we use independent logistic regression for each label. The evaluation metric is Mean Average Precision.\nWe summarize the experiment results with 128 topics in Table 3. The results of document classification for LDA and Over-Replicated Softmax are taken from (Srivastava, Salakhutdinov, and Hinton 2013). According to Table 3, the proposed model substantially outperforms other models on both datasets for document classification. For the 20 Newsgroups dataset, the overall accuracy of the Over-Replicated Softmax model is 66.8%, which is slightly higher than LDA and HTMM. Our model further improves the classification result to 73.1%. On RCV1-v2 dataset, we observe the similar results. The mean average precision increases from 0.401 (Over-Replicated Softmax) to 0.445 (our model).\nQualitative inspection of topic specialization\nSince topic models are often used for the exploratory analysis of unlabeled text, we also evaluate whether meaningful semantics are captured by our model. Due to the space limit, we only illustrate four topics extracted by our model and LDA which are topics about religion, space, sports and security. These topics are also captured as (sub)categories in the 20 Newsgroups dataset. Table 3 shows the 4 topics learnt by the GMNTM model and the corresponding topics learnt by LDA. In each topic, we visualize it using 10 words with the largest weights. The 4 topics shown in Table 3 for both models are easy for interpretation according to the top words. However, we see that the topics found by the two models are different in nature. GMNTM finds topics that consist of the words that are consecutive in the document or the words having similar semantics. For example, in the GMNTM model, \u201cChrist\u201d and \u201cchristian\u201d share the same topics, mainly because they have strong semantic connections, even though they don\u2019t co-occur that often, which makes LDA unable to put them in the same topic. On the other hand, LDA often find some general words such as \u201cwould\u201d and \u201caccept\u201d for the religion topic, which are unhelpful for interpreting the topics."}, {"heading": "Conclusion and Future Work", "text": "Rather than ignoring the semantics of the words and assuming that the topic distribution within a document is conditionally independent, in this paper, we introduce an ordering-sensitive and semantic-aware topic modeling approach. The GMNTM model jointly learns the topic of documents and the vectorized representation of words, sentences and documents. The model learns better topics and disambiguates words that belong to different topics. Comparing to state-of-the-art topic modeling approaches, the GMNTM outperforms in terms of perplexity, retrieval accuracy and classification accuracy.\nIn future works, we will explore using non-parametric\nmodels to cluster word vectors. For example, we look forward to incoporating infinite Dirichelet process to automatically detect the number of topics. We can also use hierarchical model to further capture the subtle semantics of the text. As another promising direction, we consider building topic models on popular neural probabilistic methods, such as the Recurrent Neural Network Language Model (RNNLM). The GMNTM model has appplications to several tasks in natural language processing, including entity recognition, information extraction and sentiment analysis. These applications also deserve further study,"}], "references": [{"title": "Nltk: the natural language toolkit", "author": ["S. Bird"], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions, 69\u201372. Association for Computational Linguistics.", "citeRegEx": "Bird,? 2006", "shortCiteRegEx": "Bird", "year": 2006}, {"title": "Pattern recognition and machine learning, volume 1", "author": ["C.M. Bishop"], "venue": "springer New York.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM 55(4):77\u201384.", "citeRegEx": "Blei,? 2012", "shortCiteRegEx": "Blei", "year": 2012}, {"title": "Deep learning of semantic word representations to implement a content-based recommender for the recsys", "author": ["O.U. Florez", "L. Nachman"], "venue": null, "citeRegEx": "Florez and Nachman,? \\Q2014\\E", "shortCiteRegEx": "Florez and Nachman", "year": 2014}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D. Griffiths", "M. Tenenbaum"], "venue": "Advances in neural information processing systems 16:17.", "citeRegEx": "Griffiths and Tenenbaum,? 2004", "shortCiteRegEx": "Griffiths and Tenenbaum", "year": 2004}, {"title": "Hidden topic markov models", "author": ["A. Gruber", "Y. Weiss", "M. Rosen-Zvi"], "venue": "International Conference on Artificial Intelligence and Statistics, 163\u2013170.", "citeRegEx": "Gruber et al\\.,? 2007", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 1607\u20131614.", "citeRegEx": "Hinton and Salakhutdinov,? 2009", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2009}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, 289\u2013296. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Hofmann,? 1999", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "Advances in Neural Information Processing Systems, 2708\u20132716.", "citeRegEx": "Larochelle and Lauly,? 2012", "shortCiteRegEx": "Larochelle and Lauly", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov,? 2014", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "The Journal of Machine Learning Research 5:361\u2013 397.", "citeRegEx": "Lewis et al\\.,? 2004", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Weakly supervised joint sentiment-topic detection from text", "author": ["C. Lin", "Y. He", "R. Everson", "S. Ruger"], "venue": "CIKM 24(6):1134\u20131145.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Modeling user rating profiles for collaborative filtering", "author": ["B.M. Marlin"], "venue": "Advances in neural information processing systems.", "citeRegEx": "Marlin,? 2003", "shortCiteRegEx": "Marlin", "year": 2003}, {"title": "Supervised topic models", "author": ["J.D. Mcauliffe", "D.M. Blei"], "venue": "Advances in neural information processing systems, 121\u2013128.", "citeRegEx": "Mcauliffe and Blei,? 2008", "shortCiteRegEx": "Mcauliffe and Blei", "year": 2008}, {"title": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs", "author": ["Q. Mei", "C. Liu", "H. Su", "C. Zhai"], "venue": "Proceedings of the 15th international conference on World Wide Web, 533\u2013542. ACM.", "citeRegEx": "Mei et al\\.,? 2006", "shortCiteRegEx": "Mei et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 1081\u20131088.", "citeRegEx": "Mnih and Hinton,? 2009", "shortCiteRegEx": "Mnih and Hinton", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems, 2265\u2013 2273.", "citeRegEx": "Mnih and Kavukcuoglu,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "arXiv preprint arXiv:1206.6426.", "citeRegEx": "Mnih and Teh,? 2012", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "The Journal of Machine Learning Research 12:2825\u20132830.", "citeRegEx": "Pedregosa et al\\.,? 2011", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence, 487\u2013494. AUAI Press.", "citeRegEx": "Rosen.Zvi et al\\.,? 2004", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Modeling documents with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1309.6865.", "citeRegEx": "Srivastava et al\\.,? 2013", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of latent semantic analysis 427(7):424\u2013 440.", "citeRegEx": "Steyvers and Griffiths,? 2007", "shortCiteRegEx": "Steyvers and Griffiths", "year": 2007}, {"title": "Probabilistic author-topic models for information discovery", "author": ["M. Steyvers", "P. Smyth", "M. Rosen-Zvi", "T. Griffiths"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, 306\u2013315. ACM.", "citeRegEx": "Steyvers et al\\.,? 2004", "shortCiteRegEx": "Steyvers et al\\.", "year": 2004}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the american statistical association 101(476).", "citeRegEx": "Teh et al\\.,? 2006", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Topic modeling: beyond bag-ofwords", "author": ["H.M. Wallach"], "venue": "Proceedings of the 23rd international conference on Machine learning, 977\u2013984. ACM.", "citeRegEx": "Wallach,? 2006", "shortCiteRegEx": "Wallach", "year": 2006}, {"title": "A hybrid neural network-latent topic model", "author": ["L. Wan", "L. Zhu", "R. Fergus"], "venue": "International Conference on Artificial Intelligence and Statistics, 1287\u20131294.", "citeRegEx": "Wan et al\\.,? 2012", "shortCiteRegEx": "Wan et al\\.", "year": 2012}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, 424\u2013433. ACM.", "citeRegEx": "Wang and McCallum,? 2006", "shortCiteRegEx": "Wang and McCallum", "year": 2006}, {"title": "Lda-based document models for ad-hoc retrieval", "author": ["X. Wei", "W.B. Croft"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, 178\u2013185. ACM.", "citeRegEx": "Wei and Croft,? 2006", "shortCiteRegEx": "Wei and Croft", "year": 2006}, {"title": "A topic model for building fine-grained domainspecific emotion lexicon", "author": ["M. Yang", "B. Peng", "Z. Chen", "D. Zhu", "K.-P. Chow"], "venue": "ACL 2014 421\u2013426.", "citeRegEx": "Yang et al\\.,? 2014a", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Learning domain-specific sentiment lexicon with supervised sentiment-aware lda", "author": ["M. Yang", "D. Zhu", "R. Mustafa", "K.-P. Chow"], "venue": "ECAI 2014 927\u2013932.", "citeRegEx": "Yang et al\\.,? 2014b", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Topic models have been applied to a variety of applications, including information retrieval (Wei and Croft 2006), collaborative filtering (Marlin 2003), authorship identification (Rosen-Zvi et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 13, "context": "Topic models have been applied to a variety of applications, including information retrieval (Wei and Croft 2006), collaborative filtering (Marlin 2003), authorship identification (Rosen-Zvi et al.", "startOffset": 139, "endOffset": 152}, {"referenceID": 21, "context": "Topic models have been applied to a variety of applications, including information retrieval (Wei and Croft 2006), collaborative filtering (Marlin 2003), authorship identification (Rosen-Zvi et al. 2004) and opinion extraction (Lin et al.", "startOffset": 180, "endOffset": 203}, {"referenceID": 12, "context": "2004) and opinion extraction (Lin et al. 2012), etc.", "startOffset": 29, "endOffset": 46}, {"referenceID": 5, "context": "Existing topic models (Griffiths and Tenenbaum 2004; Mcauliffe and Blei 2008; Blei 2012) are built based on the assumption that each document is represented by a mixture of topics, where each topic defines a probability distribution over words.", "startOffset": 22, "endOffset": 88}, {"referenceID": 14, "context": "Existing topic models (Griffiths and Tenenbaum 2004; Mcauliffe and Blei 2008; Blei 2012) are built based on the assumption that each document is represented by a mixture of topics, where each topic defines a probability distribution over words.", "startOffset": 22, "endOffset": 88}, {"referenceID": 3, "context": "Existing topic models (Griffiths and Tenenbaum 2004; Mcauliffe and Blei 2008; Blei 2012) are built based on the assumption that each document is represented by a mixture of topics, where each topic defines a probability distribution over words.", "startOffset": 22, "endOffset": 88}, {"referenceID": 8, "context": "These models, including the probabilistic latent semantic analysis (PLSA) (Hofmann 1999) model and latent Dirichlet allocation (LDA) (Blei, Ng, and Jordan 2003) model, can be viewed as graphical models with latent variables.", "startOffset": 74, "endOffset": 88}, {"referenceID": 25, "context": "Some nonparametric extensions to these models have also been quite successful (Teh et al. 2006; Steyvers and Griffiths 2007).", "startOffset": 78, "endOffset": 124}, {"referenceID": 23, "context": "Some nonparametric extensions to these models have also been quite successful (Teh et al. 2006; Steyvers and Griffiths 2007).", "startOffset": 78, "endOffset": 124}, {"referenceID": 7, "context": "New undirected graphical model approaches, including the Replicated softmax model (Hinton and Salakhutdinov 2009), are also successfully applied to exploring the topics of the text, and in particular cases they outperform LDA (Srivastava, Salakhutdinov, and Hinton 2013).", "startOffset": 82, "endOffset": 113}, {"referenceID": 26, "context": "When deciding which topic generated the word \u201cchair\u201d in the first sentence, knowing that it was immediately preceded by the word \u201cdepartment\u201d makes it much more likely to have been generated by a topic that assigns high probability to words related to university administration (Wallach 2006).", "startOffset": 278, "endOffset": 292}, {"referenceID": 17, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 19, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 18, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 16, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 10, "context": "To seek a distributed way of representing words that capture semantic similarities, several Neural Probabilistic Language Models (NPLMs) have been proposed (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 156, "endOffset": 266}, {"referenceID": 21, "context": "Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables.", "startOffset": 0, "endOffset": 15}, {"referenceID": 17, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 19, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 18, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 16, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 10, "context": "The work is inspired by the recent neural probabilistic language models (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 72, "endOffset": 182}, {"referenceID": 5, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009).", "startOffset": 175, "endOffset": 309}, {"referenceID": 3, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009).", "startOffset": 175, "endOffset": 309}, {"referenceID": 7, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009).", "startOffset": 175, "endOffset": 309}, {"referenceID": 28, "context": "Recent work incorporates context information into the topic modeling, such as time (Wang and McCallum 2006), geographic location (Mei et al.", "startOffset": 83, "endOffset": 107}, {"referenceID": 15, "context": "Recent work incorporates context information into the topic modeling, such as time (Wang and McCallum 2006), geographic location (Mei et al. 2006), authorship (Steyvers et al.", "startOffset": 129, "endOffset": 146}, {"referenceID": 24, "context": "2006), authorship (Steyvers et al. 2004), and sentiment (Yang et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 31, "context": "2004), and sentiment (Yang et al. 2014b; 2014a), to make topic models fit expectations better.", "startOffset": 21, "endOffset": 47}, {"referenceID": 3, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009). Among these approaches, LDA (Blei, Ng, and Jordan 2003) and its variants are the most popular models for topic modeling. The mixture of topics per document in the LDA model is generated from a Dirichlet prior mutual to all documents in the corpus. Different extensions of the LDA model have been proposed. For example, Teh et al. (2006) assumes that the number of mixture components is unknown a prior and is to be inferred from the data.", "startOffset": 176, "endOffset": 648}, {"referenceID": 3, "context": "In the past decade, a great variety of topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from texts (Blei, Ng, and Jordan 2003; Griffiths and Tenenbaum 2004; Blei 2012; Gruber, Weiss, and Rosen-Zvi 2007; Hinton and Salakhutdinov 2009). Among these approaches, LDA (Blei, Ng, and Jordan 2003) and its variants are the most popular models for topic modeling. The mixture of topics per document in the LDA model is generated from a Dirichlet prior mutual to all documents in the corpus. Different extensions of the LDA model have been proposed. For example, Teh et al. (2006) assumes that the number of mixture components is unknown a prior and is to be inferred from the data. Mcauliffe and Blei (2008) develops a supervised latent Dirichlet allocation model (sLDA) for document-response pairs.", "startOffset": 176, "endOffset": 776}, {"referenceID": 9, "context": "Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model.", "startOffset": 92, "endOffset": 119}, {"referenceID": 17, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 19, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 18, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 16, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 10, "context": "In contrast, our model is inspired by the recent work in learning vector representations of words which are proved to capture the semantics of texts (Mnih and Hinton 2009; Mnih and Teh 2012; Mnih and Kavukcuoglu 2013; Mikolov et al. 2013; Le and Mikolov 2014).", "startOffset": 149, "endOffset": 259}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document.", "startOffset": 14, "endOffset": 26}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections.", "startOffset": 14, "endOffset": 285}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections. Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model. However, all of these these topic models employ the bagof-words assumption, which is rarely true in practice. The bag-of-word assumption loses the ordering of the words and ignore the semantics of the context. There are several previous literature taking the order of words into account. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables.", "startOffset": 14, "endOffset": 953}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections. Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model. However, all of these these topic models employ the bagof-words assumption, which is rarely true in practice. The bag-of-word assumption loses the ordering of the words and ignore the semantics of the context. There are several previous literature taking the order of words into account. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. They extend a unigram topic model so that it can reflect properties of a hierarchical Dirichlet bigram model. Gruber, Weiss, and Rosen-Zvi (2007) propose modeling the topic of words a Markov chain.", "startOffset": 14, "endOffset": 1223}, {"referenceID": 3, "context": "Mcauliffe and Blei (2008) present a two-layer undirected graphical model, called \u201cReplicated Softmax\u201d, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of document. Hinton and Salakhutdinov (2009) extend \u201cReplicated Softmax\u201d by adding another layer of hidden units on top of the first with bipartite undirected connections. Neural network based approaches, such as Neural Autoregressive Density Estimators (DocNADE) (Larochelle and Lauly 2012) and Hybrid Neural Network-Latent Topic Model (Wan, Zhu, and Fergus 2012), are also shown outperforming the LDA model. However, all of these these topic models employ the bagof-words assumption, which is rarely true in practice. The bag-of-word assumption loses the ordering of the words and ignore the semantics of the context. There are several previous literature taking the order of words into account. Wallach (2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. They extend a unigram topic model so that it can reflect properties of a hierarchical Dirichlet bigram model. Gruber, Weiss, and Rosen-Zvi (2007) propose modeling the topic of words a Markov chain. Florez and Nachman (2014) exploits the semantics regularities captured by a Recurrent Neural Network (RNN) in text documents to build a recommender system.", "startOffset": 14, "endOffset": 1301}, {"referenceID": 1, "context": "The reader can refer to the book (Bishop 2006) for the implementation details.", "startOffset": 33, "endOffset": 46}, {"referenceID": 7, "context": "Following the preprocessing in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the dataset is partitioned chronologically into 11,314 training documents and 7,531 testing documents.", "startOffset": 31, "endOffset": 62}, {"referenceID": 9, "context": "Following the preprocessing in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the dataset is partitioned chronologically into 11,314 training documents and 7,531 testing documents.", "startOffset": 67, "endOffset": 94}, {"referenceID": 11, "context": "Reuters Corpus Volume 1 (RCV1-v2): This dataset is an archive of 804,414 newswire stories produced by Reuters journalists between August 20, 1996, and August 19, 1997 (Lewis et al. 2004)2.", "startOffset": 167, "endOffset": 186}, {"referenceID": 7, "context": "As in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the data was randomly split into 794,414 training documents and 10,000 testing documents.", "startOffset": 6, "endOffset": 37}, {"referenceID": 9, "context": "As in (Hinton and Salakhutdinov 2009) and (Larochelle and Lauly 2012), the data was randomly split into 794,414 training documents and 10,000 testing documents.", "startOffset": 42, "endOffset": 69}, {"referenceID": 7, "context": "It is a two hidden layer DBM model, which has been shown to obtain a state-of-the-art performance in terms of classification and retrieval tasks compared with Replicated Softmax model (Hinton and Salakhutdinov 2009) and Cannonade model (Larochelle and Lauly 2012).", "startOffset": 184, "endOffset": 215}, {"referenceID": 9, "context": "It is a two hidden layer DBM model, which has been shown to obtain a state-of-the-art performance in terms of classification and retrieval tasks compared with Replicated Softmax model (Hinton and Salakhutdinov 2009) and Cannonade model (Larochelle and Lauly 2012).", "startOffset": 236, "endOffset": 263}, {"referenceID": 0, "context": "Documents are split into sentences and words using the NLTK toolkit (Bird 2006)5.", "startOffset": 68, "endOffset": 79}, {"referenceID": 20, "context": "The Gaussian mixture model is inferred using the variational inference algorithm in scikitlearn toolkit (Pedregosa et al. 2011)6.", "startOffset": 104, "endOffset": 127}], "year": 2015, "abstractText": "Topic modeling of textual corpora is an important and challenging problem. In most previous work, the \u201cbag-of-words\u201d assumption is usually made which ignores the ordering of words. This assumption simplifies the computation, but it unrealistically loses the ordering information and the semantic of words in the context. In this paper, we present a Gaussian Mixture Neural Topic Model (GMNTM) which incorporates both the ordering of words and the semantic meaning of sentences into topic modeling. Specifically, we represent each topic as a cluster of multi-dimensional vectors and embed the corpus into a collection of vectors generated by the Gaussian mixture model. Each word is affected not only by its topic, but also by the embedding vector of its surrounding words and the context. The Gaussian mixture components and the topic of documents, sentences and words can be learnt jointly. Extensive experiments show that our model can learn better topics and more accurate word distributions for each topic. Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM obtains significantly better performance in terms of perplexity, retrieval accuracy and classification accuracy.", "creator": "TeX"}}}