{"id": "1502.05934", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Achieving All with No Parameters: Adaptive NormalHedge", "abstract": "we study the classic online learning problem of predicting with expert advice, and propose a truly parameter - free and adaptive algorithm that handles several objectives simultaneously without using any prior information. the main component of this work is an improved version of the normalhedge. dt algorithm ( luo and schapire, 2014 ), called adanormalhedge. on one hand, detecting new code ensures small regret about the competitor has small loss and relatively easy regret when the losses are stochastic. on the other hand, dc algorithm is able to gamble with any convex combination of the competing simultaneously, with a regret in terms of the relative entropy relating the outcomes and the competitor. this resolves an open hurdle proposed by chaudhuri + al. ( 2009 ) and martin and vovk ( 2010 ). moreover, we extend the hypothesis to above sleeping expert setting and provide two applications to illustrate the power of algorithms : 1 ) competing but time - varying unknown competitors and 2 ) predicting almost - well as the best input tree. two results on these applications significantly reverse previous work from different aspects, and a different case for the first application resolves another open problem proposed by warmuth ben koolen ( 2014 ) on whether one can simultaneously achieve optimal shifting capabilities for both adversarial and stochastic losses.", "histories": [["v1", "Fri, 20 Feb 2015 16:58:36 GMT  (34kb)", "http://arxiv.org/abs/1502.05934v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "robert e schapire"], "accepted": false, "id": "1502.05934"}, "pdf": {"name": "1502.05934.pdf", "metadata": {"source": "CRF", "title": "Achieving All with No Parameters: Adaptive NormalHedge", "authors": ["Haipeng Luo"], "emails": ["haipengl@cs.princeton.edu", "schapire@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n05 93\n4v 1\n[ cs\n.L G\n] 2\n0 Fe\nb 20"}, {"heading": "1 Introduction", "text": "The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al. [1997], Vovk [1998] and others two decades ago. Roughly speaking, in this problem, a player needs to decide a distribution over a set of experts on each round, and then an adversary decides and reveals the loss for each expert. The player\u2019s loss for this round is the expected loss of the experts with respect to the distribution that he chose, and his goal is to have a total loss that is not much worse than any single expert, or more generally, any fixed and unknown convex combination of experts.\nBeyond this classic goal, various more difficult objectives for this problem were studied in recent years, such as: learning with unknown number of experts and competing with all but the top small fraction of experts [Chaudhuri et al., 2009, Chernov and Vovk, 2010]; competing with a sequence of different combinations of the experts [Herbster and Warmuth, 2001, Cesa-Bianchi et al., 2012]; learning with experts who provide confidence-rated advice [Blum and Mansour, 2007]; and achieving much smaller regret when the problem is \u201ceasy\u201d while still ensuring worst-case robustness [de Rooij et al., 2014, Van Erven et al., 2014, Gaillard et al., 2014]. Different algorithms were proposed separately to solve these problems to some extent. In this work, we essentially provide one single parameter-free algorithm that achieves all these goals with absolutely no prior information and significantly improved results in some cases.\nOur algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.DT [Luo and Schapire, 2014]. We call it Adaptive NormalHedge (or AdaNormalHedge for short). NormalHedge and NormalHedge.DT provide guarantees for the so-called \u01eb-quantile regret simultaneously for any \u01eb, which essentially corresponds to competing with a uniform distribution over the top \u01eb-fraction of experts. Our new algorithm improves NormalHedge.DT from two aspects (Section 3):\n1. AdaNormalHedge can compete with not just the competitor of the specific form mentioned above, but indeed any unknown fixed competitor simultaneously, with a regret in terms of the relative entropy between the competitor and the player\u2019s prior belief of the experts.\n2. AdaNormalHedge ensures a new regret bound in terms of the cumulative magnitude of the instantaneous regrets, which is always at most the bound for NormalHedge.DT (or NormalHedge). Moreover, the power of this new form of regret is almost the same as the second order bound introduced in a recent work by Gaillard et al. [2014]. Specifically, it implies 1) a small regret when the loss of the competitor is small and 2) an almost constant regret when the losses are generated randomly with a gap in expectation.\nOur results resolve the open problem asked in Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether a better \u01eb-quantile regret in terms of the loss of the expert instead of the horizon can be achieved. In fact, our results are even better and more general.\nAdaNormalHedge is a simple and truly parameter-free algorithm. Indeed, it does not even need to know the number of experts in some sense. To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007]. We then focus on a special case of this setting called the sleeping expert problem [Blum, 1997, Freund et al., 1997], where the number of \u201cawake\u201d experts is dynamically changing and the total number of underlying experts is indeed unknown. AdaNormalHedge is thus a very suitable algorithm for this problem. To show the power of all the abovementioned properties of AdaNormalHedge, we study the following two examples of the sleeping expert problem and use AdaNormalHedge to significantly improve previous work.\nThe first example is adaptive regret, that is, regret on any time interval, introduced by Hazan and Seshadhri [2007]. This can be reduced to a sleeping expert problem by adding a new copy of each original expert on each round [Freund et al., 1997, Koolen et al., 2012]. Thus, the total number of sleeping experts is not fixed. When some information on this interval is known (such as the length, the loss of the competitor on this interval, etc), several algorithms achieve optimal regret [Hazan and Seshadhri, 2007, Cesa-Bianchi et al., 2012]. However, when no prior information is available, all previous work gives suboptimal bounds. We apply AdaNormalHedge to this problem. The resulting algorithm, which we called AdaNormalHedge.TV, enjoys the optimal adaptive regret in not only the adversarial case but also the stochastic case due to the properties of AdaNormalHedge.\nWe then extend the results to the problem of tracking the best experts where the player needs to compete with the best partition of the whole process and the best experts on each of these partitions [Herbster and Warmuth, 1995, Bousquet and Warmuth, 2003]. This resolves one of the open problems in Warmuth and Koolen [2014] on whether a single algorithm can achieve optimal shifting regret for both adversarial and stochastic losses. Note that although recent work by Sani et al. [2014] also solves this open problem in some sense, their method requires knowing the number of partitions and other information ahead of time and also gives a worse bound for stochastic losses, while AdaNormalHedge.TV is completely parameter-free and gives optimal bounds.\nWe finally consider the most general case where the competitor varies over time with no constraints, which subsumes the previous two examples (adaptive regret and shifting regret). This problem was introduced in Herbster and Warmuth [2001] and later generalized by Cesa-Bianchi et al. [2012]. Their algorithm (fixed share) also requires knowing some information on the sequence of competitors to optimally tune parameters. We avoid this issue by showing that while this problem seems more general and difficult, it is in fact equivalent to its special case: achieving adaptive regret. This equivalence theorem is independent of the concrete algorithms and may be of independent interest. Applying this result, we show that without any parameter tuning, AdaNormalHedge.TV automatically achieves a bound comparable to the one achieved by the optimally tuned fixed share algorithm when competing with time-varying competitors.\nConcrete results and detailed comparisons on this first example can be found in Section 5. To sum up, AdaNormalHedge.TV is an algorithm that is simultaneously adaptive in the number of experts, the competitors and the way the losses are generated.\nThe second example we provide is predicting almost as well as the best pruning tree [Helmbold and Schapire, 1997], which was also shown to be reducible to a sleeping expert problem [Freund et al., 1997]. Previous work either only considered the log loss setting, or assumed prior information on the best pruning tree is known. Using AdaNormalHedge, we again provide better or comparable bounds without knowing any prior information. In fact, due to the adaptivity of AdaNormalHedge in the number of experts, our regret bound depends on the total number of distinct traversed edges so far, instead of the total number of edges of the decision tree as in Freund et al. [1997] which could be exponentially larger. Concrete comparisons can be found in Section 6.\nRelated work. While competing with any unknown competitor simultaneously is relatively easy in the log loss setting [Littlestone and Warmuth, 1994, Adamskiy et al., 2012, Koolen et al., 2012], it is much harder in the bounded loss setting studied here. The well-known exponential weights algorithm gives the optimal results only when the learning rate is optimally tuned in terms of the competitor [Freund and Schapire, 1999]. Chernov and Vovk [2010] also studied \u01eb-quantile regret, but no concrete algorithm was provided. Several work considers competing with unknown competitors in a different unconstrained linear optimization setting [Streeter and Mcmahan, 2012, Orabona, 2013, McMahan and Orabona, 2014, Orabona, 2014]. Jadbabaie et al. [2015] studied general adaptive online learning algorithms against time-varying competitors, but with different and incomparable measurement of the hardness of the problem. As far as we know, none of the existing algorithms enjoys all the nice properties discussed in this work at the same time as our algorithms do."}, {"heading": "2 The Expert Problem and NormalHedge.DT", "text": "In the expert problem, on each round t = 1, . . . , T : the player first chooses a distribution pt over N experts, then the adversary decides each expert\u2019s loss \u2113t,i \u2208 [0, 1], and reveals these losses to the player. At the end of this round, the player suffers the weighted average loss \u2113\u0302t = pt \u00b7 \u2113t with \u2113t = (\u2113t,1, . . . , \u2113t,N ). We denote the instantaneous regret to expert i on round t by rt,i = \u2113\u0302t\u2212 \u2113t,i, the cumulative regret by Rt,i = \u2211t \u03c4=1 r\u03c4,i, and the cumulative loss by Lt,i = \u2211t\n\u03c4=1 \u2113\u03c4,i. Throughout the paper, a bold letter denotes a vector with N corresponding coordinates. For example, rt, Rt and Lt represent (rt,1, . . . , rt,N ), (Rt,1, . . . , Rt,N ) and (Lt,1, . . . , Lt,N ) respectively.\nUsually, the goal of the player is to minimize the regret to the best expert, that is, maxi RT,i. Here we consider a more general case where the player wants to minimize the regret to an arbitrary convex combination of experts: RT (u) = \u2211T t=1 u \u00b7 rt where the competitor u is a fixed unknown distribution over\nthe experts. In other words, this regret measures the difference between the player\u2019s loss and the loss that he would have suffered if he used a constant strategy u all the time. Clearly, RT (u) can be written as u \u00b7 RT and can then be upper bounded appropriately by a bound on each RT,i (for example, maxi RT,i). However, our goal is to get a better and more refined bound on RT (u) that depends on u. More importantly, we aim to achieve this without knowing the competitor u ahead of time. When it is clear from the context, we drop the subscript T in RT (u).\nIn fact, in Section 5, we will consider an even more general notion of regret introduced in Herbster and Warmuth [2001], where we allow the competitor to vary over time and to have different scales. Specifically, let u1, . . . ,uT be T different vectors with N nonnegative coordinates (denoted by u1:T ). Then the regret of the player to this sequence of competitors is R(u1:T ) = \u2211T t=1 ut\u00b7rt. If all these competitors are distributions (which they are not required to be), then this regret captures a very natural and general concept of comparing the player\u2019s strategy to any other strategy. Again, we are interested in developing low-regret algorithms that do not need to know any information of this sequence of competitors beforehand.\nWe briefly describe a recent algorithm for the expert problem, NormalHedge.DT [Luo and Schapire, 2014] (a variant of NormalHedge [Chaudhuri et al., 2009]), before we introduce our new improved variants. On round t, NormalHedge.DT sets pt,i \u221d exp ( [Rt\u22121,i+1] 2 +\n3t\n) \u2212exp ( [Rt\u22121,i\u22121] 2 +\n3t\n)\n,where [x]+ = max{0, x}. Let \u01eb \u2208 (0, 1] and competitor u\u2217\u01eb be a distribution that puts all the mass on the \u2308N\u01eb\u2309-th best expert, that is, the one that ranks \u2308N\u01eb\u2309 among all experts according to their total loss LT,i from the smallest to the largest. Then the regret guarantee for NormalHedge.DT states R(u\u2217\u01eb) \u2264 O ( \u221a T ln ( lnT \u01eb ) ) simultaneously for all \u01eb, which means the algorithm suffers at most this amount of regret for all but an \u01eb fraction of the experts. Note that this bound does not depend on N at all. This is the first concrete algorithm with this kind of adaptive property (the original NormalHedge [Chaudhuri et al., 2009] still has a weak dependence on N ). In fact, as we will show later, one can even extend the results to any competitor u. Moreover, we will improve NormalHedge.DT so that it has a much smaller regret when the problem is \u201ceasy\u201d in some sense.\nNotation. We use [N ] to denote the set {1, . . . , N}, \u2206N to denote the simplex of all distributions over [N ], and RE(\u00b7 || \u00b7) to denote the relative entropy between two distributions, Also define L\u0303t,i = \u2211t \u03c4=1[\u2113\u03c4,i\u2212 \u2113\u0302\u03c4 ]+. Many bounds in this work will be in terms of L\u0303T,i, which is always at most LT,i since trivially [\u2113t,i\u2212 \u2113\u0302t]+ \u2264 \u2113t,i. We consider \u201clog log\u201d terms to be nearly constant, and use O\u0302() notation to hide these terms. Indeed, as pointed out by Chernov and Vovk [2010], ln lnx is smaller than 4 even when x is as large as the age of the universe expressed in microseconds (\u2248 4.3 \u00d7 1017)."}, {"heading": "3 A New Algorithm: AdaNormalHedge", "text": "We start by writing NormalHedge.DT in a general form. We define potential function \u03a6(R,C) = exp ( [R]2+ 3C ) with \u03a6(0, 0) defined to be 1, and also a weight function with respect to this potential:\nw(R,C) = 1\n2 (\u03a6(R+ 1, C + 1)\u2212 \u03a6(R\u2212 1, C + 1)) .\nThen the prediction of NormalHedge.DT is simply to set pt,i to be proportional to w(Rt\u22121,i, Ct\u22121) where Ct = t for all t. Note that Ct is closely related to the regret. In fact, the regret is roughly of order \u221a CT (ignoring the log term). Therefore, in order to get an expert-wise and more refined bound, we replace Ct by Ct,i for each expert so that it captures some useful information for each expert i. There are several possible\nAlgorithm 1 AdaNormalHedge Input: A prior distribution q \u2208 \u2206N over experts (uniform if no prior available). Initialize: \u2200i \u2208 [N ], R0,i = 0, C0,i = 0. for t = 1 to T do\nPredict1 pt,i \u221d qiw(Rt\u22121,i, Ct\u22121,i). Adversary reveals loss vector \u2113t and player suffers loss \u2113\u0302t = pt \u00b7 \u2113t. Set \u2200i \u2208 [N ], rt,i = \u2113\u0302t \u2212 \u2113t,i, Rt,i = Rt\u22121,i + rt,i, Ct,i = Ct\u22121,i + |rt,i|.\nend for\nchoices for Ct,i (discussed at the end of Appendix A), but for now we focus on the one used in our new algorithm: Ct,i = \u2211t \u03c4=1 |r\u03c4,i|, that is, the cumulative magnitude of the instantaneous regrets up to time t. We call this algorithm AdaNormalHedge and summarize it in Algorithm 1. Note that we even allow the player to have a prior distribution q over the experts, which will be useful in some applications as we will see in Section 5. The theoretical guarantee of AdaNormalHedge is stated below.\nTheorem 1. The regret of AdaNormalHedge to any competitor u \u2208 \u2206N is bounded as follows:\nR(u) \u2264 \u221a 3(u \u00b7CT ) (RE(u || q) + lnB + ln(1 + lnN)) = O\u0302( \u221a (u \u00b7CT )RE(u || q)), (1)\nwhere CT = (CT,1, . . . , CT,N ), B = 1 + 32 \u2211 i qi (1 + ln(1 + CT,i)) \u2264 52 + 32 ln(1 + T ). Moreover, if u is a uniform distribution over a subset of [N ], then the regret can be improved to\nR(u) \u2264 \u221a 3(u \u00b7CT ) (RE(u || q) + lnB + 1). (2)\nBefore we prove this theorem (see sketch at the end of this section and complete proof in Appendix A), we discuss some implications of the regret bounds and why they are interesting. First of all, the relative entropy term RE(u || q) captures how close the player\u2019s prior is to the competitor. A bound in terms of RE(u || q) can be obtained, for example, using the classic exponential weights algorithm but requires carefully tuning the learning rate as a function of u. Without knowing u, as far as we know, AdaNormalHedge is the only algorithm that can achieve this.2\nOn the other hand, if q is a uniform distribution, then using bound (2) and the fact CT,i \u2264 T , we get an \u01ebquantile regret bound similar to the one of NormalHedge.DT: R(u\u2217\u01eb ) \u2264 R(uS\u01eb) \u2264 \u221a 3T ( ln ( 1 \u01eb ) + lnB + 1 ) where uS\u01eb is uniform over the top \u2308N\u01eb\u2309 experts. in terms of their total loss LT,i. However, the power of a bound in terms of CT is far more than this. Gaillard et al. [2014] introduced a new second order bound that implies much smaller regret when the problem is easy. It turns out that our seemingly weaker first order bound is also enough to get the exact same results! We state these implications in the following theorem which is essentially a restatement of Theorems 9 and 11 of Gaillard et al. [2014] with weaker conditions.\nTheorem 2. Suppose an expert algorithm guarantees R(u) \u2264 \u221a (u \u00b7CT )A(u) where A(u) is some function of u. Then it also satisfies the following:\n1. Recall LT,i = \u2211T t=1 \u2113t,i and L\u0303T,i = \u2211T t=1[\u2113t,i \u2212 \u2113\u0302t]+. We have\nR(u) \u2264 \u221a 2(u \u00b7 L\u0303T )A(u) +A(u) \u2264 \u221a\n2(u \u00b7 LT )A(u) +A(u). 1If pt,i \u221d 0 happens for all i, predict arbitrarily. 2In fact, one can also derive similar bounds for NormalHedge and NormalHedge.DT using our analysis. See discussion at the\nend of Appendix A.\n2. Suppose the loss vector \u2113t\u2019s are independent random variables and there exists an i\u2217 and some \u03b1 \u2208 (0, 1] such that E[\u2113t,i \u2212 \u2113t,i\u2217 ] \u2265 \u03b1 for any t and i 6= i\u2217. Let ei\u2217 be a distribution that puts all the mass on expert i\u2217. Then we have E[RT,i\u2217 ] \u2264 A(ei\u2217 )\u03b1 , and with probability at least 1 \u2212 \u03b4, RT,i\u2217 \u2264 O\u0302 (\nA(ei\u2217) \u03b1 + 1 \u03b1\n\u221a\nA(ei\u2217) ln 1 \u03b4\n)\n.\nThe proof of Theorem 2 is based on the same idea as in Gaillard et al. [2014], and is included in Appendix B for completeness. For AdaNormalHedge, the term A(u) is 3(RE(u || q) + lnB + ln(1 + lnN)) in general (or smaller for special u as stated in Theorem 1). Applying Theorem 2 we have R(u) = O\u0302 ( \u221a (u \u00b7 L\u0303T )RE(u || q) ) .3 Specifically, if q is uniform and assuming without loss of generality that LT,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 LT,N , then by a similar argument, we have for AdaNormalHedge, R(u\u2217\u01eb ) \u2264 O\u0302 ( \u221a L\u0303T,\u2308N\u01eb\u2309 ln ( 1 \u01eb ) ) for any \u01eb. This answers the open question (in the affirmative) asked by Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether an improvement for small loss can be obtained for \u01eb-quantile regret without knowing \u01eb.\nOn the other hand, when we are in a stochastic setting as stated in Theorem 2, AdaNormalHedge ensures\nRT,i\u2217 \u2264 O\u0302 ( 1 \u03b1 ln ( 1 qi\u2217 )) in expectation (or with high probability with an extra confidence term), which does not grow with T . Therefore, the new regret bound in terms of CT actually leads to significant improvements compared to NormalHedge.DT.\nComparison to Adapt-ML-Prod [Gaillard et al., 2014]. Adapt-ML-Prod enjoys a second order bound in terms of\n\u2211T t=1 r 2 t,i, which is always at most the term \u2211T t=1 |rt,i| appeared in our bounds.4 However, on\none hand, as discussed above, these two bounds have the same improvements when the problem is easy in several senses; on the other hand, Adapt-ML-Prod does not provide a bound in terms of RE(u || q) for an unknown u. In fact, as discussed at the end of Section A.3 of Gaillard et al. [2014], Adapt-ML-Prod cannot improve by exploiting a good prior q (or at least its current analysis cannot). Specifically, while the regret for AdaNormalHedge does not have an explicit dependence on N and is much smaller when the prior q is close to the competitor u, the regret for Adapt-ML-Prod always has a lnN multiplicative term for \u2211T\nt=1 r 2 t,i, which means even a good prior results in the same regret as a uniform prior! More advantages of\nAdaNormalHedge over Adapt-ML-Prod will be discussed in concrete examples in following sections.\nProof sketch of Theorem 1. The analysis of NormaHedge.DT is based on the idea of converting the expert problem into a drifting game [Schapire, 2001, Luo and Schapire, 2014]. Here, we extract and simplify the key idea of their proof and also improve it to form our analysis. The main idea is to show that the weighted sum of potentials does not increase much on each round using an improved version of Lemma 2 of Luo and Schapire [2014]. In fact, we show that the final potential\n\u2211N i=1 qi\u03a6(RT,i, CT,i)\nis exactly bounded by B (defined in Theorem 1). From this, assuming without loss of generality that q1\u03a6(RT,1, CT,1) \u2265 \u00b7 \u00b7 \u00b7 \u2265 qN\u03a6(RT,N , CT,N ), we have qi\u03a6(RT,i, CT,i) \u2264 Bi for all i, which, by solving for RT,i, gives RT,i \u2264 \u221a 3CT,i ln (\nB iqi\n)\n. Multiplying both sides by ui, summing over N and applying\nthe Cauchy-Schwarz inequality, we arrive at R(u) \u2264 \u221a 3(u \u00b7CT )(D(u || q) + lnB), where we define 3We see O(RE(u || q)) as a minor term and hide it in the big O notation, which is not completely rigorous but will ease the presentation. Same thing happens for other first order bounds in this work. 4We briefly discuss the difficulty of getting a similar second order bound for our algorithm at the end of Appendix A.\nD(u || q) = \u2211Ni=1 ui ln ( 1 iqi )\n. It remains to show that D(u || q) and RE(u || q) are close by standard analysis and Stirling\u2019s formula."}, {"heading": "4 Confidence-rated Advice and Sleeping Experts", "text": "In this section, we generalize AdaNormalHedge to deal with experts that make confidence-rated advice, a setting that subsumes many interesting applications as studied by Blum [1997] and Freund et al. [1997]. In this general setting, on each round t, each expert first reports its confidence It,i \u2208 [0, 1] for the current task. The player then predicts pt as usual with an extra yet natural restriction that if It,i = 0 then pt,i = 0. That is, the player has to ignore those experts who abstain from making advice (by reporting zero confidence). After that, the loss \u2113t,i for those experts who did not abstain (i.e. It,i 6= 0) are revealed and the player still suffers loss \u2113\u0302t = pt \u00b7 \u2113t. We redefine the instantaneous regret rt,i to be It,i(\u2113\u0302t \u2212 \u2113t,i), that is, the difference between the loss of the player and expert i weighted by the confidence. The goal of the player is, as before, to minimize cumulative regret to any competitor u: R(u) = \u2211\nt\u2264T u \u00b7rt. Clearly, the classic expert problem that we have studied in previous sections is just a special case of this general setting with It,i = 1 for all t and i.\nMoreover, with this general form of rt,i, AdaNormalHedge can be used to deal with this general setting with only one simple change of scaling the weights by the confidence:\npt,i \u221d qiIt,iw(Rt\u22121,i, Ct\u22121,i), (3)\nwhere Rt,i and Ct,i is still defined to be \u2211t \u03c4=1 r\u03c4,i and \u2211t \u03c4=1 |r\u03c4,i| respectively. The constraint It,i = 0 \u21d2 pt,i = 0 is clearly satisfied. In fact, Algorithm 1 can be seen as a special case of this general form of AdaNormalHedge with It,i \u2261 1. Furthermore, the regret bounds in Theorem 1 still hold without any changes, which are summarized below (proof deferred to Appendix A).\nTheorem 3. For the confidence-rated expert problem, regret bounds (1) and (2) still hold for general AdaNormalHedge (Eq. (3)).\nPreviously, Gaillard et al. [2014] studied a general reduction from an expert algorithm to a confidencerated expert algorithm. Applying those results here gives the exact same algorithm and regret guarantee mentioned above. However, we point out that the general reduction is not always applicable. Specifically, it is invalid if there is an unknown number of experts in the confidence-rated setting (explained more in the next paragraph) while the expert algorithm in the standard setting requires knowing the number of experts as a parameter. This is indeed the case for most algorithms (including Adapt-ML-Prod and even the original NormalHedge by Chaudhuri et al. [2009]). AdaNormalHedge naturally avoids this problem since it does not depend on N at all.\nSleeping Experts. We are especially interested in the case when It,i \u2208 {0, 1}, also called the specialist/sleeping expert problem where It,i = 0 means that expert i is \u201casleep\u201d for round t and not making any advice. This is a natural setting where the total number of experts is unknown ahead of time. Indeed, the number of awake experts can be dynamically changing over time. An expert that has never appeared before should be thought of as being asleep for all previous rounds.\nAdaNormalHedge is a very suitable algorithm to deal with this case due to its independence of the total number of experts. If an expert i appears for the first time on round t, then by definition it will naturally start with Rt\u22121,i = 0 and Ct\u22121,i = 0. Although we state the prior q as a distribution, which seems to\nrequire knowing the total number of experts, it is not an issue algorithmically since q is only used to scale the unnormalized weights (Eq. (3)). For example, if we want q to be a uniform distribution over N experts where N is unknown beforehand, then to run AdaNormalHedge we can simply treat qi in Eq. (3) to be 1 for all i, which clearly will not change the behavior of the algorithm anyway. In this case, if we let Nt denote the total number of distinct experts that have been seen up to time t and the competitor u concentrates on any of these experts, then the relative entropy term in the regret (up to time t) will be lnNt (instead of lnN ), which is changing over time.\nUsing the adaptivity of AdaNormalHedge in both the number of experts and the competitor, we provide improved results for two instances of the sleeping expert problem in the next two sections."}, {"heading": "5 Time-Varying Competitors", "text": "In this section, we study a more challenging goal of competing with time-varying competitors in the standard expert setting (that is, each expert is always awake and again rt,i = \u2113\u0302t\u2212 \u2113t,i), which turns out to be reducible to a sleeping expert problem. Results for this section are summarized in Table 1."}, {"heading": "5.1 Special Cases: Adaptive Regret and Tracking the Best Expert", "text": "We start from a special case: adaptive regret, introduced by Hazan and Seshadhri [2007] to better capture changing environments. Formally, consider any time interval t = t1, . . . , t2, and let R[t1,t2],i = \u2211t2 t=t1 rt,i be the regret to expert i on this interval (similarly define L[t1,t2],i = \u2211t2 t=t1 \u2113t,i and C[t1,t2],i = \u2211t2 t=t1\n|rt,i|). The goal of the player is to obtain relatively small regret on any interval. Freund et al. [1997] essentially introduced a way to reduce this problem to a sleeping expert problem, which was later improved by Adamskiy et al. [2012]. Specifically, for every pair of time t and expert i, we create a sleeping expert, denoted by (t, i), who is only awake after (and including) round t and since then suffers the same loss as the original expert i. So we have Nt sleeping experts in total on round t. The prediction pt,i is set to be the sum of all the weights of sleeping expert (\u03c4, i) (\u03c4 = 1, . . . , t). It is clear that doing this ensures that the cumulative regret up to time t2 with respect to sleeping expert (t1, i) is exactly R[t1,t2],i in the original problem.\nThis is a sleeping expert problem for which AdaNormalHedge is very suitable, since the number of sleeping experts keeps increasing and the total number of experts is in fact unknown if the horizon T is unknown. Theorem 3 implies that the resulting algorithm gives the following adaptive regret:\nR[t1,t2],i = O\u0302\n( \u221a\n( \u2211t2 t=t1 |rt,i| )\nln (\n1 q(t1,i)\n)\n)\n= O\u0302\n(\n\u221a\n( \u2211t2 t=t1 |rt,i| ) ln (Nt1)\n)\n,\nwhere q is a prior over the Nt2 experts and the last step is by setting the prior to be q(t,i) \u221d 1/t2 for all t and i.5 This prior is better than a simple uniform distribution which leads to a term ln(Nt2) instead of ln(Nt1). We call this algorithm AdaNormalHedge.TV.6 To be concrete, on round t AdaNormalHedge.TV predicts pt,i \u221d \u2211t \u03c4=1 1 \u03c42 w ( R[\u03c4,t\u22121],i, C[\u03c4,t\u22121],i ) .\nAgain, Theorem 2 can be applied to get a more interpretable bound O\u0302 ( \u221a L\u0303[t1,t2],i ln (Nt1) ) where\nL\u0303[t1,t2],i = \u2211t2 t=t1 [\u2113t,i\u2212 \u2113\u0302t,i]+ \u2264 L[t1,t2],i, and a much smaller bound O\u0302\n(\nln(Nt1) \u03b1\n)\nif the losses are stochastic\non interval [t1, t2] in the sense stated in Theorem 2.\n5Note that as discussed before, the fact that t2 is unknown and thus q is unknown does not affect the algorithm. 6\u201cTV\u201d stands for \u201ctime-varying\u201d.\nOne drawback of AdaNormalHedge.TV is that its time complexity per round is O(Nt) and the overall space is O(NT ). However, the data streaming technique used in Hazan and Seshadhri [2007] can be directly applied here to reduce the time and space complexity to O(N ln t) and O(N lnT ) respectively, with only an extra multiplicative O( \u221a ln(t2 \u2212 t1)) factor in the regret.\nTracking the best expert. In fact, AdaNormalHedge.TV is a solution for one of the open problems proposed by Warmuth and Koolen [2014]. Adaptive regret immediately implies the so-called K-shifting regret for the problem of tracking the best expert in a changing environment. Formally, define the K-shifting regret RK-Shift to be max \u2211K k=1R[tk\u22121+1,tk],ik where the max is taken over all i1, \u00b7 \u00b7 \u00b7 , iK \u2208 [N ] and 0 = t0 < t1 < \u00b7 \u00b7 \u00b7 < tK = T . In other words, the player is competing with the best K-partition of the whole game and the best expert on each of these partitions. Let L\u2217K-Shift = max \u2211K k=1 L[tk\u22121+1,tk ],ik be the total loss of such best partition (that is, the max is taken over the same space), and similarly define L\u0303\u2217K-Shift = max \u2211K k=1 L\u0303[tk\u22121+1,tk ],ik \u2264 L\u2217K-Shift. Since essentially RK-Shift is just the sum of K adaptive regrets, using the bounds discussed above and the Cauchy-Schwarz inequality, we conclude that AdaNormalHedge.TV ensures RK-Shift = O\u0302 ( \u221a KL\u0303\u2217K-Shift ln(NT ) ) . Also, if the loss vectors are generated randomly on these K intervals, each satisfying the condition stated in Theorem 2, then the regret is RK-Shift = O\u0302 ( K ln(NT ) \u03b1 ) in expectation (high probability bound is similar). These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007]. This is exactly what was asked in Warmuth and Koolen [2014]: whether there is an algorithm that can do optimally for both adversarial and stochastic losses in the problem of tracking the best expert. AdaNormalHedge.TV achieves this goal without knowing K or any other information, while the solution provided by Sani et al. [2014] needs to know K , L\u2217K-Shift and \u03b1 to get the same adversarial bound and a worse stochastic bound of order O(1/\u03b12).\nComparison to previous work. For adaptive regret, the FLH algorithm by Hazan and Seshadhri [2007] treats any standard expert algorithm as a sleeping expert, and has an additive term O( \u221a t2 ln t2) in addition to\nthe base algorithm\u2019s regret (when no prior information is available), which adds up to a large O(K \u221a T lnT ) term for K-shifting regret. Due to this extra additive regret, FLH also does not enjoy first order bounds nor small regret in the stochastic setting, even if the base algorithm that it builds on provides these guarantees. On the other hand, FLH was proposed to achieve adaptive regret for any general online convex optimization problem. We point out that using AdaNormalHedge as the master algorithm in their framework will give similar improvements as discussed here.\nAdapt-ML-Prod is not directly applicable here for the corresponding sleeping expert problem since the total number of experts is unknown.\nAnother well-studied algorithm for this problem is \u201cfixed share\u201d. Several works on fixed share for the simpler \u201clog loss\u201d setting were studied before [Herbster and Warmuth, 1998, Bousquet and Warmuth, 2003, Adamskiy et al., 2012, Koolen et al., 2012]. Cesa-Bianchi et al. [2012] studied a generalized fixed share algorithm for the bounded loss setting considered here. When t2\u2212t1 and L[t1,t2],i are known, their algorithm ensures R[t1,t2],i = O (\u221a L[t1,t2],i ln(N(t2 \u2212 t1)) ) for adaptive regret, and when K , T and L\u2217K-Shift are known, they have RK-Shift = O (\u221a KL\u2217K-Shift ln(NT/K) )\n. No better result is provided for the stochastic setting. More importantly, when no prior information is known, which is the case in practice, the best results one can extract from their analysis are R[t1,t2],i = O( \u221a t2 ln(Nt2)) and RK-Shift = O(K \u221a\nT ln(NT )), which are much worse than our results.\n7For fair comparison, we only consider the case when no prior information (e.g. t1, t2, V (u1:T ) etc) is known."}, {"heading": "5.2 General Time-Varying Competitors", "text": "We finally discuss the most general goal: compete with different ut on different rounds. Recall R(u1:T ) = \u2211T\nt=1 ut \u00b7 rt where ut \u2208 RN+ for all t (note that ut does not even have to be a distribution). Clearly, adaptive regret and K-shifting regret are special cases of this general notion. Intuitively, how large this regret is should be closely related to how much the competitor\u2019s sequence u1:T varies. Cesa-Bianchi et al. [2012] introduced a distance measurement to capture this variation: V (u1:T ) = \u2211T t=1 \u2211N i=1[ut,i\u2212ut\u22121,i]+ where we define u0,i = 0 for all i. Also let \u2016u1:T \u2016 = \u2211T t=1 \u2016ut\u20161 and L(u1:T ) = \u2211T\nt=1 ut \u00b7\u2113t. Fixed share is shown to ensure the following regret [Cesa-Bianchi et al., 2012]: R(u1:T ) = O ( \u221a V (u1:T )L(u1:T ) ln (N\u2016u1:T \u2016/V (u1:T )) ) when V (u1:T ), L(u1:T ) and \u2016u1:T \u2016 are known. No result was provided otherwise.9 Here, we show that our parameter-free algorithm AdaNormalHedge.TV actually achieves almost the same bound without knowing any information beforehand. Moreover, while the results in Cesa-Bianchi et al. [2012] are specific for the fixed share algorithm, we prove the following results which are independent of the concrete algorithms and may be of independent interest.\nTheorem 4. Suppose an expert algorithm ensures R[t1,t2],i \u2264 \u221a A \u2211t2 t=t1 zt,i for any t1, t2 and i, where zt,i \u2265 0 can be anything depending on t and i (e.g. |rt,i|, [\u2113t,i \u2212 \u2113\u0302t]+, \u2113t,i or constant 1), and A is a term independent of t1, t2 and i. Then this algorithm also ensures\nR(u1:T ) \u2264 \u221a AV (u1:T ) \u2211T t=1 ut \u00b7 zt.\nSpecially, for AdaNormalHedge.TV, plugging A = O\u0302(ln(NT )) and zt,i = [\u2113t,i \u2212 \u2113\u0302t]+ gives R(u1:T ) = O\u0302 ( \u221a V (u1:T )L\u0303(u1:T ) ln (NT ) ) , where L\u0303(u1:T ) = \u2211T\nt=1 \u2211N i=1 ut,i[\u2113t,i \u2212 \u2113\u0302t]+.\nThe key idea of the proof is to rewrite R(u1:T ) as a weighted sum of several adaptive regrets in an optimal way (see Appendix C for the complete proof). This theorem tells us that while playing with timevarying competitors seems to be a harder problem, it is in fact not any harder than its special case: achieving adaptive regret on any interval. Although the result is independent of the algorithms, one still cannot derive bounds on R(u1:T ) for FLH or fixed share based on their adaptive regret bounds, because when no prior information is available, the bounds on R[t1,t2],i for these algorithms are of order O( \u221a t2) instead of\nO( \u221a t2 \u2212 t1), which is not good enough. We refer the reader to Table 1 for a summary of this section.\n8The choice of the base algorithm does not matter since the dominating term in the regret comes from FLH itself. 9Although in Section 7.3 of Cesa-Bianchi et al. [2012], the authors mentioned online tuning technique for the parameters, it\nonly works for special cases (e.g. adaptive regret)."}, {"heading": "6 Competing with the Best Pruning Tree", "text": "We now turn to our second application on predicting almost as well as the best pruning tree within a template tree. This problem was studied in the context of online learning by Helmbold and Schapire [1997] using the approach of Willems et al. [1993, 1995]. It is also called the tree expert problem in Cesa-Bianchi and Lugosi [2006]. Freund et al. [1997] proposed a generic reduction from a tree expert problem to a sleeping expert problem. Using this reduction with our new algorithm, we provide much better results compared to previous work (summarized in Table 2).\nSpecifically, consider a setting where on each round t, the predictor has to make a decision yt from some convex set Y given some side information xt, and then a convex loss function ft : Y \u2192 [0, 1] is revealed and the player suffers loss ft(yt). The predictor is given a template tree G to consult. Starting from the root, each node of G performs a test on xt to decide which of its child should perform the next test, until a leaf is reached. In addition to a test, each node (except the root) also makes a prediction based on xt. A pruning tree P is a tree induced by replacing zero or more nodes (and associated subtrees) of G by leaves. The prediction of a pruning tree P given xt, denoted by P(xt), is naturally defined as the prediction of the leaf that xt reaches by traversing P. The player\u2019s goal is thus to predict almost as well as the best pruning tree in hindsight, that is, to minimize RG = \u2211T t=1 ft(yt)\u2212minP \u2211T t=1 ft(P(xt)).\nThe idea of the reduction introduced by Freund et al. [1997] is to view each edge of G as a sleeping expert (indexed by e), who is awake only when traversed by xt, and in that case predicts yt,e, the same prediction as the child node that it connects to. The predictor runs a sleeping expert algorithm with loss \u2113t,e = ft(yt,e), and eventually predicts yt = \u2211\ne\u2208E pt,eyt,e where E denotes the set of edges of G and pt,e (e \u2208 E) is the output of the expert algorithm; thus by convexity of ft, we have ft(yt) \u2264 \u2211\ne\u2208E pt,eft(yt,e) = \u2113\u0302t. Note that we only care about the predictions of those awake experts since otherwise pt,e is required to be zero. Now let P\u2217 be one of the best pruning trees, that is, P\u2217 \u2208 argminP \u2211T t=1 ft(P(xt)), and m be the number of leaves of P\u2217. In the expert problem, we will set the competitor u to be a uniform distribution over the m terminal edges (that is, the ones connecting the leaves) of P\u2217, and the prior q to be a uniform distribution over all the edges. Since on each round, one and only one of those m experts is awake, and its prediction is exactly P\u2217(xt), we have R(u) = \u2211T t=1 1 m (\u2113\u0302t \u2212 ft(P\u2217(xt))), and therefore RG \u2264 mR(u).\nIt remains to pick a concrete sleeping algorithm to apply. There are two reasons that make AdaNormalHedge very suitable for this problem. First, since m is clearly unknown ahead of time, we are competing with an unknown competitor u, which is exactly what AdaNormalHedge can deal with. Second, the number of awake experts is dynamically changing, and as discussed before, in this case AdaNormalHedge enjoys a regret bound that is adaptive in the the number of experts seen so far. Formally, recall the notation Nt, which in this case represents the total number of distinct traversed edges up to round t. Then by Theorem 3, we have\nRG = O\u0302(m \u221a (u \u00b7CT )RE(u || q)) = O\u0302 ( \u221a m ( \u2211T t=1 |\u2113\u0302t \u2212 ft(P\u2217(xt))| ) ln ( NT m ) ) ,\nwhich, by Theorem 2, implies RG = O\u0302 (\n\u221a\nmL\u0303\u2217 ln(NT /m) ) where L\u0303\u2217 = \u2211T\nt=1[ft(P\u2217(xt)) \u2212 \u2113\u0302t]+, which is at most the total loss of the best pruning tree L\u2217 =\n\u2211T t=1 ft(P\u2217(xt)). Moreover, the algorithm is efficient:\nthe overall space requirement is O(NT ), and the running time on round t is O(\u2016xt\u2016G) where we use \u2016xt\u2016G to denote the number of edges that xt traverses.\nComparison to other solutions. The work by Freund et al. [1997] considers a variant of the exponential weights algorithm in a \u201clog loss\u201d setting, and is not directly applicable here (specifically it is not clear how\nto tune the learning rate appropriately). A better choice is the Adapt-ML-Prod algorithm by Gaillard et al. [2014] (the version for the sleeping expert problem). However, there is still one issue for this algorithm: it does not give a bound in terms of RE(u || q) for an unknown u.10 So to get a bound on R(u), the best thing to do is to use the definition R(u) = u \u00b7RT and a bound on each RT,i. In short, one can verify that AdaptML-Prod ensures regret R(u) = O\u0302 ( \u221a mL\u0303\u2217 lnN )\nwhere N = |E| is the total number of edges/experts. We emphasize that N can be much larger than NT when the tree is huge. Indeed, while NT is at most T times the depth of G, N could be exponentially large in the depth. The running time and space of Adapt-ML-Prod for this problem, however, is the same as AdaNormalHedge.\nWe finally compare with a totally different approach [Helmbold and Schapire, 1997], where one simply treats each pruning tree as an expert and run the exponential weights algorithm. Clearly the number of experts is exponentially large, and thus the running time and space are unacceptable by a naive implementation. This issue is avoided by using a clever dynamic programming technique. If L\u2217 and m are known ahead of time, then the regret for this algorithm is O( \u221a mL\u2217) by tuning the learning rate optimally. As discussed in Freund et al. [1997], the linear dependence on m in this bound is much better than the one of the form O (\u221a mL\u2217 lnN ) , which, in the worst case, is linear in m. This was considered as the main drawback of\nusing the sleeping expert approach. However, the bound for AdaNormalHedge is O\u0302 (\n\u221a\nmL\u0303\u2217 ln(NT /m) ) ,\nwhich is much smaller as discussed previously and in fact comparable to O( \u221a mL\u2217). More importantly, L\u2217 and m are unknown in practice. In this case, no sublinear regret is known for this dynamic programming approach, since it relies heavily on the fact that the algorithm is using a fixed learning rate and thus the usual time-varying learning rate methods cannot be applied here. Therefore, although theoretically this approach gives small regret, it is not a practical method. The running time is also slightly worse than the sleeping expert approach. For simplicity, suppose every internal node has d children. Then the time complexity per round is O(d\u2016x\u2016G). The overall space requirement is O(Nt), the same as other approaches. Again, see Table 2 for a summary of this section.\nFinally, as mentioned in Freund et al. [1997], the sleeping expert approach can be easily generalized to predicting with a decision graph. In that case, AdaNormalHedge still enjoys all the improvements discussed in this section (details omitted).\n10In fact, even if it does, this term is still dominated by a lnN term. See the discussion at the end of Section A.3 of Gaillard et al. [2014] that we already mentioned at Section 3."}, {"heading": "A Complete proofs of Theorem 1 and 3", "text": "We need the following two lemmas. The first one is an improved version of Lemma 2 of Luo and Schapire [2014].\nLemma 1. For any R \u2208 R, C \u2265 0 and r \u2208 [\u22121, 1], we have\n\u03a6(R+ r, C + |r|) \u2264 \u03a6(R,C) + w(R,C)r + 3|r| 2(C + 1) .\nProof. We first argue that \u03a6(R + r, C + |r|), as a function of r, is piecewise-convex on [\u22121, 0] and [0, 1]. Since the value of the function is 1 when R + r < 0 and is at least 1 otherwise. It suffices to only consider the case when R+ r \u2265 0. On the interval [0, 1], we can rewrite the exponent (ignoring the constant 13 ) as:\n(R+ r)2\nC + r = (C + r) + (R\u2212 C)2 C + r + 2(R\u2212 C),\nwhich is convex in r. Combining with the fact that \u201cif g(x) is convex then exp(g(x)) is also convex\u201d proves that \u03a6(R+ r, C + |r|) is convex on [0, 1]. Similarly when r \u2208 [\u22121, 0], rewriting the exponent as\n(R+ r)2 C \u2212 r = (C \u2212 r) + (R+ C)2 C \u2212 r \u2212 2(R + C)\ncompletes the argument. Now define function f(r) = \u03a6(R+r, C+ |r|)\u2212w(R,C)r. Since f(r) is clearly also piecewise-convex on [\u22121, 0] and [0, 1], we know that the curve of f(r) is below the segment connecting points (\u22121, f(\u22121)) and (0, f(0)) on [\u22121, 0], and also below the segment connecting points (1, f(1)) and (0, f(0)) on [0, 1]. This can be mathematically expressed as:\nf(r) \u2264 max{f(0) + (f(0)\u2212 f(\u22121))r, f(0) + (f(1)\u2212 f(0))r} = f(0) + (f(1)\u2212 f(0))|r|, where we use the fact f(\u22121) = f(1). Now by Lemma 2 of Luo and Schapire [2014], we have\nf(1)\u2212 f(0) = 12 (\u03a6(R+ 1, C + 1) + \u03a6(R\u2212 1, C + 1))\u2212 \u03a6(R,C) \u2264 12 ( exp ( 4 3(C+1) ) \u2212 1 ) ,\nwhich is at most e 4 3 \u22121\n2(C+1) since C is nonnegative and e x \u2212 1 \u2264 ea\u22121 a x for any x \u2208 [0, a]. Noting that\ne 4 3 \u2212 1 \u2264 3 completes the proof.\nThe second lemma makes use of Lemma 1 to show that the weighted sum of potentials does not increase much and thus the final potential is relatively small.\nLemma 2. AdaNormalHedge ensures \u2211N i=1 qi\u03a6(RT,i, CT,i) \u2264 B = 1 + 32 \u2211N i=1 qi (1 + ln(1 + CT,i)). Proof. First note that since AdaNormalHedge predicts pt,i \u221d qiw(Rt\u22121,i, Ct\u22121,i), we have \u2211N\ni=1 qiw(Rt\u22121,i, Ct\u22121,i)rt,i = 0. (4)\nNow applying Lemma 1 with R = Rt\u22121,i, C = Ct\u22121,i and r = rt,i, multiplying the inequality by qi on both sides and summing over i gives \u2211N\ni=1 qi\u03a6(Rt,i, Ct,i) \u2264 \u2211N i=1 qi\u03a6(Rt\u22121,i, Ct\u22121,i) + 3 2 \u2211N i=1 qi|rt,i| Ct\u22121,i+1 .\nWe then sum over t \u2208 [T ] and telescope to show \u2211Ni=1 qi\u03a6(RT,i, CT,i) \u2264 1 + 32 \u2211N i=1 qi \u2211T t=1 |rt,i| Ct\u22121,i+1 . Finally applying Lemma 14 of Gaillard et al. [2014] to show \u2211T\nt=1 |rt,i| Ct\u22121,i+1 \u2264 1 + ln(1 + CT,i) completes\nthe proof.\nWe are now ready to prove Theorem 1 and Theorem 3.\nProof. (of Theorem 1) Assume q1\u03a6(RT,1, CT,1) \u2265 \u00b7 \u00b7 \u00b7 \u2265 qN\u03a6(RT,N , CT,N ) without loss of generality. Then by Lemma 2, it must be true that qi\u03a6(RT,i, CT,i) \u2264 Bi for all i, which, by solving for RT,i, gives RT,i \u2264 \u221a 3CT,i ln (\nB iqi\n)\n. Multiplying both sides by ui, summing over N and applying the Cauchy-Schwarz\ninequality, we arrive at R(u) \u2264\u2211Ni=1 \u221a 3uiCT,i \u00b7 ui ln ( B iqi ) \u2264 \u221a 3(u \u00b7CT )(D(u || q) + lnB), where we define D(u || q) =\u2211Ni=1 ui ln ( 1 iqi )\n. It remains to show that D(u || q) and RE(u || q) are close. Indeed, we have D(u || q) \u2212 RE(u || q) =\u2211Ni=1 ui ln ( 1 iui ) , which, by standard analysis, can be shown to reach its maximum when ui \u221d 1i and the maximum value is ln \u2211 i 1 i \u2264 ln(1 + lnN). This completes the proof for Eq. (1). Finally, when u is in the special form as described in Theorem 1, we have D(u || q) \u2212 RE(u || q) = ln |S| + 1|S| \u2211 i\u2208S ln ( 1 i ) \u2264 ln |S| \u2212 1|S| ln(|S|!). By Stirling\u2019s formula x! \u2265 \u221a 2\u03c0x ( x e )x , we arrive at D(u || q)\u2212 RE(u || q) \u2264 1\u2212 (ln \u221a 2\u03c0|S|)/|S| \u2264 1, proving Eq. (2).\nProof. (of Theorem 3) It suffices to point out that rt,i is still in the interval [\u22121, 1] and Eq. (4) in the proof of Lemma 2 still holds by the new prediction rule Eq. (3). The entire proof for Theorem 1 applies here exactly.\nThe algorithm and the proof can be generalized to Ct,i = \u2211t \u03c4=1 |r\u03c4,i|d for any d \u2208 [0, 1]. Indeed, the only extra work is to prove the convexity of \u03a6(R+ r, C+ |r|d). When d = 0, we recover NormalHedge.DT exactly and get a bound on R(u) for any u (in terms of \u221a T ), instead of just R(u\u2217\u01eb) as in the original work. It is clear that d = 1 gives the smallest bound, which is why we use it in AdaNormalHedge. The ideal choice, however, should be d = 2 so that a second order bound similar to the one of Gaillard et al. [2014] can be obtained. Unfortunately, the function \u03a6(R+ r, C + r2) turns out to not always be piecewise-convex, which breaks our analysis. Whether d = 2 gives a low-regret algorithm and how to analyze it remain an open question."}, {"heading": "B Proof of Theorem 2", "text": "Proof. For the first result, the key observation is u \u00b7CT = R(u)+2u \u00b7LT . We only consider the case when R(u) \u2265 0 since otherwise the statement is trivial. By the condition we thus have R(u)2 \u2264 (R(u) + 2u \u00b7 LT )A(u), which by solving for R(u) gives\nR(u) \u2264 12(A(u) + \u221a A(u)2 + 8(u \u00b7 LT )A(u)) \u2264 \u221a\n2(u \u00b7 L\u0303T )A(u) +A(u), proving the bound we want.\nFor the second result, let Et denote the expectation conditioning on all the randomness up to round t. So by the condition, we have Et[rt,i\u2217 ] = \u2211N i=1 pt,iEt[\u2113t,i \u2212 \u2113t,i\u2217 ] \u2265 \u03b1(1 \u2212 pt,i\u2217), and thus E[RT,i\u2217 ] \u2265 \u03b1S where we define S = \u2211T\nt=1 E[1 \u2212 pt,i\u2217 ]. On the other hand, by convexity we also have |rt,i\u2217 | \u2264 \u2211N\ni=1 pt,i|\u2113t,i \u2212 \u2113t,i\u2217 | \u2264 1 \u2212 pt,i\u2217 and thus E[RT,i\u2217] \u2264 E[ \u221a A(ei\u2217)CT,i\u2217 ] \u2264 \u221a A(ei\u2217)S by the concavity of\nthe square root function. Combining the above two statements gives S \u2264 A(ei\u2217 ) \u03b12 , and plugging this back shows E[RT,i\u2217 ] \u2264 A(ei\u2217 )\u03b1 . The high probability statement follows from the exact same argument of Gaillard et al. [2014] using a martingale concentration lemma."}, {"heading": "C Proof of Theorem 4", "text": "Below we use [s, t] to denote the set {s, s+ 1, . . . , t\u2212 1, t} for 1 \u2264 s \u2264 t \u2264 T and s, t \u2208 N+.\nProof. We first fix an expert i and consider the regret to this expert \u2211T t=1 ut,irt,i. Let aj \u2265 0 and Uj = [sj, tj ] for j = 1, . . . ,M be M positive numbers and M corresponding time intervals such that ut,i = \u2211M\nj=1 aj1{t \u2208 Uj}. Note that this is always possible, with a trivial choice being aj = ut,j , sj = tj = j and M = T ; we will however need a more sophisticated construction specified later. By the adaptive regret guarantee, we have\nT \u2211\nt=1\nut,irt,i =\nM \u2211\nj=1\naj\nT \u2211\nt=1\n1{t \u2208 Uj}rt,i = M \u2211\nj=1\najR[sj ,tj ],i \u2264 M \u2211\nj=1\naj\n\u221a \u221a \u221a \u221aA tj \u2211\nt=sj\nzt,i,\nwhich, by the Cauchy-Schwarz inequality, is at most\n\u221a \u221a \u221a \u221a M \u2211\nj=1\naj \u00b7\n\u221a \u221a \u221a\n\u221aA M \u2211\nj=1\naj\ntj \u2211\nt=sj\nzt,i =\n\u221a \u221a \u221a \u221a M \u2211\nj=1\naj \u00b7\n\u221a \u221a \u221a\n\u221aA M \u2211\nj=1\naj\nT \u2211\nt=1\n1{t \u2208 Uj}zt,i =\n\u221a \u221a \u221a \u221a M \u2211\nj=1\naj \u00b7\n\u221a \u221a \u221a\n\u221aA T \u2211\nt=1\nut,izt,i.\nTherefore, we need a construction of aj and Uj such that \u2211M j=1 aj is minimized. This is addressed in Lemma 3 below which shows that there is in fact always a (optimal) construction such that \u2211M\nj=1 aj is\nexactly \u2211T t=1[ut,i \u2212 ut\u22121,i]+. Now summing the resulting bound over all experts and applying the CauchySchwarz inequality again proves the theorem.\nLemma 3. Let v1, . . . , vT be T nonnegative numbers and h({v1, . . . , vT }) = min \u2211M\nj=1 aj where the minimum is taken over the set of all possible choices of M \u2208 N+, aj > 0, Uj = [sj, tj ] with 1 \u2264 sj \u2264 tj \u2264 T, sj, tj \u2208 N+ (j = 1, . . . ,M) such that vt = \u2211M j=1 aj1{t \u2208 Uj} for all t. Then with v0 defined to be 0 we have\nh({v1, . . . , vT }) = T \u2211\nt=1\n[vt \u2212 vt\u22121]+.\nProof. We prove the lemma by induction on T . The base case T = 1 is trivial. Now assume the lemma is true for any number of v\u2019s smaller than T . Suppose aj , Uj = [sj, tj ] (j = 1, . . . ,M) is an optimal construction. Let t\u2217 \u2208 argmint vt. Without loss of generality assume t\u2217 belongs and only belongs to U1, . . . , Uk for some k. By the definition of h, we must have\nh({v1, . . . , vT }) = vt\u2217 + h({v1 \u2212 w1, . . . , vt\u2217\u22121 \u2212 wt\u2217\u22121}) + h({vt\u2217+1 \u2212 wt\u2217+1, . . . , vT \u2212 wT }),\nwhere we define wt = \u2211k j=1 aj1{t \u2208 Uj} and h(\u2205) = 0 . (Note that we also use the fact that vt\u2217 = mint vt so that vt \u2212wt \u2265 vt \u2212 vt\u2217 is always nonnegative as required.) Now the key idea is to show that \u201cextending\u201d each Uj (j \u2208 [k]) to the entire time interval [1, T ] does not increase the objective value in some sense. First consider extending U1. By the inductive assumption, we have\nh({v1 \u2212 w1 \u2212 a1, . . . , vs1\u22121 \u2212ws1\u22121 \u2212 a1, vs1 \u2212 ws1 , . . . , vt\u2217\u22121 \u2212 wt\u2217\u22121}\n= (v1 \u2212 w1 \u2212 a1) + ( s1\u22121 \u2211\nt=2\n[(vt \u2212 wt \u2212 a1)\u2212 (vt\u22121 \u2212 wt\u22121 \u2212 a1)]+ )\n+ [(vs1 \u2212 ws1)\u2212 (vs1\u22121 \u2212 ws1\u22121 \u2212 a1)]+ + ( T \u2211\nt=s1+1\n[(vt \u2212 wt)\u2212 (vt\u22121 \u2212 wt\u22121)]+ )\n= h({v1 \u2212 w1, . . . , vt\u2217\u22121 \u2212 wt\u2217\u22121}) + [(vs1 \u2212 ws1)\u2212 (vs1\u22121 \u2212 ws1\u22121 \u2212 a1)]+ \u2212 ([(vs1 \u2212 ws1)\u2212 (vs1\u22121 \u2212 ws1\u22121)]+ + a1) \u2264 h({v1 \u2212 w1, . . . , vt\u2217\u22121 \u2212 wt\u2217\u22121}),\nwhere the inequality follows from the fact [b+ c]+ \u2264 [b]+ + c. Similarly, we also have\nh({vt\u2217+1 \u2212 wt\u2217+1, . . . , vt1 \u2212 wt1 , vt1+1 \u2212 wt1+1 \u2212 a1, . . . , vT \u2212 wT \u2212 a1} \u2264 h({vt\u2217+1 \u2212 wt\u2217+1, . . . , vT \u2212 wT }).\nBy extending U2, . . . , Uk one by one in a similar way, we arrive at\nh({v1, . . . , vT }) \u2265 vt\u2217 + h({v1 \u2212 vt\u2217 , . . . , vt\u2217\u22121 \u2212 vt\u2217}) + h({vt\u2217+1 \u2212 vt\u2217 , . . . , vT \u2212 vt\u2217}).\nNotice that the right hand side of the above inequality admits a valid construction for {v1, . . . , vT }: an interval U = [1, T ] with weight a = vt\u2217 , together with the optimal constructions for {v1 \u2212 vt\u2217 , . . . , vt\u2217\u22121 \u2212 vt\u2217} and {vt\u2217+1 \u2212 vt\u2217 , . . . , vT \u2212 vt\u2217}. Therefore, by the optimality of h, the above inequality must be an equality. By using the inductive assumption again, we thus have\nh({v1, . . . , vT }) = vt\u2217 + h({v1 \u2212 vt\u2217 , . . . , vt\u2217\u22121 \u2212 vt\u2217}) + h({vt\u2217+1 \u2212 vt\u2217 , . . . , vT \u2212 vt\u2217})\n= vt\u2217 +\n(\nv1 \u2212 vt\u2217 + t\u2217\u22121 \u2211\nt=2\n[vt \u2212 vt\u22121]+ ) + ( vt\u2217+1 \u2212 vt\u2217 + T \u2211\nt=t\u2217+2\n[vt \u2212 vt\u22121]+ )\n=\n(\nt\u2217\u22121 \u2211\nt=1\n[vt \u2212 vt\u22121]+ ) + ( T \u2211\nt=t\u2217+1\n[vt \u2212 vt\u22121]+ )\n= T \u2211\nt=1\n[vt \u2212 vt\u22121]+,\nwhere the last step follows from [vt\u2217 \u2212 vt\u2217\u22121]+ = 0 by the definition of t\u2217. This completes the proof."}], "references": [{"title": "A closer look at adaptive regret", "author": ["Dmitry Adamskiy", "Wouter M Koolen", "Alexey Chernov", "Vladimir Vovk"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Adamskiy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Adamskiy et al\\.", "year": 2012}, {"title": "Empirical support for Winnow and Weighted-Majority algorithms: Results on a calendar scheduling domain", "author": ["Avrim Blum"], "venue": "Machine Learning,", "citeRegEx": "Blum.,? \\Q1997\\E", "shortCiteRegEx": "Blum.", "year": 1997}, {"title": "From external to internal regret", "author": ["Avrim Blum", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blum and Mansour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Mansour.", "year": 2007}, {"title": "Tracking a small set of experts by mixing past posteriors", "author": ["Olivier Bousquet", "Manfred K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet and Warmuth.", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "How to use expert advice", "author": ["Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P. Helmbold", "Robert E. Schapire", "Manfred K. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Mirror descent meets fixed share (and feels no regret)", "author": ["Nicol\u00f2 Cesa-Bianchi", "Pierre Gaillard", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "A parameter-free hedging algorithm", "author": ["Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Prediction with advice of unknown number of experts", "author": ["Alexey Chernov", "Vladimir Vovk"], "venue": "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Chernov and Vovk.,? \\Q2010\\E", "shortCiteRegEx": "Chernov and Vovk.", "year": 2010}, {"title": "Follow the leader if you can, hedge if you must", "author": ["Steven de Rooij", "Tim van Erven", "Peter D. Gr\u00fcnwald", "Wouter M. Koolen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rooij et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rooij et al\\.", "year": 2014}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Manfred K. Warmuth"], "venue": "In Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "A second-order bound with excess losses", "author": ["Pierre Gaillard", "Gilles Stoltz", "Tim Van Erven"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "Gaillard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gaillard et al\\.", "year": 2014}, {"title": "Adaptive algorithms for online decision problems", "author": ["Elad Hazan", "C. Seshadhri"], "venue": "In Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Hazan and Seshadhri.,? \\Q2007\\E", "shortCiteRegEx": "Hazan and Seshadhri.", "year": 2007}, {"title": "Predicting nearly as well as the best pruning of a decision tree", "author": ["David P. Helmbold", "Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Helmbold and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Helmbold and Schapire.", "year": 1997}, {"title": "Tracking the best expert", "author": ["Mark Herbster", "Manfred Warmuth"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Herbster and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 1995}, {"title": "Tracking the best expert", "author": ["Mark Herbster", "Manfred Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Herbster and Warmuth.,? \\Q1998\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 1998}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Herbster and Warmuth.,? \\Q2001\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 2001}, {"title": "Online optimization: Competing with dynamic comparators", "author": ["Ali Jadbabaie", "Alexander Rakhlin", "Shahin Shahrampour", "Karthik Sridharan"], "venue": "In The 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Jadbabaie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jadbabaie et al\\.", "year": 2015}, {"title": "Putting bayes to sleep", "author": ["Wouter M Koolen", "Dmitry Adamskiy", "Manfred K Warmuth"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koolen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2012}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting", "author": ["Haipeng Luo", "Robert E. Schapire"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Luo and Schapire.,? \\Q2014\\E", "shortCiteRegEx": "Luo and Schapire.", "year": 2014}, {"title": "Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations", "author": ["H Brendan McMahan", "Francesco Orabona"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "McMahan and Orabona.,? \\Q2014\\E", "shortCiteRegEx": "McMahan and Orabona.", "year": 2014}, {"title": "Dimension-free exponentiated gradient", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Orabona.,? \\Q2013\\E", "shortCiteRegEx": "Orabona.", "year": 2013}, {"title": "Simultaneous model selection and optimization through parameter-free stochastic learning", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Orabona.,? \\Q2014\\E", "shortCiteRegEx": "Orabona.", "year": 2014}, {"title": "Exploiting easy data in online optimization", "author": ["Amir Sani", "Gergely Neu", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "Drifting games", "author": ["Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Schapire.,? \\Q2001\\E", "shortCiteRegEx": "Schapire.", "year": 2001}, {"title": "No-regret algorithms for unconstrained online convex optimization", "author": ["Matthew Streeter", "Brendan Mcmahan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Streeter and Mcmahan.,? \\Q2012\\E", "shortCiteRegEx": "Streeter and Mcmahan.", "year": 2012}, {"title": "Follow the leader with dropout perturbations", "author": ["Tim Van Erven", "Wojciech Kotlowski", "Manfred K Warmuth"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}, {"title": "A game of prediction with expert advice", "author": ["V.G. Vovk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vovk.,? \\Q1998\\E", "shortCiteRegEx": "Vovk.", "year": 1998}, {"title": "Open problem: Shifting experts on easy data", "author": ["Manfred K. Warmuth", "Wouter M. Koolen"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "Warmuth and Koolen.,? \\Q2014\\E", "shortCiteRegEx": "Warmuth and Koolen.", "year": 2014}, {"title": "Context tree weighting: A sequential universal source coding procedure for FSMX sources", "author": ["Frans M.J. Willems", "Yuri M. Shtarkov", "Tjalling J. Tjalkens"], "venue": "In Proceedings", "citeRegEx": "Willems et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Willems et al\\.", "year": 1993}, {"title": "The context tree weighting method: Basic properties", "author": ["Frans M.J. Willems", "Yuri M. Shtarkov", "Tjalling J. Tjalkens"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Willems et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Willems et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 22, "context": "DT algorithm [Luo and Schapire, 2014], called AdaNormalHedge.", "startOffset": 13, "endOffset": 37}, {"referenceID": 7, "context": "This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010].", "startOffset": 42, "endOffset": 66}, {"referenceID": 7, "context": "This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010]. Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree.", "startOffset": 42, "endOffset": 94}, {"referenceID": 7, "context": "This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010]. Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen [2014] on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses.", "startOffset": 42, "endOffset": 546}, {"referenceID": 2, "context": ", 2012]; learning with experts who provide confidence-rated advice [Blum and Mansour, 2007]; and achieving much smaller regret when the problem is \u201ceasy\u201d while still ensuring worst-case robustness [de Rooij et al.", "startOffset": 67, "endOffset": 91}, {"referenceID": 8, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al.", "startOffset": 83, "endOffset": 114}, {"referenceID": 3, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al.", "startOffset": 115, "endOffset": 142}, {"referenceID": 3, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al. [1997], Vovk [1998] and others two decades ago.", "startOffset": 143, "endOffset": 170}, {"referenceID": 3, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al. [1997], Vovk [1998] and others two decades ago.", "startOffset": 143, "endOffset": 183}, {"referenceID": 22, "context": "DT [Luo and Schapire, 2014].", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007].", "startOffset": 133, "endOffset": 157}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.", "startOffset": 30, "endOffset": 54}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.DT [Luo and Schapire, 2014]. We call it Adaptive NormalHedge (or AdaNormalHedge for short). NormalHedge and NormalHedge.DT provide guarantees for the so-called \u01eb-quantile regret simultaneously for any \u01eb, which essentially corresponds to competing with a uniform distribution over the top \u01eb-fraction of experts. Our new algorithm improves NormalHedge.DT from two aspects (Section 3): 1. AdaNormalHedge can compete with not just the competitor of the specific form mentioned above, but indeed any unknown fixed competitor simultaneously, with a regret in terms of the relative entropy between the competitor and the player\u2019s prior belief of the experts. 2. AdaNormalHedge ensures a new regret bound in terms of the cumulative magnitude of the instantaneous regrets, which is always at most the bound for NormalHedge.DT (or NormalHedge). Moreover, the power of this new form of regret is almost the same as the second order bound introduced in a recent work by Gaillard et al. [2014]. Specifically, it implies 1) a small regret when the loss of the competitor is small and 2) an almost constant regret when the losses are generated randomly with a gap in expectation.", "startOffset": 30, "endOffset": 1120}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.DT [Luo and Schapire, 2014]. We call it Adaptive NormalHedge (or AdaNormalHedge for short). NormalHedge and NormalHedge.DT provide guarantees for the so-called \u01eb-quantile regret simultaneously for any \u01eb, which essentially corresponds to competing with a uniform distribution over the top \u01eb-fraction of experts. Our new algorithm improves NormalHedge.DT from two aspects (Section 3): 1. AdaNormalHedge can compete with not just the competitor of the specific form mentioned above, but indeed any unknown fixed competitor simultaneously, with a regret in terms of the relative entropy between the competitor and the player\u2019s prior belief of the experts. 2. AdaNormalHedge ensures a new regret bound in terms of the cumulative magnitude of the instantaneous regrets, which is always at most the bound for NormalHedge.DT (or NormalHedge). Moreover, the power of this new form of regret is almost the same as the second order bound introduced in a recent work by Gaillard et al. [2014]. Specifically, it implies 1) a small regret when the loss of the competitor is small and 2) an almost constant regret when the losses are generated randomly with a gap in expectation. Our results resolve the open problem asked in Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether a better \u01eb-quantile regret in terms of the loss of the expert instead of the horizon can be achieved.", "startOffset": 30, "endOffset": 1374}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.DT [Luo and Schapire, 2014]. We call it Adaptive NormalHedge (or AdaNormalHedge for short). NormalHedge and NormalHedge.DT provide guarantees for the so-called \u01eb-quantile regret simultaneously for any \u01eb, which essentially corresponds to competing with a uniform distribution over the top \u01eb-fraction of experts. Our new algorithm improves NormalHedge.DT from two aspects (Section 3): 1. AdaNormalHedge can compete with not just the competitor of the specific form mentioned above, but indeed any unknown fixed competitor simultaneously, with a regret in terms of the relative entropy between the competitor and the player\u2019s prior belief of the experts. 2. AdaNormalHedge ensures a new regret bound in terms of the cumulative magnitude of the instantaneous regrets, which is always at most the bound for NormalHedge.DT (or NormalHedge). Moreover, the power of this new form of regret is almost the same as the second order bound introduced in a recent work by Gaillard et al. [2014]. Specifically, it implies 1) a small regret when the loss of the competitor is small and 2) an almost constant regret when the losses are generated randomly with a gap in expectation. Our results resolve the open problem asked in Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether a better \u01eb-quantile regret in terms of the loss of the expert instead of the horizon can be achieved.", "startOffset": 30, "endOffset": 1402}, {"referenceID": 1, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007]. We then focus on a special case of this setting called the sleeping expert problem [Blum, 1997, Freund et al., 1997], where the number of \u201cawake\u201d experts is dynamically changing and the total number of underlying experts is indeed unknown. AdaNormalHedge is thus a very suitable algorithm for this problem. To show the power of all the abovementioned properties of AdaNormalHedge, we study the following two examples of the sleeping expert problem and use AdaNormalHedge to significantly improve previous work. The first example is adaptive regret, that is, regret on any time interval, introduced by Hazan and Seshadhri [2007]. This can be reduced to a sleeping expert problem by adding a new copy of each original expert on each round [Freund et al.", "startOffset": 134, "endOffset": 787}, {"referenceID": 1, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007]. We then focus on a special case of this setting called the sleeping expert problem [Blum, 1997, Freund et al., 1997], where the number of \u201cawake\u201d experts is dynamically changing and the total number of underlying experts is indeed unknown. AdaNormalHedge is thus a very suitable algorithm for this problem. To show the power of all the abovementioned properties of AdaNormalHedge, we study the following two examples of the sleeping expert problem and use AdaNormalHedge to significantly improve previous work. The first example is adaptive regret, that is, regret on any time interval, introduced by Hazan and Seshadhri [2007]. This can be reduced to a sleeping expert problem by adding a new copy of each original expert on each round [Freund et al., 1997, Koolen et al., 2012]. Thus, the total number of sleeping experts is not fixed. When some information on this interval is known (such as the length, the loss of the competitor on this interval, etc), several algorithms achieve optimal regret [Hazan and Seshadhri, 2007, Cesa-Bianchi et al., 2012]. However, when no prior information is available, all previous work gives suboptimal bounds. We apply AdaNormalHedge to this problem. The resulting algorithm, which we called AdaNormalHedge.TV, enjoys the optimal adaptive regret in not only the adversarial case but also the stochastic case due to the properties of AdaNormalHedge. We then extend the results to the problem of tracking the best experts where the player needs to compete with the best partition of the whole process and the best experts on each of these partitions [Herbster and Warmuth, 1995, Bousquet and Warmuth, 2003]. This resolves one of the open problems in Warmuth and Koolen [2014] on whether a single algorithm can achieve optimal shifting regret for both adversarial and stochastic losses.", "startOffset": 134, "endOffset": 1871}, {"referenceID": 1, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007]. We then focus on a special case of this setting called the sleeping expert problem [Blum, 1997, Freund et al., 1997], where the number of \u201cawake\u201d experts is dynamically changing and the total number of underlying experts is indeed unknown. AdaNormalHedge is thus a very suitable algorithm for this problem. To show the power of all the abovementioned properties of AdaNormalHedge, we study the following two examples of the sleeping expert problem and use AdaNormalHedge to significantly improve previous work. The first example is adaptive regret, that is, regret on any time interval, introduced by Hazan and Seshadhri [2007]. This can be reduced to a sleeping expert problem by adding a new copy of each original expert on each round [Freund et al., 1997, Koolen et al., 2012]. Thus, the total number of sleeping experts is not fixed. When some information on this interval is known (such as the length, the loss of the competitor on this interval, etc), several algorithms achieve optimal regret [Hazan and Seshadhri, 2007, Cesa-Bianchi et al., 2012]. However, when no prior information is available, all previous work gives suboptimal bounds. We apply AdaNormalHedge to this problem. The resulting algorithm, which we called AdaNormalHedge.TV, enjoys the optimal adaptive regret in not only the adversarial case but also the stochastic case due to the properties of AdaNormalHedge. We then extend the results to the problem of tracking the best experts where the player needs to compete with the best partition of the whole process and the best experts on each of these partitions [Herbster and Warmuth, 1995, Bousquet and Warmuth, 2003]. This resolves one of the open problems in Warmuth and Koolen [2014] on whether a single algorithm can achieve optimal shifting regret for both adversarial and stochastic losses. Note that although recent work by Sani et al. [2014] also solves this open problem in some sense, their method requires knowing the number of partitions and other information ahead of time and also gives a worse bound for stochastic losses, while AdaNormalHedge.", "startOffset": 134, "endOffset": 2034}, {"referenceID": 15, "context": "The second example we provide is predicting almost as well as the best pruning tree [Helmbold and Schapire, 1997], which was also shown to be reducible to a sleeping expert problem [Freund et al.", "startOffset": 84, "endOffset": 113}, {"referenceID": 12, "context": "The second example we provide is predicting almost as well as the best pruning tree [Helmbold and Schapire, 1997], which was also shown to be reducible to a sleeping expert problem [Freund et al., 1997].", "startOffset": 181, "endOffset": 202}, {"referenceID": 11, "context": "The well-known exponential weights algorithm gives the optimal results only when the learning rate is optimally tuned in terms of the competitor [Freund and Schapire, 1999].", "startOffset": 145, "endOffset": 172}, {"referenceID": 8, "context": "This problem was introduced in Herbster and Warmuth [2001] and later generalized by Cesa-Bianchi et al.", "startOffset": 31, "endOffset": 59}, {"referenceID": 4, "context": "This problem was introduced in Herbster and Warmuth [2001] and later generalized by Cesa-Bianchi et al. [2012]. Their algorithm (fixed share) also requires knowing some information on the sequence of competitors to optimally tune parameters.", "startOffset": 84, "endOffset": 111}, {"referenceID": 4, "context": "This problem was introduced in Herbster and Warmuth [2001] and later generalized by Cesa-Bianchi et al. [2012]. Their algorithm (fixed share) also requires knowing some information on the sequence of competitors to optimally tune parameters. We avoid this issue by showing that while this problem seems more general and difficult, it is in fact equivalent to its special case: achieving adaptive regret. This equivalence theorem is independent of the concrete algorithms and may be of independent interest. Applying this result, we show that without any parameter tuning, AdaNormalHedge.TV automatically achieves a bound comparable to the one achieved by the optimally tuned fixed share algorithm when competing with time-varying competitors. Concrete results and detailed comparisons on this first example can be found in Section 5. To sum up, AdaNormalHedge.TV is an algorithm that is simultaneously adaptive in the number of experts, the competitors and the way the losses are generated. The second example we provide is predicting almost as well as the best pruning tree [Helmbold and Schapire, 1997], which was also shown to be reducible to a sleeping expert problem [Freund et al., 1997]. Previous work either only considered the log loss setting, or assumed prior information on the best pruning tree is known. Using AdaNormalHedge, we again provide better or comparable bounds without knowing any prior information. In fact, due to the adaptivity of AdaNormalHedge in the number of experts, our regret bound depends on the total number of distinct traversed edges so far, instead of the total number of edges of the decision tree as in Freund et al. [1997] which could be exponentially larger.", "startOffset": 84, "endOffset": 1665}, {"referenceID": 0, "context": "While competing with any unknown competitor simultaneously is relatively easy in the log loss setting [Littlestone and Warmuth, 1994, Adamskiy et al., 2012, Koolen et al., 2012], it is much harder in the bounded loss setting studied here. The well-known exponential weights algorithm gives the optimal results only when the learning rate is optimally tuned in terms of the competitor [Freund and Schapire, 1999]. Chernov and Vovk [2010] also studied \u01eb-quantile regret, but no concrete algorithm was provided.", "startOffset": 134, "endOffset": 437}, {"referenceID": 0, "context": "While competing with any unknown competitor simultaneously is relatively easy in the log loss setting [Littlestone and Warmuth, 1994, Adamskiy et al., 2012, Koolen et al., 2012], it is much harder in the bounded loss setting studied here. The well-known exponential weights algorithm gives the optimal results only when the learning rate is optimally tuned in terms of the competitor [Freund and Schapire, 1999]. Chernov and Vovk [2010] also studied \u01eb-quantile regret, but no concrete algorithm was provided. Several work considers competing with unknown competitors in a different unconstrained linear optimization setting [Streeter and Mcmahan, 2012, Orabona, 2013, McMahan and Orabona, 2014, Orabona, 2014]. Jadbabaie et al. [2015] studied general adaptive online learning algorithms against time-varying competitors, but with different and incomparable measurement of the hardness of the problem.", "startOffset": 134, "endOffset": 735}, {"referenceID": 22, "context": "DT [Luo and Schapire, 2014] (a variant of NormalHedge [Chaudhuri et al.", "startOffset": 3, "endOffset": 27}, {"referenceID": 7, "context": "DT [Luo and Schapire, 2014] (a variant of NormalHedge [Chaudhuri et al., 2009]), before we introduce our new improved variants.", "startOffset": 54, "endOffset": 78}, {"referenceID": 15, "context": "In fact, in Section 5, we will consider an even more general notion of regret introduced in Herbster and Warmuth [2001], where we allow the competitor to vary over time and to have different scales.", "startOffset": 92, "endOffset": 120}, {"referenceID": 7, "context": "This is the first concrete algorithm with this kind of adaptive property (the original NormalHedge [Chaudhuri et al., 2009] still has a weak dependence on N ).", "startOffset": 99, "endOffset": 123}, {"referenceID": 7, "context": "This is the first concrete algorithm with this kind of adaptive property (the original NormalHedge [Chaudhuri et al., 2009] still has a weak dependence on N ). In fact, as we will show later, one can even extend the results to any competitor u. Moreover, we will improve NormalHedge.DT so that it has a much smaller regret when the problem is \u201ceasy\u201d in some sense. Notation. We use [N ] to denote the set {1, . . . , N}, \u2206N to denote the simplex of all distributions over [N ], and RE(\u00b7 || \u00b7) to denote the relative entropy between two distributions, Also define L\u0303t,i = \u2211t \u03c4=1[l\u03c4,i\u2212 l\u0302\u03c4 ]+. Many bounds in this work will be in terms of L\u0303T,i, which is always at most LT,i since trivially [lt,i\u2212 l\u0302t]+ \u2264 lt,i. We consider \u201clog log\u201d terms to be nearly constant, and use \u00d4() notation to hide these terms. Indeed, as pointed out by Chernov and Vovk [2010], ln lnx is smaller than 4 even when x is as large as the age of the universe expressed in microseconds (\u2248 4.", "startOffset": 100, "endOffset": 853}, {"referenceID": 13, "context": "Gaillard et al. [2014] introduced a new second order bound that implies much smaller regret when the problem is easy.", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "Gaillard et al. [2014] introduced a new second order bound that implies much smaller regret when the problem is easy. It turns out that our seemingly weaker first order bound is also enough to get the exact same results! We state these implications in the following theorem which is essentially a restatement of Theorems 9 and 11 of Gaillard et al. [2014] with weaker conditions.", "startOffset": 0, "endOffset": 356}, {"referenceID": 13, "context": "The proof of Theorem 2 is based on the same idea as in Gaillard et al. [2014], and is included in Appendix B for completeness.", "startOffset": 55, "endOffset": 78}, {"referenceID": 7, "context": "This answers the open question (in the affirmative) asked by Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether an improvement for small loss can be obtained for \u01eb-quantile regret without knowing \u01eb.", "startOffset": 61, "endOffset": 85}, {"referenceID": 7, "context": "This answers the open question (in the affirmative) asked by Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether an improvement for small loss can be obtained for \u01eb-quantile regret without knowing \u01eb.", "startOffset": 61, "endOffset": 113}, {"referenceID": 13, "context": "Comparison to Adapt-ML-Prod [Gaillard et al., 2014].", "startOffset": 28, "endOffset": 51}, {"referenceID": 13, "context": "Comparison to Adapt-ML-Prod [Gaillard et al., 2014]. Adapt-ML-Prod enjoys a second order bound in terms of \u2211T t=1 r 2 t,i, which is always at most the term \u2211T t=1 |rt,i| appeared in our bounds.4 However, on one hand, as discussed above, these two bounds have the same improvements when the problem is easy in several senses; on the other hand, Adapt-ML-Prod does not provide a bound in terms of RE(u || q) for an unknown u. In fact, as discussed at the end of Section A.3 of Gaillard et al. [2014], Adapt-ML-Prod cannot improve by exploiting a good prior q (or at least its current analysis cannot).", "startOffset": 29, "endOffset": 498}, {"referenceID": 13, "context": "Comparison to Adapt-ML-Prod [Gaillard et al., 2014]. Adapt-ML-Prod enjoys a second order bound in terms of \u2211T t=1 r 2 t,i, which is always at most the term \u2211T t=1 |rt,i| appeared in our bounds.4 However, on one hand, as discussed above, these two bounds have the same improvements when the problem is easy in several senses; on the other hand, Adapt-ML-Prod does not provide a bound in terms of RE(u || q) for an unknown u. In fact, as discussed at the end of Section A.3 of Gaillard et al. [2014], Adapt-ML-Prod cannot improve by exploiting a good prior q (or at least its current analysis cannot). Specifically, while the regret for AdaNormalHedge does not have an explicit dependence on N and is much smaller when the prior q is close to the competitor u, the regret for Adapt-ML-Prod always has a lnN multiplicative term for \u2211T t=1 r 2 t,i, which means even a good prior results in the same regret as a uniform prior! More advantages of AdaNormalHedge over Adapt-ML-Prod will be discussed in concrete examples in following sections. Proof sketch of Theorem 1. The analysis of NormaHedge.DT is based on the idea of converting the expert problem into a drifting game [Schapire, 2001, Luo and Schapire, 2014]. Here, we extract and simplify the key idea of their proof and also improve it to form our analysis. The main idea is to show that the weighted sum of potentials does not increase much on each round using an improved version of Lemma 2 of Luo and Schapire [2014]. In fact, we show that the final potential \u2211N i=1 qi\u03a6(RT,i, CT,i) is exactly bounded by B (defined in Theorem 1).", "startOffset": 29, "endOffset": 1473}, {"referenceID": 1, "context": "4 Confidence-rated Advice and Sleeping Experts In this section, we generalize AdaNormalHedge to deal with experts that make confidence-rated advice, a setting that subsumes many interesting applications as studied by Blum [1997] and Freund et al.", "startOffset": 217, "endOffset": 229}, {"referenceID": 1, "context": "4 Confidence-rated Advice and Sleeping Experts In this section, we generalize AdaNormalHedge to deal with experts that make confidence-rated advice, a setting that subsumes many interesting applications as studied by Blum [1997] and Freund et al. [1997]. In this general setting, on each round t, each expert first reports its confidence It,i \u2208 [0, 1] for the current task.", "startOffset": 217, "endOffset": 254}, {"referenceID": 12, "context": "Previously, Gaillard et al. [2014] studied a general reduction from an expert algorithm to a confidencerated expert algorithm.", "startOffset": 12, "endOffset": 35}, {"referenceID": 7, "context": "This is indeed the case for most algorithms (including Adapt-ML-Prod and even the original NormalHedge by Chaudhuri et al. [2009]).", "startOffset": 106, "endOffset": 130}, {"referenceID": 12, "context": "1 Special Cases: Adaptive Regret and Tracking the Best Expert We start from a special case: adaptive regret, introduced by Hazan and Seshadhri [2007] to better capture changing environments.", "startOffset": 123, "endOffset": 150}, {"referenceID": 11, "context": "Freund et al. [1997] essentially introduced a way to reduce this problem to a sleeping expert problem, which was later improved by Adamskiy et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "[1997] essentially introduced a way to reduce this problem to a sleeping expert problem, which was later improved by Adamskiy et al. [2012]. Specifically, for every pair of time t and expert i, we create a sleeping expert, denoted by (t, i), who is only awake after (and including) round t and since then suffers the same loss as the original expert i.", "startOffset": 117, "endOffset": 140}, {"referenceID": 14, "context": "However, the data streaming technique used in Hazan and Seshadhri [2007] can be directly applied here to reduce the time and space complexity to O(N ln t) and O(N lnT ) respectively, with only an extra multiplicative O( \u221a ln(t2 \u2212 t1)) factor in the regret.", "startOffset": 46, "endOffset": 73}, {"referenceID": 14, "context": "However, the data streaming technique used in Hazan and Seshadhri [2007] can be directly applied here to reduce the time and space complexity to O(N ln t) and O(N lnT ) respectively, with only an extra multiplicative O( \u221a ln(t2 \u2212 t1)) factor in the regret. Tracking the best expert. In fact, AdaNormalHedge.TV is a solution for one of the open problems proposed by Warmuth and Koolen [2014]. Adaptive regret immediately implies the so-called K-shifting regret for the problem of tracking the best expert in a changing environment.", "startOffset": 46, "endOffset": 391}, {"referenceID": 14, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007].", "startOffset": 51, "endOffset": 78}, {"referenceID": 10, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007]. This is exactly what was asked in Warmuth and Koolen [2014]: whether there is an algorithm that can do optimally for both adversarial and stochastic losses in the problem of tracking the best expert.", "startOffset": 52, "endOffset": 140}, {"referenceID": 10, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007]. This is exactly what was asked in Warmuth and Koolen [2014]: whether there is an algorithm that can do optimally for both adversarial and stochastic losses in the problem of tracking the best expert. AdaNormalHedge.TV achieves this goal without knowing K or any other information, while the solution provided by Sani et al. [2014] needs to know K , LK-Shift and \u03b1 to get the same adversarial bound and a worse stochastic bound of order O(1/\u03b12).", "startOffset": 52, "endOffset": 411}, {"referenceID": 10, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007]. This is exactly what was asked in Warmuth and Koolen [2014]: whether there is an algorithm that can do optimally for both adversarial and stochastic losses in the problem of tracking the best expert. AdaNormalHedge.TV achieves this goal without knowing K or any other information, while the solution provided by Sani et al. [2014] needs to know K , LK-Shift and \u03b1 to get the same adversarial bound and a worse stochastic bound of order O(1/\u03b12). Comparison to previous work. For adaptive regret, the FLH algorithm by Hazan and Seshadhri [2007] treats any standard expert algorithm as a sleeping expert, and has an additive term O( \u221a t2 ln t2) in addition to the base algorithm\u2019s regret (when no prior information is available), which adds up to a large O(K \u221a T lnT ) term for K-shifting regret.", "startOffset": 52, "endOffset": 623}, {"referenceID": 0, "context": "Several works on fixed share for the simpler \u201clog loss\u201d setting were studied before [Herbster and Warmuth, 1998, Bousquet and Warmuth, 2003, Adamskiy et al., 2012, Koolen et al., 2012]. Cesa-Bianchi et al. [2012] studied a generalized fixed share algorithm for the bounded loss setting considered here.", "startOffset": 141, "endOffset": 213}, {"referenceID": 14, "context": "V (u1:T )L\u0303(u1:T ) ln (NT ) FLH8 [Hazan and Seshadhri, 2007] \u221a t2 ln t2 K \u221a T lnT unknown Fixed Share [Cesa-Bianchi et al.", "startOffset": 33, "endOffset": 60}, {"referenceID": 6, "context": "V (u1:T )L\u0303(u1:T ) ln (NT ) FLH8 [Hazan and Seshadhri, 2007] \u221a t2 ln t2 K \u221a T lnT unknown Fixed Share [Cesa-Bianchi et al., 2012] \u221a", "startOffset": 102, "endOffset": 129}, {"referenceID": 6, "context": "Fixed share is shown to ensure the following regret [Cesa-Bianchi et al., 2012]: R(u1:T ) = O ( \u221a", "startOffset": 52, "endOffset": 79}, {"referenceID": 5, "context": "Cesa-Bianchi et al. [2012] introduced a distance measurement to capture this variation: V (u1:T ) = \u2211T t=1 \u2211N i=1[ut,i\u2212ut\u22121,i]+ where we define u0,i = 0 for all i.", "startOffset": 0, "endOffset": 27}, {"referenceID": 5, "context": "Moreover, while the results in Cesa-Bianchi et al. [2012] are specific for the fixed share algorithm, we prove the following results which are independent of the concrete algorithms and may be of independent interest.", "startOffset": 31, "endOffset": 58}, {"referenceID": 5, "context": "3 of Cesa-Bianchi et al. [2012], the authors mentioned online tuning technique for the parameters, it only works for special cases (e.", "startOffset": 5, "endOffset": 32}, {"referenceID": 13, "context": "This problem was studied in the context of online learning by Helmbold and Schapire [1997] using the approach of Willems et al.", "startOffset": 62, "endOffset": 91}, {"referenceID": 4, "context": "It is also called the tree expert problem in Cesa-Bianchi and Lugosi [2006]. Freund et al.", "startOffset": 45, "endOffset": 76}, {"referenceID": 4, "context": "It is also called the tree expert problem in Cesa-Bianchi and Lugosi [2006]. Freund et al. [1997] proposed a generic reduction from a tree expert problem to a sleeping expert problem.", "startOffset": 45, "endOffset": 98}, {"referenceID": 4, "context": "It is also called the tree expert problem in Cesa-Bianchi and Lugosi [2006]. Freund et al. [1997] proposed a generic reduction from a tree expert problem to a sleeping expert problem. Using this reduction with our new algorithm, we provide much better results compared to previous work (summarized in Table 2). Specifically, consider a setting where on each round t, the predictor has to make a decision yt from some convex set Y given some side information xt, and then a convex loss function ft : Y \u2192 [0, 1] is revealed and the player suffers loss ft(yt). The predictor is given a template tree G to consult. Starting from the root, each node of G performs a test on xt to decide which of its child should perform the next test, until a leaf is reached. In addition to a test, each node (except the root) also makes a prediction based on xt. A pruning tree P is a tree induced by replacing zero or more nodes (and associated subtrees) of G by leaves. The prediction of a pruning tree P given xt, denoted by P(xt), is naturally defined as the prediction of the leaf that xt reaches by traversing P. The player\u2019s goal is thus to predict almost as well as the best pruning tree in hindsight, that is, to minimize RG = \u2211T t=1 ft(yt)\u2212minP \u2211T t=1 ft(P(xt)). The idea of the reduction introduced by Freund et al. [1997] is to view each edge of G as a sleeping expert (indexed by e), who is awake only when traversed by xt, and in that case predicts yt,e, the same prediction as the child node that it connects to.", "startOffset": 45, "endOffset": 1315}, {"referenceID": 12, "context": "The work by Freund et al. [1997] considers a variant of the exponential weights algorithm in a \u201clog loss\u201d setting, and is not directly applicable here (specifically it is not clear how 11", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "O (\u2016xt\u2016G) O (NT ) No Adapt-ML-Prod [Gaillard et al., 2014] \u00d4 ( \u221a", "startOffset": 35, "endOffset": 58}, {"referenceID": 15, "context": "O (\u2016xt\u2016G) O (NT ) No Exponential Weights [Helmbold and Schapire, 1997] O (\u221a mL )", "startOffset": 41, "endOffset": 70}, {"referenceID": 13, "context": "A better choice is the Adapt-ML-Prod algorithm by Gaillard et al. [2014] (the version for the sleeping expert problem).", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "We finally compare with a totally different approach [Helmbold and Schapire, 1997], where one simply treats each pruning tree as an expert and run the exponential weights algorithm.", "startOffset": 53, "endOffset": 82}, {"referenceID": 12, "context": "As discussed in Freund et al. [1997], the linear dependence on m in this bound is much better than the one of the form O (\u221a mL\u2217 lnN )", "startOffset": 16, "endOffset": 37}, {"referenceID": 12, "context": "Finally, as mentioned in Freund et al. [1997], the sleeping expert approach can be easily generalized to predicting with a decision graph.", "startOffset": 25, "endOffset": 46}, {"referenceID": 13, "context": "3 of Gaillard et al. [2014] that we already mentioned at Section 3.", "startOffset": 5, "endOffset": 28}], "year": 2015, "abstractText": "We study the classic online learning problem of predicting with expert advice, and propose a truly parameter-free and adaptive algorithm that achieves several objectives simultaneously without using any prior information. The main component of this work is an improved version of the NormalHedge.DT algorithm [Luo and Schapire, 2014], called AdaNormalHedge. On one hand, this new algorithm ensures small regret when the competitor has small loss and almost constant regret when the losses are stochastic. On the other hand, the algorithm is able to compete with any convex combination of the experts simultaneously, with a regret in terms of the relative entropy of the prior and the competitor. This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010]. Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen [2014] on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}