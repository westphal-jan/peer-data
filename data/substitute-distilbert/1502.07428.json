{"id": "1502.07428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Representative Selection in Non Metric Datasets", "abstract": "this paper considers the problem of representative selection : choosing a subset of data points from a dataset that best represents our larger set of elements. this subset needs to inherently reflect the type of information contained in the entire set, while attracting redundancy. for such purposes, clustering may seem like a natural approach. however, mixed clustering methods are not ideally desirable for representative function, especially when working with hyper - metric data, where only a pairwise similarity measure exists. here this paper we propose $ \\ delta $ - medoids, a novel subject because can be viewed during an extension to the $ k $ - r algorithm language is specifically unsuitable for sample representative selection from non - metric data. we empirically validate $ \\ delta $ - medoids in two domains, namely music analysis as memory analysis. we also show some theoretical bounds on the performance of $ \\ delta $ - medoids and the hardness of software configurations in general.", "histories": [["v1", "Thu, 26 Feb 2015 04:16:31 GMT  (1240kb,D)", "https://arxiv.org/abs/1502.07428v1", null], ["v2", "Fri, 19 Jun 2015 22:44:29 GMT  (1237kb,D)", "http://arxiv.org/abs/1502.07428v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["elad liebman", "benny chor", "peter stone"], "accepted": false, "id": "1502.07428"}, "pdf": {"name": "1502.07428.pdf", "metadata": {"source": "CRF", "title": "Representative Selection in Non Metric Datasets", "authors": ["Elad Liebman", "Benny Chor", "Peter Stone"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Consider the task of a teacher who is charged with introducing his class to a large corpus of songs (for instance, popular western music since 1950). In drawing up the syllabus, this teacher will need to select a relatively small set of songs to discuss with his students such that 1) every song in the larger corpus is represented by his selection (in the sense that it is relatively similar to one of the selected songs) and 2) the set of selected songs is small enough to cover in a single semester. This task is an instance of the representative selection problem. Similar challenges often arise in tasks related to data summarization and modeling. For instance, finding a characteristic subset of Facebook profiles out of a large set, or a subset of representative news articles from the entire set of news information gathered during a single day from many different sources.\nOn its surface, representative selection is quite similar to clustering, a more widely studied problem in unsupervised learning. Clustering is one of the most widespread tools for studying the structure of data. It has seen extensive usage in countless research disciplines. The objective of clustering is to partition a\nar X\niv :1\n50 2.\n07 42\n8v 2\n[ cs\n.A I]\n1 9\nJu n\ngiven data set of samples into subsets so that samples within the same subset are more similar to one another than samples belonging to different subsets. Several surveys of clustering techniques can be found in the literature [20,41].\nThe idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28]. It has also been discussed as a general problem in [39]. These recurring notions can be formalized as follows. Given a large set of examples, we seek a minimal subset that is rich enough to encapsulate the entire set, thus achieving two competing criteria - maintaining a representative set as small as possible, while satisfying the constraint that all samples are within \u03b4 from some representative. In the next subsections we define this problem in more exact terms, and motivate the need for such an approach.\nWhile certainly related, clustering and representative selection are not the same problem. A seemingly good cluster may not necessarily contain a natural single representative, and a seemingly good partitioning might not induce a good set of representatives. For this reason, traditional clustering techniques are not necessarily well suited for representative selection. We expand on this notion in the next sections."}, {"heading": "1.1 Representative Selection: Problem Definition", "text": "Let S be a data set, d : S \u00d7 S \u2192 R+ be a distance measure (not necessarily a metric), and \u03b4 be a distance threshold below which samples are considered sufficiently similar. We are tasked with finding a representative subset C \u2282 S that best encapsulates the data. We impose the following two requirements on any algorithm for finding a representative subset:\n\u2013 Requirement 1: The algorithm must return a subset C \u2282 S such that for any sample x \u2208 S, there exists a sample c \u2208 C satisfying d(x, c) \u2264 \u03b4.\n\u2013 Requirement 2: The algorithm cannot rely on a metric representation of the samples in S.\nTo compare the quality of different subsets returned by different algorithms, we measure the quality of encapsulation by two criteria:\n\u2013 Criterion 1: |C| - we seek the smallest possible subset C that satisfies Requirement 1:.\n\u2013 Criterion 2: We would also like the representative set to best fit the data on average. Given representative subsets of equal size, we prefer the one that minimizes the average distance of samples from their respective representatives.\nCriteria 1 and 2 are applied on a representative set solution. In addition, we expect the following desiderata for a representative selection algorithm.\n\u2013 Desideratum 1: We prefer representative selection algorithms which are stable. Let C1 and C2 be different representative subsets for dataset S obtained by two different runs of the same algorithm. Stability is defined as\nthe overlap |C1\u2229C2||C1\u222aC2| . The higher the expected overlap is, the more stable the algorithm is. This desideratum ensures the representative set is robust to randomization in data ordering or the choices made by the algorithm. \u2013 Desideratum 2: We would like the algorithm to be efficient and to scale well for large datasets.\nThough not crucial for correctness, the first desideratum is useful for consistency and repeatability. We further motivate the reason for desideratum 1 in Appendix B, and show it is reasonably attainable. The representative selection problem is similar to the -covering number problem in metric spaces [43]. The -covering number measures how many small spherical balls would be needed to completely cover (with overlap) a given space. The main difference is that in our case we also wish the representative set to closely fit the data (Criterion 2). Criteria 1 and 2 are competing goals, as larger representative sets allow for lower average distance. In this article we focus primarily on criterion 1, using criterion 2 as a secondary evaluation criterion."}, {"heading": "1.2 Testbed Applications", "text": "Representative selection is useful in many contexts, particularly when the full dataset is either redundant (due to many near-identical samples) or when using all samples is impractical. For instance, given a large document and a satisfactory measure of similarity between sentences, text summarization [25] could be framed as a representative selection task - obtain a subset of sentences that best captures the nature of the document. Similarly, one could map this problem to extracting \u201cvisual words\u201d or representative frames from visual input [42,26]. This paper examines two concrete cases in which representatives are needed:\n\u2013 Music analysis - the last decade has seen a rise in the computational analysis of music databases for music information retrieval [5], recommender systems [23] and computational musicology [8]. A problem of interest in these contexts is to extract short representative musical segments that best represent the overall character of the piece (or piece set). This procedure is in many ways analogous to text summarization. \u2013 Team strategy/behavior analysis - opponent modeling has been discussed in several contexts, including game playing [3], real-time agent tracking [37] and general multiagent settings [4]. Given a large dataset of recorded behaviors, one may benefit from reducing this large set into a smaller collection of prototypes. In the results section, we consider this problem as a second testbed domain.\nWhat makes both these domains appropriate as testbeds is that they are realistically rich and induce complex, non-metric distance relations between samples.\nThe structure of this paper is as follows. In the following section we provide a more extensive context to the problem of representative selection and discuss why existing approaches may not be suitable. In Section 3 we introduce \u03b4-medoids, an algorithm specifically designed to tackle the problem as we formally defined it. In Section 4 we show some theoretical analysis of the suggested algorithm, and in Section 5 we show its empirical performance in the testbed domains described above."}, {"heading": "2 Background and Related Work", "text": "There are several existing classes of algorithms that solve problems related to representative selection. This section reviews them and discusses the extent to which they are (or are not) applicable to our problem."}, {"heading": "2.1 Limitations of Traditional Clustering", "text": "Given the prevalence of clustering algorithms, it is tempting to solve representative selection by clustering the data and using cluster centers (if they are in the set) as representatives, or the closest point to each center. In some cases it may even seem sufficient, once the data is clustered, to select samples chosen at random from each cluster as representatives. However, such an approach usually only considers the average distance between representatives and samples, and it is unlikely to yield good results with respect to any other requirement, such as minimizing the worst case distance or maintaining the smallest set possible. Moreover, the task of determining the desirable number of clusters k for a sufficient representation can be a difficult challenge in itself. Consider the example in Figure 1: given a set of |S| = 100 points, and a distance measure (in this case, the Euclidean distance metric), we seek a set of representatives that is within distance 1 of every point in the set, thus satisfying Criterion 1 with \u03b4 = 1. Applying a standard clustering approach on this set, the distance constraint is only consistently met when k \u2265 77 (and rarely with less than 70 samples). Intuitively, a large number of clusters is required to ensure that no sample is farther than \u03b4 from a centroid. However, we can obtain the same coverage goal with only 13 samples. Defining a distance criterion rather than a desired number of clusters has a subtle but crucial impact on the problem definition."}, {"heading": "2.2 Clustering and Spatial Representation", "text": "The above limitation of clustering applies even when the data can be embedded as coordinates in some n-dimensional vector space. However, in many cases, including our motivating domain of music analysis, the data does not naturally fit in such a space. This constraint renders many common clustering techniques inapplicable, including the canonical k-means [24], or more recent works such as [39]. Furthermore, the distance function we construct (detecting both local\nand global similarities) is not a metric, since it violates the triangle inequality. Because of this property, methods reliant on a distance metric are also inapplicable. Among such methods are neighbor-joining [32], which becomes unreliable when applied on non-metric data, or the k-prototypes algorithm [1].3 Nevertheless, certain clustering algorithms still apply, such as the k-medoids algorithm, hierarchical clustering [35], and spectral clustering [38]. These methods can be employed directly on a pairwise (symmetric) similarity matrix,4 while satisfying the triangle inequality is not a requirement.\n2.3 The k-Medoids Algorithm\nThe k-medoids algorithm [31] is a variation on the classic k-means algorithm that only selects centers from the original dataset, and is applicable to data organized as a pairwise distance matrix. The algorithm partitions a set of samples to a predetermined number k based on the distance matrix. Similarly to the k-means algorithm, it does so by starting with k random centers, partitioning the data around them, and iteratively moving the k centers toward the medoids of each cluster (a medoid is defined as medoidS = argmin\ns\u2208S\n\u2211 x\u2208S d(x, s)).\nAll of the approaches mentioned so far are specifically designed for dividing the data to a fixed number of partitions. In contrast, representative selection defines a distance (or coverage) criterion \u03b4, rather than a predetermined number of clusters k. In that respect, k-medoids, or spectral and hierarchical clustering,\n3 In certain contexts, metric learning [40,10] can be applied, but current methods are not well suited for data without vector space representation, and in some sense, learning a metric is of lesser interest for representative selection, as we care less about classification or the structural relations latent in the data. 4 For spectral clustering, the requirement is actually an affinity (or proximity) matrix.\nAlgorithm 1 Extended Greedy K-Centers Approach (farthest first traversal)\n1: Input: data sampleSet = x0 . . . xm, required distance \u03b4 2: choose random starting representative xi 3: representativeSet = {xi} 4: sampleSet = x0 . . . xi\u22121, xi+1 . . . xm 5: maximalDist = maxs\u2208sampleSetd(s, representativeSet 6: while maximalDist > \u03b4 do 7: maximalElement = argmaxs\u2208sampleSetd(s, representativeSet) 8: representativeSet = representativeSet \u222a {maximalElement} 9: sampleSet = representativeSet/{maximalElement}\n10: maximalDist = maxs\u2208sampleSetd(s, representativeSet) 11: end while\nforce us to search for a partition that satisfies this distance criterion. Applying a clustering algorithm to representative selection requires an outer loop to search for an appropriate k, a process which can be quite expensive.\nWe note that traditionally, both hierarchical methods and spectral clustering require the full pairwise distance matrix. If the sample set S is large (The usual use case for representative selection), computing a pairwise |S| \u00d7 |S| distance matrix can be prohibitively expensive. In the case of spectral clustering, an efficient algorithm that does not compute the full distance matrix exists [34], but it relies on a vector space representation of the data, rendering it inapplicable in our case.5 The algorithm we introduce in this article does not require a distance metric, nor does it rely on such a spatial embedding of the data, which makes it useful even in cases where very little is known about the samples beyond some proximity relation.\n2.4 k-Centers Approach\nA different, yet related, topic in clustering and graph theory is the k-centers problem. Let the distance between a sample s and a set C be: d(s, C) = minc\u2208Cd(s, c). The k-centers problem is defined as follows: Given a set S and a number k, find a subset R \u2282 S, |R| = k so that maxs\u2208Sd(s,R) is minimal [18].\nIn metric spaces, an efficient 2-approximation algorithm for this problem exists as follows.6 First choose a random representative. Then, for k\u22121 times, add the element farthest away from the representative set R to R. This approach can be directly extended to suit representative selection - instead of repeating the addition step k\u22121 times, we can continue adding elements to the representative set until no sample is > \u03b4 away from any some representative (see Algorithm 1).\n5 In some cases the distance matrix can be made sparse via KD-trees and nearestneighbor approximations, which also require a metric embedding.[6] 6 We note that no better approximation scheme is possible under standard complexity theoretic assumptions [18].\nWhile this algorithm produces a legal representative set, it ignores criterion 2 (average distance). Another algorithm that is related to this problem is Gonzales\u2019 approximation algorithm for minimizing the maximal cluster diameter [16], which iteratively takes out elements from existing clusters to generate new clusters based on the inter-cluster distance. This algorithm is applicable in our setting since it too only requires pairwise distances, and can produce a legal coverage by partitioning the data into an increasing number of cluster until the maximal diameter is less than \u03b4 (at which point any sample within a cluster covers it). This approach is wasteful for the purpose of representative selection, since it forces a much larger number of representatives than needed. Lastly, in a recent, strongly related paper [14], the authors consider a similar problem of selecting exemplars in data to speed up learning. They do not pose hard constraints on the maximal distance between exemplars and samples, but rather frame this task as an optimization problem, softly associating each sample with a \u201clikelihood to represent\u201d any other sample, and trying to minimize the aggregated coverage distance while also minimizing the norm of the representation likelihood matrix. Though very interesting, it\u2019s hard to enable this method to guarantee a desired minimal distance, and the soft association of representatives to samples is inadequate to our purposes."}, {"heading": "3 The \u03b4-Medoids Algorithm", "text": "In this section, we present the novel \u03b4-medoids algorithm, specifically designed to solve the representative selection problem. The algorithm does not assume a metric or a spatial representation, but rather relies solely on the existence of some (not necessarily symmetric) distance or dissimilarity measure d : S \u00d7 S \u2192 R+. Similarly to the k-centers solution approach, the \u03b4-medoids approach seeks to directly find samples that sufficiently cover the full dataset. The algorithm does so by iteratively scanning the dataset and adding representatives if they are sufficiently different from the current set. As it scans, the algorithm associates a cluster with each representative, comprising the samples it represents. Then, the algorithm refines the selected list of representatives, in order to reduce the average coverage distance. This procedure is repeated until the algorithm reaches convergence. Thus, we address both minimality (criterion 1) and average-distance considerations (criterion 2). We show in Section 5 that this algorithm achieves its goal efficiently in two concrete problem domains, and does so directly, without the need for optimizing a meta-parameter k. We first introduce a simpler, single-iteration \u03b4-representative selection algorithm on which the full \u03b4-medoids algorithm is based."}, {"heading": "3.1 Straightforward \u03b4-Representative Selection", "text": "Let us consider a more straightforward \u201cone-shot\u201d representative selection algorithm that meets the \u03b4-distance criterion. The algorithm sweeps through the\nAlgorithm 2 One-shot \u03b4-representatives selection algorithm\n1: Input: data x0 . . . xm, required distance \u03b4 2: Initialize representatives = \u2205. 3: Initialize clusters = \u2205 4: representative assignment subroutine, RepAssign, lines 5-22: 5: for i = 0 to m do 6: Initialize dist =\u221e 7: Initialize representative = null 8: for rep in representatives do 9: if d(xi, rep) \u2264 dist then\n10: representative = rep 11: dist = d(xi, rep) 12: end if 13: end for 14: if dist \u2264 \u03b4 then 15: add xi to clusterrepresentative 16: else 17: representative = xi 18: Initialize clusterrepresentative = \u2205 19: add xi to clusterrepresentative 20: add clusterrepresentative to clusters 21: end if 22: end for\nelements of S, and collects a new representative each time it observes a sufficiently \u201cnew\u201d element. Such an element needs to be > \u03b4 away from any previously collected representative. The pseudocode for this algorithm is presented in Algorithm 2. While this straightforward approach works well in the sense that it does produce a legal representative set, it is sensitive to scan order, therefore violating the desired stability property. More importantly, it does not address the average distance criterion. For these reasons, we extend this algorithm into an iterative one, a hybrid of sorts between direct representative selection and EM clustering approaches."}, {"heading": "3.2 The Full \u03b4-Medoids Algorithm", "text": "This algorithm is based on the straightforward approach, as described in Section 3.1. However, unlike Algorithm 2, it repeatedly iterates through the samples. In each iteration, the algorithm associates each sample to a representative so that it is never \u2265 \u03b4 away from some representative (the RepAssign subroutine, see Algorithm 3), just as in Algorithm 2. The main difference is that at the end of each iteration it subsequently finds a closer-fitting representative for each cluster S associated with representative s. Concretely, representativeS = medoidS = argmin\ns\u2208S\n\u2211 x\u2208S d(x, s) (lines 8 \u2212 13), under the\nconstraint that no sample \u2208 S is farther than \u03b4 from medoidS . This step en-\nAlgorithm 3 The \u03b4-medoid representative selection algorithm. 1: Input: data x0 . . . xm, required distance \u03b4 2: t = 0 3: Initialize representativest=0 = \u2205. 4: Initialize clusters = \u2205 5: repeat 6: t = t+ 1 7: call RepAssign subroutine, lines 5-22 of Algorithm 2 8: Initialize representativest = \u2205 9: for cluster in clusters do 10: representative = argmin\ns\u2208cluster\n\u2211 x\u2208cluster d(x, s) s.t. \u2200x \u2208 cluster.d(x, s) \u2264 \u03b4\n11: add representative to representativest 12: end for 13: until representativest \u2261 representativest\u22121\nsures that a representative is \u201cbest-fit\u201d on average to the cluster of samples it represents, without sacrificing coverage. In other words, while trying to minimize the size of the representative set, the algorithm also addresses criterion 2 - average distance as low as possible. This step also drastically improves the stability of the retrieved representative set under different permutations of the data (desideratum 1). We note that by adding the constraint that new representatives must still cover the clusters they were selected from, we guarantee that the number of representatives k does not increase after the first scan. The process is repeated until \u03b4-medoids reaches convergence, or until we reach a representative set which is \u201cgood enough\u201d (remember that at the end of each cluster-association iteration we have a set that satisfies the distance criterion). This algorithm uses a greedy heuristic that is indeed ensured to converge to some local optimum (Theorem 1). This local optimum is dependent on the value of \u03b4 and the structure of the data. In Subsection 4.1, we show that solving the representative selection problem for a given \u03b4 is NP-hard, and therefore heuristics are required.\nTheorem 1 Algorithm 3 converges after a finite number of steps.\nSee Appendix A for proof sketch."}, {"heading": "3.3 Merging Close Clusters", "text": "Since satisfying the distance constraint with a minimal set of representatives (Criterion 1) is NP-hard (see Section 4), the \u03b4-medoids algorithm is not guaranteed to do so. A simple optimization procedure can reduce the number of representatives in certain cases. For instance, in some cases, oversegmentation may ensue. To abate such an occurrence, it is possible to iterate through representative pairs that are no more than \u03b4 apart, and see whether joining their respective clusters could yield a new representative that covers all the samples in the joined clusters. If it is possible, the two representatives are eliminated in favor of the new joint representative. The process is repeated until no pair in the potential pair list can be merged. This procedure can be generalized for larger\nrepresentative group sizes, depending on computational tractability. These refinement steps can be taken after each iteration of the algorithm. If the number of representatives is high, however, this approach may be computationally infeasible altogether. Although this procedure was not required in our problem domains (see Section 5), we believe it may still prove useful in certain cases."}, {"heading": "4 Analysis Summary", "text": "In this section, we present the hardness of the representative selection problem, and briefly discuss the efficiency of the \u03b4-medoids algorithm. We show that the problem of finding a minimal representative set is NP-hard, and provide certain bounds on the performance of \u03b4-medoids in metric spaces with respect to representative set size and average distance. We continue to show that approximating the representative selection problem is NP-hard in non-metric spaces, both in terms of the representative set size and with respect to the maximal distance. For the sake of readability, we present full details in Appendix D."}, {"heading": "4.1 NP-Hardness of the Representative Selection Problem", "text": "Theorem 2 Satisfying Criterion 1 (minimal representative set) is NP-Hard."}, {"heading": "4.2 Bounds on \u03b4-medoids in Metric Spaces", "text": "The \u03b4-medoids algorithm is agnostic to the existence of metric space in which the samples can be embedded. That being said, it can work equally well in cases where the data is metric (in Appendix C we demonstrate the performance of the \u03b4-medoids algorithm in a metric space test-case). However, we can show that if the measure which generates the pairwise distances is in fact a metric, certain bounds on performance exist.\nTheorem 3 In a metric space, the average distance of a representative set |C| = k obtained by the \u03b4-medoids algorithm is bound by 2OPT where OPT is the maximal distance obtained by an optimal assignment of k representatives (with respect to maximal distance).\nTheorem 4 The size of the representative set returned by the \u03b4-medoids algorithm, k, is bound by k \u2264 N( \u03b42 ) where N(x) is the minimal number of representatives required to satisfy distance criterion x."}, {"heading": "4.3 Hardness of Approximation of Representative Selection in Non-Metric Spaces", "text": "In non-metric spaces, the representative selection problem becomes much harder. We now show that no c-approximation exists for the representative selection problem either with respect to the first criterion (representative set size) or the second criterion (distance - we focus on maximal distance but a similar outcome for average distance is implied).\nTheorem 5 No constant-factor approximation exists for the representative selection set problem with respect to representative set size.\nTheorem 6 For representative sets of optimal size k,7 no constant-factor approximation exists with respect to the maximal distance between the optimal representative set and the samples."}, {"heading": "4.4 Efficiency of \u03b4-Medoids", "text": "The actual run time of the algorithm is largely dependent on the data and the choice of \u03b4. An important observation is that at each iteration, each sample is only compared to the current representative set, and a sample is introduced to the representative set only if it is > \u03b4 away from all other representatives. After each iteration, the representatives induce a partition to clusters and only samples within the same cluster are compared to one another. While in the worst case the runtime complexity of the algorithm can be O(|S|2), in practice we can get considerably better runtime performance, closer asymptotically to |S|1.5. We note that in each iteration of the algorithm, after the partitioning phase (the RepAssign subroutine in Algorithm 3) the algorithm maintains a legal representative set, so in practice we can halt the algorithm well before convergence, depending on need and resources."}, {"heading": "5 Empirical Results", "text": "In this section, we analyze the performance of the \u03b4-medoids algorithm empirically in two problem domains - music analysis and agent movement analysis. We show that \u03b4-medoids does well on Criterion 1 (minimizing the representative set) while obtaining a good solution for Criterion 2 (maintaining a low average distance). We compare ourselves to three alternative methods - k-medoids, the greedy k-center heuristic, and spectral clustering (using cluster medoids as representatives) [33], and show we outperform all three. We note that although these methods weren\u2019t necessarily designed to tackle the representative selection problem, they, and clustering approaches in general, are used for such purposes in practice (see [17], for instance). To obtain some measure of statistical significance, for each dataset we analyze, we take a random subset of |S| = 5000 samples and use this subset as input for the representative selection algorithm. We repeat this process N = 20 times, averaging the results and obtaining standard errors. We show that the \u03b4-medoid algorithm produces representative sets at least as compact as those produced by the k-centers approach, but obtains a much lower average distance. We further note it does so directly, without the need for first optimizing the number of clusters k, unlike k-medoids or spectral clustering.\n7 In fact, this proof applies for any value of k that cannot be directly manipulated by the algorithm.\nIn Appendix C, we also demonstrate the performance of the algorithm in a standard metric space, and show it outperforms the other methods in this setting as well."}, {"heading": "5.1 Distance Measures", "text": "In both our problem domains, no simple or commonly accepted measure of distance between samples exists. For this reason, we devised a distance function for each setting, based on domain knowledge and experimentation. Feature selection and distance measure optimization are beyond the scope of this work. For completeness, the full details of our distance functions appear in Appendix E. We believe the results are not particularly sensitive to the choice of a specific distance function, but we leave such analysis to future work."}, {"heading": "5.2 Setting 1 - musical segments", "text": "In this setting, we wish to summarize a set of musical pieces. This domain illustrates many of the motivations listed in Section 1. The need for good representative selection is driven by several tasks, including style characterization, comparing different musical corpora (see [12]), and music classification by composer, genre or period [2]. For the purpose of this work we used the Music21 corpus, provided in MusicXML format [9]. For simplicity, we focus on the melodic content of the piece, which can be characterized as the variation of pitch (or frequency) over time.\nData We use thirty musical pieces: 10 representative pieces by Mozart, Beethoven, and Haydn. The melodic lines in the pieces are isolated and segmented using basic grouping principles adapted from [29]. In the segmentation process, short overlapping melodic sequences 5 to 8 beats long are generated. For example, three such segments are plotted in Figure 2 as pitch variation over time. For each movement and each instrument, segmentation results in 55\u2212518 segments. All in all we obtain 20, 000\u2212 40, 000 segments per composer.\nDistance measure We devise a fairly complex distance measure between any two musical segments S1 and S2. Several factors are taken into account:\n\u2013 Global alignment - the global alignment score between the two segments, calculated using the Needleman-Wunsch algorithm [27]. \u2013 Local alignment - the local alignment score between the two segments, calculated using the Smith-Waterman algorithm [36]. Local alignment is useful if two sequences are different overall, but share a meaningful subsequence. \u2013 Rhythmic overlap, interval overlap, step overlap, pitch overlap - the extent to which one-step melodic and rhythmic patterns in the two segments overlap,\nusing a \u201cbag\u201d-like distance function dset(A1, A2) = |A14A2| |A1\u222aA2| .\nThe different factors are then weighted and combined. This measure was chosen because similarity between sequences is multifaceted, and the different factors above capture different aspects of similarity, such as sharing a general contour (global alignment), a common motif (local alignment), or a similar \u201cmusical vocabulary\u201d (the other factors, which by themselves each capture a different aspect of musical language). The result is a measure but not a metric since local alignment may violate the triangle inequality.\nResults We compare \u03b4-medoids to the k-medoids algorithm and the greedy k-center heuristic for five different \u03b4 values. The results are presented in Figure 3. For each composer and \u03b4, we searched exhaustively for the lowest k value for which k-medoids met the distance requirement. We study both the size of the representative set obtained and the average sample-representative distance.\nFrom the representative set size perspective, for all choices of \u03b4 the \u03b4-medoids algorithm obtains better coverage of the data compared to the k-medoids, and does at least as well (and most often better) compared to the greedy k-centers heuristic. However, in terms of average distance, \u03b4-medoids performs much better compared to the k-centers heuristic, implying that the \u03b4-medoids algorithm outperforms the other two. While spectral clustering seems to satisfy the distance criteria with a small representative set for small values of \u03b4, its non-centroid based nature makes it less suitable for representative selection, as a more lax \u03b4 criterion might not necessarily mean a smaller representative set will be needed (as apparent from the result). Indeed, as the value of \u03b4 increases, the \u03b4-medoids algorithm significantly outperforms spectral clustering."}, {"heading": "5.3 Setting 2 - agent movement in robot soccer simulation", "text": "As described in Section 1.2, analyzing agent behavior can be of interest in several domains. The robot world-cup soccer competition (RoboCup) is a wellestablished problem domain for AI in general [22]. In this work we chose to focus on the RoboCup 2D Simulation League. We have collected game data from several full games from the past two Robocup competitions. An example for the gameplay setting and potential movement trajectories can be seen in Figure 5.\nOur purpose is to extract segments that best represent agent movement patterns throughout gameplay. In the specific context of the Robocup simulation league, there are several tasks that motivate representative selection, including agent and team characterization, and learning training trajectories for optimization.\nData Using simulation log data, we extract the movement of the 22 agents over the course of the game (#timesteps = 6000). The agents move in 2-dimensional space (three example trajectories can be seen in Figure 5). We extract 1-second (10 timestamps) long, partially overlapping segments from the full game trajectories of all the agents on the field except the goalkeeper, who tends to move less and in a more confined space and for the purpose of this task is of lesser interest. That leads to 900 \u00b7 20 = 18000 movement segments in total per game. We analyzed 4 teams and 5 games (90000 segments) per team.\nDistance measure Given two trajectories, one can compare them as contours in 2-dimensional space. We take an alignment-based approach, with edit costs being the RMS distance between them. Our distance measure is comprised of three elements: global and local alignment (same as in music analysis), and a \u201cbag of words\u201d-style distance based on patterns of movement-and-turn sequences (turning is quantized into 6 angle bins). As in music analysis, the reason for this approach is that similarity in motion is hard to define, and we believe each feature captures different aspects of similarity. As in the previous setting, this is not a metric, as local alignment may violate the triangle inequality.\nResults As in the previous setting, we compare \u03b4-medoids to the k-medoids algorithm as well as the greedy k-center heuristic, for five different game logs and five different \u03b4 values. The results are presented in Figure 5. As before, for each \u03b4, we searched exhaustively for the optimal choice of k in k-medoids.\nThe results reinforce the conclusions reached in the previous domain - for all choices of \u03b4 we meet the distance requirement using a much smaller representative set compared to the k-medoids and spectral clustering approaches (which does much worse in this domain compared to the previous one). Furthermore, \u03b4-medoids once again does at least as well as the k-centers heuristic. In terms of average distance, our algorithm performs much better compared to the k-centers heuristic, suggesting that the \u03b4-medoids algorithm generally outperforms the other approaches."}, {"heading": "5.4 Stability of the \u03b4-Medoids Algorithm", "text": "In this section we establish that indeed the \u03b4-medoids algorithm is robust with respect to scan order (satisfying desideratum 1 from Section 1). To test stability, we ran \u03b4-medoids, k-medoids, the k-center heuristic and spectral clustering multiple times on a large collection of datasets, reshuffling the input order on each iteration, and examined how well preserved the representative set was across iterations and methods. Our analysis indicated that the first three algorithms consistently obtain > 90% average overlap, and the level of stability observed is almost identical. Spectral clustering yields drastically less stable representative sets. For a fuller description of these results see Appendix B."}, {"heading": "6 Summary and Discussion", "text": "In this paper, we present a novel heuristic algorithm to solve the representative selection problem: finding the smallest possible representative subset that best fits the data under the constraint that no sample in the data is more than a predetermined parameter \u03b4 away from some representative. We introduce the novel \u03b4-medoids algorithm and show it outperforms other approaches that are only concerned with either best fitting the data into a given number of clusters, or minimizing the maximal distance. There is a subtle yet significant impact to focusing on a maximal distance criterion \u03b4 rather than choosing the number of clusters k. While both \u03b4-medoids and k-medoids aim to minimize the sum of distances between representatives and\nthe full set, k-medoids does so with no regard to any individual distance. Because of this, we need to increase the value of k drastically in order to guarantee that our distance criterion is met, and that sparse regions of our sample set are sufficiently represented. This results in over-representation of dense regions in our sample set. By carefully balancing between minimality under the distance constraint and average distance minimization, the \u03b4-medoids algorithm adjusts the representation density adaptively based on the sample set, without any prior assumptions. Although this paper establishes \u03b4-medoids as a leading algorithm for representative selection, we believe that more sophisticated algorithms can be developed to handle different variations of this problem, putting different emphasis on the minimality requirement for the representative set vs. how well the set fits the data. Depending on the specific nature of the task the representatives are needed for, different tradeoffs may be most appropriate and lead to algorithmic variations. For instance, an extension of interest could be to modify the value of \u03b4 adaptively depending of the density of sample neighborhoods. However, we show that \u03b4-medoids is a promising approach to the general problem of efficient representative selection."}, {"heading": "Acknowledgements", "text": "This work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation (CNS-1330072, CNS1305287), ONR (21C184-01), AFOSR (FA8750-14-1-0070, FA9550-14-1-0087), and Yujin Robot."}, {"heading": "A Proof of Convergence for the \u03b4-medoids Algorithm", "text": "In this section, we show that the full proof that the \u03b4-medoids algorithm converges in finite time.\nTheorem 1 Algorithm 3 converges after a finite number of steps.\nProof Sketch: For any sample s let us denote its associated cluster representative at iteration i Ci(s). Let us denote the distance between the sample and its associated cluster representative as d(s, Ci(s)). Observe the overall sum of distances from each point to its associated cluster representative, \u2211 s d(s, Ci(s)). Assume that after the i-th round, we obtain a partition to k clusters, C1..Ck. Our next step is to go over each cluster and reassign a representative sample that minimizes the sum of distances from each point to the representative of that cluster, argmin\ns\u2208S\n\u2211 x\u2208S d(x, s), under the constraints that all samples within\nthe cluster are still within \u03b4 distance of the representative. Since this condition holds prior to the minimization phase, the new representative must still either preserve or reduce the sum of distances within the cluster. We do this independently for each cluster. If the representatives are unchanged, we have reached convergence and the algorithm stops. Otherwise, the overall sum of distances is diminished. At the (i+ 1)-ith round, we reassign clusters for the samples. A sample can either remain within the same cluster or move to a different cluster. If a sample remains in the same cluster its distance from its associated representative is unchanged. On the other hand, if it moves to a different cluster it means that d(s, Ci(s)) > d(s, Ci+1(s)), necessarily, so the overall sum of distances from associated cluster representatives is reduced. Therefore, after each iteration we either reach convergence or the sum of distances is reduced. Since there is a finite number of samples, there is a finite number of distance sums, which implies that the algorithm must converge after a finite number of iterations. ut"}, {"heading": "B Stability of \u03b4-Medoids", "text": "In this section we establish that indeed the \u03b4-medoids algorithm is robust with respect to scan order (satisfying desideratum 1 from Section 1). To test this issue, we generated a large collection (N = 1000) of datasets (randomly sampled from randomly generated multimodal distributions). For each dataset in the collection, we ran the algorithm #repetitions = 100 times, each time reshuffling the input order. Next, we calculated the average overlap between any two representative sets generated by this procedure for the same dataset. We then calculated a histogram of average overlap score over all the data inputs. Finally, we compared these stability results to those obtained by the k-medoids algorithm (which randomizes starting positions), the k-centers heuristic (which randomizes the starting node), and spectral clustering (which uses k-means to partition the eigenvectors of the normalized Laplacian). Our analysis indicated\nthat for the first three algorithms, in more than 90% of the generated datasets, there was a > 90% average overlap. The overlaps observed are almost exactly the same, implying the expected extent of overlap is dependent more on the structure of the data than on the type of randomization the algorithm employs. This serves as evidence that \u03b4-medoids is sufficiently stable, as desired. It should be noted that spectral clustering yields drastically less stable results (ranging between 15% and 40% overlap), implying a heightened level of stochasticity in the partitioning phase. A histogram indicating our results can be found in Figure 6. One can see that our algorithm has virtually identical stability compared\nto both k-medoids and the greedy k-center approaches (which, as stated before, are not sensitive to scan order but contain other types of randomization)."}, {"heading": "C Performance of \u03b4-Medoids in Metric Spaces", "text": "As we state in the paper, though the \u03b4-medoids algorithm is designed to handle non-metric settings, it can easily be used in metric cases as well. In this section we compare the performance of the algorithm to the benchmark methods used in the\nEmpirical Results section. To generate a standard metric setting, we consider a 10-dimensional metric space where samples are drawn from a multivariate Gaussian distribution. We sample a 1000 samples per experiment, 20 experiments per setting, with randomly chosen means and variances. The results are presented in Figure 7.\nAs one observe, the performance of the \u03b4-medoids algorithm relative to the other methods is qualitatively the same compared to the non-metric cases, despite the metric property of the data in this setting."}, {"heading": "D Extended Analysis", "text": "In this section, we consider the hardness of the representative selection problem, and discuss the efficiency of the \u03b4-medoids algorithm.\nD.1 NP-Hardness of the Representative Selection Problem\nTheorem 2 Satisfying Criterion 1 (minimal representative set) is NP-Hard.\nProof Sketch: We show this via a reduction from the vertex cover problem. Given a graph G = (V,E) we construct a distance matrix M of size |V | \u00d7 |V |. If two different vertices in the graph, vi, vj , i 6= j, are connected, we set the value of entries (i, j) and (j, i) in M to be \u03b4 \u2212 1. Otherwise, we set the value of the entry to \u03b4 + 1. Formally:\nM(i, j) =  \u03b4 \u2212 1 if (i, j) \u2208 E 0 i = j\n\u03b4 + 1 otherwise\nThis construction is polynomial in |V | and |E|. Let us assume we know how to obtain the optimal representative set for \u03b4 in this case. Then the representative set Srep can be easily translated back to a vertex cover for graph G - simply choose all the vertices that correspond to members of the representative set. Every sample i in the sample set induced by M has to be within \u03b4 range of some representative j, meaning that there is an equivalent edge (i, j) \u2208 E, which j covers. Since the representative set is minimal, the vertex set is also guaranteed to be the minimal. Therefore, if we could solve the representative selection problem efficiently, we could also solve the vertex cover problem. Since vertex cover is known to be NP-hard [21], then so is representative selection. ut\nD.2 Bounds on \u03b4-medoids in Metric Spaces\nThe \u03b4-medoids algorithm is agnostic to the existence of metric space in which the samples can be embedded. However, we can show that given that the distance measure which generates the pairwise distances is in fact a metric, certain bounds on performance ensue.\nTheorem 3 In a metric space, the average distance of a representative set |C| = k obtained by the \u03b4-medoids algorithm is bound by 2OPT where OPT is the maximal distance obtained by an optimal assignment of k representatives (with respect to maximal distance).\nTo prove this theorem, and the following one, we will first prove the following helper lemma:\nLemma 1 In a metric space, the maximal distance of a representative set |C| = k obtained by the one-shot \u03b4-representative algorithm (Algorithm 2) is bound by 2OPT where OPT is the maximal distance obtained by an optimal assignment of k representatives (with respect to maximal distance).\nProof Sketch: Let |K| = k be the representatives set returned by Algorithm 2. Let a\u2217 be the sample which is the farthest of any points in the representative set, and let that distance be \u03b4\u2217. Consider the set K \u222a {a\u2217}. All k + 1 points in this set must be of distance > \u03b4\u2217 from one another - the algorithm would not select representatives of distance \u2264 \u03b4 from one another, and \u03b4 \u2265 \u03b4\u2217, whereas\na\u2217 is defined as being exactly \u03b4\u2217 away from any point in K. Let us consider the optimal assignment of k representatives, K\u2217, and let OPT be the maximal distance it achieves. By the pigeonhole principle, at least two samples in the set K \u222a {a\u2217} must be associated with the same representative. W.log, let us call these samples x1 and x2, and k\n\u2217 \u2208 K\u2217 their associated representative. Since the distance between x1 and x2 is greater than \u03b4\n\u2217, and since this is a metric space, by the triangle inequality, the distance of k\u2217 from either cannot be smaller than \u03b4\u2217\n2 . Therefore \u03b4 \u2217 < 2OPT . ut\nThis implies that algorithm 2 is asymptotically equivalent to the k-centers farthestfirst traversal heuristic with respect to maximal distance.\nNow we can prove Theorem 3. Proof Sketch: First, let us consider Algorithm 2 (one-shot \u03b4-representatives) on which the \u03b4medoids algorithm is based. By Lemma 1, the maximal distance obtained by it for a representative set of size k is < 2OPT , where OPT is the maximal distance obtained by an optimal solution of size k (with respect to maximal distance). The average distance obtained by Algorithm 2 cannot be greater than the maximal distance, so the same bound holds the average distance as well. Now let us consider the full \u03b4-medoids algorithm - by definition, it can only reduce the average distance (while maintaining the same representative set size). So the average distance obtained by the \u03b4-medoids algorithm must be bound by 2OPT as well. ut\nTheorem 4 The size of the representative set returned by the \u03b4-medoids algorithm, k, is bound by k \u2264 N( \u03b42 ) where N(x) is the minimal number of representatives required to satisfy distance criterion x.\nProof Sketch: By Lemma 1, the maximal distance obtained by it for a representative set of size k is < 2OPT , where OPT is the maximal distance obtained by an optimal solution of size k (with respect to maximal distance). Let N(\u03b4) be the covering number for the sample set and distance criterion \u03b4 - that is, the smallest number of representative required so that no sample is farther than \u03b4 from a representative. The size of the representative set returned by the \u03b4medoids algorithm, is bound by k \u2264 N( \u03b42 ). Since the full \u03b4-medoids algorithm (Algorithm 3) first runs Algorithm 2 and is guaranteed to never increase the size of the representative set size, the same bound holds for it as well. ut In Rd, the covering number N( ) is bound by O( 1 d ). Given that N(\u03b4) \u2265 K\u2217 where K\u2217 is the optimal selection of representatives, this implies the solution returned by \u03b4-medoids is bound by a factor of O(2d). It is equivalent to the similar bound known for the k-center heuristic [18,19].\nD.3 Hardness of Approximation of Representative Selection in Non-Metric Spaces\nIn non-metric spaces, the representative selection problem becomes much harder. We now show that no c-approximation exists for the representative selection\nproblem either with respect to the first criterion (representative set size) or the second criterion (distance - we focus on maximal distance but a similar outcome for average distance is implied).\nTheorem 5 No constant-factor approximation exists for the representative selection set problem with respect to representative set size.\nProof Sketch: We show this via a reduction from the set cover problem. Given a set of n sets over |S| = s elements, we construct a graph G = (V,E) containing |V | = s+n nodes - one node for each subset, and one node for each element. The graph is fully connected (|E| = |V | \u00d7 |V |). Let |N | = n and |M | = s be the sets of nodes for subsets and elements, respectively. We define the distance matrix between elements in the graph (i.e. weights on the edges) to be as follows:\nM(i, j) =  \u03b4 \u2212 1 if i \u2208 N and j \u2208M 0 if both i \u2208 N and j \u2208 N \u03b4 + 1 if i \u2208M\nIn other words, each node representing a subset is connected to itself and the other subset nodes with an edge of weight 0, and to the respective node of each element it comprises with an edge of weight \u03b4\u2212 1. Element nodes are connected to all nodes with edges of weight \u03b4+1. This construction takes polynomial time. Note that the distance of any element in N (representing subsets) to itself is 0, and the distance of every element inM to itself is \u03b4+1. Let us assume we have a capproximating algorithm for the representative selection problem with respect to representative set size. Any solution obtained by this algorithm with parameter \u03b4 would also yield a c-approximation for the set cover problem. Let us observe any result of such an algorithm - it would not return any nodes representing elements (because they are > \u03b4 distant from any node in the graph including themselves). The distance between any nodes representing subsets in the graph is 0, so a single subset node enough is sufficient to cover all of N . Therefore, the representative set will only comprise elements from N , which directly cover elements in M . An optimal solution for the representative selection algorithm will also serve as an optimal solution for the original set cover problem, and vice versa (otherwise a contradiction ensues). Therefore, a c-approximation (with respect to set size) for the representative selection problem would also mean a c-approxmiation for the set cover problem. However, it is known that no approximation better than clogn is possible [30]. Therefore, a c-approximating algorithm for the representative selection problem (with respect to set size) cannot be obtained unless P = NP .\nut\nTheorem 6 For representative sets of optimal size k,8 no constant-factor approximation exists with respect to the maximal distance between the optimal representative set and the samples.\n8 In fact, this proof applies for any value of k that cannot be directly manipulated by the algorithm.\nProof Sketch:We show this via a reduction from the dominating set problem. Given a graph G = (V,E), a dominating set is defined as a subset V \u2217 \u2282 V so that every node v \u2208 V that\u2019s not in V \u2217 is adjacent to at least one member of V \u2217. Finding the minimal dominating set is known to be NP-complete [15].\nAssume we are given a graph G and are required to find a minimal dominating set. Let us generate a new graph G\u2032 = (V,E\u2032), where V are the original nodes of G and the graph is fully connected: |E| = |V |\u00d7 |V |. The weights on the edges are defined as follows:\nM(i, j) =  (\u03b4 \u2212 1) if (i, j) \u2208 E (original graph) 0 i=j\n2 \u00b7 c \u00b7 (\u03b4 \u2212 1) otherwise\nThis reduction is polynomial. Let us consider an optimal representative set with parameter \u03b4 for G\u2032. Assume it is of size k. This would imply there\u2019s a dominating set of size k which is the minimal dominating set obtaining in graph G. This dominating set is minimal, otherwise the representative selection set would not be optimal. Let us assume we have an algorithm for representative selection that\u2019s c-approximating with respect to maximal distance. If there is a dominating set of size k, it would imply a guarantee of c \u00b7 (\u03b4 \u2212 1) on the maximal distance, implying the algorithm would behave the same as an optimal algorithm (since it cannot use edges of weight 2 \u00b7 c \u00b7 (\u03b4\u2212 1)). For this reason, a c-maximum-distance approximation algorithm for the representative selection problem could be used to solve the dominating set problem. Since this problem is NP-hard, it implies no such approximation algorithm exists unless P = NP .\nut\nD.4 Efficiency of \u03b4-Medoids\nThe actual run time of the algorithm is largely dependent on the data and the choice of \u03b4. An important observation is that at each iteration, each sample is only compared to the current representative set, and a sample is introduced to the representative set only if it is > \u03b4 away from all other representatives. After each iteration, the representatives induce a partition to clusters and only samples within the same cluster are compared to one another. A poor choice of \u03b4, for instance \u03b4 < min{d(xi, xj)|xi, xj \u2208 S} would cause all the samples to be added to the representative set, resulting in a runtime complexity of O(|S|2). In practice, however, since we only compare from samples to representatives and within clusters, for reasonable cases, we can get considerably better runtime performance. For instance, if the number of representatives is close to \u221a |S|, the complexity would be reduced to |S|1.5, which results in a significant speed-up. Again, note that in each iteration of the algorithm, after the partitioning phase (the RepAssign subroutine in Algorithms 2 and 3) the algorithm maintains a legal representative set, so in practice we can halt the algorithm before convergence, depending on need and resources."}, {"heading": "E Calculating the Distance Measures", "text": "In this section we describe in some detail how the distance measures we used were computed, as well as some of the considerations that were involved in their formulation.\nE.1 Musical Segments Distance\nSegment Information Every segment is transposed to C. Then, the following information is extracted from each segment:\n\u2013 Pitch Sequence - the sequential representation of pitch frequency over time. \u2013 Pitch Bag - a \u201cbag\u201d containing all the pitches in the sequence, with sensitivity\nto registration. \u2013 Pitch Class Bag - a \u201cbag\u201d containing all the pitches in the sequence, without\nsensitivity to registration. \u2013 Rhythm Bag - a \u201cbag\u201d containing all rhythmic patterns in the sequence.\nA rhythmic pattern is defined, for simplicity, as pairs of subsequent note durations in the sequence. \u2013 Interval Bag - a \u201cbag\u201d containing all pitch intervals in the sequence. \u2013 Step Bag - a \u201cbag\u201d containing all one-step pitch differences in the sequence.\nthis is similar to intervals, only it is sensitive to direction.\nSegment Distance We devise a fairly complex distance measure between any two musical segments, seg1 and seg2. Several factors are taken into account:\n\u2013 Global alignment - the global alignment score between the two segments. This is calculated using the Needleman-Wunsch[27] algorithm. \u2013 Local alignment - the local alignment score between the two segments. This is calculated using the Smith-Waterman[36] algorithm. \u2013 rhythmic overlap - the extent to which one-step rhythmic patterns in the two segments overlap. \u2013 interval overlap - the extent to which one-step interval patterns in the two segments overlap. \u2013 step overlap - the extent to which melodic steps in the two segments overlap. \u2013 pitch overlap - the extent to which the pitch sets in the two segments overlap.\nThis measure is sensitive to registration. \u2013 pitch class overlap - the extent to which the pitch sets in the two segments\noverlap. This measure is invariant to registration.\nThe two alignment measures are combined to a single alignment score. The other measures were also combined to a separate score, which we name the bag distance. The two scores were combined using the l2 norm as follows:\nscorealignment = alignment 2 global + 2 \u00b7 alignment2local\nscorebag = score 2 rhythmic + score 2 interval + score 2 step\n+score2pitch + score 2 pitchClass distance = \u221a 10 \u00b7 scorebag + scorealignment\nSubstitution Function For both the local alignment and the global alignment we used a simple exponentially attenuating function based on frequency distance to characterize the likelihood for swaps between any two notes. The function is defined as follows:\ncost(A,B) =  1, |A\u2212B| = 3rd 1, |A\u2212B| = 5th 1.3 |Pitchmid(A)\u2212Pitchmid(B)| 4 , otherwise\nThe price of introducing gaps was fixed at 1.5.\nBag Distance To get the bag distance score between two bags we use the calculation |Bag1\u2206Bag2||Bag1\u222aBag2| .\nExample Two example segments are given in Figure 8 in musical notation and in Figure 9 as midi pitch over time.\nThe local alignment distance for these two segments is 0.032. The global alignment distance for these two segments is 1.5. The bag distance score for these two segments is 2.03 The overall combined distance for these two segments after weighting is 20.4.\nE.2 Movement Segments Distance\nSegment Information Each segment is comprised of one agent\u2019s (x, y) coordinates for 10 consecutive timestamps. Then, each segment is translated to\nstart from coordinates (0, 0), and rotated so that for all segments all players are facing the same goal. In addition to maintaining the coordinate sequence, from each such segment we extract a bag of movement-turn pairs, where the movement represents distance turns are qunatized into 6 angle bins: forward (\u221230 - +30 degrees), upper right (+30 - +90 degrees), lower right (+90 - +150 degrees), backwards (\u2212150 - +150 degrees), lower left (\u221290 - \u2212150 degrees), and upper left (\u221230 - \u221290 degrees). For instance, the coordinate sequence (0, 0), (0, 10), (5, 10), (8, 14) induces two movement-turn elements: 10+upper-right-turn, 5+upper-left-turn.\nSegment Distance Given two trajectories, one can compare them as contours in 2-dimensional space. We take an alignment-based approach, with edit step costs being the RMS distance between them. Our distance measure is comprised of three elements:\n\u2013 Global Alignment - The global alignment distance between the two trajectories once initially aligned together (that is, originating from (0, 0) coordinates), calculated by the Needleman-Wunsch algorithm.\n\u2013 Local Alignment - The local alignment distance between the two trajectories, calculated by the Smith-Waterman Algorithm.\n\u2013 Movement-Turn bag of words distance - we compare the bag distance of movement-turn elements. We quantize distances into a resolution of 5 meters to account for variation.\n\u2013 Overall \u2206-distance and \u2206-angle distance - We also consider the overall similarity of the segments in terms of total distance travelled (and the direction of the movement).\nThe scores are combined as follows:\nscorealign = alignment 2 global + 2.5 \u2217 alignment2local\nscoreOverall\u2206 = \u2206\u2212 distance2 + (10 \u00b7\u2206\u2212 angle)2 distance = \u221a 100 \u00b7 scorebag + scorealign + scoreOverall\u2206\nSubstitution Function To get the substitution cost for the two alignment algorithms, we simply use the RMS distance between the two coordinates we are comparing. Given two points P1 = (x1, y1) and P2 = (x2, y2) the distance is simply D(P1, P2) = \u221a (x1 \u2212 x2)2 + (y1 \u2212 y2)2. Gaps were greatly penalized with a penalty of 100 because gaps create discontinuous (and therefore physically impossible) sequences.\nBag Distance To get the bag distance score between two bags we use the calculation |Bag1\u2206Bag2||Bag1\u222aBag2| .\nExample Two example segments are given in Figure 10. The local alignment distance for these two segments is 20. The global alignment distance for these two segments is 192.7. The overall delta distance and angle score for these two segments is 61.66 The overall combined distance for these two segments after weighting is 258.72."}], "references": [{"title": "A new approach to data driven clustering", "author": ["Arik Azran", "Zoubin Ghahramani"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Aggregate features and a da b oost for music classification", "author": ["James Bergstra", "Norman Casagrande", "Dumitru Erhan", "Douglas Eck", "Bal\u00e1zs K\u00e9gl"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Opponent modeling in poker", "author": ["Darse Billings", "Denis Papp", "Jonathan Schaeffer", "Duane Szafron"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Opponent modeling in multi-agent systems. In Adaption and Learning in Multi-Agent Systems, pages 40\u201352", "author": ["David Carmel", "Shaul Markovitch"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Content-based music information retrieval: current directions and future challenges", "author": ["Michael A Casey", "Remco Veltkamp", "Masataka Goto", "Marc Leman", "Christophe Rhodes", "Malcolm Slaney"], "venue": "Proceedings of the IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Parallel spectral clustering in distributed systems", "author": ["Wen-Yen Chen", "Yangqiu Song", "Hongjie Bai", "Chih-Jen Lin", "Edward Y Chang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Automatic selection of representative photo and smart thumbnailing using near-duplicate detection", "author": ["Wei-Ta Chu", "Chia-Hung Lin"], "venue": "In Proceedings of the 16th ACM international conference on Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Computational and comparative musicology", "author": ["Nicholas Cook"], "venue": "Empirical musicology: aims,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "music21: A toolkit for computeraided musicology and symbolic music data", "author": ["Michael Scott Cuthbert", "Christopher Ariza"], "venue": "In Int. Society for Music Information Retrieval Conf.(ISMIR", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["Jason V Davis", "Brian Kulis", "Prateek Jain", "Suvrit Sra", "Inderjit S Dhillon"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Distance-based clustering of xml documents", "author": ["Francesco De Francesca", "Gianluca Gordano", "Riccardo Ortale", "Andrea Tagarelli"], "venue": "In ECML/PKDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Using machine-learning methods for musical style modeling", "author": ["Shlomo Dubnov", "Gerard Assayag", "Olivier Lartillot", "Gill Bejerano"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Using representative-based clustering for nearest neighbor dataset editing", "author": ["Christoph F Eick", "Nidal Zeidat", "Ricardo Vilalta"], "venue": "In Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Finding exemplars from pairwise dissimilarities via simultaneous sparse recovery", "author": ["Ehsan Elhamifar", "Guillermo Sapiro", "Rene Vidal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Computers and intractability: A guide to the theory of np-completeness", "author": ["Michael R Gary", "David S Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1979}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["Teofilo F Gonzalez"], "venue": "Theoretical Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "Video summarization by k-medoid clustering", "author": ["Youssef Hadi", "Fedwa Essannouni", "Rachid Oulad Haj Thami"], "venue": "In Proceedings of the 2006 ACM symposium on Applied computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A best possible heuristic for the k-center problem", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Mathematics of operations research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1985}, {"title": "A unified approach to approximation algorithms for bottleneck problems", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}, {"title": "Data clustering: a review", "author": ["Anil K Jain", "M Narasimha Murty", "Patrick J Flynn"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Reducibility Among Combinatorial Problems", "author": ["R.M. Karp"], "venue": "Complexity of Computer Computations,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1972}, {"title": "The robocup synthetic agent challenge 97", "author": ["Hiroaki Kitano", "Milind Tambe", "Peter Stone", "Manuela Veloso", "Silvia Coradeschi", "Eiichi Osawa", "Hitoshi Matsubara", "Itsuki Noda", "Minoru Asada"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Social tagging and music information retrieval", "author": ["Paul Lamere"], "venue": "Journal of New Music Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["James MacQueen"], "venue": "In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1967}, {"title": "Advances in automatic text summarization", "author": ["Inderjeet Mani", "Mark T Maybury"], "venue": "MIT press,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Wearable hand activity recognition for event summarization", "author": ["WW Mayol", "DW Murray"], "venue": "In Wearable Computers,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["Saul B Needleman", "Christian D Wunsch"], "venue": "Journal of molecular biology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1970}, {"title": "Automatic summarization", "author": ["Ani Nenkova", "Kathleen Rose McKeown"], "venue": "Now Publishers Inc,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "A comparison of statistical and rulebased models of melodic segmentation", "author": ["MT Pearce", "D M\u00fcllensiefen", "GA Wiggins"], "venue": "In Proceedings of the Ninth International Conference on Music Information Retrieval,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "A sub-constant error-probability low-degree test, and a sub-constant error-probability pcp characterization of np", "author": ["Ran Raz", "Shmuel Safra"], "venue": "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Finding groups in data: An introduction to cluster analysis", "author": ["Peter J Rousseeuw", "Leonard Kaufman"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1990}, {"title": "The neighbor-joining method: a new method for reconstructing phylogenetic trees", "author": ["Naruya Saitou", "Masatoshi Nei"], "venue": "Molecular biology and evolution,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1987}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "Efficient spectral neighborhood blocking for entity resolution", "author": ["Liangcai Shu", "Aiyou Chen", "Ming Xiong", "Weiyi Meng"], "venue": "In Data Engineering (ICDE),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Slink: an optimally efficient algorithm for the single-link cluster method", "author": ["Robin Sibson"], "venue": "The Computer Journal,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1973}, {"title": "Identification of common molecular subsequences", "author": ["T.F. Smith", "M.S. Waterman"], "venue": "Journal of molecular biology,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1981}, {"title": "Resc: An approach for real-time, dynamic agent tracking", "author": ["Milind Tambe", "Paul S Rosenbloom"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Beyond kmedoids: Sparse model based medoids algorithm for representative selection", "author": ["Yu Wang", "Sheng Tang", "Feidie Liang", "YaLin Zhang", "JinTao Li"], "venue": "In Advances in Multimedia Modeling,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P Xing", "Andrew Y Ng", "Michael I Jordan", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Survey of clustering algorithms", "author": ["Rui Xu", "Donald Wunsch"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Discovery of collocation patterns: from visual words to visual phrases", "author": ["Junsong Yuan", "Ying Wu", "Ming Yang"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}], "referenceMentions": [{"referenceID": 19, "context": "Several surveys of clustering techniques can be found in the literature [20,41].", "startOffset": 72, "endOffset": 79}, {"referenceID": 40, "context": "Several surveys of clustering techniques can be found in the literature [20,41].", "startOffset": 72, "endOffset": 79}, {"referenceID": 10, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 173, "endOffset": 177}, {"referenceID": 16, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 207, "endOffset": 213}, {"referenceID": 6, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 207, "endOffset": 213}, {"referenceID": 27, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 237, "endOffset": 241}, {"referenceID": 38, "context": "It has also been discussed as a general problem in [39].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "For instance, given a large document and a satisfactory measure of similarity between sentences, text summarization [25] could be framed as a representative selection task - obtain a subset of sentences that best captures the nature of the document.", "startOffset": 116, "endOffset": 120}, {"referenceID": 41, "context": "Similarly, one could map this problem to extracting \u201cvisual words\u201d or representative frames from visual input [42,26].", "startOffset": 110, "endOffset": 117}, {"referenceID": 25, "context": "Similarly, one could map this problem to extracting \u201cvisual words\u201d or representative frames from visual input [42,26].", "startOffset": 110, "endOffset": 117}, {"referenceID": 4, "context": "\u2013 Music analysis - the last decade has seen a rise in the computational analysis of music databases for music information retrieval [5], recommender systems [23] and computational musicology [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 22, "context": "\u2013 Music analysis - the last decade has seen a rise in the computational analysis of music databases for music information retrieval [5], recommender systems [23] and computational musicology [8].", "startOffset": 157, "endOffset": 161}, {"referenceID": 7, "context": "\u2013 Music analysis - the last decade has seen a rise in the computational analysis of music databases for music information retrieval [5], recommender systems [23] and computational musicology [8].", "startOffset": 191, "endOffset": 194}, {"referenceID": 2, "context": "\u2013 Team strategy/behavior analysis - opponent modeling has been discussed in several contexts, including game playing [3], real-time agent tracking [37] and general multiagent settings [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 36, "context": "\u2013 Team strategy/behavior analysis - opponent modeling has been discussed in several contexts, including game playing [3], real-time agent tracking [37] and general multiagent settings [4].", "startOffset": 147, "endOffset": 151}, {"referenceID": 3, "context": "\u2013 Team strategy/behavior analysis - opponent modeling has been discussed in several contexts, including game playing [3], real-time agent tracking [37] and general multiagent settings [4].", "startOffset": 184, "endOffset": 187}, {"referenceID": 23, "context": "This constraint renders many common clustering techniques inapplicable, including the canonical k-means [24], or more recent works such as [39].", "startOffset": 104, "endOffset": 108}, {"referenceID": 38, "context": "This constraint renders many common clustering techniques inapplicable, including the canonical k-means [24], or more recent works such as [39].", "startOffset": 139, "endOffset": 143}, {"referenceID": 31, "context": "Among such methods are neighbor-joining [32], which becomes unreliable when applied on non-metric data, or the k-prototypes algorithm [1].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "Among such methods are neighbor-joining [32], which becomes unreliable when applied on non-metric data, or the k-prototypes algorithm [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 34, "context": "Nevertheless, certain clustering algorithms still apply, such as the k-medoids algorithm, hierarchical clustering [35], and spectral clustering [38].", "startOffset": 114, "endOffset": 118}, {"referenceID": 37, "context": "Nevertheless, certain clustering algorithms still apply, such as the k-medoids algorithm, hierarchical clustering [35], and spectral clustering [38].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "The k-medoids algorithm [31] is a variation on the classic k-means algorithm that only selects centers from the original dataset, and is applicable to data organized as a pairwise distance matrix.", "startOffset": 24, "endOffset": 28}, {"referenceID": 39, "context": "3 In certain contexts, metric learning [40,10] can be applied, but current methods are not well suited for data without vector space representation, and in some sense, learning a metric is of lesser interest for representative selection, as we care less about classification or the structural relations latent in the data.", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "3 In certain contexts, metric learning [40,10] can be applied, but current methods are not well suited for data without vector space representation, and in some sense, learning a metric is of lesser interest for representative selection, as we care less about classification or the structural relations latent in the data.", "startOffset": 39, "endOffset": 46}, {"referenceID": 33, "context": "In the case of spectral clustering, an efficient algorithm that does not compute the full distance matrix exists [34], but it relies on a vector space representation of the data, rendering it inapplicable in our case.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The k-centers problem is defined as follows: Given a set S and a number k, find a subset R \u2282 S, |R| = k so that maxs\u2208Sd(s,R) is minimal [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 5, "context": "[6] 6 We note that no better approximation scheme is possible under standard complexity theoretic assumptions [18].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[6] 6 We note that no better approximation scheme is possible under standard complexity theoretic assumptions [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "Another algorithm that is related to this problem is Gonzales\u2019 approximation algorithm for minimizing the maximal cluster diameter [16], which iteratively takes out elements from existing clusters to generate new clusters based on the inter-cluster distance.", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Lastly, in a recent, strongly related paper [14], the authors consider a similar problem of selecting exemplars in data to speed up learning.", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "We compare ourselves to three alternative methods - k-medoids, the greedy k-center heuristic, and spectral clustering (using cluster medoids as representatives) [33], and show we outperform all three.", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "We note that although these methods weren\u2019t necessarily designed to tackle the representative selection problem, they, and clustering approaches in general, are used for such purposes in practice (see [17], for instance).", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "The need for good representative selection is driven by several tasks, including style characterization, comparing different musical corpora (see [12]), and music classification by composer, genre or period [2].", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "The need for good representative selection is driven by several tasks, including style characterization, comparing different musical corpora (see [12]), and music classification by composer, genre or period [2].", "startOffset": 207, "endOffset": 210}, {"referenceID": 8, "context": "For the purpose of this work we used the Music21 corpus, provided in MusicXML format [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 28, "context": "The melodic lines in the pieces are isolated and segmented using basic grouping principles adapted from [29].", "startOffset": 104, "endOffset": 108}, {"referenceID": 26, "context": "\u2013 Global alignment - the global alignment score between the two segments, calculated using the Needleman-Wunsch algorithm [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 35, "context": "\u2013 Local alignment - the local alignment score between the two segments, calculated using the Smith-Waterman algorithm [36].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "The robot world-cup soccer competition (RoboCup) is a wellestablished problem domain for AI in general [22].", "startOffset": 103, "endOffset": 107}], "year": 2015, "abstractText": "This paper considers the problem of representative selection: choosing a subset of data points from a dataset that best represents its overall set of elements. This subset needs to inherently reflect the type of information contained in the entire set, while minimizing redundancy. For such purposes, clustering may seem like a natural approach. However, existing clustering methods are not ideally suited for representative selection, especially when dealing with non-metric data, where only a pairwise similarity measure exists. In this paper we propose \u03b4medoids, a novel approach that can be viewed as an extension to the k-medoids algorithm and is specifically suited for sample representative selection from non-metric data. We empirically validate \u03b4-medoids in two domains, namely music analysis and motion analysis. We also show some theoretical bounds on the performance of \u03b4-medoids and the hardness of representative selection in general.", "creator": "LaTeX with hyperref package"}}}