{"id": "1503.00168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "The NLP Engine: A Universal Turing Machine for NLP", "abstract": "it is commonly true that machine translation is a more complex task than part of speech tagging. but how much more complex? in this paper we make an attempt to develop a general framework and methodology covering computing the informational and / or processing complexity of nlp applications and tasks. we define a universal framework akin to a turning machine that attempts easily fit ( linear ) nlp tasks into one paradigm. we calculate the complexities against various nlp tasks using measures of shannon depth, and compare ` simple'blocks chosen as part of image tagging to ` complex'ones such as machine speaking. this paper provides a first, only far from perfect, attempt to quantify nlp tasks under a uniform paradigm. we kiss away current accomplishments and suggest intriguing avenues for fruitful research.", "histories": [["v1", "Sat, 28 Feb 2015 19:46:50 GMT  (29kb)", "http://arxiv.org/abs/1503.00168v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "eduard hovy"], "accepted": false, "id": "1503.00168"}, "pdf": {"name": "1503.00168.pdf", "metadata": {"source": "CRF", "title": "The NLP Engine: A Universal Turing Machine for NLP", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu", "ehovy@andrew.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n00 16\n8v 1\n[ cs\n.C L\n] 2\n8 Fe\nb 20\n15"}, {"heading": "1 Introduction", "text": "The purpose of this paper is to suggest a unified framework in which modern NLP research can quantitatively describe and compare NLP tasks. Even though everyone agrees that some NLP tasks are more complex than others, e.g., machine translation is \u2018harder\u2019 than syntactic parsing, which in turn is \u2018harder\u2019 than part-of-speech tagging, we cannot compute the relative complexities of different NLP tasks and subtasks.\nIn the typical current NLP paradigm, researchers apply several machine learning algorithms to a problem, report on their performance levels, and establish the winner as setting the level to beat in the future. We have no single overall model of NLP that subsumes and regularizes its various tasks. If you were to ask NLP researchers\ntoday they would say that no such model is possible, and that NLP is a collection of several semiindependent research directions that all focus on language and mostly use machine learning techniques. Researchers will tell you that a good summarization system on DUC/TAC dataset obtains a ROUGE score of 0.40, a good French-English translation system achieves a BLUE score of 37.0, 20-news classifiers can achieve accuracy of 0.85, and named entity recognition systems a recall of 0.95, and these numbers are not comparable. Further, we usually pay little attention to additional important factors such as the performance curve with respect to the amount of training data, the amount of preprocessing required, the size and complexity of auxiliary information required, etc. And even when some studies do report such numbers, in NLP we don\u2019t know how to characterize these aspects in general and across applications, how to quantify them in relationship to each other.\nWe here describe our first attempt to develop a single generic high-level model of NLP. We adopt the model of a universal machine, akin to a Turing Machine but specific to the concerns of language processing, and show how it can be instantiated in different ways for different applications. We employ Shannon Entropy within the machine to measure the complexity of each NLP task.\nIn his epoch-making work, Shannon (1951) demonstrated how to compute the amount of information in a message. He considered the case in which a string of input symbols is considered one by one, and the uncertainty of the next is measured by counting how difficult it is to guess. We make the fundamental assumption that most NLP tasks can be viewed as transformations of notation, in which a stream of input symbols is transformed and/or embellished into a stream of output symbols (for example, POS tagging is the task of embellishing each symbol with its tag, and MT is the task of outputting the appropriate translation\nword(s)). Under this assumption one can ask: how much uncertainty is there in making the embellishment or transformation? This clearly depends on the precise nature of the task, on the associated auxiliary knowledge resources, and on the actual algorithm employed. We discuss each of these issues below. We first describe the key challenge involved in performing uncertainty comparison using the Entropy measure in Section 2. In Section 3 we provide high-level comments on what properties a framework should have to enable fair complexity comparison. In Section 4, based on the properties identified in Section 3, we consider the theoretical nature of NLP tasks and provide suggestions for instantiating the paradigm. The framework is described in Sections 5, 6 and our results are presented in Section 7. We point out current deficiencies and suggest avenues for fruitful research in Section 8, followed by a conclusion."}, {"heading": "2 The Dilemma for Shannon Entropy", "text": ""}, {"heading": "2.1 Review of Entropy and Cross Entropy", "text": "Entropy, denoted as \u2212 \u2211\nx p(y) log(y), illustrates the amount of information contained in a message, and can be characterized as the uncertainty of a random variable of a process. For example, Shannon (1951) reported an upper bound of 1.3 bits/character symbol for English character prediction and 5.9 bits/word symbol for English word prediction, meaning that it is highly likely that English word prediction is a harder task than English character prediction.\nIf the output Y n = {y0, y1, ...yn} is a sequence generated from the input, a stationary stochastic process. Then the entropy of Y is given by:\nH(Y ) = lim n\u2192\u221e H(yn|yn\u22121, yn\u22122..., ) (1)\nBy the Shannon-McMillan-Breiman theorem (Algoet and Cover, 1988) this can be written as:\nH(Y ) = lim n\u2192\u221e\n\u2212 1\nn logP (y1, y2, ..., yn) (2)\nSo we can define its hardness or complexity by computing entropy from the distribution P (Y ) for tasks like Shannon\u2019s word prediction model, or extend it to a noisy channel model (Shannon, 1948): given a sequence of inputs X, the uncertainty of the output transformation is given by H(Y |X), interpreted as the amount of uncertainty remaining about Y when X is already known.\nThe true distribution over Y is hard to estimate. Normally we estimate the upper bound of entropy \u2014 the cross entropy denoted as H(P, P\u0302 )\u2014-to approximate the true value of entropy:\nH(P, P\u0302 ) = H(P ) +DKL(P ||P\u0302 ) \u2265 H(P ) (3)\nwhere DKL(P ||P\u0302 ) denotes the KL divergence between two distributions P and P\u0302 . A good model will closely approximate P using P\u0302 , leading to smaller value of DKL(P ||P\u0302 ), i.e., bringing the value cross-entropy closer to that of the real one. Different models would obtain different values of H(P, P\u0302 ). Various studies since Shannon\u2019s work (e.g.,(Kucera and Francis, 1967; Cover and King, 1978; Gopinath and Cover, 1987; Brown et al., 1992)) have explored methods to lower the upper bound of character prediction entropy in English by using more sophisticated models."}, {"heading": "2.2 The Dilemma for Entropy", "text": "While entropy describes the intrinsic nature of the problem or task, its actual value estimation has to be determined by the specific model you adopt for prediction. When Shannon approached the character prediction task, his wife acted as the predictor. Alternatively, if Shannon had used a child as predictor, he would have obtained a much larger estimated entropy.\nSimilarly, if one wishes to compare the entropy of two tasks, for example, to determine which language sequence is harder to predict, English or French, it would be problematic if one compares the entropy computed via a linguist for English and a child for French. One requires twins who are mathematically and linguistically identical in terms of English and French for a fair comparison (Cover and Thomas, 2012). However, in real world, it is almost impossible to find such twins. Different models are attuned differently to different scenarios, tasks, datasets, evaluation metrics, parameter settings, or optimization strategies. One model might not fit all tasks equally well, e.g., SVMs are not designed to predict probabilities, CRFs offer more insights in sequence labeling tasks than SVMs but are hard to use straightforwardly for text classification, etc.\nIn summary, though entropy provides a theoretical definition about the uncertainty of a data source or task, the fact that its estimation must be performed using a real specific model poses a\ndilemma for the accurate estimation of the uncertainty of tasks and hence for their fair comparison."}, {"heading": "3 Prerequisites for Fair Comparison", "text": "We claim that a framework should incorporate the following elements to enable a fair complexity comparison of disparate NLP tasks and systems:\nA universal measure: Complexity can be measured in terms of multiple aspects (e.g., the amount of training data required, the amount of preprocessing required, the size and complexity of auxiliary information, training time, memory usage, or even lines of code). But we need a universal and appropriate metric. In this work, we propose Shannon Entropy as the universal metric, which we believe reflects the intrinsic randomness, predictability, and uncertainty of datasets and tasks. All the above aspects are highly correlated with Shannon Entropy.\nA universal Engine: A POS tagging system makes decisions by selecting tags with highest probability while a summarization system selects the top-ranked sentences. A fair comparison of complexity, however, requires a single general and unified engine to define all (at least most of) NLP tasks within the same framework. The abovementioned notation transformation paradigm, elaborated in the following section, accommodates most NLP tasks.\nA universal model: We cannot fairly compare the entropy obtained from a logistic regression model on POS tags to that produced from a large framework of interdependent alignment, phrase extraction, decoding algorithms for machine translation. A unified model should work with predictions for all (or at least most) current NLP tasks, and should make relatively accurate predictions (a random guess model, for example, is general but would not be helpful). We propose in Section 6 a candidate model."}, {"heading": "4 The Nature of NLP", "text": "In order to propose a single multi-purpose unified Engine for NLP one has to adopt a very general perspective. When constructing an NL system one typically assembles a variety of components. Some of them are active modules that instantiate algorithms and perform transformations. Others are passive resources (like lexicons, probability tables, or rulesets) that support the former.\nActive modules are sometimes built to produce passive ones. It is important to differentiate the role of modules in a framework in order to properly estimate the overall complexity. In this section we first categorize the primary roles of NLP (sub)systems and then postulate that modern NLP algorithms (largely) fall into three distinct complexity types."}, {"heading": "4.1 Three Classes of NLP System", "text": "NLP systems generally perform one of the following three functions/roles: (i) research into aspects of the nature of language(s), (ii) application tasks and subtasks, and (iii) support algorithms. The majority of NLP development today falls into the second and third classes.\nResearch into language includes such studies as determining the Zipfian nature and the entropy of language, discovering changes in patterns of use over time and across geographic regions, identifying text genres by for example creating word and constituent distribution profiles, and so on.\nApplication tasks include machine translation, information retrieval, speech recognition, natural language interpretation (both syntactic parsing and semantic analysis), information extraction, question answering, dialogue processing, text summarization, text (sentence and multi-sentence) generation, sentiment analysis / opinion mining, text mining / harvesting, and others. Subtasks include part of speech tagging, chunking, coreference resolution, text segmentation, query analysis, bitext alignment, reference generation, profiling/characterization of language producers, and many others, as well as numerous resource creation tasks including building monolingual and bilingual lexicons, distributional semantic word profiles and embeddings, word sense lists, ontologies / taxonomies, word-sentiment lists, and many others.\nSupport algorithms include a variety of generic procedures that are reused in many applications. In addition to the classic Finite State / Augmented Transition Network technology from the 1960s and later, modern NLP usually works with the statistical properties of large collections of words, and modern support algorithms such as HMMs and others generally assign and use count-derived scores to [sets of] words, such as tf.idf, PMI, and others, or distribute probability across sets of labels, words or documents, such as Expecta-\ntion Maximization, Probabilistic Graphical Models, Topic Modeling algorithms, certain clustering algorithms, etc. Some support algorithms focus on processing human labeling (annotation) and comparing the results of various different labeling agents (human and machine), such as JensenShannon and other distribution comparison scoring, annotation optimization procedures, etc."}, {"heading": "4.2 Three Levels of NLP Algorithm", "text": "We postulate that (almost every) NLP task / subtask can be defined as [a combination of] one of three basic operations, listed in order of complexity:\nLevel 1: Prediction: The algorithm reads its input, which includes principally a sequence of units of some kind, and predicts the next item in the sequence. Example: predicting the next word in a stream, as used by Shannon to calculate the information content of text.\nLevel 2: Labeling: The algorithm reads its input and generate label(s) based on it. Labeling tasks can be divided into two subcategories:\n\u2022 Aligned Labeling: there is a one-to-one correspondence between inputs and outputs. Aligned tasks include most tagging tasks (e.g., named entity tagging and part-ofspeech tagging).\n\u2022 Unaligned Labeling: no aligned correspondence exists between inputs and outputs. Unaligned sequence-label tasks can be further divided into single-label tasks such as categorization or clustering, in which a single label is assigned given the input (e.g., classification of documents each into one class), and sequence-label tasks, in which a sequence of labels is produced (e.g., MT, where the labels are target language words).\nLevel 3: Scoring: The algorithm reads its input and assigns a score (without loss of generality, a real value between zero and unity) to some unit(s) in it. The score may be a probability, rating, or some other score. Example: tf.idf scoring of words.\nIn a probabilistic paradigm, the probability of Level 1 tasks can be characterized as P (Y ) where Y denotes the sequence to predict. Level 2 tasks can be characterized as P (Y |X) where X denotes the input and Y denotes the label(s) to generate.\nOften, one operation is used to perform another. It is typical in modern-day (post-1990s) NLP to perform all kinds of labeling (Level 2 operations) by scoring all relevant possible categories (a Level 3 operation) and then returning the highest-scoring one as the selected tag. This contrasts with pre1990s NLP that generally computed a single result, such as the desired label, as the one and only possible answer.\nA task may require several operations in sequence. For example, syntactic parsing requires labeling the part of speech tag of each word, labeling the start and end words of syntactic constituents, labeling the head of each constituent, and labeling the syntactic role of each constituent with regard to its immediate head. Sometimes the label is drawn from a small set of possible tags that is predefined by theorists or the researcher, such as the part of speech tags. Sometimes the label is provided in the text, such as the head word of a syntactic constituent. Sometimes the label is a value computed by a scoring operation, such as the PMI score of a word pair in a corpus."}, {"heading": "5 The Universal NLP Engine", "text": "The generic NLP Engine contains (see Figure 1): The transformation engine E, which takes as input one or more symbols from S and produces zero or more labels in response.\nThe input stream X, which contains the text (without loss of generality, we talk about text (a sequence of words and punctuation), but S might instead be a sequence of symbols from some other vocabulary, such as part of speech tags, or a mixture of several vocabularies, such as words with their individual part of speech tags). We therefore consider S as consisting of an essentially infinite stream of units, each unit being a symbol (or set of associated symbols) for which a label (or set of labels) is to be computed by E. Let X denote the set of source symbols.\nThe data resource(s) R, typically a lexicon, a grammar, a probability model, or the output of some subtask, used by E to perform its transformation.\nThe output label(s) Y, a set of predefined symbols that E produces. Let Y denote the set of target symbols (including labels). We have \u2200y \u2208 Y, Y \u2208 Y and also possibly X \u2208 Y.\nWe next describe a generic procedure for implementing the machine based on the following as-\nsumption.\nAssumption 0.1 [Most] modern NLP tasks can be viewed as predicting a (sequence of) token(s) (i.e., Y n) using a finite-state Turing Machine.\nSuch a procedure allows one to measure and compare various aspects of [almost] any NLP task and subtask in a systematic way, and to thereby compare the computational properties of alternative approaches and implementations to any NLP (sub)task.\nThe following examples, using the same input stream Xn=\u201cDog eats apple\u201d, illustrate how the engine works by phrasing several modern NLP tasks as sequential token prediction problems:\n\u2022 Sentiment Classification: Y = {\u201c-1\u201d, \u201c0\u201d, \u201c1\u201d}, respectively for negative, neutral, and positive sentiment Y n=\u201c0\u201d (meaning: neutral sentiment).\n\u2022 POS tagging: Y = {Penn Treebank POS Tags} Yn =\u201cNNP VBZ NN\u201d.\n\u2022 Syntactic Parsing: Y = {\u201c(ROOT\u201d, \u201c(S\u201d, \u201c(NP\u201d, \u201c)\u201d, ...} Yn=(ROOT (S (NP (NNP ) ) (VP (VBZ ) (NP (NN ) ) ) ) ).\n\u2022 Semantic analysis: Y = {English Word List,Relation List...} Y n=\u201c\u2203 e . eat ( e ) \u2227 agent ( e , dog ) \u2227 patient ( e , apple )\u201d.\n\u2022 Word Sense Disambiguation: Y = {\u201c1\u201d, \u201c2\u201d, \u201c3\u201d, \u201c4\u201d, ...} Yn=\u201c1 3 1\u201d, correspond to the 1st, 3rd, and 1st senses for the correspondent token.\n\u2022 Machine Translation: Y = {French Words, Punctuations} Yn=\u201cchien mange pomme\u201d.\n\u2022 Summarization: Y = {English Words, Punctuations} Yn=\u201cdog eats apple\u201d (the gold-standard summary is the original sentence).\nThe proposed framework is inspired by recent progress of sequence-to-sequence prediction models in NLP, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014; Graves et al., 2014) and the\nwork of Andreas et al. (2013) that illustrates that semantic parsing can to some extent be viewed as a machine translation problem. Treating syntactic parsing as a string prediction task is defined and implemented in (Vinyals et al., 2014)."}, {"heading": "6 The Universal Entropy Model", "text": "In this section, we discuss the generic model for computing Entropy within a Language Engine."}, {"heading": "6.1 Requirements", "text": "We first identify requirements for the universal model. The random guess model satisfies the generality property as it can be used in any predictive model. But it of course does not constitute a good predictive model, since it delivers estimated distributions far away from the actual distribution. In contrast, n-order Markov models (n=1,2,3...) seem to serve the purpose well and can be easily applied in sequence-labeling tasks such as NER and POS tagging. However it is tricky to adapt them to single-label predictions such as text classification. Additionally, their dependence on specific feature selections make fair comparison across different implementations complicated or impossible.\nThis consideration leads to the first requirement for a model:\nRequirement 0.1 The model should be able to leverage different types of features automatically to avoid infinitely complicated feature engineering procedures.\nWe consider P (Y n) and P (Y n|Xn) to gain insights about better predictions for entropy calculation. We use P (Y n) for illustration as it can be easily extended to P (Y n|Xn). In the sequence prediction task, let Fn denote the entropy where predictions are made given previous n tokens (an n-order Markov model). As proved by Shannon (Shannon, 1951), Fn monotonically decreases with respect to n and H(Y n) is strictly bounded by Fn:\nF1 \u2265 F2 \u2265 ... \u2265 F\u221e \u2265 H(Y n) (4)\nTaking as example an n-gram word prediction model, theoretically estimated entropy decreases as predictions are made based on increasingly many preceding tokens, roughly stated in (Shannon, 1951). However, issues arise when n is too large to maintain an n-gram probability table.\nBased Shannon\u2019s proof, for the purpose of to the largest extent approximating real entropy using estimated entropy, we generalize the second property of the model as follows:\nRequirement 0.2 The model should be able to memorize earlier information as much as possible given the computing power and storage capacity available."}, {"heading": "6.2 Model", "text": "This line of thinking suggests using as model recurrent neural networks (Mikolov et al., 2010; Funahashi and Nakamura, 1993) or sophisticated versions like LSTM (Hochreiter and Schmidhuber, 1997) (Please see Appendix for details about LSTM models).\nRecurrent networks obtain a fixed-sized vector for each step within the processing sequence by convoluting current information with output from earlier step(s). Such vectors can be viewed as combining evidence obtained so far, and are used to predict the subsequent token(s), typically using a softmax function. For labeling tasks, recurrent neural networks first map the input Xn of arbitrary length to a fixed-sized vector, which can be viewed as evidence, and then map that vector to the output by convoluting feature representations at each step.\nRecurrent models have the following merits: (1) They obey requirement 1 by automatically encoding \u201cfeatures\u201d in the real-valued representation vectors without explicit feature selection and engineering. Though the models still require significant parameter tuning, they provide a relatively unified procedure for comparison. (2) By sequentially convoluting each token with output from earlier step(s) they have the ability to \u2018remember\u2019 information required to approximate (to some degree) the conditional probability of limn\u2192\u221e P (yt|yt\u22121, yt\u22122, ..., yt\u2212n), which partially addresses requirement 2. (3) The model is manageable since it uses constant memory size and runs in linear time.\nWe explicitly do not claim that recurrent neural models are a perfect choice as model in an NLP Engine. We acknowledge their numerous shortcomings, and discuss some pros and cons in the concluding section. However, we believe that they do offer advantages over other models we have considered with regard to tradeoffs of generality, computing power, and storage capacity.\nThese thoughts are inspired by recent progress of sequence-to-sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014)."}, {"heading": "7 Experiments: Comparing the Uncertainty of NLP Tasks", "text": "Using the above framework, we now calculate the exact entropy for a few NLP tasks. What must never be overlooked however is the impact of the training/testing datasets used (e.g., the complexity for guessing subsequent words in novels and newspapers can be different) and how exactly the task is defined (e.g., differences of complexity in sentiment classification between a 5-class and 2- class problem are huge)."}, {"heading": "7.1 Tasks and Datasets", "text": "Prediction Tasks: We use Wikipedia 2014 corpus, divided half and half for training and testing. We employ the most-frequent 200,000 words and add an \u201cunknown\u201d symbol to represent the remainder, making it a 200,001-class prediction problem. This is a simple Prediction task.\nSentiment Analysis (Pang et al., 2002)\u2019s dataset comprises sentences containing goldstandard sentiment labels tagged at the start of each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). This is an Unaligned Labeling (single) task.\nQuestion-Answering (UMD) The dataset comprises two domains, History and Literature, and contains roughly 2,000 questions where each question is paired with an answer (Iyyer et al., 2014). Since answers are selected from a pool of roughly 100 answer candidates, this is not an open QA problem but a multi-class classification problem; i.e., an Unaligned Labeling (single) task.\nMachine Translation We use the WMT14 English-French dataset and the OpenMT12 English-Chinese dataset. This is an Unaligned Labeling (sequence) task.\nPart-of-Speech Tagging (Penn Treebank) We use a random sample of Wiki2014 as training and testing dataset, each of which containing 1 million sentences. Gold-standard labels are assigned using the Stanford POS tagger. This is an Aligned Labeling task.\nName Entity Recognition (CoNLL) We use the CoNLL-2003 English benchmark for training, which labels four entity types (person, location, organization, miscellaneous). The models are tested on CoNLL-2003 testing data.. This is an Aligned Labeling task.\nSyntactic Parsing Training data is the OntoNotes corpus (Hovy et al., 2006) and English Web Treebank (Petrov and McDonald, 2012) with an additional 5 million random sentences, all parsed by the Stanford Parser (Socher et al., 2013). The testing dataset is Section 22 of the Penn Treebank plus 1000 sentences from the Question Treebank. We followed protocols defined in (Vinyals et al., 2014). This is an Unaligned Labeling (sequence) task.\nQuestion Answer (Open-domain) We use the Yahoo Comprehensive QA dataset. The dataset comprises roughly 4 million QA pairs. Questions and answers are sequences of tokens. Questions are treated as inputs and models predict word sequences as responsive answers. This is an Unaligned Labeling (sequence) task."}, {"heading": "7.2 Implementations", "text": ""}, {"heading": "7.2.1 Prediction Task", "text": "Implementations for prediction tasks, where P (Y n) is to be estimated, are similar to recurrent language models as defined in (Mikolov et al., 2010). Let et\u22121 denote the representation obtained for timestep t \u2212 1 based on preceding information from the LSTM. Let eYt denote the feature representation for the token to be predicted at time t. By adopting a softmax function, the conditional probability for the occurrence of the current token given earlier evidence is given by:\np(yt|yt\u22121, ..., yt\u2212n) == f(et\u22121, eyt)\u2211 Y \u2208Y f(et\u22121, ey) (5)\nwhere f(et\u22121, eyt) denotes the compositional function between vectors et\u22121 and eyt . In this paper, we adopt the form of exponential dot product for f(\u00b7):\nf(et\u22121, eyt) = exp(et\u22121 \u00b7 eyt) (6)"}, {"heading": "7.2.2 Labeling Task", "text": "We refer to frameworks (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014)) by\nfirst concatenating input and output {Xn, Y n} = {x1, .., xn, y1, .., yn}. Let et\u22121 denote the LTSM output at timestep t \u2212 1 by convoluting all preceding tokens before t in {Xn, Y n}, i.e., {x1, ..., xn, y1, ..., yt\u22121}.\nUnaligned Single Labeling Single-tag Labeling corresponds to the special case where the size of Y n is 1. Taking Sentiment Analysis as an example, sentence-level embeddings (denoted as en, where n denotes the length of the current sentence) are first obtained recurrently from the LSTM. As it is a binary classification problem, we have:\nP (y|\u00b7) = exp(en \u00b7 ey)\u2211\ny\u2032\u2208{0,1} exp(en \u00b7 ey\u2032) (7)\nQuestion-Answering (UMD) is implemented in a similar way.\nUnaligned Sequence Labeling Following (Bahdanau et al., 2014; Vinyals et al., 2014), the conditional probability for predicting the current token yt in Y n is given by\nP (Y n|Xn) = \u220f\n1\u2264t\u2264n\nP (yt|x1, ..., xn, y1, yt\u22121)\n= \u220f\n1\u2264t\u2264n\nf(et\u22121, eyt)\u2211 y\u2208Y f(et\u22121, ey)\n(8) f(\u00b7) takes the same form as in Eq.6.\nAligned Sequence Labeling In aligned sequence labeling tasks, there is a one-to-one correspondences between output yt and input xt, which should be captured in the model. Decisions at timestep t are made by combining LSTM representation et\u22121 and input representation ext :\nP (Y n|Xn) = \u220f\n1\u2264t\u2264n\nP (yt|x1, ..., xn, y1, ..., yi\u22121)\n= \u220f\n1\u2264t\u2264n\nf(et\u22121, eyt , exi)\u2211 y\u2208Y f(et\u22121, ey, exi)\n(9) f(et\u22121, eyt , exi) is given as below:\nf(et\u22121, eyt , exi) = exp(U \u00b7 (W \u00b7 [et\u22121, eyt , exi ])) (10) where [et\u22121, eyt , exi ] denotes the concatenation of the three vectors and W and U denote convolutional matrix and vector to project the concatenated vector to a scalar. Taking POS tagging as example, for the sentence Xn =\u201cdog eats bones\u201d\nwith correspondent labels Yn =\u201cNN VBZ NNS\u201d, we first concatenate Xn with Yn: \u201cdog eats bones NN VBZ NNS\u201d. When making predictions at token \u201cVBZ\u201d, let eLSTM denote the LSTM embedding computed at preceding token \u201cNN\u201d, eVBZ denote the embedding for token \u201cVBZ\u201d, eeats denote the correspondent input embedding for token \u201ceats\u201d. Then the probability for generating partof-speech tag VBZ is given by:\np(VBZ|\u00b7) = f(eLSTM , eVBZ, eeats)\u2211 y\u2208\u2208Y f(eLSTM , ey, eeats) (11)"}, {"heading": "7.3 Details", "text": "For each task, word embeddings are initialized o the same pre-trained vectors for fairness. Pretrained embeddings were obtained from word2vec on a 6-billion-word corpus with dimensionality 512. LSTM models are composed of one single hidden layer. Stochastic gradient decent (without momentum) with mini-batch (Cotter et al., 2011) is adopted. For each task, we use a learning initial learning rate of 0.5 with a linear decay. Learning stops after 4 iterations. We initialized the LSTM parameters using a uniform distribution between [-0.1, 0.1]. Referring to (Sutskever et al., 2014), the gradient is normalized if its value exceeds a threshold to avoid exploding gradients. For unaligned sequence prediction tasks (i.e., syntactic parsing, QA(open domain)), inputs are reversed, as suggested in (Sutskever et al., 2014)."}, {"heading": "7.4 Results", "text": "Estimated entropies for different tasks computed in the proposed paradigm are presented in Table 1. As can be seen, MT is less complex than word\nprediction tasks, which is in line with our expectation: for MT, output tokens are predicated on source tokens. The input data provides additional information and lowers the degree of uncertainty: H(Y |X) \u2265 H(Y ) for any X and Y .\nAs discussed earlier, estimated entropies are subjective to datasets. Being significantly short in training data, a high level of entropy is observed for summarization. This phenomenon demonstrate one key disadvantage of the proposed model\u2014the failure to consider the impact of datasets. In particular, we are computing the upper bound for a specific task given the specific dataset adopted. How to take into account the influence of different datasets (e.g., amounts of training data, quality of training data) poses a great challenge to developing a general NLP Engine."}, {"heading": "8 Deficiencies and Directions for Improvement", "text": "We have proposed a paradigm with three requirements, which we believe to be essential for a universal NLP engine. We are fully aware that it is impossible to come up with instantiations that perfectly meet all the requirements using current algorithms and frameworks. We consider the search for optimal solutions to be a long-term task. In this section we identify deficiencies involved in the proposed framework and suggest avenues for improvements.\nThe Metric: We proposed to use Shannon Entropy as uncertainty measurement to evaluate the complexity of tasks because we believe that entropy more deeply reflects the nature of uncertainty than other current measures such as accuracy or recall. However, if a theoretical computer scientist were to develop a more optimal measure that avoids the dilemma described in Section 2, we would replace entropy with that measure.\nThe Engine: In this paper, we are using an end-to-end turning string prediction engine, which says nothing substantive about complexity of resource and intermediate procedures. This could be problematic. Consider the following scenarios: in case 1 we have a long table that lists each input possibility and its output answer is a simple lookup, where the work then goes into creating the table, and in case 2 we have a small resource of rules but a lot of feature creation and rule application in the main engine to perform the same task. It is then true that the entropy from input to out-\nput is the same if the two systems produce exactly the same output (though one takes perhaps a lot more time, the other requires perhaps more space). But is the amount of work required (and hence the entropy effect) to create the two resources the same? In other words, can one argue that because the \u2018outside\u2019 end-to-end turning prediction task is constant in entropy, therefore the inner resources have to contain the same amount of entropy reducing \u2018power\u2019? This is not necessarily true. But should the one resource contain significantly more than the other, it appears that the outside engine doesn\u2019t actually use that.\nThe Model: Before discussing disadvantages of applied recurrent neural models, it is noteworthy that there is an alternative to a universal and unified model (we call it unified model for short) for the framework. One can instead find the best informants (we call such a strategy best models) from various places and ask them to perform the transformation predictions. Alternatively, one can exhaust all combinations of models, algorithms, and features, and report the best results (smallest value of entropy) as the complexity comparison. Though all these strategies have pros and cons, we postulate that unified models might be more suitable than best models, as different informants might have different levels of education.\nTo meet the two requirements described in Section 6, we adopted recurrent neural models. Recurrent models are by no means perfect: they inevitably forget previous information and are fundamentally incapable of capturing long-term dependencies (Bengio et al., 1994). This becomes especially problematic in tasks where long-term dependencies play a vital role such as discourse parsing. Without trying to defend the model too far, we note that recurrent models seem to offer advantages over other current models that we can think of, e.g., transition models. We are optimistic that other and more sophisticated variations of neural models or other models, such as LDCRFs (Long-Dependency CRFs) (Morency et al., 2007), memory networks (Weston et al., 2015) will cope with the aforementioned disadvantages bit by bit. At least, one can replace recurrent models if more suitable algorithms come up."}, {"heading": "9 Conclusion: Toward a Theory of NLP", "text": "Almost all NLP researchers today would all agree that there is no such thing as a theory of NLP. We\nhope that in this paper we lay some groundwork toward such a theory.\nAny theory addresses some complex phenomenon by (i) identifying some categories (of objects or states or events) within it, (ii) providing some characteristics and perhaps some definitions for them, (iii) if possible describing some relationships between them, and (iv) if possible quantifying (some) aspects of these relationships. A scientific theory measures aspects of some phenomena and uses rules expressing the relationships to predict the values of other phenomena under certain conditions.\nThe framework outlined in this paper names as categories the commonly used linguistic phenomena of NLP such as words, part of speech tags, syntactic classes, and any other linguistically motivated category that NLP researchers choose to study. But it also has as categories various algorithms and data structures and other aspects of computation, including language models, the notion of training data and evaluation against a gold standard, classification, scoring, etc. The General NLP Engine puts the notions together in a single generic framework and suggests a way to measure their separate individual characteristics with regard to a single whole, namely the performance of tasks phrased in a very generic manner. This allows one to hold all but one category constant and vary the characteristics of either a linguistic or a computational category and study its effect on the overall task relative to any other variation, even if applied to some other category.\nIt is of course possible to generalize the General NLP Engine to apply to many other application areas in Computer Science. However the domain of NLP has properties that make it very attractive for fleshing out the nature of the Engine and the general \u2018theory\u2019, among others that NLP is a relatively mature domain within Computer Science, being just over 60 years old; NLP addresses a very large and complex subject field, namely natural language, NLP uses a variety of quite different techniques, including finite state transformation engines, machine learning, etc., and numerous types of representations, including vector spaces, symbolic notations, and connectionist embeddings.\nIn summary, though far from perfect, this paper provides a first attempt to quantify NLP tasks under a uniform paradigm which might have the\npotential to significantly impact natural language processing areas."}, {"heading": "10 Appendix", "text": "Long-short Term Memory LSTM model, first proposed in (Hochreiter and Schmidhuber, 1997), maps an input sequence to a fixed-sized vector by sequentially convoluting the current representation with the output representation of the previous step. LSTM associates each time epoch with an input, control and memory gate, and tries to minimize the impact of unrelated information. Letting it, ft and ot correspond to gate states at time t, et\u22121 and et denote the output representation at time t \u2212 1, and t, ext denote the embedding associated with the token at time t, as defined in (Hochreiter and Schmidhuber, 1997), we have\nit = \u03c3(Wi \u00b7 ext + Vi \u00b7 et\u22121)\nft = \u03c3(Wf \u00b7 ext + Vf \u00b7 et\u22121)\not = \u03c3(Wo \u00b7 ext + Vo \u00b7 et\u22121) lt = tanh(Wl \u00b7 ext + Vl \u00b7 et\u22121)\nmt = ft \u00b7mt\u22121 + it \u00b7 lt\net = ot \u00b7mt\n(12)\nwhere \u03c3 denotes the sigmoid function. it, ft and ot are scalars within the range of [0,1]."}], "references": [{"title": "A sandwich proof of the shannonmcmillan-breiman theorem", "author": ["Algoet", "Cover1988] Paul H Algoet", "Thomas M Cover"], "venue": "The annals of probability,", "citeRegEx": "Algoet et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Algoet et al\\.", "year": 1988}, {"title": "Semantic parsing as machine translation", "author": ["Andreas Vlachos", "Stephen Clark"], "venue": "In ACL", "citeRegEx": "Andreas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "An estimate of an upper bound for the entropy of english", "author": ["Brown et al.1992] Peter F Brown", "Vincent J Della Pietra", "Robert L Mercer", "Stephen A Della Pietra", "Jennifer C Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Better minibatch algorithms via accelerated gradient methods", "author": ["Cotter et al.2011] Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "A convergent gambling estimate of the entropy of english", "author": ["Cover", "King1978] Thomas M Cover", "R King"], "venue": "Information Theory, IEEE Transactions", "citeRegEx": "Cover et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1978}, {"title": "Elements of information theory", "author": ["Cover", "Thomas2012] Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cover et al\\.", "year": 2012}, {"title": "Approximation of dynamical systems by continuous time recurrent neural networks", "author": ["Funahashi", "Nakamura1993] Ken-ichi Funahashi", "Yuichi Nakamura"], "venue": "Neural networks,", "citeRegEx": "Funahashi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Funahashi et al\\.", "year": 1993}, {"title": "Open problems in communication and computation", "author": ["Gopinath", "Cover1987] B Gopinath", "Thomas M Cover"], "venue": null, "citeRegEx": "Gopinath et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Gopinath et al\\.", "year": 1987}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: the 90% solution", "author": ["Hovy et al.2006] Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel"], "venue": "In Proceedings of the human language technology conference of the NAACL,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference on Empir-", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Computational analysis of presentday", "author": ["Kucera", "Francis1967] Henry Kucera", "Nelson Francis"], "venue": null, "citeRegEx": "Kucera et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Kucera et al\\.", "year": 1967}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["Morency et al.2007] L Morency", "Ariadna Quattoni", "Trevor Darrell"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Morency et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Morency et al\\.", "year": 2007}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "McDonald2012] Slav Petrov", "Ryan McDonald"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "A mathematical theory of communication", "author": ["Claude Elwood Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications", "citeRegEx": "Shannon.,? \\Q1948\\E", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Prediction and entropy of printed english", "author": ["Claude E Shannon"], "venue": "Bell system technical journal,", "citeRegEx": "Shannon.,? \\Q1951\\E", "shortCiteRegEx": "Shannon.", "year": 1951}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "In his epoch-making work, Shannon (1951) demonstrated how to compute the amount of information in a message.", "startOffset": 26, "endOffset": 41}, {"referenceID": 19, "context": "For example, Shannon (1951) reported an upper bound of 1.", "startOffset": 13, "endOffset": 28}, {"referenceID": 19, "context": "So we can define its hardness or complexity by computing entropy from the distribution P (Y ) for tasks like Shannon\u2019s word prediction model, or extend it to a noisy channel model (Shannon, 1948): given a sequence of inputs X, the uncertainty of the output transformation is given by H(Y |X), interpreted as the amount of uncertainty remaining about Y when X is already known.", "startOffset": 180, "endOffset": 195}, {"referenceID": 4, "context": ",(Kucera and Francis, 1967; Cover and King, 1978; Gopinath and Cover, 1987; Brown et al., 1992)) have explored methods to lower the upper bound of character prediction entropy in English by using more sophisticated models.", "startOffset": 1, "endOffset": 95}, {"referenceID": 22, "context": "The proposed framework is inspired by recent progress of sequence-to-sequence prediction models in NLP, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014; Graves et al., 2014) and the work of Andreas et al.", "startOffset": 132, "endOffset": 240}, {"referenceID": 2, "context": "The proposed framework is inspired by recent progress of sequence-to-sequence prediction models in NLP, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014; Graves et al., 2014) and the work of Andreas et al.", "startOffset": 132, "endOffset": 240}, {"referenceID": 5, "context": "The proposed framework is inspired by recent progress of sequence-to-sequence prediction models in NLP, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014; Graves et al., 2014) and the work of Andreas et al.", "startOffset": 132, "endOffset": 240}, {"referenceID": 1, "context": ", 2014) and the work of Andreas et al. (2013) that illustrates that semantic parsing can to some extent be viewed as a machine translation problem.", "startOffset": 24, "endOffset": 46}, {"referenceID": 20, "context": "As proved by Shannon (Shannon, 1951), Fn monotonically decreases with respect to n and H(Y ) is strictly bounded by Fn:", "startOffset": 21, "endOffset": 36}, {"referenceID": 20, "context": "Taking as example an n-gram word prediction model, theoretically estimated entropy decreases as predictions are made based on increasingly many preceding tokens, roughly stated in (Shannon, 1951).", "startOffset": 180, "endOffset": 195}, {"referenceID": 15, "context": "This line of thinking suggests using as model recurrent neural networks (Mikolov et al., 2010; Funahashi and Nakamura, 1993) or sophisticated versions like LSTM (Hochreiter and Schmidhuber, 1997) (Please see Appendix for details about LSTM models).", "startOffset": 72, "endOffset": 124}, {"referenceID": 22, "context": "These thoughts are inspired by recent progress of sequence-to-sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014).", "startOffset": 89, "endOffset": 176}, {"referenceID": 2, "context": "These thoughts are inspired by recent progress of sequence-to-sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014).", "startOffset": 89, "endOffset": 176}, {"referenceID": 5, "context": "These thoughts are inspired by recent progress of sequence-to-sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014).", "startOffset": 89, "endOffset": 176}, {"referenceID": 17, "context": "Sentiment Analysis (Pang et al., 2002)\u2019s dataset comprises sentences containing goldstandard sentiment labels tagged at the start of each sentence.", "startOffset": 19, "endOffset": 38}, {"referenceID": 17, "context": "Sentiment Analysis (Pang et al., 2002)\u2019s dataset comprises sentences containing goldstandard sentiment labels tagged at the start of each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). This is an Unaligned Labeling (single) task.", "startOffset": 20, "endOffset": 222}, {"referenceID": 13, "context": "Question-Answering (UMD) The dataset comprises two domains, History and Literature, and contains roughly 2,000 questions where each question is paired with an answer (Iyyer et al., 2014).", "startOffset": 166, "endOffset": 186}, {"referenceID": 12, "context": "Syntactic Parsing Training data is the OntoNotes corpus (Hovy et al., 2006) and English Web Treebank (Petrov and McDonald, 2012) with an additional 5 million random sentences, all parsed by the Stanford Parser (Socher et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 21, "context": ", 2006) and English Web Treebank (Petrov and McDonald, 2012) with an additional 5 million random sentences, all parsed by the Stanford Parser (Socher et al., 2013).", "startOffset": 142, "endOffset": 163}, {"referenceID": 15, "context": "Implementations for prediction tasks, where P (Y ) is to be estimated, are similar to recurrent language models as defined in (Mikolov et al., 2010).", "startOffset": 126, "endOffset": 148}, {"referenceID": 22, "context": "We refer to frameworks (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014)) by first concatenating input and output {X, Y } = {x1, .", "startOffset": 23, "endOffset": 92}, {"referenceID": 2, "context": "We refer to frameworks (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014)) by first concatenating input and output {X, Y } = {x1, .", "startOffset": 23, "endOffset": 92}, {"referenceID": 2, "context": "Unaligned Sequence Labeling Following (Bahdanau et al., 2014; Vinyals et al., 2014), the conditional probability for predicting the current token yt in Y n is given by", "startOffset": 38, "endOffset": 83}, {"referenceID": 6, "context": "Stochastic gradient decent (without momentum) with mini-batch (Cotter et al., 2011) is adopted.", "startOffset": 62, "endOffset": 83}, {"referenceID": 22, "context": "Referring to (Sutskever et al., 2014), the gradient is normalized if its value exceeds a threshold to avoid exploding gradients.", "startOffset": 13, "endOffset": 37}, {"referenceID": 22, "context": ", syntactic parsing, QA(open domain)), inputs are reversed, as suggested in (Sutskever et al., 2014).", "startOffset": 76, "endOffset": 100}, {"referenceID": 3, "context": "Recurrent models are by no means perfect: they inevitably forget previous information and are fundamentally incapable of capturing long-term dependencies (Bengio et al., 1994).", "startOffset": 154, "endOffset": 175}, {"referenceID": 16, "context": "We are optimistic that other and more sophisticated variations of neural models or other models, such as LDCRFs (Long-Dependency CRFs) (Morency et al., 2007), memory networks (Weston et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 23, "context": ", 2007), memory networks (Weston et al., 2015) will cope with the aforementioned disadvantages bit by bit.", "startOffset": 25, "endOffset": 46}], "year": 2015, "abstractText": "It is commonly accepted that machine translation is a more complex task than part of speech tagging. But how much more complex? In this paper we make an attempt to develop a general framework and methodology for computing the informational and/or processing complexity of NLP applications and tasks. We define a universal framework akin to a Turning Machine that attempts to fit (most) NLP tasks into one paradigm. We calculate the complexities of various NLP tasks using measures of Shannon Entropy, and compare \u2018simple\u2019 ones such as part of speech tagging to \u2018complex\u2019 ones such as machine translation. This paper provides a first, though far from perfect, attempt to quantify NLP tasks under a uniform paradigm. We point out current deficiencies and suggest some avenues for fruitful research.", "creator": "LaTeX with hyperref package"}}}