{"id": "1503.00255", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2015", "title": "An Online Convex Optimization Approach to Blackwell's Approachability", "abstract": "the notion of approachability in repeated games with vector payoffs was introduced by blackwell in the 1950s, concluding with geometric conditions for projections and corresponding strategies that rely on computing { \\ em steering directions } as projections from the current average payoff vector to the ( convex ) target vectors. accordingly, abernethy, batlett and hazan ( 1934 ) proposed restricted class of approachability algorithms that rely atop the no - regret notation of online linear programming essentially computing a suitable sequence of steering directions. algorithms is first carried out for target sets that are convex cones, and then generalized to any convex set by embedding coefficients in a higher - dimensional convex cone. in which paper we present a more direct formulation that relies on the support parameters of the set, along with suitable online convex optimization tools, which leads to a complicated class of mathematical algorithms. we further show that blackwell's original version and its convergence follow as a special case.", "histories": [["v1", "Sun, 1 Mar 2015 11:46:35 GMT  (16kb)", "http://arxiv.org/abs/1503.00255v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["nahum shimkin"], "accepted": false, "id": "1503.00255"}, "pdf": {"name": "1503.00255.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["shimkin@ee.technion.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n00 25\n5v 1\n[ cs\nThe notion of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions for approachability and corresponding strategies that rely on computing steering directions as projections from the current average payoff vector to the (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on the no-regret properties of Online Linear Programming for computing a suitable sequence of steering directions. This is first carried out for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on the support function of the set, along with suitable Online Convex Optimization algorithms, which leads to a general class of approachability algorithms. We further show that Blackwell\u2019s original algorithm and its convergence follow as a special case."}, {"heading": "1 Introduction", "text": "Both Blackwell\u2019s theory of approachability and the no-regret framework of online learning address a repeated decision problem in the presence of on an arbitrary (namely, unpredictable) adversary. The concept of approachability, introduced in [4], addresses a fundamental feasibility issue in for repeated matrix games with vector-valued payoffs. Referring to one player as the agent and to the other as Nature, a set S in the payoff space is approachable by the agent if he can ensure that the average payoff vector converges (with probability 1) to S,\nirrespectively of Nature\u2019s strategy. Blackwell provided in his paper geometric conditions for approachability, which are both necessary and sufficient for convex target sets S, and a corresponding approachability strategy for the agent. An extensive recent survey of approachability and its implications can be found in [12], and a textbook exposition is available in [11].\nConcurrently, Hannan [7] introduced the concept of no-regret play for repeated matrix games. The regret of the agent is the shortfall of the cumulative payoff that was actually obtained relative to the one that could have been obtained with the best (fixed) action in hindsight, given Nature\u2019s observed actions. A no-regret strategy, or algorithm, should ensure that the regret grows sub-linearly in time. The no-regret criterion has been widely adopted during the last two decades by the machine learning community as a standard measure for the performance of online learning algorithms, and its scope has been greatly extended. Of specific relevance here is the Online Convex Optimization (OCO) framework, where Nature\u2019s discrete action is replaced by the choice of a convex function at each stage, and the agent\u2019s decision is a point in a convex set. The textbook [6] offers a broad overview of regret and online learning. Recent surveys of OCO algorithms may be found in [15, 9].\nIt is well known that no-regret strategies for repeated games can be obtained as a special case of the approachability problem. This was already observed in [3]; an alternative formulation that leads to more explicit strategies was proposed in [8]. More recently, it was shown in [1] that any no-regret algorithm for the online linear optimization problem can be used as a basis for an approachability strategy for convex target sets. The online algorithm is used here compute a sequence of steering vectors, that replace the projection directions used in Blackwell\u2019s original algorithm.\nThe scheme suggested in [1] first considers target sets S that are convex cones. The generalization to any convex set is carried out by embedding the original target set in a convex cone in a higher dimensional payoff space. The present paper proposes a more direct scheme that avoids the above-mentioned embedding. This is done by invoking the support function of the target set, along with well-known relations between this function and the Euclidean distance to the set. As the support function is convex, the full arsenal of OCO algorithms may be applied to provide the required sequence of steering vectors.\nA natural question concerns the relation between Blackwell\u2019s original algorithm and the present framework. We first observe that Blackwell\u2019s algorithm is recovered when the standard Follow the Leader (FTL) algorithm is used for the OCO part. Establishing the (known) convergence of this algorithm via the proposed OCO framework is a bit more intricate. First, when the target set has a smooth boundary, we show that FTL guarantees logarithmic rate, which \u201dfast\u201d approachability at a rate of O( log TT ). To address the general case, we further observe that Blackwell\u2019s algorithm is still obtained when a regularized version of FTL is employed,\nfrom which the standard O(t\u22121/2) convergence rate may be deduced.\nThe paper proceeds as follows. In Section 2 we recall the relevant background on Blackwell\u2019s approachability and Online Convex Optimization. Section 3 presents the proposed scheme, in the form of a meta-algorithm that relies on a generic OCO algorithm, discusses the relation to the scheme of [1], and demonstrates a specific algorithm that is obtained by using Generalized Gradient Descent for the OCO algorithm. In Section 4 we outline the relations with Blackwell\u2019s original algorithm, and provide some concluding remarks.\nNotation: The standard inner product in Rd is denoted by \u3008\u00b7, \u00b7\u3009, \u2016 \u00b7 \u2016 is the Euclidean norm, and d(r, S) = infs\u2208S \u2016r \u2212 s\u2016 denotes the corresponding point-to-set distance. Further, B2 = {w \u2208 Rd : \u2016w\u2016 \u2264 1} denotes the Euclidean unit ball, \u2206(I) is the set of probability distributions over a finite set I, diam(S) = sups,s\u2032\u2208S \u2016s \u2212 s\u2032\u2016 is the diameter of the set S, and \u2016R \u2212 S\u2016 = supr\u2208R,s\u2208S \u2016r \u2212 s\u2016 denotes the maximal distance between points in the sets R and S."}, {"heading": "2 Model and Background", "text": "We start with a brief of review of Blackwell\u2019s approachability and of Online Convex Programming, focusing on those aspects that are relevant to this paper."}, {"heading": "2.1 Approachability", "text": "Consider a repeated game with vector-valued rewards that is played by two players, the agent and Nature. Let I and J denote the finite action sets of these players, with corresponding mixed actions x = (x(1), . . . , x(|I|)) \u2208 \u2206(I) and y = (y(1), . . . , y(|J |)) \u2208 \u2206(J). Let r : I \u00d7 J \u2192 Rd be the vector-valued reward function of the single-stage game, which is extended to mixed action as usual through the bilinear function\nr(x, y) = \u2211\ni,j\nx(i)y(j)r(i, j) .\nSimilarly, we denote\nr(x, j) = \u2211\ni\nx(i)r(i, j) .\nThe game is repeated in stages t = 1, 2, . . . , where at stage t actions it and jt are chosen by the players, and the reward vector r(it, jt) is obtained. A pure strategy for the agent is a mapping from each possible history (i1, j1, . . . , it\u22121, jt\u22121) to an action it, and a mixed strategy is a probability distribution over the pure strategies. Nature\u2019s strategies may be similarly defined.\nAs usual, we restrict attention to so-called behavior strategies of the agent, where the action it is drawn randomly according to a mixed action xt, using independent draws across stages. Furthermore, to simplify the presentation, we shall state our results and algorithms in terms of the smoothed reward vectors r(xt, jt), where the reward r(i, t, jt) is averaged over the mixed action xt. This will allow us to state the results in simpler sample-path terms, rather than probabilistic ones; we further discuss this formulation below after Theorem 1.\nLet\nr\u0304T = 1\nT\nT \u2211\nt=1\nr(xt, jt)\ndenote the T -stage average reward vector.\nDefinition 2.1 (Approachability) A closed set S \u2282 Rd is approachable if there exists a strategy for the agent and a sequence \u01eb(T ) \u2192 0 such that\nlim T\u2192\u221e\nd(r\u0304T , S) \u2264 \u01eb(T ) (1)\nholds (w.p. 1) for any strategy of Nature. A strategy of the agent that satisfies this property is an approachability strategy for S.\nTheorem 1 (Blackwell, 1956) A closed and convex set S \u2282 Rd is approachable if and only if either one of the following equivalent conditions holds:\n(i) For each unit vector u \u2208 Rd, there exists a mixed action x = xS(u) \u2208 \u2206(I) such that\n\u3008u, r(x, j)\u3009 \u2264 sup s\u2208S \u3008u, s\u3009 , for all j \u2208 J . (2)\n(ii) For each y \u2208 \u2206(J) there exists x \u2208 \u2206(I) such that r(x, y) \u2208 S.\nIf S is approachable, then the following strategy is an approachability strategy for S: For v 6\u2208 S, let uS(v) be the unit vector that points to v from ProjS(v), the closet point to v in S. Then, for t \u2265 1, if r\u0304t 6\u2208 S, choose xt+1 = xS(uS(r\u0304t)); otherwise, choose an arbitrarily action.\nThe approachability strategy introduced by Blackwell has been generalized in [8], that essentially allow different norms to be used for the projection unto S. Several recent papers have proposed approachability algorithms that depend on Blackwell\u2019s dual condition (condition (ii) in the above Theorem) and avoid the projection step altogether (see [2] and references therein). The current paper again proposes a generalization of Blackwell\u2019s strategy, but from a different viewpoint.\nLet us elaborate on the use of the smoothed rewards r(xt, jt). This offers several useful benefits:\n1. As noted, we obtain sample-path bounds rather than probabilistic ones.\n2. We can state results that hold for any sequence (jt), rather than any (mixed) strategy\nof Nature. This is closer to the spirit of Online Algorithms, where the notion of a randomized choice by Nature may not be meaningful.\n3. As is well known, the difference \u2211T t=1 r(xt, jt)\u2212 \u2211T t=1 r(it, jt) is a Martingale difference\nsequence, hence of order \u221a T . Thus, the difference in the means is of order 1\u221a\nT , and\nconvergence results derived for the smoothed mean are valid for the non-smoothed one up to that order.\nWe note that the results in [1] are developed for the rewards r(xt, yt), with the mean taken over yt as well, and the agent is allowed to observe Nature\u2019s mixed action yt (or at least the mean reward r(xt, yt)). We avoid making that extra step and assume that the agent only observes Nature\u2019s pure actions {jt}.\nAs the pure actions it of the agent do not affect the rewards rt = r(xt, jt), we may suppress them in the following discussion and focus on the mixed actions xt. In particular, we restrict attention to strategies of the agent that assign a mixed action xt to each sequence (j1, . . . , jt\u22121) of Nature\u2019s actions. (Note that there is no need to include the past mixed actions x1, . . . , xt\u22121 in the history sequence, since they may be computed recursively; in practice, however, we will express xt as a function of past the reward vector sequence (r(xk, jk))k<t.) Since there is no randomization involved, it may be seen that Definition 2.1 is equivalent to the requirement that the bound (1) holds (deterministically) for any sequence (j1, j2, . . . ) of Nature\u2019s actions."}, {"heading": "2.2 Online Convex Optimization (OCO)", "text": "OCO extends the framework of no-regret learning to function minimization. Let W be a convex and compact set in Rd, and let F be a set of convex and uniformly bounded functions f : W \u2192 R. Consider a sequential decision problem, where at each stage t \u2265 1 the agent chooses a point wt \u2208 W , and then observes a function ft \u2208 F . An Algorithm for the agent is a rule for choosing wt, t \u2265 1, based on the history {fk, wk}k\u2264t\u22121. The regret of an algorithm A is defined as\nRegretT (A) = sup f1,...,fT\u2208F\n{\nT \u2211\nt=1\nft(wt)\u2212 min w\u2208W\nT \u2211\nt=1\nft(w)\n}\n, (3)\nwhere the supremum is taken over all possible functions ft \u2208 F . An effective algorithm should guarantee a small regret, and in particular one that grows sub-linearly in T .\nThe OCO problem was introduced in this generality in [16], along with the following Online\nGradient Descent algorithm:\nwt+1 = ProjW (wt \u2212 \u03b7tgt) . (4)\nHere gt is an arbitrary element of \u2202ft(wt), the subdifferential of ft at wt, (\u03b7t) is a diminishing gain sequence, and ProjW denotes the Euclidean projection onto the convex set W . To state a regret bound for this algorithm, let diam(W ) denote the diameter of W , and suppose that all subgradients of the functions ft are uniformly bounded in norm by a constant G.\nProposition 2 (Zinkevich, 2003) For the Online Gradient Descent algorithm in (4) with gain sequence \u03b7t = \u03b7\u221a t , \u03b7 > 0, the regret is upper bounded by\nRegretT (OGD) \u2264 ( diam(W )2\n\u03b7 + 2\u03b7G2)\n\u221a T . (5)\nSeveral classes of OCO algorithms are now known, as surveyed in [6, 15, 9]. Of particular relevance here is the Regularized Follow the Leader (RTFL) algorithm, specified by\nwt+1 = argmin w\u2208W\n(\nt \u2211\nk=1\nfk(w) +Rt(w)\n)\n, (6)\nwhere Rt(w), t \u2265 1 is a sequence of regularization functions. With Rt \u2261 0, the algorithm reduces to the basic Follow the Leader (FTL) algorithm, which does not generally lead to sub-linear regret, unless additional requirements such as strong convexity are imposed on the functions ft (we will revisit the convergence of FTL in Section 4). For RFTL, we will require the following standard convergence result. Recall that a function R(w) over a convex set W is called \u03c1-strongly convex if R(w)\u2212 \u03c12\u2016w\u20162 is convex there.\nProposition 3 Suppose that each function ft is Lischitz-continuous over W , with Lipschitz coefficient Lf . Let Rt(w) = \u03c1tR(w), where 0 < \u03c1t < \u03c1t+1, and the function R : W \u2192 [0, Rmax] is Lipschitz continuous with coefficient LR, and is 1-strongly convex. Then,\nRegretT (RFTL) \u2264 2Lf T \u2211\nt=1\nLf + (\u03c1t \u2212 \u03c1t\u22121)LR \u03c1t + \u03c1t\u22121 + \u03c1TRmax . (7)\nThe last bound can be established along the lines of Theorem 2.11 in [15], which considers the case of fixed regularization parameters, \u03c1t \u2261 \u03c10. The proof is outlined in the Appendix."}, {"heading": "3 OCO-Based Approachability", "text": "This section presents the proposed OCO-based approachability algorithm. We start by introducing the support function and some of its properties, and expressing Blackwell\u2019s separation\ncondition in terms of this function. We continue to present the proposed meta-algorithm that employs a generic OCO algorithm, and then provide as an example the specific algorithm that is obtained when Online Gradient Descent is used as the OCO algorithm."}, {"heading": "3.1 The Support Function", "text": "Let set S \u2282 Rd be a closed and convex set. The support function hS : Rd \u2192 R \u222a {\u221e} of S is defined as\nhS(w) , sup s\u2208S\n\u3008w, s\u3009, w \u2208 Rd.\nIt it is evident that hS is a convex function (as a pointwise supremum over linear functions), and is positive homogeneous: hS(aw) = ahS(w) for a \u2265 0. Furthermore, the Euclidean distance from a point r to S can be expressed as\nd(r, S) = max w\u2208B2\n{\u3008w, r\u3009 \u2212 hS(w)} , (8)\nwhere B2 is the closed Euclidean unit ball (see, e.g., [5], Section 8.1.3; this equality may be readily verified using the minimax theorem). It follows that\nargmax w\u2208B2\n{\u3008w, r\u3009 \u2212 hS(w)} = { 0 : r \u2208 S uS(r) : r 6\u2208 S\n(9)\nwith uS(r) as defined in Theorem 1, namely the unit vector pointing to r from ProjS(r).\nBlackwell\u2019s separation condition in (2) can now be written in terms of the support function, as\n\u3008w, r(x, j)\u3009 \u2264 sup s\u2208S \u3008w, s\u3009 \u2261 hS(w) .\nWe thus obtain the following Corollary to Theorem 1.\nCorollary 4 A closed and convex set S is approachable if and only if for every vector w \u2208 B2 there exists x \u2208 \u2206(I) so that\n\u3008w, r(x, j)\u3009 \u2212 hS(w) \u2264 0, \u2200j \u2208 J. (10)\nNote that the last condition can be written as val(w \u00b7 r) \u2264 hS(w), where\nval(w \u00b7 r) \u25b3= min x\u2208\u2206(I) max j\u2208J \u3008w, r(x, j)\u3009 ,\nthe minimax value of the game with the scalar payoff that is obtained by projection the reward vectors r(i, j) onto w. Consequently, a mixed action x that satisfies (10) can be computed as the minimax strategy for the agent in this game."}, {"heading": "3.2 The General Algorithm", "text": "The proposed algorithm builds on the following idea. First, we employ an OCO algorithm to generate a sequence of steering vectors wt \u2208 B2, so that T \u2211\nt=1\n(\u3008wt, rt\u3009 \u2212 hS(wt)) \u2265 T max w\u2208B2 {\u3008w, r\u0304T \u3009 \u2212 hS(w)} \u2212 a(T ), (11)\nwhere rt = r(xt, jt) is considered an arbitrary vector that is revealed after wt is specified, and a(T ) = o(T ). Next, given wt, we choose xt that satisfies (10), so that \u3008wt, rt\u3009 \u2212 hS(wt) \u2264 0. Using this inequality in (11), and observing the distance formula (8), yields\nd(r\u0304t, S) \u2264 a(T )\nT \u2192 0 .\nTo secure (11), observe that the function f(w; r) = \u2212\u3008w, r\u3009 + hS(w) is convex in w for each vector r. Therefore, an OCO algorithm can be applied to the sequence of convex functions ft(w) = \u2212\u3008w, rt\u3009 + hS(w), where rt = r(xt, jt) is considered an arbitrary vector which is revealed only after wt is specified. Applying an OCO algorithm A with RegretT (A) \u2264 a(T ) to this setup, we obtain a sequence (wt) such that\nT \u2211\nt=1\nft(wt) \u2264 min w\u2208B2\nT \u2211\nt=1\nft(w) + a(T ) ,\nwhere\nT \u2211\nt=1\nft(wt) = \u2212 T \u2211\nt=1\n(\u3008wt, rt\u3009 \u2212 hS(wt)) ,\nT \u2211\nt=1\nft(w) = \u2212 T \u2211\nt=1\n(\u3008w, rt\u3009 \u2212 hS(w)) = \u2212T (\u3008w, r\u0304T \u3009 \u2212 hS(w)) .\nThis clearly implies (11).\nThe discussion above leads to the following approachability meta-algorithm.\nAlgorithm 1 (Approachability Meta-Algorithm Based on OCO)\nGiven: A closed, convex and approachable set S; a procedure (e.g., a linear program) to compute x, for a given vector w, so that (10) is satisfied; an OCO algorithm A for the functions ft(w) = \u2212\u3008wt, rt\u3009+ hS(w), with RegretT (A) \u2264 a(T ).\nRepeat for t = 1, 2, . . . :\n1. Obtain wt from the OCO algorithm applied to the convex functions fk(w) = \u2212\u3008w, rk\u3009 + hk(w), k \u2264 t\u2212 1, so that inequality (11) is satisfied.\n2. Choose xt according to (10), so that \u3008wt, r(xt, j)\u3009 \u2212 hS(wt) \u2264 0 holds for all j \u2208 J .\n3. Observe Nature\u2019s action jt, and set rt = r(xt, jt).\nProposition 5 For the algorithm above,\nd(r\u0304T , S) \u2264 a(T )\nT\nis satisfied for all T \u2265 1 and any sequence (j1, j2, . . . ) of Nature\u2019s actions.\nProof: As observed above, application of the OCO algorithm implies (11), so that\nd(r\u0304T , S) = max w\u2208B2\n{\u3008w, r\u0304T \u3009 \u2212 hS(w)}\n\u2264 1 T\nT \u2211\nt=1\n(\u3008wt, rt\u3009 \u2212 hS(wt)) + a(T ) T \u2264 a(T ) T .\nTo recap, any OCO algorithm that guarantees (11) with a(T )T \u2192 0, induces an approachability strategy with rate of convergence a(T )T .\nRemark 1 (Convex Cones) The approachability algorithm developed in [1] starts with a target sets S that are restricted to be convex cones. For S a closed convex cone, the support function is given by\nhS(w) =\n{\n0 : w \u2208 So \u221e : w 6\u2208 So\nwhere So is the polar cone of S. The required inequality in (11) therefore reduces to\nT \u2211\nt=1\n\u3008wt, rt\u3009 \u2265 T max w\u2208B2\u2229So \u3008w, r\u0304T \u3009 \u2212 a(T ) .\nThe sequence (wt) can be obtained in this case by applying an online linear optimization algorithm restricted to wt \u2208 B2 \u2229 So. This is the algorithm proposed in [1].\nThe extension to general convex sets is handled there by lifting the problem to a (d + 1)- dimensional space, with payoff vector r\u2032(x, y) = (\u03ba, r(x, y)) and target set S\u2032 = cone({\u03ba} \u00d7 S), where \u03ba = maxs\u2208S \u2016s\u2016, for which it holds that d(u, S) \u2264 2d(u\u2032, S\u2032). For further details see [1]."}, {"heading": "3.3 An OGD-based Approachability Algorithm", "text": "As a concrete example, let us apply the Online Gradient Descent algorithm specified in (4) to our problem. With W = B2 and ft(w) = \u2212(\u3008w, rt\u3009\u2212hS(w)), we obtain in step 1 of Algorithm 1,\nwt+1 = ProjB2{wt + \u03b7t(rt \u2212 yt)} , yt \u2208 \u2202hS(wt) .\nObserve that ProjB2(v) = v/max{1, \u2016v\u2016}, and (e.g., Corollary 8.25 in [14])\n\u2202hS(w) = argmax s\u2208S\n\u3008s,w\u3009 .\nTo evaluate the convergence rate in (5), observe that diam(B2) = 2, and, since yt \u2208 S, \u2016gt\u2016 = \u2016rt \u2212 yt\u2016 \u2264 \u2016R\u2212 S\u2016, where R = {r(x, y)}x\u2208\u2206(I),y\u2208\u2206(J) is the reward set. Assuming for the moment that the goal set S is bounded, we obtain\nd(r\u0304T , S) \u2264 b(\u03b7)\u221a T , with b(\u03b7) = 4 \u03b7 + 2\u03b7\u2016R \u2212 S\u20162 .\nFor \u03b7 = \u221a 2/\u2016R \u2212 S\u2016, we thus obtain b(\u03b7) = 4 \u221a 2\u2016R \u2212 S\u2016.\nIf S is not bounded, it can always be intersected with R (without affecting its approachability), yielding \u2016R \u2212 S\u2016 \u2264 diam(R). This amounts to modifying the choice of yt in the algorithm to\nyt \u2208 \u2202hS\u2229R(wt) = argmax y\u2208S\u2229R (y,w) .\nAlternatively, one may restrict attention (by projection) to vectors wt in the set {w \u2208 B2 : hS(w) < \u221e}, similarly to the case of convex cones mentioned in Remark 1 above; we will not go into further details here."}, {"heading": "4 Blackwell\u2019s Algorithm and (R)FTL", "text": "We next examine the relation between Blackwell\u2019s approachability algorithm and the present OCO-based framework. We first show that Blackwell\u2019s algorithm coincides with OCO-based approachability when FTL is used as the OCO algorithm. We use this equivalence to establish fast (logarithmic) convergence rates for Blackwell\u2019s algorithm when the target set S has a smooth boundary. Interestingly, this equivalence does not provide a convergence result for general convex sets. To complete the picture, we show that Blackwell\u2019s algorithm can more generally be obtained via a regularized version of FTL, which leads to an alternative proof of convergence of the algorithm in the general case."}, {"heading": "4.1 Blackwell\u2019s algorithm as FTL", "text": "Recall Blackwell\u2019s algorithm as specified in Theorem 1, namely xt+1 is chosen as a mixed action that satisfies (2) for u = uS(r\u0304t).\nLemma 6 For ft(w) = \u2212\u3008w, rt\u3009+ hS(w),\nargmin w\u2208B2\nt \u2211\nk=1\nfk(w) =\n{\nuS(r\u0304t) : r\u0304t 6\u2208 S 0 : r\u0304t \u2208 S .\nProof: Observe that \u2211t k=1 fk(w) = \u2212t(\u3008w, r\u0304t\u3009 \u2212 hS(w)), so that\nargmin w\u2208B2\nt \u2211\nk=1\nfk(w) = argmax w\u2208B2\n{\u3008w, r\u0304t\u3009 \u2212 hS(w)} .\nThe required equality now follows from (9).\nComparing to (6), with Rt \u2261 0, it may be seen that the sequence of projection directions uS(r\u0304t) in Blackwell\u2019s algorithm coincides with the sequence (wt) that is obtained by applying the FTL algorithm to the functions (ft) over w \u2208 B2. It follows that Blackwell\u2019s algorithm is identical to Algorithm 1 with this choice of the OCO algorithm.\nTo establish convergence of Blackwell\u2019s algorithm via this equivalence, one needs to show that FTL guarantees the regret bound in (11) for an arbitrary reward sequence (rt) \u2282 R, with a sublinear rate sequence a(T ). It is well know, however, that (unregularized) FTL does not guarantee sublinear regret, without some additional assumptions on the function ft. A simple counter-example, reformulated to the present case, is devised as follows: Let S = {0} \u2282 R, so that hS(w) = 0, and suppose that r1 = \u22121 and rt = 2(\u22121)t for t > 1. Since wt = sign(r\u0304t\u22121) and sign(rt) = \u2212sign(r\u0304t\u22121), we obtain that ft(wt) = \u2212rtwt = 1, leading to a linearly-increasing regret.\nThe failure of FTL in this example is clearly due to the fast changes in the predictors wt. We now add some smoothness assumptions on the set S that can mitigate such abrupt changes.\nAssumption 1 Let S be a compact and convex set. Suppose that the boundary \u2202S of S is smooth with curvature bounded by \u03ba0, namely:\n\u2016~n(s1)\u2212 ~n(s2)\u2016 \u2264 \u03ba0\u2016s1 \u2212 s2\u2016 for all s1, s2 \u2208 \u2202S , (12)\nwhere ~n(s) is the unique unit outer normal to S at s \u2208 \u2202S.\nFor example, for a closed Euclidean ball of radius \u03c1, (12) is satisfied with equality for \u03ba0 = \u03c1 \u22121. The assumed smoothness property may in fact be formulated in terms of an interior sphere\ncondition: For any point in s \u2208 S there exists a ball B(\u03c1) \u2282 S with radius \u03c1 = \u03ba\u221210 such that s \u2208 B(\u03c1).\nProposition 7 Let Assumption 1 hold. Consider Blackwell\u2019s algorithm as specified in Theorem 1, and denote wt = uS(r\u0304t\u22121) (with w1 arbitrary). Then, for any time T \u2265 1 such that r\u0304T 6\u2208 S, (11) holds with\na(T ) = C0(1 + lnT ), (13)\nwhere C0 = diam(R) \u2016R \u2212 S\u2016\u03ba0, C1 = \u2016R \u2212 S\u2016, and ln(\u00b7) is the natural logarithm. Consequently,\nd(r\u0304T , S) \u2264 C0 1 + lnT\nT , T \u2265 1 . (14)\nProof: See the Appendix.\nThe last result establishes a fast convergence rate (of order log T/T ) for Blackwell\u2019s approachability algorithm, under the assumed smoothness of the target set. We observe that in the stochastic version of the algorithm, which is based on the rewards r(it, jt) rather than r(xt, jt), the convergence is still of order T\u22121/2 due to the added stochastic effect (unless all mixed actions xt happen to be pure). We also note that logarithmic convergence rates for OCO algorithms were derived in [10], under strong convexity conditions on the function ft. Finally, conditions for fast approachability (of order T\u22121) were derived in [13], but are of different nature than the above."}, {"heading": "4.2 Blackwell\u2019s algorithm as RFTL", "text": "The smoothness requirement in Assumption 1 precludes such important target sets as polyhedra and cones. As observed above, in absence of such additional smoothness properties the interpretation of Blackwell\u2019s algorithm through an FTL scheme does not imply its convergence, as the regret of FTL (and the corresponding bound a(T ) in (11)) might increase linearly in general.\nTo address the general case, we show next that the Blackwell\u2019s algorithm can be identified more generally with a regularized version of FTL. This algorithm does guarantee an O( \u221a T ) regret in (11), and consequently leads to the standard O(T\u22121/2) rate of convergence of Blackwell\u2019s approachability algorithm.\nOur starting point is the following observation:\nLemma 8 For fk(w) = \u2212\u3008w, rk\u3009+ hS(w), 1 \u2264 k \u2264 t, and any \u03c1t > 0,\nwt+1 \u25b3 = argmin\nw\u2208B2\n{\nt \u2211\nk=1\nfk(w) + \u03c1t 2 \u2016w\u20162 } =\n{\n\u03b2tuS(r\u0304t) : r\u0304t 6\u2208 S 0 : r\u0304t \u2208 S . (15)\nwhere \u03b2t = min{1, t\u03c1td(r\u0304t, S)} > 0.\nProof: Recall that \u2211t k=1 fk(w) = \u2212t(\u3008w, r\u0304t\u3009 \u2212 hS(w)), so that\nargmin w\u2208B2\n{\nt \u2211\nk=1\nfk(w) + \u03c1t 2 \u2016w\u20162 } = argmax w\u2208B2 {\u3008w, r\u0304t\u3009 \u2212 hS(w) \u2212 \u03c1t 2t \u2016w\u20162} .\nTo compute the right-hand side, we first maximize over {w : \u2016w\u2016 = \u03b2}, and then optimize over \u03b2 \u2208 [0, 1]. Denote r = r\u0304t, and \u03b7 = \u03c1t/t. Similarly to Lemma 6,\nargmax \u2016w\u2016=\u03b2\n{\u3008w, r\u3009 \u2212 hS(w) \u2212 \u03b7\n2 \u2016w\u20162} = argmax \u2016w\u2016=\u03b2 {\u3008w, r\u3009 \u2212 hS(w)} =\n{\n\u03b2uS(r) : r 6\u2208 S 0 : r \u2208 S .\nNow, for r 6\u2208 S, max \u2016w\u2016=\u03b2 {\u3008w, r\u3009 \u2212 hS(w)\u2212 \u03b7 2 \u2016w\u20162} = \u03b2d(r, S) \u2212 \u03b7 2 \u03b22 . Maximizing the latter over 0 \u2264 \u03b2 \u2264 1 gives \u03b2\u2217 = min{1, d(r,S)\u03b7 }. Substituting back r and \u03b7 gives (15).\nEquation (15) defines an RFTL algorithm with quadratic regularization. When used for the OCO part in Algorithm 1, the resulting scheme turns out to be equivalent to Blackwell\u2019s algorithm. Indeed, the minimum in (15) is attained by the same unit vector uS(r\u0304t) that appears in Theorem 1, scaled by a positive constant. That scaling does not affect the choice of xt according to (10), as the support function hS(w) is positive homogeneous. However, this scaling does induce sublinear-regret for the OLO algorithm, and consequently convergence of the approachability algorithm. This is summarized as follows.\nProposition 9 Let S be a convex and compact set. Consider the RTFL algorithm specified in equation (15), with \u03c1t = \u03c1 \u221a t, \u03c1 > 0. The regret of this algorithm is bounded by\nRegretT (RTFL) \u2264 ( 2L2f \u03c1 + \u03c1) \u221a T + 2L2f \u03c1 + Lf ln(4T \u2212 3) \u25b3 = a0(T ) ,\nwhere Lf = \u2016R \u2212 S\u2016. Consequently, if this RTFL algorithm is used in step 1 of Algorithm 1 to provide wt, we obtain\nd(r\u0304T , S) \u2264 a0(T ) T = O(T\u2212 1 2 ) , T \u2265 1 . (16)\nProof: The regret bound follows from the one in Proposition 3, evaluated for ft(w) = \u2212\u3008rt, w\u3009 + hS(s), W = B2, R(w) = \u2016w\u20162, and \u03c1t = \u03c10 \u221a t. Recalling that \u2202ft(w) = \u2212rt + argmaxs\u2208S\u3008w, s\u3009, the Lipschitz constant of ft is upper bounded by \u2016R \u2212 S\u2016 \u25b3 = Lf . Furthermore, Rmax = 1 and LR = 2. Therefore,\nRegretT (RTFL) \u2264 2Lf T \u2211\nt=1\nLf + 2\u03c1( \u221a t\u2212 \u221a t\u2212 1)\n\u03c1( \u221a t+ \u221a t\u2212 1)\n+ \u03c1 \u221a T .\nUpper bounding the sums with corresponding integrals gives the stated regret bound. The second part now follows directly from Proposition 5. With \u03c1 = \u221a 2Lf , we obtain in (16) the convergence rate\nd(r\u0304T , S) \u2264 2 \u221a 2\u2016R \u2212 S\u2016\u221a\nT + o( 1\u221a T ) .\nWe emphasize that the algorithm discussed in this section is equivalent to Blackwell\u2019s algorithm, hence its convergence is well known. The proof of convergence here is certainly not the simplest, nor does it lead to the best constants in the convergence rate. Indeed, Blackwell\u2019s proof (which recursively bounds the square distance d(r\u0304T , S) 2) leads to the bound d(r\u0304T , S) \u2264 \u2016R\u2212S\u2016\u221aT . Rather, our main purpose here was to provide an alternative view and analysis of Blackwell\u2019s algorithm, which rely on a standard OCO algorithm. Nonetheless, the logarithmic convergence rate that was obtained under the smoothness Assumption 1 appears to be new."}, {"heading": "Acknowledgements", "text": "The author wishes to thank Elad Hazan for helpful comments on a preliminary version of this work. This research was supported by the Israel Science Foundation grant No. 1319/11."}], "references": [{"title": "Blackwell approachability and low-regret learning are equivalent", "author": ["J. Abernethy", "P.L. Bartlett", "E. Hazan"], "venue": "Proceedings of the 24th Conference on Learning Theory (COLT\u201911), pages 27\u201346, Budapest, Hungary, June", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Response-based approachability with applications to generalized no-regret problems", "author": ["A. Bernstein", "N. Shimkin"], "venue": "To appear in Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Controlled random walks", "author": ["D. Blackwell"], "venue": "Proceedings of the International Congress of Mathematicians, volume III, pages 335\u2013338,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1954}, {"title": "An analog of the minimax theorem for vector payoffs", "author": ["D. Blackwell"], "venue": "Pacific Journal of Mathematics, 6:1\u20138,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1956}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, Cambridge, UK,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press, New York, NY,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximation to Bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games, 3:97\u2013139,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1957}, {"title": "A general class of adaptive strategies", "author": ["S. Hart", "A. Mas-Colell"], "venue": "Journal of Economic Theory, 98:26\u201354,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "The convex optimization approach to regret minimization", "author": ["E. Hazan"], "venue": "S. Sra et al., editor, Optimization for Machine Learning, chapter 10. MIT Press, Cambridge, MA,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning, 69(2-3):169\u2013192,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Game Theory", "author": ["M. Maschler", "E. Solan", "S. Zamir"], "venue": "Cambridge University Press, Cambridge, UK,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Approachability, regret and calibration: Implications and equivalences", "author": ["V. Perchet"], "venue": "Journal of Dynamics and Games, 1:181\u2013254,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Approachability, fast and slow", "author": ["V. Perchet", "S. Mannor"], "venue": "Proc. COLT 2013: JMLR Workshop and Conference Proceedings, volume 30, pages 474\u2013488,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Variational Analysis", "author": ["R.T. Rockafellar", "R. Wets"], "venue": "Springer-Verlag,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning, 4:107\u2013194,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "Proceedings of the 20th International Conference on Machine Learning (ICML \u201903), pages 928\u2013936,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "The concept of approachability, introduced in [4], addresses a fundamental feasibility issue in for repeated matrix games with vector-valued payoffs.", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "An extensive recent survey of approachability and its implications can be found in [12], and a textbook exposition is available in [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "An extensive recent survey of approachability and its implications can be found in [12], and a textbook exposition is available in [11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "Concurrently, Hannan [7] introduced the concept of no-regret play for repeated matrix games.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "The textbook [6] offers a broad overview of regret and online learning.", "startOffset": 13, "endOffset": 16}, {"referenceID": 14, "context": "Recent surveys of OCO algorithms may be found in [15, 9].", "startOffset": 49, "endOffset": 56}, {"referenceID": 8, "context": "Recent surveys of OCO algorithms may be found in [15, 9].", "startOffset": 49, "endOffset": 56}, {"referenceID": 2, "context": "This was already observed in [3]; an alternative formulation that leads to more explicit strategies was proposed in [8].", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "This was already observed in [3]; an alternative formulation that leads to more explicit strategies was proposed in [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "More recently, it was shown in [1] that any no-regret algorithm for the online linear optimization problem can be used as a basis for an approachability strategy for convex target sets.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "The scheme suggested in [1] first considers target sets S that are convex cones.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Section 3 presents the proposed scheme, in the form of a meta-algorithm that relies on a generic OCO algorithm, discusses the relation to the scheme of [1], and demonstrates a specific algorithm that is obtained by using Generalized Gradient Descent for the OCO algorithm.", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "The approachability strategy introduced by Blackwell has been generalized in [8], that essentially allow different norms to be used for the projection unto S.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Several recent papers have proposed approachability algorithms that depend on Blackwell\u2019s dual condition (condition (ii) in the above Theorem) and avoid the projection step altogether (see [2] and references therein).", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "We note that the results in [1] are developed for the rewards r(xt, yt), with the mean taken over yt as well, and the agent is allowed to observe Nature\u2019s mixed action yt (or at least the mean reward r(xt, yt)).", "startOffset": 28, "endOffset": 31}, {"referenceID": 15, "context": "The OCO problem was introduced in this generality in [16], along with the following Online", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "Several classes of OCO algorithms are now known, as surveyed in [6, 15, 9].", "startOffset": 64, "endOffset": 74}, {"referenceID": 14, "context": "Several classes of OCO algorithms are now known, as surveyed in [6, 15, 9].", "startOffset": 64, "endOffset": 74}, {"referenceID": 8, "context": "Several classes of OCO algorithms are now known, as surveyed in [6, 15, 9].", "startOffset": 64, "endOffset": 74}, {"referenceID": 14, "context": "11 in [15], which considers the case of fixed regularization parameters, \u03c1t \u2261 \u03c10.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": ", [5], Section 8.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "Remark 1 (Convex Cones) The approachability algorithm developed in [1] starts with a target sets S that are restricted to be convex cones.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "This is the algorithm proposed in [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "For further details see [1].", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "25 in [14]) \u2202hS(w) = argmax s\u2208S \u3008s,w\u3009 .", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "We also note that logarithmic convergence rates for OCO algorithms were derived in [10], under strong convexity conditions on the function ft.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Finally, conditions for fast approachability (of order T\u22121) were derived in [13], but are of different nature than the above.", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "To compute the right-hand side, we first maximize over {w : \u2016w\u2016 = \u03b2}, and then optimize over \u03b2 \u2208 [0, 1].", "startOffset": 97, "endOffset": 103}], "year": 2015, "abstractText": "The notion of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions for approachability and corresponding strategies that rely on computing steering directions as projections from the current average payoff vector to the (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on the no-regret properties of Online Linear Programming for computing a suitable sequence of steering directions. This is first carried out for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on the support function of the set, along with suitable Online Convex Optimization algorithms, which leads to a general class of approachability algorithms. We further show that Blackwell\u2019s original algorithm and its convergence follow as a special case.", "creator": "LaTeX with hyperref package"}}}