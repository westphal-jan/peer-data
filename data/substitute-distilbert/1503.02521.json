{"id": "1503.02521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "A Single-Pass Classifier for Categorical Data", "abstract": "same paper describes a new method for classifying a dataset that partitions elements into different categories. it has relations with factor networks but works in considerably different way, absorbing only a single pass through the classifier to generate the weight sets. a grid structure is required and a novel device of converting a row covering real values is a 2 - d or grid - like structure of value bands. each cell encoding the band can generally store a cell weight value and also a set of weights that represent its own relative to each of the output categories. for any input that accepts the be categorised, all of the output weight lists for each relevant input cell can be retrieved and summed to produce a probability for discovering the correct output portion is. so the relative importance being each input point to the output is distributed to each cell. the construction process itself can simply be the reinforcement of the weight values, without requiring substantial iterative calculation process, making it potentially much faster.", "histories": [["v1", "Mon, 9 Mar 2015 15:28:32 GMT  (416kb)", "http://arxiv.org/abs/1503.02521v1", null], ["v2", "Tue, 14 Apr 2015 16:32:44 GMT  (416kb)", "http://arxiv.org/abs/1503.02521v2", "Small correction in the test results. Small amount of new information"], ["v3", "Wed, 14 Oct 2015 18:06:44 GMT  (471kb)", "http://arxiv.org/abs/1503.02521v3", "New test results and additional information"], ["v4", "Wed, 29 Jun 2016 10:40:50 GMT  (499kb)", "http://arxiv.org/abs/1503.02521v4", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kieran greer"], "accepted": false, "id": "1503.02521"}, "pdf": {"name": "1503.02521.pdf", "metadata": {"source": "CRF", "title": "A Single-Pass Classifier for Categorical Data", "authors": ["Kieran Greer"], "emails": [], "sections": [{"heading": null, "text": "different categories. It has relations with neural networks but works in a different way, requiring only a single pass through the classifier to generate the weight sets. A grid structure is required and a novel idea of converting a row of real values into a 2-D or grid-like structure of value bands. Each cell in the band can then store a cell weight value and also a set of weights that represent its own importance to each of the output categories. For any input that needs to be categorised, all of the output weight lists for each relevant input cell can be retrieved and summed to produce a probability for what the correct output category is. So the relative importance of each input point to the output is distributed to each cell. The construction process itself can simply be the reinforcement of the weight values, without requiring an iterative adjustment process, making it potentially much faster.\nKeywords: classifier, grid architecture, deconstructed data."}, {"heading": "1 Introduction", "text": "This paper describes a new method for classifying a dataset that partitions elements into different categories. It has relations with neural networks but works in a different way, requiring only a single pass through the classifier to generate the weight sets. A grid structure is required and a novel idea of converting a row of real values into a 2-D or gridlike structure of value bands. Each cell in the band can then store a cell weight value and also a set of weights that represent its own importance to each of the output categories. For any input that needs to be categorised, all of the output weight lists for each relevant input cell can be retrieved and summed to produce a probability for what the correct output category is. So the relative importance of each input point to the output is distributed to\neach cell. The construction process itself can simply be the reinforcement of the weight values, without requiring an iterative adjustment process, making it potentially much faster.\nThe rest of this paper is organised as follows: section 2 describes some related work and section 3 describes the new classifier architecture. Section 5 gives an example scenario for when or how it might be used. Section 5 gives the result of some tests using it, while section 6 gives some conclusions to the work."}, {"heading": "2 Related Work", "text": "The classifier is not really a neural network because it does not have a neural architecture, but a fixed grid-like one. It was however inspired in a small way by the topology feature of the Self-Organising Map [6][7]. Not many of the neural networks have this feature and it is more visual than what is normal [10]. The topology can present the input data as a 2-D array and makes it possible for the SOM to learn hand-written characters, for example. Another example might be [4] (or the related Neocognitron). There is also a small amount of influence from the biological/genetic classifiers, or maybe a recent and controversial paper [1][8] which suggests that neurons inside the brain store a memory of what synapses they should form. Previously, this information was thought to be in the synapses themselves. However they state that: \u2018Yet there\u2019s no known mechanism by which a neuron could store a molecular \u2018map\u2019 of its own connections and their differing strengths\u2019, where a pattern of different strengths is required. Could the deconstructed point store that map? This model is not competitive like the SOM and it is also supervised, but the training process requires relatively few stages and is automatic. The new classifier also uses a direct association approach, rather like an associative network [7], but it is probably not the same. It still tries to generalise over the input by deconstructing the datasets and not to produce an exact memory-recall mapping. A grid-like structure was also used in [5] to try to represent bits of a problem or solution, but again using a different type of system."}, {"heading": "3 New Classifier Architecture", "text": "The decision to try to use a topology, or in effect, simply a grid structure, was to try to separate the input values, which would allow them to be more distinct in their relation to other sets of values. Adding another dimension here might help to simplify the learning process. It also became clear that a typical set of real-valued inputs might not map easily to a topology that is not also a single line of nodes. To map a single row of data values onto a 2-D grid therefore, requires the introduction of bands, so that each value range can be represented by a separate band. This is a bit like mapping an analogue value onto a set of binary ones. If each value range is replaced by a set of graded bands, then the values can be placed approximately in the correct band and be placed in a distinct position from other values for the same data point. This deconstruction process would give it unique properties when linking up with other data points. There is then a problem however of how to map these cell sets onto the output categories, as an iterative learning process is not involved. As each cell is more individual however, it can be used to represent a more distinct relation with the output as well. This can be achieved by allowing each cell to store its own relation with the output categories and then combine all of the relevant cells when it comes to classifying something. It again uses the same sort of mechanism that a neural network uses, by storing inherently in it, partial pieces of information that can be combined in a generic way. A traditional neural network [10] stores the relation to the output by function transitions through layers of neurons. Only the output layer is directly related to the output categories however, even if output errors are used to adjust every node. For this classifier, there is only a single weight value for each cell, but it then stores a different or unique set of weights for its relation to the output. Each cell therefore has a single direct relation to each output category, while in a neural network this is typically condensed into the output layer weight sets only. So it is these individual output weight sets that determine what the classifier result is, while the separate cell weight might still help with accuracy."}, {"heading": "3.1 Classifier and Data Row Structure", "text": "The classifier and the data row share the same basic structure, shown in Figure 1, where in the classifier, all of the data rows are combined. Each data row is now represented by a gridlike structure, or is now 2-D and not 1-D. So an extra dimension has been added, but the\ninput is then reduced to a more binary form. In the classifier, each data row is added in turn and simply increments each cell or output category that it belongs to. As these value adjustments all overlap with each other, these combined value sets can provide the generalisation properties of a neural network, for example. A major problem with existing classifiers is whether the data is linearly separable. That means \u2013 can you divide the categories using a straight line. If the data is not linearly separable, then more complex transformation functions are often required. This classifier might not have that problem, as the linear construct is itself localised and deconstructed. Or possibly, the separating plane itself is broken-up by the localised nature of the values."}, {"heading": "4 Example Scenario", "text": "Figure 1 is a schematic showing what a data row and the classifier itself might look like and a calculation from it. The top left of the figure shows two training dataset input rows, where there are only two categories to classify. Data row 1 belongs to category 2 and data row 2 belongs to category 1. The data can be normalised first, which makes it easier to separate it into bands. For example, if the data is normalised to be in the range 0 to 1, then 10 bands of size 0.1 can easily be created. The bottom half of the schematic shows the classifier itself, where the two data rows are represented by using the same colour in each of the related cells. Each cell also stores its own relation to the output categories, which are made-up here, but show the expected weight bias. A new or test data row is presented, which falls into cells 1-4, 2-2, 3-4, 4-1. This is shown in the top right corner, with the summed output weight values. Looking at these sums shows that the new input row would be classified as belonging to category 1."}, {"heading": "4.1 Construction Process", "text": "For each training data row that is presented, the appropriate cell for each data point is calculated. The weight for that point is then incremented by some specified amount. This might be: \u20181.0 divided by the number of data rows\u2019, for example. The output array for the cell is retrieved and the element corresponding to the correct category is also incremented. If the dataset is not well balanced with respect to number of rows in each category, then this bias will influence the resulting classification. So it is best if the training dataset has the same number of training data rows for each category type. If that is the case, then there should not be a bias in any subsequent category selection. If the number of rows for each category is unbalanced, then each output category can be incremented by a different amount instead, for example: \u20181.0 divided by number of rows in the category\u2019. This will then result in balanced output weight sizes after training."}, {"heading": "5 Test Results", "text": "A test program has been written in the C# .Net language. It can read in a data file, normalise it, generate the classifier from it and measure how many row categories it subsequently\nevaluates correctly. The classifier was tested on 3 arbitrarily chosen datasets from the UCI Machine Learning Repository [9]. These were the Zoo database [11], Iris Plants database [2] and the Wine Recognition database [3]. All of the datasets were assigned output category increment amounts as described at the end of section 4.1, but it was really only the Zoo dataset that was too unbalanced. These datasets were selected earlier for testing selforganising classifiers, but they include output categories and so are suitable for testing the new classifier as well.\nFor each dataset, the input data was firstly normalised and then each row was presented to the classifier with the corresponding output category, and the appropriate cell values would be updated. After this training phase, each row would be presented again and the classifier would calculate its output category for that row; which would be compared to the correct category. So as all of the data rows have been combined into the same set of cells, it is not a direct retrieval of the input data, but still a generalisation over it. It was also possible to split a dataset into a training set and a testing set and get similar types of result for the previously unseen testing set, after the classifier was trained. The following Table 1 gives the results of how many rows in each dataset the classifier correctly classified.\nThese results would be comparable with what a SOM might produce, for example, but note that each data row was presented only once and performed only 1 weight update on the appropriate cell."}, {"heading": "6 Conclusions", "text": "This paper describes a new type of classifier that can be trained very quickly and is also very accurate. The function that is used in this version is linear, but it is not obviously one of the known types of neural network \u2013 associative or SOM, for example. Its strength might lie in the direct but also deconstructed mapping between each cell and the output categories, without any need for complex transformations. This new classifier might also have an advantage when it comes to linearly separable or non-separable datasets. The classifier might also be interesting as part of a biological model."}, {"heading": "7 References", "text": "[1] Chen, S., Cai, D., Pearce, K., Sun, P.Y-W, Roberts, A.C. and Glanzman, D.L. (2014). Reinstatement\nof long-term memory following erasure of its behavioral and synaptic expression in Aplysia, eLife 2014;3:e03896, pp. 1 - 21. DOI: 10.7554/eLife.03896.\n[2] Fisher,R.A. (1936). The use of multiple measurements in taxonomic problems, Annual Eugenics,\n7, Part II, 179-188, also in 'Contributions to Mathematical Statistics' (John Wiley, NY, 1950).\n[3] Forina, M. et al. (1991). PARVUS - An Extendible Package for Data Exploration, Classification and\nCorrelation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.\n[4] Fukishima, K. (1988). A Neural Network for Visual Pattern Recognition. IEEE Computer, 21(3), 65\n\u2013 75.\n[5] Greer, K. (2012). A Stochastic Hyper-Heuristic for Matching Partial Information, Advances in\nArtificial Intelligence, Vol. 2012, Article ID 790485, 8 pages. doi:10.1155/2012/790485, Hindawi.\n[6] Kohonen, T. (1990). The self-organizing map, Proceedings of the IEEE, Vol. 78, Issue 9, pp. 1464 -\n1480, ISSN: 0018-9219.\n[7] Rojas, R. (1996). Neural Networks: A Systematic Introduction. Springer-Verlag, Berlin and online\nat books.google.com.\n[8] Tdtechnosys.com. (2015). http://tdtechnosys.com/boldial/the-synapse-memory-doctrine-\nthreatened/\n[9] UCI Machine Learning Repository (2015). http://archive.ics.uci.edu/ml/.\n[10] Widrow, B. and Lehr, M. (1990). 30 Years of adaptive neural networks: perceptron, Madaline\nand backpropagation, Proc IEEE, Vol. 78, No. 9, pp. 1415-1442.\n[11] Zoo database (2015). https://archive.ics.uci.edu/ml/datasets/Zoo."}], "references": [{"title": "Reinstatement of long-term memory following erasure of its behavioral and synaptic expression in Aplysia, eLife 2014;3:e03896, pp. 1 - 21. DOI: 10.7554/eLife.03896", "author": ["S. Chen", "D. Cai", "K. Pearce", "Sun", "P.Y-W", "A.C. Roberts", "D.L. Glanzman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The use of multiple measurements in taxonomic problems, Annual Eugenics, 7, Part II, 179-188, also in 'Contributions to Mathematical Statistics", "author": ["Fisher", "R.A"], "venue": "(John Wiley,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1950}, {"title": "PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies", "author": ["M Forina"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "A Neural Network for Visual Pattern Recognition", "author": ["K. Fukishima"], "venue": "IEEE Computer, 21(3),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "A Stochastic Hyper-Heuristic for Matching Partial Information", "author": ["K. Greer"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The self-organizing map", "author": ["T. Kohonen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Neural Networks: A Systematic Introduction. Springer-Verlag, Berlin and online at books.google.com", "author": ["R. Rojas"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "It was however inspired in a small way by the topology feature of the Self-Organising Map [6][7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "It was however inspired in a small way by the topology feature of the Self-Organising Map [6][7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "Another example might be [4] (or the related Neocognitron).", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "There is also a small amount of influence from the biological/genetic classifiers, or maybe a recent and controversial paper [1][8] which suggests that neurons inside the brain store a memory of what synapses they should form.", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "The new classifier also uses a direct association approach, rather like an associative network [7], but it is probably not the same.", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "A grid-like structure was also used in [5] to try to represent bits of a problem or solution, but again using a different type of system.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "These were the Zoo database [11], Iris Plants database [2] and the Wine Recognition database [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "These were the Zoo database [11], Iris Plants database [2] and the Wine Recognition database [3].", "startOffset": 93, "endOffset": 96}], "year": 2015, "abstractText": "This paper describes a new method for classifying a dataset that partitions elements into different categories. It has relations with neural networks but works in a different way, requiring only a single pass through the classifier to generate the weight sets. A grid structure is required and a novel idea of converting a row of real values into a 2-D or grid-like structure of value bands. Each cell in the band can then store a cell weight value and also a set of weights that represent its own importance to each of the output categories. For any input that needs to be categorised, all of the output weight lists for each relevant input cell can be retrieved and summed to produce a probability for what the correct output category is. So the relative importance of each input point to the output is distributed to each cell. The construction process itself can simply be the reinforcement of the weight values, without requiring an iterative adjustment process, making it potentially much faster.", "creator": "Microsoft\u00ae Word 2010"}}}