{"id": "1503.04596", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2015", "title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network", "abstract": "we present a neural network algorithm and training method designed do enable very rapid training and low implementation complexity. due to its running speed and very few tunable parameters, the science introduces strong potential for embedded hardware applications experiencing frequent retraining or online training. intelligent programming is characterized by ( a ) convolutional filters based on biologically structured visual processing techniques, ( ia ) randomly - valued classifier - stage input weights, ( c ) ways of least squares regression to train the initial output weights in a timed batch, and ( d ) linear classifier - stage output units. we demonstrate the efficacy of the method recognizing an image classifier, obtaining state - of - the - art results on the mnist ( 0. 37 % error ) and norb - small ( 2. 2 % ) image classification databases, with very fast conversion times compared to standard deep network approaches. the network'api performance on the google street view preview cache ( svhn ) ( 4 % ) database remain ultimately competitive with state - of - the art methods.", "histories": [["v1", "Mon, 16 Mar 2015 10:41:30 GMT  (194kb,D)", "http://arxiv.org/abs/1503.04596v1", "6 pages, 2 figures, 2015 conference submission"], ["v2", "Wed, 10 Jun 2015 06:26:40 GMT  (187kb,D)", "http://arxiv.org/abs/1503.04596v2", "6 pages, 2 figures, Accepted for IJCNN 2015 (International Joint Conference on Neural Networks, 2015)"], ["v3", "Sat, 15 Aug 2015 13:02:08 GMT  (187kb,D)", "http://arxiv.org/abs/1503.04596v3", "7 pages, 2 figures, Paper at IJCNN 2015 (International Joint Conference on Neural Networks, 2015)"]], "COMMENTS": "6 pages, 2 figures, 2015 conference submission", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["mark d mcdonnell", "tony vladusich"], "accepted": false, "id": "1503.04596"}, "pdf": {"name": "1503.04596.pdf", "metadata": {"source": "CRF", "title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network", "authors": ["Mark D. McDonnell", "Tony Vladusich"], "emails": ["mark.mcdonnell@unisa.edu.au"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nState-of-the-art performance on many image classification databases has been achieved recently using multilayered (i.e., deep) neural networks [1]. Such performance generally relies on a convolutional feature extraction stage to obtain invariance to translations, rotations and scale [2\u20135]. Training of deep networks, however, often requires significant resources, in terms of time, memory and computing power (e.g. in the order of hours on GPU clusters). Tasks that require online learning, or periodic replacement of all network weights based on fresh data, and/or implementation in low-power embedded devices may thus not be able to benefit from deep learning techniques. It is desirable, therefore, to seek very rapid training methods, even if this is potentially at the expense of a small performance decrease.\nRecent work has shown that good performance on image classification tasks can be achieved in \u2018shallow\u2019 convolutional networks\u2014neural architectures containing a single training layer\u2014provided sufficiently many features are extracted [3]. Perhaps surprisingly, such performance arises even with the use of entirely random convolutional filters or filters based on randomly selected patches from training images [4]. Although application of a relatively large numbers of filters is common (followed by spatial image smoothing and downsampling), good classification performance can also be obtained with a sparse feature representation (i.e. relatively few filters and minimal downsampling) [5].\nBased on these insights and the goal of devising a fast\ntraining method, we introduce a method for combining several existing general techniques into what is equivalent to a five layer neural network (see Figure 1) with only a single trained layer (the output layer), and show that the method:\n1) provides state-of-the-art results on well known image classification databases; 2) is trainable in times in the order of minutes (up to several hours for large training sets) on standard desktop/laptop computers; 3) is sufficiently versatile that the same parameter sets can be applied to different datasets and still produce results comparable to dataset-specific optimisation of tunable parameters (i.e. hyper-parameters).\nThe fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews. The network architecture in the classification stage is that of a three layer neural network comprised from an input layer, a hidden layer of nonlinear units, and a linear output layer. The input weights are randomly chosen and untrained, and the output weights are trained in a single batch using least squares regression. Due to the convexity of the objective function, this method ensures the output weights are optimally chosen for a given set of random input weights. The rapid speed of training is due to the fact that the least squares optimisation problem an be solved using an O(KM2) algorithm, where M is the number of hidden units and K the number of training points [15].\nWhen applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases [15\u201320] but poor performance on more difficult ones. To our knowledge, however, the method has not yet been applied to convolutional features.\nTherefore, we have devised a network architecture (see Figure 1) that consists of three key elements that work together to ensure fast learning and good classification performance: namely, the use of (a) convolutional feature extraction, (b) random-valued input weights for classification, (c) least squares training of output weights that feed in to (d) linear output units. We apply our network to several image classification databases, including MNIST [21], CIFAR-10 [22], Google Street View House Numbers (SVHN) [23] and NORB [24]. The network produces state-of-the-art classification results on MNIST and NORB-small databases and near state-of-the-art\nar X\niv :1\n50 3.\n04 59\n6v 1\n[ cs\n.N E\n] 1\n6 M\nar 2\n01 5\nperformance on SVHN. Improved performance on CIFAR10 is likely to be obtained by better filter selection and introduction of training data augmentation."}, {"heading": "II. NETWORK ARCHITECTURE AND TRAINING ALGORITHMS", "text": "The overall network is shown in Figure 1. There are three hidden layers with nonlinear units, and four layers of weights. The first layer of weights is the convolutional filter layer. The second layer is a pooling (low pass filtering) and downsampling layer. The third layer is a random projection layer. The fourth layer is the only trained layer. The output layer has linear units.\nThe network can be conceptually divided into two stages and two algorithms. The first stage is the convolutional feature extraction stage, and largely follows that of existing approaches to image classification [3\u20135, 26]. The second stage is the classifier stage, and largely follows the approach of [15, 20]. We now describe the two stages in detail."}, {"heading": "A. Stage 1 Architecture: Convolutional filtering and pooling", "text": "The algorithm we apply to extract features from images (including multiple channel images) is given in Algorithm 1. The details of the filters hi,c and hp are given in the Experiments section, but first we denote the size of the 2D filters as W \u00d7W and Q\u00d7Q.\nInput : Set of K images, xk, each with C channels Output: Feature vectors, fk, k = 1, . . .K foreach xk do\nsplit xk into channels, xk,c, c = 1, . . . C foreach i = 1, . . . P filters do\nApply filter to each channel: yi,k,c \u2190 hi,c \u2217 xk,c Apply termwise nonlinearity: zi,k,c \u2190 g1(yi,k,c) Apply lowpass filter: wi,k,c \u2190 hp \u2217 zi,k,c Apply termwise nonlinearity: si,k,c \u2190 g2(wi,k,c) Downsample: s\u0302i,k,c \u2190 si,k,c Concatenate channels: ri,k \u2190 [s\u0302i,k,1| . . . |s\u0302i,k,C ] Normalize: fi,k = ri,k/(ri,k1>)\nend Concatenate over filters: fk \u2190 [f1,k|f2,k| . . . |fP,k]\nend Algorithm 1: Convolutional feature detection\nThis sequence of steps in Algorithm 1 suggest looping over all images and channels sequentially. However, the following mathematical formulation of the algorithm indicates a standard layered neural network formulation of this algorithm is applicable, as shown in Figure 1 and therefore that computation of all features (fk, k = 1, ..K ) can be obtained in one shot from a K-column matrix containing a batch of K training points.\nThe key to this formulation is to note that since convolution is a linear operator, a matrix can be constructed that when multiplied by a data matrix produces the same result as convolution applied to one instance of the data. Hence, for a total of L features per image, we introduce the following matrices. Let\n\u2022 F be a feature matrix of size L\u00d7K;\n\u2022 X be a data matrix with K columns; \u2022 WFilter be a concatenation of the CP convolution ma-\ntrices corresponding to hi,c i = 1, . . . P, c = 1, . . . C.;\n\u2022 W0 be a convolution matrix corresponding to hl, that also down samples by a factor of D;\n\u2022 WPool be a block diagonal matrix containing CP copies of W0 on the diagonals.\nThe entire flow described in Algorithm 1 can be written mathematically as\nF = g2(WPool g1(WFilterX)), (1)\nwhere g1(\u00b7) and g2(\u00b7) are applied term by term to all elements of their arguments. The matrices WFilter and WPool are sparse Toeplitz matrices. In practice we would not form them directly, but instead form one pooling matrix, and one filtering matrix for each filter, and sequential apply each filter to the entire data matrix, X.\nWe use a particular form for the nonlinear hidden-unit functions g1(\u00b7) and g2(\u00b7) inspired by LP-pooling [26], which is of the form g1(u) = up and g2(v) = v 1 p . For example, with p = 2 we have\nF = \u221a WPool (WFilterX)2. (2)\nAn intuitive explanation for the use of LP-pooling is as follows. First, note that each hidden unit receives as input a linear combination of a patch of the input data, i.e. u in g1(u) has the form u = \u2211W 2 j=1 hi,c,jxi,j . Hence, squaring u results in a sum that contains terms proportional to x2i,j and terms proportional to products of each xi,j . Thus, squaring is a simple way to produce hidden layer responses that depend on the product of pairs of input data elements, i.e. interaction terms, and this is important for discriminability. Second, the square root transforms the distribution of the hidden-unit responses; we have observed that in practice, the result of the square root operation is often a distribution that is closer to Gaussian than without it, which helps to regularise the least squares regression method of training the output weights.\nHowever, as will be described shortly, the Classifier stage also has a square nonlinearity. Using this nonlinearity, we have found that classification performance is generally optimised by taking the square root of the input to the random projection layer. Based on this observation, we do strictly use LP-pooling, and instead set\ng1(u) = u 2, (3)\nand g2(v) = v 0.25. (4)\nThis effectively combines the implementation of L2-pooling, and the subsequent square root operation."}, {"heading": "B. Stage 2 Architecture: Classifier", "text": "The following descriptions are applicable whether or not raw pixels are treated as features or the input is the features extracted in stage 1. First, we introduce notation. Let:\n\u2022 Ftrain, of size L \u00d7K, contain each length L feature vector;\n\u2022 Ylabel, of size N \u00d7 K, numerically represent the labels of each training vector, where there are N classes\u2014we set each column to have a 1 in a single row, corresponding to the label class for each training vector, and all other entries to be zero;\n\u2022 Win, of size L\u00d7M be the real-valued input weights matrix for the classifier stage;\n\u2022 Wout, of size M \u00d7 N be the real-valued output weights matrix for the classifier stage;\n\u2022 the function g(\u00b7) be the activation function of each hidden-unit; for example, g(\u00b7) may be the logistic sigmoid, g(z) = 1/(1 + exp(\u2212z)), or a squarer, g(z) = z2;\n\u2022 Atrain = g(WinFtrain), of size M \u00d7K, contain the hidden-unit activations that occur due to each feature vector; g(\u00b7) is applied termwise to each element in the matrix WinFtrain."}, {"heading": "C. Stage 1 Training: Filters and Pooling", "text": "In this paper we do not employ any form of training for the filters and pooling matrices. The details of the filter weights and form of pooling are given in the Results section."}, {"heading": "D. Stage 2 Training: Classifier Weights", "text": "The training approach we use is that described by e.g. [6, 7, 14], where the input weights, Win are generated randomly from a specific distribution, e.g. standard Gaussian, uniform, or bipolar. However, it is known that setting these weights non-randomly based on the training data leads to superior performance [15, 17, 20]. These weights can also be trained, if desired, using single-batch backpropagation [18].\nGiven a choice of Win, the output weights matrix is determined according to\nWout = YlabelA + train, (5)\nwhere A+train is the size K \u00d7 M Moore-Penrose pseudo inverse corresponding to Atrain. This solution is equivalent to solving a standard least squares regression for an overcomplete set of linear equations. It is known to often be useful to\nregularise such problems, and instead solve the following ridge regression problem [13, 14]:\nWout = YlabelA > train(AtrainA > train + cI) \u22121, (6)\nwhere c is a hyper-parameter and I is the M \u00d7M identity matrix. In practice, it is efficient to avoid explicit calculation of the inverse in Equation (6) [15] and instead use QR factorisation to solve the following set of NM linear equations for the NM unknown variables in Wout:\nYlabelA > train = Wout(AtrainA > train + cI). (7)\nAbove we mentioned two algorithms, and Algorithm 2 is simply to form Atrain and solve Eqn. (7), followed by optimisation of c using ridge regression. For large M and K > M (which is typically valid) the runtime bottleneck for this method is typically the O(KM2) matrix multiplication required to obtain AtrainA > train."}, {"heading": "E. Application to Test Data", "text": "For a total of Ktest test images contained in a matrix Xtest, we first obtain a matrix Ftest = g2(WPool g1(WFilterXtest)), of size L \u00d7 Ktest, by following Algorithm 1. The output of the classifier is then the N \u00d7Ktest matrix\nYtest = Wout g(WinFtest) (8) = Wout g(Win g2(WPool g1(WFilterX))). (9)\nNote that we can write the response to all test images in terms of the training data:\nYtest = Ylabel (g(WinFtrain)) +\ng(WinFtest) (10) where\nFtrain = g2(WPool g1(WFilterXtrain)) (11) Ftest = g2(WPool g1(WFilterXtest)). (12)\nThus, since the pseudo-inverse, (\u00b7)+, can be obtained from Equation (6), Equations (10), (11) and (12) constitute a closedform solution for the entire test-data classification output, given specified matrices, Wfilter, Wpool and Win, and hidden-unit activation functions, g1, g2, and g.\nThe final classification decision for each image is obtained by taking the index of the maximum value of each column of Ytest.\nIII. IMAGE CLASSIFICATION EXPERIMENTS: SPECIFIC DESIGN\nWe examined the method\u2019s performance when used as a classifier of images. Table I lists the attributes of four well known databases we used. For the two databases comprised from RGB images, we used C = 4 channels, namely the raw RGB channels, and a conversion to greyscale. This approach was shown to be effective for SVHN in [27]."}, {"heading": "A. Preprocessing", "text": "All raw image pixel values were converted to the interval [0, 1]. Due to the use of quadratic nonlinearities and LPpooling, this scaling does not affect performance. The only other preprocessing done was as follows:\n1) MNIST: None; 2) NORB-small: downsample from 96\u00d7 96 to 32\u00d732; 3) SVHN: convert from 3 channels to 4 by adding a\nconversion to greyscale from the raw RGB. We found that local and/or global contrast enhancement only diminished performance;\n4) CIFAR-10: convert from 3 channels to 4 by adding a conversion to greyscale from the raw RGB; apply ZCA whitening to each channel of each image, as in [3]."}, {"heading": "B. Stage 1 Design: Filters and Pooling", "text": "Since our objective here was to train only a single layer of the network, we did not seek to train the network to find filters optimised for the training set. Instead, for the size W\u00d7W twodimension filters, hi,c, we considered the following options:\n1) simple rotated bar and corner filters, and square uniform centre-surround filters; 2) filters trained on Imagenet and made available in Overfeat [25]; we used only the 96 stage-1 \u2018accurate\u2019 7\u00d77 filters; 3) patches obtained from the central W \u00d7 W region of randomly selected training images, with P/N training images from each class.\nThe filters from Overfeat1 are RGB filters. Hence, for the databases with RGB images, we applied each channel of the filter to the corresponding channel of each image. When applied to greyscale channels, we converted the Overfeat filter to greyscale. For NORB, we applied the same filter to both stereo channels. For all filters, we subtract the mean value over all W 2 dimensions in each channel, in order to ensure a mean of zero in each channel.\n1Available from http://cilvr.nyu.edu/doku.php?id=software:overfeat:start\nIn previous work, e.g. [26], the form of the Q \u00d7 Q twodimension filter, hp is a normalised Gaussian. Instead, we used a simple summing filter, equivalent to a kernel with all entires equal to the same value, i.e.\nhp,u,v = 1\nQ2 , u = 1, . . . Q, v = 1, . . . Q. (13)\nWhen applying the filters, we used the \u2018valid\u2019 convolutional region corresponding to hi,c and. For images of size J\u00d7J , the total dimension of the valid region is (J \u2212W +1)2. Thus, the total number of features per image obtained prior to pooling, from P filters, and images with C channels is L = CP (J \u2212 W + 1)2.\nThe remaining part of the pooling step is to downsample each image dimension by a factor of D, resulting in a total of L\u0302 = L/D2 features per image. In choosing D, we experimented with a variety of scales, but generally found D = Q \u2212 1 to be a good default. We note there exists an interesting tradeoff between the number of filters P , and the size of the pooling region, Q. For example, in [3], D = L/2, whereas in [5] D = 1. We found that to a point smaller D enables a smaller number of filters, P , for comparable performance.\nThe hyper-parameters we used for each dataset are shown in Table II."}, {"heading": "C. Stage 2 Design: Classifier projection weights", "text": "To construct the matrix Win we use the method proposed by [19]. In this method, each row of the matrix Win is chosen to be a normalized difference between the data vectors corresponding to randomly chosen examples from distinct classes of the training set. This method has previously been shown to be superior to setting the weights to values chosen from random distributions [15, 19].\nFor the nonlinearity in the classifier stage hidden units, g(z), the typical choice in other work [14] is a sigmoid. However, we found it sufficient (and much faster in an implementation) to use the quadratic nonlinearity. This suggests that good image classification is strongly dependent on the presence of interaction terms\u2014see the discussion about this in Section II-A."}, {"heading": "D. Stage 2 Design: Ridge Regression parameter", "text": "With these choices, there remains only two hyperparameters for the Classifier stage: the regression parameter, c, and the number of hidden-units, M . In our experiments, we examined classification error rates as a function of varying M . For each M , we can optimize c using cross-validation. However, we also found that a good generic heuristic for setting c was\nc = N2\nM2 min(diag(AtrainA\n> train)), (14)\nand this reduces the number of hyper-parameters for the classification stage to just one: the number of hidden-units, M ."}, {"heading": "E. Stage 1 and 2 Design: Nonlinearities", "text": "For the hidden-layer nonlinearities, to reiterate, we use:\ng1(u) = u 2, g2(v) = v 0.25, g(z) = z2. (15)"}, {"heading": "IV. RESULTS", "text": "We examined the performance of the four databases as a function of the number of filters, P , the downsampling rate D, and the number of hidden units in the classifier stage, M . We use the maximum number of channels, C available in each dataset (recall from above that we convert RGB images to greyscale, as a fourth channel).\nWe considered the three kinds of untuned filters described in Section III-B, as well as combinations of them. We did not exhaustively consider all options, but settled on the Overfeat filters as being marginally superior for NORB, SVHN and CIFAR-10 (in the order of 1% in comparison with other options), while hand-designed filters were superior for MNIST, but only marginally compared to randomly selected patches from the training data. There is clearly more that can be investigated to determine whether hand designed filters can match trained filters when using the method of this paper."}, {"heading": "A. Summary of best performance attained", "text": "The best performance we achieved is summarised in Table III."}, {"heading": "B. Trend with increasing M", "text": "We now use MNIST as an example to indicate how classification performance scales with the number of hidden units in the classifier stage, M . The remain parameters were W = 7, D = 3 and P = 43, which included hand-designed filters comprised from 20 rotated bars (width of one pixel), 20 rotated corners (dimension 4 pixels) and 3 centred squares (dimensions 3, 4 and 5 pixels), all with zero mean. The rotations were of binary filters and used standard pixel value interpolation. Figure 2 shows a power law-like decrease in error rate as M increases, with a linear trend on the log-log axes. The best error rate shown on this figure is 0.40%. As shown in Table III, we have attained a best repeatable rate of 0.37% using 60 filters and D = 2. When we combined Overfeat filters with hand designed filters and randomly selected patches from the training data, we obtained up to 0.32% error on MNIST, but this was an outlier since it was not repeatedly obtained by different samples of Win.\nC. Indicative training times\nFor an implementation in Matlab on a PC with 4 cores and 32 GB of RAM, for MNIST (60000 training points) the total time required to generate all features for all 60000 training images from one filter is approximately 2 seconds. This speed relies on using matrix multiplication on a single batch of training data to implement the convolutions. The largest number of filters we used to date was 384 (96 RGB+greyscale), and when applied to SVHN (\u223c600000 training points), the total run time for feature extraction is then about two hours.\nFor the classifier stage, on MNIST with M = 6400, the runtime is approximately 150 seconds for D = 3 (there is a small time penalty for smaller D, due to the larger dimension of the input to the classifier stage). Hence, the total run time for MNIST with 40 filters and M = 6400 is in the order of 4 minutes to achieve a correct classification rate above 99.5%. With fewer filters and smaller M , it is simple to achieve over 99.2% in a minute or less.\nFor SVHN and CIFAR-10 where we scaled up to M = 40000, the run time bottleneck is the classifier, due to the O(KM2) runtime complexity. We found it necessary to use a PC with more RAM (100 GB) for M > 20000. In the case of M = 40000, the network was trained in under an hour on CIFAR-10, while SVHN took about 8-9 hours. Results within a few % of our best, however, can be obtained in far less time."}, {"heading": "V. DISCUSSION AND CONCLUSIONS", "text": "As shown in Table III, our best result (0.37% error rate) surpasses the best ever reported performance for classification of the MNIST test set when no augmentation of the training set is done. We have also achieved, to our knowledge, the best performance reported in the literature for the NORB-small database, surpassing the previous best [30] by about 0.3%.\nFor SVHN, our best result is within \u223c 2% of state-of-theart. It is highly likely that using filters trained on the SVHN database rather than on Imagenet would reduce this gap, given the structured nature of digits, as opposed to the more complex\nnature of Imagenet images. Another avenue for closing the gap on state-of-the-art using the same filters would be to increase M and decrease D, thus resulting in more features and more classifier hidden units. Although we increased M to 40000, we did not observe saturation in the error rate as we increased M to this point.\nFor CIFAR-10, it is less clear what is lacking in our method in comparison with the gap of about 14% to state-of-the-art methods. We note that CIFAR-10 has relatively few training points, and we observed that the gap between classification performance on the actual training set, in comparison with the test set, can be up to 20%. This suggests that designing enhanced methods of regularisation (e.g. methods similar to dropout in the convolutional stage) are necessary to ensure our method can achieve good performance on CIFAR-10."}, {"heading": "ACKNOWLEDGMENT", "text": "Mark D. McDonnell\u2019s contribution was by supported by an Australian Research Fellowship from the Australian Research Council (project number DP1093425). We gratefully acknowledge Prof David Kearney and Dr Victor Stamatescu of University of South Australia for useful discussions and provision of computing resources."}], "references": [{"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, vol. 61, pp. 85\u2013117, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "Proc.14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, Fort Lauderdale, FL, USA. Volume 15 of JMLR:W&CP 15, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML), Bellevue, WA, USA, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Tiled convolutional neural networks", "author": ["Q.V. Le", "J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems 23, J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, Eds., 2010, p. 12791287.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Feed forward neural networks with random weights", "author": ["P.F. Schmidt", "M.A. Kraaijveld", "R.P.W. Duin"], "venue": "Proc. 11th IAPR Int. Conf. on Pattern Recognition, Volume II, Conf. B: Pattern Recognition Methodology and Systems (ICPR11, The Hague, Aug.30 - Sep.3), IEEE Computer Society Press, Los Alamitos, CA, 1992, 1-4, 1992.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems", "author": ["C. Eliasmith", "C.H. Anderson"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Extreme learning machine: A new learning scheme of feedforward neural networks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "In Proc. International Joint Conference on Neural Networks (IJCNN\u20192004), (Budapest, Hungary), July 25-29, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "The No-Prop algorithm: A new learning algorithm for multilayer neural networks", "author": ["B. Widrow", "A. Greenblatt", "Y. Kim", "D. Park"], "venue": "Neural Networks, vol. 37, pp. 182\u2013188, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A large-scale model of the functioning brain", "author": ["C. Eliasmith", "T.C. Stewart", "X. Choo", "T. Bekolay", "T. DeWolf", "C. Tang", "D. Rasmussen"], "venue": "Science, vol. 338, pp. 1202\u20131205, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale synthesis of functional spiking neural circuits", "author": ["T.C. Stewart", "C. Eliasmith"], "venue": "Proceedings of the IEEE, vol. 102, pp. 881\u2013 898, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Extreme learning machines and the neural engineering framework", "author": ["C. Eliasmith"], "venue": "2014, submitted, Under Review.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.-B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics\u2014Part B: Cybernetics, vol. 42, pp. 513\u2013529, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "An insight into extreme learning machines: Random neurons, random features and kernels", "author": ["G.-B. Huang"], "venue": "Cognitive Computation, vol. In Press, pp. DOI 10.1007/s12 559\u2013014\u20139255\u20132, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast, simple and accurate handwritten digit classification using extreme learning machines with shaped input-weights", "author": ["M.D. McDonnell", "M.D. Tissera", "A. van Schaik", "J. Tapson"], "venue": "2014, arXiv:1412.8307.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Online and adaptive pseudoinverse solutions for ELM weights", "author": ["A. van Schaik", "J.Tapson"], "venue": "Neurocomputing, vol. 149, pp. 233\u2013238, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Explicit computation of input weights in extreme learning machines", "author": ["J.Tapson", "P. de Chazal", "A. van Schaik"], "venue": "Proc. ELM2014 conference, Accepted, 2014, p. arXiv:1406.2889.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient and effective algorithms for training single-hidden-layer neural networks", "author": ["D. Yu", "L. Ding"], "venue": "Pattern Recognition Letters, vol. 33, pp. 554\u2013558, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Constrained extreme learning machine: a novel highly discriminative random feedforward neural network", "author": ["W. Zhu", "J. Miao", "L. Qing"], "venue": "Proc. IJCNN, 2014, p. XXX.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Constrained extreme learning machines: A study on classification cases", "author": ["\u2014\u2014"], "venue": "2015, arXiv:1501.06115.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J.C. Burges"], "venue": "Accessed August 2014, http://yann.lecun.com/exdb/mnist/.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Dept of CS, University of Toronto. See http://www.cs.toronto.edu/ kriz/cifar.html), 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "2011, nIPS Workshop on Deep Learning and Unsupervised Feature Learning. See http://ufldl.stanford.edu/housenumbers.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "Proceedings IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), vol. 2, 2004, pp. 97\u2013104.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "International Conference on Learning Representations (ICLR 2014). CBLS, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "International Conference on Pattern Recognition (ICPR), 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A deep learning pipeline for image understanding and acoustic modeling", "author": ["P. Sermanet"], "venue": "Ph.D. dissertation, Department of Computer Science, New York University., 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "Advances in Neural Information Processing Systems, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27, 2014, pp. 2627\u20132635.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeply-supervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "Deep Learning and Representation Learning Workshop, NIPS, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C.D. Cire\u015fan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, 2011, pp. 1237\u20131242.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003), 2003.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D. Cire\u015fan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural Computation, vol. 22, pp. 3207\u20133220, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Schmidhuber"], "venue": "Proc. CVPR, 2012, pp. 3642\u2013 3649.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", deep) neural networks [1].", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Such performance generally relies on a convolutional feature extraction stage to obtain invariance to translations, rotations and scale [2\u20135].", "startOffset": 136, "endOffset": 141}, {"referenceID": 2, "context": "Such performance generally relies on a convolutional feature extraction stage to obtain invariance to translations, rotations and scale [2\u20135].", "startOffset": 136, "endOffset": 141}, {"referenceID": 3, "context": "Such performance generally relies on a convolutional feature extraction stage to obtain invariance to translations, rotations and scale [2\u20135].", "startOffset": 136, "endOffset": 141}, {"referenceID": 4, "context": "Such performance generally relies on a convolutional feature extraction stage to obtain invariance to translations, rotations and scale [2\u20135].", "startOffset": 136, "endOffset": 141}, {"referenceID": 2, "context": "Recent work has shown that good performance on image classification tasks can be achieved in \u2018shallow\u2019 convolutional networks\u2014neural architectures containing a single training layer\u2014provided sufficiently many features are extracted [3].", "startOffset": 232, "endOffset": 235}, {"referenceID": 3, "context": "Perhaps surprisingly, such performance arises even with the use of entirely random convolutional filters or filters based on randomly selected patches from training images [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "relatively few filters and minimal downsampling) [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 5, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 79, "endOffset": 84}, {"referenceID": 6, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 79, "endOffset": 84}, {"referenceID": 7, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 79, "endOffset": 84}, {"referenceID": 8, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 79, "endOffset": 84}, {"referenceID": 9, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 143, "endOffset": 150}, {"referenceID": 10, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 143, "endOffset": 150}, {"referenceID": 11, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 143, "endOffset": 150}, {"referenceID": 12, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 143, "endOffset": 150}, {"referenceID": 13, "context": "The fast training method we use has been developed independently several times [6\u20139] and has gained increasing recognition in recent years\u2014see [10\u201314] for recent reviews.", "startOffset": 143, "endOffset": 150}, {"referenceID": 14, "context": "The rapid speed of training is due to the fact that the least squares optimisation problem an be solved using an O(KM) algorithm, where M is the number of hidden units and K the number of training points [15].", "startOffset": 204, "endOffset": 208}, {"referenceID": 14, "context": "When applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases [15\u201320] but poor performance on more difficult ones.", "startOffset": 154, "endOffset": 161}, {"referenceID": 15, "context": "When applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases [15\u201320] but poor performance on more difficult ones.", "startOffset": 154, "endOffset": 161}, {"referenceID": 16, "context": "When applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases [15\u201320] but poor performance on more difficult ones.", "startOffset": 154, "endOffset": 161}, {"referenceID": 17, "context": "When applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases [15\u201320] but poor performance on more difficult ones.", "startOffset": 154, "endOffset": 161}, {"referenceID": 18, "context": "When applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases [15\u201320] but poor performance on more difficult ones.", "startOffset": 154, "endOffset": 161}, {"referenceID": 19, "context": "When applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases [15\u201320] but poor performance on more difficult ones.", "startOffset": 154, "endOffset": 161}, {"referenceID": 20, "context": "We apply our network to several image classification databases, including MNIST [21], CIFAR-10 [22], Google Street View House Numbers (SVHN) [23] and NORB [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "We apply our network to several image classification databases, including MNIST [21], CIFAR-10 [22], Google Street View House Numbers (SVHN) [23] and NORB [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "We apply our network to several image classification databases, including MNIST [21], CIFAR-10 [22], Google Street View House Numbers (SVHN) [23] and NORB [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "We apply our network to several image classification databases, including MNIST [21], CIFAR-10 [22], Google Street View House Numbers (SVHN) [23] and NORB [24].", "startOffset": 155, "endOffset": 159}, {"referenceID": 2, "context": "The first stage is the convolutional feature extraction stage, and largely follows that of existing approaches to image classification [3\u20135, 26].", "startOffset": 135, "endOffset": 144}, {"referenceID": 3, "context": "The first stage is the convolutional feature extraction stage, and largely follows that of existing approaches to image classification [3\u20135, 26].", "startOffset": 135, "endOffset": 144}, {"referenceID": 4, "context": "The first stage is the convolutional feature extraction stage, and largely follows that of existing approaches to image classification [3\u20135, 26].", "startOffset": 135, "endOffset": 144}, {"referenceID": 25, "context": "The first stage is the convolutional feature extraction stage, and largely follows that of existing approaches to image classification [3\u20135, 26].", "startOffset": 135, "endOffset": 144}, {"referenceID": 14, "context": "The second stage is the classifier stage, and largely follows the approach of [15, 20].", "startOffset": 78, "endOffset": 86}, {"referenceID": 19, "context": "The second stage is the classifier stage, and largely follows the approach of [15, 20].", "startOffset": 78, "endOffset": 86}, {"referenceID": 25, "context": "We use a particular form for the nonlinear hidden-unit functions g1(\u00b7) and g2(\u00b7) inspired by LP-pooling [26], which is of the form g1(u) = u and g2(v) = v 1 p .", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "taken from Overfeat [25]; WPool describes standard average pooling and downsampling; and Win is set randomly or by using the method of [20] that specifies the weights by sampling examples of the training distribution, as described in the text.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "taken from Overfeat [25]; WPool describes standard average pooling and downsampling; and Win is set randomly or by using the method of [20] that specifies the weights by sampling examples of the training distribution, as described in the text.", "startOffset": 135, "endOffset": 139}, {"referenceID": 5, "context": "[6, 7, 14], where the input weights, Win are generated randomly from a specific distribution, e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 6, "context": "[6, 7, 14], where the input weights, Win are generated randomly from a specific distribution, e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 13, "context": "[6, 7, 14], where the input weights, Win are generated randomly from a specific distribution, e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 14, "context": "However, it is known that setting these weights non-randomly based on the training data leads to superior performance [15, 17, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 16, "context": "However, it is known that setting these weights non-randomly based on the training data leads to superior performance [15, 17, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 19, "context": "However, it is known that setting these weights non-randomly based on the training data leads to superior performance [15, 17, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 17, "context": "These weights can also be trained, if desired, using single-batch backpropagation [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "It is known to often be useful to regularise such problems, and instead solve the following ridge regression problem [13, 14]:", "startOffset": 117, "endOffset": 125}, {"referenceID": 13, "context": "It is known to often be useful to regularise such problems, and instead solve the following ridge regression problem [13, 14]:", "startOffset": 117, "endOffset": 125}, {"referenceID": 14, "context": "In practice, it is efficient to avoid explicit calculation of the inverse in Equation (6) [15] and instead use QR factorisation to solve the following set of NM linear equations for the NM unknown variables in Wout:", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "This approach was shown to be effective for SVHN in [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Database Classes Training Test Channels Pixels Ref MNIST 10 60000 10000 1 28\u00d728 [21] NORB-small 5 24300 24300 2 (stereo) 32\u00d732 [24] SVHN 10 604308 26032 3 (RGB) 32\u00d732 [23] CIFAR-10 10 50000 10000 3 (RGB) 32\u00d732 [22]", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "Database Classes Training Test Channels Pixels Ref MNIST 10 60000 10000 1 28\u00d728 [21] NORB-small 5 24300 24300 2 (stereo) 32\u00d732 [24] SVHN 10 604308 26032 3 (RGB) 32\u00d732 [23] CIFAR-10 10 50000 10000 3 (RGB) 32\u00d732 [22]", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "Database Classes Training Test Channels Pixels Ref MNIST 10 60000 10000 1 28\u00d728 [21] NORB-small 5 24300 24300 2 (stereo) 32\u00d732 [24] SVHN 10 604308 26032 3 (RGB) 32\u00d732 [23] CIFAR-10 10 50000 10000 3 (RGB) 32\u00d732 [22]", "startOffset": 167, "endOffset": 171}, {"referenceID": 21, "context": "Database Classes Training Test Channels Pixels Ref MNIST 10 60000 10000 1 28\u00d728 [21] NORB-small 5 24300 24300 2 (stereo) 32\u00d732 [24] SVHN 10 604308 26032 3 (RGB) 32\u00d732 [23] CIFAR-10 10 50000 10000 3 (RGB) 32\u00d732 [22]", "startOffset": 210, "endOffset": 214}, {"referenceID": 0, "context": "All raw image pixel values were converted to the interval [0, 1].", "startOffset": 58, "endOffset": 64}, {"referenceID": 2, "context": "We found that local and/or global contrast enhancement only diminished performance; 4) CIFAR-10: convert from 3 channels to 4 by adding a conversion to greyscale from the raw RGB; apply ZCA whitening to each channel of each image, as in [3].", "startOffset": 237, "endOffset": 240}, {"referenceID": 24, "context": "1) simple rotated bar and corner filters, and square uniform centre-surround filters; 2) filters trained on Imagenet and made available in Overfeat [25]; we used only the 96 stage-1 \u2018accurate\u2019 7\u00d77 filters; 3) patches obtained from the central W \u00d7 W region of randomly selected training images, with P/N training images from each class.", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "[26], the form of the Q \u00d7 Q twodimension filter, hp is a normalised Gaussian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "For example, in [3], D = L/2, whereas in [5] D = 1.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "For example, in [3], D = L/2, whereas in [5] D = 1.", "startOffset": 41, "endOffset": 44}, {"referenceID": 18, "context": "To construct the matrix Win we use the method proposed by [19].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "This method has previously been shown to be superior to setting the weights to values chosen from random distributions [15, 19].", "startOffset": 119, "endOffset": 127}, {"referenceID": 18, "context": "This method has previously been shown to be superior to setting the weights to values chosen from random distributions [15, 19].", "startOffset": 119, "endOffset": 127}, {"referenceID": 13, "context": "For the nonlinearity in the classifier stage hidden units, g(z), the typical choice in other work [14] is a sigmoid.", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "39% [28, 29] NORB-small 2 3200 60 2.", "startOffset": 4, "endOffset": 12}, {"referenceID": 28, "context": "39% [28, 29] NORB-small 2 3200 60 2.", "startOffset": 4, "endOffset": 12}, {"referenceID": 29, "context": "53% [30] SVHN 4 40000 96 3.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "92% [29] CIFAR-10 4 40000 96 24.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "78% [29]", "startOffset": 4, "endOffset": 8}, {"referenceID": 30, "context": "The state-of-the-art result listed for MNIST and CIFAR-10 can be improved by augmenting the training set with distortions and other methods [31\u201333]; we have not done so here, and report state-of-the-art only for methods not doing so.", "startOffset": 140, "endOffset": 147}, {"referenceID": 31, "context": "The state-of-the-art result listed for MNIST and CIFAR-10 can be improved by augmenting the training set with distortions and other methods [31\u201333]; we have not done so here, and report state-of-the-art only for methods not doing so.", "startOffset": 140, "endOffset": 147}, {"referenceID": 32, "context": "The state-of-the-art result listed for MNIST and CIFAR-10 can be improved by augmenting the training set with distortions and other methods [31\u201333]; we have not done so here, and report state-of-the-art only for methods not doing so.", "startOffset": 140, "endOffset": 147}, {"referenceID": 29, "context": "We have also achieved, to our knowledge, the best performance reported in the literature for the NORB-small database, surpassing the previous best [30] by about 0.", "startOffset": 147, "endOffset": 151}], "year": 2017, "abstractText": "We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and very few tunable parameters, the method has strong potential for embedded hardware applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method as an image classifier, obtaining state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2%) image classification databases, with very fast training times compared to standard deep network approaches. The network\u2019s performance on the Google Street View House Number (SVHN) (4%) database is also competitive with state-ofthe art methods.", "creator": "TeX"}}}