{"id": "1503.06666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "Using Generic Summarization to Improve Music Information Retrieval Tasks", "abstract": "many music information retrieval ( mir ) tasks consider only a segment of particular whole music product, in order to maximize processing time constraints. this practice may lead to misleading performance since the most important information for the task may not be in those discrete segments. in this paper, models leverage generic summarization algorithms, alternately applied as piano and speech summarization, to summarize items in music datasets. these algorithms build summaries, that are both concise for diverse, by selecting appropriate segments from the input signal which makes our feasible candidates to differentiate music as well. we evaluate the summarization process on sequential and multiclass music genre classification tasks, by comparing the performance extracted using summarized recordings against the performances obtained using continuous segments ( which is the traditional method convenient for addressing the previously mentioned time constraints ) and full songs presenting which same original message. we show that grasshopper, lexrank, lsa, mmr, and a support sets - based centrality model improve classification performance when compared to selected 30 - state baselines. we also show that these summarized datasets lead to a classification performance whose difference be not statistically significant when using full songs. furthermore, we make an argument stating the advantages of sharing classified datasets for future mir research.", "histories": [["v1", "Mon, 23 Mar 2015 14:48:24 GMT  (40kb,D)", "http://arxiv.org/abs/1503.06666v1", "24 pages, 1 figure; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"], ["v2", "Thu, 3 Dec 2015 18:38:22 GMT  (346kb)", "http://arxiv.org/abs/1503.06666v2", "24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"], ["v3", "Wed, 9 Mar 2016 16:24:42 GMT  (3170kb,D)", "http://arxiv.org/abs/1503.06666v3", "24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"]], "COMMENTS": "24 pages, 1 figure; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.SD", "authors": ["francisco raposo", "ricardo ribeiro", "david martins de matos"], "accepted": false, "id": "1503.06666"}, "pdf": {"name": "1503.06666.pdf", "metadata": {"source": "CRF", "title": "Using Generic Summarization to improve Music Information Retrieval Tasks", "authors": ["Francisco Raposo", "Ricardo Ribeiro", "David Martins de Matos"], "emails": [], "sections": [{"heading": null, "text": "Many Music Information Retrieval (MIR) tasks process only a segment of the whole music signal, in order to satisfy processing time constraints. This practice may lead to decreasing performance since the most important information for the task may not be in those processed segments. In this paper, we leverage generic summarization algorithms, previously applied to text and speech summarization, to summarize items in music datasets. These algorithms build summaries, that are both concise and diverse, by selecting appropriate segments from the input signal which makes them good candidates to summarize music as well. We evaluate the summarization process on binary and multiclass music genre classification tasks, by comparing the performance obtained using summarized datasets against the performances obtained using continuous segments (which is the traditional method used for addressing the previously mentioned time constraints) and full songs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA, MMR, and a Support Sets-based Centrality model improve classification performance when compared to selected 30-second baselines. We also show that these summarized datasets lead to a classification performance whose difference is not statistically significant from using full songs. Furthermore, we make an argument stating the advantages of sharing summarized datasets for future MIR research.\nI. INTRODUCTION\nMusic summarization has been the subject of research for at least a decade and many algorithms that address this problem, mainly for popular music, have been published in the past [1]\u2013[8]. However,\nFrancisco Raposo and David Martins de Matos are with Instituto Superior Te\u0301cnico, Universidade de Lisboa, Av. Rovisco Pais,\n1049-001 Lisboa, Portugal.\nRicardo Ribeiro is with Instituto Universita\u0301rio de Lisboa (ISCTE-IUL), Av. das Forc\u0327as Armadas, 1649-026 Lisboa, Portugal. Francisco Raposo, Ricardo Ribeiro, and David Martins de Matos are with L2F - INESC ID Lisboa, Rua Alves Redol, 9,\n1000-029 Lisboa, Portugal.\nThis work was supported by national funds through Fundac\u0327a\u0303o para a Cie\u0302ncia e a Tecnologia (FCT) with reference\nUID/CEC/50021/2013.\nMarch 24, 2015 DRAFT\nar X\niv :1\n50 3.\n06 66\n6v 1\n[ cs\n.I R\n] 2\n3 M\nar 2\n01 5\nthose algorithms focus on producing human consumption-oriented summaries, i.e., summaries that will be listened to by people motivated by the need to quickly get the gist of the whole song without having to listen to all of it. This type of summary usage entails extra requirements on the summaries besides conciseness and diversity (non-redundancy), such as clarity and coherence, so that people can enjoy listening to them.\nGeneric summarization algorithms, however, focus on extracting concise and diverse summaries and have been successfully applied in text and speech summarization [9]\u2013[13]. Their application, in music, for human consumption-oriented purposes is not ideal, for they will select and concatenate the most relevant and diverse information (according to each algorithm\u2019s definition of relevance and diversity) without taking into account whether the output is enjoyable for people or not. This is usually reflected, for instance, on the discontinuities or irregularities in beat synchronization present in the resulting summaries.\nWe focus on improving the performance of tasks recognized as important by the MIR community, such as Music Genre Classification, through summarization, as opposed to considering music summarization as the final product to be consumed by people. Thus, we can ignore some of the requirements taken into consideration by previous music summarization efforts, which are usually addressed by trying to model the musical structure of the pieces being summarized, possibly using musical knowledge. Although human-related aspects of music summarization are important in general, they are beyond the focus of this paper. We claim that, for those MIR tasks benefiting from summaries, it is sufficient to consider the most relevant parts of the signal, according to its features. In particular, the summarizers do not need to take into account the structure of the songs or human perception of music. Our rationale is that a summary clip contains more relevant and less redundant information and, thus, improves the performance of tasks that rely on processing just a portion of the whole signal. The motivation for processing just a small portion of the signal can be: faster/less processing, less disk space usage (by the datasets), and bandwidth constraints.\nWe summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13]. These algorithms summarize music for automatic (instead of human) consumption. To evaluate the effects of summarization, we consider the performance of binary and 5-class Music Genre Classification tasks, when taking as input song summaries against continuous clips (extracted from the beginning, middle, and end of the songs) and also against using the whole songs. We present results showing that all of these algorithms improve classification performance and are statistically not significantly different from using the whole songs. These results complement and solidify previous work evaluated on a binary Fado classifier [14].\nMarch 24, 2015 DRAFT\nThe rest of this document is organized as follows: section II reviews related work on music-specific summarization. Section III reviews the generic summarization algorithms we experimented with. Specifically, the following algorithms are reviewed: GRASSHOPPER in section III-A, LexRank in section III-B, LSA in section III-C, MMR in section III-D and Support Sets-based Centrality in section III-E. Section IV describes the details of the experiments we performed for each algorithm and introduces the classifier. Sections V and VI report our classification results for the Binary and Multiclass classification scenarios, respectively. Section VII discusses the results and Section VIII concludes this paper with some remarks and future work."}, {"heading": "II. MUSIC SUMMARIZATION", "text": "Several algorithms for both generic and music summarization have been proposed. However, music summarization algorithms were developed to extract an enjoyable summary so that any person can listen to it clearly and coherently. Our focus is on automatic consumption (by algorithms), so clarity and coherence are not mandatory requirements for our summaries. Hence, we briefly review some of those algorithms here and then explain the generic algorithms we experimented with more thoroughly in the next section.\nSome music-specific summarization methods start by structurally segmenting songs and then selecting meaningful segments to include in the summary. The assumption is that the song has a structure represented as a label sequence where each label represents a different part of the song (e.g., ABABCA where A is the chorus, B the verse and C the bridge). In [1], segmentation is achieved by using a Hidden Markov Model (HMM) to detect key changes between frames and Dynamic Time Warping (DTW) to detect repeating structure. In [2], a Gaussian-tempered \u201ccheckerboard\u201d kernel is correlated along the main diagonal of the song\u2019s self-similarity matrix, outputting segment boundaries. Then, a segment-indexed similarity matrix is built, containing the similarity between detected segments. Singular Value Decomposition (SVD) is applied to that matrix to find its rank-K approximation. Segments are, then, clustered to output the song\u2019s structure. In [3] and [4], songs are segmented in 3 stages. First, a similarity matrix is built and analyzed for fast changes, outputting segment boundaries. These segments are clustered to output the \u201cmiddle states\u201d. Finally, an HMM is applied to these states, producing the final segmentation. These algorithms then follow various strategies to select the appropriate segments.\nIn [5], clustering is used to group and label similar segments of the song. The summary is extracted by taking the longest sequence of segments belonging to the same cluster. The distortion measure used for clustering is a modification (ensuring symmetry) of the Kullback-Leibler (KL) divergence. In [6]\nMarch 24, 2015 DRAFT\nand [7], Average Similarity is used to extract a thumbnail L seconds long that is the most similar to the whole piece. It starts by calculating a similarity matrix through computing frame-wise similarities. Then, it calculates an aggregated similarity measure, for each possible starting frame, of the L-second segment with the whole song and picks the one that maximizes it as the summary. Another method for this task, Maximum Filtered Correlation [8], starts by building a similarity matrix and then a filtered time-lag matrix, embedding the similarity between extended segments separated by a constant lag. The starting frame of the summary corresponds to the index that maximizes the filtered time-lag matrix. In [15], music is classified as pure or vocal, in order to perform type-specific feature extraction. The summary, created from three to five seconds subsummaries (built from frame clusters), takes into account musicological and psychological aspects, since it differentiates between types of music based on feature selection and specific duration. This promotes human enjoyment when listening to the summary. Since these summaries were targeted to people, naturally, they were evaluated by people."}, {"heading": "III. GENERIC SUMMARIZATION", "text": "Applying generic summarization algorithms to music implies song segmentation into musical words/terms\nand sentences. Since we do not take into account human-related aspects of music perception, we can segment songs (into sentences) according to an arbitrarily fixed size. Fixed segmentation differs from structural segmentation in that it does not take into account human perception of musical structure and does not create meaningful segments (to people). It consists of selecting a sentence size and segmenting the whole signal into sentences of that same specific size. Nevertheless, it allows us to look at the variability and repetition of the signal and use them to find its most important parts. Furthermore, since it is not aimed at human consumption, the generated summaries are less liable to violate the copyrights of the original songs. This facilitates the sharing of datasets (using the signal itself, instead of specific features extracted from it) for MIR research efforts. In the following sections, we review the generic summarization algorithms we evaluated."}, {"heading": "A. GRASSHOPPER", "text": "The Graph Random-walk with Absorbing StateS that HOPs among PEaks for Ranking (GRASSHOPPER) [12] is a centrality-based method. It was applied to text summarization and social network analysis, focusing on improving diversity in ranking. It takes as input an n \u00d7 n weight matrix W representing a graph where each sentence is a vertex and each edge has weight wij corresponding to the similarity\nMarch 24, 2015 DRAFT\nbetween sentences i and j; a probability distribution r encoding prior ranking (the algorithm works as a re-ranking method); and a weight \u03bb that balances between the two.\nFirst, the matrix W is row-normalized: Oij = wij/ \u2211n k=1wik. Then, a matrix P is built, incorporating\nthe user-supplied prior ranking r:\nP = \u03bbO + (1\u2212 \u03bb)1rT (1)\n1 is an all-1 vector and 1rT is the outer product. The first ranking sentence/state g1 is found by taking the state with largest stationary probability: g1 = argmaxni=1 \u03c0i according to the stationary distribution of P : \u03c0 = P T\u03c0. Each time a state is extracted, it is converted into an absorbing state so that it penalizes every other state too similar to it. However, the rest of the sentences are iteratively selected according to the expected number of visits to each state instead of considering the stationary probability as for the first item. If G is the set of items ranked so far, states are turned into absorbing states by setting Pgg = 1 and Pgi = 0, \u2200i 6= g. If items are arranged so that ranked ones are listed before unranked ones, P can be written as follows:\nP = IG 0 R Q  (2) IG is the identity matrix on G and submatrices R and Q correspond to rows of unranked items. The fundamental matrix N = (I \u2212Q)\u22121 gives the expected number of visits. Nij is the expected number of visits to state j starting from state i, so the average over all starting nodes is taken to obtain the expected number of visits to state j, vj :\nv = NT1\nn\u2212 |G| (3)\n|G| is the size of G. The next item g|G|+1 is selected by taking the state with largest expected number of visits: g|G|+1 = argmaxni=|G|+1 vi."}, {"heading": "B. LexRank", "text": "LexRank [10] is another centrality-based method relying on the similarity (usually, the cosine) between sentence pairs (usually, represented as tf-idf vectors). The summary is produced by taking the most central sentences from a centrality-ranked list.\nFirst, all the sentences are compared to each other. Then, a graph is built where each sentence is a vertex and edges are created between every sentence according to their pairwise similarity (usually, the similarity score must be higher than some threshold for an edge to be created). LexRank can be used\nMarch 24, 2015 DRAFT\nwith both weighted (eq. 4) and unweighted (eq. 6) edges. Then, each sentence/vertex score is iteratively computed (until convergence) according to the following equations:\nS (Vi) = (1\u2212 d) N + S1 (Vi) (4)\nS1 (Vi) = d\u00d7 \u2211\nVj\u2208adj[Vi]\nSim (Vi, Vj)\u2211 Vk\u2208adj[Vj ] Sim (Vj , Vk) S (Vj) (5)\nS (Vi) = (1\u2212 d) N\n+ d\u00d7 \u2211\nVj\u2208adj[Vi]\nS (Vj) D (Vj) (6)\nwhere d is a damping factor to guarantee the convergence of the method; N is the total number of vertices and S (Vi) is the score of vertex i. D (Vi) is the degree (i.e., number of edges) of vertex i. A summary can be constructed by taking the highest ranked sentences until a certain summary length is reached.\nThis method can be viewed as sentences recommending each other. A sentence very similar to many other sentences will get a high score. Thus, sentence score is also determined by the score of the sentences recommending it."}, {"heading": "C. Latent Semantic Analysis (LSA)", "text": "LSA was first applied in text summarization in [16]. LSA uses the SVD to reduce the dimensionality of an original matrix representation of the text. To perform LSA-based summarization, we start by building a T terms by N sentences matrix A. Each element of A, aij = LijGi, has a local (Lij) and a global (Gi) weight. Lij is a function of the number of times a term occurs in a specific sentence and Gi is a function of the number of sentences that contain a specific term. This usually translates into a matrix composed of tf-idf vectors, each representing a sentence. The result of applying the SVD to A is A = U\u03a3V T , where U (a T \u00d7 N matrix) contains the left singular vectors; \u03a3 (a N \u00d7 N diagonal matrix) contains the singular values; and V T (a N \u00d7 N matrix) contains the right singular vectors. Singular values are sorted by descending order in matrix \u03a3 and are used to determine topic relevance. Each latent dimension corresponds to a topic. We calculate the rank-K approximation by taking the first K columns of U , the K \u00d7K sub-matrix of \u03a3 and the first K rows of V T . The most relevant sentences can be extracted by selecting the ones corresponding to the indices of the highest values for each (most relevant) right singular vector.\nIn [17], two limitations of this approach are discussed: if selecting K equal to the number of sentences in the summary, when it increases, the summary tends to include less significant sentences; and that sentences with high values in several dimensions (topics), but never the highest, will never be included\nMarch 24, 2015 DRAFT\nin the summary. To compensate for these problems, a sentence score was introduced and K is chosen so that the Kth singular value does not fall under half of the highest singular value:\nscore (j) = \u221a\u221a\u221a\u221a k\u2211 i=1 v2ij\u03c3 2 i (7)"}, {"heading": "D. Maximal Marginal Relevance (MMR)", "text": "Sentence selection in MMR [9] is done according to their relevance and diversity against previously selected sentences, in order to output low-redundancy summaries. MMR is a query-based method that has been used in speech summarization [18], [19]. It is also possible to produce generic summaries by taking the centroid vector of all the sentences as the query.\nMMR iteratively selects sentences that maximize the following mathematical model:\n\u03bbSim1 (Si, Q)\u2212 (1\u2212 \u03bb) max Sj Sim2 (Si, Sj) (8)\nSim1 and Sim2 are, possibly different, similarity metrics; Si are unselected sentences and Sj are previously selected ones; Q is the query and \u03bb is a configurable parameter that allows the selection of the next sentence to be based on relevance, diversity, or a linear combination of both. Usually, sentences are represented as tf-idf vectors, and the cosine similarity is used for both Sim1 and Sim2."}, {"heading": "E. Support Sets-based Centrality", "text": "This centrality-based method was introduced in [13] and was successfully applied in text and speech summarization. Here, sentence centrality is defined through what was called a Support Set. A sentence support set is the set of sentences that are most similar to that sentence (above a certain threshold):\nSi = {s \u2208 I : Sim (s, pi) > i \u2227 s 6= pi} (9)\nThis method calculates the support sets for every sentence and then selects the sentences that are contained in the most support sets (i.e. the ones that are recommended the most):\nargmaxs\u2208\u222ani=1Si |{Si : s \u2208 Si}| (10)\nThis is similar to the (previously described in section III-B) unweighted LexRank (or degree centrality model). In degree centrality, sentences recommend each other based on a globally defined similarity threshold. Support sets, however, allow a different threshold for each sentence ( i). From a graph theory perspective, this means that the underlying representation is directed, as opposed to both LexRanks, meaning that the sentences in each support set are recommended by the corresponding sentence associated\nMarch 24, 2015 DRAFT\nto that set. This changes centrality because each sentence will only recommend the most semantically related sentences to itself. The thresholds can be determined, for example, using a kNN approach. In [13], heuristics are explored to define the support sets, specifically, a passage order heuristic which clusters all passages into two clusters, according to their distance to each cluster\u2019s centroid. The first and second clusters are initialized with the first and second passages, respectively, and sentences are assigned to clusters, one by one, according to their appearance order in the original document (hence the passage order heuristic name). The cluster that contains the most similar passage to the passage associated with the support set under construction (the one recommending the support set) is selected as the support set. Several metrics were tested for defining semantic relatedness, namely, specific instantiations of the Minkowski distance as well as the cosine similarity."}, {"heading": "IV. EXPERIMENTS", "text": "To evaluate these algorithms on music, we tested their impact on binary and multiclass music genre classification tasks. Music classification consists of classifying songs based on a classification scheme (e.g., guessing a song\u2019s artist, genre, or mood). This meta-task is deemed important by the MIR community and annual conferences addressing this task are held such as International Society for Music Information Retrieval (ISMIR), which comprises Music Information Retrieval Evaluation eXchange (MIREX) [20] for comparing state-of-the-art algorithms performance in a standardized setting.\nClassification is performed using Support Vector Machines (SVMs) [21]. Note that there are two different feature extraction steps. The first is done by the summarizers, every time a song is summarized. The summarizers\u2019 output consists of the audio signal corresponding to the selected parts, which will be further processed for features to be used in classification. The second step is when doing classification, where features are extracted from the full, segmented, and summarized datasets."}, {"heading": "A. Classification Features", "text": "The features used by the SVM classifier consist of a 38-dimensional vector per song, which is a concatenation of several statistics on features used in [22]. The average of the first 20 Mel Frequency Cepstral Coefficients (MFCCs) concatenated with statistics (average and variance) on these spectral features describe the timbral texture of the song: centroid, spread, skewness, kurtosis, flux, rolloff, brightness, and entropy. These statistics are computed over all feature vectors extracted on 50ms frames with no overlap. This set of features and a smaller set solely composed of MFCC averages were tested in the classification task. All musical genres in our dataset are timbrically different from each other, making\nMarch 24, 2015 DRAFT\nthese feature sets good candidates for classification. These features were extracted using OpenSMILE [23]."}, {"heading": "B. Datasets", "text": "The datasets used in our experiments consist of a total of 1250 songs from 5 different genres: Bass, Fado, Hip hop, Trance, and Indie Rock. Bass music is a generic term referring to several specific styles of electronic music, such as Dubstep, Drum and Bass, Electro, and more. Although these differ in tempo, they share similar timbral characteristics such as deep basslines and the \u201cwobble\u201d bass effect. Fado is a Portuguese music genre whose instrumentation consists solely of stringed instruments, such as the classical guitar and the Portuguese guitar. Hip hop consists of drum rhythms (usually built with samples), the use of turntables and spoken lyrics on top of it. Indie Rock usually consists of guitar, drums, keyboard, and vocal sounds and was influenced by punk, psychedelia, post-punk, and country. Trance is an electronic music genre characterized by repeating melodic phrases and a musical form that builds up and down throughout a track. Each class is represented by 250 songs from several artists. The multiclass dataset contains all songs. Two binary datasets were also built from this data, in order to test our hypothesis on a wider range of classification setups: Bass vs. Fado and Bass vs. Trance, each containing the 500 corresponding songs."}, {"heading": "C. Setup", "text": "10-fold cross-validation was used in all classification tasks. First, as baselines, we performed 3 classification experiments using 30s segments, from the beginning, middle, and end of each song. Then, we obtained another baseline by using the whole songs. The baselines were compared with the classification results from using 30s summaries for each parameter combination and algorithm. We did this for both binary datasets and then for the multiclass dataset, yielding a total of 3 dataset scenarios.\nEvery algorithm was implemented in C++ using the following libraries: OpenSMILE [23] for feature extraction (in summarization), Armadillo [24] for matrix operations, and Marsyas [25] for synthesizing the summaries.\nApplying generic summarization algorithms to music requires additional steps. Since these algorithms operate on the discrete concepts of word and sentence, some preprocessing must be done to map the continuous frame representation obtained after feature extraction to a word/sentence representation. For each song being summarized, a vocabulary is created, through clustering the frames\u2019 feature vectors. mlpack\u2019s [26] implementation of the K-Means algorithm was used for this step (we experiment with\nMarch 24, 2015 DRAFT\nsome values for K and assess their impact on the results). After clustering, a vocabulary of musical words is obtained (each word is a frame cluster\u2019s centroid). From then on, each frame is assigned its own cluster centroid, effectively mapping the frame feature vectors to vocabulary words. This mapping changes the real/continuous nature of each frame (when represented by a feature vector) to a discrete nature (when represented as a word from a vocabulary). Then, the song is segmented into fixed-size sentences (e.g., 5-word sentences). Since every sentence contains discrete words from a vocabulary, it is possible to represent each as a vector of word occurrences/frequencies (depending on the type of chosen weighting scheme) which is the exact representation used by generic summarization algorithms. Sentences were compared using the cosine distance. The parameters of all of these algorithms include: features, framing, vocabulary size (final number of clusters of the K-Means algorithm), weighting (e.g., tf-idf ), and sentence size (number of words per sentence).\nOur experiments covered the following parameter values (varying between algorithms): frame and hop sizes combinations of (0.25,0.125), (0.25,0.25), (0.5,0.25), (0.5,0.5), (1,0.5) and (1,1) (in seconds); vocabulary sizes of 25, 50, and 100 (words); sentence sizes of 5, 10, and 20 (words); \u201cdampened\u201d tf-idf (takes logarithm of tf instead of tf itself) and binary weighting schemes. As summarization features, we used MFCC vectors of sizes 12, 20, and 24. These features, used in several previous research efforts on music summarization in [1]\u2013[7], describe the timbre of an acoustic signal. We also used a concatenation of MFCC vectors with the 9 spectral features enumerated in section IV-A. For MMR, we tried \u03bb values of 0.5 and 0.7. Our LSA implementation also makes use of the sentence score and the topics cardinality selection heuristic described in section III-C."}, {"heading": "V. RESULTS: BINARY", "text": "First, we analyze results on the binary datasets, Bass vs. Fado and Bass vs. Trance. The reason we chose these genre pairs was because we wanted to see summarization\u2019s impact on an easy to classify dataset (Bass vs. Fado, since these genres are timbrically very different) and a more difficult one (Bass vs. Trance, since these genres share many timbrical similarities due to their electronic and dancefloororiented nature). Note that, for all experiments, classifying using the full 38-dimensional features vector produced better results than using only the 20 MFCCs, so we only present those results here. The best results are summarized in tables I, II, III, and IV.\nThe first thing we notice on the Bass vs. Fado classification task is that the middle sections are the best continuous sections and they do a good job at distinguishing Fado from other genres. Accuracy only dropped 2 percentage points (pp) against using full songs. However, the beginning sections\u2019 accuracy\nMarch 24, 2015 DRAFT\ndropped by 5.8 pp. All summarization algorithms fully recovered the accuracy lost by any continuous sections against using full songs, achieving the 100% full songs baseline. Summarization is, in this case, helping classification in an already easy classification task. The \u03bb value in MMR\u2019s setup was 0.7 and the passage order heuristic using the cosine similarity was used for calculating the support sets.\nIn the Bass vs. Trance task, the middle sections do a very poor job at describing and distinguishing these genres \u2013 they actually perform worse than the beginning or end sections. Actually, the worst sections in the Bass vs. Fado task were the best in this one and vice versa. This means that choosing a continuous\nMarch 24, 2015 DRAFT\nsegment to extract features for classification purposes cannot be assumed to work equally well for every genre and dataset. All summarization algorithms, while not reaching the same performance as when using full songs, succeeded in improving classification performance against the continuous 30-second baselines. In this case, summarization is helping classification in a more difficult classification task. As in the first experiment, the \u03bb value in MMR\u2019s setup was 0.7 and the passage order heuristic using the cosine similarity was used for calculating the support sets."}, {"heading": "VI. RESULTS: MULTICLASS", "text": "Since we are extrinsically evaluating summarization, analyzing the impact of these algorithms on music classification must go beyond simply comparing final classification accuracy values for each scenario (as was done for the binary classification tasks). Here, we also look at the confusion matrices obtained from the classification scenarios, so that we can carefully look at the data (in this case, listen to the data) to understand what is happening when summarizing music this way and why it is improving the classification task\u2019s performance. Since our dataset consists of 250 songs per class, each confusion matrix row must sum to 250. Classes are identically sorted both in rows and columns, which means the ideal case is where we have a diagonal confusion matrix (all zeros, except for the diagonal elements, which should all be 250). Class name initials are shown to the left of the matrix and individual class accuracies are shown to the right."}, {"heading": "A. Full songs", "text": "First, we look at the confusion matrix resulting from classifying full songs (table V). We can see that Fado, although there is some confusion between it and Indie Rock, is the most distinguishable genre within this group of genres which makes sense since timbrically it is very different from every other genre\nMarch 24, 2015 DRAFT\npresent in the dataset. Trance and Bass also achieve accuracies over 90%, although they also share some confusion which is explained by the fact that they both are Electronic music styles, thus sharing many timbral characteristics derived from the virtual instruments used to produce them. The classifier performs worse when classifying Hip hop and Indie Rock, achieving accuracies around 84% and confusing both genres in approximately 10% of the tracks. This can also be explained by the fact that both of those genres have strong vocals presence (in contrast with Bass and Trance). Although Fado also has an important vocal component, its instrumentation is very different from Hip hop and Indie Rock explaining why Fado did not get confused as much as they were with each other. The overall accuracy of this classification scenario is 89.84%.\nWe can think of these accuracies as how well these classification features (and SVM) can perform on these genres, given all the possible information about the tracks. Intuitively, removing information by, for instance, only extracting features from the beginning 30 seconds of the songs, will worsen the performance of the classifier because it will have incomplete data about each song, and thus, also incomplete data for modeling each class. Tables VI, VIII, and X show that to be true when using such a blind approach to summarize music (since extracting 30-second continuous segments can also be interpreted as a naive summarization method/heuristic). This process of only extracting features from a dataset of segments is what is usually done when classifying music, since processing 30 seconds instead of the whole song saves processing time."}, {"heading": "B. Baseline segments", "text": "Table VI shows the results obtained when classifying using only the 30 seconds from the beginning of the songs. Table VII shows the comparison of the beginning sections against full songs. The classification accuracy is 77.52%, which means a 12.32 pp drop when compared to using full songs. Bass accuracy\nMarch 24, 2015 DRAFT\ndropped 19.6 pp, due to increased confusion with both Hip hop and Indie Rock. Trance was also more confused with Indie Rock. This is easily explained by the fact that the first 30 seconds of most Bass or Trance songs correspond to the intro part. These intros are lower energy parts which may contain a relatively strong vocal presence and much fewer instrumentation than other more characteristic parts of the genres. These intros are much more similar to Hip hop and Indie Rock intros than when considering the whole songs which explains why the classifier is confusing these classes more in this scenario. This means that taking the beginning of the songs for classification is, in general, not a good segment selection strategy."}, {"heading": "B -49 6 27 28 -12 -19.6%", "text": "Tables VIII and IX show the results obtained when classifying using the middle 30 seconds of the songs and the comparison of those segments against using full songs, respectively. The overall accuracy was 81.36%, which means an 8.48 pp drop against the full songs baseline. This time, both Bass and Trance accuracies dropped 16.8 pp and 20.4 pp, respectively, getting confused with each other by the classifier. Having listened to the tracks that got confused this way, the conclusion is as expected: these middle segments correspond to what is called a breakdown section of the songs. These sections correspond to lower energy segments (though not as low as an intro) of the tracks which, again, are not the most\nMarch 24, 2015 DRAFT\ncharacteristic parts of both these genres and, in the particular case of Bass vs. Trance, they are timbrically very similar due to their Electronic nature. A human listener would, probably, also be unable to distinguish between these two genres if listening only to these segments. Although, for 3 of the 5 genres, classification performance did not drop pronouncedly, it did so for 2 of them, which means that, in general, taking the middle sections of the songs for classification is also not a good segment selection strategy."}, {"heading": "B -42 0 12 3 27 -16.8%", "text": "Tables X and XI show the results obtained when classifying using the last 30 seconds of the songs and the comparison of those segments versus full songs, respectively. The end sections obtained an accuracy of 76.8%, corresponding to a 13.04 pp decrease when compared to the full songs. Again, Bass was mainly misclassified as Hip hop and Indie Rock, and Trance was mainly misclassified as Bass. This is mostly due to the fact that the last 30 seconds correspond to the outro section of the songs which shares many similarities with the intro section. When considering Trance and Bass, the outro also shares characteristics with the breakdown sections. The fade repeat effect present in many songs\u2019 endings also increases this confusion. This means that taking the last 30 seconds of a song is also not a good segment selection strategy.\nMarch 24, 2015 DRAFT"}, {"heading": "B -48 3 26 18 1 -19.2%", "text": ""}, {"heading": "C. Baseline Assessment", "text": "Although, from the above experiments, it seems that taking the middle sections of the songs is better than taking the beginning or end, it is still not good enough, at least not for all of the considered genres. The features used by the classifier are statistics (averages and variances) of features extracted along the whole signal. Those features perform well when taking the whole signal as input, which means that, in order to obtain a similar performance, those averages and variances should be similar. That cannot be guaranteed when taking 30-second continuous clips because those 30 seconds may happen to belong to a single (and not distinctive enough) structural part of the song (such as intro, breakdown, and outro). If that is the case, then there is not sufficient diversity in the segment/summary to accurately represent the whole song. Moreover, some music genres can only be accurately distinguished by some of those structural parts: the best examples in this dataset are the Bass and Trance classes, which are much more accurately distinguished and represented by their drop sections. Therefore, we need to make better choices regarding what parts of the song should be included in the 30-second summaries to be classified.\nMarch 24, 2015 DRAFT"}, {"heading": "D. GRASSHOPPER", "text": "Generic summarization algorithms define and detect relevance and diversity of the input signal, satisfying our need for a more informed way of selecting the most important parts to fit in 30-second summaries. The following tables show results demonstrating this claim. Table XII shows results obtained when classifying the dataset using summaries extracted by the GRASSHOPPER algorithm. The specific parameter values used on this experiment were: (0.5,0.5) seconds framing, 25-word vocabulary, 10-word sentences, and binary weighting. The overall accuracy was 88.16%. Table XIII shows a comparison against the best 30-second baseline - the middle sections. As can be seen, GRASSHOPPER summarization recovered most of what was lost by the middle sections in terms of classification accuracy for each individual class. Since the middle sections performed so badly when distinguishing Bass and Trance, naturally, these summaries improved accuracies mostly for both these classes, with 14.0 pp and 14.4 pp increases, respectively. When listening to some of these summaries, the diversity included in them is clear: the algorithm is selecting sentences from several different structural parts of the songs. An overall improvement of 6.80 pp was obtained this way. Note that, remarkably, these summaries did a better job than full songs at classifying Hip hop by 2.0 pp. This means that, for some tasks, well summarized data can be even more discriminative of a topic (genre) than the original full data."}, {"heading": "E. LexRank", "text": "Tables XIV and XV present the LexRank confusion matrix and its difference against the middle sections, respectively. The parameter values in this experiment were: (0.5,0.5) seconds framing, 25- word vocabulary, 5-word sentences and dampened tf-idf weighting. The overall accuracy was 88.40%. LexRank also greatly improved classification accuracy, when compared against the middle sections (7.04 pp overall), namely, for Bass and Trance, with 15.6 pp and 15.2 pp increases, respectively. LexRank\nMarch 24, 2015 DRAFT\nis clearly selecting diverse parts to include in the 30-second summaries, as we were able to observe when listening to them. It is also interesting that the classifier performed better than with full songs, individually, for another class: Indie Rock\u2019s accuracy increased 1.6 pp."}, {"heading": "F. LSA", "text": "Tables XVI and XVII correspond to the confusion matrix of the LSA summarization setup and the corresponding difference against the middle sections, respectively. The following parameter combination\nMarch 24, 2015 DRAFT\nwas used: (0.5,0.5) seconds framing, 25-word vocabulary, 10-word sentences, and binary weighting. Note that using a term frequency-based weighting on LSA, when applied to music, markedly worsens its performance. This is because noisy sentences in the songs tend to get a very high score on some latent topic, causing LSA to include them in the summaries. Moreover, when also considering inverse document frequency, the results are even worse, because those noisy terms usually appear in very few sentences. That is highly undesirable, since those sections do a very bad job at describing that song in any aspect. Using a binary weighting scheme alleviates that problem because all those noisy frames will get clustered into very few clusters/terms and only that term\u2019s presence (instead of frequency) gets counted into the sentences\u2019 vector representation. The overall accuracy for this combination was 88.32%, an improvement of 6.98 pp against the middle sections. Bass and Trance were also the genres which benefited the most from this summarization, with accuracy increases of 12.8 pp and 14.8 pp, respectively, which can also be explained by the diversity present in the summaries. Indie Rock\u2019s individual accuracy improved, once again, against full songs, with an improvement of 2.8 pp."}, {"heading": "G. MMR", "text": "Table XVIII represents the confusion matrix for an MMR summarization setup and table XIX shows its difference against the middle sections. (0.5,0.5) seconds framing was used, along with a 50-word vocabulary, 10-word sentences, 0.7 \u03bb value, and dampened tf-idf weighting. Note that, even though every other parameter setup (for the other algorithms) shown here uses 20 MFCCs as features, this one uses those same MFCCs concatenated with the 9 spectral features also used for classification (described in section IV-A). This is because MMR, unlike every other summarization algorithm, performed better using this set (instead of only using MFCCs as features). The overall accuracy was 88.80%, corresponding to an improvement of 7.44 pp over the middle sections. Bass and Trance benefited the most from the summarization process, in classification performance, achieving improvements of 14.8 pp and 16.4 pp, respectively. This increase is also explained by the diversity produced by the summarizer."}, {"heading": "H. Support Sets", "text": "Table XX shows the confusion matrix obtained when classifying the dataset using summaries extracted by the Support Sets-based algorithm. The specific parameter setup of this experiment was: (0.5,0.5) sec-\nMarch 24, 2015 DRAFT\nonds framing, 25-word vocabulary, 10-word sentences, dampened tf-idf weighting, and the passage orderbased heuristic for creating the support sets [13] using the cosine similarity. The overall accuracy was 88.80%. Table XXI shows the comparison against the middle sections. Again, summarization recovered most of what was lost by the middle sections in terms of classification accuracy for each individual class, greatly influencing Bass and Trance, with 10.8 pp and 16.8 pp increases, respectively. Listening to some of these summaries, we confirmed the diversity included in them that was clearly lacking in the middle sections. An overall improvement of 7.44 pp was obtained this way. Remarkably, there were also improvements against full songs, namely, a 4.8 pp improvement in Indie Rock."}, {"heading": "VII. DISCUSSION", "text": "We ran Wilcoxon signed-ranked tests on all of the confusion matrices presented above against the full songs scenario. The continuous sections\u2019 p-values were 3.104e-04, 3.628e-03, and 2.858e-05 for the 30-second beginning, middle, and end sections of the songs, respectively, which means that they differ markedly from using full songs (as can also be seen by the accuracy drops they cause). The summaries, however, were very close to full songs, in terms of accuracy. The p-values for GRASSHOPPER,\nMarch 24, 2015 DRAFT\nLexRank, LSA, MMR, and Support Sets were 1.018e-01, 9.324e-02, 1.555e-01, 2.048e-01, and 2.226e-01, respectively. Thus, statistically speaking, using any of these 30-second summaries does not significantly differ from using full songs for classification (considering 95% confidence intervals). Figure 1 shows a comparison of the overall accuracies (in %) obtained from all previously presented classification setups.\nAlthough every algorithm extracts summaries in a different way, they all tend to include relevant and diverse sentences in it. That compensates the reduced amount of information present in them (30 seconds of audio) allowing those clips to be representative enough of the whole musical pieces, from an automatic consumption view, as it could be observed through these classification experiments. As these experiments verified, choosing the best 30-second continuous segments is highly dependent on the genres in the dataset and tasks it will be used for, which is another reason for preferring summaries over those segments. The more varied the dataset, the less likely a fixed continuous section extraction method is to produce representative enough clips. Bass and Trance were the most influenced genres, by summarization, in these experiments. These are styles with very well defined structural borders, and a\nMarch 24, 2015 DRAFT\nvery descriptive structural element \u2013 the drop. The lack of that same element in a segment remarkably hinders classification performance, which suggests that any genre with similar characteristics may also benefit from this type of summarization. It is also worth restating that Hip hop and Indie Rock were very positively influenced by summarization, in what concerns classification performance improvements over using full songs. This shows that, sometimes, classification on summarized music can even outperform using the whole data from the original signal.\nSummarizing music prior to the classification task also takes time, but we do not claim it is worth doing it every time we are about do perform a MIR task. The idea is to compute summarized datasets offline for future use in any task (e.g., music classification) that can benefit from them. Currently, sharing music datasets for MIR research purposes, is very limited in many aspects, due to copyright issues. Usually, datasets are shared through features extracted from (30-second) continuous clips. That practice has drawbacks, such as: those 30 seconds may not contain the most relevant information and may even be highly redundant; and the features provided may not be the ones a researcher needs for his/her experiments. Summarizing datasets may also help avoiding copyright issues (because summaries are not created in a way enjoyable by humans) and still provide researchers with the most descriptive parts (according to each summarizer) of the signal itself, so that many different kinds of features can be extracted from them."}, {"heading": "VIII. CONCLUSIONS AND FUTURE WORK", "text": "We showed that generic summarization algorithms perform well when summarizing music datasets about to be classified. The resulting summaries are remarkably more descriptive of the whole songs than their continuous segments (of the same duration) counterparts. These summaries, sometimes, are even more discriminative than the full songs. We also presented an argument stating some advantages in sharing summarized datasets within the MIR community.\nAn interesting research direction would be to automatically determine which vocabulary sizes would suit best each song, individually, so that a better modeling of the acoustic vocabulary may be used. Testing summarization\u2019s performance on different classification tasks (e.g., with more classes) is also necessary to further confirm these conclusions. More experimenting should be done in other MIR tasks (besides classification) that also make use of only a portion of the whole signal."}], "references": [{"title": "Semantic Segmentation and Summarization of Music: Methods Based on Tonality and Recurrent Structure", "author": ["W. Chai"], "venue": "IEEE Signal Processing Magazine, vol. 23, no. 2, pp. 124\u2013132, 2006. March 24, 2015  DRAFT  SUBMITTED TO IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL. X, NO. X  24", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Summarizing Popular Music via Structural Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Proc. of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2003, pp. 127\u2013130.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward Automatic Music Audio Summary Generation from Signal Analysis", "author": ["G. Peeters", "A. La Burthe", "X. Rodet"], "venue": "Proc. of the 3rd International Society for Music Information Retrieval Conference, 2002, pp. 94\u2013100.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Signal-based Music Structure Discovery for Music Audio Summary Generation", "author": ["G. Peeters", "X. Rodet"], "venue": "Proc. of the 29th International Computer Music Conference, 2003, pp. 15\u201322.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Music Summary using Key Phrases", "author": ["S. Chu", "B. Logan"], "venue": "Hewlett-Packard Cambridge Research Laboratory, Tech. Rep., 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic Music Summarization via Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Proc. of the 3rd International Society for Music Information Retrieval Conference, 2002, pp. 81\u201385.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Automatic Music Summarization: A \u201dThumbnail\u201d Approach", "author": ["J. Glaczynski", "E. Lukasik"], "venue": "Archives of Acoustics, vol. 36, no. 2, pp. 297\u2013309, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio Thumbnailing of Popular Music using Chroma-based Representations", "author": ["M.A. Bartsch", "G.H. Wakefield"], "venue": "IEEE Transactions on Multimedia, vol. 7, no. 1, pp. 96\u2013104, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "Proc. of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998, pp. 335\u2013336.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research, vol. 22, pp. 457\u2013479, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dutnais"], "venue": "Psychological Review, vol. 104, no. 2, pp. 211\u2013240, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving Diversity in Ranking using Absorbing Random Walks", "author": ["X. Zhu", "A.B. Goldberg", "J.V. Gael", "D. Andrzejewski"], "venue": "Proc. of the 5th North American Chapter of the Association for Computational Linguistics - Human Language Technologies Conference, 2007, pp. 97\u2013104.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity", "author": ["R. Ribeiro", "D.M. de Matos"], "venue": "Journal of Artificial Intelligence Research, vol. 42, pp. 275\u2013308, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "On the Application of Generic Summarization Algorithms to Music", "author": ["F. Raposo", "R. Ribeiro", "D.M. de Matos"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 1, pp. 26\u201330, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic Music Classification and Summarization", "author": ["C.X. Xu", "N.C. Maddage", "X.S. Shao"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 13, no. 3, pp. 441\u2013450, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proc. of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2001, pp. 19\u201325.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Using Latent Semantic Analysis in Text Summarization and Summary Evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proc. of ISIM, 2004, pp. 93\u2013100.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Minimizing Word Error Rate in Textual Summaries of Spoken Language", "author": ["K. Zechner", "A. Waibel"], "venue": "Proc. of the 1st North American Chapter of the Association for Computational Linguistics Conference, 2000, pp. 186\u2013193.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Extractive Summarization of Meeting Recordings", "author": ["G. Murray", "S. Renals", "J. Carletta"], "venue": "Proc. of the 9th European Conference on Speech Communication and Technology, 2005, pp. 593\u2013596.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, no. 3, pp. 27:1\u201327:27, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Using Timbre Models for Audio Classification", "author": ["F. de Leon", "K. Martinez"], "venue": "Submission to Audio Classification (Train/Test) Tasks of MIREX 2013, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Recent Developments in openSMILE, the Munich Open-source Multimedia Feature Extractor", "author": ["F. Eyben", "F. Weninger", "F. Gross", "B. Schuller"], "venue": "Proc. of the 21st ACM International Conference on Multimedia, 2013, pp. 835\u2013838.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Armadillo: An Open Source C++ Linear Algebra Library for Fast Prototyping and Computationally Intensive Experiments", "author": ["C. Sanderson"], "venue": "NICTA, Tech. Rep., 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "MARSYAS: A Framework for Audio Analysis", "author": ["G. Tzanetakis", "P. Cook"], "venue": "Organised Sound, vol. 4, no. 3, pp. 169\u2013175, 1999.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "MLPACK: A Scalable C++ Machine Learning Library", "author": ["R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 801\u2013805, 2013. March 24, 2015  DRAFT", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Music summarization has been the subject of research for at least a decade and many algorithms that address this problem, mainly for popular music, have been published in the past [1]\u2013[8].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "INTRODUCTION Music summarization has been the subject of research for at least a decade and many algorithms that address this problem, mainly for popular music, have been published in the past [1]\u2013[8].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "Generic summarization algorithms, however, focus on extracting concise and diverse summaries and have been successfully applied in text and speech summarization [9]\u2013[13].", "startOffset": 161, "endOffset": 164}, {"referenceID": 12, "context": "Generic summarization algorithms, however, focus on extracting concise and diverse summaries and have been successfully applied in text and speech summarization [9]\u2013[13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 71, "endOffset": 74}, {"referenceID": 12, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "These results complement and solidify previous work evaluated on a binary Fado classifier [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "In [1], segmentation is achieved by using a Hidden Markov Model (HMM) to detect key changes between frames and Dynamic Time Warping (DTW) to detect repeating structure.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2], a Gaussian-tempered \u201ccheckerboard\u201d kernel is correlated along the main diagonal of the song\u2019s self-similarity matrix, outputting segment boundaries.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [3] and [4], songs are segmented in 3 stages.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [3] and [4], songs are segmented in 3 stages.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "In [5], clustering is used to group and label similar segments of the song.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [6]", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "and [7], Average Similarity is used to extract a thumbnail L seconds long that is the most similar to the whole piece.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "Another method for this task, Maximum Filtered Correlation [8], starts by building a similarity matrix and then a filtered time-lag matrix, embedding the similarity between extended segments separated by a constant lag.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "In [15], music is classified as pure or vocal, in order to perform type-specific feature extraction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "The Graph Random-walk with Absorbing StateS that HOPs among PEaks for Ranking (GRASSHOPPER) [12] is a centrality-based method.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "LexRank [10] is another centrality-based method relying on the similarity (usually, the cosine) between sentence pairs (usually, represented as tf-idf vectors).", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "LSA was first applied in text summarization in [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "In [17], two limitations of this approach are discussed: if selecting K equal to the number of sentences in the summary, when it increases, the summary tends to include less significant sentences; and that sentences with high values in several dimensions (topics), but never the highest, will never be included", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "Sentence selection in MMR [9] is done according to their relevance and diversity against previously selected sentences, in order to output low-redundancy summaries.", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "MMR is a query-based method that has been used in speech summarization [18], [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "MMR is a query-based method that has been used in speech summarization [18], [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "This centrality-based method was introduced in [13] and was successfully applied in text and speech summarization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "In [13], heuristics are explored to define the support sets, specifically, a passage order heuristic which clusters all passages into two clusters, according to their distance to each cluster\u2019s centroid.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Classification is performed using Support Vector Machines (SVMs) [21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "The features used by the SVM classifier consist of a 38-dimensional vector per song, which is a concatenation of several statistics on features used in [22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "These features were extracted using OpenSMILE [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "Every algorithm was implemented in C++ using the following libraries: OpenSMILE [23] for feature extraction (in summarization), Armadillo [24] for matrix operations, and Marsyas [25] for synthesizing the summaries.", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "Every algorithm was implemented in C++ using the following libraries: OpenSMILE [23] for feature extraction (in summarization), Armadillo [24] for matrix operations, and Marsyas [25] for synthesizing the summaries.", "startOffset": 138, "endOffset": 142}, {"referenceID": 23, "context": "Every algorithm was implemented in C++ using the following libraries: OpenSMILE [23] for feature extraction (in summarization), Armadillo [24] for matrix operations, and Marsyas [25] for synthesizing the summaries.", "startOffset": 178, "endOffset": 182}, {"referenceID": 24, "context": "mlpack\u2019s [26] implementation of the K-Means algorithm was used for this step (we experiment with", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "These features, used in several previous research efforts on music summarization in [1]\u2013[7], describe the timbre of an acoustic signal.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "These features, used in several previous research efforts on music summarization in [1]\u2013[7], describe the timbre of an acoustic signal.", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "onds framing, 25-word vocabulary, 10-word sentences, dampened tf-idf weighting, and the passage orderbased heuristic for creating the support sets [13] using the cosine similarity.", "startOffset": 147, "endOffset": 151}], "year": 2015, "abstractText": "Many Music Information Retrieval (MIR) tasks process only a segment of the whole music signal, in order to satisfy processing time constraints. This practice may lead to decreasing performance since the most important information for the task may not be in those processed segments. In this paper, we leverage generic summarization algorithms, previously applied to text and speech summarization, to summarize items in music datasets. These algorithms build summaries, that are both concise and diverse, by selecting appropriate segments from the input signal which makes them good candidates to summarize music as well. We evaluate the summarization process on binary and multiclass music genre classification tasks, by comparing the performance obtained using summarized datasets against the performances obtained using continuous segments (which is the traditional method used for addressing the previously mentioned time constraints) and full songs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA, MMR, and a Support Sets-based Centrality model improve classification performance when compared to selected 30-second baselines. We also show that these summarized datasets lead to a classification performance whose difference is not statistically significant from using full songs. Furthermore, we make an argument stating the advantages of sharing summarized datasets for future MIR research.", "creator": "LaTeX with hyperref package"}}}