{"id": "1503.07274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2015", "title": "Initialization Strategies of Spatio-Temporal Convolutional Neural Networks", "abstract": "results propose a new way of incorporating temporal information present in videos into image convolutional neural networks ( convnets ) measured on images, that avoids training spatio - temporal convnets from scratch. we describe several initializations of weights implementing 3d convolutional coding of spatio - th convnet using 2d convolutional weights learned from imagenet. we show firstly it is advised to initialize 3d noise weights judiciously in order to learn temporal representations of videos. we evaluate simple implementation on the ucf - 101 dataset now demonstrate improvement over spatial convnets.", "histories": [["v1", "Wed, 25 Mar 2015 03:41:47 GMT  (483kb,D)", "http://arxiv.org/abs/1503.07274v1", "Technical Report"]], "COMMENTS": "Technical Report", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["elman mansimov", "nitish srivastava", "ruslan salakhutdinov"], "accepted": false, "id": "1503.07274"}, "pdf": {"name": "1503.07274.pdf", "metadata": {"source": "CRF", "title": "Initialization Strategies of Spatio-Temporal Convolutional Neural Networks", "authors": ["Elman Mansimov", "Nitish Srivastava", "Ruslan Salakhutdinov"], "emails": ["rsalakhu@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "Recognizing the action performed in videos is one of the most challenging problems in computer vision. Compared to images that only contain spatial information about objects present in one shot (frame), videos represent a continuous flow of information that describes the physics of the world that we live in. In these settings, learning the underlying temporal representations of information present in videos becomes an important challenge faced in order to recognize the action.\nDespite the fact that GPUs are getting faster and getting more memory every year, training SpatioTemporal ConvNets on large scale video datasets such as Sports-1M [7] and Facebook-380K [15] from scratch is a very time consuming task, which requires nearly a month to complete training. At the same time, training Spatio-Temporal ConvNets on smaller datasets such as UCF-101 [13] and HMDB [9] leads to severe overfitting. Taking a ConvNet trained on ImageNet [11] and fine-tuning it on individual video frames solves the problem of overfitting, but gives unsatisfying solution because this model doesn\u2019t learn temporal representations present in multiple frames.\nTo tackle the above challenge, we propose several ways of initializing 3D convolutional weights, which learn temporal representations of videos, using 2D convolutional weights learned from im-\n\u2217emansim, nitish, rsalakhu@cs.toronto.edu\nar X\niv :1\n50 3.\n07 27\n4v 1\n[ cs\n.C V\n] 2\n5 M\nar 2\nages. This dramatically speeds up training of Spatio-Temporal ConvNets and reduces overfitting on relatively small video datasets. We found that in order to learn temporal representations present in videos and get improvements in accuracy, it is important to initialize weights in 3D convolutional layers judiciously. Otherwise, the Spatio-Temporal ConvNet remains stuck at the plateau that only extracts spatial information from videos and will not get improvements in accuracy. By appropriately initializing weights and using Composite LSTM that learned representations of video sequences [14] trained on labelled examples present in UCF-101 and unlabelled examples from Sports-1M, we managed to nearly match the current best classification accuracy [17] on RGB data extracted from UCF-101 which uses many additional tricks to improve their performance."}, {"heading": "2 Related Work", "text": "Research in action recognition was mainly driven by advancements in object recognition in images, where those approaches were extended or adapted to deal with videos. Traditional shallow approaches consisted of three main stages. Firstly, sparse spatio-temporal interest points were detected in videos and extracted using Histogram of Oriented Gradients (HOG) [1] and Histogram of Optical Flow (HOF) [2]. Recently, Wang et al. [16] have proposed dense trajectories which became state-of-the art hand-crafted features for action recognition. Next, those features got combined into a fixed-sized vector description, such as Bag of Words (BoW) or Fisher Vectors (FV). Lastly, the standard classifier such as SVM was trained on BoW or FV represenation to distinguish among the classes of interest. This approach still has state-of-the art performance on UCF-101 and HMDB datasets.\nWith the larger availability of labeled image data and advancements in parallel computing, Deep ConvNets [8] overshadowed traditional approaches in extracting features of images. Motivated by those results, researchers [7] and [15] have created large scaled labelled video datasets and trained Deep Spatio-Temporal ConvNets on them, which were originally proposed by Ji et al. [6]. Even though their models had access to temporal information presented in videos, they didn\u2019t perform better than simple Spatial ConvNet fine-tuned on image frames extracted from UCF-101 and HMDB datasets [7], [15]. Recently, Simonyan et al. [12] showed that Deep Early Fusion Spatio-Temporal ConvNet trained on dense optical flow extracted from videos significantly boosts the performance of fine-tuned spatial ConvNet trained on images. Their approach nearly matched state-of-the-art performance on UCF-101 dataset. Additionally, [17], [4], [5] recently showed that spatio-temporal pooling of Deep ConvNet features gives additional improvement in performance."}, {"heading": "3 Transforming Features", "text": "In this section we describe several ways of transforming 2D Convolutional Weights into 3D ones, without losing the spatial information learned by training Spatial ConvNet on ImageNet. Suppose that we have a 2D Convolutional Weight MatrixW (2D) derived from training a Spatial ConvNet on\nImageNet. Our goal is to create a 3D Convolutional Weight MatrixW (3D) with temporal dimension T using those weights. Each sub-matrix W (3D)t , \u2200t \u2208 {1, ..., T} has the same dimension as the original 2D Convolutional Weight Matrix.\nIn a trained Spatial ConvNet, each layer expects an input from the layer below it to be within some specific range. It is important to initialize W (3D) in a certain way, so that the output of the 3D Convolutional Layer remains to be approximately in the same range as the output of the originally learned 2D Convolutional Layer. In order to enforce this constraint, the sum of all sub-matrices of W (3D) has to be equal to W (2D) at the initialization time, i.e. \u2211T t=1W (3D) t = W (2D).\nBelow we desribe four different kinds of initializations of W (3D), which we have considered."}, {"heading": "3.1 Initialization by Averaging. (IA)", "text": "Since the consecutive image frames in videos share similarities in appearance, we would expect W (2D) to extract very similar spatial representations from each consecutive image frame. Therefore, we can initialize each sub-matrix W (3D)t to extract the same spatial representations present in its respective input feature map. This could be accomplished by diving each element of the 2D Convolutional weight matrix by the temporal size of the convolutional layer and setting all convolutional weights across temporal dimension of W (3D) to the resulting matrix. More formally, it can be expressed as:\nW (3D) t = W (2D) T , \u2200t \u2208 {1, ..., T}"}, {"heading": "3.2 Initialization by Scaling. (IS)", "text": "This method can be viewed as a generalization to the method of initializing weights by averaging. Instead of dividing W (2D) by the same number T and hence making each W (3D)t equal to each other, we can induce some diversity by setting each W (3D)t to W (2D) divided by some random\nconstant. Any combination of the values of those constants could work, as long as the constraint described in section 3 is satisfied. Therefore, W (3D) becomes:\nW (3D) t = \u03b1t \u2217W (2D), where \u03b1t > 0 and \u2211T t=1 \u03b1t = 1"}, {"heading": "3.3 Zero Weight Initialization. (ZWI)", "text": "It is natural to ask ourselves what would happen if we initialize one of W (3D)t to W (2D) and initialize other sub-matrices to zero matrix. Would those sub-matrices initialized with zeros be able to learn to extract meaningful representations from the input even in spite of having limited training data ? Additionally, despite some differences in values, the distribution of weights is the same in each sub-matrix of W (3D) in the above initializations. Because of that the network might get stuck at the plateau which was reached by learning spatial representation on images and as a result it might not learn the temporal representation presented in multiple frames. Motivated by these points, we explored the following initialization technique, that can be expressed as:\nW (3D) = { W (2D), if t = 1 O, otherwise.\nNote that, for any 1 \u2264 t \u2264 T , W (3D)t could be initialized to W (2D), as long as all values of other sub-matrices are zero. However, in our experiments we found out that changing the order made no difference in resulting accuracy."}, {"heading": "3.4 Negative Weight Initialization. (NWI)", "text": "We can extend the zero weight initialization by encouraging each sub-matrix W (3D)t , excluding the first one, to move even further from original values of W (2D). This could be accomplished by setting the values of sub-matrices W (3D)t , 2 \u2264 t \u2264 T to negative signed values of W (2D) divided by some constant. This woud make the absolute values of W (3D)t , t = 1 larger compared to other initializations. Again as in ZWI, changing the order of sub-matrices doesn\u2019t give any change in resulting accuracy. It can be expressed as following:\nW (3D) t = \u03b1t \u2217W (2D), where\n\u03b1t =\n{ 2T\u22121 T , if t = 1\n\u2212 1 T , otherwise."}, {"heading": "3.5 Architecture Details.", "text": "The ConvNet with an architecture described here https://github.com/TorontoDeepLearning/ convnet/blob/master/examples/imagenet/CLS_net_20140801232522.pbtxt was trained on ImageNet ILSVRC-2012. It yielded 13.5% top-5 error on ILSVRC-2012 validation set. In this study, we did not focus on using the best ImageNet model. Rather, we chose one that was convenient and easy to work with and focused on studying the relative impact of our proposed method. When fine-tuning, we removed the softmax layer and reduced the number of units in the last fully connected hidden layer from 4096 to 2048. Otherwise, the performance of the model dropped by 6-7 %. We also used an aggressive dropout of 0.8 in both FC layers. The first two convolutional layers were transformed into 3D Convolutional layers with temporal size of 3, stride of 1 and temporal size of 2, stride of 1 respectively. The 3D pooling operation between those layers was perfromed on regions with temporal size of 2. The optimization in all convolutional layers started after 500 iterations in order to preserve good features learned by the model on ImageNet. Due to the limited size of the training set, other 3D Convolutional Networks with larger number of 3D Convolutional Layers performed worse than this model."}, {"heading": "4 Evaluation and Discussion", "text": ""}, {"heading": "4.1 Dataset", "text": "The evaluation is performed on first split of UCF-101 dataset. UCF-101 contains 13.2K videos (25 frames/second) annotated into 101 classes, where each split contains 9.5K training videos."}, {"heading": "4.2 Effect of Different Initializations.", "text": "The results of all transfer learning experiments are shown in Table 1. The results yielded many different conclusions.\nFirst, fine-tuning 3D ConvNet initialized by averaging or scaling yielded only small marginal improvement compared to fine-tuning 2D ConvNet. For the initialization by scaling, the best results were yielded by using \u03b11 = \u03b13 = 14 and \u03b12 = 1 2\nfor the first convolutional layer, and \u03b11 = \u03b12 = 12 for the second convolutional layer. We think that since each sub-matrix across temporal dimension is initialized with similar weights which extract spatial features, the convolutional layer couldn\u2019t move beyond this plateau.\nIn contrast, despite the fact that all sub-matrices of W (3D), except for one, were initialized with zero matrix, the model did surprisingly better than models initialized using averaging or scaling. This could be explained by the fact that, at the initialization time the first sub-matrix which extracts spacial representations encouraged other sub-matrices to learn to extract temporal representations.\nAlso, compared to other IA and IS, larger differences in values between first and other sub-matrices may have helped the model to learn to extract better temporal representations from the input. This might explain why the model initialized by Negative Weight Initialization yielded the best performance among them all.\nFinally, by averaging our softmax class probabilities with softmax class probabilities of Composite LSTM, we managed to nearly match the current highest accuracy of deep learning based approaches on RGB data. We think that our performance can be further increased by using different tricks described in [17] and using SVM fusion instead of averaging. The results are summarized in Table 2.\nModel Performance Spatial ConvNet [12] 72.8 %\nC3D [15] 72.3 % C3D + fc6 [15] 76.4 %\nLRCN [3] 71.1 % Composite LSTM [14] 75.8 % NWI + Composite LSTM 78.3 % ConvNet Features [17] 79.0 %\nTable 2: Comparisons with other state-ofthe-art neural networks based approaches on RGB data."}, {"heading": "4.3 Two-Stream Spatio-Temporal ConvNets", "text": "We further ran experiments and evaluated complete two-stream model on first split, which combines RGB and Optical Flow based Spatio-Temporal ConvNets. Optical Flow based ConvNet, similar to the one proposed by [12], was trained on single frame optical flow and stacks of 10 optical flows. This gave an accuracy of 72.2% and 77.5% respectively. Additionally we trained a slow fusion version of this model, with an slow fusion setup same as in [7] and this model gave an accuracy of 79.3%. We then combine the softmax scores of this optical flow based slow fusion model with the softmax scores of RGB based NWI + Composite LSTM model and obtain the accuracy of 85.3 % on UCF-101. Also, despite the smaller accuracy of early fusion model compared to its slow fusion version, the combination of optical flow based early fusion model with NWI + Composite LSTM gave the same accuracy of 85.3 %. Esentially, despite better results of slow fusion model, it gives no additional performance improvement in final averaged scores. The comparison of our model with state-of-the-art action recognition models is summarized in Table 3.\n3 look very similar to W\n(3D)\n1 . Whereas in\nConvNet initialized by NWI, W (3D)2 and W (3D) 3 look different from W (3D) 1 . Also, despite having limited labelled training video data, W (3D)2 and W (3D) 3 look like edge and color blobs detectors in Spatio-Temporal ConvNet initialized by ZWI. 7"}, {"heading": "5 Conclusions", "text": "We proposed a new way of creating Spatio-Temporal ConvNets, by incorporating temporal information into Spatial ConvNet trained on images. Despite having limited labelled video training data, our model outperformed Spatio-Temporal ConvNets trained on large scale labelled video datasets. It is interesting to see, whether initializing Spatio-Temporal ConvNet with one of the initialization techniques proposed here gives an improvement in performance on large scaled labelled video datasets. However, it is very challenging for us to process the datasets of such scale."}], "references": [{"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In CVPR, pages 886\u2013893", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Human detection using oriented histograms of flow and appearance", "author": ["N. Dalal", "B. Triggs", "C. Schmid"], "venue": "In ECCV, pages 428\u2013441", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In EECV, pages 392\u2013407", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["K. He", "X. Zhang"], "venue": "Ren, , and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In In EECV", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 1106\u20131114", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "HMDB: a large video database for human motion recognition", "author": ["H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "Proceedings of the International Conference on Computer Vision (ICCV)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition", "author": ["Z. Lan", "M. Lin", "X. Li", "A.G. Hauptmann", "B. Raj"], "venue": "CoRR, abs/1411.6660", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "and L", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg"], "venue": "Fei-Fei. ImageNet Large Scale Visual Recognition Challenge", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A. Roshan Zamir", "M. Shah"], "venue": "CRCV-TR-12-01", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "CoRR, abs/1502.04681", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "C3D: generic features for video analysis", "author": ["D. Tran", "L.D. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "CoRR, abs/1412.0767", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "IEEE International Conference on Computer Vision, Sydney, Australia", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting image-trained cnn architectures for unconstrained video classification", "author": ["S. Zha", "F. Luisier", "W. Andrews", "N. Srivastava", "R. Salakhutdinov"], "venue": "CoRR, abs/1503.04144", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Despite the fact that GPUs are getting faster and getting more memory every year, training SpatioTemporal ConvNets on large scale video datasets such as Sports-1M [7] and Facebook-380K [15] from scratch is a very time consuming task, which requires nearly a month to complete training.", "startOffset": 163, "endOffset": 166}, {"referenceID": 14, "context": "Despite the fact that GPUs are getting faster and getting more memory every year, training SpatioTemporal ConvNets on large scale video datasets such as Sports-1M [7] and Facebook-380K [15] from scratch is a very time consuming task, which requires nearly a month to complete training.", "startOffset": 185, "endOffset": 189}, {"referenceID": 12, "context": "At the same time, training Spatio-Temporal ConvNets on smaller datasets such as UCF-101 [13] and HMDB [9] leads to severe overfitting.", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "At the same time, training Spatio-Temporal ConvNets on smaller datasets such as UCF-101 [13] and HMDB [9] leads to severe overfitting.", "startOffset": 102, "endOffset": 105}, {"referenceID": 10, "context": "Taking a ConvNet trained on ImageNet [11] and fine-tuning it on individual video frames solves the problem of overfitting, but gives unsatisfying solution because this model doesn\u2019t learn temporal representations present in multiple frames.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "By appropriately initializing weights and using Composite LSTM that learned representations of video sequences [14] trained on labelled examples present in UCF-101 and unlabelled examples from Sports-1M, we managed to nearly match the current best classification accuracy [17] on RGB data extracted from UCF-101 which uses many additional tricks to improve their performance.", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "By appropriately initializing weights and using Composite LSTM that learned representations of video sequences [14] trained on labelled examples present in UCF-101 and unlabelled examples from Sports-1M, we managed to nearly match the current best classification accuracy [17] on RGB data extracted from UCF-101 which uses many additional tricks to improve their performance.", "startOffset": 272, "endOffset": 276}, {"referenceID": 0, "context": "Firstly, sparse spatio-temporal interest points were detected in videos and extracted using Histogram of Oriented Gradients (HOG) [1] and Histogram of Optical Flow (HOF) [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "Firstly, sparse spatio-temporal interest points were detected in videos and extracted using Histogram of Oriented Gradients (HOG) [1] and Histogram of Optical Flow (HOF) [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 15, "context": "[16] have proposed dense trajectories which became state-of-the art hand-crafted features for action recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "With the larger availability of labeled image data and advancements in parallel computing, Deep ConvNets [8] overshadowed traditional approaches in extracting features of images.", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "Motivated by those results, researchers [7] and [15] have created large scaled labelled video datasets and trained Deep Spatio-Temporal ConvNets on them, which were originally proposed by Ji et al.", "startOffset": 40, "endOffset": 43}, {"referenceID": 14, "context": "Motivated by those results, researchers [7] and [15] have created large scaled labelled video datasets and trained Deep Spatio-Temporal ConvNets on them, which were originally proposed by Ji et al.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Even though their models had access to temporal information presented in videos, they didn\u2019t perform better than simple Spatial ConvNet fine-tuned on image frames extracted from UCF-101 and HMDB datasets [7], [15].", "startOffset": 204, "endOffset": 207}, {"referenceID": 14, "context": "Even though their models had access to temporal information presented in videos, they didn\u2019t perform better than simple Spatial ConvNet fine-tuned on image frames extracted from UCF-101 and HMDB datasets [7], [15].", "startOffset": 209, "endOffset": 213}, {"referenceID": 11, "context": "[12] showed that Deep Early Fusion Spatio-Temporal ConvNet trained on dense optical flow extracted from videos significantly boosts the performance of fine-tuned spatial ConvNet trained on images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Additionally, [17], [4], [5] recently showed that spatio-temporal pooling of Deep ConvNet features gives additional improvement in performance.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Additionally, [17], [4], [5] recently showed that spatio-temporal pooling of Deep ConvNet features gives additional improvement in performance.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "Additionally, [17], [4], [5] recently showed that spatio-temporal pooling of Deep ConvNet features gives additional improvement in performance.", "startOffset": 25, "endOffset": 28}, {"referenceID": 16, "context": "We think that our performance can be further increased by using different tricks described in [17] and using SVM fusion instead of averaging.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "Model Performance Spatial ConvNet [12] 72.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "8 % C3D [15] 72.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "3 % C3D + fc6 [15] 76.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "4 % LRCN [3] 71.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "1 % Composite LSTM [14] 75.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "3 % ConvNet Features [17] 79.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "Optical Flow based ConvNet, similar to the one proposed by [12], was trained on single frame optical flow and stacks of 10 optical flows.", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "Additionally we trained a slow fusion version of this model, with an slow fusion setup same as in [7] and this model gave an accuracy of 79.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Method Performance LRCN [3] 82.", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "9 % Two-Stream Convolutional Net [12] (split 1) 87.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "0 % C3D + fc6 + iDT [15] 86.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "7 % ConvNet Features + iDT [17] 89.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "7 % Multi-skip feature stacking [10] 89.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "1 % Composite LSTM Model [14] (split 1) 84.", "startOffset": 25, "endOffset": 29}], "year": 2015, "abstractText": "We propose a new way of incorporating temporal information present in videos into Spatial Convolutional Neural Networks (ConvNets) trained on images, that avoids training SpatioTemporal ConvNets from scratch. We describe several initializations of weights in 3D Convolutional Layers of Spatio-Temporal ConvNet using 2D Convolutional Weights learned from ImageNet. We show that it is important to initialize 3D Convolutional Weights judiciously in order to learn temporal representations of videos. We evaluate our methods on the UCF-101 dataset and demonstrate improvement over Spatial ConvNets.", "creator": "LaTeX with hyperref package"}}}