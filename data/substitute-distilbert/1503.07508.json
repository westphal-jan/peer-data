{"id": "1503.07508", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2015", "title": "Stable Feature Selection from Brain sMRI", "abstract": "neuroimage analysis usually involves learning hundred or even millions of variables using only a limited number of samples. in this scheme, sparse approximation, ti. g. the lasso, are applied to select the optimal features and achieve improved probability accuracy. the lasso, obviously, produces results in independent unstable features. stability, a manifest consistent reproducibility of statistical results subject to reasonable perturbations to data and the model, is an important focus in statistics, especially in the analysis of high relevance data. in this paper, we explore a nonnegative generalized fused lasso model for evaluating feature selection in the diagnosis of vincent's disease. in addition to sparsity, our evidence incorporates two important consensus priors : the spatial cohesion of lesion voxels and the positive variance between the features and the disease labels. to optimize every model, we propose an efficient algorithm by initiating a novel link between total knowledge and fast network flow algorithms via conditional duality. experiments show... the proposed nonnegative test performs much better in exploring novel intrinsic structure of statistics via selecting stable features compared with other state - of - the - arts.", "histories": [["v1", "Wed, 25 Mar 2015 19:30:14 GMT  (10769kb,D)", "http://arxiv.org/abs/1503.07508v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["bo xin", "lingjing hu", "yizhou wang 0001", "wen gao 0001"], "accepted": true, "id": "1503.07508"}, "pdf": {"name": "1503.07508.pdf", "metadata": {"source": "CRF", "title": "Stable Feature Selection from Brain sMRI", "authors": ["Bo Xin", "Lingjing Hu", "Yizhou Wang", "Wen Gao"], "emails": [], "sections": [{"heading": "Introduction", "text": "Neuroimage analysis is challenging due to its high feature dimensionality and data scarcity. Sparse models such as the lasso (Tibshirani 1996) have gained great reputation in statistics and machine learning, and they have been applied to the analysis of such high dimensional data by exploiting the sparsity property in the absence of abundant data. As a major result, automatic selection of relevant variables/features by such sparse formulation achieves promising performance. For example, in (Liu, Zhang, and Shen 2012), the lasso model was applied to the diagnosis of Alzheimer\u2019s disease (AD) and showed better performance than the support vector machine (SVM), which is one of the state-of-the-arts in brain image classification. However, in statistics, it is known that the lasso does not always provide interpretable results because of its instability (Yu 2013). \u201cStability\u201d here means the reproducibility of statistical results subject to reasonable perturbations to data and\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nthe model. (These perturbations include the often used Jacknife, bootstrap and cross-validation.) This unstable behavior of the lasso model is critical in high dimensional data analysis. The resulting irreproducibility of the feature selection are especially undesirable in neuroimage analysis/diagnosis. However, unlike the problems such as registration and classification, the stability issue of feature selection is much less studied in this field.\nIn this paper we propose a model to induce more stable feature selection from high dimensional brain structural Magnetic Resonance Imaging (sMRI) images. Besides sparsity, the proposed model harnesses two important additional pathological priors in brain sMRI: (i) the spatial cohesion of lesion voxels (via inducing fusion terms) and (ii) the positive correlation between the features and the disease labels. The correlation prior is based on the observation that in many brain image analysis problems (such as AD, frontotemporal dementia, corticobasal degeneration, etc), there exist strong correlations between the features and the labels. For example, gray matter of AD is degenerated/atrophied. Therefore, the gray matter values (indicating the volume) are positively correlated with the cognitive scores or disease labels {-1,1}. That is, the less gray matter, the lower the cognitive score. Accordingly, we propose nonnegative constraints on the variables to enforce the prior and name the model as \u201cnon-negative Generalized Fused Lasso\u201d (n2GFL). It extends the popular generalized fused lasso and enables it to explore the intrinsic structure of data via selecting stable features. To measure feature stability, we introduce the \u201cEstimation Stability\u201d recently proposed in (Yu 2013) and the (multi-set) Dice coefficient (Dice 1945). Experiments demonstrate that compared with existing models, our model selects much more stable (and pathological-prior consistent) voxels. It is worth mentioning that the non-negativeness per se is a very important prior of many practical problems, e.g. (Lee and Seung 1999). Although n2GFL is proposed to solve the diagnosis of AD in this work, the model can be applied to more general problems.\nIncorporating these priors makes the problem novel w.r.t the lasso or generalized fused lasso from an optimization standpoint. Although off-the-shelf convex solvers such as CVX (Grant and Boyd 2013) can be applied to solve the optimization, it hardly scales to high-dimensional problems in feasible time. In this regard, we propose an efficient algo-\nar X\niv :1\n50 3.\n07 50\n8v 1\n[ cs\n.L G\n] 2\n5 M\nar 2\n01 5\nrithm that solves the n2GFL problem exactly. We generalize the proximal gradient methods (such as FISTA) (Beck and Teboulle 2009) to solve our constrained optimization and prove its convergence. We then show that by using an element-wise post-processing, the resulting proximal operator can be reduced to the total variation (TV) problem. It is known that TV can be solved by parametric flow algorithms (Chambolle and Darbon 2009; Xin et al. 2014). In the present study, we provide a novel equivalence via conic duality, which gives us a minimum quadratic cost flow formulation (Hochbaum and Hong 1995). Fast flow algorithms (including parametric flow) are then easily applied. In practice, our algorithm runs hundreds of times faster than CVX at the same precision and can scale to high-dimensional problems.\nRelated work. In addition to sparsity, people leverage underlying data structures and introduce stronger priors such as the structured sparsity (Jacob, Obozinski, and Vert 2009) to increase model stability. However, for voxel-based sMRI data analysis, handcrafted grouping of the voxels or sub-structures may not coincide with various pathological topology priors. Consequently, group lasso (with overlap) (Jacob, Obozinski, and Vert 2009; Jenatton et al. 2012; Rao et al. 2013) is not an ideal model to the problem. In contrast, the graph-based structured sparse models adapt better to such a situation. The most popular one is referred here as LapL1, which adopts l2 norm regularization of neighborhood variable difference (e.g. (Ng and Abugharbieh 2011; Grosenick et al. 2013)). However, as we will show in the experiments, these models select many more features than necessary. Very recently, generalized fused lasso or total variation has been successful applied to brain image analysis problems inducing the l1 difference (Gramfort, Thirion, and Varoquaux 2013; Xin et al. 2014). In the experiments, we show that by including an extra nonnegative constraint, the features selected by our model is much more stable than that of such unconstrained models. A very recent work (Avants et al. 2014) also explored this positive correlation (partially supporting our assumption), but the problem formulation was quite different: neither structural assumption was considered, nor the stability of feature selection was discussed. From the optimization standpoint, the applied framework is similar to that of (Xin et al. 2014) but two key differences exist: (1) the FISTA and soft-thresholding process applied in (Xin et al. 2014) do not generalize to constrained optimization problems, we show important modifications and provide theoretical proof; (2) we propose a novel understanding of TV\u2019s relation with flow problems via conic duality and prove that the minimum norm point problem of (Xin et al. 2014) is a special case of our framework."}, {"heading": "The Proposed Method", "text": "Nonnegative Generalized Fused Lasso (n2GFL) Let {(xi, yi)}Ni=1 be a set of samples, where xi \u2208 Rd and yi \u2208 R are features and labels, respectively. Also, we denote\n1Although different names are given in e.g. (Ng and Abugharbieh 2011; Grosenick et al. 2013), they are in fact fundamentally applying the graph Laplacian smoothing.\nby X \u2208 Rd\u00d7N and y \u2208 RN the concatenations of xi and yi. Then, we consider the formulation\nmin \u03b2\u2208Rd l(\u03b2;X,y) + \u03bb1 d\u2211 i=1 |\u03b2i|+ \u03bb2 \u2211 (i,j)\u2208E wij |\u03b2i \u2212 \u03b2j |,\ns.t. \u03b2 \u2265 0 (1)\nwhere \u03bb1, \u03bb2 \u2265 0 are tuning parameters. l is a loss term of variable \u03b2 (assumed to be convex and smooth). wijs are pre-defined weights. Here, the variables (e.g. the sMRI voxel parameters in the AD problem) are supposed to have certain underlying structure represented by a graphG = (V,E) with nodes V and edges E. Each variable corresponds to a node on the graph. As mentioned above, in many brain image analysis, there exist strong directional correlations (positive or negative) between the features and the labels, thus we assume \u03b2 \u2265 0 (or \u03b2 \u2264 0). Due to the l1 penalties on each variable as well as each adjacent pair of variables in (1), solutions tend to be both sparse and smooth, i.e., adjacent variables tend to be similar and spatially coherent. Also because we have added the nonnegative constraints, the model will not select negatively correlated features as support. In practice, we notice that unconstrained models will systematically select many negatively correlated features. The nonnegative constraints greatly reduce these falsely recovered variables and encourage genuine disease-related features to be selected.\nEfficient Optimization of n2GFL The optimization of n2GFL is convex and off-the-shelf solver such as CVX can be applied. However, this solution hardly scales to a problem sized of thousands (mainly due to its choice of general second order frameworks), see Table 1. In this regard, we propose certain modifications to scalable first order methods (e.g. accelerated proximal methods (Beck and Teboulle 2009)). This is done by exploring Lagrange multiplier method to deal with the constraints. From the optimization standpoint, these modifications are nontrivial and compose one major contribution of this work.\nWe first extend the (fast) iterative shrinkage thresholding algorithm (ISTA and FISTA) (Beck and Teboulle 2009) as follows. Proposition 1. Let \u03b2\u2217 be the optimal solution to (1) and \u03b2k defined as follows\n\u03b2k+1 = min \u03b2\u2208Rd\n1 2 \u2016\u03b2 \u2212 zk\u201622 + \u03bb1 L d\u2211 i=1 |\u03b2i|+\n\u03bb2 L \u2211 (i,j)\u2208E wij |\u03b2i \u2212 \u03b2j |, s.t. \u03b2 \u2265 0, (2)\nwhere L > 0 is the Lipschitz constant of \u2207l(\u00b7) and k is the iteration number. If zk=\u03b2k\u2212 1L\u2207l(\u03b2 k), then F (\u03b2k)\u2212F (\u03b2 \u2217) \u2264 \u03b1L\u2016\u03b2 0\u2212\u03b2\u2217\u201622 2k , where F (\u00b7) is the objective of (1). If zk = yk \u2212 1L\u2207l(y k) where yk = \u03b2k + \u03b1k(\u03b2k \u2212 \u03b2k\u22121) with \u03b1 controlling the momentum, we have F (\u03b2k)\u2212 F (\u03b2 \u2217) \u2264 2\u03b1L\u2016\u03b20\u2212\u03b2 \u2217\u201622\n(k+1)2\nThe proof can be viewed as an instantiation of the convex analysis introduced in (Nesterov and Nesterov 2004). We provide a rigorous proof in the supplementary file.\nNow the key to solve (1) is how efficiently we solve (2). If there were no constraints, (2) is the fused lasso signal approximation proposed in (Friedman et al. 2007), where it was shown that by utilizing the separability of the l1 norm, an element-wise soft-threshold technique can be applied to remove the sparse term. Since the constraints of (2) are also separable, we show how (2) can be further reduced likewise. Proposition 2. If we define\n\u03b2\u0303 = min \u03b2\u2208Rd\n1 2 \u2016\u03b2 \u2212 z\u201622 + \u03bb2 L \u2211 (i,j)\u2208E wij |\u03b2i \u2212 \u03b2j |, (3)\nthen the optimal solution to (2) (denoted as \u03b2\u2217) can be achieved by an element-wise post-processing to \u03b2\u0303 as follows\n\u03b2\u2217 = max(sign(\u03b2\u0303) max(|\u03b2\u0303| \u2212 \u03bb1 L ,0),0); (4)\nwhere is an element-wise product operator.\nProof. We define \u03b8i = \u03bb1L and \u03b8ij = \u03bb2wij L respectively for all i \u2208 V and (i, j) \u2208 E. We denote \u03b2\u2032 as the optimal solution of the unconstrained problem of (2). According to (Friedman et al. 2007), \u03b2\u2032i = sign(\u03b2\u0303i) max(|\u03b2\u0303i| \u2212 \u03b8i, 0). We now consider the nonnegative constraints in (2). According to the Karush-Kuhn-Tucker (KKT) conditions, the necessary and sufficient conditions for \u03b2\u22171 , ...\u03b2 \u2217 d are\nLgi = (\u03b2i \u2212 zi) + \u03b8isi + \u2211\nj:(i,j)\u2208E\n\u03b8ijtij\u2212\n\u2211 j:(j,i)\u2208E \u03b8ijtji \u2212 \u03b1i = 0, s.t. \u03b1i\u03b2i = 0, (5)\nwhere \u03b1i \u2265 0 are the Lagrange multipliers and s, t are subgradients: si = sign(\u03b2i) if \u03b2i 6= 0 and si \u2208 [\u22121, 1] if \u03b2i = 0; tij = sign(\u03b2i \u2212 \u03b2j) for \u03b2i 6= \u03b2j and tij \u2208 [\u22121, 1] if \u03b2i = \u03b2j . The objective equation in (5) is to set the derivative of the Lagrange function to zero and the constraint equations are obtained from the complementary slackness condition. We now consider two cases of \u03b2: Case 1 \u03b2\u2032i \u2265 0: Note that by setting \u03b1i = 0, \u03b2\u2032i \u2265 0 satisfies the conditions in (5), thus \u03b2\u2217i = \u03b2 \u2032 i = sign(\u03b2\u0303i) max(|\u03b2\u0303i| \u2212 \u03b8i, 0) is the solution of (2). Case 2 \u03b2\u2032i < 0: We can set \u03b2\u2217i = 0 and \u03b1i = \u2212\u03b2\u2032i > 0, then we have \u03b2\u2032i = \u03b2 \u2217 i \u2212 \u03b1i.\nLgi = (\u03b2\u2217i \u2212 zi) + \u03b8isi + \u2211\nj:(i,j)\u2208E\n\u03b8ijtij \u2212 \u2211\nj:(j,i)\u2208E\n\u03b8ijtji \u2212 \u03b1i\n= (\u03b2\u2217i \u2212 \u03b1i \u2212 zi) + \u03b8isi + \u2211\nj:(i,j)\u2208E\n\u03b8ijtij \u2212 \u2211\nj:(j,i)\u2208E\n\u03b8ijtji\n= (\u03b2\u2032i \u2212 zi) + \u03b8isi + \u2211\nj:(i,j)\u2208E\n\u03b8ijtij \u2212 \u2211\nj:(j,i)\u2208E\n\u03b8ijtji = 0.\nHence, in summary, we have\n\u03b2\u2217 = max(sign(\u03b2\u0303) max(|\u03b2\u0303| \u2212 \u03bb1 L ,0),0).\nNotice that, (3) is a (continous) total variation problem, which is known can be efficiently solved by parametric flow algorithms in (Chambolle and Darbon 2009; Xin et al. 2014). Here we present a more general perspective of such an equivalence via conic duality, which gives us a natural and novel minimum quadratic cost flow formulation. Fast flow algorithms, such as but not limited to parametric flow (Gallo, Grigoriadis, and Tarja 1989) etc. are then easily applied. For example, we show that the minimum norm point problem solved by parametric flow in (Xin et al. 2014) can be viewed as a special case of the proposed dual."}, {"heading": "Conic Dual to Total Variation", "text": "To solve (3), we apply generalized inequalities and its corresponding Lagrange duality introduced in (Boyd and Vandenberghe 2004). Specifically, we first define a set C such that C={(\u03b2, \u03b1) \u2208 Rd+1 | \u2200(i, j), |\u03b2i \u2212 \u03b2j | \u2264 \u03b1}. Lemma 3. The set C is a proper cone.\nThis can be easily shown by checking all the properties required by a proper cone. See the supplementary file for a proof. We now consider the following problem (we keep using \u03b8ij = \u03bb2wkiL for \u03bb2wki L in (3)):\nmin \u2200(i,j)\u2208E,{\u03b2ij\u2208Rd,\u03b1ij\u2208R}\n1 2 \u2016\u03b2 \u2212 z\u201622 + \u2211 (i,j)\u2208E \u03b8ij\u03b1 ij\ns.t. (\u03b2ij , \u03b1ij) \u2208 C and \u03b2ijk = { \u03b2k k = i, j\n0 else .\n(6)\nSince (\u03b2ij , \u03b1ij) \u2208 C, then |\u03b2i \u2212 \u03b2j | \u2264 \u03b1ij , therefore (6) is indeed equivalent to (3). Moreover, because C is a proper cone, we can rewrite (6) as follows,\nmin \u2200(i,j)\u2208E,{\u03b2ij\u2208Rd,\u03b1ij\u2208R}\n1 2 \u2016\u03b2 \u2212 z\u201622 + \u2211 (i,j)\u2208E \u03b8ij\u03b1 ij\ns.t.\n[ \u03b2ij\n\u03b1ij\n] C 0 and \u03b2ijk = { \u03b2k k = i, j\n0 else .\n(7)\nwhere \u03b2 C 0 \u21d0\u21d2 \u03b2 \u2208 C is defined as generalized inequality (Boyd and Vandenberghe 2004). We call (7) the primal problem (which equals to the original TV problem). Since the primal problem is both convex and satisfies Slater\u2019s condition, strong Lagrange duality holds (under generalized inequality). We define the Lagrange function as\nL(\u03b2, \u03b1, \u03be, \u03c4)\n= 1\n2 \u2016\u03b2 \u2212 z\u201622 + \u2211 (i,j) \u03b8ij\u03b1 ij \u2212 \u2211 ij [ \u03beij \u03c4 ij ]T [ \u03b2ij \u03b1ij ]\ns.t. \u03beij \u2208 Rd : [ \u03beij\n\u03c4 ij\n] C\u2217 0 and \u03beijk = 0 if k 6= i, j, (8)\nwhere (\u03beij , \u03c4 ij) are the Lagrange multipliers and C\u2217 is the dual cone of C, defined as C\u2217 = {v |wTv \u2265 0, \u2200w \u2208 C}. To formulate the dual problem, we take the derivative ofL(\u00b7) with respect to the primal variables (\u03b2, \u03b1) and we have\n\u03b2 \u2212 z\u2212 \u2211 ij \u03beij = 0 and \u03b8ij \u2212 \u03c4 ij = 0. (9)\nBy applying (9) to (8), the dual problem is written as\nmax \u2200(i,j)\u2208E,{\u03beij\u2208Rd} \u22121 2 \u2016z + \u2211 (i,j)\u2208E \u03beij\u201622 + 1 2 \u2016z\u201622\ns.t. (\u03beij , \u03c4 ij) \u2208 C\u2217; \u03beijk = 0, k 6= i, j. (10)\nProposition 4. (\u03beij , \u03c4 ij) \u2208 C\u2217, \u03beijk = 0, k 6= i, j \u21d0\u21d2 \u03beiji + \u03be ij j = 0, |\u03be ij i | \u2264 \u03b8ij .\nPlease find the proof in the supplementary file. Accordingly, the dual problem becomes\nmin \u2200(i,j)\u2208E,{\u03beij\u2208Rd}\n1 2 \u2016z\u2212 \u2211 (i,j)\u2208E \u03beij\u201622\ns.t. \u03beijk = 0, k 6= i, j, \u03be ij i + \u03be ij j = 0, |\u03be ij i | \u2264 \u03b8ij ,\n(11)\nwhere we omit \u2016z\u201622 from the objective since it is a constant with respect to \u03bes and we also changed the sign of all \u03bes for better illustration of the flows.\nProblem (11) can be viewed as the following minimum quadratic cost flow formulation,\nmin \u2200(i,j)\u2208E,{\u03beij\u2208Rd}\n1 2 \u2016y \u2212 ( \u2211 (i,j)\u2208E \u03beij + \u03b3)\u201622\ns.t. \u03beijk = 0, k 6= i, j, \u03be ij i + \u03be ij j = 0, |\u03be ij i | \u2264 \u03b8ij ,\n(12)\nwhere we have induced \u03b3 (\u03b3i = max {|zi|, \u2211 j,(i,j)\u2208E \u03b8ij}) and denote y=z + \u03b3 to ensure y \u2265 0 and (\u03beij + \u03b3) \u2265 0. Thus each feasible \u03be of (12) is a possible flow on graph G=(V,E). Since \u03beiji + \u03be ij j = 0, |\u03beij | can denote a flow on edge (i,j) such that \u03beiji \u2265 0 denotes a flow coming into node i and \u03beiji \u2264 0 denotes a flow leaving node i. Figure 1 illustrates such flows by taking node 2 as an example. Thus to minimize the objective of (12) is equivalent to computing a minimum cost flow on this graph. Since the cost is quadratic with respect to the flow, this problem is a minimum quadratic cost flow problem. According to (Hochbaum\nand Hong 1995; Mairal et al. 2011), this type of problems can be efficiently solved via fast flow algorithms including but not limited to the parametric flow (Gallo, Grigoriadis, and Tarja 1989). Note that in (Xin et al. 2014), TV is shown equivalent to a minimum norm point (MNP) problem under submodular constraints which is solved via parametric flow. We now discuss the relation between the dual problem i.e. (12) or (11) and the MNP considered in (Xin et al. 2014).\nRecall that the MNP problem is defined as follows\nmin s\u2208Rd,s\u2208B(\u03bbfc)\n\u2016z\u2212 s\u201622, (13)\nwhere fc(S) is a cut function, defined as fc(S) =\u2211 i\u2208S,j\u2208V\\S wij and B(\u00b7) is the base polyhedron of fc. Proposition 5. For any minimizer \u03be\u2217 of (11), define s\u0302 such that s\u0302 = \u2211 (i,j)\u2208E \u03be\n\u2217ij , then s\u0302 is a minimizer of (13). For any minimizer s\u2217 of (13), there exists a decomposition such that s\u2217= \u2211 (i,j)\u2208E \u03be\u0302 ij , where \u03be\u0302 is one minimizer of (11).\nAccording to Prop. 5, the MNP problem i.e. (13) can be viewed as a special case of (11) (the conic dual), where \u2211 (i,j)\u2208E \u03be\nij=s. Moreover, since (11) has relatively \u201clooser\u201d constraints, it is possible to devise more efficient algorithms (than parametric flow) to solve (11) and thereafter TV. For example, in (Mairal et al. 2011), a faster (than parametric flow) flow algorithm is proposed to solve their specific minimum quadratic flow problem. Hence, the conic dual perspective opens a new opportunity to solve the famous TV problem more efficiently.\nOptimization summary. In summary, by applying Prop. 1, we can solve n2GFL by iteratively solving (2). By applying Prop. 2, we further reduce (2) to the TV problem defined in (3), we then transform it to a minimum cost flow algorithm via conic duality and solve it by a fast flow algorithm.\nIn Tab. 1, we compare the proposed algorithm with an off-the-shelf solver on synthetic data. We generate a random \u03b2 \u2208 Rd and a 2D grid graph of d nodes with each node having four neighbors. We then generate N = d/2 samples: xi \u2208 Rd and yi = \u03b2Txi + 0.01ni, where xi and ni are drawn from the standard normal distribution. All experiments are carried out on an Intel(R) Core(TM) i7-3770 CPU at 3.40GHz. The experiments show that the proposed optimization algorithm is more efficient and scalable."}, {"heading": "Application to the Diagnosis of AD", "text": "In the diagnosis of AD, two fundamental issues are AD/NC (Normal Control) classification and MCI/NC (Mild Cognitive Impairment) classification. Let xi \u2208 Rd be sub-\njects\u2019 sMRI voxels and yi = {\u22121, 1} be the disease status (AD/NC or MCI/NC). Since the problems are classifications, we use the logistic regression as the loss term\nl(\u03b2) = N\u2211 i=1 log (1 + exp (\u2212yi(\u03b2Txi + c))), (14)\nwhere c \u2208 R is the bias parameter (to be learned). For the graph structure, we define each voxel as a node and their spatial adjacency as the edges, i.e. wij=1 if voxels i and j are adjacent and 0 otherwise. The data are obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database2. We split all the baseline data into 1.5T and 3.0T MRI scans datasets (named 15T and 30T). 64 AD patients, 90 NC and 208 MCI patients are included in our 15T dataset; 66 AD patients and 110 NC are included in our 30T dataset. (Most 30T MCI data are in an on-going phase and are not included). Data preprocessing follows the DARTEL VBM pipeline (Ashburner and others 2007) as commonly done in the literature. 2,527 8\u00d78\u00d78 mm3 size voxels that have values greater than 0.2 in the mean gray matter population template serve as the input features. We design experiments on three tasks, namely, 15ADNC, 30ADNC, 15MCINC.\nClassification Accuracy. 10-fold cross-validation (CV) evaluation is applied and the classification accuracy for all tasks are summarized in Tab. 2. Under exactly the same experiment setup, we compare n2GFL with the state-of-theart classifiers: logistic regression (LR), SVM, sparse models\n2http://adni.loni.ucla.edu\ne.g. the lasso and its graph Laplacian structured variants, i.e. the LapL, the unconstrained GFL (Xin et al. 2014), and the \u201cMLDA\u201d model (Dai et al. 2012), which applies a variant of Fisher Discriminant Analysis after univariate feature selection (via T-test). For each model, we used grid-search to find the optimal parameters respectively. Note that our accuracies may not be superior to the recent work (Liu et al. 2014), the main reason is that in (Liu et al. 2014), multi-modality data (including PET and sMRI data) are used. Nevertheless, Tab. 2 demonstrates that n2GFL outperforms all the other models using only voxel-based sMRI data.\nFeature selection. For each task, the selected features are those whose \u03b2 are not zero . In Figure 2, the result of 30ADNC is used to illustrate the feature selection by different models (using the parameters at their best accuracy). As shown, the selected voxels by both GFL and n2GFL cluster into several spatially connected regions, whereas those of lasso and T-test/MLDA scatter around. Also, as mentioned before, the LapL tends to select much more voxels than nec-\nessary due to the l2 regularization. Moreover, the selected voxels by GFL and n2GFL are concentrated in Hippocampus, ParaHippocampal gyrus (which are believed to be the early damaged regions). On the other hand, the lasso and Ttest/MLDA either select less lesion voxels or select probably noisy voxels not in the early damaged regions.\nFeature Stability. In Figure 3, we show the selected voxels across different folds of CV3. As shown, the selected voxels by lasso vary much across different folds, whereas the selected voxels by GFL are more stable. However, by assuming the positive correlation between the features and the disease labels in n2GFL, we further increase the stability. To quantitatively evaluate the stability gain, we denote the variables of the kth fold of CV as \u03b2(k). We introduce two measurements here. In (Yu 2013), the Estimation Stability (ES) is proposed to measure the stability of the estimation\nES = K\u2211 k=1 \u2016X\u03b2(k)\u2212X\u03b2\u0304\u201622/K\u2016\u03b2\u0304\u201622, (15)\nwhere \u03b2\u0304 = \u2211K k=1 \u03b2(k)/K. It is shown in (Yu 2013) that ES is a fair measurement of the estimation stability. To further understand the stability of feature selection, we also extend the Dice coefficient (Dice 1945) to multiple sets and apply the multi-set Dice Coefficient (mDC) as a measurement.\n3Here, parameters were determined by accuracy. Similar results were observed using parameters producing same level of sparsity.\nWe denote set S(k) = {i : \u03b2i(k) 6= 0} and define mDC as\nmDC = K#(\u2229Kk=1S(k))/ K\u2211 k=1 #(S(k)), (16)\nwhere # is the number of elements in a set. In Tab. 3, both measurements quantitatively suggest n2GFL obtains much more stable voxels due to the consideration of the correlation between the features and the disease labels 4."}, {"heading": "Conclusions", "text": "In this paper, we explore the nonnegative generalized fused lasso model to address an important problem of neuroimage analysis, i.e. the stability of feature selection. Experiments show that our model greatly improves the stabilities\n4We notice that, in (Xin et al. 2014), the stability is computed using the top 50 positive voxels because these voxels are believe to be the most atrophied ones. By computing the stability of all non-zero voxels, the mDC of GFL drops around 30%. This clearly shows that the instability is caused largely by the undesirable voxels that disagree with the correlation prior (those scattered blue voxels in the mid row).\nof feature selection over existing methods for brain image analysis. Although n2GFL is applied to the diagnosis of AD problem, it can be applied to solve more general problems. Moreover, we believe that the theoretical points made here e.g. nonnegative FISTA, soft-thresholding and the conic dual of TV, provide motivation for future work of general interest.\nAcknowledgments This work was supported in part by Natural Science Foundation of China (NSFC) grants 973-2015CB351800, NSFC61272027, NSFC-61231010, NSFC-61121002 and NSFC61210005."}], "references": [{"title": "A fast diffeomorphic image registration algorithm", "author": ["Ashburner", "J others 2007] Ashburner"], "venue": null, "citeRegEx": "Ashburner and Ashburner,? \\Q2007\\E", "shortCiteRegEx": "Ashburner and Ashburner", "year": 2007}, {"title": "R", "author": ["B.B. Avants", "D.J. Libon", "K. Rascovsky", "A. Boller", "C.T. McMillan", "L. Massimo", "H. Coslett", "A. Chatterjee", "Gross"], "venue": "G.; and Grossman, M.", "citeRegEx": "Avants et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Teboulle", "author": ["A. Beck"], "venue": "M.", "citeRegEx": "Beck and Teboulle 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Vandenberghe", "author": ["S.P. Boyd"], "venue": "L.", "citeRegEx": "Boyd and Vandenberghe 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Darbon", "author": ["A. Chambolle"], "venue": "J.", "citeRegEx": "Chambolle and Darbon 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative analysis of early alzheimer\u2019s disease using multi-modal imaging and multilevel characterization with multi-classifier (m3)", "author": ["Dai"], "venue": null, "citeRegEx": "Dai,? \\Q2012\\E", "shortCiteRegEx": "Dai", "year": 2012}, {"title": "L", "author": ["Dice"], "venue": "R.", "citeRegEx": "Dice 1945", "shortCiteRegEx": null, "year": 1945}, {"title": "Pathwise coordinate optimization. The Annals of Applied Statistics 1(2):302\u2013332", "author": ["Friedman"], "venue": null, "citeRegEx": "Friedman,? \\Q2007\\E", "shortCiteRegEx": "Friedman", "year": 2007}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["Grigoriadis Gallo", "G. Tarja 1989] Gallo", "M. Grigoriadis", "R. Tarja"], "venue": "SIAM Journal of Computing", "citeRegEx": "Gallo et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Gallo et al\\.", "year": 1989}, {"title": "Identifying predictive regions from fmri with tv-l1 prior", "author": ["Thirion Gramfort", "A. Varoquaux 2013] Gramfort", "B. Thirion", "G. Varoquaux"], "venue": "In Pattern Recognition in Neuroimaging (PRNI),", "citeRegEx": "Gramfort et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gramfort et al\\.", "year": 2013}, {"title": "and Boyd", "author": ["M. Grant"], "venue": "S.", "citeRegEx": "Grant and Boyd 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "J", "author": ["L. Grosenick", "B. Klingenberg", "K. Katovich", "B. Knutson", "Taylor"], "venue": "E.", "citeRegEx": "Grosenick et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Hong", "author": ["D.S. Hochbaum"], "venue": "S.", "citeRegEx": "Hochbaum and Hong 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Group lasso with overlap and graph lasso", "author": ["Obozinski Jacob", "L. Vert 2009] Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Multiscale mining of fmri data with hierarchical structured sparsity", "author": ["Jenatton"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "Jenatton,? \\Q2012\\E", "shortCiteRegEx": "Jenatton", "year": 2012}, {"title": "H", "author": ["D.D. Lee", "Seung"], "venue": "S.", "citeRegEx": "Lee and Seung 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Inter-modality relationship constrained multimodality multi-task feature selection for alzheimer\u2019s disease and mild cognitive impairment identification", "author": ["Liu"], "venue": null, "citeRegEx": "Liu,? \\Q2014\\E", "shortCiteRegEx": "Liu", "year": 2014}, {"title": "Ensemble sparse classification of alzheimer\u2019s disease", "author": ["Zhang Liu", "M. Shen 2012] Liu", "D. Zhang", "D. Shen"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Convex and network flow optimization for structured sparsity", "author": ["Mairal"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Mairal,? \\Q2011\\E", "shortCiteRegEx": "Mairal", "year": 2011}, {"title": "and Nesterov", "author": ["Y. Nesterov"], "venue": "I.", "citeRegEx": "Nesterov and Nesterov 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Abugharbieh", "author": ["B. Ng"], "venue": "R.", "citeRegEx": "Ng and Abugharbieh 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "T", "author": ["N. Rao", "C. Cox", "R. Nowak", "Rogers"], "venue": "T.", "citeRegEx": "Rao et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient generalized fused lasso and its application to the diagnosis of alzheimers disease", "author": ["Xin"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Xin,? \\Q2014\\E", "shortCiteRegEx": "Xin", "year": 2014}, {"title": "B", "author": ["Yu"], "venue": "2013. Stability. Bernoulli 19(4):1484\u2013", "citeRegEx": "Yu 2013", "shortCiteRegEx": null, "year": 1500}], "referenceMentions": [], "year": 2015, "abstractText": "Neuroimage analysis usually involves learning thousands or even millions of variables using only a limited number of samples. In this regard, sparse models, e.g. the lasso, are applied to select the optimal features and achieve high diagnosis accuracy. The lasso, however, usually results in independent unstable features. Stability, a manifest of reproducibility of statistical results subject to reasonable perturbations to data and the model (Yu 2013), is an important focus in statistics, especially in the analysis of high dimensional data. In this paper, we explore a nonnegative generalized fused lasso model for stable feature selection in the diagnosis of Alzheimer\u2019s disease. In addition to sparsity, our model incorporates two important pathological priors: the spatial cohesion of lesion voxels and the positive correlation between the features and the disease labels. To optimize the model, we propose an efficient algorithm by proving a novel link between total variation and fast network flow algorithms via conic duality. Experiments show that the proposed nonnegative model performs much better in exploring the intrinsic structure of data via selecting stable features compared with other state-of-the-arts. Introduction Neuroimage analysis is challenging due to its high feature dimensionality and data scarcity. Sparse models such as the lasso (Tibshirani 1996) have gained great reputation in statistics and machine learning, and they have been applied to the analysis of such high dimensional data by exploiting the sparsity property in the absence of abundant data. As a major result, automatic selection of relevant variables/features by such sparse formulation achieves promising performance. For example, in (Liu, Zhang, and Shen 2012), the lasso model was applied to the diagnosis of Alzheimer\u2019s disease (AD) and showed better performance than the support vector machine (SVM), which is one of the state-of-the-arts in brain image classification. However, in statistics, it is known that the lasso does not always provide interpretable results because of its instability (Yu 2013). \u201cStability\u201d here means the reproducibility of statistical results subject to reasonable perturbations to data and Copyright c \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the model. (These perturbations include the often used Jacknife, bootstrap and cross-validation.) This unstable behavior of the lasso model is critical in high dimensional data analysis. The resulting irreproducibility of the feature selection are especially undesirable in neuroimage analysis/diagnosis. However, unlike the problems such as registration and classification, the stability issue of feature selection is much less studied in this field. In this paper we propose a model to induce more stable feature selection from high dimensional brain structural Magnetic Resonance Imaging (sMRI) images. Besides sparsity, the proposed model harnesses two important additional pathological priors in brain sMRI: (i) the spatial cohesion of lesion voxels (via inducing fusion terms) and (ii) the positive correlation between the features and the disease labels. The correlation prior is based on the observation that in many brain image analysis problems (such as AD, frontotemporal dementia, corticobasal degeneration, etc), there exist strong correlations between the features and the labels. For example, gray matter of AD is degenerated/atrophied. Therefore, the gray matter values (indicating the volume) are positively correlated with the cognitive scores or disease labels {-1,1}. That is, the less gray matter, the lower the cognitive score. Accordingly, we propose nonnegative constraints on the variables to enforce the prior and name the model as \u201cnon-negative Generalized Fused Lasso\u201d (nGFL). It extends the popular generalized fused lasso and enables it to explore the intrinsic structure of data via selecting stable features. To measure feature stability, we introduce the \u201cEstimation Stability\u201d recently proposed in (Yu 2013) and the (multi-set) Dice coefficient (Dice 1945). Experiments demonstrate that compared with existing models, our model selects much more stable (and pathological-prior consistent) voxels. It is worth mentioning that the non-negativeness per se is a very important prior of many practical problems, e.g. (Lee and Seung 1999). Although nGFL is proposed to solve the diagnosis of AD in this work, the model can be applied to more general problems. Incorporating these priors makes the problem novel w.r.t the lasso or generalized fused lasso from an optimization standpoint. Although off-the-shelf convex solvers such as CVX (Grant and Boyd 2013) can be applied to solve the optimization, it hardly scales to high-dimensional problems in feasible time. In this regard, we propose an efficient algoar X iv :1 50 3. 07 50 8v 1 [ cs .L G ] 2 5 M ar 2 01 5 rithm that solves the nGFL problem exactly. We generalize the proximal gradient methods (such as FISTA) (Beck and Teboulle 2009) to solve our constrained optimization and prove its convergence. We then show that by using an element-wise post-processing, the resulting proximal operator can be reduced to the total variation (TV) problem. It is known that TV can be solved by parametric flow algorithms (Chambolle and Darbon 2009; Xin et al. 2014). In the present study, we provide a novel equivalence via conic duality, which gives us a minimum quadratic cost flow formulation (Hochbaum and Hong 1995). Fast flow algorithms (including parametric flow) are then easily applied. In practice, our algorithm runs hundreds of times faster than CVX at the same precision and can scale to high-dimensional problems. Related work. In addition to sparsity, people leverage underlying data structures and introduce stronger priors such as the structured sparsity (Jacob, Obozinski, and Vert 2009) to increase model stability. However, for voxel-based sMRI data analysis, handcrafted grouping of the voxels or sub-structures may not coincide with various pathological topology priors. Consequently, group lasso (with overlap) (Jacob, Obozinski, and Vert 2009; Jenatton et al. 2012; Rao et al. 2013) is not an ideal model to the problem. In contrast, the graph-based structured sparse models adapt better to such a situation. The most popular one is referred here as LapL1, which adopts l2 norm regularization of neighborhood variable difference (e.g. (Ng and Abugharbieh 2011; Grosenick et al. 2013)). However, as we will show in the experiments, these models select many more features than necessary. Very recently, generalized fused lasso or total variation has been successful applied to brain image analysis problems inducing the l1 difference (Gramfort, Thirion, and Varoquaux 2013; Xin et al. 2014). In the experiments, we show that by including an extra nonnegative constraint, the features selected by our model is much more stable than that of such unconstrained models. A very recent work (Avants et al. 2014) also explored this positive correlation (partially supporting our assumption), but the problem formulation was quite different: neither structural assumption was considered, nor the stability of feature selection was discussed. From the optimization standpoint, the applied framework is similar to that of (Xin et al. 2014) but two key differences exist: (1) the FISTA and soft-thresholding process applied in (Xin et al. 2014) do not generalize to constrained optimization problems, we show important modifications and provide theoretical proof; (2) we propose a novel understanding of TV\u2019s relation with flow problems via conic duality and prove that the minimum norm point problem of (Xin et al. 2014) is a special case of our framework. The Proposed Method Nonnegative Generalized Fused Lasso (n2GFL) Let {(xi, yi)}i=1 be a set of samples, where xi \u2208 R and yi \u2208 R are features and labels, respectively. Also, we denote Although different names are given in e.g. (Ng and Abugharbieh 2011; Grosenick et al. 2013), they are in fact fundamentally applying the graph Laplacian smoothing. by X \u2208 Rd\u00d7N and y \u2208 R the concatenations of xi and yi. Then, we consider the formulation min \u03b2\u2208Rd l(\u03b2;X,y) + \u03bb1 d \u2211", "creator": "LaTeX with hyperref package"}}}