{"id": "1504.00028", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2015", "title": "Real-World Font Recognition Using Deep Network and Domain Adaptation", "abstract": "we introduced similarly challenging fine - edged imaging problem : recognizing a font tree from an extract of text. in this task, it is very easy to generate lots of rendered font examples but very hard candidates obtain real - world labeled images. changing real - to - synthetic domain gap caused poor generalization to new real data in previous methods ( chen et al. ( 2014 ) ). in this paper, we refer to convolutional neural networks, by use fourier adaptation technique based on random stacked convolutional auto - encoder that exploits unlabeled real - sized images combined with synthetic pixels. the proposed method achieves an accuracy of higher than 13 % ( top - 5 ) on a real - world dataset.", "histories": [["v1", "Tue, 31 Mar 2015 20:30:00 GMT  (81kb,D)", "http://arxiv.org/abs/1504.00028v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["zhangyang wang", "jianchao yang", "hailin jin", "eli shechtman", "aseem agarwala", "jonathan brandt", "thomas s huang"], "accepted": true, "id": "1504.00028"}, "pdf": {"name": "1504.00028.pdf", "metadata": {"source": "CRF", "title": "REAL-WORLD FONT RECOGNITION USING DEEP NET-", "authors": ["Zhangyang Wang", "Thomas S. Huang"], "emails": ["t-huang1}@illinois.edu", "jbrandt}@adobe.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This paper studies font recognition, i.e. identifying a particular typeface given an image of a text fragment. To apply machine learning to this problem, we require realistic text images with ground truth font labels. However, such data is scarce and expensive to obtain, since it requires a high level of domain expertise which is out of reach of most people. Therefore, it is infeasible to collect a sufficient set of real-world training images. One way to overcome the training data challenge is to synthesize the training set by rendering text fragments for all the necessary fonts. However, we must face the domain mismatch between synthetic and real-world text images (Chen et al. (2014)). Characters in real-world images are spaced, stretched and distorted in numerous ways. In (Chen et al. (2014)), the authors tried to overcome this difficulty by adding different degradations to synthetic data. In the end, introducing all possible real-world degradations into the training data is infeasible.\nWe address this domain mismatch problem in font recognition, by further leveraging a large corpus of synthetic data to train a Convolutional Neural Network (CNN), while introducing an adaptation technique based on Stacked Convolutional Auto-Encoder (SCAE) with the help of unlabeled realworld images. The proposed method reaches an impressive performance on real-world test images."}, {"heading": "2 MODEL", "text": "Our basic CNN architecture is similar to the popular ImageNet CNN structure in (Krizhevsky et al. (2012)), as depicted in Fig. 1. The numbers along with the network pipeline specify the dimensions of outputs of corresponding layers. When the CNN model trained fully on a synthetic dataset, it witnesses a significant performance drop when testing on real-world data, compared to when applied to another synthetic validation set. This also happens with other models such as in Chen et al. (2014), which uses training and testing sets of similar properties to ours. This alludes to discrepancies between the distributions of synthetic and and real-world examples.\nar X\niv :1\n50 4.\n00 02\n8v 1\n[ cs\n.C V\n] 3\n1 M\nTradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts lowlevel features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE). We extend the method in Glorot et al. (2011) to decompose the N basic CNN layers into two sub-network parts. The first K layers accounts for extracting low-level visual features shared by both synthetic and real-world data, and will be learned in a unsupervised way, using unlabeled data from both domains. The remaining N \u2212K layers accounts for learning higher-level discriminative features for classification. It will be trained in a supervised way on top of the first part, using labeled data from the synthetic domain only.\nTo train the first K layers, we exploit a Stacked Convolutional Auto-Encoder (SCAE) (Masci et al. (2011)). Its first two convolutional layers have an identical topology to the first two layers in Fig. 1. Moreover, we set its first and second half to be mirror-symmetrical. The cost function is the mean squared error (MSE) between the input and reconstructed patches. After SCAE is learned, its Conv. Layers 1 and 2 are imported to the CNN in Fig. 1. We adopt the SCAE implementation by Paine et al. (2014).\nWe also find that applying label-preserving data augmentations to synthetic training data helps reduce the domain mismatch. Chen et al. (2014) added moderate distortions and corruptions, including noise, blur, rotations and shading effects. In addition, we also vary the character spacings and aspect ratios when rendering training data. Note that these steps are not useful for the method in Chen et al. (2014) because it exploits very localized features, but they are very helpful in our case."}, {"heading": "3 EXPERIMENTS", "text": "We implemented and evaluated the local feature embedding-based algorithm (LFE) in (Chen et al. (2014)) as a baseline, and compare it with our model. A SCAE is first trained on a large collection of both synthetic data and unlabeled real world data, and then exports the first K = 2 convolutional layers. The next N \u2212 K layers are trained on labeled synthetic data covering 2,383 classes. That makes our problem quite fine-grain. Testing is conducted on the the VFRWild325 dataset used by (Chen et al. (2014)), in term of top-1 and top-5 classification errors. Our model achieves 38.15% in top-1 error and 20.62% in top-5, which outperforms 6% and 10% over LFE, respectively."}], "references": [{"title": "Large-scale visual font recognition", "author": ["G. Chen", "J. Yang", "H. Jin", "J. Brandt", "E. Shechtman", "A. Agarwala", "T.X. Han"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "In Proceedings of ICANN,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["T. Paine", "P. Khorrami", "W. Han", "T.S. Huang"], "venue": "arXiv preprint arXiv:1412.6597,", "citeRegEx": "Paine et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paine et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "This realto-synthetic domain gap caused poor generalization to new real data in previous methods (Chen et al. (2014)).", "startOffset": 98, "endOffset": 117}, {"referenceID": 0, "context": "However, we must face the domain mismatch between synthetic and real-world text images (Chen et al. (2014)).", "startOffset": 88, "endOffset": 107}, {"referenceID": 0, "context": "However, we must face the domain mismatch between synthetic and real-world text images (Chen et al. (2014)). Characters in real-world images are spaced, stretched and distorted in numerous ways. In (Chen et al. (2014)), the authors tried to overcome this difficulty by adding different degradations to synthetic data.", "startOffset": 88, "endOffset": 218}, {"referenceID": 1, "context": "Our basic CNN architecture is similar to the popular ImageNet CNN structure in (Krizhevsky et al. (2012)), as depicted in Fig.", "startOffset": 80, "endOffset": 105}, {"referenceID": 0, "context": "This also happens with other models such as in Chen et al. (2014), which uses training and testing sets of similar properties to ours.", "startOffset": 47, "endOffset": 66}, {"referenceID": 0, "context": "Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)).", "startOffset": 114, "endOffset": 133}, {"referenceID": 0, "context": "Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts lowlevel features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE).", "startOffset": 114, "endOffset": 188}, {"referenceID": 0, "context": "Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts lowlevel features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE). We extend the method in Glorot et al. (2011) to decompose the N basic CNN layers into two sub-network parts.", "startOffset": 114, "endOffset": 354}, {"referenceID": 0, "context": "Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts lowlevel features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE). We extend the method in Glorot et al. (2011) to decompose the N basic CNN layers into two sub-network parts. The first K layers accounts for extracting low-level visual features shared by both synthetic and real-world data, and will be learned in a unsupervised way, using unlabeled data from both domains. The remaining N \u2212K layers accounts for learning higher-level discriminative features for classification. It will be trained in a supervised way on top of the first part, using labeled data from the synthetic domain only. To train the first K layers, we exploit a Stacked Convolutional Auto-Encoder (SCAE) (Masci et al. (2011)).", "startOffset": 114, "endOffset": 942}, {"referenceID": 0, "context": "Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts lowlevel features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE). We extend the method in Glorot et al. (2011) to decompose the N basic CNN layers into two sub-network parts. The first K layers accounts for extracting low-level visual features shared by both synthetic and real-world data, and will be learned in a unsupervised way, using unlabeled data from both domains. The remaining N \u2212K layers accounts for learning higher-level discriminative features for classification. It will be trained in a supervised way on top of the first part, using labeled data from the synthetic domain only. To train the first K layers, we exploit a Stacked Convolutional Auto-Encoder (SCAE) (Masci et al. (2011)). Its first two convolutional layers have an identical topology to the first two layers in Fig. 1. Moreover, we set its first and second half to be mirror-symmetrical. The cost function is the mean squared error (MSE) between the input and reconstructed patches. After SCAE is learned, its Conv. Layers 1 and 2 are imported to the CNN in Fig. 1. We adopt the SCAE implementation by Paine et al. (2014). We also find that applying label-preserving data augmentations to synthetic training data helps reduce the domain mismatch.", "startOffset": 114, "endOffset": 1344}, {"referenceID": 0, "context": "Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts lowlevel features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE). We extend the method in Glorot et al. (2011) to decompose the N basic CNN layers into two sub-network parts. The first K layers accounts for extracting low-level visual features shared by both synthetic and real-world data, and will be learned in a unsupervised way, using unlabeled data from both domains. The remaining N \u2212K layers accounts for learning higher-level discriminative features for classification. It will be trained in a supervised way on top of the first part, using labeled data from the synthetic domain only. To train the first K layers, we exploit a Stacked Convolutional Auto-Encoder (SCAE) (Masci et al. (2011)). Its first two convolutional layers have an identical topology to the first two layers in Fig. 1. Moreover, we set its first and second half to be mirror-symmetrical. The cost function is the mean squared error (MSE) between the input and reconstructed patches. After SCAE is learned, its Conv. Layers 1 and 2 are imported to the CNN in Fig. 1. We adopt the SCAE implementation by Paine et al. (2014). We also find that applying label-preserving data augmentations to synthetic training data helps reduce the domain mismatch. Chen et al. (2014) added moderate distortions and corruptions, including noise, blur, rotations and shading effects.", "startOffset": 114, "endOffset": 1488}, {"referenceID": 0, "context": "Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts lowlevel features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE). We extend the method in Glorot et al. (2011) to decompose the N basic CNN layers into two sub-network parts. The first K layers accounts for extracting low-level visual features shared by both synthetic and real-world data, and will be learned in a unsupervised way, using unlabeled data from both domains. The remaining N \u2212K layers accounts for learning higher-level discriminative features for classification. It will be trained in a supervised way on top of the first part, using labeled data from the synthetic domain only. To train the first K layers, we exploit a Stacked Convolutional Auto-Encoder (SCAE) (Masci et al. (2011)). Its first two convolutional layers have an identical topology to the first two layers in Fig. 1. Moreover, we set its first and second half to be mirror-symmetrical. The cost function is the mean squared error (MSE) between the input and reconstructed patches. After SCAE is learned, its Conv. Layers 1 and 2 are imported to the CNN in Fig. 1. We adopt the SCAE implementation by Paine et al. (2014). We also find that applying label-preserving data augmentations to synthetic training data helps reduce the domain mismatch. Chen et al. (2014) added moderate distortions and corruptions, including noise, blur, rotations and shading effects. In addition, we also vary the character spacings and aspect ratios when rendering training data. Note that these steps are not useful for the method in Chen et al. (2014) because it exploits very localized features, but they are very helpful in our case.", "startOffset": 114, "endOffset": 1757}], "year": 2015, "abstractText": "We address a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This realto-synthetic domain gap caused poor generalization to new real data in previous methods (Chen et al. (2014)). In this paper, we refer to Convolutional Neural Networks, and use an adaptation technique based on a Stacked Convolutional AutoEncoder that exploits unlabeled real-world images combined with synthetic data. The proposed method achieves an accuracy of higher than 80% (top-5) on a realworld dataset.", "creator": "LaTeX with hyperref package"}}}