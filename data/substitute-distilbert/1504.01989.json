{"id": "1504.01989", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2015", "title": "Pixel-wise Deep Learning for Contour Detection", "abstract": "we address the problem of contour detection alongside per - pixel averaging of edge point. to facilitate the process, the proposed approach leverages spectral densenet, an efficient theory of continuous convolutional neural gate ( ins ), to extract an informative feature vector containing each pixel and uses an svm classifier to accomplish contour algorithms. in the experiment of mode detection, we look into the effectiveness of combining per - pixel features from different cnn layers and verify their performance on bsds500.", "histories": [["v1", "Wed, 8 Apr 2015 14:44:20 GMT  (13kb)", "http://arxiv.org/abs/1504.01989v1", "2 pages. arXiv admin note: substantial text overlap witharXiv:1412.6857"]], "COMMENTS": "2 pages. arXiv admin note: substantial text overlap witharXiv:1412.6857", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jyh-jing hwang", "tyng-luh liu"], "accepted": true, "id": "1504.01989"}, "pdf": {"name": "1504.01989.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jyhjinghwang@iis.sinica.edu.tw", "liutyng@iis.sinica.edu.tw"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 4.\n01 98\n9v 1\n[ cs\n.C V\n] 8\nA pr\n2 01"}, {"heading": "1 INTRODUCTION", "text": "We consider deep nets to construct a per-pixel feature learner for contour detection. As the task is essentially a classification problem, we adopt deep convolutional neural networks (CNNs) to establish a discriminative approach. However, one subtle deviation from typical applications of CNNs should be emphasized. In our method, we intend to use the CNN architecture, e.g., AlexNet (Krizhevsky et al., 2012), to generate features for each image pixel, not just a single feature vector for the whole input image. Such a distinction would call for a different perspective of parameter fine-tuning so that a pre-trained per-image CNN on ImageNet (Deng et al., 2009) can be adapted into a new model for per-pixel edge classifications. To investigate the property of the features from different convolutional layers, we carry out a number of experiments to evaluate their effectiveness in performing contour detection on the benchmark BSDS Segmentation dataset (Martin et al., 2001)."}, {"heading": "2 PER-PIXEL CNN FEATURES", "text": "Learning features by employing a deep architecture of neural net has been shown to be effective, but most of the existing techniques focus on yielding a feature vector for an input image (or image patch). Such a design may not be appropriate for vision applications that require investigating image characteristics in pixel level. For contour detection, the central task is to decide whether an underlying pixel is an edge point or not. Thus, it would be convenient that the deep network could yield per-pixel features. To this end, we extract per-pixel CNN features in AlexNet (Krizhevsky et al., 2012) using DenseNet (Iandola et al., 2014), and pixel-wise concatenate them to feed into a support vector machine (SVM) classifier.\nOur implementation uses DenseNet for CNN feature extraction owing to its efficiency, flexibility, and availability. DenseNet is an open source system that computes dense and multiscale features from the convolutional layers of a Caffe CNN based object classifier. The process of feature extraction proceeds as follows. Given an input image, DenseNet computes its multiscale versions and stitches them to a large plane. After processing the whole plane by CNNs, DenseNet would unstitch the descriptor planes and then obtain multiresolution CNN descriptors.\nThe dimensions of convolutional features are ratios of the image size, e.g., one-fourth for Conv1, and one-eighth for Conv2. We rescale feature maps of all the convolutional layers to the image size. That is, there is a feature vector in every pixel. The dimension of the resulting feature vector is 1376 \u00d7 1, which is concatenated by Conv1 (96 \u00d7 1), Conv2 (256 \u00d7 1), Conv3 (384 \u00d7 1), Conv4 (384\u00d7 1), and Conv5 (256\u00d7 1).\nAccepted as a workshop contribution at ICLR 2015\nFor classification, we use the combined per-pixel feature vectors to learn a binary linear SVM. It is worth noting that, in our multiscale setting, we train the SVM based on only the original resolution. In test time, we classify test images using both the original and the double resolutions. We average the two resulting edge maps for the final output of contour detection."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011). To better assess the effects of the features of different layers, we report their respective performance of contour detection. The BSDS500 dataset contains 200 training, 100 validation, and 200 testing images. Boundaries in each image are labeled by several workers and are averaged to form the ground truth. The accuracy of contour detection is evaluated by three measures: the best F-measure on the dataset for a fixed threshold (ODS), the aggregate F-measure on the dataset for the best threshold in each image (OIS), and the average precision (AP) on the full recall range (Arbelaez et al., 2011). Prior to evaluation, we apply a standard non-maximal suppression technique to edge maps to obtain thinned edges (Canny, 1986).\nIn Table 1, we see that features in Conv2 contribute the most, and then Conv3 and Conv4. These suggest that low- to mid-level features are most useful for contour detection, while the lowestand higher-level features provide additional boost. Although features in Conv1 and Conv5 are less effective when employed alone, we achieve the best results by combining all five streams. It indicates that the local edge information in low-level features and the object contour information in higherlevel features are both necessary for achieving high performance in contour detection tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by NSC 102-2221-E-001-021-MY3."}], "references": [{"title": "Contour detection and hierarchical image segmentation", "author": ["Arbelaez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Arbelaez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arbelaez et al\\.", "year": 2011}, {"title": "A computational approach to edge detection", "author": ["Canny", "John"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Canny and John.,? \\Q1986\\E", "shortCiteRegEx": "Canny and John.", "year": 1986}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Densenet: Implementing efficient convnet descriptor pyramids", "author": ["Iandola", "Forrest", "Moskewicz", "Matt", "Karayev", "Sergey", "Girshick", "Ross", "Darrell", "Trevor", "Keutzer", "Kurt"], "venue": "arXiv preprint arXiv:1404.1869,", "citeRegEx": "Iandola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["Martin", "David", "Fowlkes", "Charless", "Tal", "Doron", "Malik", "Jitendra"], "venue": "In Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 4, "context": ", AlexNet (Krizhevsky et al., 2012), to generate features for each image pixel, not just a single feature vector for the whole input image.", "startOffset": 10, "endOffset": 35}, {"referenceID": 2, "context": "Such a distinction would call for a different perspective of parameter fine-tuning so that a pre-trained per-image CNN on ImageNet (Deng et al., 2009) can be adapted into a new model for per-pixel edge classifications.", "startOffset": 131, "endOffset": 150}, {"referenceID": 5, "context": "To investigate the property of the features from different convolutional layers, we carry out a number of experiments to evaluate their effectiveness in performing contour detection on the benchmark BSDS Segmentation dataset (Martin et al., 2001).", "startOffset": 225, "endOffset": 246}, {"referenceID": 4, "context": "To this end, we extract per-pixel CNN features in AlexNet (Krizhevsky et al., 2012) using DenseNet (Iandola et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 3, "context": ", 2012) using DenseNet (Iandola et al., 2014), and pixel-wise concatenate them to feed into a support vector machine (SVM) classifier.", "startOffset": 23, "endOffset": 45}, {"referenceID": 5, "context": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 0, "context": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 0, "context": "The accuracy of contour detection is evaluated by three measures: the best F-measure on the dataset for a fixed threshold (ODS), the aggregate F-measure on the dataset for the best threshold in each image (OIS), and the average precision (AP) on the full recall range (Arbelaez et al., 2011).", "startOffset": 268, "endOffset": 291}], "year": 2015, "abstractText": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and verify their performance on BSDS500.", "creator": "LaTeX with hyperref package"}}}