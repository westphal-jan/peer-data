{"id": "1505.01221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2015", "title": "The Configurable SAT Solver Challenge (CSSC)", "abstract": "it is well known that different solution strategies competed consistently for different types of instances of hard combinatorial problems. normally a practice, most solvers for complex propositional satisfiability problem ( sat ) expose parameters that allow them to be analyzed following a particular family of instances. in each international sat complexity series, these parameters are ignored : solvers are run using a single default parameter setting ( supplied by journal authors ) for consistently benchmark instances in a given track. while this competition format rewards solvers with robust default settings, it does poorly reflect the situation faced by a practitioner who only cares about performance on one particular model and can invest some time into existing solver parameters challenging this application. the new configurable sat solver algorithm ( cssc ) compares solvers in in latter setting, scoring each round by the performance it achieved after a sufficiently automated configuration step. this article describes the cssc in more abstract, and reports the results obtained in its two instantiations so far, cssc 2013 and 2014.", "histories": [["v1", "Tue, 5 May 2015 23:39:24 GMT  (1203kb,D)", "http://arxiv.org/abs/1505.01221v1", null], ["v2", "Tue, 2 Aug 2016 08:48:53 GMT  (5750kb,D)", "http://arxiv.org/abs/1505.01221v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["frank hutter", "marius lindauer", "adrian balint", "sam bayless", "holger hoos", "kevin leyton-brown"], "accepted": false, "id": "1505.01221"}, "pdf": {"name": "1505.01221.pdf", "metadata": {"source": "CRF", "title": "The Configurable SAT Solver Challenge (CSSC)", "authors": ["Frank Hutter", "Marius Lindauer", "Adrian Balint", "Sam Bayless", "Holger Hoos", "Kevin Leyton-Brown"], "emails": ["fh@cs.uni-freiburg.de", "lindauer@cs.uni-freiburg.de", "adrian.balint@uni-ulm.de", "sbayless@cs.ubc.ca", "hoos@cs.ubc.ca", "kevinlb@cs.ubc.ca"], "sections": [{"heading": null, "text": "It is well known that different solution strategies work well for different types of instances of hard combinatorial problems. As a consequence, most solvers for the propositional satisfiability problem (SAT) expose parameters that allow them to be customized to a particular family of instances. In the international SAT competition series, these parameters are ignored: solvers are run using a single default parameter setting (supplied by the authors) for all benchmark instances in a given track. While this competition format rewards solvers with robust default settings, it does not reflect the situation faced by a practitioner who only cares about performance on one particular application and can invest some time into tuning solver parameters for this application. The new Configurable SAT Solver Competition (CSSC) compares solvers in this latter setting, scoring each solver by the performance it achieved after a fully automated configuration step. This article describes the CSSC in more detail, and reports the results obtained in its two instantiations so far, CSSC 2013 and 2014.\nKeywords: Propositional satisfiability, algorithm configuration, empirical evaluation, competition"}, {"heading": "1. Introduction", "text": "The propositional satisfiability problem (SAT) is one of the most prominent problems in AI. It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]). The SAT community has a long history of regularly assessing the state of the art via competitions [49]. The first SAT competition dates back to the year\nEmail addresses: fh@cs.uni-freiburg.de (Frank Hutter), lindauer@cs.uni-freiburg.de (Marius Lindauer), adrian.balint@uni-ulm.de (Adrian Balint), sbayless@cs.ubc.ca (Sam Bayless), hoos@cs.ubc.ca (Holger Hoos), kevinlb@cs.ubc.ca (Kevin Leyton-Brown)\nPreprint submitted to Elsevier November 2, 2017\nar X\niv :1\n50 5.\n01 22\n1v 1\n[ cs\n.A I]\n5 M\nay 2\n2002 [74], and the event has been growing over time: in 2014, it had a record participation of 58 solvers by 79 authors in 11 tracks [13].\nIn practical applications of SAT, solvers can typically be adjusted to perform well for the specific type of instances at hand, such as software verification instances generated by a particular static checker on a particular software system [3], or a particular family of bounded model checking instances [83]. To support this type of customization, most SAT solvers already expose a range of command line parameters whose settings substantially affect most parts of the solver. Solvers typically come with robust default parameter settings meant to provide good all-round performance, but it is widely known that adjusting parameter settings to particular target instance classes can yield orders-of-magnitude speedups [41, 53, 79]. Current SAT competitions do not take this possibility of customizing solvers into account, and rather evaluate solver performance with default parameters.\nUnlike the SAT competition, the Configurable SAT Solver Challenge (CSSC) evaluates SAT solver performance after application-specific customization, thereby taking into account the fact that effective algorithm configuration procedures can automatically customize solvers for a given distribution of benchmark instances. Specifically, for each type of instances T and each SAT solver S, an automated fixed-time offline configuration phase determines parameter settings of S optimized for high performance on T . Then, the performance of S on T is evaluated with these settings, and the solver with the best performance wins.\nThe competition conceptually most closely related to the CSSC is the learning track of the international planning competition (IPC, see, e.g., the description by Fern et al. (2011) or http://www.cs.colostate.edu/~ipc2014/), which also features an offline time-limited learning phase on training instances from a given planning domain and an online testing phase on a disjoint set of instances from the same domain. The main difference between this IPC learning track and the CSSC (other than their focus on different problems) is that in the IPC learning track every planner uses its own learning method, and the learning methods thus vary between entries. In contrast, in the CSSC, the corresponding customization process is part of the competition setup and uses the same algorithm configuration procedure for each submitted solver. Our approach to evaluating solver performance after configuration could of course be transferred to any other competition. (In fact, the 2014 IPC learning track for non-portfolio solvers was won by FastDownward-SMAC [73], a system that employs a similar combination of general algorithm configuration and a highly parameterized solver framework as we do in the CSSC.)\nIn the following, we first describe the criteria we used for the design of the CSSC (Section 2). Next, we provide some background on the automated algorithm configuration methods we used when running the competition (Section 3). Then, we discuss the two CSSCs we have held so far (in 2013 and 2014); we discuss each of these competitions in turn (Sections 4 and 5), including the specific benchmarks used, the participating solvers, and the results. We describe two main insights that we obtained from these results:\n1. In many cases, automated algorithm configuration found parameter settings that performed much better than the solver defaults, in several cases yielding average speedups of several orders of magnitude.\n2. Some solvers benefited more from automated configuration than others; as a result, the ranking of algorithms after configuration was often substantially different from the ranking based on the algorithm defaults (as, e.g., measured in the SAT competition).\nFinally, we analyze various aspects of these results (Section 6) and discuss the implications we see for future algorithm development (Section 7)."}, {"heading": "2. Design Criteria for the CSSC", "text": "We organized the CSSC 2013 and 2014 in coordination with the international SAT competition and presented them in the competition slots at the 2013 and 2014 SAT conferences (as well as in the 2014 FLoC Olympic Games, in which all SAT-related competitions took part). We coordinated solver submission deadlines with the SAT competition to minimize overhead for participants, who could submit their solver to the SAT competition using default parameters and then open up their parameter spaces for the CSSC.\nWe designed the CSSC to remain close to the international SAT competition\u2019s established format; in particular, we used the same general categories: industrial, crafted, and random, and, in 2014 also random satisfiable. Furthermore, we used the same input and output formats, the SAT competition\u2019s mature code for verifying correctness of solver outputs (only for checking models of satisfiable instances; we did not have a certified UNSAT track), and the same scoring function (number of instances solved, breaking ties by average runtime).\nThe main way our setup differed from that of the SAT competition was that we used a relatively small budget of five minutes per solver run. We based this choice partly on the fact that many solvers have runtime distributions with rather long tails (or even heavy tails [34]), and that practitioners often use many instances and relatively short runtimes to benchmark solvers for a new application domain. There is also evidence that SAT competition results would remain quite similar if based on shorter runtimes, but not if based on fewer instances [43]. Therefore, in order to achieve more robust performance within a fixed computational budget, we chose to use many test instances (at least 250 for each benchmark) but relatively low runtime cutoffs per solver run (five minutes). (We also note that a short time limit of five minutes has already been used in other competitions, such as the agile track of the 2014 International Planning Competition.) Due to constraints imposed by our computational infrastructure, we used a memory limit of 3GB for each solver run.\nTo simulate the situation faced by practitioners with limited computational resources, we limited the computational budget for configuring a solver on a benchmark with a given configuration procedure to two days on 4 or 5 cores (in 2014 and 2013, respectively). Our results are therefore indicative of what could\nbe obtained by performing configuration runs over the weekend on a modern desktop machine."}, {"heading": "2.1. Controlled Execution of Solver Runs", "text": "Since all configuration procedures ran in an entirely automated fashion, they had to be robust against any kind of solver failure (segmentation faults, unsupported combinations of parameters, wrong results, infinite loops, etc.). We handled all such conditions in a generic wrapper script that used Olivier Roussel\u2019s runsolver tool [71] to limit runtime and memory, and counted any errors or limit violations as timeouts at the maximum runtime of 300 seconds. We also kept track of the rich solver runtime data we gathered in our configuration runs and made it publicly available on the competition website."}, {"heading": "2.2. Choice of Configuration Pipeline", "text": "To avoid bias arising from our choice of algorithm configuration method, we independently used all three state-of-the-art methods applicable for runtime optimization (ParamILS , GGA, and SMAC , as described in detail in Section 3). We evaluated the configurations resulting from all configuration runs on the entire training data set and selected the configuration with the best training performance. We then executed only this configuration on the test set to determine the performance of the configured solver. Except where specifically noted otherwise, all performance data we report in this article is for this optimized configuration on previously unseen test instances from the respective benchmark set."}, {"heading": "2.3. Choice of Benchmarks", "text": "We chose the benchmark families for the CSSC to be relatively homogeneous in terms of the origin and/or construction process of instances in the same family. Typically, we selected benchmark families that are neither too easy (since speedups are less interesting for easy instances), nor too hard (so that solvers could solve a large fraction of instances within the available computational budgets). We aimed for benchmark sets of which at least 20-40% could be solved within the maximum runtime on a recent machine by the default configuration of a SAT solver that would perform reasonably well in the SAT competition. We also aimed for benchmark sets with a sufficient number of instances to safeguard against over-tuning; in practice, the smallest datasets we used had 500 instances: 250 for training and 250 for testing."}, {"heading": "3. Automated Algorithm Configuration Procedures", "text": "The problem of finding performance-optimizing algorithm parameter settings arises for many computational problems. In recent years, the AI community has developed several dedicated systems for this general algorithm configuration problem [46, 1, 55, 45].\nWe now describe this problem more formally. Let A be an algorithm having n parameters with domains \u03981, . . . ,\u0398n. Parameters can be real-valued (with domains [a, b], where a, b \u2208 R and a < b), integer-valued (with domains [i, j], where i, j \u2208 Z and i < j), or categorical (with finite unordered domains, such as {red, blue, green}). Parameters can also be conditional on an instantiation of other (so-called parent) parameters; as an example, consider the parameters of a heuristic mechanism h, which are completely ignored unless h is chosen to be used by means of another, categorical parameter. Finally, some combinations of parameter instantiations can be labelled as forbidden.\nAlgorithm A\u2019s configuration space \u0398 then consists of all possible combinations of parameter values: \u0398 = \u03981 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u0398n. We refer to elements \u03b8 = \u3008\u03b81, . . . , \u03b8n\u3009 of this configuration space as parameter configurations, or simply configurations. Given a benchmark set \u03a0 and a performance metric m(\u03b8, \u03c0) capturing the performance of configuration \u03b8 \u2208 \u0398 on problem instance \u03c0 \u2208 \u03a0, the algorithm configuration problem then aims to find a configuration \u03b8 \u2208 \u0398 that minimizes m over \u03a0, i.e., that minimizes\nf(\u03b8) = 1 |\u03a0| \u00b7 \u2211 \u03c0\u2208\u03a0 m(\u03b8, \u03c0).1\nIn the CSSC, the specific metric m we optimized was penalized average runtime (PAR-10), which counts runs that exceed a maximal cutoff time \u03bamax without solving the given instance as 10 \u00b7 \u03bamax. We terminated individual solver runs as unsuccessful after \u03bamax = 300 seconds.\nWe refer to an instance of the algorithm configuration problem as a configuration scenario and to a method for solving the algorithm configuration problem as a configuration procedure (or a configurator), in order to avoid confusion with the solver to be optimized (which we refer to as the target algorithm) and the problem instances the solver is being optimized for.\nAlgorithm configuration has been demonstrated to be very effective for optimizing various SAT solvers in the literature. For example, Hutter et al. [41] configured the algorithm Spear [5] on formal verification instances, achieving a 500-fold speedup on software verification instances generated with the static checker Calysto [3] and a 4.5-fold speedup on IBM bounded model checking instances by Zarpas [83]. Algorithm configuration has also enabled the development of general frameworks for stochastic local search SAT solvers that can be automatically instantiated to yield state-of-the-art performance on new types of instances; examples for such frameworks are SATenstein [53] and Captain Jack [79].\nWhile all of these applications used the local-search based algorithm configuration method ParamILS [46], in the CSSC we wanted to avoid bias that could\n1An alternative definition considers the optimization of expected performance across a distribution of instances rather than average performance across a set of instances [46]. What we consider here can be seen as a special case where the distribution is uniform over a given set of training instances. It is also possible to optimize performance metrics other than mean performance across instances, but mean performance is by far the most widely used option.\narise from commitment to one particular algorithm configuration method and thus used all three existing general algorithm configuration methods for runtime optimization: ParamILS , GGA [1], and SMAC [45].2 We refer the interested reader to Appendix B for details on each of these configurators. Here, we only mention some details that were important for the setup of the CSSC:\n\u2022 ParamILS does not natively support parameters specified only as realor integer-valued intervals, but requires all parameter values to be listed explicitly; for simplicity, we refer to the transformation used to satisfy this requirement as discretization. When multiple parameter spaces were available for a solver, we only ran ParamILS on the discretized version, whereas we ran GGA and SMAC on both the discretized and the nondiscretized versions.\n\u2022 ParamILS and SMAC have been shown to benefit substantially from multiple independent runs. Given k cores, the usual approach is simply to execute k independent configurator runs and pick the configuration from the one with best performance on the training set. GGA, on the other hand, can use multiple cores on a single machine, and in fact requires these to run effectively. Therefore, given k available cores per configuration approach, we used k independent runs of each ParamILS and SMAC , and one run using all k cores for GGA.\n\u2022 GGA could not handle the complex parameter conditionalities found in some solvers; for those solvers, we only ran ParamILS and SMAC ."}, {"heading": "4. The Configurable SAT Solver Challenge 2013", "text": "The first CSSC (http://www.cs.ubc.ca/labs/beta/Projects/CSSC2013/) was held in 2013. It featured three tracks mirroring those of the SAT competition: Industrial SAT+UNSAT , crafted SAT+UNSAT , and Random SAT+UNSAT . Table 1 lists the benchmark families we used in each of these tracks, all of which are described in detail in Appendix A. Within each track, we used the same number of test instances for each benchmark family, thereby weighting each equally in our analysis."}, {"heading": "4.1. Participating Solvers and Their Parameters", "text": "Table 2 summarizes the solvers that participated in the CSSC 2013, along with information on their configuration spaces. The eleven submitted solvers ranged from complete solvers based on conflict-directed clause learning (CDCL; [10]) to stochastic local search (SLS; [39]) solvers. The degree of parameterization varied substantially across these submitted solvers, from 2 to 241 parameters.\n2We did not use the iterated racing method I/F-Race [55], since it does not effectively support runtime optimization and its authors thus discourage its use for this purpose (personal communication with Manuel Lo\u0301pez-Iba\u0301n\u0303ez and Thomas Stu\u0308tzle).\nWe briefly discuss the main features of the solvers\u2019 parameter configuration spaces, ordering solvers by their number of parameters.\nGnovelty+GCa and Gnovelty+GCwa [29] are closely related SLS solvers. Both have two numerical parameters: the probability of selecting false clauses randomly and the probability of smoothing clause weights. The parameters were prediscretized by the solver developer to 11 and 10 values, yielding 110 possible combinations.\nGnovelty+PCL [29] is an SLS solver with five parameters: one binary parameter (determining whether the stagnation path is dynamic or static) and four numerical parameters: the length of the stagnation path, the size of the time window storing stagnation paths, the probability of smoothing stagnation weights, and the probability of smoothing clause weights. All numerical parameters were pre-discretized to ten values each by the solver developer, yielding 20 000 possible combinations.\nSimpsat [35] is a CDCL solver based on Cryptominisat [75], which adds additional strategies for explicitly handling XOR constraints [36]. It has five numerical parameters that govern both these XOR constraint strategies and the frequency of random decisions. All parameters were pre-discretized by the solver developer, yielding 2 400 possible combinations.\nSat4j [14] is full-featured library of solvers for Boolean satisfiability and optimization problems. For the contest, it applied its default CDCL SAT solver with ten exposed parameters: four categorical parameters deciding between different restart strategies, phase selection strategies, simplifications, and cleaning; and six numerical parameters pre-discretized by its developer.\nSolver43 [6] is a CDCL solver with 12 parameters: three categorical parameters concerning sorting heuristics used in bounded variable eliminiation, in definitions and in adding blocked clauses; and nine numerical parameters concerning various frequencies, factors, and limits. All parameters were pre-discretized by the solver developer.\nForl-nodrup [76] is a CDCL solver with 44 parameters. Most notably, these control variable selection, Boolean propagation, restarts, and learned clause removal. About a third of the parameters are numerical (particularly most of those concerning restarts and learned clause removal); all parameters were pre-discretized by the solver developer.\nClasp-2.1.3 [32] is a solver for the more general answer set programming (ASP) problem, but it can also solve SAT, MAXSAT and PB problems. As a SAT solver, Clasp-2.1.3 is a CDCL solver with 83 parameters: 7 for pre-processing, 14 for the variable selection heuristic, 18 for the restart policy, 34 for the deletion policy, and 10 for a variety of other uses. The configuration space is highly conditional, with several top-level parameters enabling or disabling certain strategies. Clasp-2.1.3 exposes both a mixed continuous/discrete parameter configuration space and a manually-discretized one.\nRiss3g [61] is a CDCL solver with 125 parameters. These include 6 numerical parameters from MiniSAT [30], 10 numerical parameters from Glucose [2], 17 mostly numerical Riss3G parameters, and 92 parameters controlling preprocessing/inprocessing performed by the integrated Coprocessor [60]. The inprocessor\nparameters resemble those in Lingeling [16], emphasizing blocked clause elimination [50], bounded variable addition [63], and probing [59]. About 50 of the parameters are Boolean, and most others are numerical parameters prediscretized by the solver developer. The parameter space is highly conditional, with inprocessor parameters dependent on a switch turning them on alongside various other dependencies. Indeed, there are only 18 unconditional parameters. Finally, there are also seven forbidden parameter combinations that ascertain various switches are turned on if inprocessing is used.\nRiss3gExt [61] is an experimental extension of Riss3g . It exposes all of the parameters previously discussed for Riss3g , along with an additional 11 Riss3G parameters and 57 inprocessing parameters. Its developer implemented all of these extensions in one week and did not have time for extensive testing before the CSSC; therefore, he submitted Riss3gExt as closed source, making it ineligible for medals. We discuss the results of this closed-source solver separately, in Section 4.4.\nLingeling [16] is a CDCL solver with 241 parameters (making it the solver with the largest configuration space in the CSSC 2013). 102 of these parameters are categorical, and the remaining 139 are integer-valued (76 of them with the trivial upper bound of max-integer, 231 \u2212 1). Lingeling parameterizes many details of the solution process, including probing and look-ahead (about 25 mostly numerical parameters), blocked clause elimination and bounded variable elimination (about 20 mostly categorical parameters each), glue clauses (about 15 mostly numerical parameters), and a host of other mechanisms parameterized by about 5\u201310 parameters each. Lingeling exposes its full parameter space, a discretized version of all parameters, and a subspace of only the 102 categorical parameters."}, {"heading": "4.2. Configuration Pipeline", "text": "Configuring each of the eleven solvers on each of the nine benchmark families gave rise to 99 configuration scenarios. Since our budget for each configuration procedure was two CPU days on five cores for each scenario, carrying out the competition required 990 CPU days per configuration approach. We executed this competition on the QDR partition of the Compute Canada Westgrid cluster Orcinus. Each node in this cluster was provisioned with 24 GB memory and two 6-core, 2.66 GHz Intel Xeon X5650 CPUs with 12 MB L2 cache each, and ran Red Hat Enterprise Linux Server 5.5 (kernel 2.6.18, glibc 2.5).\nIn this first edition of the CSSC, we were unfortunately unable to run GGA. This was because it requires multiple cores for effective runtime minimization, and the respective multiple-core jobs we submitted on the Orcinus cluster were stuck in the queue for months without getting started. (Single-core runs, on the other hand, were often scheduled within minutes.)\nWe thus limited ourselves to executing 5 independent ParamILS runs on the discretized parameter space and 5 independent SMAC runs on each of\nthe parameter spaces that solver authors submitted (as discussed above, two submissions included multiple parameter spaces).\nFollowing standard practice, we evaluated the configurations resulting from all configuration runs on the entire training data set and selected the configuration with the best training performance. We then executed only this configuration on the test set to assess the performance of the configured solver."}, {"heading": "4.3. Results", "text": "For each the three tracks of CSSC 2013, we configured each of the eleven submitted solvers for each of the benchmark families within the track and aggregated results across the respective test instances. We show the winners in Table 3 and discuss the results for each track in the following sections. Additional details, tables, and figures are provided in an accompanying technical report [42]."}, {"heading": "4.3.1. Results of the Industrial SAT+UNSAT Track", "text": "Our Industrial SAT+UNSAT track consisted of the four industrial benchmarks detailed in Appendix A.1: Bounded Model Checking 2008 (BMC) [15], Circuit Fuzz [23], Hardware Verification (IBM) [83], and SWV [4].\nFigure 1 visualizes the results of the configuration process for the winning solver Lingeling on these four benchmark sets. It demonstrates that even Lingeling , a highly competitive solver in terms of default performance, can be configured for improved performance on a wide range of benchmarks. We note that for the easy benchmark SWV, configuration sped up Lingeling by a factor of 20 (average runtime 3.3s vs 0.16s), and that for the harder Circuit Fuzz instances, it nearly halved the number of timeouts (39 vs 20). The improvements were smaller for more traditional hardware verification instances (IBM and BMC ) similar to those used to determine Lingeling \u2019s default parameter settings.\nTable 4 summarizes the results of the ten solvers that were eligible for medals. From this table, we note that, like Lingeling , many other solvers benefited from configuration. Indeed, some solvers (in particular Forl-nodrup and Clasp-3.0.4-p8 ) benefited much more from configuration on the BMC instances, largely because their default performance was worse on this benchmark. On the other hand, Riss3g featured stronger default performance than Lingeling but did not benefit as much from configuration.\nTable 4 also aggregates results across the four benchmark families to yield the overall results for the Industrial SAT+UNSAT track. These results show that many solvers benefited substantially from configuration, and that some benefited more than others, causing the CSSC ranking to differ substantially\nResults for CSSC 2013 Industrial SAT+UNSAT track\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(a) BMC PAR-10: 302\u2192 282\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(b) Circuit Fuzz PAR-10: 409\u2192 241\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(c) IBM PAR-10: 694\u2192 692\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10 100 C o n fi g u re d i n s e c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(d) SWV PAR-10: 3.32\u2192 0.16\nFigure 1: Speedups achieved by configuration of Lingeling. For each benchmark, we show scatter plots of solver defaults vs. configured parameter settings.\n#timeouts default \u2192 # timeouts configured (on test set) Rank BMC Circuit Fuzz IBM SWV Overall def CSSC\nLingeling 28\u2192 26 39\u2192 20 69\u2192 69 0\u2192 0 136\u2192 115 4 1 Riss3g 32\u2192 30 20\u2192 18 70\u2192 69 0\u2192 0 122\u2192 117 1 2 Solver43 30\u2192 30 20\u2192 20 77\u2192 77 0\u2192 0 127\u2192 127 2 3 Forl-nodrup 50\u2192 36 33\u2192 23 69\u2192 69 0\u2192 0 152\u2192 128 5 4 Simpsat 38\u2192 35 26\u2192 24 70\u2192 69 0\u2192 0 134\u2192 128 3 5 Clasp-3.0.4-p8 66\u2192 42 26\u2192 17 71\u2192 71 0\u2192 0 163\u2192 130 6 6 Sat4j 70\u2192 70 36\u2192 30 77\u2192 76 1\u2192 0 184\u2192 176 7 7 Gnovelty+GCwa 291\u2192 285 301\u2192 295 295\u2192 295 244\u2192 215 1131\u2192 1090 10 8 Gnovelty+PCL 289\u2192 288 302\u2192 302 295\u2192 294 215\u2192 215 1101\u2192 1099 8 9 Gnovelty+GCa 291\u2192 290 300\u2192 302 295\u2192 295 243\u2192 217 1129\u2192 1104 9 10\nTable 4: Results for CSSC 2013 competition track Industrial SAT+UNSAT. For each solver and benchmark, we show the number of test set timeouts achieved with the default and the configured parameter setting, bold-facing the better one; we broke ties by the solver\u2019s average runtime (not shown for brevity). We aggregated results across all benchmarks to compute the final ranking.\nResults for CSSC 2013 crafted SAT+UNSAT track\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10 100 C o n fi g u re d i n s e c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(a) Graph Isomorphism (GI) PAR-10: 362\u2192 65\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(b) Low Autocorrelation Binary Sequence (LABS). PAR-10:\n837\u2192 779\nFigure 2: Speedups achieved by configuration of Clasp-3.0.4-p8 on the CSSC 2013 crafted SAT+UNSAT track. We show scatter plots of default vs. configured versions of Clasp-3.0.4-p8 .\n#TOs default \u2192 #TOs configured (on test set) Rank GI LABS Overall def CSSC\nClasp-3.0.4-p8 42\u2192 6 97\u2192 90 139\u2192 96 2 1 Forl-nodrup 40\u2192 7 95\u2192 91 135\u2192 98 1 2 Lingeling 43\u2192 10 105\u2192 97 148\u2192 107 3 3 Riss3g 51\u2192 42 97\u2192 89 148\u2192 131 4 4 Simpsat 42\u2192 42 107\u2192 107 149\u2192 149 5 5 Solver43 66\u2192 65 90\u2192 87 156\u2192 152 6 6 Sat4j 62\u2192 57 110\u2192 104 172\u2192 161 7 7 Gnovelty+GCwa 180\u2192 180 195\u2192 154 375\u2192 334 8 8 Gnovelty+GCa 183\u2192 180 240\u2192 173 423\u2192 353 10 9 Gnovelty+PCL 179\u2192 178 199\u2192 183 378\u2192 361 9 10\nTable 5: Results for CSSC 2013 competition track crafted SAT+UNSAT. For each solver and benchmark, we show the number of test set timeouts achieved with the default and the configured parameter setting, bold-facing the better one. We aggregated results across all benchmarks to compute the final ranking.\nfrom the ranking according to default solver performance; for instance, based on default performance, the overall winning solver, Lingeling , would have only ranked fourth."}, {"heading": "4.3.2. Results of the crafted SAT+UNSAT Track", "text": "The crafted SAT+UNSAT track consisted of the two crafted benchmarks detailed in Appendix A.2: Graph Isomorphism (GI) and Low Autocorrelation Binary Sequence (LABS).\nFigure 2 visualizes the improvements algorithm configuration yielded for the best-performing solver Clasp-3.0.4-p8 on these benchmarks. Improvements were particularly large on the GI instances, where algorithm configuration decreased the number of timeouts from 42 to 6. Table 5 summarizes the results we obtained for all solvers on these benchmarks, showing that configuration also substantially improved the performance of many other solvers. The table also aggregates results across both benchmark families to yield overall results for the crafted SAT+UNSAT track. While Forl-nodrup showed the best default performance and benefited substantially from configuration (#timeouts reduced from 135 to 98), Clasp-3.0.4-p8 improved even more (#timeouts reduced from 139 to 96)."}, {"heading": "4.3.3. Results of the Random SAT+UNSAT Track", "text": "The Random SAT+UNSAT track consisted of three random benchmarks detailed in Appendix A.3: 5sat500 , K3 , and unif-k5 . The instances in 5sat500 were all satisfiable, those in unif-k5 all unsatisfiable, and those in K3 were mixed.\nTable 6 summarizes the results for these benchmarks. It shows that the unif-k5 benchmark set was very easy for complete solvers (although configuration still yielded up to 4-fold speedups), that the K3 benchmark was also quite easy for the best solvers, and that only the SLS solvers could tackle benchmark 5sat500 , with configuration making a big difference to performance.\nHere again, our aggregate results demonstrate that rankings were substantially different between the default and configured versions of the solvers: the three solvers with top default performance were ranked 4th to 6th in the CSSC, and vice versa. Figure 3 visualizes the very substantial speedups achieved by configuration for the winning solver Clasp-3.0.4-p8 on K3 and unif-k5 , and for the SLS solver Gnovelty+GCa on 5sat500 ."}, {"heading": "4.4. Hors-Concours Solver Riss3gExt", "text": "So far, we have limited our analysis to the ten open-source solvers that competed for medals. Recall that one additional solver, Riss3gExt , only participated hors concours. It was not eligible for a medal, because it had been submitted as closed source, being based on a highly experimental code branch of Riss3g that had not been exhaustively tested and was therefore likely to contain bugs.\nAs discussed in Section 2.1, our experimental protocol included various safeguards against such bugs: we measured runtime and memory externally, compared reported solubility status against true solubility status where this was known, and checked returned models when an instance was reported satisfiable. Our configuration pipeline detected and penalized these crashes automatically, enabling the configuration procedures to continue their search and find Riss3gExt configurations with no or few crashes. In fact, the final best configurations identified by our configuration pipeline performed very well and would have\nResults for CSSC 2013 Random SAT+UNSAT track\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(a) Gnovelty+GCa on 5sat500\nPAR-10: 1997\u2192 77\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(b) Clasp-3.0.4-p8 on K3 PAR-10: 158\u2192 2.79\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(c) Clasp-3.0.4-p8 on unif-k5 PAR-10: 1.44\u2192 0.37\nFigure 3: Speedups achieved by configuration on the CSSC 2013 Random SAT+UNSAT track. We show scatter plots of default vs. configured solvers..\n#TOs default \u2192 #TOs configured (on test set) Rank 5sat500 K3 unif-k5 Overall def CSSC\nClasp-3.0.4-p8 250\u2192 250 11\u2192 0 0\u2192 0 261\u2192 250 6 1 Lingeling 250\u2192 250 8\u2192 0 0\u2192 0 258\u2192 250 4 2 Riss3g 250\u2192 250 10\u2192 0 0\u2192 0 260\u2192 250 5 3 Solver43 250\u2192 250 6\u2192 3 0\u2192 0 256\u2192 253 2 4 Simpsat 250\u2192 250 4\u2192 4 0\u2192 0 254\u2192 254 1 5 Sat4j 250\u2192 250 7\u2192 5 0\u2192 0 257\u2192 255 3 6 Forl-nodrup 250\u2192 250 39\u2192 8 0\u2192 0 289\u2192 258 7 7 Gnovelty+GCwa 8\u2192 1 124\u2192 124 250\u2192 250 382\u2192 375 8 8 Gnovelty+GCa 163\u2192 4 124\u2192 124 250\u2192 250 537\u2192 378 9 9 Gnovelty+PCL 250\u2192 11 124\u2192 124 250\u2192 250 624\u2192 385 10 10\nTable 6: Results for CSSC 2013 competition track Random SAT+UNSAT. For each solver and benchmark, we show the number of test set timeouts achieved with the default and the configured parameter setting, bold-facing the better one. Results were aggregated across all benchmarks to compute the final ranking. We broke ties by the solver\u2019s average runtime. While we do not show runtimes for brevity, the runtimes important for the ranking were the average runtimes of the top 3 solvers on K3 and unif-k5 : 1.58s (Clasp-3.0.4-p8 ), 4.20s (Lingeling), and 7.68s (Riss3g).\nhandily won both the industrial and the crafted track of the CSSC 2013 had Riss3gExt been submitted as open source: in the industrial track, it only left 82 problem instances unsolved (compared to 115 for Lingeling); and in the crafted track only 44 (compared to 96 for Clasp-3.0.4-p8 ). Even though most of the instances Riss3gExt did not solve were due to it crashing, all of these were \u2018legal\u2019 crashes that simply did not output a solution (such as segmentation faults). In particular, we never observed Riss3gExt to produce an incorrect output for a CSSC test instance with known satisfiability status.\nHowever, empirical tests with benchmark instances are of course no substitute for formal correctness guarantees, and even seasoned solvers can have bugs. Indeed, after the competition, Riss3gExt \u2019s developer found a bug in it (in onthe-fly clause improvement [37]) that caused some satisfiable instances to be incorrectly labeled as unsatisfiable.3 This being the case, it was fortunate that Riss3gExt was ineligible for medals.\nWhile empirical testing on benchmark instances, as done in a competition, can never guarantee the correctness of a solver, in future CSSCs, we consider tightening solubility checks on the benchmark instances used, by either limiting the benchmark sets to contain only instances with known satisfiability status or to require (and check) proofs of unsatisfiability, as in the certified UNSAT track of the SAT competition."}, {"heading": "5. The Configurable SAT Solver Challenge 2014", "text": "The second CSSC (http://aclib.net/cssc2014/) was held in 2014. Compared to the inaugural CSSC in 2013, we improved the competition design in several ways:\n\u2022 We used a different compute cluster4, enabling us to run GGA as one of the configuration procedures.\n3Personal communication with Riss3gExt \u2019s developer Norbert Manthey. 4We executed this competition on the META cluster at the University of Freiburg, whose compute nodes contained 64GB of RAM and two 2.60GHz Intel Xeon E5-2650v2 8-core CPUs with 20 MB L2 cache each, running Ubuntu 14.04 LTS, 64bit.\n\u2022 We added a Random SAT track to facilitate comparisons of stochastic local search solvers.\n\u2022 We dropped the (too easy) SWV benchmark family and introduced four new benchmark families, yielding a total of three benchmark families in each of the four tracks, summarized in Table 7 and described in detail in Appendix A.\n\u2022 We let solver authors decide which tracks their solver should run in. \u2022 For fairness, for each solver, we performed the same number of configuration\nexperiments. (This is in contrast to 2013, where we performed the same number of configuration runs for every configuration space of every solver, which lead to a larger combined configuration budget for solvers submitted with multiple configuration spaces).\n\u2022 We kept track of all of the (millions of) solver runs performed during the configuration process and made all information about errors available to solver developers."}, {"heading": "5.1. Participating Solvers", "text": "The ten solvers that participated in the CSSC 2014 are summarized in Table 8; they included CDCL, SLS and hybrid solvers. These solvers differed substantially in their degree of parameterization, with the number of parameters ranging from 1 to 323. We briefly discuss the main features of each solver\u2019s parameter configuration space, ordering solvers by their number of parameters.\nDCCASat+march-rw [58] combines the SLS solver DCCASat with the CDCL solver march-rw. It was submitted to the Random SAT+UNSAT track. Its only\n(continuous) parameter is the time ratio of the SLS solver. This parameter was pre-discretized to nine values.\nCSCCSat2014 [57, 58] is an SLS solver based on configuration checking and dynamic local search methods. It was submitted to the Random SAT track. It features 3 continuous parameters that were pre-discretized to 7, 9, and 7 values each, giving rise to a total configuration space of 567 possible parameter configurations. The parameters control the weighting of the dynamic local search part and the probabilities for the linear make functions used in the random walk steps.\nProbSAT [9] is a simple SLS solver based on probability distributions that are built from simple features, such as the make and break of variables [9]. ProbSAT \u2019s 9 parameters control the type and the parameters of the probability distribution, as well as the type of restart. ProbSAT was submitted to the Random SAT track.\nMinisat-HACK-999ED [69] is a CDCL solver; it was submitted to all tracks. It has one categorical parameter (whether or not to use the Luby restarting strategy) and 9 numerical parameters fine-tuning the Luby and geometric restart strategies, as well as controlling clause removal and the treatment of glue clauses. 3 of these 9 numerical parameters are conditional on the choice of the Luby restart strategy, and all numerical parameters were pre-discretized by the solver developer. There are also 3 forbidden parameter combinations derived from a weak inequality constraint between two parameter values.\nYalSAT [17] is an SLS solver; it was submitted to the tracks crafted SAT+UNSAT and Random SAT . It has 27 parameters that parameterize the solver\u2019s restart component (7 parameters) amongst many other components. 11 of the 27 parameters are numerical, with 6 of them having a trivial upper bound of max-integer (231 \u2212 1).\nCryptominisat [75] is a CDCL solver; it was submitted to the tracks Industrial SAT+UNSAT and crafted SAT+UNSAT . It has 29 parameters that control restarts (6 mostly numerical parameters), clause removal (7 mostly numerical parameters), variable branching and polarity (3 parameters each), simplification (5 parameters), and several other mechanisms. 2 of the numerical parameters further parameterize the blocking restart mechanism and are thus conditional on that mechanism being selected.\nClasp-3.0.4-p8 [32] is a solver for the more general answer set programming (ASP) problem, but it can also solve SAT, MAXSAT and PB problems. It is fundamentally similar to the solver submitted in 2013; changes in the new version focused on the ASP solving part rather than the SAT solving part. As a SAT solver, Clasp-3.0.4-p8 has 75 parameters, of which 7 control preprocessing, 14 variable selection, 19 the restart policy, 28 the deletion policy and 7 miscellaneous\nother mechanisms. The configuration space is highly conditional, with several top-level parameters enabling or disabling certain strategies. Finally, there are also 2 forbidden parameter combinations that prevent certain combinations of deletion strategies. Clasp-3.0.4-p8 exposes both a mixed continuous/discrete parameter configuration space and a manually-discretized one. It was submitted to all tracks.\nRiss-4.27 [62] is a CDCL solver submitted to all tracks except Random SAT . Compared to the 2013 version Riss3g , it almost doubled its number of parameters, yielding 214 parameters organized into 121 simplification and 93 search parameters. In particular, it added many new preprocessing and inprocessing techniques, including XOR handling (via Gaussian elimination [36]), and extracting cardinality constraints [20]. Roughly half of the simplification parameters and a third of the search parameters are categorical (in both cases most of the categoricals are binary). The simplification parameters comprise about 20 Boolean switches for preprocessing techniques and about 100 in-processor parameters, prominently including blocked clause elimination, bounded variable addition, equivalance elimination [33], numerical limits, probing, symmetry breaking, unhiding [38], Gaussian elimination, covered literal elimination [64], and even some stochastic local search. The search parameters parameterize a wide range of mechanisms including variable selection, clause learning and removal, restarts, clause minimization, restricted extended resolution, and interleaved clause strengthening.\nSparrowToRiss [8] combines the SLS solver Sparrow with the CDCL solver Riss-4.27 by first running Sparrow, followed by Riss-4.27 . It was submitted to all tracks. SparrowToRiss\u2019s configuration space is that of Riss-4.27 plus 6 Sparrow parameters and 2 parameters controlling when to switch from Sparrow to Riss-4.27 : the maximal number of flips for Sparrow (by default 500 million) and the CPU time for Sparrow (by default 150 seconds). Also, in contrast to Riss-4.27 , SparrowToRiss does not pre-discretize its numerical parameters, but expresses them as 36 integer and 16 continuous parameters.\nLingeling [17] is a successor to the 2013 version; it was submitted to the tracks Industrial SAT+UNSAT and crafted SAT+UNSAT . Compared to 2013, Lingeling \u2019s parameter space grew by roughly a third, to a total of 323 parameters (meaning that again, Lingeling was the solver with the most parameters). As in 2013, roughly 40% of these parameters were categorical and the rest integervalued (many with a trivial upper bound of max-integer, 231 \u2212 1). Notable groups of parameters that were introduced in the 2014 version include additional preprocessing/inprocessing options and new restart strategies."}, {"heading": "5.2. Configuration Pipeline", "text": "In the CSSC 2014, we used the configurators ParamILS , GGA, and SMAC . For each benchmark and solver, we ran GGA and SMAC on the solver\u2019s full configuration space, which could contain an arbitrary combination of numerical\nand categorical parameters. We also ran all configurators on a discretized version of the configuration space (automatically constructed unless provided by the solver authors), yielding a total of five configuration approaches: ParamILS - discretized, GGA, GGA-discretized, SMAC -discretized, and SMAC . GGA could not handle the complex conditionals of some solvers; therefore, for these solvers we only ran ParamILS and the two SMAC variants.\nDue to the cost of running a third configurator on nearly every scenario, we reduced the budget for each configuration approach from two CPU days on five cores in CSCC 2013 to two CPU days on four cores in CSSC 2014. In the case of ParamILS and SMAC , as in 2013, we used these four cores to perform four independent 2-day configurator runs. In the case of GGA, we performed one 2-day run using all four cores. We evaluated the configurations resulting from each of the 14 configuration runs (4 ParamILS -discretized, 4 SMAC -discretized, 4 SMAC , 1 GGA-discretized, and 1 GGA) on the entire training data set and selected the configuration with the best performance. We then executed only this configuration on the test set to determine the performance of the configured solver."}, {"heading": "5.3. Results", "text": "For each of the four tracks of CSSC 2014, we configured the solvers submitted to the track on each of the three benchmark families from that track and aggregated results across the respective test instances. We show the winners for each track in Table 9 and discuss the results in the following sections. Additional details, tables, and figures are provided in an accompanying technical report [47]."}, {"heading": "5.3.1. Results of the Industrial SAT+UNSAT Track", "text": "The Industrial SAT+UNSAT track consisted of three industrial benchmarks detailed in Appendix A.1: BMC [15], Circuit Fuzz [23], and IBM [83]. Figure 4 visualizes the results of applying algorithm configuration to the winning solver Lingeling on these three benchmark sets. It shows similar results as in the Industrial SAT+UNSAT track of CSSC 2013: Lingeling \u2019s strong default performance on \u2018typical\u2019 hardware verification benchmarks (IBM and BMC) could only be improved slightly by configuration, but much larger improvements were possible on less standard benchmarks, such as Circuit Fuzz .\nTable 10 summarizes the results for all six solvers that participated in the Industrial SAT+UNSAT track. These results demonstrate that, in contrast to Lingeling , several solvers (in particular, Clasp-3.0.4-p8 , Riss-4.27 , and SparrowToRiss) benefited largely from configuration on the BMC benchmark, but did not\nResults for CSSC 2014 Industrial SAT+UNSAT track\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(a) BMC PAR-10: 222\u2192 221\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(b) Circuit Fuzz PAR-10: 316\u2192 193\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(c) IBM PAR-10: 697\u2192 694\nFigure 4: Scatter plots of default vs. configured Lingeling , the gold-medal winner of the Industrial SAT+UNSAT track of CSSC 2014.\n#timeouts default \u2192 # timeouts configured (on test set) Rank BMC Circuit Fuzz IBM Overall def CSSC\nLingeling 20\u2192 20 30\u2192 18 69\u2192 69 119\u2192 107 2 1 Minisat-HACK-999ED 22\u2192 22 21\u2192 19 70\u2192 70 113\u2192 111 1 2 Clasp-3.0.4-p8 44\u2192 30 18\u2192 12 71\u2192 71 133\u2192 113 4 3 Riss-4.27 39\u2192 26 20\u2192 22 72\u2192 72 131\u2192 120 3 4 Cryptominisat 40\u2192 37 31\u2192 20 70\u2192 69 141\u2192 126 5 5 SparrowToRiss 62\u2192 36 29\u2192 21 72\u2192 72 163\u2192 129 6 6\nTable 10: Results for CSSC 2014 competition track Industrial SAT+UNSAT. For each solver and benchmark, we show the number of test set timeouts achieved with the default and the configured parameter setting, bold-facing the better one. We aggregated results across all benchmarks to compute the final ranking.\nreach Lingeling \u2019s performance even after configuration. Minisat-HACK-999ED performed even better than Lingeling with its default parameters, but did not benefit from configuration as much as Lingeling (particularly on the Circuit Fuzz benchmark family)."}, {"heading": "5.3.2. Results of the crafted SAT+UNSAT Track", "text": "The crafted SAT+UNSAT track consisted of the three crafted benchmarks detailed in Appendix A.2: Graph Isomorphism (GI), Low Autocorrelation Binary Sequence (LABS), and N-Rooks . Figure 5 visualizes the improvements configuration yielded on these benchmarks for the best-performing solver, Clasp-3.0.4-p8 . The effect of configuration was particularly large on the N-Rooks instances, where it reduced the number of timeouts from 81 to 0. Similar to the results from CSSC 2013, configuration also substantially improved performance on the GI instances, decreasing the number of timeouts from 43 to 9. In contrast to 2013, an unusual effect occurred for Clasp-3.0.4-p8 on the LABS instances, where the\nResults for CSSC 2014 crafted SAT+UNSAT track\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(a) GI PAR-10: 370\u2192 90\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(b) LABS PAR-10: 755\u2192 804\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(c) N-Rooks PAR-10: 705\u2192 5\nFigure 5: Scatter plots of default vs. configured Clasp-3.0.4-p8 , the gold medal winner of the crafted SAT+UNSAT track of CSSC 2014.\n#timeouts default \u2192 # timeouts configured (on test set) Rank GI LABS N-Rooks Overall def CSSC\nClasp-3.0.4-p8 43\u2192 9 87\u2192 93 81\u2192 0 211\u2192 102 5 1 Lingeling 11\u2192 5 101\u2192 104 3\u2192 0 115\u2192 109 1 2 Cryptominisat 43\u2192 24 95\u2192 89 2\u2192 1 140\u2192 114 3 3 Riss-4.27 43\u2192 30 91\u2192 88 2\u2192 0 136\u2192 118 2 4 Minisat-HACK-999ED 50\u2192 50 91\u2192 91 0\u2192 0 141\u2192 141 4 5 YalSAT 186\u2192 186 218\u2192 207 351\u2192 351 755\u2192 744 6 6\nSparrowToRiss(disq.) 55\u2192 42 98\u2192 94 3\u2192 0 156\u2192 136 - -\nTable 11: Results for CSSC 2014 competition track crafted SAT+UNSAT. For each solver and benchmark, we show the number of test set timeouts achieved with the default and the configured parameter settings, bold-facing the better one. We aggregated results across all benchmarks to compute the final ranking. SparrowToRiss was disqualified from this track, since it returned \u2018satisfiable\u2019 for one instance without producing a model.\nnumber of timeouts on the test set increased from 87 to 93 by configuration. This phenomenon can be caused by overtuning on the training set and/or by insufficiently long configuration runs (which can result in worse-than-default performance on the entire training set). We further investigated this after the competition and found that the second of these explanations applied in this case; we therefore performed additional, longer post-competition configuration runs and found that these, as expected, did yield improved training performance. However, some overtuning to training instances also occurred and, as a consequence, the configuration with best training performance still solved two test instances less than the default.\nTable 11 summarizes the results of all solvers on the crafted SAT+UNSAT track, showing that the performance of many other solvers also substantially\nResults for CSSC 2014 Random SAT+UNSAT track\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(a) 3cnf PAR-10: 309\u2192 35\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(b) K3 PAR-10: 7.91\u2192 2.66\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(c) unif-k5 PAR-10: 0.74\u2192 0.30\nFigure 6: Scatter plots of default vs. configured Clasp-3.0.4-p8 , the gold medal winner of the Random SAT+UNSAT track of CSSC 2014.\n#timeouts default \u2192 # timeouts configured (on test set) Rank 3cnf K3 unif-k5 Overall def CSSC\nClasp-3.0.4-p8 18\u2192 0 0\u2192 0 0\u2192 0 18\u2192 0 2 1 DCCASat+march-rw 1\u2192 0 0\u2192 0 1\u2192 0 2\u2192 0 1 2 Minisat-HACK-999ED 166\u2192 99 5\u2192 1 0\u2192 0 171\u2192 100 5 3 Riss-4.27 160\u2192 113 2\u2192 2 1\u2192 0 163\u2192 115 4 4 SparrowToRiss 126\u2192 126 8\u2192 1 0\u2192 0 134\u2192 127 3 5\nTable 12: Results for CSSC 2014 competition track Random SAT+UNSAT. For each solver and benchmark, we show the number of test set timeouts achieved with the default and the configured parameter settings, boldfacing the better one; we broke ties by the solver\u2019s average runtime (not shown for brevity, but the average runtimes important for tie breaking were 13 seconds for configured Clasp-3.0.4-p8 and 21 seconds for configured DCCASat+march-rw). We aggregated results across all benchmarks to compute the final ranking.\nimproved on the benchmarks GI and N-Rooks , and only mildly (if at all) on the LABS benchmark. The aggregate results across these 3 benchmark families show that Lingeling had the best default performance, but only benefited mildly from configuration (#timeouts reduced from 115 to 109), whereas Clasp-3.0.4-p8 benefited much more from configuration and thus outperformed Lingeling after configuration (#timeouts reduced from 211 to 102). Once again, we note that the winning solver only showed mediocre performance based on its default: Clasp-3.0.4-p8 would have ranked 5th in a comparison based on default performance."}, {"heading": "5.3.3. Results of the Random SAT+UNSAT Track", "text": "The Random SAT+UNSAT track consisted of three random benchmarks detailed in Appendix A.3: 3cnf , K3 , and unif-k5 . The instances in unif-k5 are all unsatisfiable, while the other two sets contain both satisfiable and unsatisfiable instances. Figure 6 visualizes the improvements achieved by configuration on these benchmarks for the best-performing solver Clasp-3.0.4-p8 . Clasp-3.0.4-p8 benefited most from configuration on benchmark 3cnf , where it reduced the number of timeouts from 18 to 0. For the other benchmarks, it could already solve all instances in its default parameter configuration, but configuration helped reduce its average runtime by factors of 3 (K3 ) and 2 (unif-k5 ), respectively. Table 12 summarizes the results of all solvers for these benchmarks. It shows that solver DCCASat+march-rw showed the best default performance, and that after configuration, it also solved all instances from the three benchmark sets. The tie between it and Clasp-3.0.4-p8 was only broken using their average runtime across these instances: 13 seconds for Clasp-3.0.4-p8 and 21 seconds for DCCASat+march-rw ."}, {"heading": "5.3.4. Results of the Random SAT Track", "text": "The Random SAT track consisted of the three benchmarks detailed in Appendix A.3: 3sat1k, 5sat500 and 7sat90. Figure 7 visualizes the improvements configuration achieved on these benchmarks for the best-performing solver ProbSAT . ProbSAT benefited most from configuration on benchmark 5sat500 : its default did not solve a single instance in the maximum runtime of 300 seconds, while its configured version solved all instances in an average runtime below 2 seconds! Since timeouts at 300s yield a PAR-10 score of 3000, the PAR-10 speedup factor on this benchmark was 1 500, the largest we observed in the CSSC. On the other two scenarios, configuration was also very beneficial, reducing ProbSAT \u2019s number of timeouts from 24 to 0 (7sat90) and from 10 to 4 (3sat1k), respectively. Table 13 summarizes the results of all solvers for these benchmarks, showing that next to ProbSAT , only SparrowToRiss benefited from configuration. Neither of the CDCL solvers (Clasp-3.0.4-p8 and Minisat-HACK-999ED) solved a single instance in any of the three benchmarks (in either default or configured variants). For the other two SLS solvers, YalSAT and CSCCSat2014 , the defaults were already well tuned for these benchmark sets. Indeed, we observed overtuning to the training sets in one case each: YalSAT for 3sat1k and CSCCSat2014 for 7sat90. Overall, the configurability of ProbSAT and SparrowToRiss allowed them to place first and second, respectively, despite their poor default performance (especially on 5sat500, where neither of them solved a single instance with default settings)."}, {"heading": "6. Post-Competition Analyses", "text": "While the previous sections focussed on the results of the respective competitions, we now discuss several analyses we performed afterwards to study overarching phenomena and general patterns.\nResults for CSSC 2014 Random SAT track\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(a) 3sat1k PAR-10: 132\u2192 53\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(b) 5sat500 PAR-10: 3000\u2192 2\n10\u22122 10\u22121 1 10 100\nDefault in sec.\n10\u22122\n10\u22121\n1\n10\n100\nC o n fi g u re\nd i n s\ne c.\n2x\n2x\n10x\n10x\n100x\n100x\n300\n300\ntimeout\ntim eo\nut\n(c) 7sat90 PAR-10: 337\u2192 15\nFigure 7: Scatter plots of default vs. configured ProbSAT , the gold medal winner of the Random SAT track of CSSC 2014.\n#timeouts default \u2192 # timeouts configured (on test set) Rank 3sat1k 5sat500 7sat90 Overall def CSSC\nProbSAT 10\u2192 4 250\u2192 0 24\u2192 0 284\u2192 4 4 1 SparrowToRiss 9\u2192 5 250\u2192 0 3\u2192 3 262\u2192 8 3 2 CSCCSat2014 2\u2192 2 0\u2192 0 3\u2192 6 5\u2192 8 1 3 YalSAT 6\u2192 7 0\u2192 0 5\u2192 5 11\u2192 12 2 4 Clasp-3.0.4-p8 250\u2192 250 250\u2192 250 250\u2192 250 750\u2192 750 5 5 Minisat-HACK-999ED 250\u2192 250 250\u2192 250 250\u2192 250 750\u2192 750 6 6\nTable 13: Results for CSSC 2014 competition track Random SAT. For each solver and benchmark, we show the number of test set timeouts achieved with the default and the configured parameter settings, bold-facing the better one; we broke ties by the solver\u2019s average runtime (not shown for brevity). We aggregated results across all benchmarks to compute the final ranking."}, {"heading": "6.1. Overall Configurability of Solvers", "text": "Some solvers consistently benefited more from configuration than others. Here, we quantify the configurability of a solver on a given benchmark by the PAR-10 speedup factor its configured version achieved over its default version, computed on the set of instances solved by at least one of the two. We then examine the relationship between configurability and number of parameters to determine whether solvers with many parameters consistently benefited more or less from configuration than solvers with few parameters.5\n5Of course, it is simple to construct examples where a solver with a single parameter is highly configurable (e.g., let the parameter have a poor default setting) or where a solver has many parameters but does not benefit from configuration at all (e.g., a solver could expose many parameters that are not actually used at all). The focus of our analysis is therefore on the relationship between configurability and the number of parameters that a solver author reasonably expected would be useful to expose.\nFigure 8 shows that configurability was indeed high for solvers with many parameters (e.g., the variants of Lingeling , Riss, and Clasp), but that it did not increase monotonically in the number of parameters: some solvers with very few parameters were surprisingly configurable. For example, configuration sped up the single-parameter solver DCCASat+march-rw by at least by a factor of four in all three benchmarks it was configured for, while the 4-parameter solver CSCCSat2014 was not improved at all by configuration. Furthermore, ProbSAT , which achieved the best single-benchmark performance improvement (as previously discussed in Section 5.3.4), has only 9 parameters.\nWe note that the notion of configurability used here is strongly dependent on the time budget available for configuration. In the next section, we investigate this issue in more detail."}, {"heading": "6.2. Impact of Configuration Budget", "text": "The runtime budget we allow to configure each solver has an obvious impact on the results. In one extreme case, if we let this budget go towards zero, the configuration pipeline returns the solver defaults (and we are back in the setting of the standard SAT competition). For small, non-zero budgets, we can expect solvers with few parameters to benefit from configuration more, since their configuration spaces are easier to search. On the other hand, if we increase the time budget, solvers with larger parameter spaces are likely to benefit more than\nthose with smaller parameter spaces (since larger parts of their configuration space can be searched given additional time).\nFigure 9 illustrates this phenomenon for the two top solvers in the Random SAT+UNSAT track of CSSC 2014. With the competition\u2019s configuration budget of two days across 4 cores, Clasp-3.0.4-p8 performed better than DCCASat+march-rw (both solved all test instances, with average runtimes of 13 vs. 21 seconds). In the extreme case of no time budget for configuration, DCCASat+march-rw would have won against Clasp-3.0.4-p8 , since its default version performed much better (2 vs. 18 timeouts), and, in fact, Figure 9a shows that it required a configuration budget of at least 104 seconds to find improving Clasp-3.0.4-p8 parameters for the 3cnf benchmark (where the default version of Clasp-3.0.4-p8 produced 18 timeouts). While the configuration of DCCASat+march-rw \u2019s single parameter had long converged by 104 seconds, the configuration of Clasp-3.0.4-p8 \u2019s 75 parameters continued to improve performance until the end of the configuration budget, and, in particular for the 3cnf benchmark, performance would have likely continued to improve further if the budget had been larger.\nWe thus conclude that the solver\u2019s flexibility should be chosen in relation to the available budget for configuration: solvers with few parameters can often be improved more quickly than highly flexible solving frameworks, but, given enough computational resources and powerful configurators, the latter ones can typically offer a greater performance potential."}, {"heading": "6.3. Results with a Single Configurator", "text": "While the CSSC addressed the performance of SAT solvers rather than the performance of configurators, we have been asked whether our complex configuration pipeline was necessary, or whether a single configurator would have produced similar or identical results. Indeed, counting the choice of discretized vs non-discretized parameter space, our pipeline used five configuration approaches\n(ParamILS -discretized, GGA, GGA-discretized, SMAC -discretized, and SMAC ). Thus, if one of these approaches had yielded the same results all by itself, we could have reduced our overall configuration budget five-fold.\nTo determine whether this was the case, we evaluated the solver performance we would have observed if we had used each configuration approach in isolation. For each configuration scenario and each approach, we computed the PAR-10 slowdown factor over the CSSC result as the PAR-10 achieved with the respective approach, divided by the PAR-10 of the approach with best training performance (which we selected in the CSSC). If a configuration approach achieves a PAR-10 slowdown factor close to one, this means that it gives rise to solver performance close to that achieved by our full CSSC configuration pipeline. For each solver, we then computed the geometric mean of these factors across the scenarios it was configured for.\nTable 14 shows that both SMAC variants performed close to best for all solvers, meaning that we would have achieved similar results had we only used SMAC in the CSSC. ParamILS yielded the next best performance, followed by GGA. Full results can be found in the accompanying technical report [47]. Despite SMAC \u2019s strong performance, we believe it will still be useful to run several configuration approaches in future CSSCs, both to ensure robustness and to assess whether some configuration scenarios are better suited to other configuration approaches."}, {"heading": "7. Conclusion", "text": "In this article, we have described the design of the Configurable SAT Solver Challenge (CSSC) and the details of CSSC 2013 and CSSC 2014. We have highlighted two main insights that we gained from this competition.\n1. Automated algorithm configuration often improved performance substantially, in several cases yielding average speedups of orders of magnitude.\n2. Some solvers benefited more from automated configuration than others, leading to substantially different algorithm rankings after configuration than before (as, e.g., measured by the SAT competition).\nAlso, the configuration budget influenced which algorithm would perform best, and with the competition budget of 2 days on 4\u20135 cores, algorithms with larger parameter spaces exhibited more capacity for improvement.\nThese conclusions have interesting implications for algorithm design: if an algorithm is likely to be applied across a range of specialized applications, then it should be made flexible by parameterization of its key mechanisms and components, and this flexibility should be exploited by automated algorithm configuration. Our findings thus challenge the traditional approach to solver design that tries to avoid having too many algorithm parameters (since these parameters complicate manual tuning and analyis). Rather, they promote the design paradigm of Programming by Optimization (PbO) [40], which aims to avoid premature design choices and to rather actively develop promising alternatives for parts of the design that enable an automated customization to achieve peak performance on particular benchmarks of interest. Indeed, in the CSSC, we have already observed a trend towards PbO, as evidenced by the introduction of a host of new parameters into state-of-the-art solvers, such as Riss-4.27 and Lingeling , between 2013 and 2014.\nFinally, there is no reason why a configurable solver competition should be appropriate and insightful only for SAT. On the contrary, similar events would be interesting in the context of many other challenging computational problems, such as answer set programming, constraint programming or AI planning. Another interesting application domain is automatic machine learning, where algorithm configuration can adapt flexible machine learning frameworks to each new dataset at hand [78]. We believe that for those and many other problems, similar findings to those we reported here for CSSC would be obtained, leading to analogous conclusions regarding algorithm design."}, {"heading": "Acknowledgements", "text": "Many thanks go to Kevin Tierney for his generous help with running GGA, including his addition of new features, his suggestion of parameter settings and his conversion script to read pcs format. We also thank the solver developers for proofreading the description of their solvers and their parameters. For computational resources to run the competition, we thank Compute Canada (CSSC 2013) and the German Research Foundation (DFG; CSSC 2014). F.\nHutter and M. Lindauer thank the DFG for funding this research under Emmy Noether grant HU 1900/2-1. H. Hoos acknowledges funding through an NSERC Discovery Grant."}, {"heading": "Appendix A. Benchmark Sets Used", "text": "We mirrored the three main categories of instances from the SAT competition: industrial, crafted, and random. In 2014, we also included a category of satisfiable random instances from the SAT Races. For each of these categories, we used various benchmark sets, each of them split into a training set to be used for algorithm configuration and a disjoint test set.\nFor each category, to weight all benchmarks equally, we used the same number of test instances from each benchmark; these test sets were subsampled uniformly at random from the respective complete test sets.\nAll benchmarks are summarized in Tables 1 and 7 in the main text."}, {"heading": "Appendix A.1. Industrial Benchmark Sets", "text": "SWV. This set of SAT-encoded software verification instances consists of 604 instances generated with the CALYSTO static checker [4], used for the verification of five programs: the spam filter Dspam, the SAT solver HyperSAT, the Wine Windows OS emulator, the gzip archiver, and a component of xinetd (a secure version of inetd). We used the same training/test split as Hutter et al. (2007), containing 302 training instances and 302 test instances. We used this benchmark set in the 2013 CSSC. (In 2014, we only used it for preliminary tests since it is quite easy for modern solvers.)\nHardware Verification (IBM). This set of SAT-encoded bounded model checking instances consists of 765 instances generated by Zarpas (2005) These instances were originally selected by Hutter et al. (2007) as the instances in 40 randomlyselected folders from the IBM Formal Verification Benchmarks Library. We used their original training/test split, containing 382 training instances and 383 test instances. We used this benchmark set in both the 2013 and 2014 CSSCs.\nCircuit Fuzz. These instances were produced by a circuit-based CNF fuzzing tool, FuzzSAT [23] (version 0.1). As FuzzSAT was originally designed to produce semi-realistic test cases for debugging SAT solvers, the majority of the instances it produces are trivial; however, occasionally, it produces more challenging instances. The CircuitFuzz instances were found by generating 10,000 FuzzSAT instances and removing all those that could be solved within one second by Lingeling . This instance generator was originally described in detail by Bayless et al. (2014); we used the 300 instances from that paper as the training set (except one quite easy instance, \u2018fuzz 100 25433.cnf\u2019, which was dropped unintentionally by a script) and produced 585 additional instances using the same method, to form a testing set. We used this benchmark set in both the 2013 and 2014 CSSCs.\nBounded Model Checking 2008 (BMC). This set of SAT instances was derived by unrolling the 2008 Hardware Model Checking Competition circuits [18]. Each of these instances is a sequential circuit with safety properties. Each circuit was unrolled to 50, 100, and 200 iterations using the tool aigunroll (version 1.9.4) from the AIGER tools [15]. We omitted trivial instances that were proven SAT or UNSAT during the unrolling process. While we used the entire set in 2013, in 2014 we removed the 60 instances provided by Intel in order to allow us to publicly share the instances."}, {"heading": "Appendix A.2. Crafted Benchmark Sets", "text": "Graph Isomorphism (GI). These instances were first used in the 2013 SAT Competition [66] and were generated by encoding the graph isomorphism problem to SAT according to the procedure described by Torn (2013). Given two graphs G1 and G2 with n vertices and m edges (for whom the isomorphism problem is to be solved) the generator creates a SAT formula with n2 variables and O(n) + O(n3) + O(n4) clauses. Consequently, the generated instances can contain very many clauses. The 2 064 SAT instances in this set were generated from different types of graphs. We split the instances uniformly at random into 1 032 training and 1 032 test instances, which we used in both the 2013 and 2014 CSSCs.\nLow Autocorrelation Binary Sequence (LABS). This set contains 651 lowautocorrelation binary sequence (LABS) search problems that were encoded to SAT problems by first encoding them as pseudo-Boolean problems and then as SAT problems. Instances from this set were first used in the SAT Competition 2013 in the crafted category [67]. We split this benchmark set uniformly at\nrandom into 350 training and 351 test instances, and used it in both the 2013 and 2014 CSSCs.\nN-Rooks. These 835 instances [65] represent a parameterized unsatisfiable variation of the well-known n-queens problem, in which the task is to place n queens on a chess board with n\u00d7 n fields such that they do not attack each other. In the variation considered here, the (unsatisfiable) problem is to place n+ 1 rooks on a board of size n\u00d7n. Additional constraints enforcing that there is a piece in each row/column make it easier to prove unsatisfiability, and these constraints can be enabled or disabled by a generator parameter. We used this benchmark set in the 2014 CSSC, selecting 484 training instances uniformly at random and the remaining 351 as test instances."}, {"heading": "Appendix A.3. Random Benchmark Sets", "text": "K3. This is a set of 600 randomly-generated 3-SAT instances at the phase transition (clause to variable ratio of approximately 4.26). It includes both satisfiable and unsatisfiable instances. The set includes 100 instances each with 200 variables (853 clauses), 225 variables (960 clauses), 250 variables (1066 clauses), 275 variables (1172 clauses), 300 variables (1279 clauses), and 325 variables (1385 clauses). These 600 instances were generated by Lin Xu using the random instance generator from the 2009 SAT competition, and were previously described by Bayless et al. (2014). We used their uniform random split into 300 training and 300 test instances in both the 2013 and 2014 CSSCs (random track).\n3cnf. This is a set of 750 random 3-SAT instances (satisfiable and unsatisfiable) at the phase transition, with 350 variables and 1493 clauses. These instances were generated by the ToughSAT instance generator [12] and split into 500 training and 250 test instances uniformly at random. We used this benchmark set in the 2014 CSSC (random track).\nunif-k5. This set contains only unsatisfiable 5-SAT instances generated uniformly at random with 50 variables and 1 056 clauses (a clause-to-variable ratio sharply on the phase transition). The instances were generated by the uniform random generator used in the SAT Challenge 2012 and SAT Competition 2013, with satisfiable instances being filtered out by running the SLS solver ProbSAT .We used this benchmark set in both the 2013 and 2014 CSSCs (random track).\n3sat1k. This is a set of 500 3-SAT instances at the phase transition, all satisfiable. Each instance has 1000 variables and 4260 clauses. These instances were previously described by Tompkins et al. (2009). We used their uniform random split into 250 training and test instances in the 2013 CSSC (random track) and in the 2014 CSSC (random satisfiable track).\n5sat500. This set contains 500 5-SAT instances generated uniformly at random with a clause-to-variable ratio of 20. Each instance is satisfiable and has 500 variables and 10000 clauses. This set was first used for tuning the SAT solver Captain Jack and other SLS solvers [79]. We used the original uniform random split into 250 training and test instances in the 2014 CSSC (random satisfiable track).\n7sat90. This set contains 500 7-SAT instances generated uniformly at random with a clause-to-variable ratio of 85. Each instance is satisfiable and has 90 variables and 7650 clauses. This set was also first used for tuning the SAT solver Captain Jack and other SLS solvers [79]. We used the original uniform random split into 250 training and test instances in the 204 CSSC (random satisfiable track)."}, {"heading": "Appendix A.4. Instance Features Used for these Benchmark Sets", "text": "As described in Appendix B.3, SMAC can use instance features to guide its search. Such instance features have predominantly been studied in the work on SATzilla for algorithm selection [68, 82] and in machine learning models for predicting algorithm runtime [48]. These features range from simple summary statistics, such as the number of variables or clauses in an instance, to the results of short, runtime-limited probes with local search solvers. In the context of algorithm configuration, we can afford somewhat more expensive features than for algorithm selection since we only require them on the training instances (not the test instances) and can compute them once, offline. Nevertheless, we kept feature computation costs low to not add substantially to the time required for algorithm configuration.\nFor the instance sets where we already had available instance features from previous work, we used those features. In particular, we used the 138 features described by Hutter et al. (2014) for the datasets SWV , IBM , 3sat1k , 5sat500 , and 7sat90 . For the set unif-k5 , we did not compute features since these instances were very easy to solve even with algorithm defaults (note that SMAC also worked very well without features). For the other datasets, we computed a subset of 119 features, including basic features and feature groups based on survey propagation, clause learning, local search probing, and search space size estimates. The code for computing these features is available at http: //www.cs.ubc.ca/labs/beta/Projects/EPMs/."}, {"heading": "Appendix B. Configuration Procedures", "text": "This appendix describes the configuration procedures we used in more detail. Configurators typically iterate the following steps: (1) execute the target algorithm on one or more instances with one or more configurations for a limited amount of time; (2) measure the resulting performance metric and (3) decide upon the next target algorithm execution. Beyond the key question of which configuration to try next, configurators also need to decide how many runs and\nwhich instances to use for each evaluation, and after which time to terminate unsuccessful runs. ParamILS, SMAC , and GGA differ in how they instantiate these components."}, {"heading": "Appendix B.1. ParamILS: Local Search in Configuration Space", "text": "ParamILS [46], short for iterated local search in parameter configuration space, generalizes the simple (often manually performed) tuning approach of changing one parameter at a time and keeping changes if performance improves. While that simple tuning approach is a local search that terminates in the first local optimum, ParamILS carries out an iterated local search [56] that applies perturbation steps in each local optimum o in order to escape o\u2019s basin of attraction and carry out another local search that leads to another local optimum o\u2032. Iterated local search then decides whether to continue from the new optimum o\u2032 or to return to the previous optimum o, thereby performing a biased random walk over locally optimal solutions. ParamILS only supports categorical parameters, so numerical parameters need to be discretized before ParamILS is run.\nParamILS is an algorithm framework with two different instantiations that differ in their strategy of deciding how many runs to use to evaluate each configuration. The most straightforward instantiation, BasicILS (N), resembles the approach most frequently used in manual parameter optimization: it evaluates each configuration according to a fixed number of N runs on a fixed set of instances. While this approach is simple and intuitive, it gives rise to the problem of how to set the number N . Setting N to a large value yields slow evaluations; using a small number yields fast evaluations, but the evaluations are often not representative for the instance set \u03a0 (for example, if we choose N runs we can cover at most N instances, even if we only allow a single run per instance). The second ParamILS instantiation, FocusedILS, solves this problem by allocating most of its runs to strong configurations: it starts with a single run per configuration and incrementally performs more runs for promising configurations. This means that it can often afford a large number of runs for the best configurations while rejecting most poor configurations based on a few runs. There is also a guarantee that configurations that were \u2018unlucky\u2019 can be revisited in the search, allowing for a proof that FocusedILS\u2014if run indefinitely\u2014will eventually identify the configuration with the best performance on the entire training set.\nFinally, ParamILS also implements a mechanism for adaptively choosing the time after which to terminate unsuccessful target algorithm runs. Intuitively, when comparing the performance of two configurations \u03b81 and \u03b82 on an instance, and we already know that \u03b81 solves the instance in time t1, we do not need to run \u03b82 for longer than t1: we do not need to know precisely how bad \u03b82 is, as long as we know that \u03b81 is better. More precisely, each comparison of configurations in ParamILS is with respect to an instance set \u03a0sub \u2282 \u03a0, and evaluations of \u03b82 can be terminated prematurely when \u03b82\u2019s aggregated performance on \u03a0sub is provably worse than that of \u03b81. In practice, this so-called adaptive capping mechanism can speed up ParamILS \u2019s progress by orders of magnitude when the\nbest configuration solves instances much faster than the overall maximal cutoff time [46].\nFor all experiments in this paper, we used the FocusedILS variant of the most recent publicly available ParamILS release 2.3.76 with default parameters."}, {"heading": "Appendix B.2. GGA: Gender-based Genetic Algorithm", "text": "The Gender-based Genetic Algorithm (GGA) [1] is a configuration procedure that maintains a population of configurations and proceeds according to an evolutionary metaphor, evolving the population over a number of generations in which pairs of configurations mate and produce offspring. GGA also uses the concept of gender : each configuration is labeled with a gender chosen uniformly at random, and when configurations are selected to mate there are separate selection pressures for each gender: configurations from the first gender are selected based on their empirical performance, whereas configurations from the other gender are selected uniformly at random. The second gender thus serves as a pool of diversity, countering premature convergence to a poor parameter configuration.\nUnlike ParamILS \u2019 local search mechanism, GGA\u2019s recombination operator for combining the parameter values of two parent configurations can operate directly on numerical parameter domains, avoiding the need for discretization.\nLike ParamILS , GGA implements an adaptive capping mechanism, elegantly combining it with a parallelization mechanism that lets it effectively use multiple processing units. GGA only ever evaluates configurations in the selection step for the first gender, and its strategy is to evaluate several candidates in parallel until the first one succeeds. Here, the number of configurations to be evaluated in parallel is taken to be identical to the number of processing units available, #units.7\nLike the FocusedILS variant of ParamILS , GGA also implements an \u201cintensification\u201d mechanism for increasing the number of runs N it performs for each configuration over time. Specifically, it keeps N constant in each generation, starting with small Nstart in the first generation, and linearly increasing N up to a larger Ntarget in generation Gtarget and thereafter; Nstart, Ntarget, and Gtarget, are parameters of GGA.\nFor all experiments in the CSSC, we used the most recent publicly available version of GGA, version 1.3.2.8 GGA\u2019s author Kevin Tierney kindly provided a script to convert the parameter configuration space description for each solver from the competition\u2019s pcs format (http://aclib.net/cssc2014/pcs-format. pdf) to GGA\u2019s native xml format. This script allowed us to run GGA for all solvers except those with complex conditionals.\n6http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/ 7This coupling of adaptive capping and parallelization is the reason that GGA should not\nbe run on a single core if the objective is to minimize runtime. 8https://wiwi.uni-paderborn.de/dep3/entscheidungsunterstuetzungssysteme-undoperations-research-jun-prof-dr-tierney/research/source-code/\nNext to the parameters #units, Nstart, Ntarget, and Gtarget mentioned above, free GGA parameters include the maximal number of generations, Gmax and the size of the population, Psize. The setting of these parameters considerably affects GGA\u2019s behaviour and also determines its overall runtime (when run to completion). If there is an external fixed time budget (as in the CSSC), these parameters can be modified to ensure that GGA does not finish far too early (thus not making effective use of the available configuration budget) while simultaneously ensuring that runs do not take far too long (in which case configuration would be cut off in one of the first generations, where the search is basically still random sampling). It is thus important to set GGA\u2019s parameters carefully. We set the following parameters to values hand-chosen by Kevin Tierney for the CSSC (leaving all other parameters at their default values): #units = 4, Psize = 50, Gtarget = 75, Gmax = 100, Nstart = 4, Ntarget = #(training instances in the scenario). 9\nWe performed a post hoc analysis, which suggests that these parameters may yet not be optimal: GGA often finished relatively few generations within its configuration budget. It might thus make sense to use a smaller value of Ntarget in the future to reduce the number of instances considered per configuration. However, this means that GGA would never consider all instances and may overtune as a result. How to best set GGA\u2019s parameters is therefore an open research question."}, {"heading": "Appendix B.3. SMAC: Sequential Model-based Algorithm Configuration", "text": "In contrast to the model-free configurators ParamILS and GGA, SMAC [45] is a sequential model-based algorithm configuration method, which means that it uses predictive models of algorithm performance [48] to guide its search for good configurations. More specifically, it uses previously observed \u3008configuration, performance\u3009 pairs \u3008\u03b8, f(\u03b8)\u3009 to learn a random forest of regression trees (see, e.g., [22]) that express a function f\u0302 : \u0398\u2192 R predicting the performance of arbitrary parameter configurations (including those not yet evaluated) and then uses this function to guide its search. When instance characteristics x\u03c0 \u2208 F are available for each problem instance \u03c0, SMAC uses observed \u3008configuration, instance characteristic, performance\u3009 triplets \u3008\u03b8, x\u03c0, f(\u03b8, \u03c0)\u3009 to learn a function g\u0302 : \u0398\u00d7 F \u2192 R that predicts the performance of arbitrary parameter configurations on instances with arbitrary characteristics. These so-called empirical performance models [48] are then marginalized over the instance characteristics of all training\nbenchmark instances in order to derive the function f\u0302 that predicts average performance for each parameter configuration: f\u0302(\u03b8) = E\u03c0\u223c\u03a0train [g\u0302(\u03b8, \u03c0)] .\nThis performance model is used in a sequential optimization process as follows. After an initialization phase, SMAC iterates the following three steps: (1) use\n9Actually, due to a miscommunication, we first ran experiments with Ntarget = 2000, obtaining somewhat worse results than reported here. After double-checking with Kevin Tierney we then re-ran everything with the correct value of Ntarget that depended on the number of training instances in each configuration scenario. We only report these latter results here.\nthe performance measurements observed so far to fit a marginal random forest model f\u0302 ; (2) use f\u0302 to select a promising configuration \u03b8 \u2208 \u0398 to evaluate next, trading off exploration in new parts of the configuration space and exploitation in parts of the space known to perform well; and (3) run \u03b8 on one or more benchmark instances and compare its performance to the best configuration observed so far.\nSMAC employs a similar criterion as FocusedILS to determine how many runs to perform for each configuration, and for finite configuration spaces in the limit it also provably converges to the best configuration on the training set. Unlike ParamILS, SMAC does not require that the parameter space be discretized.\nWhen used to optimize target algorithm runtime, SMAC implements an adaptive capping mechanism similar to the one used in ParamILS . When this capping mechanism prematurely terminates an algorithm run we only observe a lower bound of the algorithm\u2019s runtime. In order to construct predictive models of algorithm runtime in the presence of such so-called right-censored data points, SMAC applies model-building techniques derived from the survival analysis literature [44]."}], "references": [{"title": "A gender-based genetic algorithm for the automatic configuration of algorithms", "author": ["C. Ans\u00f3tegui", "M. Sellmann", "K. Tierney"], "venue": "Proceedings of the Fifteenth International Conference on Principles and Practice of Constraint Programming (CP\u201909),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Predicting learnt clauses quality in modern SAT solvers", "author": ["G. Audemard", "L. Simon"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Structural abstraction of software verification conditions", "author": ["D. Babi\u0107", "A. Hu"], "venue": "Proceedings of the international conference on Computer Aided Verification (CAV\u201907),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Exploiting shared structure in software verification conditions", "author": ["D. Babi\u0107", "A.J. Hu"], "venue": "Proceedings of the International Conference on Hardware and Software: Verification and Testing (HVC\u201908),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Spear theorem prover. Solver description, SAT competition", "author": ["D. Babi\u0107", "F. Hutter"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Proceedings of SAT Competition 2013: Solver and Benchmark Descriptions, volume B- 2013-1 of Department of Computer Science Series of Publications B. University of Helsinki", "author": ["A. Balint", "A. Belov", "M. Heule", "M. J\u00e4rvisalo"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Choosing probability distributions for stochastic local search and the role of make versus break", "author": ["A. Balint", "U. Sch\u00f6ning"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Using CSP look-back techniques to solve real-world SAT instances", "author": ["R.J. Bayardo Jr.", "R. Schrag"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Evaluating instance generators by configuration", "author": ["S. Bayless", "D. Tompkins", "H. Hoos"], "venue": "Proceedings of the Eighth International Conference on Learning and Intelligent Optimization (LION\u201914), Lecture Notes in Computer Science. Springer-Verlag", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Hard SAT instances based on factoring", "author": ["J. Bebel", "H. Yuen"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Proceedings of SAT Competition 2014: Solver and Benchmark Descriptions, volume B- 2014-2 of Department of Computer Science Series of Publications B. University of Helsinki", "author": ["A. Belov", "D. Diepold", "M. Heule", "M. J\u00e4rvisalo"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The sat4j library, release 2.2, system description", "author": ["D.L. Berre", "A. Parrain"], "venue": "Journal on Satisfiability, Boolean Modeling and Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "The AIGER and-inverter graph (AIG) format. Available at fmv.jku.at/aiger", "author": ["A. Biere"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Lingeling, Plingeling and Treengeling entering the SAT competition", "author": ["A. Biere"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Yet another local search solver and Lingeling and friends entering the SAT competition", "author": ["A. Biere"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Benchmarks from the 2008 hardware model checking competition (HWMCC\u201908)", "author": ["A. Biere", "A. Cimatti", "K.L. Claessen", "T. Jussila", "K. McMillan", "F. Somenzi"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Symbolic model checking using SAT procedures instead of BDDs", "author": ["A. Biere", "A. Cimatti", "E. Clarke", "M. Fujita", "Y. Zhu"], "venue": "In Proceedings of Design Automation Conference (DAC\u201999),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Detecting cardinality constraints in CNF", "author": ["A. Biere", "D. Le Berre", "E. Lonca", "N. Manthey"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Automated testing and debugging of SAT and QBF solvers", "author": ["R. Brummayer", "F. Lonsing", "A. Biere"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs", "author": ["C. Cadar", "D. Dunbar", "D.R. Engler"], "venue": "In Proceedings of the 8th USENIX conference on Operating systems design and implementation (OSDI\u201908),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "A tool for checking ANSI-C programs. In Tools and Algorithms for the Construction and Analysis of Systems", "author": ["E. Clarke", "D. Kroening", "F. Lerda"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "The complexity of theorem proving procedures", "author": ["S. Cook"], "venue": "Proceedings of the Third Annual ACM Symposium on the Theory of Computing", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1971}, {"title": "Experimental results on the application of satisfiability algorithms to scheduling problems", "author": ["J. Crawford", "A. Baker"], "venue": "In Proceedings of the national conference on Artificial Intelligence (AAAI\u201994),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1994}, {"title": "gNovelty+GC: Weight-Enhanced Diversification on Stochastic Local Search for SAT", "author": ["Duong", "T.-T", "Pham", "D.-N"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "An extensible sat-solver", "author": ["N. E\u00e9n", "N. S\u00f6rensson"], "venue": "Proceedings of the Sixth International Conference on Theory and Applications of Satisfiability Testing (SAT\u201903),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "The first learning track of the international planning competition", "author": ["A. Fern", "R. Khardon", "P. Tadepalli"], "venue": "Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Conflict-driven answer set solving: From theory to practice", "author": ["M. Gebser", "B. Kaufmann", "T. Schaub"], "venue": "Artificial Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Toward leaner binary-clause reasoning in a satisfiability solver", "author": ["A.V. Gelder"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Heavy-tailed phenomena in satisfiability and constraint satisfaction problems", "author": ["C. Gomes", "B. Selman", "N. Crato", "H. Kautz"], "venue": "Journal of Automated Reasoning.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Simpsat 1.0 for sat challenge", "author": ["Han", "C.-S"], "venue": "Proceedings of SAT Challenge", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "When Boolean satisfiability meets Gaussian elimination in a simplex way", "author": ["Han", "C.-S", "Jiang", "J.-H"], "venue": "Computer Aided Verification,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "On-the-fly clause improvement", "author": ["H. Han", "F. Somenzi"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Efficient CNF simplification based on binary implication graphs", "author": ["M.J. Heule", "M. Jrvisalo", "A. Biere"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Stochastic Local Search: Foundations & Applications", "author": ["H. Hoos", "T. Sttzle"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Programming by optimization", "author": ["H.H. Hoos"], "venue": "Commun. ACM,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Boosting verification by automatic tuning of decision procedures", "author": ["F. Hutter", "D. Babi\u0107", "H. Hoos", "A. Hu"], "venue": "Formal Methods in Computer Aided Design (FMCAD\u201907),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Results of the Configurable SAT Solver Challenge", "author": ["F. Hutter", "A. Balint", "S. Bayless", "H. Hoos", "K. Leyton-Brown"], "venue": "Technical Report 276,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Tradeoffs in the empirical evaluation of competing algorithm designs", "author": ["F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "Annals of Mathematics and Artificial Intelligenc (AMAI), Special Issue on Learning and Intelligent Optimization,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Bayesian optimization with censored response data", "author": ["F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "In NIPS workshop on Bayesian Optimization,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Sequential modelbased optimization for general algorithm configuration", "author": ["F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION\u201911),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "ParamILS: An automatic algorithm configuration framework", "author": ["F. Hutter", "H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "Results of the Configurable SAT Solver Challenge", "author": ["F. Hutter", "M. Lindauer", "S. Bayless", "H. Hoos", "K. Leyton-Brown"], "venue": "Technical Report 277,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Algorithm runtime prediction: Methods and evaluation", "author": ["F. Hutter", "L. Xu", "H. Hoos", "K. Leyton-Brown"], "venue": "Artificial Intelligence,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "The international SAT solver competitions", "author": ["M. J\u00e4rvisalo", "D.L. Berre", "O. Roussel", "L. Simon"], "venue": "AI Magazine,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Blocked clause elimination", "author": ["M. J\u00e4rvisalo", "A. Biere", "M.J. Heule"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Pushing the envelope: Planning, propositional logic, and stochastic search", "author": ["H. Kautz", "B. Selman"], "venue": "Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1996}, {"title": "Unifying SAT-based and graph-based planning", "author": ["H. Kautz", "B. Selman"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "SATenstein: Automatically building local search sat solvers from components", "author": ["A. KhudaBukhsh", "L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}, {"title": "The irace package, iterated race for automatic algorithm configuration", "author": ["M. L\u00f3pez-Ib\u00e1\u00f1ez", "J. Dubois-Lacoste", "T. St\u00fctzle", "M. Birattari"], "venue": "Technical report,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2011}, {"title": "Iterated local search. In Handbook of Metaheuristics (pp. 321\u2013353)", "author": ["H. Louren\u00e7o", "O. Martin", "T. St\u00fctzle"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "Focused random walk with configuration checking and break minimum for satisfiability", "author": ["C. Luo", "S. Cai", "W. Wu", "K. Su"], "venue": "Proceedings of the Ninth International Conference on Principles and Practice of Constraint Programming (CP\u201913),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2012}, {"title": "Double configuration checking in stochastic local search for satisfiability", "author": ["C. Luo", "S. Cai", "W. Wu", "K. Su"], "venue": "Proceedings of the Twenty-eighth National Conference on Artificial Intelligence", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Probing-based preprocessing techniques for propositional satisfiability", "author": ["I. Lynce", "J.P. Marques-Silva"], "venue": "IEEE International Conference on Tools with Artificial Intelligence,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2003}, {"title": "Coprocessor 2.0\u2013a flexible CNF simplifier", "author": ["N. Manthey"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2012}, {"title": "The SAT solver RISS3G at SC", "author": ["N. Manthey"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Automated reencoding of Boolean formulas", "author": ["N. Manthey", "M.J. Heule", "A. Biere"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2013}, {"title": "Formula simplifications as DRAT derivations", "author": ["N. Manthey", "T. Philipp"], "venue": "KI 2014: Advances in Artificial Intelligence,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2014}, {"title": "Too many rooks", "author": ["N. Manthey", "P. Steinke"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2014}, {"title": "2013a). Sat encoded graph isomorphism benchmark description", "author": ["F. Mugrauer", "A. Balint"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2013}, {"title": "Sat encoded low autocorrelation binary sequence (labs) benchmark description", "author": ["F. Mugrauer", "A. Balint"], "venue": null, "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2013}, {"title": "Understanding random sat: Beyond the clauses-to-variables ratio", "author": ["E. Nudelman", "K. Leyton-Brown", "H.H. Hoos", "A. Devkar", "Y. Shoham"], "venue": "Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming (CP\u201904),", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2004}, {"title": "Minisat hack 999ed, minisat hack 1430ed and swdia5by", "author": ["C. Oh"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "A survey of recent advances in SAT-based formal verification", "author": ["M. Prasad", "A. Biere", "A. Gupta"], "venue": "International Journal on Software Tools for Technology Transfer,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2005}, {"title": "Controlling a solver execution with the runsolver tool", "author": ["O. Roussel"], "venue": "Journal on Satisfiability, Boolean Modeling and Computation,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}, {"title": "Fast downward SMAC", "author": ["J. Seipp", "S. Sievers", "F. Hutter"], "venue": "Planner abstract,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2014}, {"title": "The sat2002 competition report", "author": ["L. Simon", "D.L. Berre", "E. Hirsch"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2005}, {"title": "Extending SAT solvers to cryptographic problems", "author": ["M. Soos", "K. Nohl", "C. Castelluccia"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2009}, {"title": "Combinational test generation using satisfiability", "author": ["P. Stephan", "R. Brayton", "A. Sangiovanni-Vencentelli"], "venue": "IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1996}, {"title": "Auto- WEKA: combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\u201913),", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2013}, {"title": "Captain jack: New variable selection heuristics in local search for sat", "author": ["D. Tompkins", "A. Balint", "H. Hoos"], "venue": null, "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2009}, {"title": "On the resolution complexity of graph non-isomorphism", "author": ["J. Torn"], "venue": "Theory and Applications of Satisfiability Testing SAT 2013,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2013}, {"title": "Another look at graph coloring via propositional satisfiability", "author": ["A. van Gelder"], "venue": "In Proceedings of Computational Symposium on Graph Coloring and Generalizations (COLOR-02),", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2002}, {"title": "SATzilla: Portfolio-based algorithm selection for SAT", "author": ["L. Xu", "F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2008}], "referenceMentions": [{"referenceID": 21, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 196, "endOffset": 208}, {"referenceID": 62, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 196, "endOffset": 208}, {"referenceID": 20, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 196, "endOffset": 208}, {"referenceID": 67, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 231, "endOffset": 239}, {"referenceID": 19, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 231, "endOffset": 239}, {"referenceID": 45, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 253, "endOffset": 261}, {"referenceID": 46, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 253, "endOffset": 261}, {"referenceID": 22, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 274, "endOffset": 278}, {"referenceID": 71, "context": "It is relevant both for theory (having been the first problem proven to be NP-hard [27]) and for practice (having important applications in many fields, such as hardware and software verification [19, 70, 26], test-case generation [77, 24], AI planning [51, 52], scheduling [28], and graph colouring [81]).", "startOffset": 300, "endOffset": 304}, {"referenceID": 43, "context": "The SAT community has a long history of regularly assessing the state of the art via competitions [49].", "startOffset": 98, "endOffset": 102}, {"referenceID": 65, "context": "2002 [74], and the event has been growing over time: in 2014, it had a record participation of 58 solvers by 79 authors in 11 tracks [13].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "2002 [74], and the event has been growing over time: in 2014, it had a record participation of 58 solvers by 79 authors in 11 tracks [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 2, "context": "In practical applications of SAT, solvers can typically be adjusted to perform well for the specific type of instances at hand, such as software verification instances generated by a particular static checker on a particular software system [3], or a particular family of bounded model checking instances [83].", "startOffset": 241, "endOffset": 244}, {"referenceID": 35, "context": "Solvers typically come with robust default parameter settings meant to provide good all-round performance, but it is widely known that adjusting parameter settings to particular target instance classes can yield orders-of-magnitude speedups [41, 53, 79].", "startOffset": 241, "endOffset": 253}, {"referenceID": 47, "context": "Solvers typically come with robust default parameter settings meant to provide good all-round performance, but it is widely known that adjusting parameter settings to particular target instance classes can yield orders-of-magnitude speedups [41, 53, 79].", "startOffset": 241, "endOffset": 253}, {"referenceID": 69, "context": "Solvers typically come with robust default parameter settings meant to provide good all-round performance, but it is widely known that adjusting parameter settings to particular target instance classes can yield orders-of-magnitude speedups [41, 53, 79].", "startOffset": 241, "endOffset": 253}, {"referenceID": 64, "context": "(In fact, the 2014 IPC learning track for non-portfolio solvers was won by FastDownward-SMAC [73], a system that employs a similar combination of general algorithm configuration and a highly parameterized solver framework as we do in the CSSC.", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "We based this choice partly on the fact that many solvers have runtime distributions with rather long tails (or even heavy tails [34]), and that practitioners often use many instances and relatively short runtimes to benchmark solvers for a new application domain.", "startOffset": 129, "endOffset": 133}, {"referenceID": 37, "context": "There is also evidence that SAT competition results would remain quite similar if based on shorter runtimes, but not if based on fewer instances [43].", "startOffset": 145, "endOffset": 149}, {"referenceID": 63, "context": "We handled all such conditions in a generic wrapper script that used Olivier Roussel\u2019s runsolver tool [71] to limit runtime and memory, and counted any errors or limit violations as timeouts at the maximum runtime of 300 seconds.", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "In recent years, the AI community has developed several dedicated systems for this general algorithm configuration problem [46, 1, 55, 45].", "startOffset": 123, "endOffset": 138}, {"referenceID": 0, "context": "In recent years, the AI community has developed several dedicated systems for this general algorithm configuration problem [46, 1, 55, 45].", "startOffset": 123, "endOffset": 138}, {"referenceID": 48, "context": "In recent years, the AI community has developed several dedicated systems for this general algorithm configuration problem [46, 1, 55, 45].", "startOffset": 123, "endOffset": 138}, {"referenceID": 39, "context": "In recent years, the AI community has developed several dedicated systems for this general algorithm configuration problem [46, 1, 55, 45].", "startOffset": 123, "endOffset": 138}, {"referenceID": 35, "context": "[41] configured the algorithm Spear [5] on formal verification instances, achieving a 500-fold speedup on software verification instances generated with the static checker Calysto [3] and a 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[41] configured the algorithm Spear [5] on formal verification instances, achieving a 500-fold speedup on software verification instances generated with the static checker Calysto [3] and a 4.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "[41] configured the algorithm Spear [5] on formal verification instances, achieving a 500-fold speedup on software verification instances generated with the static checker Calysto [3] and a 4.", "startOffset": 180, "endOffset": 183}, {"referenceID": 47, "context": "Algorithm configuration has also enabled the development of general frameworks for stochastic local search SAT solvers that can be automatically instantiated to yield state-of-the-art performance on new types of instances; examples for such frameworks are SATenstein [53] and Captain Jack [79].", "startOffset": 267, "endOffset": 271}, {"referenceID": 69, "context": "Algorithm configuration has also enabled the development of general frameworks for stochastic local search SAT solvers that can be automatically instantiated to yield state-of-the-art performance on new types of instances; examples for such frameworks are SATenstein [53] and Captain Jack [79].", "startOffset": 289, "endOffset": 293}, {"referenceID": 40, "context": "While all of these applications used the local-search based algorithm configuration method ParamILS [46], in the CSSC we wanted to avoid bias that could", "startOffset": 100, "endOffset": 104}, {"referenceID": 40, "context": "1An alternative definition considers the optimization of expected performance across a distribution of instances rather than average performance across a set of instances [46].", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "arise from commitment to one particular algorithm configuration method and thus used all three existing general algorithm configuration methods for runtime optimization: ParamILS , GGA [1], and SMAC [45].", "startOffset": 185, "endOffset": 188}, {"referenceID": 39, "context": "arise from commitment to one particular algorithm configuration method and thus used all three existing general algorithm configuration methods for runtime optimization: ParamILS , GGA [1], and SMAC [45].", "startOffset": 199, "endOffset": 203}, {"referenceID": 7, "context": "The eleven submitted solvers ranged from complete solvers based on conflict-directed clause learning (CDCL; [10]) to stochastic local search (SLS; [39]) solvers.", "startOffset": 108, "endOffset": 112}, {"referenceID": 33, "context": "The eleven submitted solvers ranged from complete solvers based on conflict-directed clause learning (CDCL; [10]) to stochastic local search (SLS; [39]) solvers.", "startOffset": 147, "endOffset": 151}, {"referenceID": 48, "context": "2We did not use the iterated racing method I/F-Race [55], since it does not effectively support runtime optimization and its authors thus discourage its use for this purpose (personal communication with Manuel L\u00f3pez-Ib\u00e1\u00f1ez and Thomas St\u00fctzle).", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "0k 182k\u00b1 206k [4] IBM 383 302 96.", "startOffset": 14, "endOffset": 17}, {"referenceID": 18, "context": "3k [23] BMC 807 302 446k\u00b1 992k 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "70m [18] GI 1032 351 11.", "startOffset": 4, "endOffset": 8}, {"referenceID": 58, "context": "03m [66, 80] LABS 350 351 75.", "startOffset": 4, "endOffset": 12}, {"referenceID": 70, "context": "03m [66, 80] LABS 350 351 75.", "startOffset": 4, "endOffset": 12}, {"referenceID": 59, "context": "7k 154k\u00b1 153k [67] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 5sat500 250 250 500\u00b1 0 10000\u00b1 0 [79]", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "7k 154k\u00b1 153k [67] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 5sat500 250 250 500\u00b1 0 10000\u00b1 0 [79]", "startOffset": 48, "endOffset": 52}, {"referenceID": 69, "context": "7k 154k\u00b1 153k [67] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 5sat500 250 250 500\u00b1 0 10000\u00b1 0 [79]", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "subset Gnovelty+GCa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+GCwa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+PCL 5 0 0 0 20 000 \u2013 \u2013 [29] Simpsat 5 0 0 0 2 400 \u2013 \u2013 [35] Sat4j 10 0 0 4 2\u00d7 10 \u2013 \u2013 [14] Solver43 12 0 0 0 5\u00d7 10 \u2013 \u2013 [6] Forl-nodrup 44 0 0 0 3\u00d7 10 \u2013 \u2013 [76] Clasp-2.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "subset Gnovelty+GCa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+GCwa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+PCL 5 0 0 0 20 000 \u2013 \u2013 [29] Simpsat 5 0 0 0 2 400 \u2013 \u2013 [35] Sat4j 10 0 0 4 2\u00d7 10 \u2013 \u2013 [14] Solver43 12 0 0 0 5\u00d7 10 \u2013 \u2013 [6] Forl-nodrup 44 0 0 0 3\u00d7 10 \u2013 \u2013 [76] Clasp-2.", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "subset Gnovelty+GCa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+GCwa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+PCL 5 0 0 0 20 000 \u2013 \u2013 [29] Simpsat 5 0 0 0 2 400 \u2013 \u2013 [35] Sat4j 10 0 0 4 2\u00d7 10 \u2013 \u2013 [14] Solver43 12 0 0 0 5\u00d7 10 \u2013 \u2013 [6] Forl-nodrup 44 0 0 0 3\u00d7 10 \u2013 \u2013 [76] Clasp-2.", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "subset Gnovelty+GCa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+GCwa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+PCL 5 0 0 0 20 000 \u2013 \u2013 [29] Simpsat 5 0 0 0 2 400 \u2013 \u2013 [35] Sat4j 10 0 0 4 2\u00d7 10 \u2013 \u2013 [14] Solver43 12 0 0 0 5\u00d7 10 \u2013 \u2013 [6] Forl-nodrup 44 0 0 0 3\u00d7 10 \u2013 \u2013 [76] Clasp-2.", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "subset Gnovelty+GCa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+GCwa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+PCL 5 0 0 0 20 000 \u2013 \u2013 [29] Simpsat 5 0 0 0 2 400 \u2013 \u2013 [35] Sat4j 10 0 0 4 2\u00d7 10 \u2013 \u2013 [14] Solver43 12 0 0 0 5\u00d7 10 \u2013 \u2013 [6] Forl-nodrup 44 0 0 0 3\u00d7 10 \u2013 \u2013 [76] Clasp-2.", "startOffset": 169, "endOffset": 173}, {"referenceID": 66, "context": "subset Gnovelty+GCa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+GCwa 2 0 0 0 110 \u2013 \u2013 [29] Gnovelty+PCL 5 0 0 0 20 000 \u2013 \u2013 [29] Simpsat 5 0 0 0 2 400 \u2013 \u2013 [35] Sat4j 10 0 0 4 2\u00d7 10 \u2013 \u2013 [14] Solver43 12 0 0 0 5\u00d7 10 \u2013 \u2013 [6] Forl-nodrup 44 0 0 0 3\u00d7 10 \u2013 \u2013 [76] Clasp-2.", "startOffset": 237, "endOffset": 241}, {"referenceID": 26, "context": "3 42 34 7 60 \u221e 10 \u2013 [32] Riss3g 125 0 0 107 2\u00d7 10 \u2013 \u2013 [61] Riss3gExt 193 0 0 168 2\u00d7 10 \u2013 \u2013 [61] Lingeling 102 139 0 0 1\u00d7 10 1\u00d7 10 2\u00d7 10 [16]", "startOffset": 20, "endOffset": 24}, {"referenceID": 54, "context": "3 42 34 7 60 \u221e 10 \u2013 [32] Riss3g 125 0 0 107 2\u00d7 10 \u2013 \u2013 [61] Riss3gExt 193 0 0 168 2\u00d7 10 \u2013 \u2013 [61] Lingeling 102 139 0 0 1\u00d7 10 1\u00d7 10 2\u00d7 10 [16]", "startOffset": 54, "endOffset": 58}, {"referenceID": 54, "context": "3 42 34 7 60 \u221e 10 \u2013 [32] Riss3g 125 0 0 107 2\u00d7 10 \u2013 \u2013 [61] Riss3gExt 193 0 0 168 2\u00d7 10 \u2013 \u2013 [61] Lingeling 102 139 0 0 1\u00d7 10 1\u00d7 10 2\u00d7 10 [16]", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "3 42 34 7 60 \u221e 10 \u2013 [32] Riss3g 125 0 0 107 2\u00d7 10 \u2013 \u2013 [61] Riss3gExt 193 0 0 168 2\u00d7 10 \u2013 \u2013 [61] Lingeling 102 139 0 0 1\u00d7 10 1\u00d7 10 2\u00d7 10 [16]", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "Gnovelty+GCa and Gnovelty+GCwa [29] are closely related SLS solvers.", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "Gnovelty+PCL [29] is an SLS solver with five parameters: one binary parameter (determining whether the stagnation path is dynamic or static) and four numerical parameters: the length of the stagnation path, the size of the time window storing stagnation paths, the probability of smoothing stagnation weights, and the probability of smoothing clause weights.", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "Simpsat [35] is a CDCL solver based on Cryptominisat [75], which adds additional strategies for explicitly handling XOR constraints [36].", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "Simpsat [35] is a CDCL solver based on Cryptominisat [75], which adds additional strategies for explicitly handling XOR constraints [36].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "Sat4j [14] is full-featured library of solvers for Boolean satisfiability and optimization problems.", "startOffset": 6, "endOffset": 10}, {"referenceID": 66, "context": "Forl-nodrup [76] is a CDCL solver with 44 parameters.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "3 [32] is a solver for the more general answer set programming (ASP) problem, but it can also solve SAT, MAXSAT and PB problems.", "startOffset": 2, "endOffset": 6}, {"referenceID": 54, "context": "Riss3g [61] is a CDCL solver with 125 parameters.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "These include 6 numerical parameters from MiniSAT [30], 10 numerical parameters from Glucose [2], 17 mostly numerical Riss3G parameters, and 92 parameters controlling preprocessing/inprocessing performed by the integrated Coprocessor [60].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "These include 6 numerical parameters from MiniSAT [30], 10 numerical parameters from Glucose [2], 17 mostly numerical Riss3G parameters, and 92 parameters controlling preprocessing/inprocessing performed by the integrated Coprocessor [60].", "startOffset": 93, "endOffset": 96}, {"referenceID": 53, "context": "These include 6 numerical parameters from MiniSAT [30], 10 numerical parameters from Glucose [2], 17 mostly numerical Riss3G parameters, and 92 parameters controlling preprocessing/inprocessing performed by the integrated Coprocessor [60].", "startOffset": 234, "endOffset": 238}, {"referenceID": 13, "context": "parameters resemble those in Lingeling [16], emphasizing blocked clause elimination [50], bounded variable addition [63], and probing [59].", "startOffset": 39, "endOffset": 43}, {"referenceID": 44, "context": "parameters resemble those in Lingeling [16], emphasizing blocked clause elimination [50], bounded variable addition [63], and probing [59].", "startOffset": 84, "endOffset": 88}, {"referenceID": 55, "context": "parameters resemble those in Lingeling [16], emphasizing blocked clause elimination [50], bounded variable addition [63], and probing [59].", "startOffset": 116, "endOffset": 120}, {"referenceID": 52, "context": "parameters resemble those in Lingeling [16], emphasizing blocked clause elimination [50], bounded variable addition [63], and probing [59].", "startOffset": 134, "endOffset": 138}, {"referenceID": 54, "context": "Riss3gExt [61] is an experimental extension of Riss3g .", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Lingeling [16] is a CDCL solver with 241 parameters (making it the solver with the largest configuration space in the CSSC 2013).", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "Additional details, tables, and figures are provided in an accompanying technical report [42].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "1: Bounded Model Checking 2008 (BMC) [15], Circuit Fuzz [23], Hardware Verification (IBM) [83], and SWV [4].", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "1: Bounded Model Checking 2008 (BMC) [15], Circuit Fuzz [23], Hardware Verification (IBM) [83], and SWV [4].", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "1: Bounded Model Checking 2008 (BMC) [15], Circuit Fuzz [23], Hardware Verification (IBM) [83], and SWV [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 18, "context": "3k [23] BMC 604 302 424k\u00b1 843k 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "30m [18] GI 1032 351 11.", "startOffset": 4, "endOffset": 8}, {"referenceID": 58, "context": "03m [66, 80] LABS 350 351 75.", "startOffset": 4, "endOffset": 12}, {"referenceID": 70, "context": "03m [66, 80] LABS 350 351 75.", "startOffset": 4, "endOffset": 12}, {"referenceID": 59, "context": "7k 154k\u00b1 153k [67] N-Rooks 484 351 38.", "startOffset": 14, "endOffset": 18}, {"referenceID": 57, "context": "4k 125k\u00b1 126k [65] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] 3cnf 500 250 350\u00b1 0 1493\u00b1 0 [12] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 3sat1k 250 250 500\u00b1 0 10000\u00b1 0 [79] 5sat500 250 250 1000\u00b1 0 4260\u00b1 0 [79] 7sat90 250 250 90\u00b1 0 7650\u00b1 0 [79]", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "4k 125k\u00b1 126k [65] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] 3cnf 500 250 350\u00b1 0 1493\u00b1 0 [12] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 3sat1k 250 250 500\u00b1 0 10000\u00b1 0 [79] 5sat500 250 250 1000\u00b1 0 4260\u00b1 0 [79] 7sat90 250 250 90\u00b1 0 7650\u00b1 0 [79]", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "4k 125k\u00b1 126k [65] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] 3cnf 500 250 350\u00b1 0 1493\u00b1 0 [12] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 3sat1k 250 250 500\u00b1 0 10000\u00b1 0 [79] 5sat500 250 250 1000\u00b1 0 4260\u00b1 0 [79] 7sat90 250 250 90\u00b1 0 7650\u00b1 0 [79]", "startOffset": 81, "endOffset": 85}, {"referenceID": 69, "context": "4k 125k\u00b1 126k [65] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] 3cnf 500 250 350\u00b1 0 1493\u00b1 0 [12] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 3sat1k 250 250 500\u00b1 0 10000\u00b1 0 [79] 5sat500 250 250 1000\u00b1 0 4260\u00b1 0 [79] 7sat90 250 250 90\u00b1 0 7650\u00b1 0 [79]", "startOffset": 149, "endOffset": 153}, {"referenceID": 69, "context": "4k 125k\u00b1 126k [65] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] 3cnf 500 250 350\u00b1 0 1493\u00b1 0 [12] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 3sat1k 250 250 500\u00b1 0 10000\u00b1 0 [79] 5sat500 250 250 1000\u00b1 0 4260\u00b1 0 [79] 7sat90 250 250 90\u00b1 0 7650\u00b1 0 [79]", "startOffset": 186, "endOffset": 190}, {"referenceID": 69, "context": "4k 125k\u00b1 126k [65] K3 300 250 262\u00b1 43 1116\u00b1 182 [11] 3cnf 500 250 350\u00b1 0 1493\u00b1 0 [12] unif-k5 300 250 50\u00b1 0 1056\u00b1 0 \u2013 3sat1k 250 250 500\u00b1 0 10000\u00b1 0 [79] 5sat500 250 250 1000\u00b1 0 4260\u00b1 0 [79] 7sat90 250 250 90\u00b1 0 7650\u00b1 0 [79]", "startOffset": 220, "endOffset": 224}, {"referenceID": 31, "context": "Indeed, after the competition, Riss3gExt \u2019s developer found a bug in it (in onthe-fly clause improvement [37]) that caused some satisfiable instances to be incorrectly labeled as unsatisfiable.", "startOffset": 105, "endOffset": 109}, {"referenceID": 51, "context": "discretized original DCCASat+march-rw 1 0 0 0 9 9 Random [58] CSCCSat2014 3 0 0 0 567 567 Random SAT [57, 58] ProbSAT 5 1 3 4 1\u00d7 105 \u221e Random SAT [9] Minisat-HACK-999ED 10 0 0 3 8\u00d7 105 8\u00d7 105 All categories [69] YalSAT 16 10 0 0 5\u00d7 106 2\u00d7 1072 Crafted&Random SAT [17] Cryptominisat 14 15 7 2 3\u00d7 1024 \u221e Industrial & Crafted [75] Clasp-3.", "startOffset": 57, "endOffset": 61}, {"referenceID": 50, "context": "discretized original DCCASat+march-rw 1 0 0 0 9 9 Random [58] CSCCSat2014 3 0 0 0 567 567 Random SAT [57, 58] ProbSAT 5 1 3 4 1\u00d7 105 \u221e Random SAT [9] Minisat-HACK-999ED 10 0 0 3 8\u00d7 105 8\u00d7 105 All categories [69] YalSAT 16 10 0 0 5\u00d7 106 2\u00d7 1072 Crafted&Random SAT [17] Cryptominisat 14 15 7 2 3\u00d7 1024 \u221e Industrial & Crafted [75] Clasp-3.", "startOffset": 101, "endOffset": 109}, {"referenceID": 51, "context": "discretized original DCCASat+march-rw 1 0 0 0 9 9 Random [58] CSCCSat2014 3 0 0 0 567 567 Random SAT [57, 58] ProbSAT 5 1 3 4 1\u00d7 105 \u221e Random SAT [9] Minisat-HACK-999ED 10 0 0 3 8\u00d7 105 8\u00d7 105 All categories [69] YalSAT 16 10 0 0 5\u00d7 106 2\u00d7 1072 Crafted&Random SAT [17] Cryptominisat 14 15 7 2 3\u00d7 1024 \u221e Industrial & Crafted [75] Clasp-3.", "startOffset": 101, "endOffset": 109}, {"referenceID": 6, "context": "discretized original DCCASat+march-rw 1 0 0 0 9 9 Random [58] CSCCSat2014 3 0 0 0 567 567 Random SAT [57, 58] ProbSAT 5 1 3 4 1\u00d7 105 \u221e Random SAT [9] Minisat-HACK-999ED 10 0 0 3 8\u00d7 105 8\u00d7 105 All categories [69] YalSAT 16 10 0 0 5\u00d7 106 2\u00d7 1072 Crafted&Random SAT [17] Cryptominisat 14 15 7 2 3\u00d7 1024 \u221e Industrial & Crafted [75] Clasp-3.", "startOffset": 146, "endOffset": 149}, {"referenceID": 61, "context": "discretized original DCCASat+march-rw 1 0 0 0 9 9 Random [58] CSCCSat2014 3 0 0 0 567 567 Random SAT [57, 58] ProbSAT 5 1 3 4 1\u00d7 105 \u221e Random SAT [9] Minisat-HACK-999ED 10 0 0 3 8\u00d7 105 8\u00d7 105 All categories [69] YalSAT 16 10 0 0 5\u00d7 106 2\u00d7 1072 Crafted&Random SAT [17] Cryptominisat 14 15 7 2 3\u00d7 1024 \u221e Industrial & Crafted [75] Clasp-3.", "startOffset": 207, "endOffset": 211}, {"referenceID": 14, "context": "discretized original DCCASat+march-rw 1 0 0 0 9 9 Random [58] CSCCSat2014 3 0 0 0 567 567 Random SAT [57, 58] ProbSAT 5 1 3 4 1\u00d7 105 \u221e Random SAT [9] Minisat-HACK-999ED 10 0 0 3 8\u00d7 105 8\u00d7 105 All categories [69] YalSAT 16 10 0 0 5\u00d7 106 2\u00d7 1072 Crafted&Random SAT [17] Cryptominisat 14 15 7 2 3\u00d7 1024 \u221e Industrial & Crafted [75] Clasp-3.", "startOffset": 263, "endOffset": 267}, {"referenceID": 26, "context": "4-p8 38 30 7 55 1\u00d7 1049 \u221e All categories [32] Riss-4.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "27 214 0 0 160 5\u00d7 1086 5\u00d7 1086 All but Random SAT [62] SparrowToRiss 170 36 16 176 1\u00d7 10112 \u221e All categories [8] Lingeling 137 186 0 0 1\u00d7 1053 2\u00b7101341 All categories [17]", "startOffset": 167, "endOffset": 171}, {"referenceID": 51, "context": "DCCASat+march-rw [58] combines the SLS solver DCCASat with the CDCL solver march-rw.", "startOffset": 17, "endOffset": 21}, {"referenceID": 50, "context": "CSCCSat2014 [57, 58] is an SLS solver based on configuration checking and dynamic local search methods.", "startOffset": 12, "endOffset": 20}, {"referenceID": 51, "context": "CSCCSat2014 [57, 58] is an SLS solver based on configuration checking and dynamic local search methods.", "startOffset": 12, "endOffset": 20}, {"referenceID": 6, "context": "ProbSAT [9] is a simple SLS solver based on probability distributions that are built from simple features, such as the make and break of variables [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "ProbSAT [9] is a simple SLS solver based on probability distributions that are built from simple features, such as the make and break of variables [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 61, "context": "Minisat-HACK-999ED [69] is a CDCL solver; it was submitted to all tracks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "YalSAT [17] is an SLS solver; it was submitted to the tracks crafted SAT+UNSAT and Random SAT .", "startOffset": 7, "endOffset": 11}, {"referenceID": 26, "context": "4-p8 [32] is a solver for the more general answer set programming (ASP) problem, but it can also solve SAT, MAXSAT and PB problems.", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "In particular, it added many new preprocessing and inprocessing techniques, including XOR handling (via Gaussian elimination [36]), and extracting cardinality constraints [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "In particular, it added many new preprocessing and inprocessing techniques, including XOR handling (via Gaussian elimination [36]), and extracting cardinality constraints [20].", "startOffset": 171, "endOffset": 175}, {"referenceID": 27, "context": "The simplification parameters comprise about 20 Boolean switches for preprocessing techniques and about 100 in-processor parameters, prominently including blocked clause elimination, bounded variable addition, equivalance elimination [33], numerical limits, probing, symmetry breaking, unhiding [38], Gaussian elimination, covered literal elimination [64], and even some stochastic local search.", "startOffset": 234, "endOffset": 238}, {"referenceID": 32, "context": "The simplification parameters comprise about 20 Boolean switches for preprocessing techniques and about 100 in-processor parameters, prominently including blocked clause elimination, bounded variable addition, equivalance elimination [33], numerical limits, probing, symmetry breaking, unhiding [38], Gaussian elimination, covered literal elimination [64], and even some stochastic local search.", "startOffset": 295, "endOffset": 299}, {"referenceID": 56, "context": "The simplification parameters comprise about 20 Boolean switches for preprocessing techniques and about 100 in-processor parameters, prominently including blocked clause elimination, bounded variable addition, equivalance elimination [33], numerical limits, probing, symmetry breaking, unhiding [38], Gaussian elimination, covered literal elimination [64], and even some stochastic local search.", "startOffset": 351, "endOffset": 355}, {"referenceID": 14, "context": "Lingeling [17] is a successor to the 2013 version; it was submitted to the tracks Industrial SAT+UNSAT and crafted SAT+UNSAT .", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "Additional details, tables, and figures are provided in an accompanying technical report [47].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "1: BMC [15], Circuit Fuzz [23], and IBM [83].", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "1: BMC [15], Circuit Fuzz [23], and IBM [83].", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "Full results can be found in the accompanying technical report [47].", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "Rather, they promote the design paradigm of Programming by Optimization (PbO) [40], which aims to avoid premature design choices and to rather actively develop promising alternatives for parts of the design that enable an automated customization to achieve peak performance on particular benchmarks of interest.", "startOffset": 78, "endOffset": 82}, {"referenceID": 68, "context": "Another interesting application domain is automatic machine learning, where algorithm configuration can adapt flexible machine learning frameworks to each new dataset at hand [78].", "startOffset": 175, "endOffset": 179}], "year": 2017, "abstractText": "It is well known that different solution strategies work well for different types of instances of hard combinatorial problems. As a consequence, most solvers for the propositional satisfiability problem (SAT) expose parameters that allow them to be customized to a particular family of instances. In the international SAT competition series, these parameters are ignored: solvers are run using a single default parameter setting (supplied by the authors) for all benchmark instances in a given track. While this competition format rewards solvers with robust default settings, it does not reflect the situation faced by a practitioner who only cares about performance on one particular application and can invest some time into tuning solver parameters for this application. The new Configurable SAT Solver Competition (CSSC) compares solvers in this latter setting, scoring each solver by the performance it achieved after a fully automated configuration step. This article describes the CSSC in more detail, and reports the results obtained in its two instantiations so far, CSSC 2013 and 2014.", "creator": "LaTeX with hyperref package"}}}