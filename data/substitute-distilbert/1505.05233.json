{"id": "1505.05233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Visual Understanding via Multi-Feature Shared Learning with Global Consistency", "abstract": "image / video data is usually represented by multiple semantic features. fusion of input - sources information for establishing the identity has been widely recognized. multi - feature visual recognition features later received coverage in multimedia applications. two approaches studies visual understanding via a tightly proposed ph _ 2 - norm based 15 - feature jointly managed learning framework, which can simultaneously learn the global label matrix and explicit classifiers from the labeled discrete data represented by multiple feature modalities. additionally, a multi - modal group graph manifold regularizer formed featuring mixed laplacian subgroup hessian graph are proposed for better preserving the manifold structure of different features on the labeled data, while preserving the label consistency and improving the label prediction power via semi - supervised learning. the merits of the proposed multi - feature learning framework lie in jointly sharing the resultant information from multiple functions in several classifier learning communities based on a mixed graph regularizer on one hand, and an efficient alternating optimization method for fast classifier training on the one hand. experiments on several benchmark visual datasets, such as 17 - category oxford flower dataset, the challenging 101 - label caltech dataset, youtube & amp ; consumer videos simulation and large - scale nus - lcd dataset for language understanding all indicated that the proposed approach compares strongly with state - of - the - art algorithms.", "histories": [["v1", "Wed, 20 May 2015 03:01:08 GMT  (1767kb)", "http://arxiv.org/abs/1505.05233v1", "13 pages,8 figures"], ["v2", "Wed, 9 Sep 2015 10:07:11 GMT  (1660kb)", "http://arxiv.org/abs/1505.05233v2", "13 pages,6 figures, this paper is accepted for publication in IEEE Transactions on Multimedia"]], "COMMENTS": "13 pages,8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["lei zhang", "david zhang"], "accepted": false, "id": "1505.05233"}, "pdf": {"name": "1505.05233.pdf", "metadata": {"source": "CRF", "title": "multi-modal group graph manifold regularizer formed by mixed Laplacian and Hessian graph is proposed for better preserving the manifold structure of different features on the labeled data, while preserving the label consistency and improving the label prediction power via semi-supervised learning. The merits of the proposed multi-feature learning framework lie in jointly sharing the structural information from multiple features in global classifier learning phase based on a mixed graph regularizer on one hand, and an efficient alternating optimization method for fast classifier training on the other hand. Experiments on several benchmark visual datasets, such as 17-category Oxford Flower", "authors": [], "emails": ["leizhang@cqu.edu.cn).", "csdzhang@comp.polyu.edu.hk)."], "sections": [{"heading": null, "text": "\nAbstract\u2014Image/video data is usually represented by multiple visual features. Fusion of multi-sources information for establishing the identity has been widely recognized. Multi-feature visual recognition has recently received attention in multimedia applications. This paper studies visual understanding via a newly proposed -norm based multi-feature jointly sharing learning framework, which can simultaneously learn the global label matrix and explicit classifiers from the labeled visual data represented by multiple feature modalities. Additionally, a multi-modal group graph manifold regularizer formed by mixed Laplacian and Hessian graph is proposed for better preserving the manifold structure of different features on the labeled data, while preserving the label consistency and improving the label prediction power via semi-supervised learning. The merits of the proposed multi-feature learning framework lie in jointly sharing the structural information from multiple features in global classifier learning phase based on a mixed graph regularizer on one hand, and an efficient alternating optimization method for fast classifier training on the other hand. Experiments on several benchmark visual datasets, such as 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, YouTube & Consumer Videos dataset and large-scale NUS-WIDE dataset for multimedia understanding all demonstrate that the proposed approach compares favorably with state-of-the-art algorithms.\nIndex Terms\u2014Visual recognition, multimedia understanding, sharing learning, multi-feature learning, semi-supervised learning\nI. INTRODUCTION\nulti-modality, multiple views and multiple features are usually used to represent multimedia content and images. For example, given a face image or a video frame, its\nvisual content can be represented by several kinds of weak modalities such as left and right periocular, mouth and nose regions [4] for robust face recognition, or different feature types such as histogram, SIFT, HSV, etc. [9] for action recognition. It therefore becomes a challenging task in multimedia analysis for improving the visual classification performance by simultaneously sharing the structural information from multiple information sources of independent or correlated feature representations.\n L. Zhang is with the College of Communication Engineering, Chongqing University and the Department of Computing, The Hong Kong Polytechnic\nUniversity, Hong Kong (e-mail: leizhang@cqu.edu.cn).\n D. Zhang is with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong (e-mail: csdzhang@comp.polyu.edu.hk).\nIn early, information fusion can be done at three levels: feature level, score level and decision level, whereas feature level fusion can be more discriminative than other two level-fusions [16]. Feature concatenation is a prevalent fusion method which has been used in patter recognition [18], [19]. However, it is less effective in multimedia content analysis, especially when the features are independent or heterogeneous [17], and in particular, simple feature concatenation for high dimensional feature vectors may also become inefficient and non-robust. Therefore, multi-view learning and multiple kernel learning have been developed by researchers in machine learning community to address this problem. One popular work is the two-view based support vector machine (SVM-2k) [11], [21], [22], which jointly learns SVM with two views. Another popular work is multiple kernel learning (MKL) [10], [20], which focus on the information integration from multiple features by combining multiple kernels with respective weights. Besides, the concept of multi-modal joint learning was also involved in dictionary learning and sparse representation. Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24]. Also, several multi-modal joint sparse representation methods were developed for face and visual recognition applications. For examples, in [3], a multi-task joint sparse representation classifier (MTJSRC) was proposed for visual classification in which group sparsity was used to combine multiple features. In [4], a multimodal joint sparse representation and kernel space multimodal sparse model were proposed for robust face recognition. In [30], a joint dynamic sparse representation classifier model was proposed for object recognition. In [48], a very efficient multi-task feature selection model (FSSI) with information sharing in low-rank solution was proposed for multimedia analysis.\nThough these multi-task/multi-view based joint learning performs better than single modality, they depend on sufficient labeled data that may conflict with real-world applications. Therefore, we focus on a semi-supervised learning mechanism that can improve the robustness of learned classifier when label information of training samples is insufficient. It\u2019s known that among semi-supervised learning methods, manifold regularization, specifically, Laplacian graph regularization\nVisual Understanding via Multi-Feature Jointly\nSharing Learning\nLei Zhang, Member, IEEE, and David Zhang, Fellow, IEEE\nM\nlearning becomes the main stream for well exploring the geometry of the intrinsic data. Although Laplacian graph achieves good performance, it has been identified to be biased towards a constant function due to its constant null space and the not well preserved local topology, especially when there are only few labeled data. Comparatively, Hessian regularization has better extrapolating power shown in two facets [5]: first, it has a richer null space and second, it can exploit the intrinsic local geometry of the data manifold very well. For better exploitation of multiple features in classifier learning, excited by spirits of these joint learning concepts discussed above, we consider a more intuitive manner, i.e. multi-feature global label consistency based classifier learning under a mixed Hessian and Laplacian regularization based semi-supervised framework. It\u2019s worth noting that there is no explicit mapping matrix in manifold regression during testing process, therefore, in this work, we will simultaneously learn an explicit global classifier.\nMotivated by these above concerns, a multi-feature group graph regularizer by jointly learning mixed Laplacian and Hessian regularization is proposed for visual categorization. We uniformly name the proposed algorithm as multi-feature sharing based Global Label Consistent Classifier framework (GLCC). The merits of this paper are shown as follows.\n Multiple feature modalities are jointly learned with\neffective knowledge and feature structure sharing for robust visual classification.  To better preserve the manifold structure of training data,\na multi-modal group graph regularizer based on Hessian and Laplacian regularization is presented for label\nconsistency preservation.  Considering that there is no explicit classifier in manifold\nregression, an explicit classifier for global label prediction is simultaneously learned by minimizing the weighted least square loss with global label prediction.  In the proposed method, a -norm based global classifier\nsolved with a very efficient alternating optimization in low computational cost is presented. The overview of the proposed GLCC framework is illustrated in Fig.1. The visual experiments have been conducted on the benchmark visual datasets, including the Oxford flower 17 dataset 1 from [12], the Caltech 101 dataset 2\n1http://www.robots.ox.ac.uk/~vgg/data/flowers/17/index.html 2http://www.robots.ox.ac.uk/~vgg/software/MKL/\nfrom [14], the YouTube & Consumer video dataset 3 from [45], and the large scale real-world NUS-WIDE web image dataset 4 from [53] for multimedia analysis. All experiments demonstrate that our GLCC method outperforms many existing multi-feature and semi-supervised learning methods.\nThe rest of this paper is organized as follows. In section II, we review the most related works in visual recognition and multi-view graph based learning. The proposed -norm based multi-feature global label consistent classifier framework including its formulation and training algorithm is described in section III. The experiments on several benchmark datasets for visual application are employed in Section IV. The convergence and computational time analysis is briefly discussed in section V. Section VI concludes this paper."}, {"heading": "II. RELATED WORKS", "text": "As discussed previously, this work is closely related to the efforts on visual recognition and multi-modal graph based learning. In this section, we will briefly review the current prevailing approaches.\n3 http://vc.sce.ntu.edu.sg/transfer learning domain adaptation/domain adaptation home.html 4http://lms.comp.nus.edu.sg/research/NUS-WIDE.html\nA. Visual Recognition\nA number of methods have been developed for visual recognition, such as face recognition, gender recognition, age estimation, scene categories and object recognition in computer vision community. The bag-of-features (BoF) model has been a popular image categorization, but it discards the spatial order of local descriptors which limits the descriptive power of the image representation. In [2], a spatial pyramid matching (SPM) beyond bags of features was proposed for natural scene categories and object recognition. Yang et al. [40] also proposed a linear SPM based on sparse coding (ScSPM) for visual classification and obtained significant improvement. In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition. However, the common flaw of these methods is their large computational cost. Recently, Yuan et al. [3] proposed a multi-task joint sparse representation (MTJSRC) using mixed-norm for visual\nclassification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28]. Zhang et al. [30] proposed a multi-observation joint dynamic sparse representation for visual recognition, and obtain comparable performance. All these works demonstrate that multi-feature joint learning has a positive effect on robust classifier learning for visual understanding."}, {"heading": "B. Graph based Semi-supervised Learning", "text": "Semi-supervised learning has been widely deployed in the recognition task, due to the fact that training a small amount of labeled data is prone to overfitting, while manual labeling of a large amount of precisely labeled data is tedious and time-consuming. Most related to the paper, a subspace sharing based semi-supervised multiple feature analysis method for action recognition was proposed [8], in which both the global and local structural consistency has been considered in discriminative classifier training. Zhou et al. [31] proposed a graph based semi-supervised method (LGC) for learning local and global consistency by a regularization framework. In [7], a Laplacian graph manifold based semi-supervised learning was proposed, in which a manifold assumption that the manifold structure information of the unlabeled data can be preserved was given. The assumption of consistency means that the nearby points are likely to have the same label and the points on the same cluster/manifold are likely to have the same label. Note that cluster assumption is local while manifold assumption is global. In [51], a semi-supervised feature selection algorithm SFSS for multimedia analysis based on Laplacian graph and l2,1-norm regularization was proposed. Graph manifold based algorithms make use only of the nearest neighbor information to classify the unlabeled data. Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality. In [36], a graph Laplacian based multi-view spectral embedding\n(MSE) method was proposed for dimension reduction. Recently, Yang et al. [9] proposed a multi-feature Laplacian graph based hierarchical semi-supervised regression (MLHR) for multimedia analysis and achieved better performance in video concept annotation. Throughout these manifold based methods presented above, Laplacian graph and single feature are the mainstream of semi-supervised learning, however, it has been identified in [5] that it suffers from the fact that the solution is biased towards a constant with weaker extrapolating power. Hessian graph exploited for semi-supervised dimension reduction [5] was proved to have a good extrapolating power. Therefore, this paper further explores a multi-feature joint learning framework into which Hessian regularization is incorporated for global label consistent classifier learning."}, {"heading": "C. Multi-view Graph based Learning", "text": "Multi-view graph manifold regression has been reported in recent years. Belkin et al. [41] proposed a manifold regularization framework for semi-supervised learning. Laplacian regularized least square and Laplacian support vector machine have been discussed in their work, but in single view. Tong et al. [42] proposed a graph based multi-modality learning method with linear and sequential fusion schemes, but the mapping function in the objective function is implicit. Xia et al. [35] proposed a multi-view graph embedding which calculates an eigenvalue problem in optimization, but for dimension reduction. Wu et al. [43] proposed a sparse multi-modal dictionaries learning with Laplacian hyper-graph as regularization. Wang et al. [8] proposed a semi-supervised multiple feature learning framework in which graph Laplacian regularizer and subspace sharing were studied for action recognition. Therefore, under the multi-view learning and graph manifold, an idea of multi-modal global label consistent classifier is verified in this work."}, {"heading": "III. MULTI-FEATURE GLOBAL LABEL CONSISTENT CLASSIFIER", "text": "In this section, the -norm based GLCC framework with model formulation, optimization, training algorithm, and recognition is presented."}, {"heading": "A. Notations", "text": "Assume that there are n training samples with d-dimensional\nvector from c classes. Denote as the training set of the i-th feature modality,\nas the global label information of\nthe training samples with c classes, and as the predicted label matrix of training data. di denotes the dimension. In this paper, and denote Frobenius norm and -norm, Tr(\u00b7) denotes trace operator. Given a sample vector xi, if xi belongs to the j-th class, and , otherwise. The learned classifier of the i-th feature\nis parameterized as with a bias . The Laplacian and Hessian graph matrix are represented to be and , respectively."}, {"heading": "B. Formulation of GLCC", "text": "Semi-supervised learning approach holds the assumption\nthat nearby points are more likely to have the same labels. In graph based manifold learning, label consistency is embedded in data manifold structure. Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.\n(1)\nwhere \u03b3 and \u03bb are positive trade-off parameters, is a full one vector, F is the global prediction, is a loss function, and is a graph manifold preservation term.\nFor convenient analysis, let , the objective function of graph based manifold regression model can then be written as\n(2)\nwhere denotes the least square loss function used in this work, is a regularization parameter ( ), and\ndenotes the adjacency matrix defined as\n(3)\nwhere denotes the local set represented by k-nearest\nneighbors of xj.\nThe least square loss term of (2) is considered as a weighted\nloss function here, which can be written as\n(4)\nwhere denotes trace operator, and W is a diagonal matrix with entries Wii defined as follows: for semi-supervised use, Wii is set as a large value (e.g. 10 10 ) if the i-th sample is labeled, and 0 otherwise.\nThe second term of (2) is a manifold structure preservation term for exploring label consistency. Specifically, Laplacian graph is used in part to preserve the label information in the manifold built on the training data. It can be written in trace-form as\n(5)\nwhere is a diagonal matrix with entries , and\nis Laplacian graph matrix. As denoted in [5], the graph Laplacian based semi-supervised regression suffers from the fact that the solution is biased towards a constant and the extrapolating power is lost, and further proposed a second-order Hessian energy regularizer that shows a better extrapolation capability than Laplacian regularizer in semi-supervised learning, particularly if only few labeled points are available. Specifically, the total estimated Hessian energy is shown by\n(6)\nwhere is the Hessian energy matrix which is sparse since each data point has only contributions from its neighbors. The\ndetails of Hessian energy estimation and the computation of Hessian energy matrix are shown in Appendix A. Therefore, for exploiting the advantages of both regularizers, the manifold regression model with group graph regularizers can be represented as\n(7)\nwhere in terms of (2) and (4) can be rewritten as\n(8)\nHowever, the representation of in (8) is in single feature. In this paper, a multi-modal concept is studied. Therefore, in multi-modal learning tasks, the objective function\nconstructed with m modalities is rewritten as\n(9)\nwhere\ndenote the contribution\ncoefficients of Laplacian matrix and Hessian energy\nmatrix for the i-th feature modality,\n.\nIn this paper, we let denotes the group graph regularizer. Note that the setting of r>1 is to better exploit the complementary information of multiple modalities and avoid that case with only the best feature\nconsidered, therefore we use and instead of and . In graph based manifold regularization model (7), we observe that there is no explicit classifier to predict the label matrix F. We therefore propose to simultaneously learn the global label matrix F by multi-feature based global classifiers\nand , as formulated as (1).\nSuppose\nto be the training set with n\nsamples of the i-th feature, a multi-feature sharing based global classifier can be shown as\n(10)\nwhere 1n denotes a column vector with all ones, denotes the balance parameter (0< <1), and controls classifier complexity to avoid overfitting.\nBy combining the group graph based manifold regression model (9) and the multi-feature global classifier (10), a novel global label consistent classifier framework is formulated. In summary, the GLCC framework as shown in (1) can be specifically rewritten as follows\n(11)\nThe first term denotes the explicitly multi-modal global label prediction, the second term is to control the classifier complexity and avoid overfitting in multi-feature learning phase, the third term is a weighted least square loss function and the final term is the multi-modal group graph manifold\nregularizer for preserving the label consistency and similarity\nof labeled data. Parameters and\ndenote the weights of the\ni th feature, and r>1 denotes that it can make full use of the information of each feature, otherwise, only the best feature is selected by our method (e.g. \u03b1i=1, \u03b2j=1), such that the complementary structure information of different feature modalities cannot be exploited [36]."}, {"heading": "C. Classifier Training", "text": "From the structure of the proposed GLCC framework (11), we observe that the solutions can be solved by a very efficient alternating optimization approach.\nFirst, we fix . The initialized F can be solved by setting the derivative of the following objective function w.r.t. F to be 0,\n(12)\nThen the initial value of F can be obtained as\n(13)\nAfter fixing , and , the optimization problem shown in (11) becomes\n(14)\nBy setting the derivatives of the objective function (14) w.r.t.\nPi and Bi to be 0, respectively, we can have\n(15)\n(16)\nwhere I is an identity matrix and is a full one vector. Note that in computation of Pi, the initial Bi is set as zero-vector.\nAfter fixing , the optimization problem becomes\n(17)\nBy setting the derivative of the objective function (17) w.r.t.\nF to be 0, the predicted label matrix F can be solved as\n(18)\nwhere\n.\nAfter fixing , the optimization of and once again becomes\n(19)\nThe Lagrange equation of (19) can be written as\n(20)\nwhere \u00b5 and \u03b7 denote the Lagrange multiplier coefficients.\nBy setting the derivative of (19) w.r.t. \u03b1i, \u03b2i , \u00b5, \u03b7 to be 0,\nrespectively, we have\n(21)\nwhere parameters and can be solved as follows\n(22)\nwhere F is solved in (18). The details of the solution of \u03b1 and \u03b2 by solving (21) are shown in Appendix B.\nConsequently, an iterative training procedure in Algorithm 1 is proposed to solve the optimization problem (11) of the GLCC framework. According to the algorithm framework, it is not difficult to infer that the objective function value of (11) monotonically decreases until convergence with proofs given in subsection D. The convergence depends on the maximum number of iterations in this work.\nAlgorithm 1. The GLCC framework\nInput: The training data of m modalities ; The training labels ; Parameters \u03bb, \u03b3, and r; Output: Converged and ; Procedure:\n1. Compute the graph Laplacian matrices ;\n2. Compute the Hessian energy matrices ; 3. Compute the decision matrix ; 4. Initialize , , ; 5. Initialize F according to (13); 6. While not converged do\nCompute Pi according to (15); Compute Bi according to (16); Update F according to (18); Update and according to (22); Check convergence;\nend while;\n7. Return Pi and Bi;"}, {"heading": "D. Recognition", "text": "Once and\nare obtained, the label of a given\ntest sample with m feature modalities can be determined as\n(23)\nwhich is the index w.r.t. the maximum value of the output vector. Specifically, the recognition procedure of the proposed GLCC framework is summarized in Algorithm 2.\nAlgorithm 2. Recognition of GLCC framework\nInput:\nTraining set , training labels Y, and one test sample of m modalities;\nProcedure: Obtain and\nby solving model\n(11) using the proposed Algorithm 1. Output:"}, {"heading": "E. Convergence", "text": "To explore the convergence behavior of the proposed\nAlgorithm 1, we first provide a lemma as follows. Lemma 1: For alternative optimization, when update one\nvariable with other variables fixed, that is, update , , , ,\nand (t denotes the index of iterations) will not increase the objective function value. Three claims are given:\nClaim 1.\nProof. When fix , , , , and update , the objective function is convex w.r.t. , as solved in (15) by setting the derivative of the objective function w.r.t. to be 0, then it\u2019s\nclear that\n.\nClaim 2.\nProof. Similar to the proof of claim 1, the objective function becomes convex w.r.t. when fix , , , , as solved in (16).\nThen,\n.\nClaim 3.\nProof. When are fixed, the optimization problem becomes (17) which is convex w.r.t. F. By setting the derivative of the objective function (17) w.r.t. F to be 0, its solution in (18) can make Claim 3 hold.\nClaim 4.\nProof. As can be seen from (21), with , , and F fixed, the update rule of and are obtained by setting the derivatives of objective function (20) w.r.t. and to be 0. Also, since the second-order derivatives w.r.t. and are positive, i.e.\nThus, the update rule (22) of and can make the objective function (20) decrease, and claim 4 is proven. Further, the proposed iteration method in Algorithm 1 can be proved to converge by the following theorem. Theorem 1: The objective function (11) monotonically decreases until convergence after several iterations by using Algorithm 1.\nProof. Suppose the updated , , , and\nare ,\n, ,\n, and , respectively. According to claim 1,\nclaim 2, claim 3 and claim 4 presented in lemma 1, we observe that\nThen Theorem 1 is proven."}, {"heading": "F. Computational Complexity", "text": "We now briefly analyze the computational complexity of the GLCC method, which involves T iterations and m modalities. Before stepping into the learning phase, the time complexity of computing the Laplacian and Hessian energy matrices is O(mn 3 ). In learning, each iteration involves four update steps, and the time complexity in all iterations is O(m 2 ndT). Hence, the computational complexity of our method is O(mn 3 )+ O(m 2 ndT). Note that the Laplacian and Hessian energy matrices are not involved in iterations, and can therefore be computed before algorithm learning such that the computational complexity O(mn 3 ) can be well avoided to reduce the total computational cost. Specifically, the total computational time for different datasets in experiments is presented in Sections IV, and discussed in Section V."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, to explore the effectiveness of our GLCC method, the experiments on Oxford Flowers 17 dataset, Caltech 101 dataset, YouTube & Consumer Videos dataset and a large-scale NUS-WIDE dataset for multimedia understanding are conducted."}, {"heading": "A. Datasets, Features and Experimental Setup", "text": "Oxford Flowers 17 Dataset: Flower 17 dataset consists of 17 species and 1360 images with 80 images per category. The authors in [44] provide seven distance matrices of features, such as clustered HSV, HOG, SIFT on the foreground internal region (SIFTint), SIFT on the foreground boundary (SIFTbdy) and three matrices derived from color, shape and texture vocabularies, along with three predefined splits of training (40 images per class), validation (20 images per class) and testing (20 images per class) sets. We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task. The 17 kinds of flower species are shown in Fig.2.\nCaltech 101 Dataset: Caltech 101 dataset is a challenging object recognition dataset containing 9144 images from 101 categories as well as a background class. For fair comparison, we strictly follow the experimental settings stated by the developer of the dataset. Four kinds of kernel matrices, such as geometric blur (GB), Phow-gray (L=0, 1, 2), Phow-color (L=0, 1, 2), and SSIM (L=0, 1, 2) extracted using MKL code package [39] have been used in this paper, where L is the spatial pyramid level. For all algorithms, 15 training images per category and 15 testing images per category according to the three predefined training/testing splits [3] are employed for verification. The first 10 classes from the Caltech 101 dataset\nwith 100% recognition accuracy by our GLCC are described in Fig.3.\nYouTube & Consumer Videos Dataset: This dataset, which contains 195 consumer videos (target domain) and 906 YouTube videos (source domain or auxiliary domain, i.e. web video domain) including six events such as birthday, picnic, parade, show, sports and wedding is developed for testing those semi-supervised domain adaptation and transfer learning methods in [45], as shown in Fig.4. We strictly follow the experimental setting in [45] for all methods. 906 loosely labeled YouTube videos are used as labeled training data in source domain. Besides, 18 videos (three consumer videos from each event) are selected as the labeled training videos in target domain. The remaining videos in target domain are used as the test data. Five splits of the labeled training videos from target domain are experimented and evaluated by using the means and standard deviations of MAPs (mean average precision). The features used in this work follow distance matrices including SIFT (level L=0 and L=1) features and space-time (ST with L=0 and L=1) features.\nNUS-WIDE Dataset: this dataset is a large-scale web image dataset including 269648 real-world scene and object images, such as airport, animals, clouds, buildings, and so on. The ground truths for 81 concepts are constructed. In this dataset, six types of descriptors including 144-D color correlogram\n(CORR), 73-D edge direction histogram (EDH), 128-D wavelet texture (WT), 225-D block-wise color moments (CM), 64-D color histogram (CH), and 500-D bag of words (BOG) based on SIFT are used to extract low level features. In experiments, the first three types of visual features like CORR, EDH, and WT are used for algorithm analysis. We randomly generated 3000 training samples from the dataset to optimize the model parameters, and the remaining data are used for performance test. The percentage of labeled samples is set as 10%, 30%, 50%, 70% and 90% of the total training set. We run the procedure for 10 times, and report the average results. The mean average precision (MAP) is used as the evaluation metric for this dataset."}, {"heading": "B. Parameter Settings", "text": "In GLCC model, there are two regularization parameters \u03bb\nand \u03b3. The parameters \u03bb and \u03b3 are tuned from the set {10 -4 , 10 -2 , 1, 10 2 , 10 4 } throughout the experiments, and report the best results. The maximum number of training iterations is set as 5. The parameter sensitivity analysis is presented in subsection G."}, {"heading": "C. Experimental Results on Flower 17 Data", "text": "The comparison experiments of Flower 17 dataset are conducted in two parts. First, we compare with baseline and state-of-the-art results for this dataset of 11 methods reported in\nthe previous work. Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC. The brief description of these methods is shown in Table I. In experiments, we have tuned the parameters of each method, and report their best results for discussion. The test results of all methods are described in Table II, in which the average recognition accuracy and the standard deviation of the predefined three train/test splits, and the total training and testing time in seconds are given. The state-of-the art result reported in previous work is 86.8% obtained by KMTJSRC [3], while the proposed GLCC obtains the highest recognition accuracy of 87.2% which is also better than the multi-feature and semi-supervised learning method FSSI [48] and MLHR [9], respectively. Besides the accuracy, the total computational time is also presented in Table II, from which, we see that 14 seconds are consumed by our GLCC and it is slightly higher than FSSI and SVM based methods. For deep comparisons with FSNM, FSSI, SFSS, and MLHR in different number of labeled training samples, we randomly select 10%, 30%, 50%, 70% and 90% samples from the training set as labeled samples, and observe the performance variation of different methods with increasing number of labeled training data. The test results of the five methods on flower 17 data are shown in Fig.5-(a). The bar plot clearly shows the superiority of our method."}, {"heading": "D. Experimental Results on Caltech 101 Data", "text": "This data with much more categories shows a more challenging task than Flower 17 data. Similarly, we first show the baseline and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons. From Table III, we can observe that our proposed GLCC achieves an average recognition accuracy 73.5% which outperforms the state-of-the-art KMTJSRC. Second, the four multi-feature and\nsemi-supervised methods like FSNM, FSSI, SFSS, and MLHR are also tested on this dataset, and the best results after parameter tuning are reported in Table III. We can see that FSSI obtains the second better accuracy 73.2% which is 0.3% lower than our GLCC. Notably, we observe that FSNM and SFSS achieve the worst recognition performance, which clearly demonstrate the importance of multi-feature joint learning for improving the robustness of classification in different tasks. The computational time for each method is also provided. From\nthe perspective of algorithm, our GLCC is more effective and computationally efficient than other methods.\nThe performance variation with increasing percentage of labeled training samples is described in Fig.5-(b). It\u2019s clear that the proposed GLCC outperforms other related methods. The FSSI and MLHR benefiting from multi-feature learning have more competitive performance than FNSM and SFSS."}, {"heading": "E. Experimental Results for Video Event Recognition", "text": "For this YouTube & Consumer videos dataset, as claimed in the experimental protocol of this dataset, all methods are compared in three cases: a) classifiers learned based on SIFT features with L=0 and L=1; b) classifiers learned based on ST features with L=0 and L=1; c) classifiers learned based on both SIFT and ST features with L=0 and L=1. The results of three cases are shown in Table VI with the mean average precision (MAP) as evaluation metric.\nFirst, we compare our GLCC method with SVMs, MKL,\nadaptive SVM (A-SVM) [46], and FR [47] methods as baseline. Note that SVM-AT denotes that the labeled training samples are from two domains (i.e. the auxiliary domain and the target domain), while SVM-T denotes that the labeled training samples are only from the target domain. From Table IV we observe that the proposed method achieves the highest MAP of\n44.92% which outperforms the state-of-the-art MKL in average as baseline. It\u2019s worth noting that the state-of-the-art domain adaptation method reported in [45] for this dataset are not compared because our method does not belong to such transfer learning framework, and only exploit the data for research.\nSecond, FSNM, FSSI, SFSS, and MLHR are tested on this data for video event recognition. We can see that MLHR obtains the second better result 43.68% which is 1.24% lower than GLCC in average. From the numeric results of MAP in three cases (a-c) shown by GLCC, it is clear that SIFT features (case (a)) is much better than ST features (case (b)), and the multi-feature learning integrated by SIFT and ST features together (case (c)) shows comparative results as well as case (a). As can be seen from case (c), the multi-feature learning methods FSSI, MLHR and GLCC show significant higher precision than single-feature based learning, which clearly demonstrate the importance of multi-feature joint learning for robust classifier. The computational time shown in Table I V demonstrates the comparative efficiency of our GLCC.\nThe performance variation with increasing percentage of labeled training samples is described in Fig.5-(c). Generally, our method outperforms other related algorithms, except for the cases of 10% and 50% where GLCC is a litter lower than FSSI."}, {"heading": "F. Experimental Results on Large-Scale NUS-WIDE Data", "text": "For this large-scale NUS-WIDE web image data, we compare our GLCC with the existing multi-feature learning and semi-supervised methods according to the experimental protocol. The test results trained on all 3000 training data are illustrated in Table V. The results of MAP denote the better performance of GLCC than other related methods.\nThe performance variation with 10%, 30%, 50%, 70% and 90% of labeled training data is shown in Fig.5-d. The results of FSSI, MLHR and GLCC are much better than FSNM and SFSS, which demonstrate the significance of multi-feature learning."}, {"heading": "G. Weights of Laplacian and Hessian Graph", "text": "The proposed method uses a mixed graph regularization of Laplacian and Hessian graphs for learning the weights \u03b1 and \u03b2 of multiple features. The learned weights of Laplacian and Hessian graphs for each feature in different datasets are provided in Table VI. It can be noted that on a particular dataset, different features have varying importance. However, the learned weights based on Laplacian graph are close to the average value (1/m, where m is the number of features), that is, for flower data the weight is around 0.14, for caltech and video data the weight is around 0.25, and for NUS-WIDE data the weight is close to 0.33. Instead, the divergence of the weights from Hessian graph is more distinguishable that automatically selects the optimal weight for each feature. Thus, the results show that in multi-feature joint learning, Hessian graph will be more flexible in trade-off among the features for pursuit of better recognition performance.\nTABLE VI LEARNED WEIGHTS OF LAPLACIAN AND HESSIAN GRAPHS FOR FOUR DATASETS\nDataset Flower 17 data Caltech 101 data YouTube&Consumer video data Large scale NUS-WIDE data\nFeature HOG HSV Sift\nInt\nSift Bdy Color Shape Texture Phow Color Phow Gray SSIM GB SIFT (l=0) SIFT (l=1) STIP (l=0) STIP (l=1)\nEDH CORR WT\n\u03b1 0.14 0.14 0.15 0.14 0.14 0.15 0.14 0.25 0.25 0.25 0.25 0.26 0.26 0.24 0.24 0.32 0.34 0.34 \u03b2 0.14 0.16 0.12 0.10 0.16 0.14 0.18 0.24 0.25 0.25 0.26 0.01 0.15 0.25 0.59 0.31 0.39 0.30"}, {"heading": "H. Parameter Analysis", "text": "We investigate the effect of the model parameters \u03bb and \u03b3that controls the overfitting of classifier in our proposed GLCC method for Flower 17, Caltech 101, YouTube&Consumer Videos and large-scale NUS-WIDE experiments. For analysis, \u03bb and \u03b3 from the set {10 -4 , 10 -2 , 1, 10 2 , 10 4 } are tuned sequentially. The performance variations (i.e. recognition accuracy/MAP) with parameter \u03bb and \u03b3 are described in Fig.6, from which we have following observations: 1) a smaller value of parameter \u03bb and \u03b3 has significantly better performance for\nFlower 17 and Caltech 101 data (see Fig.6-a and Fig.6-b) and the performance becomes deteriorated sharply when \u03b3 is larger than 1; 2) for YouTube&Consumer videos (see Fig.6-c), a larger value of \u03b3 shows better performance; 3) for NUS-WIDE (see Fig.6-d), the best result is obtained when \u03b3=100; 4) the parameter \u03bb shows a relatively stable recognition, when it is larger than 1 the performance becomes worse for all experiments, and thus it can be pre-fixed as 1 during the tuning process of parameter \u03b3 for pursuit of the optimal performance.\nFig. 8. Covergence of of GLCC on four experimental datasets, where t dnotes the current index of iteration.\n5 10 15 20 0\n500\n1000\nIteration index\n(a) Flower 17 dataset\n||P t-\nP t-\n1 || F\n5 10 15 20 0\n500\nIteration index\n(b) Caltech 101 dataset\n||P t-\nP t-\n1 || F\n5 10 0\n0.5\n1\nIteration index\n(c) YouTube & Consumer Video dataset\n||P t-\nP t-\n1 || F\n2 4 0\n0.005\n0.01\nIteration index\n(d) NUS-WIDE dataset\n||P tP\nt1 || F"}, {"heading": "V. CONVERGENCE AND COMPUTATIONAL TIME ANALYSIS", "text": "In this section, the convergence analysis of the objective function and the classifier (i.e. the mapping matrix P) is been presented. The computational complexity of the proposed approach is also analyzed."}, {"heading": "A. Convergence Analysis", "text": "The proof of the GLCC convergence is provided in section III.D. The convergence of GLCC indicated by the objective function (11) over iterations on three benchmark datasets used in this paper for object recognition and video event recognition is described in Fig.7. One can observe that after a few iterations our GLCC algorithm always empirically converged. Besides, we have also analyzed the convergence of the learned classifier\nP by calculating the difference\nbetween iteration t and t-1. The convergence of of the proposed GLCC over iterations is described in Fig.8. It is clearly seen that the learned classifier P for each dataset always converges to a small value after several iterations."}, {"heading": "B. Computational Time Analysis", "text": "From the structure of GLCC, jointly learn multi-matrices will increase the computation. -norm definition used in the model can make the optimization and learning efficiently. The total computation time for Flower 17 dataset, Caltech 101 dataset and YouTube & Consumer Video dataset have been presented together with the recognition performance in Table II, Table V, and Table VI, from which we can clearly observe that the proposed method has an efficient computational power. Note that the experiments on Flower 17, Caltech 101 and YouTube&Consumer videos are executed in a laptop with an Inter Core i5 CPU (2.50GHz) and 4 GB RAM. The experiments on large-scale NUS-WIDE web image data is executed in a computer with Inter Core i7 CPU and 32GB RAM."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we propose a global label consistent classifier that exploits multiple feature matrices representing visual data and a joint learning for object recognition and video event recognition, respectively. First, the proposed GLCC can make full consideration of the multi-view features for better recognition performance, which is different from the simple feature concatenation that is non-robust and the structural information from different views cannot be and exploited and shared in classifier learning. Second, inspired by the semi-supervised manifold regression, a group graph manifold regularizer coupled with Laplacian and Hessian energy of multiple views on the labeled data is presented in GLCC. It holds an assumption that the label prediction for each view is consistent with the global prediction based on multiple views. Third, a -norm based global classifier with an alternating optimization method is solved. Finally, the framework has been exploited on various datasets for visual annotation. Comparisons with state of the arts demonstrate that the proposed method is effective in recognition performance and efficient in computation.\nThis paper proposes a multi-feature sharing based global classifier by exploiting a joint learning framework. In the future work, active learning and selection of the most useful feature modality without manual adjustment is an interesting topic in large scale multimedia applications."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by Hong Kong Scholar Program (No.XJ2013044), National Natural Science Foundation of China (No. 61401048) and also funded by China Postdoctoral Science Foundation (No. 2014M550457)."}, {"heading": "APPENDIX A", "text": "The total Hessian energy estimation of single view/feature\ncan be represented as [5]\n\u2460\nwhere is the sparse Hessian energy matrix of the training set. Proof:\nFirst, define a local tangent space of data point Xi. In order to estimate the local tangent space, a PCA is performed on the k nearest neighbors space , then m leading eigenvectors can be obtained as the orthogonal basis of .\nThe Hessian regularizer defined as of data\npoint Xi, is the squared norm of the second covariant derivative which corresponds to the Frobenius norm of the Hessian of f at normal coordinates.\n\u2461\nwhere\n\u2462\nSubstitute \u2462 into \u2461, the estimation of the Frobenius norm of the Hessian of f at data point Xi is\nwhere\n.\nThen, the total estimated Hessian energy is represented as the\nsum over all data points, i.e.\nThe proof of \u2460 is completed."}, {"heading": "APPENDIX B", "text": "To solve the equation group (21) in the paper, we first solve\n\u03b1i as follows.\nCombine the first and the third equations as\n\u2463\nTo the first equation in \u2463, there is\n\u2193\n\u2464\n\u2193\n\u2465\nConsider the 2 th equation in \u2463 and the equation \u2465, we have\n\u2466\nSubstitute \u2466 into \u2464, we can obtain as (22). Similarly, can also be calculated in the same way."}], "references": [{"title": "On feature combination for multiclass objective classification,", "author": ["P. Gehler", "S. Nowozin"], "venue": "in Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "in Proc. CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Visual Classification with Multi-Task Joint Sparse Representation,", "author": ["X.T. Yuan", "X. Liu", "S. Yan"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Joint Sparse Representation for Robust Multimodal Biometrics Recognition,", "author": ["S. Shekhar", "V.M. Patel", "N.M. Nasrabadi", "R. Chellappa"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Semi-supervised Regression using Hessian Energy with an Application to Semi-supervised Dimensionality Reduction,", "author": ["K.I. Kim", "F. Steinke", "M. Hein"], "venue": "in Proc. NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation,", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Semi-supervised learning on manifolds,", "author": ["M. Belkin", "P. Niyogi"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Semi-Supervised Multiple Feature Analysis for Action Recognition,", "author": ["S. Wang", "Z. Ma", "Y. Yang", "X. Li", "C. Pang", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Multi-Feature Fusion via Hierarchical Regression for Multi-media Analysis,", "author": ["Y. Yang", "J. Song", "Z. Huang", "Z. Ma", "N. Sebe", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Alpaydn, \u201cMultiple Kernel Learning Algorithms,", "author": ["E.M. G\u04e7nen"], "venue": "J. Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Shawetaylor, \u201cTwo View Learning: SVM-2k, Theory and Practice,", "author": ["J. Farquhar", "H. Meng", "S. Szedmak", "D. Hardoon"], "venue": "Proc. Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A visual vocabulary for flower classification,", "author": ["M. Nilsback", "A. Zisserman"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Column-generation boosting methods for mixture of kernels,", "author": ["J. Bi", "T. Zhang", "K.P. Bennett"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories,", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "CVPR Workshop on Generative-Model Based Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Linear programming boosting via column generation,", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Vehicle Classification on Multi-Sensor Smart Cameras Using Feature- and Decision-Fusion,", "author": ["A. Klausner", "A. Tengg", "B. Rinner"], "venue": "Proc. IEEE Conf. Distributed Smart Cameras,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Retrieval based interactive cartoon synthesis via unsupervised bi-distance metric learning,", "author": ["Y. Yang", "Y. Zhuang", "D. Xu", "Y. Pan", "D. Tao", "S. Maybank"], "venue": "in Proc. ACM MM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Feature Level Fusion of Hand and Face Biometrics,", "author": ["A.A. Ross", "R. Govindarajan"], "venue": "Proc. SPIE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Feature Fusion of Face and Gait for Human Recognition at a Distance in Video,", "author": ["X. Zhou", "B. Bhanu"], "venue": "Proc. Int. Conf. Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Learning Mid-Level  Features for Recognition,", "author": ["Y.L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recogntion,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Two-view transductive support vector machines,", "author": ["G. Li", "S. Hoi", "K. Chang"], "venue": "in Proc. SDM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Discriminative learning and recognition of image set classess using canonical correlations,", "author": ["T. Kim", "J. Kittler", "R. Cipolla"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Multiple Kernels for Object Detection,", "author": ["A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Robust face recognition via sparse representation,", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent K-SVD,", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Fisher Discrimination Dictionary Learning for sparse representation,", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Discriminative K-SVD for dictionary learning in face recognition,", "author": ["Q. Zhang", "B. Li"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Jointly Learning Visually Correlated Dictionaries for Large-Scale Visual Recognition Applications,", "author": ["N. Zhou", "J. Fan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Classification and Clustering via Dictionary Learning with Structured Incoherence and Shared Features,", "author": ["I. Ram\u00edrez", "P. Sprechmann", "G. Sapiro"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Multi-Observation Visual Recognition via Joint Dynamic Sparse Representation,", "author": ["H. Zhang", "N.M. Nasrabadi", "Y. Zhang", "T.S. Huang"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Scholkopf, \u201cLearning with local and global consistency,", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston"], "venue": "in Proc. NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,", "author": ["S. Yan", "D. Xu", "B. Zhang", "H.J. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding,", "author": ["S. Roweis", "L. Saul"], "venue": "Science, vol. 290,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction,", "author": ["J. Tenenbaum", "V. Silva", "J. Langford"], "venue": "Science, vol. 290,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Multiview Spectral Embedding,", "author": ["T. Xia", "T. Mei", "Y. Zhang"], "venue": "IEEE Trans. Systems, Man, and Cybernetics-part B: Cybernetics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Cost-Sensitive Subspace Analysis and Extensions for Face Recognition,", "author": ["J. Lu", "Y.P. Tan"], "venue": "IEEE Trans. Information Forensics and Security,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "More efficiency in multiple kernel learning,", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Sch\u04e7lkopf, \u201cLarge scale multiple kernel learning,", "author": ["S. Sonnenburg", "G. R\u00e4tch", "C. Sch\u00e4fer"], "venue": "JMLR, vol", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Learning the discriminative power-invariance trade-off,", "author": ["M. Varma", "D. Ray"], "venue": "in ICCV, pp", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Linear Spatial Pyramid Matching Using Sparse Coding for Image Classification,", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples,", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Graph based multi-modality learning,", "author": ["H. Tong", "J. He", "M. Li", "C. Zhang", "W.Y. Ma"], "venue": "Proc. ACM Int. Conf. Multimedia,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "Sparse Multi-Modal Hashing,", "author": ["F. Wu", "Z. Yu", "Y. Yang", "S. Tang", "Y. Zhang", "Y. Zhuang"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Automated flower classification over a large number of classes,", "author": ["M. Nilsback", "A. Zisserman"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Visual event recognition in videos by learning from web data,", "author": ["L. Duan", "D. Xu", "I.W. Tsang", "J. Luo"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Cross-domain video concept detection using adaptive svms,", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "Proc. ACM Int\u2019l Conf. Multimedia,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "Frustratingly easy domain adaption,", "author": ["H. Daum\u00e9"], "venue": "Proc. Ann. Meeting Assoc. for Computational Linguistics,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2007}, {"title": "Feature Selection for Multimedia Analysis by Sharing Information Among Multiple Tasks,", "author": ["Y. Yang", "Z. Ma", "A.G. Hauptmann", "N. Sebe"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Multiple Features But Few Labels? A Symbiotic Solution Exemplified for Video Analysis,", "author": ["Z. Ma", "Y. Yang", "N. Sebe", "A.G. Hauptmann"], "venue": "ACM MM,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "A New Approach to Cross-Modal Multimedia Retrieval,", "author": ["N. Rasiwasia", "J.C. Pereira", "E. Coviello", "G. Doyle", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACM MM,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Discriminating Joint Feature Analysis for Multimedia Data Understanding,", "author": ["Z. Ma", "F. Nie", "Y. Yang", "J.R.R. Uijlings", "N. Sebe", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Efficient and Robust Feature Selection via Joint l2,1-norms Minimization,", "author": ["F. Nie", "H. Huang", "X. Cai", "C. Ding"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "NUS-WIDE: A Real-World Web Image Database from National University of Singapore,", "author": ["T.S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y.T. Zheng"], "venue": "ACM International Conference on Image and Video Retrieval,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "For example, given a face image or a video frame, its visual content can be represented by several kinds of weak modalities such as left and right periocular, mouth and nose regions [4] for robust face recognition, or different feature types such as histogram, SIFT, HSV, etc.", "startOffset": 182, "endOffset": 185}, {"referenceID": 8, "context": "[9] for action recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In early, information fusion can be done at three levels: feature level, score level and decision level, whereas feature level fusion can be more discriminative than other two level-fusions [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 17, "context": "Feature concatenation is a prevalent fusion method which has been used in patter recognition [18], [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Feature concatenation is a prevalent fusion method which has been used in patter recognition [18], [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "However, it is less effective in multimedia content analysis, especially when the features are independent or heterogeneous [17], and in particular, simple feature concatenation for high dimensional feature vectors may also become inefficient and non-robust.", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "One popular work is the two-view based support vector machine (SVM-2k) [11], [21], [22], which jointly learns SVM with two views.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "One popular work is the two-view based support vector machine (SVM-2k) [11], [21], [22], which jointly learns SVM with two views.", "startOffset": 77, "endOffset": 81}, {"referenceID": 21, "context": "One popular work is the two-view based support vector machine (SVM-2k) [11], [21], [22], which jointly learns SVM with two views.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Another popular work is multiple kernel learning (MKL) [10], [20], which focus on the information integration from multiple features by combining multiple kernels with respective weights.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "Another popular work is multiple kernel learning (MKL) [10], [20], which focus on the information integration from multiple features by combining multiple kernels with respective weights.", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 27, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 425, "endOffset": 429}, {"referenceID": 2, "context": "For examples, in [3], a multi-task joint sparse representation classifier (MTJSRC) was proposed for visual classification in which group sparsity was used to combine multiple features.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "In [4], a multimodal joint sparse representation and kernel space multimodal sparse model were proposed for robust face recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 29, "context": "In [30], a joint dynamic sparse representation classifier model was proposed for object recognition.", "startOffset": 3, "endOffset": 7}, {"referenceID": 47, "context": "In [48], a very efficient multi-task feature selection model (FSSI) with information sharing in low-rank solution was proposed for multimedia analysis.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Comparatively, Hessian regularization has better extrapolating power shown in two facets [5]: first, it has a richer null space and second, it can exploit the intrinsic local geometry of the data manifold very well.", "startOffset": 89, "endOffset": 92}, {"referenceID": 11, "context": "The visual experiments have been conducted on the benchmark visual datasets, including the Oxford flower 17 dataset 1 from [12], the Caltech 101 dataset 2", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "uk/~vgg/software/MKL/ from [14], the YouTube & Consumer video dataset 3 from [45], and the large scale real-world NUS-WIDE web image dataset 4", "startOffset": 27, "endOffset": 31}, {"referenceID": 44, "context": "uk/~vgg/software/MKL/ from [14], the YouTube & Consumer video dataset 3 from [45], and the large scale real-world NUS-WIDE web image dataset 4", "startOffset": 77, "endOffset": 81}, {"referenceID": 52, "context": "from [53] for multimedia analysis.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "In [2], a spatial pyramid matching (SPM) beyond bags of features was proposed for natural scene categories and object recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 39, "context": "[40] also proposed a linear SPM based on sparse coding (ScSPM) for visual classification and obtained significant improvement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 204, "endOffset": 208}, {"referenceID": 36, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 210, "endOffset": 214}, {"referenceID": 37, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 216, "endOffset": 220}, {"referenceID": 12, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 260, "endOffset": 264}, {"referenceID": 14, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 314, "endOffset": 318}, {"referenceID": 2, "context": "[3] proposed a multi-task joint sparse representation (MTJSRC) using mixed-norm for visual", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 115, "endOffset": 119}, {"referenceID": 25, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 27, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "[30] proposed a multi-observation joint dynamic sparse representation for visual recognition, and obtain comparable performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Most related to the paper, a subspace sharing based semi-supervised multiple feature analysis method for action recognition was proposed [8], in which both the global and local structural consistency has been considered in discriminative classifier training.", "startOffset": 137, "endOffset": 140}, {"referenceID": 30, "context": "[31] proposed a graph based semi-supervised method (LGC) for learning local and global consistency by a regularization framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "In [7], a Laplacian graph manifold based semi-supervised learning was proposed, in which a manifold assumption that the manifold structure information of the unlabeled data can be preserved was given.", "startOffset": 3, "endOffset": 6}, {"referenceID": 50, "context": "In [51], a semi-supervised feature selection algorithm SFSS for multimedia analysis based on Laplacian graph and l2,1-norm regularization was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 109, "endOffset": 112}, {"referenceID": 31, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 120, "endOffset": 124}, {"referenceID": 33, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 126, "endOffset": 130}, {"referenceID": 35, "context": "In [36], a graph Laplacian based multi-view spectral embedding (MSE) method was proposed for dimension reduction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "[9] proposed a multi-feature Laplacian graph based hierarchical semi-supervised regression (MLHR) for multimedia analysis and achieved better performance in video concept annotation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Throughout these manifold based methods presented above, Laplacian graph and single feature are the mainstream of semi-supervised learning, however, it has been identified in [5] that it suffers from the fact that the solution is biased towards a constant with weaker extrapolating power.", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "Hessian graph exploited for semi-supervised dimension reduction [5] was proved to have a good extrapolating power.", "startOffset": 64, "endOffset": 67}, {"referenceID": 40, "context": "[41] proposed a manifold regularization framework for semi-supervised learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] proposed a graph based multi-modality learning method with linear and sequential fusion schemes, but the mapping function in the objective function is implicit.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] proposed a multi-view graph embedding which calculates an eigenvalue problem in optimization, but for dimension reduction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] proposed a sparse multi-modal dictionaries learning with Laplacian hyper-graph as regularization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] proposed a semi-supervised multiple feature learning framework in which graph Laplacian regularizer and subspace sharing were studied for action recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 12, "endOffset": 15}, {"referenceID": 34, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 17, "endOffset": 21}, {"referenceID": 40, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 23, "endOffset": 27}, {"referenceID": 41, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 29, "endOffset": 33}, {"referenceID": 42, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 35, "endOffset": 39}, {"referenceID": 4, "context": "As denoted in [5], the graph Laplacian based semi-supervised regression suffers from the fact that the solution is biased towards a constant and the extrapolating power is lost, and further proposed a second-order Hessian energy regularizer that shows a better extrapolation capability than Laplacian regularizer in semi-supervised learning, particularly if only few labeled points are available.", "startOffset": 14, "endOffset": 17}, {"referenceID": 35, "context": "\u03b1i=1, \u03b2j=1), such that the complementary structure information of different feature modalities cannot be exploited [36].", "startOffset": 115, "endOffset": 119}, {"referenceID": 43, "context": "The authors in [44] provide seven distance matrices of features, such as clustered HSV, HOG, SIFT on the foreground internal region (SIFTint), SIFT on the foreground boundary (SIFTbdy) and three matrices derived from color, shape and texture vocabularies, along with three predefined splits of training (40 images per class), validation (20 images per class) and testing (20 images per class) sets.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 70, "endOffset": 74}, {"referenceID": 37, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "Four kinds of kernel matrices, such as geometric blur (GB), Phow-gray (L=0, 1, 2), Phow-color (L=0, 1, 2), and SSIM (L=0, 1, 2) extracted using MKL code package [39] have been used in this paper, where L is the spatial pyramid level.", "startOffset": 161, "endOffset": 165}, {"referenceID": 2, "context": "For all algorithms, 15 training images per category and 15 testing images per category according to the three predefined training/testing splits [3] are employed for verification.", "startOffset": 145, "endOffset": 148}, {"referenceID": 44, "context": "web video domain) including six events such as birthday, picnic, parade, show, sports and wedding is developed for testing those semi-supervised domain adaptation and transfer learning methods in [45], as shown in Fig.", "startOffset": 196, "endOffset": 200}, {"referenceID": 44, "context": "We strictly follow the experimental setting in [45] for all methods.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "2 AK-SVM [1] 84.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "2 2 PK-SVM [1] 85.", "startOffset": 11, "endOffset": 14}, {"referenceID": 37, "context": "2 10 MKL(SILP) [38] 85.", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "5 97 MKL(simple) [37] 85.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "CG-Boost [13] 84.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "2e3 LP-\u03b2 [15] 85.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "0 80 LPBoost [15] 85.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "4 98 FDDL [26] 86.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "9e3 KMTJSRC [3] 86.", "startOffset": 12, "endOffset": 15}, {"referenceID": 51, "context": "FSNM [52] 85.", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "7 24 FSSI [48] 86.", "startOffset": 10, "endOffset": 14}, {"referenceID": 50, "context": "4 12 SFSS [51] 85.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "MLHR [9] 86.", "startOffset": 5, "endOffset": 8}, {"referenceID": 51, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 185, "endOffset": 189}, {"referenceID": 47, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 196, "endOffset": 200}, {"referenceID": 50, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 207, "endOffset": 211}, {"referenceID": 8, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "8% obtained by KMTJSRC [3], while the proposed GLCC obtains the highest recognition accuracy of 87.", "startOffset": 23, "endOffset": 26}, {"referenceID": 47, "context": "2% which is also better than the multi-feature and semi-supervised learning method FSSI [48] and MLHR [9], respectively.", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "2% which is also better than the multi-feature and semi-supervised learning method FSSI [48] and MLHR [9], respectively.", "startOffset": 102, "endOffset": 105}, {"referenceID": 38, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 44, "endOffset": 47}, {"referenceID": 51, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 53, "endOffset": 57}, {"referenceID": 47, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 63, "endOffset": 67}, {"referenceID": 50, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 83, "endOffset": 86}, {"referenceID": 38, "context": "Similarly, we first show the baseline and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Similarly, we first show the baseline and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons.", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "Similarly, we first show the baseline and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons.", "startOffset": 146, "endOffset": 149}, {"referenceID": 45, "context": "First, we compare our GLCC method with SVMs, MKL, adaptive SVM (A-SVM) [46], and FR [47] methods as baseline.", "startOffset": 71, "endOffset": 75}, {"referenceID": 46, "context": "First, we compare our GLCC method with SVMs, MKL, adaptive SVM (A-SVM) [46], and FR [47] methods as baseline.", "startOffset": 84, "endOffset": 88}, {"referenceID": 44, "context": "It\u2019s worth noting that the state-of-the-art domain adaptation method reported in [45] for this dataset are not compared because our method does not belong to such transfer learning framework, and only exploit the data for research.", "startOffset": 81, "endOffset": 85}, {"referenceID": 46, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 23, "endOffset": 27}, {"referenceID": 45, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 34, "endOffset": 38}, {"referenceID": 38, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 43, "endOffset": 47}, {"referenceID": 51, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 53, "endOffset": 57}, {"referenceID": 47, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 63, "endOffset": 67}, {"referenceID": 50, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 83, "endOffset": 86}, {"referenceID": 51, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 12, "endOffset": 16}, {"referenceID": 47, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 22, "endOffset": 26}, {"referenceID": 50, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "The total Hessian energy estimation of single view/feature can be represented as [5]", "startOffset": 81, "endOffset": 84}], "year": 2015, "abstractText": "Image/video data is usually represented by multiple visual features. Fusion of multi-sources information for establishing the identity has been widely recognized. Multi-feature visual recognition has recently received attention in multimedia applications. This paper studies visual understanding via a newly proposed -norm based multi-feature jointly sharing learning framework, which can simultaneously learn the global label matrix and explicit classifiers from the labeled visual data represented by multiple feature modalities. Additionally, a multi-modal group graph manifold regularizer formed by mixed Laplacian and Hessian graph is proposed for better preserving the manifold structure of different features on the labeled data, while preserving the label consistency and improving the label prediction power via semi-supervised learning. The merits of the proposed multi-feature learning framework lie in jointly sharing the structural information from multiple features in global classifier learning phase based on a mixed graph regularizer on one hand, and an efficient alternating optimization method for fast classifier training on the other hand. Experiments on several benchmark visual datasets, such as 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, YouTube & Consumer Videos dataset and large-scale NUS-WIDE dataset for multimedia understanding all demonstrate that the proposed approach compares favorably with state-of-the-art algorithms.", "creator": "Microsoft\u00ae Office Word 2007"}}}