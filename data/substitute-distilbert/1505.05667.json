{"id": "1505.05667", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "abstract": "in this work, we address the problem here model all the instances ( words including phrases ) in a dependency tree with the dense representations. we propose 3d recursive convolutional neural network ( rcnn ) architecture to capture syntactic and compositional - semantic representations of phrases after words generating a dependency tree. different toward the original canonical neural network, we assume the convolution and pooling layers, which can model a variety called texts by the feature maps and choose the specified informative compositions by the pooling layers. based on optimization, we use a discriminative model to re - rank a $ k $ - best list to candidate dependency parsing trees. the experiments show that rcnn is also effective to improve the state - among - the - art dependency parsing on both tamil and chinese datasets.", "histories": [["v1", "Thu, 21 May 2015 10:23:10 GMT  (199kb,D)", "http://arxiv.org/abs/1505.05667v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["chenxi zhu", "xipeng qiu", "xinchi chen", "xuanjing huang"], "accepted": true, "id": "1505.05667"}, "pdf": {"name": "1505.05667.pdf", "metadata": {"source": "CRF", "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "authors": ["Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang"], "emails": ["czhu13@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xinchichen13@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness.\nRecently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many\n\u2217 Corresponding author.\nnatural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations.\nFor dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks.\nBesides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree.\nIn this work, we address the problem to rep-\nar X\niv :1\n50 5.\n05 66\n7v 1\n[ cs\n.C L\n] 2\n1 M\nay 2\n01 5\nresent all level nodes (words or phrases) with dense representations in a dependency tree. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words. RCNN is a general architecture and can deal with k-ary parsing tree, therefore it is very suitable for dependency parsing. For each node in a given dependency tree, we first use a RCNN unit to model the interactions between it and each of its children and choose the most informative features by a pooling layer. Thus, we can apply the RCNN unit recursively to get the vector representation of the whole dependency tree. The output of each RCNN unit is used as the input of the RCNN unit of its parent node, until it outputs a single fixed-length vector at root node. Figure 1 illustrates an example how a RCNN unit represents the phrases \u201ca red bike\u201d as continuous vectors.\nThe contributions of this paper can be summarized as follows.\n\u2022 RCNN is a general architecture to model the distributed representations of a phrase or sentence with its dependency tree. Although RCNN is just used for the re-ranking of the dependency parser in this paper, it can be regarded as semantic modelling of text sequences and handle the input sequences of varying length into a fixed-length vector. The parameters in RCNN can be learned jointly with some other NLP tasks, such as text classification.\n\u2022 Each RCNN unit can model the complicated interactions of the head word and its children. Combined with a specific task, RCNN can capture the most useful semantic and structure information by the convolution and pooling layers.\n\u2022 When applied to the re-ranking model for parsing, RCNN improve the accuracy of base parser to make accurate parsing decisions. The experiments on two benchmark datasets show that RCNN outperforms the state-ofthe-art models."}, {"heading": "2 Recursive Neural Network", "text": "In this section, we briefly describe the recursive neural network architecture of (Socher et al., 2013a).\nThe idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure (Pollack, 1990; Socher et al., 2013c). RNN can be also regarded as a general structure to model sentence. At every node in the tree, the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the whole sentence.\nFollowing the binary tree structure, RNN can assign a fixed-length vector to each word at the leaves of the tree, and combine word and phrase pairs recursively to create intermediate node vectors of the same length, eventually having one final vector representing the whole sentence. Multiple recursive combination functions have been explored, from linear transformation matrices to tensor products (Socher et al., 2013c). Figure 2 illustrates the architecture of RNN.\nThe binary tree can be represented in the form of branching triplets (p\u2192 c1c2). Each such triplet denotes that a parent node p has two children and each ck can be either a word or a non-terminal node in the tree.\nGiven a labeled binary parse tree, ((p2 \u2192 ap1), (p1 \u2192 bc)), the node representations are computed by\np1 = f(W [ b c ] ),p2 = f(W [ a p1 ] ), (1)\nwhere (p1,p2,a,b, c) are the vector representation of (p1, p2, a, b, c) respectively, which are denoted by lowercase bold font letters; W is a matrix of parameters of the RNN.\nBased on RNN, Socher et al. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations. In order to compute the score of\nhow plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all pi:\ns(pi) = v \u00b7 pi, (2)\nwhere v is a vector of parameters that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011).\nCosta et al. (2003) applied recursive neural networks to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough information to make the correct parsing decisions. Menchetti et al. (2005) used RNNs to re-rank different constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003)."}, {"heading": "3 Recursive Convolutional Neural Network", "text": "The dependency grammar is a widely used syntactic structure, which directly reflects relationships among the words in a sentence. In a dependency tree, all nodes are terminal (words) and each node may have more than two children. Therefore, the standard RNN architecture is not suitable for dependency grammar since it is based on the binary tree. In this section, we propose a more general architecture, called recursive convolutional neural network (RCNN), which borrows the idea of convolutional neural network (CNN) and can deal with to k-ary tree."}, {"heading": "3.1 RCNN Unit", "text": "For ease of exposition, we first describe the basic unit of RCNN. A RCNN unit is to model a head word and its children. Different from the constituent tree, the dependency tree does not have non-terminal nodes. Each node consists of a word and its POS tags. Each node should have a different interaction with its head node.\nWord Embeddings Given a word dictionaryW , each word w \u2208 W is represented as a real-valued vector (word embedding) w \u2208 Rm where m is the dimensionality of the vector space. The word embeddings are then stacked into a embedding matrix M \u2208 Rm|W|. For a word w \u2208 W , its corresponding word embedding Embed(w) \u2208 Rm is retrieved by the lookup table layer. The matrix M\nis initialized with pre-training embeddings and updated by back-propagation.\nDistance Embeddings Besides word embeddings, we also use distributed vector to represent the relative distance of a head word h and one of its children c. For example, as shown in Figure 1, the relative distances of \u201cbike\u201d to \u201ca\u201d and \u201cred\u201d are -2 and -1, respectively. The relative distances also are mapped to a vector of dimension md (a hyperparameter); this vector is randomly initialized. Distance embedding is a usual way to encode the distance information in neural model, which has been proven effectively in several tasks. Our experimental results also show that the distance embedding gives more benefits than the traditional representation. The relative distance can encode the structure information of a subtree.\nConvolution The word and distance embeddings are subsequently fed into the convolution component to model the interactions between two linked nodes.\nDifferent with standard RNN, there are no nonterminal nodes in dependency tree. Each node h in dependency tree has two associated distributed representations:\n1. word embedding wh \u2208 Rm, which is denoted as its own information according to its word form;\n2. phrase representation xh \u2208 Rm, which is denoted as the joint representation of the whole subtree rooted at h. In particular, when h is leaf node, xh = wh.\nGiven a subtree rooted at h in dependency tree, we define ci, 0 < i \u2264 L as the i-th child node of h, where L represents the number of children.\nFor each pair (h, ci), we use a convolutional hidden layer to compute their combination representation zi.\nzi = tanh(W (h,ci)pi), 0 < i \u2264 K, (3)\nwhere W(h,ci) \u2208 Rm\u00d7n is the linear composition matrix, which depends on the POS tags of h and ci; pi \u2208 Rn is the concatenated representation of h and the i-th child, which consists of the head word embeddings wh, the child phrase representation xci and the distance embeddings d\nh,ci of h and ci,\npi = xh \u2295 xci \u2295 d(h,ci), (4)\nwhere \u2295 represents the concatenation operation. The distances dh,ci is the relative distance of h and ci in a given sentence. Then, the relative distances also are mapped to m-dimensional vectors. Different from constituent tree, the combination should consider the order or position of each child in dependency tree.\nIn our model, we do not use the POS tags embeddings directly. Since the composition matrix varies on the different pair of POS tags of h and ci, it can capture the different syntactic combinations. For example, the combination of adjective and noun should be different with that of verb and noun.\nAfter the composition operations, we use tanh as the non-linear activation function to get a hidden representation z.\nMax Pooling After convolution, we get Z(h) = [z1, z2, \u00b7 \u00b7 \u00b7 , zK ], where K is dynamic and depends on the number of children of h. To transform Z to a fixed length and determine the most useful semantic and structure information, we perform a max pooling operation to Z on rows.\nx (h) j = maxi Z (h) j,i , 0 < j \u2264 m. (5)\nThus, we obtain the vector representation xh \u2208 Rm of the whole subtree rooted at node h.\nFigure 3 shows the architecture of our proposed RCNN unit.\nGiven a whole dependency tree, we can apply the RCNN unit recursively to get the vector representation of the whole sentence. The output of each RCNN unit is used as the input of the RCNN unit of its parent node.\nThus, RCNN can be used to model the distributed representations of a phrase or sentence with its dependency tree and applied to many NLP tasks. The parameters in RCNN can be learned jointly with the specific NLP tasks. Each RCNN unit can model the complicated interactions of the head word and its children. Combined with a specific task, RCNN can select the useful semantic and structure information by the convolution and max pooling layers.\nFigure 4 shows an example of RCNN to model the sentence \u201cI eat sashimi with chopsitcks\u201d."}, {"heading": "4 Parsing", "text": "In order to measure the plausibility of a subtree rooted at h in dependency tree, we use a singleunit linear layer neural network to compute the score of its RCNN unit.\nFor constituent parsing, the representation of a non-terminal node only depends on its two children. The combination is relative simple and its correctness can be measured with the final representation of the non-terminal node (Socher et al., 2013a).\nHowever for dependency parsing, all combinations of the head h and its children ci(0 < i \u2264 K) are important to measure the correctness of the subtree. Therefore, our score function s(h) is computed on all of hidden layers zi(0 < i \u2264 K):\ns(h) = K\u2211 i=1 v(h,ci) \u00b7 zi, (6)\nwhere v(h,ci) \u2208 Rm\u00d71 is the score vector, which\nalso depends on the POS tags of h and ci. Given a sentence x and its dependency tree y, the goodness of a complete tree is measured by summing the scores of all the RCNN units.\ns(x, y,\u0398) = \u2211 h\u2208y s(h), (7)\nwhere h \u2208 y is the node in tree y; \u0398 = {\u0398W,\u0398v,\u0398w,\u0398d} including the combination matrix set \u0398W, the score vector set \u0398v, the word embeddings \u0398w and distance embeddings \u0398d.\nFinally, we can predict dependency tree y\u0302 with highest score for sentence x.\ny\u0302 = arg max y\u2208gen(x) s(x, y,\u0398), (8)\nwhere gen(x) is defined as the set of all possible trees for sentence x. When applied in re-ranking, gen(x) is the set of the k-best outputs of a base parser."}, {"heading": "5 Training", "text": "For a given training instance (xi, yi), we use the max-margin criterion to train our model. We first predict the dependency tree y\u0302i with the highest score for each xi and define a structured margin loss \u2206(yi, y\u0302i) between the predicted tree y\u0302i and the given correct tree yi. \u2206(yi, y\u0302i) is measured by counting the number of nodes yi with an incorrect span (or label) in the proposed tree (Goodman, 1998).\n\u2206(yi, y\u0302i) = \u2211 d\u2208y\u0302i \u03ba1{d /\u2208 yi} (9)\nwhere \u03ba is a discount parameter and d represents the nodes in trees.\nGiven a set of training dependency parses D, the final training objective is to minimize the loss function J(\u0398), plus a l2-regulation term:\nJ(\u0398) = 1 |D| \u2211\n(xi,yi)\u2208D\nri(\u0398) + \u03bb\n2 \u2016\u0398\u201622, (10)\nwhere\nri(\u0398) = max y\u0302i\u2208Y (xi) ( 0, st(xi, y\u0302i,\u0398)\n+ \u2206(yi, y\u0302i)\u2212 st(xi, yi,\u0398) ) . (11)\nBy minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree y\u0302i is decreased.\nWe use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direction. The subgradient of equation is:\n\u2202J \u2202\u0398 = 1 |D| \u2211\n(xi,yi)\u2208D\n( \u2202st(xi, y\u0302i,\u0398)\n\u2202\u0398 \u2212\n\u2202st(xi, yi,\u0398)\n\u2202\u0398 ) + \u03bb\u0398. (12)\nTo minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011). The parameter update for the i-th parameter \u0398t,i at time step t is as follows:\n\u0398t,i = \u0398t\u22121,i \u2212 \u03c1\u221a\u2211t \u03c4=1 g 2 \u03c4,i gt,i, (13)\nwhere \u03c1 is the initial learning rate and g\u03c4 \u2208 R|\u03b8i| is the subgradient at time step \u03c4 for parameter \u03b8i."}, {"heading": "6 Re-rankers", "text": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances.\nGiven T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser.\ny\u0302i = arg max y\u2208T (xi)\n\u03b1st(xi, y,\u0398) + (1\u2212 \u03b1)sb(xi, y)\n(14) where \u03b1 \u2208 [0, 1] is a hyperparameter; st(xi, y,\u0398) and sb(xi, y) are the scores given by RCNN and the base parser respectively.\nTo apply RCNN into re-ranking model, we first get the k-best outputs of all sentences in train set with a base parser. Thus, we can train the RCNN in a discriminative way and optimize the re-ranking strategy for a particular base parser.\nNote that the role of RCNN is not fully valued when applied in re-ranking model since that the gen(x) in Eq.(8) is just the k-best outputs of a base\nparser, not the set of all possible trees for sentence x. The parameters of RCNN could overfit to kbest outputs of training set."}, {"heading": "7 Experiments", "text": ""}, {"heading": "7.1 Datasets", "text": "To empirically demonstrate the effectiveness of our approach, we use two datasets in different languages (English and Chinese) in our experimental evaluation and compare our model against the other state-of-the-art methods using the unlabeled attachment score (UAS) metric ignoring punctuation.\nEnglish For English dataset, we follow the standard splits of Penn Treebank (PTB), using sections 2-21 for training, section 22 as development set and section 23 as test set. We tag the development and test sets using an automatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknifing similar to (Collins and Koo, 2005).\nChinese For Chinese dataset, we follow the same split of the Penn Chinese Treeban (CTB5) as described in (Zhang and Clark, 2008) and use sections 001-815, 1001-1136 as training set, sections 886-931, 1148- 1151 as development set, and sections 816-885, 1137-1147 as test set. Dependencies are converted by using the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008) (Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input.\nWe use the linear-time incremental parser (Huang and Sagae, 2010) as our base parser and calculate the 64-best parses at the top cell of the chart. Note that we optimize the training settings for base parser and the results are slightly improved on (Huang and Sagae, 2010). Then we use max-margin criterion to train RCNN. Finally, we use the mixture strategy to re-rank the top 64-best parses.\nFor initialization of parameters, we train word2vec embeddings (Mikolov et al., 2013) on Wikipedia corpus for English and Chinese respectively. For the combination matrices and score vectors, we use the random initialization within (0.01, 0.01). The parameters which achieve the best unlabeled attachment score on the development set will be chosen for the final evaluation."}, {"heading": "7.2 English Dataset", "text": "We first evaluate the performances of the RCNN and re-ranker (Eq. (14)) on the development set. Figure 5 shows UASs of different models with varying k. The base parser achieves 92.45%. When k = 64, the oracle best of base parser achieves 97.34%, while the oracle worst achieves 73.30% (-19.15%) . RCNN achieves the maximum improvement of 93.00%(+0.55%) when k = 6. When k > 6, the performance of RCNN declines with the increase of k but is still higher than baseline (92.45%). The reason behind this is that RCNN could require more negative samples to avoid overfitting when k is large. Since the negative samples are limited in the k-best outputs of a base parser, the learnt parameters could easily overfits to the training set.\nThe mixture re-ranker achieves the maximum improvement of 93.50%(+1.05%) when k = 64. In mixture re-ranker, \u03b1 is optimised by searching with the step-size 0.005.\nTherefore, we use the mixture re-ranker in the following experiments since it can take the advantages of both the RCNN and base models.\nFigure 6 shows the accuracies on the top ten POS tags of the modifier words with the largest improvements. We can see that our re-ranker can improve the accuracies of CC and IN, and therefore may indirectly result in rising the the well-known coordinating conjunction and PPattachment problems.\nThe final experimental results on test set are shown in Table 1. The hyperparameters of our\nmodel are set as in Table 2. Our re-ranker achieves the maximum improvement of 93.83%(+1.48%) on test set. Our system performs slightly better than many state-of-the-art systems such as Zhang and Clark (2008) and Huang and Sagae (2010). It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy.\nSince the result of ranker is conditioned to kbest results of base parser, we also do an experiment to avoid this limitation by adding the oracle to k-best candidates. With including oracle, the re-ranker can achieve 94.16% on UAS, which is shown in the last line (\u201cour re-ranker (with oracle)\u201d) of Table 1."}, {"heading": "7.3 Chinese Dataset", "text": "We also make experiments on the Penn Chinese Treebank (CTB5). The hyperparameters is the same as the previous experiment on English except that \u03b1 is optimised by searching with the step-size 0.005.\nThe final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (\u201cour re-ranker (with oracle)\u201d) of Table 3. Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted\nfeatures, our model can achieve a competitive performance with the minimal feature engineering."}, {"heading": "7.4 Discussions", "text": "The performance of the re-ranking model is affected by the base parser. The small divergence of the dependency trees in the output list also results to overfitting in training phase. Although our reranker outperforms the state-of-the-art methods, it can also benefit from improving the quality of the\ncandidate results. It was also reported in other reranking works that a larger k (eg. k > 64) results the worse performance. We think the reason is that the oracle best increases when k is larger, but the oracle worst decrease with larger degree. The error types increase greatly. The re-ranking model requires more negative samples to avoid overfitting. When k is larger, the number of negative samples also needs to multiply increase for training. However, we just can obtain at most k negative samples from the k-best outputs of the base parser.\nThe experiments also show that the our model can achieves significant improvements by adding the oracles into the output lists of the base parser. This indicates that our model can be boosted by a better set of the candidate results, which can be implemented by combining the RCNN in the decoding algorithm."}, {"heading": "8 Related Work", "text": "There have been several works to use neural networks and distributed representation for dependency parsing.\nStenetorp (2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his\nmodel is still unsatisfactory. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their methods aim to transition-based parsing and can not model the sentence in semantic vector space for other NLP tasks.\nSocher et al. (2013b) proposed a compositional vectors computed by dependency tree RNN (DT-RNN) to map sentences and images into a common embedding space. However, there are two major differences as follows. 1) They first summed up all child nodes into a dense vector vc and then composed subtree representation from vc and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pooling layer. 2) We represent the relative position of each child and its parent with distributed representation (position embeddings), which is very useful for convolutional layer. Figure 7 shows an example of DTRNN to illustrates how RCNN represents phrases as continuous vectors.\nSpecific to the re-ranking model, Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Unlike IORNN, our proposed RCNN is a discriminative model and can optimize the re-ranking strategy for a particular base parser. Another difference is that RCNN computes the score of tree in a recursive way, which is more natural for the hierarchical structure of natural lan-\nguage. Besides, the RCNN can not only be used for the re-ranking, but also be regarded as general model to represent sentence with its dependency tree."}, {"heading": "9 Conclusion", "text": "In this work, we address the problem to represent all level nodes (words or phrases) with dense representations in a dependency tree. We propose a recursive convolutional neural network (RCNN) architecture to capture the syntactic and compositional-semantic representations of phrases and words. RCNN is a general architecture and can deal with k-ary parsing tree, therefore RCNN is very suitable for many NLP tasks to minimize the effort in feature engineering with a external dependency parser. Although RCNN is just used for the re-ranking of the dependency parser in this paper, it can be regarded as semantic modelling of text sequences and handle the input sequences of varying length into a fixed-length vector. The parameters in RCNN can be learned jointly with some other NLP tasks, such as text classification.\nFor the future research, we will develop an integrated parser to combine RCNN with a decoding algorithm. We believe that the integrated parser can achieve better performance without the limitation of base parser. Moreover, we also wish to investigate the ability of our model for other NLP tasks."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by National Natural Science Foundation of China (61472088, 61473092), The National High Technology Research and Development Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200), Shanghai Leading Academic Discipline Project (B114)."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Coarse-to-fine n-best parsing", "author": ["Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson"], "venue": null, "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Feature embedding for dependency parsing", "author": ["Chen et al.2014] Wenliang Chen", "Yue Zhang", "Min Zhang"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Discriminative reranking for natural language parsing", "author": ["Collins", "Koo2005] Michael Collins", "Terry Koo"], "venue": "Computational Linguistics,", "citeRegEx": "Collins et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins"], "venue": "Computational linguistics,", "citeRegEx": "Collins.,? \\Q2003\\E", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Towards incremental parsing of natural language using recursive neural networks", "author": ["Costa et al.2003] Fabrizio Costa", "Paolo Frasconi", "Vincenzo Lombardo", "Giovanni Soda"], "venue": "Applied Intelligence,", "citeRegEx": "Costa et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Costa et al\\.", "year": 2003}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Parsing inside-out. arXiv preprint cmp-lg/9805007", "author": ["Joshua Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q1998\\E", "shortCiteRegEx": "Goodman.", "year": 1998}, {"title": "Efficient stacked dependency parsing by forest reranking", "author": ["Shuhei Kondo", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Hayashi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hayashi et al\\.", "year": 2013}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Sagae2010] Liang Huang", "Kenji Sagae"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of ICML", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "The inside-outside recursive neural network model for dependency parsing", "author": ["Le", "Zuidema2014] Phong Le", "Willem Zuidema"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Wide coverage natural language processing using kernel methods and neural networks for structured data", "author": ["Fabrizio Costa", "Paolo Frasconi", "Massimiliano Pontil"], "venue": null, "citeRegEx": "Menchetti et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Menchetti et al\\.", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50\u201357", "author": ["Joakim Nivre"], "venue": null, "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Recursive distributed representations", "author": ["Jordan B Pollack"], "venue": "Artificial Intelligence,", "citeRegEx": "Pollack.,? \\Q1990\\E", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "online) subgradient methods for structured prediction", "author": ["J Andrew Bagnell", "Martin A Zinkevich"], "venue": "In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats)", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "A generative re-ranking model for dependency parsing", "author": ["Willem Zuidema", "Rens Bod"], "venue": "In Proceedings of the 11th International Conference on Parsing Technologies,", "citeRegEx": "Sangati et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sangati et al\\.", "year": 2009}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Q Le", "C Manning", "A Ng"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Transitionbased dependency parsing using recursive neural networks", "author": ["Pontus Stenetorp"], "venue": "In NIPS Workshop on Deep Learning", "citeRegEx": "Stenetorp.,? \\Q2013\\E", "shortCiteRegEx": "Stenetorp.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003] H. Yamada", "Y. Matsumoto"], "venue": "In Proceedings of the International Workshop on Parsing Technologies (IWPT),", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beam-search", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data.", "startOffset": 97, "endOffset": 161}, {"referenceID": 16, "context": "Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data.", "startOffset": 97, "endOffset": 161}, {"referenceID": 29, "context": "natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al.", "startOffset": 56, "endOffset": 147}, {"referenceID": 18, "context": "natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al.", "startOffset": 56, "endOffset": 147}, {"referenceID": 6, "context": "natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al.", "startOffset": 56, "endOffset": 147}, {"referenceID": 13, "context": ", 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013).", "startOffset": 46, "endOffset": 88}, {"referenceID": 19, "context": ", 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013).", "startOffset": 46, "endOffset": 88}, {"referenceID": 1, "context": "For dependency parsing, Chen et al. (2014) and Bansal et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": "(2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation.", "startOffset": 11, "endOffset": 32}, {"referenceID": 21, "context": "The idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure (Pollack, 1990; Socher et al., 2013c).", "startOffset": 197, "endOffset": 234}, {"referenceID": 24, "context": "Based on RNN, Socher et al. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations.", "startOffset": 14, "endOffset": 36}, {"referenceID": 24, "context": "For more details on how standard RNN can be used for parsing, see (Socher et al., 2011).", "startOffset": 66, "endOffset": 87}, {"referenceID": 5, "context": "For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003).", "startOffset": 105, "endOffset": 120}, {"referenceID": 10, "context": "\u2206(yi, \u0177i) is measured by counting the number of nodes yi with an incorrect span (or label) in the proposed tree (Goodman, 1998).", "startOffset": 112, "endOffset": 127}, {"referenceID": 22, "context": "We use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direction.", "startOffset": 70, "endOffset": 92}, {"referenceID": 8, "context": "To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011).", "startOffset": 66, "endOffset": 86}, {"referenceID": 11, "context": "Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser.", "startOffset": 120, "endOffset": 164}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005).", "startOffset": 42, "endOffset": 65}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing.", "startOffset": 42, "endOffset": 97}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser.", "startOffset": 42, "endOffset": 216}, {"referenceID": 5, "context": "Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing.", "startOffset": 42, "endOffset": 318}, {"referenceID": 19, "context": "For initialization of parameters, we train word2vec embeddings (Mikolov et al., 2013) on Wikipedia corpus for English and Chinese respectively.", "startOffset": 63, "endOffset": 85}, {"referenceID": 11, "context": "It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy.", "startOffset": 15, "endOffset": 37}, {"referenceID": 11, "context": "It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy.", "startOffset": 15, "endOffset": 63}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.", "startOffset": 38, "endOffset": 60}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.", "startOffset": 38, "endOffset": 147}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.", "startOffset": 38, "endOffset": 175}, {"referenceID": 9, "context": "Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.", "startOffset": 38, "endOffset": 225}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.", "startOffset": 3, "endOffset": 22}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.", "startOffset": 3, "endOffset": 52}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.", "startOffset": 3, "endOffset": 90}, {"referenceID": 2, "context": "25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.", "startOffset": 3, "endOffset": 118}, {"referenceID": 2, "context": "20 Distributed Representations Chen et al. (2014) 82.", "startOffset": 31, "endOffset": 50}, {"referenceID": 2, "context": "20 Distributed Representations Chen et al. (2014) 82.94 Chen and Manning (2014) 83.", "startOffset": 31, "endOffset": 80}, {"referenceID": 2, "context": "20 Distributed Representations Chen et al. (2014) 82.94 Chen and Manning (2014) 83.9 Re-rankers Hayashi et al. (2013) 85.", "startOffset": 31, "endOffset": 118}, {"referenceID": 9, "context": "Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990).", "startOffset": 137, "endOffset": 150}], "year": 2015, "abstractText": "In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.", "creator": "LaTeX with hyperref package"}}}