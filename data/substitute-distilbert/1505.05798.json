{"id": "1505.05798", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret", "abstract": "lifelong reinforcement learning provides more promising framework for developing versatile agents which can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. however, current lifelong learning implementation exhibit non - canonical consistency as the amount of delay increases and practical limitations potentially can lead to suboptimal or unsafe control policies. to address these issues, we develop a lifelong policy gradient learner that operates in an adversarial set - ting to learn multiple tasks alone while enforcing safety constraints controlling the learned policies. we demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm atop several benchmark verification systems having an application to quadrotor algorithms.", "histories": [["v1", "Thu, 21 May 2015 17:24:57 GMT  (283kb,D)", "http://arxiv.org/abs/1505.05798v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haitham bou-ammar", "rasul tutunov", "eric eaton"], "accepted": true, "id": "1505.05798"}, "pdf": {"name": "1505.05798.pdf", "metadata": {"source": "META", "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret", "authors": ["Haitham Bou Ammar", "Rasul Tutunov", "Eric Eaton"], "emails": ["HAITHAMB@SEAS.UPENN.EDU", "TUTUNOV@SEAS.UPENN.EDU", "EEATON@CIS.UPENN.EDU"], "sections": [{"heading": null, "text": "1. Introduction Reinforcement learning (RL) (Busoniu et al., 2010; Sutton & Barto, 1998) often requires substantial experience before achieving acceptable performance on individual control problems. One major contributor to this issue is the tabula-rasa assumption of typical RL methods, which learn from scratch on each new task. In these settings, learning performance is directly correlated with the quality of the acquired samples. Unfortunately, the amount of experience necessary for high-quality performance increases exponentially with the tasks\u2019 degrees of freedom, inhibiting the application of RL to high-dimensional control problems.\nWhen data is in limited supply, transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training (Taylor & Stone, 2009; Gheshlaghi Azar et al., 2013; Lazaric, 2011; Ferrante et al., 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultane-\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).\nIn the lifelong learning setting (Thrun & O\u2019Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al. (2014) developed a lifelong learner for policy gradient RL. To ensure efficient learning over consecutive tasks, these works employ a second-order Taylor expansion around the parameters that are (locally) optimal for each task without transfer. This assumption simplifies the MTL objective into a weighted quadratic form for online learning, but since it is based on single-task learning, this technique can lead to parameters far from globally optimal. Consequently, the success of these methods for RL highly depends on the policy initializations, which must lead to near-optimal trajectories for meaningful updates. Also, since their objective functions average loss over all tasks, these methods exhibit non-vanishing regrets of the form O(R), where R is the total number of rounds in a non-adversarial setting.\nIn addition, these methods may produce control policies with unsafe behavior (i.e., capable of causing damage to the agent or environment, catastrophic failure, etc.). This is a critical issue in robotic control, where unsafe control policies can lead to physical damage or user injury. This problem is caused by using constraint-free optimization over the shared knowledge during the transfer process, which may lead to uninformative or unbounded policies.\nIn this paper, we address these issues by proposing the first safe lifelong learner for policy gradient RL operating in an adversarial framework. Our approach rapidly learns highperformance safe control policies based on the agent\u2019s previously learned knowledge and safety constraints on each task, accumulating knowledge over multiple consecutive tasks to optimize overall performance. We theoretically analyze the regret exhibited by our algorithm, showing sublinear dependency of the form O( \u221a R) for R rounds, thus outperforming current methods. We then evaluate our approach empirically on a set of dynamical systems.\nar X\niv :1\n50 5.\n05 79\n8v 1\n[ cs\n.L G\n] 2\n1 M\nay 2\n01 5\n2. Background 2.1. Reinforcement Learning\nAn RL agent sequentially chooses actions to minimize its expected cost. Such problems are formalized as Markov decision processes (MDPs) \u3008X ,U ,P, c, \u03b3\u3009, where X \u2282 Rd is the (potentially infinite) state space, U \u2208 Rda is the set of all possible actions, P : X \u00d7 U \u00d7 X \u2192 [0, 1] is a state transition probability describing the system\u2019s dynamics, c : X \u00d7 U \u00d7 X \u2192 R is the cost function measuring the agent\u2019s performance, and \u03b3 \u2208 [0, 1] is a discount factor. At each time step m, the agent is in state xm \u2208 X and must choose an action um \u2208 U , transitioning it to a new state xm+1 \u223c P (xm+1|xm,um) and yielding a cost cm+1 = c(xm+1,um,xm). The sequence of state-action pairs forms a trajectory \u03c4 = [x0:M\u22121,u0:M\u22121] over a (possibly infinite) horizon M . A policy \u03c0 : X \u00d7U \u2192 [0, 1] specifies a probability distribution over state-action pairs, where \u03c0 (u|x) represents the probability of selecting an actionu in state x. The goal of RL is to find an optimal policy \u03c0? that minimizes the total expected cost.\nPolicy search methods have shown success in solving high-dimensional problems, such as robotic control (Kober & Peters, 2011; Peters & Schaal, 2008a; Sutton et al., 2000). These methods represent the policy \u03c0\u03b1(u|x) using a vector \u03b1 \u2208 Rd of control parameters. The optimal policy \u03c0? is found by determining the parameters \u03b1? that minimize the expected average cost:\nl(\u03b1) = n\u2211 k=1 p\u03b1 ( \u03c4 (k) ) C ( \u03c4 (k) ) , (1)\nwhere n is the total number of trajectories, and p\u03b1 ( \u03c4 (k) ) andC ( \u03c4 (k) ) are the probability and cost of trajectory \u03c4 (k):\np\u03b1\n( \u03c4 (k) ) = P0 ( x (k) 0 )M\u22121\u220f m=0 P ( x (k) m+1|x(k)m ,u(k)m ) \u00d7 \u03c0\u03b1 ( u(k)m |x(k)m\n) (2) C ( \u03c4 (k) ) = 1\nM M\u22121\u2211 m=0 c ( x (k) m+1,u (k) m ,x (k) m ) , (3)\nwith an initial state distribution P0 : X \u2192 [0, 1]. We handle a constrained version of policy search, in which optimality not only corresponds to minimizing the total expected cost, but also to ensuring that the policy satisfies safety constraints. These constraints vary between applications, for example corresponding to maximum joint torque or prohibited physical positions.\n2.2. Online Learning & Regret Analysis\nIn this paper, we employ a special form of regret minimization games, which we briefly review here. A regret minimization game is a triple \u3008K,F , R\u3009, where K is a nonempty decision set, F is the set of moves of the adversary\nwhich contains bounded convex functions from Rn to R, and R is the total number of rounds. The game proceeds in rounds, where at each round j = 1, . . . , R, the agent chooses a prediction \u03b8j \u2208 K and the environment (i.e., the adversary) chooses a loss function lj \u2208 F . At the end of the round, the loss function lj is revealed to the agent and the decision \u03b8j is revealed to the environment. In this paper, we handle the full-information case, where the agent may observe the entire loss function lj as its feedback and can exploit this in making decisions. The goal is to minimize the cumulative regret \u2211R j=1 lj(\u03b8j)\u2212infu\u2208K [\u2211R j=1 lj(u) ] . When analyzing the regret of our methods, we use a variant of this definition to handle the lifelong RL case:\nRR = R\u2211 j=1 ltj (\u03b8j)\u2212 inf u\u2208K  R\u2211 j=1 ltj (u)  , where ltj (\u00b7) denotes the loss of task t at round j. For our framework, we adopt a variant of regret minimization called \u201cFollow the Regularized Leader,\u201d which minimizes regret in two steps. First, the unconstrained solution \u03b8\u0303 is determined (see Sect. 4.1) by solving an unconstrained optimization over the accumulated losses observed so far. Given \u03b8\u0303, the constrained solution is then determined by learning a projection into the constraint set via Bregman projections (see Abbasi-Yadkori et al. (2013)).\n3. Safe Lifelong Policy Search We adopt a lifelong learning framework in which the agent learns multiple RL tasks consecutively, providing it the opportunity to transfer knowledge between tasks to improve learning. Let T denote the set of tasks, each element of which is an MDP. At any time, the learner may face any previously seen task, and so must strive to maximize its performance across all tasks. The goal is to learn optimal policies \u03c0?\u03b1?1 , . . . , \u03c0 ? \u03b1?|T | for all tasks, where policy \u03c0?\u03b1?t for task t is parameterized by \u03b1?t \u2208 Rd. In addition, each task is equipped with safety constraints to ensure acceptable policy behavior: At\u03b1t \u2264 bt, with At \u2208 Rd\u00d7d and bt \u2208 Rd representing the allowed policy combinations. The precise form of these constraints depends on the application domain, but this formulation supports constraints on (e.g.) joint torque, acceleration, position, etc.\nAt each round j, the learner observes a set of ntj trajectories { \u03c4 (1) tj , . . . , \u03c4 (ntj ) tj } from a task tj \u2208 T , where each trajectory has length Mtj . To support knowledge transfer between tasks, we assume that each task\u2019s policy parameters \u03b1tj \u2208 Rd at round j can be written as a linear combination of a shared latent basis L \u2208 Rd\u00d7k with coefficient vectors stj \u2208 Rk; therefore, \u03b1tj = Lstj . Each column of L represents a chunk of transferrable knowledge; this task construction has been used successfully in previous\nmulti-task learning work (Kumar & Daume\u0301 III, 2012; Ruvolo & Eaton, 2013; Bou Ammar et al., 2014). Extending this previous work, we ensure that the shared knowledge repository is \u201cinformative\u201d by incorporating bounding constraints on the Frobenius norm \u2016 \u00b7 \u2016F of L. Consequently, the optimization problem after observing r rounds is:\nmin L,S r\u2211 j=1 [ \u03b7tj ltj ( Lstj )] + \u00b51 ||S||2F + \u00b52 ||L|| 2 F (4)\ns.t. Atj\u03b1tj \u2264 btj \u2200tj \u2208 Ir \u03bbmin ( LLT ) \u2265 p and \u03bbmax ( LLT ) \u2264 q ,\nwhere p and q are the constraints on \u2016L\u2016F, \u03b7tj \u2208 R are design weighting parameters1, Ir = {t1, . . . , tr} denotes the set of all tasks observed so far through round r, and S is the collection of all coefficients\nS(:, h) = { sth if th \u2208 Ir 0 otherwise \u2200h \u2208 {1, . . . , |T |} .\nThe loss function ltj (\u03b1tj ) in Eq. (4) corresponds to a policy gradient learner for task tj , as defined in Eq. (1). Typical policy gradient methods (Kober & Peters, 2011; Sutton et al., 2000) maximize a lower bound of the expected cost ltj ( \u03b1tj ) , which can be derived by taking the logarithm and applying Jensen\u2019s inequality: log [ ltj ( \u03b1tj )] = log ntj\u2211 k=1 p (tj) \u03b1tj ( \u03c4 (k) tj ) C(tj) ( \u03c4 (k) tj\n) (5) \u2265 log [ ntj ] + E Mtj\u22121\u2211 m=0 log [ \u03c0\u03b1tj ( u(k,tj)m | x(k,tj)m )]ntj k=1 +const .\nTherefore, our goal is to minimize the following objective:\ner = r\u2211 j=1 \u2212 \u03b7tj ntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 log [ \u03c0\u03b1tj ( u(k,tj)m | x(k,tj)m )] (6)\n+ \u00b51 \u2016S\u20162F + \u00b52 \u2016L\u2016 2 F\ns.t. Atj\u03b1tj \u2264 btj \u2200tj \u2208 Ir \u03bbmin ( LLT ) \u2265 p and \u03bbmax ( LLT ) \u2264 q .\n3.1. Online Formulation\nThe optimization problem above can be mapped to the standard online learning framework by unrolling L and S into a vector \u03b8 = [vec(L) vec(S)]T \u2208 Rdk+k|T |. Choosing \u21260(\u03b8) = \u00b52 \u2211dk i=1 \u03b8 2 i + \u00b51 \u2211dk+k|T | i=dk+1 \u03b8 2 i , and \u2126j(\u03b8) = \u2126j\u22121(\u03b8) + \u03b7tj ltj (\u03b8), we can write the safe lifelong policy search problem (Eq. (6)) as:\n\u03b8r+1 = argmin \u03b8\u2208K \u2126r(\u03b8) , (7)\nwhere K \u2286 Rdk+k|T | is the set of allowable policies under the given safety constraints. Note that the loss for task tj\n1We describe later how to set the \u03b7\u2019s later in Sect. 5 to obtain regret bounds, and leave them as variables now for generality.\ncan be written as a bilinear product in \u03b8:\nltj (\u03b8) = \u2212 1\nntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 log [ \u03c0 (tj) \u0398L\u0398stj ( u(k, tj)m | x(k, tj)m )]\n\u0398L =  \u03b81 . . . \u03b8d(k\u22121)+1... ... ... \u03b8d . . . \u03b8dk  , \u0398stj =  \u03b8dk+1... \u03b8(d+1)k+1  . We see that the problem in Eq. (7) is equivalent to Eq. (6) by noting that at r rounds, \u2126r = \u2211r j=1 \u03b7tj ltj (\u03b8)+\u21260(\u03b8).\n4. Online Learning Method We solve Eq. (7) in two steps. First, we determine the unconstrained solution \u03b8\u0303r+1 when K = Rdk+k|T | (see Sect. 4.1). Given \u03b8\u0303r+1, we derive the constrained solution \u03b8\u0302r+1 by learning a projection Proj\u2126r,K ( \u03b8\u0303r+1 ) to the constraint set K \u2286 Rdk+k|T |, which amounts to minimizing the Bregman divergence over \u2126r(\u03b8) (see Sect. 4.2)2. The complete approach is given in Algorithm 1 and is available as a software implementation on the authors\u2019 websites.\n4.1. Unconstrained Policy Solution\nAlthough Eq. (6) is not jointly convex in both L and S, it is separably convex (for log-concave policy distributions). Consequently, we follow an alternating optimization approach, first computing L while holding S fixed, and then updating S given the acquiredL. We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b). The derivations of the update rules below can be found in Appendix A.\nThese updates are governed by learning rates \u03b2 and \u03bb that decay over time; \u03b2 and \u03bb can be chosen using line-search methods as discussed by Boyd & Vandenberghe (2004). In our experiments, we adopt a simple yet effective strategy, where \u03b2 = cj\u22121 and \u03bb = cj\u22121, with 0 < c < 1.\nStep 1: UpdatingL HoldingS fixed, the latent repository can be updated according to:\nL\u03b2+1 = L\u03b2 \u2212 \u03b7\u03b2L\u2207Ler(L,S) (eREINFORCE) L\u03b2+1 = L\u03b2 \u2212 \u03b7\u03b2LG \u22121(L\u03b2 ,S\u03b2)\u2207Ler(L,S) (eNAC)\nwith learning rate \u03b7\u03b2L \u2208 R, and G\u22121(L,S) as the inverse of the Fisher information matrix (Peters & Schaal, 2008b).\nIn the special case of Gaussian policies, the update for L\n2In Sect. 4.2, we linearize the loss around the constrained solution of the previous round to increase stability and ensure convergence. Given the linear losses, it suffices to solve the Bregman divergence over the regularizer, reducing the computational cost.\ncan be derived in a closed form as L\u03b2+1 = Z\u22121L vL, where\nZL =2\u00b52Idk\u00d7dk+ r\u2211 j=1 \u03b7tj ntj\u03c3 2 tj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 vec ( \u03a6sTtj )( \u03a6T\u2297sTtj )\nvL = \u2211 j \u03b7tj ntj\u03c3 2 tj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 vec ( u(k, tj)m \u03a6s T tj ) ,\n\u03c32tj is the covariance of the Gaussian policy for a task tj , and \u03a6 = \u03a6 ( x (k, tj) m ) denotes the state features.\nStep 2: Updating S Given the fixed basis L, the coefficient matrix S is updated column-wise for all tj \u2208 Ir:\ns (tj) \u03bb+1 = s (tj) \u03bb+1 \u2212 \u03b7 \u03bb S\u2207stj er(L,S) (eREINFORCE)\ns (tj) \u03bb+1 = s (tj) \u03bb+1 \u2212 \u03b7 \u03bb SG \u22121(L\u03b2 ,S\u03b2)\u2207stj er(L,S) (eNAC) with learning rate \u03b7\u03bbS \u2208 R. For Gaussian policies, the closed-form of the update is stj = Z \u22121 stj vstj , where\nZstj = 2\u00b51Ik\u00d7k + \u2211 tk=tj \u03b7tj ntj\u03c3 2 tj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 LT\u03a6\u03a6TL\nvtj = \u2211 tk=tj \u03b7tj ntj\u03c3 2 tj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 u(k, tj)m L T\u03a6 .\n4.2. Constrained Policy Solution\nOnce we have obtained the unconstrained solution \u03b8\u0303r+1 (which satisfies Eq. (7), but can lead to policy parameters in unsafe regions), we then derive the constrained solution to ensure safe policies. We learn a projection Proj\u2126r,K ( \u03b8\u0303r+1 ) from \u03b8\u0303r+1 to the constraint set:\n\u03b8\u0302r+1 = argmin \u03b8\u2208K B\u2126r,K\n( \u03b8, \u03b8\u0303r+1 ) , (8)\nwhereB\u2126r,K ( \u03b8, \u03b8\u0303r+1 ) is the Bregman divergence over \u2126r:\nB\u2126r,K ( \u03b8, \u03b8\u0303r+1 ) = \u2126r(\u03b8)\u2212\u2126r(\u03b8\u0303r+1)\n\u2212 trace ( \u2207\u03b8\u2126r (\u03b8) \u2223\u2223\u2223 \u03b8\u0303r+1 ( \u03b8 \u2212 \u03b8\u0303r+1 )) .\nSolving Eq. (8) is computationally expensive since \u2126r(\u03b8) includes the sum back to the original round. To remedy this problem, ensure the stability of our approach, and guarantee that the constrained solutions for all observed tasks lie within a bounded region, we linearize the current-round loss function ltr (\u03b8) around the constrained solution of the previous round \u03b8\u0302r:\nltr (u\u0302) = f\u0302tr \u2223\u2223\u2223T \u03b8\u0302r u\u0302 , (9)\nwhere\nf\u0302tr \u2223\u2223\u2223 \u03b8\u0302r =\n \u2207\u03b8ltr (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r\nltr (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2212\u2207\u03b8ltr (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u03b8\u0302r\n , u\u0302 = [ u 1 ] .\nGiven the above linear form, we can rewrite the optimization problem in Eq. (8) as:\n\u03b8\u0302r+1 = argmin \u03b8\u2208K B\u21260,K\n( \u03b8, \u03b8\u0303r+1 ) . (10)\nConsequently, determining safe policies for lifelong policy search reinforcement learning amounts to solving:\nmin L,S\n\u00b51\u2016S\u20162F + \u00b52\u2016L\u20162F\n+ 2\u00b51trace ( ST \u2223\u2223\u2223 \u03b8\u0303r+1 S ) + 2\u00b52trace ( L \u2223\u2223\u2223 \u03b8\u0303r+1 L ) s.t.AtjLstj \u2264 btj \u2200tj \u2208 Ir LLT \u2264 pI and LLT \u2265 qI .\nTo solve the optimization problem above, we start by converting the inequality constraints to equality constraints by introducing slack variables ctj \u2265 0. We also guarantee that these slack variables are bounded by incorporating \u2016ctj\u2016 \u2264 cmax, \u2200tj \u2208 {1, . . . , |T |}:\nmin L,S,C\n\u00b51\u2016S\u20162F + \u00b52\u2016L\u20162F\n+ 2\u00b52trace ( LT \u2223\u2223\u2223 \u03b8\u0303r+1 L ) + 2\u00b51trace ( ST \u2223\u2223\u2223 \u03b8\u0303r+1 S ) s.t.AtjLstj = btj \u2212 ctj \u2200tj \u2208 Ir ctj > 0 and \u2016ctj\u20162 \u2264 cmax \u2200tj \u2208 Ir LLT \u2264 pI and LLT \u2265 qI .\nWith this formulation, learning Proj\u2126r,K ( \u03b8\u0303r+1 ) amounts to solving second-order cone and semi-definite programs.\n4.2.1. SEMI-DEFINITE PROGRAM FOR LEARNING L\nThis section determines the constrained projection of the shared basisL given fixedS andC. We show thatL can be acquired efficiently, since this step can be relaxed to solving a semi-definite program in LLT (Boyd & Vandenberghe, 2004). To formulate the semi-definite program, note that\ntrace ( LT \u2223\u2223\u2223 \u03b8\u0303r+1 L ) = k\u2211 i=1 l (i) r+1 T\u2223\u2223\u2223 \u03b8\u0303r+1 li\n\u2264 k\u2211 i=1 \u2225\u2225\u2225\u2225l(i)r+1\u2223\u2223\u2223\u03b8\u0303r+1 \u2225\u2225\u2225\u2225 2 \u2016li\u20162\n\u2264 \u221a\u221a\u221a\u221a k\u2211 i=1 \u2225\u2225\u2225\u2225l(i)r \u2223\u2223\u2223 \u03b8\u0303r+1 \u2225\u2225\u2225\u22252 2 \u221a\u221a\u221a\u221a k\u2211 i=1 ||li||22\n= \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223L\u2223\u2223\u2223 \u03b8\u0303r+1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u221a trace (LLT) .\nFrom the constraint set, we recognize:\nsTtjL T = ( btj \u2212 ctj )T ( A\u2020tj )T =\u21d2 sTtjL TLstj = a T tjatj with atj = A \u2020 tj ( btj \u2212 ctj ) .\nAlgorithm 1 Safe Online Lifelong Policy Search 1: Inputs: Total number of rounds R, weighting factor \u03b7 = 1/ \u221a R, regularization parameters \u00b51 and \u00b52, con-\nstraints p and q, number of latent basis vectors k. 2: S = zeros(k, |T |), L = diagk(\u03b6) with p \u2264 \u03b62 \u2264 q 3: for j = 1 to R do 4: tj \u2190 sampleTask(), and update Ij 5: Compute unconstrained solution \u03b8\u0303j+1 (Sect. 4.1) 6: Fix S and C, and update L (Sect. 4.2.1) 7: Use updated L to derive S and C (Sect. 4.2.2) 8: end for 9: Output: Safety-constrained L and S\nSince spectrum ( LLT ) = spectrum ( LTL ) , we can write:\nmin X\u2282S++ \u00b52trace(X) + 2\u00b52 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223L\u2223\u2223\u2223 \u03b8\u0303r+1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u221a trace (X)\ns.t. sTtjXstj = a T tjatj \u2200tj \u2208 Ir\nX \u2264 pI and X \u2265 qI , with X = LTL .\n4.2.2. SECOND-ORDER CONE PROGRAM FOR LEARNING TASK PROJECTIONS\nHaving determined L, we can acquire S and update C by solving a second-order cone program (Boyd & Vandenberghe, 2004) of the following form:\nmin st1 ,...,stj ,ct1 ,...,ctj \u00b51 r\u2211 j=1 \u2016stj\u201622 + 2\u00b51 r\u2211 j=1 sTtj \u2223\u2223\u2223 \u03b8\u0302r stj\ns.t. AtjLstj = btj \u2212 ctj ctj > 0 \u2016ctj\u201622 \u2264 c2max \u2200tj \u2208 Ir .\n5. Theoretical Guarantees This section quantifies the performance of our approach by providing formal analysis of the regret after R rounds. We show that the safe lifelong reinforcement learner exhibits sublinear regret in the total number of rounds. Formally, we prove the following theorem:\nTheorem 1 (Sublinear Regret). After R rounds and choosing \u2200tj \u2208 IR \u03b7tj = \u03b7 = 1\u221aR , L \u2223\u2223\u2223 \u03b8\u03021 = diagk(\u03b6), with diagk(\u00b7) being a diagonal matrix among the k columns of L, p \u2264 \u03b62 \u2264 q, and S\n\u2223\u2223\u2223 \u03b8\u03021 = 0k\u00d7|T |, the safe lifelong rein-\nforcement learner exhibits sublinear regret of the form: R\u2211 j=1 ltj ( \u03b8\u0302j ) \u2212 ltj (u) = O (\u221a R ) for any u \u2208 K.\nProof Roadmap: The remainder of this section completes our proof of Theorem 1; further details are given in Appendix B. We assume linear losses for all tasks in the constrained case in accordance with Sect. 4.2. Although linear\nlosses for policy search RL are too restrictive given a single operating point, as discussed previously, we remedy this problem by generalizing to the case of piece-wise linear losses, where the linearization operating point is a resultant of the optimization problem. To bound the regret, we need to bound the dual Euclidean norm (which is the same as the Euclidean norm) of the gradient of the loss function, then prove Theorem 1 by bounding: (1) task tj\u2019s gradient loss (Sect. 5.1), and (2) linearized losses with respect to L and S (Sect. 5.2).\n5.1. Bounding tj\u2019s Gradient Loss\nWe start by stating essential lemmas for Theorem 1; due to space constraints, proofs for all lemmas are available in the supplementary material. Here, we bound the gradient of a loss function ltj (\u03b8) at round r under Gaussian policies\n3. Assumption 1. We assume that the policy for a task tj is Gaussian, the action set U is bounded by umax, and the feature set is upper-bounded by \u03a6max.\nLemma 1. Assume task tj\u2019s policy at round r is given by\n\u03c0 (tj) \u03b1tj\n( u (k, tj) m |x(k, tj)m )\u2223\u2223\u2223 \u03b8\u0302r = N ( \u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x (k, tj) m ) ,\u03c3tj ) ,\nfor states x(k, tj)m \u2208 Xtj and actions u (k, tj) m \u2208 Utj . For ltj ( \u03b1tj ) = \u2212 1ntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 log [ \u03c0 (tj) \u03b1tj ( u(k, tj)m |x(k, tj)m )] , the\ngradient \u2207\u03b1tj ltj ( \u03b1tj )\u2223\u2223\u2223 \u03b8\u0302r satisfies \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj(\u03b1tj)\u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264\nMtj \u03c32tj\n( umax + max\ntk\u2208Ir\u22121\n{\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)}\u03a6max ) \u03a6max\nfor all trajectories and all tasks, with umax = max k,m {\u2223\u2223\u2223u(k, tj)m \u2223\u2223\u2223} and \u03a6max=max k,m {\u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223 2 } .\n5.2. Bounding Linearized Losses\nAs discussed previously, we linearize the loss of task tr around the constraint solution of the previous round \u03b8\u0302r. To acquire the regret bounds in Theorem 1, the next step is to\nbound the dual norm, \u2225\u2225\u2225\u2225f\u0302tr \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u2225? 2 = \u2225\u2225\u2225\u2225f\u0302tr \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 of Eq. (9). It\ncan be easily seen\u2225\u2225\u2225\u2225f\u0302tr \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 \u2264 \u2223\u2223\u2223\u2223ltr (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38 constant + \u2225\u2225\u2225\u2225\u2207\u03b8ltr (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nLemma 2\n(11)\n+ \u2225\u2225\u2225\u2225\u2207\u03b8ltr (\u03b8)\u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 \u00d7 \u2225\u2225\u2225\u03b8\u0302r\u2225\u2225\u2225\n2\ufe38 \ufe37\ufe37 \ufe38 Lemma 3 .\n3Please note that derivations for other forms of log-concave policy distributions could be derived in similar manner. In this work, we focus on Gaussian policies since they cover a broad spectrum of real-world applications.\nSince \u2223\u2223\u2223\u2223ltr (\u03b8) \u2223\u2223\u2223\n\u03b8\u0302r \u2223\u2223\u2223\u2223 can be bounded by \u03b4ltr (see Sect. 2), the next step is to bound \u2225\u2225\u2225\u2225\u2207\u03b8ltr (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 , and \u2016\u03b8\u0302r\u20162. Lemma 2. The norm of the gradient of the loss function evaluated at \u03b8\u0302r satisfies\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltr (\u03b8) \u2223\u2223\u2223\n\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tr ltr (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u22232 2 ( q \u00d7 d\n( 2d/p2 max\ntk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||22 + c2max) } + 1 )) .\nTo finalize the bound of \u2225\u2225\u2225\u2225f\u0302tr \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u2225 2 as needed for deriving\nthe regret, we must derive an upper-bound for \u2016\u03b8\u0302r\u20162: Lemma 3. The L2 norm of the constraint solution at round r \u2212 1, \u2016\u03b8\u0302r\u201622 is bounded by\n\u2016\u03b8\u0302r\u201622 \u2264 q \u00d7 d [ 1 + |Ir\u22121| 1\np2\nmax tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 }] ,\nwhere |Ir\u22121| is the number of unique tasks observed so far.\nGiven the previous two lemmas, we can prove the bound for \u2225\u2225\u2225\u2225f\u0302tr \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u2225 2 :\nLemma 4. The L2 norm of the linearizing term of ltr (\u03b8) around \u03b8\u0302r, \u2225\u2225\u2225\u2225f\u0302tr \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u2225 2\n, is bounded by\u2225\u2225\u2225\u2225f\u0302tr \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225\u2225\u2207\u03b8ltr(\u03b8)\u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 ( 1+\u2016\u03b8\u0302r\u20162 ) + \u2223\u2223\u2223\u2223ltr(\u03b8)\u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223 (12) \u2264 \u03b31(r) (1 + \u03b32(r)) + \u03b4ltr ,\nwhere \u03b4ltr is the constant upper-bound on \u2223\u2223\u2223\u2223ltr (\u03b8)\u2223\u2223\u2223\n\u03b8\u0302r \u2223\u2223\u2223\u2223, and \u03b31(r) = 1\nntj\u03c3 2 tj\n[( umax\n+ max tk\u2208Ir\u22121\n{\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)}\u03a6max ) \u03a6max ]\n\u00d7 ( d\np\n\u221a 2q \u221a max\ntk\u2208Ir\u22121\n{ \u2016A\u2020tk\u2016 2 2 (\u2016btk\u201622 + c2max) } + \u221a qd ) \u03b32(r) \u2264 \u221a q \u00d7 d\n+ \u221a |Ir\u22121| \u221a 1+ 1\np2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322(||btk ||2 + cmax)2 } .\n5.3. Completing the Proof of Sublinear Regret\nGiven the lemmas in the previous section, we now can derive the sublinear regret bound given in Theorem 1. Using\nresults developed by Abbasi-Yadkori et al. (2013), it is easy to see that \u2207\u03b8\u21260 ( \u03b8\u0303j ) \u2212\u2207\u03b8\u21260 ( \u03b8\u0303j+1 ) = \u03b7tj f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j . From the convexity of the regularizer, we obtain:\n\u21260 ( \u03b8\u0302j ) \u2265 \u21260 ( \u03b8\u0302j+1 ) + \u2329 \u2207\u03b8\u21260 ( \u03b8\u0302j+1 ) , \u03b8\u0302j \u2212 \u03b8\u0302j+1 \u232a + 1\n2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b8\u0302j \u2212 \u03b8\u0302j+1\u2223\u2223\u2223\u2223\u2223\u22232 2 .\nWe have: \u2225\u2225\u2225\u03b8\u0302j \u2212 \u03b8\u0302j+1\u2225\u2225\u2225 2 \u2264 \u03b7tj \u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j \u2225\u2225\u2225\u2225 2 . Therefore, for any u \u2208 K r\u2211 j=1 \u03b7tj ( ltj ( \u03b8\u0302j ) \u2212 ltj (u) ) \u2264 r\u2211 j=1 \u03b7tj \u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j \u2225\u2225\u2225\u22252 2\n+ \u21260(u)\u2212\u21260(\u03b8\u03021) . Assuming that \u2200tj \u03b7tj = \u03b7, we can derive: r\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212 ltj (u) ) \u2264 \u03b7 r\u2211 j=1 \u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j \u2225\u2225\u2225\u22252 2\n+ 1/\u03b7 ( \u21260(u)\u2212\u21260(\u03b8\u03021) ) .\nThe following lemma finalizes the proof of Theorem 1:\nLemma 5. AfterR rounds with \u2200tj \u03b7tj = \u03b7 = 1\u221aR , for any u \u2208 K we have that \u2211R j=1 ltj (\u03b8\u0302j)\u2212 ltj (u) \u2264 O (\u221a R ) .\nProof. From Eq. (12), it follows that\u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 2 \u2264 \u03b33(R) + 4\u03b321(R)\u03b322(R)\n\u2264 \u03b33(R) + 8 d\np2 \u03b321(R)qd\n( 1 + |IR\u22121|\n\u00d7 max tk\u2208IR\u22121\n{ \u2016A\u2020tk\u20162 (\u2016btk\u20162 + cmax) 2 })\nwith \u03b33(R) = 4\u03b321(R) + 2maxtj\u2208IR\u22121 \u03b4 2 tj . Since |IR\u22121| \u2264 |T |, we have that \u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u22252 2 \u2264 \u03b35(R)|T | with\n\u03b35 = 8d/p 2q\u03b321(R) max\ntk\u2208IR\u22121\n{ \u2016A\u2020tk\u2016 2 2 (\u2016btk\u20162 + cmax) 2 } .\nGiven that \u21260(u) \u2264 qd + \u03b35(R)|T |, with \u03b35(R) being a constant, we have: r\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212ltj(u) ) \u2264 \u03b7 r\u2211 j=1 \u03b35(R)|T |\n+ 1\n\u03b7\n( qd+ \u03b35(R)|T | \u2212\u21260(\u03b8\u03021) ) .\nInitializing L and S: We initialize L \u2223\u2223\u2223 \u03b8\u03021 = diagk(\u03b6), with\np \u2264 \u03b62 \u2264 q and S \u2223\u2223\u2223 \u03b8\u03021 = 0k\u00d7|T | to ensure the invertibility\nof L and that the constraints are met. This leads to r\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212ltj(u) ) \u2264 \u03b7 r\u2211 j=1 \u03b35(R)|T |\n+ 1/\u03b7 (qd+ \u03b35(R)|T | \u2212 \u00b52k\u03b6) . Choosing \u2200tj \u03b7tj = \u03b7 = 1/ \u221a R, we acquire sublinear regret, finalizing the statement of Theorem 1: r\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212ltj(u) ) \u2264 1/\u221aR\u03b35(R)|T |R\n+ \u221a R (qd+ \u03b35(R)|T | \u2212 \u00b52k\u03b6)\n\u2264 \u221a R ( \u03b35(R)|T |+ qd\u03b35(R)|T | \u2212 \u00b52k\u03b6 ) \u2264 O (\u221a R ) .\n6. Experimental Validation To validate the empirical performance of our method, we applied our safe online PG algorithm to learn multiple consecutive control tasks on three dynamical systems (Figure 1). To generate multiple tasks, we varied the parameterization of each system, yielding a set of control tasks from each domain with varying dynamics. The optimal control policies for these systems vary widely with only minor changes in the system parameters, providing substantial diversity among the tasks within a single domain.\nSimple Mass Spring Damper: The simple mass (SM) system is characterized by three parameters: the spring constant k in N/m, the damping constant d in Ns/m and the mass m in kg. The system\u2019s state is given by the position x and x\u0307 of the mass, which varies according to a linear force F . The goal is to train a policy for controlling the mass in a specific state gref = \u3008xref, x\u0307ref\u3009. Cart Pole: The cart-pole (CP) has been used extensively as a benchmark for evaluating RL methods (Busoniu et al., 2010). CP dynamics are characterized by the cart\u2019s mass mc in kg, the pole\u2019s mass mp in kg, the pole\u2019s length in meters, and a damping parameter d in Ns/m. The state is given by the cart\u2019s position x and velocity x\u0307, as well as the pole\u2019s angle \u03b8 and angular velocity \u03b8\u0307. The goal is to train a policy that controls the pole in an upright position.\n6.1. Experimental Protocol\nWe generated 10 tasks for each domain by varying the system parameters to ensure a variety of tasks with diverse op-\ntimal policies, including those with highly chaotic dynamics that are difficult to control. We ran each experiment for a total of R rounds, varying from 150 for the simple mass to 10, 000 for the quadrotor to train L and S, as well as for updating the PG-ELLA and PG models. At each round j, the learner observed a task tj through 50 trajectories of 150 steps and updated L and stj . The dimensionality k of the latent space was chosen independently for each domain via cross-validation over 3 tasks, and the learning step size for each task domain was determined by a line search after gathering 10 trajectories of length 150. We used eNAC, a standard PG algorithm, as the base learner.\nWe compared our approach to both standard PG (i.e., eNAC) and PG-ELLA (Bou Ammar et al., 2014), examining both the constrained and unconstrained variants of our algorithm. We also varied the number of iterations in our alternating optimization from 10 to 100 to evaluate the effect of these inner iterations on the performance, as shown in Figures 2 and 3. For the two MTL algorithms (our approach and PG-ELLA), the policy parameters for each task tj were initialized using the learned basis (i.e., \u03b1tj = Lstj ). We configured PG-ELLA as described by Bou Ammar et al. (2014), ensuring a fair comparison. For the standard PG learner, we provided additional trajectories in order to ensure a fair comparison, as described below.\nFor the experiments with policy constraints, we generated a set of constraints (At, bt) for each task that restricted the policy parameters to pre-specified \u201csafe\u201d regions, as shown in Figures 2(c) and 2(d). We also tested different values for the constraints on L, varying p and q between 0.1 to 10; our approach showed robustness against this broad range, yielding similar average cost performance.\n6.2. Results on Benchmark Systems\nFigure 2 reports our results on the benchmark simple mass and cart-pole systems. Figures 2(a) and 2(b) depicts the performance of the learned policy in a lifelong learning setting over consecutive unconstrained tasks, averaged over all 10 systems over 100 different initial conditions. These results demonstrate that our approach is capable of outperforming both standard PG (which was provided with 50 additional trajectories each iteration to ensure a more fair comparison) and PG-ELLA, both in terms of initial performance and learning speed. These figures also show that the performance of our method increases as it is given more alternating iterations per-round for fitting L and S.\nWe evaluated the ability of these methods to respect safety constraints, as shown in Figures 2(c) and 2(d). The thicker black lines in each figure depict the allowable \u201csafe\u201d region of the policy space. To enable online learning per-task, the same task tj was observed on each round and the shared basis L and coefficients stj were updated using alternating optimization. We then plotted the change in the policy pa-\nrameter vectors per iterations (i.e., \u03b1tj = Lstj ) for each method, demonstrating that our approach abides by the safety constraints, while standard PG and PG-ELLA can violate them (since they only solve an unconstrained optimization problem). In addition, these figures show that increasing the number of alternating iterations in our method causes it to take a more direct path to the optimal solution.\n6.3. Application to Quadrotor Control\nWe also applied our approach to the more challenging domain of quadrotor control. The dynamics of the quadrotor system (Figure 1) are influenced by inertial constants around e1,B , e2,B , and e3,B , thrust factors influencing how the rotor\u2019s speed affects the overall variation of the system\u2019s state, and the lengths of the rods supporting the rotors. Although the overall state of the system can be described by a 12-dimensional vector, we focus on stability and so consider only six of these state-variables. The quadrotor system has a high-dimensional action space, where the goal is to control the four rotational velocities {wi}4i=1 of the rotors to stabilize the system. To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors.\nWe generated 10 different quadrotor systems by varying the inertia around the x, y and z-axes. We used a linear quadratic regulator, as described by Bouabdallah (2007), to initialize the policies in both the learning and testing phases. We followed a similar experimental procedure to that discussed above to update the models.\nFigure 3 shows the performance of the unconstrained solution as compared to standard PG and PG-ELLA. Again, our approach clearly outperforms standard PG and PG-ELLA in both the initial performance and learning speed. We also evaluated constrained tasks in a similar manner, again showing that our approach is capable of respecting constraints. Since the policy space is higher dimensional, we cannot visualize it as well as the benchmark systems, and so instead report the number of iterations it takes our approach\nto project the policy into the safe region. Figure 4 shows that our approach requires only one observation of the task to acquire safe policies, which is substantially lower then standard PG or PG-ELLA (e.g., which require 545 and 510 observations, respectively, in the quadrotor scenario).\n7. Conclusion We described the first lifelong PG learner that provides sublinear regret O( \u221a R) with R total rounds. In addition, our approach supports safety constraints on the learned policy, which are essential for robust learning in real applications. Our framework formalizes lifelong learning as online MTL with limited resources, and enables safe transfer by sharing policy parameters through a latent knowledge base that is efficiently updated over time.\nReferences Yasin Abbasi-Yadkori, Peter Bartlett, Varun Kanade,\nYevgeny Seldin, & Csaba Szepesva\u0301ri. Online learning in Markov decision processes with adversarially chosen transition probability distributions. Advances in Neural Information Processing Systems 26, 2013.\nHaitham Bou Ammar, Karl Tuyls, Matthew E. Taylor, Kurt Driessen, & Gerhard Weiss. Reinforcement learning transfer via sparse coding. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2012.\nHaitham Bou Ammar, Eric Eaton, Paul Ruvolo, & Matthew Taylor. Online multi-task learning for policy gradient methods. In Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.\nSamir Bouabdallah. Design and Control of Quadrotors with Application to Autonomous Flying. PhD Thesis, E\u0301cole polytechnique fe\u0301de\u0301rale de Lausanne, 2007.\nStephen Boyd & Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, 2004.\nLucian Busoniu, Robert Babuska, Bart De Schutter, & Damien Ernst. Reinforcement Learning and Dynamic Programming Using Function Approximators. CRC Press, Boca Raton, FL, 2010.\nEliseo Ferrante, Alessandro Lazaric, & Marcello Restelli. Transfer of task representation in reinforcement learning using policy-based proto-value functions. In Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2008.\nMohammad Gheshlaghi Azar, Alessandro Lazaric, & Emma Brunskill. Sequential transfer in multi-armed bandit with finite set of models. Advances in Neural Information Processing Systems 26, 2013.\nRoger A. Horn & Roy Mathias. Cauchy-Schwarz inequalities associated with positive semidefinite matrices. Linear Algebra and its Applications 142:63\u201382, 1990.\nJens Kober & Jan Peters. Policy search for motor primitives in robotics. Machine Learning, 84(1\u20132):171\u2013203, 2011.\nAbhishek Kumar & Hal Daume\u0301 III. Learning task grouping and overlap in multi-task learning. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.\nAlessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In M. Wiering & M. van Otterlo, editors, Reinforcement Learning: State of the Art. Springer, 2011.\nJan Peters & Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural Networks, 2008a.\nJan Peters & Stefan Schaal. Natural Actor-Critic. Neurocomputing 71, 2008b.\nPaul Ruvolo & Eric Eaton. ELLA: An Efficient Lifelong Learning Algorithm. In Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.\nRichard S. Sutton & Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, 1998.\nRichard S. Sutton, David Mcallester, Satinder Singh, & Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems 12, 2000.\nMatthew E. Taylor & Peter Stone. Transfer learning for reinforcement learning domains: a survey. Journal of Machine Learning Research, 10:1633\u20131685, 2009.\nSebastian Thrun & Joseph O\u2019Sullivan. Discovering structure in multiple learning tasks: the TC algorithm. In Proceedings of the 13th International Conference on Machine Learning (ICML), 1996a.\nSebastian Thrun & Joseph O\u2019Sullivan. Learning more from less data: experiments in lifelong learning. Seminar Digest, 1996b.\nHolger Voos & Haitham Bou Ammar. Nonlinear tracking and landing controller for quadrotor aerial robots. In Proceedings of the IEEE Multi-Conference on Systems and Control, 2010.\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8(3\u20134):229\u2013256, 1992.\nAaron Wilson, Alan Fern, Soumya Ray, & Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical Bayesian approach. In Proceedings of the 24th International Conference on Machine Learning (ICML), 2007.\nJian Zhang, Zoubin Ghahramani, & Yiming Yang. Flexible latent variable models for multi-task learning. Machine Learning, 73(3):221\u2013242, 2008.\nA. Update Equations Derivation In this appendix, we derive the update equations forL and S in the special case of Gaussian policies. Please note that these derivations can be easily extended to other policy forms in higher dimensional action spaces.\nFor a task tj , the policy \u03c0 (tj) \u03b1tj\n( u (k,tj) m |x(k,tj)m ) is given by:\n\u03c0 (tj) \u03b1tj ( u(k,tj)m |x(k,tj)m ) =\n1\u221a 2\u03c0\u03c32tj exp ( \u2212 1 2\u03c32tj ( u(k,tj)m \u2212 ( Lstj )T \u03a6 ( x(k,tj)m ))2) .\nTherefore, the safe lifelong reinforcement learning optimization objective can be written as:\ner(L,S) = r\u2211 j=1 \u03b7tj 2\u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 ( u(k,tj)m \u2212 ( Lstj )T \u03a6 ( x(k,tj)m ))2 + \u00b51||S||2F + \u00b52||L||2F . (13)\nTo arrive at the update equations, we need to derive Eq. (13) with respect to each L and S.\nA.1. Update Equations for L\nStarting with the derivative of er(L,S) with respect to the shared repository L, we can write:\n\u2207Ler(L,S) = \u2207L  r\u2211 j=1 \u03b7tj 2\u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 ( u(k,tj)m \u2212 ( Lstj )T \u03a6 ( x(k,tj)m ))2 + \u00b51||S||2F + \u00b52||L||2F  = \u2212\nr\u2211 j=1  \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 ( u(k,tj)m \u2212 ( Lstj )T \u03a6 ( x(k,tj)m )) \u03a6 ( x(k,tj)m ) sTtj + 2\u00b52L . To acquire the minimum, we set the above to zero:\nr\u2211 j=1  \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 ( u(k,tj)m \u2212 ( Lstj )T \u03a6 ( x(k,tj)m )) \u03a6 ( x(k,tj)m ) sTtj + 2\u00b52L = 0 r\u2211 j=1  \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 sTtjL T\u03a6 ( x(k,tj)m ) \u03a6 ( x(k,tj)m ) sTtj + 2\u00b52L = r\u2211 j=1 \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 u(k,tj)m \u03a6 ( x(k,tj)m ) sTtj .\nNoting that sTtjL T\u03a6 ( x (k,tj) m ) \u2208 R, we can write:\nr\u2211 j=1  \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 \u03a6 ( x(k,tj)m ) sTtj\u03a6 T ( x(k,tj)m ) Lstj + 2\u00b52L = r\u2211 j=1 \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 u(k,tj)m \u03a6 ( x(k,tj)m ) sTtj .\n(14) To solve Eq. (14), we introduce the standard vec(\u00b7) operator leading to:\nvec  r\u2211 j=1  \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 \u03a6 ( x(k,tj)m ) sTtj\u03a6 T ( x(k,tj)m ) Lstj + 2\u00b52L \n= vec  r\u2211 j=1 \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 u(k,tj)m \u03a6 ( x(k,tj)m ) sTtj  r\u2211 j=1 \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 vec ( \u03a6 ( x(k,tj)m ) sTtj ) vec ( \u03a6T ( x(k,tj)m ) Lstj ) + 2\u00b52vec(L)\n= r\u2211 j=1 \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 vec ( u(k,tj)m \u03a6 ( x(k,tj)m ) sTtj ) .\nKnowing that for a given set of matricesA,B, andX , vec(AXB) = ( BT \u2297A ) vec(X), we can write r\u2211 j=1 \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 vec ( \u03a6 ( x(k,tj)m ) sTtj )( sTtj \u2297\u03a6 T ( x(k,tj)m )) vec(L) + 2\u00b52vec(L)\n= r\u2211 j=1 \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 vec ( u(k,tj)m \u03a6 ( x(k,tj)m ) sTtj ) .\nBy choosing ZL = 2\u00b52Idk\u00d7dk + \u2211r j=1\n\u03b7tj ntj\u03c3 2 tj\n\u2211ntj k=1 \u2211Mtj\u22121 m=0 vec ( \u03a6 ( x (k,tj) m ) sTtj )( \u03a6 ( x (k,tj) m ) \u2297 sTtj ) , and vL =\u2211r\nj=1 \u03b7tj ntj\u03c3 2 tj\n\u2211ntj k=1 \u2211Mtj\u22121 m=0 vec ( u (k,tj) m \u03a6 ( x (k,tj) m ) sTtj ) , we can update L = Z\u22121L vL.\nA.2. Update Equations for S\nTo derive the update equations with respect to S, similar approach to that ofL can be followed. The derivative of er(L,S) with respect to S can be computed column-wise for all tasks observed so far:\n\u2207stj er(L,S) = \u2207stj  r\u2211 j=1 \u03b7tj 2\u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 ( u(k,tj)m \u2212 ( Lstj )T \u03a6 ( x(k,tj)m ))2 + \u00b51||S||2F + \u00b52||L||2F  = \u2212\n\u2211 tk=tj  \u03b7tj \u03c32tjntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 ( u(k,tj)m \u2212 ( Lstj )T \u03a6 ( x(k,tj)m )) LT\u03a6 ( x(k,tj)m )+ 2\u00b52stj . Using a similar analysis to the previous section, choosing\nZstj = 2\u00b51Ik\u00d7k + \u2211 tk=tj \u03b7tj ntj\u03c3 2 tj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 LT\u03a6 ( x(k,tj)m ) \u03a6T ( x(k,tj)m ) L ,\nvstj = \u2211 tk=tj \u03b7tj ntj\u03c3 2 tj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 u(k,tj)m L T\u03a6 ( x(k,tj)m ) ,\nwe can update stj = Z \u22121 stj vstj .\nB. Proofs of Theoretical Guarantees In this appendix, we prove the claims and lemmas from the main paper, leading to sublinear regret (Theorem 1).\nLemma 1. Assume the policy for a task tj at a round r to be given by \u03c0 (tj) \u03b1tj\n( u (k, tj) m |x(k, tj)m ) \u2223\u2223\u2223 \u03b8\u0302r =\nN ( \u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x (k, tj) m ) ,\u03c3tj ) , for x(k, tj)m \u2208 Xtj and u (k, tj) m \u2208 Utj with Xtj and Utj representing the state and action\nspaces, respectively. The gradient\u2207\u03b1tj ltj ( \u03b1tj ) \u2223\u2223\u2223 \u03b8\u0302r , for ltj ( \u03b1tj ) = \u22121/ntj \u2211ntj k=1 \u2211Mtj\u22121 m=0 log [ \u03c0 (tj) \u03b1tj ( u (k, tj) m |x(k, tj)m )] satisfies \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 Mtj \u03c32tj [( umax + max tk\u2208Ir\u22121\n{\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)}\u03a6max)\u03a6max] , with umax = maxk,m {\u2223\u2223\u2223u(k, tj)m \u2223\u2223\u2223} and \u03a6max = maxk,m {\u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223 2 } for all trajectories and all tasks.\nProof. The proof of the above lemma will be provided as a collection of claims. We start with the following: Claim: Given \u03c0(tj)\u03b1tj ( u (k) m |x(k)m ) \u2223\u2223\u2223 \u03b8\u0302r = N ( \u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x (k, tj) m ) ,\u03c3tj ) , for x(k, tj)m \u2208 Xtj and u (k, tj) m \u2208 Utj , and\nltj ( \u03b1tj ) = \u22121/ntj \u2211ntj k=1 \u2211Mtj\u22121 m=0 log [ \u03c0 (tj) \u03b1tj ( u (k, tj) m |x(k, tj)m )] , \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2\nsatisfies\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 Mtj \u03c32tj [( umax + \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u03a6max ) \u03a6max ] . (15)\nProof: Since \u03c0(tj)\u03b1tj ( u (k, tj) m |x(k, tj)m ) \u2223\u2223\u2223 \u03b8\u0302r = N ( \u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x (k, tj) m ) ,\u03c3tj ) , we can write\nlog [ \u03c0 (tj) \u03b1tj ( u(k, tj)m |x(k, tj)m ) \u2223\u2223\u2223 \u03b8\u0302r ] = \u2212 log [\u221a 2\u03c0\u03c32tj ] \u2212 1\n2\u03c32tj\n( u(k, tj)m \u2212\u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x(k, tj)m ))2 .\nTherefore:\n\u2207\u03b1tj ltj ( \u03b1tj ) \u2223\u2223\u2223 \u03b8\u0302r = \u2212 1 ntj ntj\u2211 k=1 Mtj\u22121\u2211 m=0 1 \u03c32tj ( u(k, tj)m \u2212\u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x(k, tj)m )) \u03a6 ( x(k, tj)m ) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 Mtj \u03c32tj [ max k,m {\u2223\u2223\u2223\u2223u(k, tj)m \u2212\u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x(k, tj)m )\u2223\u2223\u2223\u2223\u00d7 \u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223 2 }]\n\u2264 Mtj \u03c32tj [ max k,m {\u2223\u2223\u2223u(k, tj)m \u2223\u2223\u2223\u00d7 \u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223 2 } +max\nk,m {\u2223\u2223\u2223\u2223\u03b1Ttj \u2223\u2223\u2223 \u03b8\u0302r \u03a6 ( x(k, tj)m )\u2223\u2223\u2223\u2223\u00d7 \u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223 2 }]\n\u2264 Mtj \u03c32tj [ max k,m {\u2223\u2223\u2223u(k, tj)m \u2223\u2223\u2223}max k,m {\u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223 2 } +max\nk,m {\u2223\u2223\u2223\u2223\u2329\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r ,\u03a6 ( x(k, tj)m )\u232a\u2223\u2223\u2223\u2223}maxk,m {\u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u22232} ] .\nDenoting maxk,m {\u2223\u2223\u2223u(k, tj)m \u2223\u2223\u2223} = umax and maxk,m {\u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223\n2\n} = \u03a6max for all trajectories and all tasks, we can\nwrite \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 Mtj \u03c32tj [( umax +max k,m {\u2223\u2223\u2223\u2223\u2329\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r ,\u03a6 ( x(k, tj)m )\u232a\u2223\u2223\u2223\u2223})\u03a6max] . Using the Cauchy-Shwarz inequality (Horn & Mathias, 1990), we can upper bound maxk,m {\u2223\u2223\u2223\u2223\u2329\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r ,\u03a6 ( x (k, tj) m )\u232a\u2223\u2223\u2223\u2223} as\nmax k,m {\u2223\u2223\u2223\u2223\u2329\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r ,\u03a6 ( x(k, tj)m )\u232a\u2223\u2223\u2223\u2223} \u2264 maxk,m {\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2223\u2223\u2223\u2223\u2223\u2223\u03a6(x(k, tj)m )\u2223\u2223\u2223\u2223\u2223\u2223 2 } \u2264 max k,m {\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 } \u03a6max\n\u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u03b1tj \u2223\u2223\u2223\n\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u03a6max .\nFinalizing the statement of the claim, the overall bound on the norm of the gradient of ltj (\u03b1tj ) can be written as\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 Mtj \u03c32tj [( umax + \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u03b1tj \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u03a6max ) \u03a6max ] . (16)\nClaim: The norm of the gradient of the loss function satisfies:\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 Mtj \u03c32tj [( umax + max tk\u2208Ir\u22121 { \u2016A+tk\u20162 (\u2016btk\u20162 + cmax) } \u03a6max ) \u03a6max ] .\nProof: As mentioned previously, we consider the linearization of the loss function ltj around the constraint solution of the previous round, \u03b8\u0302r. Since \u03b8\u0302r satisfiesAtk\u03b1tk = btk \u2212 ctk ,\u2200tk \u2208 Ir\u22121. Hence, we can write\nAtk\u03b1tk + ctk = btk \u2200tk \u2208 Ir\u22121\n=\u21d2 \u03b1tk = A + tk (btk \u2212 ctk) withA + tk = ( ATtkAtk )\u22121 ATtk being the left pseudo-inverse.\nTherefore\n||\u03b1tk ||2 \u2264 \u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + ||ctk ||2)\n\u2264 \u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax) .\nCombining the above results with those of Eq. (16) we arrive at\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b1tj)\u2223\u2223\u2223\u2223\u2223\u22232 \u2264 Mtj\u03c32tj [( umax + max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)}\u03a6max)\u03a6max] .\nThe previous result finalizes the statement of the lemma, bounding the gradient of the loss function in terms of the safety constraints.\nLemma 2. The norm of the gradient of the loss function evaluated at \u03b8\u0302r satisfies\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b8) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u22232 2 ( q \u00d7 d ( 2d/p2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tj \u2223\u2223\u2223\u2223\u2223\u22232 2 (\u2223\u2223\u2223\u2223btj \u2223\u2223\u2223\u222322 + c2max)}+ 1) ) . Proof. The derivative of ltj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r can be written as\n\u2207\u03b8ltj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r =  \u2207\u03b1tj l T tj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r  \u2202\u03b1 (1) tj \u2202\u03b81 \u2223\u2223\u2223 \u03b8\u0302r ... \u2202\u03b1 (d) tj \u2202\u03b81 \u2223\u2223\u2223 \u03b8\u0302r  ... \u2207\u03b1tj l T tj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r  \u2202\u03b1 (1) tj \u2202\u03b8dk+k|T | \u2223\u2223\u2223 \u03b8\u0302r ... \u2202\u03b1\n(d) tj\n\u2202\u03b8dk+k|T | \u2223\u2223\u2223 \u03b8\u0302r\n  =  \u2207\u03b1tj l T tj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r  \u03b8dk+1 \u2223\u2223\u2223 \u03b8\u0302r 0 ... 0  ... \u2207\u03b1tj l T tj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r  0 ... \u03b8(d+1)k+1 \u2223\u2223\u2223 \u03b8\u0302r  ... \u2207\u03b1tj l T tj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r  \u03b8d(k+1)+1 \u2223\u2223\u2223 \u03b8\u0302r ...\n\u03b8dk \u2223\u2223\u2223 \u03b8\u0302r\n  =\u21d2 \u2225\u2225\u2225\u2225\u2207\u03b8ltj (\u03b8)\u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 2 \u2264 \u2225\u2225\u2225\u2225\u2207\u03b1tj ltj (\u03b1tj )\u2223\u2223\u2223\u03b8\u0302r \u2225\u2225\u2225\u22252 2 [ d \u2225\u2225\u2225\u2225stj \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 2 + \u2225\u2225\u2225\u2225L\u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 F ] .\nThe results of Lemma 1 bound \u2225\u2225\u2225\u2225\u2207\u03b1tj ltj (\u03b8)\u2223\u2223\u2223\u03b8\u0302r \u2225\u2225\u2225\u22252 2 .\nNow, we target to bound each of \u2223\u2223\u2223\u2223\u2223\u2223stj \u2223\u2223\u2223\n\u03b8\u0302r\n\u2223\u2223\u2223\u2223\u2223\u22232 2 and \u2223\u2223\u2223\u2223\u2223\u2223L\u2223\u2223\u2223\n\u03b8\u0302r\n\u2223\u2223\u2223\u2223\u2223\u22232 F .\nBounding \u2225\u2225\u2225\u2225stj \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u22252 2 and \u2016L \u2223\u2223\u2223 \u03b8\u0302r \u20162F: Considering the constraint AtjLstj + ctj = btj for a task tj , we realize that\nstj = L + ( A+tj ( btj \u2212 ctj )) . Therefore,\u2225\u2225\u2225\u2225stj \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u2225 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223L+ (A+tj (btj \u2212 ctj))\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 \u2223\u2223\u2223\u2223L+\u2223\u2223\u2223\u2223 2 \u2223\u2223\u2223\u2223\u2223\u2223A+tj \u2223\u2223\u2223\u2223\u2223\u2223 2 (\u2223\u2223\u2223\u2223btj \u2223\u2223\u2223\u22232 + \u2223\u2223\u2223\u2223ctj \u2223\u2223\u2223\u22232) . (17) Noting that \u2223\u2223\u2223\u2223L+\u2223\u2223\u2223\u2223 2 = \u2223\u2223\u2223\u2223\u2223\u2223(LTL)\u22121LT\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223(LTL)\u22121\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2223\u2223\u2223\u2223LT\u2223\u2223\u2223\u2223 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223(LTL)\u22121\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2223\u2223\u2223\u2223LT\u2223\u2223\u2223\u2223 F\n= \u2223\u2223\u2223\u2223\u2223\u2223(LTL)\u22121\u2223\u2223\u2223\u2223\u2223\u2223\n2 ||L||F .\nTo relate ||L+||2 to ||L||F, we need to bound \u2223\u2223\u2223\u2223\u2223\u2223(LTL)\u22121\u2223\u2223\u2223\u2223\u2223\u2223\n2 in terms of \u2016L\u2016F. Denoting the spectrum of LTL as spec ( LTL ) = {\u03bb1, . . . ,\u03bbk} such that 0 < \u03bb1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbk, then spect (( LTL )\u22121) = {1/\u03bb1, . . . , 1/\u03bbk} such\nthat 1/\u03bbk \u2264 \u00b7 \u00b7 \u00b7 \u2264 1/\u03bbk. Hence, \u2223\u2223\u2223\u2223\u2223\u2223(LTL)\u22121\u2223\u2223\u2223\u2223\u2223\u2223\n2 = max\n{ spec (( LTL )\u22121)} = 1/\u03bb1 = 1/\u03bbmin ( LTL ) . Noticing that\nspec ( LTL ) = spec ( LLT ) , we recognize \u2223\u2223\u2223\u2223\u2223\u2223(LTL)\u22121\u2223\u2223\u2223\u2223\u2223\u2223 2 = 1/\u03bbmin ( LLT\n) \u2264 1/p. Therefore\u2223\u2223\u2223\u2223L+\u2223\u2223\u2223\u2223\n2 \u2264 1 p ||L||F . (18)\nPlugging the results of Eq. (18) into Eq. (17), we arrive at\u2225\u2225\u2225\u2225stj \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 \u2264 1/p \u2225\u2225\u2225\u2225L\u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 F max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)} . (19) Finally, since \u03b8\u0302r satisfies the constraints, we note that \u2225\u2225\u2225\u2225L\u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 F \u2264 q \u00d7 d. Consequently,\n\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b8) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u22232 2 ( q \u00d7 d ( 2d p2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||22 + c2max) } + 1 )) .\nLemma 3. The L2 norm of the constraint solution at round r \u2212 1, \u2016\u03b8\u0302r\u201622 is bounded by \u2016\u03b8\u0302r\u201622 \u2264 q \u00d7 d [ 1 + |Ir\u22121| 1\np2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 }] .\nwith |Ir\u22121| being the cardinality of Ir\u22121 representing the number of different tasks observed so-far. Proof. Noting that \u03b8\u0302r = [ \u03b81, . . . ,\u03b8dk\ufe38 \ufe37\ufe37 \ufe38\nL \u2223\u2223\u2223 \u03b8\u0302r\n,\u03b8dk+1, . . .\ufe38 \ufe37\ufe37 \ufe38 si1 \u2223\u2223\u2223 \u03b8\u0302r , . . . , . . .\ufe38 \ufe37\ufe37 \ufe38 sir\u22121 \u2223\u2223\u2223 \u03b8\u0302r , . . . ,\u03b8dk+kT?\ufe38 \ufe37\ufe37 \ufe38 0\u2019s: unobserved tasks\n]T , it is easy to see\n\u2016\u03b8\u0302r\u201622 \u2264 \u2225\u2225\u2225\u2225L\u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u22252 F + |Ir\u22121| max tk\u2208Ir\u22121 {\u2225\u2225\u2225\u2225stk \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 2 }\n\u2264 q \u00d7 d+ |Ir\u22121| max tk\u2208Ir\u22121 [ q \u00d7 d p2 \u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 ]\n\u2264 q \u00d7 d [ 1 + |Ir\u22121| 1\np2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 }] .\nLemma 4. The L2 norm of the linearizing term of ltj (\u03b8) around \u03b8\u0302r, \u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223\n\u03b8\u0302r \u2225\u2225\u2225\u2225 2\n, is bounded by\u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225\u2225\u2207\u03b8ltj (\u03b8)\u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u2225 2 ( 1 + \u2016\u03b8\u0302r\u20162 ) + \u2223\u2223\u2223\u2223ltj (\u03b8)\u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223 \u2264 \u03b31(r) (1 + \u03b32(r)) + \u03b4ltj , with \u03b4ltj being the constant upper-bound on \u2223\u2223\u2223\u2223ltj (\u03b8)\u2223\u2223\u2223 \u03b8\u0302r\n\u2223\u2223\u2223\u2223, and \u03b31(r) = 1\nntj\u03c3 2 tj\n[( umax + max\ntk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)}\u03a6max)\u03a6max] \u00d7 ( d/p \u221a 2q \u221a max\ntk\u2208Ir\u22121\n{ \u2016A\u2020tk\u2016 2 2 (\u2016btk\u201622 + c2max) } + \u221a qd ) .\n\u03b32(r) \u2264 \u221a q \u00d7 d+ \u221a |Ir\u22121| \u221a[ 1 + 1\np2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 }] .\nProof. We have previously shown that \u2223\u2223\u2223\u2223\u2223\u2223f\u0302tj \u2223\u2223\u2223\n\u03b8\u0302r\n\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltj (\u03b8) \u2223\u2223\u2223\n\u03b8\u0302r\n\u2223\u2223\u2223\u2223\u2223\u2223 2 + \u2223\u2223\u2223ltj (\u03b8\u0302r) \u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltj (\u03b8)\u2223\u2223\u2223\n\u03b8\u0302r\n\u2223\u2223\u2223\u2223\u2223\u2223 2 \u00d7 \u2223\u2223\u2223\u2223\u2223\u2223\u03b8\u0302r\u2223\u2223\u2223\u2223\u2223\u2223 2 . Using\nthe previously derived lemmas we can upper-bound \u2223\u2223\u2223\u2223\u2223\u2223f\u0302tj \u2223\u2223\u2223\n\u03b8\u0302r\n\u2223\u2223\u2223\u2223\u2223\u2223 2\nas follows\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltj (\u03b8) \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b8) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u22232 2 ( q \u00d7 d ( 2d p2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||22 + c2max) } + 1 )) \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltj (\u03b8) \u2223\u2223\u2223\n\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b1tj ltj (\u03b8) \u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223 2 ( d/p \u221a 2q \u221a max tk\u2208Ir\u22121 { \u2016A\u2020tk\u2016 2 2 (\u2016btk\u201622 + c2max) } + \u221a qd ) \u2264 1 ntj\u03c3 2 tj [( umax + max tk\u2208Ir\u22121\n{\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)}\u03a6max)\u03a6max] \u00d7 ( d/p \u221a 2q \u221a max\ntk\u2208Ir\u22121\n{ \u2016A\u2020tk\u2016 2 2 (\u2016btk\u201622 + c2max) } + \u221a qd ) .\nFurther, \u2223\u2223\u2223\u2223\u2223\u2223\u03b8\u0302r\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 q \u00d7 d+ |Ir\u22121| max tk\u2208Ir\u22121 [ 1 + 1 p2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 }]\n=\u21d2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b8\u0302r\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 \u221a q \u00d7 d+ \u221a |Ir\u22121| \u221a[ 1 + 1 p2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 }] .\nTherefore \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223f\u0302tj \u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2207\u03b8ltj (\u03b8)\u2223\u2223\u2223\u03b8\u0302r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 ( 1 + \u2223\u2223\u2223\u2223\u2223\u2223\u03b8\u0302r\u2223\u2223\u2223\u2223\u2223\u2223 2 ) + \u2223\u2223\u2223\u2223ltj (\u03b8)\u2223\u2223\u2223 \u03b8\u0302r \u2223\u2223\u2223\u2223 (20) \u2264 \u03b31(r) (1 + \u03b32(r)) + \u03b4ltj ,\nwith \u03b4ltj being the constant upper-bound on \u2223\u2223\u2223\u2223ltj (\u03b8)\u2223\u2223\u2223\n\u03b8\u0302r \u2223\u2223\u2223\u2223, and \u03b31(r) = 1\nntj\u03c3 2 tj\n[( umax + max\ntk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223A+tk \u2223\u2223\u2223\u22232 (||btk ||2 + cmax)}\u03a6max)\u03a6max] \u00d7 ( d/p \u221a 2q \u221a max\ntk\u2208Ir\u22121\n{ \u2016A\u2020tk\u2016 2 2 (\u2016btk\u201622 + c2max) } + \u221a qd ) .\n\u03b32(r) \u2264 \u221a q \u00d7 d+ \u221a |Ir\u22121| \u221a[ 1 + 1\np2 max tk\u2208Ir\u22121 {\u2223\u2223\u2223\u2223\u2223\u2223A\u2020tk \u2223\u2223\u2223\u2223\u2223\u222322 (||btk ||2 + cmax)2 }] .\nTheorem 1 (Sublinear Regret; restated from the main paper). After R rounds and choosing \u03b7t1 = \u00b7 \u00b7 \u00b7 = \u03b7tj = \u03b7 = 1\u221aR , L \u2223\u2223\u2223 \u03b8\u03021 = diagk(\u03b6), with diagk(\u00b7) being a diagonal matrix among the k columns of L, p \u2264 \u03b62 \u2264 q, and S \u2223\u2223\u2223 \u03b8\u03021 = 0k\u00d7|T |, for any u \u2208 K our algorithm exhibits a sublinear regret of the form R\u2211 j=1 ltj ( \u03b8\u0302r ) \u2212 ltj (u) = O (\u221a R ) .\nProof. Given the ingredients of the previous section, next we derive the sublinear regret results which finalize the statement of the theorem. First, it is easy to see that\n\u2207\u03b8\u21260 ( \u03b8\u0303j ) \u2212\u2207\u03b8\u21260 ( \u03b8\u0303j+1 ) = \u03b7tj f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j .\nFurther, from strong convexity of the regularizer we obtain:\n\u21260 ( \u03b8\u0302j ) \u2265 \u21260 ( \u03b8\u0302j+1 ) + \u2329 \u2207\u03b8\u21260 ( \u03b8\u0302j+1 ) , \u03b8\u0302j \u2212 \u03b8\u0302j+1 \u232a + 1\n2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b8\u0302j \u2212 \u03b8\u0302j+1\u2223\u2223\u2223\u2223\u2223\u22232 2 .\nIt can be seen that \u2225\u2225\u2225\u03b8\u0302j \u2212 \u03b8\u0302j+1\u2225\u2225\u2225 2 \u2264 \u03b7tj \u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j \u2225\u2225\u2225\u2225 2 .\nFinally, for any u \u2208 K, we have: r\u2211 j=1 \u03b7tj ( ltj ( \u03b8\u0302j ) \u2212 ltj (u) ) \u2264 r\u2211 j=1 [ \u03b7tj (\u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j \u2225\u2225\u2225\u2225 2 )2] + \u21260(u)\u2212\u21260(\u03b8\u03021) .\nAssuming \u03b7t1 = \u00b7 \u00b7 \u00b7 = \u03b7tj = \u03b7, we can derive r\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212 ltj (u) ) \u2264 \u03b7 r\u2211 j=1 (\u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j \u2225\u2225\u2225\u2225 2 )2 + 1/\u03b7 ( \u21260(u)\u2212\u21260(\u03b8\u03021) ) .\nThe following lemma finalizes the statement of the theorem:\nLemma 5. After T rounds and for \u03b7t1 = \u00b7 \u00b7 \u00b7 = \u03b7tj = \u03b7 = 1\u221aR , our algorithm exhibits, for any u \u2208 K, a sublinear regret of the form\nR\u2211 j=1 ltj (\u03b8\u0302j)\u2212 ltj (u) \u2264 O (\u221a R ) .\nProof. It is then easy to see\u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 2 \u2264 \u03b33(R) + 4\u03b321(R)\u03b322(R) with \u03b33(R) = 4\u03b321(R) + 2 max tj\u2208IR\u22121 \u03b42tj\n\u2264 \u03b33(R) + 8 d\np2 \u03b321(R)qd+ 8\nd\np2 \u03b321(R)qd |IR\u22121| max tk\u2208IR\u22121\n{ \u2016A\u2020tk\u20162 (\u2016btk\u20162 + cmax) 2 } .\nSince |IR\u22121| \u2264 |T | with |T | being the total number of tasks available, then we can write\u2225\u2225\u2225\u2225f\u0302tj \u2223\u2223\u2223 \u03b8\u0302r \u2225\u2225\u2225\u22252 2 \u2264 \u03b35(R)|T | ,\nwith \u03b35 = 8d/p2q\u03b321(R)maxtk\u2208IR\u22121 { \u2016A\u2020tk\u2016 2 2 (\u2016btk\u20162 + cmax) 2 }\n. Further, it is easy to see that \u21260(u) \u2264 qd+ \u03b35(R)|T | with \u03b35(R) being a constant, which leads to\nr\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212 ltj (u) ) \u2264 \u03b7 r\u2211 j=1 \u03b35(R)|T |+ 1/\u03b7 ( qd+ \u03b35(R)|T | \u2212\u21260(\u03b8\u03021) ) .\nInitializing L and S: We initialize L \u2223\u2223\u2223 \u03b8\u03021 = diagk(\u03b6), with p \u2264 \u03b62 \u2264 q and S \u2223\u2223\u2223 \u03b8\u03021 = 0k\u00d7|T | ensures the invertability of L and that the constraints are met. This leads us to r\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212 ltj (u) ) \u2264 \u03b7 r\u2211 j=1 \u03b35(R)|T |+ 1/\u03b7 (qd+ \u03b35(R)|T | \u2212 \u00b52k\u03b6) . Choosing \u03b7t1 = \u00b7 \u00b7 \u00b7 = \u03b7tj = \u03b7 = 1/ \u221a R, we acquire sublinear regret, finalizing the statement of the theorem:\nr\u2211 j=1 ( ltj ( \u03b8\u0302j ) \u2212 ltj (u) ) \u2264 1/\u221aR\u03b35(R)|T |R+ \u221a R (qd+ \u03b35(R)|T | \u2212 \u00b52k\u03b6)\n\u2264 \u221a R (\u03b35(R)|T |+ qd\u03b35(R)|T | \u2212 \u00b52k\u03b6) \u2264 O (\u221a R ) ,\nwith \u03b35(R) being a constant."}], "references": [{"title": "Online learning in Markov decision processes with adversarially chosen transition probability distributions", "author": ["Yasin Abbasi-Yadkori", "Peter Bartlett", "Varun Kanade", "Yevgeny Seldin", "Csaba Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2013}, {"title": "Reinforcement learning transfer via sparse coding", "author": ["Haitham Bou Ammar", "Karl Tuyls", "Matthew E. Taylor", "Kurt Driessen", "Gerhard Weiss"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Ammar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2012}, {"title": "Online multi-task learning for policy gradient methods", "author": ["Haitham Bou Ammar", "Eric Eaton", "Paul Ruvolo", "Matthew Taylor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Design and Control of Quadrotors with Application to Autonomous Flying", "author": ["Samir Bouabdallah"], "venue": "PhD Thesis, E\u0301cole polytechnique fe\u0301de\u0301rale de Lausanne,", "citeRegEx": "Bouabdallah.,? \\Q2007\\E", "shortCiteRegEx": "Bouabdallah.", "year": 2007}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximators", "author": ["Lucian Busoniu", "Robert Babuska", "Bart De Schutter", "Damien Ernst"], "venue": null, "citeRegEx": "Busoniu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2010}, {"title": "Sequential transfer in multi-armed bandit with finite set of models", "author": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Azar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2013}, {"title": "Cauchy-Schwarz inequalities associated with positive semidefinite matrices", "author": ["Roger A. Horn", "Roy Mathias"], "venue": "Linear Algebra and its Applications 142:63\u201382,", "citeRegEx": "Horn and Mathias.,? \\Q1990\\E", "shortCiteRegEx": "Horn and Mathias.", "year": 1990}, {"title": "Policy search for motor primitives in robotics", "author": ["Jens Kober", "Jan Peters"], "venue": "Machine Learning,", "citeRegEx": "Kober and Peters.,? \\Q2011\\E", "shortCiteRegEx": "Kober and Peters.", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Kumar and III.,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Transfer in reinforcement learning: a framework and a survey", "author": ["Alessandro Lazaric"], "venue": "Reinforcement Learning: State of the Art. Springer,", "citeRegEx": "Lazaric.,? \\Q2011\\E", "shortCiteRegEx": "Lazaric.", "year": 2011}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Jan Peters", "Stefan Schaal"], "venue": "Neural Networks,", "citeRegEx": "Peters and Schaal.,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal.", "year": 2008}, {"title": "ELLA: An Efficient Lifelong Learning Algorithm", "author": ["Paul Ruvolo", "Eric Eaton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Ruvolo and Eaton.,? \\Q2013\\E", "shortCiteRegEx": "Ruvolo and Eaton.", "year": 2013}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David Mcallester", "Satinder Singh", "Yishay Mansour"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Transfer learning for reinforcement learning domains: a survey", "author": ["Matthew E. Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Discovering structure in multiple learning tasks: the TC algorithm", "author": ["Sebastian Thrun", "Joseph O\u2019Sullivan"], "venue": "In Proceedings of the 13th International Conference on Machine Learning (ICML),", "citeRegEx": "Thrun and O.Sullivan.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and O.Sullivan.", "year": 1996}, {"title": "Learning more from less data: experiments in lifelong learning", "author": ["Sebastian Thrun", "Joseph O\u2019Sullivan"], "venue": "Seminar Digest,", "citeRegEx": "Thrun and O.Sullivan.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and O.Sullivan.", "year": 1996}, {"title": "Nonlinear tracking and landing controller for quadrotor aerial robots", "author": ["Holger Voos", "Haitham Bou Ammar"], "venue": "In Proceedings of the IEEE Multi-Conference on Systems and Control,", "citeRegEx": "Voos and Ammar.,? \\Q2010\\E", "shortCiteRegEx": "Voos and Ammar.", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Multi-task reinforcement learning: a hierarchical Bayesian approach", "author": ["Aaron Wilson", "Alan Fern", "Soumya Ray", "Prasad Tadepalli"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "Flexible latent variable models for multi-task learning", "author": ["Jian Zhang", "Zoubin Ghahramani", "Yiming Yang"], "venue": "Machine Learning,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "Introduction Reinforcement learning (RL) (Busoniu et al., 2010; Sutton & Barto, 1998) often requires substantial experience before achieving acceptable performance on individual control problems.", "startOffset": 41, "endOffset": 85}, {"referenceID": 10, "context": "When data is in limited supply, transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training (Taylor & Stone, 2009; Gheshlaghi Azar et al., 2013; Lazaric, 2011; Ferrante et al., 2008; Bou Ammar et al., 2012).", "startOffset": 161, "endOffset": 275}, {"referenceID": 20, "context": "ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).", "startOffset": 60, "endOffset": 101}, {"referenceID": 21, "context": "ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008).", "startOffset": 60, "endOffset": 101}, {"referenceID": 1, "context": ", 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneProceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008). In the lifelong learning setting (Thrun & O\u2019Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al.", "startOffset": 12, "endOffset": 658}, {"referenceID": 1, "context": ", 2008; Bou Ammar et al., 2012). Multitask learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneProceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). ously and share knowledge during the joint learning process (Wilson et al., 2007; Zhang et al., 2008). In the lifelong learning setting (Thrun & O\u2019Sullivan, 1996a;b), which can be framed as an online MTL problem, agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Recently, based on the work of Ruvolo & Eaton (2013) on supervised lifelong learning, Bou Ammar et al. (2014) developed a lifelong learner for policy gradient RL.", "startOffset": 12, "endOffset": 715}, {"referenceID": 14, "context": "Policy search methods have shown success in solving high-dimensional problems, such as robotic control (Kober & Peters, 2011; Peters & Schaal, 2008a; Sutton et al., 2000).", "startOffset": 103, "endOffset": 170}, {"referenceID": 0, "context": "Given \u03b8\u0303, the constrained solution is then determined by learning a projection into the constraint set via Bregman projections (see Abbasi-Yadkori et al. (2013)).", "startOffset": 132, "endOffset": 161}, {"referenceID": 14, "context": "Typical policy gradient methods (Kober & Peters, 2011; Sutton et al., 2000) maximize a lower bound of the expected cost ltj ( \u03b1tj ) , which can be derived by taking the logarithm and applying Jensen\u2019s inequality:", "startOffset": 32, "endOffset": 75}, {"referenceID": 19, "context": "We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b).", "startOffset": 63, "endOffset": 79}, {"referenceID": 19, "context": "We detail this process for two popular PG learners, eREINFORCE (Williams, 1992) and eNAC (Peters & Schaal, 2008b). The derivations of the update rules below can be found in Appendix A. These updates are governed by learning rates \u03b2 and \u03bb that decay over time; \u03b2 and \u03bb can be chosen using line-search methods as discussed by Boyd & Vandenberghe (2004). In our experiments, we adopt a simple yet effective strategy, where \u03b2 = cj\u22121 and \u03bb = cj\u22121, with 0 < c < 1.", "startOffset": 64, "endOffset": 351}, {"referenceID": 0, "context": "Using results developed by Abbasi-Yadkori et al. (2013), it is easy to see that \u2207\u03b8\u03a90 ( \u03b8\u0303j ) \u2212\u2207\u03b8\u03a90 ( \u03b8\u0303j+1 ) = \u03b7tj f\u0302tj \u2223\u2223\u2223 \u03b8\u0302j .", "startOffset": 27, "endOffset": 56}, {"referenceID": 5, "context": "Cart Pole: The cart-pole (CP) has been used extensively as a benchmark for evaluating RL methods (Busoniu et al., 2010).", "startOffset": 97, "endOffset": 119}, {"referenceID": 1, "context": ", eNAC) and PG-ELLA (Bou Ammar et al., 2014), examining both the constrained and unconstrained variants of our algorithm. We also varied the number of iterations in our alternating optimization from 10 to 100 to evaluate the effect of these inner iterations on the performance, as shown in Figures 2 and 3. For the two MTL algorithms (our approach and PG-ELLA), the policy parameters for each task tj were initialized using the learned basis (i.e., \u03b1tj = Lstj ). We configured PG-ELLA as described by Bou Ammar et al. (2014), ensuring a fair comparison.", "startOffset": 25, "endOffset": 525}, {"referenceID": 3, "context": "To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors.", "startOffset": 71, "endOffset": 114}, {"referenceID": 3, "context": "To ensure realistic dynamics, we used the simulated model described by (Bouabdallah, 2007; Voos & Bou Ammar, 2010), which has been verified and used in the control of physical quadrotors. We generated 10 different quadrotor systems by varying the inertia around the x, y and z-axes. We used a linear quadratic regulator, as described by Bouabdallah (2007), to initialize the policies in both the learning and testing phases.", "startOffset": 72, "endOffset": 356}], "year": 2015, "abstractText": "Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.", "creator": "LaTeX with hyperref package"}}}