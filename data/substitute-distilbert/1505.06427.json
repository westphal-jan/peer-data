{"id": "1505.06427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2015", "title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification", "abstract": "recent researcher shows that descriptive neural networks ( ar ) easily be used to extract deep speaker vectors ( d - vectors ) that preserve speaker characteristics and can be used in speaker searches. this new method has been tested on text - dependent speaker registration tasks, and improvement was reported when combined with the conventional i - vector method.", "histories": [["v1", "Sun, 24 May 2015 11:22:40 GMT  (1253kb,D)", "http://arxiv.org/abs/1505.06427v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["lantian li", "dong wang", "zhiyong zhang", "thomas fang zheng"], "accepted": false, "id": "1505.06427"}, "pdf": {"name": "1505.06427.pdf", "metadata": {"source": "CRF", "title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification", "authors": ["Lantian Li", "Dong Wang", "Zhiyong Zhang", "Thomas Fang Zheng"], "emails": ["fzheng@tsinghua.edu.cn)"], "sections": [{"heading": null, "text": "This paper extends the d-vector approach to semi textindependent speaker verification tasks, i.e., the text of the speech is in a limited set of short phrases. We explore various settings of the DNN structure used for d-vector extraction, and present a phone-dependent training which employs the posterior features obtained from an ASR system. The experimental results show that it is possible to apply d-vectors on semi text-independent speaker recognition, and the phone-dependent training improves system performance.\nIndex Terms\u2014deep neural networks, speaker vector, speaker verification\nI. INTRODUCTION\nSPEAKER verification, also known as voiceprint verifica-tion, is an important biometric authentication technique that has been widely used to verify speakers\u2019 identities. According to the text that are allowed to speak in enrollment and test, speaker verification systems can be categorized into either text-dependent or text-independent. While a textdependent system requires the same words/sentences to be spoken in enrollment and test, a text-independent system permits any words to speak. This paper focuses on a semi text-independent scenario where the words for enrollment and test are constrained in a limited set of short phrases, e.g., \u2018turn on the radio\u2019. With this limitation, people can speak different sentences in enrollment and test while the system performance is not significantly deteriorated, which makes the system more acceptable in practice.\nMost of the successful approaches to speaker verification are based on generative models and with unsupervised learning, e.g., the famous Gaussian mixture model-universal background model (GMM-UBM) framework [1]. A number of advanced models have been proposed based on the GMM-UBM architecture, among which the i-vector model [2] [3] is perhaps the most successful. Despite the impressive success, the GMMUBM model and the subsequent i-vector model share the intrinsic disadvantage of all unsupervised learning methods:\nThis work was supported by the National Natural Science Foundation of China under Grant No. 61371136 and No. 61271389, it was also supported by the National Basic Research Program (973 Program) of China under Grant No. 2013CB329302. The authors are with Division of Technical Innovation and Development of Tsinghua National Laboratory for Information Science and Technology and Research Institute of Information Technology (RIIT) of Tsinghua University. This paper is also supported by Sinovoice and Pachira. (Corresponding e-mail: fzheng@tsinghua.edu.cn)\nthe goal of the model training is to describe the distributions of the acoustic features, instead of discriminating speakers.\nThis problem can be solved in two directions. The first direction is to employ various discriminative models to enhance the generative framework. For example, the SVM model for GMM-UBMs [4], and the PLDA model for i-vectors [5]. All these approaches provide significant improvement over the baseline. Another direction is to look for more discriminative features, i.e., the features that are more sensitive to speaker change and largely invariant to change of other irrelevant factors, such as phone contents and channels [6]. However, the improvement obtained by the \u2018feature engineering\u2019 is much less significant compared to the achievements obtained by the discriminative models such as SVM and PLDA. A possible reason is that most of the features are human-crafted and thus tend to be suboptimal in practical usage.\nRecent research on deep learning offers a new idea of \u2018feature learning\u2019. It has been shown that with a deep neural network (DNN), task-oriented features can be learned layer by layer from very raw features. For example in automatic speech recognition (ASR), phone-discriminative features can be learned from spectrum or filter bank energies (Fbanks). The learned features are very powerful and have defeated the conventional feature based on Mel frequency cepstral coefficients (MFCCs) that has dominated in ASR for several decades [7].\nThis favorable property of DNNs in learning task-oriented features can be utilized to learn speaker-discriminative features as well. A recent study shows that this is possible at least in text-dependent tasks [8]. The authors constructed a DNN model and set the training objective as to discriminate a set of speakers, and for each frame, the speaker-discriminative features were read from the activations of the last hidden layer. They tested the method on a foot-print text-dependent speaker verification task (only a short phrase \u2018ok, google\u2019). The experimental results showed that reasonable performance can be achieved with the DNN-based features, although it is still difficult to compete the i-vector baseline.\nIn this paper, we extend the application of the DNN-based feature learning approach to semi text-independent tasks, and present a phone-dependent training which involves phone posteriors obtained from an ASR system in the training. The experimental results show that the DNN-based feature learning works well on text-independent tasks, actually even better than on text-dependent tasks, and the phone-dependent training offers marginal but consistent gains.\nThe rest of this paper is organized as follows. Section II describes the related work, and Section III presents the DNN-\nar X\niv :1\n50 5.\n06 42\n7v 1\n[ cs\n.C L\n] 2\n4 M\nay 2\n01 5\nbased speaker feature learning. The experiments are presented in Section IV, and Section V concludes the paper."}, {"heading": "II. RELATED WORK", "text": "This paper follows the work in [8]. The difference is that we extend the application of the DNN-based feature learning approach to semi text-independent tasks, and we introduce a phone-dependent training. Due to the mismatched content of the enrollment and test speech, our task is more challenging.\nThe DNN model has been employed in speaker verification in other ways. For example, in [9], DNNs trained for ASR were used to replace the UBM model to derive the acoustic statistics for i-vector model training. In [10], a DNN was used to replace PLDA to improve discriminative capability of ivectors. All these methods rely on the generative framework, i.e., the i-vector model. The DNN-based feature learning presented in this paper is purely discriminative, without any generative model involved."}, {"heading": "III. DNN-BASED FEATURE LEARNING", "text": "This section presents the DNN-based feature learning. We first describe the main structure of the model and the learning process, and propose the phone-dependent learning. Finally the difference between the i-vector approach and the DNN-based approach is discussed."}, {"heading": "A. DNN-based feature extraction", "text": "It is well-known that DNNs can learn task-oriented features from raw features layer by layer. This property has been employed in ASR where phone-discriminative features are learned from very low-level features such as Fbanks or even spectrum [7]. It has been shown that with a well-trained DNN, variations irrelevant to the learning task are gradually eliminated when the input feature is propagated through the DNN structure layer by layer. This feature learning is so powerful that in ASR, the primary Fbank feature has defeated the MFCC feature that was carefully designed by people and has dominated in ASR for several decades.\nThis property can be also employed to learn speakerdiscriminative features. Actually researchers have put much effort in looking for features that are more discriminative for speakers [6], but the effort is mostly vain and the MFCC is still the most popular choice. The success of DNNs in ASR suggests a new direction that speaker-discriminative features can be learned from data instead of crafted by hand. The learning can be easily done and the process is rather similar as in ASR, with the only difference that in speaker verification, the learning goal is to discriminate different speakers.\nFig. 1 presents the DNN structure used for the speakerdiscriminative feature learning. Following the convention of ASR, the input layer involves a window of 40-dimensional Fbanks. In this work, the window size is set to 21, which was found to be optimal in our work. There are 4 hidden layers, and each consists of 200 units. The units of the output layer correspond to the speakers in the training data, and the number is 80 in our experiment. The 1-hot encoding scheme is used to label the target, and the training criterion is set to cross\nentropy. The learning rate is set to 0.008 at the beginning, and is halved whenever no improvement on a cross-validation (CV) set is found. The training process stops when the learning rate is too small and the improvement on the CV set is too marginal.\nOnce the DNN has been trained successfully, the speakerdiscriminative features can be read from any hidden layer. More the layer is close to the output, more the features are speaker-discriminative. Our experiments show that features extracted from the last hidden layer perform the best, which is similar to the observation in [8].\nIn the test phase, the features are extracted for all the frames of the given utterance, and the features are averaged to form a speaker vector. Following the nomenclature in [8], we call this speaker vector as \u2018d-vector\u2019. Similar to i-vectors, a d-vector represents the speaker identity of an utterance in the speaker space. The same methods used for i-vectors can be used for d-vectors to conduct the test, for example by computing the cosine distance or applying PLDA."}, {"heading": "B. Phone-dependent training", "text": "A potential problem of the DNN-based speakerdiscriminative feature learning described in the previous section is that it is a \u2018blind learning\u2019, i.e., the features are learned from raw data without any prior information. This means that the learning purely relies on the complex deep structure of the DNN model and a large amount of data to discover speaker-discriminative patterns. If the training data is abundant, this is often not a problem; however in tasks with a limited amount of data, for instance the semi text-independent task in our hand, this blind learning tends to be difficult because there are too many speaker-irrelevant variations involved in the raw data, particularly phone contents.\nA possible solution is to inform the DNN which phone the current input frame belongs to. This can be simply achieved by adding a phone indicator in the DNN input. However, it is often not easy to get the phone alignment for the speech data. An alternative to the phone indicator is a vector of phone posterior probabilities, which can be easily obtained from any phone discriminant model. In this work, we choose a DNN model that was trained for an ASR system to produce the phone posteriors. Fig. 2 illustrates the DNN structure with the phone posterior vector involved in the input. The training process for the new structure does not change.\nWe note that this phone-dependent training is more important for text-independent recognition. For the text-dependent recognition, the acoustic features are limited in a small set of phones, and so involving the phone information in the training does not help much."}, {"heading": "C. Comparison between i-vectors and d-vectors", "text": "The two kinds of speaker vectors, the d-vector and the ivector, are fundamentally different. I-vectors are based on a linear Gaussian model, for which the learning is unsupervised and the learning criterion is maximum likelihood on acoustic features. In contrast, d-vectors are based on neural networks, for which the learning is supervised, and the learning criterion is maximum discrimination for speakers. This difference in model structures and learning methods leads to significant different properties of these two vectors.\nFirst, the i-vector is \u2018descriptive\u2019, which represents the speaker by constructing a GMM (derived from the i-vector) to fit the acoustic features. In contrast, the d-vector is \u2018discriminative\u2019, which represents the speaker by removing speakerirrelevant variance.\nSecond, the i-vector can be regarded as a \u2018global\u2019 speaker description, which is inferred from \u2018all\u2019 the frames of an utterance; however the d-vector is a \u2018local\u2019 description, which is inferred from \u2018each\u2019 frame, and only the context information is used in the inference. This means that the d-vector tends to be more superior with a short utterance, while the i-vector tends to perform better with a relative long utterance.\nThird, the i-vector approach more relies on the enrollment data to form a reasonable distribution that can be used to discriminate different speakers; whereas the d-vector approach more relies on the \u2018universal\u2019 data to learn speakerdiscriminative features. This means that a large amount of training data (labelled with speakers) is much more important and useful for the d-vector approach."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Database", "text": "The experiments are performed on a short phrase speech database provided by Pachira. The entire database contains recordings of 10 short phrases from 100 speakers (gender balanced), and each phrase contains 2 \u223c 5 Chinese characters. For each speaker, every phrase is recorded 15 times, amounting to 150 utterances per speaker.\nThe training set involves 80 randomly selected speakers, which results in 12000 utterances in total. To prevent overfitting, a cross-validation (CV) set containing 1000 utterances is selected from the training data, and the remaining 11000 utterances are used for model training, including the DNN model in the d-vector approach, and the UBM, the T matrix, the LDA and PLDA model in the i-vector approach.\nThe evaluation set consists of the remaining 20 speakers. In the text-dependent experiment, the evaluation is performed for each particular phrase; and in the semi text-independent experiment, all the utterances in the evaluation set (3000 in total) are cross evaluated, resulting in 223500 target trials and 4275000 non-target trials."}, {"heading": "B. Text-dependent recognition", "text": "The first experiment investigates the performance of the dvector approach on text-dependent speaker verification tasks, and compare it to the i-vector baseline. A similar work has been reported in [8], here we just reproduce that work and propose some improvement by leveraging text-independent data.\nFor clearance, we report the results on two randomly selected phrases, denoted by P1 and P2 respectively. For each phrase, the corresponding utterances are selected from the training set to train the i-vector system and the d-vector system respectively, and the corresponding utterances in the evaluation set are selected to perform the test. This means that the training data for each phrase consists of 1200 utterances, and the test consists of 300 utterances. For the i-vector system, the number of Gaussian mixtures of the UBM is 64, and the i-vector dimension is 200. These values have been chosen to optimize the performance. The DNN architecture for the d-vector system has been shown in Section III. For a fair comparison, the dimension of the d-vector is set to 200 as well.\nThe tests are based on three scoring methods: the basic cosine distance, the cosine distance after reducing the dimension to 80 by LDA, and the score provided by PLDA. Table I reports the results in terms of equal error rate (EER). It can be seen that the d-vector system obtains reasonable performance, however, the results are much worse than those with the ivector system. Similar observations have been reported in [8].\nAs discussed in Section III, the DNN model of the dvector system can be enhanced by borrowing data from textindependent tasks. The results are reported in Table II. It can be observed that with more training data, the performance of d-vector systems is generally improved, despite that the extra data are recordings of other phrases. Another observation is that with more training data, the PLDA model tends to be less\neffective. This can be possibly explained by the fact that dvectors are derived from activations of neural network units and so probably do not fit the linear Gaussian model that PLDA assumes."}, {"heading": "C. Semi text-independent recognition", "text": "This experiment examines the d-vector approach on the semi text-independent task. The dimension of both i-vectors and d-vectors is fixed to 200, and the dimension of the LDAprojected vectors is 80. In order to have the two systems involve the same amount of parameters, the number of Gaussian components of the i-vector system is set to 128. All the utterances in the training dataset are used to train the DNN model and the i-vector model.\nThe results of the two systems are reported in Table III. It can be observed that with the simple cosine distance, the d-vector system outperforms the i-vector system in a significant way. This demonstrates that the discriminatively learned d-vectors are more discriminative for speakers when compared with the generatively learned i-vectors. However, when the discriminative normalization methods (LDA and PLDA) are employed, the performance of the i-vector system is significantly improved and better than that of the d-vector system. The discriminative methods contribute very little to the d-vector system. This is not supervising, as the d-vectors have been discriminative already.\nNevertheless, the slight improvement with LDA suggests that there is some redundancy in d-vectors. Motivated by this idea, a new hidden layer with a small number of units is inserted into the DNN structure, as shown in Fig. 3. The dimension of the new layer is set to 100, which is the best choice in our test. Compared to LDA, this approach can be regarded as a non-linear dimension reduction (NLDR). Additional performance is achieved with this method, as has been shown in the last column of Table III."}, {"heading": "D. Phone-dependent training", "text": "In this experiment, the phone posteriors are included in the input of the DNN structure, as shown in Fig. 2. The phone posteriors are produced by a DNN model that was trained for ASR with a Chinese database consisting of 6000 hours of speech data. The phone set consists of 66 toneless initial and finals in Chinese, plus the silence phone. The results are shown in the third row of Table III. It can be seen that the phone-dependent training leads to marginal but consistent performance improvement for the d-vector system. The NLDR approach is also applied, and an additional gain is obtained."}, {"heading": "E. Combination system", "text": "Following [8], we combine the best i-vector system (PLDA) and the best d-vector system (NLDR with phone-dependent training). The combination is simply done by interpolating the scores obtained from the two systems. The EER results with various values of the interpolation factor (denoted by \u03b1) are drawn in Fig. 4. It can be seen that the combination leads to the better performance with an appropriate set of \u03b1. The best EER is 7.14%, which is the lowest EER we can obtain with this dataset so far."}, {"heading": "V. CONCLUSIONS", "text": "This paper investigated DNN-based discriminative feature leaning for speaker recognition, and studied the performance of this approach on a semi text-independent task. The experimental results demonstrated that the DNN-based approach can offer reasonable performance, and outperformed the ivector baseline with simple cosine distance. However, when discriminative normalization methods such as LDA and PLDA are applied, the i-vector approach exhibits better performance.\nAlthough it has not beat the i-vector approach at present, the d-vector approach is very promising and potentially can be improved in several ways. Particularly, a powerful probabilistic model on d-vectors would deal with inter-frame uncertainty and so may considerably enhance system performance. We leave this as the future work."}], "references": [{"title": "Speaker verification using adapted gaussian mixture models", "author": ["D. Reynolds", "T. Quatieri", "R. Dunn"], "venue": "Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Speaker and session variability in gmm-based speaker verification", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1448\u20131460, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Support vector machines using gmm supervectors for speaker verification", "author": ["W. Campbell", "D. Sturim", "D. Reynolds"], "venue": "Signal Processing Letters, IEEE, vol. 13, no. 5, pp. 308\u2013311, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic linear discriminant analysis", "author": ["S. Ioffe"], "venue": "Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "An overview of text-independent speaker recognition: From features to supervectors", "author": ["T. Kinnunen", "H. Li"], "venue": "Speech communication, vol. 52, no. 1, pp. 12\u201340, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving wideband speech recognition using mixed-bandwidth training data in cd-dnn-hmm", "author": ["J. Li", "D. Yu", "J. Huang", "Y. Gong"], "venue": "SLT, pp. 131\u2013136, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "M. Erik", "L.M. Ignacio", "G.-D. Javier"], "venue": "IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP), vol. 28, no. 4, pp. 357\u2013366, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Odyssey, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative scoring for speaker recognition based on i-vectors", "author": ["J. Wang", "D. Wang", "Z.-W. Zhu", "T. Zheng", "F. Song"], "venue": "APSIPA, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": ", the famous Gaussian mixture model-universal background model (GMM-UBM) framework [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "A number of advanced models have been proposed based on the GMM-UBM architecture, among which the i-vector model [2] [3] is perhaps the most successful.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "A number of advanced models have been proposed based on the GMM-UBM architecture, among which the i-vector model [2] [3] is perhaps the most successful.", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "For example, the SVM model for GMM-UBMs [4], and the PLDA model for i-vectors [5].", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "For example, the SVM model for GMM-UBMs [4], and the PLDA model for i-vectors [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": ", the features that are more sensitive to speaker change and largely invariant to change of other irrelevant factors, such as phone contents and channels [6].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "The learned features are very powerful and have defeated the conventional feature based on Mel frequency cepstral coefficients (MFCCs) that has dominated in ASR for several decades [7].", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "A recent study shows that this is possible at least in text-dependent tasks [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "This paper follows the work in [8].", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "For example, in [9], DNNs trained for ASR were used to replace the UBM model to derive the acoustic statistics for i-vector model training.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "In [10], a DNN was used to replace PLDA to improve discriminative capability of ivectors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "This property has been employed in ASR where phone-discriminative features are learned from very low-level features such as Fbanks or even spectrum [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "Actually researchers have put much effort in looking for features that are more discriminative for speakers [6], but the effort is mostly vain and the MFCC is still the most popular choice.", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "Our experiments show that features extracted from the last hidden layer perform the best, which is similar to the observation in [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Following the nomenclature in [8], we call this speaker vector as \u2018d-vector\u2019.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "A similar work has been reported in [8], here we just reproduce that work and propose some improvement by leveraging text-independent data.", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "Similar observations have been reported in [8].", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "Following [8], we combine the best i-vector system (PLDA) and the best d-vector system (NLDR with phone-dependent training).", "startOffset": 10, "endOffset": 13}], "year": 2015, "abstractText": "Recent research shows that deep neural networks (DNNs) can be used to extract deep speaker vectors (d-vectors) that preserve speaker characteristics and can be used in speaker verification. This new method has been tested on text-dependent speaker verification tasks, and improvement was reported when combined with the conventional i-vector method. This paper extends the d-vector approach to semi textindependent speaker verification tasks, i.e., the text of the speech is in a limited set of short phrases. We explore various settings of the DNN structure used for d-vector extraction, and present a phone-dependent training which employs the posterior features obtained from an ASR system. The experimental results show that it is possible to apply d-vectors on semi text-independent speaker recognition, and the phone-dependent training improves system performance.", "creator": "LaTeX with hyperref package"}}}