{"id": "1505.06907", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2015", "title": "Using Dimension Reduction to Improve the Classification of High-dimensional Data", "abstract": "in this series we show that the classification function of high - dimensional structural mri data with only a small section of training examples is simplified by the usage of dimensions reduction methods. we assessed two different dimension reduction variants : feature selection by geometric f - model and feature transformation by pca. on the reduced datasets, researches applied common approach algorithms using 5 - fold cross - validation. training, inspection of the hyperparameters, as well as the performance evaluation of the structures was conducted using two different performance measures : accuracy, and receiver operating characteristic curve ( auc ). different hypothesis is supported by experimental results.", "histories": [["v1", "Tue, 26 May 2015 11:33:04 GMT  (189kb,D)", "http://arxiv.org/abs/1505.06907v1", "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)"]], "COMMENTS": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["andreas gr\\\"unauer", "markus vincze"], "accepted": false, "id": "1505.06907"}, "pdf": {"name": "1505.06907.pdf", "metadata": {"source": "CRF", "title": "Using Dimension Reduction to Improve the Classification of High-dimensional Data", "authors": ["Andreas Gr\u00fcnauer", "Markus Vincze"], "emails": ["vincze}@acin.tuwien.ac.at"], "sections": [{"heading": "1 Introduction", "text": "Machine learning algorithms are used in various fields for learning patterns from data and make predictions on unseen data. Unfortunately, the probability of overfitting of a learning algorithm increases with the number of features [14]. Dimension reduction methods are not only powerful tools to avoid overfitting [10], but also capable of making the training of high-dimensional data a computationally more feasible task. In this work, we want to study the influence of dimension reduction techniques on the performance of various well-known classification methods. Dimension reduction methods are categorized into two groups: feature selection and feature transformation.\nFeature selection methods [9, 6] aim to identify a subset of \u201cmeaning-ful\u201d features out of the original set of features. They can be subdivided into filter, wrapper and embedded methods. Filter methods compute a score for each feature, and then select only the features that have the best scores. Wrapper methods train a predictive model on subsets of features, before the subset with the best score is selected. The search for subsets can be done either in a deterministic (e.g. forward selection, backward elimination) or random (i.e. genetic algorithms) way. Embedded methods determine the optimal subset of features directly by the trained weights of the classification method.\nIn contrast to feature selection methods, feature transformation methods project the original high dimensional data into a lower dimensional space. Principal Component Analysis (PCA) is one of the most-known techniques in this category. PCA finds the principal axis in the dataset that explain most of the variance, without considering the class labels. Therefore we use PCA as the baseline for dimension reduction methods in this study.\nAmong various feature selection methods, we limit our scope on filter methods, as they do not depend on a specific classification method and therefore are suitable for the comparison of different classifiers [9]. Comparative studies on text classification have revealed, that univariate statistical tests like the \u03c72-test\nar X\niv :1\n50 5.\n06 90\n7v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 5\nand ANOVA F-test are among the most effective scores for filtered feature selection [15]. As the \u03c72-test is only applicable on categorical data, we use the filter selection method based on the ANOVA F-test which is applicable on continuous features used for the evaluation of this study.\nAs part of the 17th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), the MICCAI 2014 Machine Learning Challenge (MLC) aims for the objective comparison of the latest machine learning algorithms applied on Structural MRI data [13]. The subtask of binary classification of clinical phenotypes is in particular challenging, since in the opinion of the challenge authors, a prediction accuracy of 0.6 is acceptable. Motivated by this challenge, the goal of this study is to show, that the selected dimension reduction methods improve the performances of various classifiers trained on a small set of high-dimensional Structural MRI data.\nThe report is organized as follows. Section 2 describes the dimension reduction methods. Section 3 describes the classifiers. Section 4 describes the datasets, evaluation measures and the experiment methodology. Section 5 presents the results. Section 6 discusses the major findings. Section 7 summarizes the conclusion."}, {"heading": "2 Dimension reduction", "text": "In the following we will give an overview of the used techniques for dimension reduction.\nFiltered feature selection by ANOVA F-test Feature selection methods based on filtering determine the relevance of features by calculating a score (usually based on a statistical measure or test). Given a number of selected features s, only the s top-scoring features are afterwards forwarded to the classification algorithm. In this study, we use the ANOVA F-Test statistic [8] for the feature scoring. The F-test score assesses, if the expected values of a quantitative random variable x within a number of m predefined groups differ from each other. The F-value is defined as\nF = MSB MSW ,\nMSB reflects the \u201cbetween-group variability\u201d, expressed as MSB = \u2211 i ni(x\u0304i \u2212 x\u0304)2\nm\u2212 1 ,\nwhere ni is the number of observations in the i-th group, x\u0304i denotes the sample mean in the i-th group, and x\u0304 denotes the overall mean of the data. MSW refers to the \u201cwithin-group variability\u2019, defined as\nMSW = \u2211 ij(xij \u2212 x\u0304i)2\nn\u2212m ,\nwhere xij denotes the j-th observation in the i-th group. For the binary classification problem assessed in this report, the number of groups m = 2.\nFeature transformation by PCA PCA [7] reduces the dimension of the data by finding the first s orthogonal linear combinations of the original variables with the largest variance. PCA is defined in such a way that the first principal component has the largest possible variance. Each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components."}, {"heading": "3 Classifiers", "text": "In the following we give a short description of each used classification methods in this study.\nk-Nearest Neighbors (k-NN) The k-NN classifier [2, p.125] does not train a specific model, but stores a labeled training set as \u201creference set\u201d. The classification of a sample is then determined by the class with the most representatives among the k nearest neighbors of the sample in the reference set. Odd values of k prevent tie votes. Among other possible metrics, we use the Euclidean distance metric for this study.\nGaussian Naive Bayes (GNB) Bayes classifiers are based on the Bayes\u2019 theorem and depend on naive (strong) independence assumptions [16, 4]. Using Bayes\u2019 theorem, the probability P (\u03c9j | x) of some class \u03c9w given a d-dimensional random feature vector x \u2208 Rd can be expressed as the equation:\nP (\u03c9j | x) = P (\u03c9j)p(x | \u03c9j)\np(x) ,\nwhere P (\u03c9j) denotes the prior probability of the j-th class \u03c9j , p(x | \u03c9j) refers to the class conditional probability density function for x given class \u03c9j , and p(x) is the evidence factor used for scaling, which in the case of two classes is defined as\np(x) = 2\u2211\nj=2 p(x | \u03c9j)P (\u03c9j).\nUnder the naive assumption that all the individual components xi, i = 1, . . . , d of x are conditionally independent given the class, p(x | \u03c9j) can be decomposed into the product p(x1 | \u03c9j) . . . p(xd | \u03c9j). Therefore we can rearrange P (\u03c9j | x) as\nP (\u03c9j | x) = P (\u03c9j) \u220fd i=1 p(xi | \u03c9j) p(x) .\nSince p(x) is constant for a given input under the Bayes\u2019 rule, the naive Bayes classifier predicts the class \u03c9k that maximizes the following function:\n\u03c9k = arg max j P (\u03c9j) d\u220f i=1 p(xi | \u03c9j)\nUnder the typical assumption that continuous values associated with each class are Gaussian distributed, the probability density of a component xi given a class \u03c9j can be expressed as\np(xi | \u03c9j) = 1\u221a\n2\u03c0\u03c32ij exp\n( \u2212 (xi \u2212 \u00b5ij) 2\n2\u03c0\u03c32ij\n) ,\nwhere \u00b5ij denotes the class conditional mean and \u03c32ij the class conditional variance. The corresponding classifier is called Gaussian Naive Bayes.\nLinear Discriminant Analysis (LDA) Given a two class problem, the LDA algorithm separates the projected mean of two classes maximally by a defined separating hyperplane, while minimizing the variance within each class [4, p. 117\u2013124]. LDA is based on the assumption that both classes are normally distributed and share the same covariance matrix.\nRidge The Ridge classifier is based on Ridge Regression, which extends the Ordinary Least Squares (OLS) method with an additional penalty term to limit the L2-norm of the weight vector [12]. This penalty term shrinks the weights to prevent overfitting.\nSupport Vector Machine (SVM) A support vector machine [2, p. 325] solves the classification of a dataset by constructing a hyperplane in a high or infinite dimensional space in such a way that the distance of the nearest points of the training data to the hyperplane is maximized. The idea of the large margin is to ensure, that samples which are not exactly equal to the training data can still be classified in a reliable way. To prevent overfitting by permitting some degree of misclassifications, a cost parameter C controls the trade off between allowing training errors and forcing rigid margins. Increasing the value of C increases the cost of misclassifying points and forces the creation of a more accurate model that may not generalize well. For our experiment we use a SVM classifier with linear kernel (SVM-L) as well as a SVM classifier with a non-linear kernel using radial basis functions (SVM-RBF).\nRandom Forests (RF) Bagging predictors [3] generate multiple versions of a predictor (in this case decision trees) which are used to get an aggregated predictor. By generating a set of trees in randomly selected subspaces of the feature space [1], the different trees generalize their classification in complementary ways."}, {"heading": "4 Experiment settings", "text": "In the following section we illustrate the conducted experiments in detail. In this study we used the machine learning library scikit-learn 1 version 0.14.1 for all proposed methods and scoring measures in this study. This open source Python library provides a wide variety of machine learning algorithms based on a consistent interface, which eases the comparison of different methods for a given task."}, {"heading": "4.1 Dataset", "text": "In our experiments, we used the dataset for the binary classification task of the MLC 2014 [13]. This dataset consists of 250 T1-weighted structural brain MRI scans: 150 scans including the target class labels for training and additional 100 samples with unknown class labels, reserved for the challenge submission. For each scan, a number of 184 morphological summary features are provided. These features represent volumes of cortical and sub-cortical structures, as well as average thickness measurements within cortical regions. The volume measures have been normalized with the intracranial volume (ICV) to account for different head sizes. All features have been extracted using the brain MRI software FreeSurfer [5]."}, {"heading": "4.2 Evaluation measures", "text": "As recommended by the MLC 2014 challenge [13], we used two common performance measures: Accuracy and the area under the Receiver Operating Characteristic curve (AUC). Both compare the predictions of the classifier with the groundtruth provided in the training data.\n1http://scikit-learn.org\nAccuracy The accuracy is defined as follows:\naccuracy = tp+ tn tp+ fp+ tn+ fn,\nwhere where tp, tn, fp, fn present the number of true positives, true negatives, false positives and false negatives, respectively.\nArea under the ROC curve (AUC) The ROC curve presents the tradeoff between the true positive rate (TPR), expressed as\nTPR = tptp + fn ,\nand the false positive rate (FPR), defined as\nFPR = fpfp + tn .\nGiven a two class problem, a ROC curve can be plotted by varying the probability threshold for predicting positive examples in an interval between zero and one. Informally, one point in ROC space is better than another if it is to the northwest (tp rate is higher, fp rate is lower, or both) of the first. Hence the curve visualizes, for what region a model is more superior compared to another. The area under the ROC curve (AUC) maps this relation to a single value."}, {"heading": "4.3 5-Fold cross-validation (CV)", "text": "We used 5-fold CV by randomly splitting the training dataset (D) of 150 samples into five mutually exclusive subsets (D1, D2, D3, D4, D5) of approximately equal size. Each classification model was trained and tested five times, where each time (t \u2208 {1, 2, 3, 4, 5}), it was trained on all except one fold (D \\Dt) and tested on the remaining fold (Dt). The accuracy and AUC measures were averaged over the particular measures of the five individual test folds."}, {"heading": "4.4 Experiment Methodology", "text": "Our experiments were conducted in the following way. We applied each dimension reduction method on the original training set with a different number of s \u2208 {3, 6, 12, 24, 48, 92, 184} selected components. We trained the classifiers on the 150 datasets with known target class labels using 5-fold CV in two ways: the first by optimizing the accuracy measure and the second by optimizing the AUC measure. For classifiers based on a set of specific hyperparameters, we performed a grid search to find the optimal configuration of hyperparameters. As an exhaustive search over all possible hyperparameters would be an unfeasible task, we limited our scope on a subset of hyperparameters for each classifier with a discrete set of tested values. Table 1 shows the selected hyperparameters and the corresponding set of values for each classifier."}, {"heading": "5 Results", "text": "Fig. 1 shows the performance of the classifiers on the basis of ANOVA F-test feature selection using accuracy (Fig. 1a) and AUC (Fig. 1b) for hyperparametertuning and performance evaluation, respectively. Both figures reveal that, with\ns = 12 selected features, the classifiers achieve already the same or better performances than using the original s = 184 features. When the number of selected features is further increased, the performance of the RBF-SVM peaks at s = 92, while the performances of the other classifiers do not improve or rather decline. This observation shows the importance of feature selection, as more features do not necessarily lead to better performance (overfitting).\nFig. 2 displays the classifier performance on the basis of PCA-reduced data using accuracy (Fig. 2a) and AUC (Fig. 2b) for hyperparameter-tuning and performance evaluation, respectively. Both figures show that the performances of SVM-RBF and KNN are both independent from the amount of used components, with the difference that SVM-RBF outperforms the other classifiers, while KNN exhibits a constantly weak performance over all used components. The other classifiers perform already better on the first s = 3 components of the PCA, than on the original features. When the number of used components s is further increased, the classifiers show a common performance breakdown at s = 12. Increasing the number of components s leads to the performance maximum at a number of s = 24 components.\nIn this study we additionally observe that the GNB classifier performs better than the LDA classifier for a number of s > 12 selected components, although both methods share the assumption that the random variables are independent from each other and normally distributed. The key difference is that the LDA method additionally considers the covariance of the dimensions. When the num-\nber of samples is lower than the number of dimensions, like it is the case in this study, the accurate estimation of the covariance matrix can not be guaranteed. This phenomenon is known as the \u201csmall sample size\u201d problem [11]. This observation suggets that, due to its simpler assumptions, GNB is a more robust classification method than LDA, given a small sized training set."}, {"heading": "6 Discussion", "text": "The performances of the majority of investigated classifiers converge consistently at the same number of s selected features, independent of the measure used for the tuning of hyperparameters. This indicates that the search for the optimal number of selected features is a robust way to improve the performance of classifiers given high dimensional data. The results confirm that the RBFSVM classifier outperforms the other classifiers independent from the number of reduced features. But the results also show that linear classifiers like GNB and Ridge are able to produce equal or even better results on reduced dimensions using the chosen feature selection methods than the RBF-SVM classifier."}, {"heading": "7 Conclusion", "text": "The performances of classifiers under various scores for hyperparameter tuning combined with different dimension-reduction methods are analyzed. Both dimension reductions improved the performance of all classifiers in comparison to the original high-dimensional data. The results indicated that ANOVA F-Test feature selection yielded the better results compared to the PCA-based feature transformation."}, {"heading": "Acknowledgments", "text": "The research leading to these results has received funding from the European Community, Horizon 2020 Programme (H2020-ICT-2014-1), under grant agreement No. 641474, FLOBOT. This work was supported by the Pattern Recognition and Image Processing Group at TU Wien. I thank Yll Haximusa and Roxane Licandro for their thoughtful comments and proofreading."}], "references": [{"title": "Shape quantization and recognition with randomized trees", "author": ["Y Amit", "D Geman"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher Bishop"], "venue": "Springer, 1st ed", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Pattern Classification and Scene Analysis: Pattern Classification. Wiley", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "An introduction to variable and feature selection", "author": ["I Guyon", "A Elisseeff"], "venue": "Journal of Machine Learning Research: Special Issue on Variable and Feature Selection, 3:1157\u20131182", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Principal Component Analysis", "author": ["IT Jolliffe"], "venue": "Springer-Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Statistical concepts: a second course", "author": ["Richard G Lomax", "Debbie L Hahs-Vaughn"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Feature selection for dimensionality reduction", "author": ["Dunja Mladeni\u0107"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Feature selection, L1 vs. L2 regularization, and rotational invariance", "author": ["Andrew Y. Ng"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Small sample size effects in statistical pattern recognition: recommendations for practitioners and open problems", "author": ["S.J. Raudys", "A.K. Jain"], "venue": "Pattern Recognition", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1990}, {"title": "Regularized Least-Squares Classification", "author": ["Ryan Rifkin", "Gene Yeo", "Tomaso Poggio"], "venue": "Nato Science Series Sub Series III Computer and Systems Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "MICCAI 2014 machine learning challenge.  https://www.nmr.mgh.harvard", "author": ["Mert R. Sabuncu", "Ender Konukoglu"], "venue": "edu/lab/laboratory-computational-imaging-biomarkers/ miccai-2014-machine-learning-challenge", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The curse of dimensionality in data mining and time series prediction", "author": ["Michel Verleysen", "Damien Fran\u00e7ois"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "A comparative study on feature selection in text categorization", "author": ["Yiming Yang", "Jan O. Pedersen"], "venue": "In Proceedings of ICML-97, 14th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "The Optimality of Naive Bayes", "author": ["Harry Zhang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}], "referenceMentions": [{"referenceID": 11, "context": "Unfortunately, the probability of overfitting of a learning algorithm increases with the number of features [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "Dimension reduction methods are not only powerful tools to avoid overfitting [10], but also capable of making the training of high-dimensional data a computationally more feasible task.", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "Feature selection methods [9, 6] aim to identify a subset of \u201cmeaning-ful\u201d features out of the original set of features.", "startOffset": 26, "endOffset": 32}, {"referenceID": 3, "context": "Feature selection methods [9, 6] aim to identify a subset of \u201cmeaning-ful\u201d features out of the original set of features.", "startOffset": 26, "endOffset": 32}, {"referenceID": 6, "context": "Among various feature selection methods, we limit our scope on filter methods, as they do not depend on a specific classification method and therefore are suitable for the comparison of different classifiers [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 12, "context": "and ANOVA F-test are among the most effective scores for filtered feature selection [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "As part of the 17th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), the MICCAI 2014 Machine Learning Challenge (MLC) aims for the objective comparison of the latest machine learning algorithms applied on Structural MRI data [13].", "startOffset": 273, "endOffset": 277}, {"referenceID": 5, "context": "In this study, we use the ANOVA F-Test statistic [8] for the feature scoring.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "Feature transformation by PCA PCA [7] reduces the dimension of the data by finding the first s orthogonal linear combinations of the original variables with the largest variance.", "startOffset": 34, "endOffset": 37}, {"referenceID": 13, "context": "Gaussian Naive Bayes (GNB) Bayes classifiers are based on the Bayes\u2019 theorem and depend on naive (strong) independence assumptions [16, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 2, "context": "Gaussian Naive Bayes (GNB) Bayes classifiers are based on the Bayes\u2019 theorem and depend on naive (strong) independence assumptions [16, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 9, "context": "Ridge The Ridge classifier is based on Ridge Regression, which extends the Ordinary Least Squares (OLS) method with an additional penalty term to limit the L2-norm of the weight vector [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "By generating a set of trees in randomly selected subspaces of the feature space [1], the different trees generalize their classification in complementary ways.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "1 Dataset In our experiments, we used the dataset for the binary classification task of the MLC 2014 [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "2 Evaluation measures As recommended by the MLC 2014 challenge [13], we used two common performance measures: Accuracy and the area under the Receiver Operating Characteristic curve (AUC).", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "This phenomenon is known as the \u201csmall sample size\u201d problem [11].", "startOffset": 60, "endOffset": 64}], "year": 2015, "abstractText": "In this work we show that the classification performance of high-dimensional structural MRI data with only a small set of training examples is improved by the usage of dimension reduction methods. We assessed two different dimension reduction variants: feature selection by ANOVA F-test and feature transformation by PCA. On the reduced datasets, we applied common learning algorithms using 5-fold crossvalidation. Training, tuning of the hyperparameters, as well as the performance evaluation of the classifiers was conducted using two different performance measures: Accuracy, and Receiver Operating Characteristic curve (AUC). Our hypothesis is supported by experimental results.", "creator": "LaTeX with hyperref package"}}}