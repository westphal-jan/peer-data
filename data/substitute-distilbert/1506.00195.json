{"id": "1506.00195", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Recurrent Neural Networks with External Memory for Language Understanding", "abstract": "recurrent neural networks ( rnns ) networks shifted increasingly popular outside the task of language understanding. in this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. the properties of rnn may be attributed to strong ability to memorize discrete - term dependence that relates the current - day semantic label prediction to the observations many time instances away. however, concurrent memory capacity of efficient algorithms is limited because of the gradient vanishing and exploding bias. we propose to assume an semantic memory to improve memorization capability of rnns. we conducted experiments on the atis dataset, and observed that the proposed model was able to achieve the state - of - the - art results. we compare our proposed model with alternative projections and report analysis results that may provide insights for future research.", "histories": [["v1", "Sun, 31 May 2015 05:10:03 GMT  (91kb,D)", "http://arxiv.org/abs/1506.00195v1", "submitted to Interspeech 2015"]], "COMMENTS": "submitted to Interspeech 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["baolin peng", "kaisheng yao"], "accepted": false, "id": "1506.00195"}, "pdf": {"name": "1506.00195.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Networks with External Memory for Language Understanding", "authors": ["Baolin Peng", "Kaisheng Yao"], "emails": ["blpeng@se.cuhk.edu.hk,", "kaisheny@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Neural network based methods have recently demonstrated promising results on many natural language processing tasks [1, 2]. Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.\nThe main task of a language understanding (LU) system is to associate words with semantic meanings [7\u20139]. For example, in the sentence \u201dPlease book me a ticket from Hong Kong to Seattle\u201d, a LU system should tag \u201dHong Kong\u201d as the departurecity of a trip and \u201dSeattle\u201d as its arrival city. The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].\nA RNN consists of an input, a recurrent hidden layer, and an output layer. The input layer reads each word and the output layer produces probabilities of semantic labels. The success of RNNs can be attributed to the fact that RNNs, if successfully trained, can relate the current prediction with input words that are several time steps away. However, RNNs are difficult to train, because of the gradient vanishing and exploding problem [13]. The problem also limits RNNs\u2019 memory capacity because error signals may not be able to back-propagated far enough.\nThere have been two lines of researches to address this problem. One is to design learning algorithms that can avoid gradient exploding, e.g., using gradient clipping [14], and/or gradient vanishing, e.g., using second-order optimization methods [15]. Alternatively, researchers have proposed more advanced model architectures, in contrast to the simple RNN that\nuses, e.g., Elman architecture [16]. Specifically, the long shortterm memory (LSTM) [17,18] neural networks have three gates that control flows of error signals. The recently proposed gated recurrent neural networks (GRNN) [6] may be considered as a simplified LSTM with fewer gates.\nAlong this line of research on developing more advanced architectures, this paper focuses on a novel neural network architecture. Inspired by the recent work in [19], we extend the simple RNN with Elman architecture to using an external memory. The external memory stores the past hidden layer activities, not only from the current sentence but also from past sentences. To predict outputs, the model uses input observation together with a content retrieved from the external memory. The proposed model performs strongly on a common language understanding dataset and achieves new state-of-the-art results.\nThis paper is organized as follows. We briefly describe background of this research in Sec. 2. Section 3 presents details of the proposed model. Experiments are in section 4. We relate our research with other works in Sec. 5. Finally, we have conclusions and discussions in Sec. 6."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Language understanding", "text": "A language understanding system predicts an output sequence with tags such as named-entity given an input sequence words. Often, the output and input sequences have been aligned. In these alignments, an input may correspond to a null tag or a single tag. An example is given in Table 1.\nGiven a T -length input word sequence xT1 , a corresponding output tag sequence yT1 , and an alignment A, the posterior probability p(yT1 |A, xT1 ) is approximated by\np(yT1 |xT1 ) \u2248 T\u220f t=1 p(yt|xt+kt\u2212k), (1)\nwhere k is the size of a context window and t indexes the positions in the alignment.\nar X\niv :1\n50 6.\n00 19\n5v 1\n[ cs\n.C L\n] 3\n1 M\nay 2\n01 5"}, {"heading": "2.2. Simple recurrent neural networks", "text": "The above posterior probability can be computed using a RNN. A RNN consists of an input layer xt, a hidden layer ht, and an output layer yt. In Elman architecture [16], hidden layer activity ht is dependent on both the input xt and also recurrently on the past hidden layer activity ht\u22121.\nBecause of the recurrence, the hidden layer activity ht is dependent on the observation sequence from its beginning. The posterior probability is therefore computed as follows\np(yT1 |xT1 ) \u2248 T\u220f t=1 p(yt|xt1)\n= T\u220f t=1 p(yt|ht, xt) (2)\nwhere the output yt and hidden layer activity ht are computed as\nyt = g(ht), (3) ht = \u03c3(xt, ht\u22121). (4)\nIn the above equation, g(\u00b7) is softmax function and \u03c3(\u00b7) is sigmoid or tanh function. The above model is denoted as simple RNN, to contrast it with more advanced recurrent neural networks described below."}, {"heading": "2.3. Recurrent neural networks using gating functions", "text": "The current hidden layer activity ht of a simple RNN is related to its past hidden layer activity ht\u22121 via the nonlinear function in Eq. (4). The non-linearity can cause errors back-propagated from ht to explode or to vanish. This phenomenon prevents simple RNN from learning patterns that are spanned with long time dependence [14].\nTo tackle this problem, long short-term memory (LSTM) neural network was proposed in [17] with an introduction of memory cells, linearly dependent on their past values. LSTM also introduces three gating functions, namely input gate, forget gate and output gate. We follow a variant of LSTM in [18].\nMore recently, a gated recurrent neural network (GRNN) [6] was proposed. Instead of the three gating functions in LSTM, it uses two gates.\nOne is a reset gate rt that relates a candidate activation with the past hidden layer activity ht\u22121; i.e.,\nh\u0302t = tanh(Wxhxt +Whh(rt ht\u22121)) (5)\nwhere h\u0302t is the candidate activation. Wxh and Whh are the matrices relate the current observation xt and the past hidden layer activity. is element-wise product.\nThe second gate is an update gate zt that interpolates the candidate activation and the past hidden layer activity to update the current hidden layer activity; i.e.,\nht = (1\u2212 zt) ht\u22121 + zt h\u0302t. (6)\nThese gates are usually computed as functions of the current observation xt and the past hidden layer activity; i.e.,\nrt = \u03c3(Wxrxt +Whrht\u22121) (7) zt = \u03c3(Wxzxt +Whzht\u22121) (8)\nwhere Wxr and Whr are the weights to observation and to the past hidden layer activity for the reset gate. Wxz and Whz are similarly defined for the update gate."}, {"heading": "3. The RNN-EM architecture", "text": "We extend simple RNN in this section to using external memory. Figure 1 illustrates the proposed model, which we denote it as RNN-EM. Same as with the simple RNN, it consists of an input layer, a hidden layer and an output layer. However, instead of feeding the past hidden layer activity directly to the hidden layer as with the simple RNN, one input to the hidden layer is from a content of an external memory. RNN-EM uses a weight vector to retrieve the content from the external memory to use in the next time instance. The element in the weight vector is proportional to the similarity of the current hidden layer activity with the content in the external memory. Therefore, content that is irrelevant to the current hidden layer activity has small weights. We describe RNN-EM in details in the following sections. All of the equations to be described are with their bias terms, which we omit for simplicity of descriptions. We implemented RNN-EM using Theano [20, 21]."}, {"heading": "3.1. Model input and output", "text": "The input to the model is a dense vector xt \u2208 Rd\u00d71. In the context of language understanding, xt is a projection of input words, also known as word embedding.\nThe hidden layer reads both the input xt and a content ct vector from the memory. The hidden layer activity is computed as follows\nht = \u03c3(Wihxt +Wcct) (9)\nwhere \u03c3(\u00b7) is tanh function. Wih \u2208 Rp\u00d7d is the weight to the input vector. ct \u2208 Rm\u00d71 is the content from a read operation to be described in Eq. (15). Wc \u2208 Rp\u00d7m is the weight to the content vector.\nThe output from this model is fed into the output layer as follows\nyt = g(Whoht) (10)\nwhere Who is the weight to the hidden layer activity and g(\u00b7) is softmax function.\nNotice that in case of ct = ht\u22121, the above model is simple RNN."}, {"heading": "3.2. External memory read", "text": "RNN-EM has an external memory Mt \u2208 Rm\u00d7n. It can be considered as a memory with n slots and each slot is a vector with m elements. Similar to the external memory in computers, the memory capacity of RNN-EM may be increased if using a large n.\nThe model generates a key vector kt to search for content in the external memory. Though there are many possible ways to generate the key vector, we choose a simple linear function that relates hidden layer activity ht as follows\nkt =Wkht (11)\nwhereWk \u2208 Rm\u00d7p is a linear transformation matrix. Our intuition is that the memory should be in the same space of or affine to the hidden layer activity.\nWe use cosine distance K(u, v) = u\u00b7v\u2016u\u2016\u2016v\u2016 to compare this key vector with contents in the external memory. The weight for the c-th slot Mt(:, c) in memory Mt is computed as follows\nw\u0302t(c) = exp\u03b2tK(kt,Mt(:, c))\u2211 q exp\u03b2tK(kt,Mt(:, q))\n(12)\nwhere the above weight is normalized and sums to 1.0. \u03b2t is a scalar larger than 0.0. It sharpens the weight vector when \u03b2t is larger than 1.0. Conversely, it smooths or dampens the weight vector when \u03b2t is between 0.0 and 1.0. We use the following function to obtain \u03b2t; i.e.,\n\u03b2t = log(1 + exp(W\u03b2ht)) (13)\nwhereW\u03b2 \u2208 R1\u00d7p maps the hidden layer activity ht to a scalar. Importantly, we also use a scalar coefficient gt to interpolate the above weight estimate with the past weight as follows:\nwt = (1\u2212 gt)wt\u22121 + gtw\u0302t (14)\nThis function is similar to Eq. (6) in the gated RNN, except that we use a scalar gt to interpolate the weight updates and the gated RNN uses a vector to update its hidden layer activity.\nThe memory content is retrieved from the external memory at time t\u2212 1 using\nct =Mt\u22121wt\u22121. (15)"}, {"heading": "3.3. External memory update", "text": "RNN-EM generates a new content vector vt to be added to its memory; i.e,\nvt =Wvht (16)\nwhere Wv \u2208 Rm\u00d7p. We use the above linear function based on the same intuition in Sec. 3.2 that the new content and the hidden layer activity are in the same space of or affine to each other.\nRNN-EM has a forget gate as follows\nft = 1\u2212 wt et (17)\nwhere et \u2208 Rn\u00d71 is an erase vector, generated as et = \u03c3(Wheht). Notice that the c-th element in the forget gate is zero only if both read weight wt and erase vector et have their c-th element set to one. Therefore, memory cannot be forgotten if it is not to be read.\nRNN-EM has an update gate ut. It simply uses the weight wt as follows\nut = wt. (18)\nTherefore, memory is only updated if it is to be read. With the above described two gates, the memory is updated as follows\nMt = diag(ft)Mt\u22121 + diag(ut)vt (19)\nwhere diag(\u00b7) transforms a vector to a diagonal matrix with diagonal elements from the vector.\nNotice that when the number of memory slots is small, it may have similar performances as a gated RNN. Specifically, when n = 1, Eqs. (19) and (6) are qualitatively similar."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Dataset", "text": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22\u201324]. The training part of this dataset consists of 4978 sentences and 56590 words. There are 893 sentences and 9198 words for test. The number of semantic label is 127, including the common null label. We use lexicon-only features in experiments."}, {"heading": "4.2. Comparison with the past results", "text": "The input xt in RNN-EM has a window size of 3, consisting of the current input word and its neighboring two words. We use the AdaDelta method to update gradients [25]. The maximum number of training iterations was 50. Hyper parameters for tuning included the hidden layer size p, the number of memory slots n, and the dimension for each memory slot m. The best performing RNN-EM had 100 dimensional hidden layer and 8 memory slots with 40 dimensional memory slot.\nTable 2 lists performance in F1 score of RNN-EM, together with the previous best results of alternative models in the literature. Since there are no previous results from GRNN, we use our own implementation of it for this study. These results are optimal in their respective systems. The previous best result was achieved using LSTM. A change of 0.38% of F1 score from LSTM result is significant at the 90% confidence level. Results in Table 2 show that RNN-EM is significantly better than the previous best result using LSTM."}, {"heading": "4.3. Analysis on convergence and averaged performances", "text": "Results in the previous sections were obtained with models using different sizes. This section further compares neural network models given that they have approximately the same number of parameters, listed in Table 3. We use AdaDelta [25] gradient update method for all these models. Figure 2 plots their\ntraining set entropy with respect to iteration numbers. To better illustrate their convergences, we have converted entropy values to their logarithms. The results show that RNN-EM converges to lower training entropy than other models. RNN-EM also converges faster than the simple RNN and LSTM.\nWe further repeated ATIS experiments for 10 times with different random seeds for these neural network models. We evaluated their performances after their convergences. Table 4 lists their averaged F1 scores, together with their maximum and minimum F1 scores. A change of 0.12% is significant at the 90% confidence level, when comparing against LSTM result. Results in Table 4 show that RNN-EM, on average, significantly outperforms LSTM. The best performance by RNN-EM is also significantly better than the best performing LSTM."}, {"heading": "4.4. Analysis on memory size", "text": "The size of the external memoryMt is proportional to the number of memory slots n. We fixed the dimension of memory slots to 40 and varied the number of slots. Table 5 lists their test set F1 scores. The best performing RNN-EM was with n = 8. Notice that RNN-EM with n = 1 performed better than the simple RNN with 94.09% F1 score in Table 4. This can be explained as using gate functions in Eqs. (17) and (18) in RNN-EM, which are absent in simple RNNs. RNN-EM with n = 1 also performed similarly as the gated RNN with 94.70% F1 score in Table 4, partly because of these gate functions.\nMemory capacity may be measured using training set entropy. Table 5 shows that training set entropy is decreased initially with n increased from 1 to 8, showing that the memory capacity of the RNN-EM is improved. However, the entropy is increased with ns further increased. This suggests that memory\ncapacity of RNN-EM cannot be increased simply by increasing the number of slots."}, {"heading": "5. Related works", "text": "The RNN-EM is along the same line of research in [19, 29] that uses external memory to improve memory capacity of neural networks. Perhaps the closest work is the Neural Turing Machine (NTM) work in [19], which focuses on those tasks that require simple inference and has proved its effectiveness in copy, repeat and sorting tasks. NTM requires complex models because of these tasks. The proposed model is considerably simpler than NTM and can be considered as an extension of simple RNN. Importantly, we have shown through experiments on a common language understanding dataset the promising results from using the external memory architecture."}, {"heading": "6. Conclusions and discussions", "text": "In this paper, we have proposed a novel neural network architecture, RNN-EM, that uses external memory to improve memory capacity of simple recurrent neural networks. On a common language understanding task, RNN-EM achieves new state-ofthe-art results and performs significantly better than the previous best result using long short-term memory neural networks. We have conducted experiments to analyze its convergence and memory capacity. These experiments provide insights for future research directions such as mechanisms of accessing memory contents and methods to increase memory capacity."}, {"heading": "7. Acknowledgement", "text": "The authors would like to thank Shawn Tan and Kai Sheng Tai for useful discussions on NTM structure and implementation."}, {"heading": "8. References", "text": "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, \u201cA\nneural probabilistic language model,\u201d Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.\n[2] R. Collobert and J. Weston, \u201cA unified architecture for natural language processing: deep neural networks with multitask learning,\u201d in ICML, 2008, pp. 160\u2013167.\n[3] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0301, and S. Khudanpur, \u201cRecurrent neural network based language model,\u201d in INTERSPEECH, 2010, pp. 1045\u20131048.\n[4] K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu, \u201cRecurrent neural networks for language understanding,\u201d in INTERSPEECH, 2013, pp. 2524\u20132528.\n[5] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. M. Schwartz, and J. Makhoul, \u201cFast and robust neural network joint models for statistical machine translation,\u201d in ACL, 2014, pp. 1370\u20131380.\n[6] K. Cho, B. van Merrienboer, C\u0327. Gu\u0308lc\u0327ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d in EMNLP, 2014, pp. 1724\u20131734.\n[7] W. Ward et al., \u201cThe cmu air travel information service: Understanding spontaneous speech,\u201d in Proceedings of the DARPA Speech and Natural Language Workshop, 1990, pp. 127\u2013129.\n[8] C. Raymond and G. Riccardi, \u201cGenerative and discriminative algorithms for spoken language understanding,\u201d in INTERSPEECH, 2007, pp. 1605\u20131608.\n[9] R. de Mori, \u201cSpoken language understanding: a survey,\u201d in ASRU, 2007, pp. 365\u2013376.\n[10] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, \u201cConditional random fields: Probabilistic models for segmenting and labeling sequence data,\u201d in ICML, 2001, pp. 282\u2013289.\n[11] T. Kudo and Y. Matsumoto, \u201cChunking with support vector machines,\u201d in NAACL, 2001.\n[12] G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. Hakkani-Tur, X. He, L. Heck, G. Tur, D. Yu, and G. Zweig, \u201cUsing recurrent neural networks for slot filling in spoken language understanding,\u201d IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530\u2013539, 2015.\n[13] Y. Bengio, P. Y. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.\n[14] R. Pascanu, T. Mikolov, and Y. Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d in ICML, 2013, pp. 1310\u20131318.\n[15] J. Martens and I. Sutskever, \u201cTraining deep and recurrent networks with hessian-free optimization,\u201d in Neural Networks: Tricks of the Trade - Second Edition, 2012, pp. 479\u2013535.\n[16] J. Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[17] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.\n[18] A. Graves, A. Mohamed, and G. E. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in ICASSP, 2013, pp. 6645\u20136649.\n[19] A. Graves, G. Wayne, and I. Danihelka, \u201cNeural turing machines,\u201d CoRR, vol. abs/1410.5401, 2014.\n[20] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio, \u201cTheano: new features and speed improvements,\u201d Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.\n[21] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \u201cTheano: a CPU and GPU math expression compiler,\u201d in Proceedings of the Python for Scientific Computing Conference (SciPy), Jun. 2010, oral Presentation.\n[22] D. Dahl, M. Bates, M. Brown, W. Fisher, K. HunickeSmith, D. Pallett, C. Pao, A. Rudnicky, and E. Shriberg, \u201cExpanding the scope of the ATIS task: The ATIS-3 corpus,\u201d in Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 43\u201348.\n[23] Y.-Y. Wang, A. Acero, M. Mahajan, and J. Lee, \u201cCombining statistical and knowledge-based spoken language understanding in conditional models,\u201d in COLING/ACL, 2006, pp. 882\u2013889.\n[24] G. Tur, D. Hakkani-Tr, and L. Heck, \u201cWhat\u2019s left to be understood in ATIS?\u201d in IEEE Workshop on Spoken Language Technologies, 2010.\n[25] M. D. Zeiler, \u201cADADELTA: An adaptive learning rate method,\u201d arXiv:1212.5701, 2012.\n[26] G. Mesnil, X. He, L. Deng, and Y. Bengio, \u201cInvestigation of recurrent-neural-network architectures and learning methods for language understanding,\u201d in INTERSPEECH, 2013.\n[27] P. Xu and R. Sarikaya, \u201cConvolutional neural network based triangular CRF for joint intent detection and slot filling,\u201d in ASRU, 2013, pp. 78\u201383.\n[28] K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and Y. Shi, \u201cSpoken language understanding using long short-term memory neural networks,\u201d in IEEE SLT, 2014.\n[29] J. Weston, S. Chopra, and A. Bordes, \u201cMemory networks,\u201d submitted to ICLR, vol. abs/1410.3916, 2015. [Online]. Available: http://arxiv.org/abs/1410.3916"}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML, 2008, pp. 160\u2013167.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH, 2010, pp. 1045\u20131048.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M. Hwang", "Y. Shi", "D. Yu"], "venue": "INTERSPEECH, 2013, pp. 2524\u20132528.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R.M. Schwartz", "J. Makhoul"], "venue": "ACL, 2014, pp. 1370\u20131380.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014, pp. 1724\u20131734.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "The cmu air travel information service: Understanding spontaneous speech", "author": ["W. Ward"], "venue": "Proceedings of the DARPA Speech and Natural Language Workshop, 1990, pp. 127\u2013129.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["C. Raymond", "G. Riccardi"], "venue": "INTERSPEECH, 2007, pp. 1605\u20131608.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Spoken language understanding: a survey", "author": ["R. de Mori"], "venue": "ASRU, 2007, pp. 365\u2013376.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "ICML, 2001, pp. 282\u2013289.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Chunking with support vector machines", "author": ["T. Kudo", "Y. Matsumoto"], "venue": "NAACL, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530\u2013539, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P.Y. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML, 2013, pp. 1310\u20131318.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Neural Networks: Tricks of the Trade - Second Edition, 2012, pp. 479\u2013535.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure in time", "author": ["J. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "ICASSP, 2013, pp. 6645\u20136649.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, vol. abs/1410.5401, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), Jun. 2010, oral Presentation.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Expanding the scope of the ATIS task: The ATIS-3 corpus", "author": ["D. Dahl", "M. Bates", "M. Brown", "W. Fisher", "K. Hunicke- Smith", "D. Pallett", "C. Pao", "A. Rudnicky", "E. Shriberg"], "venue": "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 43\u201348.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Combining statistical and knowledge-based spoken language understanding in conditional models", "author": ["Y.-Y. Wang", "A. Acero", "M. Mahajan", "J. Lee"], "venue": "COLING/ACL, 2006, pp. 882\u2013889.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "What\u2019s left to be understood in ATIS?", "author": ["G. Tur", "D. Hakkani-Tr", "L. Heck"], "venue": "IEEE Workshop on Spoken Language Technologies,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv:1212.5701, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "INTER- SPEECH, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "ASRU, 2013, pp. 78\u201383.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "IEEE SLT, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "submitted to ICLR, vol. abs/1410.3916, 2015. [Online]. Available: http://arxiv.org/abs/1410.3916", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Neural network based methods have recently demonstrated promising results on many natural language processing tasks [1, 2].", "startOffset": 116, "endOffset": 122}, {"referenceID": 1, "context": "Neural network based methods have recently demonstrated promising results on many natural language processing tasks [1, 2].", "startOffset": 116, "endOffset": 122}, {"referenceID": 2, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 184, "endOffset": 190}, {"referenceID": 5, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 184, "endOffset": 190}, {"referenceID": 6, "context": "The main task of a language understanding (LU) system is to associate words with semantic meanings [7\u20139].", "startOffset": 99, "endOffset": 104}, {"referenceID": 7, "context": "The main task of a language understanding (LU) system is to associate words with semantic meanings [7\u20139].", "startOffset": 99, "endOffset": 104}, {"referenceID": 8, "context": "The main task of a language understanding (LU) system is to associate words with semantic meanings [7\u20139].", "startOffset": 99, "endOffset": 104}, {"referenceID": 7, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 68, "endOffset": 75}, {"referenceID": 9, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 68, "endOffset": 75}, {"referenceID": 10, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 131, "endOffset": 138}, {"referenceID": 11, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 131, "endOffset": 138}, {"referenceID": 12, "context": "However, RNNs are difficult to train, because of the gradient vanishing and exploding problem [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": ", using gradient clipping [14], and/or gradient vanishing, e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": ", using second-order optimization methods [15].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": ", Elman architecture [16].", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Specifically, the long shortterm memory (LSTM) [17,18] neural networks have three gates that control flows of error signals.", "startOffset": 47, "endOffset": 54}, {"referenceID": 17, "context": "Specifically, the long shortterm memory (LSTM) [17,18] neural networks have three gates that control flows of error signals.", "startOffset": 47, "endOffset": 54}, {"referenceID": 5, "context": "The recently proposed gated recurrent neural networks (GRNN) [6] may be considered as a simplified LSTM with fewer gates.", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "Inspired by the recent work in [19], we extend the simple RNN with Elman architecture to using an external memory.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "In Elman architecture [16], hidden layer activity ht is dependent on both the input xt and also recurrently on the past hidden layer activity ht\u22121.", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "This phenomenon prevents simple RNN from learning patterns that are spanned with long time dependence [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "To tackle this problem, long short-term memory (LSTM) neural network was proposed in [17] with an introduction of memory cells, linearly dependent on their past values.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "We follow a variant of LSTM in [18].", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "More recently, a gated recurrent neural network (GRNN) [6] was proposed.", "startOffset": 55, "endOffset": 58}, {"referenceID": 19, "context": "We implemented RNN-EM using Theano [20, 21].", "startOffset": 35, "endOffset": 43}, {"referenceID": 20, "context": "We implemented RNN-EM using Theano [20, 21].", "startOffset": 35, "endOffset": 43}, {"referenceID": 25, "context": "Method F1 score CRF [26] 92.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "94 simple RNN [4] 94.", "startOffset": 14, "endOffset": 17}, {"referenceID": 26, "context": "11 CNN [27] 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "35 LSTM [28] 94.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 22, "context": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 23, "context": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 24, "context": "We use the AdaDelta method to update gradients [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "We use AdaDelta [25] gradient update method for all these models.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "The RNN-EM is along the same line of research in [19, 29] that uses external memory to improve memory capacity of neural networks.", "startOffset": 49, "endOffset": 57}, {"referenceID": 28, "context": "The RNN-EM is along the same line of research in [19, 29] that uses external memory to improve memory capacity of neural networks.", "startOffset": 49, "endOffset": 57}, {"referenceID": 18, "context": "Perhaps the closest work is the Neural Turing Machine (NTM) work in [19], which focuses on those tasks that require simple inference and has proved its effectiveness in copy, repeat and sorting tasks.", "startOffset": 68, "endOffset": 72}], "year": 2015, "abstractText": "Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.", "creator": "LaTeX with hyperref package"}}}