{"id": "1506.01066", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Visualizing and Understanding Neural Models in NLP", "abstract": "while neural networks demonstrate been successfully applied to many nlp tasks { resulting vector - based projects consider very impossible to defeat. for example it's not clear how algorithms achieve { \\ em compositionality }, building sentence meaning from the meanings of words and phrases. in this way we describe new strategies for visualizing them in various models for nlp, advanced by fundamental work in computer vision. we first plot unit values to visualize compositionality where negation, intensification, and concessive clauses, allow us to see well - known markedness asymmetries supporting negation. we then introduce three simple and straightforward methods for visualizing language path's { \\ em salience }, the amount it contributes to the final syllable meaning : ( 1 ) gradient back - propagation, ( 2 ) the variance of output token from the average terminating node, ( 3 ) lstm - style gates that measure information flow. others test our methods on sentiment using pure persistent nets and lstms. our general - purpose methods may have wide applications for understanding compositionality and other associated properties of deep networks, and also shed light on why lstms outperform simple recurrent nets,", "histories": [["v1", "Tue, 2 Jun 2015 21:17:31 GMT  (1681kb,D)", "http://arxiv.org/abs/1506.01066v1", null], ["v2", "Fri, 8 Jan 2016 18:10:22 GMT  (2467kb,D)", "http://arxiv.org/abs/1506.01066v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "xinlei chen", "eduard h hovy", "dan jurafsky"], "accepted": true, "id": "1506.01066"}, "pdf": {"name": "1506.01066.pdf", "metadata": {"source": "CRF", "title": "Visualizing and Understanding Neural Models in NLP", "authors": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu", "xinleic@andrew.cmu.edu", "ehovy@andrew.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural models match or outperform the performance of other state-of-the-art systems on a variety of NLP tasks. Yet unlike traditional featurebased classifiers that assign and optimize weights to varieties of human interpretable features (partsof-speech, named entities, word shapes, syntactic parse features) the behavior of deep learning models is much less easily interpreted. Deep\nlearning models mainly operate on word embeddings (low-dimensional, continuous, real-valued vectors) through multi-layer neural architectures, each layer of which is characterized as an array of hidden neuron units. It is unclear how deep learning models deal with composition, implementing functions like negation or intensification, or combining meaning from different parts of the sentence, filtering away the informational chaff from the wheat, to build sentence meaning\nIn this paper, we explore multiple strategies to interpret meaning composition in neural models. We employ traditional methods like representation plotting, and introduce 3 simple strategies for measuring how much a neural unit contributes to meaning composition, its \u2018salience\u2019 or importance: (1) using first derivatives (2) computing how much a unit differs from the average unit for a sentence, and (3) introducing a LSTM-inspired model that uses gates to measure the flow of information from unit to unit.\nTo simplify our explanation of the models, we focus on phrase-based sentiment analysis from the Stanford Sentiment Treebank dataset (Socher et al., 2013). Sentiment analysis uses a relatively unidirectional aspect of meaning that is much simpler than for general semantic parsing or understanding, making visualizing much simpler. However, that means our findings may not extend to other tasks. Our methods and results should therefore be considered preliminary ones subject to replication on other domains.\nNonetheless, though our attempts only touch superficial points in neural models, and each method has its pros and cons, together they may offer some insights into the behaviors of neural models in language based tasks, marking one initial step toward understanding how they achieve meaning composition in natural language processing.\nThe next section describes some visualization\nar X\niv :1\n50 6.\n01 06\n6v 1\n[ cs\n.C L\n] 2\nJ un\n2 01\n5\nmodels that have inspired this work. In section 3 we describe the dataset and task. Different visualization strategies are presented in Sections 4, 5, 6, 7, followed by a brief conclusion."}, {"heading": "2 A Brief Review of Neural Visualization", "text": "Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). But methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represent in the input, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014). Such methods include:\n(1) Inversion: Inverting the representations by training an additional model to project outputs from different neural levels back to the initial input images (Mahendran and Vedaldi, 2014; Vondrick et al., 2013; Weinzaepfel et al., 2011). The intuition behind reconstruction is that the pixels that are reconstructable from the current representations are the content of the representation. The inverting algorithms allow the current representation to align with corresponding parts of the original images.\n(2) Back-propagation (Erhan et al., 2009; Simonyan et al., 2013) and Deconvolutional Networks (Zeiler and Fergus, 2014): Errors are back propagated from output layers to each intermediate layer and finally to the original image inputs. Deconvolutional Networks work in a similar way by projecting outputs back to initial inputs layer by layer, each layer associated with one supervised model for projecting upper ones to lower ones These strategies make it possible to spot active regions or ones that contribute the most to the\nfinal classification decision. (3) Generation: This group of work generates images in a specific class from a sketch guided by already trained neural models (Szegedy et al., 2013; Nguyen et al., 2014). Models begin with an image whose pixels are randomly initialized and mutated at each step. The specific layers that are activated at different stages of image construction can help in interpretation.\nWhile the above strategies inspire the work we present in this paper, there are fundamental differences between vision and NLP. In NLP words function as basic units, and hence (word) vectors rather than single pixels are the basic units. Sequences of words (e.g., phrases and sentences) are also presented in a more structured way than arrangements of pixels."}, {"heading": "3 Dataset and Models to be Visualized", "text": "We chose the Stanford Sentiment Treebank dataset (Socher et al., 2013), a benchmark dataset widely used for neural model evaluations. The dataset contains gold-standard sentiment labels for every parse tree constituent, from sentences to phrases to individual words, for 215,154 phrases in 11,855 sentences. The task is to perform both fine-grained (very positive, positive, neutral, negative and very negative) and coarse-grained (positive vs negative) classification at both the phrase and sentence level. For more details about the dataset, please refer to (Socher et al., 2013).\nWhile many studies on this dataset use recursive parse-tree models, in this work we employ only standard sequence models (RNNs and LSTMs) since these are the most widely used current neural models, and sequential visualization is more straightforward. We therefore first transform each parse tree node to a sequence of tokens. The sequence is first mapped to a phrase/sentence representation and fed into a softmax classifier. Phrase/sentence representations are built with the following three models:\n1. Standard recurrent sequence models with TANH activation functions\n2. LSTMs.\n3. Bidirectional LSTMs\nFor details about the three models, please refer to Appendix.\nTraining AdaGrad with mini-batch was used for training, with parameters (L2 penalty, learning rate, mini batch size) tuned on the development set. The number of iterations is treated as a variable to tune and parameters are harvested based on the best performance on the dev set. The number of dimensions for the word and hidden layer are set to 60 with 0.1 dropout rate. Parameters are tuned on the dev set.\nThe standard recurrent model achieves 0.429 (fine grained) and 0.850 (coarse grained) accuracy at the sentence level; LSTM achieves 0.469 and 0.870, and Bidirectional LSTM 0.488 and 0.878, respectively."}, {"heading": "4 Representation Plotting", "text": "Local Composition We begin with simple plots of representations to shed light on local compositions, showing a 60d heat-map vector for the representation of selected phrases/sentences. Figure 1 illustrates extent modifications (adverbial and adjectival) and negation.\nThe intensification part of Figure 1 shows suggestive patterns where values for a few dimensions are strengthened by modifiers like \u201ca lot\u201d (the red bar in the first example) \u201cso much\u201d (the red bar in the second example), and \u201cincredibly\u201d. Though the patterns for negations are not as clear, there is still a consistent reversal for some dimensions, visible as a shift between blue and red for dimensions boxed on the left.\nWe then visualize words and phrases using tsne (Van der Maaten and Hinton, 2008) in Figure 2, deliberately adding in some random words for comparative purposes. As can be seen, neural models nicely learn the properties of local compositionally, clustering negation+positive words (\u2018not nice\u2019, \u2019not good\u2019) together with negative words. Note also the asymmetry of negation: \u201cnot bad\u201d is clustered more with the negative than the positive words (as shown both in Figure 1 and 2). This asymmetry has been widely discussed in linguistics, for example as arising from markedness, since \u2018good\u2019 is the unmarked direction of the scale (3; Horn, 1989; Fraenkel and Schul, 2008). This suggests that although the model does seem to focus on certain units for negation in Figure 1, the neural model is not just learning to apply a fixed transform for \u2018not\u2019 but is able to capture the subtle differences in the composition of different words.\nIntensification\nNegation\nConcessive Sentences In concessive sentences, two clauses have opposite polarities, usually related by a contrary-to-expectation implicature. We plot evolving representations over time for two concessives in Figure 3. The plots suggest:\n1. For tasks like sentiment analysis whose goal is to predict a specific semantic dimension (as opposed to general tasks like language model word prediction), too large a dimensionality leads to many dimensions being abandoned, causing two sentences of opposite sentiment to differ only in a few dimensions. This may explain why more dimensions don\u2019t necessarily lead to better performance on such tasks.\n2. LSTMs behave differently than standard recurrent models on these abandoned dimensions. LSTMs assign 0 values to many dimensions, caused by the 0 value of the FORGET gates. Assigning a 0 value to abandoned dimensions makes\nthem exert no further impact on later calculations. Recurrent models, by contrast, assign very large values to useless dimensions1. One possible explanation of the behavior of recurrent models is that since they contain far fewer convolutional parameters than LSTMs, they lack flexibility in controlling information flow. To deal with that, recurrent models use abandoned dimensions for buffering, but however large the original values are, numbers after tanh operation would always fall into [-1, 1]. This enables models to focus on useful dimensions without worrying too much about other dimensions. This is just a speculation, and would require further investigation.\n3. Both sentences contain two clauses connected by the conjunction \u201cthough\u201d. Such twoclause sentences might work collaboratively\u2014 models would remember the word \u201cthough\u201d and make the second clause share the same sentiment orientation as first\u2014or competitively, with the stronger one dominating. The region within dotted line in Figure 3(a) favors the second assumption: the difference between the two sentences is diluted when the final words (\u201cinteresting\u201d and \u201cboring\u201d) appear.\nClause Composition In Figure 4 we explore this clause composition in more detail. Repre-\n1We adopt tanh as activation functions. Most output dimensions take the value close to 1 or -1, which means values before tanh activations are very large positive or negative values given tanh(x) = (ex \u2212 e\u2212x)/(ex + e\u2212x).\nsentations move closer to the negative sentiment region by adding negative clauses like \u201calthough it had bad acting\u201d or \u201cbut it is too long\u201d to the end of a simply positive \u201cI like the movie\u201d. By contrast, adding a concessive clause to a negative clause does not move toward the positive; \u201cI hate X but ...\u201d is still very negative, not that different than \u201cI hate X\u201d. This difference again suggests the model is able to capture negative asymmetry (3; Horn, 1989; Fraenkel and Schul, 2008)."}, {"heading": "5 Model 1: First Derivatives", "text": "Our first new model, inspired by the backpropagation strategy in vision (Erhan et al., 2009; Simonyan et al., 2013), measures how much each input unit contributes to the final decision, which can be approximated by first derivatives.\nMore formally, for a classification model, an inputE is associated with a gold-standard class label c. (Depending on the NLP task, an input could be the embedding for a word or a sequence of words, while labels could be POS tags, sentiment labels, etc.) Given embeddings E for input words with the associated gold class label c, the trained model associates the pair (E, c) with a score Sc(E). The goal is to decide which units of E make the most significant contribution to Sc(e), and thus the decision, the choice of class label c.\nIn the case of deep neural models, the class score Sc(e) is a highly non-linear function. We approximate Sc(e) with a linear function of e by\ncomputing the first-order Taylor expansion\nSc(e) \u2248 w(e)T e+ b (1)\nwhere w(e) is the derivative of Sc with respect to the embedding e.\nw(e) = \u2202(Sc)\n\u2202e |e (2)\nThe magnitude (absolute value) of the derivative indicates the sensitiveness of the final decision to the change in one particular dimension, telling us how much one specific dimension of the word embedding contributes to the final decision. The saliency score is given by\nS(e) = |w(e)| (3)\nVisualization We plot in Figures 5, 6 and 7 the saliency scores (the absolute value of the derivative of the loss function with respect to each dimension of all word inputs) for three sentences, applying the trained model to each sentence. The examples are based on the clear sentiment indicator \u201chate\u201d that lends them all negative sentiment.\n\u201cI hate the movie\u201d All three models assign high saliency to \u201chate\u201d and dampen the influence of other tokens. LSTM offers a clearer focus on \u201chate\u201d than the standard recurrent model, but the bi-directional LSTM shows the clearest focus, attaching almost zero emphasis on words other than \u201chate\u201d. This is presumably due to the gates structures in LSTMs and Bi-LSTMs that controls information flow, making these architectures better\nat filtering out less relevant information.\n\u201cI hate the movie that I saw last night\u201d All three models assign the correct sentiment. The simple recurrent models again do poorly at filtering out irrelevant information, assigning too much salience to words unrelated to sentiment. However none of the models suffer from the gradi-\nent vanishing problems despite this sentence being longer; the salience of \u201chate\u201d still stands out after 7-8 following convolutional operations.\n\u201cI hate the movie though the plot is interesting\u201d The simple recurrent model emphasizes only the second clause \u201cthe plot is interesting\u201d, assigning no credit to the first clause \u201cI hate the movie\u201d.\nThis might seem to be caused by a vanishing gradient, yet the model correctly classifies the sentence as very negative, suggesting that it is successfully incorporating information from the first negative clause. We separately tested the individual clause \u201cthough the plot is interesting\u201d. The standard recurrent model confidently labels it as positive. Thus despite the lower saliency scores for words in the first clause, the simple recurrent system manages to rely on that clause and downplay the information from the latter positive clause\u2014despite the higher saliency scores of the\nlater words. This illustrates a limitation of saliency visualization. first-order derivatives don\u2019t capture all the information we would like to visualize, perhaps because they are only a rough approximate to individual contributions and might not suffice to deal with highly non-linear cases.\nBy contrast, the LSTM emphasizes the first clause, sharply dampening the influence from the second clause, while the Bi-LSTM focuses on both \u201chate the movie\u201d and \u201cplot is interesting\u201d."}, {"heading": "6 Model 2: Average and Variance", "text": "For settings where word embeddings are treated as parameters to optimize from scratch (as opposed to using pre-trained embeddings), we propose a second, surprisingly easy and direct way to visualize important indicators. We first compute the average of the word embeddings for all the words within the sentences. The measure of salience or influence for a word is its deviation from this average. The idea is that during training, models would learn to render indicators different from non-indicator words, enabling them to stand out even after many layers of computation.\nFigure 8 shows a map of variance; each grid corresponds to the value of ||ei,j \u2212 1NS \u2211 i\u2032\u2208NS ei\u2032j || 2 where ei,j denotes the value for j th dimension of word i and N denotes the number of token within the sentences.\nAs the figure shows, the variance-based salience measure also does a good job of emphasizing the relevant sentiment words. The model does have shortcomings: (1) it can only be used in to scenarios where word embeddings are parameters to learn (2) it\u2019s clear how well the model is able to visualize local compositionality."}, {"heading": "7 Model 3: Gate Based Models", "text": "The final strategy, gate-based modeling, is based on the intuition of LSTMs: gates are placed at each time step for controlling information flow, but modified from the LSTM gates to enable more straightforward visualization.\nLet ht,l denote the embedding for time step t at the lth layer. ht,0 denotes the input word embeddings at the 0th layer. We put a more direct control gate at each node: for layer l and time t, we associate it with a scalar gate value Mt,l in the range [0,1].\nInstead of outputting the current representation (i.e., hl,t) for the downstream calculation as in standard recurrent models, the current node outputs the product of the gate valueMt,l and the current embedding hl,t.\nht,l = tanh(W \u00b7 [Mt\u22121,l \u00b7 ht\u22121,l,Mt,l\u22121 \u00b7 ht,l\u22121]) (4)\nwhere W \u2208 R2K\u2217K . The gate values Mt\u22121,l and Mt,l\u22121 are achieved by evaluating the relative importance between the two participant presentations ht\u22121,l and ht,l\u22121 based on the assumption that the\ninformation for the degree of importance has already been encoded in the representations.\nMt\u22121,l = sigmoid (UT (W \u2032[ht\u22121,l, ht,l\u22121])) Mt,l\u22121 = sigmoid (UT (W \u2032[ht,l\u22121, ht\u22121,l])) (5) where W \u2032 \u2208 RK\u22172K , U \u2208 R2K\u22171. Mt,l controls the proportion of currently obtained information that should flow into later computation.\nIf the model thinks not too much relevant information is embedded in ht,l, the value of Mt,l would be small, pushing the output vector towards the other. Mt,l gives straightforward evidence about the relative importance of each node. Models are trained similarly as described in Section 3 and gate values for each node are shown in Figure 9.\nAs the figure shows, in the first example, sequences that involve the keyword \u201chate\u201d (\u201ci hate\u201d, \u201ci hate the\u201d, \u201ci hate the movie\u201d, etc.) are consistently associated with higher weights than the words they are combined with (\u201cthe\u201d, \u201cmovie\u201d and \u201ci\u201d). in the second example, During the last composition step, the model assigns a small weight to the sequence \u201cthe movie i saw yesterday is\u201d and a large weight to the sentiment indicator \u201camazing\u201d.\nOne shortcoming about gate visualization techniques is that they can only provide evidence for local compositions\u2014-the relative importance between two nodes involved in the current step of convolution. Gates can\u2019t globally show individual word contributions."}, {"heading": "8 Conclusion", "text": "In this paper, we offer several methods to help visualize how neural models are able to compose meanings, demonstrating asymmetries of negation and explain some aspects of the strong performance of LSTMs at these tasks."}], "references": [{"title": "Psychology and language: An introduction to psycholinguistics", "author": ["Herbert H. Clark", "Eve V. Clark."], "venue": "Harcourt Brace Jovanovich.", "citeRegEx": "Clark and Clark.,? 1977", "shortCiteRegEx": "Clark and Clark.", "year": 1977}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs."], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886\u2013893. IEEE.", "citeRegEx": "Dalal and Triggs.,? 2005", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Representation and structure in connectionist models", "author": ["Jeffrey L. Elman."], "venue": "Technical Report 8903, Center for Research in Language, University of California, San Diego.", "citeRegEx": "Elman.,? 1989", "shortCiteRegEx": "Elman.", "year": 1989}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "Dept. IRO, Universit\u00e9 de Montr\u00e9al, Tech. Rep.", "citeRegEx": "Erhan et al\\.,? 2009", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of EACL, volume 2014.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "The meaning of negated adjectives", "author": ["Tamar Fraenkel", "Yaacov Schul."], "venue": "Intercultural Pragmatics, 5(4):517\u2013540.", "citeRegEx": "Fraenkel and Schul.,? 2008", "shortCiteRegEx": "Fraenkel and Schul.", "year": 2008}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580\u2013587. IEEE.", "citeRegEx": "Girshick et al\\.,? 2014", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A natural history of negation, volume 960", "author": ["Laurence R. Horn."], "venue": "University of Chicago Press Chicago.", "citeRegEx": "Horn.,? 1989", "shortCiteRegEx": "Horn.", "year": 1989}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 13\u201324.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["David G Lowe."], "venue": "International journal of computer vision, 60(2):91\u2013110.", "citeRegEx": "Lowe.,? 2004", "shortCiteRegEx": "Lowe.", "year": 2004}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi."], "venue": "arXiv preprint arXiv:1412.0035.", "citeRegEx": "Mahendran and Vedaldi.,? 2014", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune."], "venue": "arXiv preprint arXiv:1412.1897.", "citeRegEx": "Nguyen et al\\.,? 2014", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "Signal Processing, IEEE Transactions on, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1312.6034.", "citeRegEx": "Simonyan et al\\.,? 2013", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus."], "venue": "arXiv preprint arXiv:1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2013", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Hoggles: Visualizing object detection features", "author": ["Carl Vondrick", "Aditya Khosla", "Tomasz Malisiewicz", "Antonio Torralba."], "venue": "Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1\u20138. IEEE.", "citeRegEx": "Vondrick et al\\.,? 2013", "shortCiteRegEx": "Vondrick et al\\.", "year": 2013}, {"title": "Reconstructing an image from its local descriptors", "author": ["Philippe Weinzaepfel", "Herv\u00e9 J\u00e9gou", "Patrick P\u00e9rez."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 337\u2013 344. IEEE.", "citeRegEx": "Weinzaepfel et al\\.,? 2011", "shortCiteRegEx": "Weinzaepfel et al\\.", "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "Computer Vision\u2013ECCV 2014, pages 818\u2013833. Springer.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "To simplify our explanation of the models, we focus on phrase-based sentiment analysis from the Stanford Sentiment Treebank dataset (Socher et al., 2013).", "startOffset": 132, "endOffset": 153}, {"referenceID": 10, "context": "But methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to", "startOffset": 177, "endOffset": 202}, {"referenceID": 2, "context": ", Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)).", "startOffset": 2, "endOffset": 15}, {"referenceID": 2, "context": ", Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)).", "startOffset": 2, "endOffset": 41}, {"referenceID": 2, "context": ", Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)).", "startOffset": 2, "endOffset": 66}, {"referenceID": 11, "context": "ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represent in the in-", "startOffset": 122, "endOffset": 134}, {"referenceID": 1, "context": "ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represent in the in-", "startOffset": 143, "endOffset": 167}, {"referenceID": 15, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 12, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 13, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 17, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 6, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 21, "context": "put, and how units in these layers contribute to any final decisions (Simonyan et al., 2013; Mahendran and Vedaldi, 2014; Nguyen et al., 2014; Szegedy et al., 2013; Girshick et al., 2014; Zeiler and Fergus, 2014).", "startOffset": 69, "endOffset": 212}, {"referenceID": 12, "context": "put images (Mahendran and Vedaldi, 2014; Vondrick et al., 2013; Weinzaepfel et al., 2011).", "startOffset": 11, "endOffset": 89}, {"referenceID": 19, "context": "put images (Mahendran and Vedaldi, 2014; Vondrick et al., 2013; Weinzaepfel et al., 2011).", "startOffset": 11, "endOffset": 89}, {"referenceID": 20, "context": "put images (Mahendran and Vedaldi, 2014; Vondrick et al., 2013; Weinzaepfel et al., 2011).", "startOffset": 11, "endOffset": 89}, {"referenceID": 3, "context": "(2) Back-propagation (Erhan et al., 2009; Simonyan et al., 2013) and Deconvolutional Networks (Zeiler and Fergus, 2014): Errors are back propagated from output layers to each intermediate layer and finally to the original image inputs.", "startOffset": 21, "endOffset": 64}, {"referenceID": 15, "context": "(2) Back-propagation (Erhan et al., 2009; Simonyan et al., 2013) and Deconvolutional Networks (Zeiler and Fergus, 2014): Errors are back propagated from output layers to each intermediate layer and finally to the original image inputs.", "startOffset": 21, "endOffset": 64}, {"referenceID": 21, "context": ", 2013) and Deconvolutional Networks (Zeiler and Fergus, 2014): Errors are back propagated from output layers to each intermediate layer and finally to the original image inputs.", "startOffset": 37, "endOffset": 62}, {"referenceID": 17, "context": "(3) Generation: This group of work generates images in a specific class from a sketch guided by already trained neural models (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 126, "endOffset": 169}, {"referenceID": 13, "context": "(3) Generation: This group of work generates images in a specific class from a sketch guided by already trained neural models (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 126, "endOffset": 169}, {"referenceID": 16, "context": "We chose the Stanford Sentiment Treebank dataset (Socher et al., 2013), a benchmark dataset widely used for neural model evaluations.", "startOffset": 49, "endOffset": 70}, {"referenceID": 16, "context": "For more details about the dataset, please refer to (Socher et al., 2013).", "startOffset": 52, "endOffset": 73}, {"referenceID": 8, "context": "This asymmetry has been widely discussed in linguistics, for example as arising from markedness, since \u2018good\u2019 is the unmarked direction of the scale (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 149, "endOffset": 190}, {"referenceID": 5, "context": "This asymmetry has been widely discussed in linguistics, for example as arising from markedness, since \u2018good\u2019 is the unmarked direction of the scale (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 149, "endOffset": 190}, {"referenceID": 8, "context": "This difference again suggests the model is able to capture negative asymmetry (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 79, "endOffset": 120}, {"referenceID": 5, "context": "This difference again suggests the model is able to capture negative asymmetry (3; Horn, 1989; Fraenkel and Schul, 2008).", "startOffset": 79, "endOffset": 120}, {"referenceID": 3, "context": "Our first new model, inspired by the backpropagation strategy in vision (Erhan et al., 2009; Simonyan et al., 2013), measures how much each input unit contributes to the final decision, which can be approximated by first derivatives.", "startOffset": 72, "endOffset": 115}, {"referenceID": 15, "context": "Our first new model, inspired by the backpropagation strategy in vision (Erhan et al., 2009; Simonyan et al., 2013), measures how much each input unit contributes to the final decision, which can be approximated by first derivatives.", "startOffset": 72, "endOffset": 115}], "year": 2015, "abstractText": "While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it\u2019s not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three methods for visualizing a unit\u2019s salience, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,", "creator": "TeX"}}}