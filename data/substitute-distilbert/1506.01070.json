{"id": "1506.01070", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Do Multi-Sense Embeddings Improve Natural Language Understanding?", "abstract": "adequate and distinct representation for each sense of an ambiguous word could lead to more powerful and fine - grained models of vector - space representations. although while ` multi - sense'methods have been proposed and tested on real word - similarity tasks, we don't know if they induce real natural language understanding tasks. in this paper we introduce a neural architecture for incorporating multi - sense embeddings into language understanding, and assessing the performance of a bank - of - the - net multi - sense embedding, ( based on chinese restaurant processes ).", "histories": [["v1", "Tue, 2 Jun 2015 21:30:21 GMT  (163kb)", "http://arxiv.org/abs/1506.01070v1", null], ["v2", "Tue, 18 Aug 2015 04:17:48 GMT  (197kb)", "http://arxiv.org/abs/1506.01070v2", null], ["v3", "Tue, 24 Nov 2015 18:29:40 GMT  (197kb)", "http://arxiv.org/abs/1506.01070v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "dan jurafsky"], "accepted": true, "id": "1506.01070"}, "pdf": {"name": "1506.01070.pdf", "metadata": {"source": "CRF", "title": "Do Multi-Sense Embeddings Improve Natural Language Understanding?", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n01 07\n0v 1\n[ cs\n.C L\n] 2\nJ un\n2 01\n5\nWe apply the model to part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness. We find that\u2014 if we carefully control for the number of dimensions\u2014 sense-specific embeddings, whether alone or concatenated with standard (one vector for all senses) embeddings, introduce slight performance boost in semantic-related tasks, but is of little in others that depend on correctly identifying a few key words such as sentiment analysis."}, {"heading": "1 Introduction", "text": "Enriching vector models of word meaning so they can represent multiple word senses per word type seems to offer the potential to improve many language understanding tasks. Most traditional embedding models associate each word type with a single embedding (e.g., (Bengio et al., 2006)). Thus the embedding for homonymous words like bank (with various senses including \u2018sloping land\u2019 and \u2018financial institution\u2019) is forced to represent\nsome uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding applications.\nRecent research has proposed just such models, representing each word type by different senses, each sense associated with a sense-specific embedding. (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sensespecific embeddings have shown improved peformance on simple artificial tasks like matching human word similarity judgements\u2014 WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012).\nPipelined archetectures that incorporate multisense word embeddings into general neural NLP algorithms intuitively are consisted of the following three major steps:\n\u2022 Sense-specific representation learning learning word sense specific embeddings from large corpus in an unsupervised way or with the help of external resources like WordNet.\n\u2022 Sense induction given a text unit (e.g., phrase, sentence, document etc), inferring word senses for its containing tokens and associate them with correspondent sensespecific embeddings.\n\u2022 Representation acquisition for phrases/sentences learning representations for the text units given sense-specific embeddings. Outputting representations to a machine learning classifier for classification or regression1 .\n1To note, existing work on multi-sense embeddings emphasizes the first step by learning sense specific embeddings\nWe apply the above architecture to a varieties of NLP tasks, i.e., part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness. Our discoveries are as follows :\n\u2022 Multi-sense embeddings offer a slight trend for improved performance in some of tasks, and that concatenating global (one-sensepervector) embeddings with sense-specific embeddings gives significantly improved performance. But none of these differences are significant when carefully compared with embeddings of the same dimensionality\n\u2022 Improvements become even less significant when using more sophisticated neural models like LSTMs which have more flexibility in filtering away the informational chaff from the wheat.\n\u2022 Multi-sense embeddings slightly help artificial semantic motivated tasks like semantic relatedness, but offers little help in tasks that depend on correctly identifying a few key words like sentiment analysis.\n\u2022 We conclude that the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.\nAfter describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks."}, {"heading": "2 Related Work", "text": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation.\nRecent work has begun to augment the neural paradigm to address the multi-sense problem\nbut do not explore the rest two steps. It is clear how existing multi-sense embeddings can be incorporated into and benefit real-world NLU tasks.\nby associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence\u2014 e.g., the fruit \u201capple\u201d tends to co-occur with the words \u201ccider, tree, pear\u201d while the homophonous IT company co-occurs with words like \u201ciphone\u201d, \u201cGoogle\u201d or \u201cipod\u201d.\nFor example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off;; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguates sense embeddings from Wikipedia by first clustering wiki documents. Chen et al. (2014) turned to external resources and used a predefined inventory of senses like Wordnet, building a distinct representation for every sense defined by the Wordnet dictionary. Other relevant work includes (Qiu et al., 2014) that maintains separate representations for different part-of-speech tags of the same word.\nRecent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments."}, {"heading": "3 Learning Sense-Specific Embeddings", "text": "We propose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense. Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is sufficiently different from its early senses. Such a line of thinking naturally points to Chinese Restaurant Processes (CRP) (Griffiths and Tenenbaum, 2004; Teh et al., 2006) which have been applied in the related field of word sense induction. In the analogy\nof CRP, the current word could either sit at one of the existing tables (belonging to one of the existing senses) or choose a new table (a new sense). The decision is made by measuring semantic relatedness (based on local context information and global document information) and the number of customers already sitting of that table (the popularity of word senses). We propose such a model and show that it improves on a standard word similarity task."}, {"heading": "3.1 Chinese Restaurant Processes", "text": "We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995). CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for non-parametric clustering. In the analogy, each data point is compared to a customer in a restaurant. The restaurant has a series of tables t, each of which serves a dish dt. This dish can be viewed as the index of a cluster or a topic. The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choosing a new cluster based on the following probability distribution:\nPr(tw = t) \u221d\n{\nNtP (w|dt) if t already exists\n\u03b3P (w|dnew) if t is new (1)\nwhere Nt denotes the number of customers already sitting at table t and P (w|dt) denotes the probability assigning the current data point to cluster dt. \u03b3 is the hyper parameter controlling the preference for sitting at a new table2.\nCRPs exhibit a useful \u201crich get richer\u201d property because they take into account the popularity of different word senses. They are also more flexible than a simple threshold strategy for setting up new clusters, due to the robustness introduced by adopting the relative ratio of P (w|dt) and P (w|dnew)."}, {"heading": "3.2 Incorporating CRP into Distributed Language Models", "text": "We describe how we incorporate CRP into a standard distributed language model3.\n2 \u03b3 is set to 10\u22125\n3Due to space limits, we omit details about training standard distributed models, which can be readily found in (Collobert and Weston, 2008; Mikolov et al., 2013).\nAs in the standard vector-space model, each token w is associated with a K dimensional global embedding ew. Additionally, it is associated with a set of senses Zw = {z1w, z 2 w, ..., z |Zw | w } where |Zw| denotes the number of senses discovered for word w. Each sense z is associated with a distinct sense-specific embedding ezw. When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current token given its context as in standard language models using global vector ew:\np(ew|eneigh) = F (ew, eneigh) (2)\nF() can take different forms in different learning paradigms, e.g., F = \u220f\nw\u2032\u2208neigh p(ew, ew\u2032) for skip-gram or F = p(ew, g(ew)) for SENNA(Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013).\nUnlike traditional one-word-one-vector frameworks, eneigh includes sense information in addition to the global vectors for neighbors. For example, suppose the preceding word wn\u22121 is associated with its sense index label zwn\u22121 . eneigh is then comprised of both the global embedding ewn\u22121 and its sense-specific embedding e zwn\u22121 wn\u22121 . eneigh can therefore be written as4.\neneigh = {en\u2212k, e zn\u2212k n\u2212k , ..., en\u22121, e zn\u22121 n\u22121 ,\nen+1, e zn+1 n+1 , ..., en\u2212k, e zn\u2212k n\u2212k }\n(3) Next we would use CRP to decide which sense the current occurrence corresponds to, or construct a new sense if it is a new meaning that we have not encountered before. Based on CRP, the probability that assigns the current occurrence to each of the discovered senses or a new sense is given by:\nPr(zw = z) \u221d\n\n \n \nNwz P (e z w|context)\nif z already exists\n\u03b3P (w|znew) if z is new\n(4)\nwhere Nwz denotes the number of times already assigned to sense z for token w. P (ezw|context)\n4For models that predict succeeding words, sense labels for preceding words have already been decided. For models that predict words using both left and right contexts, the labels for right-context words have not been decided yet. In such cases we just use its global word vector to fill up the position.\n01: Input : Token sequence {wn, wneigh}, sentence-embedding es, document embedding ew. 02: Update parameters involved in Equ (3)(4) based on current word prediction. 03: Sample sense label z from CRP. 04: If a new sense label z is sampled: 05: - add z to Zwn 06: - ezwn = argmax p(wn|zm) 07: else: update parameters involved based on sampled sense label z.\nFigure 1: Incorporating CRP into Neural Language Models.\ndenotes the probability that current occurrence belonging to (or generated by) sense z. It takes the similar form as in global vector prediction but is computed based on both local evidence eneigh and global information at the sentence and document level. Specifically, we first obtain the sentence level embedding es and document level embedding ed by averaging all the contained word embeddings with tf-idf5. We concatenate es, ed, and global vector for the current token ew with neighbors eneigh. P (ezw|context) is therefore given by:\nP (ezw|context) = F (e z w, [eneigh, ewn , es, ed]) (5)\nAgain F () denotes the similar activation function as described for global vector prediction.\nThe algorithm for parameter update for the one token predicting procedure is illustrated in Figure 1: Line 2 shows parameter updating through predicting the occurrence of current token. Lines 4-6 illustrate the situation when a new word sense is detected, in which case we would add the newly detected sense z into Zwn . The vector representation ezw for the newly detected sense would be obtained by maximizing the function p(ezw|context).\nAs we can see, the model performs word-sense clustering and embedding learning jointly, each one of which would affect the other. The prediction of the global vector of the current token (line2) is based on both global and sense-specific embeddings of its neighbors, as will be updated through predicting the current token. Similarly,\n5This idea is inspired by many early findings in WSD literature (0; Agirre and Soroa, 2007) that global (i.e., document-level) evidence offers important cues to the sense of the current token. Similar strategy is also found in multisense embedding learning algorithm in (Huang et al., 2012).\nonce the sense label is decided (line7), the model will adjust the embeddings for neighboring words, both global word vectors and sense-specific vectors.\nTraining We train embeddings using Gigaword5 + Wikipedia2014. The training approach is implemented using skip-grammar (SG) (Mikolov et al., 2013). We induced senses for the top 200,000 most frequent words (and used a unified \u201cunknown\u201d token for other less-frequent tokens). The window size is set to 11. We iterate three times over the corpus."}, {"heading": "4 Obtaining Word Representations for NLU tasks", "text": "Next we describe how we decide sense labels for tokens in context. The scenario is treated as a inference procedure for sense labels where all global word embeddings and sense-specific embeddings are kept fixed.\nGiven a document or a sentence, we have an objective function with respect to sense labels by multiplying Eq.(5) over each containing token. Computing the global optimum sense labeling\u2014 in which every word gets an optimal sense label\u2014 requires searching over the space of all senses for all words, which can be expensive. We therefore chose two simplified heuristic approaches:\n\u2022 Greedy Search: Assign each token the locally optimum sense label and represent the current token with the embedding associated with that sense.\n\u2022 Expectation: Compute the probability of each possible sense for the current word, and represent the word with the expectation vector:\n~ew = \u2211\nz\u2208Zw\np(w|z, context) \u00b7 ezw"}, {"heading": "5 Word Similarity Evaluation", "text": "We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching huyman word similarity judgments.\nEarly work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them a poor evaluation. We therefore adopt\nStanford\u2019s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context. Thus for example \u201cbank\u201d in the context of \u201criver bank\u201d would have low relatedness with \u201cdeficit\u201d in the context \u201cfinancial deficit\u201d.\nWe first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context. These vectors are then used as input to get the value of cosine similarity between two words. We include another baseline, No-Global, using only the local sense-disambiguated embedding without the global level embeddings (i.e., sentence and document).\nPerformances are reported in Table 1. Consistent with earlier work (e.g.., (Neelakantan et al., 2014)), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG). As expected, performance a little bit decreases (NoGlobal) when global level information is ignored when choosing word senses, as neighboring words don\u2019t provide sufficient information for word sense disambiguation.\nVisualization Table 2 shows examples of semantically related words given the local context. Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity. Like earlier models (e.g., (Neelakantan et al., 2014))., the model can disambiguate different word senses (in examples like bank, rock and apple) based on their local context; although of course the model is also capable of dealing with polysemy\u2014senses\nthat are less distinct."}, {"heading": "6 Experiments on NLP Tasks", "text": "Having shown that multi-sense embeddings improve word similarity tasks, we turn to ask whether they improve real-world NLU tasks: POS tagging, NER tagging, sentiment analysis at phrase level and sentence level, semantic relationship identification and sentence-level semantic relatedness. For each task, we experimented on the following sets of embeddings, which are trained using the word2vec package on the same corpus:\n\u2022 Standard one-word-one-vector embeddings from skip-gram (50d).\n\u2022 Sense disambiguated embeddings from Section 3 and 4 using Greedy Search and Expectation (50d)\n\u2022 Concatenation of global word embeddings and sense-specific embeddings (100d).\n\u2022 Standard one-word-one-vector skip-gram embeddings with dimensionality doubled (100d) (100d is the correct corresponding baseline since the concatenation above doubles the dimensionality of word vectors)\n\u2022 Embeddings with very high dimensionality (300d).\nAs far as possible we try to perform an appleto-apple comparison on these tasks, and our goal is an analytic one\u2014to investigate how well semantic information can be encoded in multi-sense embeddings and how they can improve NLU performances\u2014rather than an attempt to create state-of-the-art results. Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in (Pennington et al., 2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated stateof-the-art methods (e.g., (Tai et al., 2015; Irsoy and Cardie, 2014)).\nSignificance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994)."}, {"heading": "6.1 The Tasks", "text": "Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data. We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model. We employ a five-layer neural architecture, comprised of an input layer, three convolutional layers with rectifier linear activation function and a softmax output layer. Training is done by gradient descent with minibatches where each sentence is treated as one batch. Learning rate, window size, number of hidden units of hidden layers, L2 regularizations and number of iterations are tuned on the development set.\nPart-of-Speech Tagging We use Sections 0\u201318 of Wall Street Journal (WSJ) data for training, sections 19\u201321 for validation and sections 22\u201324 for testing. Similar to NER, we trained 5-layer neural models which take the concatenation of neighboring embeddings as inputs. We adopt the similar training and parameter tuning strategy as POS.\nSentence-level Sentiment Classification (Pang) The sentiment dataset of Pang et al. (2002) consists of movie reviewes with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure. Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) and then fed into a sigmoid classifier. Convolutional matrixes at the word level are randomized from [-0.1, 0.1] and learned from sequence models. For training, we adopt AdaGrad with mini-batch. Parameters (i.e., L2 penalty, learning rate and mini batch size)\nare tuned on the development set. Due to space limitations, we omit details of recurrent models and training. We followed the exact model architecture and training strategy of Li et al. (2015); refer to that paper for more details.\nSentiment Analysis\u2013Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al. (2002) where labels are only sound at top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.\nFollowing (Socher et al., 2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children. The embeddings for each parse tree constituent are output to a softmax layer; see Socher et al. (2013) and Li et al. (2015) for more details.\nWe focus on the standard version of recursive neural models. Again we fixed word embeddings to each of the different embedding settings described above6. Similarly, we adopted AdaGrad with mini-batch. Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on development set. The number of iterations is treated as a variable to tune and parameters are harvested based on the best performance in the development set.\nSemantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of\n6Note that this is different from the settings used in (Socher et al., 2013) where word vectors were treated as parameters to optimize.\nnominals, e.g., in \u201cMy [apartment]e1 has a pretty large [kitchen]e2\u201d classifying the relation between [apartment] and [kitchen] as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details.\nWe follow the recursive implementations defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier. For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features. We adopt the same training strategy as sentiment task (e.g., Adagrad, minibatches, etc).\nSentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927). Each sentence pair is associated with a gold-standard label raning from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related).\nIn our setting, the similarity between two sen-\ntences is measured based on sentence-level embeddings. Let s1 and s2 denote two sentences and es1 and es2 denote correspondent embeddings. es1 and es2 are achieved from convolutional strategies based on word embeddings within the sentences. Again, word embeddings are obtained by simple table look up in one-word-one-vector settings and inferred from Greedy or Expectation strategy in multi-sense settings. We adopt recurrent model for sentence-level embedding acquisition. We adopt two version of recurrent model, standard one and LSTM model (Hochreiter and Schmidhuber, 1997). We follow the protocols defined in Li et al. (2015). Please reference to that paper for more details.\nThe similarity score is predicted using a regression model built on the structure of a three layer convolutional model, with concatenation of es1 and es2 as input, and a regression score from 1- 5 as output. We adopted the same training strategy as described earlier. The trained model is then used to predict the relatedness score between two new sentences. Performance is measured using Pearson\u2019s r between the predicted score and goldstandard labels."}, {"heading": "6.2 Results", "text": "Results for the different tasks are presented in Tables 3, 4 and 5.\nAt first glance it seems that multi-sense em-\nbeddings do indeed offer superior performance, since combining global vectors with sense-specific vectors introduces a consistent performance boost for every task, when compared with the standard (50d) setting. But of course this is an unfair comparison; combining global vector with sensespecific vector doubles the dimensionality of vector to 100, making comparison with standard dimensionality (50d) unfair. When comparing with standard (100), conclusions become far from clear.\nFor every task, the 100d +Expectation method has performances that look higher than simple 100d baseline (skip-gram embeddings of 100d). The absolute values of these differences are largest for tasks like semantic relatedness, and smallest for tasks like sentiment. This is sensible since sentence meaning is sensitive to the semantics of one particular word, which would directly be reflected on the relatedness score. By contrast, for sentiment analysis, much of the task depends on correctly identifying a few sentiment words like \u201cgood\u201d or \u201cbad\u201d, in which multi-sense embeddings offer little help. Furthermore, the advantages of neural models in sentiment analysis tasks presumably lie in its capability to capture local composition like negation, and it\u2019s not clear how helpful multi-sense embeddings are for that aspect. Similarly, multi-sense embeddings seem to help more for POS tagging than for NER tagging. Word senses have long been known to be related to POS tags. But the largest proportion of NER tags is the negative not-a-NER (\u201cO\u201d) tag, each of which is likely correctly labelable regardless of whether senses are disambiguated or not (since presumably if a word is not a named entity, its senses are mostly not a named entity either).\nNonetheless, despite these trends toward a difference in absolute range, it turns out that for every task the difference between using multi-sense embeddings and using the standard skip-gram baselines of equivalent dimensionality is not statistically significant. That is, just using double the number of dimensions is sufficient to increase performance just as much as using the complex multisense algorithm. Of course increasing vector dimensionality (to 300) boosts performance even more, although at the significant cost of exponentially increasing time complexity.\nWhy multi-sense models do not introduce much of improvement in general natural understanding tasks ? Our interpretations are as follows :\ninformation about distinct senses is encoded in one-word-one-vector embeddings in a mixed and less structured way. A deep neural model would manage to separate chaff from grain and choose what information to take up, bridging the gap between single vector and multi-sense paradigms. As LSTM models are better at doing such a job by putting gates to control information flow, the difference between two paradigms would be further narrowed down."}, {"heading": "7 Conclusion", "text": "In this paper, we expand ongoing research into multi-sense embeddings by first proposing a new version based on Chinese restaurant processes that achieves state of the art performance on simple word similarity matching tasks. We then introduce a pipeline system for incorporating multisense embeddings to NLP applications, and examine multiple NLP tasks to see whether and when multi-sense embeddings can introduce performance boosts. Our results suggest that simply increasing the dimensionality of baseline skipgram embeddings is sufficient to achieve the performance wins from using multi-sense embeddings. That is, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.\nOur results come with some caveats. First, our conclusions are based on the pipelined system that we introduce. There is thus the possibility that our conclusions are are restricted to our proposed framework, and that results from other multi-sense embedding systems (e.g., a more advanced sense learning model or a better sense label model or a completely different pipeline system) may conflict with our discoveries. Second, we try to make the comparison as fair as possible in this comparisonoriented paper, enforcing that models using different embeddings be trained in the same way (minibatch, Adagrad, etc). However, this is not necessarily the optimal way to train every model. Additionally, because we use very standard compositional neural models to make fair comparison easier, our conclusions may only be limited to the models we adopt and may not extend to state-ofthe-art sophisticated algorithm variants."}], "references": [{"title": "Semeval-2007 task 02: Evaluating word sense induction and discrimination systems", "author": ["Eneko Agirre", "Aitor Soroa."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7\u201312. Association for Computational Linguis-", "citeRegEx": "Agirre and Soroa.,? 2007", "shortCiteRegEx": "Agirre and Soroa.", "year": 2007}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025\u20131035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "A bayesian analysis of some nonparametric problems", "author": ["Thomas S Ferguson."], "venue": "The annals of statistics, pages 209\u2013230.", "citeRegEx": "Ferguson.,? 1973", "shortCiteRegEx": "Ferguson.", "year": 1973}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["DMBTL Griffiths", "MIJJB Tenenbaum."], "venue": "Advances in neural information processing systems, 16:17.", "citeRegEx": "Griffiths and Tenenbaum.,? 2004", "shortCiteRegEx": "Griffiths and Tenenbaum.", "year": 2004}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems, pages 2096\u20132104.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185", "author": ["Jiwei Li", "Dan Jurafsky", "Eudard Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning, pages 641\u2013648. ACM.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79\u201386. As-", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Learning state space trajectories in recurrent neural networks", "author": ["Barak A Pearlmutter."], "venue": "Neural Computation, 1(2):263\u2013269.", "citeRegEx": "Pearlmutter.,? 1989", "shortCiteRegEx": "Pearlmutter.", "year": 1989}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A simple and efficient method to generate word sense representations", "author": ["Luis Nieto Pina", "Richard Johansson."], "venue": "arXiv preprint arXiv:1412.6045.", "citeRegEx": "Pina and Johansson.,? 2014", "shortCiteRegEx": "Pina and Johansson.", "year": 2014}, {"title": "Exchangeable and partially exchangeable random partitions", "author": ["Jim Pitman."], "venue": "Probability theory and related fields, 102(2):145\u2013158.", "citeRegEx": "Pitman.,? 1995", "shortCiteRegEx": "Pitman.", "year": 1995}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["Lin Qiu", "Yong Cao", "Zaiqing Nie", "Yong Rui."], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J Mooney."], "venue": "NAACL.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Hierarchical dirichlet processes", "author": ["Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei."], "venue": "Journal of the american statistical association, 101(476).", "citeRegEx": "Teh et al\\.,? 2006", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia", "author": ["Zhaohui Wu", "C Lee Giles"], "venue": null, "citeRegEx": "Wu and Giles.,? \\Q2015\\E", "shortCiteRegEx": "Wu and Giles.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": ", (Bengio et al., 2006)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 26, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 19, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 11, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 2, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 23, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 32, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 14, "context": "(Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).", "startOffset": 0, "endOffset": 157}, {"referenceID": 27, "context": "Such sensespecific embeddings have shown improved peformance on simple artificial tasks like matching human word similarity judgements\u2014 WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al.", "startOffset": 142, "endOffset": 175}, {"referenceID": 11, "context": "Such sensespecific embeddings have shown improved peformance on simple artificial tasks like matching human word similarity judgements\u2014 WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012).", "startOffset": 184, "endOffset": 204}, {"referenceID": 1, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 3, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 18, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 17, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 16, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 22, "context": "Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).", "startOffset": 179, "endOffset": 320}, {"referenceID": 25, "context": "Other relevant work includes (Qiu et al., 2014) that maintains separate representations for different part-of-speech tags of the same word.", "startOffset": 29, "endOffset": 47}, {"referenceID": 21, "context": "For example Reisinger and Mooney (2010) and Huang et al.", "startOffset": 12, "endOffset": 40}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings.", "startOffset": 44, "endOffset": 64}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off;; Liu et al.", "startOffset": 44, "endOffset": 332}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off;; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models.", "startOffset": 44, "endOffset": 557}, {"referenceID": 10, "context": "For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off;; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguates sense embeddings from Wikipedia by first clustering wiki documents.", "startOffset": 44, "endOffset": 670}, {"referenceID": 2, "context": "Chen et al. (2014) turned to external resources and used a predefined inventory of senses like Wordnet, building a distinct representation for every sense defined by the Wordnet dictionary.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "most specifically Huang et al. (2012) and Neelakantan et al.", "startOffset": 18, "endOffset": 38}, {"referenceID": 11, "context": "most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense.", "startOffset": 18, "endOffset": 68}, {"referenceID": 8, "context": "of thinking naturally points to Chinese Restaurant Processes (CRP) (Griffiths and Tenenbaum, 2004; Teh et al., 2006) which have been applied in the related field of word sense induction.", "startOffset": 67, "endOffset": 116}, {"referenceID": 31, "context": "of thinking naturally points to Chinese Restaurant Processes (CRP) (Griffiths and Tenenbaum, 2004; Teh et al., 2006) which have been applied in the related field of word sense induction.", "startOffset": 67, "endOffset": 116}, {"referenceID": 8, "context": "We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995).", "startOffset": 142, "endOffset": 205}, {"referenceID": 31, "context": "We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995).", "startOffset": 142, "endOffset": 205}, {"referenceID": 24, "context": "We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Griffiths and Tenenbaum, 2004; Teh et al., 2006; Pitman, 1995).", "startOffset": 142, "endOffset": 205}, {"referenceID": 6, "context": "CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for non-parametric clustering.", "startOffset": 71, "endOffset": 87}, {"referenceID": 3, "context": "2 \u03b3 is set to 10 Due to space limits, we omit details about training standard distributed models, which can be readily found in (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 128, "endOffset": 178}, {"referenceID": 17, "context": "2 \u03b3 is set to 10 Due to space limits, we omit details about training standard distributed models, which can be readily found in (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 128, "endOffset": 178}, {"referenceID": 3, "context": "w\u2208neigh p(ew, ew\u2032) for skip-gram or F = p(ew, g(ew)) for SENNA(Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al.", "startOffset": 62, "endOffset": 90}, {"referenceID": 17, "context": "w\u2208neigh p(ew, ew\u2032) for skip-gram or F = p(ew, g(ew)) for SENNA(Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013).", "startOffset": 282, "endOffset": 304}, {"referenceID": 0, "context": "This idea is inspired by many early findings in WSD literature (0; Agirre and Soroa, 2007) that global (i.", "startOffset": 63, "endOffset": 90}, {"referenceID": 11, "context": "Similar strategy is also found in multisense embedding learning algorithm in (Huang et al., 2012).", "startOffset": 77, "endOffset": 97}, {"referenceID": 17, "context": "The training approach is implemented using skip-grammar (SG) (Mikolov et al., 2013).", "startOffset": 61, "endOffset": 83}, {"referenceID": 7, "context": "Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and", "startOffset": 47, "endOffset": 73}, {"referenceID": 19, "context": "The performance from baselines is reprinted from (Neelakantan et al., 2014; Chen et al., 2014) and we report the best performance across all different settings mentioned in their paper.", "startOffset": 49, "endOffset": 94}, {"referenceID": 2, "context": "The performance from baselines is reprinted from (Neelakantan et al., 2014; Chen et al., 2014) and we report the best performance across all different settings mentioned in their paper.", "startOffset": 49, "endOffset": 94}, {"referenceID": 11, "context": "Stanford\u2019s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context.", "startOffset": 47, "endOffset": 67}, {"referenceID": 19, "context": ", (Neelakantan et al., 2014)), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG).", "startOffset": 2, "endOffset": 28}, {"referenceID": 19, "context": ", (Neelakantan et al., 2014)).", "startOffset": 2, "endOffset": 28}, {"referenceID": 4, "context": ", NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input fea-", "startOffset": 40, "endOffset": 64}, {"referenceID": 30, "context": ", (Tai et al., 2015; Irsoy and Cardie, 2014)).", "startOffset": 2, "endOffset": 44}, {"referenceID": 12, "context": ", (Tai et al., 2015; Irsoy and Cardie, 2014)).", "startOffset": 2, "endOffset": 44}, {"referenceID": 5, "context": "done via the bootstrap test (Efron and Tibshirani, 1994).", "startOffset": 28, "endOffset": 56}, {"referenceID": 4, "context": "We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model.", "startOffset": 27, "endOffset": 51}, {"referenceID": 20, "context": "Sentence-level Sentiment Classification (Pang) The sentiment dataset of Pang et al. (2002) consists of movie reviewes with a sentiment label", "startOffset": 72, "endOffset": 91}, {"referenceID": 21, "context": "Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) and then fed into a sigmoid classifier.", "startOffset": 90, "endOffset": 109}, {"referenceID": 13, "context": "We followed the exact model architecture and training strategy of Li et al. (2015); refer to that paper for more details.", "startOffset": 66, "endOffset": 83}, {"referenceID": 29, "context": "Sentiment Analysis\u2013Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al.", "startOffset": 69, "endOffset": 90}, {"referenceID": 20, "context": ", 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al. (2002) where labels are only sound at top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.", "startOffset": 184, "endOffset": 203}, {"referenceID": 20, "context": ", 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al. (2002) where labels are only sound at top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.", "startOffset": 184, "endOffset": 337}, {"referenceID": 20, "context": ", 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granuarity than the dataset in Pang et al. (2002) where labels are only sound at top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.", "startOffset": 184, "endOffset": 351}, {"referenceID": 29, "context": "Following (Socher et al., 2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children.", "startOffset": 10, "endOffset": 31}, {"referenceID": 13, "context": "(2013) and Li et al. (2015) for more details.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of", "startOffset": 57, "endOffset": 81}, {"referenceID": 29, "context": "Note that this is different from the settings used in (Socher et al., 2013) where word vectors were treated as parameters to optimize.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": "The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details.", "startOffset": 166, "endOffset": 190}, {"referenceID": 28, "context": "We follow the recursive implementations defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier.", "startOffset": 51, "endOffset": 72}, {"referenceID": 15, "context": "Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927).", "startOffset": 100, "endOffset": 122}, {"referenceID": 10, "context": "We adopt two version of recurrent model, standard one and LSTM model (Hochreiter and Schmidhuber, 1997).", "startOffset": 69, "endOffset": 103}, {"referenceID": 10, "context": "We adopt two version of recurrent model, standard one and LSTM model (Hochreiter and Schmidhuber, 1997). We follow the protocols defined in Li et al. (2015). Please reference to that paper for more details.", "startOffset": 70, "endOffset": 157}], "year": 2017, "abstractText": "Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while \u2018multi-sense\u2019 methods have been proposed and tested on artificial wordsimilarity tasks, we don\u2019t know if they improve real natural language understanding tasks. In this paper we introduce a pipelined architecture for incorporating multi-sense embeddings into language understanding, and test the performance of a state-of-the-art multi-sense embedding model (based on Chinese Restaurant Processes). We apply the model to part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness. We find that\u2014 if we carefully control for the number of dimensions\u2014 sense-specific embeddings, whether alone or concatenated with standard (one vector for all senses) embeddings, introduce slight performance boost in semantic-related tasks, but is of little in others that depend on correctly identifying a few key words such as sentiment analysis.", "creator": null}}}