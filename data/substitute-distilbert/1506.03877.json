{"id": "1506.03877", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "Bidirectional Helmholtz Machines", "abstract": "unsupervised strategies towards binary generative models combining latent variables and systematic inference remains a challenging problem for complex, high dimen - sional distributions. one basic approach to this problem is the so called helmholtz machine and it involves training lazy auxiliary model that is to perform approx - imate tests jointly with the generative model which hopes to be fitted to the train - ing data. the last - down generative model is typically realized as a directed model that starts from some prior at the top, down to the empirical distribution at the bot - tom. the approximate inference model runs in the opposite direction and is typi - cally trained to efficiently infer high probability latent states given some observed data. here we proposes a new method, referred to as geometric mean matching ( iii ), ii is based on the idea properties the generative model should be close to the atlas of distributions that can be modeled by our approximate sum dis - tribution. we achieve this by translating both the top - down and some bottom - up directed models as approximate reasoning distributions and by defining the tar - get estimation we fit to the training data to be the geometric mean of these two. we present an upper - bound for low log - likelihood of this model and we show that optimizing this bound will pressure any model to stay tailored to the approximate inference distributions. in the computational section we estimate that we can use this problem to introduce deep generative models with many layers of hidden binary numerical statistics to complex and high dimensional estimated distributions.", "histories": [["v1", "Fri, 12 Jun 2015 00:08:20 GMT  (701kb,D)", "http://arxiv.org/abs/1506.03877v1", null], ["v2", "Tue, 7 Jul 2015 19:08:07 GMT  (691kb,D)", "http://arxiv.org/abs/1506.03877v2", null], ["v3", "Fri, 20 Nov 2015 18:48:00 GMT  (942kb,D)", "http://arxiv.org/abs/1506.03877v3", null], ["v4", "Mon, 4 Jan 2016 00:07:47 GMT  (1034kb,D)", "http://arxiv.org/abs/1506.03877v4", null], ["v5", "Wed, 25 May 2016 02:54:26 GMT  (880kb,D)", "http://arxiv.org/abs/1506.03877v5", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["j\u00f6rg bornschein", "samira shabanian", "asja fischer", "yoshua bengio"], "accepted": true, "id": "1506.03877"}, "pdf": {"name": "1506.03877.pdf", "metadata": {"source": "CRF", "title": "Training opposing directed models using geometric mean matching", "authors": ["J\u00f6rg Bornschein", "Samira Shabanian", "Asja Fischer", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction and background", "text": "Training good generative models and fitting them to complex and high dimensional training data with probability mass in multiple disjunct locations remains a major challenge. This is especially true for models with multiple layers of deterministic or stochastic variables, which is unfortunate because it has been argued previously [1, 2] that deeper generative models have the potential to capture higher-level abstractions and thus generalize better. With the Helmholtz machine [3, 4], a concept was introduced that proposed to not only fit a powerful but intractable generative model p(x,h) to the training data, but also to jointly train an approximate inference model q(h|x). The q model would be used to efficiently perform approximate inference over the latent variables h of the generative model given an observed example x. This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]). Most of these approaches rely on the variational bound to perform appropriate inference and to obtain an objective function that contains the parameters of both the generative model p and the approximate inference model q in one joint expression (e.g. WS, VAE and NVIL). Not directly\n\u2217Jo\u0308rg Bornschein is a CIFAR Global Scholar; Yoshua Bengio is a CIFAR Senior Fellow\nar X\niv :1\n50 6.\n03 87\n7v 1\n[ cs\n.L G\n] 1\n2 Ju\nn 20\nrelated to the Helmholtz machine, but similar in spirit is the approach described in [10]. It also relies on a bottom-up distribution q(h|x) to perform approximate inference for the latent variables in a very deep generative model p(x,h). But the inference distribution q is fixed and only the parameters of p are learned.\nIn contrast to these previous approaches we here propose to interpret both p and q as approximate inference models for our actual generative model p\u2217(x,h). We define the target distribution p\u2217 to be the geometric mean over the top-down and bottom-up approximate inference models, i.e. p\u2217(x,h) = 1/Z \u221a p(x,h)q(x,h), which gives rise to the name geometric mean matching (GMM) . The motivation behind this definition is to ensure that the intractable generative model p\u2217 stays close to the approximate inference models we have at our disposal.\nIn Section 2 we will introduce the GMM model in detail and discuss important theoretical properties and in Section 3 we will explain how to perform importance sampling based training and inference. The ability to model complex distributions is demonstrated empirically in Section 4."}, {"heading": "2 Model definition and properties", "text": "We introduce the concept by defining a joint probability distribution over three variables, an observed vector x and two latent variable vectors h1 and h2. Analogous to a Deep Boltzmann Machine [11] we think of these as layers in a neural network with links between x and h1 on the one side, and h1 and h2 on the other side. We will present the approach in the specific case of an architecture with two hidden layers, but it can be applied to arbitrary graphs of variables without loops. It can especially be used to train architectures with more than two stacked layers of latent variables.\nLet p\u2217(x,h1,h2) be a joint probability distribution constructed in a specific way from two constituent distributions p(x,h1,h2) and q(x,h1,h2),\np\u2217(x,h1,h2) = 1\nZ\n\u221a p(x,h1,h2) q(x,h1,h2) , (1)\nwhere Z is a normalization constant and p and q are directed graphical models from h2 to x and vice versa. In other words p and q can be factorized as\np(x,h1,h2) = p(h2) p(h1|h2) p(x|h1) and q(x,h1,h2) = q(x) q(h1|x) q(h2|h1) . (2)\nWe assume that the prior distribution p(h2) and all conditional distributions belong to some parametrized families of distributions which can be evaluated and sampled from efficiently. For q(x) we do not assume an explicit form, but define it to be the marginal\nq(x) = p\u2217(x) = \u2211 h1,h2 p\u2217(x,h1,h2) (3)\n=\n\u221a q(x)\nZ \u2211 h1,h2 \u221a p(x,h1,h2) q(h1|x)q(h2|h1) (4)\n=  1 Z \u2211 h1,h2 \u221a p(x,h1,h2) q(h1|x)q(h2|h1) 2 . (5) The normalization constant Z guarantees that \u222b x,h1,h2 p\u2217(x,h1,h2) = 1. Using the Cauchy-\nSchwarz inequality \u2223\u2223\u222b f(y)g(y)dy \u2223\u22232 \u2264 \u222b |f(y)|2 dy \u222b |g(y\u2032)|2 dy\u2032 and identifying \u221ap(y) with\nf(y) and \u221a q(y) with g(y), it becomes clear that Z = \u222b x \u221a p(y)q(y) dy \u2264 1 for arbitrary p and q. Furthermore, we see that Z = 1 only when p(x,h1,h2) = q(x,h1,h2). We can therefore obtain a lower bound on the marginal probability p\u2217(x):\np\u0303\u2217(x) = \u2211 h1,h2 \u221a p(x,h1,h2)q(h1|x) q(h2|h1) 2 = Z2 p\u2217(x) \u2264 p\u2217(x) . (6)\nThis suggests that the model distribution p\u2217(x) can be fitted to some training data by maximizing the bound of the log-likelihood p\u0303\u2217(x) instead of p\u2217(x), as we elaborate in the following section.\nSince log p\u0303\u2217(x) can reach the maximum only when Z \u2192 1, the model is implicitly pressured to find a maximum likelihood solution that yields p(x,h1,h2) \u2248 q(x,h1,h2) \u2248 p\u2217(x,h1,h2)."}, {"heading": "2.1 Alternative view using the Bhattacharyya distance", "text": "Let us consider the case where we would actually maximize log p\u2217(x) instead of the lower bound log p\u0303\u2217(x). We can then decompose the objective using the Bhattacharyya distance DB(p, q) = \u2212 log \u2211 y \u221a p(y)q(y) into two terms:\nlog p\u2217(x) = 2 log \u2211 h1,h2 \u221a p(x,h1,h2)q(h1|x)q(h2|h1)\n\u2212 2 log \u2211\nx\u2032,h\u20321,h \u2032 2\n\u221a p(x\u2032,h\u20321,h \u2032 2)q(x \u2032,h\u20321,h \u2032 2)\n= log p\u0303\u2217(x)\u2212 2 logZ = log p\u0303\u2217(x) + 2DB(p, q) (7) with DB(p, q) \u2265 0 for arbitrary p, q and DB(p, q) = 0 only when p = q.\nWe can compare this to the variational approach, where the marginal probability log p(y) of some model containing latent variables z is rewritten in terms of the KL-divergence DKL(q || p) =\u2211 z q(z|y) log q(z|y) p(z|y) \u2265 0 to obtain a lower bound:\nlog p(y) = E z\u223cq(z|y) [log p(y, z)\u2212 log q(z|y)] +DKL(q(z|y) || p(z|x))\n\u2265 E z\u223cq(z|y) [log p(y, z)\u2212 log q(z|y)] . (8)\nAnalogous to variational methods that maximize the lower bound (8), we can thus maximize log p\u0303\u2217(x) and it will tighten the bound as DB(p, q) approaches zero. While this seems very similar to the variational lower bound, we should highlight that there are some important conceptual differences: 1) The KL-divergence in variational methods typically measures the distance between distributions given some training data. The Bhattacharyya distance here in contrast quantifies a property of the model p\u2217(x,h1,h2) independently of any training data. In fact, we saw that DB(p, q) = \u2212 logZ. 2) The variational lower bound is typically used to construct approximate inference algorithms. We here use our bound p\u0303\u2217(x) just to remove the normalization constant Z from our target distribution p\u2217(x,h1,h2). Even after applying the lower-bound we still have to tackle the inference problem which manifests itself in form of the full combinatorial sum over h1 and h2 in equation (6). Although it seems intuitively reasonable to use a variational approximation on top of the bound p\u0303\u2217(x), we will here not follow this direction but rather use importance sampling to perform approximate inference and learning (see section 3). Combining a variational method with the bound p\u0303\u2217(x) is therefore subject to future work.\nWe can also argue that optimizing log p\u0303\u2217(x) instead of log p\u2217(x) is beneficial in the light of the original goal we formulated in section 1: To learn a generative model p\u2217(x) which is regularized to be close to the model q which we use to perform approximate inference for p\u2217. Let us assume we have two equally well trained models p\u2217\u03b81 and p \u2217 \u03b82 : Ex\u223cD [ log p\u2217\u03b81(x) ] = E [ log p\u2217\u03b82(x) ] , but the expected bound p\u0303\u2217(x) for the first model is closer to the log-likelihood than the expected bound for the second model: E [ log p\u0303\u2217\u03b81(x) ] > E [ log p\u0303\u2217\u03b82(x) ] . Using equation (7) we see thatDB(p\u03b81 , q\u03b81) < DB(p\u03b82 , q\u03b82) which indicates that q\u03b81 is closer to p \u2217 \u03b81 than q\u03b82 is to p \u2217 \u03b82\n(when we measure their distance using the Bhattacharyya distance). According to our original goal we thus prefer solution p\u2217\u03b81 where the bound p\u0303 \u2217(x) is maximized and the distance DB(p, q) minimized. Note that the decomposition (7) also emphasizes why our recursive definition q(x) =\u2211 h1,h2\np\u2217(x,h1,h2) is a consistent and reasonable one: Minimizing DB(p, q) during learning means that the joint distributions p(x,h1,h2) and q(x,h1,h2) approach each other. This implies that the marginals p(hl) and q(hl) for all layers l become more similar. This also implies\nAlgorithm 1 Training p\u2217(x) using important sampling with q as proposal for number of training iterations do \u2022 Sample example(s) x \u223c D from the training distribution for k = 1 to K do \u2022 Layerwise sample latent variables h(k) from q(h|x) \u2022 Compute q(h(k)|x) and p(x,h(k))\nend for \u2022 Compute unnormalized weights \u03c9k =\n\u221a p(x,h(k))\nq(h(k) |x)\n\u2022 Normalize the weights \u03c9\u0303k = \u03c9k\u2211 k\u2032 \u03c9k\u2032\n\u2022 Update parameters in p and q: Use gradient estimator 2 \u2211 k \u03c9\u0303k \u2202 log p\u2217(x,h(k)) \u2202\u03b8\nend for\np(x) \u2248 q(x) in the limit of DB(p, q) \u2192 0; a requirement that most simple parametrized distributions q(x) could never fulfill. It is thus a piece of good fortune that the optimal, but intuitively clumsy definition q(x) = \u2211 h1,h2\np\u2217(x,h1,h2) results in the likelihood expression (6), an expression that has no higher computational complexity and that has mostly the same basic structure as the likelihood expression for any other model containing latent variables."}, {"heading": "3 Inference and training with importance sampling", "text": "Based on the construction of p\u2217(x) outlined in the previous section we can define a wide range of possible models. We furthermore have a wide range of potential training and appropriate inference methods we could employ to maximize log p\u0303\u2217(x). In this text we concentrate on binary latent and observed variables x and hl \u2208 {0, 1}, and we will use simple sigmoid belief network layers for all our conditional distributions: P (x |y) = \u220f i B(xi |\u03c3(Wi y + bi)) where B(a | b) refers to the Bernoulli distribution for value a with probability parameter b, and \u03c3(\u00b7) is the logistic sigmoid function. For our top-level prior p(h2) we use a factorized Bernoulli distribution: p(h2) = \u220f i B(h2,i |\u03c3(b2,i)).\nWe form an estimate of p\u0303\u2217(x) by using important sampling instead of the exhaustive sum over h1 and h2 in equation (6). We use q(h1|x)q(h2|h1) as the proposal distribution which is by construction easy to evaluate and to sample from:\np\u0303\u2217(x) = \u2211 h1,h2 \u221a p(x,h1,h2) q(h1|x) q(h2|h1) 2 =  E\nh2\u223cq(h2|h1) h1\u223cq(h1|x)\n[\u221a p(x,h1,h2)\nq(h1|x) q(h2|h1)\n] 2\n'  1 K K\u2211 k=1 \u221a\u221a\u221a\u221a p(x,h(k)1 ,h(k)2 ) q(h (k) 1 |x) q(h (k) 2 |h (k) 1 ) 2 = p\u0302\u2217(x) with h(k)2 \u223c q(h2|h1) h (k) 1 \u223c q(h1|x) . (9)\nUsing the same approach we can also derive the well known estimator for the marginal probability of a datapoint under the top-down generative model p:\np\u0302(x) = E h2\u223cq(h2|h1) h1\u223cq(h1|x)\n[ p(x,h1,h2)\nq(h1|x) q(h2|h1)\n] (10)\nAnalogous to the parameter updates in reweighted wake-sleep (RWS, [9]) we can derive an important sampling based estimate for the parameter gradients and use them to optimize towards a\nmaximum likelihood solution (see supplement for more details) :\n\u2202\n\u2202\u03b8 log p\u0303\u2217(x) ' 2 K\u2211 k=1 \u03c9\u0303k \u2202 \u2202\u03b8 log \u221a p(x,h(k))q(h(k)|x) (11)\nwith h(k) = (h(k)1 ,h (k) 2 ) , h (k) 1 \u223c q (h1 |x) , h (k) 2 \u223c q (h2 |h1) and the\nimportance weights \u03c9k =\n\u221a p(x,h(k))\nq ( h(k) |x ) , \u03c9\u0303k = \u03c9k\u2211\u2032 k \u03c9 \u2032 k .\nThe updates do not require any form of backpropagation through more than one layer because, as far as the gradient computation \u2202\u2202\u03b8 log p \u2217(x,h (k) 1 ,h (k) 2 ) is concerned, these samples are considered fully observed. The final gradient is determined by computing the normalized weighted average over the individual gradients (equation 11). These properties are basically inherited from the RWS training algorithm [9]. But in contrast to RWS, and in contrast to most other algorithms which employ a generative model p and an approximate inference model q, we here automatically obtain parameter updates for both p and q because we optimize p\u2217 which contains both. The resulting training method is summarized in algorithm 1."}, {"heading": "3.1 Sampling from the model p\u2217(x)", "text": "One way to approximately sample from p\u2217(x) is to sample proposal candidates from p(x,h1,h2) and then use importance resampling to obtain samples that are effectively distributed according to p\u2217(x). The resampling weights can be written as\n\u03c9 = p\u2217(x,h1,h2)\np(x,h1,h2) =\n\u221a q(x,h1,h2)\np(x,h1,h2) (12)\n= 1\nZ\n\u221a p\u0303\u2217(x)q(h1|x)q(h2|h1)\np(x,h1,h2) . (13)\nThese are unfortunately intractable as we have to evaluate the marginal q(x) = p\u2217(x). As an approximation we can use the sampling based estimate p\u0302\u2217(x) (equation 9). Note that we don\u2019t have to evaluate the normalization constant Z as the resampling procedure only evaluates the relative magnitude of the importance weights. We thus end up with the procedure described in algorithm 2."}, {"heading": "3.2 Conditional sampling and inpainting", "text": "We can use a similar importance resampling approach to approximately draw samples from conditional distributions, e.g. from p\u2217(h1 |x,h2). We here choose to draw the proposal samples from the mixture distribution pproposal(h1 |x,h2) = 1/2 p(h1|h2)+1/2 q(h1|x) which should ensure that we\nAlgorithm 2 Sampling from p\u2217(x) using important resampling for k = 1 to K do \u2022 Layerwise draw primary proposal sample x(k),h(k)1 ,h (k) 2 \u223c p(x,h1,h2)\nfor l = 1 to K do \u2022 Layerwise sample latent variables h\u0303(l)1 , h\u0303 (l) 2 \u223c q(h1,h2|x(k)) end for \u2022 Compute the estimated marginal bound p\u0302\u2217(x(k)) using the samples h\u0303(l)1 , h\u0303 (l) 2 (see equation 9)\n\u2022 Compute \u03c9\u0302(k) = \u221a p\u0302(x(k))q(h\n(k) 1 |x)q(h (k) 2 |h (k) 1 )/p(x (k),h (k) 1 ,h (k) 2 )\nend for \u2022 Subsample a final sample x from the primary proposals x(k) proportional to their weights \u03c9\u0302(k)\nhave a symmetric chance of covering the high probability configurations of p\u2217(h1|x,h2) induced by p and q. The importance weights we use to resample a final sample from p\u2217(h1|x,h2) are thus given by\nw(k) =\n\u221a p(h\n(k) 1 |h (k) 2 )q(h (k) 1 |x)\np(h (k) 1 |h2) + q(h (k) 1 |h2)\n(14)\nwith h(k)1 randomly drawn from p(h1|h2) or q(h1|x). For the bottommost layer p\u2217(x|h1) we choose to draw the proposal samples just from p(x|h1) instead of from a proposal mixture distribution that includes q(x).\nEquipped with approximate sampling procedures for the conditional distributions it is straightforward to construct an algorithm for inpainting: Given a corrupted input datapoint x\u0303, we first initialize a Markov chain by drawing h(0)l \u223c q(hl|x). We then successively update all layers drawing h(m+1)l \u223c p\u2217(hl|h (m) l\u22121 ,h (m) l+1) like in Gibbs-sampling. We here perform multiple up- and downward sweeps until we consider the chain converged. Whenever we sample the bottom layer x(m+1) \u223c p\u2217(x|h(m)1 ), we keep the non-corrupted elements of x\u0303 fixed. Note that this method approximately samples reconstructions x \u223c p\u2217(x) which are consistent with x\u0303 \u2013 it does not provide MAP reconstruction which would maximize log p\u2217(x) given x\u0303."}, {"heading": "4 Experimental results", "text": "In this section we present experimental results obtained when applying the algorithm to various small and medium scale datasets. Our main goal is to ensure that the theoretical properties discussed in section 2 translate into a robust algorithm that gives competitive results even when used with simple sigmoid belief network layers as conditional distributions. All models were trained using RMSProp (see e.g. [12] for a description) with a mini-batch size of 100. We initialize all weights according to [13] and set all biases to -1. Our implementation is available at http://anonymized."}, {"heading": "4.1 UCI binary datasets", "text": "To ascertain that GMM with the importance sampling as training method works in general we applied it to the 8 binary datasets from the UCI dataset repository that where evaluated e.g. in [14]. The results are summarized in table 4.1. We report two upper bounds on the model negative loglikelihood: log p\u0302\u2217(x) and log p\u0302(x) (see equations 9 and 10). Both are conservative estimators of the model NLL because they overestimate by an unknown offset: the former because it is a conservative unbiased estimator for the likelihood of the approximate generative model p and not of the model p\u2217; the latter because it evaluates log p\u2217 under the assumption Z = 1."}, {"heading": "4.2 Binarized MNIST", "text": "We use the MNIST dataset that was binarized according to [17] and downloaded in binarized from [18]. We train a model with 6 hidden layers containing 800,800,600,450,350,150 latent variables. We set the learning rate to 3 \u2217 10\u22124 and use otherwise the same hyperparameters as in the previous experiments. The training of the model converges after \u2248 200 epochs. The importance sampling based estimators for log p\u0302 and log p\u0303\u2217 report a negative log-likelihood of 92.7 and 95.1 respectively (on the MNIST testset). Again, note that both overestimate the true model NLL by an unknown amount (see previous section).\nTo demonstrate that the model learned a reasonable concept of binarized digits we use algorithm 2 to draw samples. The results are visualized in Figure 1. The samples obtained from p\u2217(x) show almost always well defined digits with sharp (non-blurry) edges. But unfortunately they are biased: It draws more ones than any other digit. Note that the proposal samples drawn from p do not show any obvious bias. We nevertheless suspect that the failure of our model p\u2217 to correctly learn the prior distribution over different model classes is mostly due to p failing to learn an approximate variability of digit shapes: we observe that the estimator p\u0303\u2217(x) generally assigns a much higher probability to individual datapoints that resemble ones, than to datapoints that resemble other digit classes. This seems intuitively reasonable, because the variability within the class of ones is probably lower than the variability within other classes. One source of variability in binarized images is for example the blurry fringe of the strokes in the gray scale version of the original images. Digits with a shorter overall stroke length and with fewer gray pixels will result in binarized digits with lower variability. To compensate for the presumed lower variability within the class of ones (and their higher probability q(x)), p would have to propose more samples resembling other digits than one. With a more detailed analysis of this issue still outstanding we want to stress that the failure of our model to learn the correct digit-class distribution (10 classes) results in a relatively small loss in terms of log-likelihood compared to its ability to model a sharp and non-blurry distribution over the 784 pixels of the images.\nTo highlight the models ability to generate sharp digits we use the inpainting algorithm described in section 3.2 to reconstruct partially occluded images. To obtain the results in Figure 2 we ran 20 upand downward sweeps across all layers.\nA B\nIn Figure 1 C we show the histograms of the importance weights when we use p(x,h) as a proposal distribution for p\u2217(x,h) (top); and when we use q(h|x) as a proposal for p\u2217(h|x) (bottom). According to our goal formulated in section 1 both p and q should stay close to p\u2217. The weights of the former occur whenever we perform approximate inference (e.g. during learning); the weights of the latter occur when we sample from the model. Unsurprisingly we observe that the quality of both proposal distributions is roughly symmetric relative to the target distribution p\u2217. As a control we also plotted a histogram of the weights when p(x,h1|h2) is used as a proposal for p\u2217(x,h1|h2) \u2013 a sampling mechanism we never used for any of our algorithms, but that is totally symmetric to the histogram for the q(h|x) proposals. The resulting weight histogram was visually indistinguishable from the one we show in Figure 1 C (bottom)."}, {"heading": "4.3 Toronto Face Database", "text": "We also trained models on the 98058 examples from the unlabeled section of the Toronto face database (TFD, [19]). Each training example is of size 48\u00d748 pixels and we interpret the gray-level as Bernoulli probability for the bottommost layer. We observe that training proceeds rapidly during the first few epochs but mostly only learns the mean-face. During the next few hundred epochs training proceeds much slower but the log-likelihood bound log p\u0302\u2217(x) increases steadily. Figure 3 A shows random samples from a model with 1000,700,700,300 latent variables in 4 hidden layers. It was trained with a learning rate of 3 \u2217 10\u22125; all other hyperparameters were set to the same values as before. Figure 3 B shows the results from inpainting experiments with the very same model (50 up- and downward sweeps)."}, {"heading": "5 Conclusion and future work", "text": "We introduced a new scheme to construct probabilistic generative models which are automatically regularized to be close to approximate inference distributions we have at our disposal. Using the Bhattacharyya distance we derived a lower-bound on the log-likelihood and we demonstrated that the bound can be used to fit deep generative models with multiple layers of latent variables to complex training distributions. Note that our definition for p\u2217 forced us to choose a prior distribution q(x) which will be part of our generative model p\u2217(x,h). This is different from the typical variational approaches to train Helmholtz machines where we would think of q(h|x) solely as an approximate inference method given a training example x, and where q(x) would be the (empirical) training distribution \u2013 something we cannot assume because q(x) is part of our model p\u2217.\nBut many question remain open and are subject to future work: During our experiments we have not made any serious attempts to calculate the normalization constant Z. Knowing Z would enable us to report proper log-likelihood estimates (or tighter bounds) given some test data. For now we can just suspect that log 1/Z is significantly larger than 0 because our estimators for log p\u0302(x) and log p\u0303\u2217(x) report numbers that differ by multiple nats (for MNIST and TFD experiments). A first attempt to es-\ntimate Z could certainly be made by applying AIS [20] or RAISE [21]. Another direction for future research could involve semisupervised learning: The symmetric nature of the generative model p\u2217 (it is always close to the bottom-up and top-down directed models q and p) might make it suitable for learning tasks that require inference given changing sets of observed and hidden variables. And last but not least we have a wide range of potential choices for our parametrized conditional distributions. Assuming continuous latent variables for example and eventually choosing an alternative inference method might make p\u2217 a better suited model for some training distributions."}, {"heading": "Acknowledgments", "text": "We thank the developers of Theano [22] and Blocks [23] for their awesome work."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Now Publishers,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["Geoffrey E. Hinton", "Peter Dayan", "Brendan J. Frey", "Radford M. Neal"], "venue": "Science, 268:1558\u20131161,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "The Helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton", "Radford M Neal", "Richard S Zemel"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Varieties of helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": "Neural Networks,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Auto-encoding variational bayes", "author": ["Durk P. Kingma", "Max Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo J. Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In ICML\u20192014,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Reweighted wake-sleep", "author": ["Jorg Bornschein", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations (ICLR\u20192015),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli"], "venue": "CoRR, abs/1503.03585,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Unit tests for stochastic optimization", "author": ["Tom Schaul", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1312.6055,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS\u20192010,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["Hugo Larochelle", "Ian Murray"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A deep and tractable density estimator", "author": ["Benigno Uria Iain Murray", "Hugo Larochelle"], "venue": "In ICML\u20192014,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["Iain Murray", "Ruslan Salakhutdinov"], "venue": "In NIPS\u201908,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Binarized mnist dataset. http://www.cs.toronto.edu/ \u0303larocheh/ public/datasets/binarized_mnist/binarized_mnist_[train|valid|test] .amat, 2011. URL http://www.cs.toronto.edu/ \u0303larocheh/public/datasets/ binarized_mnist/binarized_mnist_train.amat", "author": ["Hugo Larochelle"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "The Toronto face dataset", "author": ["Joshua Susskind", "Adam Anderson", "Geoffrey E. Hinton"], "venue": "Technical Report UTML TR 2010-001, U. Toronto,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Annealed importance sampling", "author": ["Radford M. Neal"], "venue": "Statistics and Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Accurate and conservative estimates of MRF log-likelihood using reverse annealing", "author": ["Yuri Burda", "Roger B. Grosse", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1412.8566,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proc. SciPy,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This is especially true for models with multiple layers of deterministic or stochastic variables, which is unfortunate because it has been argued previously [1, 2] that deeper generative models have the potential to capture higher-level abstractions and thus generalize better.", "startOffset": 157, "endOffset": 163}, {"referenceID": 1, "context": "This is especially true for models with multiple layers of deterministic or stochastic variables, which is unfortunate because it has been argued previously [1, 2] that deeper generative models have the potential to capture higher-level abstractions and thus generalize better.", "startOffset": 157, "endOffset": 163}, {"referenceID": 2, "context": "With the Helmholtz machine [3, 4], a concept was introduced that proposed to not only fit a powerful but intractable generative model p(x,h) to the training data, but also to jointly train an approximate inference model q(h|x).", "startOffset": 27, "endOffset": 33}, {"referenceID": 3, "context": "With the Helmholtz machine [3, 4], a concept was introduced that proposed to not only fit a powerful but intractable generative model p(x,h) to the training data, but also to jointly train an approximate inference model q(h|x).", "startOffset": 27, "endOffset": 33}, {"referenceID": 2, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 103, "endOffset": 109}, {"referenceID": 4, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 103, "endOffset": 109}, {"referenceID": 5, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 253, "endOffset": 256}, {"referenceID": 7, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 307, "endOffset": 310}, {"referenceID": 8, "context": "This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).", "startOffset": 344, "endOffset": 347}, {"referenceID": 9, "context": "related to the Helmholtz machine, but similar in spirit is the approach described in [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "Analogous to a Deep Boltzmann Machine [11] we think of these as layers in a neural network with links between x and h1 on the one side, and h1 and h2 on the other side.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "Analogous to the parameter updates in reweighted wake-sleep (RWS, [9]) we can derive an important sampling based estimate for the parameter gradients and use them to optimize towards a", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "These properties are basically inherited from the RWS training algorithm [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "[12] for a description) with a mini-batch size of 100.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "We initialize all weights according to [13] and set all biases to -1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "in [14].", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Model ADULT CONNECT4 DNA MUSHROOMS NIPS-0-12 OCR-LETTERS RCV1 WEB auto regressive NADE[14] 13.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "39 EoNADE[15] 13.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "87 DARN[16] 13.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "83 RWS - NADE[9] 13.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "38 RWS - SBN[9] 13.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "We use the MNIST dataset that was binarized according to [17] and downloaded in binarized from [18].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "We use the MNIST dataset that was binarized according to [17] and downloaded in binarized from [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "We also trained models on the 98058 examples from the unlabeled section of the Toronto face database (TFD, [19]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "timate Z could certainly be made by applying AIS [20] or RAISE [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "timate Z could certainly be made by applying AIS [20] or RAISE [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "We thank the developers of Theano [22] and Blocks [23] for their awesome work.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "We thank the developers of Theano [22] and Blocks [23] for their awesome work.", "startOffset": 50, "endOffset": 54}], "year": 2017, "abstractText": "Unsupervised training of deep generative models containing latent variables and performing inference remains a challenging problem for complex, high dimensional distributions. One basic approach to this problem is the so called Helmholtz machine and it involves training an auxiliary model that helps to perform approximate inference jointly with the generative model which is to be fitted to the training data. The top-down generative model is typically realized as a directed model that starts from some prior at the top, down to the empirical distribution at the bottom. The approximate inference model runs in the opposite direction and is typically trained to efficiently infer high probability latent states given some observed data. Here we propose a new method, referred to as geometric mean matching (GMM), that is based on the idea that the generative model should be close to the class of distributions that can be modeled by our approximate inference distribution. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the target distribution we fit to the training data to be the geometric mean of these two. We present an upper-bound for the log-likelihood of this model and we show that optimizing this bound will pressure the model to stay close to the approximate inference distributions. In the experimental section we demonstrate that we can use this approach to fit deep generative models with many layers of hidden binary stochastic variables to complex and high dimensional training distributions.", "creator": "LaTeX with hyperref package"}}}