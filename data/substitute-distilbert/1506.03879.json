{"id": "1506.03879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "Leading Tree in DPCLUS and Its Impact on Building Hierarchies", "abstract": "this paper reveals branching tree structure as only alternate result of clustering methods fast search and find of density peaks ( dpclus ), so explores the power of using this tree to perform hierarchical selection. the array used to hold the index of the nearest higher - densitied object for each object can happen transformed upon a leading tree ( lt ), in which each parent node p causes its child nodes to join the same cluster as p itself, and the child nodes are sorted by their g values in descendant order to accelerate the disconnecting of root down each subtree. there are two major advantages with the lt : blocking is dramatically reducing the running time of assigning noncenter data points amongst their cluster systems, because the assigning method is tricked into just disconnecting the links from each center to its parent. the other is that the simple model for representing clusters is more informative. because attempts already check which objects are culturally likely to be classified as data in finer grained clustering, or where objects reach to its center via less jumps. experiment planning and analysis show the effectiveness and efficiency of the assigning process with an lt.", "histories": [["v1", "Fri, 12 Jun 2015 00:37:54 GMT  (715kb)", "http://arxiv.org/abs/1506.03879v1", "11 Pages, 5 figures. It is a very fundamental topic with respect to the research (clustering by fast search and find of density peaks)"], ["v2", "Mon, 15 Jun 2015 00:38:53 GMT  (714kb)", "http://arxiv.org/abs/1506.03879v2", "11 Pages, 5 figures. It is a very fundamental topic with respect to the research (clustering by fast search and find of density peaks)"]], "COMMENTS": "11 Pages, 5 figures. It is a very fundamental topic with respect to the research (clustering by fast search and find of density peaks)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ji xu", "guoyin wang"], "accepted": false, "id": "1506.03879"}, "pdf": {"name": "1506.03879.pdf", "metadata": {"source": "CRF", "title": "Leading Tree in DP_CLUS and Its Impact for Building Hierarchies", "authors": ["Ji Xu", "Guoyin Wang"], "emails": [], "sections": [{"heading": null, "text": "search and find of density peaks\u201d (DP CLUS), and explores the power of using this tree to perform hierarchical clustering. The array used to hold the index of the nearest higher-densitied object for each object can be transformed into a Leading Tree (LT), in which each parent node P leads its child nodes to join the same cluster as P itself, and the child nodes are sorted by their g values in descendant order to accelerate the disconnecting of root in each subtree. There are two major advantages with the LT: One is dramatically reducing the running time of assigning noncenter data points to their cluster ID, because the assigning process is turned into just disconnecting the links from each center to its parent. The other is that the tree model for representing clusters is more informative. Because we can check which objects are more likely to be selected as centers in finer grained clustering, or which objects reach to its center via less jumps. Experiment results and analysis show the effectiveness and high efficiency of the assigning process with an LT.\nKeywords: Leading Tree; Clustering; Density Peaks; Hierarchical clustering"}, {"heading": "1. Introduction", "text": "Clustering is a general methodology and a rich conceptual and algorithmic framework for data analysis and interpretation, which gathers the objects into groups [1]. Clustering can be divided into two categories regarding the structure of the result returned. Flat clustering (also called partition clustering in some literature) returns the clusters just in one layer, which is efficient and conceptually simple but has some drawbacks. One case is that the number of clusters is just too large to make good sense in the perspective of human cognition. Because according to Miller's \"seven plus or minus two\" theory, there are the 7 objects in the span of attention, and the 7 digits in the span of immediate memory [2]. Too large number of clusters doesn't offer people a good understanding of the data. Another drawback is some of the real-world datasets are hierarchically structured in nature, but flat clustering is unable to reflect the truth. These limitations can be broken through by hierarchical clustering, which outputs a hierarchy, a structure that is more informative than flat clustering [3].\nDP_CLUS is a flat clustering method able to efficiently and accurately cluster datasets of any\nshape with the aid of defining two simple measures: local density \u03c1 and the distance to the nearest neighbor with higher density \u03b4[4]. Since it was published in June, 2014, over 30 papers have been published (some are informally) to follow this research. The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth. Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.; Wang discussed its parameter the cut off distance dc setting [19]; Zhang extended it to cluster the datasets without density peaks [20]. There are some works only used its partial idea, such as neighbor to build a k-nearest neighborhood to explore the clusters in high dimension and large scale datasets [21]; or just mentioned it as a clustering method [22, 23].\nAmong the research works mentioned above, Teng Qiu\u2019s [24] and our method appear somehow alike. But the differences are obvious and significant: a) Qiu constructs an In-Tree with hopping the data points to the locations of their first transfer points; but we directly transform the array Nn, which holds the index of the nearest point of higher density and are computed in DP_CLUS, into a plain tree. In this tree, each node except the root is led by its parent to join the same cluster. Thus we call this tree a Leading Tree (LT). b) Qiu evolves the In-Tree constructed into the final stars to represent the clustering result; but we remain the LT unchanged as an intermediate result permanently ready for further hierarchical clustering with arbitrary number of clusters.\nIn the paper, we present the algorithms of transforming the array Nn into an LT and performing hierarchical clustering based on the LT data structure. Experiment results and analysis find the boundary condition of using an LT to save time in hierarchical clustering with DP_CLUS, and show that the clustering results described with trees are more informative than the original approach. The rest of the paper is organized as follows. Section 2 reviews DP_CLUS. Section 3 presents the algorithms of transforming Nn into an LT and performing hierarchical clustering based on the LT. The experimental setting, datasets and efficiency evaluation and discussion are described in Section 4, and we draw a conclusion in Section 5."}, {"heading": "2. Preliminaries", "text": "The method proposed is mainly based on [4], so we give a brief introduction to its idea and the algorithm here. The authors firstly made a sound intuitive assumption that no matter what the shape of clusters looks like, centers are always surrounded by non-center data points with lower density, and the distance between two centers are relatively long. Then two simple measures, namely local density (denoted as \u03c1) and minimal distance to data points with higher density\n(denoted as \u03b4 ), are employed to accomplish the clustering job.\nThe notations used to describe the algorithms in DP_CLUS and our method are listed in\nTable 1.\nTABLE 1: Notations in DP_CLUS\nDP_CLUS takes the distance matrix of a given dataset as input, and performing the following\nsteps:\n(1) Compute 1 2 { , ,..., } N    via cut-off kernel:\n, \\{ } \u03c7( )i i j c j I i d - d \n  , where 1, 0;\n\u03c7( )= 0, 0.\nx x\nx\n \n (1),\ncd is the cutoff distance; or via Gaussian kernel:\n, 2( )\n\\{ }\ni j c\nd d\ni\nj I i\ne \n\n  (2).\nEquation (2) is used in the authors\u2019 implementation.\n(2) Sort 1 2 { , ,..., } N    in descending order to get ;\n(3) Compute  via\n,\n, 2\nmin{ }, 2;\nmax{ }, 1.\ni j j\ni\ni j\nq q q\nj i q\nq q j\nd i\nd i  \n\n \n  \n\n(3),\nand write the index of the nearest neighbor with larger  in vector Nn;\n(4) Interactively choose the points with \u201canomalously large\u201d  and  as centers; (5) Assign each data points to the same cluster as its nearest neighbor with larger  . This process is done by scanning the vector  only once with referring to Nn.\nThe authors use a parameter named bord_rho to distinguish core and hallo data points of a\ncluster, but for simplicity, we omit the discussion of hallo and core in this study."}, {"heading": "3. Leading Tree as an Intermediate Result of DP_CLUS", "text": "The assigning process in DP_CLUS is based on two arrays: Nn and Q. Q holds the indices of data points sorted by their \u03c1 values in descending order. The whole data points are assigned to the clusters one by one in the \u03c1 \u2212descending order by referring to Nn . See Algorithm 1.\nAlgorithm 1. AssignDP_CLUS [4] Input: Nn and Q Output: Cluster label for every objects in the form of a array cl Step 1. Initialize every elements in cl with -1; Step 2. Label each center with a cluster id; Step 3. For each qi in Q do\nif cl[qi]==-1\ncl[qi]:= Nn[qi];\nEnd if\nEnd for\nStep 4. return cl;\nThe computational complexity for Algorithm 1 is 2\u00d7N, where N is the number of objects."}, {"heading": "3.1 Constructing Leading Tree", "text": "The LT of a give distance matrix can be constructed by directly transforming the array Nn and another array named GammaSortInds. Since Nn indicates its direct leading parent P for each object, then it is easy to find out the child nodes led by P. By adding the corresponding child nodes to all parent nodes, an LT is constructed. See Algorithm 2. Note that the child nodes of each parent are added in the \u03b3- descending order, so that when a newly upgrade center is detached from its parent, we simply remove the links from the parent to its first child. Thus the construction of the LT is accelerated. See Algorithm 2. Algorithm 2. Transforming the Nn into an LT Input: Nn and SortedGammaInds Output: An LT in the form of adjacent list AL Step 1. Initialize the adjacent List for each object; Step 2. For i from 2 to N\nChildID:= SortedGammaInds[i]; ParentID:= Nneigh[ChildID]; AL[ParentID].add(ChildID);\nEnd For\nStep 3. return AL;\nThe computational complexity for Algorithm 2 is 3\u00d7N, where N is the number of objects, and the data accessing on a linked list is less efficient than on an array."}, {"heading": "3.2 Using Leading Tree to build clustering Hierarchy", "text": "From the angle of center sets, hierarchical clustering can be regarded as a series of flat clustering taking different sets of objects as its centers. So, in this section, we only discuss the method of building hierarchy of clusters with an LT and each given set of centers on a corresponding layer. How to choose the centers to form a layer in the hierarchy is beyond the scope of this paper.\nWith an LT constructed, the assigning process is turned into just disconnecting the m-1 links\nfrom each center to its parent. See Algorithm 3.\nAlgorithm 3. Split the LT Input: An LT in the form of an adjacency list AL, Nn, and array of m centers C sorted by gamma\nvalue in descending order\nOutput: A forest to represent the clustering result Step 1. For i=2 to m\nroot= C [i]; parentID= Nneigh[root]; AL [parentID].RemoveFirst();\nEnd For\nStep 2. return AL;\nThe computational complexity of Algorithm 3 is 3\u00d7(m-1), where m is the number of centers."}, {"heading": "3.3 Example", "text": "We sample the longitude and latitude of 13 cities in north China to form an illustrative dataset (see Fig. 1), to illustrate the process and result of using LT to represent the CP_CLUS intermediate result and to cluster data points based on the constructed LT.\nUsing the algorithm of DB_CLUS, the intermediate results Nn, Q, and SortGammaInd for DS1 as well as the final result cl for DS1 are computed, as shown in Table 2.\n(Fig. 2a), and the result of clustering taking {13, 6, 11} as centers is presented in the form of a forest (Fig. 2b).\n13\n212\n3 1\n10\n6\n48\n7\n5 11\n9\n6\n48\n7\n5\n1\n13\n212\n3\n10\n11\n9\n(a) (b)\nFig.2.(a) The intermediate result of DS1 in the form of an LT, (b) Taking {13, 6, 11} as centers, the LT for DS1 is split into a forest. Each new tree represents a cluster taking the root as its center.\nThe physical structures to implement the logical structure of the LT and the split forest for DS1 are shown in Fig 3a and Fig. 3b respectively.\nThe process of retrieve the cluster objects may be a litter slow than sequential table, but it is\nmore informative. Unlike with the array cl, we can check which objects are more likely to be selected as centers in finer grained clustering, or which objects reach to its center via less jumps with the trees representing the clusters."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1 Datasets and running settings", "text": "The experiments are conducted on a personal computer with Intel i5-2430M CPU,8G RAM, Windows 7 64bit OS, and Eclipse programming environment with JDK 1.7.\nWe test our method on 3 datasets: two of them are synthetic and one real world dataset from\nUCI Machine Learning Repository. The first dataset (5Spherical) is generated by setting centers of five spheres and randomly sampling dots on the surface of the spheres, and then projecting the dots to the plane (see Fig. 4a). 5Spherical is designed to show that the whole dataset maybe clustered as five, four or two groups. The second dataset (5Spiral) are 5 spiral curves using Function (4):\n/ 8 cos( ) / 8 sin( ) x t t y t t            \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026(4)\nwhere \ud835\udc61 \u2208 (2,4\u03c0), and \u03b8 is the parameter controls the start point of the spiral curves. Among the five spirals, two spirals in two pairs are arranged relatively close to each other and one spiral is separated, thus they can be clustered into five, three or two groups (see Fig. 4b).The two artificial datasets are both of hierarchical structure, and of spherical and nonspherical shape respectively, with which the effectiveness and robustness of a hierarchical clustering method can be tested.\nThe brief information of the 6 datasets is tabulated in Table 3."}, {"heading": "4.2 Results and discussion", "text": "The running time of assigning objects to cluster with Nn, splitting the LTs to obtain the clusters and constructing the LTs is depicted in Fig. 5.\nFrom the figure, we can see that the running time of split an LT into a forest TSplitLT is always\nmuch shorter than that of assigning with Nn ( denoted as TAssign). But the cost is the time for constructing the LT (TConstrLT). If the number of the potential layers in the hierarchy Nl is small like the datasets we tested, then TConstrLT + Nl \u00d7TSplitLT > Nl \u00d7TAssign. However, if Nl is so large that Equation 5 is satisfied, then using an LT to perform hierarchical clustering will save computing time.\nConsLT\nAssign SplitLT\nT Nl\nT T   (5)\nBesides, with the clusters being described by trees, users can tell the differences among the\nobjects within the same cluster. For example, as shown in Fig 2.b we can say that object No.2 is closer to the center than No.1 and No.3, because there is an object (No.12) separating them from the center. This is more informative than the original approach of DP CLUS, which treat each objects in a cluster equally, just as most of the clustering methods."}, {"heading": "5. Conclusion", "text": "In this paper, we reveal the hidden tree structure of the intermediate result in DP_CLUS, and develop the algorithms to construct the leading tree (LT) and to perform hierarchical clustering with a series of center sets with the LT. Both theoretical analysis and experimental results show that the LT approach is much more efficient to assign the noncenter objects in DP_CLUS than the original Nn approach, and the clusters presented with trees can provide more information on the noncenters\u2019 potential to be selected as centers in future and on how many jumps a given object needs to reach its center. This discovery not only holds the potential to accelerate the assigning processing during the hierarchical clustering with DP_CLUS, but also provides us with a more profound understanding on the result structure of DP_CLUS."}, {"heading": "Acknowledgement", "text": "This work is partly supported by National Science and Technology Major Project\n(NO.2014ZX07104-006), the National Natural Science Foundation of China under Grant numbers of 61272060 and 61472056, and Natural Science Foundation Key Project of Chongqing of P. R. China under Grant No. CSTC2013jjB40003."}], "references": [{"title": "Knowledge-Based Clustering: From Data to Information Granules", "author": ["W. Pedrycz"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "The magical number seven, plus or minus two: some limits on our capacity for processing information", "author": ["G.A. Miller"], "venue": "Psychological review, vol. 63, no. 2, p. 81, 1956.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1956}, {"title": "Clustering by fast search and find of density peaks", "author": ["A. Rodriguez", "A. Laio"], "venue": "Science, vol. 344, no. 6191, pp. 1492\u20131496, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Demixed principal component analysis of population activity in higher cortical areas reveals independent representation of task parameters", "author": ["D. Kobak", "W. Brendel", "C. Constantinidis", "C.E. Feierstein", "A. Kepecs", "Z.F. Mainen", "R. Romo", "X.-L. Qi", "N. Uchida", "C.K. Machens"], "venue": "arXiv preprint arXiv:1410.6031, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Exemplar component analysis: A fast band selection method for hyperspectral imagery", "author": ["K. Sun", "X. Geng", "L. Ji"], "venue": "Geoscience and Remote Sensing Letters, IEEE, vol. 12, no. 5, pp. 998\u20131002, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "An efficient algorithm to perform local concerted movements of a chain molecule", "author": ["S. Zamuner", "A. Rodriguez", "F. Seno", "A. Trovato"], "venue": "PloS one, vol. 10, no. 3, p. e0118342, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Both piston-like and rotational motions are present in bacterial chemoreceptor signaling", "author": ["D. Yu", "X. Ma", "Y. Tu", "L. Lai"], "venue": "Scientific reports, vol. 5, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A new method to estimate ages of facial image for large database", "author": ["Y.-W. Chen", "D.-H. Lai", "H. Qi", "J.-L. Wang", "J.-X. Du"], "venue": "Multimedia Tools and Applications, pp. 1\u201319, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding your food soulmate", "author": ["A. Sy", "L. Liu", "B. Villanueva"], "venue": "2014. http://web.stanford.edu/ angelasy/ data/food soulmate paper.pdf.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering assisted fundamental matrix estimation", "author": ["H. Wu", "Y. Wan"], "venue": "arXiv preprint arXiv:1504.03409, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "High-speed multiparameter photophysical analyses of fluorophore libraries", "author": ["K.M. Dean", "L.M. Davis", "J.L. Lubbeck", "P. Manna", "P. Friis", "A.E. Palmer", "R. Jimenez"], "venue": "Analytical chemistry, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering sentences with density peaks for multi-document summarization", "author": ["Y. Zhang", "Y. Xia", "Y. Liu", "P. IMSL", "W. Wang"], "venue": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pp. 1262\u20131267, June 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "High density-focused uncertainty sampling for active learning over evolving stream data", "author": ["D. Ienco", "B. Pfahringer", "I. Zliobaite"], "venue": "Proceedings of the 3rd International Workshop on Big Data, Streams and Heterogeneous  Source Mining: Algorithms, Systems, Programming Models and Applications, pp. 133\u2013148, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A synthetic minority oversampling method based on local densities in low-dimensional space for imbalanced learning", "author": ["Z. Xie", "L. Jiang", "T. Ye", "X. Li"], "venue": "Database Systems for Advanced Applications, pp. 3\u201318, Springer, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A generalized affinity propagation clustering algorithm for nonspherical cluster discovery", "author": ["T. Qiu", "Y. Li"], "venue": "arXiv preprint arXiv: 1501.04318, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Solving the inverse ising problem by mean-field methods in a clustered phase space with many states", "author": ["A. Decelle", "F. Ricci-Tersenghi"], "venue": "arXiv preprint arXiv:1501.03034, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "K-means clustering based on density for scene image classification", "author": ["K. Xie", "J. Wu", "W. Yang", "C. Sun"], "venue": "Proceedings of the 2015 Chinese Intelligent Automation Conference, pp. 379\u2013386, Springer, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Comment on\u201d clustering by fast search and find of density peaks", "author": ["S. Wang", "D. Wang", "C. Li", "Y. Li"], "venue": "arXiv preprint arXiv: 1501.04267, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Extended fast search clustering algorithm: widely density clusters, no density peaks", "author": ["W. Zhang", "J. Li"], "venue": "arXiv preprint arXiv: 1505.05610, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient cluster detection by ordered neighborhoods", "author": ["E. Aksehirli", "E. M \u0308 uller", "B. Goethals"], "venue": "http://win.ua.ac.be/ adrem/bibrem/pubs/clon.pdf.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 0}, {"title": "Spike sorting for large, dense electrode arrays", "author": ["C. Rossant", "S.N. Kadir", "D.F. Goodman", "J. Schulman", "M. Belluscio", "G. Buzsaki", "K.D. Harris"], "venue": "bioRxiv, p. 015198, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "An improved community detection method in massive social networks", "author": ["Y. Yao", "B. Li", "L. Peng", "Z. Liu"], "venue": "2015 International Conference on Automation, Mechanical Control and Computational Engineering, Atlantis Press, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Clustering is a general methodology and a rich conceptual and algorithmic framework for data analysis and interpretation, which gathers the objects into groups [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Because according to Miller's \"seven plus or minus two\" theory, there are the 7 objects in the span of attention, and the 7 digits in the span of immediate memory [2].", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "DP_CLUS is a flat clustering method able to efficiently and accurately cluster datasets of any shape with the aid of defining two simple measures: local density \u03c1 and the distance to the nearest neighbor with higher density \u03b4[4].", "startOffset": 225, "endOffset": 228}, {"referenceID": 3, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 175, "endOffset": 178}, {"referenceID": 6, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 205, "endOffset": 208}, {"referenceID": 7, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 245, "endOffset": 248}, {"referenceID": 8, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 274, "endOffset": 278}, {"referenceID": 9, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 329, "endOffset": 333}, {"referenceID": 10, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 354, "endOffset": 358}, {"referenceID": 11, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 414, "endOffset": 418}, {"referenceID": 12, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 172, "endOffset": 176}, {"referenceID": 15, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 263, "endOffset": 267}, {"referenceID": 17, "context": "; Wang discussed its parameter the cut off distance dc setting [19]; Zhang extended it to cluster the datasets without density peaks [20].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "; Wang discussed its parameter the cut off distance dc setting [19]; Zhang extended it to cluster the datasets without density peaks [20].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "There are some works only used its partial idea, such as neighbor to build a k-nearest neighborhood to explore the clusters in high dimension and large scale datasets [21]; or just mentioned it as a clustering method [22, 23].", "startOffset": 167, "endOffset": 171}, {"referenceID": 20, "context": "There are some works only used its partial idea, such as neighbor to build a k-nearest neighborhood to explore the clusters in high dimension and large scale datasets [21]; or just mentioned it as a clustering method [22, 23].", "startOffset": 217, "endOffset": 225}, {"referenceID": 21, "context": "There are some works only used its partial idea, such as neighbor to build a k-nearest neighborhood to explore the clusters in high dimension and large scale datasets [21]; or just mentioned it as a clustering method [22, 23].", "startOffset": 217, "endOffset": 225}, {"referenceID": 2, "context": "The method proposed is mainly based on [4], so we give a brief introduction to its idea and the algorithm here.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "AssignDP_CLUS [4]", "startOffset": 14, "endOffset": 17}], "year": 2015, "abstractText": "This paper reveals the tree structure as an intermediate result of \u201dclustering by fast search and find of density peaks\u201d (DP CLUS), and explores the power of using this tree to perform hierarchical clustering. The array used to hold the index of the nearest higher-densitied object for each object can be transformed into a Leading Tree (LT), in which each parent node P leads its child nodes to join the same cluster as P itself, and the child nodes are sorted by their g values in descendant order to accelerate the disconnecting of root in each subtree. There are two major advantages with the LT: One is dramatically reducing the running time of assigning noncenter data points to their cluster ID, because the assigning process is turned into just disconnecting the links from each center to its parent. The other is that the tree model for representing clusters is more informative. Because we can check which objects are more likely to be selected as centers in finer grained clustering, or which objects reach to its center via less jumps. Experiment results and analysis show the effectiveness and high efficiency of the assigning process with an LT.", "creator": "Microsoft\u00ae Word 2010"}}}