{"id": "1506.04364", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Localized Multiple Kernel Learning---A Convex Approach", "abstract": "we propose a localized approach to concurrent player learning that, in contrast to prevalent computation, can be formulated as a convex optimization principle over a given pixel structure. from which we obtain the first generalization error bounds for localized multiple problem graphs and derive an efficient optimization algorithm based on the fenchel dual norm. experiments on real - world datasets from the application domains of computational biology and computer vision show that the convex approach to localized multiple kernel learning can achieve higher prediction accuracies than its global and non - fixed local counterparts.", "histories": [["v1", "Sun, 14 Jun 2015 09:11:13 GMT  (102kb,D)", "https://arxiv.org/abs/1506.04364v1", null], ["v2", "Thu, 13 Oct 2016 00:54:24 GMT  (107kb,D)", "http://arxiv.org/abs/1506.04364v2", "to appear in ACML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yunwen lei", "alexander binder", "\\\"ur\\\"un dogan", "marius kloft"], "accepted": false, "id": "1506.04364"}, "pdf": {"name": "1506.04364.pdf", "metadata": {"source": "CRF", "title": "Localized Multiple Kernel Learning\u2014A Convex Approach", "authors": ["Yunwen Lei", "Alexander Binder", "\u00dcr\u00fcn Dogan", "Marius Kloft"], "emails": ["yunwelei@cityu.edu.hk", "alexander_binder@sutd.edu.sg", "udogan@microsoft.com", "kloft@hu-berlin.de"], "sections": [{"heading": null, "text": "Keywords: Multiple kernel learning, Localized algorithms, Generalization analysis"}, {"heading": "1 Introduction", "text": "Kernel-based methods such as support vector machines have found diverse applications due to their distinct merits such as the descent computational complexity, high usability, and the solid mathematical foundation [e.g., 44]. The performance of such algorithms, however, crucially depends on the involved kernel function as it intrinsically specifies the feature space where the learning process is implemented, and thus provides a similarity measure on the input space. Yet in the standard setting of these methods the choice of the involved kernel is typically left to the user.\nA substantial step toward the complete automatization of kernel-based machine learning is achieved in Lanckriet et al. [32], who introduce the multiple kernel learning (MKL) framework [15]. MKL offers a principal way of encoding complementary information with distinct base kernels and automatically learning an optimal combination of those [45]. MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56]. While early sparsity-inducing approaches failed to live up to its expectations in terms of improvement \u2217yunwelei@cityu.edu.hk \u2020alexander_binder@sutd.edu.sg \u2021udogan@microsoft.com \u00a7kloft@hu-berlin.de\nar X\niv :1\n50 6.\n04 36\n4v 2\n[ cs\n.L G\n] 1\n3 O\nover uniform combinations of kernels [cf. 9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].\nCurrently, most of the existing algorithms fall into the global setting of MKL, in the sense that all input instances share the same kernel weights. However, this ignores the fact that instances may require sampleadaptive kernel weights.\nFor instance, consider the two images of a horses given to the right. Multiple kernels can be defined, capturing the shapes in the image and the color distribution over various channels. On the image to the left, the depicted horse and the image backgrounds exhibit distinctly different color distributions, while for the image to the right the contrary is the case. Hence, a color kernel is more significant to detect a horse in the image to the left than for the image the right. This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].\nExisting approaches to localized MKL (reviewed in Section 1.1) optimize non-convex objective functions. This puts their generalization ability into doubt. Indeed, besides the recent work by [34], the generalization performance of localized MKL algorithms (as measured through large-deviation bounds) is poorly understood, which potentially could make these algorithms prone to overfitting. Further potential disadvantages of non-convex localized MKL approaches include computationally difficulty in finding good local minima and the induced lack of reproducibility of results (due to varying local optima).\nThis paper presents a convex formulation of localized multiple kernel learning, which is formulated as a single convex optimization problem over a precomputed cluster structure, obtained through a potentially convex or non-convex clustering method. We derive an efficient optimization algorithm based on Fenchel duality. Using Rademacher complexity theory, we establish large-deviation inequalities for localized MKL, showing that the smoothness in the cluster membership assignments crucially controls the generalization error. Computational experiments on data from the domains of computational biology and computer vision show that the proposed convex approach can achieve higher prediction accuracies than its global and nonconvex local counterparts (up to +5% accuracy for splice site detection)."}, {"heading": "1.1 Related Work", "text": "G\u00f6nen and Alpaydin [14] initiate the work on localized MKL by introducing gating models\nf(x) = M\u2211 m=1 \u03b7m(x; v)\u3008wm, \u03c6m(x)\u3009+ b, \u03b7m(x; v) \u221d exp(\u3008vm, x\u3009+ vm0)\nto achieve local assignments of kernel weights, resulting in a non-convex MKL problem. To not overly respect individual samples, Yang et al. [55] give a group-sensitive formulation of localized MKL, where kernel weights vary at, instead of the example level, the group level. Mu and Zhou [42] also introduce a non-uniform MKL allowing the kernel weights to vary at the cluster-level and tune the kernel weights under the graph embedding framework. Han and Liu [19] built on G\u00f6nen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class. Li et al. [36] propose a multiple kernel clustering method by maximizing local kernel alignments. Liu et al. [37] present sample-adaptive approaches to localized MKL, where kernels can be switched on/off at the example level by introducing a latent binary vector for each individual sample, which and the kernel weights are then jointly optimized via margin maximization principle. Moeller et al.\n[41] present a unified viewpoint of localized MKL by interpreting gating functions in terms of local reproducing kernel Hilbert spaces acting on the data. All the aforementioned approaches to localized MKL are formulated in terms of non-convex optimization problems, and deep theoretical foundations in the form of generalization error or excess risk bounds are unknown. Although Cortes et al. [11] present a convex approach to MKL based on controlling the local Rademacher complexity, the meaning of locality is different in Cortes et al. [11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning. Liu et al. [38] extend the idea of sample-adaptive MKL to address the issue with missing kernel information on some examples. More recently, Lei et al. [34] propose a MKL method by decoupling the locality structure learning with a hard clustering strategy from optimizing the parameters in the spirit of multi-task learning. They also develop the first generalization error bounds for localized MKL."}, {"heading": "2 Convex Localized Multiple Kernel Learning", "text": ""}, {"heading": "2.1 Problem setting and notation", "text": "Suppose that we are given n training samples (x1, y1), . . . , (xn, yn) that are partitioned into l disjoint clusters S1, . . . , Sl in a probabilistic manner, meaning that, for each cluster Sj , we have a function cj : X \u2192 [0, 1] indicating the likelihood of x falling into cluster j, i.e., \u2211 j\u2208Nl cj(x) = 1 for all x \u2208 X . Here, for any d \u2208 N, we introduce the notation Nd = {1, . . . , d}. Suppose that we are given M base kernels k1, . . . , kM with km(x, x\u0303) = \u3008\u03c6m(x), \u03c6m(x\u0303)\u3009km , corresponding to linear models fj(x) = \u3008wj , \u03c6(x)\u3009+b =\u2211 m\u2208NM \u3008w (m) j , \u03c6m(x)\u3009 + b, where wj = (w (1) j , . . . , w (M) j ) and \u03c6 = (\u03c61, . . . , \u03c6M ). Then we consider the following proposed model, which is a weighted combination of these l local models:\nf(x) = \u2211 j\u2208Nl cj(x)fj(x) = \u2211 j\u2208Nl cj(x) [ \u2211 m\u2208NM \u3008w(m)j , \u03c6m(x)\u3009 ] + b. (1)"}, {"heading": "2.2 Proposed convex localized MKL method", "text": "Using the above notation, the proposed convex localized MKL model can be formulated as follows.\nProblem 1 (CONVEX LOCALIZED MULTIPLE KERNEL LEARNING (CLMKL)\u2014PRIMAL). Let C > 0 and p \u2265 1. Given a loss function `(t, y) : R\u00d7Y \u2192 R convex w.r.t. the first argument and cluster likelihood functions cj : X \u2192 [0, 1], j \u2208 Nl, solve\ninf w,t,\u03b2,b \u2211 j\u2208Nl \u2211 m\u2208NM \u2016w(m)j \u201622 2\u03b2jm + C \u2211 i\u2208Nn `(ti, yi)\ns.t. \u03b2jm \u2265 0, \u2211 m\u2208NM\n\u03b2pjm \u2264 1 \u2200j \u2208 Nl,m \u2208 NM\u2211 j\u2208Nl cj(xi) [ \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009 ] + b = ti, \u2200i \u2208 Nn.\n(P)\nThe core idea of the above problem is to use cluster likelihood functions for each example and separate `p-norm constraint on the kernel weights \u03b2j := (\u03b2j1, . . . , \u03b2jM ) for each cluster j [31] . Thus each instance can obtain separate kernel weights. The above problem is convex, since a quadratic over a linear function is convex [e.g., 6, p.g. 89]. Note that Slater\u2019s condition can be directly checked, and thus strong duality holds."}, {"heading": "2.3 Dualization", "text": "In this section we derive a dual representation of Problem 1. We consider two levels of duality: a partially dualized problem, with fixed kernel weights \u03b2jm, and the entirely dualized problem with respect to all occurring primal variables. From the former we derive an efficient two-step optimization scheme (Section 3). The latter allows us to compute the duality gap and thus to obtain a sound stopping condition for the proposed algorithm. We focus on the entirely dualized problem here. The partial dualization is deferred to Supplemental Material C.\nDual CLMKL Optimization Problem Forwj = (w (1) j , . . . , w (M) j ), we define the `2,p-norm by \u2016wj\u20162,p := \u2016(\u2016w(1)j \u2016k1 , . . . , \u2016w (M) j \u2016kM )\u2016p = ( \u2211 m\u2208NM \u2016w (m) j \u2016 p km ) 1 p . For a function h, we denote by h\u2217(x) = sup\u00b5[x\n>\u00b5\u2212 h(\u00b5)] its Fenchel-Legendre conjugate. This results in the following dual.\nProblem 2 (CLMKL\u2014DUAL). The dual problem of (P) is given by\nsup\u2211 i\u2208Nn \u03b1i=0\n{ \u2212 C \u2211 i\u2208Nn `\u2217(\u2212\u03b1i C , yi)\u2212 1 2 \u2211 j\u2208Nl \u2225\u2225\u2225( \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) )M m=1 \u2225\u2225\u22252 2, 2pp\u22121 } . (D)\nDualization. Using Lemma A.2 from Supplemental Material A.1 to express the optimal \u03b2jm in terms of w\n(m) j , the problem (P) is equivalent to\ninf w,t,b\n1\n2 \u2211 j\u2208Nl ( \u2211 m\u2208NM \u2016w(m)j \u2016 2p p+1 2 ) p+1 p + C \u2211 i\u2208Nn `(ti, yi)\ns.t. \u2211 j\u2208Nl [ cj(xi) \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009 ] + b = ti, \u2200i \u2208 Nn. (2)\nIntroducing Lagrangian multipliers \u03b1i, i \u2208 Nn, the Lagrangian saddle problem of Eq. (2) is\nsup \u03b1 inf w,t,b\n1\n2 \u2211 j\u2208Nl ( \u2211 m\u2208NM \u2016w(m)j \u2016 2p p+1 2 ) p+1 p + C \u2211 i\u2208Nn `(ti, yi)\u2212 \u2211 i\u2208Nn \u03b1i (\u2211 j\u2208Nl cj(xi) \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009+ b\u2212 ti )\n= sup \u03b1\n{ \u2212 C \u2211 i\u2208Nn sup ti [\u2212`(ti, yi)\u2212 1 C \u03b1iti]\u2212 sup b \u2211 i\u2208Nn \u03b1ib\u2212\nsup w [ \u2211 j\u2208Nl \u2211 m\u2208NM \u2329 w (m) j , \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) \u232a \u2212 1 2 \u2211 j\u2208Nl ( \u2211 m\u2208NM \u2016w(m)j \u2016 2p p+1 2 ) p+1 p ]}\n(3)\ndef = sup\u2211\ni\u2208Nn \u03b1i=0\n{ \u2212 C \u2211 i\u2208Nn `\u2217(\u2212\u03b1i C , yi)\u2212 \u2211 j\u2208Nl [1 2 \u2225\u2225( \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) )M m=1 \u2225\u22252 2, 2pp+1 ]\u2217}\nThe result (2) now follows by recalling that for a norm \u2016 \u00b7 \u2016, its dual norm \u2016 \u00b7 \u2016\u2217 is defined by \u2016x\u2016\u2217 = sup\u2016\u00b5\u2016=1\u3008x, \u00b5\u3009 and satisfies: ( 12\u2016 \u00b7 \u2016 2)\u2217 = 12\u2016 \u00b7 \u2016 2 \u2217 [6]. Furthermore, it is straightforward to show that \u2016 \u00b7 \u20162, 2pp\u22121 is the dual norm of \u2016 \u00b7 \u20162, 2pp+1 ."}, {"heading": "2.4 Representer Theorem", "text": "We can use the above derivation to obtain a lower bound on the optimal value of the primal optimization problem (P), from which we can compute the duality gap using the theorem below. The proof is given in Supplemental Material A.2.\nTheorem 3 (REPRESENTER THEOREM). For any dual variable (\u03b1i)ni=1 in (D), the optimal primal variable {w(m)j (\u03b1)} l,M j,m=1 in the Lagrangian saddle problem (3) can be represented as\nw (m) j (\u03b1)= [ \u2211 m\u0303\u2208NM \u2016 \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m\u0303(xi)\u2016 2p p\u22121 2 ]\u2212 1p \u2225\u2225\u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) \u2225\u2225 2p\u22121 2 [\u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) ] ."}, {"heading": "2.5 Support-Vector Classification", "text": "For the hinge loss, the Fenchel-Legendre conjugate becomes `\u2217(t, y) = ty (a function of t) if \u22121 \u2264 t y \u2264 0 and \u221e elsewise. Hence, for each i, the term `\u2217(\u2212\u03b1iC , yi) translates to \u2212 \u03b1i Cyi\n, provided that 0 \u2264 \u03b1iyi \u2264 C. With a variable substitution of the form \u03b1newi = \u03b1i yi , the complete dual problem (D) reduces as follows.\nProblem 4 (CLMKL\u2014SVM FORMULATION). For the hinge loss, the dual CLMKL problem (D) is given by:\nsup \u03b1:0\u2264\u03b1\u2264C, \u2211 i\u2208Nn \u03b1iyi=0 \u2212 1 2 \u2211 j\u2208Nl \u2225\u2225\u2225( \u2211 i\u2208Nn \u03b1iyicj(xi)\u03c6m(xi) )M m=1 \u2225\u2225\u22252 2, 2pp\u22121 + \u2211 i\u2208Nn \u03b1i, (4)\nA corresponding formulation for support-vector regression is given in Supplemental Material B."}, {"heading": "3 Optimization Algorithms", "text": "As pioneered in Sonnenburg et al. [45], we consider here a two-layer optimization procedure to solve the problem (P) where the variables are divided into two groups: the group of kernel weights {\u03b2jm}l,Mj,m=1 and the group of weight vectors {w(m)j } l,M j,m=1. In each iteration, we alternatingly optimize one group of variables while fixing the other group of variables. These iterations are repeated until some optimality conditions are satisfied. To this aim, we need to find efficient strategies to solve the two subproblems.\nIt is not difficult to show (cf. Supplemental Material C) that, given fixed kernel weights \u03b2 = (\u03b2jm), the CLMKL dual problem is given by\nsup \u03b1: \u2211 i\u2208Nn \u03b1i=0 \u22121 2 \u2211 j\u2208Nl \u2211 m\u2208NM \u03b2jm \u2225\u2225\u2225 \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) \u2225\u2225\u22252 2 \u2212 C \u2211 i\u2208Nn `\u2217(\u2212\u03b1i C , yi), (5)\nwhich is a standard SVM problem using the kernel\nk\u0303(xi, xi\u0303) := \u2211 m\u2208NM \u2211 j\u2208Nl \u03b2jmcj(xi)cj(xi\u0303)km(xi, xi\u0303) (6)\nThis allows us to employ very efficient existing SVM solvers [8]. In the degenerate case with cj(x) \u2208 {0, 1}, the kernel k\u0303 would be supported over those sample pairs belonging to the same cluster.\nNext, we show that, the subproblem of optimizing the kernel weights for fixed w(m)j and b has a closedform solution.\nProposition 5 (SOLUTION OF THE SUBPROBLEM W.R.T. THE KERNEL WEIGHTS). Given fixed w(m)j and b, the minimal \u03b2jm in optimization problem (P) is attained for\n\u03b2jm = \u2016w(m)j \u2016 2 p+1 2 ( \u2211 k\u2208NM \u2016w(k)j \u2016 2p p+1 2 )\u2212 1p . (7)\nWe defer the detailed proof to Supplemental Material A.3 due to lack of space. To apply Proposition 5 for updating \u03b2jm, we need to compute the norm of w (m) j , and this can be accomplished by the following representation of w(m)j given fixed \u03b2jm: (cf. Supplemental Material C)\nw (m) j = \u03b2jm \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi). (8)\nThe prediction function is then derived by plugging the above representation into Eq. (1). The resulting optimization algorithm for CLMKL is shown in Algorithm 1. The algorithm alternates between solving an SVM subproblem for fixed kernel weights (Line 4) and updating the kernel weights in a closed-form manner (Line 6). To improve the efficiency, we start with a crude precision and gradually improve the precision of solving the SVM subproblem. The proposed optimization approach can potentially be extended to an interleaved algorithm where the optimization of the MKL step is directly integrated into the SVM solver. Such a strategy can increase the computational efficiency by up to 1-2 orders of magnitude (cf. [45] Figure 7 in Kloft et al. [31]). The requirement to compute the kernel k\u0303 at each iteration can be further relaxed by updating only some randomly selected kernel elements.\nAlgorithm 1: Training algorithm for convex localized multiple kernel learning (CLMKL). input: examples {(xi, yi)ni=1} \u2282 ( X \u00d7 {\u22121, 1} )n together with the likelihood functions {cj(x)}lj=1, M base kernels k1, . . . , kM .\n1 initialize \u03b2jm = p \u221a 1/M,w (m) j = 0 for all j \u2208 Nl,m \u2208 NM 2 while Optimality conditions are not satisfied do 3 calculate the kernel matrix k\u0303 by Eq. (6) 4 compute \u03b1 by solving canonical SVM with k\u0303 5 compute \u2016w(m)j \u2016 2 2 for all j,m with w (m) j given by Eq. (8) 6 update \u03b2jm for all j,m according to Eq. (7) 7 end\nAn alternative strategy would be to directly optimize (2) (without the need of a two-step wrapper approach). Such an approach has been presented in Sun et al. [49] in the context of `p-norm MKL."}, {"heading": "3.1 Convergence Analysis of the Algorithm", "text": "The theorem below, which is proved in Supplemental Material A.4, shows convergence of Algorithm 1. The core idea is to view Algorithm 1 as an example of the classical block coordinate descent (BCD) method, convergence of which is well understood.\nTheorem 6 (CONVERGENCE ANALYSIS OF ALGORITHM 1). Assume that\n(B1) the feature map \u03c6m(x) is of finite dimension, i.e, \u03c6m(x) \u2208 Rem , em <\u221e,\u2200m \u2208 NM (B2) the loss function ` is convex, continuous w.r.t. the first argument and `(0, y) <\u221e,\u2200y \u2208 Y (B3) any iterate \u03b2jm traversed by Algorithm 1 has \u03b2jm > 0 (B4) the SVM computation in line 4 of Algorithm 1 is solved exactly in each iteration.\nThen, any limit point of the sequence traversed by Algorithm 1 minimizes the problem (P)."}, {"heading": "3.2 Runtime Complexity Analysis", "text": "At each iteration of the training stage, we need O(n2Ml) operations to calculate the kernel (6), O(n2ns) operations to solve a standard SVM problem, O(Mln2s) operations to calculate the norm according to the representation (8) and O(Ml) operations to update the kernel weights. Thus, the computational cost at each iteration is O(n2Ml). The time complexity at the test stage is O(ntnsMl). Here, ns and nt are the number of support vectors and test points, respectively."}, {"heading": "4 Generalization Error Bounds", "text": "In this section we present generalization error bounds for our approach. We give a purely data-dependent bound on the generalization error, which is obtained using Rademacher complexity theory [3]. To start with, our basic strategy is to plug the optimal \u03b2jm established in Eq. (7) into (P), so as to equivalently rewrite (P) as a block-norm regularized problem as follows:\nmin w,b\n1\n2 \u2211 j\u2208Nl [ \u2211 m\u2208NM \u2016w(m)j \u2016 2p p+1 2 ] p+1 p + C \u2211 i\u2208Nn ` (\u2211 j\u2208Nl cj(xi) [ \u2211 m\u2208NM \u3008w(m)j ,\u03c6m(xi)\u3009 ] + b, yi ) . (9)\nSolving (9) corresponds to empirical risk minimization in the following hypothesis space:\nHp,D := Hp,D,M = { fw : x \u2192 \u2211 j\u2208Nl cj(xi) [ \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009 ] : \u2211 j\u2208Nl \u2016wj\u201622, 2pp+1 \u2264 D } .\nThe following theorem establishes the Rademacher complexity bounds for the function classHp,D, from which we derive generalization error bounds for CLMKL in Theorem 9. The proofs of the Theorems 8, 9 are given in Supplemental Material A.5.\nDefinition 7. For a fixed sample S = (x1, . . . , xn), the empirical Rademacher complexity of a hypothesis space H is defined as\nR\u0302n(H) := E\u03c3 sup f\u2208H\n1\nn \u2211 i\u2208Nn \u03c3if(xi),\nwhere the expectation is taken w.r.t. \u03c3 = (\u03c31, . . . , \u03c3n)> with \u03c3i, i \u2208 Nn, being a sequence of independent uniform {\u00b11}-valued random variables.\nTheorem 8 (CLMKL RADEMACHER COMPLEXITY BOUNDS). The empirical Rademacher complexity of Hp,D can be controlled by\nR\u0302n(Hp,D) \u2264 \u221a D\nn inf\n2\u2264t\u2264 2pp\u22121 ( t \u2211 j\u2208Nl \u2225\u2225\u2225( \u2211 i\u2208Nn c2j (xi)km(xi, xi) )M m=1 \u2225\u2225\u2225 t 2 )1/2 . (10)\nIf, additionally, km(x, x) \u2264 B for any x \u2208 X and any m \u2208 NM , then we have\nR\u0302n(Hp,D) \u2264 \u221a DB\nn inf\n2\u2264t\u2264 2pp\u22121\n( tM 2 t \u2211 j\u2208Nl \u2211 i\u2208Nn c2j (xi) )1/2 .\nTheorem 9 (CLMKL GENERALIZATION ERROR BOUNDS). Assume that km(x, x) \u2264 B, \u2200m \u2208 NM , x \u2208 X . Suppose the loss function ` is L-Lipschitz and bounded by B`. Then, the following inequality holds with probability larger than 1\u2212 \u03b4 over samples of size n for all classifiers h \u2208 Hp,D:\nE`(h) \u2264 E`,z(h) +B`\n\u221a log(2/\u03b4)\n2n + 2\n\u221a DB\nn inf\n2\u2264t\u2264 2pp\u22121\n( tM 2 t [ \u2211 j\u2208Nl \u2211 i\u2208Nn c2j (xi) ])1/2 ,\nwhere E`(h) := E[`(h(x), y)] and E`,z(h) := 1n \u2211 i\u2208Nn `(h(xi), yi).\nThe above bound enjoys a mild dependence on the number of kernels. One can show (cf. Supplemental Material A.5) that the dependence is O(logM) for p \u2264 (logM \u2212 1)\u22121 logM and O(M p\u22121 2p ) otherwise. In particular, the dependence is logarithmically for p = 1 (sparsity-inducing CLMKL). These dependencies recover the best known results for global MKL algorithms in Cortes et al. [10], Kloft and Blanchard [25], Kloft et al. [31].\nThe bounds of Theorem 8 exhibit a strong dependence on the likelihood functions, which inspires us to derive a new algorithmic strategy as follows. Consider the special case where cj(x) takes values in {0, 1} (hard cluster membership assignment), and thus the term determining the bound has \u2211 j\u2208Nl \u2211 m\u2208NM c 2 j (xi) = n. On the other hand, if cj(x) \u2261 1l , j \u2208 Nl (uniform cluster membership assignment), we have the favorable term \u2211 j\u2208N \u2211 i\u2208Nn c 2 j (xi) = n l . This motivates us to introduce a parameter \u03c4 controlling the complexity of the bound by considering likelihood functions of the form\ncj(x) \u221d exp(\u2212\u03c4dist2(x, Sj)), (11)\nwhere dist(x, Sj) is the distance between the example x and the cluster Sj . By letting \u03c4 = 0 and \u03c4 = \u221e, we recover uniform and hard cluster assignments, respectively. Intermediate values of \u03c4 correspond to more balanced cluster assignments. As illustrated by Theorem 8, by tuning \u03c4 we optimally adjust the resulting models\u2019 complexities."}, {"heading": "5 Empirical Analysis and Applications", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "We implement the proposed convex localized MKL (CLMKL) algorithm in MATLAB and solve the involved canonical SVM problem with LIBSVM [8]. The clusters {S1, . . . , Sl} are computed through kernel k-means [e.g., 12], but in principle other clustering methods (including convex ones such as Hocking et al. [21]) could be used. To further diminish k-means\u2019 potential fluctuations (which are due to random initialization of the cluster means), we repeat kernel k-means t times, and choose the one with minimal clustering error (the summation of the squared distance between the examples and the associated nearest cluster) as the final partition {S1, . . . , Sl}. To tune the parameter \u03c4 in (11) in a uniform manner, we introduce the notation\nAE(\u03c4) := 1\nnl \u2211 i\u2208Nn \u2211 j\u2208Nl exp(\u2212\u03c4dist2(xi, Sj)) maxj\u0303\u2208Nl exp(\u2212\u03c4dist 2(xi, Sj\u0303))\nto measure the average evenness (or average excess over hard partition) of the likelihood function. It can be checked that AE(\u03c4) is a strictly decreasing function of \u03c4 , taking value 1 at the point \u03c4 = 0 and l\u22121 at the point \u03c4 = \u221e. Instead of tuning the parameter \u03c4 directly, we propose to tune the average excess/evenness over a subset in [l\u22121, 1]. The associated parameter \u03c4 are then fixed by the standard binary search algorithm.\nWe compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case. We optimize `pnorm MKL and CLMKL until the relative duality gap drops below 0.001. The calculation of the gradients in LMKL [14] requires O(n2M2d) operations, which scales poorly, and the definition of the gating model\nrequires the information of primitive features, which is not available for the biological applications studied below, all of which involve string kernels. In Supplemental Material D, we therefore give a fast and general formulation of LMKL, which requires only O(n2M) operations per iteration. Our implementation of which is available from the following webpage, together with our CLMKL implementation and scripts to reproduce the experiments: https://www.dropbox.com/sh/hkkfa0ghxzuig03/AADRdtSSdUSm8hfVbsdjcRqva?dl=0.\nIn the following we report detailed results for various real-world experiments. Further details are shown in Supplemental Material E."}, {"heading": "5.2 Splice Site Recognition", "text": "Our first experiment aims at detecting splice sites in the organism Caenorhabditis elegans, which is an important task in computational gene finding as splice sites are located on the DNA strang right at the boundary of exons (which code for proteins) and introns (which do not). We experiment on the mkl-splice data set, which we download from http://mldata.org/repository/data/viewslug/mkl-splice/. It includes 1000 splice site instances and 20 weighted-degree kernels with degrees ranging from 1 to 20 [4]. The experimental setup for this experiment is as follows. We create random splits of this dataset into training set, validation set and test set, with size of training set traversing over the set {50, 100, 200, 300, . . . , 800}. We apply kernel-kmeans with uniform kernel to generate a partition with l = 3 clusters for both CLMKL and HLMKL, and use this kernel to define the gating model in LMKL. To be consistent with previous studies, we use the area under the ROC curve (AUC) as an evaluation criterion. We tune the SVM regularization parameter from 10{\u22121,\u22120.5,...,2}, and the average evenness over the interval [0.4, 0.8] with eight linearly equally spaced points, based on the AUCs on the validation set. All the base kernel matrices are multiplicatively normalized before training. We repeat the experiment 50 times, and report mean AUCs on the test set as well as standard deviation. Figure 1 (a) shows the results as a function of the training set size n.\nWe observe that CLMKL achieves, for all n, a significant gain over all baselines. This improvement is especially strong for small n. For n = 50, CLMKL attains 90.9% accuracy, while the best baseline only achieves 85.4%, improving by 5.5%. Detailed results with standard deviation are reported in Table 1. A hypothetical explanation of the improvement from CLMKL is that splice sites are characterized by nucleotide sequences\u2014so-called motifs\u2014the length of which may differ from site to site [47]. The 20\nemployed kernels count matching subsequences of length 1 to 20, respectively. For sites characterized by smaller motifs, low-degree WD-kernels are thus more effective than high-degree ones, and vice versa for sites containing longer motifs."}, {"heading": "5.3 Transcription Start Site Detection", "text": "Our next experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding genes in genomic DNA sequences. We experiment on the TSS data set, which we downloaded from http:// mldata.org/repository/data/viewslug/tss/. This data set, which is included in the larger study of [46], comes with 5 kernels. The SVM based on the uniform combination of these 5 kernels was found to have the highest overall performance among 19 promoter prediction programs [1]. It therefore constitutes a strong baseline. To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion. We consider the same experimental setup as in the splice detection experiment. The gating function and the partition are computed with the TSS kernel, which carries most of the discriminative information [46]. All kernel matrices were normalized with respect to their trace, prior to the experiment.\nFigure 1 (b) shows the AUCs on the test data sets as a function of the number of training examples.We observe that CLMKL attains a consistent improvement over other competing methods. Again, this improvement is most significant when n is small. Detailed results with standard deviation are reported in Table 2."}, {"heading": "5.4 Protein Fold Prediction", "text": "Protein fold prediction is a key step towards understanding the function of proteins, as the folding class of a protein is closely linked with its function; thus it is crucial for drug design. We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25]. This dataset consists of 27 fold classes with 311 proteins used for training and 383 proteins for testing. We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity. We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.1. We report the mean prediction accuracies, as well as standard deviations in Table 3.\nThe results show that CLMKL surpasses regular `p-norm MKL for all values of p, and achieves accuracies up to 0.6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7]. LMKL works poorly in this dataset, possibly because LMKL based on precomputed custom kernels requires to optimize nM additional variables, which may overfit."}, {"heading": "5.5 Visual Image Categorization\u2014UIUC Sports", "text": "We experiment on the UIUC Sports event dataset [35] consisting of 1574 images, belonging to 8 image classes of sports activities. We compute 9 \u03c72-kernels based on SIFT features and global color histograms, which is described in detail in Supplemental Material E.2, where we also give background on the experimental setup.\nFrom the results shown in Table 4, we observe that CLMKL achieves a performance improvement by 0.26% over the `p-norm MKL baseline while localized MKL as in G\u00f6nen and Alpaydin [14] underperforms the MKL baseline."}, {"heading": "5.6 Execution Time Experiments", "text": "To demonstrate the efficiency of the proposed implementation, we compare the training time for UNIF, LMKL, `p-norm MKL, HLMKL and CLMKL on the TSS dataset. We fix the regularization parameter"}, {"heading": "6 Conclusions", "text": "Localized approaches to multiple kernel learning allow for flexible distribution of kernel weights over the input space, which can be a great advantage when samples require varying kernel importance. As we show in this paper, this can be the case in image recognition and several computational biology applications. However, almost prevalent approaches to localized MKL require solving difficult non-convex optimization problems, which makes them potentially prone to overfitting as theoretical guarantees such as generalization error bounds are yet unknown.\nIn this paper, we propose a theoretically grounded approach to localized MKL, consisting of two subsequent steps: 1. clustering the training instances and 2. computation of the kernel weights for each cluster through a single convex optimization problem. For which we derive an efficient optimization algorithm based on Fenchel duality. Using Rademacher complexity theory, we establish large-deviation inequalities for localized MKL, showing that the smoothness in the cluster membership assignments crucially controls the generalization error. The proposed method is well suited for deployment in the domains of computer vision and computational biology. For splice site detection, CLMKL achieves up to 5% higher accuracy than its global and non-convex localized counterparts.\nFuture work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features."}, {"heading": "Acknowledgments", "text": "YL acknowledges support from the NSFC/RGC Joint Research Scheme [RGC Project No. N_CityU120/14 and NSFC Project No. 11461161006]. AB acknowledges support from Singapore University of Technology and Design Startup Grant SRIS15105. MK acknowledges support from the German Research Foundation (DFG) award KL 2698/2-1 and from the Federal Ministry of Science and Education (BMBF) awards 031L0023A and 031B0187B.\nSupplemental Material"}, {"heading": "A Lemmata and Proofs", "text": "A.1 Lemmata Used for Dualization in Section 2.3\nLetH1, . . . ,HM beM Hilbert spaces and p \u2265 1. Define the function gp(v1, . . . , vM ) : H1\u00d7\u00b7 \u00b7 \u00b7\u00d7HM \u2192 R by\ngp(v1, . . . , vM ) = 1\n2 \u2016(v1, . . . , vM )\u201622,p, p \u2265 1.\nFor any p \u2265 1, denote by p\u2217 the conjugated exponent satisfying 1p + 1 p\u2217 = 1.\nLemma A.1. The gradient of gp is\n\u2202gp(v1, . . . , vM ) \u2202vm = [ \u2211 m\u0303\u2208NM \u2016vm\u0303\u2016p2 ] 2 p\u22121\u2016vm\u2016p\u221222 vm.\nProof. By the chain rule, we have\n\u2202gp(v1, . . . , vM )\n\u2202vm =\n1\np [ \u2211 m\u0303\u2208NM \u2016vm\u0303\u2016p2 ] 2 p\u22121 \u2202\u3008vm, vm\u3009 p 2 \u2202vm\n= 1\n2 [ \u2211 m\u0303\u2208NM \u2016vm\u0303\u2016p2 ] 2 p\u22121 \u2202\u3008vm, vm\u3009 \u2202vm \u3008vm, vm\u3009 p 2\u22121\n= [ \u2211 m\u0303\u2208NM \u2016vm\u0303\u2016p2 ] 2 p\u22121\u2016vm\u2016p\u221222 vm.\nLemma A.2 (Micchelli and Pontil 40). Let ai \u2265 0, i \u2208 Nd and 1 \u2264 r <\u221e. Then\nmin \u03b7:\u03b7i\u22650, \u2211 i\u2208Nd \u03b7ri\u22641 \u2211 i\u2208Nd ai \u03b7i = ( \u2211 i\u2208Nd a r r+1 i )1+ 1r and the minimum is attained at \u03b7i = a 1 r+1\ni (\u2211 k\u2208Nd a r r+1 k )\u2212 1r .\nA.2 Proof of Representer Theorem (Theorem 3)\nProof of Theorem 3. In our derivation (3) of the dual problem, the variablewj(\u03b1) := ( w (1) j (\u03b1), . . . , w (M) j (\u03b1) ) should meet the optimality in the sense\nwj(\u03b1) = arg max v1\u2208H1,...,vM\u2208HM\n[\u2329 (vm) M m=1, ( n\u2211 i=1 \u03b1icj(xi)\u03c6m(xi) )M m=1 \u232a \u2212 1 2 \u2016(vm)Mm=1\u201622, 2pp+1 ] .\nSince (5f)\u22121 = 5f\u2217 for any convex function f and the Fenchel-conjugate of gp is gp\u2217 , we obtain\nwj(\u03b1) = 5g\u221212p p+1 ( \u2211 i\u2208Nn \u03b1icj(xi)\u03c61(xi), . . . , \u2211 i\u2208Nn \u03b1icj(xi)\u03c6M (xi) )\n= 5g 2p p\u22121 ( \u2211 i\u2208Nn \u03b1icj(xi)\u03c61(xi), . . . , \u2211 i\u2208Nn \u03b1icj(xi)\u03c6M (xi) ) Lem. A.1 =\n[ \u2211 m\u0303\u2208NM \u2016 \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m\u0303(xi)\u2016 2p p\u22121 2 ]\u2212 1p(\u2225\u2225 \u2211 i\u2208Nn \u03b1icj(xi)\u03c61(xi) \u2225\u2225 2p\u22121 2 [ \u2211 i\u2208Nn \u03b1icj(xi)\u03c61(xi) ] ,\n. . . , \u2225\u2225 \u2211 i\u2208Nn \u03b1icj(xi)\u03c6M (xi) \u2225\u2225 2p\u22121 2 [ \u2211 i\u2208Nn \u03b1icj(xi)\u03c6M (xi) ]) .\nNote that the above derivation uses Lemma A.1 from Supplemental Material A.1.\nA.3 Proof of Proposition 5\nProof of Proposition 5. Fixing the variables w(m)j and b, the optimization problem (P) reduces to\nmin \u03b2 \u2211 j\u2208Nl \u2211 m\u2208NM \u2016w(m)j \u201622 2\u03b2jm s.t. \u2211 m\u2208NM \u03b2pjm \u2264 1, \u03b2jm \u2265 0, \u2200j \u2208 Nl,m \u2208 NM .\nThis problem can be decomposed into l independent subproblems, one at each locality. For example, the subproblem at the j-th locality is as follows\nmin \u03b2 \u2211 m\u2208NM \u2016w(m)j \u201622 2\u03b2jm s.t. \u2211 m\u2208NM \u03b2pjm \u2264 1, \u03b2jm \u2265 0, \u2200m \u2208 NM .\nApplying Lemma A.2 with \u03b1m = \u2016w(m)j \u201622, \u03b7m = \u03b2jm and r = p completes the proof.\nA.4 Proof of Theorem 6: Convergence of the CLMKL Optimization Algorithm\nThe following lemma is a direct consequence of Lemma 3.1 and Theorem 4.1 in Tseng [50].\nLemma A.3. Let f : Rd1+\u00b7\u00b7\u00b7+dR \u2192 R \u222a {\u221e} be a function. Put d = d1 + \u00b7 \u00b7 \u00b7 + dR. Suppose that f can be decomposed into f(\u03b11, . . . , \u03b1R) + \u2211 r\u2208NR fr(\u03b1r) for some f0 : R\nd \u2192 R \u222a {\u221e} and fr : Rdr \u2192 R\u222a{\u221e}, r \u2208 NR. Initialize the block coordinate descent method by \u03b10 = (\u03b101, . . . , \u03b10R). Define the iterates \u03b1k = (\u03b1k1 , . . . , \u03b1 k R) by\n\u03b1k+1r = arg min u\u2208Rdk f(\u03b1k+11 , . . . , \u03b1 k+1 r\u22121 , u, \u03b1 k r+1, . . . , \u03b1 k R), \u2200r \u2208 NR, k \u2208 N+. (A.1)\nAssume that\n(A1) f is convex and proper, i.e., f 6\u2261 \u221e\n(A2) the sublevel set A0 := {\u03b1 \u2208 Rd : f(\u03b1) \u2264 f(\u03b10)} is compact and f is continuous on A0\n(A3) dom(f0) := {\u03b1 \u2208 Rd : f0(\u03b1) \u2264 \u221e} is open and f0 is continuously differentiable on dom(f0).\nThen, the minimizer in (A.1) exists and any limit point of the sequence (\u03b1k)k\u2208N+ minimizes f over A0.\nThe proof of Theorem 6 is a direct consequence of the above lemma.\nProof of Theorem 6. The primal problem (P) can be rewritten as follows:\ninf w,\u03b2j\u2208\u0398p,j\u2208Nl \u2211 j\u2208Nl \u2211 m\u2208NM \u2016w(m)j \u201622 2\u03b2jm + C \u2211 i\u2208Nn `( \u2211 j\u2208Nl cj(xi) \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009, yi), (A.2)\nwhere \u0398p = {(\u03b81, . . . , \u03b8M ) \u2208 RM : \u03b8m \u2265 0, \u2016(\u03b8m)Mm=1\u2016p \u2264 1}, and w (m) j \u2208 Rem ,\u2200j \u2208 Nl,m \u2208 NM . Note that Eq. (A.2) can be written as the following unconstrained problem:\ninf w,\u03b2 f(w, \u03b2), where f(w, \u03b2) = f0(w, \u03b2) + f1(w) + f2(\u03b2),\nwith\nf0(w, \u03b2) = \u2211 j\u2208Nl \u2211 m\u2208NM [\u2016w(m)j \u201622 2\u03b2jm + I\u03b2jm\u22650(\u03b2jm) ]\nf1(w) = C \u2211 i\u2208Nn `( \u2211 j\u2208Nl cj(xi) \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009, yi) and f2(\u03b2) = \u2211 j\u2208Nl I\u2016\u03b2j\u2016p\u22641(\u03b2j).\nHere I is the indicator function, i.e., IS(s) = 0 if s \u2208 S and\u221e otherwise. Now, it remains to check the assumption (A1), (A2) and (A3) in Lemma A.3 for Algorithm 1.\nVALIDITY OF A1. It is known that a quadratic over a linear function is convex, so the term \u2211 j\u2208Nl \u2211 m\u2208NM \u2016w(m)j \u2016 2 2 2\u03b2jm\nis convex. Also, since ` is convex w.r.t. the first argument and the term \u2211 j\u2208Nl cj(xi) \u2211 m\u2208NM \u3008w (m) j , \u03c6m(xi)\u3009\nis a linear function ofw, we immediately know the term \u2211 i\u2208Nn `( \u2211 j\u2208Nl cj(xi) \u2211 m\u2208NM \u3008w (m) j , \u03c6m(xi)\u3009, yi) is convex. The convexity of f follows immediately. For the initial assignment w(0) = {w(m,0)j }j,m with w\n(m,0) j = 0 and \u03b2 (0) = {\u03b2(0)jm}jm with \u03b2 (0) jm = M \u22121/p, we know\nf(w(0), \u03b2(0)) = Cn sup y\u2208Y `(0, y) <\u221e\nand therefore f is proper. VALIDITY OF A2. Recall that our algorithm is initialized with w(0) = 0 and \u03b2(0) = M\u22121/p. For any (w, \u03b2) \u2208 A0 := {(w\u0304, \u03b2\u0304) : f(w\u0304, \u03b2\u0304) \u2264 Cn supy\u2208Y `(0, y)}, we have\n\u2016w(m)j \u2016 2 2 \u2264 2\u03b2jmCn sup y\u2208Y `(0, y) \u2264 2Cn sup y\u2208Y `(0, y),\nwhich, coupled with the constraint \u2016(\u03b2jm)Mm=1\u2016p \u2264 1,\u2200j \u2208 Nl, immediately shows that A0 is bounded. Furthermore, since f0, f1 and f2 are continuous on their respective domains, the function f is therefore continuous on A0 \u2282 dom(f0) \u2229 dom(f1) \u2229 dom(f2). It is also known that the preimage of a closed set is closed under a continuous function, from which we know the set A0 = f\u22121(\u2212\u221e, f(w(0), \u03b2(0))] is closed. Any closed and bounded subset in Rd, d \u2208 N is compact and thus A(0) is compact.\nVALIDITY OF A3. Clearly, dom(f0) = {(w, \u03b2) : \u03b2 > 0} is open and f0 is continuously differentiable on dom(f0).\nA.5 Proof of Generalization Error Bounds (Theorem 8)\nIn this section we present the proof of the achieved generalization error bounds (Theorem 9 in the main text). Denote p\u0304 = 2pp+1 for any p \u2265 1 and observe that p\u0304 \u2264 2, which implies p\u0304\n\u2217 \u2265 2. To start with, we give a discussion on the interpretation and tightness of Rademacher complexity bounds in Theorem 8.\nInterpretation and Tightness of Rademacher complexity bounds It can be directly checked that the function x\u2192 xM2/x is decreasing along the interval (0, 2 logM) and increasing along the interval (2 logM,\u221e). Therefore, under the assumption km(x, x) \u2264 B the Rademacher complexity bounds thus satisfy the inequalities:\nR\u0302n(Hp,D) \u2264 \u221a DB\nn \u00d7  1 n \u221a 2eDB logM [ \u2211 j\u2208Nl \u2211 i\u2208Nn c 2 j (xi)], if p \u2264 logM logM\u22121 ,\n1 n \u221a 2p p\u22121DBM p\u22121 p [ \u2211 j\u2208Nl \u2211 i\u2208Nn c 2 j (xi)], otherwise.\nIn particular, the former expression can be taken for p = 1, resulting in a mild logarithmic dependence on the number of kernels. Note that in the limiting case of just one cluster, i.e., l = 1, the Rademacher complexity bounds match the result by Cortes et al. [10], which was shown to be tight.\nThe proof of Theorem 8 is based on the following lemmata.\nLemma A.4 (Khintchine-Kahane inequality [23]). Let v1, . . . , vn \u2208 H. Then, for any q \u2265 1, it holds\nE\u03c3 \u2225\u2225 \u2211 i\u2208Nn \u03c3ivi \u2225\u2225q 2 \u2264 ( q \u2211 i\u2208Nn \u2016vi\u201622 ) q 2 .\nLemma A.5 (Block-structured H\u00f6lder inequality [26]). Let x = (x(1), . . . , x(n)), y = (y(1), . . . , y(n)) \u2208 H = H1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Hn. Then, for any p \u2265 1, it holds\n\u3008x, y\u3009 \u2264 \u2016x\u20162,p\u2016y\u20162,p\u2217 .\nProof of Theorem 8. Firstly, for any 1 \u2264 t\u0304 \u2264 2 we can apply a block-structured version of H\u00f6lder inequality to bound \u2211 i\u2208Nn \u03c3ifw(xi) by\u2211\ni\u2208Nn \u03c3ifw(xi) = \u2211 i\u2208Nn \u03c3i [ \u2211 j\u2208Nl cj(xi) \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009 ]\n= \u2211 i\u2208Nn \u03c3i [ \u2211 j\u2208Nl cj(xi)\u3008wj , \u03c6(xi)\u3009 ]\n= \u2211 j\u2208Nl \u2329 wj , \u2211 i\u2208Nn \u03c3icj(xi)\u03c6(xi) \u232a H\u00f6lder \u2264\n\u2211 j\u2208Nl [ \u2016wj\u20162,t\u0304 \u2225\u2225\u2225 \u2211 i\u2208Nn \u03c3icj(xi)\u03c6(xi) \u2225\u2225\u2225 2,t\u0304\u2217 ] C. S \u2264 [ \u2211 j\u2208Nl \u2016wj\u201622,t\u0304 ] 1 2 [ \u2211 j\u2208Nl \u2225\u2225\u2225 \u2211 i\u2208Nn \u03c3icj(xi)\u03c6(xi) \u2225\u2225\u22252 2,t\u0304\u2217 ] 1 2 .\n(A.3)\nFor any j \u2208 Nl, the Khintchine-Kahane (K.-K.) inequality and Jensen inequality (since t\u0304\u2217 \u2265 2) permit us to bound E\u03c3 \u2225\u2225\u2225\u2211i\u2208Nn \u03c3icj(xi)\u03c6(xi)\u2225\u2225\u222522,t\u0304\u2217 by E\u03c3 \u2225\u2225\u2225 \u2211 i\u2208Nn \u03c3icj(xi)\u03c6(xi) \u2225\u2225\u22252 2,t\u0304\u2217 def = E\u03c3 [ \u2211 m\u2208NM \u2225\u2225\u2225 \u2211 i\u2208Nn \u03c3icj(xi)\u03c6m(xi) \u2225\u2225\u2225t\u0304\u2217 2 ] 2 t\u0304\u2217\nJensen \u2264 [ E\u03c3 \u2211 m\u2208NM \u2225\u2225\u2225 \u2211 i\u2208Nn \u03c3icj(xi)\u03c6m(xi) \u2225\u2225\u2225t\u0304\u2217 2 ] 2 t\u0304\u2217\nK.-K. \u2264 [ \u2211 m\u2208NM ( t\u0304\u2217 \u2211 i\u2208Nn c2j (xi)\u2016\u03c6m(xi)\u201622 ) t\u0304\u2217 2 ] 2 t\u0304\u2217\n= t\u0304\u2217 [ \u2211 m\u2208NM ( \u2211 i\u2208Nn c2j (xi)km(xi, xi) ) t\u0304\u2217 2 ] 2 t\u0304\u2217\n= t\u0304\u2217 \u2225\u2225\u2225\u2225( \u2211\ni\u2208Nn\nc2j (xi)km(xi, xi) )M m=1 \u2225\u2225\u2225\u2225 t\u0304\u2217 2 .\nPlugging the above inequalities into Eq. (A.3) and noticing the trivial inequality \u2016wj\u20162,t\u0304 \u2264 \u2016wj\u20162,p\u0304,\u2200t \u2265 p \u2265 1, we get the following bound:\nR\u0302n(Hp,D) \u2264 inf t\u2265p\nR\u0302n(Ht,D) \u2264 \u221a D\nn inf t\u2265p ( t\u0304\u2217 \u2211 j\u2208Nl \u2225\u2225\u2225\u2225( \u2211 i\u2208Nn c2j (xi)km(xi, xi) )M m=1 \u2225\u2225\u2225\u2225 t\u0304\u2217 2 )1/2 .\nThe above inequality can be equivalently written as Eq. (10). Under the condition km(x, x) \u2264 B, the term in the brace of Eq. (10) can be controlled by\u2211\nj\u2208Nl \u2225\u2225\u2225\u2225( \u2211 i\u2208Nn c2j (xi)km(xi, xi) )M m=1 \u2225\u2225\u2225\u2225 t 2 = \u2211 j\u2208Nl [ \u2211 m\u2208NM ( \u2211 i\u2208Nn c2j (xi)km(xi, xi)) t 2 ] 2 t\n\u2264 BM 2t \u2211 j\u2208Nl \u2211 i\u2208Nn c2j (xi).\nTherefore, the inequality (10) further translates to\nR\u0302n(Hp,D) \u2264 \u221a DB\nn inf\n2\u2264t\u2264 2pp\u22121\n( tM 2 t [ \u2211 j\u2208Nl \u2211 i\u2208Nn c2j (xi) ]) 12 .\nProof of Theorem 9. The proof now simply follows by plugging in the bound of Theorem 8 into Theorem 7 of Bartlett and Mendelson [3]."}, {"heading": "B Support Vector Regression Formulation of CLMKL", "text": "For the -insensitive loss `(t, y) = [|y \u2212 t| \u2212 ]+, denoting a+ = max(a, 0) for all a \u2208 R, we have `\u2217(\u2212\u03b1iC , yi) = \u2212 1 C\u03b1iyi + | \u03b1i C | if |\u03b1i| \u2264 C and\u221e elsewise [20]. Hence, the complete dual problem (D) reduces to\nsup \u03b1 \u2212 1 2 \u2211 j\u2208Nl \u2225\u2225\u2225( \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) )M m=1 \u2225\u2225\u22252 2, 2pp\u22121 + \u2211 i\u2208Nn (\u03b1iyi \u2212 |\u03b1i|),\ns.t. \u2211 i\u2208Nn \u03b1i = 0,\n|\u03b1i| \u2264 C, \u2200i \u2208 Nn.\n(B.1)\nLet \u03b1+i , \u03b1 \u2212 i \u2265 0 be the positive and negative parts of \u03b1i, that is, \u03b1i = \u03b1 + i \u2212\u03b1 \u2212 i , |\u03b1i| = \u03b1 + i +\u03b1 \u2212 i . Then,\nthe optimization problem (B.1) translates as follows.\nProblem B.1 (CLMKL\u2014Regression Problem). For the -insensitive loss, the dual CLMKL problem (D) is given by:\nsup \u03b1 \u2212 1 2 \u2211 j\u2208Nl \u2225\u2225\u2225( \u2211 i\u2208Nn cj(xi)(\u03b1 + i \u2212 \u03b1 \u2212 i )\u03c6m(xi) )M m=1 \u2225\u2225\u22252 2, 2pp\u22121 + \u2211 i\u2208Nn (\u03b1+i \u2212 \u03b1 \u2212 i )yi \u2212 \u2211 i\u2208Nn (\u03b1+i + \u03b1 \u2212 i )\ns.t. \u2211 i\u2208Nn (\u03b1+i \u2212 \u03b1 \u2212 i ) = 0\n0 \u2264 \u03b1+i , \u03b1 \u2212 i \u2264 C,\u03b1 + i \u03b1 \u2212 i = 0, \u2200i \u2208 Nn.\n(B.2)"}, {"heading": "C Primal and Dual CLMKL Problem Given Fixed Kernel Weights", "text": "Temporarily fixing the kernel weights \u03b2, the partial primal and dual optimization problems become as follows.\nProblem C.1 (PRIMAL CLMKL OPTIMIZATION PROBLEM). Given a loss function `(t, y) : R \u00d7 Y \u2192 R convex in the first argument and kernel weights \u03b2 = (\u03b2jm), solve\nmin w,t,b \u2211 j\u2208Nl \u2211 m\u2208NM \u2016w(m)j \u201622 2\u03b2jm + C \u2211 i\u2208Nn `(ti, yi)\ns.t. \u2211 j\u2208Nl [ cj(xi) \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009 ] + b = ti, \u2200i \u2208 Nn. (C.1)\nProblem C.2 (DUAL CLMKL PROBLEM\u2014PARTIALLY DUALIZED). Given a loss function `(t, y) : R \u00d7 Y \u2192 R convex in the first argument and kernel weights \u03b2 = (\u03b2jm), solve\nsup \u03b1: \u2211 i\u2208Nn \u03b1i=0 \u22121 2 \u2211 j\u2208Nl \u2211 m\u2208NM \u03b2jm \u2225\u2225\u2225 \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) \u2225\u2225\u22252 2 \u2212 C \u2211 i\u2208Nn `\u2217(\u2212\u03b1i C , yi). (C.2)\nFor any feasible dual variables, the primal variable w(m)j (\u03b1) minimizing the associated Lagrangian saddle problem is\nw (m) j (\u03b1) = \u03b2jm \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi). (C.3)\nDualization. With the Lagrangian multipliers \u03b1i, i \u2208 Nn, the Lagrangian saddle problem of Eq. (C.1) is\nsup \u03b1 inf w,t,b \u2211 j\u2208Nl \u2211 m\u2208NM \u2016w(m)j \u201622 2\u03b2jm + C \u2211 i\u2208Nn `(ti, yi)\n\u2212 \u2211 i\u2208Nn \u03b1i (\u2211 j\u2208Nl [cj(xi) \u2211 m\u2208NM \u3008w(m)j , \u03c6m(xi)\u3009] + b\u2212 ti )\n= sup \u03b1\n{ \u2212 C \u2211 i\u2208Nn sup ti [\u2212`(ti, yi)\u2212 1 C \u03b1iti]\u2212 sup b \u2211 i\u2208Nn \u03b1ib\u2212\n\u2211 j\u2208Nl \u2211 m\u2208NM sup w (m) j 1 \u03b2jm [ \u3008w(m)j , \u2211 i\u2208Nn \u03b2jm\u03b1icj(xi)\u03c6m(xi)\u2212 1 2 \u2016w(m)j \u2016 2 2 ]} def = sup\u2211\ni\u2208Nn \u03b1i=0\n{ \u2212 C \u2211 i\u2208Nn `\u2217(\u2212\u03b1i C , yi)\u2212 1 2 \u2211 j\u2208Nl \u2211 m\u2208NM \u03b2jm \u2225\u2225 \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi) \u2225\u22252 2 } .\n(C.4)\nFrom the above deduction, the variable w(m)j (\u03b1) is a solution of the following problem\nw (m) j (\u03b1) = arg min\nv\u2208Hm [ \u3008v, \u2211 i\u2208Nn \u03b1i\u03b2jmcj(xi)\u03c6m(xi)\u3009 \u2212 1 2 \u2016v\u201622 ] and it can be directly checked that this w(m)j (\u03b1) can be analytically represented by\nw (m) j (\u03b1) = \u03b2jm \u2211 i\u2208Nn \u03b1icj(xi)\u03c6m(xi).\nPlugging the Fenchel conjugate function of the hinge loss and the -insensitive loss into Problem C.2, we have the following partial dual problems for the hinge loss and -insensitive loss. Here k\u0303 is the kernel defined in Eq. (6).\nProblem C.3 (DUAL CLMKL PROBLEM\u2014PARTIALLY DUALIZED FOR HINGE LOSS).\nsup \u03b1i \u2211 i\u2208Nn \u03b1i \u2212 1 2 \u2211 i,\u0303i\u2208Nn yiyi\u0303\u03b1i\u03b1i\u0303k\u0303(xi, xi\u0303)\ns.t. \u2211 i\u2208Nn \u03b1iyi = 0\n0 \u2264 \u03b1i \u2264 C \u2200i \u2208 Nn.\nProblem C.4 (DUAL CLMKL PROBLEM\u2014PARTIALLY DUALIZED FOR -INSENSITIVE LOSS).\nmax \u03b1i \u2212 1 2 \u2211 i,\u0303i\u2208Nn (\u03b1+i \u2212 \u03b1 \u2212 i )(\u03b1 + i\u0303 \u2212 \u03b1\u2212 i\u0303 )k\u0303(xi, xi\u0303) + \u2211 i\u2208Nn (\u03b1+i \u2212 \u03b1 \u2212 i )yi \u2212 \u2211 i\u2208Nn (\u03b1+i + \u03b1 \u2212 i )\ns.t. \u2211 i\u2208Nn (\u03b1+i \u2212 \u03b1 \u2212 i ) = 0\n0 \u2264 \u03b1+i , \u03b1 \u2212 i \u2264 C,\u03b1 + i \u03b1 \u2212 i = 0, \u2200i \u2208 Nn."}, {"heading": "D Details on Our Implementation of Localized MKL", "text": "G\u00f6nen and Alpaydin [14] give the first formulation of localized MKL algorithm by using gating model \u03b7m(x) \u221d exp(\u3008vm, x\u3009 + vm0) to realize locality, and optimize the parameters vm, vm0,m \u2208 NM with a gradient descent method. However, the calculation of the gradients requires O(n2M2d) operations in G\u00f6nen and Alpaydin [14], which scales poorly w.r.t. the dimension d and the number of kernels. Also, the definition of gating model requires the information of primitive features, which is not accessible in some application areas. For example, data in bioinformatics may appear in a non-vectorial format such as trees and graphs for which the representation of the data with vectors is non-trivial but the calculation of kernel matrices is direct. Although G\u00f6nen and Alpayd\u0131n [16] propose to use the empirical feature map xG = [kG(x1, x), . . . , kG(xn, x)] to replace the primitive feature in this case (kG is a kernel), this turns out to not strictly obey the spirit of the gating function: the empirical feature does not reflect the location of the example in the feature space induced from the kernel. Furthermore, with this strategy the computation of gradient scales as O(n3M2), which is quite computationally expensive. In this paper, we give a natural definition of the gating model in a kernel-induced feature space, and provide a fast implementation of the resulting LMKL algorithm. Let k0 be the kernel used to define the gating model, and let \u03c60 be the associated feature map. Our basic idea is based on the discovery that the parameter vm can always be represented as a linear combination of \u03c60(x1), . . . , \u03c60(xn), so the calculation of the representation coefficients is sufficient to restore vm. We consider the gating model of the form\n\u03b7m(x) = exp\n( \u3008vm, \u03c60(x) + vm0 )\u2211 m\u0303\u2208NM exp ( \u3008vm\u0303, \u03c60(x) + vm\u03030\n) . G\u00f6nen and Alpaydin [14] proposed to optimize the objective function\nJ(v) := \u2211 i\u2208Nn \u03b1i \u2212 1 2 \u2211 i\u2208Nn \u2211 i\u0303\u2208Nn \u03b1i\u03b1i\u0303yiyi\u0303 [ \u2211 m\u2208NM \u03b7m(xi)km(xi, xi\u0303)\u03b7m(xi\u0303) ]\nwith a gradient descent method. The gradient of J(v) can be expressed as:\n\u2202J(v)\n\u2202vm = \u2212 \u2211 i\u2208Nn \u2211 i\u0303\u2208Nn \u2211 m\u0303\u2208NM \u03b1i\u03b1i\u0303yiyi\u0303\u03b7m\u0303(xi)km\u0303(xi, xi\u0303)\u03b7m\u0303(xi\u0303)\u03c60(xi)[\u03b4 m\u0303 m \u2212 \u03b7m(xi)], (D.1)\nwhere \u03b4m\u0303m = 1 ifm = m\u0303 and 0 otherwise. Let v (t) = (v (t) 1 , . . . , v (t) M ) be the value of v = (v1, . . . , vM ) at the t-th iteration and let r(t)(i,m) be the representation of v(t)m in terms of \u03c60(xi), i.e., v (t) m = \u2211 i\u2208Nn r\n(t)(i,m)\u03c60(xi). Analogously, let g(t)(i,m) be the representation coefficient of \u2202J(v(t))/\u2202vm in terms of \u03c60(xi). Introduce two arrays for convenience:\nB(i,m) = \u2211 i\u0303\u2208Nn \u03b1i\u0303yi\u0303km(xi, xi\u0303)\u03b7m(xi\u0303), A(i) = \u2211 m\u2208NM \u03b7m(xi)B(i,m), i \u2208 Nn,m \u2208 NM .\nEq. (D.1) then implies that\ng(t)(i,m) = \u2212 \u2211 i\u0303\u2208Nn \u2211 m\u0303\u2208NM \u03b1i\u03b1i\u0303yiyi\u0303\u03b7m\u0303(xi)km\u0303(xi, xi\u0303)\u03b7m\u0303(xi\u0303)[\u03b4 m\u0303 m \u2212 \u03b7m(xi)]\n= \u2212\u03b1iyi\u03b7m(xi) [ \u2211 i\u0303\u2208Nn \u03b1i\u0303yi\u0303km(xi, xi\u0303)\u03b7m(xi\u0303)\u2212 \u2211 i\u0303\u2208Nn \u2211 m\u0303\u2208NM \u03b1i\u0303yi\u0303\u03b7m\u0303(xi)km\u0303(xi, xi\u0303)\u03b7m\u0303(xi\u0303) ] = \u2212\u03b1iyi\u03b7m(xi)[B(i,m)\u2212A(i)]. (D.2)\nWith the line search v(t+1)m = v (t) m + \u00b5(t) \u2202J(v(t)) \u2202vm\n,m \u2208 NM , the representation coefficient can be simply updated by taking\nr(t+1)(i,m) = r(t)(i,m)\u2212 \u00b5(t)g(t)(i,m), \u2200i \u2208 Nn,m \u2208 NM .\nAlso, in the calculation of gating model, we need to calculate \u3008\u03c60(xi), v(t)m \u3009, and this can be fulfilled by\n\u3008\u03c60(xi), v(t)m \u3009 = \u2211 i\u0303\u2208Nn \u3008\u03c60(xi), r(t)(\u0303i,m)\u03c60(xi\u0303)\u3009 = \u2211 i\u0303\u2208Nn k0(xi, xi\u0303)r (t)(\u0303i,m).\nAt each iteration, we can useO(n2M) operations to calculate the arraysA,B. Subsequently, the calculation of the gradients as illustrated by Eq. (D.2) can be fulfilled with O(nM) operations. The updating of the representation coefficients r(t)(i,m) requiresO(nM) operations, while calculating the gating model \u03b7m(xi) requires further O(n2M) operations. Putting the above discussions together, our implementation of LMKL based on the kernel trick requiresO(n2M) operations at each iteration, which is much faster than the original implementation in G\u00f6nen and Alpaydin [14] with O(n2M2d) operations at each iteration. Here, d is the dimension of the primitive feature."}, {"heading": "E Background on the Experimental Setup and Empirical Results", "text": "E.1 Details on the Protein Fold Prediction Experiment\nWe precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes. We apply kernel k-means to the uniform kernel to generate a partition with 3 clusters for CLMKL and HLMKL, and, since we have no access to primitive features, use this kernel to define gating model in LMKL. All the base kernel matrices are multiplicatively normalized before training. We validate the regularization parameter C over 10{\u22121,\u22120.5,...,2}, and the average evenesses over the interval [0.4, 0.7] with eight linearly equally spaced points. Note that the model parameters are tuned separately for each training set and only based on the training set, not the test set. We repeat the experiment 15 times.\nE.2 Details on the Visual Image Categorization Experiment\nWe compute 9 bag-of-words features, each with a dictionary size of 512, resulting in 9 \u03c72-Kernels [57]. The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51]. The remaining 3 bag-of-words features are computed over quantiles of color values at the same three scales. The quantiles are concatenated over RGB channels.\nFor each channel within a set of color channels, the quantiles are concatenated. Local features are extracted at a grid of step size 5 on images that were down-scaled to 600 pixels in the largest dimension. Assignment of local features to visual words is done using rank-mapping [5]. The kernel width of the kernels is set to the mean of the \u03c72-distances. All kernels are multiplicatively normalized.\nThe dataset is split into 11 parts for outer crossvalidation. The performance reported in Table 4 is the average over the 11 test splits of the outer cross validation. For each outer cross validation training split, a 10-fold inner crossvalidation is performed for determining optimal parameters. The parameters are selected using only the samples of the outer training split. This avoids to report a result merely on the most favorable train test split from the outer cross validation. For the proposed CLMKL we employ kernel k-means with 3 clusters on the outer training split of the dataset.\nWe compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14]. For all methods, we employ a one-versus-all setup, running over `p-norms in {1.125, 1.333, 2} and regularization constants in {10k/2}k=5k=0 (optima attained inside the respective grids). CLMKL uses the same set of `p-norms, regularization constants from {10k/2}k=0,...,5, and average excesses in {0.5 + i/12}i=5i=0. Performance is measured through multi-class classification accuracy."}], "references": [{"title": "Toward a gold standard for promoter prediction evaluation", "author": ["T. Abeel", "Y. Van de Peer", "Y. Saeys"], "venue": "Bioinformatics, 25(12):i313\u2013i320,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["F.R. Bach", "G.R. Lanckriet", "M.I. Jordan"], "venue": "ICML, page 6,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Support vector machines and kernels for computational biology", "author": ["A. Ben-Hur", "C.S. Ong", "S. Sonnenburg", "B. Sch\u00f6lkopf", "G. R\u00e4tsch"], "venue": "PLoS Computational Biology, 4,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Enhanced representation and multi-task learning for image annotation", "author": ["A. Binder", "W. Samek", "K.-R. M\u00fcller", "M. Kawanabe"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge Univ. Press, New York,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with support vector machines", "author": ["C. Campbell", "Y. Ying"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 5(1):1\u201395,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Invited talk: Can learning kernels help performance", "author": ["C. Cortes"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Generalization bounds for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML\u201910,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning kernels using local rademacher complexity", "author": ["C. Cortes", "M. Kloft", "M. Mohri"], "venue": "Advances in Neural Information Processing Systems, pages 2760\u20132768,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "ACM SIGKDD international conference on Knowledge discovery and data mining, pages 551\u2013556. ACM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-class protein fold recognition using support vector machines and neural networks", "author": ["C.H. Ding", "I. Dubchak"], "venue": "Bioinformatics, 17(4):349\u2013358,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Localized multiple kernel learning", "author": ["M. G\u00f6nen", "E. Alpaydin"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 352\u2013359. ACM,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple kernel learning algorithms", "author": ["M. G\u00f6nen", "E. Alpaydin"], "venue": "J. Mach. Learn. Res., 12:2211\u20132268, July", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Localized algorithms for multiple kernel learning", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "Pattern Recognition, 46(3):795\u2013 807,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Active and semi-supervised data domain description", "author": ["N. G\u00f6rnitz", "M. Kloft", "U. Brefeld"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 407\u2013422. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Toward supervised anomaly detection", "author": ["N. G\u00f6rnitz", "M.M. Kloft", "K. Rieck", "U. Brefeld"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Probability-confidence-kernel-based localized multiple kernel learning with norm", "author": ["Y. Han", "G. Liu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, 42(3):827\u2013837,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Fenchel duality-based algorithms for convex optimization problems with applications in machine learning and image restoration", "author": ["A. Heinrich"], "venue": "PhD thesis, Chemnitz University of Technology,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Clusterpath: an algorithm for clustering using convex fusion penalties", "author": ["T.D. Hocking", "A. Joulin", "F. Bach", "J.-P. Vert"], "venue": "In 28th international conference on machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Improved loss bounds for multiple kernel learning", "author": ["Z. Hussain", "J. Shawe-Taylor"], "venue": "AISTATS, pages 370\u2013377,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Some random series of functions", "author": ["J.-P. Kahane"], "venue": "Cambridge University Press, Cambridge,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1985}, {"title": "`p-norm multiple kernel learning", "author": ["M. Kloft"], "venue": "PhD thesis, Berlin Institute of Technology,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "The local Rademacher complexity of `p-norm multiple kernel learning", "author": ["M. Kloft", "G. Blanchard"], "venue": "Advances in Neural Information Processing Systems 24, pages 2438\u20132446. MIT Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "On the convergence rate of lp-norm multiple kernel learning", "author": ["M. Kloft", "G. Blanchard"], "venue": "Journal of Machine Learning Research, 13(1):2465\u20132502,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic feature selection for anomaly detection", "author": ["M. Kloft", "U. Brefeld", "P. D\u00fcessel", "C. Gehl", "P. Laskov"], "venue": "Proceedings of the 1st ACM workshop on Workshop on AISec, pages 71\u201376. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-sparse multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "P. Laskov", "S. Sonnenburg"], "venue": "NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, volume 4,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient and accurate lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "P. Laskov", "K.-R. M\u00fcller", "A. Zien", "S. Sonnenburg"], "venue": "Advances in neural information processing systems, pages 997\u20131005,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "A unifying view of multiple kernel learning", "author": ["M. Kloft", "U. R\u00fcckert", "P. Bartlett"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 66\u201381,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "The Journal of Machine Learning Research, 12:953\u2013997,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research, 5:27\u201372,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Refined Rademacher chaos complexity bounds with applications to the multikernel learning problem", "author": ["Y. Lei", "L. Ding"], "venue": "Neural. Comput., 26(4):739\u2013760,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Theory and algorithms for the localized setting of learning kernels", "author": ["Y. Lei", "A. Binder", "\u00dc. Dogan", "M. Kloft"], "venue": "Proceedings of The 1st International Workshop on \u201cFeature Extraction: Modern Questions and Challenges\u201d, NIPS, pages 173\u2013195,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "What, where and who? classifying events by scene and object recognition", "author": ["L.-J. Li", "L. Fei-Fei"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1\u20138. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiple kernel clustering with local kernel alignment maximization", "author": ["M. Li", "X. Liu", "L. Wang", "Y. Dou", "J. Yin", "E. Zhu"], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI\u201916,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Sample-adaptive multiple kernel learning", "author": ["X. Liu", "L. Wang", "J. Zhang", "J. Yin"], "venue": "Proceedings of the Twenty- Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada., pages 1975\u2013 1981,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Absent multiple kernel learning", "author": ["X. Liu", "L. Wang", "J. Yin", "Y. Dou", "J. Zhang"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 2807\u20132813,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, 60(2):91\u2013110,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning the kernel function via regularization", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, pages 1099\u20131125,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "A unified view of localized kernel learning", "author": ["J. Moeller", "S. Swaminathan", "S. Venkatasubramanian"], "venue": "arXiv preprint arXiv:1603.01374,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Non-uniform multiple kernel learning with cluster-based gating functions", "author": ["Y. Mu", "B. Zhou"], "venue": "Neurocomputing, 74(7):1095\u20131101,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press, Cambridge, MA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "The Journal of Machine Learning Research, 7:1531\u20131565,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Arts: accurate recognition of transcription starts in human", "author": ["S. Sonnenburg", "A. Zien", "G. R\u00e4tsch"], "venue": "Bioinformatics, 22(14):e472\u2013e480,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Poims: positional oligomer importance matrices\u2014understanding support vector machine-based signal detectors", "author": ["S. Sonnenburg", "A. Zien", "P. Philips", "G. R\u00e4tsch"], "venue": "Bioinformatics, 24(13):i6\u2013i14,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning bounds for support vector machines with learned kernels", "author": ["N. Srebro", "S. Ben-David"], "venue": "COLT, pages 169\u2013183. Springer-Verlag, Berlin,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiple kernel learning and the smo algorithm", "author": ["Z. Sun", "N. Ampornpunt", "M. Varma", "S. Vishwanathan"], "venue": "Advances in neural information processing systems, pages 2361\u20132369,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal of optimization theory and applications, 109(3):475\u2013494,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2001}, {"title": "Evaluating color descriptors for object and scene recognition", "author": ["K.E.A. van de Sande", "T. Gevers", "C.G.M. Snoek"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Probabilistic clustering of time-evolving distance data", "author": ["J.E. Vogt", "M. Kloft", "S. Stark", "S.S. Raman", "S. Prabhakaran", "V. Roth", "G. R\u00e4tsch"], "venue": "Machine Learning, 100(2-3):635\u2013654,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Z. Xu", "R. Jin", "H. Yang", "I. King", "M.R. Lyu"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 1175\u20131182,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient sparse generalized multiple kernel learning", "author": ["H. Yang", "Z. Xu", "J. Ye", "I. King", "M.R. Lyu"], "venue": "IEEE Transactions on Neural Networks, 22(3):433\u2013446,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "Group-sensitive multiple kernel learning for object categorization", "author": ["J. Yang", "Y. Li", "Y. Tian", "L. Duan", "W. Gao"], "venue": "2009 IEEE 12th International Conference on Computer Vision, pages 436\u2013443. IEEE,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalization bounds for learning the kernel", "author": ["Y. Ying", "C. Campbell"], "venue": "COLT,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Local features and kernels for classification of texture and object categories: A comprehensive study", "author": ["J. Zhang", "M. Marszalek", "S. Lazebnik", "C. Schmid"], "venue": "International Journal of Computer Vision, 73(2):213\u2013238,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 31, "context": "[32], who introduce the multiple kernel learning (MKL) framework [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[32], who introduce the multiple kernel learning (MKL) framework [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "MKL offers a principal way of encoding complementary information with distinct base kernels and automatically learning an optimal combination of those [45].", "startOffset": 151, "endOffset": 155}, {"referenceID": 1, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 26, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 28, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 43, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 51, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 52, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 9, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 10, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 21, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 24, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 25, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 29, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 32, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 46, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 54, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 27, "context": "9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].", "startOffset": 131, "endOffset": 139}, {"referenceID": 30, "context": "9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].", "startOffset": 131, "endOffset": 139}, {"referenceID": 13, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 18, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 33, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 35, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 41, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 53, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 33, "context": "Indeed, besides the recent work by [34], the generalization performance of localized MKL algorithms (as measured through large-deviation bounds) is poorly understood, which potentially could make these algorithms prone to overfitting.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "1 Related Work G\u00f6nen and Alpaydin [14] initiate the work on localized MKL by introducing gating models", "startOffset": 34, "endOffset": 38}, {"referenceID": 53, "context": "[55] give a group-sensitive formulation of localized MKL, where kernel weights vary at, instead of the example level, the group level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Mu and Zhou [42] also introduce a non-uniform MKL allowing the kernel weights to vary at the cluster-level and tune the kernel weights under the graph embedding framework.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "Han and Liu [19] built on G\u00f6nen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Han and Liu [19] built on G\u00f6nen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class.", "startOffset": 45, "endOffset": 49}, {"referenceID": 35, "context": "[36] propose a multiple kernel clustering method by maximizing local kernel alignments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] present sample-adaptive approaches to localized MKL, where kernels can be switched on/off at the example level by introducing a latent binary vector for each individual sample, which and the kernel weights are then jointly optimized via margin maximization principle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] present a unified viewpoint of localized MKL by interpreting gating functions in terms of local reproducing kernel Hilbert spaces acting on the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] present a convex approach to MKL based on controlling the local Rademacher complexity, the meaning of locality is different in Cortes et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.", "startOffset": 108, "endOffset": 116}, {"referenceID": 25, "context": "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.", "startOffset": 108, "endOffset": 116}, {"referenceID": 37, "context": "[38] extend the idea of sample-adaptive MKL to address the issue with missing kernel information on some examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] propose a MKL method by decoupling the locality structure learning with a hard clustering strategy from optimizing the parameters in the spirit of multi-task learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": ", Sl in a probabilistic manner, meaning that, for each cluster Sj , we have a function cj : X \u2192 [0, 1] indicating the likelihood of x falling into cluster j, i.", "startOffset": 96, "endOffset": 102}, {"referenceID": 0, "context": "the first argument and cluster likelihood functions cj : X \u2192 [0, 1], j \u2208 Nl, solve", "startOffset": 61, "endOffset": 67}, {"referenceID": 30, "context": ", \u03b2jM ) for each cluster j [31] .", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "The result (2) now follows by recalling that for a norm \u2016 \u00b7 \u2016, its dual norm \u2016 \u00b7 \u2016\u2217 is defined by \u2016x\u2016\u2217 = sup\u2016\u03bc\u2016=1\u3008x, \u03bc\u3009 and satisfies: ( 12\u2016 \u00b7 \u2016 2)\u2217 = 12\u2016 \u00b7 \u2016 2 \u2217 [6].", "startOffset": 163, "endOffset": 166}, {"referenceID": 43, "context": "[45], we consider here a two-layer optimization procedure to solve the problem (P) where the variables are divided into two groups: the group of kernel weights {\u03b2jm} j,m=1 and the group of weight vectors {w j } l,M j,m=1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "This allows us to employ very efficient existing SVM solvers [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 43, "context": "[45] Figure 7 in Kloft et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49] in the context of `p-norm MKL.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "We give a purely data-dependent bound on the generalization error, which is obtained using Rademacher complexity theory [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "[10], Kloft and Blanchard [25], Kloft et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[10], Kloft and Blanchard [25], Kloft et al.", "startOffset": 26, "endOffset": 30}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "1 Experimental Setup We implement the proposed convex localized MKL (CLMKL) algorithm in MATLAB and solve the involved canonical SVM problem with LIBSVM [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 20, "context": "[21]) could be used.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 199, "endOffset": 202}, {"referenceID": 30, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 220, "endOffset": 224}, {"referenceID": 31, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 255, "endOffset": 259}, {"referenceID": 13, "context": "The calculation of the gradients in LMKL [14] requires O(nMd) operations, which scales poorly, and the definition of the gating model 8", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "It includes 1000 splice site instances and 20 weighted-degree kernels with degrees ranging from 1 to 20 [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 45, "context": "A hypothetical explanation of the improvement from CLMKL is that splice sites are characterized by nucleotide sequences\u2014so-called motifs\u2014the length of which may differ from site to site [47].", "startOffset": 186, "endOffset": 190}, {"referenceID": 44, "context": "This data set, which is included in the larger study of [46], comes with 5 kernels.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The SVM based on the uniform combination of these 5 kernels was found to have the highest overall performance among 19 promoter prediction programs [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.", "startOffset": 39, "endOffset": 50}, {"referenceID": 23, "context": "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.", "startOffset": 39, "endOffset": 50}, {"referenceID": 44, "context": "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.", "startOffset": 39, "endOffset": 50}, {"referenceID": 44, "context": "The gating function and the partition are computed with the TSS kernel, which carries most of the discriminative information [46].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 129, "endOffset": 132}, {"referenceID": 23, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.", "startOffset": 59, "endOffset": 62}, {"referenceID": 23, "context": "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.", "startOffset": 91, "endOffset": 94}, {"referenceID": 23, "context": "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.", "startOffset": 102, "endOffset": 106}, {"referenceID": 24, "context": "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7].", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 34, "context": "5 Visual Image Categorization\u2014UIUC Sports We experiment on the UIUC Sports event dataset [35] consisting of 1574 images, belonging to 8 image classes of sports activities.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "26% over the `p-norm MKL baseline while localized MKL as in G\u00f6nen and Alpaydin [14] underperforms the MKL baseline.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 83, "endOffset": 91}, {"referenceID": 17, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 133, "endOffset": 141}, {"referenceID": 50, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 133, "endOffset": 141}, {"referenceID": 48, "context": "1 in Tseng [50].", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "[10], which was shown to be tight.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "4 (Khintchine-Kahane inequality [23]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "5 (Block-structured H\u00f6lder inequality [26]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "The proof now simply follows by plugging in the bound of Theorem 8 into Theorem 7 of Bartlett and Mendelson [3].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "B Support Vector Regression Formulation of CLMKL For the -insensitive loss `(t, y) = [|y \u2212 t| \u2212 ]+, denoting a+ = max(a, 0) for all a \u2208 R, we have `\u2217(\u2212\u03b1i C , yi) = \u2212 1 C\u03b1iyi + | \u03b1i C | if |\u03b1i| \u2264 C and\u221e elsewise [20].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "G\u00f6nen and Alpaydin [14] give the first formulation of localized MKL algorithm by using gating model \u03b7m(x) \u221d exp(\u3008vm, x\u3009 + vm0) to realize locality, and optimize the parameters vm, vm0,m \u2208 NM with a gradient descent method.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "However, the calculation of the gradients requires O(nMd) operations in G\u00f6nen and Alpaydin [14], which scales poorly w.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "Although G\u00f6nen and Alpayd\u0131n [16] propose to use the empirical feature map xG = [kG(x1, x), .", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "G\u00f6nen and Alpaydin [14] proposed to optimize the objective function", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Putting the above discussions together, our implementation of LMKL based on the kernel trick requiresO(nM) operations at each iteration, which is much faster than the original implementation in G\u00f6nen and Alpaydin [14] with O(nMd) operations at each iteration.", "startOffset": 213, "endOffset": 217}, {"referenceID": 6, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 143, "endOffset": 146}, {"referenceID": 23, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 154, "endOffset": 158}, {"referenceID": 24, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 247, "endOffset": 250}, {"referenceID": 55, "context": "2 Details on the Visual Image Categorization Experiment We compute 9 bag-of-words features, each with a dictionary size of 512, resulting in 9 \u03c7-Kernels [57].", "startOffset": 153, "endOffset": 157}, {"referenceID": 38, "context": "The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51].", "startOffset": 66, "endOffset": 70}, {"referenceID": 49, "context": "The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51].", "startOffset": 152, "endOffset": 156}, {"referenceID": 4, "context": "Assignment of local features to visual words is done using rank-mapping [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 30, "context": "We compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "We compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14].", "startOffset": 72, "endOffset": 76}], "year": 2016, "abstractText": "We propose a localized approach to multiple kernel learning that can be formulated as a convex optimization problem over a given cluster structure. For which we obtain generalization error guarantees and derive an optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that convex localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts.", "creator": "LaTeX with hyperref package"}}}