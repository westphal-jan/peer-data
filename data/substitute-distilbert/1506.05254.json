{"id": "1506.05254", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Gradient Estimation Using Stochastic Computation Graphs", "abstract": "in a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function variable defined by an expectation over minimal collection of random variables, which might be part is a probabilistic model or the external intelligence. estimating the gradient of this loss function, using samples, lies at the core of gradient - based learning algorithms for these problems. we introduce diverse features of stochastic computation graphs - - - directed acyclic graphs as include both deterministic functions and conditional probability graphs - - - and describe how to easily and automatically minimize an unbiased estimator determining the loss function's gradient. the effective algorithm for computing the gradient estimator is through simple modification of the standard filtering algorithm. the generic scheme we propose independent estimators derived under variety amongst prior work, contrasted with g - reduction classes therein. it could assist researchers in developing intricate models involving a combination of stochastic descriptive deterministic information, enabling, for example, attention, memory, and control actions.", "histories": [["v1", "Wed, 17 Jun 2015 09:32:31 GMT  (277kb,D)", "http://arxiv.org/abs/1506.05254v1", null], ["v2", "Fri, 13 Nov 2015 03:19:18 GMT  (277kb,D)", "http://arxiv.org/abs/1506.05254v2", null], ["v3", "Tue, 5 Jan 2016 19:56:22 GMT  (277kb,D)", "http://arxiv.org/abs/1506.05254v3", "Advances in Neural Information Processing Systems 28 (NIPS 2015)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john schulman", "nicolas heess", "theophane weber", "pieter abbeel"], "accepted": true, "id": "1506.05254"}, "pdf": {"name": "1506.05254.pdf", "metadata": {"source": "CRF", "title": "Gradient Estimation Using Stochastic Computation Graphs", "authors": ["John Schulman", "Nicolas Heess", "Pieter Abbeel"], "emails": ["joschu@eecs.berkeley.edu", "heess@google.com", "theophane@google.com", "pabbeel@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "The great success of neural networks is due in part to the simplicity of the backpropagation algorithm, which allows one to efficiently compute the gradient of any loss function defined as a composition of differentiable functions. This simplicity has allowed researchers to search in the space of architectures for those that are both highly expressive and conducive to optimization; yielding, for example, convolutional neural networks in vision [12] and LSTMs for sequence data [9]. However, the backpropagation algorithm is only sufficient when the loss function is a deterministic, differentiable function of the parameter vector.\nA rich class of problems arising throughout machine learning requires optimizing loss functions that involve an expectation over random variables. Two broad categories of these problems are (1) likelihood maximization in probabilistic models with latent variables [17, 18], and (2) policy gradients in reinforcement learning [5, 23, 26]. Combining ideas from from those two perennial topics, recent models of attention [15] and memory [29] have used networks that involve a combination of stochastic and deterministic operations.\nIn most of these problems, from probabilistic modeling to reinforcement learning, the loss functions and their gradients are intractable, as they involve either a sum over an exponential number of latent variable configurations, or high-dimensional integrals that have no analytic solution. Prior work (see Section 6) has provided problem-specific derivations of Monte-Carlo gradient estimators, however, to our knowledge, no previous work addresses the general case.\nAppendix C recalls several classic and recent techniques in variational inference [14, 10, 21] and reinforcement learning [23, 25, 15], where the loss functions can be straightforwardly described using\nar X\niv :1\n50 6.\n05 25\n4v 1\n[ cs\n.L G\n] 1\n7 Ju\nthe formalism of stochastic computation graphs that we introduce. For these examples, the variancereduced gradient estimators derived in prior work are special cases of the results in Sections 3 and 4.\nThe contributions of this work are as follows:\n\u2022 We introduce a formalism of stochastic computation graphs, and in this general setting, we derive unbiased estimators for the gradient of the expected loss.\n\u2022 We show how this estimator can be computed as the gradient of a certain differentiable function (which we call the surrogate loss), hence, it can be computed efficiently using the backpropagation algorithm. This observation enables a practitioner to write an efficient implementation using automatic differentiation software.\n\u2022 We describe variance reduction techniques that can be applied to the setting of stochastic computation graphs, generalizing prior work from reinforcement learning and variational inference.\n\u2022 We briefly describe how to generalize some other optimization techniques to this setting: majorization-minimization algorithms, by constructing an expression that bounds the loss function; and quasi-Newton / Hessian-free methods [13], by computing estimates of Hessian-vector products.\nThe main practical result of this article is that to compute the gradient estimator, one just needs to make a simple modification to the backpropagation algorithm, where extra gradient signals are introduced at the stochastic nodes. Equivalently, the resulting algorithm is just the backpropagation algorithm, applied to the surrogate loss function, which has extra terms introduced at the stochastic nodes. The modified backpropagation algorithm is presented in Section 5."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Gradient Estimators for a Single Random Variable", "text": "This section will discuss computing the gradient of an expectation taken over a single random variable\u2014the estimators described here will be the building blocks for more complex cases with multiple variables. Suppose that x is a random variable, f is a function (say, the cost), and we are interested in computing \u2202\u2202\u03b8Ex [f(x)]. There are a few different ways that the process for generating x could be parameterized in terms of \u03b8, which lead to different gradient estimators.\n\u2022 We might be given a parameterized probability distribution x \u223c p(\u00b7; \u03b8). In this case, we can use the score function (SF) estimator [3]:\n\u2202\n\u2202\u03b8 Ex [f(x)] = Ex\n[ f(x) \u2202\n\u2202\u03b8 log p(x; \u03b8)\n] . (1)\nThis classic equation is derived as follows: \u2202\n\u2202\u03b8 Ex [f(x)] =\n\u2202\n\u2202\u03b8\n\u222b dx p(x; \u03b8)f(x) = \u222b dx \u2202\n\u2202\u03b8 p(x; \u03b8)f(x)\n= \u222b dx p(x; \u03b8) \u2202\n\u2202\u03b8 log p(x; \u03b8)f(x) = Ex\n[ f(x) \u2202\n\u2202\u03b8 log p(x; \u03b8)\n] (2)\nThis equation is valid if and only if p(x; \u03b8) is a continuous function of \u03b8; however, it does not need to be a continuous function of x [4].\n\u2022 x may be a deterministic, differentiable function of \u03b8 and another random variable z, i.e., we can write x(z, \u03b8). Then, we can use the pathwise derivative (PD) estimator, defined as follows.\n\u2202\n\u2202\u03b8 Ez [f(x(z, \u03b8))] = Ez\n[ \u2202\n\u2202\u03b8 f(x(z, \u03b8))\n] . (3)\nThis equation, which merely swaps the derivative and expectation, is valid if and only if f(x(z, \u03b8)) is a continuous function of \u03b8 for all z [4]. 1 That is not true if, for example, f is a step function. 1 Note that for the pathwise derivative estimator, f(x(z, \u03b8)) merely needs to be a continuous function of \u03b8\u2014it is sufficient that this function is almost-everywhere differentiable. A similar statement can be made about p(x; \u03b8) and the score function estimator. See Glasserman [4] for a detailed discussion of the technical requirements for these gradient estimators to be valid.\n\u2022 Finally \u03b8 might appear both in the probability distribution and inside the expectation, e.g., in \u2202 \u2202\u03b8Ez\u223cp(\u00b7; \u03b8) [f(x(z, \u03b8))]. Then the gradient estimator has two terms:\n\u2202\n\u2202\u03b8 Ez\u223cp(\u00b7; \u03b8) [f(x(z, \u03b8))] = Ez\u223cp(\u00b7; \u03b8)\n[ \u2202\n\u2202\u03b8 f(x(z, \u03b8)) +\n( \u2202\n\u2202\u03b8 log p(z; \u03b8)\n) f(x(z, \u03b8)) ] . (4)\nThis formula can be derived by writing the expectation as an integral and differentiating, as in Equation (2).\nIn some cases, it is possible to reparameterize a probabilistic model\u2014moving \u03b8 from the distribution to inside the expectation or vice versa. See [3] for a general discussion, and see [10, 21] for a recent application of this idea to variational inference.\nThe SF and PD estimators are applicable in different scenarios and have different properties.\n1. SF is valid under more permissive mathematical conditions than PD. SF can be used if f is discontinuous, or if x is a discrete random variable.\n2. SF only requires sample values f(x), whereas PD requires the derivatives f \u2032(x). In the context of control (reinforcement learning), SF can be used to obtain unbiased policy gradient estimators in the \u201cmodel-free\u201d setting where we have no model of the dynamics, we only have access to sample trajectories.\n3. SF tends to have higher variance than PD, when both estimators are applicable (see for instance [3, 21]). The variance of SF increases (often linearly) with the dimensionality of the sampled variables. Hence, PD is usually preferable when x is high-dimensional. On the other hand, PD has high variance if the function f is rough, which occurs in many time-series problems due to an \u201cexploding gradient problem\u201d / \u201cbutterfly effect\u201d.\n4. PD allows for a deterministic limit, SF does not. This idea is exploited by the deterministic policy gradient algorithm [22].\nNomenclature. The methods of estimating gradients of expectations have been independently proposed in several different fields, which use differing terminology. What we call the score function estimator (via [3]) is alternatively called the likelihood ratio estimator [5] and REINFORCE [26]. We chose this term because the score function is a well-known object in statistics. What we call the pathwise derivative estimator (from the mathematical finance literature [4] and reinforcement learning [16]) is alternatively called infinitesimal perturbation analysis and stochastic backpropagation [21]. We chose this term because pathwise derivative is evocative of propagating a derivative through a sample path."}, {"heading": "2.2 Stochastic Computation Graphs", "text": "The results of this article will apply to stochastic computation graphs, which are defined as follows:\nDefinition 1 (Stochastic Computation Graph). A directed, acyclic graph, with three types of nodes:\n1. Input nodes, which are set externally, including the parameters we differentiate with respect to.\n2. Deterministic nodes, which are functions of their parents.\n3. Stochastic nodes, which are distributed conditionally on their parents.\nEach parent v of a non-input node w is connected to it by a directed edge (v, w).\nIn the subsequent diagrams of this article, we will use circles to denote stochastic nodes and squares to denote deterministic nodes, as illustrated below. The structure of the graph fully specifies what estimator we will use: SF, PD, or a combination thereof. This graphical notation is shown below, along with the single-variable estimators from Section 2.1.\n\u03b8 Input node\nDeterministic node\nStochastic node\n\u03b8 x f\nGives SF estimator\n\u03b8\nz\nx f\nGives PD estimator"}, {"heading": "2.3 Simple Examples", "text": "Several simple examples that illustrate the stochastic computation graph formalism are shown below. The gradient estimators can be described by writing the expectations as integrals and differentiating, as with the simpler estimators from Section 2.1. However, they are also implied by the general results that we will present in Section 3.\nThese simple examples illustrate several important motifs, where stochastic and deterministic nodes are arranged in series or in parallel. For example, note that in (2) the derivative of y does not appear in the estimator, since the path from \u03b8 to f is \u201cblocked\u201d by x. Similarly, in (3), p(y | x) does not appear (this type of behavior is particularly useful if we only have access to a simulator of a system, but not access to the actual likelihood function). On the other hand, (4) has a direct path from \u03b8 to f , which contributes a term to the gradient estimator. (5) resembles a parameterized Markov reward process, and it illustrates that we\u2019ll obtain score function terms of the form grad log-probability \u00d7 future costs.\nx h1 h2\nW1 W2b1 b2\nsoftmax\ny=label\ncrossentropy loss The examples above all have one input \u03b8, but the formalism accommodates models with multiple inputs, for example a stochastic neural network with multiple layers of weights and biases, which may influence different subsets of the stochastic and cost nodes. See Appendix C for nontrivial examples with stochastic nodes and multiple inputs. The figure on the right shows a deterministic computation graph representing classification loss for a two-layer neural network, which has four parameters (W1, b1,W2, b2) (weights and biases). Of course, this deterministic computation graph is a special type of stochastic computation graph."}, {"heading": "3 Main Results on Stochastic Computation Graphs", "text": ""}, {"heading": "3.1 Gradient Estimators", "text": "This section will consider a general stochastic computation graph, in which a certain set of nodes are designated as costs, and we would like to compute the gradient of the sum of costs with respect to some input node \u03b8.\nIn brief, the main results of this section are as follows:\n1. We derive a gradient estimator for an expected sum of costs in a stochastic computation graph. This estimator contains two parts (1) a score function part, which is a sum of terms grad logprob of variable \u00d7 sum of costs influenced by variable; and (2) a pathwise derivative term, that propagates the dependence through differentiable functions.\n2. This gradient estimator can be computed efficiently by differentiating an appropriate \u201csurrogate\u201d objective function.\nLet \u0398 denote the set of input nodes, D the set of deterministic nodes, and S the set of stochastic nodes. Further, we will designate a set of cost nodes C, which are scalar-valued and deterministic. (Note that there is no loss of generality in assuming that the costs are deterministic\u2014if a cost is stochastic, we can simply append a deterministic node that applies the identity function to it.) We will use \u03b8 to denote an input node (\u03b8 \u2208 \u0398) that we differentiate with respect to. In the context of machine learning, we will usually be most concerned with differentiating with respect to a parameter vector (or tensor), however, the theory we present does not make any assumptions about what \u03b8 represents.\nNotation Glossary \u0398: Input nodes D: Deterministic nodes S: Stochastic nodes C: Cost nodes v \u227a w: v influences w v \u227aD w: v deterministically influences w DEPSv:\u201cdependencies\u201d, {w \u2208 \u0398 \u222a S | w \u227aD V } Q\u0302v: sum of cost nodes influenced by v. v\u0302: denotes the sampled value of the node v. For the results that follow, we need to define the notion of \u201cinfluence\u201d, for which we will introduce two relations \u227a and \u227aD. The relation v \u227a w (\u201cv influences w\u201d) means that there exists a sequence of nodes a1, a2, . . . , aK , with K \u2265 0, such that (v, a1), (a1, a2), . . . , (aK\u22121, aK), (aK , w) are edges in the graph. The relation v \u227aD w (\u201cv is deterministically influences w\u201d) is defined similarly, except that now we require that each ak is a deterministic node. For example, in Figure 1, diagram (5) above, \u03b8 influences {x1, x2, f1, f2}, but it only deterministically influences {x1, x2}. Next, we will establish a condition that is sufficient for the existence of the gradient. Namely, we will stipulate that every edge (v, w) with w lying in the \u201cinfluenced\u201d set of \u03b8 corresponds to a differentiable dependency: if w is deterministic, then the Jacobian \u2202w\u2202v must exist; if w is stochastic, then the probability mass function p(w | v, . . . ) must be differentiable with respect to v.\nMore formally:\nCondition 1 (Differentiability Requirements). Given input node \u03b8 \u2208 \u0398, for all edges (v, w) which satisfy \u03b8 \u227aD v and x \u227aD w, then the following condition holds: if w is deterministic, Jacobian \u2202w\u2202v exists, and if w is stochastic, then the derivative of the probability mass function \u2202 \u2202vp(w | PARENTSw) exists.\nNote that 1 does not require that all the functions in the graph are differentiable. If the path from an input \u03b8 to deterministic node v is blocked by stochastic nodes, then v may be a nondifferentiable function of its parents. If a path from input \u03b8 to stochastic node v is blocked by other stochastic nodes, the likelihood of v given its parents need not be differentiable; in fact, it does not need to be known2.\n2This fact is particularly important for reinforcement learning, allowing us to compute policy gradient estimates despite having a discontinuous dynamics function or reward function.\nWe need a few more definitions to state the main theorems. Let DEPSv := {w \u2208 \u0398 \u222a S | w \u227aD V }, the \u201cdependencies\u201d of node v, i.e., the set of nodes that deterministically influence it. Note the following:\n\u2022 If v \u2208 S, the probability mass function of v is a function of DEPSv , i.e., we can write p(v |DEPSv). \u2022 If v \u2208 D, v is a deterministic function of DEPSv , so we can write v(DEPSv).\nLet Q\u0302v := \u2211 c v, c\u2208C c\u0302, i.e., the sum of costs downstream of node v. These costs will be treated as constant, fixed to the values obtained during sampling. In general, we will use the hat symbol v\u0302 to denote a sample value of variable v, which will be treated as constant in the gradient formulae.\nNow we can write down a general expression for the gradient of the expected sum of costs in a stochastic computation graph:\nTheorem 1. Suppose that \u03b8 \u2208 \u0398 satisfies 1. Then the following two equivalent equations hold:\n\u2202\n\u2202\u03b8 E [\u2211 c\u2208C c ] = E  \u2211 w\u2208S, \u03b8\u227aDw ( \u2202 \u2202\u03b8 log p(w | DEPSw) ) Q\u0302w + \u2211 c\u2208C \u03b8\u227aDc \u2202 \u2202\u03b8 c(DEPSc)  (5)\n= E \u2211 c\u2208C c\u0302 \u2211 w\u227ac, \u03b8\u227aDw \u2202 \u2202\u03b8 log p(w | DEPSw) + \u2211 c\u2208C, \u03b8\u227aDc \u2202 \u2202\u03b8 c(DEPSc)  (6) Proof: See Appendix A.\nThe estimator expressions above have two terms. The first term is due to the influence of \u03b8 on probability distributions. The second term is due to the influence of \u03b8 on the cost variables through a chain of differentiable functions. The distribution term involves a sum of gradients times \u201cdownstream\u201d costs. The first term in Equation (5) involves a sum of gradients times \u201cdownstream\u201d costs, whereas the first term in Equation (6) has a sum of costs times \u201cupstream\u201d gradients.\n3.2 Surrogate Loss Functions Surrogate Loss Computation Graph\nThe next corollary lets us write down a \u201csurrogate\u201d objective L, which is a function of the inputs that we can differentiate to obtain an unbiased gradient estimator.\nCorollary 1. Let L(\u0398,S) := \u2211w log p(w | DEPSw)Q\u0302w +\u2211 c\u2208C c(DEPSc). Then differentiation ofL gives us an unbiased gra-\ndient estimate: \u2202\u2202\u03b8E [\u2211 c\u2208C c ] = E [ \u2202 \u2202\u03b8L(\u0398,S) ] .\nOne practical consequence of this result is that we can apply a standard automatic differentiation procedure to L to obtain an unbiased gradient estimator. In other words, we convert the stochastic computation graph into a deterministic computation graph, to which we can apply the backpropagation algorithm.\nThere are several alternative ways to define the surrogate objective function that give the same gradient as L from Corollary 1. We could also write L(\u0398,S) := \u2211v p(v\u0302 | DEPSv)P\u0302v Q\u0302w +\u2211c\u2208C c(DEPSc), where P\u0302v is the probability p(v | DEPSv) obtained during sampling, which is viewed as a constant.\nThe surrogate objective from Corollary 1 is actually an upper bound on the true objective in the case that (1) all costs c \u2208 C are negative,\n(2) the the costs are not deterministically influenced by the parameters \u0398. This construction allows from majorization-minimization algorithms (similar to EM) to be applied to general stochastic computation graphs. See Appendix B for details."}, {"heading": "3.3 Higher-Order Derivatives.", "text": "The gradient estimator for a stochastic computation graph is itself a stochastic computation graph. Hence, it is possible to compute the gradient yet again (for each component of the gradient vector), and get an estimator of the Hessian. For most problems of interest, it is not efficient to compute this dense Hessian. On the other hand, one can also differentiate the gradient-vector product to get a Hessian-vector product\u2014this computation is usually not much more expensive than the gradient computation itself. The Hessian-vector product can be used to implement a quasi-Newton algorithm via the conjugate gradient algorithm [28]. A variant of this technique, called Hessian-free optimization [13], has been used to train large neural networks."}, {"heading": "4 Variance Reduction", "text": "Consider estimating \u2202\u2202\u03b8Ex\u223cp(\u00b7; \u03b8) [f(x)]. Clearly this expectation is unaffected by subtracting a constant b from the integrand, which gives \u2202\u2202\u03b8Ex\u223cp(\u00b7; \u03b8) [f(x)\u2212 b]. Taking the score function estimator, we get \u2202\u2202\u03b8Ex\u223cp(\u00b7; \u03b8) [f(x)] = Ex\u223cp(\u00b7; \u03b8) [ \u2202 \u2202\u03b8 log p(x; \u03b8)(f(x)\u2212 b) ] . Taking b = Ex [f(x)] generally leads to substantial variance reduction\u2014b is often called a baseline3 (see [6] for a more thorough discussion of baselines and their variance reduction properties).\nWe can make a general statement for the case of stochastic computation graphs\u2014that we can add a baseline to every stochastic node, which depends all of the nodes it doesn\u2019t influence. Let NONINFLUENCED(v) := {W | v \u2280W}.\nTheorem 2.\n\u2202\n\u2202\u03b8 E [\u2211 c\u2208C c ] = E \u2211 v\u2208S v \u03b8 ( \u2202 \u2202\u03b8 log p(v | PARENTSv) ) (Q\u0302v \u2212 b(NONINFLUENCED(v)) + \u2211 c\u2208C \u03b8 \u2202 \u2202\u03b8 c  Proof: See Appendix A."}, {"heading": "5 Algorithms", "text": "As shown in Section 3, the gradient estimator can be obtained by differentiating a surrogate objective function L. Hence, this derivative can be computed by performing the backpropagation algorithm on L. That is likely to be the most practical and efficient method, and can be facilitated by automatic differentiation software.\nAlgorithm 1 shows explicitly how to compute the gradient estimator in a backwards pass through the stochastic computation graph. The algorithm will recursively compute gv := \u2202\u2202vE [\u2211\nc\u2208C v\u227ac\nc ] at\nevery deterministic and input node v."}, {"heading": "6 Related Work", "text": "As discussed in Section 2, the score function and pathwise derivative estimators have been used in a variety of different fields, under different names. See [3] for a review of gradient estimation, mostly from the simulation optimization literature. Glasserman\u2019s textbook provides an extensive treatment of various gradient estimators and Monte Carlo estimators in general. Griewank and Walther\u2019s textbook [8] is a comprehensive reference on computation graphs and automatic differentiation (of deterministic programs.) The notation and nomenclature we use is inspired by Bayes nets and influence diagrams [19]. (In fact, a stochastic computation graph is a type of Bayes network; where the deterministic nodes correspond to degenerate probability distributions.)\nThe topic of gradient estimation has drawn significant recent interest in machine learning. Gradients for networks with stochastic units was investigated in Bengio et al. [2], though they are concerned\n3The optimal baseline is in fact the weighted expectation Ex[f(x)s(x)2]\nEx[s(x)2] where s(x) = \u2202 \u2202\u03b8 log p(x; \u03b8)\nAlgorithm 1 Compute Gradient Estimator for Stochastic Computation Graph for v \u2208 Graph do . Initialization at output nodes\ngv = { 1dim v if v \u2208 C 0dim v otherwise\nend for Compute Q\u0302w for all nodes w \u2208 Graph for v in REVERSETOPOLOGICALSORT(NONINPUTS) do . Reverse traversal\nfor w \u2208 PARENTSv do if not ISSTOCHASTIC(w) then\nif ISSTOCHASTIC(v) then gw += ( \u2202 \u2202w log p(v | PARENTSv))Q\u0302w else gw += ( \u2202v \u2202w )\nTgv end if\nend if end for\nend for return [g\u03b8]\u03b8\u2208\u0398\nwith differentiating through individual units and layers; not how to deal with arbitrarily structured models and loss functions. Kingma and Welling [11] consider a similar framework, although only with continuous latent variables, and point out that reparameterization can be used to to convert hierarchical Bayesian models into neural networks, which can then be trained by backpropagation.\nThe score function method is used to perform variational inference in general models (in the context of probabilistic programming) in Wingate and Weber [27], and similarly in Ranganath et al. [20]; both papers mostly focus on mean-field approximations without amortized inference. It is used to train generative models using neural networks with discrete stochastic units in Mnih and Gregor [14] and Gregor et al. in [7]; both amortize inference by using an inference network.\nGenerative models with continuous valued latent variables networks are trained (again using an inference network) with the reparametrization method by Rezende, Mohamed, and Wierstra [21] and by Kingma and Welling [10]. Rezende et al. also provide a detailed discussion of reparameterization, including a discussion comparing the variance of the SF and PD estimators.\nBengio, Leonard, and Courville [2] have recently written a paper about gradient estimation in neural networks with stochastic units or non-differentiable activation functions\u2014including Monte Carlo estimators and heuristic approximations. The notion that policy gradients can be computed in multiple ways was pointed out in early work on policy gradients by Williams [26]. However, all of this prior work deals with specific structures of the stochastic computation graph and does not address the general case."}, {"heading": "7 Conclusion", "text": "We have developed a framework for describing a computation with stochastic and deterministic operations, called a stochastic computation graph. Given a stochastic computation graph, we can automatically obtain a gradient estimator, given that the graph satisfies the appropriate conditions on differentiability of the functions at its nodes. The gradient can be computed efficiently in a backwards traversal through the graph: one approach is to apply the standard backpropagation algorithm to one of the surrogate loss functions from Section 3; another approach (which is roughly equivalent) is to apply a modified backpropagation procedure shown in Algorithm 1. The results we have presented are sufficiently general to automatically reproduce a variety of gradient estimators that have been derived in prior work in reinforcement learning and probabilistic modeling, as we show in Appendix C. We hope that this work will facilitate further development of interesting and expressive models."}, {"heading": "8 Acknowledgements", "text": "We would like to thank Shakir Mohamed, Dave Silver, Yuval Tassa, Andriy Mnih, and others at DeepMind for insightful comments."}, {"heading": "A Proofs", "text": "Theorem 1\nWe will consider the case that all of the random variables are continuous-valued, thus the expectations can be written as integrals. The case of discrete random variables is similar. We will differentiate the expectation of a single cost term; summing over these terms yields Equation (6).\nEv\u2208S, v\u227ac [c] = \u222b \u220f v\u2208S, v\u227ac p(v | DEPSv)dv c(DEPSc) (7)\n\u2202\n\u2202\u03b8 Ev\u2208S, v\u227ac\n[c] = \u2202\n\u2202\u03b8 \u222b \u220f v\u2208S, v\u227ac p(v | DEPSv)dv c(DEPSc) (8)\n= \u222b \u220f v\u2208S, v\u227ac p(v | DEPSv)dv \u2211 v\u2208S, v\u227ac [ \u2202 \u2202\u03b8p(v | DEPSv) p(v | DEPSv) c(DEPSc) + \u2202 \u2202\u03b8 c(DEPSc) ] (9)\n= \u222b \u220f v\u2208S, v\u227ac p(v | DEPSv)dv \u2211 v\u2208S, v\u227ac [( \u2202 \u2202\u03b8 log p(v | DEPSv) ) c(DEPSc) + \u2202 \u2202\u03b8 c(DEPSc) ] (10)\n= Ev\u2208S, v\u227ac\n[ \u2202\n\u2202\u03b8 log p(v | DEPSv)c\u0302+\n\u2202\n\u2202\u03b8 c(DEPSc)\n] (11)\nEquation (9) requires that the integrand is differentiable, which is satisfied if all of the PDFs and c(DEPSc) are differentiable. Clearly\nEquation (6) follows by summing over all costs c \u2208 C. Equation (5) follows from rearrangement of the terms in this equation.\nTheorem 2\nIt suffices to show that for a particular node v \u2208 S, the following expectation (taken over all variables) vanishes\n= E [( \u2202\n\u2202\u03b8 log p(v | PARENTSv)\n) b(NONINFLUENCED(v)) ] . (12)\nAnalogously to NONINFLUENCED(v), define INFLUENCED(v) := {w | w v}. Note that the nodes can be ordered so that NONINFLUENCED(v) all come before v in the ordering. Thus, we can write\nENONINFLUENCED(v) [ EINFLUENCED(v) [( \u2202\n\u2202\u03b8 log p(v | PARENTSv)\n) b(NONINFLUENCED(v)) ]] (13)\n= ENONINFLUENCED(v) [ EINFLUENCED(v) [( \u2202\n\u2202\u03b8 log p(v | PARENTSv)\n)] b(NONINFLUENCED(v)) ] (14)\n= ENONINFLUENCED(v) [0 \u00b7 b(NONINFLUENCED(v))] (15) = 0 (16)\nwhere we used EINFLUENCED(v) [( \u2202 \u2202\u03b8 log p(v | PARENTSv) )] = Ev [( \u2202 \u2202\u03b8 log p(v | PARENTSv) )] = 0."}, {"heading": "B Surrogate as an Upper Bound, and MM Algorithms", "text": "L has additional significance besides allowing us to estimate the gradient of the expected sum of costs. Under certain conditions, L is a upper bound on on the true objective (plus a constant).\nWe shall make two restrictions on the stochastic computation graph: (1) first, that all costs c \u2208 C are negative. (2) the the costs are not deterministically influenced by the parameters \u0398. First, let us use importance sampling to write down the expectation of a given cost node, when the sampling\ndistribution is different from the distribution we are evaluating: for parameter \u03b8 \u2208 \u0398, \u03b8 = \u03b8old is used for sampling, but we are evaluating at \u03b8 = \u03b8new.\nEv\u227ac | \u03b8new [c\u0302] = Ev\u227ac | \u03b8old c\u0302 \u220f v\u227ac, \u03b8\u227aDv Pv(v | DEPSv\\\u03b8, \u03b8new) Pv(v | DEPSv\\\u03b8, \u03b8old)  (17)\n\u2264 Ev\u227ac | \u03b8old c\u0302 log  \u220f v\u227ac, \u03b8\u227aDv Pv(v | DEPSv\\\u03b8, \u03b8new) Pv(v | DEPSv\\\u03b8, \u03b8old) + 1   (18)\nwhere the second line used the inequality x \u2265 log x+ 1, and the sign is reversed since c\u0302 is negative. Summing over c \u2208 C and rearranging we get\nES | \u03b8new [\u2211 c\u2208C c\u0302 ] \u2264 ES | \u03b8old [\u2211 c\u2208C c\u0302+ \u2211 v\u2208S log ( p(v | DEPSv\\\u03b8, \u03b8new) p(v | DEPSv\\\u03b8, \u03b8old) ) Q\u0302v ] (19)\n= ES | \u03b8old [\u2211 v\u2208S log p(v | DEPSv\\\u03b8, \u03b8new)Q\u0302v ] + const (20)\nEquation (20) allows for majorization-minimization algorithms (like the EM algorithm) to be used to optimize with respect to \u03b8. In fact, similar equations have been derived by interpreting rewards (negative costs) as probabilities, and then taking the variational lower bound on log-probability (e.g., [24])."}, {"heading": "C Examples", "text": "C.1 Generalized EM Algorithm and Variational Inference.\nThe generalized EM algorithm maximizes likelihood in a probabilistic model with latent variables [18]. Suppose the probabilistic model defines a probability distribution p(x, z; \u03b8) where x is observed, z is a latent variable, and \u03b8 is a parameter of the distribution. The generalized EM algorithm maximizes the variational lower bound, which is defined by an expectation over q:\nL(\u03b8, q) = Ez\u223cq [ log ( p(x, z; \u03b8)\nq(z)\n)] . (21)\nThe generalized EM algorithm can take many different forms, leading to different gradient estimation problems.\nx h1 h2 h3\nr1 r2 r3\n\u03c61 \u03c62 \u03c63\n\u03b81 \u03b82 \u03b83\nNeural variational inference. [14] propose a generalized EM algorithm for multi-layered latent variable models such as sigmoidal belief networks that employs an inference network, an explicit parameterization of q as a function of the observed data x, to allow for fast approximate inference. The generative model and inference network take the form\np\u03b8(x) = \u2211 h1,h2 p\u03b81(x|h1)p\u03b82(h1|h2)p\u03b83(h2)\nq\u03c6(h1, h2|x) = q\u03c61(h1|x)q\u03c62(h2|h1), and thus\nL(\u03b8, \u03c6) = Eh\u223cq\u03c6 log p\u03b81(x|h1)q\u03c61(h1|x)\ufe38 \ufe37\ufe37 \ufe38 =r1 + log p\u03b82(h1|h2)p\u03b83(h2) q\u03c62(h2|h1)\ufe38 \ufe37\ufe37 \ufe38 =r2  .\nGiven a sample h \u223c q\u03c6 an unbiased estimate of the gradient is obtained \u2202L\n\u2202\u03b8 \u2248 \u2202 \u2202\u03b8 log p\u03b81(x|h1) + \u2202 \u2202\u03b8 log p\u03b82(h1|h2) + \u2202 \u2202\u03b8 log p\u03b83(h2) \u2202L \u2202\u03c6\n\u2248 \u2202 \u2202\u03c6 log q\u03c61(h1|x)(r1 + r2 \u2212 b(x)) + \u2202 \u2202\u03c6 log q\u03c62(h2|h1)(r2 \u2212 b(h1)) (22)\nx h1 z h2 x\u0303\nL\n\u03c6 \u03b8\n\u21d3 Reparameterization\nx h1 z h2 x\u0303\nL\n\u03c6 \u03b8\nVariational Autoencoder, Deep Latent Gaussian Models and Reparameterization. [10, 21] consider a similar formulation to [14] but have continuous latent variables and can thus re-parameterize their inference network to enable the use of the PD estimator:\nLorig(\u03b8, \u03c6) = Eh\u223cq\u03c6 [ log\np\u03b8(x|h)p\u03b8(h) q\u03c6(h|x)\n] (23)\nLre(\u03b8, \u03c6) = E \u223c\u03c1 [log p\u03b8(x|h\u03c6( , x)) + log p\u03b8(h\u03c6( , x))] +H[q\u03c6(\u00b7|x)] (24)\nwhere the second term, the entropy of qphi can be computed analytically for the parametric forms of q considered in the paper (Gaussians). Given \u223c q\u03c6 an estimate of the gradient is obtained as\n\u2202Lre \u2202\u03b8 \u2248 \u2202 \u2202\u03b8\n[log p\u03b8(x|h\u03c6( , x)) + log p\u03b8(h\u03c6( , x))] , (25)\n\u2202Lre \u2202\u03c6 \u2248 \u2202 \u2202\u03c6\n[ \u2202\n\u2202h log p\u03b8(x|h\u03c6( , x)) +\n\u2202\n\u2202h log p\u03b8(h\u03c6( , x))\n] \u2202h\n\u2202\u03c6 + \u2202 \u2202\u03c6 H[q\u03c6(\u00b7|x)]\n(26)\nC.2 Policy Gradients in Reinforcement Learning.\nIn reinforcement learning, an agent interacts with an environment according to its policy \u03c0 and receives a reward. The goal is to maximize the expected sum of rewards, the return, under the trajectory distribution that is specified jointly by the environment dynamics and the policy. Policy gradient methods seek to directly estimate the gradient of expected return with respect to the policy parameters [26, 1, 23]. The RL case is especially interesting as we typically assume that the environment dynamics are not available analytically and can only be sampled (i.e. some of the nodes in the SCG can only be simulated. This has implication for gradient estimation. Below we distinguish two important cases: Markov decision processes (MDP) and partially observable Markov decision processes (POMDP).\n\u03b8\ns1 s2 . . . sT\na1 a2 . . . aT\nr1 r2 . . . rT\nMDPs: In the MDP case, the expectation is taken with respect to the distribution over state (s) and action (a) sequences\nL(\u03b8) = E\u03c4\u223cp\u03b8 [ T\u2211 t=1 r(st, at) ] , (27)\nwhere \u03c4 = (s1, a1, s2, a2, . . . ) are trajectories and the distribution over trajectories is defined in terms of the environment dynamics pE(st+1|st, at) and the policy \u03c0\u03b8: p\u03b8(\u03c4) = pE(s1) \u220f t \u03c0\u03b8(at|st)pE(st+1|st, at). r are rewards (negative costs in the terminology of the rest of the paper). The classic REINFORCE [26] estimate of the gradient is given by\n\u2202\n\u2202\u03b8 L = E\u03c4\u223cp\u03b8 [ T\u2211 t=1 \u2202 \u2202\u03b8 log \u03c0\u03b8(at|st) ( T\u2211 t\u2032=t r(st\u2032 , at\u2032)\u2212 bt(st) )] , (28)\nwhere bt(st) is an arbitrary baseline which is often chosen to be Vt(st) = E\u03c4\u223cp\u03b8 [\u2211T t\u2032=t r(st\u2032 , at\u2032) ] , i.e. the well-known state-value function. (Equation (28)) corresponds to an application of the SF estimator at the stochastic nodes at. It is worth noting that a Monte Carlo estimate of (Equation (28)) only requires simulating from the environment by running trajectories forward according to the current policy. This is due to the property of the SF estimator which only requires evaluation (sampling in the stochastic case) of the nodes downstream of the stochastic node at\nPOMDPs.\n\u03b8\ns1 s2 . . . sT\no1 o2 . . . oT\nm1 m2 . . . mT\na1 a2 . . . aT\nr1 r2 . . . rT\nPOMDPs differ from MDPs in that the state st of the environment is not observed directly but, as in latent-variable time series models, only through stochastic observations ot, which depend on the latent states st via pE(ot|st). The policy therefore has to be a function of the history of past observations \u03c0\u03b8(at|m\u03b8(o1 . . . ot\u22121). For instance it can take the form of a RNN [25, 15]. A REINFORCE gradient estimate is then given by\n\u2202\n\u2202\u03b8 L = E\u03c4\u223cp\u03b8 [ T\u2211 t=1 \u2202\n\u2202\u03b8 log \u03c0\u03b8(at|m\u03b8(o1 . . . ot\u22121))( T\u2211 t\u2032=t r(st\u2032 , at\u2032)\u2212 bt(st) )] .\n(29)\nNote that, at each time step t, the gradient \u2202\u2202mt log \u03c0\u03b8 at the stochastic node at is estimated using the SF estimator, and then backpropagated in the usual manner in the RNN."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In a variety of problems originating in supervised, unsupervised, and reinforce-<lb>ment learning, the loss function is defined by an expectation over a collection<lb>of random variables, which might be part of a probabilistic model or the exter-<lb>nal world. Estimating the gradient of this loss function, using samples, lies at<lb>the core of gradient-based learning algorithms for these problems. We introduce<lb>the formalism of stochastic computation graphs\u2014directed acyclic graphs that in-<lb>clude both deterministic functions and conditional probability distributions\u2014and<lb>describe how to easily and automatically derive an unbiased estimator of the loss<lb>function\u2019s gradient. The resulting algorithm for computing the gradient estimator<lb>is a simple modification of the standard backpropagation algorithm. The generic<lb>scheme we propose unifies estimators derived in variety of prior work, along with<lb>variance-reduction techniques therein. It could assist researchers in developing in-<lb>tricate models involving a combination of stochastic and deterministic operations,<lb>enabling, for example, attention, memory, and control actions.", "creator": "LaTeX with hyperref package"}}}