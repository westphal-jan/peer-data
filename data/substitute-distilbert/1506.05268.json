{"id": "1506.05268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Deep Denoising Auto-encoder for Statistical Speech Synthesis", "abstract": "this paper proposes highly deep denoising auto - encoder technique to extract better acoustic features for speech synthesis. the technique allows us algorithms automatically extract low - dimensional features from high dimensional spectral features in a non - linear, data - driven, unsupervised way. we compared the new physiological feature extractor with conventional mel - cepstral analysis in recognition - by - synthesis and text - to - speech filtering. later results confirm yes the proposed method increases the utility of audible speech outside both experiments.", "histories": [["v1", "Wed, 17 Jun 2015 10:17:59 GMT  (5330kb,D)", "http://arxiv.org/abs/1506.05268v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["zhenzhou wu", "shinji takaki", "junichi yamagishi"], "accepted": false, "id": "1506.05268"}, "pdf": {"name": "1506.05268.pdf", "metadata": {"source": "CRF", "title": "DEEP DENOISING AUTO-ENCODER FOR STATISTICAL SPEECH SYNTHESIS", "authors": ["Zhenzhou Wu", "Shinji Takaki", "Junichi Yamagishi"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Speech synthesis, HMM, DNN, Auto-encoder\n1. INTRODUCTION\nCurrent statistical parametric speech synthesis typically uses hidden Markov models (HMMs) to represent probability densities of speech trajectories given text [1]. This is a well-established method and it is straightforward to apply this framework for new languages. It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5]. It is known, however, that speech synthesized from statistical models still sounds somehow artificial and less natural compared to speech synthesized by the best unit selection systems.\nIt is often said that averaging in statistical synthesis systems partly removes spectral fine structure of natural speech, and thus there is room for the improving the segmental quality. A stochastic postfilter approach [6] proposes to use a deep neural network (DNN) to model the conditional probability of the spectral differences between natural and synthetic speech. The approach is able to reconstruct the spectral fine structure lost during modeling and has achieved significantly quality improvement for synthetic speech [6]. In this experiment, the HMM-based speech synthesiser was trained in the mel-cepstral domain, while the DNN-based postfiler was trained in the spectral domain.\nThis indicates that the current statistical parametric speech synthesis suffers from quality loss due to statistical averaging in the melcepstral domain, but also due to conversion from high-dimensional spectral features to lower dimensional mel-cepstral parameters and hence this brings us a new question: are current intermediate representations such as mel-cepstral coefficients appropriate for statistical training of acoustic models? Can we automatically find a more appropriate intermediate representation that suits acoustic modelling and results in better quality of synthetic speech?\nThe first author performed this work while at the National Institute of Informatics, Japan.\nThis work was supported in part by EPSRC through Programme Grant EP/I031022/1 (NST) and EP/J002526/1 (CAF). Shinji Takaki was supported in part by NAVER Labs.\nTo answer this question, this paper proposes a DNN-based feature extraction method. More specifically we propose to use a deep denoising auto-encoder technique as a non-linear robust feature extractor for speech synthesis and apply it to high-dimensional spectral features obtained from STRAIGHT vocoder [7]. We compare this data-driven, unsupervised feature extraction approach with the conventional mel-cepstral analysis, which is based on a linear discrete cosine transform of the log spectrum.\nThis paper is organised as follows: in Section 2, we outline related DNN-based approaches and in Section 3 we describe the proposed deep denoising auto-encoder technique. In Section 4, we mention how we train the model and the experimental conditions and evaluation results are shown in Section 5. Discussions and the summary of our findings are given in Section 6.\n2. RELATED WORK USING DNN AND AUTO-ENCODER\nThis section overviews related work using DNN and/or auto-encoder in the speech information processing field. DNN has been applied for acoustic modelling of speech synthesis. For instance [8] uses DNN to learn the relationship between input texts and the extract features instead of decision tree-based state tying. Restricted Boltzmann machines or deep belief networks have been used for modelling output probabilities of HMM states instead of GMMs [9]. Recurrent neural network or long-short term memory was used for prosody modelling [10] or acoustic trajectory modelling [11].\nTo the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].\nTechniques that are closely related to this paper are a spectral binary coding approach using deep auto-encoder proposed by Deng et al [17] and a speech enhancement approach using deep denoising auto-encoder where they try to reconstruct clean spectrum from noisy spectrum [18]. The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23]. Our key idea is however different from these as we use deep auto-encoder based continuous bottleneck features calculated from spectrum to reconstruct high-quality synthetic speech.\n3. AUTO-ENCODER"}, {"heading": "3.1. Basic Auto-encoder", "text": "Auto-encoder is an artificial neural network that is used generally for learning a compressed and distributed representation of a dataset. It consists of the encoder and the decoder. The encoder maps a input\nar X\niv :1\n50 6.\n05 26\n8v 1\n[ cs\n.S D\n] 1\n7 Ju\nn 20\n15\nvector x to a hidden representation y as follows:\ny = f\u03b8(x) = s(Wx+ b), (1)\nwhere \u03b8 = {W,b}. W and b represent a m \u00d7 n weight matrix and a bias vector of dimensionality m respectively, where n is the dimension of x. The function s is a non-linear transformation on the linear mapping Wx+ b. Frequently s is a sigmoid, a tanh, and a relu function. y, the output of the encoder, is then mapped to z, the output of the decoder. The mapping is performed by a linear function alone that employs a n \u00d7m weight matrix W \u2032 and a bias vector of dimensionality n as follows:\nz = g\u03b8\u2032(y) = W \u2032y + b\u2032, (2)\nor a linear mapping followed by a non-linear transformation t,\nz = g\u03b8\u2032(y) = t(W \u2032y + b\u2032), (3)\nwhere \u03b8\u2032 = {W\u2032,b\u2032}. The weight for the decoding is set as the transpose of the encoding weight [24] in order to allow more layers to be stacked together and be fine-tuned with stochastic gradient descend (SGD).\nIn general, the output z should be interpreted as a function of parameters {\u03b8, \u03b8\u2032} as z = g\u03b8\u2032(f\u03b8(x)). The parameters {\u03b8, \u03b8\u2032} are optimized such that the reconstructed z is as close as possible to the original x and maximizes P (x|z). A typical loss function used is the mean square error (MSE), i.e. L(x, z) = 1\nn |x\u2212 z|2."}, {"heading": "3.2. Denoising Auto-encoder", "text": "The denoising auto-encoder is a variant of the basic auto-encoder. It is reported that the denoising auto-encoder can extract features more robustly than the basic auto-encoder [25]. In the denoising auto-encoder, the original data x is first corrupted to x\u0303 before it is mapped to a higher representation f\u03b8(x\u0303) by an encoder. The decoder then maps the higher representation to the output z for reconstructing the original x. The denoising auto-encoder is trained such that the reconstructed z is as close as possible to the original data x. Note that it is only during training that the denoising auto-encoder is used to reconstruct the original x from the corrupted x\u0303."}, {"heading": "3.3. Deep Auto-encoder", "text": "Auto-encoder or denoisng auto-encoder can be made deeper by stacking multiple layers of encoders and decoders to form a deep architecture. [26] shows that deeper architecture produces better high-level features compared to the shallow architecture up to 4 encoding and 4 decoding layers. For constructing a deep auto-encoder pre-training is widely used. In pre-training, the number of layers in a deep auto-encoder increases twice as compare to a deep neural network (DNN) when stacking each pre-trained unit. It is reported that fine-tuning with back-propagation through a deep auto-encoder is ineffective due to vanishing gradients at the lower layers [27]. To over come this issue we restrict the decoding weight as the transpose of the encoding weight following [24], that is, W\u2032 = WT where WT denotes transpose of W. We describe the detail of training a deep auto-encoder in the next session.\n4. TRAINING A DEEP DENOISING AUTO-ENCODER"}, {"heading": "4.1. Greedy Layer-wise Pre-training", "text": "Each layer of a deep auto-encoder can be pre-trained greedily to minimize the reconstruction loss L(x, z) of the data locally. Figure 1 shows a procedure of constructing a deep auto-encoder using\npre-training. In pre-training a 1-hidden-layer auto-encoder is trained and the encoding output of the locally trained layer is used as the input for the next layer. This layer-wise training is repeated until the desired layer size is obtained. The encoding, decoding and loss functions of each layer are represented as follows:\nLayer 1:\ny1 = fW1,b1(x),\nz1 = gW\u20321,b\u20321(y1),\nL(x, z1) = |x\u2212 z1|2, Layer k (k>1):\nyk = fWk,bk (yk\u22121),\nzk = gW\u2032 k ,b\u2032 k (yk),\nL(yk\u22121, zk) = |yk\u22121 \u2212 zk|2.\n(4)\nNote that during the pre-training of the deep denoising auto-encoder, the input x, yk for each layer are corrupted to x\u0303 and y\u0303k respectively. After all layers are pre-trained, all the pre-trained layers are stacked for constructing a deep denoising auto-encoder in the same way as the deep auto-encoder."}, {"heading": "4.2. Fine-tuning", "text": "The purpose of fine-tuning is to minimize the reconstruction error L(x, z) over the entire dataset and a model architecture using error back-propagation [28]. We use the mean square error (MSE) for the loss function of a deep auto-encoder and it is represented as follows:\nE = N\u2211 i=1 |x(i) \u2212 z(i)|2, (5)\nwhereN is the total number of training examples. The partial derivatives w.r.t weight w(l)i,j is represented as follows:\n\u2202E\n\u2202w (l) i,j\n= \u2202E\n\u2202t (l) j\n\u00d7 \u2202t\n(l) j \u2202w (l) i,j\n(6)\nwhere t(l)j is the fan-in input to neuron j in layer l, and \u2202t\n(l) j \u2202w (l) i,j =\no (l\u22121) i , where o (l\u22121) i is the output from neuron i at layer l\u2212 1. \u2202E\u2202t(l)j\nis the error transfer function which can be calculated recursively following\n\u2202E\n\u2202t (l\u22121) j\n= \u2202o\n(l\u22121) i \u2202t (l\u22121) i \u00d7 L\u2211 j=1 \u2202E \u2202t (l) j \u00d7 \u2202t (l) j \u2202o (l\u22121) i\n(7)\nwhere \u2202t\n(l) j\n\u2202o (l\u22121) i\n= w (l) i,j . For the output tanh layer we have\n\u2202o (L) i \u2202t (L) i =\nsech2(t(L)i ). Once we have the gradients of error function w.r.t to the weight parameters, we can fine-tune the network with error backpropagation."}, {"heading": "4.3. Corrupted data", "text": "We used a masking technique reported in [25] to corrupt the training data for the denoising auto-encoder. This technique independently and randomly set the values of the training data in different dimensions to zero following a Bernoulli distribution. Figure 2 shows an example of original and masked spectra. In this figure, black points indicate masked regions.\n5. EVALUATION\nThis section shows experimental results. We have evaluated the proposed auto-encoder method in the context of analysis-by-synthesis condition and text-to-speech conditions. In the text-to-speech experiments, the synthetic voices using the proposed acoustic features were modeled using two state-of-the-art speech synthesis systems: HMM and DNN."}, {"heading": "5.1. Dataset", "text": "The dataset we use consists of 4569 short audio waveforms uttered by a professional English female speaker and each waveform is around 5 seconds long. For each waveform, we first extract its frequency spectra using STRAIGHT vocoder with 2049 FFT points. We then extract the low dimensional feature from each 2049-dim STRAIGHT spectrum using autoencoder. All data was sampled at 48 kHz. For comparison of the proposed method, we extracted mel-cepstral coefficients that use the same dimensions as that of auto-encoder. All other acoustic features such as log F0 and 25 aperiodicity band energies are the same for all the systems."}, {"heading": "5.2. Configurations of the deep denoising auto-encoder", "text": "Figure 3 shows the reconstruction mean square errors of autoencoders trained on raw frequency-warped spectrum with different number of hidden layers. It shows that the error decreases with more hidden layers, and that deep auto-encoder is better than shallow auto-encoder with the same bottleneck dimension. For the results in the rest of paper, we use architecture of the auto-encoder as 2049-500-180-120 for producing the 120-dim acoustic features, tanh units for all the layers and the inputs are 2049-dim Bark-scalebased frequency-warped spectrum, which are preprocessed with global contrast normalization. The hyperparameters used for the layer-by-layer pre-training are searched randomly and the set of values that produce the best results are selected. Table 1 shows the hyperparameters for the auto-encoders used in the experiments."}, {"heading": "5.3. Analysis-by-synthesis experimental results", "text": "First we report the analysis-by-synthesis experimental results. For this evaluation, we have divided the above database into three subsets, that is, training, validation and test. The training subset was used as training data for building the auto-encoder, the validation subset was used as a stopping criteria during training to prevent overfitting, and the test subset was used for measuring log-spectral distortion and listening test.\nFigure 4 shows the original and reconstructed spectra using each technique (mel-cepstral analysis, deep auto-encoder, deep denoising auto-encoder). We can clearly see that the deep auto-encoders reconstruct high-frequency parts more precisely than mel-cepstral analysis. Figure 5 shows log spectral distortion between the original spectra and reconstructed spectra, calculated on the test subset. We can observe that the deep auto-encoder has reduced the distortion significantly compared to the mel-cepstral analysis and denoising version further reduced the distortion. Figure 6 shows subjective preference scores of these methods. The number of listeners\nthat performed this test were seven. They have participated in two preference tests. In the first preference test, they were asked to compare deep auto-encoder (DA) with mel-cepstral analysis (MCEP). In the second preference test, they were asked to compare deep autoencoder with deep denoising autoencoder (DDA). From the figure, we can see that deep auto-encoder based speech samples sound more natural than mel-cesptral analysis based speech samples. Deep denoising auto-encoder reduced the distortion, however, perceptual difference between clean and denoising auto-encoder is not statistically significant."}, {"heading": "5.4. Text-to-speech experimental results", "text": "Next we report the text-to-speech experimental results. For the HMM-based speech synthesis, we have used a hidden semi-Markov model and the observation vectors for the spectral and excitation parameters contained static, delta and delta-delta values, with one stream for the spectrum, three streams for F0 and one for the bandlimited aperiodicity. The context-dependnet labels are built using the\npronunciation lexicon Combilex [30]. For the DNN-based speech synthesis, we have trained a five-hidden-layer DNN for mapping between linguistic contexts and auto-encoder-based or mel-cepstral acoustic features. The number of units in each of the hidden layers was set to 512. Random initialisation was used in a similar way to [8]. Figure 7 shows subjective preference scores where we have compared the proposed auto-encoder feature with the conventional mel-cepstral feature in each of the HMM-based speech synthesis and the DNN-based speech synthesis systems. Listeners are the same as those for Figure 6. We can see that synthetic speech using the proposed feature sound more natural than the conventional mel-cepstral features in both the synthesis methods. The proposed feature seems to suit the DNN-based speech synthesis better, but, this requires further investigation.\n6. CONCLUSIONS\nIn this paper we have proposed the deep denoising auto-encoder technique to extract better acoustic features for speech synthesis. We have compared the new stochastic feature extractor with the conventional mel-cepstral analysis in the analysis-by-synthesis and text-tospeech experiments and have confirmed that the proposed method can increase the quality of synthetic speech in both the conditions.\nOur future work includes the improvement of the deep denoising auto-encoder. In this paper, we have used the simplest noise, i.e. masking and the improvement was observed only from objective evaluation. We shall use or design different types of noises to improve the deep denoising auto-encoder for speech synthesis further.\n7. REFERENCES\n[1] H. Zen, K. Tokuda, and A. W. Black, \u201cStatistical parametric speech synthesis,\u201d Speech Communication, vol. 51, pp. 1039\u2013 1064, 2009.\n[2] T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi, and T. Kitamura, \u201cSpeaker interpolation in HMM-based speech synthesis system,\u201d Proceedings of Eurospeech 1997, pp. 2523\u20132526, 1997.\n[3] T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi, and T. Kitamura, \u201cSimultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis,\u201d Proceedings of Eurospeech 1999, pp. 2347\u20132350, 1999.\n[4] R. Tsuzuki, H. Zen, K. Tokuda, T. Kitamura, M. Bulut, and S. Narayanan, \u201cConstructing emotional speech synthesizers with limited speech database,\u201d Proceedings of ICSLP, vol. 2, pp. 1185\u20131188, 2004.\n[5] J. Yamagishi, K. Onishi, T. Masuko, and T. Kobayashi, \u201cAcoustic modeling of speaking styles and emotional expressions in HMM-based speech synthesis,\u201d IEICE Transactions on Information & Systems, vol. E88-D, no. 3, pp. 502\u2013509, 2005.\n[6] L.-H. Chen, T. Raitio, C. Valentini-Botinhao, J. Yamagishi, and Z.-H. Ling, \u201cDNN-based stochastic postfilter for HMM-based speech synthesis,\u201d Proceedings of Interspeech, pp. 1954\u20131958, 2014.\n[7] H. Kawahara, I. Masuda-Katsuse, and A. Cheveigne, \u201cRestructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds,\u201d Speech Communication, vol. 27, pp. 187\u2013207, 1999.\n[8] H. Zen, A. Senior, and M. Schuster, \u201cStatistical parametric speech synthesis using deep neural networks,\u201d Proceedings of ICASSP, pp. 7962\u20137966, 2013.\n[9] Z.-H. Ling, L. Deng, and D. Yu, \u201cModeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, pp. 2129\u20132139, 2013.\n[10] Y. Fan, Y. Qian, F. Xie, and F. K. Soong, \u201cTTS synthesis with bidirectional LSTM based recurrent neural networks,\u201d Proceedings of Interspeech, pp. 1964\u20131968, 2014.\n[11] R. Fernandez, A. Rendel, B. Ramabhadran, and R. Hoory, \u201cProsody contour prediction with long short-term memory, bidirectional, deep recurrent neural networks,\u201d Proceedings of Interspeech, pp. 2268\u20132272, 2014.\n[12] T. N. Sainath, B. Kingsbury, and B. Ramabhadran, \u201cAutoencoder bottleneck features using deep belief networks,\u201d Proceedings of ICASSP, pp. 4153\u20134156, 2012.\n[13] J. Gehring, Y. Miao, F. Metze, and A. Waibel, \u201cExtracting deep bottleneck features using stacked auto-encoders,\u201d Proceedings of ICASSP, pp. 3377\u20133381, 2013.\n[14] A. L. Maas, Q. V. Le, T. M. O?Neil, O. Vinyals, P. Nguyen, Andrew Ng, and Y., \u201cRecurrent neural networks for noise reduction in robust ASR,\u201d Proceedings of Interspeech, pp. 22\u2013 25, 2012.\n[15] T. Ishii, H. Komiyama, T. Shinozaki, Y. Horiuchi, and S Kuroiwa, \u201cReverberant speech recognition based on denoising autoencoder,\u201d Proceedings of Interspeech, pp. 3512\u20133516, 2013.\n[16] X. Feng, Y. Zhang, and J. Glass, \u201cSpeech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,\u201d Proceedings of ICASSP, pp. 1778\u2013 1782, 2014.\n[17] L. Deng, M. Seltzer1, D. Yu, A. Acero, A. Mohamed, and G. Hinton, \u201cBinary coding of speech spectrograms using a deep auto-encoder,\u201d Proceedings of Interspeech, pp. 1692\u2013 1695, 2010.\n[18] X. Lu, Y. Tsao, S. Matsuda1, and C. Hori, \u201cSpeech enhancement based on deep denoising autoencoder,\u201d Proceedings of Interspeech, pp. 436\u2013440, 2013.\n[19] N. Kumar and A. G. Andreou, \u201cHeteroscedastic discriminant analysis and reduced rank hmms for improved speech recognition,\u201d Speech Communication, pp. 283\u2013297, 1998.\n[20] M. J. F. Gales, \u201cMaximum likelihood multiple subspace projections for hidden Markov models,\u201d Speech and Audio Processing, IEEE Transactions on, vol. 10.\n[21] S. J. D. Prince and J. H. Elder, \u201cProbabilistic linear discriminant analysis for inferences about identity,\u201d ICCV, pp. 1\u20138, 2007.\n[22] P. Kenny, \u201cBayesian speaker verification with heavy-tailed priors,\u201d Odyssey, p. 14, 2010.\n[23] L. Lu and S. Renals, \u201cProbabilistic linear discriminant analysis for acoustic modelling,\u201d Signal Processing Letters, IEEE, pp. 702\u2013706, 2014.\n[24] G. E. Hinton and R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science 28, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[25] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol, \u201cExtracting and composing robust features with denoising autoencoders,\u201d ICML, pp. 1096\u20131103, 2008.\n[26] D. Yu and M. Seltzer, \u201cImproved bottleneck features using pretrained deep neural networks,\u201d Proceedings of Interspeech, pp. 237\u2013240, 2011.\n[27] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber, \u201cGradient flow in recurrent nets: the difficulty of learning longterm dependencies,\u201d Citeseer, 2001.\n[28] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \u201cParallel distributed processing: Explorations in the microstructure of cognition, vol. 1,\u201d pp. 318\u2013362, 1986.\n[29] I. Sutskever, J. Martens, George E. Dahl, and Geoffrey E. Hinton, \u201cOn the importance of initialization and momentum in deep learning,\u201d ICML, pp. 1139\u20131147, 2013.\n[30] K. Richmond, R. Clark, and S. Fitt, \u201cOn generating combilex pronunciations via morphological analysis,\u201d Proceedings of Interspeech, pp. 1974?\u20131977, 2010.\n[31] J. Bergstra and Y. Bengio, \u201cRandom search for hyperparameter optimization,\u201d The Journal of Machine Learning Research, vol. 13, pp. 281\u2013305, 2012.\n[32] X. Feng, Y. Zhang, and J. Glass, \u201cSpeech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,\u201d Proceedings of ICASSP, pp. 1759\u2013 1763, May 2014."}], "references": [{"title": "Statistical parametric speech synthesis,", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Speaker interpolation in HMM-based speech synthesis system,", "author": ["T. Yoshimura", "K. Tokuda", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proceedings of Eurospeech", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis,", "author": ["T. Yoshimura", "K. Tokuda", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proceedings of Eurospeech", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Constructing emotional speech synthesizers with limited speech database,", "author": ["R. Tsuzuki", "H. Zen", "K. Tokuda", "T. Kitamura", "M. Bulut", "S. Narayanan"], "venue": "Proceedings of ICSLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Acoustic modeling of speaking styles and emotional expressions in HMM-based speech synthesis,", "author": ["J. Yamagishi", "K. Onishi", "T. Masuko", "T. Kobayashi"], "venue": "IEICE Transactions on Information & Systems, vol. E88-D,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "DNN-based stochastic postfilter for HMM-based speech synthesis,", "author": ["L.-H. Chen", "T. Raitio", "C. Valentini-Botinhao", "J. Yamagishi", "Z.-H. Ling"], "venue": "Proceedings of Interspeech,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1954}, {"title": "Cheveigne, \u201cRestructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds,", "author": ["H. Kawahara", "I. Masuda-Katsuse"], "venue": "Speech Communication,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Statistical parametric speech synthesis using deep neural networks,", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proceedings of ICASSP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis,", "author": ["Z.-H. Ling", "L. Deng", "D. Yu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks,", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F.K. Soong"], "venue": "Proceedings of Interspeech,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1964}, {"title": "Prosody contour prediction with long short-term memory, bidirectional, deep recurrent neural networks,", "author": ["R. Fernandez", "A. Rendel", "B. Ramabhadran", "R. Hoory"], "venue": "Proceedings of Interspeech,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Autoencoder bottleneck features using deep belief networks,", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proceedings of ICASSP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Extracting deep bottleneck features using stacked auto-encoders,", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "Proceedings of ICASSP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recurrent neural networks for noise reduction in robust ASR,", "author": ["A.L. Maas", "Q.V. Le", "T.M. O?Neil", "O. Vinyals", "P. Nguyen", "Andrew Ng"], "venue": "Proceedings of Interspeech,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Reverberant speech recognition based on denoising autoencoder,", "author": ["T. Ishii", "H. Komiyama", "T. Shinozaki", "Y. Horiuchi", "S Kuroiwa"], "venue": "Proceedings of Interspeech,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,", "author": ["X. Feng", "Y. Zhang", "J. Glass"], "venue": "Proceedings of ICASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder,", "author": ["L. Deng", "M. Seltzer", "D. Yu", "A. Acero", "A. Mohamed", "G. Hinton"], "venue": "Proceedings of Interspeech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Speech enhancement based on deep denoising autoencoder,", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "Proceedings of Interspeech,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Heteroscedastic discriminant analysis and reduced rank hmms for improved speech recognition,", "author": ["N. Kumar", "A.G. Andreou"], "venue": "Speech Communication,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Probabilistic linear discriminant analysis for inferences about identity,", "author": ["S.J.D. Prince", "J.H. Elder"], "venue": "ICCV, pp", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Bayesian speaker verification with heavy-tailed priors,", "author": ["P. Kenny"], "venue": "Odyssey, p", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Probabilistic linear discriminant analysis for acoustic modelling,", "author": ["L. Lu", "S. Renals"], "venue": "Signal Processing Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks,", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science 28,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders,", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P. Manzagol"], "venue": "ICML, pp", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Improved bottleneck features using pretrained deep neural networks,", "author": ["D. Yu", "M. Seltzer"], "venue": "Proceedings of Interspeech, pp", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Gradient flow in recurrent nets: the difficulty of learning longterm dependencies,", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "On the importance of initialization and momentum in deep learning,", "author": ["I. Sutskever", "J. Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "On generating combilex pronunciations via morphological analysis,", "author": ["K. Richmond", "R. Clark", "S. Fitt"], "venue": "Proceedings of Interspeech, pp. 1974?\u20131977,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Random search for hyperparameter optimization,", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,", "author": ["X. Feng", "Y. Zhang", "J. Glass"], "venue": "Proceedings of ICASSP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Current statistical parametric speech synthesis typically uses hidden Markov models (HMMs) to represent probability densities of speech trajectories given text [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 2, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 3, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 4, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 5, "context": "A stochastic postfilter approach [6] proposes to use a deep neural network (DNN) to model the conditional probability of the spectral differences between natural and synthetic speech.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "The approach is able to reconstruct the spectral fine structure lost during modeling and has achieved significantly quality improvement for synthetic speech [6].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "More specifically we propose to use a deep denoising auto-encoder technique as a non-linear robust feature extractor for speech synthesis and apply it to high-dimensional spectral features obtained from STRAIGHT vocoder [7].", "startOffset": 220, "endOffset": 223}, {"referenceID": 7, "context": "For instance [8] uses DNN to learn the relationship between input texts and the extract features instead of decision tree-based state tying.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "Restricted Boltzmann machines or deep belief networks have been used for modelling output probabilities of HMM states instead of GMMs [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "Recurrent neural network or long-short term memory was used for prosody modelling [10] or acoustic trajectory modelling [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "Recurrent neural network or long-short term memory was used for prosody modelling [10] or acoustic trajectory modelling [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 12, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 13, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 273, "endOffset": 277}, {"referenceID": 14, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 303, "endOffset": 311}, {"referenceID": 15, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 303, "endOffset": 311}, {"referenceID": 16, "context": "Techniques that are closely related to this paper are a spectral binary coding approach using deep auto-encoder proposed by Deng et al [17] and a speech enhancement approach using deep denoising auto-encoder where they try to reconstruct clean spectrum from noisy spectrum [18].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "Techniques that are closely related to this paper are a spectral binary coding approach using deep auto-encoder proposed by Deng et al [17] and a speech enhancement approach using deep denoising auto-encoder where they try to reconstruct clean spectrum from noisy spectrum [18].", "startOffset": 273, "endOffset": 277}, {"referenceID": 18, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 98, "endOffset": 106}, {"referenceID": 19, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 161, "endOffset": 173}, {"referenceID": 20, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 161, "endOffset": 173}, {"referenceID": 21, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 161, "endOffset": 173}, {"referenceID": 22, "context": "The weight for the decoding is set as the transpose of the encoding weight [24] in order to allow more layers to be stacked together and be fine-tuned with stochastic gradient descend (SGD).", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "It is reported that the denoising auto-encoder can extract features more robustly than the basic auto-encoder [25].", "startOffset": 110, "endOffset": 114}, {"referenceID": 24, "context": "[26] shows that deeper architecture produces better high-level features compared to the shallow architecture up to 4 encoding and 4 decoding layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "It is reported that fine-tuning with back-propagation through a deep auto-encoder is ineffective due to vanishing gradients at the lower layers [27].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "To over come this issue we restrict the decoding weight as the transpose of the encoding weight following [24], that is, W\u2032 = W where W denotes transpose of W.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "We used a masking technique reported in [25] to corrupt the training data for the denoising auto-encoder.", "startOffset": 40, "endOffset": 44}, {"referenceID": 26, "context": "lr: learning rate, m: momentum, b: batch size, s: numpy random variable weight initialization seed [29], d: masking probability of each input dimension [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 23, "context": "lr: learning rate, m: momentum, b: batch size, s: numpy random variable weight initialization seed [29], d: masking probability of each input dimension [25].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "pronunciation lexicon Combilex [30].", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "Random initialisation was used in a similar way to [8].", "startOffset": 51, "endOffset": 54}], "year": 2015, "abstractText": "This paper proposes a deep denoising auto-encoder technique to extract better acoustic features for speech synthesis. The technique allows us to automatically extract low-dimensional features from high dimensional spectral features in a non-linear, data-driven, unsupervised way. We compared the new stochastic feature extractor with conventional mel-cepstral analysis in analysis-by-synthesis and text-to-speech experiments. Our results confirm that the proposed method increases the quality of synthetic speech in both experiments.", "creator": "LaTeX with hyperref package"}}}