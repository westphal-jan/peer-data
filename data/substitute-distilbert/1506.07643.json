{"id": "1506.07643", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2015", "title": "Conservativeness of Untied Auto-Encoders", "abstract": "we discuss necessary and sufficient conditions for an auto - reconstruction to define true conservative vector field, note which case integration is associated with an energy function akin to the unnormalized log - probability of the process. we demonstrate that these conditions for it are more general than for encoder and decoder weights should be indeed same ( \" tied rule \" ), and that they also depend on the form of the hidden unit activation function, but that contractive training criteria, such as denoising, will enforce these conditions locally. based around certain observations, we show how we can use auto - encoders to extract the conservative component of a vector field.", "histories": [["v1", "Thu, 25 Jun 2015 07:23:56 GMT  (4434kb,D)", "https://arxiv.org/abs/1506.07643v1", null], ["v2", "Tue, 30 Jun 2015 14:18:50 GMT  (4434kb,D)", "http://arxiv.org/abs/1506.07643v2", null], ["v3", "Mon, 21 Sep 2015 22:11:41 GMT  (4452kb,D)", "http://arxiv.org/abs/1506.07643v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel jiwoong im", "mohamed ishmael diwan belghazi", "roland memisevic"], "accepted": true, "id": "1506.07643"}, "pdf": {"name": "1506.07643.pdf", "metadata": {"source": "CRF", "title": "Conservativeness of untied auto-encoders", "authors": ["Daniel Jiwoong Im", "Mohamed Ishmael Diwan Belghanzi", "Roland Memisevic"], "emails": ["imdaniel@iro.umontreal.ca", "mohamed.2.belghazi@hec.ca", "roland.memisevic@umontreal.ca"], "sections": [{"heading": "Introduction", "text": "An auto-encoder is a feature learning model that learns to reconstruct its inputs by going though one or more capacityconstrained \u201cbottleneck\u201d-layers. Since it defines a mapping r : Rn \u2192 Rn, an auto-encoder can also be viewed as dynamical system, that is trained to have fixed points at the data (Seung 1998). Recent renewed interest in the dynamical systems perspective led to a variety of results that help clarify the role of auto-encoders and their relationship to probabilistic models. For example, (Vincent et al. 2008; Swersky et al. 2011) showed that training an auto-encoder to denoise corrupted inputs is closely related to performing score matching (Hyva\u0308rinen 2005) in an undirected model. Similarly, (Alain and Bengio 2014) showed that training the model to denoise inputs, or to reconstruct them under a suitable choice of regularization penalty, lets the autoencoder approximate the derivative of the empirical data density. And (Kamyshanska 2013) showed that, regardless of training criterion, any auto-encoder whose weights are tied (decoder-weights are identical to the encoder weights) can be written as the derivative of a scalar \u201cpotential-\u201d or energy-function, which in turn can be viewed as unnormalized data log-probability. For sigmoid hidden units the potential function is exactly identical to the free energy of an RBM, which shows that there is tight link between these two types of model.\nThe same is not true for untied auto-encoders, for which it has not been clear whether such an energy function exists. It has also not been clear under which conditions an\n\u2217Authors constributed equally.\nenergy function exists or does not exist, or even how to define it in the case where decoder-weights differ from encoder weights. In this paper, we describe necessary and sufficient conditions for the existence of an energy function and we show that suitable learning criteria will lead to an autoencoder that satisfies these conditions at least locally, near the training data. We verify our results experimentally. We also show how we can use an auto-encoder to extract the conservative part of a vector field."}, {"heading": "Background", "text": "We will focus on auto-encoders of the form\nr(x) = Rh ( WTx + b ) + c (1)\nwhere x \u2208 Rn is an observation, R and W are decoder and encoder weights, respectively, b and c are biases, and h(\u00b7) is an elementwise hidden activation function. An auto-encoder can be identified with its vector field, r(x) \u2212 x, which is the set of vectors pointing from observations to their reconstructions. The vector field is called conservative if it can be written as the gradient of a scalar function F (x), called potential or energy function:\nr(x)\u2212 x = \u2207F (x) (2) The energy function corresponds to the unnormalized probability of data.\nIn this case, we can integrate the vector field to find the energy function (Kamyshanska 2013). For an auto-encoder with tied weights and real-valued observations it takes the form\nF (x) = \u222b h(u)du\u2212 1\n2 \u2016x\u2212 c\u201622 + const (3)\nwhere u = WTx + b is an auxiliary variable and h(\u00b7) can be any elementwise activation function with known antiderivative. For example, the energy function of an autoencoder with sigmoid activation function is identical to the (Gaussian) RBM free energy (Hinton 2010):\nFsig(x) = \u2211 k log ( 1 + exp ( WT\u00b7kx + bk )) \u22121 2 \u2016x\u2212c\u201622+const (4) A sufficient condition for the existence of an energy function is that the weights are tied (Kamyshanska 2013), but it\nhas not been clear if this is also necessary. A peculiar phenomenon in practice is that it is very common for decoder and encoder weights to be \u201csimilar\u201d (albeit not necessarily tied) in response to training. An example of this effect is shown in Figure 11. This raises the question of why this happens, and whether the quasi-tying of weights has anything to do with the emergence of an energy function, and if yes, whether there is a way to compute the energy function despite the lack of exact symmetry. We shall address these questions in what follows."}, {"heading": "Conservative auto-encoders", "text": "One of the central objectives of this paper is understanding the conditions for an auto-encoder to be conservative2 and thus to have a well-defined energy function. In the following subsection we derive and explain said conditions."}, {"heading": "Conditions for conservative auto-encoders", "text": "Proposition 1. Consider an m-hidden-layer auto-encoder defined as\nr(x; \u03b8) = W (m)h(m) ( W (m\u22121)h(m\u22121)(\n\u00b7 \u00b7 \u00b7W (1)h(1) (x) \u00b7 \u00b7 \u00b7 ) + c(m\u22121) ) + c(m),\nwhere \u03b8 = \u222amk=0\u03b8(k) such that \u03b8(k) = {W (k), c(k)} are the parameters of the model, and h(k)(\u00b7) is a smooth elementwise activation function at layer k. Then the auto-encoder\n1We found these kinds of behaviours not only for unwhitened, but also for binary data.\n2The expressions, \u201cconservative vector field\u201d and \u201cconservative auto-encoders\u201d will be used interchangeably.\nis said to be conservative over a smooth simply connect domain K \u2286 RD if and only if its reconstruction\u2019s Jacobian \u2202r(x) \u2202x is symmetric for all x \u2208 K. A formal proof is provided in the Appendix. A region K is said to be simply connected if and only if any simple curve in K can be shrunk to a point. It is not always the case that a region of RD is simply connected. For instance, a curve surrounding a punctured circle in R2 cannot be continuously deformed to a point without crossing the punctured region. However, as long as we make the reasonable assumption that the activation function does not have a continuum of discontinuities, we should not run into trouble. This makes our analysis valid for activation functions with cusps such as ReLUs.\nThroughout the paper, our focus will be on one\u2013hiddenlayer auto-encoders. Although the necessary and sufficient conditions for their conservativeness are a special case of the above proposition, it is worthwhile to derive them explicitly.\nProposition 2. Let r(x) be a one-hidden-layer autoencoder with D dimensional inputs and H hidden units,\nr(x) = Rh ( WTx + b ) + c,\nwhereR,W,b, c are the parameters of the model. Then r(x) defines a conservative vector field over a smooth simply connect domain K \u2286 RD if and only if RDh\u2032WT is symmetric for all x \u2208 K where Dh\u2032 = diag (h\u2032(x)).\nProof. Following proposition 1, an auto-encoder defines a conservative vector field if and only if its Jacobian is symmetric for all x \u2208 K.\n\u2202r(x)\n\u2202x =\n( \u2202r(x)\n\u2202x\n)T (5)\nBy explicitly calculating the Jacobian, this is equivalent to\n(\u22001 \u2264 i < j \u2264 D) H\u2211 l=0 ( RjlWli \u2212RilWlj ) h\u2032l (x) = 0 (6)\nDefining Dh\u2032 = diag(h \u2032 (x)), this holds if and only if\nRDh\u2032W T = WDh\u2032R T (7)\nFor one-hidden-layer auto-encoders with tied weights, Equation 7 holds regardless of the choice of activation function h and x.\nCorollary 1. An auto-encoder with tied weights always defines a conservative vector field.\nProposition 2 illustrates that the set of all one-layered tied auto-encoders is actually a subset of the set of all conservative one-layered auto-encoders. Moreover, the inclusion is strict. That is to say there are untied conservative autoencoders that are not trivially equivalent to tied ones. As example, let us compare the parametrization of tied and conservative untied linear one-layered auto-encoders. runtied(x) in Eq. 7 defines a conservative vector field if and only RWT = WRT which offers a richer parametrization than the tied linear auto-encoder rtied(x) = WWTx.\nIn the following section we explore in more detail and generality of the parametrization imposed by the conditions above."}, {"heading": "Understanding the symmetricity condition", "text": "Note that if symmetry holds in the Jacobian of an autoencoder\u2019s reconstruction function, then the vector field is conservative. A sufficient condition for symmetry of the Jacobian is that R can be written\nR = CWDh\u2032E. (8)\nwhere C and E are symmetric matrices, and C commutes with WDh\u2032EDh\u2032WT , as this will ensure symmetry of the partial derivatives:\n\u2202r(x)\n\u2202x = RDh\u2032W\nT = CWDh\u2032EDh\u2032W T (9)\n= WDh\u2032EDh\u2032W TC = WDh\u2032R T =\n( \u2202r(x)\n\u2202x\n)T .\nThe case of tied weights (R = W ) follows if C and E are the identity, since then \u2202r(x)\u2202x = RDh\u2032W T = WDh\u2032W T .\nNotice that R = CWDh\u2032 and R = WDh\u2032E are further special cases of the condition R = CWDh\u2032E when E is the identity (first case) or C is the identity (second case). Moreover, we can also find matrices E and C given the parameters W and R, which is shown in Section 1.2 of the supplementary material3.\n3www.uoguelph.ca/\u02dcimj/files/conservative_ ae_supplementary.pdf"}, {"heading": "Conservativeness of trained auto-encoders", "text": "Following (Alain and Bengio 2014) we will first assume that the true data distribution is known and the auto-encoder is trained. We then analyze the conservativeness of autoencoders around fixed points of the data manifold. After that, we will proceed to empirically investigate and explain the tendency of trained auto-encoders to become conservative away from the data manifold. Finally, we will use the obtained results to explain why the product of the encoder and decoder weights become increasingly symmetric in response to training."}, {"heading": "Local Conservativeness", "text": "Let r(x) be an auto-encoder that minimizes a contractionregularized squared loss function averaged over the true data distribution p,\nL\u03c3(x) = \u222b Rd p(x) [ \u2016r(x)\u2212 x\u201622 + \u2016 \u2202r(x) \u2202x \u201622 ] dx (10)\nA point x \u2208 Rd is a fixed point of the auto-encoder if and only if r(x) = x. Proposition 3. Let r(x) be an untied one-layer autoencoder minimizing Equation 10. Then r(x) is locally conservative as the contraction parameter tends to zero.\nTaking a first order Taylor expansion of r(x) around a fixed point x yields\nr(x + ) = x + \u2202r(x)\n\u2202x\nT\n+ o( ) as \u2192 0. (11)\n(Alain and Bengio 2014) shows that the reconstruction r(x) \u2212 x becomes an estimator of the score when \u2016r(x) \u2212 x\u20162 is small and the contraction parameters \u03bb \u2192 0. Hence around a fixed point we have\nr(x + )\u2212 x = \u2202 log(p(x)) \u2202x , and (12)\n\u2202(r(x + )\u2212 x) \u2202x = \u22022 log(p(x)) \u2202x2 (13)\nwhere I is the identity matrix. By explicitly expressing the Jacobian of the autoencoder\u2019s dynamics \u2202r(x)\u2212x\u2202x and using the Taylor expansion of r(x), we have\nWTDh\u2032R T \u2212 I = \u2202\n2 log(p(x))\n\u2202x2 (14)\nThe Hessian of log p(x) being symmetric, Equation 14 illustrates that around fixed points,RDh\u2032W is symmetric. In conjunction with Proposition 2, this shows that untied autoencoders, when trained using a contractive regularizer, are locally conservative. Remark that when the auto-encoder is trained with patterns drawn from a continuous family, then auto-encoder forms a continuous attractor that lies near the examples it is trained on (Seung 1998).\nIt is worth noting that dynamics around fixed points can be understood by analyzing the eigenvalues of the Jacobian. The latter being symmetric implies that its eigenvalues cannot have complex parts, which corresponds to the lack of\noscillations one would naturally expect of a conservative vector field. Moreover, in directions orthogonal to the fixed point, the eigenvalues of the reconstruction will be negative. Thus the fixed point is actually a sink."}, {"heading": "Empirical Conservativeness", "text": "We now empirically analyze the conservativeness of trained untied auto-encoders. To this end, we train an untied autoencoder with 500 hidden units with and without weight length constraints4 on the MNIST dataset. We measure symmetricity using sym(A) = \u2016(A+A\nT )/2\u20162 \u2016A\u20162 which yields values\nbetween [0, 1] with 1 representing complete symmetricity. Figure 2a and 2c shows the evolution of the symmetricity of \u2202r(x) \u2202x = RDh\u2032W during training. For untied auto-encoders, we observe that the Jacobian becomes increasingly symmetric as training proceeds and hence, by Proposition 2, the auto-encoder becomes increasingly conservative.\nThe contractive auto-encoder tends more towards symmetry than the unregularized auto-encoder. The reach plateaus\n4Weight length constraints : ||wi||2 = \u03b1 for all i = 1 \u00b7 \u00b7 \u00b7H and \u03b1 is a constant term.\naround 0.951 and 0.974 respectively. It is interesting to note that auto-encoders with weight length constraints yield sensibly higher symmetricity scores as shown in Table 1. The details of the experiments and further interpretations are provided in the supplementary material.\nTo explicitly confirm the conservativeness of the autoencoder in 2D, we monitor the curl of the vector field during training. In our experiments, we created three 2D synthetic datasets by adding gaussian white noise to the parametrization of a line, a circle, and a spiral. As shown in Figure 3, we notice that the curl decrease very sharply during training, which further demonstrates how untied auto-encoders become more conservative during training. Hence, together with symmetricity measurement and decay of curliness advocates that vector fields near the data manifold has the tendancy of becoming conservative. More results on line, circle, and spiral synthetic datasets can be found in the supplementary materials."}, {"heading": "Symmetricity of weights product", "text": "The product of weight RWT tends to become increasingly symmetric during training. This behavior is more marked for sigmoid activations than for ReLUs as shown in Figures 2b and 2d. This can be explained by considering the Jacobian symmetricity. We approximately have\nH\u2211 l=1 (RilWlj \u2212RjlWli)h\u2032l(x) = 0,\u22001 \u2264 i, j \u2264 d (15)\nThis implies that the activations of sigmoid hidden units, at least for training data points, are independent of h\u2032(x) or\na constant. As shown in the supplementary material, most hidden unit activities are concentrated in the highest curvature region when training with weight length constraints. This forces hl(x) to be concentrated on high curvature regions of the sigmoid activation. This may be due to either h\u2032l(x) being nearly constant for all l given x, or h\u2032l(x) being close to linearly independent. In both cases, the Jacobian becomes close to the identity and hence RWT \u2248WRT ."}, {"heading": "Decomposing the Vector Field", "text": "In this section, we consider finding the closest conservative vector field, in a least square sense, to a non-conservative vector field. Finding this vector field is of great practical importance in many areas of science and engineering (Bhatia et al. 2013). Here we show that conservative auto-encoders can provide a powerful, deep learning based perspective onto this problem.\nThe fundamental theorem of vector calculus, also known as Helmhotz decomposition states that any vector field in R3 can be expressed as the orthogonal sum of an irrotational and a solenoidal field. The Hodge decomposition is a generalization of this result to high dimensional space (James 1966). A complete statement of the result requires careful analysis of boundary conditions as well as differential form formalism. But since 1-forms correspond to vector field, and our interest lies in the latter, we abuse notation to state the result in the special case of 1-forms as\n\u03c9 = d\u03b1+ \u03b4\u03b2 + \u03b3 (16)\nwhere d is the exterior derivative, \u03b4 the co-differential, and \u2206\u03b3 = 0 5. This means that any 1-form (vector field) can be orthogonally decomposed into a direct sum of a scalar, solenoidal, and harmonic components.\nThis shows that it is always theoretically possible to get the closest conservative vector field, in a least square sense, to a non-conservative one. When applied to auto-encoders, this guarantees the existence of a best approximate energy function for any untied conservative auto-encoder. For a\n5For Laplace-deRham, \u2206 = d\u03b4 + \u03b4d. Standard \u2206 on 1-forms is d\u03b4.\nmore detailed background on the vector field decomposition we refer to the supplementary material."}, {"heading": "Extracting the Conservative Vector Field through Learning", "text": "Although the explicit computation of the projection might be theoretically possible in special cases, we propose to find the best approximate conservative vector through learning. There are several advantages to learning the conservative part of a vector field: i) Learning the scalar vector field component \u03b1 from some vector field \u03c9 with an auto-encoder is straightforward due to the intrinsic tendency of the trained auto-encoder to become conservative, ii) although there is a large body of literature to explicitly compute the projections, these methods are highly sensitive to boundary conditions (Bhatia et al. 2013), while learning based methods eschew this difficulty.\nThe advantage of deep learning based methods over existing approaches, such as matrix-valued radial basis function kernels (Macedo and Castro 2008), is that they can be trained on very large amounts of data. To the best of our knowledge, this is the first application of neural networks to extract the conservative part of any vector field, effectively recovering the scalar part of Eq. 16.\nTwo Dimensional space As a proof of concept, we first extract the conservative part of a two dimensional vector field F (x, y) = (\u2212x+ y,\u2212x\u2212 y). The field corresponds to a spiralling sink. We train an untied auto-encoder with 1000 ReLU units for 500 epochs using BFGS over an equally spaced grid of 100 points in each dimension. Figure 4 clearly shows that the conservative part is perfectly recovered.\nHigh Dimensional space We also conducted experiments with high dimensional vector fields. We created a continum of vector fields by considering convex combinations of a conservative and a non-conservative field. The former is obtained by training a tied auto-encoder on MNIST and the latter by setting the parameters of an auto-encoder to random values. That is, we have (Wi, Ri) = \u03b2(W0, R0) + (1\u2212 \u03b2)(WK , RK) where (W0, R0) is the non-conservative autoencoder and (WK , RK) is the conservative auto-encoder. We repeatedly train a tied auto-encoder on this continuum\nTable 2: The fraction of observations with E(x) > E(xrand) for different \u03b2 values.\n\u03b2 0.0 0.2 0.4 0.6 0.8 1.0 CVF=Tied AE 0.5036 0.7357 0.9338 0.98838 0.9960 0.9968 CVF=Untied AE 0.5072 0.7496 0.9373 0.98595 0.9958 0.9968\nin order to learn its conservative part. The pseudocode for the experiment is presented in Algorithm 1. Figure 5 shows\nAlgorithm 1 Learning to approximate a conservative field with an auto-encoder 1: procedure (D be a data set ) 2: Let (W0, R0) be a random weights for AE. 3: Let (WK , RK) be trained AE on D. 4: Generate Fi \u2200i = 1 \u00b7 \u00b7 \u00b7K as follows: \u2022 (Wi, Ri) = \u03b2(W0, R0) + (1\u2212 \u03b2)(WK , RK) \u2022 Sample xi from uniform distributon in the data space. \u2022 Fi = {(xi, r(xi))fori = 1 \u00b7 \u00b7 \u00b7N}\n5: for each vector field Fi, do 6: Train a tied Auto-encoder on Fi 7: Compute E(x) where x \u2208 D 8: Compute E(x\u0303) where x\u0303 \u223c Binomial 9: Count number of E(x) > E(x\u0303).\nthe mean squared error as a function of training epoch for different values of \u03b2. We observe that the auto-encoder\u2019s loss function decreases as \u03b2 gets closer to 1. This is due to auto-encoder only being able to learn the conservative component of the vector field. We then compare the unnormalized model evidence of the auto-encoders. The comparison is based on computing the potential energy of autoencoders given two points at a time. These two points are from the MNIST and a corrupted version of the latter using salt and pepper noise. We validate our experiments by counting the number of times where E(x) > E(xrand). Given that the weights (WK , RK) of the conservative auto-encoder are obtained by training it on MNIST, the potential energy at MNIST data points should be higher than that at the corrupted MNIST data points. However, this does not hold for \u03b2 < 1. Even for \u03b2 = 0.6, we can recover the conservative component of the vector field up to 93% . Thus, we conclude that the tied auto-encoder is able to learn the conservative component of the vector field. The procedure is detailed in Algorithm 1.\n0 50 100 150 200 Epochs\n0\n2\n4\n6\n8\n10\n12\n14\nM e a n R\ne co\nn st\nru ct\nio n E\nrr o r\nbeta=0.0 beta=0.2 beta=0.4 beta=0.6 beta=0.8 beta=1.0\nFigure 5: Learning curves for tied (dashed) and untied (solid) auto-encoders.\nTable 2 shows that, on average, the auto-encoders potential energy increasingly favors the original MNIST point over the corrupted ones as the vector field Fi moves from 0 to K. \u201cCVF=Tied AE\u201d refers to conservative vector field FK trained by tied auto-encoder and \u201cCVF=Untied AE\u201d refers to conservative vector field FK trained by untied autoencoder."}, {"heading": "Discussion", "text": "In this paper we derived necessary and sufficient conditions for autoencoders to be conservative, and we studied why the Jacobian of the autoencoder tends to become symmetric during training. Moreover, we introduced a way to extract the conservative component of a vector field based on these properties of auto-encoders.\nAn interesting direction for future research is the use of annealed importance sampling or similar sampling-based approaches to globally normalize the energy function values obtained from untied autoencoders. Another interesting direction is the use of parameterizations during training that will automatically satisfy the sufficient conditions for conservativeness but are less restrictive than weight tying."}, {"heading": "Appendix", "text": ""}, {"heading": "Conservative auto-encoders", "text": "This section provides detailed derivations of Proposition 1 in Section 3.\nProposition 1. Consider an m-hidden-layer auto-encoder defined as\nr(x; \u03b8) = W (m)h(m) ( W (m\u22121)h(m\u22121)(\n\u00b7 \u00b7 \u00b7W (1)h(1) (x) \u00b7 \u00b7 \u00b7 ) + c(m\u22121) ) + c(m),\nwhere \u03b8 = \u222amk=0\u03b8(k) such that \u03b8(k) = {W (k), c(k)} are the parameters of the model, and h(k)(\u00b7) is a smooth elementwise activation function at layer k. Then the auto-encoder is said to be conservative over a smooth simply connect domain K \u2286 RD if and only if its reconstruction\u2019s Jacobian \u2202r(x) \u2202x is symmetric for all x \u2208 K.\nThe high level idea is that simply finding the antiderivative of an auto-encoder vector field as proposed in (Kamyshanska 2013) does not work for untied autoencoders. This is due to the difference in solving first order ordinary differential equations for tied auto-encoders and first order partial differential equations for untied autoencoders. Therefore, here we present a different approach that uses differential forms to facilitate the derivation of the existence condition of a potential energy function in the case of untied auto-encoders.\nThe advantage of differential forms is that they allow us to work with a generalized, coordinate free system. A differential form \u03b1 of degree l (l-form) on a smooth domain K \u2286 Rd is an expression:\n\u03b1 = D\u2211 i=1 fidxi. (17)\nUsing differential form algebra and exterior derivatives, we can show that the 1-form implied by an untied auto-encoder is exact, which means that \u03b1 can be expressed as \u03b1 = d\u03b2 for some \u03b2 \u2208 \u039bl\u22121(K). Let \u03b1 be the 1-form implied by the vector field of an untied auto-encoder. Then, we have\n\u03b1 = D\u2211 i=1 ridxi, and d\u03b1 = D\u2211 i=1 d(ri \u2227 dxi) (18)\nwhere \u2227 is the exterior multiplication, d is the differential operatior on differential forms, and r(\u00b7) is the reconstruction function of the auto-encoder. Based on the exterior derivative properties, i) if f \u2208 \u039b0(K) then df = \u2211D i=1 \u2202f \u2202xi\ndxi and ii) if \u03b1 \u2208 \u039bl(K) and \u03b2 \u2208 \u039bm(K) (Edelen 2011) then\n\u03b1\u03b2 = (\u22121)lm\u03b2\u03b1,\nd\u03b1 = D\u2211 i=1 d(ri \u2227 dxi) (19)\n= D\u2211 i,j=1 \u2202ri \u2202xj (dxj \u2227 dxi) (20)\n= \u2212 \u2211\n1\u2264i<j<D\n\u2202ri \u2202xj\ndxi \u2227 dxj + \u2211\n1\u2264i<j<D\n\u2202rj \u2202xi dxi \u2227 dxj\n(21) = \u2211\n1\u2264i<j<D\n( \u2202ri \u2202xj \u2212 \u2202rj \u2202xi ) dxi \u2227 dxj (22)\nAccording to the Poincare\u2019s theorem, which states that every exact form is closed and convsersely, if \u03b1 is closed then it is exact in a simply connected region and \u03b1 \u2208 \u039bl(K), where \u03b1 is closed if d\u03b1 = 0. Then, by Poincare\u2019s theorem, we see that\nd\u03b1 = \u2211\n1\u2264i<j<D\n( \u2202ri \u2202xj \u2212 \u2202rj \u2202xi ) dxi \u2227 dxj = 0 (23)\nThis is equivalent to requiring the Jacobian to be symmetric for all x \u2208 K."}], "references": [{"title": "and Bengio", "author": ["G. Alain"], "venue": "Y.", "citeRegEx": "Alain and Bengio 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The helmtholz-hodge decomposition-a survey", "author": ["Bhatia"], "venue": "IEEE Transactions on visualization and computer graphics 19:1386\u20131404", "citeRegEx": "Bhatia,? \\Q2013\\E", "shortCiteRegEx": "Bhatia", "year": 2013}, {"title": "D", "author": ["Edelen"], "venue": "G.", "citeRegEx": "Edelen 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Castro", "author": ["I. Macedo"], "venue": "R.", "citeRegEx": "Macedo and Castro 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "On autoencoders and score matching for energy based models", "author": ["Swersky"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Swersky,? \\Q2011\\E", "shortCiteRegEx": "Swersky", "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML)", "citeRegEx": "Vincent,? \\Q2008\\E", "shortCiteRegEx": "Vincent", "year": 2008}], "referenceMentions": [], "year": 2015, "abstractText": "We discuss necessary and sufficient conditions for an auto-encoder to define a conservative vector field, in which case it is associated with an energy function akin to the unnormalized log-probability of the data. We show that the conditions for conservativeness are more general than for encoder and decoder weights to be the same (\u201ctied weights\u201d), and that they also depend on the form of the hidden unit activation function, but that contractive training criteria, such as denoising, will enforce these conditions locally. Based on these observations, we show how we can use auto-encoders to extract the conservative component of a vector field. Introduction An auto-encoder is a feature learning model that learns to reconstruct its inputs by going though one or more capacityconstrained \u201cbottleneck\u201d-layers. Since it defines a mapping r : R \u2192 R, an auto-encoder can also be viewed as dynamical system, that is trained to have fixed points at the data (Seung 1998). Recent renewed interest in the dynamical systems perspective led to a variety of results that help clarify the role of auto-encoders and their relationship to probabilistic models. For example, (Vincent et al. 2008; Swersky et al. 2011) showed that training an auto-encoder to denoise corrupted inputs is closely related to performing score matching (Hyv\u00e4rinen 2005) in an undirected model. Similarly, (Alain and Bengio 2014) showed that training the model to denoise inputs, or to reconstruct them under a suitable choice of regularization penalty, lets the autoencoder approximate the derivative of the empirical data density. And (Kamyshanska 2013) showed that, regardless of training criterion, any auto-encoder whose weights are tied (decoder-weights are identical to the encoder weights) can be written as the derivative of a scalar \u201cpotential-\u201d or energy-function, which in turn can be viewed as unnormalized data log-probability. For sigmoid hidden units the potential function is exactly identical to the free energy of an RBM, which shows that there is tight link between these two types of model. The same is not true for untied auto-encoders, for which it has not been clear whether such an energy function exists. It has also not been clear under which conditions an \u2217Authors constributed equally. energy function exists or does not exist, or even how to define it in the case where decoder-weights differ from encoder weights. In this paper, we describe necessary and sufficient conditions for the existence of an energy function and we show that suitable learning criteria will lead to an autoencoder that satisfies these conditions at least locally, near the training data. We verify our results experimentally. We also show how we can use an auto-encoder to extract the conservative part of a vector field. Background We will focus on auto-encoders of the form r(x) = Rh ( Wx + b ) + c (1) where x \u2208 R is an observation, R and W are decoder and encoder weights, respectively, b and c are biases, and h(\u00b7) is an elementwise hidden activation function. An auto-encoder can be identified with its vector field, r(x) \u2212 x, which is the set of vectors pointing from observations to their reconstructions. The vector field is called conservative if it can be written as the gradient of a scalar function F (x), called potential or energy function: r(x)\u2212 x = \u2207F (x) (2) The energy function corresponds to the unnormalized probability of data. In this case, we can integrate the vector field to find the energy function (Kamyshanska 2013). For an auto-encoder with tied weights and real-valued observations it takes the form F (x) = \u222b h(u)du\u2212 1 2 \u2016x\u2212 c\u20162 + const (3) where u = Wx + b is an auxiliary variable and h(\u00b7) can be any elementwise activation function with known antiderivative. For example, the energy function of an autoencoder with sigmoid activation function is identical to the (Gaussian) RBM free energy (Hinton 2010):", "creator": "LaTeX with hyperref package"}}}