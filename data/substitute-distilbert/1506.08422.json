{"id": "1506.08422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2015", "title": "Topic2Vec: Learning Distributed Representations of Topics", "abstract": "latent dirichlet allocation ( lda ) proposed thematic structure of documents plays great instrumental impact in nature language processing and machine learning areas. surprisingly, the probability gained from lda only describes the statistical relationship of occurrences in the corpus and usually in practice, probability means not the best choice for feature representations. recently, embedding methods have been proposed to reduce words nor documents by learning essential concepts and representations, e as nfc and doc2vec. the embedded representations have shown more effectiveness than lda - style representations in many tasks. in this paper, we propose the topic2vec approach which processes learn topic representations in the same semantic logical space with words, as an alternative to probability. the experimental results show that topic2vec achieves interesting outcome constructive results.", "histories": [["v1", "Sun, 28 Jun 2015 16:17:40 GMT  (525kb,D)", "http://arxiv.org/abs/1506.08422v1", "6 pages, 3 figures"]], "COMMENTS": "6 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["li-qiang niu", "xin-yu dai"], "accepted": false, "id": "1506.08422"}, "pdf": {"name": "1506.08422.pdf", "metadata": {"source": "CRF", "title": "Topic2Vec: Learning Distributed Representations of Topics", "authors": ["Li-Qiang Niu", "Xin-Yu Dai"], "emails": ["daixinyu}@nlp.nju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Modeling text (words, topics and documents) is a key problem in nature language processing (NLP) and information retrieval (IR). The goal is to find short and essential descriptions which enable efficient processing of large systems and benefit basic tasks such as classification, clustering, summarization and estimation of similarity or relevance.\nDuring the past decades, various models and solutions are proposed, such as Bag-of-Words (BOW) (Harris, 1954), TF-IDF (Salton and McGill, 1983), Latent Semantic Analysis (LSA) (Landauer et al., 1998) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999). But the best-known model is Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which describes\nthe hierarchical relationships between words, topics and documents. In LDA, documents are represented as probability distributions over latent topics where each topic is characterized by a distribution over words. However, the probability distribution generated from LDA prefers to describe the statistical relationship of occurrences rather than real semantic information embedded in words, topics and documents. Also LDA will assign high probabilities to high frequency words and those words with low probabilities are hard to be chosen as representatives of topics. But in practice, low probability words sometimes distinguish topics better. For example, LDA will assign higher probability and choose \u201cfood\u201d as representative other than \u201ccheeseburger\u201d, \u201cdrug\u201d other than \u201caricept\u201d and \u201ctechnology\u201d other than \u201csmartphone\u201d.\nRecently, distributed representations with neural probabilistic language models (NPLMs) (Bengio et al., 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Le and Mikolov, 2014). In particular, Word2Vec proposed by Mikolov et al. (2013a) could automatically learn concepts and semantic-syntactic relationships between words like vec(\u201cBerlin\u201d) - vec(\u201cGermany\u201d) = vec(\u201cParis\u201d) - vec(\u201cFrance\u201d). Doc2Vec (Para2Vec) proposed by Le and Mikolov (2014) achieves state-of-the-art performance on sentiment analysis. Naturally, in this paper, we want to answer the question that, what will happen if we embed topics in the semantic vector space?\nFollowing the ideas of previously proposed models for words and documents, we propose the model Topic2Vec as shown in Fig. 1. Based on the Word2Vec, we incorporates topics into the NPLM framework for learning distributed representations\nar X\niv :1\n50 6.\n08 42\n2v 1\n[ cs\n.C L\n] 2\n8 Ju\nn 20\n15\nof topics in the same semantic space with words. Furthermore, words and topics naturally can estimate similarity and relevance with each other such as using cosine function rather than using probability.\nIn the experiments, we evaluate two different topic representations including embedding of Topic2Vec and probability of LDA in two aspects: listed examples and t-SNE 2D embedding of nearest words for each topic. The experimental results show that our Topic2Vec achieves distinctive and meaningful results compared to LDA."}, {"heading": "2 Related Models", "text": ""}, {"heading": "2.1 Latent Dirichlet Allocation", "text": "Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a probabilistic generative model that assumes each document is a mixture of latent topics, where each topic is a probability distribution over all words in vocabulary. Briefly, LDA generates a sequence of words as follows:\n\u2022 For each of the N word wn in document d:\n\u2013 Sample a topic zn \u223cMultinomial(\u03b8d) \u2013 Sample a word wn \u223cMultinomial(\u03c6zn).\nBy Gibbs Sampling 1 estimation, we obtain document-topic probability matrix \u0398 and topicword probability matrix \u03a6. For a new document of arbitrary length, we can infer its involved latent topics and meanwhile we will assign a topic label for each word in the document."}, {"heading": "2.2 Word2Vec", "text": "Inspired by Neural Probabilistic Language Model (NPLM) (Bengio et al., 2003), Mikolov et al. (2013a) proposed Word2Vec including CBOW and Skip-gram for computing continuous vector representations of words from large data sets.\nWhen training, given a word sequence D = {w1, ..., wM}, the learning objective functions are defined to maximize the following log-likelihoods, based on CBOW and Skip-gram, respectively.\n(1a)LCBOW (D) = 1\nM M\u2211 i=1 log p(wi|wcxt),\n(1b)\nLSkip\u2212gram(D)\n= 1\nM M\u2211 i=1 \u2211 \u2212k\u2264c\u2264k,c6=0 log p(wi+c|wi).\n1http://gibbslda.sourceforge.net/\nHere, in Equation (1a), wcxt indicates the context of the current word wi. In Equation (1b), k is the window size of context. For any variables wj and wi, the conditional probability p(wj |wi) is calculated using softmax function as follows,\np(wj |wi) = exp(wj \u00b7wi)\u2211\nw\u2208W exp(w \u00b7wi) , (2)\nwhere w, wi and wj are respectively the word representations of word w, wi and wj , W is the word vocabulary."}, {"heading": "3 Topic2Vec", "text": "Inspired by word2vec, we incorporate topics and words into the NPLM. We propose Topic2Vec as shown in Fig. 1 for learning distributed topic representations together with word representations. Topic2Vec is also separated in CBOW and Skip-gram situations. For instance, given a word sequence (wt\u22122, wt\u22121, wt, wt+1, wt+2), in which wt is the current word assigned with topic zt by LDA. The CBOW predicts the word wt and topic zt based on the surrounding words (wt\u22122, wt\u22121, wt+1, wt+2), while the Skip-gram predicts surrounding words (wt\u22122, wt\u22121, wt+1, wt+2) given current wt and zt.\nWhen training, given a word-topic sequence of a document D = {w1 : z1, ..., wM : zM}, where zi is the word wi\u2019s topic inferred from LDA, the learning objective functions can be defined to maximize the following log-likelihoods, based on CBOW and Skip-gram, respectively.\n(3a)LCBOW (D) = 1\nM M\u2211 i=1 (log p(wi|wcxt)\n+ log p(zi|wcxt)),\n(3b)\nLSkip\u2212gram(D)\n= 1\nM M\u2211 i=1 \u2211 \u2212k\u2264c\u2264k,c6=0 (log p(wi+c|wi)\n+ log p(wi+c|zi)).\nTopic2Vec aims at learning topic representations along with word representations. Considering the simplicity and efficient solution, we just follow the optimization scheme that used in Word2Vec (Mikolov et al., 2013a). To approximately maximize the probability of the softmax, we use Negative Sampling without Hierarchical Softmax (Mikolov et al., 2013b). Stochastic gradient descent (SGD) and back-propagation algorithm are used to optimize our model. By the way, complexity of our Topic2Vec is linear with size of dataset, same with Word2Vec."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "We use the English Gigaword Fifth Edition2 as our training data for learning fundamental word and topic representations. We randomly extract part of documents and construct our training set described as follows: we chose 100,000 documents, where each consists of more than 1,000 characters from subfolder ltw eng (Los Angeles Times) containing 411,032 documents. Besides, we eliminate those words that occur less than 5 times and the stop words. In the end, training set contains about 42 million words and the vocabulary size is 102,644."}, {"heading": "4.2 Evaluation Methods", "text": "In experiments, we run Topic2Vec in Skip-gram and learn topic representations together with word representations. And then we evaluate topic repre-\n2https://catalog.ldc.upenn.edu/LDC2011T07\nsentations via comparing Topic2Vec with LDA in two aspects: (1) we select most related topics or words conditioned on selected topics and (2) we embed these related words or topics in 2D space using t-SNE (Maaten et al., 2008). During the process, we cluster words into topics as follows:\n\u2022 LDA: each topic is a probability distribution over words. We select the top N = 10 words with highest conditional probability.\n\u2022 Topic2Vec: topics and words are equally represented as the low-dimensional vectors, we can immediately calculate the cosine similarity between words and topics. For each topic, we select higher similarity words."}, {"heading": "4.3 Analysis of Results", "text": "Fig. 2 shows top 10 nearest words from LDA and Topic2Vec for eight typically selected topics, respectively. We now give more detailed analysis to understand the difference between them. As shown in Fig. 2, in Topic 19, LDA returns the words like \u201cdrug\u201d, \u201cdrugs\u201d, \u201ccancer\u201d and \u201cpatients\u201d, while Topic2Vec returns \u201caricept\u201d, \u201cmemantine\u201d, \u201cenbrel\u201d and \u201cgabapentin\u201d. In Topic 27, LDA returns the words of \u201cmedical\u201d, \u201chospital\u201d, \u201ccare\u201d, \u201cpatients\u201d and \u201cdoctors\u201d, while Topic2Vec returns \u201cneonatal\u201d, \u201canesthesiologists\u201d, \u201canesthesia\u201d and \u201ccomatose\u201d. We only know that Topic 19 and Topic 27 share the same topic about \u201cpatients\u201d or \u201cmedical\u201d, but we can\u2019t get their further difference from the results of LDA. But from the result of Topic2Vec, we can easily discover that Topic 19 focuses on a more specific topic about drugs (\u201caricept\u201d, \u201cmemantine\u201d, \u201cenbrel\u201d and \u201cgabapentin\u201d), while Topic 27\nfocuses on another specific topic about treatment (\u201canesthesiologists\u201d, \u201canesthesia\u201d and \u201ccomatose\u201d), they are absolutely different. Obviously, Topic2Vec presents more distinguished results between two similar topics.\nFig. 3 shows the 2D embedding of the corresponding related words for each topic by using t-SNE. Obviously, Topic2Vec produces a better grouping and separation of the words in different topics. In contrast, LDA does not produce a well separated embedding, and words in different topics tend to mix together.\nIn summary, for each topic, words selected by Topic2Vec are more typical and representative compared to those returned by LDA. Eventually, Topic2Vec can better distinguish different topics."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, via integrating NPLM, Word2Vec and LDA, we propose the Topic2Vec which successfully embeds latent topics in the same semantic vector space with words. In principle, our purpose clearly aims at learning new fashion topic representation by Topic2Vec. From the observation of experiments, Topic2Vec presents more distinguished results than LDA and we have the conclusion that Topic2Vec can model topics better.\nBut now, we just qualitatively evaluate the performance of Topic2Vec and LDA, we will quantitatively do more detailed analysis about their difference in the future. Besides, we have to run LDA firstly to assign a topic for each word in the corpus before Topic2Vec. We also will explore new independent topic models which can mine thematic structure of documents as LDA and learn inherent representations and model topics better as\nTopic2Vec, simultaneously."}, {"heading": "Acknowledgments", "text": ""}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "An introduction to latent semantic analysis", "author": ["Peter W. Foltz", "Darrell Laham"], "venue": "Discourse processes", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Gerg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Penniington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural network", "author": ["Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Mass et al.2011] Andrew L. Mass", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Mass et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mass et al\\.", "year": 2011}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani"], "venue": "ACM Computing Survey,", "citeRegEx": "Sebastiani.,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani.", "year": 2002}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Manaal Faruqui. Chris Dyer", "Noah A. Smith"], "venue": "In NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "Yogatama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2014}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Topic-based language models using EM", "author": ["Gildea", "Hofmann1999] Daniel Gildea", "Thomas Hofmann"], "venue": "In Proceedings of EUROSPEECH,", "citeRegEx": "Gildea et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 1999}, {"title": "Topical word embeddings", "author": ["Liu et al.2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Inducing ontological co-occurrence vectors", "author": ["Patrick Pantel", "Marina del Rey"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Pantel and Rey.,? \\Q2005\\E", "shortCiteRegEx": "Pantel and Rey.", "year": 2005}, {"title": "The distributional hypothesis", "author": ["Magnus Sahlgren"], "venue": "Italian Journal of Linguistics,", "citeRegEx": "Sahlgren.,? \\Q2008\\E", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Placing search in context: the concept revisited", "author": ["Evgeniy Gabrilocivh", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th International Conference", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North America Chapter of the As-", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Re-examining Machine Translation Metrics for Paraphrase Identification", "author": ["Madnani", "Tetreault2012] Nitin Madnani", "Joel Tetreault"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources", "author": ["Dolan et al.2014] Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In COLING,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Co-learning of Word Representations and Morpheme Representations", "author": ["Qiu et al.2014] Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "More data usually beats better algorithms. Datawocky Blog", "author": ["Anand Rajaraman"], "venue": null, "citeRegEx": "Rajaraman.,? \\Q2008\\E", "shortCiteRegEx": "Rajaraman.", "year": 2008}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations", "author": ["Kiros et al.2014] Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Visualizig data using t-SNE", "author": ["Laurens", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Introduction to Modern Information Retrieval", "author": ["Salton", "McGill1983] Gerard Salton", "Michael J. McGill"], "venue": null, "citeRegEx": "Salton et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1983}, {"title": "Probabilistic Latent Semantic Analysis", "author": ["Thomas Hofmann"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Face Recognition Using LDA Based Algorithms", "author": ["Lu et al.2003] Juwei Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Neural Networks, IEEE Transactions on 14.1", "citeRegEx": "Lu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2003}, {"title": "LDA-Based Document Models for Ad-hoc Retrieval", "author": ["Wei", "Croft2006] Xing Wei", "W. Bruce Croft"], "venue": "In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM,", "citeRegEx": "Wei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2006}, {"title": "Document Clustering using Locality Preserving Indexing", "author": ["Deng et al.2005] Cai Deng", "Xiaofei He", "Jiawei Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 17.12", "citeRegEx": "Deng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "During the past decades, various models and solutions are proposed, such as Bag-of-Words (BOW) (Harris, 1954), TF-IDF (Salton and McGill, 1983), Latent Semantic Analysis (LSA) (Landauer et al., 1998) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999).", "startOffset": 176, "endOffset": 199}, {"referenceID": 35, "context": ", 1998) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999).", "startOffset": 58, "endOffset": 73}, {"referenceID": 3, "context": "But the best-known model is Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which describes the hierarchical relationships between words, topics and documents.", "startOffset": 62, "endOffset": 81}, {"referenceID": 0, "context": "Recently, distributed representations with neural probabilistic language models (NPLMs) (Bengio et al., 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 10, "context": ", 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Le and Mikolov, 2014).", "startOffset": 163, "endOffset": 307}, {"referenceID": 0, "context": "Recently, distributed representations with neural probabilistic language models (NPLMs) (Bengio et al., 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Le and Mikolov, 2014). In particular, Word2Vec proposed by Mikolov et al. (2013a) could automatically learn concepts and semantic-syntactic relationships between words like vec(\u201cBerlin\u201d) vec(\u201cGermany\u201d) = vec(\u201cParis\u201d) - vec(\u201cFrance\u201d).", "startOffset": 89, "endOffset": 470}, {"referenceID": 0, "context": "Recently, distributed representations with neural probabilistic language models (NPLMs) (Bengio et al., 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Le and Mikolov, 2014). In particular, Word2Vec proposed by Mikolov et al. (2013a) could automatically learn concepts and semantic-syntactic relationships between words like vec(\u201cBerlin\u201d) vec(\u201cGermany\u201d) = vec(\u201cParis\u201d) - vec(\u201cFrance\u201d). Doc2Vec (Para2Vec) proposed by Le and Mikolov (2014) achieves state-of-the-art performance on sentiment analysis.", "startOffset": 89, "endOffset": 675}, {"referenceID": 3, "context": "Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a probabilistic generative model that assumes each document is a mixture of latent topics, where each topic is a probability distribution over all words in vocabulary.", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "Inspired by Neural Probabilistic Language Model (NPLM) (Bengio et al., 2003), Mikolov et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 0, "context": "Inspired by Neural Probabilistic Language Model (NPLM) (Bengio et al., 2003), Mikolov et al. (2013a) proposed Word2Vec including CBOW and Skip-gram for computing continuous vector representations of words from large data sets.", "startOffset": 56, "endOffset": 101}, {"referenceID": 33, "context": "sentations via comparing Topic2Vec with LDA in two aspects: (1) we select most related topics or words conditioned on selected topics and (2) we embed these related words or topics in 2D space using t-SNE (Maaten et al., 2008).", "startOffset": 205, "endOffset": 226}], "year": 2015, "abstractText": "Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.", "creator": "LaTeX with hyperref package"}}}