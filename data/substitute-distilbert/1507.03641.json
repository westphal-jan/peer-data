{"id": "1507.03641", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2015", "title": "Neural CRF Parsing", "abstract": "this paper describes a parsing model that combines the exact dynamic programming strategies crf parsing on the rich nonlinear featurization of neural net approaches. our mouse is structurally a crf that factors over anchored rule ones, but instead of linear potential functions based including sparse features, we use nonlinear potentials computed via a feedforward neural interface. because potentials are still local to uncertain rules, gradient inference ( cky ) is unchanged from the static case. computing gradients during learning involves backpropagating an adaptive signal formed from standard crf sufficient statistics ( expected rule counts ). using quantitative dense programming, our neural crf already exceeds a strong baseline crf model ( hall et al., 2014 ). in combination with sparse coding, agile system achieves 91. 1 f1 on section 23 of the penn treebank, and more generally outperforms even poorly observed single parser results out a range of languages.", "histories": [["v1", "Mon, 13 Jul 2015 22:23:51 GMT  (539kb,D)", "http://arxiv.org/abs/1507.03641v1", "Accepted for publication at ACL 2015"]], "COMMENTS": "Accepted for publication at ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["greg durrett", "dan klein"], "accepted": true, "id": "1507.03641"}, "pdf": {"name": "1507.03641.pdf", "metadata": {"source": "CRF", "title": "Neural CRF Parsing", "authors": ["Greg Durrett", "Dan Klein"], "emails": ["gdurrett@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured\n1System available at http://nlp.cs.berkeley.edu\noutput spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; I\u0307rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the case of sequence modeling: feedforward neural networks can be used to score local decisions which are then \u201creconciled\u201d in a discrete structured modeling framework, allowing inference via dynamic programming.\nIn this work, we present a CRF constituency parser based on these principles, where individual anchored rule productions are scored based on nonlinear features computed with a feedforward neural network. A separate, identicallyparameterized replicate of the network exists for each possible span and split point. As input, it takes vector representations of words at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs.\nPrior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model\u2019s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net\u2019s capacity to learn nonlinear features without modifying\nar X\niv :1\n50 7.\n03 64\n1v 1\n[ cs\n.C L\n] 1\n3 Ju\nl 2 01\n5\nour core inference mechanism, allowing us to use tricks like coarse pruning that make inference efficient in the purely sparse model. Our model can be trained by gradient descent exactly as in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point.\nUsing dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014)."}, {"heading": "2 Model", "text": "Figure 1 shows our neural CRF model. The model decomposes over anchored rules, and it scores each of these with a potential function; in a standard CRF, these potentials are typically linear functions of sparse indicator features, whereas\nin our approach they are nonlinear functions of word embeddings.2 Section 2.1 describes our notation for anchored rules, and Section 2.2 talks about how they are scored. We then discuss specific choices of our featurization (Section 2.3) and the backbone grammar used for structured inference (Section 2.4)."}, {"heading": "2.1 Anchored Rules", "text": "The fundamental units that our parsing models consider are anchored rules. As shown in Figure 2, we define an anchored rule as a tuple (r, s), where r is an indicator of the rule\u2019s identity and s = (i, j, k) indicates the span (i, k) and split point j of the rule.3 A tree T is simply a collection of anchored rules subject to the constraint that those rules form a tree. All of our parsing models are CRFs that decompose over anchored rule productions and place a probability distribution over trees conditioned on a sentence w as follows:\nP (T |w) \u221d exp  \u2211 (r,s)\u2208T \u03c6(w, r, s)  2Throughout this work, we will primarily consider two potential functions: linear functions of sparse indicators and nonlinear neural networks over dense, continuous features. Although other modeling choices are possible, these two points in the design space reflect common choices in NLP, and past work has suggested that nonlinear functions of indicators or linear functions of dense features may perform less well (Wang and Manning, 2013).\n3For simplicity of exposition, we ignore unary rules; however, they are easily supported in this framework by simply specifying a null value for the split point.\nwhere \u03c6 is a scoring function that considers the input sentence and the anchored rule in question. Figure 1 shows this scoring process schematically. As we will see, the module on the left can be be a neural net, a linear function of surface features, or a combination of the two, as long as it provides anchored rule scores, and the structured inference component is the same regardless (CKY).\nA PCFG estimated with maximum likelihood has \u03c6(w, r, s) = logP (r|parent(r)), which is independent of the anchoring s and the words w except for preterminal productions; a basic discriminative parser might let this be a learned parameter but still disregard the surface information. However, surface features can capture useful syntactic cues (Finkel et al., 2008; Hall et al., 2014). Consider the example in Figure 2: the proposed parent NP is preceded by the word reflected and followed by a period, which is a surface context characteristic of NPs or PPs in object position. Beginning with the and ending with personality are typical properties of NPs as well, and the choice of the particular rule NP \u2192 NP PP is supported by the fact that the proposed child PP begins with of. This information can be captured with sparse features (fs in Figure 2) or, as we describe below, with a neural network taking lexical context as input."}, {"heading": "2.2 Scoring Anchored Rules", "text": "Following Hall et al. (2014), our baseline sparse scoring function takes the following bilinear form:\n\u03c6sparse(w, r, s;W ) = fs(w, s) >Wfo(r)\nwhere fo(r) \u2208 {0, 1}no is a sparse vector of features expressing properties of r (such as the rule\u2019s identity or its parent label) and fs(w, s) \u2208 {0, 1}ns is a sparse vector of surface features associated with the words in the sentence and the anchoring, as shown in Figure 2. W is a ns \u00d7 no matrix of weights.4 The scoring of a particular anchored rule is depicted in Figure 3a; note that surface features and rule indicators are conjoined in a systematic way.\nThe role of fs can be equally well played by a vector of dense features learned via a neural net-\n4A more conventional expression of the scoring function for a CRF is \u03c6(w, r, s) = \u03b8>f(w, r, s), with a vector \u03b8 for the parameters and a single feature extractor f that jointly inspects the surface and the rule. However, when the feature representation conjoins each rule r with surface properties of the sentence in a systematic way (an assumption that holds in our case as well as for standard CRF models for POS tagging and NER), this is equivalent to our formalism.\nwork. We will now describe how to compute these features, which represent a transformation of surface lexical indicators fw. Define fw(w, s) \u2208 Nnw to be a function that produces a fixed-length sequence of word indicators based on the input sentence and the anchoring. This vector of word identities is then passed to an embedding function v : N \u2192 Rne and the dense representations of the words are subsequently concatenated to form a vector we denote by v(fw).5 Finally, we multiply this by a matrix H \u2208 Rnh\u00d7(nwne) of realvalued parameters and pass it through an elementwise nonlinearity g(\u00b7). We use rectified linear units g(x) = max(x, 0) and discuss this choice more in Section 6.\nReplacing fs with the end result of this computation h(w, s;H) = g(Hv(fw(w, s))), our scoring function becomes\n\u03c6neural(w, r, s;H,W ) = h(w, s;H) >Wfo(r)\nas shown in Figure 3b. For a fixed H , this model can be viewed as a basic CRF with dense input features. By learning H , we learn intermediate feature representations that provide the model with\n5Embedding words allows us to use standard pre-trained vectors more easily and tying embeddings across word positions substantially reduces the number of model parameters. However, embedding features rather than words has also been shown to be effective (Chen et al., 2014).\nmore discriminating power. Also note that it is possible to use deeper networks or more sophisticated architectures here; we will return to this in Section 6.\nOur two models can be easily combined:\n\u03c6(w, r, s;W1, H,W2) = \u03c6sparse(w, r, s;W1)\n+ \u03c6neural(w, r, s;H,W2)\nWeights for each component of the scoring function can be learned fully jointly and inference proceeds as before."}, {"heading": "2.3 Features", "text": "We take fs to be the set of features described in Hall et al. (2014). At the preterminal layer, the model considers prefixes and suffixes up to length 5 of the current word and neighboring words, as well as the words\u2019 identities. For nonterminal productions, we fire indicators on the words6 before and after the start, end, and split point of the anchored rule (as shown in Figure 2) as well as on two other span properties, span length and span shape (an indicator of where capitalized words, numbers, and punctuation occur in the span).\nFor our neural model, we take fw for all productions (preterminal and nonterminal) to be the words surrounding the beginning and end of a span and the split point, as shown in Figure 2; in particular, we look two words in either direction around each point of interest, meaning the neural net takes 12 words as input.7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5. Contrary to standard practice, we do not update these vectors during training; we found that doing so did not provide an accuracy benefit and slowed down training considerably."}, {"heading": "2.4 Grammar Refinements", "text": "A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). Using finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters into W and\n6The model actually uses the longest suffix of each word occurring at least 100 times in the training set, up to the entire word. Removing this abstraction of rare words harms performance.\n7The sparse model did not benefit from using this larger neighborhood, so improvements from the neural net are not simply due to considering more lexical context.\nincreases the ability to overfit. Following Hall et al. (2014), we use grammars with very little annotation: we use no horizontal Markovization for any of experiments, and all of our English experiments with the neural CRF use no vertical Markovization (V = 0). This also has the benefit of making the system much faster, due to the smaller state space for dynamic programming. We do find that using parent annotation (V = 1) is useful on other languages (see Section 7.2), but this is the only grammar refinement we consider."}, {"heading": "3 Learning", "text": "To learn weights for our neural model, we maximize the conditional log likelihood of our D training trees T \u2217:\nL(H,W ) = D\u2211 i=1 logP (T \u2217i |wi;H,W )\nBecause we are using rectified linear units as our nonlinearity, our objective is not everywhere differentiable. The interaction of the parameters and the nonlinearity also makes the objective nonconvex. However, in spite of this, we can still follow subgradients to optimize this objective, as is standard practice.\nRecall that h(w, s;H) are the hidden layer activations. The gradient of W takes the standard form of log-linear models:\n\u2202L \u2202W =  \u2211 (r,s)\u2208T \u2217 h(w, s;H)fo(r) > \u2212 \u2211\nT\nP (T |w;H,W ) \u2211\n(r,s)\u2208T\nh(w, s;H)fo(r) >  Note that the outer products give matrices of feature counts isomorphic to W . The second expression can be simplified to be in terms of expected feature counts. To update H , we use standard backpropagation by first computing:\n\u2202L \u2202h =  \u2211 (r,s)\u2208T \u2217 Wfo(r) \u2212 \u2211\nT\nP (T |w;H,W ) \u2211\n(r,s)\u2208T\nWfo(r)  Since h is the output of the neural network, we can then apply the chain rule to compute gradients for H and any other parameters in the neural network.\nLearning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014). We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box. We set the momentum term \u03c1 = 0.95 (as suggested by Zeiler (2012)) and did not regularize the weights at all. We used a minibatch size of 200 trees, although the system was not particularly sensitive to this. For each treebank, we trained for either 10 passes through the treebank or 1000 minibatches, whichever is shorter.\nWe initialized the output weight matrix W to zero. To break symmetry, the lower level neural network parameters H were initialized with each entry being independently sampled from a Gaussian with mean 0 and variance 0.01; Gaussian performed better than uniform initialization, but the variance was not important."}, {"heading": "4 Inference", "text": "Our baseline and neural model both score anchored rule productions. We can use CKY in the standard fashion to compute either expected anchored rule counts EP (T |w)[(r, s)] or the Viterbi tree arg maxT P (T |w).\nWe speed up inference by using a coarse pruning pass. We follow Hall et al. (2014) and prune according to an X-bar grammar with headoutward binarization, ruling out any constituent whose max marginal probability is less than e\u22129. With this pruning, the number of spans and split points to be considered is greatly reduced; however, we still need to compute the neural network activations for each remaining span and split point, of which there may be thousands for a given sentence.8 We can improve efficiency further by noting that the same word will appear in the same position in a large number of span/split point combinations, and cache the contribution to the hidden layer caused by that word (Chen and Manning, 2014). Computing the hidden layer then simply requires adding nw vectors together and applying the nonlinearity, instead of a more costly matrix multiply.\nBecause the number of rule indicators no is fairly large (approximately 4000 in the Penn Treebank), the multiplication byW in the model is also\n8One reason we did not choose to include the rule identity fo as an input to the network is that it requires computing an even larger number of network activations, since we cannot reuse them across rules over the same span and split point.\nexpensive. However, because only a small number of rules can apply to a given span and split point, fo is sparse and we can selectively compute the terms necessary for the final bilinear product.\nOur combined sparse and neural model trains on the Penn Treebank in 24 hours on a single machine with a parallelized CPU implementation. For reference, the purely sparse model with a parentannotated grammar (necessary for the best results) takes around 15 hours on the same machine."}, {"heading": "5 System Ablations", "text": "Table 1 shows results on section 22 (the development set) of the English Penn Treebank (Marcus et al., 1993), computed using evalb. Full test results and comparisons to other systems are shown in Table 4. We compare variants of our system along two axes: whether they use standard linear sparse features, nonlinear dense features from the neural net, or both, and whether any word representations (vectors or clusters) are used.\nSparse vs. neural The neural CRF (line (d) in Table 1) on its own outperforms the sparse CRF (a, b) even when the sparse CRF has a more heavily annotated grammar. This is a surprising result: the features in the sparse CRF have been carefully engineered to capture a range of linguistic phenomena (Hall et al., 2014), and there is no guarantee that word vectors will capture the same. For example, at the POS tagging layer, the sparse model looks at prefixes and suffixes of words, which give the model access to morphology for predicting tags of unknown words, which typically have regular inflection patterns. By contrast, the neural model must rely on the geometry of the vector space exposing useful regularities. At the same time, the strong performance of the combination of the two systems (g) indicates that not only are both featurization approaches highperforming on their own, but that they have complementary strengths.\nUnlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014).\nHowever, as these embeddings are trained on a relatively small corpus (BLLIP minus the Penn Treebank), it is natural to wonder whether lesssyntactic embeddings trained on a larger corpus might be more useful. This is not the case: line (e) in Table 1 shows the performance of the neural CRF using the Wikipedia-trained word embeddings of Collobert et al. (2011), which do not perform better than the vectors of Bansal et al. (2014).\nTo isolate the contribution of continuous word representations themselves, we also experimented with vectors trained on just the text from the training set of the Penn Treebank using the skip-gram model with a window size of 1. While these vectors are somewhat lower performing on their own (f), they still provide a surprising and noticeable gain when stacked on top of sparse features (h), again suggesting that dense and sparse representations have complementary strengths. This result also reinforces the notion that the utility of word vectors does not come primarily from importing information about out-of-vocabulary words (Andreas and Klein, 2014).\nSince the neural features incorporate information from unlabeled data, we should provide the\nsparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or that different regularities are being captured by the word embeddings."}, {"heading": "6 Design Choices", "text": "The neural net design space is large, so we wish to analyze the particular design choices we made for this system by examining the performance of several variants of the neural net architecture used in our system. Table 2 shows development results from potential alternate architectural choices, which we now discuss.\nChoice of nonlinearity The choice of nonlinearity g has been frequently discussed in the neural network literature. Our choice g(x) = max(x, 0), a rectified linear unit, is increasingly popular in\ncomputer vision (Krizhevsky et al., 2012). g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003). g(x) = x3 (cube) was found to be most successful by Chen and Manning (2014).\nTable 2 compares the performance of these three nonlinearities. We see that rectified linear units perform the best, followed by tanh units, followed by cubic units.9 One drawback of tanh as an activation function is that it is easily \u201csaturated\u201d if the input to the unit is too far away from zero, causing the backpropagation of derivatives through that unit to essentially cease; this is known to cause problems for training, requiring special purpose machinery for use in deep networks (Ioffe and Szegedy, 2015).\nDepth Given that we are using rectified linear units, it bears asking whether or not our implementation is improving substantially over linear features of the continuous input. We can use the embedding vector of an anchored span v(fw) directly as input to a basic linear CRF, as shown in Figure 4a. Table 1 shows that the purely linear architecture (0 HL) performs surprisingly well, but is still less effective than the network with one hidden layer. This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling. We also compare against a two-layer neural network, but find that this also performs worse than the one-layer architecture.\nDensifying output features Overall, it appears beneficial to use dense representations of surface features; a natural question that one might ask is whether the same technique can be applied to the sparse output feature vector fo. We can apply the approach of Srikumar and Manning (2014) and multiply the sparse output vector by a dense matrix K, giving the following scoring function (shown in Figure 4b):\n\u03c6(w, r, s;H,W,K) = g(Hv(fw(w, s))) >WKfo(r)\nwhere W is now nh \u00d7 noe and K is noe \u00d7 no. WK can be seen a low-rank approximation of the original W at the output layer, similar to low-rank factorizations of parameter matrices used in past\n9The performance of cube decreased substantially late in learning; it peaked at around 90.52. Dropout may be useful for alleviating this type of overfitting, but in our experiments we did not find dropout to be beneficial overall.\nwork (Lei et al., 2014). This approach saves us from having to learn a separate row of W for every rule in the grammar; if rules are given similar embeddings, then they will behave similarly according to the model.\nWe experimented with noe = 20 and show the results in Table 2. Unfortunately, this approach does not seem to work well for parsing. Learning the output representation was empirically very unstable, and it also required careful initialization. We tried Gaussian initialization (as in the rest of our model) and initializing the model by clustering rules either randomly or according to their parent symbol. The latter is what is shown in the table, and gave substantially better performance. We hypothesize that blurring distinctions between output classes may harm the model\u2019s ability to differentiate between closely-related symbols, which is required for good parsing performance. Using pretrained rule embeddings at this layer might also improve performance of this method."}, {"heading": "7 Test Results", "text": "We evaluate our system under two conditions: first, on the English Penn Treebank, and second, on the nine languages used in the SPMRL 2013 and 2014 shared tasks."}, {"heading": "7.1 Penn Treebank", "text": "Table 4 reports results on section 23 of the Penn Treebank (PTB). We focus our comparison on single parser systems as opposed to rerankers, ensembles, or self-trained methods (though these are also mentioned for context). First, we compare against\nfour parsers trained only on the PTB with no auxiliary data: the CRF parser of Hall et al. (2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and\nthe single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1, though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser.\nWe also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014)."}, {"heading": "7.2 SPMRL", "text": "We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014). We train word vectors on the monolingual data distributed with the SPMRL 2014 shared task (typically 100M-200M tokens per language) using the skip-gram approach of word2vec with a window size of 1\n(Mikolov et al., 2013).10 Here we use V = 1 in the backbone grammar, which we found to be beneficial overall. Table 3 shows that our system improves upon the performance of the parser from Hall et al. (2014) as well as the top single parser from the shared task (Crabbe\u0301 and Seddah, 2014), with robust improvements on all languages."}, {"heading": "8 Conclusion", "text": "In this work, we presented a CRF parser that scores anchored rule productions using dense input features computed from a feedforward neural net. Because the neural component is modularized, we can easily integrate it into a preexisting learning and inference framework based around dynamic programming of a discrete parse chart. Our combined neural and sparse model gives strong performance both on English and on other languages.\nOur system is publicly available at http://nlp.cs.berkeley.edu."}, {"heading": "Acknowledgments", "text": "This work was partially supported by BBN under DARPA contract HR0011-12-C-0014, by a Facebook fellowship for the first author, and by a Google Faculty Research Award to the second author. Thanks to David Hall for assistance with the Epic parsing framework and for a preliminary implementation of the neural architecture, to Kush Rastogi for training word vectors on the SPMRL data, to Dan Jurafsky for helpful discussions, and to the anonymous reviewers for their insightful comments."}], "references": [{"title": "How much do word embeddings encode about syntax", "author": ["Jacob Andreas", "Dan Klein"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Andreas and Klein.,? \\Q2014\\E", "shortCiteRegEx": "Andreas and Klein.", "year": 2014}, {"title": "Tailoring Continuous Word Representations for Dependency Parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment", "author": ["Yonatan Belinkov", "Tao Lei", "Regina Barzilay", "Amir Globerson."], "venue": "Transactions of the Association for Computational Linguistics, 2:561\u2013572.", "citeRegEx": "Belinkov et al\\.,? 2014", "shortCiteRegEx": "Belinkov et al\\.", "year": 2014}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Re)ranking Meets Morphosyntax: State-of-the-art Results from the SPMRL 2013 Shared Task", "author": ["Anders Bj\u00f6rkelund", "Ozlem Cetinoglu", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker."], "venue": "Proceedings of the Fourth Workshop on Statistical Pars-", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2013", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2013}, {"title": "Introducing the IMS-Wroc\u0142aw-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Agnieszka Fale\u0144ska", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo."], "venue": "Proceedings of the Conference on Computational Natural Language Learning.", "citeRegEx": "Carreras et al\\.,? 2008", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of Empirical Methods in Natural Language Processing.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Feature Embedding for Dependency Parsing", "author": ["Wenliang Chen", "Yue Zhang", "Min Zhang."], "venue": "Proceedings of the International Conference on Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Multilingual Discriminative Shift-Reduce Phrase Structure Parsing for the SPMRL 2014 Shared Task", "author": ["Benoit Crabb\u00e9", "Djam\u00e9 Seddah."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntac-", "citeRegEx": "Crabb\u00e9 and Seddah.,? 2014", "shortCiteRegEx": "Crabb\u00e9 and Seddah.", "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Efficient, Feature-based, Conditional Random Field Parsing", "author": ["Jenny Rose Finkel", "Alex Kleeman", "Christopher D. Manning."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Finkel et al\\.,? 2008", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Less Grammar, More Features", "author": ["David Hall", "Greg Durrett", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Inducing History Representations for Broad Coverage Statistical Parsing", "author": ["James Henderson."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Henderson.,? 2003", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv preprint, arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["Ozan \u0130rsoy", "Claire Cardie."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "\u0130rsoy and Cardie.,? 2014", "shortCiteRegEx": "\u0130rsoy and Cardie.", "year": 2014}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Accurate Unlexicalized Parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Simple Semi-supervised Dependency Parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Low-Rank Tensors for Scoring Dependency Structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2).", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improved Inference for Unlexicalized Parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Petrov and Klein.,? 2008", "shortCiteRegEx": "Petrov and Klein.", "year": 2008}, {"title": "Products of Random Latent Variable Grammars", "author": ["Slav Petrov."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Petrov.,? 2010", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Overview of the SPMRL 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morpho", "author": ["Nivre", "Adam Przepi\u00f3rkowski", "Ryan Roth", "Wolfgang Seeker", "Yannick Versley", "Veronika Vincze", "Marcin Woli\u0144ski", "Alina Wr\u00f3blewska"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2013}, {"title": "Introducing the SPMRL 2014 Shared Task on Parsing Morphologically-rich Languages", "author": ["Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and", "citeRegEx": "Seddah et al\\.,? 2014", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Bayesian Symbol-refined Tree Substitution Grammars for Syntactic Parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing With Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning Distributed Representations for Structured Output Prediction", "author": ["Vivek Srikumar", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Srikumar and Manning.,? 2014", "shortCiteRegEx": "Srikumar and Manning.", "year": 2014}, {"title": "Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging", "author": ["Yuta Tsuboi."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Tsuboi.,? 2014", "shortCiteRegEx": "Tsuboi.", "year": 2014}, {"title": "Word Representations: A Simple and General Method for Semi-supervised Learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "CoRR, abs/1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Effect of Non-linear Deep Architecture in Sequence Labeling", "author": ["Mengqiu Wang", "Christopher D. Manning."], "venue": "Proceedings of the International Joint Conference on Natural Language Processing.", "citeRegEx": "Wang and Manning.,? 2013", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Fast and Accurate ShiftReduce Constituent Parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014).", "startOffset": 86, "endOffset": 105}, {"referenceID": 8, "context": "In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al.", "startOffset": 108, "endOffset": 155}, {"referenceID": 2, "context": "In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al.", "startOffset": 108, "endOffset": 155}, {"referenceID": 18, "context": ", 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 29, "endOffset": 67}, {"referenceID": 19, "context": ", 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 29, "endOffset": 67}, {"referenceID": 15, "context": "Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al.", "startOffset": 60, "endOffset": 122}, {"referenceID": 34, "context": "Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al.", "startOffset": 60, "endOffset": 122}, {"referenceID": 17, "context": "Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al.", "startOffset": 60, "endOffset": 122}, {"referenceID": 34, "context": ", 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014).", "startOffset": 171, "endOffset": 214}, {"referenceID": 23, "context": ", 2013; \u0130rsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014).", "startOffset": 171, "endOffset": 214}, {"referenceID": 10, "context": "However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the case of sequence modeling: feedforward neural networks can be used to score local decisions which are then \u201creconciled\u201d in a discrete structured modeling framework, allowing inference via dynamic programming.", "startOffset": 118, "endOffset": 142}, {"referenceID": 15, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al.", "startOffset": 139, "endOffset": 194}, {"referenceID": 8, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al.", "startOffset": 139, "endOffset": 194}, {"referenceID": 36, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al.", "startOffset": 139, "endOffset": 194}, {"referenceID": 34, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model\u2019s structured interactions are purely discrete and do not involve continuous hidden state.", "startOffset": 217, "endOffset": 260}, {"referenceID": 23, "context": "Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model\u2019s structured interactions are purely discrete and do not involve continuous hidden state.", "startOffset": 217, "endOffset": 260}, {"referenceID": 28, "context": "(2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 13, "context": "Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.", "startOffset": 115, "endOffset": 134}, {"referenceID": 13, "context": "Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al.", "startOffset": 115, "endOffset": 298}, {"referenceID": 6, "context": "(2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al.", "startOffset": 105, "endOffset": 128}, {"referenceID": 6, "context": "(2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014).", "startOffset": 105, "endOffset": 261}, {"referenceID": 39, "context": "Although other modeling choices are possible, these two points in the design space reflect common choices in NLP, and past work has suggested that nonlinear functions of indicators or linear functions of dense features may perform less well (Wang and Manning, 2013).", "startOffset": 241, "endOffset": 265}, {"referenceID": 13, "context": "However, surface features can capture useful syntactic cues (Finkel et al., 2008; Hall et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 14, "context": "However, surface features can capture useful syntactic cues (Finkel et al., 2008; Hall et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 14, "context": "Following Hall et al. (2014), our baseline sparse scoring function takes the following bilinear form:", "startOffset": 10, "endOffset": 29}, {"referenceID": 9, "context": "However, embedding features rather than words has also been shown to be effective (Chen et al., 2014).", "startOffset": 82, "endOffset": 101}, {"referenceID": 14, "context": "We take fs to be the set of features described in Hall et al. (2014). At the preterminal layer, the model considers prefixes and suffixes up to length 5 of the current word and neighboring words, as well as the words\u2019 identities.", "startOffset": 50, "endOffset": 69}, {"referenceID": 1, "context": "7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5.", "startOffset": 66, "endOffset": 87}, {"referenceID": 13, "context": "A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014).", "startOffset": 110, "endOffset": 174}, {"referenceID": 29, "context": "A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014).", "startOffset": 110, "endOffset": 174}, {"referenceID": 14, "context": "A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014).", "startOffset": 110, "endOffset": 174}, {"referenceID": 14, "context": "Following Hall et al. (2014), we use grammars with very little annotation: we use no horizontal Markovization for any of experiments, and all of our English experiments with the neural CRF use no vertical Markovization (V = 0).", "startOffset": 10, "endOffset": 29}, {"referenceID": 40, "context": "Learning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014).", "startOffset": 23, "endOffset": 37}, {"referenceID": 19, "context": "Learning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014).", "startOffset": 76, "endOffset": 87}, {"referenceID": 12, "context": "We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box.", "startOffset": 22, "endOffset": 42}, {"referenceID": 12, "context": "We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box. We set the momentum term \u03c1 = 0.95 (as suggested by Zeiler (2012)) and did not regularize the weights at all.", "startOffset": 23, "endOffset": 226}, {"referenceID": 8, "context": "8 We can improve efficiency further by noting that the same word will appear in the same position in a large number of span/split point combinations, and cache the contribution to the hidden layer caused by that word (Chen and Manning, 2014).", "startOffset": 217, "endOffset": 241}, {"referenceID": 13, "context": "We follow Hall et al. (2014) and prune according to an X-bar grammar with headoutward binarization, ruling out any constituent whose max marginal probability is less than e\u22129.", "startOffset": 10, "endOffset": 29}, {"referenceID": 26, "context": "Table 1 shows results on section 22 (the development set) of the English Penn Treebank (Marcus et al., 1993), computed using evalb.", "startOffset": 87, "endOffset": 108}, {"referenceID": 14, "context": "This is a surprising result: the features in the sparse CRF have been carefully engineered to capture a range of linguistic phenomena (Hall et al., 2014), and there is no guarantee that word vectors will capture the same.", "startOffset": 134, "endOffset": 153}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014).", "startOffset": 163, "endOffset": 209}, {"referenceID": 25, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014).", "startOffset": 163, "endOffset": 209}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al.", "startOffset": 164, "endOffset": 262}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014).", "startOffset": 164, "endOffset": 317}, {"referenceID": 1, "context": "Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014).", "startOffset": 164, "endOffset": 421}, {"referenceID": 9, "context": "This is not the case: line (e) in Table 1 shows the performance of the neural CRF using the Wikipedia-trained word embeddings of Collobert et al. (2011), which do not perform better than the vectors of Bansal et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 1, "context": "(2011), which do not perform better than the vectors of Bansal et al. (2014).", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": "This result also reinforces the notion that the utility of word vectors does not come primarily from importing information about out-of-vocabulary words (Andreas and Klein, 2014).", "startOffset": 153, "endOffset": 178}, {"referenceID": 21, "context": "Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014).", "startOffset": 68, "endOffset": 128}, {"referenceID": 37, "context": "Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014).", "startOffset": 68, "endOffset": 128}, {"referenceID": 1, "context": "Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014).", "startOffset": 68, "endOffset": 128}, {"referenceID": 1, "context": ", 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al.", "startOffset": 8, "endOffset": 322}, {"referenceID": 1, "context": ", 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or that different regularities are being captured by the word embeddings.", "startOffset": 8, "endOffset": 397}, {"referenceID": 22, "context": "computer vision (Krizhevsky et al., 2012).", "startOffset": 16, "endOffset": 41}, {"referenceID": 3, "context": "g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003).", "startOffset": 95, "endOffset": 116}, {"referenceID": 3, "context": "g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003). g(x) = x3 (cube) was found to be most successful by Chen and Manning (2014).", "startOffset": 96, "endOffset": 194}, {"referenceID": 16, "context": "9 One drawback of tanh as an activation function is that it is easily \u201csaturated\u201d if the input to the unit is too far away from zero, causing the backpropagation of derivatives through that unit to essentially cease; this is known to cause problems for training, requiring special purpose machinery for use in deep networks (Ioffe and Szegedy, 2015).", "startOffset": 324, "endOffset": 349}, {"referenceID": 39, "context": "This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling.", "startOffset": 32, "endOffset": 56}, {"referenceID": 35, "context": "We can apply the approach of Srikumar and Manning (2014) and multiply the sparse output vector by a dense matrix K, giving the following scoring function (shown in Figure 4b):", "startOffset": 29, "endOffset": 57}, {"referenceID": 24, "context": "work (Lei et al., 2014).", "startOffset": 5, "endOffset": 23}, {"referenceID": 11, "context": "89 Crabb\u00e9 and Seddah (2014) 77.", "startOffset": 3, "endOffset": 28}, {"referenceID": 11, "context": "89 Crabb\u00e9 and Seddah (2014) 77.66 85.35 79.68 77.15 86.19 87.51 79.35 91.60 82.72 83.02 Hall et al. (2014) 78.", "startOffset": 3, "endOffset": 107}, {"referenceID": 14, "context": "Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb\u00e9 and Seddah, 2014).", "startOffset": 89, "endOffset": 133}, {"referenceID": 11, "context": "Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb\u00e9 and Seddah, 2014).", "startOffset": 89, "endOffset": 133}, {"referenceID": 4, "context": "2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj\u00f6rkelund et al., 2013; Bj\u00f6rkelund et al., 2014).", "startOffset": 121, "endOffset": 171}, {"referenceID": 5, "context": "2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj\u00f6rkelund et al., 2013; Bj\u00f6rkelund et al., 2014).", "startOffset": 121, "endOffset": 171}, {"referenceID": 6, "context": "1 Carreras et al. (2008) 91.", "startOffset": 2, "endOffset": 25}, {"referenceID": 6, "context": "1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.", "startOffset": 2, "endOffset": 51}, {"referenceID": 28, "context": "We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 34, "context": "We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 33, "context": ", 2013) and we match the performance of sophisticated generative (Shindo et al., 2012) and discriminative (Carreras et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 6, "context": ", 2012) and discriminative (Carreras et al., 2008) parsers.", "startOffset": 27, "endOffset": 50}, {"referenceID": 28, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 13, "context": "four parsers trained only on the PTB with no auxiliary data: the CRF parser of Hall et al. (2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al.", "startOffset": 79, "endOffset": 98}, {"referenceID": 6, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 6, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.", "startOffset": 83, "endOffset": 157}, {"referenceID": 6, "context": "(2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1, though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters.", "startOffset": 83, "endOffset": 441}, {"referenceID": 34, "context": "We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et", "startOffset": 68, "endOffset": 89}, {"referenceID": 20, "context": "The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010).", "startOffset": 128, "endOffset": 153}, {"referenceID": 30, "context": "The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010).", "startOffset": 247, "endOffset": 261}, {"referenceID": 20, "context": "The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014).", "startOffset": 129, "endOffset": 368}, {"referenceID": 32, "context": "We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014).", "startOffset": 160, "endOffset": 202}, {"referenceID": 27, "context": "(Mikolov et al., 2013).", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "(2014) as well as the top single parser from the shared task (Crabb\u00e9 and Seddah, 2014), with robust improvements on all languages.", "startOffset": 61, "endOffset": 86}, {"referenceID": 13, "context": "Table 3 shows that our system improves upon the performance of the parser from Hall et al. (2014) as well as the top single parser from the shared task (Crabb\u00e9 and Seddah, 2014), with robust improvements on all languages.", "startOffset": 79, "endOffset": 98}], "year": 2015, "abstractText": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.", "creator": "TeX"}}}