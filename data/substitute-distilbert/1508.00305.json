{"id": "1508.00305", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2015", "title": "Compositional Semantic Parsing on Semi-Structured Tables", "abstract": "supporting important aspects of semantic strategy for complex answering are some breadth of the knowledge source and the depth across logical compositionality. while existing work trades provide anything aspect for another, this procedure simultaneously emphasizes progress on both fronts through a new task : answering complex questions on y - structured tables using question - answer pairs as keys. the central challenge arises from two compounding factors : the broader domain involved in an open - ended set of relations, and the deeper compositionality results in a combinatorial extension in the space of logical forms. we propose a logical - form driven parsing algorithm guided as strong typing constraints and show that it obtains significant improvements while natural baselines. for evaluation, we created a new dataset of 22, 160 complex questions on wikipedia tables, which is made publicly available.", "histories": [["v1", "Mon, 3 Aug 2015 02:53:01 GMT  (271kb,D)", "http://arxiv.org/abs/1508.00305v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["panupong pasupat", "percy liang"], "accepted": true, "id": "1508.00305"}, "pdf": {"name": "1508.00305.pdf", "metadata": {"source": "CRF", "title": "Compositional Semantic Parsing on Semi-Structured Tables", "authors": ["Panupong Pasupat", "Percy Liang"], "emails": ["ppasupat@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a\nrigid schema over entities and relation types, thus restricting the scope of answerable questions.\nTo simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose a new task (with an associated dataset): answering a question using an HTML table as the knowledge source. Figure 1 shows several question-answer pairs and an accompanying table, which are typical of those in our dataset. Note that the questions are logically quite complex, involving a variety of operations such as comparison (x2), superlatives (x3), aggregation (x4), and arithmetic (x5).\nThe HTML tables are semi-structured and not normalized. For example, a cell might contain multiple parts (e.g., \u201cBeijing, China\u201d or \u201c200 km\u201d). Additionally, we mandate that the training and test tables are disjoint, so at test time, we will see relations (column headers; e.g., \u201cNations\u201d) and entities (table cells; e.g., \u201cSt. Louis\u201d)\nar X\niv :1\n50 8.\n00 30\n5v 1\n[ cs\n.C L\n] 3\nA ug\n2 01\n5\nthat were not observed during training. This is in contrast to knowledge bases like Freebase, which have a global fixed relation schema with normalized entities and relations.\nOur task setting produces two main challenges. Firstly, the increased breadth in the knowledge source requires us to generate logical forms from novel tables with previously unseen relations and entities. We therefore cannot follow the typical semantic parsing strategy of constructing or learning a lexicon that maps phrases to relations ahead of time. Secondly, the increased depth in compositionality and additional logical operations exacerbate the exponential growth of the number of possible logical forms.\nWe trained a semantic parser for this task from question-answer pairs based on the framework illustrated in Figure 2. First, relations and entities from the semi-structured HTML table are encoded in a graph. Then, the system parses the question into candidate logical forms with a high-coverage grammar, reranks the candidates with a log-linear model, and then executes the highest-scoring logical form to produce the answer denotation. We use beam search with pruning strategies based on type and denotation constraints to control the combinatorial explosion.\nTo evaluate the system, we created a new dataset, WIKITABLEQUESTIONS, consisting of 2,108 HTML tables from Wikipedia and 22,033 question-answer pairs. When tested on unseen tables, the system achieves an accuracy of 37.1%, which is significantly higher than the information retrieval baseline of 12.7% and a simple semantic parsing baseline of 24.3%."}, {"heading": "2 Task", "text": "Our task is as follows: given a table t and a question x about the table, output a list of values y that answers the question according to the table. Example inputs and outputs are shown in Figure 1. The system has access to a training set D = {(xi, ti, yi)}Ni=1 of questions, tables, and answers, but the tables in test data do not appear during training.\nThe only restriction on the question x is that a person must be able to answer it using just the table t. Other than that, the question can be of any type, ranging from a simple table lookup question to a more complicated one that involves various logical operations.\nDataset. We created a new dataset, WIKITABLEQUESTIONS, of question-answer pairs on HTML tables as follows. We randomly selected data tables from Wikipedia with at least 8 rows and 5 columns. We then created two Amazon Mechanical Turk tasks. The first task asks workers to write trivia questions about the table. For each question, we put one of the 36 generic prompts such as \u201cThe question should require calculation\u201d or \u201ccontains the word \u2018first\u2019 or its synonym\u201d to encourage more complex utterances. Next, we submit the resulting questions to the second task where the workers answer each question based on the given table. We only keep the answers that are agreed upon by at least two workers. After this filtering, approximately 69% of the questions remains.\nThe final dataset contains 22,033 examples on 2,108 tables. We set aside 20% of the tables and their associated questions as the test set and develop on the remaining examples. Simple preprocessing was done on the tables: We omit all non-textual contents of the tables, and if there is a merged cell spanning many rows or columns, we unmerge it and duplicate its content into each unmerged cell. Section 7.2 analyzes various aspects of the dataset and compares it to other datasets."}, {"heading": "3 Approach", "text": "We now describe our semantic parsing framework for answering a given question and for training the model with question-answer pairs.\nPrediction. Given a table t and a question x, we predict an answer y using the framework illustrated in Figure 2. We first convert the table t into a knowledge graph w (\u201cworld\u201d) which encodes different relations in the table (Section 4). Next, we generate a set of candidate logical forms Zx by parsing the question x using the information from w (Section 6.1). Each generated logical form z \u2208 Zx is a graph query that can be executed on the knowledge graph w to get a denotation JzKw. We extract a feature vector \u03c6(x,w, z) for each z \u2208 Zx (Section 6.2) and define a loglinear distribution over the candidates:\np\u03b8(z | x,w) \u221d exp{\u03b8>\u03c6(x,w, z)}, (1)\nwhere \u03b8 is the parameter vector. Finally, we choose the logical form z with the highest model probability and execute it on w to get the answer denotation y = JzKw.\nTraining. Given training examples D = {(xi, ti, yi)}Ni=1, we seek a parameter vector \u03b8 that maximizes the regularized log-likelihood of the correct denotation yi marginalized over logical forms z. Formally, we maximize the objective function\nJ(\u03b8) = 1\nN N\u2211 i=1 log p\u03b8(yi | xi, wi)\u2212 \u03bb \u2016\u03b8\u20161 , (2)\nwhere wi is deterministically generated from ti, and\np\u03b8(y | x,w) = \u2211\nz\u2208Zx;y=JzKw p\u03b8(z | x,w). (3)\nWe optimize \u03b8 using AdaGrad (Duchi et al., 2010), running 3 passes over the data. We use L1 regularization with \u03bb = 3 \u00d7 10\u22125 obtained from cross-validation.\nThe following sections explain individual system components in more detail."}, {"heading": "4 Knowledge graph", "text": "Inspired by the graph representation of knowledge bases, we preprocess the table t by deterministically converting it into a knowledge graph w as illustrated in Figure 3. In the most basic form, table rows become row nodes, strings in table cells become entity nodes,1 and table columns become directed edges from the row nodes to the entity\n1Two occurrences of the same string constitute one node.\nnodes of that column. The column headers are used as edge labels for these row-entity relations.\nThe knowledge graph representation is convenient for three reasons. First, we can encode different forms of entity normalization in the graph. Some entity strings (e.g., \u201c1900\u201d) can be interpreted as a number, a date, or a proper name depending on the context, while some other strings (e.g., \u201c200 km\u201d) have multiple parts. Instead of committing to one normalization scheme, we introduce edges corresponding to different normalization methods from the entity nodes. For example, the node 1900 will have an edge called Date to another node 1900-XX-XX of type date. Apart from type checking, these normalization nodes also aid learning by providing signals on the appropriate answer type. For instance, we can define a feature that associates the phrase \u201chow many\u201d with a logical form that says \u201ctraverse a row-entity edge, then a Number edge\u201d instead of just \u201ctraverse a row-entity edge.\u201d\nThe second benefit of the graph representation is its ability to handle various logical phenomena via graph augmentation. For example, to answer questions of the form \u201cWhat is the next . . . ?\u201d or \u201cWho came before . . . ?\u201d, we augment each row node with an edge labeled Next pointing to the next row node, after which the questions can be answered by traversing the Next edge. In this work, we choose to add two special edges on each row node: the Next edge mentioned above and an Index edge pointing to the row index number (0, 1, 2, . . . ).\nFinally, with a graph representation, we can query it directly using a logical formalism for knowledge graphs, which we turn to next."}, {"heading": "5 Logical forms", "text": "As our language for logical forms, we use lambda dependency-based compositional semantics (Liang, 2013), or lambda DCS, which we briefly describe here. Each lambda DCS logical form is either a unary (denoting a list of values) or a binary (denoting a list of pairs). The most basic unaries are singletons (e.g., China represents an entity node, and 30 represents a single number), while the most basic binaries are relations (e.g., City maps rows to city entities, Next maps rows to rows, and >= maps numbers to numbers). Logical forms can be combined into larger ones via various operations listed in Table 1. Each operation produces a unary except lambda abstraction: \u03bbx[f(x)] is a binary mapping x to f(x)."}, {"heading": "6 Parsing and ranking", "text": "Given the knowledge graph w, we now describe how to parse the utterance x into a set of candidate logical forms Zx"}, {"heading": "6.1 Parsing algorithm", "text": "We propose a new floating parser which is more flexible than a standard chart parser. Both parsers recursively build up derivations and corresponding logical forms by repeatedly applying deduction rules, but the floating parser allows logical form predicates to be generated independently from the utterance.\nChart parser. We briefly review the CKY algorithm for chart parsing to introduce notation. Given an utterance with tokens x1, . . . , xn, the CKY algorithm applies deduction rules of the fol-\nlowing two kinds:\n(TokenSpan, i, j)[s]\u2192 (c, i, j)[f(s)], (4) (c1, i, k)[z1] + (c2, k + 1, j)[z2] (5)\n\u2192 (c, i, j)[f(z1, z2)].\nThe first rule is a lexical rule that matches an utterance token span xi \u00b7 \u00b7 \u00b7xj (e.g., s = \u201cNew York\u201d) and produces a logical form (e.g., f(s) = NewYorkCity) with category c (e.g., Entity). The second rule takes two adjacent spans giving rise to logical forms z1 and z2 and builds a new logical form f(z1, z2). Algorithmically, CKY stores derivations of category c covering the span xi \u00b7 \u00b7 \u00b7xj in a cell (c, i, j). CKY fills in the cells of increasing span lengths, and the logical forms in the top cell (ROOT, 1, n) are returned.\nFloating parser. Chart parsing uses lexical rules (4) to generate relevant logical predicates, but in our setting of semantic parsing on tables, we do not have the luxury of starting with or inducing a full-fledged lexicon. Moreover, there is a mismatch between words in the utterance and predicates in the logical form. For instance, consider the question \u201cGreece held its last Summer Olympics in which year?\u201d on the table in Figure 1 and the correct logical form R[\u03bbx[Year.Date.x]].argmax(Country.Greece, Index). While the entity Greece can be anchored to the token \u201cGreece\u201d, some logical predicates (e.g., Country) cannot be clearly anchored to a token span. We could potentially learn to anchor the logical form Country.Greece to \u201cGreece\u201d, but if the relation Country is not seen during training, such a mapping is impossible to learn from the\ntraining data. Similarly, some prominent tokens (e.g., \u201cOlympics\u201d) are irrelevant and have no predicates anchored to them.\nTherefore, instead of anchoring each predicate in the logical form to tokens in the utterance via lexical rules, we propose parsing more freely. We replace the anchored cells (c, i, j) with floating cells (c, s) of category c and logical form size s. Then we apply rules of the following three kinds:\n(TokenSpan, i, j)[s]\u2192 (c, 1)[f(s)], (6) \u2205 \u2192 (c, 1)[f()], (7)\n(c1, s1)[z1] + (c2, s2)[z2] (8)\n\u2192 (c, s1 + s2 + 1)[f(z1, z2)].\nNote that rules (6) are similar to (4) in chart parsing except that the floating cell (c, 1) only keeps track of the category and its size 1, not the span (i, j). Rules (7) allow us to construct predicates out of thin air. For example, we can construct a logical form representing a table relation Country in cell (Relation, 1) using the rule \u2205 \u2192 Relation [Country] independent of the utterance. Rules (8) perform composition, where the induction is on the size s of the logical form rather than the span length. The algorithm stops when the specified maximum size is reached, after which the logical forms in cells (ROOT, s) for any\ns are included in Zx. Figure 4 shows an example derivation generated by our floating parser.\nThe floating parser is very flexible: it can skip tokens and combine logical forms in any order. This flexibility might seem too unconstrained, but we can use strong typing constraints to prevent nonsensical derivations from being constructed.\nTables 2 and 3 show the full set of deduction\n\u201cGreece held its last Summer Olympics in which year?\u201d z = R[\u03bbx[Year.Number.x]].argmax(Type.Row, Index)\ny = {2012} (type: NUM, column: YEAR) Feature Name Note (\u201clast\u201d, predicate = argmax) lex phrase = predicate unlex (\u2235 \u201cyear\u201d = Year) missing entity unlex (\u2235 missing Greece) denotation type = NUM denotation column = YEAR (\u201cwhich year\u201d, type = NUM) lex phrase = column unlex (\u2235 \u201cyear\u201d = YEAR) (Q = \u201cwhich\u201d, type = NUM) lex (H = \u201cyear\u201d, type = NUM) lex H = column unlex (\u2235 \u201cyear\u201d = YEAR)\nTable 4: Example features that fire for the (incorrect) logical form z. All features are binary. (lex = lexicalized)\nrules we use. We assume that all named entities will explicitly appear in the question x, so we anchor all entity predicates (e.g., Greece) to token spans (e.g., \u201cGreece\u201d). We also anchor all numerical values (numbers, dates, percentages, etc.) detected by an NER system. In contrast, relations (e.g., Country) and operations (e.g., argmax) are kept floating since we want to learn how they are expressed in language. Connections between phrases in x and the generated relations and operations in z are established in the ranking model through features."}, {"heading": "6.2 Features", "text": "We define features \u03c6(x,w, z) for our log-linear model to capture the relationship between the question x and the candidate z. Table 4 shows some example features from each feature type. Most features are of the form (f(x), g(z)) or (f(x), h(y)) where y = JzKw is the denotation, and f , g, and h extract some information (e.g., identity, POS tags) from x, z, or y, respectively.\nphrase-predicate: Conjunctions between ngrams f(x) from x and predicates g(z) from z. We use both lexicalized features, where all possible pairs (f(x), g(z)) form distinct features, and binary unlexicalized features indicating whether f(x) and g(z) have a string match.\nmissing-predicate: Indicators on whether there are entities or relations mentioned in x but not in z. These features are unlexicalized.\ndenotation: Size and type of the denotation y = JxKw. The type can be either a primitive type (e.g., NUM, DATE, ENTITY) or the name of the column containing the entity in y (e.g., CITY).\nphrase-denotation: Conjunctions between n-\ngrams from x and the types of y. Similar to the phrase-predicate features, we use both lexicalized and unlexicalized features.\nheadword-denotation: Conjunctions between the question word Q (e.g., what, who, how many) or the headword H (the first noun after the question word) with the types of y."}, {"heading": "6.3 Generation and pruning", "text": "Due to their recursive nature, the rules allow us to generate highly compositional logical forms. However, the compositionality comes at the cost of generating exponentially many logical forms, most of which are redundant (e.g., logical forms with an argmax operation on a set of size 1). We employ several methods to deal with this combinatorial explosion:\nBeam search. We compute the model probability of each partial logical form based on available features (i.e., features that do not depend on the final denotation) and keep only the K = 200 highest-scoring logical forms in each cell.\nPruning. We prune partial logical forms that lead to invalid or redundant final logical forms. For example, we eliminate any logical form that does not type check (e.g., Beijing t Greece), executes to an empty list (e.g., Year.Number.24), includes an aggregate or superlative on a singleton set (e.g., argmax(Year.Number.2012, Index)), or joins two relations that are the reverses of each other (e.g., R[City].City.Beijing)."}, {"heading": "7 Experiments", "text": ""}, {"heading": "7.1 Main evaluation", "text": "We evaluate the system on the development sets (three random 80:20 splits of the training data) and the test data. In both settings, the tables we test on do not appear during training.\nEvaluation metrics. Our main metric is accuracy, which is the number of examples (x, t, y) on which the system outputs the correct answer y. We also report the oracle score, which counts the number of examples where at least one generated candidate z \u2208 Zx executes to y.\nBaselines. We compare the system to two baselines. The first baseline (IR), which simulates information retrieval, selects an answer y among the entities in the table using a log-linear model over entities (table cells) rather than logical forms. The features are conjunctions between phrases in x and\nproperties of the answers y, which cover all features in our main system that do not involve the logical form. As an upper bound of this baseline, 69.1% of the development examples have the answer appearing as an entity in the table.\nIn the second baseline (WQ), we only allow deduction rules that produce join and count logical forms. This rule subset has the same logical coverage as Berant and Liang (2014), which is designed to handle the WEBQUESTIONS (Berant et al., 2013) and FREE917 (Cai and Yates, 2013) datasets.\nResults. Table 5 shows the results compared to the baselines. Our system gets an accuracy of 37.1% on the test data, which is significantly higher than both baselines, while the oracle is 76.6%. The next subsections analyze the system components in more detail."}, {"heading": "7.2 Dataset statistics", "text": "In this section, we analyze the breadth and depth of the WIKITABLEQUESTIONS dataset, and how the system handles them.\nNumber of relations. With 3,929 unique column headers (relations) among 13,396 columns,\nthe tables in the WIKITABLEQUESTIONS dataset contain many more relations than closed-domain datasets such as Geoquery (Zelle and Mooney, 1996) and ATIS (Price, 1990). Additionally, the logical forms that execute to the correct denotations refer to a total of 2,056 unique column headers, which is greater than the number of relations in the FREE917 dataset (635 Freebase relations).\nKnowledge coverage. We sampled 50 examples from the dataset and tried to answer them manually using Freebase. Even though Freebase contains some information extracted from Wikipedia, we can answer only 20% of the questions, indicating that WIKITABLEQUESTIONS contains a broad set of facts beyond Freebase.\nLogical operation coverage. The dataset covers a wide range of question types and logical operations. Table 6(a) shows the drop in oracle scores when different subsets of rules are used to generate candidates logical forms. The join only subset corresponds to simple table lookup, while join + count is the WQ baseline for Freebase question answering on the WEBQUESTIONS dataset. Finally, join + count + superlative roughly corresponds to the coverage of the Geoquery dataset.\nTo better understand the distribution of logical operations in the WIKITABLEQUESTIONS dataset, we manually classified 200 examples based on the types of operations required to answer the question. The statistics in Table 7 shows that while a few questions only require simple operations such as table lookup, the majority of the questions demands more advanced operations. Additionally, 21% of the examples cannot be answered using any logical form generated from the current deduction rules; these examples are discussed in Section 7.4.\nCompositionality. From each example, we compute the logical form size (number of rules applied) of the highest-scoring candidate that executes to the correct denotation. The histogram in Figure 5 shows that a significant number of logical\nforms are non-trivial.\nBeam size and pruning. Figure 6 shows the results with and without pruning on various beam sizes. Apart from saving time, pruning also prevents bad logical forms from clogging up the beam which hurts both oracle and accuracy metrics."}, {"heading": "7.3 Features", "text": "Effect of features. Table 6(b) shows the accuracy when some feature types are ablated. The most influential features are lexicalized phrasepredicate features, which capture the relationship between phrases and logical operations (e.g., relating \u201clast\u201d to argmax) as well as between phrases and relations (e.g., relating \u201cbefore\u201d to < or Next, and relating \u201cwho\u201d to the relation Name).\nAnchoring with trigger words. In our parsing algorithm, relations and logical operations are not anchored to the utterance. We consider an alternative approach where logical operations are anchored to \u201ctrigger\u201d phrases, which are hand-coded based on co-occurrence statistics (e.g., we trigger a count logical form with how, many, and total).\nTable 6(c) shows that the trigger words do not significantly impact the accuracy, suggesting that the original system is already able to learn the relationship between phrases and operations even without a manual lexicon. As an aside, the huge drop in oracle is because fewer \u201csemantically incorrect\u201d logical forms are generated; we discuss this phenomenon in the next subsection."}, {"heading": "7.4 Semantically correct logical forms", "text": "In our setting, we face a new challenge that arises from learning with denotations: with deeper compositionality, a larger number of nonsensical logical forms can execute to the correct denotation. For example, if the target answer is a small number (say, 2), it is possible to count the number of rows with some random properties and arrive at the correct answer. However, as the system encounters more examples, it can potentially learn to disfavor them by recognizing the characteristics of semantically correct logical forms.\nGenerating semantically correct logical forms. The system can learn the features of semantically correct logical forms only if it can generate them in the first place. To see how well the system can generate correct logical forms, looking at the oracle score is insufficient since bad logical forms can execute to the correct denotations. Instead, we randomly chose 200 examples and manually annotated them with logical forms to see if a trained system can produce the annotated logical form as a candidate.\nOut of 200 examples, we find that 79% can be manually annotated. The remaining ones include artifacts such as unhandled question types (e.g., yes-no questions, or questions with phrases \u201csame\u201d or \u201cconsecutive\u201d), table cells that require advanced normalization methods (e.g., cells with comma-separated lists), and incorrect annotations.\nThe system generates the annotated logical form among the candidates in 53.5% of the examples. The missing examples are mostly caused by anchoring errors due to lexical mismatch (e.g., \u201cItalian\u201d\u2192 Italy, or \u201cno zip code\u201d\u2192 an empty cell in the zip code column) or the need to generate complex logical forms from a single phrase (e.g., \u201cMay 2010\u201d\u2192 >=.2010-05-01u<=.2010-05-31)."}, {"heading": "7.5 Error analysis", "text": "The errors on the development data can be divided into four groups. The first two groups are unhandled question types (21%) and the failure to anchor entities (25%) as described in Section 7.4. The third group is normalization and type errors (29%): although we handle some forms of entity normalization, we observe many unhandled string formats such as times (e.g., 3:45.79) and city-country pairs (e.g., Beijing, China), as well as complex calculation such as computing time periods (e.g., 12pm\u20131am\u2192 1 hour). Finally, we have\nranking errors (25%) which mostly occur when the utterance phrase and the relation are obliquely related (e.g., \u201cairplane\u201d and Model)."}, {"heading": "8 Discussion", "text": "Our work simultaneously increases the breadth of knowledge source and the depth of compositionality in semantic parsing. This section explores the connections in both aspects to related work.\nLogical coverage. Different semantic parsing systems are designed to handle different sets of logical operations and degrees of compositionality. For example, form-filling systems (Wang et al., 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations. This work aims to increase the logical coverage even further. For example, compared to the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., \u201cWhat states border states that border states that border Florida?\u201d), our dataset contains fairly compositional questions on average.\nTo parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations. A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance.\nKnowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for\nimproving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014).\nOur work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly.\nIn future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). Pasupat and Liang (2014) used a framework similar to ours to extract entities from web pages, where the \u201clogical forms\u201d were XPath expressions. A natural direction is to combine the logical compositionality of this work with the even broader knowledge source of general web pages.\nAcknowledgements. We gratefully acknowledge the support of the Google Natural Language Understanding Focused Program and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040.\nData and reproducibility. The WIKITABLEQUESTIONS dataset can be downloaded at http: //nlp.stanford.edu/software/sempre/wikitable/. Additionally, code, data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0xf26cd79d4d734287868923ad1067cf4c/."}], "references": [{"title": "Natural language interfaces to databases \u2013 an introduction", "author": ["G.D. Ritchie", "P. Thanisch"], "venue": "Journal of Natural Language Engineering,", "citeRegEx": "Androutsopoulos et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Androutsopoulos et al\\.", "year": 1995}, {"title": "Semantic parsing via paraphrasing. In Association for Computational Linguistics (ACL)", "author": ["Berant", "Liang2014] J. Berant", "P. Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Berant et al.2013] J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "WebTables: exploring the power of tables on the web", "author": ["A. Halevy", "D.Z. Wang", "E. Wu", "Y. Zhang"], "venue": "In Very Large Data Bases (VLDB),", "citeRegEx": "Cafarella et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cafarella et al\\.", "year": 2008}, {"title": "Largescale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL)", "author": ["Cai", "Yates2013] Q. Cai", "A. Yates"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2010] J. Duchi", "E. Hazan", "Y. Singer"], "venue": "In Conference on Learning Theory (COLT)", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Paraphrase-driven learning for open question answering. In Association for Computational Linguistics (ACL)", "author": ["Fader et al.2013] A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": null, "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader et al.2014] A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Answering table augmentation queries from unstructured lists on the web", "author": ["Gupta", "Sarawagi2009] R. Gupta", "S. Sarawagi"], "venue": "In Very Large Data Bases (VLDB),", "citeRegEx": "Gupta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2009}, {"title": "Knowledge base population: Successful approaches and challenges", "author": ["Ji", "Grishman2011] H. Ji", "R. Grishman"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Ji et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2011}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics (TACL), 1:193\u2013206", "author": ["Krishnamurthy", "Kollar2013] J. Krishnamurthy", "T. Kollar"], "venue": null, "citeRegEx": "Krishnamurthy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2013}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Lambda dependencybased compositional semantics", "author": ["P. Liang"], "venue": "Technical report,", "citeRegEx": "Liang.,? \\Q2013\\E", "shortCiteRegEx": "Liang.", "year": 2013}, {"title": "Annotating and searching web tables using entities, types and relationships", "author": ["Limaye et al.2010] G. Limaye", "S. Sarawagi", "S. Chakrabarti"], "venue": "In Very Large Data Bases (VLDB),", "citeRegEx": "Limaye et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Limaye et al\\.", "year": 2010}, {"title": "Open language learning for information extraction", "author": ["Masaum et al.2012] Masaum", "M. Schmitz", "R. Bart", "S. Soderland", "O. Etzioni"], "venue": "In Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Masaum et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Masaum et al\\.", "year": 2012}, {"title": "Zero-shot entity extraction from web pages. In Association for Computational Linguistics (ACL)", "author": ["Pasupat", "Liang2014] P. Pasupat", "P. Liang"], "venue": null, "citeRegEx": "Pasupat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasupat et al\\.", "year": 2014}, {"title": "Answering table queries on the web using column keywords", "author": ["Pimplikar", "Sarawagi2012] R. Pimplikar", "S. Sarawagi"], "venue": "In Very Large Data Bases (VLDB),", "citeRegEx": "Pimplikar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pimplikar et al\\.", "year": 2012}, {"title": "Towards a theory of natural language interfaces to databases", "author": ["Popescu et al.2003] A. Popescu", "O. Etzioni", "H. Kautz"], "venue": "In International Conference on Intelligent User Interfaces (IUI),", "citeRegEx": "Popescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2003}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P. Price"], "venue": "In Proceedings of the Third DARPA Speech and Natural Language Workshop,", "citeRegEx": "Price.,? \\Q1990\\E", "shortCiteRegEx": "Price.", "year": 1990}, {"title": "Large-scale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics (TACL), 2(10):377\u2013392", "author": ["S. Reddy", "M. Lapata", "M. Steedman"], "venue": null, "citeRegEx": "Reddy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Exploiting a web of semantic data for interpreting tables", "author": ["Syed et al.2010] Z. Syed", "T. Finin", "V. Mulwad", "A. Joshi"], "venue": "In Proceedings of the Second Web Science Conference", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}, {"title": "Pythia: compositional meaning construction for ontology-based question answering on the semantic web", "author": ["Unger", "Cimiano2011] C. Unger", "P. Cimiano"], "venue": "In Proceedings of the 16th international conference on Natural language processing", "citeRegEx": "Unger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Unger et al\\.", "year": 2011}, {"title": "Template-based question answering over RDF data", "author": ["Unger et al.2012] C. Unger", "L. B\u00fchmann", "J. Lehmann", "A. Ngonga", "D. Gerber", "P. Cimiano"], "venue": "In World Wide Web (WWW),", "citeRegEx": "Unger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Unger et al\\.", "year": 2012}, {"title": "Recovering semantics of tables on the web", "author": ["Venetis et al.2011] P. Venetis", "A. Halevy", "J. Madhavan", "M. Pa\u015fca", "W. Shen", "F. Wu", "G. Miao", "C. Wu"], "venue": "In Very Large Data Bases (VLDB),", "citeRegEx": "Venetis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Venetis et al\\.", "year": 2011}, {"title": "Semantic frame-based spoken language understanding. Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, pages 41\u201391", "author": ["Y. Wang", "L. Deng", "A. Acero"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Wong", "Mooney2007] Y.W. Wong", "R.J. Mooney"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Wong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2007}, {"title": "Scalable attribute-value extraction from semi-structured text", "author": ["Y.W. Wong", "D. Widdows", "T. Lokovic", "K. Nigam"], "venue": "In IEEE International Conference on Data Mining Workshops,", "citeRegEx": "Wong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2009}, {"title": "Open information extraction using Wikipedia", "author": ["Wu", "Weld2010] F. Wu", "D.S. Weld"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["Zelle", "Mooney1996] M. Zelle", "R.J. Mooney"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Zelle et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Zelle et al\\.", "year": 1996}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["Zettlemoyer", "Collins2007] L.S. Zettlemoyer", "M. Collins"], "venue": "In Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2007}, {"title": "Automatic extraction of top-k lists from the web", "author": ["Z. Zhang", "K.Q. Zhu", "H. Wang", "H. Li"], "venue": "In International Conference on Data Engineering", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011).", "startOffset": 158, "endOffset": 262}, {"referenceID": 2, "context": "More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014).", "startOffset": 112, "endOffset": 194}, {"referenceID": 7, "context": "More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014).", "startOffset": 112, "endOffset": 194}, {"referenceID": 20, "context": "More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014).", "startOffset": 112, "endOffset": 194}, {"referenceID": 5, "context": "We optimize \u03b8 using AdaGrad (Duchi et al., 2010), running 3 passes over the data.", "startOffset": 28, "endOffset": 48}, {"referenceID": 13, "context": "As our language for logical forms, we use lambda dependency-based compositional semantics (Liang, 2013), or lambda DCS, which we briefly describe here.", "startOffset": 90, "endOffset": 103}, {"referenceID": 2, "context": "This rule subset has the same logical coverage as Berant and Liang (2014), which is designed to handle the WEBQUESTIONS (Berant et al., 2013) and FREE917 (Cai and Yates, 2013) datasets.", "startOffset": 120, "endOffset": 141}, {"referenceID": 11, "context": "This rule subset has the same logical coverage as Berant and Liang (2014), which is designed to handle the WEBQUESTIONS (Berant et al.", "startOffset": 61, "endOffset": 74}, {"referenceID": 19, "context": "the tables in the WIKITABLEQUESTIONS dataset contain many more relations than closed-domain datasets such as Geoquery (Zelle and Mooney, 1996) and ATIS (Price, 1990).", "startOffset": 152, "endOffset": 165}, {"referenceID": 25, "context": "For example, form-filling systems (Wang et al., 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": ", 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations.", "startOffset": 253, "endOffset": 305}, {"referenceID": 18, "context": ", 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations.", "startOffset": 253, "endOffset": 305}, {"referenceID": 23, "context": "A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al.", "startOffset": 41, "endOffset": 86}, {"referenceID": 11, "context": ", 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al.", "startOffset": 27, "endOffset": 84}, {"referenceID": 2, "context": ", 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data.", "startOffset": 44, "endOffset": 86}, {"referenceID": 1, "context": ", 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance.", "startOffset": 66, "endOffset": 256}, {"referenceID": 2, "context": "In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014).", "startOffset": 100, "endOffset": 162}, {"referenceID": 7, "context": "In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014).", "startOffset": 100, "endOffset": 162}, {"referenceID": 12, "context": "The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al.", "startOffset": 161, "endOffset": 187}, {"referenceID": 6, "context": ", 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 20, "context": ", 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014).", "startOffset": 57, "endOffset": 109}, {"referenceID": 3, "context": "There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al.", "startOffset": 76, "endOffset": 162}, {"referenceID": 24, "context": "There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al.", "startOffset": 76, "endOffset": 162}, {"referenceID": 21, "context": "There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al.", "startOffset": 76, "endOffset": 162}, {"referenceID": 14, "context": "There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al.", "startOffset": 76, "endOffset": 162}, {"referenceID": 3, "context": ", 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012).", "startOffset": 74, "endOffset": 151}, {"referenceID": 15, "context": "In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data.", "startOffset": 41, "endOffset": 81}, {"referenceID": 27, "context": "In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al.", "startOffset": 109, "endOffset": 128}, {"referenceID": 31, "context": ", 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013).", "startOffset": 68, "endOffset": 88}, {"referenceID": 13, "context": "Pasupat and Liang (2014) used a framework similar to ours to extract entities from web pages, where the \u201clogical forms\u201d were XPath expressions.", "startOffset": 12, "endOffset": 25}], "year": 2015, "abstractText": "Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.", "creator": "LaTeX with hyperref package"}}}