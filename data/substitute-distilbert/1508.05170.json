{"id": "1508.05170", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2015", "title": "Adaptive Online Learning", "abstract": "chapters propose a broader constraint for studying adaptive regret bounds in the online learning model, including model selection bounds and data - dependent bounds. given weak data - or model - dependent bound we ask, \" does there possess some algorithm achieving this bound? \" we show unless modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates could be achieved. in particular each adaptive rate induces a set around so - called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability. probable result of our analysis analysis is the use having one - sided tail inequalities to bound suprema of offset complex processes.", "histories": [["v1", "Fri, 21 Aug 2015 03:44:43 GMT  (32kb,D)", "http://arxiv.org/abs/1508.05170v1", "27 pages"]], "COMMENTS": "27 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["dylan j foster", "alexander rakhlin", "karthik sridharan"], "accepted": true, "id": "1508.05170"}, "pdf": {"name": "1508.05170.pdf", "metadata": {"source": "CRF", "title": "Adaptive Online Learning", "authors": ["Dylan J. Foster", "Alexander Rakhlin", "Karthik Sridharan"], "emails": [], "sections": [{"heading": null, "text": "Our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second-order data-dependent bounds, and small loss bounds. In addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online PAC-Bayes theorem that holds for countably infinite sets."}, {"heading": "1 Introduction", "text": "Some of the recent progress on the theoretical foundations of online learning has been motivated by the parallel developments in the realm of statistical learning. In particular, this motivation has led to martingale extensions of empirical process theory, which were shown to be the \u201cright\u201d notions for online learnability. Two topics, however, have remained elusive thus far: obtaining data-dependent bounds and establishing model selection (or, oracle-type) inequalities for online learning problems. In this paper we develop new techniques for addressing both these topics.\nOracle inequalities and model selection have been topics of intense research in statistics in the last two decades [1, 2, 3]. Given a sequence of modelsM1,M2, . . . whose union isM, one aims to derive a procedure that selects, given an i.i.d. sample of size n, an estimator f\u0302 from a model Mm\u0302 that trades off bias and variance. Roughly speaking the desired oracle bound takes the form\nerr(f\u0302) \u2264 inf m { inf f\u2208Mm err(f) + penn(m)} ,\nwhere penn(m) is a penalty for the model m. Such oracle inequalities are attractive because they can be shown to hold even if the overall modelM is too large. A central idea in the proofs of such statements (and an idea that will appear throughout the present paper) is that penn(m) should be \u201cslightly larger\u201d than the fluctuations of the empirical process for the model m. It is therefore not surprising that concentration inequalities\u2014and particularly Talagrand\u2019s celebrated inequality for the supremum of the empirical process\u2014 have played an important role in attaining oracle bounds. In order to select a good model in a data-driven manner, one first establishes non-asymptotic data-dependent bounds on the fluctuations of an empirical process indexed by elements in each model (see the monograph [4]).\nLifting the ideas of oracle inequalities and data-dependent bounds from statistical to online learning is not an obvious task. For one, there is no concentration inequality available, even for the simple case of a\nar X\niv :1\n50 8.\n05 17\n0v 1\n[ cs\n.L G\n] 2\n1 A\nug 2\n01 5\nsequential Rademacher complexity. (For the reader already familiar with this complexity: a change of the value of one Rademacher variable results in a change of the remaining path, and hence an attempt to use a version of a bounded difference inequality grossly fails). Luckily, as we show in this paper, the concentration machinery is not needed and one only requires a one-sided tail inequality. This realization is motivated by the recent work of [5, 6, 7]. At the high level, our approach will be to develop one-sided inequalities for the suprema of certain offset processes [7], with an offset that is chosen to be \u201cslightly larger\u201d than the complexity of the corresponding model. We then show that these offset processes also determine which data-dependent adaptive rates are achievable for a given online learning problem, drawing strong connections to the ideas of statistical learning described earlier."}, {"heading": "1.1 Framework", "text": "Let X be the set of observations, D the space of decisions, and Y the set of outcomes. Let \u2206(S) denote the set of distributions on a set S. Let ` \u2236 D\u00d7Y \u2192 R be a loss function. The online learning framework is defined by the following process: For t = 1, . . . , n, Nature provides input instance xt \u2208 X ; Learner selects prediction distribution qt \u2208 \u2206(D); Nature provides label yt \u2208 Y, while the learner draws prediction y\u0302t \u223c qt and suffers loss `(y\u0302t, yt).\nTwo specific scenarios of interest are supervised learning (Y \u2286 R, D \u2286 R) and online linear (or convex) optimization (X = {0} is the singleton set, Y and D are unit balls in dual Banach spaces and `(y\u0302, y) = \u27e8y\u0302, y\u27e9).\nFor a class F \u2286 DX , we define the learner\u2019s cumulative regret to F as n\n\u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F\nn\n\u2211 t=1 `(f(xt), yt).\nA uniform regret bound Bn is achievable if there exists a randomized algorithm for selecting y\u0302t such that\nE[ n\n\u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F\nn\n\u2211 t=1 `(f(xt), yt)] \u2264 Bn \u2200x1\u2236n, y1\u2236n, (1)\nwhere a1\u2236n stands for {a1, . . . , an}. Achievable rates Bn depend on complexity of the function class F . For example, sequential Rademacher complexity of F is one of the tightest achievable uniform rates for a variety of loss functions [8, 7].\nAn adaptive regret bound has the form Bn(f ;x1\u2236n, y1\u2236n) and is said to be achievable if there exists a randomized algorithm for selecting y\u0302t such that\nE[ n\n\u2211 t=1 `(y\u0302t, yt) \u2212\nn\n\u2211 t=1 `(f(xt), yt)] \u2264 Bn(f ;x1\u2236n, y1\u2236n) \u2200x1\u2236n, y1\u2236n, \u2200f \u2208 F . (2)\nWe distinguish three types of adaptive bounds, according to whether Bn(f ;x1\u2236n, y1\u2236n) depends only on f , only on (x1\u2236n, y1\u2236n), or on both quantities. Whenever Bn depends on f , an adaptive regret can be viewed as an oracle inequality which penalizes each f according to a measure of its complexity (e.g. the complexity of the smallest model to which it belongs). As in statistical learning, an oracle inequality (2) may be proved for certain functions Bn(f ;x1\u2236n, y1\u2236n) even if a uniform bound (1) cannot hold for any nontrivial Bn."}, {"heading": "1.2 Related Work", "text": "The case when Bn(f ;x1\u2236n, y1\u2236n) = Bn(x1\u2236n, y1\u2236n) does not depend on f has received most of the attention in the literature. The focus is on bounds that can be tighter for \u201cnice sequences,\u201d yet maintain near-optimal worst-case guarantees. An incomplete list of prior work includes [9, 10, 11, 12], couched in the setting of online linear/convex optimization, and [13] in the experts setting.\nThe present paper was partly motivated by the work of [14] who presented an algorithm that competes with all experts simultaneously, but with varied regret with respect to each of them, depending on the quantile of the expert. This is a bound of the type Bn(f) (dependent only on f , where f denotes the quantile we\ncompete against) for the finite experts setting. The work of [15] considers online linear optimization with an unbounded set and provides oracle inequalities with an appropriately chosen function Bn(f).\nFinally, the third category of adaptive bounds are those that depend on both the hypothesis f \u2208 F and the data. The bounds that depend on the loss of the best function (so-called \u201csmall-loss\u201d bounds, [16, Sec. 2.4], [17, 13]) fall in this category trivially, since one may overbound the loss of the best function by the performance of f . We would like to draw attention to the recent result of [18] who show an adaptive bound in terms of both the loss of comparator and the KL divergence between the comparator and some pre-fixed prior distribution over experts. An MDL-style bound in terms of the variance of the loss of the comparator (under the distribution induced by the algorithm) was recently given in [19].\nOur study was also partly inspired by Cover [20] who characterized necessary and sufficient conditions for achievable bounds in prediction of binary sequences. The methods in [20], however, rely on the structure of the binary prediction problem and do not readily generalize to other settings.\nThe framework we propose recovers the vast majority of known adaptive rates in literature, including variance bounds, quantile bounds, localization-based bounds, and fast rates for small losses. It should be noted that while existing literature on adaptive online learning has focused on simple hypothesis classes such as finite experts and finite-dimensional p-norm balls, our results extend to general hypothesis classes, including large nonparametric ones discussed in [7]."}, {"heading": "2 Adaptive Rates and Achievability: General Setup", "text": "The first step in building a general theory for adaptive online learning is to identify what adaptive regret bounds are possible to achieve. Recall that an adaptive regret bound of Bn \u2236 F \u00d7 Xn \u00d7 Yn \u2192 R is said to be achievable if there exists an online learning algorithm that produces predictions/decisions such that (2) holds.\nIn the rest of this work, we use the notation \u27ea. . .\u27ebnt=1 to denote the interleaved application of the operators inside the brackets, repeated over t = 1, . . . , n rounds (see [21]). Achievability of an adaptive rate can be formalized by the following minimax quantity.\nDefinition 1. Given an adaptive rate Bn we define the offset minimax value:\nAn(F ,Bn) \u225c \u27ea sup xt\u2208X inf qt\u2208\u2206(D) sup yt\u2208Y E y\u0302t\u223cqt\n\u27eb n t=1 [ n \u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}].\nAn(F ,Bn) quantifies how \u2211nt=1 `(y\u0302t, yt) \u2212 inff\u2208F {\u2211nt=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)} behaves when the optimal learning algorithm that minimizes this difference is used against Nature trying to maximize it. Directly from this definition,\nAn adaptive rate Bn is achievable if and only if An(F ,Bn) \u2264 0.\nIf Bn is a uniform rate, i.e., Bn(f ;x1\u2236n, y1\u2236n) = Bn, achievability reduces to the minimax analysis explored in [8]. The uniform rate Bn is achievable if and only if Bn \u2265 Vn(F), where Vn(F) is the minimax value of the online learning game.\nWe now focus on understanding the minimax value An(F ,Bn) for general adaptive rates. We first show that the minimax value is bounded by an offset version of the sequential Rademacher complexity studied in [8]. The symmetrization Lemma 1 below provides us with the first step towards a probabilistic analysis of achievable rates. Before stating the lemma, we need to define the notion of a tree and the notion of sequential Rademacher complexity.\nGiven a set Z, a Z-valued tree z of depth n is a sequence (zt)nt=1 of functions zt \u2236 {\u00b11}t\u22121 \u2192 Z. One may view z as a complete binary tree decorated by elements of Z. Let = ( t)nt=1 be a sequence of independent Rademacher random variables. Then (zt( )) may be viewed as a predictable process with respect to the filtration St = \u03c3( 1, . . . , t). For a tree z, the sequential Rademacher complexity of a function class G \u2286 RZ\non z is defined as\nRn(G,z) \u225c E sup g\u2208G\nn\n\u2211 t=1 tg(zt( )),\nand we denote Rn(G) \u225c supzRn(G,z). Let z1\u2236n( ) \u225c (z1( ), . . . ,zn( )) be the labels of the tree z along the path given by .\nLemma 1. For any lower semi-continuous loss `, and any adaptive rate Bn that only depends on outcomes (i.e. Bn(f ;x1\u2236n, y1\u2236n) = Bn(y1\u2236n)), we have that\nAn \u2264 sup x,y E [sup f\u2208F\n{2 n\n\u2211 t=1 t`(f(xt( )),yt( ))} \u2212 Bn(y1\u2236n( ))] . (3)\nFurther, for any general adaptive rate Bn,\nAn \u2264 sup x,y,y\u2032 E [sup f\u2208F\n{2 n\n\u2211 t=1 t`(f(xt( )),yt( )) \u2212 Bn(f ;x1\u2236n( ),y\u20322\u2236n+1( ))}] . (4)\nFinally, if one considers the supervised learning problem where F \u2236 X \u2192 R, Y \u2282 R and ` \u2236 R\u00d7R\u2192 R is a loss that is convex and L-Lipschitz in its first argument, then for any adaptive rate Bn,\nAn \u2264 sup x,y E [sup f\u2208F\n{2L n\n\u2211 t=1 tf(xt( )) \u2212 Bn(f ;x1\u2236n( ),y1\u2236n( ))}] . (5)\nThe above lemma tells us that to check whether an adaptive rate is achievable, it is sufficient to check that the corresponding adaptive sequential complexity measures are non-positive. We remark that if the above complexities are bounded by some positive quantity of a smaller order, one can form a new achievable rate B\u2032n by adding the positive quantity to Bn."}, {"heading": "3 Probabilistic Tools", "text": "As mentioned in the introduction, our technique rests on certain one-sided probabilistic inequalities. We now state the first building block: a rather straightforward maximal inequality.\nProposition 2. Let I = {1, . . . ,N}, N \u2264 \u221e, be a set of indices and let (Xi)i\u2208I be a sequence of random variables satisfying the following tail condition: for any \u03c4 > 0,\nP (Xi \u2212Bi > \u03c4) \u2264 C1 exp (\u2212\u03c42/(2\u03c32i )) +C2 exp (\u2212\u03c4si) (6)\nfor some positive sequence (Bi), nonnegative sequence (\u03c3i) and nonnegative sequence (si) of numbers, and for constants C1,C2 \u2265 0. Then for any \u03c3\u0304 \u2264 \u03c31, s\u0304 \u2265 s1, and\n\u03b8i = max{ \u03c3i Bi\n\u221a 2 log(\u03c3i/\u03c3\u0304) + 4 log(i), (Bisi)\u22121 log (i2(s\u0304/si))} + 1,\nit holds that\nE sup i\u2208I\n{Xi \u2212Bi\u03b8i} \u2264 3C1\u03c3\u0304 + 2C2(s\u0304)\u22121. (7)\nWe remark that Bi need not be the expected value of Xi, as we are not interested in two-sided deviations around the mean.\nOne of the approaches to obtaining oracle-type inequalities is to split a large class into smaller ones according to a \u201ccomplexity radius\u201d and control a certain stochastic process separately on each subset (also known as the peeling technique). In the applications below, Xi will often stand for the (random) supremum of this process, and Bi will be an upper bound on its typical size. Given deviation bounds for Xi above\nBi, the dilated size Bi\u03b8i then allows one to pass to maximal inequalities (7) and thus verify achievability in Lemma 1. The same strategy works for obtaining data-dependent bounds, where we first prove tail bounds for the given size of the data-dependent quantity, and then appeal to (7).\nA simple yet powerful example for the control of the supremum of a stochastic process is an inequality due to Pinelis [22] for the norm (which can be written as a supremum over the dual ball) of a martingale in a 2-smooth Banach space. Here we state a version of this result that can be found in [23, Appendix A].\nLemma 3. Let Z be a unit ball in a separable (2,D)-smooth Banach space H. Then for any Z-valued tree z,\nP (\u2225 n\n\u2211 t=1 tzt( )\u2225 \u2265 \u03c4) \u2264 2 exp(\u2212\n\u03c42\n8D2n )\nwhenever n > \u03c4/4D2.\nWhen the class of functions is not linear, we may no longer appeal to the above lemma. Instead, we make use of the following result from [24] that extends Lemma 3 at a price of a poly-logarithmic factor. Before stating the lemma, we briefly define the relevant complexity measures (see [24] for more details). First, a set V of R-valued trees is called an \u03b1-cover of G \u2286 RZ on z with respect to `p if\n\u2200g \u2208 G,\u2200 \u2208 {\u00b11}n,\u2203v \u2208 V s.t. n\n\u2211 t=1 (g(zt( )) \u2212 vt( ))p \u2264 n\u03b1p.\nThe size of the smallest \u03b1-cover is denoted by Np(G, \u03b1,z), and Np(G, \u03b1, n) \u225c supzNp(G, \u03b1,z). The set V is an \u03b1-cover of G on z with respect to `\u221e if\n\u2200g \u2208 G,\u2200 \u2208 {\u00b11},\u2203v \u2208 V s.t. \u2223g(zt( )) \u2212 vt( )\u2223 \u2264 \u03b1 \u2200t \u2208 [n].\nWe let N\u221e(G, \u03b1,z) be the smallest such cover and set N\u221e(G, \u03b1, n) = supzN\u221e(G, \u03b1,z).\nLemma 4 ([24]). Let G \u2286 [\u22121,1]Z . Suppose Rn(G)/n \u2192 0 with n \u2192\u221e and that the following mild assumptions hold: Rn(G) \u2265 1/n, N\u221e(G,2\u22121, n) \u2265 4, and there exists a constant \u0393 such that \u0393 \u2265 \u2211\u221ej=1N\u221e(G,2\u2212j , n)\u22121. Then for any \u03b8 > \u221a 12/n, for any Z-valued tree z of depth n,\nP (sup g\u2208G\n\u2223 n\n\u2211 t=1 tg(zt( ))\u2223 > 8(1 + \u03b8\n\u221a 8n log3(en2)) \u22c5Rn(G))\n\u2264 P (sup g\u2208G\n\u2223 n\n\u2211 t=1 tg(zt( ))\u2223 > n inf \u03b1>0 {4\u03b1 + 6\u03b8\u222b\n1\n\u03b1\n\u221a logN\u221e(G, \u03b4, n)d\u03b4}) \u2264 2\u0393e\u2212 n\u03b82 4 .\nThe above lemma yields a one-sided control on the size of the supremum of the sequential Rademacher process, as required for our oracle-type inequalities.\nNext, we turn our attention to an offset Rademacher process, where the supremum is taken over a collection of negative-mean random variables. The behavior of this offset process was shown to govern the optimal rates of convergence for online nonparametric regression [7]. Such a one-sided control of the supremum will be necessary for some of the data-dependent upper bounds we develop.\nLemma 5. Let z be a Z-valued tree of depth n, and let G \u2286 RZ . For any \u03b3 \u2265 1/n and \u03b1 > 0,\nP (sup g\u2208G\nn\n\u2211 t=1\n( tg(zt( )) \u2212 2\u03b1g2(zt( ))) \u2212 logN2(G, \u03b3,z)\n\u03b1 \u2212 12\n\u221a 2\u222b \u03b3\n1/n\n\u221a n logN2(G, \u03b4,z)d\u03b4 \u2212 1 > \u03c4)\n\u2264 \u0393 exp(\u2212 \u03c4 2 2\u03c32 ) + exp(\u2212\u03b1\u03c4 2 ) ,\nwhere \u0393 \u2265 \u2211log2(2n\u03b3)j=1 N2(G,2\u2212j\u03b3,z)\u22122 and \u03c3 = 12 \u222b \u03b3 1 n\n\u221a n logN2(G, \u03b4,z)d\u03b4.\nWe observe that the probability of deviation has both subgaussian and subexponential components. Using the above result and Proposition 2 leads to useful bounds on the quantities in Lemma 1 for specific types of adaptive rates. Given a tree z, we obtain a bound on the expected size of the sequential Rademacher process when we subtract off the data-dependent `2-norm of the function on the tree z, adjusted by logarithmic terms.\nCorollary 6. Suppose G \u2286 [\u22121,1]Z , and let z be any Z-valued tree of depth n. Assume logN2(G, \u03b4, n) \u2264 \u03b4\u2212p for some p < 2. Then\nE \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup g\u2208G,\u03b3 \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 n \u2211 t=1 tg(zt( )) \u2212 4 \u00bf \u00c1\u00c1\u00c02(logn) logN2(G, \u03b3/2,z)( n \u2211 t=1 g2(zt( )) + 1) \u2212 24 \u221a 2 logn\u222b \u03b3 1/n \u221a n logN2(G, \u03b4,z)d\u03b4 \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6\nis at most 7 + 2 logn.\nThe next corollary yields slightly faster rates than Corollary 6 when \u2223G\u2223 <\u221e.\nCorollary 7. Suppose G \u2286 [\u22121,1]Z with \u2223G\u2223 = N , and let z be any Z-valued tree of depth n. Then\nE \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup g\u2208G \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 n \u2211 t=1 tg(zt( )) \u2212 2 log(logN n \u2211 t=1 g2(z( )) + e) \u00bf \u00c1\u00c1\u00c032(logN n \u2211 t=1 g2(z( )) + e) \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 \u2264 1."}, {"heading": "4 Achievable Bounds", "text": "In this section we use Lemma 1 along with the probabilistic tools from the previous section to obtain an array of achievable adaptive bounds for various online learning problems. We subdivide the section into one subsection for each category of adaptive bound described in Section 1.1."}, {"heading": "4.1 Adapting to Data", "text": "Here we consider adaptive rates of the form Bn(x1\u2236n, y1\u2236n) or Bn(y1\u2236n), uniform over f \u2208 F . We show the power of the developed tools on the following example.\nExample 4.1 (Online Linear Optimization in Rd). Consider the problem of online linear optimization where F = {f \u2208 Rd \u2236 \u2225f\u22252 \u2264 1}, Y = {y \u2236 \u2225y\u22252 \u2264 1}, X = {0}, and `(y\u0302, y) = \u27e8y\u0302, y\u27e9. The following adaptive rate is achievable:\nBn(y1\u2236n) = 16 \u221a d log(n) XXXXXXXXXXXX ( n \u2211 t=1 yty \u22ba t ) 1/2XXXXXXXXXXXX\u03c3 + 16 \u221a d log(n),\nwhere \u2225\u22c5\u2225\u03c3 is the spectral norm. Let us deduce this result from Corollary 6. First, observe that\nXXXXXXXXXXXX ( n \u2211 t=1 yty \u22ba t ) 1/2XXXXXXXXXXXX\u03c3 = sup f \u2236\u2225f\u22252\u22641 XXXXXXXXXXXX ( n \u2211 t=1 yty \u22ba t ) 1/2 f XXXXXXXXXXXX = sup f \u2236\u2225f\u22252\u22641 \u00bf \u00c1\u00c1\u00c0f\u22ba n \u2211 t=1 yty \u22ba t f = sup f\u2208F \u00bf \u00c1\u00c1\u00c0 n \u2211 t=1 `2(f, yt).\nThe linear function class F can be covered point-wise at any scale \u03b4 with (3/\u03b4)d balls and thus\nN (` \u25cbF ,1/(2n),z) \u2264 (6n)d\nfor any Y-valued tree z. We apply Corollary 6 with \u03b3 = 1/n (the integral vanishes) to conclude the claimed statement."}, {"heading": "4.2 Model Adaptation", "text": "In this subsection we focus on achievable rates for oracle inequalities and model selection, but without dependence on data. The form of the rate is therefore Bn(f). Assume we have a class F = \u22c3R\u22651F(R), with the property that F(R) \u2286 F(R\u2032) for any R \u2264 R\u2032. If we are told by an oracle that regret will be measured with respect to those hypotheses f \u2208 F with R(f) \u225c inf{R \u2236 f \u2208 F(R)} \u2264 R\u2217, then using the minimax algorithm one can guarantee a regret bound of at most the sequential Rademacher complexity Rn(F(R\u2217)). On the other hand, given the optimality of the sequential Rademacher complexity for online learning problems for commonly encountered losses, we can argue that for any f \u2208 F chosen in hindsight, one cannot expect a regret better than order Rn(F(R(f))). In this section we show that simultaneously for all f \u2208 F , one can attain an adaptive upper bound of O (Rn(F(R(f))) \u221a log (Rn(F(R(f)))) log3/2 n). That is, we may predict as if we knew the optimal radius, at the price of a logarithmic factor. This is the price of adaptation.\nCorollary 8. For any class of predictors F with F(1) non-empty, if one considers the supervised learning problem with 1-Lipschitz loss `, the following rate is achievable:\nBn(f) =K1Rn(F(2R(f))) log3/2 n \u239b \u239c \u239d 1 + \u00bf \u00c1\u00c1\u00c0log(Rn(F(2R(f)))Rn(F(1)) ) + log(log(2R(f))) \u239e \u239f \u23a0\n+K2\u0393Rn(F(1)) log3/2 n,\nfor absolute constants K1,K2, and \u0393 defined in Lemma 4.\nIn fact, this statement is true more generally with F(2R(f)) replaced by ` \u25cbF(2R(f)). It is tempting to attempt to prove the above statement with the exponential weights algorithm running as an aggregation procedure over the solutions for each R. In general, this approach will fail for two reasons. First, if function values grow with R, the exponential weights bound will scale linearly with this value. Second, an experts bound yields a n1/2 rate which spoils any faster rates one may obtain using offset Rademacher complexities.\nAs a special case of the above lemma, we obtain an online PAC-Bayesian theorem for infinite classes of experts. However, we postpone this example to the next sub-section where we get a data-dependent version of this result. Neither of these bounds appear to be available in the literature, to the best of our knowledge.\nWe now provide a bound for online linear optimization in 2-smooth Banach spaces that automatically adapts to the norm of the comparator. To prove it, we use the concentration bound from [22] (Lemma 3) within the proof of the above corollary to remove the extra logarithmic factors.\nExample 4.2 (Unconstrained Linear Optimization). Consider linear optimization with Y being the unit ball of some reflexive Banach space with norm \u2225\u22c5\u2225\u2217. Let F = D be the dual space and the loss `(y\u0302, y) = \u27e8y\u0302, y\u27e9 (where we are using \u27e8\u22c5, \u22c5\u27e9 to represent the linear functional in the first argument to the second argument). Define F(R) = {f \u2223 \u2225f\u2225 \u2264 R} where \u2225\u22c5\u2225 is the norm dual to \u2225\u22c5\u2225\u2217. If the unit ball of Y is (2,D)-smooth, then the following rate is achievable for all f with \u2225f\u2225 \u2265 1:\nB(f) =D \u221a n(8\u2225f\u2225(1 + \u221a log(2\u2225f\u2225) + log log(2\u2225f\u2225)) + 12).\nFor the case of a Hilbert space, the above bound was achieved by [15]."}, {"heading": "4.3 Adapting to Data and Model Simultaneously", "text": "We now study achievable bounds that perform online model selection in a data-adaptive way. Of specific interest is the example of online optimistic PAC-Bayesian bound which \u2014in contrast to earlier results\u2014does not have dependence on the number of experts, and so holds for countably infinite sets of experts. The bound simultaneously adapts to the loss of the mixture of experts. This example subsumes and improves upon the recent results from [18, 14] and provides an exact analogue to the PAC Bayesian theorem from statistical learning. Further, quantile experts bounds can be easily recovered from the result.\nExample 4.3 (Generalized Predictable Sequences (Supervised Learning)). Consider an online supervised learning problem with a convex 1-Lipschitz loss. Let (Mt)t\u22651 be any predictable sequence that the learner can compute at round t based on information provided so far, including xt (One can think of the predictable sequence Mt as a prior guess for the hypothesis we would compare with in hindsight). Then the following adaptive rate is achievable:\nBn(f ;x1\u2236n) = inf \u03b3 {K1\n\u00bf \u00c1\u00c1\u00c0logn \u22c5 logN2(F , \u03b3/2, n) \u22c5 ( n\n\u2211 t=1\n(f(xt) \u2212Mt)2 + 1)\n+K2 logn\u222b \u03b3\n1/n\n\u221a n logN2(F , \u03b4, n)d\u03b4 + 2 logn + 7},\nfor constants K1 = 4 \u221a 2,K2 = 24 \u221a\n2 from Corollary 6. The achievability is a direct consequence of Eq. (5) in Lemma 1, followed by Corollary 6 (one can include any predictable sequence in the Rademacher average part because \u2211tMt t is zero mean). Particularly, if we assume that the sequential covering of class F grows as logN2(F , , n) \u2264 \u2212p for some p < 2, we get that\nBn(f) = O\u0303 \u239b \u239c\u239c \u239d \u239b \u239d\n\u00bf \u00c1\u00c1\u00c0 n\n\u2211 t=1 (f(xt) \u2212Mt)2 + 1 \u239e \u23a0\n1\u2212 p2 ( \u221a n)p/2 \u239e \u239f\u239f \u23a0 .\nAs p gets closer to 0, we get full adaptivity and replace n by \u2211nt=1 (f(xt) \u2212Mt) 2 + 1. On the other hand, as p gets closer to 2 (i.e. more complex function classes), we do not adapt and get a uniform bound in terms of n. For p \u2208 (0,2), we attain a natural interpolation.\nExample 4.4 (Regret to Fixed Vs Regret to Best (Supervised Learning)). Consider an online supervised learning problem with a convex 1-Lipschitz loss and let \u2223F \u2223 = N . Let f\u22c6 \u2208 F be a fixed expert chosen in advance. The following bound is achievable:\nBn(f, x1\u2236n) = 4 log(logN n\n\u2211 t=1\n(f(xt) \u2212 f\u22c6(xt))2 + e) \u00bf \u00c1\u00c1\u00c032(logN n\n\u2211 t=1\n(f(xt) \u2212 f\u22c6(xt))2 + e) + 2.\nIn particular, against f\u22c6 we have Bn(f\u22c6, x1\u2236n) = O(1),\nand against an arbitrary expert we have\nBn(f, x1\u2236n) = O( \u221a n logN(logn + log logN)).\nThis bound follows directly from Eq. (5) in Lemma 1 followed by Corollary 7. This extends the study of [25] to supervised learning and a general class of experts F .\nExample 4.5 (Optimistic PAC-Bayes). Assume that we have a countable set of experts and that the loss for each expert on any round is non-negative and bounded by 1. The function class F is the set of all distributions over these experts, and X = {0}. This setting can be formulated as online linear optimization where the loss of mixture f over experts, given instance y, is \u27e8f, y\u27e9, the expected loss under the mixture. The following adaptive bound is achievable:\nBn(f ; y1\u2236n) = \u00bf \u00c1\u00c1\u00c050 (KL(f \u2223\u03c0) + log(n)) n\n\u2211 t=1\nEi\u223cf \u27e8ei, yt\u27e92 + 50 (KL(f \u2223\u03c0) + log(n)) + 10.\nThis adaptive bound is an online PAC-Bayesian bound. The rate adapts not only to the KL divergence of f with fixed prior \u03c0 but also replaces n with \u2211nt=1 Ei\u223cf \u27e8ei, yt\u27e9 2 . Note that we have \u2211nt=1 Ei\u223cf \u27e8ei, yt\u27e9 2 \u2264\n\u2211nt=1 \u27e8f, yt\u27e9, yielding the small-loss type bound described earlier. This is an improvement over the bound in [18] in that the bound is independent of number of experts, and thus holds even for countably infinite sets of experts. The KL term in our bound may be compared to the MDL-style term in the bound of [19]. If we have a large (but finite) number of experts and take the uniform distribution \u03c0, the above bound provides an improvement over both [14] and [18] for quantile bounds for experts. Specifically, if we want quantile bounds simultaneously for every quantile then for any given quantile we can use uniform distribution over the top 1/ experts and hence the KL term is replaced by log(1/ ). Evaluating the above bound with a distribution f that places all its weight on any one expert appears to address the open question posed by [13] of obtaining algorithm-independent oracle-type variance bounds for experts.\nThe proof of achievability of the above rate is shown in the appendix because it requires a slight variation on the symmetrization lemma specific to the problem."}, {"heading": "5 Relaxations for Adaptive Learning", "text": "To design algorithms for achievable rates, we extend the framework of online relaxations from [26]. A relaxation Reln \u2236 \u22c3nt=0X t \u00d7Yt \u2192 R is admissible for an adaptive rate Bn if Rel satisfies the initial condition\nReln(x1\u2236n, y1\u2236n) \u2265 \u2212 inf f\u2208F\n{ n\n\u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)} (8)\nand the recursive condition\nReln(x1\u2236t\u22121, y1\u2236t\u22121) \u2265 sup xt\u2208X inf qt\u2208\u2206(D) sup yt\u2208Y Ey\u0302\u223cqt[`(y\u0302t, yt) +Reln(x1\u2236t, y1\u2236t)]. (9)\nThe corresponding strategy q\u0302t = arg minqt\u2208\u2206(D) supyt\u2208Y Ey\u0302\u223cqt[`(y\u0302t, yt) +Reln(x1\u2236t, y1\u2236t)] enjoys the adaptive bound\nn\n\u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)} \u2264 Reln(\u22c5) \u2200x1\u2236n, y1\u2236n.\nIt follows immediately that the strategy achieves the rate Bn(f ;x1\u2236n, y1\u2236n)+Reln(\u22c5). Our goal is then to find relaxations for which the strategy is computationally tractable and Reln(\u22c5) \u2264 0 or at least has smaller order than Bn. Similar to [26], conditional versions of the offset minimax values An yield admissible relaxations, but solving these relaxations may not be computationally tractable.\nExample 5.1 (Online PAC-Bayes). Consider the experts setting described in Example 4.5 and an adaptive bound,\nBn(f) = 3 \u221a 2nmax{KL(f \u2223 \u03c0),1} + 4 \u221a n.\nLet Ri = 2i\u22121 for i \u2208 N and let qRt (y) denote the exponential weights distribution with learning rate \u221a R/n given losses y1\u2236t: qR(y1\u2236t) \u221d Ek\u223c\u03c0ek exp(\u2212 \u221a R n \u27e8\u2211ts=1 yt, ek\u27e9) (wherein ek is the kth standard basis vector). The following is an admissible relaxation achieving Bn:\nReln(y1\u2236t) = inf \u03bb>0 [ 1 \u03bb log(\u2211 i exp(\u2212\u03bb[ t \u2211 s=1 \u27e8qRi(y1\u2236s\u22121), ys\u27e9 + \u221a nRi])) + 2\u03bb(n \u2212 t)].\nTo achieve this strategy we maintain a distribution q\u22c6t with (q\u22c6t )i \u221d exp(\u2212 1\u221an[\u2211 t\u22121 s=1\u27e8qRi(y1\u2236s\u22121), ys\u27e9 \u2212 \u221a nRi]). We predict by drawing i according to q\u22c6t , then drawing an expert according to q Ri(y1\u2236t\u22121).\nThis algorithm can be interpreted as running a \u201clow-level\u201d instance of the exponential weights algorithm for each complexity radius Ri, then combining the predictions of these algorithms with a \u201chigh-level\u201d instance. The high-level distribution q\u22c6t differs slightly from the usual exponential weights distribution in that\nit incorporates a prior whose weight decreases as the complexity radius increases. The prior distribution prevents the strategy from incurring a penalty that depends on the range of values the complexity radii take on, which would happen if the standard exponential weights distribution were used.\nWhile in general the problem of obtaining an efficient adaptive relaxation might be hard, one can ask the question, \u201cIf and efficient relaxation RelRn is available for each F(R), can one obtain an adaptive model selection algorithm for all of F?\u201d. To this end for supervised learning problem with convex Lipschitz loss we delineate a meta approach which utilizes existing relaxations for each F(R) to obtain algorithm for general adaptation.\nLemma 9. Let qRt (y1, . . . , yt\u22121) be the randomized strategy corresponding to RelRn , obtained after observing outcomes y1, . . . , yt\u22121, and let \u03b8 \u2236 R \u2192 R be nonnegative. The following relaxation is admissible for the rate Bn(R) = RelRn (\u22c5)\u03b8(RelRn (\u22c5)):\nAdan(x1\u2236t, y1\u2236t) =\nsup x,y,y\u2032 E t+1\u2236n sup R\u22651\n[RelRn (x1\u2236t, y1\u2236t) \u2212RelRn (\u22c5)\u03b8(RelRn (\u22c5)) + 2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))[`(y\u0302s(R),ys( ))]].\nPlaying according to the strategy for Adan will guarantee a regret bound of the form Bn(R)+Adan(\u22c5), and Adan(\u22c5) can be bounded using proposition 2 when the form of \u03b8 is as in that proposition.\nWe remark that the above strategy is not necessarily obtained by running a high-level experts algorithm over the discretized values of R. It is an interesting question to determine the cases when such a strategy is optimal. More generally, whenever the adaptive rate Bn depends on data, it is not possible to obtain the rates we show non-constructively in this paper using some form of exponential weights algorithms using meta-experts as the required weighting over experts would be data dependent (and hence is not a prior over experts). Further, the bounds from exponential-weights-type algorithms are more akin to having subexponential tails in Proposition 2, but for many problems we might have sub-gaussian tails.\nObtaining computationally efficient methods from the proposed framework is an interesting research direction. Proposition 2 provides a useful non-constructive tool to establish achievable adaptive bounds, and a natural question to ask is if one can obtain a constructive counterpart for the proposition."}, {"heading": "A Appendix", "text": "Proof of Lemma 1. We first prove Eq. (3) and (4). We start from the definition of An(F). Our proof proceeds \u201cinside out\u201d by starting with the nth term and then working backwards by repeatedly applying the minimax theorem. To this end on similar lines as in [24, 7, 21], we start with the inner most term as,\nsup xn\u2208X inf qn\u2208\u2206(D) sup yn\u2208Y (Ey\u0302n\u223cqn [`(y\u0302n, yn) \u2212 inf f\u2208F\n{ n\n\u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}])\n= sup xn\u2208X inf qn\u2208\u2206(D) sup pn\u2208\u2206(Y) (Ey\u0302n\u223cqn yn\u223cpn\n[ n\n\u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}])\n= sup xn\u2208X sup pn\u2208\u2206(Y) inf qn\u2208\u2206(D) (Ey\u0302n\u223cqn yn\u223cpn\n[ n\n\u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}])\n= sup xn\u2208X sup pn\u2208\u2206(Y) inf y\u0302n\u2208D\n(Eyn\u223cpn [ n\n\u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}])\n= sup xn\u2208X sup pn\u2208\u2206(Y) (Eyn\u223cpn [sup f\u2208F { inf y\u0302n\u2208D\nEyn\u223cpn [ n\n\u2211 t=1 `(y\u0302t, yt)] \u2212\nn\n\u2211 t=1 `(f(xt), yt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]) .\nTo apply the minimax theorem in step 3 above, we note that the term in the round bracket is linear in qn and in pn (as it is an expectation). Hence under mild assumptions on the sets D and Y, the losses, and the adaptive rate Bn, one can apply a generalized version of the minimax theorem to swap suppn and infqn . Compactness of the sets and lower semi-continuity of the losses and Bn are sufficient, but see [24, 21] for milder conditions. Proceeding backward from n to 1 in a similar fashion we end up with\nAn(F) = \u27ea sup xt\u2208X inf qt\u2208\u2206(D) sup yt\u2208Y E y\u0302t\u223cqt\n\u27eb n t=1 [ n \u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}]\n= \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 inf y\u0302t\u2208D\nEyt\u223cpt [`(y\u0302t, yt)] \u2212 n\n\u2211 t=1 `(f(xt), yt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]\n\u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\nEy\u2032t\u223cpt [`(f(xt), y \u2032 t)] \u2212 `(f(xt), yt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]. (10)\nSee [21] for more details of the steps involved in obtaining the above equality. Form this point on we split the proof for Equations 3 and 4. To prove the bound in Equation 3, note that, Bn(f ;x1\u2236n, y1\u2236n) = Bn(y1\u2236n) and so, (this proof is similar in spirit to the one in [7])\nAn(F) \u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\nEy\u2032t\u223cpt [`(f(xt), y \u2032 t)] \u2212 `(f(xt), yt)} \u2212 Bn(y1\u2236n)]\n= \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\nEy\u2032t\u223cpt [`(f(xt), y \u2032 t)] \u2212 `(f(xt), yt)} \u2212\n1 2 Bn(y1\u2236n) \u2212 1 2 Bn(y1\u2236n)].\nUsing linearity of expectation repeatedly (since Bn is independent of f and xt\u2019s ),\nAn(F) \u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F { n \u2211 t=1 Ey\u2032t\u223cpt [`(f(xt), y \u2032 t)] \u2212 `(f(xt), yt)}\n\u2212 1 2 Bn(y1\u2236n) \u2212 1 2 Ey\u20321\u2236n\u223cp1\u2236n [Bn(y \u2032 1\u2236n)] \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nBy Jensen\u2019s inequality, we pull out the expectations w.r.t. y\u2032t\u2019s to further upper bound the above quantity by\n\u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 `(f(xt), y\u2032t) \u2212 `(f(xt), yt)} \u2212\n1 2 Bn(y1\u2236n) \u2212 1 2 Bn(y\u20321\u2236n)]\n= \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 t (`(f(xt), y\u2032t) \u2212 `(f(xt), yt))} \u2212\n1 2 Bn(y1\u2236n) \u2212 1 2 Bn(y\u20321\u2236n)]\n\u2264 \u27ea sup xt\u2208X sup yt,y\u2032t\u2208Y\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 t (`(f(xt), y\u2032t) \u2212 `(f(xt), yt))} \u2212\n1 2 Bn(y1\u2236n) \u2212 1 2 Bn(y\u20321\u2236n)]\n\u2264 \u27ea sup xt\u2208X sup yt\u2208Y\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 2 t`(f(xt), yt)} \u2212 Bn(y1\u2236n)]\n= sup x,y E [sup f\u2208F\n{2 n\n\u2211 t=1 t`(f(xt( )),yt( ))} \u2212 Bn(y1\u2236n( ))] .\nwhere the last but one step is by sub-additivity of supremum and linearity of expectation and last step is by skolemizing the supremum interleaved with average w.r.t. Rademacher random variables in the binary tree format.\nWe now move to proving Eq. (4). We start from Eq. (10):\nAn(F) \u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\nEy\u2032t\u223cpt [`(f(xt), y \u2032 t)] \u2212 `(f(xt), yt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}].\nUsing Jensen\u2019s inequality to pull out the expectations w.r.t. y\u2032t\u2019s, we get\n\u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 `(f(xt), y\u2032t) \u2212 `(f(xt), yt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]\n\u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt sup y\u2032\u2032t \u2208Y\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 `(f(xt), y\u2032t) \u2212 `(f(xt), yt) \u2212 Bn(f ;x1\u2236n, y\u2032\u20321\u2236n)}]\n= \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt E t sup y\u2032\u2032t \u2208Y\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 t (`(f(xt), y\u2032t) \u2212 `(f(xt), yt)) \u2212 Bn(f ;x1\u2236n, y\u2032\u20321\u2236n)}]\n\u2264 \u27ea sup xt\u2208X sup yt,y\u2032t\u2208Y E t sup y\u2032\u2032t \u2208Y\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 t (`(f(xt), y\u2032t) \u2212 `(f(xt), yt)) \u2212 Bn(f ;x1\u2236n, y\u2032\u20321\u2236n)}]\n\u2264 \u27ea sup xt\u2208X sup yt\u2208Y E t sup y\u2032\u2032t \u2208Y\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\n2 t`(f(xt), yt) \u2212 Bn(f ;x1\u2236n, y\u2032\u20321\u2236n)}]\n= sup x,y,y\u2032 E [sup f\u2208F\n{2 n\n\u2211 t=1 t`(f(xt( )),yt( )) \u2212 Bn(f ;x1\u2236n( ),y\u20322\u2236n+1( ))}] ,\nwhere in the last step we switch to tree notation, but keep in mind that each y\u2032\u2032t is picked after drawing t, and thus the tree y\u2032 appears with one index shifted.\nFinally, we proceed to prove inequality (5). Here, we employ the convexity assumption `(y\u0302t, yt) \u2212 `(f(xt), yt) \u2264 `\u2032(y\u0302t, yt)(y\u0302t \u2212 f(xt)), where the derivative is with respect to the first argument. As before,\napplying the minimax theorem,\nAn(F) = \u27ea sup xt\u2208X inf qt\u2208\u2206(D) sup yt\u2208Y E y\u0302t\u223cqt\n\u27eb n t=1 [ n \u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}]\n= \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) inf y\u0302t\u2208D E yt\u223cpt\n\u27eb n t=1 [ n \u2211 t=1 `(y\u0302t, yt) \u2212 inf f\u2208F { n \u2211 t=1 `(f(xt), yt) + Bn(f ;x1\u2236n, y1\u2236n)}]\n\u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) inf y\u0302t\u2208D E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 `\u2032(y\u0302t, yt)(y\u0302t \u2212 f(xt)) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}].\nWe may now pick y\u0302t = y\u0302\u2217t (pt) \u225c arg miny\u0302 Eyt\u223cpt [`(y\u0302t, yt)]. By convexity (and assuming the loss allows swapping of derivative and expectation), Eyt\u223cpt [`\u2032(y\u0302t, yt)] = 0. This (sub)optimal strategy yields an upper bound of\n\u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\n(`\u2032(y\u0302\u2217t , yt) \u2212Ey\u2032t\u223cpt [` \u2032(y\u0302\u2217t , y\u2032t)]) (y\u0302\u2217t \u2212 f(xt)) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}].\nSince (`\u2032(y\u0302\u2217t , yt) \u2212Ey\u2032t\u223cpt [` \u2032(y\u0302\u2217t , y\u2032t)]) y\u0302\u2217t is independent of f and has expected value of 0, the above quantity is equal to\n\u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\n(Ey\u2032t\u223cpt [` \u2032(y\u0302\u2217t , y\u2032t)] \u2212 `\u2032(y\u0302\u2217t , yt)) f(xt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]\n\u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt\n\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1\n(`\u2032(y\u0302\u2217t , y\u2032t) \u2212 `\u2032(y\u0302\u2217t , yt)) f(xt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]\n= \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 t (`\u2032(y\u0302\u2217t , y\u2032t) \u2212 `\u2032(y\u0302\u2217t , yt)) f(xt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}].\nReplacing (`\u2032(y\u0302\u2217t , y\u2032t) \u2212 `\u2032(y\u0302\u2217t , yt)) by 2Lst for st \u2208 [\u22121,1] and taking supremum over st we get,\n\u2264 \u27ea sup xt\u2208X sup pt\u2208\u2206(Y) E yt,y\u2032t\u223cpt sup st\u2208[\u22121,1]\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 2L tstf(xt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]\n\u2264 \u27ea sup xt\u2208X sup yt sup st\u2208[\u22121,1]\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 2L tstf(xt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}].\nSince the suprema over st are achieved at {\u00b11} by convexity, the last expression is equal to\n\u27ea sup xt\u2208X sup yt sup st\u2208{\u22121,1}\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 2L tstf(xt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]\n= \u27ea sup xt\u2208X sup yt\nE t\u27eb n\nt=1 [sup f\u2208F\n{ n\n\u2211 t=1 2L tf(xt) \u2212 Bn(f ;x1\u2236n, y1\u2236n)}]\n= sup x,y E [sup f\u2208F\n{ n\n\u2211 t=1 2L tf(xt( )) \u2212 Bn(f ;x1\u2236n( ),y1\u2236n( ))}] .\nIn the last but one step we removed st, since for any function \u03a8, and any s \u2208 {\u00b11}, E [\u03a8(s )] = 12 (\u03a8(s) +\u03a8(\u2212s)) = 1 2 (\u03a8(1) +\u03a8(\u22121)) = E [\u03a8( )].\nProof of Proposition 2. Define Zi = [Xi \u2212Bi\u03b8i]+. As long as \u03b8i \u2265 1, for any strictly positive \u03c4 we have the tail behavior\nP (Zi \u2265 t) = P (Xi \u2212Bi\u03b8i \u2265 \u03c4) \u2264 C1 exp(\u2212 (Bi(\u03b8i \u2212 1) + \u03c4)2\n2\u03c32i ) +C2 exp (\u2212(Bi(\u03b8i \u2212 1) + \u03c4)si) .\nNote that for any positive sequence (\u03b4i)i\u2208I with \u03b4 = \u2211i\u2208I \u03b4i,\nE [sup i\u2208I {Xi \u2212Bi\u03b8i}] \u2264 E [sup i\u2208I Zi] \u2264\u2211 i\u2208I E [Zi] \u2264 \u03b4 +\u2211 i\u2208I \u222b\n\u221e\n\u03b4i P (Zi \u2265 \u03c4)d\u03c4.\nThe sum of the integrals above is equal to\n\u2211 i\u2208I \u222b\n\u221e\n\u03b4i P (Xi \u2212Bi\u03b8i \u2265 \u03c4)d\u03c4\n\u2264 C1\u2211 i\u2208I \u222b\n\u221e\n0 exp(\u2212(Bi(\u03b8i \u2212 1) + \u03c4)\n2\n2\u03c32i )dt +C2\u2211 i\u2208I \u222b\n\u221e\n0 exp (\u2212 (Bi(\u03b8i \u2212 1) + \u03c4) si)d\u03c4\n\u2264 C1\u2211 i\u2208I exp(\u22121 2 (Bi \u03c3i\n) 2 (\u03b8i \u2212 1)2)\u222b \u221e 0 e \u2212 \u03c42 2\u03c32 i d\u03c4 +C2\u2211 i\u2208I exp (\u2212Bisi (\u03b8i \u2212 1))\u222b \u221e 0 e\u2212\u03c4sid\u03c4\n\u2264 \u221a \u03c0\n2 C1\u2211 i\u2208I \u03c3i exp(\u2212\n1 2 (Bi \u03c3i\n) 2\n(\u03b8i \u2212 1)2) +C2\u2211 i\u2208I s\u22121i exp (\u2212Bisi (\u03b8i \u2212 1))\n\u2264 \u03c0 2\u221a\u03c0 6 \u221a 2 C1\u03c3\u0304 + \u03c02 6 C2(s\u0304)\u22121,\nwhere the last step is obtained by plugging in\n\u03b8i = max{ \u03c3i Bi\n\u221a 2 log(\u03c3i/\u03c3\u0304) + 4 log(i), (Bisi)\u22121 log (i2(s\u0304/si))} + 1\nand using as an upper bound \u03c3i Bi\n\u221a 2 log(i2\u03c3i/\u03c3\u0304)+1 for \u03b8i in the sub-gaussian part and (Bisi)\u22121 log (i2s\u0304/si)+1\nfor \u03b8i in the sub-exponential part. Since \u03b4 can be chosen arbitrarily small, we may over-bound the above constant and obtain the result.\nProof of Lemma 5. Fix \u03b3 > 0. For j \u2265 0, let Vj be a minimal sequential cover of G on z at scale \u03b2j = 2\u2212j\u03b3 and with respect to empirical `2 norm. Let v\nj[g, ] be an element guaranteed to be \u03b2j-close to f at the j-th level, for the given . Choose N = log2(2\u03b3n), so that \u03b2Nn \u2264 1. Let us use the shorthand N2(\u03b3) \u225c N2(G, \u03b3,z).\nFor any \u2208 {\u00b11}n and g \u2208 G, n\n\u2211 t=1 tg(zt( )) \u2212 2\u03b1g(zt( ))2\ncan be written as n\n\u2211 t=1\n( t(g(zt( )) \u2212 v0t [g, ]( ))) + n\n\u2211 t=1 ( tv0t [g, ]( ) \u2212 2\u03b1g(zt( ))2)\n\u2264 n\n\u2211 t=1\n( t(g(zt( )) \u2212 v0t [g, ]( ))) + n\n\u2211 t=1 ( tv0t [g, ]( ) \u2212 \u03b1v0t [g, ]( )2)\n= n\n\u2211 t=1\n( t(g(zt( )) \u2212 vNt [g, ]( )) + n\n\u2211 t=1\nN\n\u2211 k=1\nt (vkt [g, ]( ) \u2212 vk\u22121t [g, ]( )) + n\n\u2211 t=1 ( tv0t [g, ]( ) \u2212 \u03b1v0t [g, ]( )2) .\nBy Cauchy-Schwartz, the first term is upper bounded by n\u03b2N \u2264 1. The second term above is upper bounded by\nN\n\u2211 k=1\nn\n\u2211 t=1 t (vkt [g, ]( ) \u2212 vk\u22121t [g, ]( )) \u2264\nN\n\u2211 k=1 sup wk\u2208Wk\nn\n\u2211 t=1 tw k t ( ),\nwhere Wk is a set of differences of trees for levels k and k \u2212 1 (see [24, Proof of Theorem 3]). Finally, the third term is controlled by\nn\n\u2211 t=1 ( tv0t [g, ]( ) \u2212 \u03b1v0t [g, ]( )2) \u2264 sup v\u2208V0\nn\n\u2211 t=1 ( tvt( ) \u2212 \u03b1v2t ( )) .\nThe probability in the statement of the Lemma can now be upper bounded by\nP \u239b \u239d N \u2211 k=1 sup wk\u2208Wk n \u2211 t=1 tw k t ( ) + sup v\u2208V0 n \u2211 t=1 ( tvt( ) \u2212 \u03b1v2t ( )) \u2212 logN2(\u03b3) \u03b1 \u2212 12 \u221a 2\u222b \u03b3 1/n \u221a n logN2(\u03b4)d\u03b4 > \u03c4 \u239e \u23a0 .\nIn view of \u221a\n72 N\n\u2211 k=1\n\u03b2k \u221a n logN2(\u03b2k) \u2264 12 \u221a 2\u222b \u03b3\n1/n\n\u221a n logN2(\u03b4)d\u03b4\nthis probability can be further upper bounded by\nP ( N\n\u2211 k=1 sup wk\u2208Wk\nn\n\u2211 t=1 tw k t ( ) + sup v\u2208V0\nn\n\u2211 t=1\n( tvt( ) \u2212 \u03b1v2t ( )) \u2212 logN2(\u03b3) \u03b1 \u2212 \u221a 72 N \u2211 k=1 \u03b2k \u221a n logN2(\u03b2k) > \u03c4) .\nDefine a distribution p on {1, . . . ,N} by\npk = \u03b2k\n\u221a n logN2(\u03b2k)\n\u2211Nk=1 \u03b2j \u221a n logN2(\u03b2j) .\nThen the above probability can be upper bounded by\nP (\u2203k \u2208 [N] s.t. sup wk\u2208Wk\nn\n\u2211 t=1 tw k t ( ) \u2212\n\u221a 72\u03b2k \u221a n logN2(\u03b2k) >\n\u03c4pk 2\n\u2228 sup v\u2208V0\nn\n\u2211 t=1\n( tvt( ) \u2212 \u03b1v2t ( )) \u2212 logN2(\u03b3) \u03b1 > \u03c4 2 )\n\u2264 N\n\u2211 k=1 P ( sup wk\u2208Wk\nn\n\u2211 t=1 tw k t ( ) \u2212\n\u221a 72\u03b2k \u221a n logN2(\u03b2k) >\n\u03c4pk 2 )\n+ P (sup v\u2208V0\nn\n\u2211 t=1\n( tvt( ) \u2212 \u03b1v2t ( )) \u2212 logN2(\u03b3) \u03b1 > \u03c4 2 ) .\nThe second term can be upper bounded using Chernoff method by\n\u2211 v\u2208V0\nP ( n\n\u2211 t=1\n( tvt( ) \u2212 \u03b1v2t ( )) \u2212 logN2(\u03b3) \u03b1 > \u03c4 2 ) \u2264 N2(\u03b3) exp(\u2212 \u03b1\u03c4 2 \u2212 logN2(\u03b3)) \u2264 exp(\u2212 \u03b1\u03c4 2 )\nwhile the first sum of probabilities can be upper bounded by\nN\n\u2211 k=1 \u2211 wk\u2208Wk\nP \u239b \u239d n \u2211 t=1 tw k t ( ) \u2212 \u221a 72\u03b2k \u221a n logN2(\u03b2k) >\n\u03c4\u03b2k \u221a n logN2(\u03b2k)\n2\u2211Nk=1 \u03b2k \u221a n logN2(\u03b2k)\n\u239e \u23a0 . (11)\nFor any k, the tail probability above is controlled by Hoeffding-Azuma inequality as\nP \u239b \u239c \u239d n \u2211 t=1 tw k t ( ) > \u03b2k \u221a n logN2(\u03b2k) \u239b \u239d 6 \u221a 2 + \u03c4 2\u2211Nk=1 \u03b2k \u221a n logN2(\u03b2k) \u239e \u23a0 2\u239e \u239f \u23a0\n\u2264 exp \u239b \u239c \u239d \u2212 1 18 logN2(\u03b2k) \u239b \u239d 6 \u221a 2 + \u03c4 2\u2211Nk=1 \u03b2k \u221a n logN2(\u03b2k) \u239e \u23a0 2\u239e \u239f \u23a0\n\u2264 exp (\u22124 logN2(\u03b2k)) exp \u239b \u239c\u239c \u239d \u2212 \u03c4 2 18 (2\u2211Nk=1 \u03b2k \u221a n logN2(\u03b2k)) 2 \u239e \u239f\u239f \u23a0 ,\nbecause 1 n \u2211 n t=1 w k t ( )2 \u2264 3\u03b22k for any by triangle inequality (see [24]). Then the double sum in (11) is upper bounded by\n\u0393 exp \u239b \u239c\u239c \u239d \u2212 \u03c4 2 18 (2\u2211Nk=1 \u03b2k \u221a n logN2(\u03b2k)) 2 \u239e \u239f\u239f \u23a0 ,\nwhere \u0393 \u2265 \u2211Nk=1N2(\u03b2k)\u22122. This upper bound can be further relaxed to\n\u0393 exp \u239b \u239c\u239c \u239d \u2212 \u03c4 2 2 (12 \u222b \u03b3 1/n \u221a n logN2(\u03b4)d\u03b4) 2 \u239e \u239f\u239f \u23a0 .\nSince N = log2(2\u03b3n), we may take\n\u0393 = log2(2\u03b3n) \u2211 k=1 N2(\u03b32\u2212k)\u22122.\nProof of Corollary 6. Let us write N2(\u03b3) \u225c N2(G, \u03b3,z). Observe that\n2 \u00bf \u00c1\u00c1\u00c02(logn) (logN2(\u03b3/2))( n\n\u2211 t=1 g2(zt( )) + 1) = inf \u03b1 {(logn) (logN2(\u03b3/2)) \u03b1 + 2\u03b1( n \u2211 t=1 g2(zt( )) + 1)}\nand, furthermore, the optimal \u03b1 is \u00bf \u00c1\u00c1\u00c0(logn) (logN2(\u03b3/2))\n2(\u2211nt=1 g2(zt( )) + 1)\nwhich is a number between d` = \u221a (logn)(logN2(\u03b3/2)) 2(n+1) and du = \u221a (logn) (logN2(\u03b3/2)) as long as N2(\u03b3/2) > 1. With this, we get\nsup g\u2208G\n\u03b3\u2208[n\u22121,1]\nn\n\u2211 t=1 tg(zt( )) \u2212 \u239b \u239c \u239d 4\n\u00bf \u00c1\u00c1\u00c02(logn) (logN2(\u03b3/2))( n\n\u2211 t=1\ng2(zt( )) + 1) + 24 \u221a 2 logn\u222b \u03b3\n1/n\n\u221a n logN2(\u03b4)d\u03b4 + 2 logn \u239e \u239f \u23a0\n\u2264 sup g\u2208G\n\u03b3\u2208[n\u22121,1],\u03b1\u2208[d`,du]\nn\n\u2211 t=1\ntg(zt( )) \u2212 2(logn) (logN2(\u03b3/2))\n\u03b1 \u2212 4\u03b1\nn\n\u2211 t=1\ng2(zt( )) \u2212 24 \u221a 2 logn\u222b \u03b3\n1/n\n\u221a n logN2(\u03b4)d\u03b4 \u2212 2 logn.\n(12)\nThe case of \u03b3 \u2208 [1/n,2/n) will be considered separately. Let us assume \u03b3 \u2265 2/n. We now discretize both \u03b1 and \u03b3 by defining \u03b1i = 2\u2212(i\u22121)du and \u03b3j = 2jn\u22121, i, j \u2265 1. We go to an upper bound by mapping each \u03b1 to \u03b1i or \u03b1i/2, depending on the direction of the sign. Similarly, we map \u03b3 to either \u03b3i or 2\u03b3i. The upper bound becomes\nmax i,j sup g\u2208G\nn\n\u2211 t=1\n( tg(zt( )) \u2212 2\u03b1ig2(zt( ))) \u2212 (2 logn) ( logN2(\u03b3j)\n\u03b1i + 12\n\u221a 2\u222b \u03b3j\n1/n\n\u221a n logN2(\u03b4)d\u03b4 + 1) .\nGiven the doubling nature of \u03b1i and \u03b3j , the indices i, j are upper bounded by O(logn). Now define a collection of random variables indexed by (i, j)\nXi,j = sup g\u2208G\nn\n\u2211 t=1 tg(zt( )) \u2212 2\u03b1ig2(zt( ))\nand constants\nBi,j = logN2(\u03b3j)\n\u03b1i + 12\n\u221a 2\u222b \u03b3j\n1/n\n\u221a n logN2(\u03b4)d\u03b4 + 1.\nLemma 5 establishes that\nP (Xi,j \u2212Bi,j > \u03c4) \u2264 \u0393 exp(\u2212 \u03c42 2\u03c32j ) + exp(\u2212\u03b1i\u03c4 2 )\nwhere \u03c3j = 12 \u221a\n2 \u222b \u03b3j 1 n\n\u221a n logN2(\u03b4)d\u03b4 and \u0393 as specified in Lemma 5. Whenever \u03b4-entropy grows as \u03b4\u2212p,\n\u03c3j \u2264 12 \u221a 2 \u221a n, ensuring log(\u03c3j/\u03c31) \u2264 log(n). Further, we can take 1 \u2264 \u0393 \u2264 log(2n).\nProposition 2 is used with a sequence of random variables, but we can easily put the pairs (i, j) into a vector of size at most log2(n)2. Observe that si = \u03b1i/2, (Bi,jsi)\u22121 \u2264 2, \u03c3j/Bi,j \u2264 1, s1/si \u2264 \u221a 2(n + 1). Then, by taking \u03c3\u0304 = min{1/\u0393, \u03c31} and s\u0304 = s1,\n\u03b8ki,j = max{ \u03c3j\nBi,j\n\u221a 2 log(\u03c3j/\u03c3\u0304) + 4 log(ki,j), (Bi,jsi)\u22121 log (k2i,j(s\u0304/si))} + 1\n\u2264 max{ \u221a 2 log(n) + 2 log(log(2n)) + 4 log(ki,j),2 log (k2i,j \u221a 2(n + 1))} + 1\nwhere ki,j = (logn) \u22c5 (i \u2212 1) + j. This choice of the multiplier ensures\nEmax i,j\n{Xi,j \u2212 \u03b8ki,jBi,j} \u2264 3\u0393\u03c3\u0304 + 4\u03b1\u221211 \u2264 7\nand \u03b8i,j is shown to be upper bounded by 2 logn. Hence\nE \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup g\u2208G,\u03b3 n \u2211 t=1 tg(zt( )) \u2212 4 \u00bf \u00c1\u00c1\u00c02(logn) logN2(\u03b3/2)( n \u2211 t=1 g2(zt( )) + 1) \u2212 24 \u221a 2 logn\u222b \u03b3 1/n \u221a n logN2(\u03b4)d\u03b4 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 \u2264 7 + 2 logn.\nNow, consider the case \u03b3 \u2208 [1/n,2/n). We upper bound (12) by\nmax i sup g\u2208G\nn\n\u2211 t=1\n( tg(zt( )) \u2212 2\u03b1ig2(zt( ))) \u2212 (2 logn)( logN2(1/n)\n\u03b1i + 1) ,\nwhich is controlled by setting \u03b3 = 1/n in Lemma 5. This case is completed by invoking Proposition 2 as before.\nProof of Corollary 7. Assume N > e and let C > 0. We first note that\ninf \u03b1>0 \u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23a9 C log ( \u221a C logN \u03b1 ) logN \u03b1 + \u03b1( n \u2211 t=1 g2(zt( )) + e logN ) \u23ab\u23aa\u23aa\u23aa\u23ac\u23aa\u23aa\u23aa\u23ad \u2264 2 log(logN n \u2211 t=1 g2(z( )) + e) \u00bf \u00c1\u00c1\u00c0C(logN n \u2211 t=1 g2(z( )) + e)\nwith the inequality obtained using\n\u03b1\u22c6 = \u221a\nC logN\n\u2211nt=1 g2(z( )) + e/ logN ,\nwhich is a number between d` \u225c \u221a C logN n+e/logN and du \u225c \u221a C e logN . Subsequently,\nsup g\u2208G\nn\n\u2211 t=1 tg(zt( )) \u2212 2 log(logN\nn\n\u2211 t=1 g2(z( )) + e)\n\u00bf \u00c1\u00c1\u00c0C(logN n\n\u2211 t=1 g2(z( )) + e)\n\u2264 sup g\u2208G\n\u03b1\u2208[d`,du]\n[ n\n\u2211 t=1 tg(zt( )) \u2212 \u03b1\nn\n\u2211 t=1 g2(zt( )) \u2212\nC logN\n\u03b1 log(\n\u221a C logN\n\u03b1 )].\nLet L = \u2308log2( \u221a n logN e\n+ 1) + 1\u2309. We discretize the range of \u03b1 by defining \u03b1i = du2\u2212(i\u22121) for i \u2208 [L]. The following upper bound holds:\nsup g\u2208G i\u2208[L]\n[ n\n\u2211 t=1 tg(zt( )) \u2212 \u03b1i 2\nn\n\u2211 t=1 g2(zt( )) \u2212\nC logN\n\u03b1i log(\n\u221a C logN\n\u03b1i )].\nDefine a collection of random variables indexed by i \u2208 [L] with\nXi = sup g\u2208G\n[ n\n\u2211 t=1 tg(zt( )) \u2212 \u03b1i 2\nn\n\u2211 t=1 g2(zt( ))]\nand let Bi = 4 logN\u03b1i . Applying Lemma 5 with \u03b3 = 1/n establishes\nP (Xi \u2212Bi > \u03c4) \u2264 exp(\u2212 \u03b1i\u03c4\n8 ).\nWe now set si = \u03b1i/8 and s\u0304 = s1, and apply Proposition 2, yielding\nE{Xi \u2212Bi\u03b8i} \u2264 16\n\u221a e\nC .\nIt remains to relate this quantity to the rate we are trying to achieve. Note that our bound on P (Xi\u2212Bi > \u03c4) has a pure exponential tail, so we only need to consider \u03b8i = (Bisi)\u22121 log(i2(s\u0304/si)) + 1. Taking C \u2265 32 and observing that (Bisi)\u22121 \u2264 2, we obtain\n\u03b8i = (Bisi)\u22121 log(i2(s\u0304/si)) + 1 \u2264 2 log(i2(s\u0304/si)) + 1 = 2 log(i22i\u22121) + 1 \u2264 2 log (i22i)\n\u2264 C 4\nlog( \u221a C logN\n\u03b1i ).\nFinally, we have\nsup g\u2208G i\u2208[L]\n[ n\n\u2211 t=1 tg(zt( )) \u2212 \u03b1i 2\nn\n\u2211 t=1 g2(zt( )) \u2212\n32 logN\n\u03b1i log(\n\u221a 32 logN\n\u03b1i )] \u2264 E{Xi \u2212Bi\u03b8i} \u2264\n\u221a e\n2 \u2264 1.\nProof of Corollary 8. We prove the corollary for convex Lipschitz loss where we remove the loss function using the symmetrization lemma shown earlier. However even if we consider non-convex classes, the loss is readily removed in the step in the proof below where we apply Lemma 4 where the Lipchitz constant is removed when we move to covering numbers. However this is a well known technique and to make the proof simpler we simply assume convexity of loss as well. Our starting point to proving the bounds is Lemma 1, Eq. (4). To show achievability it suffices to show that\nE \u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F n \u2211 t=1 tf(xt( )) \u2212K1Rn(F(2R(f))) log3/2 n \u239b \u239c \u239d 1 + \u00bf \u00c1\u00c1\u00c0log(Rn(F(2R(f))) Rn(F(R(1))) ) + log(log(2R(f))) \u239e \u239f \u23a0 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 \u2264K2\u0393Rn(F(1)) log3/2 n\nwhere \u0393 is the constant that will be inherited from Lemma 4. Define Ri = 2i and note that since the\nRademacher complexity of the class F(R) is non-decreasing with R,\nsup f\u2208F\nn\n\u2211 t=1 tf(xt( )) \u2212K1Rn(F(2R(f))) log3/2 n \u239b \u239c \u239d 1 +\n\u00bf \u00c1\u00c1\u00c0log(Rn(F(2R(f)))\nRn(F(1)) ) + log(log(2R(f))) \u239e \u239f \u23a0\n= sup R\u22651 sup f\u2208F(R)\nn\n\u2211 t=1 tf(xt( )) \u2212K1Rn(F(2R)) log3/2 n \u239b \u239c \u239d 1 +\n\u00bf \u00c1\u00c1\u00c0log(Rn(F(2R))\nRn(F(1)) ) + log(log(2R)) \u239e \u239f \u23a0\n\u2264 max i\u2208N sup f\u2208F(Ri)\nn\n\u2211 t=1 tf(xt( )) \u2212K1Rn(F(Ri)) log3/2 n \u239b \u239c \u239d 1 +\n\u00bf \u00c1\u00c1\u00c0log(Rn(F(Ri))\nRn(F(1)) ) + log(log(Ri)) \u239e \u239f \u23a0 . (13)\nDenote a shorthand Cn = \u221a\n96 log3(en2) and Din = Rn(F(Ri)). Now note that by Lemma 4 we have that for every i and every \u03b8 > 1,\nP \u239b \u239d sup f\u2208F(Ri) \u2223 n \u2211 t=1 tf(xt( ))\u2223 > 8 (1 + \u03b8Cn) \u22c5Din) \u239e \u23a0 \u2264 2\u0393e\u22123\u03b8 2 .\nLet Xi = supf\u2208F(Ri) \u2223\u2211 n t=1 tf(xt( ))\u2223 and let Bi = 8 (1 +Cn) \u22c5Din. In this case rewriting the above one sided tail bound appropriately (with \u03b8 = 1 + \u03c4/(8CnDin)) we see that for any \u03c4 > 0,\nP (Xi \u2212Bi > \u03c4) \u2264 2\u0393 e3 exp(\u2212 \u03c4\n2\n28 log3(en2)R2n(F(Ri)) ) .\nThis establishes one-sided subgaussian tail behavior. Now applying Proposition 2 and setting \u03b8i as suggested by the proposition we conclude that\nE \u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 max i\u2208N sup f\u2208F(Ri) n \u2211 t=1 tf(xt( ) \u2212K1Rn(F(Ri)) log3/2 n \u239b \u239c \u239d 1 + \u00bf \u00c1\u00c1\u00c0log(Rn(F(Ri))Rn(F(1)) ) + log(log(Ri)) \u239e \u239f \u23a0 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 \u2264K2\u0393Rn(F(1)) log3/2 n.\nThis concludes the proof by appealing to Eq. (13).\nProof of Achievability for Example 4.2.\nLemma 10. The following bound is achievable in the setting of Example 4.2:\nB(f) =D \u221a n(8\u2225f\u2225(1 + \u221a log(2\u2225f\u2225) + log log(2\u2225f\u2225)) + 12).\nThis proof specializes the proof of Corollary 8 to the regime where Lemma 3 applies. Recall our parameterization of F : F(R) = {f \u2208 F \u2236 \u2225f\u2225 \u2264 R}. It was shown in [26] that Cn(F(R)) \u225c\n2RD \u221a n is an upper bound for Rn(F(R)). We consider the rate\nBn(f) = 2Cn(F(2R(f))) \u239b \u239c \u239d 1 + \u00bf \u00c1\u00c1\u00c0log(Cn(F(2R(f)))Cn(F(1)) ) + log log2(2R(f)) \u239e \u239f \u23a0 .\nWe begin by applying Lemma 1 (5), yielding\nAn \u2264 sup y E sup f \u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 2 n \u2211 t=1 t\u27e8f,yt( )\u27e9 \u2212 2Cn(F(2R(f))) \u239b \u239c \u239d 1 + \u00bf \u00c1\u00c1\u00c0log(Cn(F(2R(f)))Cn(F(1)) ) + log log2(2R(f)) \u239e \u239f \u23a0 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 .\nWe now discretize the range of R via Ri = 2i. By analogy with the proof of Corollary 8 we get the upper bound,\nsup y E sup i\u2208N \u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F(Ri) 2 n \u2211 t=1 t\u27e8f,yt( )\u27e9 \u2212 2Cn(F(Ri)) \u239b \u239c \u239d 1 + \u00bf \u00c1\u00c1\u00c0log(Cn(F(Ri))Cn(F(1)) ) + log log2(Ri) \u239e \u239f \u23a0 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6\n= sup y E sup i\u2208N\n[2Ri\u2225 n\n\u2211 t=1 tyt( )\u2225 \u22c6 \u2212 4D\n\u221a nRi \u221a log(Ri) + log(i)].\nFix a Y-valued tree y and define a set of random variables Xi = 2Ri\u2225\u2211nt=1 tyt( )\u2225\u22c6. Let Bi = 2D \u221a nRi. Lemma 3 shows that\nP (Xi \u2212Bi \u2265 \u03c4) \u2264 2 exp(\u2212 \u03c42\n8D2R2in ).\nSo we have \u03c3i = 2DRi \u221a n, and it will be sufficient to set \u03c3\u0304 = 2D\u221an. Since our tail bound is purely sub-\ngaussian, we apply Proposition 2 with \u03b8i = \u03c3iBi \u221a 2 log(\u03c3i/\u03c3\u0304) + 4 log(i) + 1, yielding the following bound:\nsup y E sup i\u2208N\n[2Ri\u2225 n\n\u2211 t=1 tyt( )\u2225 \u22c6 \u2212 4D\n\u221a nRi \u221a log(Ri) + log(i)] \u2264 12D \u221a n.\nProof of Achievability for Example 4.5. Unfortunately, the general symmetrization proof in Lemma 1 does not suffice for this problem. In what follows we use a more specialized symmetrization technique to prove the lemma.\nLemma 11. For any countable class of experts, when we consider F to be the class of all distributions over the set of experts, the following adaptive bound is achievable:\nBn(f ; y1\u2236n) = \u00bf \u00c1\u00c1\u00c050 (KL(f \u2223\u03c0) + log(n)) n\n\u2211 t=1 \u27e8f, yt\u27e9 + 50 (KL(f \u2223\u03c0) + log(n)) + 1.\nTo show that the rate is achievable we need to show that An \u2264 0. Since each y\u0302t is a distribution over experts and we are in the linear setting, we do not need to randomize in the definition of the minimax value. Let us use the shorthand C(f) = KL(f \u2223\u03c0) + log(n), and take constants K1,K2 to be determined later. Define\nAn = \u27ea inf y\u0302t\u2208\u2206 sup yt\u2208Y\n\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 n \u2211 t=1 \u27e8y\u0302t, yt\u27e9 \u2212 inf f\u2208\u2206 \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 n \u2211 t=1 \u27e8f, yt\u27e9 + \u00bf \u00c1\u00c1\u00c0KC(f) n \u2211 t=1 Ei\u223cf \u27e8ei, yt\u27e92 + \u221a K\u2032C(f) \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nUsing repeated minimax swap, this expression is equal to\n\u27ea sup pt\u2208\u2206(Y) inf y\u0302t\u2208\u2206\n\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 n \u2211 t=1 \u27e8y\u0302t, yt\u27e9 \u2212 inf f\u2208\u2206 \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 n \u2211 t=1 \u27e8f, yt\u27e9 + \u00bf \u00c1\u00c1\u00c0KC(f) n \u2211 t=1 Ei\u223cf \u27e8ei, yt\u27e92 + \u221a K\u2032C(f) \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6\n= \u27ea sup pt\u2208\u2206(Y)\nEyt\u223cpt\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 n \u2211 t=1 inf y\u0302t\u2208\u2206 Eyt\u223cpt [\u27e8y\u0302t, yt\u27e9] \u2212 inf f\u2208\u2206 \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 n \u2211 t=1 \u27e8f, yt\u27e9 + \u00bf \u00c1\u00c1\u00c0KC(f) n \u2211 t=1 Ei\u223cf \u27e8ei, yt\u27e92 + \u221a K\u2032C(f) \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nBy sub-additivity of square-root, we pass to an upper bound\n\u27easup pt\nEyt\u223cpt\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F n \u2211 t=1 inf y\u0302t\u2208\u2206 Eyt\u223cpt [\u27e8y\u0302t, yt\u27e9] \u2212 Eei\u223cf [\u27e8ei, yt\u27e9] \u2212 \u00bf \u00c1\u00c1\u00c0C(f)(K n \u2211 t=1 Ei\u223cf [\u27e8ei, yt\u27e92] +K\u2032C(f)) \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nWe now split the square root according to the formula \u221a ab = inf\u03b1>0 {a/2\u03b1 + \u03b1b/2} and note the range of the optimal value:\n1\u221a n \u2264 \u03b1\u2217 =\n\u00bf \u00c1\u00c1\u00c0 C(f)\n(K\u2211nt=1 Ei\u223cf [\u27e8ei, yt\u27e9 2] +K\u2032C(f)) \u2264 1\u221a K\u2032 . (14)\nLet us discretize the interval by setting \u03b1i = 1\u221a K\u2032 2\u2212(i\u22121) for i = 1, . . . ,N and note that we only need to take N = O(log(n)) elements. Write I = {\u03b11, . . . , \u03b1N}. Observe that\n\u221a ab = inf\n\u03b1>0 {a/2\u03b1 + \u03b1b/2} \u2265 min \u03b1\u2208I {a/4\u03b1 + \u03b1b/2} .\nFor the rest of the proof, the maximum over \u03b1 is taken within the set I. We have\nAn \u2264 \u27easup pt\nEyt\u223cpt\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208\u2206,\u03b1 n \u2211 t=1 inf y\u0302t\u2208\u2206(F) Eyt [\u27e8y\u0302t, yt\u27e9] \u2212 Eei\u223cf [\u27e8ei, yt\u27e9] \u2212 \u03b1 2 (K n \u2211 t=1 Ei\u223cf [\u27e8ei, yt\u27e92] +K\u2032C(f)) \u2212 C(f) 4\u03b1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\n(15)\nDropping some negative terms, we upper bound the last expression by\n\u27easup pt\nEyt\u223cpt\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F,\u03b1 n \u2211 t=1 \u27e8f,E [y\u2032t] \u2212 yt\u27e9 \u2212 K\u03b1 2 n \u2211 t=1 Ei\u223cf [\u27e8ei, yt\u27e92] \u2212 C(f) 4\u03b1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nAdding and subtracting \u03b1 4 \u2211 n t=1 Ey\u2032t [Ei\u223cf [\u27e8ei, y \u2032 t\u27e9 2]],\n\u2264 \u27easup pt\nEyt\u223cpt\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F,\u03b1 n \u2211 t=1 \u27e8f,E [y\u2032t] \u2212 yt\u27e9 \u2212 K\u03b1 4 n \u2211 t=1 Ei\u223cf [\u27e8ei, yt\u27e92] \u2212 K\u03b1 4 n \u2211 t=1 Ey\u2032t [Ei\u223cf [\u27e8ei, y \u2032 t\u27e9 2]]\n+ K\u03b1 4 ( n \u2211 t=1 Ey\u2032t [Ei\u223cf [\u27e8ei, y \u2032 t\u27e9 2]] \u2212 Ei\u223cf [\u27e8ei, yt\u27e92]) \u2212 C(f) 4\u03b1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nUsing Jensen\u2019s inequality to pull out expectations, we obtain an upper bound,\n\u27easup pt\nEyt,y\u2032t\u223cpt\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F,\u03b1 n \u2211 t=1 \u27e8f, y\u2032t \u2212 yt\u27e9 \u2212 K\u03b1 4 n \u2211 t=1 Ei\u223cf [\u27e8ei, yt\u27e92] \u2212 K\u03b1 4 n \u2211 t=1 Ei\u223cf [\u27e8ei, y\u2032t\u27e9 2]\n+ K\u03b1 4 ( n \u2211 t=1 Ei\u223cf [\u27e8ei, y\u2032t\u27e9 2] \u2212 Ei\u223cf [\u27e8ei, yt\u27e92]) \u2212 C(f) 4\u03b1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nNext, we introduce Rademacher random variables:\n\u27easup pt\nEyt,y\u2032t\u223cptE t\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F,\u03b1 n \u2211 t=1 t(\u27e8f, y\u2032t \u2212 yt\u27e9 + K\u03b1 4 (Ei\u223cf [\u27e8ei, y\u2032t\u27e9 2] \u2212 Ei\u223cf [\u27e8ei, yt\u27e92]))\n\u2212 K\u03b1 4\nn\n\u2211 t=1\nEi\u223cf [\u27e8ei, yt\u27e92] \u2212 K\u03b1\n4\nn\n\u2211 t=1\nEi\u223cf [\u27e8ei, y\u2032t\u27e9 2] \u2212 C(f)\n4\u03b1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6\n\u2264 \u27easup yt\nE t\u27eb n\nt=1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 sup f\u2208F,\u03b1 n \u2211 t=1 t(2\u27e8f, yt\u27e9 + K\u03b1 2 Ei\u223cf [\u27e8ei, yt\u27e92]) \u2212 K\u03b1 2 n \u2211 t=1 Ei\u223cf [\u27e8ei, yt\u27e92] \u2212 C(f) 4\u03b1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nMoving to the tree notation, we get\nsup y E [ sup f\u2208F,\u03b1\nn\n\u2211 t=1\nt(2\u27e8f,yt( )\u27e9 + K\u03b1\n2 Ei\u223cf [\u27e8ei,yt( )\u27e92]) \u2212\nK\u03b1\n2\nn\n\u2211 t=1\nEi\u223cf [\u27e8ei,yt( )\u27e92] \u2212 KL(f \u2223\u03c0) 4\u03b1 \u2212 log(n) 4\u03b1 ] .\nNote that the convex conjugate of KL(f\u2225\u03c0) is given by \u03a8\u2217(X) = 1 \u03b1 log (Ee\u223c\u03c0 [exp (\u03b1\u27e8e,X\u27e9)]) and we express the last quantity as\nsup y E [max \u03b1\n1\n4\u03b1 log(Ei\u223c\u03c0 [exp(\nn\n\u2211 t=1\nt(8\u03b1\u27e8ei,yt( )\u27e9 + 2K\u03b12\u27e8ei,yt( )\u27e92) \u2212 2K\u03b12 (\u27e8ei,yt( )\u27e9)2)]) \u2212 log(n)\n4\u03b1 ] .\nDefine X\u03b1 = 14\u03b1 log (Ei\u223c\u03c0 [exp (\u2211 n t=1 t(8\u03b1\u27e8ei,yt( )\u27e9 + 2K\u03b12\u27e8ei,yt( )\u27e9 2) \u2212 2K\u03b12 (\u27e8ei,yt( )\u27e9)2)]). Our goal is to bound E [max\u03b1{X\u03b1 \u2212 log(n)/4\u03b1}]. Now notice that\nP (X\u03b1 > t) \u2264 inf \u03bb E [e\u03bbX\u03b1\u2212\u03bbt]\n= inf \u03bb \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 E \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 (Ei\u223c\u03c0 [exp( n \u2211 t=1 t(8\u03b1\u27e8ei,yt( )\u27e9 + 2K\u03b12\u27e8ei,yt( )\u27e92) \u2212 2K\u03b12\u27e8ei,yt( )\u27e92)]) \u03bb 4\u03b1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 exp(\u2212\u03bbt) \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad \u2264 E [Ei\u223c\u03c0 [exp( n\n\u2211 t=1 t(8\u03b1\u27e8ei,yt( )\u27e9 + 2K\u03b12\u27e8ei,yt( )\u27e92) \u2212 2K\u03b12\u27e8ei,yt( )\u27e92)]] exp(\u22124\u03b1t)\n\u2264 E [Ei\u223c\u03c0 [exp( n\n\u2211 t=1\n(8\u03b1\u27e8ei,yt( )\u27e9 + 2K\u03b12\u27e8ei,yt( )\u27e92) 2 \u2212 2K\u03b12\u27e8ei,yt( )\u27e92)]] exp(\u22124\u03b1t)\n\u2264 E [Ei\u223c\u03c0 [exp( n\n\u2211 t=1\n4\u03b1(4 +K\u03b1)2\u27e8ei,yt( )\u27e92 \u2212 2K\u03b12\u27e8ei,yt( )\u27e92)]] exp(\u22124\u03b1t).\nThe above term is upper bounded by exp(\u22124\u03b1t) as soon as 4\u03b12(4 +K\u03b1)2 \u2264 2K\u03b12, which happens when\n0 < \u03b1 \u2264 ( \u221a K/2 \u2212 4)/K. (16)\nIn view of (14), we know that \u03b1 \u2264 1\u221a K\u2032 . Thus, to ensure (16), it is sufficient to take K = 50 and K \u2032 = 502. Other choices lead to a different balance of constants. We thus have\nP (X\u03b1 > t) \u2264 exp (\u22124\u03b1t) .\nNow that we have the tail bound, we appeal to Proposition 2. Setting si = 4\u03b1i and Bi = 1/4\u03b1i, we obtain that\nE [ max i=1,...,N\n{X\u03b1i \u2212 log(n)\n4\u03b1 }] \u2264 10."}, {"heading": "B Relaxations and Algorithms", "text": "Proof of Admissibility for Example 5.1.\nLemma 12. The following bound is achievable in the setting given in example 5.1:\nBn(f) = 3 \u221a 2nmax{KL(f \u2223 \u03c0),1} + 4 \u221a n. (17)\nFollowing the analysis style of Corollary 8, we directly consider an upper bound based on KL(f \u2223 \u03c0) but instead use a complexity-radius-based upper bound with the KL divergence controlling the complexity radius: F(R) = {f \u2236 KL(f \u2223 \u03c0) \u2264 R}. Concretely, we move from (17) to the bound\nBn(i) = 3 \u221a nRi + 4 \u221a n\nfor Ri = 2i\u22121 with i \u2208 N. To keep the analysis as tidy as possible, we will study the achievability of Bn(i) = D \u221a Rin, setting D and including additive constants only when we reach a point in the analysis where it becomes necessary to do so. The relaxation we consider is thus\nReln(y1\u2236t) = inf \u03bb>0 [ 1 \u03bb log(\u2211 i exp(\u2212\u03bb[ t \u2211 s=1 \u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)])) + 2\u03bb(n \u2212 t)].\nInitial Condition: This inequality follows from Lemma 13 and an application of the softmax function as an upper bound on the supremum over i:\n\u2212 inf i \u23a1\u23a2\u23a2\u23a2\u23a3 inf f\u2208F(Ri) n \u2211 t=1 `(f, yt) + Bn(i) \u23a4\u23a5\u23a5\u23a5\u23a6\n= sup i\n[\u2212 t\n\u2211 s=1\n\u27e8qRis (y1\u2236s\u22121), ys\u27e9 + 2 \u221a nRi \u2212 Bn(i)]\n\u2264 inf \u03bb>0\n1 \u03bb log(\u2211\ni\nexp(\u2212\u03bb[ t\n\u2211 s=1\n\u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)]))\n= Reln(y1\u2236n).\nAdmissibility Condition: Define a strategy q\u22c6t via\n(q\u22c6t )i = exp(\u2212\u03bb\u22c6t [\u2211t\u22121s=1\u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2\n\u221a nRi + Bn(i)])\n\u2211j exp(\u2212\u03bb\u22c6t [\u2211t\u22121s=1\u27e8q Rj s (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRj + Bn(Rj)]) ,\nwhere we have set\n\u03bb\u22c6t = arg min \u03bb>0 [ 1 \u03bb log(\u2211 i exp(\u2212\u03bb[ t\u22121 \u2211 s=1 \u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)])) + 2\u03bb(n \u2212 t + 1)].\nWe proceed to demonstrate admissibility:\ninf qt sup yt\n[\u27e8qt, yt\u27e9 +Reln(y1\u2236t)]\n= inf qt sup yt [\u27e8qt, yt\u27e9 + inf \u03bb>0 [ 1 \u03bb log(\u2211 i exp(\u2212\u03bb[ t \u2211 s=1 \u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)])) + 2\u03bb(n \u2212 t)]].\nWe now plug in q\u22c6t and \u03bb \u22c6 t as described above:\n\u2264 sup yt [ 1 \u03bb\u22c6t log(exp(\u03bb\u22c6tEi\u223cq\u22c6t \u27e8q Ri t (y1\u2236t\u22121), yt\u27e9)) + 1 \u03bb\u22c6t log(Ei\u223cq\u22c6t exp(\u2212\u03bb \u22c6 t \u27e8qRit (y1\u2236t\u22121), yt\u27e9))\n+ 1 \u03bb\u22c6t log(\u2211 i exp(\u2212\u03bb\u22c6t [ t\u22121 \u2211 s=1 \u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)])) + 2\u03bb\u22c6t (n \u2212 t)].\nWe combine the first two terms in the expression and apply Jensen\u2019s inequality to arrive at an upper bound:\n\u2264 sup yt [ 1 \u03bb\u22c6t log(Ei,i\u2032\u223cq\u22c6t exp(\u03bb \u22c6 t \u27e8qRit (y1\u2236t\u22121) \u2212 q Ri\u2032 t (y1\u2236t\u22121), yt\u27e9))\n+ 1 \u03bb\u22c6t log(\u2211 i exp(\u2212\u03bb\u22c6t [ t\u22121 \u2211 s=1 \u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)])) + 2\u03bb\u22c6t (n \u2212 t)].\nThe first term is now bounded using sub-gaussianity.\n\u2264 1 \u03bb\u22c6t log(\u2211 i exp(\u2212\u03bb\u22c6t [ t\u22121 \u2211 s=1 \u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)])) + 2\u03bb\u22c6t (n \u2212 t + 1)\n= inf \u03bb>0 [ 1 \u03bb log(\u2211 i exp(\u2212\u03bb[ t\u22121 \u2211 s=1 \u27e8qRis (y1\u2236s\u22121), ys\u27e9 \u2212 2 \u221a nRi + Bn(i)])) + 2\u03bb(n \u2212 t + 1)] = Reln(y1\u2236t\u22121).\nHaving shown that Reln is an admissible relaxation, it remains to show that the relaxation\u2019s final value,\nReln(\u22c5) = inf \u03bb>0 [ 1 \u03bb log(\u2211 i\nexp(\u03bb[2 \u221a nRi \u2212D \u221a nRi])) + 2\u03bbn]\nis not too large. Setting D = 3,\n= inf \u03bb>0 [ 1 \u03bb log(\u2211 i\nexp(\u2212\u03bb \u221a nRi)) + 2\u03bbn].\nThe complexity radius Ri is discretized such that Ri \u2212Ri\u22121 \u2265 1, yielding\n\u2264 inf \u03bb>0 [ 1 \u03bb\nlog(exp(\u2212\u03bb \u221a n) + \u221e \u2211 i=2 (Ri \u2212Ri\u22121) exp(\u2212\u03bb \u221a nRi)) + 2\u03bbn]\n\u2264 inf \u03bb>0 [ 1 \u03bb log(exp(\u2212\u03bb \u221a n) + \u222b \u221e 1 exp(\u2212\u03bb \u221a nR)dR) + 2\u03bbn].\nThe integral is a routine calculation.\n\u222b \u221e\n1 exp(\u2212\u03bb\n\u221a nR)dR = \u22122 1\n\u03bb2n exp(\u2212\u03bb\n\u221a nR)[\u03bb \u221a nR + 1]\u2223\n\u221e\n1\n.\nFinally, set \u03bb = 1/\u221an yielding\nReln(\u22c5) \u2264 4 \u221a n.\nNote that instead of setting \u03bbt = \u03bb\u22c6t as described above, we could have set \u03bbt = 1/ \u221a n and achieved the same regret bound.\nLemma 13. Consider the experts setting from Example 4.5, but with hypothesis class F(R) = {f \u2236 KL(f \u2223 \u03c0) \u2264 R}. The following inequality holds:\n\u2212 inf f\u2208F(R)\nn\n\u2211 t=1\n\u27e8yt, f\u27e9 \u2264 \u2212 n\n\u2211 t=1\n\u27e8yt, qR(y1\u2236t\u22121)\u27e9 + 2 \u221a Rn.\nProof. Our strategy is to move to an upper bound based on the Kullback-Leibler divergence and exploit convex duality:\n\u2212 inf f\u2208F(R)\nn\n\u2211 t=1 \u27e8yt, f\u27e9\n\u2264 \u2212 inf f\u2208F(R)\n{ n\n\u2211 t=1 \u27e8yt, f\u27e9 + \u03b1KL(f \u2223 \u03c0)} + \u03b1R\n\u2264 \u2212 inf f\u2208F\n{ n\n\u2211 t=1 \u27e8yt, f\u27e9 + \u03b1KL(f \u2223 \u03c0)} + \u03b1R.\nWe use \u03a8\u22c6 to denote the Fenchel conjugate of KL(\u22c5 \u2223 \u03c0):\n= \u03b1\u03a8\u22c6(\u2212 1 \u03b1\nn\n\u2211 t=1 yt)\u03a8 + \u03b1R.\nThe function KL(\u22c5 \u2223 \u03c0) is 1-strongly convex, which implies that \u03a8\u2217 is 1-strongly smooth. We peel off one term at a time:\n\u03b1\u03a8\u22c6(\u2212 1 \u03b1\nn\n\u2211 t=1 yt) \u2264 \u03b1\u03a8\u22c6(\u2212\n1\n\u03b1 n\u22121 \u2211 t=1 yt) + \u27e8\u2212yn,\u2207\u03a8\u22c6(\u2212 1 \u03b1 n\u22121 \u2211 t=1 yt)\u27e9 + 1 \u03b1 .\nThis obtains the following upper bound:\n\u2212 n\n\u2211 t=1\n\u27e8yt,\u2207\u03a8\u22c6(\u2212 1\n\u03b1 t\u22121 \u2211 s=1 ys)\u27e9 + KCn \u03b1 + \u03b1R.\nSetting \u03b1 = \u221a n/R and noting that \u2207\u03a8\u22c6(\u2212 \u221a R n \u2211 t\u22121 s=1 ys) = qR(y1\u2236t\u22121) yields the result.\nProof of Lemma 9. Recall the form of the Adan relaxation, where we have abbreviated Rel R n to R R:\nAdan(y1\u2236t) = sup y,y\u2032 E sup R\n[RR(y1\u2236t) \u2212RR\u03b8(RR) + 2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))].\nInitial Condition: This directly follows from the fact that RR satisfy the initial condition:\nAdan(y1\u2236n) = sup R [RR(y1\u2236n) \u2212RR\u03b8(RR)]\n\u2265 sup R \u23a1\u23a2\u23a2\u23a2\u23a3 \u2212 inf f\u2208F(R) n \u2211 t=1 `(f, yt) \u2212RR\u03b8(RR) \u23a4\u23a5\u23a5\u23a5\u23a6\n= \u2212 inf R inf f\u2208F(R)\n[ n\n\u2211 t=1 `(f, yt) +RR\u03b8(RR)].\nTherefore, playing the strategy corresponding to Adan yields an adaptive regret bound of the form Bn(R) = RelRn (\u22c5)\u03b8(RelRn (\u22c5)) +Adan(\u22c5).\nAdmissibility Condition: We obtain the following equalities using the same minimax swap technique as in the Lemma 1 proof:\ninf qt sup yt\nEy\u0302t\u223cqt[`(y\u0302t, yt) +Adan(y1\u2236t)]\n= inf qt sup yt Ey\u0302t\u223cqt sup y,y\u2032 E sup R\n[`(y\u0302t, yt) +RR(y1\u2236t) \u2212RR\u03b8(RR) + 2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))]\n= sup pt Eyt\u223cpt sup y,y\u2032 E sup R [inf y\u0302t\nEy\u2032t\u223cpt`(y\u0302t, y \u2032 t) +RR(y1\u2236t) \u2212RR\u03b8(RR) + 2\nn\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))].\nNote that inf y\u0302t Ey\u2032t\u223cpt`(y\u0302t, y \u2032 t) = inf qt\u2208\u2206(D) Ey\u0302t\u223cqtEy\u2032t\u223cpt`(y\u0302t, y \u2032 t),\nand we may replace the infimizing distribution with the randomized strategy qRt corresponding to Rel R n . The fact that this strategy depends on y1\u2236t\u22121 is left implicit. This yields an upper bound,\nsup pt Eyt\u223cpt sup y,y\u2032 E sup R\n[Ey\u2032t\u223cptEy\u0302t\u223cqRt `(y\u0302t, y \u2032 t) +RR(y1\u2236t) \u2212RR\u03b8(RR) + 2\nn\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))],\nwhich we can write by adding and subtracting Ey\u0302t\u223cqRt `(y\u0302t, yt) as\nsup pt Eyt\u223cpt sup y,y\u2032 E sup R\n[Ey\u2032t\u223cptEy\u0302t\u223cqRt `(y\u0302t, y \u2032 t) \u2212Ey\u0302t\u223cqRt `(y\u0302t, yt) +Ey\u0302t\u223cqRt `(y\u0302t, yt) +R R(y1\u2236t) \u2212RR\u03b8(RR)\n+2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))] .\nNow, using the fact that RR are admissible,\n\u2264 sup pt Eyt\u223cpt sup y,y\u2032 E sup R\n[Ey\u2032t\u223cptEy\u0302t\u223cqRt `(y\u0302t, y \u2032 t) \u2212Ey\u0302t\u223cqRt `(y\u0302t, yt) +R R(y1\u2236t\u22121) \u2212RR\u03b8(RR)\n+2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))] .\nBy Jensen\u2019s inequality, we upper bound the last expression by\nsup pt Eyt,y\u2032t\u223cpt sup y,y\u2032 E sup R\n[Ey\u0302t\u223cqRt `(y\u0302t, y \u2032 t) \u2212Ey\u0302t\u223cqRt `(y\u0302t, yt) +R R(y1\u2236t\u22121) \u2212RR\u03b8(RR)\n+2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))] .\nWe now replace each choice yt in the last sum by a worst-case choice y \u2032\u2032 t :\n\u2264 sup pt Eyt,y\u2032t\u223cpt sup y\u2032\u2032t sup y,y\u2032 E sup R\n[Ey\u0302t\u223cqRt `(y\u0302t, y \u2032 t) \u2212Ey\u0302t\u223cqRt `(y\u0302t, yt) +R R(y1\u2236t\u22121) \u2212RR\u03b8(RR)\n+2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t\u22121,y\u2032\u2032t ,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))] .\nWe then introduce t since yt, y \u2032 t can be renamed. The last expression is equal to\nsup pt Eyt,y\u2032t\u223cptE t sup y\u2032\u2032t sup y,y\u2032 E sup R\n[Ey\u0302t\u223cqRt [ t(`(y\u0302t, y \u2032 t) \u2212 `(y\u0302t, yt))] +RR(y1\u2236t\u22121) \u2212RR\u03b8(RR)\n+2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t\u22121,y\u2032\u2032t ,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))] .\nBy splitting into two terms we arrive at an upper bound of\nsup pt Eyt\u223cptE t sup y\u2032\u2032t sup y,y\u2032 E sup R [2 tEy\u0302t\u223cqRt [`(y\u0302t, yt)] +R R(y1\u2236t\u22121) \u2212RR\u03b8(RR)\n+2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t\u22121,y\u2032\u2032t ,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))]\n= sup yt E t sup y\u2032\u2032t sup y,y\u2032 E sup R [2 tEy\u0302t\u223cqRt [`(y\u0302t, yt)] +R R(y1\u2236t\u22121) \u2212RR\u03b8(RR)\n+2 n\n\u2211 s=t+1 sEy\u0302s\u223cqRs (y1\u2236t\u22121,y\u2032\u2032t ,y\u2032t+1\u2236s\u22121( ))`(y\u0302s,ys( ))]\n= Adan(y1\u2236t\u22121)."}], "references": [{"title": "Minimum contrast estimators on sieves: exponential bounds and rates of convergence", "author": ["Lucien Birg\u00e9", "Pascal Massart"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Adaptive model selection using empirical complexities", "author": ["G\u00e1bor Lugosi", "Andrew B Nobel"], "venue": "Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Model selection and error estimation", "author": ["Peter L. Bartlett", "St\u00e9phane Boucheron", "G\u00e1bor Lugosi"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Concentration inequalities and model selection, volume", "author": ["Pascal Massart"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Learning without Concentration", "author": ["Shahar Mendelson"], "venue": "In Conference on Learning Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning with square loss: Localization through offset rademacher complexity", "author": ["Tengyuan Liang", "Alexander Rakhlin", "Karthik Sridharan"], "venue": "Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Online nonparametric regression", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Online learning: Random averages, combinatorial parameters, and learnability", "author": ["Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Online optimization with gradual variations", "author": ["Chao-Kai Chiang", "Tianbao Yang", "Chia-Jung Lee", "Mehrdad Mahdavi", "Chi-Jen Lu", "Rong Jin", "Shenghuo Zhu"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Online learning with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["Nicolo Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "A parameter-free hedging algorithm", "author": ["Kamalika Chaudhuri", "Yoav Freund", "Daniel J Hsu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations", "author": ["H. Brendan McMahan", "Francesco Orabona"], "venue": "Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Smoothness, low noise and fast rates", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Achieving all with no parameters: Adaptive normalhedge", "author": ["Haipeng Luo", "Robert E. Schapire"], "venue": "CoRR, abs/1502.05934,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Second-order quantile methods for experts and combinatorial games", "author": ["Wouter M. Koolen", "Tim van Erven"], "venue": "In Proceedings of the 28th Annual Conference on Learning Theory (COLT),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Behavior of sequential predictors of binary sequences. In in Trans. 4th Prague Conference on Information Theory, Statistical Decision Functions, Random Processes, pages 263\u2013272", "author": ["Thomas M. Cover"], "venue": "Publishing House of the Czechoslovak Academy of Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1967}, {"title": "Statistical learning theory and sequential prediction", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Optimum bounds for the distributions of martingales in banach spaces", "author": ["Iosif Pinelis"], "venue": "The Annals of Probability, 22(4):1679\u20131706,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Online learning: Beyond regret", "author": ["Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari"], "venue": "arXiv preprint arXiv:1011.3168,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Sequential complexities and uniform martingale laws of large numbers", "author": ["Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Regret to the best vs. regret to the average", "author": ["Eyal Even-Dar", "Michael Kearns", "Yishay Mansour", "Jennifer Wortman"], "venue": "Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Oracle inequalities and model selection have been topics of intense research in statistics in the last two decades [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 1, "context": "Oracle inequalities and model selection have been topics of intense research in statistics in the last two decades [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 2, "context": "Oracle inequalities and model selection have been topics of intense research in statistics in the last two decades [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": "In order to select a good model in a data-driven manner, one first establishes non-asymptotic data-dependent bounds on the fluctuations of an empirical process indexed by elements in each model (see the monograph [4]).", "startOffset": 213, "endOffset": 216}, {"referenceID": 4, "context": "This realization is motivated by the recent work of [5, 6, 7].", "startOffset": 52, "endOffset": 61}, {"referenceID": 5, "context": "This realization is motivated by the recent work of [5, 6, 7].", "startOffset": 52, "endOffset": 61}, {"referenceID": 6, "context": "This realization is motivated by the recent work of [5, 6, 7].", "startOffset": 52, "endOffset": 61}, {"referenceID": 6, "context": "At the high level, our approach will be to develop one-sided inequalities for the suprema of certain offset processes [7], with an offset that is chosen to be \u201cslightly larger\u201d than the complexity of the corresponding model.", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "For example, sequential Rademacher complexity of F is one of the tightest achievable uniform rates for a variety of loss functions [8, 7].", "startOffset": 131, "endOffset": 137}, {"referenceID": 6, "context": "For example, sequential Rademacher complexity of F is one of the tightest achievable uniform rates for a variety of loss functions [8, 7].", "startOffset": 131, "endOffset": 137}, {"referenceID": 8, "context": "An incomplete list of prior work includes [9, 10, 11, 12], couched in the setting of online linear/convex optimization, and [13] in the experts setting.", "startOffset": 42, "endOffset": 57}, {"referenceID": 9, "context": "An incomplete list of prior work includes [9, 10, 11, 12], couched in the setting of online linear/convex optimization, and [13] in the experts setting.", "startOffset": 42, "endOffset": 57}, {"referenceID": 10, "context": "An incomplete list of prior work includes [9, 10, 11, 12], couched in the setting of online linear/convex optimization, and [13] in the experts setting.", "startOffset": 42, "endOffset": 57}, {"referenceID": 11, "context": "An incomplete list of prior work includes [9, 10, 11, 12], couched in the setting of online linear/convex optimization, and [13] in the experts setting.", "startOffset": 42, "endOffset": 57}, {"referenceID": 12, "context": "An incomplete list of prior work includes [9, 10, 11, 12], couched in the setting of online linear/convex optimization, and [13] in the experts setting.", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "The present paper was partly motivated by the work of [14] who presented an algorithm that competes with all experts simultaneously, but with varied regret with respect to each of them, depending on the quantile of the expert.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "The work of [15] considers online linear optimization with an unbounded set and provides oracle inequalities with an appropriately chosen function Bn(f).", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "4], [17, 13]) fall in this category trivially, since one may overbound the loss of the best function by the performance of f .", "startOffset": 4, "endOffset": 12}, {"referenceID": 12, "context": "4], [17, 13]) fall in this category trivially, since one may overbound the loss of the best function by the performance of f .", "startOffset": 4, "endOffset": 12}, {"referenceID": 17, "context": "We would like to draw attention to the recent result of [18] who show an adaptive bound in terms of both the loss of comparator and the KL divergence between the comparator and some pre-fixed prior distribution over experts.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "An MDL-style bound in terms of the variance of the loss of the comparator (under the distribution induced by the algorithm) was recently given in [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "Our study was also partly inspired by Cover [20] who characterized necessary and sufficient conditions for achievable bounds in prediction of binary sequences.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "The methods in [20], however, rely on the structure of the binary prediction problem and do not readily generalize to other settings.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "It should be noted that while existing literature on adaptive online learning has focused on simple hypothesis classes such as finite experts and finite-dimensional p-norm balls, our results extend to general hypothesis classes, including large nonparametric ones discussed in [7].", "startOffset": 277, "endOffset": 280}, {"referenceID": 20, "context": ", n rounds (see [21]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": ", Bn(f ;x1\u2236n, y1\u2236n) = Bn, achievability reduces to the minimax analysis explored in [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "We first show that the minimax value is bounded by an offset version of the sequential Rademacher complexity studied in [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 21, "context": "A simple yet powerful example for the control of the supremum of a stochastic process is an inequality due to Pinelis [22] for the norm (which can be written as a supremum over the dual ball) of a martingale in a 2-smooth Banach space.", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "Instead, we make use of the following result from [24] that extends Lemma 3 at a price of a poly-logarithmic factor.", "startOffset": 50, "endOffset": 54}, {"referenceID": 23, "context": "Before stating the lemma, we briefly define the relevant complexity measures (see [24] for more details).", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "Lemma 4 ([24]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "The behavior of this offset process was shown to govern the optimal rates of convergence for online nonparametric regression [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 21, "context": "To prove it, we use the concentration bound from [22] (Lemma 3) within the proof of the above corollary to remove the extra logarithmic factors.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "For the case of a Hilbert space, the above bound was achieved by [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "This example subsumes and improves upon the recent results from [18, 14] and provides an exact analogue to the PAC Bayesian theorem from statistical learning.", "startOffset": 64, "endOffset": 72}, {"referenceID": 13, "context": "This example subsumes and improves upon the recent results from [18, 14] and provides an exact analogue to the PAC Bayesian theorem from statistical learning.", "startOffset": 64, "endOffset": 72}, {"referenceID": 24, "context": "This extends the study of [25] to supervised learning and a general class of experts F .", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "This is an improvement over the bound in [18] in that the bound is independent of number of experts, and thus holds even for countably infinite sets of experts.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "The KL term in our bound may be compared to the MDL-style term in the bound of [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "If we have a large (but finite) number of experts and take the uniform distribution \u03c0, the above bound provides an improvement over both [14] and [18] for quantile bounds for experts.", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "If we have a large (but finite) number of experts and take the uniform distribution \u03c0, the above bound provides an improvement over both [14] and [18] for quantile bounds for experts.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "Evaluating the above bound with a distribution f that places all its weight on any one expert appears to address the open question posed by [13] of obtaining algorithm-independent oracle-type variance bounds for experts.", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "[1] Lucien Birg\u00e9, Pascal Massart, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] G\u00e1bor Lugosi and Andrew B Nobel.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Peter L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Pascal Massart.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Shahar Mendelson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Alexander Rakhlin and Karthik Sridharan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Elad Hazan and Satyen Kale.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Alexander Rakhlin and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] John Duchi, Elad Hazan, and Yoram Singer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Kamalika Chaudhuri, Yoav Freund, and Daniel J Hsu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Haipeng Luo and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Wouter M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Thomas M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Alexander Rakhlin and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Iosif Pinelis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "To this end on similar lines as in [24, 7, 21], we start with the inner most term as,", "startOffset": 35, "endOffset": 46}, {"referenceID": 6, "context": "To this end on similar lines as in [24, 7, 21], we start with the inner most term as,", "startOffset": 35, "endOffset": 46}, {"referenceID": 20, "context": "To this end on similar lines as in [24, 7, 21], we start with the inner most term as,", "startOffset": 35, "endOffset": 46}, {"referenceID": 23, "context": "Compactness of the sets and lower semi-continuity of the losses and Bn are sufficient, but see [24, 21] for milder conditions.", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "Compactness of the sets and lower semi-continuity of the losses and Bn are sufficient, but see [24, 21] for milder conditions.", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "See [21] for more details of the steps involved in obtaining the above equality.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "To prove the bound in Equation 3, note that, Bn(f ;x1\u2236n, y1\u2236n) = Bn(y1\u2236n) and so, (this proof is similar in spirit to the one in [7])", "startOffset": 129, "endOffset": 132}, {"referenceID": 23, "context": "because 1 n \u2211 n t=1 w k t ( ) \u2264 3\u03b2 k for any by triangle inequality (see [24]).", "startOffset": 73, "endOffset": 77}], "year": 2015, "abstractText": "We propose a general framework for studying adaptive regret bounds in the online learning framework, including model selection bounds and data-dependent bounds. Given a dataor model-dependent bound we ask, \u201cDoes there exist some algorithm achieving this bound?\u201d We show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved. In particular each adaptive rate induces a set of so-called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability. A cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes. Our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second-order data-dependent bounds, and small loss bounds. In addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online PAC-Bayes theorem that holds for countably infinite sets.", "creator": "LaTeX with hyperref package"}}}