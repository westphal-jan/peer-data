{"id": "1508.05817", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2015", "title": "Echoes of Persuasion: The Effect of Euphony in Persuasive Communication", "abstract": "while the effect of various lexical, pictorial, semantic and stylistic features and been addressed surrounding persuasive language from a functional point against view, the persuasive effect of emotion has received little attention. by modeling a notion spanning euphony and analyzing four languages comprising persuasive speakers non - persuasive sentences in different domains ( political speeches, movie quotes, slogans and tweets ), we explore the impact of sounds on different forms allowing persuasiveness. we conduct a series of analyses and prediction experiments within and from generations. our results highlight the positive effects of phonetic devices on persuasion.", "histories": [["v1", "Mon, 24 Aug 2015 14:15:39 GMT  (27kb)", "http://arxiv.org/abs/1508.05817v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CY cs.SI", "authors": ["marco guerini", "g\u00f6zde \u00f6zbal", "carlo strapparava"], "accepted": true, "id": "1508.05817"}, "pdf": {"name": "1508.05817.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["G\u00f6zde \u00d6zbal"], "emails": ["marco.guerini@trentorise.eu", "gozbalde@gmail.com", "strappa@fbk.eu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n05 81\n7v 1\n[ cs\n.C L\n] 2\n4 A\nWhile the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention. By modeling a notion of euphony and analyzing four datasets comprising persuasive and nonpersuasive sentences in different domains (political speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness. We conduct a series of analyses and prediction experiments within and across datasets. Our results highlight the positive role of phonetic devices on persuasion."}, {"heading": "1 Hocus Pocus", "text": "Historically, in human sciences, several definitions of persuasion have been proposed \u2013 see for example (Toulmin, 1958; Walton, 1996; Chaiken, 1980; Cialdini, 1993; Petty and Cacioppo, 1986). Most of them have a common core addressing: methodologies aiming to change the mental state of the receiver by means of communication in view of a possible action to be performed by her/him. (Perelman and Olbrechts-Tyteca, 1969; Moulin et al., 2002).\nThese methodologies might take into account the overall structure of a text such as the ordering of the arguments or simply single word choices. For a successful text both of them are often required. The focus of persuasion may vary according to the goal of the communication and it can take different forms according to the domain: from memorability (e.g.,\nmaking people remember a statement or a product) to diffusion (e.g., making people pass on a content in social networks by sharing it), from behavioral change (e.g., political communication) to influencing purchasing decisions (e.g., slogans to convince people to try or buy a product) \u2013 see for example (Heath and Heath, 2007). While many techniques such as resorting to expert opinion, utilizing the framing effect, emotive language or exaggeration can be used to obtain such persuasive effects, we devote this study to explore particular techniques pertaining to euphony.\nEuphony refers to the inherent pleasantness of the sounds of words, phrases and sentences, and it is utilized to achieve pleasant, rhythmical and harmonious effects. The idea that the pleasantness of the sounds in a sentence can foster its effectiveness is rooted in our culture, and is connected to the concepts of rhythm and music. The fact that language and music interact in our brain has been shown by localizing low-level syntactic processes of music and language in the temporal lobe (Sammler et al., 2013). It has also been shown that changes in the cardiovascular and respiratory systems can be induced by music \u2013 specifically tempo, rhythm, melodic structure (Bernardi et al., 2006). The importance of euphony has its roots also in ancient human psychology. As Julian Jaynes suggests (Jaynes, 2000), poetry used to be divine knowledge. It was the sound and tenor of authorization and it commanded where plain prose could only ask. A paradigmatic example of this conception is the act of casting a spell. Spells (incantations) are special linguistic objects that are meant not only to change how\npeople think or behave but they are also so powerful that they can \u2013 allegedly \u2013 change reality. Spells are often very euphonic (and meaningless) sentences, e.g. \u201cHocus Pocus\u201d.\nVarious psycholinguistic studies addressed the effects of phonetics on the audience in different aspects such as memorability (Wales, 2001; Benczes, 2013) or more specifically advertisement (Leech, 1966; Bergh et al., 1984). There are also computational studies that address the problem of recognizing persuasive sentences according to various syntactic, lexical and semantic features (DanescuNiculescu-Mizil et al., 2012; Tan et al., 2014). However, to the best of our knowledge, the direct impact of phonetic elements on persuasiveness has not been explored in computational settings yet.\nIn this paper, we fill in this gap by conducting a series of analyses and prediction experiments on four datasets representing different aspects of persuasive language to evaluate the importance of a set of phonetic devices (i.e. rhyme, alliteration, homogeneity and plosives) on various forms of persuasiveness. Our experiments show that phonetic features play an important role in the detection of persuasiveness and encode a notion of \u201cmelodious language\u201d that operates both within and across datasets."}, {"heading": "2 Related Work", "text": "In the following, we first revise some NLP studies addressing linguistic features of successful communication. Then, we summarize a selection of studies devoted to the effects of phonetics on persuasion."}, {"heading": "2.1 NLP studies on persuasion", "text": "Berger and Milkman (2009) focus on a particular form of persuasion by using New York Times articles to examine the relationship between virality (i.e., the tendency of a content to be circulated on the Web) and emotions evoked by the content. They conduct semi-automated sentiment analysis to quantify the affectivity and emotionality of each article. Results suggest a strong relationship between affect and virality, in this case measured as the count of how many people emailed each article. As suggested by the authors, this metric represents a form of \u201cnarrowcasting\u201d, as opposed to other \u201cbroadcasting\u201d actions such as sharing on Twitter.\nAnother line of research investigates the impact of various textual features on audience reactions. The work by Guerini et al. (2011) correlates several viral phenomena with the wording of a post, while Guerini et al. (2012) show that features such as the readability level of an abstract influence the number of downloads, bookmarking and citations.\nA particular approach to content virality is presented by Simmons et al. (2011), who explore the impact of different types of modification on memes spreading from one person to another.\nDanescu-Niculescu-Mizil et al. (2012) measure a different ingredient of persuasion by analyzing the features of a movie quote that make it \u201cmemorable\u201d. They compile a corpus consisting of memorable and non-memorable movie quote pairs and conduct a detailed analysis to investigate the lexical and syntactic differences between these pairs.\nLouis and Nenkova (2013) focus on influential science articles in newspapers by considering characteristics such as readability, description vividness, use of unusual words and affective content. High quality articles (NYT articles appearing in \u201cThe Best American Science Writing\u201d anthology) are compared against typical NYT articles.\nBorghol et al. (2012) investigate how differences in textual description affect the spread of contentcontrolled videos. Lakkaraju et al. (2013) focus on the act of resubmissions (i.e., content that is submitted multiple times with multiple titles to multiple different communities) to understand the extent to which each factor influences the success of a content. Tan et al. (2014) consider how content spreads in an on-line community by pinpointing the effect of wording in terms of content informativeness, generality and affect. Althoff et al. (2014) develop a model that can predict the success of requests for a free pizza gifted from the Reddit community. The authors consider high-level textual features such as politeness, reciprocity, narrative and gratitude."}, {"heading": "2.2 Studies on the effects of phonetics", "text": "Benczes (2013) states that alliteration and rhyme can be considered as attention-seeking devices as they enhance emphasis. The author also suggests that they are useful for acceptability and long-term retention of original expressions, decrypting their meanings, indicating informality, and breaking the ice be-\ntween an audience and a speaker. Therefore, these devices are commonly used in original metaphorical and metonymical compounds.\nAccording to Leech (1966), phonetic devices such as rhyme and alliteration are systematically exploited by advertisers to achieve memorability. Similarly, Wales (2001) underlines the effectiveness of alliteration and rhyme on emphasis and memorability of an expression.\nThe relation between the usage of plosives (i.e., consonants in which the vocal tract is blocked so that all airflow ceases, such as \u201cp\u201d, \u201ct\u201d or \u201ck\u201d) and memorability has also been investigated. According to the study carried out by Bergh et al. (1984) brand names starting with plosive sounds are recalled and recognized more than the ones starting with other sounds. O\u0308zbal et al. (2012) carry out an analysis of brand names and discover that plosives are very commonly used.\nDanescu-Niculescu-Mizil et al. (2012), whom we previously mentioned, carry out an auxiliary analysis and observe the differences in letter and sound distribution (e.g. usage of labials or front vowels, back sounds, coordinating conjunctions) of memorable and non-memorable quotes.\nO\u0308zbal et al. (2013) propose a phonetic scorer for creative sentence generation such that generated sentences can contain various phonetic features including alliteration, rhyme and plosive sounds. The authors evaluate the proposed model on automatic slogan generation. In a more recent work (O\u0308zbal et al., 2014), they enforce the existence of these features in the sentences that are automatically generated for second language learning to introduce hooks to echoic memory."}, {"heading": "3 Phonetic Scorer", "text": "For the design of the phonetic features, we were mostly inspired by the work of O\u0308zbal et al. (2013), who built and used three phonetic scorers for creative sentence generation. Similarly to this work, all the phonetic features that we used are based on the phonetic representation of English words of the Carnegie Mellon University pronouncing dictionary1. We selected four classes of phonetic devices,\n1The CMU pronunciation dictionary is freely available at http://www.speech.cs.cmu.edu/cgi-bin/ cmudict. We have used version 0.7a in our implementation.\nnamely plosives, alliteration, rhyme and homogeneity, which can easily be modeled by observing the distribution of specific classes of phonemes within the sentence. The plosive score is calculated as the ratio of the number of plosive sounds in a sentence to the overall number of phonemes. For both alliteration and rhyme scorers, we provide a na\u0131\u0308ve implementation that does not consider stresses or syllables, but only counts the number of repeated sounds at the beginning or end of words in the sentence. The alliteration score is calculated as the number of repeated phonetic prefixes in a sentence normalized by the total number of phonemes. Similarly, the rhyme score is calculated as the ratio of the number of repeated phonetic endings in a sentence to the total number of phonemes. Lastly, the homogeneity scorer simply calculates the degree of homogeneity in terms of phonemes used in a sentence independently from their positions. If we let dph be the count of distinct phonemes and tph be the total count of phonemes in a sentence, then the homogeneity score is calculated as 1\u2212 (dph/tph)."}, {"heading": "4 Dataset", "text": "In this section, we describe the four datasets we used to conduct our analyses and experiments. As we mentioned previously, the definition of persuasion is a debated topic and it can comprise distinct strategies or facets. For this reason, we experimented with datasets where at least one ingredient is clearly in the equation. To explore the effects of wording and euphonics on persuasion, the datasets were built in a controlled setting (topic, author, sentence length) to avoid confounding factors such as author or topic popularity, by following the procedure described in (Danescu-Niculescu-Mizil et al., 2012; Tan et al., 2014). In addition, these datasets comprise short texts (mostly single sentences) to focus on surface realization of persuasion, where strategic planning \u2013 which might act as a confounding factor \u2013 plays a minor role. The idea of using controlled experiments (usually in an A/B test setting) to study persuasive communication can be traced back at least to Hovland et al. (1953). While two of these datasets (Twitter and Movies) were already available, the other two (CORPS and Slogans) were collected by following the methodology proposed in the first two as\nclosely as possible2. All datasets are built around the core idea of collecting pairs consisting of a persuasive sentence (P ) and a non-persuasive counterpart (\u00acP ), where P and \u00acP are structurally very similar and controlled for the above mentioned confounding factors.\nTwitter. A set of 11,404 tweet pairs, where each pair comes from the same user (author control) and contains the same URL (topic control). P and \u00acP are determined based on their retweet counts (Tan et al., 2014). It is worth noting that in our experiments we were able to collect only 11,019 of such tweet pairs since some of them were deleted in the meanwhile.\nMovie. A set of 2,198 single-sentence memorable movie quotes (P ) paired with non-memorable quotes (\u00acP ). For each P , the dataset contains a contrasting quote \u00acP from the same movie such that (i) P and \u00acP are uttered by the same speaker, (ii) P and \u00acP have the same number of words, (iii) \u00acP does not occur in the IMDb list of memorable quotes and (iv) P and \u00acP are as close as possible to each other in the script (Danescu-Niculescu-Mizil et al., 2012).\nCORPS. A set of 2,600 sentence pairs uttered by various politicians. We collected these pairs from CORPS, a freely available corpus of political speeches tagged with audience reactions (Guerini et al., 2013). The methodology that we used to build the pairs is very similar to Danescu-NiculescuMizil et al. (2012): for each P , where P is the sentence preceding an audience reaction (e.g. APPLAUSE, LAUGHTER), we selected a contrasting single-sentence \u00acP from the same speech. We required \u00acP to be close to P in the speech transcription, subject to the conditions that (i) P and \u00acP are uttered by the same speaker - which is trivial since these are monologues, where a single speaker is addressing the audience - (ii) P and \u00acP have the same number of words, and (iii) \u00acP is 5 to 15 sentences away from P . This last condition had to be imposed since, differently from movie quotes, we do not have the evidence of which fragment of the speech exactly provoked the audience reaction (i.e. it could be the combination of more than one sentence).\nSlogan. A set of 1,533 slogans taken from on-\n2CORPS and Slogans datasets can be downloaded at the following link: https://github.com/marcoguerini/ paired_datasets_for_persuasion/\nline resources paired with non-slogans that are similar in content. We collected the non-slogans from the subset of the New York Times articles in English GigaWord \u2013 5th Edition \u2013 released by Linguistic Data Consortium (LDC)3. For each slogan, we picked the most similar sentence in the New York Times articles having the same length and the highest LSA similarity (Deerwester et al., 1990) with the slogan. The LSA similarity approach that we used to collect the non-slogans is very similar to the approach used by Louis and Nenkova (2013) to collect the non-persuasive counterparts of successful news articles.\nIn Table 1, we sum up the criteria used in the construction of each dataset. As can be observed from the table, each dataset satisfies at least two of the three criteria described above. In the last two\ncolumns of the table, we also provide the average token length of the persuasive and non-persuasive sentences in each dataset. Finally, in Table 2 we provide examples of euphonic and persuasive sentences for each dataset together with their phonetic scores."}, {"heading": "5 Data Analysis", "text": "To provide a first insight on the data, in Table 3 we report the average phonetic scores for each data set (Mann-Whitney U Test is used for statistical significance between P and \u00acP samples, with Bonferroni correction to ameliorate issues with multiple comparisons). The results are partially in line with our expectations of the euphony phenomena being more relevant in the persuasive sentences across the datasets.\nAs can be observed from the table, the average rhyme scores are higher in persuasive sentences and\n3http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2011T07\nthe difference is highly significant for Slogan and Twitter (p < .001), slightly significant for Movie quotes (p < .05), but not significant for CORPS. The average alliteration scores are again higher in persuasive sentences and all the differences are highly significant in all datasets (apart from CORPS with p < .01). Plosives seem not to correlate well with our intuition of persuasiveness and euphony: either there is no significance (movie quotes) or the averages of euphonic scores are higher in the nonpersuasive sentences (the difference is highly significant in slogans, and significant in Twitter). The only dataset that meets our expectation is CORPS with a highly significant difference in favor of persuasive sentences. Finally, the average homogeneity scores are significantly (p < .001) higher in persuasive sentences in all datasets except CORPS, where the scores of non-persuasive sentences are significantly higher (p < .01) than persuasive ones.\nWithout going into details of cross-dataset comparisons we would like to note that CORPS seems a very peculiar dataset in terms of average scores, as compared to the others. In terms of rhyme and alliteration, the average scores of non-persuasive\nsentences (\u00acP ) in CORPS are always higher than the persuasive sentences (P ) in the other datasets (p < .001 in all cases), while for homogeneity the same holds apart from Twitter. These results may derive from the fact that a political speech is a carefully crafted text \u2013 aimed at influencing the audience \u2013 in its entirety, so also \u201cnon-persuasive\u201d sentences in CORPS are on average more persuasive than in other datasets.\nAs a next step, we conducted another analysis on the distribution of \u201cextreme cases\u201d, i.e. sentences that have a very high phonetic score at least in one feature. This analysis derives from the intuition that a euphonic sentence might be recognized as such by humans only if its phonetic scores are above a certain threshold. In fact, sound repetition in a sentence may occur by chance, as in \u201cI saw the knife in the drawer\u201d, and the longer the sentence is, the higher the probability that phonetic scores will be non-zero even in absence of a euphonic effect. Therefore, the average scores for each phonetic device, as reported in Table 3, are only partially informative.\nGiven this premise, to evaluate the \u201cpersuasive power\u201d of the phonetic devices taken into account,\nwe compare them in terms of empirical Complementary Cumulative Distribution Functions (CCDFs) of the persuasive/non-persuasive pairs in various datasets. These functions are commonly used to analyze online social networks in terms of growth in size and activity (see for example (Ahn et al., 2007; Jiang et al., 2010; Leskovec, 2008)) and also for measuring content diffusion, e.g. the number of retweets of a given content (Kwak et al., 2010). Here, we use CCDFs to account for the probability P that the score of a phonetic device d will be greater than n indicating it with F\u0302d(n). For example, the probability of having a text with more than .75 rhyme score is indicated with F\u0302rh(.75) = P(#rhyme > .75). To assess whether the CCDFs of the several types of texts we take into account show significant differences, we use the Kolmogorov-Smirnov goodnessof-fit test, which specifically targets cumulative distribution functions. In particular, for each phonetic device and dataset, we use a two-tailed KolmogorovSmirnov test (again with Bonferroni correction) to test whether the number of examples above the threshold is higher in the persuasive sentences than in their non-persuasive counterparts for that device.\nSince we do not have a theoretical way to define such thresholds, we resort to empirically define them by using a specific dataset of euphonic sentences. Even if it might seem reasonable to consider poems as paradigmatic examples of \u201ceuphonic\u201d writing, we discard them as the phonetic devices used in poems may span across sentences. Instead, we resort to tongue twisters as a gold reference of how a euphonic sentence should be. Accordingly, we collected a set of 534 tongue twisters from various on-\nline resources. Then, for each phonetic index we defined our thresholds as the average of the phonetic scores in this data, in particular: trh = 0.55 for rhyme, tal = 0.58 for alliteration, tpl = 0.20 for plosives and tho = 0.68 for homogeneity.\nIn Table 4, we report the results of our CCDF analysis. After analyzing the \u201cextreme cases\u201d, where euphony is granted, we see that the trends found in Table 3 on the correlation between persuasiveness and euphony are confirmed and strengthened. The number of persuasive sentences with a rhyme score above threshold is 30% more than the non-persuasive ones in CORPS, while the difference is 90% in Twitter4. The ratio of persuasive sentences above threshold to non-persuasive ones is very high in movies and slogans (more than 2 and 10 respectively). All results are either highly significant or significant. For comparison, in Table 3 these differences are not significant for CORPS and only slightly significant (p < .05) for movies. Concerning alliteration, there are 85% more cases above threshold in the persuasive sentences of CORPS than the non-persuasive ones. For movie quotes and Twitter, the persuasive sentences above threshold are more than two times as many as the non-persuasive ones, while the ratio is more than 13 for slogans. All results are either highly significant or significant in line with the results of Table 3. Instead, for plosive scores we observe a negative or no correlation with persuasiveness, the only exception being CORPS. Regarding homogeneity, for CORPS the difference between persuasive and non-persuasive sentences is not significant (in Table 3 it was significantly in favor of non-persuasive sentences), while for the other datasets there is a highly significant difference in favor of persuasive sentences (between 20% and 80%). As a whole, these results confirm our intuition that phonetic features play a significant role with respect to persuasiveness. In the next section we will validate this claim by means of prediction experiments."}, {"heading": "6 Prediction Experiments", "text": "In this section, we describe the prediction tasks (both within and across datasets) that we carried out to in-\n4In the following the ratios are computed on the real values while Table 4 presents the rounded values.\nvestigate the impact of the phonetic features on the detection of various forms of persuasiveness. We compare three different sets of features, namely phonetic, n-grams and their combination to understand whether phonetic information can improve the performance of standard lexical approaches. Similarly to Danescu-Niculescu-Mizil et al. (2012) and Tan et al. (2014), we formulate a pairwise classification problem such that given a pair (s1, s2) consisting of sentences s1 and s2, the goal is to determine the more persuasive one (i.e., the one on the left or right). We can consider this as a binary classification task where for each instance (i.e., pair) the possible labels are left or right."}, {"heading": "6.1 Dataset and preprocessing", "text": "For the prediction experiments, we used the four datasets described in Section 4 (i.e., CORPS, Twitter, Slogan and Movie), all of which consist of a persuasive sentence P and its non-persuasive counterpart (\u00acP ) labeled as either left or right. To make the positions of the sentences in a pair irrelevant (i.e. to provide symmetry), for each instance occurring in the original datasets (e.g., (s1, s2) with label left), we added another instance including the same sentence pair in reverse order (i.e., (s2, s1) with label right). As a preprocessing step, all the sentences were tokenized by using Stanford CoreNLP (Manning et al., 2014)."}, {"heading": "6.2 Classifier and features", "text": "We performed a 10-fold cross-validation on each dataset and experimented with three feature sets by using a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995). We preferred SVM as a classifier due to its characteristic property to especially perform well on high-dimensional data (Weichselbraun et al., 2011).\nThe first feature set consists of the phonetic features (i.e. plosive, alliteration, rhyme and homogeneity scores as detailed in Section 3). The second feature set is a standard bag of word n-grams including unigrams, bigrams and trigrams. All the nonascii characters, punctuations and numbers were ignored. The URLs and mentions in Twitter data were replaced with tags (i.e. URL and MENTION respectively). In addition, for the unigram features, stop words were filtered out. We did not apply this\nfiltering for bigrams and trigrams to capture longerrange usage patterns such as propositional phrases. The third feature set is simply the union of both phonetic and n-gram features.\nTo find the best configuration for each dataset and feature set, we conducted a grid search over the degree of the polynomial kernel (1 or 2) and the number of features to be used (in the range between 1,000 and 20,000). Due to the low dimensionality of the phonetic feature set, feature selection was performed only for the feature sets including n-grams. The selection was performed based on the information gain of each feature."}, {"heading": "6.3 Within-dataset experiments", "text": "For this set of experiments, we conducted a 10-fold cross validation on each dataset separately. In Table 5, for each dataset listed in the first column, in the subsequent columns we report the performance of the best model obtained with 10-fold cross validation using i) only phonetic features (Phonetic), ii) only n-grams (N-Gram), iii) both phonetic and ngram features (All). As mentioned previously, for each pair (s1, s2) consisting of sentences s1 and s2, our dataset contains another pair including the same sentences in reverse order (i.e., (s2, s1)), resulting in a symmetric and balanced dataset. Therefore, classification performance is measured in terms of accuracy (i.e., the percentage of pairs of which labels were correctly predicted). For each accuracy value, we also report in parenthesis the number of features selected and the kernel degree of the corresponding model. While the kernel degree did not make a big difference in the performance, the number of selected features had an important effect on the accuracy of the models. As can be observed from these values, the best performance on all the datasets is achieved with a relatively small number of features.\nAmong the values reported in the table, the ones followed by \u2217\u2217\u2217 are significantly different (p < .001)\nfrom the ones to their left, while \u2020 represents no significance, as calculated according to McNemar\u2019s test (McNemar, 1947). For each dataset, the weakest models (i.e. the ones using only the phonetic features in all cases) are still significantly (p < .001) more accurate than a random baseline (accuracy = 50%). As can be observed from the table, the models using only n-grams significantly outperform the ones only based on phonetic features in all datasets. However, while the phonetic features are not very strong by themselves, their combination with ngrams results in models outperforming the n-gram based models in all cases. The difference is highly significant for all datasets except CORPS, where ngrams alone are sufficient to achieve a good performance. We speculate that the kind of persuasiveness used in political speeches is more dependent on the lexical choices of the speaker and on the use of a specific set of semantically loaded words such as bless, victory, God and justice or military. This is in line with the work of Guerini et al. (2008), who built a domain specific lexicon to study the persuasive impact of words in political speeches.\nWe also conducted an additional set of experiments to investigate if some phonetic features stand out among the others, and to find out the contribution and importance of each phonetic feature in isolation. To achieve that, for each dataset we conducted a 10-fold cross validation to obtain the best four models containing a single phonetic feature on top of n-gram features (i.e. N-Gram+Rhyme,\nN-Gram+Plosive, N-Gram+Homogeneity and NGram+Alliteration). In Table 6, we report the accuracy of the n-gram model and these four models for each dataset. Similarly to Table 5, for each accuracy value, we also report in parenthesis the number of features selected and the kernel degree of the corresponding model obtained with grid search. The results demonstrate that homegeneity is the most effective feature when added on top of n-grams, resulting in highly significant improvement against the basic n-gram models in three out of four datasets. Alliteration and rhyme closely follow homogeneity by yielding models that significantly outperform the ngram models in three and two datasets respectively. Finally, the models containing plosives do not improve over the n-gram models in any of the four datasets. It is worth noting that in CORPS none of the n-gram models enriched with phonetic features improves over the basic n-gram models as in line with the results of the within-dataset experiments reported in Table 5."}, {"heading": "6.4 Cross-dataset experiments", "text": "After observing that the combination of phonetic and n-gram features can be effective in the withindataset prediction experiments, we took a further step and investigated the interaction of the three feature sets across datasets. More specifically, we classified each dataset with the best models (one for each feature set) trained on the other datasets. With these experiments, we investigated the ability of phonetic\nfeatures to generalize across the different lexicons of the datasets. As we discussed previously, the four datasets represent different forms of persuasiveness. In this respect, the results of the cross-dataset experiments can also be interpreted as a measure of the degree of compatibility among these kinds of persuasiveness.\nIn Table 7, we present the results of the crossdataset prediction experiments. For each training and test set pair, we report the accuracy of the best models, one for each feature set, based on crossvalidation on the training set. As can be observed from the table, the figures are generally low and various domain adaptation techniques could be employed to improve the results. However, the objective of this evaluation is not to train an optimized cross-domain classifier, but to assess the potential of the feature sets to model different kinds of persuasiveness.\nAs expected, n-gram features show poor performance due to the lexical and stylistic differences among the datasets. In many cases, the phonetic models outperform the n-gram models, and in several cases the combination of the two feature sets deteriorates the performance of the phonetic features alone. These findings support our hypothesis that phonetic features, due to their generality, have better correlation with different forms of persuasiveness than lexical features. The experiments involving the CORPS dataset, both for training and testing, do not share this behavior. Indeed, when CORPS is used as a training or test dataset, the performance of the models is quite low (very close to or worse than the baseline in many cases) independently from the feature sets. These results suggest that the notion of persuasiveness encoded in this dataset is remarkably different from the others, as previously discussed in the data analysis in Section 5. As seen in the within dataset experiments (see Table 5), CORPS is the only dataset in which the combination of lexical and phonetic features do not improve the classification accuracy. This explains the inability of the phonetic features to improve the accuracy in cross-dataset experiments when this dataset is employed."}, {"heading": "7 Conclusion", "text": "In this paper, we focused on the impact of a set of phonetic features \u2013 namely rhyme, alliteration,\nhomogeneity and plosives \u2013 on various forms of persuasiveness including memorability of slogans and movie quotes, re-tweet counts of tweets, and effectiveness of political speeches. We conducted our analysis and experiments on four datasets comprising pairs of a persuasive sentence and a nonpersuasive counterpart.\nOur data analysis shows that persuasive sentences are generally euphonic. This finding is confirmed by the prediction experiments, in which we observed that phonetic features consistently help in the detection of persuasiveness. When combined with lexical features, they help improving classification performance on three of the four datasets that we considered. The key role played by phonetic features is further underlined by the cross-dataset experiments, in which we observed that phonetic features alone generally outperform the lexical features. To the best of our knowledge, this is the first systematic analysis of the impact of phonetic features on several types of persuasiveness. Our results should encourage researchers dealing with different aspects of persuasiveness to consider the inclusion of phonetic attributes in their models.\nAs future work, we will investigate the impact of other phonetic devices such as assonance, consonance and rhythm on persuasiveness. It would also be interesting to focus on the connection between sound symbolism and persuasiveness, and investigate how the context or domain of persuasive statements interacts with the sounds in those statements.\nWe would like to conclude this paper with the most favorite and retweeted tweet of @NAACL2015 (the Twitter account of the conference whose proceedings comprise this paper), which is a good example of the positive effect of euphony in persuasiveness:\nThe deadline for @NAACL2015 paper submissions is approaching:\nRemember, remember, the 4th of December!"}, {"heading": "Acknowledgments", "text": "This work has been partially supported by the Trento RISE PerTe project."}], "references": [{"title": "Analysis of topological characteristics of huge online social networking services", "author": ["Yong-Yeol Ahn", "Seungyeop Han", "Haewoon Kwak", "Sue Moon", "Hawoong Jeong."], "venue": "Proceedings of the 16th international conference on World Wide Web, pages 835\u2013844. ACM.", "citeRegEx": "Ahn et al\\.,? 2007", "shortCiteRegEx": "Ahn et al\\.", "year": 2007}, {"title": "How to ask for a favor: A case study on the success of altruistic requests", "author": ["Tim Althoff", "Cristian Danescu-Niculescu-Mizil", "Dan Jurafsky."], "venue": "Proocedings of ICWSM.", "citeRegEx": "Althoff et al\\.,? 2014", "shortCiteRegEx": "Althoff et al\\.", "year": 2014}, {"title": "The role of alliteration and rhyme in novel metaphorical and metonymical compounds", "author": ["Rka Benczes."], "venue": "Metaphor and Symbol, 28(3):167\u2013184.", "citeRegEx": "Benczes.,? 2013", "shortCiteRegEx": "Benczes.", "year": 2013}, {"title": "Social Transmission, Emotion, and the Virality of Online Content", "author": ["Jonah A. Berger", "Katherine L. Milkman."], "venue": "Social Science Research Network Working Paper Series, December.", "citeRegEx": "Berger and Milkman.,? 2009", "shortCiteRegEx": "Berger and Milkman.", "year": 2009}, {"title": "Sound advice on brand names", "author": ["Bruce G. Vanden Bergh", "Janay Collins", "Myrna Schultz", "Keith Adler."], "venue": "Journalism Quarterly, 61(4):835, dec.", "citeRegEx": "Bergh et al\\.,? 1984", "shortCiteRegEx": "Bergh et al\\.", "year": 1984}, {"title": "Cardiovascular, cerebrovascular, and respiratory changes induced by different types of music in musicians and non-musicians: the importance of silence", "author": ["Luciano Bernardi", "Cesare Porta", "Peter Sleight."], "venue": "Heart, 92(4):445\u2013452.", "citeRegEx": "Bernardi et al\\.,? 2006", "shortCiteRegEx": "Bernardi et al\\.", "year": 2006}, {"title": "The untold story of the clones: Content-agnostic factors that impact youtube video popularity", "author": ["Youmna Borghol", "Sebastien Ardon", "Niklas Carlsson", "Derek Eager", "Anirban Mahanti."], "venue": "Proceedings of the 18th ACM SIGKDD international conference on", "citeRegEx": "Borghol et al\\.,? 2012", "shortCiteRegEx": "Borghol et al\\.", "year": 2012}, {"title": "Heuristic vs", "author": ["Shelly Chaiken."], "venue": "sistematic information processing and the use of source vs message cues in persuasion. Journal of Personality and Social Psychology, 39:752\u2013766.", "citeRegEx": "Chaiken.,? 1980", "shortCiteRegEx": "Chaiken.", "year": 1980}, {"title": "Influence", "author": ["Robert B. Cialdini."], "venue": "The psychology of persuasion. William Morrow & Company, Inc., New York.", "citeRegEx": "Cialdini.,? 1993", "shortCiteRegEx": "Cialdini.", "year": 1993}, {"title": "Supportvector networks", "author": ["Corinna Cortes", "Vladimir Vapnik."], "venue": "Mach. Learn., 20(3):273\u2013297, September.", "citeRegEx": "Cortes and Vapnik.,? 1995", "shortCiteRegEx": "Cortes and Vapnik.", "year": 1995}, {"title": "You had me at hello: How phrasing affects memorability", "author": ["Cristian Danescu-Niculescu-Mizil", "Justin Cheng", "Jon Kleinberg", "Lillian Lee."], "venue": "Proceedings of the ACL.", "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? 2012", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2012}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013 407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Corps: A corpus of tagged political speeches", "author": ["Marco Guerini", "Carlo Strapparava", "Oliviero Stock"], "venue": null, "citeRegEx": "Guerini et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2008}, {"title": "Exploring text virality in social networks", "author": ["Marco Guerini", "Carlo Strapparava", "G\u00f6zde \u00d6zbal."], "venue": "Proceedings of ICWSM-11, Barcelona, Spain, July.", "citeRegEx": "Guerini et al\\.,? 2011", "shortCiteRegEx": "Guerini et al\\.", "year": 2011}, {"title": "Do linguistic style and readability of scientific abstracts affect their virality", "author": ["Marco Guerini", "Alberto Pepe", "Bruno Lepri."], "venue": "Proceedings of ICWSM-12.", "citeRegEx": "Guerini et al\\.,? 2012", "shortCiteRegEx": "Guerini et al\\.", "year": 2012}, {"title": "The new release of corps: A corpus of political speeches annotated with audience reactions", "author": ["Marco Guerini", "Danilo Giampiccolo", "Giovanni Moretti", "Rachele Sprugnoli", "Carlo Strapparava."], "venue": "Multimodal Communication in Political Speech. Shaping Minds", "citeRegEx": "Guerini et al\\.,? 2013", "shortCiteRegEx": "Guerini et al\\.", "year": 2013}, {"title": "Made to stick: Why some ideas survive and others die", "author": ["Chip Heath", "Dan Heath."], "venue": "Random House.", "citeRegEx": "Heath and Heath.,? 2007", "shortCiteRegEx": "Heath and Heath.", "year": 2007}, {"title": "The origin of consciousness in the breakdown of the bicameral mind", "author": ["Julian Jaynes."], "venue": "Houghton Mifflin Harcourt.", "citeRegEx": "Jaynes.,? 2000", "shortCiteRegEx": "Jaynes.", "year": 2000}, {"title": "Understanding latent interactions in online social networks", "author": ["Jing Jiang", "Christo Wilson", "Xiao Wang", "Peng Huang", "Wenpeng Sha", "Yafei Dai", "Ben Y Zhao."], "venue": "Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pages 369\u2013382.", "citeRegEx": "Jiang et al\\.,? 2010", "shortCiteRegEx": "Jiang et al\\.", "year": 2010}, {"title": "What is twitter, a social network or a news media? In Proceedings of the 19th international conference on World wide web, pages 591\u2013600", "author": ["Haewoon Kwak", "Changhyun Lee", "Hosung Park", "Sue Moon."], "venue": "ACM.", "citeRegEx": "Kwak et al\\.,? 2010", "shortCiteRegEx": "Kwak et al\\.", "year": 2010}, {"title": "What\u2019s in a name? understanding the interplay between titles, content, and communities in social media", "author": ["Himabindu Lakkaraju", "Julian J McAuley", "Jure Leskovec."], "venue": "ICWSM.", "citeRegEx": "Lakkaraju et al\\.,? 2013", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2013}, {"title": "English in advertising : a linguistic study of advertising in Great Britain / [by] Geoffrey N", "author": ["Geoffrey N. Leech."], "venue": "Leech. Longmans London.", "citeRegEx": "Leech.,? 1966", "shortCiteRegEx": "Leech.", "year": 1966}, {"title": "Dynamics of large networks", "author": ["Jurij Leskovec."], "venue": "ProQuest.", "citeRegEx": "Leskovec.,? 2008", "shortCiteRegEx": "Leskovec.", "year": 2008}, {"title": "What makes writing great? first experiments on article quality prediction in the science journalism domain", "author": ["Annie Louis", "Ani Nenkova."], "venue": "TACL, 1:341\u2013 352.", "citeRegEx": "Louis and Nenkova.,? 2013", "shortCiteRegEx": "Louis and Nenkova.", "year": 2013}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Note on the sampling error of the difference between correlated proportions or percentages", "author": ["Quinn McNemar."], "venue": "Psychometrika, 12(2):153\u2013157.", "citeRegEx": "McNemar.,? 1947", "shortCiteRegEx": "McNemar.", "year": 1947}, {"title": "Brand pitt: A corpus to explore the art of naming", "author": ["G\u00f6zde \u00d6zbal", "Carlo Strapparava", "Marco Guerini."], "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012). European Language Resources Associ-", "citeRegEx": "\u00d6zbal et al\\.,? 2012", "shortCiteRegEx": "\u00d6zbal et al\\.", "year": 2012}, {"title": "BRAINSUP: Brainstorming Support for Creative Sentence Generation", "author": ["G\u00f6zde \u00d6zbal", "Daniele Pighin", "Carlo Strapparava."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1446\u20131455, Sofia, Bul-", "citeRegEx": "\u00d6zbal et al\\.,? 2013", "shortCiteRegEx": "\u00d6zbal et al\\.", "year": 2013}, {"title": "Automation and evaluation of the keyword method for second language learning", "author": ["G\u00f6zde \u00d6zbal", "Daniele Pighin", "Carlo Strapparava."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-", "citeRegEx": "\u00d6zbal et al\\.,? 2014", "shortCiteRegEx": "\u00d6zbal et al\\.", "year": 2014}, {"title": "The new Rhetoric: a treatise on Argumentation", "author": ["Chaim Perelman", "Lucie Olbrechts-Tyteca."], "venue": "Notre Dame Press.", "citeRegEx": "Perelman and Olbrechts.Tyteca.,? 1969", "shortCiteRegEx": "Perelman and Olbrechts.Tyteca.", "year": 1969}, {"title": "The elaboration likelihood model of persuasion", "author": ["Richard E. Petty", "John T. Cacioppo."], "venue": "Advances in Experimental Social Psychology, 19:123\u2013205.", "citeRegEx": "Petty and Cacioppo.,? 1986", "shortCiteRegEx": "Petty and Cacioppo.", "year": 1986}, {"title": "Co-localizing linguistic and musical syntax with intracranial eeg", "author": ["Daniela Sammler", "Stefan Koelsch", "Tonio Ball", "Armin Brandt", "Maren Grigutsch", "Hans-J\u00fcrgen Huppertz", "Thomas R Kn\u00f6sche", "J\u00f6rg Wellmer", "Guido Widman", "Christian E Elger"], "venue": null, "citeRegEx": "Sammler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sammler et al\\.", "year": 2013}, {"title": "Memes online: Extracted, subtracted, injected, and recollected", "author": ["Matthew Simmons", "Lada A Adamic", "Eytan Adar."], "venue": "Proceedings of ICWSM-11.", "citeRegEx": "Simmons et al\\.,? 2011", "shortCiteRegEx": "Simmons et al\\.", "year": 2011}, {"title": "The effect of wording on message propagation: Topic-and author-controlled natural experiments on twitter", "author": ["Chenhao Tan", "Lillian Lee", "Bo Pang."], "venue": "ACL.", "citeRegEx": "Tan et al\\.,? 2014", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "The Use of Arguments", "author": ["Stephen Toulmin."], "venue": "Cambridge University Press, Cambridge MA.", "citeRegEx": "Toulmin.,? 1958", "shortCiteRegEx": "Toulmin.", "year": 1958}, {"title": "A dictionary of stylistics", "author": ["Katie Wales."], "venue": "Harlow, Eng. ; New York : Longman, 2nd ed edition. Includes bibliographical references (p. 413-429).", "citeRegEx": "Wales.,? 2001", "shortCiteRegEx": "Wales.", "year": 2001}, {"title": "Argumentation Schemes for Presumptive Reasoning", "author": ["Douglas N. Walton."], "venue": "Lawrence Erlbaum Associates, Mahwah, New Jersey.", "citeRegEx": "Walton.,? 1996", "shortCiteRegEx": "Walton.", "year": 1996}, {"title": "Using games with a purpose and bootstrapping to create domain-specific sentiment lexicons", "author": ["Albert Weichselbraun", "Stefan Gindl", "Arno Scharl."], "venue": "Proceedings of the 20th ACM International Conference on", "citeRegEx": "Weichselbraun et al\\.,? 2011", "shortCiteRegEx": "Weichselbraun et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 34, "context": "Historically, in human sciences, several definitions of persuasion have been proposed \u2013 see for example (Toulmin, 1958; Walton, 1996; Chaiken, 1980; Cialdini, 1993; Petty and Cacioppo, 1986).", "startOffset": 104, "endOffset": 190}, {"referenceID": 36, "context": "Historically, in human sciences, several definitions of persuasion have been proposed \u2013 see for example (Toulmin, 1958; Walton, 1996; Chaiken, 1980; Cialdini, 1993; Petty and Cacioppo, 1986).", "startOffset": 104, "endOffset": 190}, {"referenceID": 7, "context": "Historically, in human sciences, several definitions of persuasion have been proposed \u2013 see for example (Toulmin, 1958; Walton, 1996; Chaiken, 1980; Cialdini, 1993; Petty and Cacioppo, 1986).", "startOffset": 104, "endOffset": 190}, {"referenceID": 8, "context": "Historically, in human sciences, several definitions of persuasion have been proposed \u2013 see for example (Toulmin, 1958; Walton, 1996; Chaiken, 1980; Cialdini, 1993; Petty and Cacioppo, 1986).", "startOffset": 104, "endOffset": 190}, {"referenceID": 30, "context": "Historically, in human sciences, several definitions of persuasion have been proposed \u2013 see for example (Toulmin, 1958; Walton, 1996; Chaiken, 1980; Cialdini, 1993; Petty and Cacioppo, 1986).", "startOffset": 104, "endOffset": 190}, {"referenceID": 29, "context": "(Perelman and Olbrechts-Tyteca, 1969; Moulin et al., 2002).", "startOffset": 0, "endOffset": 58}, {"referenceID": 16, "context": ", slogans to convince people to try or buy a product) \u2013 see for example (Heath and Heath, 2007).", "startOffset": 72, "endOffset": 95}, {"referenceID": 31, "context": "been shown by localizing low-level syntactic processes of music and language in the temporal lobe (Sammler et al., 2013).", "startOffset": 98, "endOffset": 120}, {"referenceID": 5, "context": "It has also been shown that changes in the cardiovascular and respiratory systems can be induced by music \u2013 specifically tempo, rhythm, melodic structure (Bernardi et al., 2006).", "startOffset": 154, "endOffset": 177}, {"referenceID": 17, "context": "As Julian Jaynes suggests (Jaynes, 2000), poetry used to be divine knowledge.", "startOffset": 26, "endOffset": 40}, {"referenceID": 35, "context": "Various psycholinguistic studies addressed the effects of phonetics on the audience in different aspects such as memorability (Wales, 2001; Benczes, 2013) or more specifically advertisement (Leech, 1966; Bergh et al.", "startOffset": 126, "endOffset": 154}, {"referenceID": 2, "context": "Various psycholinguistic studies addressed the effects of phonetics on the audience in different aspects such as memorability (Wales, 2001; Benczes, 2013) or more specifically advertisement (Leech, 1966; Bergh et al.", "startOffset": 126, "endOffset": 154}, {"referenceID": 21, "context": "Various psycholinguistic studies addressed the effects of phonetics on the audience in different aspects such as memorability (Wales, 2001; Benczes, 2013) or more specifically advertisement (Leech, 1966; Bergh et al., 1984).", "startOffset": 190, "endOffset": 223}, {"referenceID": 4, "context": "Various psycholinguistic studies addressed the effects of phonetics on the audience in different aspects such as memorability (Wales, 2001; Benczes, 2013) or more specifically advertisement (Leech, 1966; Bergh et al., 1984).", "startOffset": 190, "endOffset": 223}, {"referenceID": 33, "context": "There are also computational studies that address the problem of recognizing persuasive sentences according to various syntactic, lexical and semantic features (DanescuNiculescu-Mizil et al., 2012; Tan et al., 2014).", "startOffset": 160, "endOffset": 215}, {"referenceID": 12, "context": "The work by Guerini et al. (2011) correlates several viral phenomena with the wording of a post, while Guerini et al.", "startOffset": 12, "endOffset": 34}, {"referenceID": 12, "context": "The work by Guerini et al. (2011) correlates several viral phenomena with the wording of a post, while Guerini et al. (2012) show that features such as the readability level of an abstract influence the number of downloads, bookmarking and citations.", "startOffset": 12, "endOffset": 125}, {"referenceID": 32, "context": "A particular approach to content virality is presented by Simmons et al. (2011), who explore the impact of different types of modification on memes spreading from one person to another.", "startOffset": 58, "endOffset": 80}, {"referenceID": 1, "context": "Althoff et al. (2014) develop a model that can predict the success of requests for a free pizza gifted from the Reddit community.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "In a more recent work (\u00d6zbal et al., 2014), they enforce the existence of these features in the sentences that are automatically gener-", "startOffset": 22, "endOffset": 42}, {"referenceID": 19, "context": "According to Leech (1966), phonetic devices such as rhyme and alliteration are systematically exploited by advertisers to achieve memorability.", "startOffset": 13, "endOffset": 26}, {"referenceID": 19, "context": "According to Leech (1966), phonetic devices such as rhyme and alliteration are systematically exploited by advertisers to achieve memorability. Similarly, Wales (2001) underlines the effectiveness of alliteration and rhyme on emphasis and memorability of an expression.", "startOffset": 13, "endOffset": 168}, {"referenceID": 4, "context": "According to the study carried out by Bergh et al. (1984) brand names starting with plosive sounds are recalled and recognized more than the ones starting with other sounds.", "startOffset": 38, "endOffset": 58}, {"referenceID": 4, "context": "According to the study carried out by Bergh et al. (1984) brand names starting with plosive sounds are recalled and recognized more than the ones starting with other sounds. \u00d6zbal et al. (2012) carry out an analysis of brand names and discover that plosives are very commonly used.", "startOffset": 38, "endOffset": 194}, {"referenceID": 4, "context": "According to the study carried out by Bergh et al. (1984) brand names starting with plosive sounds are recalled and recognized more than the ones starting with other sounds. \u00d6zbal et al. (2012) carry out an analysis of brand names and discover that plosives are very commonly used. Danescu-Niculescu-Mizil et al. (2012), whom we previously mentioned, carry out an auxiliary analysis and observe the differences in letter and sound distribution (e.", "startOffset": 38, "endOffset": 320}, {"referenceID": 4, "context": "According to the study carried out by Bergh et al. (1984) brand names starting with plosive sounds are recalled and recognized more than the ones starting with other sounds. \u00d6zbal et al. (2012) carry out an analysis of brand names and discover that plosives are very commonly used. Danescu-Niculescu-Mizil et al. (2012), whom we previously mentioned, carry out an auxiliary analysis and observe the differences in letter and sound distribution (e.g. usage of labials or front vowels, back sounds, coordinating conjunctions) of memorable and non-memorable quotes. \u00d6zbal et al. (2013) propose a phonetic scorer for creative sentence generation such that generated sentences can contain various phonetic features including alliteration, rhyme and plosive sounds.", "startOffset": 38, "endOffset": 583}, {"referenceID": 26, "context": "For the design of the phonetic features, we were mostly inspired by the work of \u00d6zbal et al. (2013), who built and used three phonetic scorers for creative sentence generation.", "startOffset": 80, "endOffset": 100}, {"referenceID": 10, "context": "To explore the effects of wording and euphonics on persuasion, the datasets were built in a controlled setting (topic, author, sentence length) to avoid confounding factors such as author or topic popularity, by following the procedure described in (Danescu-Niculescu-Mizil et al., 2012; Tan et al., 2014).", "startOffset": 249, "endOffset": 305}, {"referenceID": 33, "context": "To explore the effects of wording and euphonics on persuasion, the datasets were built in a controlled setting (topic, author, sentence length) to avoid confounding factors such as author or topic popularity, by following the procedure described in (Danescu-Niculescu-Mizil et al., 2012; Tan et al., 2014).", "startOffset": 249, "endOffset": 305}, {"referenceID": 10, "context": "To explore the effects of wording and euphonics on persuasion, the datasets were built in a controlled setting (topic, author, sentence length) to avoid confounding factors such as author or topic popularity, by following the procedure described in (Danescu-Niculescu-Mizil et al., 2012; Tan et al., 2014). In addition, these datasets comprise short texts (mostly single sentences) to focus on surface realization of persuasion, where strategic planning \u2013 which might act as a confounding factor \u2013 plays a minor role. The idea of using controlled experiments (usually in an A/B test setting) to study persuasive communication can be traced back at least to Hovland et al. (1953). While two of these datasets (Twitter and Movies) were already available, the other two (CORPS and Slogans) were collected by following the methodology proposed in the first two as", "startOffset": 250, "endOffset": 679}, {"referenceID": 33, "context": "P and \u00acP are determined based on their retweet counts (Tan et al., 2014).", "startOffset": 54, "endOffset": 72}, {"referenceID": 10, "context": "For each P , the dataset contains a contrasting quote \u00acP from the same movie such that (i) P and \u00acP are uttered by the same speaker, (ii) P and \u00acP have the same number of words, (iii) \u00acP does not occur in the IMDb list of memorable quotes and (iv) P and \u00acP are as close as possible to each other in the script (Danescu-Niculescu-Mizil et al., 2012).", "startOffset": 310, "endOffset": 348}, {"referenceID": 15, "context": "We collected these pairs from CORPS, a freely available corpus of political speeches tagged with audience reactions (Guerini et al., 2013).", "startOffset": 116, "endOffset": 138}, {"referenceID": 12, "context": "We collected these pairs from CORPS, a freely available corpus of political speeches tagged with audience reactions (Guerini et al., 2013). The methodology that we used to build the pairs is very similar to Danescu-NiculescuMizil et al. (2012): for each P , where P is the sentence preceding an audience reaction (e.", "startOffset": 117, "endOffset": 244}, {"referenceID": 11, "context": "For each slogan, we picked the most similar sentence in the New York Times articles having the same length and the highest LSA similarity (Deerwester et al., 1990) with the slogan.", "startOffset": 138, "endOffset": 163}, {"referenceID": 11, "context": "For each slogan, we picked the most similar sentence in the New York Times articles having the same length and the highest LSA similarity (Deerwester et al., 1990) with the slogan. The LSA similarity approach that we used to collect the non-slogans is very similar to the approach used by Louis and Nenkova (2013) to collect the non-persuasive counterparts of successful news articles.", "startOffset": 139, "endOffset": 314}, {"referenceID": 0, "context": "These functions are commonly used to analyze online social networks in terms of growth in size and activity (see for example (Ahn et al., 2007; Jiang et al., 2010; Leskovec, 2008)) and also for measuring content diffusion, e.", "startOffset": 125, "endOffset": 179}, {"referenceID": 18, "context": "These functions are commonly used to analyze online social networks in terms of growth in size and activity (see for example (Ahn et al., 2007; Jiang et al., 2010; Leskovec, 2008)) and also for measuring content diffusion, e.", "startOffset": 125, "endOffset": 179}, {"referenceID": 22, "context": "These functions are commonly used to analyze online social networks in terms of growth in size and activity (see for example (Ahn et al., 2007; Jiang et al., 2010; Leskovec, 2008)) and also for measuring content diffusion, e.", "startOffset": 125, "endOffset": 179}, {"referenceID": 19, "context": "the number of retweets of a given content (Kwak et al., 2010).", "startOffset": 42, "endOffset": 61}, {"referenceID": 10, "context": "Similarly to Danescu-Niculescu-Mizil et al. (2012) and Tan et al.", "startOffset": 13, "endOffset": 51}, {"referenceID": 10, "context": "Similarly to Danescu-Niculescu-Mizil et al. (2012) and Tan et al. (2014), we formulate a pairwise classification problem such that given a pair (s1, s2) consisting of sentences s1 and s2, the goal is to determine the more persuasive one (i.", "startOffset": 13, "endOffset": 73}, {"referenceID": 24, "context": "As a preprocessing step, all the sentences were tokenized by using Stanford CoreNLP (Manning et al., 2014).", "startOffset": 84, "endOffset": 106}, {"referenceID": 9, "context": "by using a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995).", "startOffset": 51, "endOffset": 76}, {"referenceID": 37, "context": "We preferred SVM as a classifier due to its characteristic property to especially perform well on high-dimensional data (Weichselbraun et al., 2011).", "startOffset": 120, "endOffset": 148}, {"referenceID": 25, "context": "from the ones to their left, while \u2020 represents no significance, as calculated according to McNemar\u2019s test (McNemar, 1947).", "startOffset": 107, "endOffset": 122}, {"referenceID": 12, "context": "This is in line with the work of Guerini et al. (2008), who built a domain specific lexicon to study the persuasive impact of words in political speeches.", "startOffset": 33, "endOffset": 55}], "year": 2015, "abstractText": "While the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention. By modeling a notion of euphony and analyzing four datasets comprising persuasive and nonpersuasive sentences in different domains (political speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness. We conduct a series of analyses and prediction experiments within and across datasets. Our results highlight the positive role of phonetic devices on persuasion.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}