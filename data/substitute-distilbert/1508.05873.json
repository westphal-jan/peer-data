{"id": "1508.05873", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2015", "title": "Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for Stationary Gaussian Inputs and Slow Learning", "abstract": "some system identification problems impose nonnegativity constraints on the parameters a estimate due to inherent physical characteristics of the unknown system. integrating nonnegative total - mean - square ( nnlms ) algorithm across its variants allow to address this problem in an online manner. a nonnegative least mean symmetric ( nnlmf ) algorithm has been recently proposed to improve the detection of candidate algorithms : cases where the measurement noise is not gaussian. this paper provides a first theoretical analysis of the stochastic behavior of the nnlmf algorithm instead introducing gaussian inputs implying slow fading. simulation results illustrate the accuracy of the proposed analysis.", "histories": [["v1", "Mon, 24 Aug 2015 16:26:38 GMT  (468kb,D)", "http://arxiv.org/abs/1508.05873v1", "11 pages, 8 figures, submitted for publication"]], "COMMENTS": "11 pages, 8 figures, submitted for publication", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["jingen ni", "jian yang", "jie chen", "c\\'edric richard", "jos\\'e carlos m bermudez"], "accepted": false, "id": "1508.05873"}, "pdf": {"name": "1508.05873.pdf", "metadata": {"source": "CRF", "title": "Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for Stationary Gaussian Inputs and Slow Learning", "authors": ["Jingen Ni", "Jian Yang", "Jie Chen", "Jos\u00e9 Carlos M. Bermudez"], "emails": ["jni@suda.edu.cn).", "20124228029@suda.edu.cn).", "dr.jie.chen@ieee.org).", "cedric.richard@unice.fr).", "j.bermudez@ieee.org)."], "sections": [{"heading": null, "text": "Index Terms\u2014Adaptive filter, least mean fourth (LMF), mean weight behavior, nonnegativity constraint, second-order moment, system identification.\nI. INTRODUCTION\nADAPTIVE filtering algorithms are widely used to addresssystem identification problems in applications such as adaptive noise cancellation, echo cancellation, active noise control, and distributed learning [1]\u2013[5]. Due to physical characteristics, some problems require the imposition of nonnegativity constraints on the parameters to estimate in order to avoid uninterpretable results [6]. Over the last decades, nonnegativity as a physical constraint has been studied extensively (see, e.g., the nonnegative least-squares [7]\u2013[10] and the nonnegative matrix factorization [11]\u2013[15]).\nThis work was supported in part by the National Natural Science Foundation of China (NSFC) under Grants 61471251 and 61101217, in part by the Natural Science Foundation of Jiangsu Province under Grant BK20131164, and in part by the Jiangsu Overseas Research and Training Program for University Prominent Young and Middle-Aged Teachers and Presents. The work of J. C. M. Bermudez has been partly supported by CNPq grants Nos. 307071/2013-8 and 400566/2013-3.\nJ. Ni is with the School of Electronic and Information Engineering, Soochow University, Suzhou 215006, China (e-mail: jni@suda.edu.cn).\nJ. Yang was with the School of Electronic and Information Engineering, Soochow University, Suzhou 215006, China. He is now with Seagate Technology (Suzhou) Corporation, Suzhou 215021, China (e-mail: 20124228029@suda.edu.cn).\nJ. Chen is with the Centre of Intelligent Acoustics and Immersive Communications, School of Marine Science and Technology (CIAIC), University of Northwestern Polytechnical University, Xi\u2019an 710072, China (e-mail: dr.jie.chen@ieee.org).\nC. Richard is with the Universite\u0301 de Nice Sophia-Antipolis, UMR CNRS 7293, Observatoire de la Co\u0302te d\u2019azur, Laboratoire Lagrange, Parc Valrose, 06102 Nice, France (e-mail: cedric.richard@unice.fr).\nJ. C. M. Bermudez is with the Department of Electrical Engineering, Federal University of Santa Catarina 88040-900, Floriano\u0301polis, SC, Brazil (e-mail: j.bermudez@ieee.org).\nThe nonnegative least-mean-square (NNLMS) algorithm was derived in [16] to address online system identification problems subject to nonnegativity constraints. Its convergence behavior was analyzed in [16], [17]. The NNLMS algorithm is a fixed-point iteration scheme based on the Karush-KuhnTucker (KKT) optimality conditions. The NNLMS algorithm updates the parameter estimate from streaming data at each time instant and is suitable for online system identification. Variants of the NNLMS algorithm were proposed in [18] and [19] to address specific robustness and convergence issues of NNLMS algorithm.\nIn certain practical contexts, it has been shown that adaptive algorithms with weight updates based on higher order moments of the estimation error may have better mean-square error (MSE) convergence properties than the LMS algorithm. This is the case, for instance, of the least mean fourth (LMF) algorithm, whose weight update is proportional to the third power of the estimation error. The LMF algorithm was proposed in [20], where it was verified that it could outperform the LMS algorithm in the presence of non-Gaussian measurement noise. This desirable property has led to a series of studies about the convergence behaviors of the LMF algorithm and some of its variants [21]\u2013[31]. Recently, a nonnegative LMF (NNLMF) algorithm was proposed in [32] to improve the performance of the NNLMS algorithm under non-Gaussian measurement noise. It was shown in [32] that, when compared to the NNLMS algorithm, the NNLMF algorithm can lead to faster convergence speed and equivalent steady-state performance, and to improved steady-state performance for the same convergence speed. The results shown in [32] were exclusively based on Monte Carlo simulations. Nevertheless, they clearly show that there is interest in better understanding the convergence properties of the NNLMF algorithm. To this moment, there has been no study of the stochastic behavior of the NNLMF algorithm.\nThis paper provides a first statistical analysis of the NNLMF algorithm behavior. We derive an analytical model for the algorithm behavior for slow learning and Gaussian inputs. Based on statistical assumptions typical to the analysis of adaptive algorithms, we derive recursive analytical expressions for the mean-weight behavior and for the excess MSE. The high order nonlinearities imposed on the weight update by the third error power and by the non-negativity constraints result in a difficult mathematical analysis, requiring novel approximations not usually employed in adaptive filter analyses. Monte\nar X\niv :1\n50 8.\n05 87\n3v 1\n[ cs\n.N A\n] 2\n4 A\nug 2\n01 5\n2 Carlo simulation results illustrate the accuracy of the analysis. It is known that the study of the stability of the LMF algorithm is relatively complex [22], [23]. This complexity increases for the NNLMF algorithm. The stability region of the NNLMF algorithm is studied in this paper through simulations, where it is explored how the algorithm stability depends on the step-size and on the initialization of the adaptive weights. A theoretical stability study could not accommodated in this work, as is left for a future work.\nThe paper is organized as follows. In Section II, we describe the system model and provide an overview of the NNLMS and NNLMF algorithms. In Section III, we introduce the statistical assumptions used in the analysis of the NNLMF algorithm. The mean and mean-square analyses are performed in Sections IV and V, respectively. Simulation results validate the analysis in Section VI. Finally, Section VII concludes the paper.\nIn the sequel, normal font letters are used for scalars, boldface lowercase letters for vectors, and boldface uppercase letters for matrices. Furhermore, (\u00b7)> denotes vector or matrix transposition, E{\u00b7} denotes statistical expectation, \u2016\u00b7\u2016 is the `2-norm of a vector, \u25e6 denotes the Hadamard product, Tr{\u00b7} computes the trace of a matrix, D\u03b1 represents the diagonal matrix whose main diagonal is the vector \u03b1, 1 is the all-one column vector, and I is the identity matrix."}, {"heading": "II. NONNEGATIVE SYSTEM IDENTIFICATION", "text": "Online system identification problem aims at estimating the system impulse response from observations of both the input signal u(n) and the desired response d(n), as shown in Figure 1. The desired response is assumed to be modeled by\nd(n) = w\u2217>u(n) + z(n) (1)\nwhere u(n) = [u(n), u(n \u2212 1), \u00b7 \u00b7 \u00b7 , u(n \u2212M + 1)]> is the input vector consisting of the M most recent input samples, w\u2217 = [w\u22170 , w \u2217 1 , \u00b7 \u00b7 \u00b7 , w\u2217M\u22121]> denotes the unconstrained weight vector of the unknown system, and z(n) represents the measurement noise. For nonnegative system identification, nonnegativity constraints are imposed on the estimated weights, leading to the constrained optimization problem [6]\nwo = arg min w J(w)\nsubject to wi \u2265 0 (2)\nwhere i \u2208 {0, 1, \u00b7 \u00b7 \u00b7 ,M\u22121}, w is the estimated weight vector with wi being its ith entry, J(w) is a differentiable and strictly convex objective function ofw, andwo represents the solution to the above constrained optimization problem.\nBased on the Karush-Kuhn-Tucker conditions, the authors in [16] derived a fixed-point iteration scheme to address the constrained optimization problem (2). Using the mean square error (MSE) cost function\nJ [w(n)] = E {[ d(n)\u2212w>(n)u(n) ]2} (3)\nand a stochastic approximation yielded the NNLMS algorithm update equation\nw(n+ 1) = w(n) + \u00b5Du(n)w(n)e(n) (4)\n( )u n w\n( )nw\n( )z n\n \n( )d n ( )e n\n( )y n\nAlgo.\n\n\nFig. 1. Block diagram of system identification using an adaptive filter, which are widely used in many practical applications.\nwhere w(n) denotes the weight vector of the adaptive filter at instant n, e(n) = d(n)\u2212w>(n)u(n) is the error signal, and \u00b5 is a positive step-size.\nTo improve the convergence performance of the adaptive filter for non-Gaussian measurement noise, the authors in [32] proposed to replace the MSE criterion with the mean fourth error (MFE) criterion\nJ [w(n)] = E {[ d(n)\u2212w>(n)u(n) ]4} . (5)\nThis has led to the NNLMF algorithm update equation\nw(n+ 1) = w(n) + \u00b5Du(n)w(n)e 3(n) (6)\nThe entry-wise form of (6) is\nwi(n+ 1) = wi(n) + \u00b5u(n\u2212 i)wi(n)e3(n) (7)\nwhere wi(n) is the ith entry of the weight vector w(n), i.e., w(n) = [w0(n), w1(n), \u00b7 \u00b7 \u00b7 , wM\u22121(n)]>. The update term of (7) is highly nonlinear in w(n), leading to a more complex behavior than that of the already studied LMF algorithm. In the following we study the stochastic behavior of (7)."}, {"heading": "III. STATISTICAL ASSUMPTIONS", "text": "The statistical analysis of any adaptive filtering algorithm requires the use of statistical assumptions for feasibility. The analysis is based on the study of the behavior of the weighterror vector, defined as\nw\u0303(n) = w(n)\u2212w\u2217 (8)\nand we employ the following frequently used statistical assumptions:\nA.1) The input signal u(n) is stationary, zero-mean and Gaussian.\nA.2) The input vector u(n) and the weight vector w(n) are independent.\nA.3) The measurement noise z(n) is zero-mean, i.i.d., and independent of any other signal. Moreover, it has an even probability density function so that all odd moments of z(n) are equal to zero.\nA.4) The statistical dependence of w\u0303(n)w\u0303>(n) and w\u0303(n) can be neglected.\nA.5) The weight-error vector w\u0303(n) and [ u>(n)w\u0303(n) ]2 are\nstatistically independent.\n3 Assumption A.2) is the well-known independence assumption, which has been successfully used in the analysis of many adaptive algorithms, including the LMF algorithm [21]. Assumption A.3) is often used in the analysis of higherorder moments adaptive algorithms, and is practically reasonable. Assumption A.4) is reasonable because one can find infinitely many vectors w\u0303(n) that would lead to the same matrix w\u0303(n)w\u0303>(n). Assumption A.5) is reasonable for a large number of taps. Simulation results will show that the models obtained using these assumptions can accurately predict the behavior of the NNLMF algorithm."}, {"heading": "IV. MEAN WEIGHT BEHAVIOR ANALYSIS", "text": "The ith entry of the weight-error vector (8) is given by\nw\u0303i(n) = wi(n)\u2212 w\u2217i . (9)\nSubtracting w\u2217i from both sides of (7), we have\nw\u0303i(n+ 1) = w\u0303i(n) + \u00b5u(n\u2212 i)wi(n)e3(n). (10)\nSubstituting (9) into (10) yields w\u0303i(n+1) = w\u0303i(n)+\u00b5u(n\u2212i) { [w\u0303i(n)+w \u2217 i ]e 2(n) } e(n). (11)\nBy employing (1) and (9), the estimation error e(n) can be equivalently expressed in terms of w\u0303(n) in the following form:\ne(n) = z(n) +w\u2217>u(n)\u2212w>(n)u(n) = z(n)\u2212 w\u0303>(n)u(n) (12)\nwhere w\u0303(n) = w(n) \u2212 w\u2217. Expression (9) implies that the term [w\u0303i(n) + w\u2217i ]e\n2(n) in (11) is of fourth-order in w\u0303i(n). This makes the analysis significantly more difficult than that of the LMS or LMF algorithm.\nTo make the problem tractable, we linearize the nonlinear term\nf [w\u0303i(n)] = [w \u2217 i + w\u0303i(n)]e 2(n) (13)\nvia a first-order Taylor expansion as done in [18]. Taking first the derivative of f [w\u0303i(n)] with respect to w\u0303i(n), we have\n\u2202f [w\u0303i(n)]\n\u2202w\u0303i(n) = e2(n)\u2212 2e(n)u(n\u2212 i)[w\u2217i + w\u0303i(n)]. (14)\nConsidering that w\u0303i(n) fluctuates around E{w\u0303i(n)}, we approximate the high-order stochastic term f [w\u0303i(n)] at time instant n by its first-order Taylor expansion about E{w\u0303i(n)}. Hence,\nf [w\u0303i(n)]\n' f [E{w\u0303i(n)}] + \u2202f [w\u0303i(n)]\n\u2202w\u0303i(n) \u2223\u2223\u2223 E{w\u0303i(n)} [w\u0303i(n)\u2212 E{w\u0303i(n)}]\n= e2E,i(n)w \u2217 i\n+ 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}]E{w\u0303i(n)} + { e2E,i(n)\u2212 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}] } w\u0303i(n)\n(15)\nwhere eE,i(n) = e(n)|w\u0303E,i(n), i.e.,\neE,i(n) = z(n)\u2212 w\u0303>E,i(n)u(n) = e(n)\u2212 u(n\u2212 i)[E{w\u0303i(n)} \u2212 w\u0303i(n)] (16)\nwith\nw\u0303E,i(n) = [w\u03030(n), \u00b7 \u00b7 \u00b7 , w\u0303i\u22121(n),E{w\u0303i(n)}, w\u0303i+1(n), \u00b7 \u00b7 \u00b7 , w\u0303M\u22121(n)]>. (17)\nCombining (11), (13), and (15) yields\nw\u0303i(n+1) = w\u0303i(n)+\u00b5u(n\u2212i)[xi(n)+si(n)w\u0303i(n)]e(n) (18)\nwhere\nxi(n) = e 2 E,i(n)w \u2217 i + 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}]E{w\u0303i(n)} (19) si(n) = e 2 E,i(n)\u2212 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}]. (20)\nExpressions in (19) and (20) can be easily written in vector form as follows:\nx(n) = [x0(n), x1(n), \u00b7 \u00b7 \u00b7 , xM\u22121(n)]>\n=D2eE(n)w \u2217 + 2DeE(n)Du(n)DE{w\u0303(n)}\n\u00d7 [w\u2217 + E{w\u0303(n)}] (21) s(n) = [s0(n), s1(n), \u00b7 \u00b7 \u00b7 , sM\u22121(n)]>\n=D2eE(n)1\u2212 2DeE(n)Du(n)[w \u2217 + E{w\u0303(n)}] (22)\nwhere\neE(n) = [eE,0(n), eE,1(n), \u00b7 \u00b7 \u00b7 , eE,M\u22121(n)]>\n= e(n)1\u2212Du(n)[E{w\u0303(n)} \u2212 w\u0303(n)]. (23)\nThus, we can write (18) in matrix form as\nw\u0303(n+ 1)\n= w\u0303(n) + \u00b5Du(n)[x(n) +Ds(n)w\u0303(n)] \u00d7 [ z(n)\u2212 w\u0303>(n)u(n) ] = w\u0303(n)\u2212 \u00b5Du(n) [ x(n) +Ds(n)w\u0303(n) ] w\u0303>(n)u(n)\n+ \u00b5Du(n) [ x(n) +Ds(n)w\u0303(n) ] z(n)\n= w\u0303(n)\u2212 \u00b5p(n) + \u00b5q(n) (24)\nwhere p(n) =Du(n) [ x(n) +Ds(n)w\u0303(n) ] w\u0303>(n)u(n) (25)\nq(n) =Du(n) [ x(n) +Ds(n)w\u0303(n) ] z(n). (26)\nTaking expectations of both sides of (24) yields\nE{w\u0303(n+ 1)} = E{w\u0303(n)} \u2212 \u00b5E{p(n)}+ \u00b5E{q(n)}. (27)\nNext, we need to calculate E{p(n)} and E{q(n)} to express (27) in an explicit form. Using (25), the ith entry of p(n) can be written as\npi(n) = u(n\u2212 i) { e2E,i(n)w \u2217 i\n+ 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}]E{w\u0303i(n)} + { e2E,i(n)\u2212 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}] } \u00d7 w\u0303i(n) } w\u0303>(n)u(n). (28)\nWe rewrite (28) as\npi(n) = pi,a(n) + pi,b(n) + pi,c(n) (29)\n4 where\npi,a(n) = u(n\u2212 i)e2E,i(n)w\u2217i w\u0303>(n)u(n) (30) pi,b(n) = u(n\u2212 i)e2E,i(n)w\u0303i(n)w\u0303>(n)u(n) (31) pi,c(n) = 2[w \u2217 i + E{w\u0303i(n)}]u2(n\u2212 i)eE,i(n)\n\u00d7 [E{w\u0303i(n)} \u2212 w\u0303i(n)]w\u0303>(n)u(n). (32)\nDefine ri as the ith column vector of the input vector correlation matrix R = E { u(n)u>(n) } . Using assumptions A.1)\u2013 A.5), it is shown in Appendices A and B that the expected values of (30) and (31) can be approximated by\nE{pi,a(n)} = E { u(n\u2212 i)e2E,i(n)w\u2217i w\u0303>(n)u(n) } ' 3Tr { RE{w\u0303(n)}E { w\u0303>(n) }} E { w\u0303>(n) } riw \u2217 i\n+ \u03c32zE { w\u0303>(n) } riw \u2217 i (33)\nE{pi,b(n)} = E { u(n\u2212 i)e2E,i(n)w\u0303i(n)w\u0303>(n)u(n) } ' 3Tr { RE{w\u0303(n)}E { w\u0303>(n) }} E { w\u0303>(n) } riE{w\u0303i(n)}\n+ \u03c32zE { w\u0303>(n) } riE{w\u0303i(n)} (34)\nwhere \u03c32z = E { z2(n) } denotes the variance of the measurement noise. It is also shown in Appendix A that w\u0303>E,i(n)u(n) can be approximated by w\u0303>(n)u(n). Using this approximation in (16) yields\neE,i(n) ' z(n)\u2212 w\u0303>(n)u(n). (35)\nSubstituting (35) into (32), we have\npi,c(n) = 2[w \u2217 i + E{w\u0303i(n)}]u2(n\u2212 i)w\u0303>(n)u(n)\n\u00d7 [E{w\u0303i(n)} \u2212 w\u0303i(n)]z(n) \u2212 2[w\u2217i + E{w\u0303i(n)}]u2(n\u2212 i) [ w\u0303>(n)u(n) ]2 \u00d7 [E{w\u0303i(n)} \u2212 w\u0303i(n)]. (36)\nThen, using assumptions A.2) A.3) and A.5), the expected value of (36) becomes\nE{pi,c(n)} = \u22122[w\u2217i + E{w\u0303i(n)}]E { u2(n\u2212 i) [ w\u0303>(n)u(n) ]2} \u00d7 E { [E{w\u0303i(n)} \u2212 w\u0303i(n)]\n} = 0 (37)\ndue to the last expectation. Therefore, we have\nE{pi(n)} = E{pi,a(n)}+ E{pi,b(n)}. (38)\nIts vector form is consequently given by\nE{p(n)} = E{pa(n)}+ E{pb(n)} (39)\nwhere E{pa(n)} = 3Tr { RE{w\u0303(n)}E { w\u0303>(n) }} Dw\u2217RE{w\u0303(n)}\n+ \u03c32zDw\u2217RE{w\u0303(n)} (40) E{pb(n)} = 3Tr { RE{w\u0303(n)}E { w\u0303>(n) }} DE{w\u0303(n)}\n\u00d7RE{w\u0303(n)}+ \u03c32zDE{w\u0303(n)}RE{w\u0303(n)}. (41)\nTherefore, we have\nE{p(n)} = 3Tr { RE{w\u0303(n)}E { w\u0303>(n) }} D[w\u2217+E{w\u0303(n)}]RE{w\u0303(n)}\n+ \u03c32zD[w\u2217+E{w\u0303(n)}]RE{w\u0303(n)}. (42)\nFrom (26), we directly find that the ith entry of q(n) can be written as\nqi(n) = u(n\u2212 i) { e2E,i(n)w \u2217 i\n+ 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}]E{w\u0303i(n)} + { e2E,i(n)\u2212 2eE,i(n)u(n\u2212 i)[w\u2217i + E{w\u0303i(n)}] } \u00d7 w\u0303i(n) } z(n). (43)\nUsing (16), we rewrite the above equation as\nqi(n) = qi,a(n) + qi,b(n)\u2212 qi,c(n) + qi,d(n) (44)\nwhere qi,a(n) = u(n\u2212 i) [ z(n)\u2212 w\u0303>E,i(n)u(n) ]2 w\u2217i z(n) (45)\nqi,b(n) = u(n\u2212 i) [ z(n)\u2212 w\u0303>E,i(n)u(n) ]2 w\u0303i(n)z(n) (46)\nqi,c(n) = 2 [ z(n)\u2212 w\u0303>E,i(n)u(n) ] u2(n\u2212 i) \u00d7 [w\u2217i + E{w\u0303i(n)}]w\u0303i(n)z(n) (47) qi,d(n) = 2 [ z(n)\u2212 w\u0303>E,i(n)u(n) ] u2(n\u2212 i)\n\u00d7 [w\u2217i + E{w\u0303i(n)}]E{w\u0303i(n)}z(n). (48)\nUsing assumptions A.1)\u2013A.4) as well as the approximation E{w\u0303i(n)w\u0303j(n)} ' E{w\u0303i(n)}E{w\u0303j(n)},\u2200i, j, for the reason that the mean weight behavior is usually not sensitive in such kind of approximation (see [16] and [18] for detailed explanation), we have\nE{qi,a(n)} ' \u22122E { z2(n)u(n\u2212 i)w\u0303>E,i(n)u(n) } w\u2217i\n= \u22122\u03c32zw\u2217i E { w\u0303>(n) } ri (49)\nE{qi,b(n)} ' \u22122E { z2(n)u(n\u2212 i)w\u0303>E,i(n)u(n)E{w\u0303i(n)} } = \u22122\u03c32zw\u2217i E { w\u0303>(n) } ri (50)\nE{qi,c(n)} ' 2\u03c32zE { u2(n\u2212 i)[w\u2217i + E{w\u0303i(n)}] } \u00d7 E{w\u0303i(n)} (51)\nE{qi,d(n)} = 2\u03c32zE { u2(n\u2212 i)[w\u2217i + E{w\u0303i(n)}] } \u00d7 E{w\u0303i(n)}. (52)\nConsequently, (44) leads to\nE{qi(n)} = E{qi,a(n)}+ E{qi,b(n)} \u2212 E{qi,c(n)}+ E{qi,d(n)} ' \u22122\u03c32z [w\u2217i + E{w\u0303i(n)}]E { w\u0303>(n) } ri (53)\nwhich can be written in matrix form as\nE{q(n)} = \u22122\u03c32zD[w\u2217+E{w\u0303(n)}]RE{w\u0303(n)}. (54)\nFinally, using (42) and (54) in (27), we obtain\nE{w\u0303(n+ 1)} = E{w\u0303(n)} \u2212 3\u00b5 \u03c32zD[w\u2217+E{w\u0303(n)}]RE{w\u0303(n)}\n\u2212 3\u00b5Tr { RE{w\u0303(n)}E { w\u0303>(n) }} D[w\u2217+E{w\u0303(n)}]RE{w\u0303(n)}.\n(55)\n5 Expression (55) predicts the mean weight behavior of the NNLMF algorithm. It will be used to compute the secondorder moment in the next section.\nIn the case that the input to the system is a zero-mean white noise, i.e., R = \u03c32uI with \u03c3 2 u = E { u2(n) } , (55) reduces to\nE{w\u0303(n+ 1)} = E{w\u0303(n)} \u2212 3\u00b5\u03c32z\u03c32uD[w\u2217+E{w\u0303(n)}]E{w\u0303(n)}\n\u2212 3\u00b5\u03c34uTr { E{w\u0303(n)}E { w\u0303>(n) }} D[w\u2217+E{w\u0303(n)}]E{w\u0303(n)}\n= E{w\u0303(n)} \u2212 3\u00b5\u03c32z\u03c32uDE{w\u0303(n)}[w\u2217 + E{w\u0303(n)}] \u2212 3\u00b5\u03c34u\u2016E { w\u0303(n) } \u20162DE{w\u0303(n)}[w\u2217 + E{w\u0303(n)}]. (56)\nIts entry-wise form can be expressed as\nE{w\u0303i(n+ 1)} = E{w\u0303i(n)} \u2212 3\u00b5\u03c32z\u03c32uE{w\u0303i(n)}[w\u2217i + E{w\u0303i(n)}]\n\u2212 3\u00b5\u03c34u M\u22121\u2211 m=0 E2{w\u0303m(n)}E{w\u0303i(n)}[w\u2217i + E{w\u0303i(n)}].(57)\nUsing the equality E{w\u0303i(n + 1)} = E{w\u0303i(n)} as n \u2192 \u221e, (57) becomes\nE{w\u0303i(\u221e)}[w\u2217i + E{w\u0303i(\u221e)}] \u00d7 { 3\u00b5\u03c32z\u03c3 2 u + 3\u00b5\u03c3 4 u M\u22121\u2211 m=0 E2{w\u0303m(\u221e)} } = 0.(58)\nSolving (58), we obtain E{w\u0303i(\u221e)} = 0 or E{w\u0303i(\u221e)} = \u2212w\u2217i , which means that wo,i = w\u2217i and wo,i = 0 are the two fixed points of the mean weight behavior of the NNLMF algorithm. This result is consistent with that of the NNLMS algorithm."}, {"heading": "V. SECOND-ORDER MOMENT ANALYSIS", "text": "Let K(n) = E { w\u0303(n)w\u0303>(n) } be the covariance matrix of the weight-error vector. Under certain simplifying assumptions [2], the excess mean square error (EMSE) is given by\n\u03be(n) = Tr{RK(n)}. (59)\nIn the previous section, we used the approximation K(n) ' E{w\u0303(n)}E { w\u0303>(n) } . This approximation is accurate enough for the analysis of the mean weight behavior. However, the effect of the second-order moments of the weight-error vector on the EMSE behavior becomes more significant [16]. Thus, we need a more accurate expression for K(n) in order to characterize the EMSE.\nSubtracting w\u2217 from both sides of (6) yields\nw\u0303(n+ 1) = w\u0303(n) + \u00b5Du(n)w(n)e 3(n). (60)\nPost-multiplying (60) by its transpose, considering the equality Du(n)w(n) =Dw(n)u(n), and taking the expected value, we have\nK(n+ 1) =K(n) + \u00b5\u03a61(n) + \u00b5 2\u03a62(n) (61)\nwhere \u03a61(n) = E { e3(n) [ w\u0303(n)u>(n)Dw(n)\n+Dw(n)u(n)w\u0303 >(n) ]} (62)\n\u03a62(n) = E { e6(n)Dw(n)u(n)u >(n)Dw(n) } . (63)\nBy using (12), e3(n) and e6(n) can be expanded, respectively, as follows:\ne3(n) = z3(n)\u2212 3z2(n)u>(n)w\u0303(n) + 3z(n) [ u>(n)w\u0303(n) ]2 \u2212 [ u>(n)w\u0303(n) ]3 (64)\ne6(n) = z6(n)\u2212 6z5(n)u>(n)w\u0303(n) + 15z4(n) [ u>(n)w\u0303(n) ]2 \u2212 20z3(n) [ u>(n)w\u0303(n) ]3 + 15z2(n) [ u>(n)w\u0303(n)\n]4 \u2212 6z(n) [ u>(n)w\u0303(n) ]5 + [ u>(n)w\u0303(n) ]6 . (65)\nUsing (64) in (62) with assumption A.3), we have\n\u03a61(n) = E\n{{ \u2212 3z2(n)u>(n)w\u0303(n)\u2212 [ u>(n)w\u0303(n) ]3} \u00d7 { w\u0303(n)u>(n)Dw(n) +Dw(n)u(n)w\u0303 >(n) }}\n=\u22123\u03c32z\u03981(n)\u2212 3\u03c32z\u0398>1 (n) +\u03982(n) +\u0398>2 (n)(66)\nwhere \u03981(n) = E { w\u0303(n)w\u0303>(n)u(n)u>(n)Dw(n) } (67)\n\u03982(n) =\u2212E { Dw(n)u(n)u >(n)w\u0303(n)w\u0303>(n)u(n)\n\u00d7 u>(n)w\u0303(n)w\u0303>(n) } . (68)\nSimilarly, using (65) in (63) with assumption A.3), we have \u03a62(n) = E { z6(n) } \u03983(n) + 15E { z4(n) } \u03984(n)\n+ 15\u03c32z\u03985(n) +\u03986(n) (69)\nwhere \u03983(n) = E { Dw(n)u(n)u >(n)Dw(n) } (70)\n\u03984(n) = E { [u>(n)w\u0303(n)]2Dw(n)u(n)u >(n)Dw(n) } (71)\n\u03985(n) = E { [u>(n)w\u0303(n)]4Dw(n)u(n)u >(n)Dw(n) } (72)\n\u03986(n) = E {[ u>(n)w\u0303(n) ]6 Dw(n)u(n)u >(n)Dw(n) } .(73)\nIn the following, we compute \u03981(n) through \u03986(n). \u03981(n): Using (8) in (67), and considering assumptions A.2) and A.4), we can approximate (67) by\n\u03981(n) = E { w\u0303(n)w\u0303>(n)u(n)u>(n)Dw\u0303(n) } + E { w\u0303(n)w\u0303>(n)u(n)u>(n)Dw\u2217\n} 'K(n)RDE{w\u0303(n)} +K(n)RDw\u2217 =K(n)R[DE{w\u0303(n)} +Dw\u2217 ]. (74)\n\u03982(n): Using (8) in (68), and considering assumptions A.2) and A.4), we can approximate (68) by\n\u03982(n) =\u2212E{Dw\u0303(n)\u039e(n)} \u2212 E{Dw\u2217\u039e(n)} ' \u2212DE{w\u0303(n)}E{\u039e(n)} \u2212Dw\u2217E{\u039e(n)} (75)\nwhere\n\u039e(n) = u(n)u>(n)w\u0303(n)w\u0303>(n)u(n)u>(n)w\u0303(n)w\u0303>(n).\n(76)\n6 Notice that in the second line of (75) we neglect the correlation between Dw\u0303(n) and \u039e(n). This approximation is reasonable as \u039e(n) is a function of fourth-order products of elements of w\u0303(n), whose values can be obtained from infinitely many different vectors w\u0303(n). In [21], the expected value of \u039e(n) for zero-mean Gaussian inputs has been approximated using assumptions A.4) and A.5) by\nE{\u039e(n)} ' 3Tr{RK(n)}RK(n). (77)\nUsing (77) in (75) yields\n\u03982(n) ' \u22123Tr{RK(n)}[DE{w\u0303(n)} +Dw\u2217 ]RK(n). (78)\n\u03983(n): Substituting (8) into (70) and using assumption A.3), we have\n\u03983(n) = E { Dw\u0303(n)u(n)u >(n)Dw\u0303(n) }\n+ E { Dw\u0303(n)u(n)u >(n)Dw\u2217 }\n+ E { Dw\u2217u(n)u >(n)Dw\u0303(n) }\n+ E { Dw\u2217u(n)u >(n)Dw\u2217 } . (79)\nIt was shown in [16] that E { Du(n)w\u0303(n)w\u0303 >(n)Du(n) } ' R \u25e6K(n). Since\nDw\u0303(n)u(n)u >(n)Dw\u0303(n) =Du(n)w\u0303(n)w\u0303 >(n)Du(n) (80)\nwe can approximate (79) by\n\u03983(n) 'R \u25e6K(n) +DE{w\u0303(n)}RDw\u2217 +Dw\u2217RDE{w\u0303(n)} +Dw\u2217RDw\u2217 . (81)\n\u03984(n): Substituting (8) into (71) and using assumption A.3), (71) can be written by\n\u03984(n) = 15E {[ u>(n)w\u0303(n) ]2 Dw(n)u(n)u >(n)Dw(n) } = 15E {[ u>(n)w\u0303(n) ]2 Du(n)w(n)w >(n)Du(n)\n} = 15E { Du(n) [ w\u0303(n) +w\u2217 ] u>(n)w\u0303(n)\n\u00d7 w\u0303>(n)u(n)[w\u0303(n) +w\u2217]>Du(n) }\n= 15[\u03984a(n) +\u03984b(n) +\u03984c(n) +\u03984d(n)] (82)\nwhere\n\u03984a(n) = E { Du(n)w \u2217u>(n)w\u0303(n)w\u0303>(n)u(n)w\u2217>Du(n) } (83)\n\u03984b(n) = E { Du(n)w \u2217u>(n)w\u0303(n)u>(n)w\u0303(n)w\u0303>(n)Du(n) } (84)\n\u03984c(n) = E { Du(n)w\u0303(n)w\u0303 >(n)u(n)w\u0303>(n)u(n)w\u2217>Du(n) } (85)\n\u03984d(n) = E { Du(n)w\u0303(n)w\u0303 >(n)u(n)u>(n)w\u0303(n)w\u0303>(n)Du(n) } .(86)\nWe find that the above quantities,\u03984a(n)\u2013\u03984d(n), correspond to equations (45)\u2013(48) in [16], respectively, which have been\ncomputed under assumptions A.1)\u2013A.5). Therefore, the results obtained in [16] can be used directly here, yielding \u03984a(n) 'Dw\u2217 { 2RK(n)R+ Tr{RK(n)}R } Dw\u2217 (87)\n\u03984b(n) 'Dw\u2217 { 2RK(n)R+ Tr{RK(n)}R } DE{w\u0303(n)}(88)\n\u03984c(n) 'DE{w\u0303(n)} { 2RK(n)R+ Tr{RK(n)}R } Dw\u2217(89)\n\u03984d(n) ' { 2RK(n)R+ Tr{RK(n)}R } \u25e6K(n). (90)\nDefining \u03a5(n) = 2RK(n)R+ Tr{RK(n)}R and substituting (87)\u2212(90) into (82) leads to\n\u03984(n) ' [Dw\u2217\u03a5(n)Dw\u2217 +Dw\u2217\u03a5(n)DE{w\u0303(n)} +DE{w\u0303(n)}\u03a5(n)Dw\u2217 + \u03a5(n) \u25e6K(n)]. (91)\n\u03985(n): The term \u03985(n) contains higher order moments of w\u0303(n) and u(n). Computing this term also requires approximations. One approximation that preserves the second order moments is to split the expectation as E { [u>(n)w\u0303(n)]4 } E { Dw(n)u(n)u >(n)Dw(n) }\n. The intuition behind this approximation is that each element of the matrix Dw(n)u(n)u>(n)Dw(n) corresponds to only one of the M2 terms of the sum [u>(n)w\u0303(n)]2, which tends to reduce their correlation for reasonably large M . Moreover, we shall assume that u>(n)w\u0303(n) is zero-mean Gaussian to simplify the evaluation of the above expectations. This assumption becomes more valid as M increases (by the Central Limit theorem) and tends to be reasonable for practical values of M . Under these assumptions, we have\n\u03985(n) = E { [u>(n)w\u0303(n)]4 } E { Dw(n)u(n)u >(n)Dw(n) } = 3 ( E {[ u>(n)w\u0303(n) ]2})2 \u03983(n)\n= 3 ( Tr{RK(n)} )2{ R \u25e6K(n) +DE{w\u0303(n)}RDw\u2217\n+Dw\u2217RDE{w\u0303(n)} +Dw\u2217RDw\u2217 } . (92)\n\u03986(n): Using the same assumptions used to calculate \u03985(n), \u03986(n) in (73) can be approximated by\n\u03986(n) = E { [u>(n)w\u0303(n)]6 } E { Dw(n)u(n)u >(n)Dw(n) } = 15 ( E {[ u>(n)w\u0303(n) ]2})3 \u03983(n)\n= 15 ( Tr{RK(n)} )3{ R \u25e6K(n) +DE{w\u0303(n)}RDw\u2217\n+Dw\u2217RDE{w\u0303(n)} +Dw\u2217RDw\u2217 } . (93)\nFinally, using \u03981(n) through \u03986(n) in (62) and (63) , we obtain\n\u03a61(n) ' \u2212 3 { \u03c32z + Tr{RK(n)} }{ K(n)R[DE{w\u0303(n)} +Dw\u2217 ]\n+[DE{w\u0303(n)} +Dw\u2217 ]RK(n) }\n(94)\n7 and\n\u03a62(n) '{ E { z6(n) } + 45\u03c32z ( Tr{RK(n)} )2 + 15 ( Tr{RK(n)} )3} \u00d7 { R \u25e6K(n) +DE{w\u0303(n)}RDw\u2217\n+Dw\u2217RDE{w\u0303(n)} +Dw\u2217RDw\u2217 }\n+15E { z4(n) }{ Dw\u2217\u03a5(n)Dw\u2217 +Dw\u2217\u03a5(n)DE{w\u0303(n)}\n+DE{w\u0303(n)}\u03a5(n)Dw\u2217 + \u03a5(n) \u25e6K(n) } .\n(95)\nSubstituting (94) and (95) into (61) we obtain a recursive analytical model for the behavior of K(n), which can then be used in (59) to predict the EMSE behavior for the NNLMF algorithm. Note that E { z4(n) } and E { z6(n) } depend on the statistical distribution of the noise z(n). For instance, if z(n) is zero-mean Gaussian, then E { z4(n) } = 3\u03c34z and\nE { z6(n) } = 15\u03c36z ."}, {"heading": "VI. SIMULATION RESULTS", "text": "This section presents simulations in the context of system identification with nonnegativity constraints to illustrate the accuracy of the models derived in Sections IV and V. The impulse response w\u2217 of the unknown system was given by [0.8, 0.6, 0.5, 0.4, 0.3, 0.2.0.1,\u22120.1,\u22120.3,\u22120.6]>. The initial weight w(0) was made equal to a vector \u03c80 drawn from the uniform distribution U([0; 1]) and kept the same for all realizations. Both w\u2217 and \u03c80 are shown in Fig. 2. The input was either a zero-mean white Gaussian signal of unit power, or a correlated signal obtained by filtering a zero-mean white Gaussian noise with variance 34 through a first-order system H(z) = 1/(1 \u2212 0.5z\u22121), which yields a correlated input with unit variance. The above simulation setups are the same as those in [16]. The measurement noise was either uniformly distributed in [\u22125, 5] (SNR = \u22129.2 dB) or a binary sequence with samples randomly drawn from the set {2,\u22122} (SNR = \u22126 dB). All mean weights and EMSE curves were obtained by averaging over 200 independent realizations.\nFigs. 3 and 4 show the mean weight behavior for white and correlated inputs, respectively, and \u00b5 = 2 \u00d7 10\u22125 was chosen for slow learning. An excellent match can be verified between the behavior predicted by the proposed model and that obtained from Monte Carlo simulations. Figs. 5 and 6 show the EMSE (in dB) behavior for the same example. Here again one can verify an excellent match between theory and simulation results.\nThe theoretical models presented in this paper predict the stochastic behavior of the NNLMF algorithm under the assumption of convergence. It is well known that the convergence of both the LMF and NNLMS algorithms depends on the step-size as well as on the weight initialization. This is because higher-order moments of the weights in the update equations lead to coupled nonlinear oscillation systems. This is also the case for the NNLMF algorithm. The stability analysis for the LMF algorithm is relatively complex [22], [23], and this complexity increases for the NNLMF algorithm. Thus, the\n1\ntheoretical stability study is left for future work. We present in the following some simulation results that illustrate the dependence of the NNLMF algorithm stability on the stepsize and on the adaptive weights initialization.\nDefining d = w\u0303>(0)w\u0303(0) as a measure of the distance between the initial weight vector and the weight vector of the unknown system, we experimentally determined the regions for which the EMSE of the NNLMF algorithm diverges. To vary d, the initial weight vector was set to w(0) = k\u03c80 in this simulation so that d = [k\u03c80 \u2212 w\u2217]>[k\u03c80 \u2212 w\u2217]. We varied \u00b5 from 0.1\u00d7 10\u22125 to 2.1\u00d7 10\u22125 with step 0.2\u00d7 10\u22125 and d from 2 to 102 with step 10. For the sake of convenience, we marked the experimentally observed test results of the algorithm in the \u00b5 \u2212 d plane to illustrate convergence and divergence regions, as well as the transition between them. The divergence or convergence of the NNLMF algorithm in each point (\u00b5, d) was tested by more than 1000 independent realizations of 5\u00d7105 input samples. The obtained results are shown in Figs. 7 and 8 for white and correlated input signals, respectively. Notice that these convergence and divergence regions are just statistically observed results under the given simulation conditions. It is clear from these figures that the use of larger step-sizes requires some information that permits a better initialization of the weight vector."}, {"heading": "VII. CONCLUSION", "text": "The NNLMF algorithm can outperform the NNLMS algorithm when the measurement noise is non-Gaussian. This paper studied the mean and second-moment behavior of adaptive weights of the the NNLMF algorithm for stationary Gaussian input signals and slow learning. The analysis was based on typical statistical assumptions and has led to a recursive model for predicting the algorithm behavior. Simulation results have shown an excellent matching between the simulation results and the behavior predicted by the theoretical models. As the stability conditions for the NNLMF algorithm for convergence is difficult to determine analytically, we have shown through\nsimulations that it depends on both the step-size value and on the initialization of the weights. A theoretical stability analysis will be a topic for future work."}, {"heading": "APPENDIX A DETAILED CALCULATION OF (33)", "text": "Substituting (16) into (33) yields\nE{pi,a(n)} = E { u(n\u2212 i) [ z(n)\u2212 w\u0303>E,i(n)u(n) ]2 w\u2217i w\u0303 >(n)u(n) }\n= E { u(n\u2212 i) [ w\u0303>E,i(n)u(n) ]2 w\u2217i w\u0303 >(n)u(n) }\n\u2212 2E { u(n\u2212 i)z(n)w\u0303>E,i(n)u(n)w\u2217i w\u0303>(n)u(n) } + E { u(n\u2212 i)z2(n)w\u2217i w\u0303>(n)u(n) } . (96)\nUsing assumptions A.2) and A.3) and noting that w\u2217i is deterministic, we can simplify (96) to\nE{pi,a(n)} = E { u(n\u2212 i) [ w\u0303>E,i(n)u(n) ]2 w\u0303>(n)u(n) } w\u2217i\n+\u03c32zE { w\u0303>(n) } riw \u2217 i . (97)\nUsing (17) and the definition of u(n), we obtain\nw\u0303>E,i(n)u(n) = i\u22121\u2211 j=0 w\u0303j(n)u(n\u2212 j) + E{w\u0303(n\u2212 i)}u(n\u2212 i)\n+ M\u22121\u2211 j=i+1 w\u0303j(n)u(n\u2212 j). (98)\nAlso,\nw\u0303>(n)u(n) = i\u22121\u2211 j=0 w\u0303j(n)u(n\u2212 j) + w\u0303(n\u2212 i)u(n\u2212 i)\n+ M\u22121\u2211 j=i+1 w\u0303j(n)u(n\u2212 j). (99)\nWe note that (98) has the same expression as (99) except for the ith term, E{w\u0303(n \u2212 i)}u(n \u2212 i)}. Therefore, we can approximate w\u0303>E,i(n)u(n) by w\u0303\n>(n)u(n) for large values of N . With this approximation, (97) can be written as\nE{pi,a(n)} ' E { u(n\u2212 i) [ w\u0303>(n)u(n) ]3} w\u2217i\n+ \u03c32zE { w\u0303>(n) } riw \u2217 i . (100)\nThe following approximation has been derived in [21]: E { u(n) [ w\u0303>(n)u(n) ]3} ' 3Tr{RK(n)}RE{w\u0303(n)}. (101) The ith entry of (101) satisfies\nE { u(n\u2212 i) [ w\u0303>(n)u(n) ]3} ' 3Tr{RK(n)}r>i E{w\u0303(n)} = 3Tr{RK(n)}E { w\u0303>(n) } ri. (102)\nSubstituting (102) into (100) yields E{pi,a(n)} ' 3Tr{RK(n)}E { w\u0303>(n) } riw \u2217 i\n+ \u03c32zE { w\u0303>(n) } riw \u2217 i . (103)\nIn order to simplify the model and avoid higher-order statistics, the approximation E{w\u0303(n)w\u0303>(n)} ' E{w\u0303(n)}E{w\u0303>(n)}\n10\nwas used in the mean weight behavior analysis of the NNLMS, for which the detailed explanation was given in [16]. Using this approximation in (103), we obtain (33). Notice that the recursive model derived for K(n) in Section V can also be employed to predict the mean weight behavior of the NNLMF algorithm. Nevertheless, a sufficiently accurate mean weight behavior model can be obtained by using this first-order approximation."}, {"heading": "APPENDIX B DETAILED CALCULATION OF (34)", "text": "Using the approximation w\u0303>E,i(n)u(n) ' w\u0303>(n)u(n) shown in Appendix A, E{pi,b(n)} can be written as\nE{pi,b(n)} ' E { u(n\u2212 i) [ w\u0303>(n)u(n) ]3 w\u0303i(n) } + \u03c32zE { u(n\u2212 i)w\u0303>(n)u(n)w\u0303i(n) } . (104)\nThe term w\u0303i(n) in (104) can be considered weakly correlated with u(n \u2212 i)[w\u0303>(n)u(n)]3 according to assumptions A.2), A.4) and A.5). Therefore, the first term of the right-hand side\n11\nof (104) can be approximated by E { u(n\u2212 i) [ w\u0303>(n)u(n) ]3 w\u0303i(n) } ' E { u(n\u2212 i) [ w\u0303>(n)u(n)\n]3}E{w\u0303i(n)} ' 3Tr{RK(n)}E { w\u0303>(n) } riE{w\u0303i(n)}\n' 3Tr { RE{w\u0303(n)}E { w\u0303>(n) }} E { w\u0303>(n) } riE{w\u0303i(n)}.\n(105) The approximation E { w\u0303(n)w\u0303>(n) } ' E{w\u0303(n)}E { w\u0303>(n) } implies that E{w\u0303i(n)w\u0303j(n)} ' E{w\u0303i(n)}E{w\u0303j(n)},\u2200i, j. Thus, with assumptions A.2) and A.4), the second term of the right-hand side of (104) can be written as\n\u03c32zE { u(n\u2212 i)w\u0303>(n)u(n)w\u0303i(n) } ' \u03c32zE { w\u0303>(n) } riE{w\u0303i(n)}. (106)\nFinally, using (105) and (106) in (104) one obtains (34)."}], "references": [{"title": "Adaptive Filter Theory, 4th ed", "author": ["S. Haykin"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Diffusion adaptation over networks", "author": ["\u2014\u2014"], "venue": "Academic Press Libraray in Signal Processing, R. Chellapa and S. Theodoridis, Eds. Elsevier, 2013. Also available as arXiv:1205.4220 [cs.MA], May 2012., pp. 322\u2013 454.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Multitask diffusion adaptation over networks", "author": ["J. Chen", "C. Richard", "A.H. Sayed"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 16, pp. 4129\u2013 4144, Aug. 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Diffusion LMS over multitask networks", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Signal Process., vol. 63, no. 11, pp. 2733\u20132748, Jun. 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "System identification under non-negativity constraints", "author": ["J. Chen", "C. Richard", "P. Honeine", "H. Lant\u00e9ri", "C. Theys"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), Aalborg, Denmark, Aug. 2010, pp. 1728\u20131732.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Solving Least Squares Problems", "author": ["C.L. Lawson", "R.J. Hanson"], "venue": "Philadelphia, PA: Society for Indust. and Appl. Math.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "A fast non-negativity-constrained least squares algorithm", "author": ["R. Bro", "S.D. Jong"], "venue": "J. of Chemometr., vol. 11, no. 5, pp. 393\u2013401, Sep. 1997.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Fast algorithm for the solution of large-scale non-negativity-constrained least squares problems", "author": ["M.H.V. Benthem", "M.R. Keenan"], "venue": "J. of Chemometr., vol. 18, no. 10, pp. 441\u2013450, Oct. 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonnegative leastsquares image deblurring: improved gradient projection approaches", "author": ["F. Benvenuto", "R. Zanella", "L. Zanni", "M. Bertero"], "venue": "Inverse Problems, vol. 26, no. 2, pp. 25 004\u201325 021, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, Oct. 1999.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "On the convergence of multiplicative update algorithms for nonnegative matrix factorization", "author": ["C.J. Lin"], "venue": "IEEE Trans. Neural Netw., vol. 18, no. 6, pp. 1589\u20131596, Nov. 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Stability analysis of multiplicative update algorithms and application to nonnegative matrix factorization", "author": ["R. Badeau", "N. Bertin", "E. Vincent"], "venue": "IEEE Trans. Neural Netw., vol. 21, no. 12, pp. 1869\u2013 1881, Dec. 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1869}, {"title": "Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition", "author": ["K. Huang", "N.S.A. Swami"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 1, pp. 211\u2013224, Jan. 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonnegative matrix and tensor factorizations: An algorithmic perspective", "author": ["G. Zhou", "A. Cichocki", "Q. Zhao", "S. Xie"], "venue": "IEEE Signal Process. Mag., vol. 31, no. 3, pp. 54\u201365, May 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonnegative least-mean-square algorithm", "author": ["J. Chen", "C. Richard", "J.C.M. Bermudez", "P. Honeine"], "venue": "IEEE Trans. Signal Process., vol. 59, no. 11, pp. 5225\u20135235, Nov. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Steadystate performance of non-negative least-mean-square algorithm and its variants", "author": ["J. Chen", "J.C.M. Bermudez", "C. Richard", "P. Honeine"], "venue": "IEEE Signal Process. Lett., vol. 21, no. 8, pp. 928\u2013932, Aug. 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Variants of non-negative least-mean-square algorithm and convergence analysis", "author": ["J. Chen", "C. Richard", "J.C.M. Bermudez", "P. Honeine"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 15, pp. 3990\u20134005, Aug. 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Reweighted nonnegative least-mean-square algorithm", "author": ["J. Chen", "C. Richard", "J.C.M. Bermudez"], "venue": "submitted for publication.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 0}, {"title": "The least mean fourth (LMF) adaptive algorithm and its family", "author": ["E. Walach", "B. Widrow"], "venue": "IEEE Trans. Inf. Theory, vol. 30, no. 2, pp. 275\u2013283, Mar. 1984.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1984}, {"title": "An improved statistical analysis of the least mean fourth (LMF) adaptive algorithm", "author": ["P.I. Hubscher", "J.C.M. Bermudez"], "venue": "IEEE Trans. Signal Process., vol. 51, no. 3, pp. 664\u2013671, Mar. 2003.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Probability of divergence for the least-mean fourth algorithm", "author": ["V.H. Nascimento", "J.C.M. Bermudez"], "venue": "IEEE Trans. Signal Process., vol. 54, no. 4, pp. 1376\u20131385, Apr. 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "A meansquare stability analysis of the least mean fourth adaptive algorithm", "author": ["P.I. Hubscher", "J.C.M. Bermudez", "V.H. Nascimento"], "venue": "IEEE Trans. Signal Process., vol. 55, no. 8, pp. 4018\u20134028, Aug. 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Convergence and tracking analysis of a variable normalised LMF (XE-NLMF) algorithm", "author": ["A. Zerguine", "M.K. Chan", "T.Y. Al-Naffouri", "M. Moinuddin", "C.F.N. Cowan"], "venue": "Signal Process., vol. 89, no. 5, pp. 778\u2013 790, May 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Mean-square stability of the normalized least-mean fourth algorithm for white gaussian inputs", "author": ["N.J. Bershad", "J.C.M. Bermudez"], "venue": "Digital Signal Process., vol. 21, no. 6, pp. 694\u2013700, Dec. 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic analysis of a stable normalized least mean fourth algorithm for adaptive noise canceling with a white gaussian reference", "author": ["E. Eweda", "N.J. Bershad"], "venue": "IEEE Trans. Signal Process., vol. 60, no. 12, pp. 6235\u20136244, Dec. 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "New insights into the normalization of the least mean fourth algorithm", "author": ["E. Eweda", "A. Zerguine"], "venue": "Signal, Image, Video Process., vol. 7, no. 2, pp. 255\u2013262, Mar. 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Leaky least mean fourth adaptive algorithm", "author": ["O.R. Khattak", "A. Zerguine"], "venue": "IET Signal Process., vol. 7, no. 2, pp. 134\u2013145, Apr. 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Dependence of the stability of the least mean fourth algorithm on target weights non-stationarity", "author": ["E. Eweda"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 7, pp. 1634\u20131643, Apr. 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Mean-square stability analysis of a normalized least mean fourth algorithm for a markov plant", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 24, pp. 6545\u20136553, Dec. 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The Krylovproportionate normalized least mean fourth approach: Formulation and performance analysis", "author": ["M.O. Sayina", "Y. Yilmazb", "A. Demirc", "S.S. Kozat"], "venue": "Signal Process., vol. 109, pp. 1\u201313, Apr. 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonnegative least-mean-fourth algorithm", "author": ["J. Yang", "J. Ni"], "venue": "Proc. International Conference on Wireless, Mobile and Multimedia Networks (ICWMMN), Nov. 2013, pp. 185\u2013188.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "ADAPTIVE filtering algorithms are widely used to address system identification problems in applications such as adaptive noise cancellation, echo cancellation, active noise control, and distributed learning [1]\u2013[5].", "startOffset": 207, "endOffset": 210}, {"referenceID": 3, "context": "ADAPTIVE filtering algorithms are widely used to address system identification problems in applications such as adaptive noise cancellation, echo cancellation, active noise control, and distributed learning [1]\u2013[5].", "startOffset": 211, "endOffset": 214}, {"referenceID": 4, "context": "Due to physical characteristics, some problems require the imposition of nonnegativity constraints on the parameters to estimate in order to avoid uninterpretable results [6].", "startOffset": 171, "endOffset": 174}, {"referenceID": 5, "context": ", the nonnegative least-squares [7]\u2013[10] and the nonnegative matrix factorization [11]\u2013[15]).", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": ", the nonnegative least-squares [7]\u2013[10] and the nonnegative matrix factorization [11]\u2013[15]).", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": ", the nonnegative least-squares [7]\u2013[10] and the nonnegative matrix factorization [11]\u2013[15]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": ", the nonnegative least-squares [7]\u2013[10] and the nonnegative matrix factorization [11]\u2013[15]).", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "The nonnegative least-mean-square (NNLMS) algorithm was derived in [16] to address online system identification problems subject to nonnegativity constraints.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Its convergence behavior was analyzed in [16], [17].", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "Its convergence behavior was analyzed in [16], [17].", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "Variants of the NNLMS algorithm were proposed in [18] and [19] to address specific robustness and convergence issues of NNLMS algorithm.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Variants of the NNLMS algorithm were proposed in [18] and [19] to address specific robustness and convergence issues of NNLMS algorithm.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "The LMF algorithm was proposed in [20], where it was verified that it could outperform the LMS algorithm in the presence of non-Gaussian measurement noise.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "This desirable property has led to a series of studies about the convergence behaviors of the LMF algorithm and some of its variants [21]\u2013[31].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "This desirable property has led to a series of studies about the convergence behaviors of the LMF algorithm and some of its variants [21]\u2013[31].", "startOffset": 138, "endOffset": 142}, {"referenceID": 30, "context": "Recently, a nonnegative LMF (NNLMF) algorithm was proposed in [32] to improve the performance of the NNLMS algorithm under non-Gaussian measurement noise.", "startOffset": 62, "endOffset": 66}, {"referenceID": 30, "context": "It was shown in [32] that, when compared to the NNLMS algorithm, the NNLMF algorithm can lead to faster convergence speed and equivalent steady-state performance, and to improved steady-state performance for the same convergence speed.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "The results shown in [32] were exclusively based on Monte Carlo simulations.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "It is known that the study of the stability of the LMF algorithm is relatively complex [22], [23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "It is known that the study of the stability of the LMF algorithm is relatively complex [22], [23].", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "For nonnegative system identification, nonnegativity constraints are imposed on the estimated weights, leading to the constrained optimization problem [6]", "startOffset": 151, "endOffset": 154}, {"referenceID": 14, "context": "Based on the Karush-Kuhn-Tucker conditions, the authors in [16] derived a fixed-point iteration scheme to address the constrained optimization problem (2).", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "To improve the convergence performance of the adaptive filter for non-Gaussian measurement noise, the authors in [32] proposed to replace the MSE criterion with the mean fourth error (MFE) criterion", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "2) is the well-known independence assumption, which has been successfully used in the analysis of many adaptive algorithms, including the LMF algorithm [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "via a first-order Taylor expansion as done in [18].", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "4) as well as the approximation E{w\u0303i(n)w\u0303j(n)} ' E{w\u0303i(n)}E{w\u0303j(n)},\u2200i, j, for the reason that the mean weight behavior is usually not sensitive in such kind of approximation (see [16] and [18] for detailed explanation), we have E{qi,a(n)} ' \u22122E { z(n)u(n\u2212 i)w\u0303> E,i(n)u(n) } w\u2217 i = \u22122\u03c3 zw i E { w\u0303>(n) } ri (49) E{qi,b(n)} ' \u22122E { z(n)u(n\u2212 i)w\u0303> E,i(n)u(n)E{w\u0303i(n)} }", "startOffset": 181, "endOffset": 185}, {"referenceID": 16, "context": "4) as well as the approximation E{w\u0303i(n)w\u0303j(n)} ' E{w\u0303i(n)}E{w\u0303j(n)},\u2200i, j, for the reason that the mean weight behavior is usually not sensitive in such kind of approximation (see [16] and [18] for detailed explanation), we have E{qi,a(n)} ' \u22122E { z(n)u(n\u2212 i)w\u0303> E,i(n)u(n) } w\u2217 i = \u22122\u03c3 zw i E { w\u0303>(n) } ri (49) E{qi,b(n)} ' \u22122E { z(n)u(n\u2212 i)w\u0303> E,i(n)u(n)E{w\u0303i(n)} }", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "However, the effect of the second-order moments of the weight-error vector on the EMSE behavior becomes more significant [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "In [21], the expected value of \u039e(n) for zero-mean Gaussian inputs has been approximated using assumptions A.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "It was shown in [16] that E { Du(n)w\u0303(n)w\u0303 (n)Du(n) } ' R \u25e6K(n).", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "We find that the above quantities,\u03984a(n)\u2013\u03984d(n), correspond to equations (45)\u2013(48) in [16], respectively, which have been computed under assumptions A.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "Therefore, the results obtained in [16] can be used directly here, yielding", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "The above simulation setups are the same as those in [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "The stability analysis for the LMF algorithm is relatively complex [22], [23], and this complexity increases for the NNLMF algorithm.", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "The stability analysis for the LMF algorithm is relatively complex [22], [23], and this complexity increases for the NNLMF algorithm.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "(100) The following approximation has been derived in [21]: E { u(n) [ w\u0303>(n)u(n) ]3} ' 3Tr{RK(n)}RE{w\u0303(n)}.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "was used in the mean weight behavior analysis of the NNLMS, for which the detailed explanation was given in [16].", "startOffset": 108, "endOffset": 112}], "year": 2015, "abstractText": "Some system identification problems impose nonnegativity constraints on the parameters to estimate due to inherent physical characteristics of the unknown system. The nonnegative least-mean-square (NNLMS) algorithm and its variants allow to address this problem in an online manner. A nonnegative least mean fourth (NNLMF) algorithm has been recently proposed to improve the performance of these algorithms in cases where the measurement noise is not Gaussian. This paper provides a first theoretical analysis of the stochastic behavior of the NNLMF algorithm for stationary Gaussian inputs and slow learning. Simulation results illustrate the accuracy of the proposed analysis.", "creator": "LaTeX with hyperref package"}}}