{"id": "1509.01618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Efficient Sampling for k-Determinantal Point Processes", "abstract": "determinantal point processes ( dpps ) deliver probabilistic models over discrete sets having items that help model repulsion distribution diversity. applicability of dpps to large sets of data is, however, hindered by the classical matrix computational involved, especially when sampling. we therefore constructed a new efficient approximate two - stage mapping algorithm for discrete distribution - dpps. as opposed to previous approximations, our algorithm specializes at minimizing the variational distance to the target distribution. experiments indicate that generally resulting sampling algorithm works well on large data and yields more accurate samples than previous approaches.", "histories": [["v1", "Fri, 4 Sep 2015 21:38:17 GMT  (140kb,D)", "https://arxiv.org/abs/1509.01618v1", null], ["v2", "Sat, 28 May 2016 00:37:56 GMT  (176kb,D)", "http://arxiv.org/abs/1509.01618v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chengtao li", "stefanie jegelka", "suvrit sra"], "accepted": false, "id": "1509.01618"}, "pdf": {"name": "1509.01618.pdf", "metadata": {"source": "CRF", "title": "Efficient Sampling for k-Determinantal Point Processes", "authors": ["Chengtao Li", "Stefanie Jegelka", "Suvrit Sra"], "emails": ["ctli@mit.edu", "stefje@csail.mit.edu", "suvrit@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Subset selection problems lie at the heart of many applications where a small subset of items must be selected to represent a larger population. Typically, the selected subsets are expected to fulfill various criteria such as sparsity, grouping, or diversity. Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].\nDiverse subset selection amounts to sampling from the set of all subsets of a ground set according to a measure that places more mass on subsets with qualitatively different items. An elegant realization of this idea is given by Determinantal Point Processes (Dpps), which are probabilistic models that capture diversity by assigning subset probabilities proportional to (sub)determinants of a kernel matrix.\nDpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33]. But despite being polynomial-time, these tasks remain infeasible for large data sets. Dpp sampling, for example, relies on an eigendecomposition of the Dpp kernel, whose cubic complexity is a huge impediment. Cubic preprocessing costs also impede wider use of the cardinality constrained variant k-Dpp [32].\nThese drawbacks have triggered work on approximate sampling methods. Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystro\u0308m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation. However, these methods are somewhat inappropriate for sampling because they aim to project the Dpp kernel onto a lower dimensional space while minimizing a matrix norm, rather than minimizing an error measure sensitive to\nar X\niv :1\n50 9.\n01 61\n8v 2\n[ cs\n.L G\n] 2\n8 M\nay 2\n01 6\ndeterminants. Alternative methods use a dual formulation [30], which however presupposes a decomposition L = XX> of the DPP kernel, which may be unavailable and inefficient to compute in practice. Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.\nWe pursue a yet different approach. While being similar to matrix approximation methods in exploiting redundancy in the data, in sharp contrast to methods that minimize matrix norms, we focus on minimizing the total variation distance between the original Dpp and our approximation. As a result, our approximation models the true Dpp probability distribution more faithfully, while permitting faster sampling. We make the following key contributions:\n\u2013 An algorithm that constructs coresets for approximating a k-Dpp by exploiting latent structure in the data. The construction, aimed at minimizing the total variation distance, takes O(NM3) time; linear in the number N of data points. The construction works as the overhead of sampling algorithm and is much faster than standard cubic-time overhead that exploits eigendecomposition of kernel matrices. We also investigate conditions under which such an approximation is good.\n\u2013 A sampling procedure that yields approximate k-Dpp subsets using the constructed coresets. While most other sampling methods sample diverse subsets in O(k2N) time, the sampling time for our coreset-based algorithm is O(k2M), where M N is a user-specified parameter independent of N.\nOur experiments indicate that our construction works well for a wide range of datasets, delivers more accurate approximations than the state-of-the-art, and is more efficient, especially when multiple samples are required.\nOverview of our approach. Our sampling procedure runs in two stages. Its first stage constructs an approximate probability distribution close in total variation distance to the true k-Dpp. The next stage efficiently samples from this approximate distribution.\nOur approximation is motivated by the diversity sampling nature of Dpps: in a Dpp most of the probability mass will be assigned to diverse subsets. This leaves room for exploiting redundancy. In particular, if the data possesses latent grouping structure, certain subsets will be much more likely to be sampled than others. For instance, if the data are tightly clustered, then any sample that draws two points from the same cluster will be very unlikely.\nThe key idea is to reduce the effective size of the ground set. We do this via the idea of coresets [17, 25], small subsets of the data that capture function values of interest almost as well as the full dataset. Here, the function of interest is a k-Dpp distribution. Once a coreset is constructed, we can sample a subset of core points, and then, based on this subset, sample a subset of the ground set. For a coreset of size M, our sampling time is O(k2M), which is independent of N since we are using k-Dpps [32].\nRelated work. Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44]. Cardinality-conditioned Dpp sampling is also referred to as \u201cvolume sampling\u201d, which has been used for matrix approximations [15, 16]. Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28]. Except for MCMC, even if we exclude preprocessing, known sampling methods still require O(k2N) time for a single sample; we reduce this to O(k2M). Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].\nCoresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42]."}, {"heading": "2 Setup and basic definitions", "text": "A determinantal point process Dpp(L) is a distribution over all subsets of a ground set Y of cardinality N. It is determined by a positive semidefinite kernel L \u2208 RN\u00d7N . Let LY be the submatrix of L consisting of the entries Lij with i, j \u2208 Y \u2286 Y . Then, the probability PL(Y) of observing Y \u2286 Y is proportional to det(LY); consequently, PL(Y) = det(LY)/ det(L + I). Conditioning on sampling sets of fixed cardinality k, one obtains a k-Dpp [32]:\nPL,k(Y) : = PL(Y | |Y| = k) = det(LY)ek(L)\u22121J |Y| = kK,\nwhere ek(L) is the k-th coefficient of the characteristic polynomial det(\u03bbI\u2212 L) = \u2211Nk=0(\u22121)kek(L)\u03bbN\u2212k. We assume that PL,k(Y) > 0 for all subsets Y \u2286 Y of cardinality k. To simplify notation, we also write Pk , PL,k.\nOur goal is to construct an approximation P\u0302k to Pk that is close in total variation distance\n\u2016P\u0302k \u2212 Pk\u2016tv := 12 \u2211 Y\u2286Y ,|Y|=k |P\u0302k(Y)\u2212 Pk(Y)|, (2.1)\nand permits faster sampling than Pk. Broadly, we proceed as follows. First, we define a partition \u03a0 = {Y1, . . . ,YM} of Y and extract a subset C \u2282 Y of M core points, containing one point from each part. Then, for the set C we construct a special kernel L\u0303 (as described in Section 3). When sampling, we first sample a set Y\u0303 \u223c Dppk(L\u0303) and then, for each c \u2208 Y\u0303 we uniformly sample one of its assigned points y \u2208 Yc. These second-stage points y form our final sample. We denote the resulting distribution by P\u0302k = PC,k. Algorithm 1 formalizes the sampling procedure, which, after one eigendecomposition of the small matrix L\u0303.\nWe begin by analyzing the effect of the partition on the approximation error, and then devise an algorithm to approximately minimize the error. We empirically evaluate our approach in Section 6."}, {"heading": "3 Coreset sampling", "text": "Let \u03a0 = {Y1, . . . ,YM} be a partition of Y , i.e., \u222aMi=1Yi = Y and Yi \u2229Yj = \u2205 for i 6= j. We call C \u2286 Y a coreset with respect to a partition \u03a0 if |C \u2229 Yi| = 1 for i \u2208 [M]. With a slight abuse of notation, we index each part Yc \u2208 \u03a0 by its core c \u2208 C \u2229 Yc. Based on the partition \u03a0, we call a set Y \u2286 Y singular1 with respect to \u03a0\u2032 \u2286 \u03a0, if for Yi \u2208 \u03a0\u2032 we have |Y \u2229 Yi| \u2264 1 and for Yj \u2208 \u03a0\\\u03a0\u2032 we have |Y \u2229 Yj| = 0. We say Y is k-singular if Y is singular and |Y| = k.\nGiven a partition \u03a0 and core C, we construct a rescaled core kernel L\u0303 \u2208 RM\u00d7M with entries L\u0303c,c\u2032 = \u221a |Yc||Yc\u2032 |Lc,c\u2032 . We then use this smaller matrix L\u0303 and its eigendecomposition as an input to our two-stage sampling procedure in Algorithm 1, which we refer to as CoreDpp. The two stages are: (i) sample a k-subset from C according to Dppk(L\u0303); and (ii) for each c, pick an element y \u2208 Yc uniformly at random. This algorithm uses only the much smaller matrix L\u0303 and samples a subset from Y in O(k2M) time. When M N and we want many samples, it translates into a notable improvement over the O(k2N) time of sampling directly from Dppk(L).\nThe following lemma shows that CoreDpp is equivalent to sampling from a k-Dpp where we replace each point in Y by its corresponding core point, and sample with the resulting induced kernel LC(Y).\nLemma 1. CoreDpp is equivalent to sampling from Dppk(LC(Y)), where in LC(Y) we replace each element in Yc by c, for all c \u2208 C.\n1In combinatorial language, Y is an independent set in the partition matroid defined by \u03a0.\nAlgorithm 1: CoreDpp Sampling\nInput: core kernel L\u0303 \u2208 RM\u00d7M and its eigendecomposition; partition \u03a0; size k sample C \u223c Dppk(L\u0303) sample yi \u223c Uniform(Yc) for c \u2208 C return Y = {y1, . . . , yk}\nProof. We denote the distribution induced by Algo. 1 by PC,k and that induced by Dppk(LC(Y)) by P\u2032k.\nFirst we claim that both sampling algorithms can only sample k-singular subsets. By construction, PC,k picks one or zero elements from each Yc. For P\u2032k, if Y is k-nonsingular, then there would be identical rows in (LC(Y))Y = LC(Y), resulting in det(LC(Y)) = 0. Hence both PC,k and P\u2032k only assign nonzero probability to k-singular sets Y. As a result, we have\nek(LC(Y)) = \u2211 C is k-singular (\u220f c\u2208C |Yc|)det(LC)\n= \u2211 C\u2286C,|C|=k (\u220f c\u2208C |Yc|det(LC)) = \u2211 |C|=k det(L\u0303C) = ek(L\u0303).\nFor any Y = {y1, . . . , yk} \u2286 Y that is k-singular, we have\nPC,k(Y) = det(L\u0303C(Y))\nek(L\u0303)\u220fki=1 |YC(yi)| = (\u220fki=1 |YC(yi)|)det(LC(Y)) ek(LC(Y))\u220f k i=1 |YC(yi)|\n= det(LC(Y)) ek(LC(Y)) = P\u2032k(Y),\nwhich shows that these two distributions are identical, i.e., sampling from Dppk(L\u0303) followed by uniform sampling is equivalent to directly sampling from Dppk(LC(Y))."}, {"heading": "4 Partition, distortion and approximation error", "text": "Let us provide some insight on quantities that affect the distance \u2016PC,k \u2212 Pk\u2016tv when sampling with Algo. 1. In a nutshell, this distance depends on three key quantities (defined below): the probability of nonsingularity \u03b4\u03a0, the distortion factor 1 + \u03b5\u03a0, and the normalization factor.\nFor a partition \u03a0 we define the nonsingularity probability \u03b4\u03a0 as the probability that a draw Y \u223c Dppk(L) is not singular with respect to any \u03a0\u2032 \u2286 \u03a0.\nGiven a coreset C, we define the distortion factor 1 + \u03b5\u03a0 (for \u03b5\u03a0 \u2265 0) as a partition-dependent quantity, so that for any c \u2208 C, for all u, v \u2208 Yc, and for any (k\u2212 1)-singular set S with respect to \u03a0 \\ Yc the following bound holds:\ndet(LS\u222a{u}) det(LS\u222a{v}) = Lu,u \u2212 Lu,SL\u22121S LS,u Lv,v \u2212 Lv,SL\u22121S LS,v \u2264 1 + \u03b5\u03a0. (4.1)\nIf \u03c6 is the feature map corresponding to the kernel L, then geometrically, the numerator of (4.1) is the length of the projection of \u03c6(u) onto the orthogonal complement of span{\u03c6(s) | s \u2208 S}.\nThe normalization factor for a k-Dpp (L) is simply ek(L). Given \u03a0, C and the corresponding nonsingularity probability and distortion factors, we have\nthe following bound:\nLemma 2. Let Y \u223c Dppk(L) and C(Y) be the set where we replace each y \u2208 Y by its core c \u2208 C, i.e., y \u2208 Yc. With probability 1\u2212 \u03b4\u03a0, it holds that\n(1 + \u03b5\u03a0)\u2212k \u2264 det(LC(Y))\ndet(LY) \u2264 (1 + \u03b5\u03a0)k. (4.2)\nProof. Let c \u2208 C and consider any (k \u2212 1)-singular set S with respect to \u03a0 \\ Yc. Then, for any v \u2208 Yc, using Schur complements and by the definition of \u03b5\u03a0 we see that\n(1 + \u03b5\u03a0)\u22121 \u2264 det(LS\u222a{c}) det(LS\u222a{v}) = Lc,c \u2212 Lc,SL\u22121S LS,c Lv,v \u2212 Lv,SL\u22121S LS,v\n= ||QS\u22a5\u03c6(c)||2 ||QS\u22a5\u03c6(v)||2 \u2264 (1 + \u03b5\u03a0).\nHere, QS\u22a5 is the projection onto the orthogonal complement of span{\u03c6(s) | s \u2208 S}, and \u03c6 the feature map corresponding to the kernel L.\nWith a minor abuse of notation, we denote by C(y) = c the core point corresponding to y, i.e., y \u2208 Yc. For any Y = {y1, . . . , yk}, we then define the sets Yi = {C(y1), . . . , C(yi), yi+1, . . . , yk}, where we gradually replace each point by its core point, with Y0 = Y. If Y is k-singular, then C(yi) 6= C(yj) whenever i 6= j, and, for any 0 \u2264 i \u2264 k\u2212 1, it holds that\n(1 + \u03b5\u03a0)\u22121 \u2264 det(LYi+1) det(LYi ) \u2264 1 + \u03b5\u03a0.\nHence we have\n(1 + \u03b5\u03a0)\u2212k \u2264 det(LC(Y))\ndet(LY) = k\u22121 \u220f i=0 det(LYi+1) det(LYi ) \u2264 (1 + \u03b5\u03a0)k.\nThis bound holds when Y is k-singular, and, by definition of \u03b4\u03a0, this happens with probability 1\u2212 \u03b4\u03a0.\nAssuming \u03b5\u03a0 is small, Lemma 2 states that if replacing a single element in a given subset with another one in the same part does not cause much distortion, then replacing all elements in the subset with their corresponding cores will cause little distortion. This observation is key to our approximation: if we can construct such a partition and coreset, we can safely replace all elements with core points and then approximately sample with little distortion. More precisely, we then obtain the following result that bounds the variational error. Our subsequent construction aims to minimize this bound.\nTheorem 3. Let Pk = Dppk(L) and let PC,k be the distribution induced by Algo. 1. With the normalization factors Z = ek(L) and ZC = ek(L\u0303), the total variation distance between Pk and PC,k is bounded by\n\u2016Pk \u2212 PC,k\u2016tv \u2264 |1\u2212 ZCZ |+ k\u03b5\u03a0 + (1\u2212 k\u03b5\u03a0)\u03b4\u03a0.\nProof. From the definition of Z and ZC we know that Z = \u2211|Y|=k det(LY) and\nZC = \u2211 |Y|=k det((LC(Y))Y) = \u2211 |Y|=k det(LC(Y))\n= \u2211 Y k-singular det(LC(Y)).\nThe last equality follows since, as argued above, det(LC(Y)) = 0 for nonsingular Y. It follows that\n\u2016Pk \u2212 PC,k\u2016tv = \u2211 |Y|=k |Pk(Y)\u2212 PC,k(Y)|\n= \u2211 Y k-singular |Pk(Y)\u2212 PC,k(Y)|+ \u2211 Y k-nonsingular Pk(Y). (4.3)\nFor the first term, we have\n\u2211 Y k-singular |Pk(Y)\u2212 PC,k(Y)| = \u2211 Y k-singular\n\u2223\u2223\u2223det(LY) Z \u2212 det(LC(Y))\nZC \u2223\u2223\u2223 \u2264 \u2211\nY k-singular\n\u2223\u2223\u2223 1 Z (det(LY)\u2212 det(LC(Y))) \u2223\u2223\u2223+ \u2211 Y k-singular \u2223\u2223\u2223det(LC(Y))( 1Z \u2212 1ZC )\u2223\u2223\u2223\n= 1 Z \u2211Y k-singular\ndet(LY) \u2223\u2223\u22231\u2212 det(LC(Y))\ndet(LY) \u2223\u2223\u2223+ ZC \u2223\u2223\u2223 1Z \u2212 1ZC \u2223\u2223\u2223\n\u2264 k\u03b5\u03a0(1\u2212 \u03b4\u03a0) + \u2223\u2223\u22231\u2212 ZC\nZ \u2223\u2223\u2223, where the first inequality uses the triangle inequality and the second inequality relies on Lemma 2. For the second term in (4.3), we use that, by definition of \u03b4\u03a0,\n\u2211 Y k-nonsingular Pk(Y) = \u03b4\u03a0.\nThus the total variation difference is bounded as \u2016Pk \u2212 PC,k\u2016tv \u2264 \u2223\u2223\u22231\u2212 ZC\nZ \u2223\u2223\u2223+ k\u03b5\u03a0(1\u2212 \u03b4\u03a0) + \u03b4\u03a0 = \u2223\u2223\u22231\u2212 ZC\nZ \u2223\u2223\u2223+ k\u03b5\u03a0 + (1\u2212 k\u03b5\u03a0)\u03b4\u03a0. In essence, if the probability of nonsingularity and the distortion factor are low, then it is possible to obtain a good coreset approximation. This holds, for example, if the data has intrinsic (grouping) structure. In the next subsection we provide further intuition on when we can achieve low error.\n4.1 Sufficient conditions for a good bound\nTheorem 3 depends on the data and the partition \u03a0. Here, we aim to obtain some further intuition on the properties of \u03a0 that govern the bound. At the same time, these properties suggest sufficient conditions for a \u201cgood\u201d coreset C. For each Yc, we define the diameter\n\u03c1c := max u,v\u2208Yc\n\u221a Luu + Lvv \u2212 2Luv. (4.4)\nNext, define the minimum distance of any point u \u2208 Yc to the subspace spanned by the feature vectors of points in a \u201ccomplementary\u201d set S that is singular with respect to \u03a0 \\ Yc:\ndc := min S,u\n\u221a det(LS\u222a{u})\ndet(LS) = min\nS,u\n\u221a Lu,u \u2212 Lu,SL\u22121S LS,u.\nLemma 4 connects these quantities with \u03b5\u03a0; it essentially poses a separability condition on \u03a0 (i.e., \u03a0 needs to be \u201caligned\u201d with the data) so that the bound on \u03b5\u03a0 holds.\nLemma 4. If dc > \u03c1c for all c \u2208 C, then\n\u03b5\u03a0 \u2264 max c\u2208C (2dc \u2212 \u03c1c)\u03c1c (dc \u2212 \u03c1c)2 . (4.5)\nProof. For any c \u2208 C and any u, v \u2208 Yc and S (k\u2212 1)-singular with respect to \u03a0\\Yc, we have\ndet(LS\u222a{u}) det(LS\u222a{v}) = det(LS)(Lu,u \u2212 Lu,SL\u22121S LS,u) det(LS)(Lv,v \u2212 Lvi ,SL \u22121 S LS,vi )\n= Lu,u \u2212 Lu,SL\u22121S LS,u Lv,v \u2212 Lv,SL\u22121S LS,v = \u2016QS\u22a5\u03c6(u)\u20162 \u2016QS\u22a5\u03c6(v)\u20162 .\nWithout loss of generality, we assume det(LS\u222a{u}) \u2265 det(LS\u222a{v}). By definition of \u03c1c we know that\n0 \u2264 \u2016QS\u22a5\u03c6(u)\u2016 \u2212 \u2016QS\u22a5\u03c6(v)\u2016 \u2264 \u2016QS\u22a5(\u03c6(u)\u2212 \u03c6(v))\u2016 \u2264 \u2016\u03c6(u)\u2212 \u03c6(v)\u2016 \u2264 \u03c1c.\nSince 0 < \u2016QS\u22a5\u03c6(v)\u2016 \u2264 \u2016QS\u22a5\u03c6(u)\u2016 \u2264 \u2016\u03c6(u)\u2016 by assumption, we have\n\u2016QS\u22a5\u03c6(u)\u20162 \u2016QS\u22a5\u03c6(v)\u20162 \u2264 \u2016QS\u22a5\u03c6(u)\u20162 (\u2016QS\u22a5\u03c6(u)\u2016 \u2212 \u03c1c)2\n\u2264 ( \u2016\u03c6(u)\u2016 \u2016\u03c6(u)\u2016 \u2212 \u03c1c )2 \u2264 ( dc (dc \u2212 \u03c1c) )2 .\nThen, by definition of \u03b5\u03a0, we have\n1 + \u03b5\u03a0 \u2264 maxc d2c (dc \u2212 \u03c1c)2 ,\nfrom which it follows that\n\u03b5\u03a0 \u2264 maxc (2dc \u2212 \u03c1c)\u03c1c (dc \u2212 \u03c1c)2 ."}, {"heading": "5 Efficient construction", "text": "Thm. 3 states an upper bound on the error induced by CoreDpp and relates the total variation distance to \u03a0 and C. Next, we explore how to efficiently construct \u03a0 and C that approximately minimize the upper bound.\n5.1 Constructing \u03a0\nAny set Y sampled via CoreDpp is, by construction, singular with respect to \u03a0. In other words, CoreDpp assigns zero mass to any nonsingular set. Hence, we wish to construct a partition \u03a0 such that its nonsingular sets have low probability under Dppk(L). The optimal such partition minimizes the probability \u03b4\u03a0 of nonsingularity. A small \u03b4\u03a0 value also means that the parts of \u03a0 are dense and compact, i.e., the diameter \u03c1c in Equation (4.4) is small.\nFinding such a partition optimally is hard, so we resort to local search. Starting with a current partition \u03a0, we re-assign each y to a part Yc to minimize \u03b4\u03a0. If we assign y to Yc, then the\nprobability of sampling a set Y that is singular with respect to the new partition \u03a0 is\nP[Y \u223c Dppk(L) is singular] = 1 Z \u2211Yk-singular det(LY)\n= 1 Z\n( \u2211\nYk-sing.,y/\u2208Y det(LY) + \u2211 Yk-sing.,y\u2208Y det(LY) ) =\n1 Z\n( const + \u2211\nY\u2032 (k\u22121)-sing. w.r.t \u03a0 \\ Yc det(LY\u2032\u222a{y}) ) =\n1 Z ( const + Lyys\u03a0k\u22121(L y 8c) ) ,\nwhere s\u03a0k (L) := \u2211Y k-sing. det(LY). The matrix L8c denotes L with rows Yc and columns Yc deleted, and Ly = L\u2212 LY ,yLy,Y . For local search, we would hence compute Lyys\u03a0k\u22121(L y 8c) for each point y and core c, assign y to the highest-scoring c. Since this testing is still expensive, we introduce further speedups in Section 5.3.\n5.2 Constructing C When constructing C, we aim to minimize the upper bound on the total variation distance between Pk and PC,k stated in Theorem 3. Since \u03b4\u03a0 and \u03b5\u03a0 only depend on \u03a0 and not on C, we here focus on minimizing |1\u2212 ZCZ |, i.e., bringing ZC as close to Z as possible. To do so, we again employ local search and subsequently swap each c \u2208 C with its best replacement v \u2208 Yc. Let Cc,v be C with c replaced by v. We aim to find the best swap\nv = argminv\u2208Yc |Z\u2212 ZCc,v | (5.1) = argminv\u2208Yc |Z\u2212 ek(LCc,v(Y))|. (5.2)\nComputing Z requires computing the coefficients ek(L), which takes a total of O(N3) time2. In the next section, we therefore consider a fast approximation.\n5.3 Faster constructions and further speedups\nLocal search procedures for optimizing \u03a0 and C can be further accelerated by a sequence of relaxations that we found to work well in practice (see Section 6). We begin with the quantity s\u03a0k\u22121(L y 8c) that involves summing over sub-determinants of the large matrix L. Assuming the initialization is not too bad, we can use the current C to approximate Y . In particular, when re-assigning y, we substitute all other elements with their corresponding cores, resulting in the kernel L\u0302 = LC(Y). This changes our objective to finding the c \u2208 C that maximizes s\u03a0k\u22121(L\u0302 y 8c). Key to a fast approximation is now Lemma 5, which follows from Lemma 1.\nLemma 5. For all k \u2264 |\u03a0|, it holds that\ns\u03a0k (LC(Y)) = ek(LC(Y)) = ek(L\u0303).\n2In theory, this can be computed in O(N\u03c9 log(N)) time [13], but the eigendecompositions and dynamic programming used in practice typically take cubic time.\nAlgorithm 2: Iterative construction of \u03a0 and C Require: \u03a0 initial partition; C initial coreset; k the size of sampled subset; \u03bd number of nearest neighbors\ntaken into consideration while not converged do\nfor all y \u2208 Y do c\u2190 group in which y lies currently: y \u2208 Yc if y \u2208 C then\ncontinue end if G \u2190 {groups of \u03bd cores nearest to Xy} g\u2217 = argmaxg\u2208Gs \u03a0 k\u22121(L\u0302 y 8g) if c 6= g\u2217 then Yc = Yc\\{y} Yg\u2217 = Yg\u2217 \u222a {y} if ek(LCg\u2217 ,j(Y)) > ek(LCc,j(Y)) then C \u2190 Cg\u2217 ,y\nend if end if\nend for for all g \u2208 [M] do\nj = argmaxj\u2208Yg ek(LCg,j(Y))\nC = Cg,j end for\nend while\nProof.\ns\u03a0k (LC(Y)) = \u2211 Y k-sing. det((LC(Y))Y) = \u2211 Y k-sing. det(LC(Y))\n= \u2211 |Y|=k det(LC(Y)) = ek(LC(Y)) = ek(L\u0303);\nthe last equality was shown in the proof of Thm. 3. Computing the normalizer ek(L\u0303) only needs O(M3) time. We refer to this acceleration as CoreDpp -z. Second, when constructing C, we observed that ZC is commonly much smaller than Z. Hence, a fast approximation merely greedily increases ZC without computing Z. Third, we can be lazy in a number of updates: for example, we only consider changing cores for the part that changes. When a part Yc receives a new member, we check whether to switch the current core c to the new member. This reduction keeps the core adjustment at time O(M3). Moreover, when re-assigning an element y to a different part Yc, it is usually sufficient to only check a few, say, \u03bd parts with cores closest to y, and not all parts. The resulting time complexity for each element is O(M3).\nWith this collection of speedups, the approximate construction of \u03a0 and C takes O(NM3) for each iteration, which is linear in N, and hence a huge speedup over direct methods that require O(N3) preprocessing. The iterative algorithm is shown in Algorithm 2. The initialization also affects the algorithm performance, and in practice we find that kmeans++ as an initialization works well. Thus we use CoreDpp to refer to the algorithm that is initialized with kmeans++ and uses all the above accelerations. In practice, the algorithm converges very quickly, and most of the progress occurs in the first pass through the data. Hence, if desired, one can even use early stopping."}, {"heading": "6 Experiments", "text": "We next evaluate CoreDpp, and compare its efficiency and effectiveness against three competing approaches: - Partitioning using k-means (with kmeans++ initialization [7]), with C chosen as the centers of\nthe clusters; referred to as K++ in the results. - The adaptive, stochastic Nystro\u0308m sampler of [3] (NysStoch). We used M dimensions for NysStoch,\nto use the same dimensionality as CoreDpp. - The Metropolis-Hastings DPP sampler MCDPP [28]. We use the well-known Gelman and Rubin\nmultiple sequence diagnostic [19] to empirically judge mixing.\nIn addition, we show results using different variants of CoreDpp: CoreDpp -z described in Sec. 5.3 and variants that are initialized either randomly (CoreDpp -r) or via kmeans++ (CoreDpp).\n6.1 Synthetic Dataset\nWe first explore the effect of our fast approximate sampling on controllable synthetic data. The experiments here compare the accuracy of the faster CoreDpp from Section 5.3 to CoreDpp -z, CoreDpp -r and K++.\nWe generate an equal number of samples from each of nClust 30-dimensional Gaussians with means of varying length (`2-norm) and unit variance, and then rescale the samples to have the same length. As the length of the samples increases, \u03b5\u03a0 and \u03b4\u03a0 shrink. Finally, L is a linear kernel. Throughout this experiment we set k = 4 and N = 60 to be able to exactly compute \u2016P\u0302k \u2212 Pk\u2016tv. We extract M = 10 core points and use \u03bd = 3 neighboring cores. Recall from Sec. 5.3 that when considering the parts that one element should be assigned to, it is usually sufficient to only check \u03bd parts with cores closest to y. Thus, \u03bd = 3 means we only consider re-assigning each element to its three closest parts.\nResults. Fig. 1 shows the total variation distance \u2016P\u0302k \u2212 Pk\u2016tv defined in Equation (2.1) for the partition and cores generated by K++, CoreDpp, CoreDpp -r and CoreDpp -z as nClust and the length vary. We see that in general, most approximations improve as \u03b5\u03a0 and \u03b4\u03a0 shrink. Remarkably, the CoreDpp variants achieve much lower error than K++. Moreover, the results suggest that the relaxations from Section 5.3 do not noticeably increase the error in practice. Also, CoreDpp -r performs comparable with CoreDpp, indicating that our algorithm is robust against initialization. Since, in addition, the CoreDpp construction makes most progress in the first pass through\nthe data, and the kmeans++ initialization yields the best performance, we use only one pass of CoreDpp initialized with kmeans++ in the subsequent experiments.\n6.2 Real Data\nWe apply CoreDpp to two larger real data sets:\n1. MNIST [35]. MNIST consists of images of hand-written digits, each of dimension 28\u00d7 28.\n2. GENES [9]. This dataset consists of different genes. Each sample in GENES corresponds to a gene, and the features are shortest path distances to 330 different hubs in the BioGRID gene interaction network.\nFor our first set of experiments on both datasets, we use a subset of 2000 data points and an RBF kernel to construct L. To evaluate the effect of model parameters on performance, we vary M from 20 to 100 and k from 2 to 8 and fix \u03bd = 2 (see Section 6.1 for an explanation of the parameters). Larger-scale experiments on these datasets are reported in Section 6.3.\nPerformance Measure and Results. On these larger data sets, it becomes impossible to compute the total variation distance exactly. We therefore approximate it by uniform sampling and computing an empirical estimate.\nThe results in Figure 2 and Figure 3 indicate that the approximations improve as the number of parts M increases and k decreases. This is because increasing M increases the models\u2019 approximation power, and decreasing k leads to a simpler target probability distribution to approximate. In general, CoreDpp always achieves lower error than K++, and NysStoch performs poorly in terms of total variation distance to the original distribution. This phenomenon is perhaps not so surprising when recalling that the Nystro\u0308m approximation minimizes a different type of error, a distance between the kernel matrices. These observations suggest to be careful when using matrix approximations to approximate L.\nFor an intuitive illustration, Figure 4 shows a core C constructed by CoreDpp, and the elements of one part Yc.\n6.3 Running Time on Large Datasets\nLastly, we address running times for CoreDpp, NysStoch and the Markov chain k-DPP (MCDPP [28]). For the latter, we evaluate convergence via the Gelman and Rubin multiple sequence diagnostic [19]; we run 10 chains simultaneously and use the CODA [40] package to calculate the potential scale reduction factor (PSRF), and set the number of iterations to the point when PSRF drops below 1.1. Finally we run MCDPP again for this specific number of iterations.\nFor overhead time, i.e., time to set up the sampler that is spent once in the beginning, we compare against NysStoch: CoreDpp constructs the partition and L\u0303, while NysStoch selects landmarks and constructs an approximation to the data. For sampling time, we compare against both NysStoch and MCDPP: CoreDpp uses Algo. 1, and NysStoch uses the dual form of k-Dpp sampling [30]. We did not include the time for convergence diagnostics into the running time of MCDPP, giving it an advantage in terms of running time.\nOverhead. Fig. 5 shows the overhead times as a function of N. For MNIST we vary N from 6,000 to 20,000 and for GENES we vary N from 6,000 to 10,000. These values of N are already quite large, given that the Dpp kernel is a dense RBF kernel matrix; this leads to increased running time for all compared methods. The construction time for NysStoch and CoreDpp is comparable for small-sized data, but NysStoch quickly becomes less competitive as the data gets larger. The construction time for CoreDpp is linear in N, with a mild slope. If multiple samples are sought, this construction can be performed offline as preprocessing as it is needed only once.\nSampling. Fig. 6 shows the time to draw one sample as a function of N, comparing CoreDpp against NysStoch and MCDPP. CoreDpp yields samples in time independent of N and is extremely efficient \u2013 it is orders of magnitude faster than NysStoch and MCDPP.\nWe also consider the time taken to sample a large number of subsets, and compare against both NysStoch and MCDPP\u2014the sampling times for drawing approximately independent samples with MCDPP add up. Fig. 7 shows the results. As more samples are required, CoreDpp becomes increasingly efficient relative to the other methods."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed a fast, two-stage sampling method for sampling diverse subsets with k-Dpps. As opposed to other approaches, our algorithm directly aims at minimizing the total variation distance between the approximate and original probability distributions. Our experiments demonstrate the effectiveness and efficiency of our approach: not only does our construction have lower error in total variation distance compared with other methods, it also produces these more\naccurate samples efficiently, at comparable or faster speed than other methods.\nAcknowledgements. This research was partially supposed by an NSF CAREER award 1553284 and a Google Research Award."}], "references": [{"title": "Markov Determinantal Point Processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox"], "venue": "Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate inference in continuous determinantal processes", "author": ["R.H. Affandi", "E. Fox", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Nystrom Approximation for Large-Scale Determinantal Processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox", "B. Taskar"], "venue": "Proc. Int. Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning the Parameters of Determinantal Point Process Kernels", "author": ["R.H. Affandi", "E.B. Fox", "R.P. Adams", "B. Taskar"], "venue": "Int. Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Notes on using determinantal point processes for clustering with applications to text clustering", "author": ["A. Agarwal", "A. Choromanska", "K. Choromanski"], "venue": "arXiv preprint arXiv:1410.6975,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Monte Carlo Markov Chain algorithms for sampling strongly Rayleigh distributions and determinantal point processes", "author": ["N. Anari", "S.O. Gharan", "A. Rezaei"], "venue": "arXiv preprint arXiv:1602.05242,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SIAM-ACM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Coresets for nonparametric estimation-the case of dp-means", "author": ["O. Bachem", "M. Lucic", "A. Krause"], "venue": "Int. Conference on Machine Learning (ICML), pages 209\u2013217,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Diversifying sparsity using variational determinantal point processes", "author": ["N.K. Batmanghelich", "G. Quon", "A. Kulesza", "M. Kellis", "P. Golland", "L. Bornn"], "venue": "arXiv preprint arXiv:1411.6307,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Proceedings of the National Academy of Sciences, pages 369\u2013374,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Determinantal point processes", "author": ["A. Borodin"], "venue": "arXiv preprint arXiv:0911.1153,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Eynard-Mehta Theorem, Schur Process, and Their Pfaffian Analogs", "author": ["A. Borodin", "E.M. Rains"], "venue": "Journal of Statistical Physics, 121(3-4):291\u2013317,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Algebraic complexity theory", "author": ["P. B\u00fcrgisser", "M. Clausen", "M.A. Shokrollahi"], "venue": "Grundlehren der mathematischen Wissenschaften, 315,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Perfect simulation of determinantal point processes", "author": ["L. Decreusefond", "I. Flint", "K.C. Low"], "venue": "arXiv preprint arXiv:1311.1027,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient Volume Sampling for Row/Column Subset Selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "SIAM-ACM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Coresets for weighted facilities and their applications", "author": ["D. Feldman", "A. Fiat", "M. Sharir"], "venue": "IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["D. Feldman", "M. Schmidt", "C. Sohler"], "venue": "SIAM-ACM Symposium on Discrete Algorithms (SODA), pages 1434\u20131453,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Inference from iterative simulation using multiple sequences", "author": ["A. Gelman", "D.B. Rubin"], "venue": "Statistical science, pages 457\u2013472,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1992}, {"title": "Near-Optimal MAP Inference for Determinantal Point Processes", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Discovering diverse and salient threads in document collections", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Expectation-Maximization for Learning Determinantal Point Processes", "author": ["J. Gillenwater", "E. Fox", "A. Kulesza", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Diverse Sequential Subset Selection for Supervised Video Summarization", "author": ["B. Gong", "W.-L. Chao", "K. Grauman", "S. Fei"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Smaller coresets for k-median and k-means clustering", "author": ["S. Har-Peled", "A. Kushal"], "venue": "Proceedings of the twenty-first annual symposium on Computational geometry, pages 126\u2013134,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Symposium on Theory of Computing (STOC), pages 291\u2013300,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Symposium on Theory of Computing (STOC), pages 291\u2013300,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Fast Determinantal Point Process Sampling with Application to Clustering", "author": ["B. Kang"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research, 9: 235\u2013284,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Structured Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "k-DPPs: Fixed-Size Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Int. Conference on Machine Learning (ICML),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "arXiv preprint arXiv:1207.6083,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Priors for diversity in generative latent variable models", "author": ["J.T. Kwok", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "Anal. Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Near optimal dimensionality reductions that preserve volumes", "author": ["A. Magen", "A. Zouzias"], "venue": "Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning Determinantal Point Processes", "author": ["Z. Mariet", "S. Sra"], "venue": "Int. Conference on Machine Learning (ICML),", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Core-sets for canonical correlation analysis", "author": ["S. Paul"], "venue": "Int. Conference on Information and Knowledge Management (CIKM), pages 1887\u20131890,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Coda: Convergence diagnosis and output analysis for mcmc", "author": ["M. Plummer", "N. Best", "K. Cowles", "K. Vines"], "venue": "R News, 6(1):7\u201311,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Random Features for Large-scale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Coresets for k-segmentation of streaming data", "author": ["G. Rosman", "M. Volkov", "D. Feldman", "J.W. Fisher III", "D. Rus"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 559\u2013567,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Determinantal clustering processes - a nonparametric bayesian approach to kernel based semi-supervised clustering", "author": ["A. Shah", "Z. Ghahramani"], "venue": "arXiv preprint arXiv:1309.6862,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data", "author": ["J. Snoek", "R. Zemel", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Using The Matrix Ridge Approximation to Speedup Determinantal Point Processes Sampling Algorithms", "author": ["S. Wang", "C. Zhang", "H. Qian", "Z. Zhang"], "venue": "Proc. AAAI Conference on Artificial Intelligence,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "The Matrix Ridge Approximation: Algorithms", "author": ["Z. Zhang"], "venue": "Machine Learning, 97:227\u2013258,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Solving the apparent diversity-accuracy dilemma of recommender systems", "author": ["T. Zhou", "Z. Kuscsik", "J.-G. Liu", "M. Medo", "J.R. Wakeling", "Y.-C. Zhang"], "venue": "Proceedings of the National Academy of Sciences, 107(10):4511\u20134515,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 124, "endOffset": 127}, {"referenceID": 35, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 178, "endOffset": 182}, {"referenceID": 3, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 206, "endOffset": 209}, {"referenceID": 46, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 231, "endOffset": 235}, {"referenceID": 28, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 254, "endOffset": 258}, {"referenceID": 0, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 4, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 20, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 29, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 32, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 42, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 43, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 3, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 22, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 27, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 29, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 31, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 33, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 37, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 26, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 236, "endOffset": 244}, {"referenceID": 32, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 236, "endOffset": 244}, {"referenceID": 31, "context": "Cubic preprocessing costs also impede wider use of the cardinality constrained variant k-Dpp [32].", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 164, "endOffset": 171}, {"referenceID": 40, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 164, "endOffset": 171}, {"referenceID": 44, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 204, "endOffset": 212}, {"referenceID": 45, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 204, "endOffset": 212}, {"referenceID": 29, "context": "Alternative methods use a dual formulation [30], which however presupposes a decomposition L = XX> of the DPP kernel, which may be unavailable and inefficient to compute in practice.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 9, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 13, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 27, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 16, "context": "We do this via the idea of coresets [17, 25], small subsets of the data that capture function values of interest almost as well as the full dataset.", "startOffset": 36, "endOffset": 44}, {"referenceID": 24, "context": "We do this via the idea of coresets [17, 25], small subsets of the data that capture function values of interest almost as well as the full dataset.", "startOffset": 36, "endOffset": 44}, {"referenceID": 31, "context": "For a coreset of size M, our sampling time is O(k2M), which is independent of N since we are using k-Dpps [32].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 62, "endOffset": 74}, {"referenceID": 11, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 62, "endOffset": 74}, {"referenceID": 26, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 62, "endOffset": 74}, {"referenceID": 20, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 22, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 29, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 31, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 32, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 33, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 37, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 43, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 14, "context": "Cardinality-conditioned Dpp sampling is also referred to as \u201cvolume sampling\u201d, which has been used for matrix approximations [15, 16].", "startOffset": 125, "endOffset": 133}, {"referenceID": 15, "context": "Cardinality-conditioned Dpp sampling is also referred to as \u201cvolume sampling\u201d, which has been used for matrix approximations [15, 16].", "startOffset": 125, "endOffset": 133}, {"referenceID": 2, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 14, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 29, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 36, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 9, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 92, "endOffset": 100}, {"referenceID": 27, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 92, "endOffset": 100}, {"referenceID": 3, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 21, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 30, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 37, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 19, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 17, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 23, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 25, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 17, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 82, "endOffset": 90}, {"referenceID": 38, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 82, "endOffset": 90}, {"referenceID": 41, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "Conditioning on sampling sets of fixed cardinality k, one obtains a k-Dpp [32]: PL,k(Y) : = PL(Y | |Y| = k) = det(LY)ek(L)J |Y| = kK, where ek(L) is the k-th coefficient of the characteristic polynomial det(\u03bbI\u2212 L) = \u2211 k=0(\u22121)ek(L)\u03bb.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "2In theory, this can be computed in O(N\u03c9 log(N)) time [13], but the eigendecompositions and dynamic programming used in practice typically take cubic time.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "6 Experiments We next evaluate CoreDpp, and compare its efficiency and effectiveness against three competing approaches: - Partitioning using k-means (with kmeans++ initialization [7]), with C chosen as the centers of the clusters; referred to as K++ in the results.", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "- The adaptive, stochastic Nystr\u00f6m sampler of [3] (NysStoch).", "startOffset": 46, "endOffset": 49}, {"referenceID": 27, "context": "- The Metropolis-Hastings DPP sampler MCDPP [28].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "We use the well-known Gelman and Rubin multiple sequence diagnostic [19] to empirically judge mixing.", "startOffset": 68, "endOffset": 72}, {"referenceID": 34, "context": "MNIST [35].", "startOffset": 6, "endOffset": 10}, {"referenceID": 8, "context": "GENES [9].", "startOffset": 6, "endOffset": 9}, {"referenceID": 27, "context": "3 Running Time on Large Datasets Lastly, we address running times for CoreDpp, NysStoch and the Markov chain k-DPP (MCDPP [28]).", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "For the latter, we evaluate convergence via the Gelman and Rubin multiple sequence diagnostic [19]; we run 10 chains simultaneously and use the CODA [40] package to calculate the potential scale reduction factor (PSRF), and set the number of iterations to the point when PSRF drops below 1.", "startOffset": 94, "endOffset": 98}, {"referenceID": 39, "context": "For the latter, we evaluate convergence via the Gelman and Rubin multiple sequence diagnostic [19]; we run 10 chains simultaneously and use the CODA [40] package to calculate the potential scale reduction factor (PSRF), and set the number of iterations to the point when PSRF drops below 1.", "startOffset": 149, "endOffset": 153}, {"referenceID": 29, "context": "1, and NysStoch uses the dual form of k-Dpp sampling [30].", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "Determinantal Point Processes (Dpps) are elegant probabilistic models of repulsion and diversity over discrete sets of items. But their applicability to large sets is hindered by expensive cubic-complexity matrix operations for basic tasks such as sampling. In light of this, we propose a new method for approximate sampling from discrete k-Dpps. Our method takes advantage of the diversity property of subsets sampled from a Dpp, and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter, it efficiently samples subsets based on the constructed coresets. As opposed to previous approaches, our algorithm aims to minimize the total variation distance to the original distribution. Experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets, and yields more accurate samples than previous approaches.", "creator": "LaTeX with hyperref package"}}}