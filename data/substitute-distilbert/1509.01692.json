{"id": "1509.01692", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2015", "title": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning", "abstract": "recent work on sequence embeddings has shown that simple vector subtraction over pre - trained embeddings is surprisingly excellent at capturing different lexical relations, despite lacking explicit supervision. prior work has evaluated this intriguing result using a word analogy learning formulation regarding hand - selected relations, but the possibilities of independent finding over a broader range of lexical relation types and different learning settings has exclusively been evaluated. in this paper, we carry out such an evaluation in more visual settings : ( 1 ) spectral clustering to induce symbolic relations, and ( 2 ) supervised learning to classify specific differences into relation types. we find that word embeddings capture a surprising amount of information, and that, under classical supervised information, vector subtraction generalises concepts to a broad range historical contexts, including over unseen lexical items.", "histories": [["v1", "Sat, 5 Sep 2015 11:23:44 GMT  (113kb,D)", "http://arxiv.org/abs/1509.01692v1", null], ["v2", "Fri, 11 Sep 2015 12:20:03 GMT  (113kb,D)", "http://arxiv.org/abs/1509.01692v2", null], ["v3", "Wed, 17 Feb 2016 05:44:33 GMT  (86kb,D)", "http://arxiv.org/abs/1509.01692v3", null], ["v4", "Sat, 13 Aug 2016 17:56:01 GMT  (141kb,D)", "http://arxiv.org/abs/1509.01692v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ekaterina vylomova", "laura rimell", "trevor cohn", "timothy baldwin"], "accepted": true, "id": "1509.01692"}, "pdf": {"name": "1509.01692.pdf", "metadata": {"source": "CRF", "title": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning", "authors": [], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Learning to identify lexical relations is a fundamental task in natural language processing (\u201cNLP\u201d). Accurate relation classification, relational similarity prediction, and wide-coverage and adaptable relation discovery can contribute to numerous NLP applications including paraphrasing and generation, machine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010).\nRecently, attention has been focused on identifying lexical relations using contextual vector space representations, particularly neural language embeddings, which are dense, low-dimensional vectors obtained from a neural network trained to predict word contexts. The skip-gram model of Mikolov et al.\n(2013a) and other neural language models have been to shown to perform well on an analogy completion task (Mikolov et al., 2013c; Mikolov et al., 2013b), in the space of relational similarity prediction (Turney, 2006). Linear operations on word vectors appear to capture the lexical relation governing the analogy. The most famous example involves predicting the vector queen from the vector combination king \u2212man + woman, which captures a gender relation. The results also extend to semantic relations such as CAPITAL-OF-CITY (paris \u2212 france + poland \u2248 warsaw) and morphosyntactic relations such as PLURALISATION (cars\u2212car+ apple \u2248 apples). This is particularly remarkable because the model is not trained for this task, so the relational structure of the vector space appears to be an emergent property of the model.\nThe key operation in these models is vector difference, or vector offset. For example, it is the paris\u2212 france vector that appears to encode CAPITAL-OF, presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (Levy and Goldberg, 2014a). The success of the simple offset method on analogy completion suggests that the difference vectors (\u201cDIFFVEC\u201d hereafter) must themselves be meaningful: their direction and/or magnitude encodes a semantic relation. We would then expect the vector helsinki \u2212 finland to be quite similar, in a quantifiable way, to paris\u2212 france.\nHowever, the now-standard analogy task does not adequately probe the semantics and morphosyntactics of DIFFVECs. On one hand, the task is too challenging, because it requires a one-best answer. Ko\u0308per et al. (2015) found that a neural language model performed poorly on analogies involv-\nar X\niv :1\n50 9.\n01 69\n2v 1\n[ cs\n.C L\n] 5\nS ep\n2 01\ning antonyms and hypernyms, often predicting synonyms or other related terms instead; but this does not preclude antonymy and hypernymy being encoded in DIFFVECs in a meaningful way. On the other hand, the task is too limited: its coverage of cognitively salient relations is incomplete, and it leaves open the question of whether all vector offsets encode meaningful relations, or just a small subset of them. There may also be more fine-grained structure in the offsets: Fu et al. (2014) found that vector offsets representing the hypernym relation could be grouped into semantic sub-clusters, as the difference between carpenter and laborer, e.g., was quite distinct from the one between goldfish and fish.\nIn this paper we investigate how well DIFFVECs calculated over different word embeddings capture lexical relations from a variety of linguistic resources. We systematically study the expressivity of vector difference in distributed spaces in two ways. First, we cluster the DIFFVECs to test whether the clusters map onto true lexical relations. We explore a parameter space consisting of the number of clusters and two distance measures, and find that syntactic relations are captured better than semantic relations.\nSecond, we perform classification over the DIFFVECs and obtain surprisingly high accuracy in a closed-world setting (over a predefined set of word pairs, each of which corresponds to a lexical relation in the training data). When we move to an openworld setting and attempt to classify random word pairs \u2014 many of which do not correspond to any lexical relation in the training data \u2014 the results are poor. We then investigate methods for better attuning the learned class representation to the lexical relations, focusing on methods for automatically engineering negative instances. We find that this improves the model performance substantially."}, {"heading": "2 Background and Related Work", "text": "A lexical relation is a binary relation r holding between a word pair (wi, wj); for example, the pair (cart,wheel) stands in the WHOLE-PART relation. NLP tasks related to lexical relation learning include relation extraction and discovery, relation classification, and relational similarity prediction. In relation extraction, word pairs standing in a given relation are mined from a corpus. The relations may be pre-defined or, in the Open Information Extrac-\ntion paradigm (Banko et al., 2007; Weikum and Theobald, 2010), the relations themselves are also learned from the text (e.g. in the form of text labels). In relation classification, the task is to assign a word pair to the correct relation, from a pre-defined set of relations. Relational similarity prediction involves assessing the degree to which a word pair (a, b) stands in the same relation as another pair (c, d), or to complete an analogy a : b :: c : ?. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012).\nRelation extraction and discovery has involved generic semantic relations such as IS-A and WHOLE-PART, but also corpus-specific relations such as CEO-OF-COMPANY (Pantel and Pennacchiotti, 2006). Some datasets are task-specific, for example focused on paraphrasing the relation holding between nouns in noun-noun compounds (Girju et al., 2007), or analogy questions from the American SAT exam for relational similarity (Turney et al., 2003).\nHistorically, approaches to relation learning have generally been supervised or semi-supervised. Relation extraction has used pattern-based approaches such as A such as B, either explicitly (Hearst, 1992; Kozareva et al., 2008; McIntosh et al., 2011) or implicitly (Snow et al., 2005), although not all relations are equally amenable to this style of approach (Yamada and Baldwin, 2004). Relation classification involves supervised classifiers (Chklovski and Pantel, 2004; Snow et al., 2005; Davidov and Rappoport, 2008). Relational similarity prediction has also mostly used classification based on lexico-syntactic patterns linking word pairs in text (Se\u0301aghdha and Copestake, 2009; Jurgens et al., 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al., 2013).\nRecently, attention has turned to using vector space models of words for relation classification and relational similarity. Distributional word vectors, while mostly applied to measuring semantic similarity and relatedness (Mitchell and Lapata, 2010), have also been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman\net al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over neural word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the DIFFVEC idea in different contexts. The original analogy dataset has been used to evaluate neural language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot\u2014warm\u2014cool\u2014 cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, but instead of using a single DIFFVEC, they cluster words by topic and show that the hypernym DIFFVEC can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).\nAnother strand of work responding to the vector difference approach has analysed the structure of neural embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smaller-scale investigations in this direction. Makrai et al. (2013) divided antonym pairs into semantic classes such as quality, time, gender, and distance, and tested whether the DIFFVECs internal to each antonym class were significantly more correlated than random. They found that for about two-thirds of the antonym classes, the DIFFVECs were significantly correlated. Necs\u0327ulescu et al. (2015) trained a classifier on word pairs using word embeddings in order to predict coordinates, hypernyms, and meronyms. Ko\u0308per et al. (2015) undertook a systematic study of morphosyntactic and semantic relations on word embeddings produced with word2vec (\u201cw2v\u201d hereafter; see \u00a73.1) for English and Ger-\nman. They tested a variety of relations including word similarity, antonyms, synonyms, hypernyms, and meronyms, in a novel analogy task. Although the set of relations tested by Ko\u0308per et al. (2015) is somewhat more constrained than the set we use, there is a good deal of overlap. However, their evaluation was performed in the context of relational similarity, and they did not perform clustering or classification on the DIFFVECs."}, {"heading": "3 General Approach and Resources", "text": "For our purposes, we define the task of lexical relation learning to take a set of (ordered) word pairs {(wi, wj)} and a set of binary lexical relations R = {rk}, and map each word pair (wi, wj) as follows: (a) (wi, wj) 7\u2192 rk \u2208 R, i.e. the \u201cclosed-world\u201d setting, where we assume that all word pairs can be uniquely classified according to a relation in R; or (b) (wi, wj) 7\u2192 rk \u2208 R \u222a {\u03c6} where \u03c6 signifies the fact that none of the relations in R apply to the word pair in question, i.e. the \u201copen-world\u201d setting.\nOur starting point for lexical relation learning is the assumption that important information about various types of relations is implicitly embedded in the offset vectors. We consider solely DIFFVEC w2 \u2212 w1, and hypothesise that these DIFFVECs should capture a wide spectrum of possible lexical contrasts. A second assumption is that there exist dimensions, or directions, in the embedding vector spaces responsible for a particular lexical relation. Such dimensions could be identified and exploited as part of a clustering or classification method, in the context of identifying relations between word pairs or classes of DIFFVECs.\nIn order to test the generalisability of the DIFFVEC method, we require: (1) word embeddings, and (2) a set of lexical relations to evaluate against. As the focus of this paper is not the word embedding pre-training approaches so much as the utility of the DIFFVECs for lexical relation learning, we take a selection of four pre-trained word embeddings with strong currency in the literature, as detailed in \u00a73.1.\nFor the lexical relations, we are after a range of relations that is representative of the types of relational learning tasks targeted in the literature, and where there is availability of annotated data. To this end, we construct a dataset from a variety of sources, focusing on lexical semantic relations (which are less\nwell represented in the analogy dataset of Mikolov et al. (2013c)), but including morphosyntactic and morphosemantic relations (see \u00a73.2)."}, {"heading": "3.1 Word Embeddings", "text": "We consider four highly successful word embedding models in our experiments: w2v (Mikolov et al., 2013a), GloVE (Pennington et al., 2014), SENNA (Collobert et al., 2011), and HLBL (Mnih and Hinton, 2009). Embeddings from these sources exhibit a variety of influences, through their use of different modelling tasks, linearity, manner of relating words to their contexts, dimensionality, and scale and domain of training datasets (as listed in Tab 1). w2v was developed to predict the context of a word using the skip-gram model with the objective:\nJ = 1\nT T\u2211 i=1 \u2211 i\u2212c\u2264j\u2264i+c\nj 6=i\nexp(w>i w\u0303j)\u2211V k=1 exp(w>i w\u0303k) ,\nwhere wi and w\u0303i are the vector representations for the ith word (as a focus or context word, respectively), V is the vocabulary size, T is the number of tokens in the corpus, and c is the context window size.1 Google News data was used to train the model. We use the focus word vectors, W = {wk}Vk=1, normalised such that each \u2016wk\u2016 = 1.\nThe GloVE model is based on a similar bilinear formulation, framed as a low-rank decomposition of the matrix of corpus coocurrence frequencies:\nJ = 1\n2 V\u2211 i,j=1 f(Pij)(w>i w\u0303j \u2212 logPij)2 ,\n1In a slight abuse of notation, the subscripts of w play double duty, denoting either the embedding for the ith token, wi, or kth word type, wk.\nwhere wi is a vector for the left context, wj is a vector for the right context, Pij is the relative frequency of word j in the context of word i, and f is a heuristic weighting function to balance the influence of high versus low term frequencies. The model was trained on Wikipedia 2014 and the English Gigaword corpus version 5. HLBL is a bilinear formulation of an n-gram language model, which predicts the ith word based on context words (i\u2212n, . . . , i\u2212 2, i\u2212 1). This leads to the following training objective:\nJ = 1\nT T\u2211 i=1 exp(w\u0303>i wi + bi)\u2211V k=1 exp(w\u0303 > i wk + bk) ,\nwhere w\u0303i = \u2211n\u22121\nj=1 Cjwi\u2212j is the context embedding, {Cj} are scaling matrices and b\u2217 bias terms.\nThe final model, SENNA, was initially proposed for multi-task training of several language processing tasks, from language modelling through to semantic role labelling. Here we focus on the statistical language modelling component, which has a pairwise ranking objective to maximise the relative score of each word in its local context:\nJ = 1\nT T\u2211 i=1 V\u2211 k=1 max [ 0, 1\u2212 f(wi\u2212c, . . . ,wi\u22121,wi)\n+ f(wi\u2212c, . . . ,wi\u22121,wk) ] ,\nwhere the last c \u2212 1 words are used as context, and f(x) is a non-linear function of the input, defined as a multi-layer perceptron. We use Turian et al.\u2019s word embeddings for HLBL and SENNA, trained on the Reuters English newswire corpus. In both cases, the embeddings were scaled by the global standard deviation over the word-embedding matrix, Wscaled = 0.1\u00d7 W\u03c3(W ) .\nOur expectation is that the differences in initial training conditions will affect performance, e.g. the bidirectional models are expected to work better than left to right ones, and linear models should outperform their non-linear counterparts due to our use of linear vector difference."}, {"heading": "3.2 Lexical Relations", "text": "In order to evaluate the applicability of the DIFFVEC approach to relations of different types, we assembled a set of lexical relations in three broad categories: lexical semantic relations, morphosyntactic\nparadigm relations, and morphosemantic relations. We constrained the lexical relations to be binary and to have fixed directionality. Consequently we excluded symmetric lexical relations such as synonymy. We additionally constrained the dataset to the words occurring in all four pre-trained embeddings. There is some overlap between our relations and those included in the analogy task of Mikolov et al. (2013c), but we include a much wider range of lexical semantic relations, especially those standardly evaluated in the relation classification literature. We preprocessed the data to exclude all undirected relations, remove duplicate triples and normalise the directionality.\nThe final dataset consists of 12,943 triples \u3008relation,word1,word2\u3009, comprising 18 relation types, extracted from SemEval\u201912 (Jurgens et al., 2012), BLESS (Baroni et al., 2014), the MSR analogy dataset (Mikolov et al., 2013c), the dataset of Tan et al. (2006a), Princeton WordNet, and Wiktionary, as listed in Tab 2 and detailed below (wherein we define each relation relative to the directed word pair (x, y)). We will release this dataset as part of the publication of this paper.\nLexical Semantic Relations Our dataset includes the seven top-level asymmetric lexical semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012): SEMEVALClass: x names a class that includes entity y; e.g. (animal, dog) SEMEVALPart: y names a part of entity x or is an instance of class x; e.g. (airplane, cockpit) SEMEVALAttr: y names a characteristic quality, property, or action of x; e.g. (cloud, rain) SEMEVALCase: x is an action that y is usually in-\nvolved in, e.g., as agent, object, recipient, or instrument of the action; e.g. (hunt, deer)\nSEMEVALCause: y represents the cause, purpose, or goal of x or using x; e.g. (cook, eat) SEMEVALSpace: y is a thing or action that is associated with x (a location or time); e.g. (aquarium, fish) SEMEVALRef: x is an expression or representation of, or a plan or design for, or provides information about, y; e.g. (song, emotion) It also includes three lexical semantic relations from BLESS (Baroni and Lenci, 2011): BLESSHyper: x names a noun class that includes\nentity y; e.g. (weapon, rifle) BLESSMero: y names a part/component/member of entity x; e.g. (coat, zipper) BLESSEvent: x refers to an action that entity y is usually involved in; e.g. (zip, coat) Although there is some overlap between SemEval and BLESS relations, e.g. SEMEVALPart and BLESSMero, they are not exactly equivalent, and we did not attempt to merge classes.\nMorphosyntactic Paradigm Relations As morphosyntactic paradigm lexical relations, we include four relations from the original Mikolov et al. (2013c) DIFFVEC paper: NOUNSP: y is the plural form (NNS, in Penn\ntagset terms) of singular noun x (an NN); e.g. (year, years)\nVERB3: y is the 3rd person singular present-tense verb form (VBZ) of base-form verb x (a VB); e.g. (accept, accepts) VERBPast: y is the past-tense verb form (VBD) of base verb x (a VB); e.g. (know, knew) VERB3Past: y is the past-tense verb form (VBD) of 3rd person singular present-tense verb form x (a VBZ); e.g. (creates, created)\nMorphosemantic Relations The dataset also includes the following morphosemantic relations: LVC: x is the light verb associated with noun y,\nfrom the \u201cleninently\u201d-annotated dataset of Tan et al. (2006b); e.g. (give, approval)\nVERBNOUN: y is the nominalisation of verb x, as extracted (exhaustively) from Princeton WordNet v3.0; e.g. (americanize, americanization) PREFIX: y is x prefixed with the re bound morpheme, as extracted (exhaustively) from Wiktionary; e.g. (vote, revote) NOUNColl: x is the collective noun for noun y, based on an online list;2 e.g. (army, ants)"}, {"heading": "4 Clustering", "text": "Assuming DIFFVECs are capable of capturing all lexical relations equally, we would expect clustering to be able to identify sets of word pairs with high relational similarity, or equivalently clusters of similar offset vectors. Under the additional assumption\n2http://www.rinkworks.com/words/ collective.shtml\nthat a given word pair corresponds to a unique lexical relation (in line with our definition of the lexical relation learning task in \u00a73), a hard clustering approach is appropriate. In order to test these assumptions, we cluster our 18-relation closed-world dataset in the first instance, and evaluate the resulting clusters against the lexical resources in \u00a73.2.\nAs further motivation, consider Fig 1, which presents the DIFFVEC space for 10 samples of each class (based on a projection learned over the full dataset). The samples corresponding to the verb\u2013 verb morphosyntactic relations (VERB3, VERBPast, VERB3Past) each form a tight cluster. Other verbal relations, VERBNOUN and LVC are spread amongst them. Similarly, NOUNSP samples form another tight cluster. Note that BLESSMero and SEMEVALPart are intermingled, which is encouraging given the semantic similarity of the two relations.\nWe cluster the DIFFVECs between all word pairs in our dataset using spectral clustering (Von Luxburg, 2007). Spectral clustering has two hyperparameters: (1) the number of clusters; and (2) the pairwise similarity measure for comparing DIFFVECs. We tune the hyperparameters over development data, selecting the configuration that maximises the V-Measure (Rosenberg and Hirschberg, 2007). V-Measure is an information theoretic measure that combines homegeneity and completeness, and is defined in terms of normalised conditional entropy of the true classes given a clustering, and vice-\nversa. Our use of V-Measure is based on the findings of Christodoulopoulos et al. (2010), who showed for part-of-speech induction that out of seven clustering evaluation measures, V-Measure is the most effective and least sensitive to the number of clusters.\nTo populate the affinity matrix for spectral clustering, we experiment with two options, each of which we scale using a Gaussian kernel:3\nexp ( \u2212\u03b3 \u00d7 dist (\u2206i,j ,\u2206k,l)\n\u03c3\n) ,\nwhere \u2206i,j = wj \u2212 wi is the vector difference between the embeddings of the ith and jth word types, \u03c3 is the standard deviation of the corpus dist (\u2206i,j ,\u2206k,l) values, and \u03b3 is a hyper-parameter which determines the decay rate as the distance increases. The distance metric, dist (\u2206i,j ,\u2206k,l) is defined as either:\n1\u2212 cos(\u2206i,j ,\u2206k,l) , cosine distance; or \u2016\u2206i,j \u2212\u2206k,l\u20162 , Euclidean distance.\nThe \u03b3 parameter in the kernel function affects how quickly the score drops with distance: high \u03b3 values have a faster decay and effectively impose a threshold distance, beyond which points are assigned a near-zero similarity value. \u03b3 = 0.1 provided the best performance over the development data and is used in all experiments.\n3The Gaussian kernel introduces an extra non-linearity into the formulation. In preliminary experiments, we found this to outperform the basic cosine and Euclidean distances.\nNote that the results of spectral clustering partially depend on random initialisation, so we ran several experiments using the same parameters, and average across them in the final results.\nFig 2 presents V-Measure values over the test data for each of the four word embedding models, based on Euclidean distance. We show results for different numbers of clusters, from N = 10 in increasing steps of 10, up to N = 80 (beyond which the clustering quality diminishes).4 Observe that w2v achieves the best results, with a V-Measure value of around 0.35,5 which is relatively constant over varying numbers of clusters. GloVE mirrors this result, but is consistently below w2v at a V-Measure of around 0.3. HLBL and SENNA performed very similarly, at a substantially lower V-Measure than w2v or GloVE, closer to 0.2.\nTo better understand these results, and the clustering performance over the different lexical relations, we additionally calculated the entropy for each lexical relation. We base this on the clustering output for a given word embedding where V-Measure was optimal over the development data (indicated by the squares in Fig 2). Since the samples are distributed non-uniformly, we normalise entropy re-\n4Although 80 clusters our 18 relation types, it should be noted that the SemEval\u201912 classes each contain numerous subclasses, so the larger number may be more realistic.\n5V-Measure returns a value in the range [0, 1], with 1 indicating perfect homogeneity and completeness.\nsults for each method by log(n) where n is the number of samples in a particular relation.\nTab 3 presents the entropy values for each relation and embedding, with the lowest entropy (purest clustering) for each relation indicated in bold. Combining the V-Measure and entropy results we can see that the clustering does remarkably well, without any supervision in terms of either the training of the word embeddings6 or the clustering of the DIFFVECs, nor indeed any explicit representation of the component words (as all instances are DIFFVECs). While it is hard to calibrate the raw numbers, for the somewhat related lexical semantic clustering task of word sense induction, the best-performing systems in SemEval-2010 Task 4 (Manandhar et al., 2010) achieved a V-Measure of under 0.2.\nLooking across the different lexical relation types, the morphosyntactic paradigm relations (NOUNSP and the three VERB relations) are by far the easiest, with w2v notably achieving a perfect clustering of the word pairs for VERB3Past. The SEMEVAL and BLESS lexical semantic relations, on the other hand, are the hardest to capture for all embeddings.\nLooking in depth at the composition of the clusters, taking w2v as our exemplar word embedding (based on it achieving the highest V-Measure),\n6With the minor exception of SENNA, in that the word embeddings were indirectly learned using multi-task learning.\nfor VERB3 there was a single cluster consisting of around 90% VERB3 word pairs. The remaining 10% of instances tended to include a word that was ambiguous in POS, leading to confusion with VERBNOUN in particular. Example incorrect word pairs in this category are: (study, studies), (run, runs), (remain, remains), (save, saves), (like, likes) and (increase, increases). This polysemy results in the distance represented in the vector difference for such pairs being above the average for VERB3, and the word pairs consequently being clustered with word pairs associated with other cross-POS relations.\nFor VERBPast, a single relatively pure cluster was generated, with minor contamination due to semantic and syntactic ambiguity with word pairs from lexical semantic relations such as (hurt, saw), (utensil, saw), and (wipe, saw). Here, the noun saw is ambiguous with a high-frequency past-tense verb, and for the first and last example, the first word is also ambiguous with a base verb, but from a different paradigm. A similar effect was observed for NOUNSP. This suggests a second issue: the words in a word pair individually having the correct lexical property (in terms of verb tense/form) for the lexical relation, but not satisfying the additional paradigmatic constraint associated with the relation.\nA related phenomenon was observed for NOUNColl, where the instances were assigned to a large mixed cluster containing word pairs where word y referred to an animal, reflecting the fact that most of the collective nouns in our dataset relate to animals, e.g. (stand, horse), (ambush, tigers), (antibiotics, bacteria). This is interesting from a DIFFVEC point of view, since it shows that the lexical semantics of one word in the pair can overwhelm the semantic content of the DIFFVEC.\nBLESSMero was split into multiple clusters along domain lines, with separate clusters for weapons, dwellings, vehicles, etc. Other semantic relations were clustered in similar ways, with one cluster largely made up of (ANIMAL NOUN,MOVEMENT VERB) word pairs, and another comprised largely of (FOOD NOUN, FOOD VERB) word pairs. Interestingly, there was also a large cluster of (PROFESSION NOUN, ACTION VERB) pairs.\nWhile the primary focus of this paper is not on cross-comparison of different embeddings, the dif-\nference in results between w2v and GloVE on the one hand, and HLBL and SENNA on the other, is striking. One possible explanation for the overall worse results for HLBL and SENNA is that they were trained on a much smaller corpus (over two orders of magnitude smaller than either w2v or GloVE), and also the fact that they were trained in a language modelling context. As observed by Pennington et al. (2014) and Curran (2004), training based on one-sided context reduces the ability of a model to capture lexical semantic relations in particular. Syntactic relations, on the other hand, tend to be better modelled with only left context in the case of English, and indeed, for LVC\u2014 the relation with the strongest direct correlation with syntactic cooccurrence \u2014 HLBL and SENNA outperformed w2v and GloVE.\nOur clustering methodology could, of course, be applied to an open-world dataset including randomly-sampled word pairs, and the resultant clusters examined to determine their relational composition, perhaps showing that relation discovery is possible using word embeddings and DIFFVECs. Instead, however, we opt to investigate open-world relation learning based on a supervised approach, as detailed in the next section."}, {"heading": "5 Classification", "text": "A natural question is whether we can more accurately characterise lexical relations based on DIFFVECs through selecting or scaling the embedding dimensions. While several dimensions might encode lexical semantic information, other dimensions might encode other information pertinent to their training objectives (see \u00a73.1) such as domain, syntax or selectional restrictions. The former dimensions should be selected (weighted highly) and the latter dimensions ignored. We seek to test this hypothesis using supervised classification, that is by learning a discriminative classifier to distinguish between different relation types based solely on the DIFFVECs between a pair of words, \u2206i,j . For these experiments we use the w2v embeddings, and a subset of the relations for which we have sufficient data for supervised training and evaluation, namely NOUNColl, BLESSEvent, BLESSHyper, BLESSMero, NOUNSP, PREFIX, VERB3, VERB3Past, and VERBPast. We consider two applications: (1) a CLOSED-WORLD set-\nting similar to the unsupervised evaluation, in which the classifier only encounters related word pairs; and (2) a more challenging OPEN-WORLD setting where random distractor word pairs \u2014 which may or may not correspond to one of our relations \u2014 are included in the evaluation."}, {"heading": "5.1 CLOSED-WORLD Classification", "text": "For the CLOSED-WORLD setting, we train and test a multiclass classifier on datasets comprising \u3008\u2206i,j , r\u3009 pairs, where r is one of our nine relation types. We use an SVM with a linear kernel and report results from 10-fold cross-validation in Table 4. Most of the relations, even the most difficult ones from our clustering experiment, are classified with high precision and recall. The PREFIX relation was the only exception, achieving much lower recall, due to various other semantic relations which could be expressed by the same prefix type (e.g., (grade, regrade), (union, reunion), (entry, reentry)). Somewhat surprisingly, given the small dimensionality of the input (w2v vectors of size 300), we found that the linear SVM slightly outperformed a nonlinear SVM using a RBF kernel. Consequently the decision surfaces correspond to simple linear transformations of the embedding dimensions."}, {"heading": "5.2 OPEN-WORLD Classification", "text": "We now turn to a more challenging evaluation setting: a test set including word pairs drawn at random. This aims to illustrate whether a DIFFVECbased classifier is capable of differentiating related word pairs from noise, and can be applied to open data to learn new related word pairs.\nFor these experiments, we train a binary classifier for each relation type, using 23 of our relation\ndata for training and 13 for testing. The test data is augmented with an equal quantity of noise samples, generated as follows: (1) we first sample a seed lexicon by drawing words proportional to their frequency in Wikipedia;7 (2) next we take the Cartesian product over pairs of words from the seed lexicon; and (3) finally we sample word pairs uniformly from this set. This procedure generates word pairs that are representative of the frequency profile of our corpus.\nWe train 9 binary SVM classifiers with RBF kernels on the training partition, and evaluate on our noise-augmented test set. We classify each word pair either as a sample corresponding to a relation, or as noise. Fully annotating our random word pairs is prohibitively extensive, so instead, we manually annotated only the word pairs which were positively classified by one of our models. The results of our experiments are presented in Tab 5, in which we report on the combination of the original (CLOSEDWORLD) and random (OPEN-WORLD) test data, noting that recall (R) for OPEN-WORLD takes the form of relative recall (Pantel et al., 2004) over the positively-classified word pairs. The results are much lower than for the closed-word setting (Table 4), most notably in terms of precision. While the classifier has still correctly captured many of the true classes of the relations (high recall), this comes at the expense of misclassifying many of the noise samples as being related (low precision). For instance, (have,works), (turn, took), (works, started) were classified as VERB3, VERBPast and VERB3Past, respectively. That is, the model captures syntax, but lacks the ability to capture lexical paradigms."}, {"heading": "5.3 OPEN-WORLD Training with Noise", "text": "To address the problem of classifying distractor word pairs as valid relations, we retrain the classifier on a dataset comprising both valid and negative \u2018distractor\u2019 samples. The basic intuition behind this approach is to hedge against cases when correct samples are too general and may be reduced to multiple alternative relations. Careful construction of the distractor samples will force the model to learn discriminative class boundaries that not only separate each relation from other classes of relation, but also other unrelated word pairs. To this end, we automatically generated two types of distractors:\n7Filtered to words for which we have embeddings.\nopposite pairs: generated by switching the order of word pairs, Opposw1 ,w2 = word1 \u2212 word2. This ensures the classifier adequately represents the asymmetry in the relations. shuffled pairs: generated by replacing w2 with a random word from the same relation, Shuffw1 ,w2 = word \u2032 2 \u2212 word1. This is ap-\npropriate for relations that take specific word classes in each position, e.g., (VB,VBD) word pairs, such that model does not simply learn the properties of the words, but instead encodes the actual relation.\nBoth types of distractors are added to the training set, such that there are equal numbers of valid relations, opposite pairs and shuffled pairs.\nAfter training our classifier, we evaluate its predictions in the same way as in \u00a75.2, using the same test set combining related and random word pairs.8 The results are shown in Tab 5 (as \u201c+neg\u201d). Observe that the precision is much higher and recall somewhat lower compared to the classifier trained with only positive samples. This follows from the adversarial training scenario: using negative samples results in a more conservative classifier, that predicts many more samples as being noise. This allows for better identification of relations from noise samples (higher precision), but at the expense of misclassifying several true relations as noise (lower recall). Overall this leads to much higher F1 scores, as shown in Fig 3, other than for collective nouns (NOUNColl). This was the one of the most difficult relations to learn, which is unsurprising given the\n8But noting that relative recall for the random word pairs is based on the pool of positive predictions from both models.\noften arbitrary nature of the relation. The standard classifier learned to match word pairs including an animal name (e.g., (plague, rats)), while training on negative samples resulted in much more conservative predictions and consequently much lower precision. For instance, the classifier was able to capture (herd, horses) but not (run, salmon), (party, jays) or (singular, boar) as instances of NOUNColl, possibly because of polysemy. The most striking difference in performance was for BLESSMero, where the standard classifier generated many false positive noun pairs (e.g. (series, radio)), but the false positive rate was considerably reduced with negative sampling."}, {"heading": "6 Conclusions", "text": "This paper is the first to test the generalizability of the vector difference approach across a broad range of lexical relations (in raw number and also variety). First, clustering showed us that many types of morphosyntactic and morphosemantic differences are captured by DIFFVECs, but that lexical semantic relations are captured less well, consistent with previous work (Ko\u0308per et al., 2015). We then showed that classification over the DIFFVECs works extremely well in a closed-world setup, but less well over open data. With the introduction of automaticallygenerated negative samples, however, the results improved substantially. Overall, therefore, we conclude that the DIFFVEC approach has impressive utility over a broad range of lexical relations, especially under supervised classification."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv.", "citeRegEx": "Arora et al\\.,? 2015", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Open information extraction for the web", "author": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni."], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-2007), pages 2670\u20132676, Hyder-", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "How we blessed distributional semantic evaluation", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS \u201911, pages 1\u201310, Stroudsburg, PA, USA. Association", "citeRegEx": "Baroni and Lenci.,? 2011", "shortCiteRegEx": "Baroni and Lenci.", "year": 2011}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko."], "venue": "Advances in Neural Information Processing Systems 25 (NIPS-13), pages 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Multi-relational latent semantic analysis", "author": ["Kai-Wei Chang", "Wen tau Yih", "Christopher Meek."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 1602\u20131612.", "citeRegEx": "Chang et al\\.,? 2013", "shortCiteRegEx": "Chang et al\\.", "year": 2013}, {"title": "VerbOcean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 33\u201340, Barcelona, Spain.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Two decades of unsupervised pos induction: How far have we come", "author": ["Christos Christodoulopoulos", "Sharon Goldwater", "Mark Steedman"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "From distributional to semantic similarity", "author": ["James Richard Curran."], "venue": "Ph.D. thesis, University of Edinburgh.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "Classification of semantic relationships between nominals using pattern clusters", "author": ["Dmitry Davidov", "Ari Rappoport."], "venue": "Proceedings of the 46th Annual Meeting of the ACL: HLT (ACL 2008), pages 227\u2013235, Columbus, USA.", "citeRegEx": "Davidov and Rappoport.,? 2008", "shortCiteRegEx": "Davidov and Rappoport.", "year": 2008}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Jauhar", "Chris Dyer", "Ed Hovy", "Noah Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics \u2014 Human", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Incorporating both distributional and relational semantics in word representations", "author": ["Daniel Fried", "Kevin Duh."], "venue": "Proceedings of the International Conference on Learning Representations Workshop (ICLR 2015).", "citeRegEx": "Fried and Duh.,? 2015", "shortCiteRegEx": "Fried and Duh.", "year": 2015}, {"title": "Learning semantic hierarchies via word embeddings", "author": ["Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 1199\u20131209.", "citeRegEx": "Fu et al\\.,? 2014", "shortCiteRegEx": "Fu et al\\.", "year": 2014}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["M. Geffet", "I. Dagan."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), Michigan.", "citeRegEx": "Geffet and Dagan.,? 2005", "shortCiteRegEx": "Geffet and Dagan.", "year": 2005}, {"title": "Semeval-2007 task 04: Classification of semantic relations between nominals", "author": ["Roxana Girju", "Preslav Nakov", "Vivi Nastase", "Stan Szpakowicz", "Peter Turney", "Deniz Yuret."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages", "citeRegEx": "Girju et al\\.,? 2007", "shortCiteRegEx": "Girju et al\\.", "year": 2007}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti Hearst."], "venue": "Proceedings of the 14th International Conference on Computational Linguistics (COLING \u201992), Nantes, France.", "citeRegEx": "Hearst.,? 1992", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Semeval-2012 task 2: Measuring degrees of relational similarity", "author": ["David Jurgens", "Saif Mohammad", "Peter Turney", "Keith Holyoak."], "venue": "Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), pages 356\u2013364, Montr\u00e9al, Canada.", "citeRegEx": "Jurgens et al\\.,? 2012", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Deriving adjectival scales from continuous space word", "author": ["Joo-Kyung Kim", "Marie-Catherine de Marneffe"], "venue": null, "citeRegEx": "Kim and Marneffe.,? \\Q2013\\E", "shortCiteRegEx": "Kim and Marneffe.", "year": 2013}, {"title": "Multilingual reliability and \u201csemantic\u201d structure of continuous word spaces", "author": ["Maximilian K\u00f6per", "Christian Scheible", "Sabine Schulte im Walde."], "venue": "Proceedings of the Eleventh International Workshop on Computational Semantics", "citeRegEx": "K\u00f6per et al\\.,? 2015", "shortCiteRegEx": "K\u00f6per et al\\.", "year": 2015}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."], "venue": "Natural Language Engineering, 16:359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Semantic class learning from the web with hyponym pattern linkage graphs", "author": ["Zornitsa Kozareva", "Ellen Riloff", "Eduard Hovy."], "venue": "Proceedings of the 46th Annual Meeting of the ACL: HLT (ACL 2008), pages 1048\u20131056, Columbus, USA.", "citeRegEx": "Kozareva et al\\.,? 2008", "shortCiteRegEx": "Kozareva et al\\.", "year": 2008}, {"title": "Identifying hypernyms in distributional semantic spaces", "author": ["Alessandro Lenci", "Giulia Benotto."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM 2012), pages 75\u201379, Montr\u00e9al, Canada.", "citeRegEx": "Lenci and Benotto.,? 2012", "shortCiteRegEx": "Lenci and Benotto.", "year": 2012}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 18th Conference on Natural Language Learning (CoNLL-2014), pages 171\u2013180, Baltimore, USA.", "citeRegEx": "Levy and Goldberg.,? 2014a", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Neural word embeddings as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of NIPS.", "citeRegEx": "Levy and Goldberg.,? 2014b", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Applicative structure in vector space models", "author": ["M\u00e1rton Makrai", "D\u00e1vid Nemeskey", "Andr\u00e1s Kornai."], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 59\u201363, Sofia, Bulgaria.", "citeRegEx": "Makrai et al\\.,? 2013", "shortCiteRegEx": "Makrai et al\\.", "year": 2013}, {"title": "SemEval-2010 Task 14: Word sense induction & disambiguation", "author": ["Suresh Manandhar", "Ioannis Klapaftis", "Dmitriy Dligach", "Sameer Pradhan."], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63\u201368, Uppsala, Sweden.", "citeRegEx": "Manandhar et al\\.,? 2010", "shortCiteRegEx": "Manandhar et al\\.", "year": 2010}, {"title": "Relation guided bootstrapping of semantic lexicons", "author": ["Tara McIntosh", "Lars Yencken", "James R. Curran", "Timothy Baldwin."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011),", "citeRegEx": "McIntosh et al\\.,? 2011", "shortCiteRegEx": "McIntosh et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Workshop at the International Conference on Learning Representations, 2013, Scottsdale, USA.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1081\u2013 1088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Proceedings of NIPS.", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["Silvia Nec\u015fulescu", "Sara Mendes", "David Jurgens", "N\u00faria Bel", "Roberto Navigli."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computa-", "citeRegEx": "Nec\u015fulescu et al\\.,? 2015", "shortCiteRegEx": "Nec\u015fulescu et al\\.", "year": 2015}, {"title": "Espresso: Leveraging generic patterns for automatically harvesting semantic relations", "author": ["Patrick Pantel", "Marco Pennacchiotti."], "venue": "Proceedings of COLING/ACL 2006, pages 113\u2013120, Sydney, Australia.", "citeRegEx": "Pantel and Pennacchiotti.,? 2006", "shortCiteRegEx": "Pantel and Pennacchiotti.", "year": 2006}, {"title": "Towards terascale semantic acquisition", "author": ["Patrick Pantel", "Deepak Ravichandran", "Eduard Hovy."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), pages 771\u2013777, Geneva, Switzerland.", "citeRegEx": "Pantel et al\\.,? 2004", "shortCiteRegEx": "Pantel et al\\.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distributional lexical entailment by topic coherence", "author": ["Laura Rimell."], "venue": "Proceedings of the 14th Conference of the EACL (EACL 2014), Gothenburg, Sweden.", "citeRegEx": "Rimell.,? 2014", "shortCiteRegEx": "Rimell.", "year": 2014}, {"title": "VMeasure: A conditional entropy-based external cluster evaluation measure", "author": ["Andrew Rosenberg", "Julia Hirschberg."], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Rosenberg and Hirschberg.,? 2007", "shortCiteRegEx": "Rosenberg and Hirschberg.", "year": 2007}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde."], "venue": "Proceedings of the 14th Conference of the EACL (EACL 2014), pages 38\u201342.", "citeRegEx": "Santus et al\\.,? 2014", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Using lexical and relational similarity to classify semantic relations", "author": ["Diarmuid \u00d3 S\u00e9aghdha", "Ann Copestake."], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 621\u2013629. Association for", "citeRegEx": "S\u00e9aghdha and Copestake.,? 2009", "shortCiteRegEx": "S\u00e9aghdha and Copestake.", "year": 2009}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng."], "venue": "Advances in Neural Information Processing Systems 17 (NIPS-05), pages 1297\u20131304, Vancouver, Canada.", "citeRegEx": "Snow et al\\.,? 2005", "shortCiteRegEx": "Snow et al\\.", "year": 2005}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Advances in Neural Information Processing Systems 25 (NIPS-13).", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Introduction to Data Mining", "author": ["Pang-Ning Tan", "Michael Steinbach", "Vipin Kumar."], "venue": "Addison Wesley.", "citeRegEx": "Tan et al\\.,? 2006a", "shortCiteRegEx": "Tan et al\\.", "year": 2006}, {"title": "Extending corpus-based identification of light verb constructions using a supervised learning framework", "author": ["Yee Fan Tan", "Min-Yen Kan", "Hang Cui."], "venue": "Proceedings of the EACL 2006 Workshop on Multiword-expressions in a Multilingual Context, pages 49\u2013", "citeRegEx": "Tan et al\\.,? 2006b", "shortCiteRegEx": "Tan et al\\.", "year": 2006}, {"title": "Combining independent modules to solve multiple-choice synonym and analogy problems", "author": ["Peter D. Turney", "Michael L. Littman", "Jeffrey Bigham", "Victor Shnayder."], "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Pro-", "citeRegEx": "Turney et al\\.,? 2003", "shortCiteRegEx": "Turney et al\\.", "year": 2003}, {"title": "Similarity of semantic relations", "author": ["Peter D. Turney."], "venue": "Computational Linguistics, 32(3):379416.", "citeRegEx": "Turney.,? 2006", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "author": ["Peter D. Turney."], "venue": "Transactions of the Association for Computational Linguistics, 1:353\u2013366.", "citeRegEx": "Turney.,? 2013", "shortCiteRegEx": "Turney.", "year": 2013}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg."], "venue": "Statistics and computing, 17(4):395\u2013416.", "citeRegEx": "Luxburg.,? 2007", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014).", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "From information to knowledge: harvesting entities and relationships from web sources", "author": ["Gerhard Weikum", "Martin Theobald."], "venue": "Proceedings of the Twenty Ninth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, pages 65\u2013", "citeRegEx": "Weikum and Theobald.,? 2010", "shortCiteRegEx": "Weikum and Theobald.", "year": 2010}, {"title": "Rcnet: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yanlong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proceedings of the 23rd ACM Conference on Information and Knowledge", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Automatic discovery of telic and agentive roles from corpus data", "author": ["Ichiro Yamada", "Timothy Baldwin."], "venue": "Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation (PACLIC 18), pages 115\u2013126, Tokyo, Japan.", "citeRegEx": "Yamada and Baldwin.,? 2004", "shortCiteRegEx": "Yamada and Baldwin.", "year": 2004}, {"title": "Hypernym discovery based on distributional similarity and hierarchical structures", "author": ["Ichiro Yamada", "Kentaro Torisawa", "Jun\u2019ichi Kazama", "Kow Kuroda", "Masaki Murata", "Stijn De Saeger", "Francis Bond", "Asuka Sumida"], "venue": "In Proceedings of the 2009 Conference", "citeRegEx": "Yamada et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2009}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 545\u2013550.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Combining heterogeneous models for measuring relational similarity", "author": ["A. Zhila", "W.T. Yih", "C. Meek", "G. Zweig", "T. Mikolov."], "venue": "Proceedings of NAACLHLT.", "citeRegEx": "Zhila et al\\.,? 2013", "shortCiteRegEx": "Zhila et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Accurate relation classification, relational similarity prediction, and wide-coverage and adaptable relation discovery can contribute to numerous NLP applications including paraphrasing and generation, machine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010).", "startOffset": 245, "endOffset": 289}, {"referenceID": 18, "context": "Accurate relation classification, relational similarity prediction, and wide-coverage and adaptable relation discovery can contribute to numerous NLP applications including paraphrasing and generation, machine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010).", "startOffset": 245, "endOffset": 289}, {"referenceID": 31, "context": "(2013a) and other neural language models have been to shown to perform well on an analogy completion task (Mikolov et al., 2013c; Mikolov et al., 2013b), in the space of relational similarity prediction (Turney, 2006).", "startOffset": 106, "endOffset": 152}, {"referenceID": 48, "context": ", 2013b), in the space of relational similarity prediction (Turney, 2006).", "startOffset": 59, "endOffset": 73}, {"referenceID": 30, "context": "The skip-gram model of Mikolov et al. (2013a) and other neural language models have been to shown to perform well on an analogy completion task (Mikolov et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 25, "context": "france vector that appears to encode CAPITAL-OF, presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (Levy and Goldberg, 2014a).", "startOffset": 185, "endOffset": 211}, {"referenceID": 21, "context": "K\u00f6per et al. (2015) found that a neural language model performed poorly on analogies involvar X iv :1 50 9.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "There may also be more fine-grained structure in the offsets: Fu et al. (2014) found that vector offsets representing the hypernym relation could be grouped into semantic sub-clusters, as the difference", "startOffset": 62, "endOffset": 79}, {"referenceID": 1, "context": "The relations may be pre-defined or, in the Open Information Extraction paradigm (Banko et al., 2007; Weikum and Theobald, 2010), the relations themselves are also learned from the text (e.", "startOffset": 81, "endOffset": 128}, {"referenceID": 53, "context": "The relations may be pre-defined or, in the Open Information Extraction paradigm (Banko et al., 2007; Weikum and Theobald, 2010), the relations themselves are also learned from the text (e.", "startOffset": 81, "endOffset": 128}, {"referenceID": 16, "context": "Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 18, "context": "Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 19, "context": "Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 36, "context": "WHOLE-PART, but also corpus-specific relations such as CEO-OF-COMPANY (Pantel and Pennacchiotti, 2006).", "startOffset": 70, "endOffset": 102}, {"referenceID": 16, "context": "Some datasets are task-specific, for example focused on paraphrasing the relation holding between nouns in noun-noun compounds (Girju et al., 2007), or analogy questions from the American SAT exam for relational similarity (Turney et", "startOffset": 127, "endOffset": 147}, {"referenceID": 17, "context": "Relation extraction has used pattern-based approaches such as A such as B, either explicitly (Hearst, 1992; Kozareva et al., 2008; McIntosh et al., 2011) or implicitly (Snow et al.", "startOffset": 93, "endOffset": 153}, {"referenceID": 23, "context": "Relation extraction has used pattern-based approaches such as A such as B, either explicitly (Hearst, 1992; Kozareva et al., 2008; McIntosh et al., 2011) or implicitly (Snow et al.", "startOffset": 93, "endOffset": 153}, {"referenceID": 29, "context": "Relation extraction has used pattern-based approaches such as A such as B, either explicitly (Hearst, 1992; Kozareva et al., 2008; McIntosh et al., 2011) or implicitly (Snow et al.", "startOffset": 93, "endOffset": 153}, {"referenceID": 43, "context": ", 2011) or implicitly (Snow et al., 2005), although not all relations are equally amenable to this style of approach (Yamada and Baldwin, 2004).", "startOffset": 22, "endOffset": 41}, {"referenceID": 55, "context": ", 2005), although not all relations are equally amenable to this style of approach (Yamada and Baldwin, 2004).", "startOffset": 83, "endOffset": 109}, {"referenceID": 6, "context": "sification involves supervised classifiers (Chklovski and Pantel, 2004; Snow et al., 2005; Davidov and Rappoport, 2008).", "startOffset": 43, "endOffset": 119}, {"referenceID": 43, "context": "sification involves supervised classifiers (Chklovski and Pantel, 2004; Snow et al., 2005; Davidov and Rappoport, 2008).", "startOffset": 43, "endOffset": 119}, {"referenceID": 10, "context": "sification involves supervised classifiers (Chklovski and Pantel, 2004; Snow et al., 2005; Davidov and Rappoport, 2008).", "startOffset": 43, "endOffset": 119}, {"referenceID": 42, "context": "Relational similarity prediction has also mostly used classification based on lexico-syntactic patterns linking word pairs in text (S\u00e9aghdha and Copestake, 2009; Jurgens et al., 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 19, "context": "Relational similarity prediction has also mostly used classification based on lexico-syntactic patterns linking word pairs in text (S\u00e9aghdha and Copestake, 2009; Jurgens et al., 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 49, "context": "Relational similarity prediction has also mostly used classification based on lexico-syntactic patterns linking word pairs in text (S\u00e9aghdha and Copestake, 2009; Jurgens et al., 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 48, "context": ", 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al., 2013).", "startOffset": 153, "endOffset": 187}, {"referenceID": 5, "context": ", 2012; Turney, 2013), or generalised from manually crafted resources such as WordNet (Fellbaum, 1998) using techniques such as Latent Semantic Analysis (Turney, 2006; Chang et al., 2013).", "startOffset": 153, "endOffset": 187}, {"referenceID": 32, "context": "Distributional word vectors, while mostly applied to measuring semantic similarity and relatedness (Mitchell and Lapata, 2010),", "startOffset": 99, "endOffset": 126}, {"referenceID": 56, "context": ", 2014) and qualia structure (Yamada et al., 2009).", "startOffset": 29, "endOffset": 50}, {"referenceID": 31, "context": "ral word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks.", "startOffset": 20, "endOffset": 43}, {"referenceID": 30, "context": "ral word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the DIFFVEC idea in different contexts. The original analogy dataset has been used to evaluate neural language models by Mnih and Kavukcuoglu (2013) and also Zhila et al.", "startOffset": 21, "endOffset": 286}, {"referenceID": 30, "context": "ral word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the DIFFVEC idea in different contexts. The original analogy dataset has been used to evaluate neural language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model", "startOffset": 21, "endOffset": 315}, {"referenceID": 14, "context": "Fu et al. (2014) similarly use embeddings to predict hypernym relations, but instead of using a single DIFFVEC, they cluster words by topic and show that the hypernym DIFFVEC can be broken", "startOffset": 0, "endOffset": 17}, {"referenceID": 4, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 44, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 54, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 57, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 11, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 13, "context": "Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015).", "startOffset": 143, "endOffset": 266}, {"referenceID": 25, "context": "Another strand of work responding to the vector difference approach has analysed the structure of neural embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015).", "startOffset": 192, "endOffset": 264}, {"referenceID": 26, "context": "Another strand of work responding to the vector difference approach has analysed the structure of neural embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015).", "startOffset": 192, "endOffset": 264}, {"referenceID": 0, "context": "Another strand of work responding to the vector difference approach has analysed the structure of neural embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015).", "startOffset": 192, "endOffset": 264}, {"referenceID": 26, "context": "Makrai et al. (2013) divided antonym pairs into semantic classes such as quality, time, gender, and distance, and tested whether the DIFFVECs internal to each antonym class were significantly more correlated than random.", "startOffset": 0, "endOffset": 21}, {"referenceID": 26, "context": "Makrai et al. (2013) divided antonym pairs into semantic classes such as quality, time, gender, and distance, and tested whether the DIFFVECs internal to each antonym class were significantly more correlated than random. They found that for about two-thirds of the antonym classes, the DIFFVECs were significantly correlated. Nec\u015fulescu et al. (2015) trained a classifier on word pairs using word embeddings in order to predict coordinates, hypernyms, and meronyms.", "startOffset": 0, "endOffset": 351}, {"referenceID": 21, "context": "K\u00f6per et al. (2015) undertook a systematic study of morphosyntactic and semantic relations on word embeddings produced with word2vec (\u201cw2v\u201d hereafter; see \u00a73.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "K\u00f6per et al. (2015) undertook a systematic study of morphosyntactic and semantic relations on word embeddings produced with word2vec (\u201cw2v\u201d hereafter; see \u00a73.1) for English and German. They tested a variety of relations including word similarity, antonyms, synonyms, hypernyms, and meronyms, in a novel analogy task. Although the set of relations tested by K\u00f6per et al. (2015) is somewhat more constrained than the set we use, there is a good deal of overlap.", "startOffset": 0, "endOffset": 377}, {"referenceID": 30, "context": "well represented in the analogy dataset of Mikolov et al. (2013c)), but including morphosyntactic and morphosemantic relations (see \u00a73.", "startOffset": 43, "endOffset": 66}, {"referenceID": 30, "context": "We consider four highly successful word embedding models in our experiments: w2v (Mikolov et al., 2013a), GloVE (Pennington et al.", "startOffset": 81, "endOffset": 104}, {"referenceID": 38, "context": ", 2013a), GloVE (Pennington et al., 2014), SENNA", "startOffset": 16, "endOffset": 41}, {"referenceID": 8, "context": "(Collobert et al., 2011), and HLBL (Mnih and Hinton, 2009).", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": ", 2011), and HLBL (Mnih and Hinton, 2009).", "startOffset": 18, "endOffset": 41}, {"referenceID": 30, "context": "There is some overlap between our relations and those included in the analogy task of Mikolov et al. (2013c), but we include a much wider range of lexical semantic relations, especially those standardly evaluated in the relation classification literature.", "startOffset": 86, "endOffset": 109}, {"referenceID": 19, "context": "The final dataset consists of 12,943 triples \u3008relation,word1,word2\u3009, comprising 18 relation types, extracted from SemEval\u201912 (Jurgens et al., 2012), BLESS (Baroni et al.", "startOffset": 125, "endOffset": 147}, {"referenceID": 3, "context": ", 2012), BLESS (Baroni et al., 2014), the MSR analogy dataset (Mikolov et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 31, "context": ", 2014), the MSR analogy dataset (Mikolov et al., 2013c), the dataset of Tan et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 3, "context": ", 2012), BLESS (Baroni et al., 2014), the MSR analogy dataset (Mikolov et al., 2013c), the dataset of Tan et al. (2006a), Princeton WordNet, and Wiktionary, as listed in Tab 2 and detailed below (wherein we define each relation relative to the directed word pair (x, y)).", "startOffset": 16, "endOffset": 121}, {"referenceID": 19, "context": "Our dataset includes the seven top-level asymmetric lexical semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012): SEMEVALClass: x names a class that includes entity y; e.", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "It also includes three lexical semantic relations from BLESS (Baroni and Lenci, 2011): BLESSHyper: x names a noun class that includes entity y; e.", "startOffset": 61, "endOffset": 85}, {"referenceID": 30, "context": "As morphosyntactic paradigm lexical relations, we include four relations from the original Mikolov et al. (2013c) DIFFVEC paper: NOUNSP: y is the plural form (NNS, in Penn tagset terms) of singular noun x (an NN); e.", "startOffset": 91, "endOffset": 114}, {"referenceID": 45, "context": "Relation Pairs Source SEMEVALClass 123 SemEval\u201912 SEMEVALPart 280 SemEval\u201912 SEMEVALAttr 71 SemEval\u201912 SEMEVALCase 255 SemEval\u201912 SEMEVALCause 255 SemEval\u201912 SEMEVALSpace 261 SemEval\u201912 SEMEVALRef 192 SemEval\u201912 BLESSHyper 1095 BLESS BLESSMero 2631 BLESS BLESSEvent 3163 BLESS NOUNSP 100 MSR VERB3 100 MSR VERBPast 100 MSR VERB3Past 100 MSR LVC 58 Tan et al. (2006b) VERBNOUN 3309 WordNet PREFIX 147 Wiktionary NOUNColl 257 Wiktionary", "startOffset": 348, "endOffset": 367}, {"referenceID": 40, "context": "We tune the hyperparameters over development data, selecting the configuration that maximises the V-Measure (Rosenberg and Hirschberg, 2007).", "startOffset": 108, "endOffset": 140}, {"referenceID": 7, "context": "Our use of V-Measure is based on the findings of Christodoulopoulos et al. (2010), who showed for part-of-speech induction that out of seven clustering evaluation measures, V-Measure is the most effective and least sensitive to the number of clusters.", "startOffset": 49, "endOffset": 82}, {"referenceID": 28, "context": "somewhat related lexical semantic clustering task of word sense induction, the best-performing systems in SemEval-2010 Task 4 (Manandhar et al., 2010) achieved a V-Measure of under 0.", "startOffset": 126, "endOffset": 150}, {"referenceID": 37, "context": "As observed by Pennington et al. (2014) and Curran (2004), training based on one-sided context reduces the ability of a model to capture lexical semantic relations in particular.", "startOffset": 15, "endOffset": 40}, {"referenceID": 9, "context": "(2014) and Curran (2004), training based on one-sided context reduces the ability of a model to capture lexical semantic relations in particular.", "startOffset": 11, "endOffset": 25}, {"referenceID": 37, "context": "experiments are presented in Tab 5, in which we report on the combination of the original (CLOSEDWORLD) and random (OPEN-WORLD) test data, noting that recall (R) for OPEN-WORLD takes the form of relative recall (Pantel et al., 2004) over the positively-classified word pairs.", "startOffset": 211, "endOffset": 232}, {"referenceID": 21, "context": "First, clustering showed us that many types of morphosyntactic and morphosemantic differences are captured by DIFFVECs, but that lexical semantic relations are captured less well, consistent with previous work (K\u00f6per et al., 2015).", "startOffset": 210, "endOffset": 230}], "year": 2017, "abstractText": "Recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items.", "creator": "TeX"}}}