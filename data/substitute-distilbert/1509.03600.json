{"id": "1509.03600", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2015", "title": "Hardness of Online Sleeping Combinatorial Optimization Problems", "abstract": "we show that several online combinatorial optimization problems that admit efficient no - regret algorithms become computationally hard in instant waking setting where a subset causing actions becomes unavailable in each round. specifically, we show that the sleeping versions of these problems are at least as hard as pac learning dnf expressions, a long standing allocation problem. we show hardness for the slower versions of online shortest sections, online minimum spanning tree, online k - subsets, online k - truncated preferences, online minimum cut, and online cache matching. the hardest result for infinite sleeping version and the online shortest paths problem resolves an inaccessible problem presented at colt 2015 ( mead et res., 2015 ).", "histories": [["v1", "Fri, 11 Sep 2015 18:27:42 GMT  (207kb,D)", "http://arxiv.org/abs/1509.03600v1", null], ["v2", "Tue, 22 Sep 2015 18:12:37 GMT  (211kb,D)", "http://arxiv.org/abs/1509.03600v2", null], ["v3", "Mon, 19 Dec 2016 22:28:15 GMT  (209kb,D)", "http://arxiv.org/abs/1509.03600v3", "A version of this paper was published in NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["satyen kale", "chansoo lee", "d\u00e1vid p\u00e1l"], "accepted": true, "id": "1509.03600"}, "pdf": {"name": "1509.03600.pdf", "metadata": {"source": "CRF", "title": "Hardness of Online Sleeping Combinatorial Optimization Problems", "authors": ["Satyen Kale", "Chansoo Lee"], "emails": ["satyen@yahoo-inc.com", "chansool@umich.edu", "dpal@yahoo-inc.com"], "sections": [{"heading": "1 Introduction", "text": "Online learning is a sequential decision-making problem where learner repeatedly chooses an action in response to adversarially chosen losses for the available actions. The goal of the learner is to minimize the regret, defined as the difference between the total loss of the algorithm and the loss of the best fixed action in hindsight. In online combinatorial optimization, the actions are subsets of a ground set of elements (also called components) with some combinatorial structure. The loss of an action is the sum of the losses of its elements. A particular well-studied instance is the Online Shortest Path problem, in which the actions are the paths between two fixed vertices in a fixed graph, with the edges forming the elements.\nWe study a sleeping variant of online combinatorial optimization where the adversary not only chooses losses but also availability of the elements. The unavailable elements are called sleeping or sabotaged. In Online Sabotaged Shortest Path problem, for example, in each round the adversary specifies a subset of edges as unavailable, and consequently any path using those edges is unavailable to the learner as well. The design of a computationally efficient algorithm for Online Sabotaged Shortest Path problem was presented as an open problem at COLT 2015 by Koolen et al. (2015).\nIn this paper, we resolve this open problem and prove that Online Sabotaged Shortest Path problem is computationally hard. Specifically, we show that a polynomial-time low-regret algorithm for this problem implies a polynomial-time algorithm for PAC learning DNF expressions, which is a long-standing open problem. The best known algorithm for PAC learning DNF expressions on n variables has time complexity 2O\u0303(n 1/3) (Klivans and Servedio, 2001).\n\u2217This work was done while the author was a summer intern at Yahoo Labs New York.\nar X\niv :1\n50 9.\n03 60\n0v 1\n[ cs\n.L G\n] 1\n1 Se\np 20\nOur reduction technique can be abstracted and generalized to other problems. Our main result shows that any online sleeping combinatorial problem where the set of actions can be constructed with two (fairly easy to satisfy) structural properties is as hard as PAC learning DNF expressions. Leveraging our main result allows us to immediately obtain hardness results for the sleeping versions of several other problems such as Minimum Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, and Online Bipartite Matching. All these problems are notable in that efficient, polynomial-time no-regret algorithms are known for each of them in the standard (non-sleeping) setting. Our hardness result constructs an adversarial sequence of element availabilities and losses that is stochastic and i.i.d., implying that stronger restrictions need to be made on the adversary in order to obtain efficient algorithms."}, {"heading": "1.1 Related Work", "text": "The standard problem of online linear optimization with d actions (Experts setting) admits algorithms with O(d) running time per-round and O( \u221a T log d) regret after T rounds (Littlestone and Warmuth, 1994; Freund and Schapire, 1997); the regret is known to be minimax optimal (CesaBianchi and Lugosi, 2006, Chapter 2).\nOnline combinatorial optimization problems over a ground set of d elements may have exp(O(d)) actions, and thus a naive application of the algorithms for the Experts setting will result in exp(O(d)) running time and O( \u221a Td) regret. Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013). Common to these six problems is that their corresponding off-line problems have a polynomial-time algorithm. In fact, as shown by Kalai and Vempala (2005), a polynomial-time algorithm for an offline combinatorial problem implies the existence of an algorithm for the corresponding online optimization problem with the same per-round running time and O(poly(d) \u221a T ) regret.\nIn studying online sleeping optimization, three different notions of regret have been used: (a) policy regret, (b) ranking regret, and (c) per-action regret. Policy regret is the total difference between the loss of the algorithm and the loss of the best policy, which maps a set of available actions to one of the available actions (Neu and Valko, 2014). Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009). Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015). Note that policy regret upper bounds ranking regret, and per-action regret is incomparable to either policy or ranking regret.\nThere are several results about the sleeping Experts setting (also known as Specialists setting). First, there exists an algorithm with O(d) running time per round that achieves per-action regret of order O( \u221a T log d) (Freund et al., 1997). Second, Kleinberg et al. (2010) and Kanade and Steinke (2014) show that achieving O(poly(d)T 1\u2212\u03b4) ranking regret is computationally hard, even under the assumption that the action availability and losses are drawn i.i.d. from a fixed but unknown joint\n1In this paper, we use the poly(\u00b7) notation to indicate a polynomially bounded function of the arguments.\ndistribution. However, the setting where the set of available actions in each round is drawn i.i.d. from a fixed but unknown probability distribution, and the losses are chosen adversarially but independent of the choice of the available actions in each round turns out to be tractable. Under these assumptions, for the sleeping Experts setting, Kanade et al. (2009) give an algorithm running in poly(d) time per iteration with policy regret bounded by O( \u221a T log d), and for the general online sleeping combinatorial optimization setting, Neu and Valko (2014) give an algorithm running in poly(d) time per round and with policy regret bounded by O(m \u221a Td log d), where m is an upper bound on the size of each action. The latter work is the only published result on online sleeping combinatorial optimization to date, although it doesn\u2019t give bounds on the per-action regret, the primary performance measure in this paper.\nOur reduction technique is closely related to that of Kanade and Steinke (2014), who reduced agnostic learning of disjunctions to ranking regret minimization in the sleeping Experts setting."}, {"heading": "1.2 Overview of the Paper", "text": "In section 2, we formally define online sleeping combinatorial optimization problems. In section 3, we introduce the problem of Agnostic Online Learning of Disjunctions and we explain that a computationally efficient algorithm with sublinear per-action regret implies computationally efficient algorithm for learning DNF expressions. The core of the paper is section 4, where we reduce agnostic online learning of disjunctions to any online sleeping combinatorial optimization problem which admits instances with decision sets that satisfy two properties which capture the essence of the computational hardness. In section 5, we show that each of the six online sleeping optimization problems (Online Shortest Paths, Online Minimum Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, and Online Bipartite Matching) admit instances with decision sets satisfying these two properties, thus establishing their computational hardness."}, {"heading": "2 Preliminaries", "text": "An instance of online combinatorial optimization is defined by a ground set U of d elements, and a decision set D of actions, each of which is a subset of U . In each round t, the online learner is required to choose an action Vt \u2208 D, while simultaneously an adversary chooses a loss function `t : U \u2192 [\u22121, 1]. The loss of any V \u2208 D is given by (with some abuse of notation)\n`t(V ) := \u2211 e\u2208V `t(e).\nThe learner suffers loss `t(Vt) and obtains `t as feedback. The regret of the learner with respect to an action V \u2208 D is defined to be\nRegretT (V ) := T\u2211 t=1 `t(Vt)\u2212 `t(V ).\nWe now define an instance of the online sleeping combinatorial optimization. In this setting, at the start of each round t, the adversary selects a set of sleeping elements St \u2286 U and reveals it to the learner. Define At = {V \u2208 D | V \u2229 St = \u2205}, the set of awake actions at round t; the remaining\nactions in D are called sleeping actions and are unavailable to the learner for that round. If At is empty, i.e., there are no awake actions, then the learner is not required to do anything for that round and the round is discarded from computation of the regret.\nFor the rest of the paper, we use per-action regret as our performance measure. Per-action regret with respect to V \u2208 D is defined as:\nRegretT (V ) := \u2211\nt:V \u2208At\n`t(Vt)\u2212 `t(V ). (1)\nIn other words, our notion of regret considers only the rounds in which V is awake. We say that a learning algorithm for the online (sleeping) combinatorial optimization problem has a regret bound of f(n, T ) if RegretT (V ) \u2264 f(n, T ) for all V \u2208 D. We say that a learning algorithm has no regret if f(n, T ) = poly(n)T 1\u2212\u03b4 for some \u03b4 \u2208 (0, 1), and it is computationally efficient if it has a per-round running time of order poly(n, T ).\nFor clarity, we define online combinatorial optimization problem as a family of instances of online combinatorial optimization (and correspondingly for online sleeping combinatorial optimization). For example, online shortest path problem is the family of all instances over all graphs with designated source and sink vertices, where the decision set D is a set of paths from the source to sink, and the elements are edges of the graph. The family is primarily parameterized by size of the ground set, d, but other parameters may also be necessary, such as the value of k in the k-subsets problem.\nOur main result is that many natural online sleeping combinatorial optimization problems are unlikely to admit a computationally efficient no-regret algorithm, although their non-sleeping versions (i.e., At = D for all t) do. More precisely, we show that these online sleeping combinatorial optimization problems are at least as hard as PAC learning DNF expressions, a long-standing open problem."}, {"heading": "3 Online Agnostic Learning of Disjunctions", "text": "To show that PAC learning DNF expressions reduces to obtaining efficient algorithms for the online sleeping combinatorial optimization problems considered in this paper, we use an intermediate problem, online agnostic learning of disjunctions. By a standard online-to-batch conversion argument (Kanade and Steinke, 2014), online agnostic learning of disjunctions is at least as hard as agnostic improper PAC-learning of disjunctions (Kearns et al., 1994), which in turn is at least as hard as PAC-learning of DNF expressions (Kalai et al., 2012).\nOnline agnostic learning of disjunctions is a repeated game between the adversary and a learning algorithm. In each round t, the adversary chooses a vector xt \u2208 {0, 1}n, the algorithm predicts a label y\u0302t \u2208 {0, 1} and then the adversary reveals the correct label yt \u2208 {0, 1}. If y\u0302t 6= yt, we say that algorithm makes an error.\nThe goal of the algorithm is make as few errors as possible. For any predictor \u03c6 : {0, 1}n \u2192 {0, 1}, we define the regret with respect to \u03c6 after T rounds as\nRegretT (\u03c6) = T\u2211 t=1 1[y\u0302t 6= yt]\u2212 1[\u03c6(xt) 6= yt].\nIn online agnostic learning of disjunctions, we desire an algorithm that is competitive with any disjunction, i.e. for any disjunction \u03c6 over n variables, the regret is bounded by poly(n) \u00b7 T 1\u2212\u03b4 for\nsome \u03b4 \u2208 (0, 1). Recall that a disjunction over n variables is a boolean function \u03c6 : {0, 1}n \u2192 {0, 1} that on an input x = (x(1), x(2), . . . , x(n)) outputs\n\u03c6(x) = (\u2228 i\u2208P x(i) ) \u2228 (\u2228 i\u2208N x(i) )\nwhere P,N are disjoint subsets of {1, 2, . . . , n}. We allow either P or N to be empty, and the empty disjunction is interpreted as the constant 0 function. For any index i \u2208 {1, 2, . . . , n}, we call it a relevant index for \u03c6 if i \u2208 P \u222aN and irrelevant index for \u03c6 otherwise. For any relevant index i, we call it positive for \u03c6 if i \u2208 P and negative for \u03c6 if i \u2208 N .\nThe online-to-batch conversion of online agnostic learning of disjunctions to agnostic improper PAC-learning of disjunctions (Kanade and Steinke, 2014) mentioned above implies that we may assume that the input sequence (xt, yt) for the online problem is drawn i.i.d. from an unknown distribution; the problem remains as hard as PAC learning DNF expressions. This implies that in our reduction to online sleeping combinatorial optimization the adversary can be assumed to be drawing availabilities and losses i.i.d. from an unknown distribution as well."}, {"heading": "4 Base Hardness Result", "text": "Definition 1. Let n be a positive integer. An instance of Online Sleeping Combinatorial Optimization is called a hard instance with parameter n, if the ground set U has d elements with 3n+ 2 \u2264 d \u2264 poly(n), and there are 3n+ 2 special elements of U denoted\nn\u22c3 i=1 {(i, 0), (i, 1), (i, ?)} \u222a {0, 1},\nsuch that the decision set D satisfies the following properties:\n1. (Heaviness) Any action V \u2208 D has at least n+ 1 elements.\n2. (Richness) For all (s1, . . . , sn+1) \u2208 {0, 1, ?}n\u00d7{0, 1}, the action {(1, s1), (2, s2), . . . , (n, sn), sn+1} is in D.\nWe now show how to use the above definition of hard instances to prove the hardness of an online sleeping combinatorial optimization problem.\nTheorem 1. Consider an online sleeping combinatorial optimization problem such that for any positive integer n, there is a hard instance with parameter n of the problem. Suppose there is an algorithm A that for any instance of the problem with ground set U of size d, runs in time poly(T, d) and has regret bounded by poly(d) \u00b7T 1\u2212\u03b4 for some \u03b4 \u2208 (0, 1). Then, there exists an algorithm B for online agnostic learning of disjunctions over n variables with running time poly(T, n) and regret poly(n) \u00b7 T 1\u2212\u03b4.\nProof. B is given in Algorithm 1. First, we note that in each round t, we have\n`t(Vt) \u2265 1[yt 6= y\u0302t]. (2)\nWe prove this separately for two different cases; in both cases, the inequality follows from the heaviness property, i.e., the fact that |Vt| \u2265 n+ 1.\nAlgorithm 1 Algorithm B for learning disjunctions Require: An algorithm A for the online sleeping combinatorial optimization problem over D. 1: for t = 1, 2, . . . , T do 2: Receive xt \u2208 {0, 1}n. 3: Set the set of sleeping elements for A to be St = {(i, 1\u2212 xt(i)) | i = 1, 2, . . . , n}. 4: Obtain an action Vt \u2208 D by running A such that Vt \u2229 St = \u2205. 5: Set y\u0302t = 1[0 /\u2208 Vt]. 6: Predict y\u0302t, and receive true label yt. 7: In algorithm A, set the loss of the awake elements e \u2208 U \\ St as follows:\n`t(e) = { 1\u2212yt n+1 if e 6= 0 yt \u2212 n(1\u2212yt)n+1 if e = 0.\n8: end for\n1. If 0 /\u2208 Vt, then the prediction of B is y\u0302t = 1, and thus\n`t(Vt) = |Vt| \u00b7 1\u2212 yt n+ 1 \u2265 1\u2212 yt = 1[yt 6= y\u0302t].\n2. If 0 \u2208 Vt, then the prediction of B is y\u0302t = 0, and thus\n`t(Vt) = (|Vt| \u2212 1) \u00b7 1\u2212 yt n+ 1 +\n( yt \u2212\nn(1\u2212 yt) n+ 1\n) \u2265 yt = 1[yt 6= y\u0302t].\nNote that if Vt satisfies the equality |Vt| = n+ 1, then we have an equality `t(Vt) = 1[yt 6= y\u0302t]; this property will be useful later.\nNext, let \u03c6 be an arbitrary disjunction, and let i1 < i2 < \u00b7 \u00b7 \u00b7 < im be its relevent indices sorted in increasing order. Define f\u03c6 : {1, 2, . . . ,m} \u2192 {0, 1} as f\u03c6(j) := 1[ij is a positive index for \u03c6], and define the set of elements W\u03c6 := {(i, ?) | i is an irrelevant index for \u03c6}. Finally, let D\u03c6 = {V 1\u03c6 , V 2\u03c6 , . . . , V m+1 \u03c6 } be the set of m+ 1 actions where for j = 1, 2, . . . ,m, we define\nV j\u03c6 := {(i`, 1\u2212 f\u03c6(`)) | 1 \u2264 ` < j} \u222a {(ij , f\u03c6(j))} \u222a {(i`, ?) | j < ` \u2264 m} \u222aW\u03c6 \u222a {1},\nand V m+1\u03c6 := {(i`, 1\u2212 f\u03c6(`)) | 1 \u2264 ` \u2264 m} \u222aW\u03c6 \u222a {0}. The actions in D\u03c6 are indeed in the decision set D due to the richness property. See Figure 1 for an example of this construction.\nWe claim that D\u03c6 contains exactly one awake action in every round and the awake action contains the element 1 if and only if \u03c6(xt) = 1. First, we prove uniqueness: if V j \u03c6 and V k \u03c6 , where j < k, are both awake in the same round, then (ij , f\u03c6(j)) \u2208 V j\u03c6 and (ij , 1 \u2212 f\u03c6(j)) \u2208 V k \u03c6 are both awake elements, contradicting our choice of St. To prove the rest of the claim, we consider two cases:\n1. If \u03c6(xt) = 1, then there is at least one j \u2208 {1, 2, . . . ,m} such that xt(ij) = f\u03c6(j). Let j\u2032 be the smallest such j. Then, by construction, the set V j \u2032\n\u03c6 is awake at time t, and 1 \u2208 V j\u2032\n\u03c6 , as required.\n2. If \u03c6(xt) = 0, then for all j \u2208 {1, 2, . . . ,m} we must have xt(ij) = 1 \u2212 f\u03c6(j). Then, by construction, the set V m+1\u03c6 is awake at time t, and 0 \u2208 V m+1 \u03c6 , as required.\nSince every action in D\u03c6 has exactly n+ 1 elements and 1 \u2208 V if and only if \u03c6(xt) = 1, exactly the same argument as in the beginning of this proof implies that\n\u2200V \u2208 D\u03c6 `t(V ) = 1[yt 6= \u03c6(xt)]. (3)\nFurthermore, since exactly one action in D\u03c6 is awake every round, we have\nT\u2211 t=1 1[yt 6= \u03c6(xt)] = \u2211 V \u2208D\u03c6 \u2211 t:V \u2208At `t(V ). (4)\nFinally, we can bound the regret of algorithm B (denoted RegretBT ) in terms of the regret of algorithm A (denoted RegretAT ) as follows:\nRegretBT (\u03c6) = T\u2211 t=1 1[y\u0302t 6= yt]\u2212 1[\u03c6(xt) 6= yt]\n\u2264 \u2211 V \u2208D\u03c6 \u2211 t:V \u2208At `t(Vt)\u2212 `t(V ) (By (2) and (4))\n= \u2211 V \u2208D\u03c6 RegretAT (V ) \u2264 |D\u03c6| \u00b7 poly(d) \u00b7 T 1\u2212\u03b4 = poly(n) \u00b7 T 1\u2212\u03b4,\nsince |D\u03c6| \u2264 n+ 1 and d \u2264 poly(n)."}, {"heading": "4.1 Non-negative Losses", "text": "While the possibility of assigning a negative loss to element 0 in Algorithm B may cause some alarm, this is done only to keep the exposition clean. It is easy to see that the hardness result given above goes through even if we restrict losses of each element to be non-negative, say in the range\n[0, 2]. This is achieved by simply adding 1 to the loss of each awake element e in Algorithm B. The only difference in the analysis is that (2) now becomes\n`t(Vt) \u2265 1[yt 6= y\u0302t] + n+ 1,\nand (3) becomes `t(V ) = 1[yt 6= \u03c6(xt)] + n+ 1.\nThe additive constant of n+ 1 cancels out when computing the regret, and hence the calculations go through just as before."}, {"heading": "5 Hardness Results", "text": "In this section, we apply Theorem 1 to prove that many online sleeping combinatorial optimization problems are computationally hard. Note that all these problems admit efficient no-regret algorithms in the non-sleeping setting."}, {"heading": "5.1 Online Shortest Path Problem", "text": "In the online shortest path problem, the learner is given a directed graph G = (V,E) and designated source and sink vertices s and t, which will be fixed over time. The ground set is the set of edges, i.e. U = E, and the decision set D is the set of all paths from s to t. The sleeping version of this problem has been called the Online Sabotaged Shortest Path problem by Koolen et al. (2015), who posed the open question of whether it admits an efficient no-regret algorithm. We show the following hardness result and resolve the open question:\nTheorem 2. For any n \u2208 N, there is a hard instance with parameter n of the Online Shortest Paths problem with d = 3n+ 2 and hence, the Online Sabotaged Paths problem is as hard as PAC learning DNF expressions.\nProof. For any given positive integer n, consider the graph G(n) shown in Figure 2. It has 3n+ 2 edges that are labeled by the elements of ground set U = \u22c3n i=1{(i, 0), (i, 1), (i, ?)} \u222a {0, 1}, as required. Now note that any s-t path in this graph has length exactly n+1, so D satisfies the heaviness property. Furthermore, the richness property is clearly satisfied, since for any s \u2208 {0, 1, ?}n\u00d7{0, 1}, the set of edges {(1, s1), (2, s2), . . . , (n, sn), sn+1} is an s-t path and therefore in D. The result follows by Theorem 1."}, {"heading": "5.2 Online Minimum Spanning Tree Problem", "text": "In the online minimum spanning tree problem, the learner is given a fixed graph G = (V,E). The ground set here is the set of edges, i.e. U = E, and the decision set D is the set of spanning trees in the graph. We prove the following hardness result:\nTheorem 3. For any n \u2208 N, there is a hard instance with parameter n of the Online Minimum Spanning Tree problem with d = 3n+ 2, and hence its sleeping version is as hard as PAC learning DNF expressions.\nProof. For any given positive integer n, consider the same graph G(n) shown in Figure 2, except that the edges are undirected. Note that the spanning trees in G(n) are exactly the paths from s to t. The hardness of this problem immediately follows from the hardness of the online shortest paths problem.\n5.3 Online k-Subsets Problem\nThe online k-Subsets problem the learner is given a fixed ground set of elements U . The decision set D is the set of subsets of U of size k. We prove the following hardness result:\nTheorem 4. For any n \u2208 N, there is a hard instance with parameter n of the Online k-Subsets Problem with k = n+ 1 and d = 3n+ 2, and hence its sleeping version is as hard as PAC learning DNF expressions.\nProof. The set D of all subsets of size k = n+1 of a ground set U of size d = 3n+2 clearly satisfies both the heaviness and richness properties, and hence the hardness follows by Theorem 1.\n5.4 Online k-Truncated Permutations Problem\nIn the online k-truncated permutations problem (also called the k-ranking problem) the learner is given a complete bipartite graph with k nodes on one side and m \u2265 k nodes on the other, and the ground set U is the set of all edges; thus d = km. The decision set D is the set of all maximal matchings, which can be interpreted as truncated permutations of k out of m objects. We prove the following hardness result:\nTheorem 5. For any n \u2208 N, there is a hard instance with parameter n of the Online k-Truncated Permutations problem with k = n + 1, m = 3n + 2 and d = km = (n + 1)(3n + 2), and hence its sleeping version is as hard as PAC learning DNF expressions.\nProof. Let L = {u1, u2, . . . , un+1} be the nodes on the left side of the bipartite graph, and since m = 3n + 2, let R = {vi,0, vi,1, vi,? | i = 1, 2, . . . , n} \u222a {v0, v1} denote the nodes on the right side of the graph. The ground set U consists of all d = km = (n + 1)(3n + 2) edges joining nodes in L to nodes in R. We now specify the special 3n + 2 elements of the ground set U : for i = 1, 2, . . . , n, label the edges (ui, vi,0), (ui, vi,1), (ui, vi,?) by (i, 0), (i, 1), (i, ?) respectively. Finally, label the edges (un+1, v0), (un+1, v1) by 0 and 1 respectively. The resulting bipartite graph P\n(n) is shown in Figure 3, where only the special labeled edges are shown for clarity.\nNow note that any maximal matching in this graph has exactly n + 1 edges, so the heaviness condition is satisfied. Furthermore, the richness property is satisfied, since for any s \u2208 {0, 1, ?}n \u00d7 {0, 1}, the set of edges {(1, s1), (2, s2), . . . , (n, sn), sn+1} is a maximal matching and therefore in D. The result follows by Theorem 1."}, {"heading": "5.5 Online Bipartite Matching Problem", "text": "In the online bipartite matching path problem the learner is given a fixed bipartite graphG = (V,E). The ground set here is the set of edges, i.e. U = E, and the decision set D is the set of maximal matchings in G. We prove the following hardness result:\nTheorem 6. For any n \u2208 N, there is a hard instance with parameter n of the Online Bipartite Matching problem with d = 3n+ 2, and hence its sleeping version is as hard as PAC learning DNF expressions.\nProof. For any given positive integer n, consider the graph M (n) shown in Figure 4. It has 3n+ 2 edges that are labeled by the elements of ground set U = \u22c3n i=1{(i, 0), (i, 1), (i, ?)} \u222a {0, 1}, as required. Now note that any maximal matching in this graph has size exactly n+ 1, so D satisfies the heaviness property. Furthermore, the richness property is clearly satisfied, since for any s \u2208 {0, 1, ?}n \u00d7 {0, 1}, the set of edges {(1, s1), (2, s2), . . . , (n, sn), sn+1} is a maximal matching and therefore in D. The result follows by Theorem 1."}, {"heading": "5.6 Online Minimum Cut Problem", "text": "In the online minimum cut problem the learner is given a fixed bipartite graph G = (V,E) with a designated pair of vertices s and t. The ground set here is the set of edges, i.e. U = E, and the decision set D is the set of cuts separating s and t: a cut here is a set of edges that when removed from the graph disconnects s from t. We prove the following hardness result:\nTheorem 7. For any n \u2208 N, there is a hard instance with parameter n of the Online Minimum Cut problem with d = 3n + 2, and hence its sleeping version is as hard as PAC learning DNF expressions.\nProof. For any given positive integer n, consider the graph C(n) shown in Figure 5. It has 3n+ 2 edges that are labeled by the elements of ground set U = \u22c3n i=1{(i, 0), (i, 1), (i, ?)} \u222a {0, 1}, as required. Now note that any cut in this graph has size at least n + 1, so D satisfies the heaviness property. Furthermore, the richness property is clearly satisfied, since for any s \u2208 {0, 1, ?}n\u00d7{0, 1}, the set of edges {(1, s1), (2, s2), . . . , (n, sn), sn+1} is a cut and therefore in D. The result follows by Theorem 1."}, {"heading": "6 Conclusion", "text": "In this paper we established that obtaining an efficient no-regret algorithm for sleeping versions of several natural online combinatorial optimization problems is as hard as efficiently PAC learning\nDNF expressions, a long-standing open problem. Our reduction technique requires only very modest conditions for hard instances of the problem of interest, and in fact is considerably more flexible than the specific form presented in this paper. We believe that almost any natural combinatorial optimization problem that includes instances with exponentially many solutions will be a hard problem in its online sleeping variant. Furthermore, our hardness result is via stochastic i.i.d. availabilities and losses, a rather benign form of adversary. This suggests that obtaining sublinear per-action regret is perhaps a rather hard objective, and motivates a search (a subject of future work) for suitable simplifications of the regret criterion or restrictions on the adversary\u2019s power that would allow efficient algorithms."}], "references": [{"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "Bubeck S\u00e9bastien", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "Prediction, Learning and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Warmuth K. Manfred"], "venue": "In Proceedings of the 29th Annual ACM symposium on Theory of Computing,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Reliable agnostic learning", "author": ["Adam Tauman Kalai", "Varun Kanade", "Yishay Mansour"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "Learning hurdles for sleeping experts", "author": ["Varun Kanade", "Thomas Steinke"], "venue": "ACM Transactions on Computation Theory (TOCT),", "citeRegEx": "Kanade and Steinke.,? \\Q2014\\E", "shortCiteRegEx": "Kanade and Steinke.", "year": 2014}, {"title": "Sleeping experts and bandits with stochastic action availability and adversarial rewards", "author": ["Varun Kanade", "H. Brendan McMahan", "Brent Bryan"], "venue": "In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Kanade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanade et al\\.", "year": 2009}, {"title": "Toward efficient agnostic learning", "author": ["Michael J. Kearns", "Robert E. Schapire", "Linda M. Sellie"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["Robert Kleinberg", "Alexandru Niculescu-Mizil", "Yogeshwer Sharma"], "venue": "Machine learning,", "citeRegEx": "Kleinberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2010}, {"title": "Hedging structured concepts", "author": ["Wouter M. Koolen", "Manfred K. Warmuth", "Jyrki Kivinen"], "venue": "Proceedings of the 23th Conference on Learning Theory (COLT),", "citeRegEx": "Koolen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2010}, {"title": "Online sabotaged shortest path", "author": ["Wouter M. Koolen", "Manfred K. Warmuth", "Dmitry Adamskiy"], "venue": "In Proceedings of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "Koolen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2015}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Information and computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Online combinatorial optimization with stochastic decision sets and adversarial losses", "author": ["Gergely Neu", "Michal Valko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neu and Valko.,? \\Q2014\\E", "shortCiteRegEx": "Neu and Valko.", "year": 2014}, {"title": "Path kernels and multiplicative updates", "author": ["Eiji Takimoto", "Manfred K. Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth.", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 (Koolen et al., 2015).", "startOffset": 130, "endOffset": 151}, {"referenceID": 10, "context": "The design of a computationally efficient algorithm for Online Sabotaged Shortest Path problem was presented as an open problem at COLT 2015 by Koolen et al. (2015). In this paper, we resolve this open problem and prove that Online Sabotaged Shortest Path problem is computationally hard.", "startOffset": 144, "endOffset": 165}, {"referenceID": 12, "context": "1 Related Work The standard problem of online linear optimization with d actions (Experts setting) admits algorithms with O(d) running time per-round and O( \u221a T log d) regret after T rounds (Littlestone and Warmuth, 1994; Freund and Schapire, 1997); the regret is known to be minimax optimal (CesaBianchi and Lugosi, 2006, Chapter 2).", "startOffset": 190, "endOffset": 248}, {"referenceID": 2, "context": "1 Related Work The standard problem of online linear optimization with d actions (Experts setting) admits algorithms with O(d) running time per-round and O( \u221a T log d) regret after T rounds (Littlestone and Warmuth, 1994; Freund and Schapire, 1997); the regret is known to be minimax optimal (CesaBianchi and Lugosi, 2006, Chapter 2).", "startOffset": 190, "endOffset": 248}, {"referenceID": 14, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 4, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 10, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 0, "context": "Despite this, many problems (Online Shortest Path, Online Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, Online Bipartite Matching) admit algorithms with1 poly(d) running time per round and O(poly(d) \u221a T ) regret (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005; Koolen et al., 2010; Audibert et al., 2013).", "startOffset": 252, "endOffset": 349}, {"referenceID": 13, "context": "Policy regret is the total difference between the loss of the algorithm and the loss of the best policy, which maps a set of available actions to one of the available actions (Neu and Valko, 2014).", "startOffset": 175, "endOffset": 196}, {"referenceID": 9, "context": "Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009).", "startOffset": 204, "endOffset": 275}, {"referenceID": 6, "context": "Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009).", "startOffset": 204, "endOffset": 275}, {"referenceID": 7, "context": "Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009).", "startOffset": 204, "endOffset": 275}, {"referenceID": 3, "context": "Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015).", "startOffset": 189, "endOffset": 231}, {"referenceID": 11, "context": "Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015).", "startOffset": 189, "endOffset": 231}, {"referenceID": 3, "context": "First, there exists an algorithm with O(d) running time per round that achieves per-action regret of order O( \u221a T log d) (Freund et al., 1997).", "startOffset": 121, "endOffset": 142}, {"referenceID": 0, "context": ", 2010; Audibert et al., 2013). Common to these six problems is that their corresponding off-line problems have a polynomial-time algorithm. In fact, as shown by Kalai and Vempala (2005), a polynomial-time algorithm for an offline combinatorial problem implies the existence of an algorithm for the corresponding online optimization problem with the same per-round running time and O(poly(d) \u221a T ) regret.", "startOffset": 8, "endOffset": 187}, {"referenceID": 0, "context": ", 2010; Audibert et al., 2013). Common to these six problems is that their corresponding off-line problems have a polynomial-time algorithm. In fact, as shown by Kalai and Vempala (2005), a polynomial-time algorithm for an offline combinatorial problem implies the existence of an algorithm for the corresponding online optimization problem with the same per-round running time and O(poly(d) \u221a T ) regret. In studying online sleeping optimization, three different notions of regret have been used: (a) policy regret, (b) ranking regret, and (c) per-action regret. Policy regret is the total difference between the loss of the algorithm and the loss of the best policy, which maps a set of available actions to one of the available actions (Neu and Valko, 2014). Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009). Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015). Note that policy regret upper bounds ranking regret, and per-action regret is incomparable to either policy or ranking regret. There are several results about the sleeping Experts setting (also known as Specialists setting). First, there exists an algorithm with O(d) running time per round that achieves per-action regret of order O( \u221a T log d) (Freund et al., 1997). Second, Kleinberg et al. (2010) and Kanade and Steinke (2014) show that achieving O(poly(d)T 1\u2212\u03b4) ranking regret is computationally hard, even under the assumption that the action availability and losses are drawn i.", "startOffset": 8, "endOffset": 1673}, {"referenceID": 0, "context": ", 2010; Audibert et al., 2013). Common to these six problems is that their corresponding off-line problems have a polynomial-time algorithm. In fact, as shown by Kalai and Vempala (2005), a polynomial-time algorithm for an offline combinatorial problem implies the existence of an algorithm for the corresponding online optimization problem with the same per-round running time and O(poly(d) \u221a T ) regret. In studying online sleeping optimization, three different notions of regret have been used: (a) policy regret, (b) ranking regret, and (c) per-action regret. Policy regret is the total difference between the loss of the algorithm and the loss of the best policy, which maps a set of available actions to one of the available actions (Neu and Valko, 2014). Ranking regret is the total difference between the loss of the algorithm and the loss of a ranking of actions, which corresponds to a policy that chooses in each round the highest-ranked available action (Kleinberg et al., 2010; Kanade and Steinke, 2014; Kanade et al., 2009). Per-action regret, which we study in this paper, is the difference between the loss of the algorithm and the loss of an action, summed over only the rounds in which the action is available (Freund et al., 1997; Koolen et al., 2015). Note that policy regret upper bounds ranking regret, and per-action regret is incomparable to either policy or ranking regret. There are several results about the sleeping Experts setting (also known as Specialists setting). First, there exists an algorithm with O(d) running time per round that achieves per-action regret of order O( \u221a T log d) (Freund et al., 1997). Second, Kleinberg et al. (2010) and Kanade and Steinke (2014) show that achieving O(poly(d)T 1\u2212\u03b4) ranking regret is computationally hard, even under the assumption that the action availability and losses are drawn i.", "startOffset": 8, "endOffset": 1703}, {"referenceID": 6, "context": "Under these assumptions, for the sleeping Experts setting, Kanade et al. (2009) give an algorithm running in poly(d) time per iteration with policy regret bounded by O( \u221a T log d), and for the general online sleeping combinatorial optimization setting, Neu and Valko (2014) give an algorithm running in poly(d) time per round and with policy regret bounded by O(m \u221a Td log d), where m is an upper bound on the size of each action.", "startOffset": 59, "endOffset": 80}, {"referenceID": 6, "context": "Under these assumptions, for the sleeping Experts setting, Kanade et al. (2009) give an algorithm running in poly(d) time per iteration with policy regret bounded by O( \u221a T log d), and for the general online sleeping combinatorial optimization setting, Neu and Valko (2014) give an algorithm running in poly(d) time per round and with policy regret bounded by O(m \u221a Td log d), where m is an upper bound on the size of each action.", "startOffset": 59, "endOffset": 274}, {"referenceID": 6, "context": "Our reduction technique is closely related to that of Kanade and Steinke (2014), who reduced agnostic learning of disjunctions to ranking regret minimization in the sleeping Experts setting.", "startOffset": 54, "endOffset": 80}, {"referenceID": 6, "context": "By a standard online-to-batch conversion argument (Kanade and Steinke, 2014), online agnostic learning of disjunctions is at least as hard as agnostic improper PAC-learning of disjunctions (Kearns et al.", "startOffset": 50, "endOffset": 76}, {"referenceID": 8, "context": "By a standard online-to-batch conversion argument (Kanade and Steinke, 2014), online agnostic learning of disjunctions is at least as hard as agnostic improper PAC-learning of disjunctions (Kearns et al., 1994), which in turn is at least as hard as PAC-learning of DNF expressions (Kalai et al.", "startOffset": 189, "endOffset": 210}, {"referenceID": 5, "context": ", 1994), which in turn is at least as hard as PAC-learning of DNF expressions (Kalai et al., 2012).", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": "The online-to-batch conversion of online agnostic learning of disjunctions to agnostic improper PAC-learning of disjunctions (Kanade and Steinke, 2014) mentioned above implies that we may assume that the input sequence (xt, yt) for the online problem is drawn i.", "startOffset": 125, "endOffset": 151}, {"referenceID": 10, "context": "The sleeping version of this problem has been called the Online Sabotaged Shortest Path problem by Koolen et al. (2015), who posed the open question of whether it admits an efficient no-regret algorithm.", "startOffset": 99, "endOffset": 120}], "year": 2017, "abstractText": "We show that several online combinatorial optimization problems that admit efficient noregret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of Online Shortest Paths, Online Minimum Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, and Online Bipartite Matching. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 (Koolen et al., 2015).", "creator": "LaTeX with hyperref package"}}}