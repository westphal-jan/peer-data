{"id": "1509.04581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Kernelized Deep Convolutional Neural Network for Describing Complex Images", "abstract": "with the impressive capability to capture visual content, intrinsic convolutional neural networks ( cnn ) have demon - strated promising performance in various vision - based ap - plications, functions as classification, search, but objec - t detection. however, due to the intrinsic structure design of cnn, supporting images with complex content, it achieves lim - ited capability on invariance to translation, rotation, and re - sizing changes, which is strongly emphasized in the s - cenario of content - based image retrieval. in this paper, to reduce this problem, we proposed a new prototype deep convolutional neural network. we first illustrates our motiva - evolution by an experimental study what demonstrate the sensitivi - tolerance of the global geo feature to the basic geometric trans - formations. then, can propose to represent visual content with approximate invariance to the analyzed terrain trans - interfaces from their kernelized perspective. we extract cnn faces above the detected object - like patches and aggregate these patch - level cnn features to form a vectorial repre - sentation with the fisher metric model. the effectiveness of our current algorithm is demonstrated atop image routing application with continuous benchmark datasets.", "histories": [["v1", "Tue, 15 Sep 2015 14:35:11 GMT  (3687kb,D)", "http://arxiv.org/abs/1509.04581v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.IR cs.MM", "authors": ["zhen liu"], "accepted": false, "id": "1509.04581"}, "pdf": {"name": "1509.04581.pdf", "metadata": {"source": "CRF", "title": "Kernelized Deep Convolutional Neural Network for Describing Complex Images", "authors": ["Zhen Liu"], "emails": ["liuzheng@mail.ustc.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "Vectorial image representation is a fundamental problem in computer vision field. In many visual analysis systems, the visual content in an image is usually represented into a fix-sized vector for convenience of the followed processing. In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].\nThe bag-of-visual-words (BoVW) model is one of the famous methods to construct image representation. In the BoVW model, firstly, a set of local invariant visual features are extracted on the detected image patches or the densely sampled grids. Then an image is represented into a visual word histogram based on the quantization results of local features with an off-line trained visual vocabulary. The\nvisual vocabulary is usually trained with the unsupervised clustering algorithm, such as the standard k-means, hierarchical k-means [29], approximate k-means [34]. Usually the quantization is performed by the nearest neighbor or the approximate nearest neighbor method. Namely each local invariant visual feature is quantized to its nearest or approximate nearest visual word in the vocabulary, which is a kind of hard vector quantization. Instead of the hard vector quantization, in [39], Wang et al. proposed a locality linear coding approach to quantize each local visual feature.\nKernel method is another alternative to transform a set of features into a vectorial representation, such as Fisher kernel [32], and democratic kernel [22]. Fisher kernel models the joint probability distribution of the visual features detected in an image. The vectorial representation is constructed based on the derivatives in the parameter space. Besides the quantization results in the BoVW model, Fisher kernel also includes the residual information between the local visual features and their visual words [21]. Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33]. One non probabilistic version of Fisher kernel is carefully investigated in [20, 21], which is named as vector of locally aggregated descriptors (VLAD).\nInstead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12]. With the learned non-linear transformation model, each image can be transformed to a feature vector [23]. With deep nets to learn from large-scale dataset, the CNN model can well discriminate diverse visual content, which is desired in many visual information processing systems. With breakthrough in many computer vision tasks, the CNN model has made a milestone in visual representation and become a new benchmark baseline [36].\nA lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35]. In [15], Goodfellow et al. test the invariance of deep networks with a natural video dataset and find that the \u201cdeep\u201d structure can obtain more invariance than\nar X\niv :1\n50 9.\n04 58\n1v 1\n[ cs\n.C V\n] 1\n5 Se\np 20\n15\nthe \u201cshallow\u201d ones. In [40], Zeiler and Fergus try to understand why deep convolutional neural network works very well. They propose to visualize the patterns activated by the intermediate layers with a deconvolutional network. It is revealed that some complex patterns can be captured by top layers, which is very amazing. In [25], Lenc et al. study the mathematical properties of equivariance, invariance, equivalence of image representations such as SIFT or CNN from the theoretical perspective. In [9], Cimpoi et al. conduct a range of experiments on material and texture attribute recognition and find that CNN can also obtain excellent result on this topic. In [26], Long et al. study the learned correspondence at a fine level of CNN and reveal that good keypoint prediction can be obtained with the learned intermediate CNN features. More specifically, in [35], Razavian et al. demonstrate that local spatial information of image is also conveyed by CNN and this local information can be used to perform facial landmark prediction, semantic segmentation, and object keypoints detection.\nHowever, CNN is suitable to describe these images with a single object localized at the center, namely those roughly aligned images as shown in Fig. 1(a). For a complex image with multiple objects, it is unsuitable to extract a single global CNN feature as shown in Fig. 1(b) because there may exits geometric transformations on these objects. As a more reasonable alternative, we can firstly align the content of the image and then construct the global vectorial representation. Hence, inspired by the invariant representation via pooling local features, in this paper we propose to represent image with local CNN to address the translation and resizing invariance issue and pool the transformed CNN feature to achieve a fix-sized rotation-invariant representation,\nwhich we call the kernelized convolutional neural network (KCNN) in the following. Specifically, we first detect some object-like patches from the given image. Then for each detected object-like patch, we extract CNN feature to describe the object in it. Finally to form a vectorial representation of the whole image, we aggregate these object-level CNN features with kernel function as shown in Fig. 1(c).\nWe organize the rest of the paper as follows. In Section 2, we present some studies on the sensitivity of global CNN feature to three specific transformations. In Section 3, we introduce our algorithm in detail. The experimental results are presented in Section 4. Finally we make conclusions in Section 5."}, {"heading": "2. Sensitivity of Global CNN Feature", "text": "In this section, we study the sensitivity of global CNN feature to geometric transformations, i.e., translation, scaling, and rotation in detail. The study is made on the Holidays [20] dataset which is a benchmark dataset for image search with 1491 high resolution images. We use the Caffe-based CNN implementation [23] to extract our CNN feature. In the following, given an image I , we use f(\u00b7) to denote its extracted CNN feature in \u201dfc7\u201d layer and use m(\u00b7) to denote the cosine similarity between two CNN features. All CNN feature are L2-normalized in default. To reveal the impact of geometric transformations to the global CNN feature independently, we design the following experiments to make sure that each image undergoes only one kind of geometric transformations.\nTranslation. Generally, a translation can be made in vertical and horizontal directions. To simplify the study, we\nconsider only the translation in the horizontal direction as shown in Fig. 2. The extension to the general translation is straightforward. Given an image I with size M by N , we generate a larger image with size M \u00d7 2N , as shown in Fig. 2(a) and pad the left half part with image I by the border extrapolation method. Then we circularly translate I by t pixels to the left and construct its transformed version I(t) and extract the global CNN feature f(I(t)). We measure the consistency score between global CNN features of I(t = 0) and I(t) with their cosine similarity, as shown by the following equation.\nm(I(t)) =< f(I(t = 0)), f(I(t)) > (1)\nin which < \u00b7, \u00b7 > means the inner product operation. In Fig. 2(b), we illustrate two examples of the similarity between the global CNN features before and after the translation transformation. It can be seen that with the increase of horizontal translation, the similarity first declines and then grows after it reaches a valley. The decrease in similarity reflects the fact that the global CNN feature is sensitive to the translation transformation. On the other hand, the increase of the similarity after the valley point demonstrates the effect of the flipping operation which is make during the training stage of the CNN model. Similar phenomenon is also demonstrated by the statistical results shown in Fig. 2(c). The difference in the trends of the similarity curves reflects the tolerance capability of global CNN feature to the translation transformation is also related\nto the content of image.\nScaling. In Fig. 3, we show our experiment to study the scaling property of the global CNN feature. The similarity to measure the image scaling transformation is defined as\nm(I(s)) =< f(I(s = 1)), f(I(s)) >, (2)\nwhere I(s) denotes the new image re-sized from the original image I with the width and height being s times of I . To keep the image I(s) in the same size, we pad the region beyond the image boundary by the border extrapolation method. Another choice is to crop the sub-images different size at the same location. However, there should not be substantial difference between these two methods to construct I(s). Then we extract the global CNN feature f(I(s)).\nIn Fig. 3(b), we illustrate two examples of the similarity of the global CNN features before and after the scaling transformation. It can be seen that the similarity score decreases as the image is scaled with different ratios, which means the global CNN feature is not invariance to the scaling transformation. Similar phenomenon is also demonstrated by the statistical results shown in Fig. 3(c).\nRotation. In Fig. 4, we show our experiment to study the rotation property of the global CNN feature. We measure the consistency score of CNN feature to rotation transfor-\nmation as\nm(I(\u03b8)) =< f(I(\u03b8 = 0\u25e6)), f(I(\u03b8)) >, (3)\nwhere I(\u03b8) denotes the new image after the image I is rotated by \u03b8 degree, as shown in Fig. 4(a). Please note that the image size will change after rotation as shown by comparing the figure of \u03b8 = 0\u25e6and the figure of \u03b8 = 45\u25e6in Fig. 4(a). To study the property of global CNN feature when only rotation transformation exists, we extract the CNN feature on the sub-image located at the center of I(s) as illustrated in the blue square of the red inscribed circle in Fig. 4(a).\nIn Fig. 4(b), we illustrate two examples of the similarity of the global CNN features before and after the rotation transformation. It can be seen that the similarity varies as the image is rotated with different degrees and the similarity curves of these two examples have different trends. Similar phenomenon is also demonstrated by the statistical results shown in Fig. 4(c), which demonstrated that the global CNN feature is sensitive to the rotation transformation. That the similarity curves have different trends means the tolerance ability to the rotation transformation of global CNN feature is also related to the content of image.\nDiscussion. From the experiments above, it can be observed that the similarity m of the global CNN features before and after transformation is sensitive to translation,\nrotation, and scaling. This comes from the architecture of the CNN model in which the neurons are highly related to the spatial positions of the image pixels in local perception field. When the image is transformed, the spatial positions of those pixels are changed, which results in the inconsistent CNN feature and limits the robustness of CNN feature to these geometric transformations such as translation, scaling, and rotation. To address this problem we propose to firstly align the image content in the patch level before extracting the CNN feature. Such a strategy makes the feature robust to translation and scaling change. Moreover, to enhance the robustness to rotation changes, each image patch is rotated circularly by 8 times. Then to build a vectorial image level representation, we aggregate the extracted patchlevel CNN features with kernel functions."}, {"heading": "3. Kernelized Convolutional Neural Network", "text": "In this section, we introduce our algorithm to construct the vectorial representations on the roughly content-aligned images with the kernel method and the deep convolutional neural network in detail.\nGiven two sets of image patches X and Y with card(X ) = n and card(Y) = m. Let\u2019s consider using match kernel K(\u00b7, \u00b7) [17, 7, 22] to measure the similarity between X and Y , hence we have\nK(X ,Y) = \u2211 x\u2208X \u2211 y\u2208Y k(x, y), (4)\nwhere k(\u00b7, \u00b7) measures the similarity between two feature descriptors and x stands for an image patch and y has the similar meaning.\nTo construct a vectorial image representation for each image, we consider these separable kernel functions. Namely the similarity between two feature descriptors k(x, y) can be computed by the inner product operation, as\nshown by the following equation\nK(X ,Y) = \u2211 x\u2208X \u2211 y\u2208Y k(x, y)\n= \u2211 x\u2208X \u2211 y\u2208Y < \u03c6(x), \u03c6(y) >\n= < ( \u2211 x\u2208X \u03c6(x)), ( \u2211 y\u2208Y \u03c6(y)) >\n= < \u03a8(X ),\u03a8(Y) >,\n(5)\nwhere \u03c6(\u00b7) means a kind of linear or nonlinear transformation and \u03a8(X ) is the final image-level vectorial representation we need.\nIn Eq. 5, the key issue is how to define the function \u03c6(\u00b7). Firstly, as the size of x is not fixed, we need a function to transform x into a fixed dimensional vectorial representation, which can be denoted by \u03b3(\u00b7). Secondly, to aggregate these patch-level vectorial representations \u03b3(x) into the final image-level vectorial representation \u03a8(X ), we need a function to map \u03b3(x) into another space. This step can be denoted by \u03b2(\u00b7). Such that we have the form\n\u03c6(x) = \u03b2(\u03b3(x)). (6)\nIn the following, we will discuss how to design the function \u03b3(\u00b7) and \u03b2(\u00b7).\n\u03b3(\u00b7): In computer vision, it is a fundamental problem to describe an image patch of various sizes into a fixedlength feature vector. There are many classic works on it [27, 10, 5]. For example, in SIFT [27] algorithm, the spatially constrained gradient histogram is used to represent the image patch. With the development of the technology, some researchers turn to the large-scale machine learning techniques. The recently research works revealed that the deep convolutional neural network (CNN) is very powerful for many computer vision tasks [36]. The CNN model is learned from a million-scale database, ImageNet. With the advantage of the non-linearity and large number of parameters, CNN can easily handle the immense variants of vision tasks. In this paper, we adopt the CNN model [23] to transform the image patch into its vectorial representation. In [23], a pre-trained CNN model and well organized code are provided to be publicly available for academic uses. We adopt the CNN model to obtain the vectorial representation of each image patch.\n\u03b2(\u00b7): After the image patches are transformed into vectorial representations, we adopt the separable kernel methods to aggregate them together to represent the image. There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32]. One classic separable kernel is the Fisher kernel which models the joint probability\ndistribution of a set of features [33, 32, 31, 20]. Perronnin et.al. [31, 33] applied Fisher kernel to image classification and image retrieval applications. They model the features\u2019 joint probability distribution with a Gaussian mixture (GMM) model. In Fisher kernel, the mapping function \u03b2(\u00b7) corresponds to the gradient function of the features\u2019 joint probability distribution with respect to the parameters of this distribution, scaled by the inverse square root of the Fisher information matrix. It gives the direction in parameter space into which the learned distribution should be modified to better fit the observed data. In comparison with the BoVW model, the Fisher kernel model can obtain higher accuracy. Hence given a set of features, we adopt the Fisher kernel to construct their vectorial representation.\nx. To analyze the visual content in a given image, researchers usually extract some interesting patches from it. The word \u201cinteresting\u201d means some clearly defined rules, which can make the detected patches have the desired properties. For example, in SIFT algorithm [27], the image patches are detected with different of Gaussian (DoG) method to obtain the scale invariant property. Then to obtain the rotation invariant property, the detected patches are aligned with the dominant orientation of its gradients. In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image. After some object-like patches are detected, these patches are spatially aligned, which can provide the property of invariance to translation and scaling transformations. In a most recently published work named BING object detector [8], Cheng et al. proposed a very efficient algorithm to detect objectlike image patches with a quite higher detection rate, which can process 300 frames per second on a single CPU. BING object detection algorithm [8] output a real value for each patch to indicate how the detected image patch is like to be an object. With this real value, we can control the number of image patches we want. Considering the excellent speed of BING algorithm, we adopt it [8] to extract our image patches. To achieve the rotation invariant property of the extracted image patches, we rotate each image patch, x, by 8 discrete degrees which consist of 0\u25e6, 45\u25e6, 90\u25e6, 135\u25e6, 180\u25e6, 225\u25e6, 270\u25e6, 315\u25e6. Intuitively, a dominant angle for each object patch can be estimated in the similar way as SIFT. However, our study reveals that such a strategy yields low performance, due to the unreliability of the dominant angle estimation in object-patch level. Some examples of detected object-like patches with BING algorithm are shown in Fig. 5.\nTime cost. Besides the time cost to extract object-like patches with BING detector and the aggregation cost with Fisher kernel, the time to extract KCNN will be 8\u00d7N times of regular CNN, where N means the number of detected objects. But, this can be accelerated with GPU clusters.\nSince our paper focus on addressing the sensitivity of regular CNN, in our implementation we use the CPU mode of Caffe library. To fairly show the effectiveness of KCNN, we use the linear search method to search the database with the inner product operation to compute the similarity of two images. Therefore the complexity will depend on the dimension of image vectorial representation."}, {"heading": "4. Experimental Results", "text": "In this section, we evaluate our algorithm on the image retrieval application. We adopt three public available benchmark datasets, i.e, Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the impact of the parameters in our algorithm. We also compare our algorithm with some other methods for image retrieval application.\nHolidays dataset [20] contains 1491 high-resolution images of different scenes and objects with 500 queries. To evaluate the performance we use the average precision measure computed as the area under the precision-recall curve for a query. We compute the mean of the average precision for all queries to obtain a mean Average Precision (mAP) score, which is used to evaluate the overall performance [34].\nUKBench dataset [29] contains 2550 objects or scenes, each with four images taken under different views or imaging conditions, resulting in 10200 images in total. In terms of accuracy measurement, the top-4 accuracy [29] is used as evaluation metric. For top-4 accuracy, for each query, the retrieval accuracy is measured by counting the number of correct images in top-4 returned results. Then the retrieval performance is averaged over all test queries.\nOxford Building dataset [3, 34] consists of 5062 images of buildings and 55 query images corresponding to 11 distinct buildings in Oxford. Images are annotated as either relevant, not relevant, or junk indicating that it is unclear whether a user would consider the image as relevant or not. Following the recommended protocol, the junk images are removed from the ranking results. The retrieval performance is also measured by the mean Average Precision (mAP) computed over the 55 queries.\nOur experiments are implemented on a server with 32GB memory and 2.4GHz CPU of Intel Xeon."}, {"heading": "4.1. Impact of Parameters", "text": "In this section, we study the impact of parameters. There are three parameters in our algorithm. The first one is the number of image patches x detected by BING detector [8], which can be denoted by N . The second one is the dimension of vectorial representation of image patch, \u03b3(x). We adopt the CNN model to construct the vectorial representation of x resulting in a 4096-D \u03b3(x) [23]. For convenience\nwithout loss of generality, we perform the principle components analysis (PCA) to reduce the 4096-D \u03b3(x) to D dimension. The last parameter is the visual vocabulary size used in Fisher vector [32] corresponding to the \u03b2(\u00b7) in Eq. 6, which can be denoted by V .\nThe results are demonstrated in Fig. 6. It can be seen that better accuracy can be obtained when more patches (larger N ) are used. Similarly with larger D and V , we can obtain higher accuracy. However the impacts of D and V are minor than N . In Table 1, we demonstrate the performance of the proposed KCNN algorithm on Holidays, UKBench, and Oxford Building datasets. We can see that it is benefical to perform rotation operation to image patch x for Holidays and UKBench dataset. Especially for the UKBench dataset, the accuracy for the CNN feature is 3.41 and is improved to 3.51 (+2.9%) with our KCNN algorithm without rotating x. After performing the rotation to x, the accuracy is improved\nfrom 3.51 to 3.74 (+6.6%). This is because there are many rotation transformations in UKBench dataset, as shown in Fig. 7(a). However, the rotation operation to image patch x is harmful on Oxford Building dataset for our KCNN algorithm. Similar result has also been observed when SIFT features are used to perform retrieval on this dataset [30] [21]. That is, in the construction of SIFT descriptor, better retrieval performance is obtained with the orientation selected as the gravity orientation instead of the traditional dominant gradient orientation [27] [21], since there is very few rotation transformations for the building images, as demonstrated in Fig. 7(b).\nTo further demonstrate the performance of the proposed kernelized convolutional neural network (KCNN) algorithm, we show the Average Precision (AP) of each query of Oxford Building dataset in Table 2. It can be seen that the proposed KCNN algorithm can get better retrieval application than the original convolutional neural network (CNN) algorithm for most queries. There are 38 queries out of total 55 queries (69.1%) whose retrieval performance have been improved. The highest improvement comes from the query \u201cashmolean 2\u201d whose retrieval performance is improved by 355.3% from 0.0987 to 0.4495. Some examples on Holidays dataset are shown in Fig. 8, in which we give their rank number with CNN representation and our KCNN representation. From Fig. 8(a) and Fig. 8(b), it can be seen that our KCNN well addresses the sensitivity of rotation transformation which may fail the global CNN feature. From the first result of Fig. 8(a) and the second result of Fig. 8(c), it can be seen that global CNN can also tolerate the slightly scaling transformation while our KCNN can do much better as shown in the first result of Fig. 8(c)."}, {"heading": "4.2. Comparisons", "text": "In this section, we give some comparisons with the results reported in other research works. As shown in Table 3, it can be seen that the proposed KCNN method obtains best result on both Holidays and UKBench datasets. However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22]. The reason is that the Oxford Building dataset consists of building images and the retrieval on this dataset is more like a fine-grained problem [28]. On the other hand deep convolutional neu-\nral network is designed to tackle the generic classification problem [11, 24] and fine-tune is usually required for the fine-grained vision tasks.\nThere also exists some works on performing image search with CNN. However our work has substantial difference with them. Comparing with [36], our goal is totally different. Our goal is to construct a vectorial representation for an image while [36] use the Spatial Search that is not a vectorial image representation. The spatial search means extensively search all the sub-patches extracted on the grids at several levels. The search complexity will be O(N2) whereN means the number of extracted sub-patches. Comparing with [14], on Holidays, we get 0.829 mAP while [14] gets 0.802 mAP. Besides the higher accuracy, we also address the rotation transformation while [14] not. Comparing with [4], they focus on construct compressed codes of image representation with the retrained regular CNN while we focus on addressing the object transformations in the vectorial representation of complex images without retraining."}, {"heading": "5. Conclusion", "text": "In this paper, we have analyzed the sensitivity of the global CNN feature to the geometric transformations of image such as translation, scaling, and rotation. Based on our analysis, inspired by the well-studied local feature based\nimage representation methods, we proposed our kernelized convolutional network (KCNN) algorithm to describe the content of complex images. With our KCNN method, we can obtain a more robust vectorial representation. Besides the CNN structure implemented in Caffe library, there are also some other emerging CNN structures [37, 16]. In the future, we would like to investigate the potential of these different CNN models on image retrieval and investigate the performance of our KCNN model integrated with these CNN models."}], "references": [{"title": "What is an object", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "In Proceedings of IEEE International Conference on Computer Vision and Patteren Recognation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Measuring the objectness of image windows", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "Proceedings of IEEE Trans-  actions on Pattern Analysis and Machine Intelligence, volume 34, pages 2189\u20132202,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "All about vlad", "author": ["R. Arandjelovic", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky"], "venue": "Proceedings of European Conference on Computer Vision, pages 584\u2013599.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Surf: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": "Proceedings of the European Conference on Computer Vision, pages 404\u2013417,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Object recognition with hierarchical kernel descriptors", "author": ["L. Bo", "K. Lai", "X. Ren", "D. Fox"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition, June", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient Match Kernel between Sets of Features for Visual Recognition", "author": ["L. Bo", "C. Sminchisescu"], "venue": "Proceedings of Advances in Neural Information Processing Systems, December", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "BING: Binarized normed gradients for objectness estimation at 300fps", "author": ["M.-M. Cheng", "Z. Zhang", "W.-Y. Lin", "P.H.S. Torr"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3828\u20133836,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pages 886\u2013893,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Category independent object proposals", "author": ["I. Endres", "D. Hoiem"], "venue": "Proceedings of European Conference on Computer Vision, pages 575\u2013588,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "Proceedings of European Conference on Computer Vision, pages 392\u2013407.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "H. Lee", "Q.V. Le", "A. Saxe", "A.Y. Ng"], "venue": "Proceedings of Advances in neural information processing systems, pages 646\u2013654,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Densenet: Implementing efficient convnet descriptor pyramids", "author": ["F. Iandola", "M. Moskewicz", "S. Karayev", "R. Girshick", "T. Darrell", "K. Keutzer"], "venue": "arXiv preprint arXiv:1404.1869,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Negative evidences and cooccurences in image retrieval: The benefit of pca and whitening", "author": ["H. J\u00e9gou", "O. Chum"], "venue": "Proceedings of the European Conference on Computer Vision, pages 774\u2013787.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Hamming embedding and weak geometric consistency for large scale image search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "Proceedings of the European Conference on Computer Vision, pages 304\u2013317,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3304\u20133311,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Aggregating local image descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Triangulation embedding and democratic aggregation for image search", "author": ["H. J\u00e9gou", "A. Zisserman"], "venue": "CVPR - International Conference on Computer Vision and Pattern Recognition, Columbus, United States, June", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["K. Lenc", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1411.5908,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Do convnets learn correspondence", "author": ["J.L. Long", "N. Zhang", "T. Darrell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, 60(2):91\u2013110,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Automated flower classification over a large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Computer Vision, Graphics & Image Processing, 2008. ICVGIP\u201908. Sixth Indian Conference on, pages 722\u2013729,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable recognition with a vocabulary tree", "author": ["D. Nister", "H. Stewenius"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2161\u20132168,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient representation of local geometry for large scale object retrieval", "author": ["M. Perd\u2019och", "O. Chum", "J. Matas"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale image retrieval with compressed fisher vectors", "author": ["F. Perronnin", "Y. Liu", "J. S\u00e1nchez", "H. Poirier"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3384\u20133391,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "Proceedings of European Conference on Computer Vision, pages 143\u2013156.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Persistent evidence of local image properties in generic convnets", "author": ["A.S. Razavian", "H. Azizpour", "A. Maki", "J. Sullivan", "C.H. Ek", "S. Carlsson"], "venue": "Image Analysis, pages 249\u2013262. Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 512\u2013519,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1470\u20131477,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3360\u20133367,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Proceedings of European Conference on Computer Vision, pages 818\u2013833.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 95, "endOffset": 106}, {"referenceID": 9, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 95, "endOffset": 106}, {"referenceID": 4, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 95, "endOffset": 106}, {"referenceID": 37, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 38, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 31, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 19, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 21, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 28, "context": "The visual vocabulary is usually trained with the unsupervised clustering algorithm, such as the standard k-means, hierarchical k-means [29], approximate k-means [34].", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "The visual vocabulary is usually trained with the unsupervised clustering algorithm, such as the standard k-means, hierarchical k-means [29], approximate k-means [34].", "startOffset": 162, "endOffset": 166}, {"referenceID": 38, "context": "Instead of the hard vector quantization, in [39], Wang et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 31, "context": "Kernel method is another alternative to transform a set of features into a vectorial representation, such as Fisher kernel [32], and democratic kernel [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "Kernel method is another alternative to transform a set of features into a vectorial representation, such as Fisher kernel [32], and democratic kernel [22].", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "Besides the quantization results in the BoVW model, Fisher kernel also includes the residual information between the local visual features and their visual words [21].", "startOffset": 162, "endOffset": 166}, {"referenceID": 31, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 21, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 19, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 17, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 32, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 19, "context": "One non probabilistic version of Fisher kernel is carefully investigated in [20, 21], which is named as vector of locally aggregated descriptors (VLAD).", "startOffset": 76, "endOffset": 84}, {"referenceID": 20, "context": "One non probabilistic version of Fisher kernel is carefully investigated in [20, 21], which is named as vector of locally aggregated descriptors (VLAD).", "startOffset": 76, "endOffset": 84}, {"referenceID": 26, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 247, "endOffset": 251}, {"referenceID": 22, "context": "With the learned non-linear transformation model, each image can be transformed to a feature vector [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 35, "context": "With breakthrough in many computer vision tasks, the CNN model has made a milestone in visual representation and become a new benchmark baseline [36].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 39, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 24, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 8, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 25, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 34, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 14, "context": "In [15], Goodfellow et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "There should not be substantial differences from the original CNN model in [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 39, "context": "In [40], Zeiler and Fergus try to understand why deep convolutional neural network works very well.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [25], Lenc et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [9], Cimpoi et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "In [26], Long et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "More specifically, in [35], Razavian et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "The study is made on the Holidays [20] dataset which is a benchmark dataset for image search with 1491 high resolution images.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "We use the Caffe-based CNN implementation [23] to extract our CNN feature.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "Let\u2019s consider using match kernel K(\u00b7, \u00b7) [17, 7, 22] to measure the similarity between X and Y , hence we have", "startOffset": 42, "endOffset": 53}, {"referenceID": 6, "context": "Let\u2019s consider using match kernel K(\u00b7, \u00b7) [17, 7, 22] to measure the similarity between X and Y , hence we have", "startOffset": 42, "endOffset": 53}, {"referenceID": 21, "context": "Let\u2019s consider using match kernel K(\u00b7, \u00b7) [17, 7, 22] to measure the similarity between X and Y , hence we have", "startOffset": 42, "endOffset": 53}, {"referenceID": 26, "context": "There are many classic works on it [27, 10, 5].", "startOffset": 35, "endOffset": 46}, {"referenceID": 9, "context": "There are many classic works on it [27, 10, 5].", "startOffset": 35, "endOffset": 46}, {"referenceID": 4, "context": "There are many classic works on it [27, 10, 5].", "startOffset": 35, "endOffset": 46}, {"referenceID": 26, "context": "For example, in SIFT [27] algorithm, the spatially constrained gradient histogram is used to represent the image patch.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "The recently research works revealed that the deep convolutional neural network (CNN) is very powerful for many computer vision tasks [36].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "In this paper, we adopt the CNN model [23] to transform the image patch into its vectorial representation.", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "In [23], a pre-trained CNN model and well organized code are provided to be publicly available for academic uses.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 6, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 21, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 32, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 5, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 31, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 32, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 31, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 30, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 19, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 30, "context": "[31, 33] applied Fisher kernel to image classification and image retrieval applications.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[31, 33] applied Fisher kernel to image classification and image retrieval applications.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "For example, in SIFT algorithm [27], the image patches are detected with different of Gaussian (DoG) method to obtain the scale invariant property.", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 1, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 0, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 12, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 7, "context": "In a most recently published work named BING object detector [8], Cheng et al.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "BING object detection algorithm [8] output a real value for each patch to indicate how the detected image patch is like to be an object.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "Considering the excellent speed of BING algorithm, we adopt it [8] to extract our image patches.", "startOffset": 63, "endOffset": 66}, {"referenceID": 19, "context": "e, Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the impact of the parameters in our algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "e, Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the impact of the parameters in our algorithm.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "e, Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the impact of the parameters in our algorithm.", "startOffset": 54, "endOffset": 57}, {"referenceID": 19, "context": "Holidays dataset [20] contains 1491 high-resolution images of different scenes and objects with 500 queries.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "We compute the mean of the average precision for all queries to obtain a mean Average Precision (mAP) score, which is used to evaluate the overall performance [34].", "startOffset": 159, "endOffset": 163}, {"referenceID": 28, "context": "UKBench dataset [29] contains 2550 objects or scenes, each with four images taken under different views or imaging conditions, resulting in 10200 images in total.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "In terms of accuracy measurement, the top-4 accuracy [29] is used as evaluation metric.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Oxford Building dataset [3, 34] consists of 5062 images of buildings and 55 query images corresponding to 11 distinct buildings in Oxford.", "startOffset": 24, "endOffset": 31}, {"referenceID": 33, "context": "Oxford Building dataset [3, 34] consists of 5062 images of buildings and 55 query images corresponding to 11 distinct buildings in Oxford.", "startOffset": 24, "endOffset": 31}, {"referenceID": 7, "context": "The first one is the number of image patches x detected by BING detector [8], which can be denoted by N .", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "We adopt the CNN model to construct the vectorial representation of x resulting in a 4096-D \u03b3(x) [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "N is the number of object detected with BING detector [8].", "startOffset": 54, "endOffset": 57}, {"referenceID": 31, "context": "V is the number of Gaussian functions used in Fisher vector model [32].", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "D is the dimension of the CNN features [23] after performing the PCA dimension reduction.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "The performance of the proposed kernelized convolutional neural network (KCNN) algorithm on three benchmark datasets, namely Holidays [19], Oxford Building [34], and UKBench [29].", "startOffset": 134, "endOffset": 138}, {"referenceID": 33, "context": "The performance of the proposed kernelized convolutional neural network (KCNN) algorithm on three benchmark datasets, namely Holidays [19], Oxford Building [34], and UKBench [29].", "startOffset": 156, "endOffset": 160}, {"referenceID": 28, "context": "The performance of the proposed kernelized convolutional neural network (KCNN) algorithm on three benchmark datasets, namely Holidays [19], Oxford Building [34], and UKBench [29].", "startOffset": 174, "endOffset": 178}, {"referenceID": 31, "context": "The last parameter is the visual vocabulary size used in Fisher vector [32] corresponding to the \u03b2(\u00b7) in Eq.", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "Similar result has also been observed when SIFT features are used to perform retrieval on this dataset [30] [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Similar result has also been observed when SIFT features are used to perform retrieval on this dataset [30] [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 26, "context": "That is, in the construction of SIFT descriptor, better retrieval performance is obtained with the orientation selected as the gravity orientation instead of the traditional dominant gradient orientation [27] [21], since there is very few rotation transformations for the building images, as demonstrated in Fig.", "startOffset": 204, "endOffset": 208}, {"referenceID": 20, "context": "That is, in the construction of SIFT descriptor, better retrieval performance is obtained with the orientation selected as the gravity orientation instead of the traditional dominant gradient orientation [27] [21], since there is very few rotation transformations for the building images, as demonstrated in Fig.", "startOffset": 209, "endOffset": 213}, {"referenceID": 26, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 93, "endOffset": 96}, {"referenceID": 21, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "The reason is that the Oxford Building dataset consists of building images and the retrieval on this dataset is more like a fine-grained problem [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "ral network is designed to tackle the generic classification problem [11, 24] and fine-tune is usually required for the fine-grained vision tasks.", "startOffset": 69, "endOffset": 77}, {"referenceID": 23, "context": "ral network is designed to tackle the generic classification problem [11, 24] and fine-tune is usually required for the fine-grained vision tasks.", "startOffset": 69, "endOffset": 77}, {"referenceID": 35, "context": "Comparing with [36], our goal is totally different.", "startOffset": 15, "endOffset": 19}, {"referenceID": 35, "context": "Our goal is to construct a vectorial representation for an image while [36] use the Spatial Search that is not a vectorial image representation.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "Comparing with [14], on Holidays, we get 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "829 mAP while [14] gets 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Besides the higher accuracy, we also address the rotation transformation while [14] not.", "startOffset": 79, "endOffset": 83}, {"referenceID": 3, "context": "Comparing with [4], they focus on construct compressed codes of image representation with the retrained regular CNN while we focus on addressing the object transformations in the vectorial representation of complex images without retraining.", "startOffset": 15, "endOffset": 18}, {"referenceID": 31, "context": "Dataset [32] [3] [22] CNN KCNN", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "Dataset [32] [3] [22] CNN KCNN", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "Dataset [32] [3] [22] CNN KCNN", "startOffset": 17, "endOffset": 21}, {"referenceID": 36, "context": "Besides the CNN structure implemented in Caffe library, there are also some other emerging CNN structures [37, 16].", "startOffset": 106, "endOffset": 114}, {"referenceID": 15, "context": "Besides the CNN structure implemented in Caffe library, there are also some other emerging CNN structures [37, 16].", "startOffset": 106, "endOffset": 114}], "year": 2015, "abstractText": "With the impressive capability to capture visual content, deep convolutional neural networks (CNN) have demonstrated promising performance in various vision-based applications, such as classification, recognition, and object detection. However, due to the intrinsic structure design of CNN, for images with complex content, it achieves limited capability on invariance to translation, rotation, and re-sizing changes, which is strongly emphasized in the scenario of content-based image retrieval. In this paper, to address this problem, we proposed a new kernelized deep convolutional neural network. We first discuss our motivation by an experimental study to demonstrate the sensitivity of the global CNN feature to the basic geometric transformations. Then, we propose to represent visual content with approximate invariance to the above geometric transformations from a kernelized perspective. We extract CNN features on the detected object-like patches and aggregate these patch-level CNN features to form a vectorial representation with the Fisher vector model. The effectiveness of our proposed algorithm is demonstrated on image search application with three benchmark datasets.", "creator": "LaTeX with hyperref package"}}}