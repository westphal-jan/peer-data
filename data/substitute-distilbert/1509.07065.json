{"id": "1509.07065", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2015", "title": "A Novel Pre-processing Scheme to Improve the Prediction of Sand Fraction from Seismic Attributes using Neural Networks", "abstract": "this paper presents also novel pre - processing scheme to improve the prediction of sand fraction from multiple correlated attributes such as seismic impedance, amplitude and frequency information machine learning and information filtering. the available well logs along as compressed 3 - d dynamics data have been used jointly benchmark the proposed pre - processing stage using a methodology which subsequently consists of three steps : pre - processing, training and post - assessment. an artificial neural network ( nl ) with conjugate - gradient training infrastructure has been used at model the sand fraction. the available sand fraction data from the high resolution well logs has far more information content than the low resolution seismic attributes. commercially, regularization schemes based on fourier transform ( ft ), wavelet decomposition ( wd ) and empirical mode decomposition ( emd ) have been proposed to shape the high resolution dirt fraction material for effective machine learning. the input data fragments have been segregated into training, testing trigger validation sets. the test results are primarily used to check different source specification and activation validation performances. once the network passes the testing phase with an acceptable performance in terms of the selected evaluators, the validation phase holds. in the validation stage, the prediction model is tested against unseen data. the network yielding satisfactory performance in the validation stage is used to predict lithological properties from seismic attributes containing such given volume. finally, a parallel - processing scheme carrying 3 - d elastic filtering is implemented providing smoothing whole sand fraction in different volume. prediction of lithological properties using this framework is helpful for reservoir characterization.", "histories": [["v1", "Wed, 23 Sep 2015 17:15:21 GMT  (2498kb)", "http://arxiv.org/abs/1509.07065v1", "13 pages, volume 8, no 4, pp. 1808-1820, April 2015 in IEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2015"]], "COMMENTS": "13 pages, volume 8, no 4, pp. 1808-1820, April 2015 in IEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2015", "reviews": [], "SUBJECTS": "cs.CE cs.LG", "authors": ["soumi chaki", "aurobinda routray", "william k mohanty"], "accepted": false, "id": "1509.07065"}, "pdf": {"name": "1509.07065.pdf", "metadata": {"source": "CRF", "title": "enriched\u2013formations by characterization of each layer in the borehole is of enormous importance to the explorers", "authors": ["Soumi Chaki"], "emails": ["soumibesu2008@gmail.com,", "aroutray@ee.iitkgp.ernet.in)."], "sections": [{"heading": null, "text": "> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n1\n Abstract\u2014 This paper presents a novel pre-processing scheme to improve the prediction of sand fraction from multiple seismic attributes such as seismic impedance, amplitude and frequency using machine learning and information filtering. The available well logs along with the 3-D seismic data have been used to benchmark the proposed pre-processing stage using a methodology which primarily consists of three steps: preprocessing, training and post-processing. An Artificial Neural Network (ANN) with conjugate-gradient learning algorithm has been used to model the sand fraction. The available sand fraction data from the high resolution well logs has far more information content than the low resolution seismic attributes. Therefore, regularization schemes based on Fourier Transform (FT), Wavelet Decomposition (WD) and Empirical Mode Decomposition (EMD) have been proposed to shape the high resolution sand fraction data for effective machine learning. The input data sets have been segregated into training, testing and validation sets. The test results are primarily used to check different network structures and activation function performances. Once the network passes the testing phase with an acceptable performance in terms of the selected evaluators, the validation phase follows. In the validation stage, the prediction model is tested against unseen data. The network yielding satisfactory performance in the validation stage is used to predict lithological properties from seismic attributes throughout a given volume. Finally, a post-processing scheme using 3-D spatial filtering is implemented for smoothing the sand fraction in the volume. Prediction of lithological properties using this framework is helpful for Reservoir Characterization.\nIndex Terms\u2014 Artificial Neural Network (ANN), wavelets, Empirical Mode Decomposition (EMD), entropy, Fourier Transform, Normalized Mutual Information (NMI), preprocessing, regularization, Reservoir Characterization (RC), sand fraction, 3-D median filtering.\nI. INTRODUCTION YDROCARBONS migrate from source rock through porous medium to reach reservoir rock for temporary\npreservation [1]. Finally, the mobile hydrocarbons get seized in the cap rocks. As such, the identification of hydrocarbon\u2013 enriched\u2013formations by characterization of each layer in the borehole is of enormous importance to the explorers.\nSoumi Chaki and Aurobinda Routray are with the Department of Electrical Engineering, Indian Institute of Technology Kharagpur, Kharagpur- 721302, India (e-mail: soumibesu2008@gmail.com, aroutray@ee.iitkgp.ernet.in).\nRecognition of potential hydrocarbon\u2013enriched zones in a prospective oil exploration field can be carried out using well logs which can categorize layers into different sections such as dry, water containing, and hydrocarbon bearing layers. The lithological properties in the neighborhood of a borehole can be estimated from well logs, whereas their distributions become difficult to predict away from the wells. In such cases, available seismic attributes can be used as a guidance to predict lithological information at all traces of the area of interest [2]. Well logs and seismic attributes are integrated at available well locations to design a reservoir model with the least uncertainty. However, mapping between well logs and seismic attributes is governed by nonlinear relationship and characterized by mismatch in information content. Such nonlinear problems can be approached using state-of-art computer\u2013based methods like hybrid systems [3], [4], multiple regression, neural networks [5], Neuro-fuzzy Systems [6] etc. It has been found from literature that ANN has been widely used by researchers and engineers from diverse backgrounds to model single or multiple target properties from predictor variables in different research problems because of its accurate prediction and generalization capability. For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc. A diverse dataset containing information assembled from multiple domains can be used for learning and validation of ANN. However, it has some inherent limitations. Firstly, performance of ANN is dependent on the selection of network structure and associated parameters. Secondly, training a complex multilayered network is a time intensive process. Furthermore, a complex network trained with relatively smaller number of learning patterns may lead to over fitting. It is equally important to assess the possibility of modelling the target property from predictor variables using ANN or any other nonlinear modelling approach. Sometimes the model performance can be improved by applying suitable filtering techniques to the predictor/target variables in the pre-processing stage. Several studies have contributed to the performance analysis of ANN along with other machine learning algorithms to model a target variable from single or multiple predictors with respect to RC\nWilliam K. Mohanty is with the Department of Geology and Geoscience, Indian Institute of Technology Kharagpur, Kharagpur- 721302, India (e-mail: wkmohanty@gg.iitkgp.ernet.in).\nA Novel Pre-processing Scheme to Improve the Prediction of Sand Fraction from Seismic\nAttributes using Neural Networks Soumi Chaki, Student Member, IEEE, Aurobinda Routray, Member, IEEE, William K. Mohanty\nH\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n2\nproblem; however, the following aspects still remain unexplored:  Design of an appropriate pre-processing stage for effectiveness of machine learning algorithms  Proper choice of structure and parameters associated with selected machine learning algorithms (here, ANN model parameters- e.g. activation function type, number of hidden layers etc.)  Suitable post-processing methods for the predicted output\nThis paper proposes a novel pre-processing scheme and demonstrates the use of the said scheme in an appropriately designed framework, consisting of three stages- pre-processing, modelling, and post-processing, to estimate sand fraction (SF) from multiple seismic attributes. Here the target variable SF represents per unit sand volume within the rock.\nThe workflow starts with pre-processing stage, which uses three alternative approaches for target variable regularization based on Fourier Transform (FT), Wavelet Decomposition (WD), and Empirical Mode Decomposition (EMD). Next, a functional relationship between the regularized target sand fraction and the seismic attributes is calibrated using ANN. In this study, a simple network structure consisting a single hidden layer is selected over relatively complex networks. The network parameters and training algorithm are decided empirically. The results obtained, are evaluated in terms of four performance indicators\u2013 Correlation Coefficient (CC), Root Mean Square Error (RMSE), Absolute Error Mean (AEM) and Scatter Index (SI) [7]. The inclusion of the pre-processing stage in the workflow is found to produce remarkable improvement in the prediction results as opposed to use of the original sand fraction using the same ANN structure. After learning and validation, post processing is carried out to improve the prediction result over the study area. In this way, a complete workflow is devised for modelling a lithological property (here, sand fraction) from given seismic attributes.\nThe paper is organized as follows. Section II describes the study area and the complete workflow adopted in this paper. Section III explains the pre-processing steps. Section IV illustrates the ANN model building and validation. In Section V, the post-processing is described. Finally, Section VI concludes the paper with a discussion on achievements in this study and possible future directions of research."}, {"heading": "II. STUDY AREA AND WORKFLOW", "text": ""}, {"heading": "A. Study Area", "text": "In this study, the working dataset has been acquired from a\nwestern onshore hydrocarbon field in India. The hydrocarbon field is located at an intra-cratonic basin, spread along the western periphery of central India. It is surrounded by the Aravalli range. Deccan craton and Saurashtra craton are located in the east-west direction along the basin. Structurally, the field is located as a broad nosing feature; thus, housing the hydrocarbons between two major synclines. The hydrocarbon is present within a series of vertically stacked sandstone reservoirs individually separated by intervening shale. The\naverage thickness of a sand layer is in the order of 5-6 m. Due to the discrete sand depositions in thin layers, imaging of the seismic data and obtaining a mapping between seismic and borehole dataset are challenging tasks.\nA spatial database containing seismic attributes and well logs has been acquired from the study area in .sgy and .las format respectively. The SEG-Y file format is one of the numerous standards developed by the Society of Exploration Geophysicists (SEG) for storing geophysical data [18]. The Log ASCII Standard (LAS) format has been developed by the Canadian Well Logging Society to standardize the organization of digital log curves information in 1989 [19]. As the workflow is developed on MATLAB platform, the .sgy data files are converted in .mat format (MATLAB platform compatible format) for MATLAB compatibility.\nFig. 1 represents the placements of the four wells across the study area. These wells will be hereafter referred as A, B, C, and D in terms of inlines and crosslines (xlines). Additionally, well A (XXX3645, XXX7387), well B (XXX2107, XXX7285), well C (XXX2768, XXX7073), and well D (XXX0916, XXX7807) are the locations in terms of Universal Transverse Mercator (UTM) coordinates. The depth of the each well is around 3000 meter from ground, whereas the zone of interest is from 2750 meter to 2975 meter in subsurface for well A. The zone of interests are 2720 meter \u2013 2950 meter for well B, C, and D.\nFig. 1. Location of four wells in the study area in terms of inlines and crosslines (xlines)\nThe borehole dataset contains basic logs such as gamma ray, resistivity, density along with derived geo-scientific logs such as sand fraction, permeability, porosity, water saturation, etc. These well logs are treated as one dimensional signals for further processing in this study. On the other hand, the seismic dataset contains seismic impedance, amplitude, and instantaneous frequency throughout the volume."}, {"heading": "B. Workflow", "text": "The systematic workflow consisting three stages e.g. pre-\nprocessing, model building\u2013validation and post-processing is demonstrated in Fig. 2. The workflow is implemented on a 64- bit MATLAB platform installed in Dell Precision M6800 with Intel\u00ae core\u2122 i7-4700MQ CPU @2.40 GHz having 32GB RAM. The regularization approach in the pre-processing stage is highlighted using a small dotted rectangle in Fig. 2.\n250 300 350 400 450 100\n110\n120\n130\n140\n150\nA\nB C\nD\nXline\nIn lin\ne\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n3\nThree alternative regularization methods based on FT, WD, and EMD are implemented in this study. In the model building stage, training, testing, and validation are carried out consecutively. If the validation performance does not improve as expected even after implementing either of the three regularization techniques, then the parameters associated with the selected regularization method are modified before carrying out training, testing, and validation again. This tuning is carried out using a trade-off between loss of high frequency information from target attribute and acceptable performance evaluators.\nThe improvement in the ANN performance with the regularization technique is quantified using the performance evaluators presented in Tables-III and IV later. First, the ANN is trained with the original sand fraction as target variable without involving any pre\u2013processing. The same procedure is repeated with the regularized sand fraction as target variable of the ANN. This is helpful to reassure the improvement in ANN performance while using the regularized target variables. The validated network is used to obtain sand fraction over the study area from the three seismic attributes. The post-processing stage includes further smoothing and filtering of the estimated output to remove unwanted predictions."}, {"heading": "III. PRE-PROCESSING", "text": "Pre-processing plays a crucial role on the performance of a machine learning algorithm. In this paper, an efficient preprocessing approach is proposed as part of the adopted methodology to obtain a functional relationship between seismic attributes and lithological properties."}, {"heading": "A. Signal Reconstruction", "text": "The borehole data are recorded at specific well locations\nalong depth with a high vertical resolution. Conversely, the seismic data are collected spatially in time domain with a sampling interval of two milliseconds. Signal reconstruction is an important pre-processing step and has been discussed in existing literature. In [20], the up-scaling of an acoustic impedance log has been carried out by a wavelet transform based method as well as a geostatistical technique. The acoustic impedance log is computed from the recorded logs of the Pwave velocity and the density at the corresponding welllocation. The reconstructed acoustic impedance logs obtained from the raw impedance log by wavelet transform and geostatistical methods have turned out to be similar. In [21], the seismic attributes and well logs have been integrated to predict facies using a data-driven methodology. The complexities regarding resolution of individual datasets (seismic and well logs) and difference in the individual sampling intervals have been discussed in detail in [21]. For the present work, the well logs are converted from depth to time domain at 0.15 milliseconds sampling interval using the given velocity profile obtained from well-seismic-tie. It can be noted that the sampling intervals of the two datasets (seismic attributes and well logs) are different. Hence in order to integrate the two, the band limited seismic attributes are reconstructed at each time instant corresponding to the well logs by a sinc interpolator while adhering to the Nyquist\u2013Shannon sampling theorem [22].\nFig. 3 shows the three seismic attributes and the sand fraction, along well A. The red dots on the seismic attributes represent their original values at time interval of two milliseconds and the green curves represent the corresponding reconstructed signals at the time instants marked on the well logs. The blue high frequency curve in Fig. 3(d) represents sand fraction along the same well.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n4"}, {"heading": "B. Signal Regularization", "text": "It can again be observed from Fig. 3 that the frequencies present in seismic signals are much lower compared to that of the sand fraction. In other words, the sand fraction carries much more information as compared to the seismic attributes. According to laws of information theory, a higher informationcarrying signal cannot be modelled using single or multiple lower information-carrying predictor signals [23]. Only a part of the target variable that is dependent on the predictor variables can be modelled. For the present case, the spatially distributed seismic attributes have low vertical resolution; whereas the high resolution borehole data are recorded at the specific well locations and carry high information content [24]. Hence, the estimation of subsurface lithological properties from seismic attributes deals with uncertainty due to the mismatch in the information content. In order to circumvent this disparity, the seismic attributes are re-sampled at each time instants corresponding to the well logs to integrate with the lithological properties. However, this re-sampling does not contribute any additional information [23]. The seismic information would not be able to successfully delineate the rock properties completely, which is a key step for RC. As no new information can be added to the seismic attributes, some amount of information has to be filtered from the high information carrying SF signal in order to calibrate a functional relationship between them. Thus, the necessity of information filtering for the SF prediction through regularization is established.\nIn this paper, three different signal processing approaches are selected and implemented in order to filter the target signal. The parameters belonging to this stage are tuned following the changes in entropy before and after filtering along with visual inspection of the output signal with respect to that of the original target signal. The entropy has been computed from the Power Spectral Density (PSD) of the signal. The average amount of information gained from a measurement that specifies X is defined to be the entropy ( )H X of a system. It can be formally defined as\n2( ) ( ) log ( )i i i H X p x p x \n(1)\nwhere ( )ip x is the probability of X having the thi value ix in the dataset. This is known as Shannon entropy [23], [25]. If\nA is another random variable described on the same dataset then the mutual information between the two can be expressed as ( ; ) ( ) ( | )I X A H X H X A  (2) where, ( | )H X A is the conditional entropy of X after A has been observed. A reservoir property (SF for the present case) can be represented by X and seismic attribute e.g. seismic impedance can be represented by A . The statistical property of ( ; )I X A can be interpreted as the reduction in the uncertainty of the reservoir property, due to observation of attribute A . In [26], Normalized Mutual Information (NMI) is defined as the mutual information normalized by minimum entropy of both the variables. ( ; ) ( ; ) / min( ( ), ( ))NMI X A I X A H X H A (3) In this study, the NMI computed between predictor and target signal has been used to adjust the parameters of the information filtering algorithms.\nBefore starting the regularization, the seismic inputs and sand fraction values are normalized by Z-score and min-max methods respectively using the data from all four wells taken together. In this paper, the modelling of the target SF from the three seismic attributes is carried out by a single hidden layer ANN trained using a back propagation algorithm. The network structure and training procedure are discussed in detail in the following section. The target variable is normalized within the range of output activation function keeping some offset from limiting value of the activation function. Otherwise, the back propagation algorithm tends to drive the free network parameters to infinity. As a result, the learning process may slow down [27]. Hence, the target variables are normalized between 0.1 and 0.9 so that it does not overlap with the saturation region of the log-sigmoid function."}, {"heading": "1) Fourier Transform (FT)", "text": "The first regularization approach is based on Fourier transform (Algorithm I). Here, the spectrums of target and predictor variables are compared and higher frequency components of the target signal are truncated. Then, the target signal is reconstructed using Inverse Fourier transform (IFT).\nALGORITHM I: SF regularization Based on Fourier Transform Task : Regularizing target sand fraction based on Fourier Transform Input : Predictor signal ( )x t and target signal ( )y t a) The sand fraction ( )y t extracted from a well-log by proper depth to time\nconversion using the given velocity profile. Let the seismic amplitude along the same well-log be ( )x t .\nb) Compute Fourier Transform of ( )x t : ( 1)( 1)\n1\n( ) ( ) N\nj k N\nj\nX k x j \u03c9  \n  where, 2 /\u03c0i NN\u03c9 e is an thN root of unity\nSimilarly, FT of target ( )y t is computed as: ( 1)( 1)\n1\n( ) ( ) N\nj k N\nj\nY k y j \u03c9  \n  where, 2 /\u03c0i NN\u03c9 e is an\nthN root of unity c) Compare the spectrums of target and predictor signals d) Select the bandwidth parameter max\u03be Hz\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n5\ne) The part of the target spectrum exceeding max\u03be Hz is truncated to zero.\nModified Target : mod ( )Y k\nf) Construct regularized target signal ( )ry t by carrying out IFT of the\ntruncated spectrum : ( 1)( 1)mod 1\n1( ) ( ) N\nj k r N\nk\ny t Y k \u03c9 N\n  \n   , where, 2 /\u03c0i N\nN\u03c9 e  is an thN root of unity\ng) Calculate entropies of predictors (seismic attributes here) as well as original and regularized target signals (sand fraction here). h) If entropy of regularized target is comparable with that of the predictor signal and the regularization result is satisfactory, then regularization is completed else go to step d). Output : Regularized target signal ( )ry t\nComparison between the spectrums of the sand fraction (Fig. 4 (a)) and that of the seismic impedance (Fig. 4 (b)), reveals the presence of higher order frequencies in the former.\nIt can be observed from Fig. 4(b) that the spectrum of bandlimited seismic impedance diminishes beyond a certain frequency range (-0.2: +0.2 Hz). Then, the part of the sand fraction spectrum belonging to a slightly wider frequency range (green curve, Fig. 4(a)) is reconstructed to obtain the regularized target. The wider range of frequency is chosen with the assumption that ANN as a nonlinear predictor is capable of mapping input signals of lower frequencies to output signals with higher frequencies. Of course it needs an entirely different\nresearch to find the prediction capability of a given nonlinear mapping process. The original and regularized SF signals are presented in Fig. 4(c) by the blue and red curves respectively.\nThe FT based regularization method is more sophisticated than bandpass filtering in the time domain. In order to carry out bandpass filtering, two cut-off frequencies of the ideal bandpass filter are defined beyond which all frequency components are removed. In case of ideal bandpass filters, zero phase shift is an important requirement. However, ideal bandpass filter is not realizable while working with practical datasets having finite number of samples. Moreover, in case of practical time-domain bandpass filtering, a transition takes place between the pass band and the stop band i.e. the frequency component beyond the cut-off frequencies does not diminish abruptly. In case of FT based regularization, the frequency components of the SF spectrum beyond the selected parameter range (-0.2:+0.2 Hz) are completely truncated before reconstruction of regularized SF log. Additionally, the phase shift resulting from bandpass filtering can yield lead-lag relationships between actual and filtered logs [28]. On the other hand, FT based regularization does not incur phase shift between actual and regularized logs as shown in Fig. 4(c).\nAs shown in Table I, the information content of the original sand fraction is higher as compared to that of the seismic predictor variables which makes it difficult to model the target (sand fraction) from predictor attributes. The regularization process decreases the information content in the sand fraction as seen in Table I. The dependency between predictor and target variables in terms of NMI (Table II) also improves as a result of regularization."}, {"heading": "2) Wavelet Decomposition (WD)", "text": "Wavelet decomposition has been extensively used in different fields of research. For example, it is applied in EEG signal for artifact removal [29], [30] and study of geomagnetic\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n6\nsignals [31]\u2013[33] etc. A time-frequency representation of a one dimensional non-stationary signal is obtained using wavelet analysis. Wavelets are oscillating functions localized in time and frequency [29], [34], [35]. A finite energy time domain signal can be decomposed and expressed in terms of scaled and shifted versions of a mother wavelet ( )t and a corresponding scaling function ( )t after decomposition. The scaled and shifted form of the mother wavelet , ( )l k t and the corresponding scaling function , ( )l k t are mathematically represented as\n/ 2 , ( ) 2 (2 ), , l l l k t t k l k R    (4)\n/ 2 , ( ) 2 (2 ), , l l l k t t k l k R    (5)\nThe original signal ( )X t is first decomposed into high frequency and low frequency components using high pass and low pass filters. After each filtering step, the output time series is down-sampled by two. The low frequency part approximates the signal while the high frequency part denotes residuals between original and approximate signal. At successive levels the approximate component is further decomposed using the same set of high-pass and low-pass filters. A time domain signal ( )X t can be expressed in terms of the aforementioned mother wavelet , ( )l k t and corresponding scaling function , ( )l k t at level l as\n, ,( ) ( ) ( ) ( ) ( )l l k l l k k k X t a k t d k t    (6) where ( )la k and ( )ld k are the approximate and detailed coefficients at level l . These coefficients are computed using filter bank approach as in [36]. Fig. 5 describes the steps of wavelet decomposition for three levels. Here, a signal is decomposed into approximate and detailed coefficients using low pass ( )H k and high pass ( )G k filters respectively. After decomposition, the coefficients can be modified. In case of signal reconstruction, the modified approximate and detailed coefficients are up sampled by two and then convolved with respective synthesis filters and then the resulting pair is summed. Finally, modified signal is acquired following l level synthesis.\nThe performance of wavelet analysis is dependent on the mother wavelet selection and decomposition level. The Daubechies family of wavelets has a compact support with relatively more number of vanishing moments [34]. Therefore, in most of the cases different variants of Daubechies family wavelets are used for signal analysis. The initial choice for mother wavelet and decomposition level can be db4 and six respectively for this kind of study. However, the initial choice of wavelet and decomposition level can be modified if the regularization results are not satisfactory. In this paper, the choice of mother wavelet and decomposition level has been carried out empirically based on the regularization result and performance of the ANN. Algorithm II describes the steps associated with WD based regularization.\nFig. 6(a)-(b) represent the results of WD\u2013based regularization of target sand fraction with predefined wavelet type, and decomposition level. The first three detailed coefficients of original sand fraction signal are demonstrated in Fig. 6 (a). After the decomposition, detailed coefficients of\ninitial levels of original target signal are made zero and regularized signal is constructed by performing Inverse Discrete Wavelet Transform (IDWT) from the modified coefficients.\nAlgorithm II: SF Regularization Based on WD Task : Regularizing target SF based on Wavelet Decomposition Input : Predictor signal ( )x t and target signal ( )y t\na) Same as Algorithm-I. b) Select the wavelet type and number of decomposition levels. c) Apply the procedure as in Fig. 5 to the target signal. d) Decide: detailed coefficients to be truncated for regularization e) The selected detailed coefficients are made zero. f) The regularized target signal is reconstructed from the modified\ncoefficients. g) Calculate entropies of predictors as well as original and\nregularized target signals. h) If entropy of regularized target is comparable with that of the\npredictor signal and the regularization result is satisfactory, then regularization is completed, else go to step d).\nFor Well A, first five detailed coefficients are truncated and regularized target is reconstructed from approximate and detailed coefficients of the sixth level by IDWT. The\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n7\nregularization result for Well A is presented in Fig. 6 (b), where blue and red curves represent original and regularized target sand fraction signals respectively. Table I-II reveal the changes in information content of the original and regularized sand fraction by WD and increase of dependency between target and predictor variables as a result of regularization.\n3) Empirical Mode Decomposition (EMD)\nSeismic and well log signals are non-stationary signals. Reports suggest that in most of the cases the frequency analysis of signals are carried out in selected windows with respect to a given orthogonal basis [37]\u2013[39]. The disadvantage of basis decomposition techniques is the mismatch between signal trend and constant basis functions. These necessitate a new decomposition method, namely Empirical Mode Decomposition (EMD). EMD is an algorithmic decomposition method which decomposes the input signal into a set of Intrinsic Mode Functions (IMFs) and a residue signal [40]. There are two properties associated with IMFs such that (1) the numbers of zero\u2013crossings and extrema present in IMFs are same, and (2) IMFs are symmetric with respect to the local mean [40]. In other words, EMD detects and extracts the highest frequency component in the signal [34], [41]\u2013[47]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k. Moreover, being an adaptive data-driven method, EMD decomposes an input signal into a variable number of components. Thus, EMD overcomes the inherent limitation of deciding a priori the number of decomposition levels as in WD. Algorithm III describes the detailed steps associated with EMD based regularization of target SF.\nAlgorithm III SF Regularization Based on EMD Task : Regularizing target sand fraction based on EMD Input : Predictor signal ( )x t and target signal ( )y t\na) Same as Algorithm I. b) Initialize: 0 ( ) ( )r t x t , 1i \nc) Extract the thi IMF: i. Initialize: 0 ( ) ( ) ih t r t , 1j \nii. Extract the local minima and maxima of 1 ( )jh t\niii. Create upper envelope max ( )e t and lower envelope min ( )e t of 1 ( )jh t by interpolating local maxima and minima\niv. Calculate mean envelope: max min1 ( ) ( ) ( ) 2  j e t e t m t\nv. 1 1( ) ( ) ( )j j jh t h t m t  \nvi. If ( )jh t is an IMF, then, ( ) ( )i jimf t h t\nelse go to step- (ii). with 1j j \nd) 1( ) ( ) ( )i i ir t r t imf t \ne) If ( )ir t has at least two extrema, then, go to c) with 1i i \nelse, 1\n( ) ( ) ( ) n\ni n i x t imf t r t    is decomposed into n numbers of IMFs and residue signal.\nf) EMD of target signal ( )y t is carried out following steps b)-e)\n1\n( ) ( ) ( ) p\ni p i y t imf t r t   \ng) The number of IMFs and distribution of IMFs are observed for target ( )y t and predictor ( )x t : p n\nh) Decide 1p : number of IMFs truncated from the EMD of ( )y t for\nregularization where, 1p p\ni) Construct regularized target signal ( )ry t :\n1\n1\n( ) ( ) ( ) p\nr i p i y t imf t r t    j) Calculate entropies of predictors as well as original and regularized\ntarget signals. k) If entropy of regularized target is comparable with that of the predictor\nsignal and the regularization result is satisfactory, then regularization is completed else go to step h).\nOutput : Regularized target signal ( )ry t\nIn Fig. 7 (a)-(b), the first four IMFs of the sand fraction log and the IMFs and residue of the decomposed seismic impedance along the Well A are plotted.\nThe comparison between Fig. 7 (a)-(b) reveals that the number of IMFs obtained is higher in case of target signal (sand fraction) than its predictor counterpart (seismic-impedance). The first IMF component is suppressed and the other IMFs are used to reconstruct the regularized sand fraction. The superimposed plots of actual (blue curve) and regularized sand fraction (red curve) signals are presented in Fig. 7 (c). The user decides the regularization result is satisfactory or not based on visual inspection of original and regularized target variables. The regularized target is smoother compared to original signal;\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n8\nnevertheless, the trend of the original signal is preserved even after information filtering based on either of the three proposed regularization methods. The number 1p is selected such that it is less than that of the decomposed IMFs of the original signal to be regularized. It is usually selected as one less than the number of decomposed IMFs of the original signal. If the regularization result is satisfactory in terms of entropy criterion and superimposed plots of actual and regularized SF, the initial\n1p value is retained as in Algorithm III. Otherwise, the value of 1p is to be modified. The initial selection of 1p would be one less than the number of decomposed IMFs of the original target signal for the working datasets. Table I represents the entropies of predictor and target attributes for Well A. The improvement in mutual dependency between predictor and target variables in terms of NMI is evident from Table II.\nThe FT is a linear transform where the frequency spectrum of a time-domain signal is obtained. Based on the comparison between the frequency spectrums of the predictor seismic impedance and target SF, the regularization parameter is selected. The result of FT based regularization changes according to the value of the selected regularization parameter. If the value of the selected regularization parameter were very small, then the regularized signal would become smooth by shedding the finer trends of the original target signal. Again, regularization with large parameter value would not carry out sufficient information filtering to enable effective modelling of the target variable. The basic assumption in FT is stationarity of a signal. However, seismic and SF signals are non-stationary. Therefore, the next two approaches based on WD and EMD are opted to cross-validate the performance of FT based regularization. The selection of the mother wavelet and the decomposition level determines the granularity of the decomposed target signal. The regularized target signal is reconstructed from the modified coefficients. These parameters (wavelet type, decomposition level, and detailed coefficients to be truncated) are finalized empirically based on the regularization result. However, in case of EMD, the number of parameters is less. Only, the number of IMFs to be used for reconstruction of the target property needs to be decided. In all the three cases, it is ensured that the regularized target signal retains the trend of the original signal. In case of FT based approach, the regularization parameter is decided based on the FT results of both the predictor as well as the target signal. The number of IMFs of the seismic attributes is less compared to those of SF in case of EMD. While in both the cases of WD and EMD based regularization, the number of detailed coefficients and IMFs are decided irrespective of the decomposition results of the predictor seismic signals. Instead these parameters are decided based on the regularization results and entropy criterion as mentioned in Algorithm II and Algorithm III respectively."}, {"heading": "IV. MODEL BUILDING AND VALIDATION", "text": "In order to establish the efficacy of the proposed\nregularization method, the task of model building and validation stage is carried out using both the original and regularized sand fraction as target variable. The training dataset is created by aggregating 70% sample patterns from each of the wells. The training patterns are scrambled to remove any\npossible trend along the depths. The remaining 30% samples from each of the four wells are combined and then divided into two parts to create the testing and the validation datasets. First, the network is trained using training patterns with initial parameter values. Then, the network structure and activation functions are tuned using testing patterns. The testing phase is important for evaluating the generalization capability of the trained network [11]. The network which performs satisfactorily in terms of CC, RMSE, AEM and SI is then selected for use in the validation stage. The statistical analysis of the errors involved in the model are important for proper understanding of the performance. The initial network structure is decided intuitively depending on nature of the problem and amount of available training patterns. In this study several runs of training, testing and validation of neural network structures with varying number of neurons, layers, activation functions and learning methods have been carried out to decide the best structure as well as the most effective learning algorithm. Finally in the hidden layer, hyperbolic tangent sigmoid transfer function has been used. The tangent sigmoid transfer function is an automatic choice for researchers for use in hidden layer to achieve the bi-directional swing [48], [49]. The activation function used in the output layer is log-sigmoid which is nonsymmetric in nature and facilitates faster learning rate [27]. The number of nodes in the input layer is same as the number of predictor attributes to be used to model the ANN. For example, in case of predicting sand fraction from three predictor attributes\u2013 namely, seismic impendence, instantaneous frequency and amplitude, the number of input and output nodes will be three and one respectively. Finally, a network with single hidden layer is trained using the Scaled Conjugate Gradient (SCG) Back propagation Algorithm [50]. The backpropagation learning using the SCG method is given in the Appendix. SCG proceeds in the conjugate direction of the previous step instead of following the gradient direction as in other gradient descent based algorithms. In case of the conjugate gradient algorithms, the step size can be selected by a line search without evaluating the Hessian matrix. However, the line search method is itself computationally expensive due to the evaluation of multiple error functions. Moreover, the line search deals with multiple parameters crucial for its performance. Sometimes the Hessian matrix becomes non positive definite which leads to increase in the error in weight updates due to ill conditioning in the matrix inversion. In SCG, the Hessian matrix is made positive definite. The computationally inefficient line search in every iteration is avoided using a step size scaling mechanism in SCG. Another advantage of SCG is that it does not involve any user dependent parameters. The SCG method has been used to solve problems in different research domains such as prediction of groundwater level [51], and telemarketing success [52], etc.\nTable II reveals that the NMIs between instantaneous frequency and SF (original/regularized) are relatively lower compared to other cases. In the first attempt, two input attributes (seismic impedance and amplitude) have been used to build the prediction model and the corresponding results are documented in Table III in terms of the performance evaluators.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n9\nThough variation of instantaneous frequency is comparatively lower, it is an important attribute. Therefore, all three seismic attributes have been used as inputs to the prediction model (Table IV) in the second attempt. The performance comparison of the method of averaging and the proposed regularization procedures has been carried out. The original SF is filtered by a moving average filter with a span of nine samples, before being passed to the ANN training module. Table V represents the validation performance of the ANN in modelling the averaged SF from the seismic impedance, amplitude and instantaneous frequency. The prediction results improve in terms of performance evaluators in case of filtered SF by averaging as compared to the original SF as target. It can be observed from Table IV and Table V that the performance of the trained ANN is superior while working with the processed SF using the proposed regularization as target compared to the original and averaged SF.\nThe results reported in Table III, Table IV, and Table V lead to the following important observations:  The performance of the trained networks is improved with\nthe use of regularized target signals. This can be quantified in terms of higher CCs and lower error values.\n In all cases, inclusion of the instantaneous frequency as the third predictor improves the prediction.\n The proposed regularization technique is superior to the averaging method. Further it can be observed from Table III and Table IV that the network performance is superior in case of regularization based on WD with two predictor variables. On the other hand, with three predictors FT based regularization outperformed the other two regularization approaches in terms of performance evaluators. However, for all cases, the performance is improved while using regularized sand fraction as target instead of original log. The extent of advantage of each scheme over the others may vary among different datasets. Therefore a given scheme cannot be universally recommended. A user is provided with multiple choices among which the user can select the regularization technique that best suits with the working dataset before application of machine learning algorithm.\nLiterature study reveals that pre-processing is an important step prior to modelling. For example, normalization, relevant attributes selection, importance sampling etc. have been part of\nthe pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58]. Generally, CC values above 80%; RMSE, AEM values below 0.15 and SI value below 0.35 can be considered as good fit in such studies. If the prediction model attains the values of the respective performance indicators beyond the above mentioned limits, then the model should be retrained after adjusting associated parameters. Baziar et al. [57] has compared the performance of three machine learning techniques such as SVM, multilayer perceptron (MLP), and Co-Active Neuro-Fuzzy Inference System (CANFIS) to predict permeability from well logs. Further, the normalization has been carried out as the preprocessing technique before modelling. However, integration of seismic and borehole dataset are not required since only well logs are used as the predictor variables in [57]. The performances of the three approaches (SVM, MLP, CANFIS) have been quantified in terms of CC, average absolute error (AAE, equivalent to AEM), and mean squared error (MSE i.e. square of RMSE). The methodology used to model the original SF is almost similar to the methodology adopted for the permeability prediction using multilayer perceptron as in [57]. Comparison between the validation performance (prediction using data not part of training) reported in [57] and those achieved in this study has revealed that the regularization stage improves the prediction result in terms of CC, AEM, and RMSE. The pre-processing step involving a single ANN in [17] is similar to the case in the present paper while modelling with original SF as target. Comparison among the ANN performances with and without the regularization step reveals that the results of ANN has improved in terms of evaluators in the former case.\nIn the case of unsatisfactory validation performance, the user may modify the ANN structures and network parameters and retrain the modified ANN. If the validation performance still does not improve, the user can modify the regularization parameters and carry out the modelling with regularized target signal. By changing the regularization parameters the information retention in the filtered/reconstructed changes e.g. by choosing a narrower bandwidth in the FT based method the well logs become smoother with a reduced entropy.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n10\nThe prediction in the entire volume along each inline and cross line has been carried out using the network selected after acceptable performance in the validation step. Fig. 8 represents the variation of the seismic amplitude at a specific inline (inline 136 containing Well D). The sand fraction variation at the same inline shown in Fig. 9 is obtained by prediction of the sand fraction over the study area from available seismic attributes using validated network parameters. The network used here for prediction has been calibrated using EMD\u2013regularized\u2013sand fraction as target."}, {"heading": "V. POST-PROCESSING", "text": "It can be observed from Fig. 8 that the variation of the seismic attributes over the study area is smooth. However, the sand fraction across the study area as shown in Fig. 9 changes abruptly. The transition of the sand fraction values should be smoother and more or less agree to the patterns of seismic data. Thereby rises the need for the post\u2013processing stage in order to obtain a smoother sand fraction variation across the volume. To incorporate this rationale, the predicted values are filtered through a 3-D median filter.\nMedian filter is an order statistic filter i.e. non-linear spatial filter. In case of order statistics filters, the filtered output is dependent on the ordering (ranking) of the pixels in the image\narea contained by the selected window [59], [60]. Thus, the neighborhood values of the particular pixel play an important role in the determination of the modified pixel value as a result of order filtering. In case of order statistics filtering the window size is selected to define the neighborhood around the centered pixel. Selection of window value is crucial for the degree of smoothing. The predicted sand fraction in the volume is used as input to the post-processing operation. Every element in the volume is considered as a pixel and is smoothened using 3-D median filter with respect to its neighborhood within a 3x3x3 window size. The missing values along the boundaries are ignored. In order to carry out median filtering, the values of the pixel and its neighbors within the selected window are first sorted. Then, the centered pixel value is replaced by the median value determined from the sorted pixel values. For example, in case of 3-D median filtering with a 3x3x3 window, the 14th largest value of the neighborhood replaces the pixel value at the center of the neighborhood. The SF logs over the area are predicted from seismic attributes and variation of the predicted SF at a particular inline is presented in Fig. 9. The detailing of the SF variation is revealed by the color code in Fig. 9. The smoothened SF values attained by median filtering may lose the exactness of the predicted SF logs; however, the volumetric representation of the filtered SF collectively offers better understanding of the SF variation. Fig. 10 represents the result of median filtering along inline 136. The effect of localizing different levels of sand fraction values can be observed by comparing Fig. 9 and Fig. 10.\nPost-processing with larger window sizes (for example 5x5x5 or higher order) would further smoothen out the predicted SF volume at the cost of detailed SF variation. In our subsequent research attempts, adaptive post processing method having the potential to further improve the prediction result will be worked out. This would help in subsurface characterization. Different types of spatial filters would be experimented to observe the changes in filtered SF.\nThus, the complete framework including pre-processing, learning and validation, and finally post-processing, successfully carries out mapping between seismic attributes and the sand fraction."}, {"heading": "VI. DISCUSSION AND CONCLUSION", "text": "This paper brings out a complete workflow consisting of an elegant regularization step in pre-processing to enhance the\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n11\nlearning capability of ANN in carrying out mapping between seismic attributes and lithological property (sand fraction) successfully. The improvement in mapping with introduction of the regularization step is observed from the performance analysis. The comparison among the results achieved in this work and existing literatures [17], [57] reveals the superiority of the proposed regularization step to obtain improved performance in terms of the performance evaluators. In the present study, synthetic SF logs are generated over the study area from available seismic information using the validated network parameters. After obtaining SF volume, postprocessing is carried out to improve visualization by means of median filtering. Similarly, other important petrophysical properties such as porosity, permeability can be modelled from seismic attributes following the proposed workflow. The volume-wise prediction results of SF and other petrophysical properties (i.e. porosity, permeability) enables the user to identify zones with high sand content and porosity, and higher probability of hydrocarbon presence. Thus, this study would help to identify potential drilling locations of a new well in a study area.\nThe present work differs from the work done in [17] in terms of pre-processing stage, division of training-testing dataset, adopted machine learning technique, and post-processing algorithm. In addition the well tops and horizon information are used [17] to carry out zone wise division of the overall dataset and modular ANN is applied to model SF from multiple seismic attributes. The performance attained in [17] would be improved with the use of regularized SF log as target instead of original one.\nThe selection of initial parameters are crucial for achieving acceptable performance of ANN. In the present work, the ANN structure and activation functions in hidden and output layers have been empirically adjusted. Also the initial values of weight and bias matrices have been chosen randomly. As a possible future extension, the domain of metaheuristic algorithms can be explored in order to automate such selections to further strengthen the framework. Additionally, while in the postprocessing stage, the use of spatial filtering has provided a significant improvement in the sand fraction variation over the study area; adaptive post\u2013processing method can be investigated in future.\nAPPENDIX THE BACK-PROPAGATION LEARNING OF AN ANN USING SCALED CONJUGATE\nGRADIENT LEARNING\nThe back-propagation learning of a multilayer perceptron is carried out in two phases. The synoptic weights of the network are constant and the input signal is propagated through the hidden layers to the output layer in the forward phase. The changes only take place in the activation potentials and output of the neurons [27]. In contrast, an error signal is computed as the difference between network output and actual (desired) target (response). The error signal is propagated through layers in backward direction from output layer to input layer in the backward phase. The training procedure is described in details as follows.\nAssume,\n1{ ( ), ( )} N nx n d n  (7)\n to be the set of training samples used for learning of a back propagation multilayer perceptron. The input vector ( )x n is applied to the input layer nodes and the desired output vector\n( )d n is present at the output node to compute the error between the desired and the actual output. The forward signal is propagated from the input layer to the output layer through single or multiple hidden layers. The induced local field for neuron j in layer l can be computed from the output of neuron\ni in the previous layer ( 1)l  at iteration n i.e. 1( )liy n and the\nsynaptic weight ( )ljiw n that is connected from neuron i in\n( 1)l  layer and is expressed as ( ) ( ) ( 1)( ) ( ) ( )l l lj ji i\ni v n w n y n (8)\nIn case of output layer (here, l L and L is the network depth), the output of neuron j is written as\n( )( ) ( )L jjy n o n (9)\nTherefore, the error is computed as ( ) ( ) ( )j j je n d n o n  (10)\nThe supervised learning of a multilayer ANN can be viewed as a problem of numerical optimization. The error surface of a multilayer ANN is a nonlinear function of weight vector w . Assume that, the error energy averaged over the training samples or the empirical risk be avg . Using (10), avg can be\ncomputed as 2\n1\n1 ( ) 2 N avg j n j C e n N       (11)\nwhere, the set C contains all the neurons in the output layer. The second derivative of the cost function avg with respect to\nthe weight vector w is called the Hessian matrix and denoted by H so that,\n2\n2 avgH\nw\n \n (12)\nThe Hessian matrix is considered as positive definite unless mentioned otherwise. There are several algorithms to train an ANN. In case of conjugate gradient methods, the computational complexity and memory usage are large because of calculation and storage of the Hessian matrix at each stage. The indefiniteness of H is handled by a scaling coefficient 0k  in case of the scaled conjugate gradient (SCG). The other parameters kr and  kp represent the search direction and the steepest descent direction respectively. Different training algorithms have been experimented to train the single layer ANN to model SF from multiple seismic attributes. Finally, the SCG algorithm is selected over other algorithms to train the ANN due to its superiority in terms of obtained performances evaluators. The steps associated with the SCG can be presented as follows [50]:\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n12\nAlgorithm IV: Scaled Conjugate Gradient (SCG) to train a network Task: Obtain weight vector after training Input : Training dataset, weight vector, and parameters associated with training\na) Selection of parameters: Weight vector 1w and 40 10   , 4\n10 10   , 1 0  . Assume,\n  111 '( ), 1 and success = truep r E w k    b) If success = true , then compute the second-order information:\n/k kp \n  ( '( ) '( )) /k k kk kks E w p E w   \nT kk kp s  \nc) Modify k :   2( )kk k k kp     \nd) If 0k  , then make the Hessian matrix positive definite\n \n\n\n2\n2\n2( / )k k k k\nk k k k\nkk\np\np\n \n\n \n \n   \n\ne) Evaluate step size: \n/\nT kk k\nk k k\np r     \n\nf) Compute the comparison parameter:    22 [ ( ) ( )] /k kk k k kkE w E w p    \ng) If 0k  , then error can be reduced:\n  1k k k kw w p   1 1'( )k kr E w  \n 0, success = truek  If mod 0k N  then\n 11 kkp r   \nelse:  \n2 1 1\n11\n( ) /k T k kk k\nk kk k\nr r r\np r p\n \n\n \n\n \n \n  \n\nIf 0.75k , modify the scale parameter, 0.25k k  else  , success = falsek k \nh) If 0.25k , increase the scale parameter\n 2( (1 ) / )k k k k kp    \ni) Check: If the steepest descent direction 0,kr  then  1kw  as the desired minimum. Otherwise, 1k k  and go to b).\nOutput: Trained weight vector  1kw "}], "references": [{"title": "Lithologic Characterization of a Reservoir Using Continuous-Wavelet Transforms", "author": ["G. \u00c1lvarez", "B. Sans\u00f3", "R.J. Michelena", "J.R. Jim\u00e9nez"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 41, no. 1, pp. 59\u201365, Jan. 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Seismic inversion for reservoir properties combining statistical rock physics and geostatistics: A review", "author": ["M. Bosch", "T. Mukerji", "E.F. Gonzalez"], "venue": "Geophysics, vol. 75, no. 5, pp. 75A165\u201375A176, Sep. 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Reservoir permeability prediction by neural networks combined with hybrid genetic algorithm and particle swarm optimization", "author": ["M. Ali Ahmadi", "S. Zendehboudi", "A. Lohi", "A. Elkamel", "I. Chatzis"], "venue": "Geophys. Prospect., vol. 61, no. 3, pp. 582\u2013598, May 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A neuro-fuzzy GA-BP method of seismic reservoir fuzzy rules extraction", "author": ["S. Yu", "X. Guo", "K. Zhu", "J. Du"], "venue": "Expert Syst. with Appl., vol. 37, no. 3, pp. 2037\u20132042, Mar. 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Use of multiattribute transforms to predict log properties from seismic data", "author": ["D.P. Hampson", "J.S. Schuelke", "J. a. Quirein"], "venue": "Geophysics, vol. 66, no. 1, pp. 220\u2013236, Jan. 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Confidence bounds of petrophysical predictions from conventional neural networks", "author": ["P.M. Wong", "A.G. Bruce", "T.T.D. Gedeon"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 40, no. 6, pp. 1440\u20131444, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "A neural network approach to improve the vertical resolution of atmospheric temperature profiles from geostationary satellites", "author": ["N. Sharma", "M.M. Ali"], "venue": "IEEE Geosci. Remote Sens. Lett., vol. 10, no. 1, pp. 34\u201337, Jan. 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural network approach to estimate tropical cyclone heat potential in the Indian ocean", "author": ["M.M. Ali", "P.S.V. Jagadeesh", "I.-I. Lin", "J.-Y. Hsu"], "venue": "IEEE Geosci. Remote Sens. Lett., vol. 9, no. 6, pp. 1114\u20131117, Nov. 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Macrocell path-loss prediction using artificial neural networks", "author": ["E. Ostlin", "H.-J. Zepernick", "H. Suzuki"], "venue": "IEEE Trans. Veh. Technol., vol. 59, no. 6, pp. 2735\u20132747, Jul. 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving offline handwritten text recognition with hybrid HMM/ANN models", "author": ["S. Espa\u00f1a-Boquera", "M.J. Castro-Bleda", "J. Gorbe-Moya", "F. Zamora- Martinez"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 4, pp. 767\u2013779, Apr. 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Designing a neural network for forecasting financial and economic time series", "author": ["I. Kaastra", "M. Boyd"], "venue": "Neurocomputing, vol. 10, pp. 215\u2013 236, 1996.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Modular artificial neural network for prediction of petrophysical properties from well log data", "author": ["C.C. Fung", "K.W. Wong", "H. Eren"], "venue": "IEEE Trans. Instrum. Meas., vol. 46, no. 6, pp. 1295\u20131299, Dec. 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "An improved technique in porosity prediction: a neural network approach", "author": ["P.M. Wong", "T.D. Gedeon", "I.J. Taggart"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 33, no. 4, pp. 971\u2013980, Jul. 1995.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Reservoir properties determination using fuzzy logic and neural networks from well data in offshore Korea", "author": ["J.-S. Lim"], "venue": "J. Pet. Sci. Eng., vol. 49, no. 3\u20134, pp. 182\u2013192, Dec. 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Past, present and future intelligent reservoir characterization trends", "author": ["M. Nikravesh", "F. Aminzadeh"], "venue": "J. Pet. Sci. Eng., vol. 31, no. 2\u20134, pp. 67\u201379, Nov. 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Intelligent reservoir characterization ( IRESC)", "author": ["M. Nikravesh", "M. Hassibi"], "venue": "Proc. IEEE Int. Conference on Ind. Informatics, 2003, pp. 369\u2013373.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Well tops guided prediction of reservoir properties using modular neural network concept: a case study from western onshore, India", "author": ["S. Chaki", "A.K. Verma", "A. Routray", "W.K. Mohanty", "M. Jenamani"], "venue": "J. Pet. Sci. Eng., vol. 123, pp. 155 \u2013 163, Jul. 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Report on recommended standard for digital tape formats", "author": ["K.M. Barry", "D.A. Cavers", "C.W. Kneale"], "venue": "Geophysics, vol. 40, no. 2, pp. 344\u2013 352, 1975.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1975}, {"title": "Kansas Geological Survey", "author": ["J.R. Victorine"], "venue": "http://www.kgs.ku.edu/stratigraphic/PROFILE/HELP/Help-PC- SaveLASFile.html. .", "citeRegEx": "19", "shortCiteRegEx": null, "year": 0}, {"title": "Upscaling well logs to seismic scale \u2013 comparison of a wavelet-transform based method with a geostatistical technique", "author": ["A. Douillard", "D. Schlumberger", "O. Balz"], "venue": "Petrophysics meets Geophysics, 2000, no. November, pp. 1\u20139.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Revisiting the use of seismic attributes as soft data for subseismic facies prediction: proportions versus probabilities", "author": ["L. Stright", "A. Boucher", "T. Mukherji", "R. Derksen"], "venue": "Lead. Edge, vol. 28, no. 12, pp. 1460\u2013 1679, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Discrete-Time Signal Processing", "author": ["A. V Oppenheim", "R.W. Schafer", "J.R. Buck"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1991}, {"title": "Statistical rock physics: combining rock physics, information theory, and geostatistics to reduce uncertainty in seismic reservoir characterization", "author": ["T. Mukerji", "P. Avseth", "G. Mavko", "I. Takahashi", "E.F. Gonz\u00e1lez"], "venue": "Lead. Edge, vol. 20, no. 3, pp. 313\u2013319, Mar. 2001.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Statistical Mechanics and Information- Theoretic Perspectives on Complexity in the Earth System", "author": ["G. Balasis", "R. Donner", "S. Potirakis", "J. Runge", "C. Papadimitriou", "I. Daglis", "K. Eftaxias", "J. Kurths"], "venue": "Entropy, vol. 15, no. 11, pp. 4844\u20134888, Nov. 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Normalized mutual information feature selection", "author": ["P. a Est\u00e9vez", "M. Tesmer", "C. a Perez", "J.M. Zurada"], "venue": "IEEE Trans. Neural Netw., vol. 20, no. 2, pp. 189\u2013201, Feb. 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural networks: a comprehensive foundation", "author": ["S. Haykin"], "venue": "New Jersey, USA: Prentice hall,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "A Zero Phase Shift Band Pass Filter", "author": ["H. Steehouwer", "K.M. Lee"], "venue": "2012. > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 13", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Functional network changes associated with sleep deprivation and fatigue during simulated driving: validation using blood biomarkers", "author": ["S. Kar", "A. Routray", "B.P. Nayak"], "venue": "Clin. Neurophysiol., vol. 122, no. 5, pp. 966\u2013974, May 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Effect of sleep deprivation on estimated distributed sources for scalp EEG signals : A case study on human drivers", "author": ["A. Chaudhuri", "A. Routray", "S. Kar"], "venue": "IEEE 4th Int. Conf. Intelligent Human Comput. Interaction, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Magnetospheric ULF wave studies in the frame of Swarm mission: a time-frequency analysis tool for automated detection of pulsations in magnetic and electric field observations", "author": ["G. Balasis", "I.A. Daglis", "M. Georgiou", "C. Papadimitriou", "R. Haagmans"], "venue": "Earth, Planets Sp., vol. 65, no. 11, pp. 1385\u20131398, Nov. 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "ULF wave activity during the 2003 Halloween superstorm: multipoint observations from CHAMP, Cluster and Geotail missions", "author": ["G. Balasis", "I. a. Daglis", "E. Zesta", "C. Papadimitriou", "M. Georgiou", "R. Haagmans", "K. Tsinganos"], "venue": "Ann. Geophys., vol. 30, no. 12, pp. 1751\u20131768, Dec. 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Wavelet-based multiscale analysis of geomagnetic disturbance", "author": ["N. Zaourar", "M. Hamoudi", "M. Mandea", "G. Balasis", "M. Holschneider"], "venue": "Earth, Planets Sp., vol. 65, no. 12, pp. 1525\u20131540, Dec. 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A Wavelet Tour of Signal Processing, 2nd Ed", "author": ["S. Mallat"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "The wavelet transform, time-frequency localization and signal analysis", "author": ["I. Daubechies"], "venue": "IEEE Trans. Inf. Theory, vol. 36, no. 5, pp. 961\u20131005, Sep. 1990.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Wavelets and Signal Processing", "author": ["O. Rioul", "M. Vetterli"], "venue": "IEEE Signal Processing Mag., pp. 14\u201338, 1991.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1991}, {"title": "Time-frequency analysis theory and applications", "author": ["L. Cohen"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "Empirical mode decomposition based time-frequency attributes", "author": ["I. Magrin-chagnolleau", "R.G. Baraniuk"], "venue": "69th Annual International Meeting, SEG, Expanded Abstracts, 1999, pp. 1949\u20131953.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "The empirical mode decomposition and the Hilbert spectrum for non-linear and non-stationary time series analysis", "author": ["N.E. Huang", "Z. Shen", "S.R. Long", "M.C. Wu", "H.H. Shih", "Q. Zheng", "N.- C. Yen", "C.C. Tung", "H.H. Liu"], "venue": "Proc. R. Soc. London. Ser. A Math. Phys. Eng. Sci., vol. 454, no. 1971, pp. 903\u2013995, 1998.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1971}, {"title": "Empirical Mode Decomposition vs . Wavelet Decomposition for the Extraction of Respiratory Signal from Single- Channel ECG : A Comparison", "author": ["D. Labate", "F. La Foresta", "G. Occhiuto", "F.C. Morabito", "A. Lay-ekuakille", "P. Vergallo"], "venue": "IEEE Sensors J., vol. 13, no. 7, pp. 2666\u2013 2674, Jul. 2013.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic artifact rejection from multichannel scalp EEG by wavelet ICA", "author": ["N. Mammone", "F. La Foresta", "F.C. Morabito", "S. Member"], "venue": "IEEE Sensors J., vol. 12, no. 3, pp. 533\u2013542, Mar. 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Wavelet Methods for Time Series Analysis", "author": ["D.B. Perciva", "A.T. Walden"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2000}, {"title": "Filter Bank Property of Multivariate Empirical Mode Decomposition", "author": ["N. ur Rehman", "D.P. Mandic"], "venue": "IEEE Trans. Signal Process., vol. 59, no. 5, pp. 2421\u20132426, May 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Multivariate empirical mode decomposition", "author": ["N. Rehman", "D.P. Mandic"], "venue": "Proc. R. Soc. London. Ser. A Math. Phys. Eng. Sci., vol. 466, no. 2117, pp. 1291\u20131302, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "A Fast Empirical Mode Decomposition Technique for Nonstationary Nonlinear Time", "author": ["C.D. Blakely"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Why Tanh : chossing a sigmoidal function", "author": ["B.L. Kalman", "S.C. Kwasny"], "venue": "Int. Joint Conf. on Neural Networks, 1992, vol. 4, pp. 578\u2013 581.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1992}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks, vol. 6, pp. 861\u2013867, 1993.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1993}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning", "author": ["M.F. Moller"], "venue": "Neural Networks, vol. 6, pp. 525\u2013533, 1993.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1993}, {"title": "A comparative study of artificial neural networks, Bayesian neural networks and adaptive neuro-fuzzy inference system in groundwater level prediction", "author": ["S. Maiti", "R.K. Tiwari"], "venue": "Environ. Earth Sci., vol. 71, no. 7, pp. 3147 \u20133160, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "A data-driven approach to predict the success of bank telemarketing", "author": ["S. Moro", "P. Cortez", "P. Rita"], "venue": "Decis. Support Syst., vol. 62, pp. 22\u201331, 2014.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural networks for the prediction and forecasting of water resources variables : a review of modelling issues and applications", "author": ["H.R. Maier", "G.C. Dandy"], "venue": "Environ. Model. Softw., vol. 15, pp. 101\u2013124, 2000.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2000}, {"title": "Review of the most common pre-processing techniques for near-infrared spectra", "author": ["\u00c5. Rinnan", "F. Van Den Berg", "S.B. Engelsen"], "venue": "Trends Anal. Chem., vol. 28, no. 10, pp. 1201\u20131222, Nov. 2009.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Current breathomics--a review on data pre-processing techniques and machine learning in metabolomics breath analysis", "author": ["A. Smolinska", "A.-C. Hauschild", "R.R.R. Fijten", "J.W. Dallinga", "J. Baumbach", "F.J. van Schooten"], "venue": "J. Breath Res., vol. 8, no. 2, p. 027105, Jun. 2014.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "An approach to predict breast cancer and drug suggestion using machine learning techniques", "author": ["M. Rathi", "C. Gupta"], "venue": "ACEEE Int. J. Inf. Technol., vol. 4, no. 1, pp. 23 \u2013 31, 2014.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Prediction of permeability in a tight gas reservoir by using three soft computing approaches: A comparative study", "author": ["S. Baziar", "M. Tadayoni", "M. Nabi-Bidhendi", "M. Khalili"], "venue": "J Nat. Gas Sci. Eng., vol. 21, pp. 718\u2013 724, Nov. 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "A one-class classification framework using SVDD : application to an imbalanced geological dataset", "author": ["S. Chaki", "A.K. Verma", "A. Routray", "W.K. Mohanty", "M. Jenamani"], "venue": "Proc. IEEE Students\u2019 Technology Symp. (TechSym), 2014, pp. 76\u201381.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Digital image processing, Second", "author": ["R. Gonzalez", "R. Woods"], "venue": "New Jersey, USA: Prentice hall,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "YDROCARBONS migrate from source rock through porous medium to reach reservoir rock for temporary preservation [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "In such cases, available seismic attributes can be used as a guidance to predict lithological information at all traces of the area of interest [2].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Such nonlinear problems can be approached using state-of-art computer\u2013based methods like hybrid systems [3], [4], multiple regression, neural networks [5], Neuro-fuzzy Systems [6] etc.", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "Such nonlinear problems can be approached using state-of-art computer\u2013based methods like hybrid systems [3], [4], multiple regression, neural networks [5], Neuro-fuzzy Systems [6] etc.", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "Such nonlinear problems can be approached using state-of-art computer\u2013based methods like hybrid systems [3], [4], multiple regression, neural networks [5], Neuro-fuzzy Systems [6] etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "Such nonlinear problems can be approached using state-of-art computer\u2013based methods like hybrid systems [3], [4], multiple regression, neural networks [5], Neuro-fuzzy Systems [6] etc.", "startOffset": 176, "endOffset": 179}, {"referenceID": 6, "context": "For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc.", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": "For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc.", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc.", "startOffset": 187, "endOffset": 191}, {"referenceID": 16, "context": "For example, ANN has been used in climatological studies [7], ocean engineering [8], telecommunications [9], text recognition [10], financial time series [11], reservoir characterization [12]\u2013[17], etc.", "startOffset": 192, "endOffset": 196}, {"referenceID": 6, "context": "The results obtained, are evaluated in terms of four performance indicators\u2013 Correlation Coefficient (CC), Root Mean Square Error (RMSE), Absolute Error Mean (AEM) and Scatter Index (SI) [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 17, "context": "The SEG-Y file format is one of the numerous standards developed by the Society of Exploration Geophysicists (SEG) for storing geophysical data [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 18, "context": "The Log ASCII Standard (LAS) format has been developed by the Canadian Well Logging Society to standardize the organization of digital log curves information in 1989 [19].", "startOffset": 166, "endOffset": 170}, {"referenceID": 19, "context": "In [20], the up-scaling of an acoustic impedance log has been carried out by a wavelet transform based method as well as a geostatistical technique.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [21], the seismic attributes and well logs have been integrated to predict facies using a data-driven methodology.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "The complexities regarding resolution of individual datasets (seismic and well logs) and difference in the individual sampling intervals have been discussed in detail in [21].", "startOffset": 170, "endOffset": 174}, {"referenceID": 21, "context": "Hence in order to integrate the two, the band limited seismic attributes are reconstructed at each time instant corresponding to the well logs by a sinc interpolator while adhering to the Nyquist\u2013Shannon sampling theorem [22].", "startOffset": 221, "endOffset": 225}, {"referenceID": 22, "context": "According to laws of information theory, a higher informationcarrying signal cannot be modelled using single or multiple lower information-carrying predictor signals [23].", "startOffset": 166, "endOffset": 170}, {"referenceID": 23, "context": "For the present case, the spatially distributed seismic attributes have low vertical resolution; whereas the high resolution borehole data are recorded at the specific well locations and carry high information content [24].", "startOffset": 218, "endOffset": 222}, {"referenceID": 22, "context": "However, this re-sampling does not contribute any additional information [23].", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "This is known as Shannon entropy [23], [25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "This is known as Shannon entropy [23], [25].", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "In [26], Normalized Mutual Information (NMI) is defined as the mutual information normalized by minimum entropy of both the variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "As a result, the learning process may slow down [27].", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "Additionally, the phase shift resulting from bandpass filtering can yield lead-lag relationships between actual and filtered logs [28].", "startOffset": 130, "endOffset": 134}, {"referenceID": 28, "context": "For example, it is applied in EEG signal for artifact removal [29], [30] and study of geomagnetic", "startOffset": 62, "endOffset": 66}, {"referenceID": 29, "context": "For example, it is applied in EEG signal for artifact removal [29], [30] and study of geomagnetic", "startOffset": 68, "endOffset": 72}, {"referenceID": 30, "context": "signals [31]\u2013[33] etc.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "signals [31]\u2013[33] etc.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "Wavelets are oscillating functions localized in time and frequency [29], [34], [35].", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "Wavelets are oscillating functions localized in time and frequency [29], [34], [35].", "startOffset": 73, "endOffset": 77}, {"referenceID": 34, "context": "Wavelets are oscillating functions localized in time and frequency [29], [34], [35].", "startOffset": 79, "endOffset": 83}, {"referenceID": 35, "context": "These coefficients are computed using filter bank approach as in [36].", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "The Daubechies family of wavelets has a compact support with relatively more number of vanishing moments [34].", "startOffset": 105, "endOffset": 109}, {"referenceID": 36, "context": "Reports suggest that in most of the cases the frequency analysis of signals are carried out in selected windows with respect to a given orthogonal basis [37]\u2013[39].", "startOffset": 153, "endOffset": 157}, {"referenceID": 37, "context": "EMD is an algorithmic decomposition method which decomposes the input signal into a set of Intrinsic Mode Functions (IMFs) and a residue signal [40].", "startOffset": 144, "endOffset": 148}, {"referenceID": 37, "context": "There are two properties associated with IMFs such that (1) the numbers of zero\u2013crossings and extrema present in IMFs are same, and (2) IMFs are symmetric with respect to the local mean [40].", "startOffset": 186, "endOffset": 190}, {"referenceID": 33, "context": "In other words, EMD detects and extracts the highest frequency component in the signal [34], [41]\u2013[47]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k.", "startOffset": 87, "endOffset": 91}, {"referenceID": 38, "context": "In other words, EMD detects and extracts the highest frequency component in the signal [34], [41]\u2013[47]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k.", "startOffset": 93, "endOffset": 97}, {"referenceID": 44, "context": "In other words, EMD detects and extracts the highest frequency component in the signal [34], [41]\u2013[47]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k.", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "The testing phase is important for evaluating the generalization capability of the trained network [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 45, "context": "The tangent sigmoid transfer function is an automatic choice for researchers for use in hidden layer to achieve the bi-directional swing [48], [49].", "startOffset": 137, "endOffset": 141}, {"referenceID": 46, "context": "The tangent sigmoid transfer function is an automatic choice for researchers for use in hidden layer to achieve the bi-directional swing [48], [49].", "startOffset": 143, "endOffset": 147}, {"referenceID": 26, "context": "The activation function used in the output layer is log-sigmoid which is nonsymmetric in nature and facilitates faster learning rate [27].", "startOffset": 133, "endOffset": 137}, {"referenceID": 47, "context": "Finally, a network with single hidden layer is trained using the Scaled Conjugate Gradient (SCG) Back propagation Algorithm [50].", "startOffset": 124, "endOffset": 128}, {"referenceID": 48, "context": "The SCG method has been used to solve problems in different research domains such as prediction of groundwater level [51], and telemarketing success [52], etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 49, "context": "The SCG method has been used to solve problems in different research domains such as prediction of groundwater level [51], and telemarketing success [52], etc.", "startOffset": 149, "endOffset": 153}, {"referenceID": 50, "context": "have been part of the pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58].", "startOffset": 99, "endOffset": 103}, {"referenceID": 51, "context": "have been part of the pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58].", "startOffset": 115, "endOffset": 119}, {"referenceID": 52, "context": "have been part of the pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58].", "startOffset": 132, "endOffset": 136}, {"referenceID": 53, "context": "have been part of the pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58].", "startOffset": 138, "endOffset": 142}, {"referenceID": 16, "context": "have been part of the pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58].", "startOffset": 166, "endOffset": 170}, {"referenceID": 54, "context": "have been part of the pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58].", "startOffset": 172, "endOffset": 176}, {"referenceID": 55, "context": "have been part of the pre-processing technique in different fields such as environmental modelling [53], chemistry [54], biomedical [55], [56], reservoir engineering [17], [57], [58].", "startOffset": 178, "endOffset": 182}, {"referenceID": 54, "context": "[57] has compared the performance of three machine learning techniques such as SVM, multilayer perceptron (MLP), and Co-Active Neuro-Fuzzy Inference System (CANFIS) to predict permeability from well logs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "However, integration of seismic and borehole dataset are not required since only well logs are used as the predictor variables in [57].", "startOffset": 130, "endOffset": 134}, {"referenceID": 54, "context": "The methodology used to model the original SF is almost similar to the methodology adopted for the permeability prediction using multilayer perceptron as in [57].", "startOffset": 157, "endOffset": 161}, {"referenceID": 54, "context": "Comparison between the validation performance (prediction using data not part of training) reported in [57] and those achieved in this study has revealed that the regularization stage improves the prediction result in terms of CC, AEM, and RMSE.", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "The pre-processing step involving a single ANN in [17] is similar to the case in the present paper while modelling with original SF as target.", "startOffset": 50, "endOffset": 54}, {"referenceID": 56, "context": "In case of order statistics filters, the filtered output is dependent on the ordering (ranking) of the pixels in the image area contained by the selected window [59], [60].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "The comparison among the results achieved in this work and existing literatures [17], [57] reveals the superiority of the proposed regularization step to obtain improved performance in terms of the performance evaluators.", "startOffset": 80, "endOffset": 84}, {"referenceID": 54, "context": "The comparison among the results achieved in this work and existing literatures [17], [57] reveals the superiority of the proposed regularization step to obtain improved performance in terms of the performance evaluators.", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "The present work differs from the work done in [17] in terms of pre-processing stage, division of training-testing dataset, adopted machine learning technique, and post-processing algorithm.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "In addition the well tops and horizon information are used [17] to carry out zone wise division of the overall dataset and modular ANN is applied to model SF from multiple seismic attributes.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "The performance attained in [17] would be improved with the use of regularized SF log as target instead of original one.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "The changes only take place in the activation potentials and output of the neurons [27].", "startOffset": 83, "endOffset": 87}, {"referenceID": 47, "context": "The steps associated with the SCG can be presented as follows [50]:", "startOffset": 62, "endOffset": 66}], "year": 2015, "abstractText": "This paper presents a novel pre-processing scheme to improve the prediction of sand fraction from multiple seismic attributes such as seismic impedance, amplitude and frequency using machine learning and information filtering. The available well logs along with the 3-D seismic data have been used to benchmark the proposed pre-processing stage using a methodology which primarily consists of three steps: preprocessing, training and post-processing. An Artificial Neural Network (ANN) with conjugate-gradient learning algorithm has been used to model the sand fraction. The available sand fraction data from the high resolution well logs has far more information content than the low resolution seismic attributes. Therefore, regularization schemes based on Fourier Transform (FT), Wavelet Decomposition (WD) and Empirical Mode Decomposition (EMD) have been proposed to shape the high resolution sand fraction data for effective machine learning. The input data sets have been segregated into training, testing and validation sets. The test results are primarily used to check different network structures and activation function performances. Once the network passes the testing phase with an acceptable performance in terms of the selected evaluators, the validation phase follows. In the validation stage, the prediction model is tested against unseen data. The network yielding satisfactory performance in the validation stage is used to predict lithological properties from seismic attributes throughout a given volume. Finally, a post-processing scheme using 3-D spatial filtering is implemented for smoothing the sand fraction in the volume. Prediction of lithological properties using this framework is helpful for Reservoir Characterization.", "creator": null}}}