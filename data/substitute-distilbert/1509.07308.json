{"id": "1509.07308", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", "abstract": "we propose a new model for learning bilingual word representations from non - parallel tree - aligned data. following the recent exploration in word representation learning, our mouse learns dense real - valued interpretation vectors, blended accompany, bilingual word embeddings ( bwes ). unlike prior work techniques inducing bwes which heavily leans on parallel sentence - aligned corpora and / or readily available translation resources such small dictionaries, the article reveals that bwes may be learned well on the basis of document - aligned comparable data without sharing specialised lexical resources nor syntactic information. we present a comparison, our approach with previous state - of - it - art models efficiently measuring bilingual word representations. comparable data that rely on the framework of multilingual probabilistic topic modeling ( muptm ), as well as with distributional local context - focusing models. we demonstrate the utility of the induced ensemble in comparative semantic tasks : ( 1 ) bilingual lexicon extraction, ( 2 ) suggesting word translations in context for consecutive words. our simple yet effective dictionary - based models significantly outperform the muptm - based and context - counting representation models from living organisms as well as prior bwe - based models, and recover the best reported results on both tasks for all three simultaneous language pairs.", "histories": [["v1", "Thu, 24 Sep 2015 11:00:04 GMT  (391kb,D)", "http://arxiv.org/abs/1509.07308v1", null], ["v2", "Sun, 28 Feb 2016 12:47:15 GMT  (419kb,D)", "http://arxiv.org/abs/1509.07308v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ivan vuli\\'c", "marie-francine moens"], "accepted": false, "id": "1509.07308"}, "pdf": {"name": "1509.07308.pdf", "metadata": {"source": "CRF", "title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", "authors": ["Ivan Vuli\u0107"], "emails": ["ivan.vulic@cs.kuleuven.be", "marie-francine.moens@cs.kuleuven.be"], "sections": [{"heading": "1. Introduction", "text": "A huge body of work in distributional semantics and word representation learning almost exclusively revolves around the distributional hypothesis (Harris, 1954) - an idea which states that similar words occur in similar contexts. All current corpus-based approaches to semantics rely on the contextual evidence in one way or another. Roughly speaking, word representations are typically learned using these two families of distributional context-based models: (1) global matrix factorization models such as latent semantic analysis (LSA) (Landauer & Dumais, 1997) or generative probabilistic models such as latent Dirichlet allocation (LDA) (Blei, Ng, & Jordan, 2003), which model the word co-occurrence at the document or paragraph level; or (2) local context window models that represent words as sparse high-dimensional context vectors, and model the word co-occurrence at the level of selected neighboring words (Turney & Pantel, 2010), or generative probabilistic models that learn the probability distribution of a vocabulary word in the context window as a latent variable (Deschacht & Moens, 2009; Deschacht, De Belder, & Moens, 2012).\nOn the other hand, dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (e.g., Bengio, Ducharme, Vincent, & Janvin, 2003; Collobert & Weston, 2008; Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have been introduced recently, first as part of neural network based archi-\nc\u00a9xxxx AI Access Foundation. All rights reserved.\nar X\niv :1\n50 9.\n07 30\n8v 1\n[ cs\n.C L\n] 2\n4 Se\np 20\ntectures for statistical language modeling. WEs serve as richer and more coherent word representations than the ones obtained by the aforementioned traditional distributional semantic models, with illustrative comparative studies available in the recently published relevant work (e.g., Mikolov, Yih, & Zweig, 2013d; Baroni, Dinu, & Kruszewski, 2014; Levy, Goldberg, & Dagan, 2015).\nA natural extension of interest from monolingual to multilingual word embeddings has occurred recently (e.g., Klementiev, Titov, & Bhattarai, 2012; Hermann & Blunsom, 2014b). When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared bilingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These BWEs may then be used in a myriad of multilingual natural language processing tasks and beyond, such as fundamental tasks leaning on such bilingual meaning representations, e.g., computing cross-lingual and multilingual semantic word similarity and extracting bilingual word lexicons using the induced bilingual embedding space (see fig. 1). However, all these models critically require sentence-aligned parallel data and readily-available translation dictionaries to induce bilingual word embeddings (BWEs) that are consistent and closely aligned over different languages.\nContributions. To the best of our knowledge, this article presents the first work to showcase that bilingual word embeddings may be induced directly on the basis of comparable data without any additional bilingual resources such as sentence-aligned parallel data or translation dictionaries. The focus is on document-aligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links, news texts discussing the same theme).\nOur new bilingual distributed representation learning model makes use of pseudo-bilingual documents constructed by merging the content of two coupled documents from a document pair, where we propose and evaluate two different strategies on how to construct such pseudo-bilingual documents: (1) merge and randomly shuffle strategy which randomly permutes words from both languages in each pseudo-bilingual document, and (2) length-ratio shuffle strategy, a deterministic method that retains monolingual word order while intermingling the words cross-lingually. These additional pre-training shuffling strategies ensure that both source language words and target language words occur in the contexts of each source and target language word. A monolingual model such as skip-gram with negative sampling from the word2vec package (Mikolov, Sutskever, Chen, Corrado, & Dean, 2013c) is then trained on these \u201cshuffled\u201d pseudo-bilingual documents. By this procedure, we steer semantically similar words from different languages towards similar representations in the shared bilingual embedding space, and effectively use available bilingual contexts instead of monolingual ones. The model treats documents as bags-of-words (i.e., it does not include any syntactic information) and does not even rely on any sentence boundary information. In summary, the main contributions of this article are:\n(1) We present the first model that induces bilingual word embeddings directly from document-aligned non-parallel data. We test and evaluate two main variants of the model based on the pre-training shuffling step. (2) We provide a qualitative and quantitative analysis of the model. We draw analogies and comparisons with prior work on inducing word representations from the same data type: document-aligned comparable corpora (e.g., models relying on the multilingual probabilistic\ntopic modeling framework (MuPTM)). (3) We demonstrate the utility of induced BWEs at the word type level in the task of bilingual lexicon extraction (BLE) from Wikipedia data (Vulic\u0301 & Moens, 2013b) for three language pairs. A BLE model based on our BWEs significantly outperforms MuPTM-based and context-counting BLE models, and acquires the best reported scores on the benchmarking BLE datasets. (4) We demonstrate the utility of induced BWEs at the word token level in the task of suggesting word translations in context (SWTC) (Vulic\u0301 & Moens, 2014) for the same three language pairs. A SWTC model based on our BWEs again significantly outscores the best scoring MuPTM-based SWTC models in the same setting without any use of parallel data and translation dictionaries, and again acquires the best reported results on the benchmarking SWTC datasets. (5) We also present a comparison with state-of-the-art BWE induction models (Mikolov, Le, & Sutskever, 2013b; Gouws, Bengio, & Corrado, 2015; Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, & Saha, 2014) in BLE and SWTC. Results reveal that our simple yet effective approach is on-par with or outperforms other BWE induction models that rely on parallel data or readily available dictionaries to learn shared bilingual embedding spaces."}, {"heading": "2. Related Work", "text": "In this section we further motivate why we opt for building a model for inducing bilingual word embeddings from comparable data. For a clearer overview, we have split related work into three broad clusters: (1) monolingual word embeddings, (2) bilingual word embeddings, and (3) bilingual word representations from comparable data."}, {"heading": "2.1 Monolingual Word Embeddings", "text": "The idea of representing words as continuous real-valued vectors dates way back to mid80s (Rumelhart, Hinton, & Williams, 1986; Elman, 1990). The idea met its resurgence a decade ago (Bengio et al., 2003), where a neural language model learns word embeddings as part of a neural network architecture for statistical language modeling. This work inspired other approaches that learn word embeddings within the neural-network language modeling framework (Collobert & Weston, 2008; Collobert, Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011). Word embeddings are tailored to capture semantics and encode a continuous notion of semantic similarity (as opposed to semantically poorer discrete representations), necessary to share information between words and other text units.\nRecently, the skip-gram and continuous bag-of-words (CBOW) model from Mikolov et al. (2013a, 2013c) revealed that the full neural-network structure is not needed at all to learn high-quality word embeddings (with extremely decreased training times compared to the full-fledged neural network models, see Mikolov et al.\u2019s (2013a) work for the full analysis of complexity of the models). These models are in fact simple single-layered architectures, where the objective is to predict a word\u2019s context given the word itself (skip-gram) or predict a word given its context (CBOW). Similar models called vector log-bilinear models were recently proposed (Mnih & Kavukcuoglu, 2013). Other models inspired by skip-gram and CBOW are GloVe (Global Vectors for Word Representation) (Pennington et al., 2014), which combines local and global contexts of a word into a unified model, and a model which relies on dependency-based contexts instead of simpler word-based contexts (Levy &\nGoldberg, 2014a), and new models are steadily emerging (e.g., Lebret & Collobert, 2014; Lu, Wang, Bansal, Gimpel, & Livescu, 2015; Stratos, Collins, & Hsu, 2015; Trask, Gilmore, & Russell, 2015; Liu, Jiang, Wei, Ling, & Hu, 2015).\nAn interesting finding has been discussed recently (Levy & Goldberg, 2014b): the popular skip-gram model with negative sampling (SGNS) (Goldberg & Levy, 2014) is simply a model which implicitly factorizes a word-context matrix, with its cells containing pointwise mutual information (PMI) scores of the respective word and context pairs, shifted by a global constant. In other words, the SGNS performs exactly the same thing as traditional distributional models (i.e., context counting plus context weighting and/or dimensionality reduction), with a slight improvement in performance with SGNS (Baroni et al., 2014; Levy et al., 2015).\nAll these low-dimensional vectors, besides improving computational efficiency, lead to better generalizations, even allowing to generalize over the vocabularies observed in labelled data, and hence partially alleviating the ubiquitous problem of data sparsity. Their utility has been validated and proven in various semantic tasks such as semantic word similarity, synonymy detection or word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014; Pennington et al., 2014). Moreover, word embeddings have been proven to serve as useful unsupervised features for plenty of downstream NLP tasks such as named entity recognition, chunking, semantic role labeling, part-of-speech tagging, or selectional preferences (Turian, Ratinov, & Bengio, 2010; Collobert et al., 2011).\nDue to its simplicity, as well as its efficacy and consequent popularity in various tasks (Mikolov et al., 2013c; Levy & Goldberg, 2014b), with a clear advantage on similarity tasks when compared to traditional models from distributional semantics (Levy et al., 2015) in this article we will focus on the adaptation of the skip-gram model with negative sampling (Mikolov et al., 2013c). In sect. 3, we provide a very brief overview of the model, and then follow up with our new bilingual model which is based on SGNS."}, {"heading": "2.2 Bilingual Word Embeddings", "text": "Bilingual word representations could serve as an useful source knowledge for problems in cross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vulic\u0301, De Smet, & Moens, 2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification (Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014; Vulic\u0301, De Smet, Tang, & Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vulic\u0301 & Moens, 2013a), or knowledge transfer and annotation projection from resource-rich to resource-poor languages for a myriad of NLP tasks (Yarowsky & Ngai, 2001; Pado\u0301 & Lapata, 2009; Peirsman & Pado\u0301, 2010; Das & Petrov, 2011; Ta\u0308ckstro\u0308m, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agic\u0301, & Nivre, 2014). Moreover, by making the transition from monolingual to bilingual settings and building a shared bilingual embedding space (see again fig. 1 for an illustrative example), one is able to extend or rather generalize semantic tasks such as semantic similarity computation, synonymy detection or word analogy computation across languages. Following the success in monolingual settings, a body of recent work on word representation learning has therefore focused on learning bilingual word embeddings (BWEs).\nThe current research on inducing BWEs critically relies on sentence-aligned parallel data or readily available bilingual lexicons to achieve the coherence of representations across languages (e.g., to build similar representations for similar concepts in different languages such as January-januari, dog-hund or sky-hemel). We may cluster the current work in three different groups: (1) the models that rely on hard word alignments obtained from parallel data to constrain the learning of BWEs (Klementiev et al., 2012; Zou, Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014); (2) the models that use the alignment of parallel data at the sentence level (Koc\u030cisky\u0301, Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi, Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) the models that critically require readily available bilingual lexicons (Mikolov et al., 2013b; Faruqui & Dyer, 2014; Xiao & Guo, 2014). The main disadvantage of all these models is the limited availability of parallel data and bilingual lexicons, resources which are scarce and/or domain-restricted for plenty of language pairs. In this work, we significantly alleviate the requirements: unlike prior work, we show that BWEs may be induced solely on the basis of document-aligned comparable data without any additional need for parallel data or bilingual lexicons.1"}, {"heading": "2.3 Bilingual Word Representations from Document-Aligned Data", "text": "Prior work on inducing bilingual word representations in the early days followed the tradition of window-based context-counting distributional models (Rapp, 1999; Gaussier, Renders, Matveeva, Goutte, & De\u0301jean, 2004; Laroche & Langlais, 2010) and it again required a bilingual lexicon as a critical resource. In order to tackle this issue, recent work relies on the knowledge-lighter framework of multilingual probabilistic topic modeling (MuPTM)\n1. In theory, the work from (Hermann & Blunsom, 2014b; Chandar et al., 2014) may also be extended to the same setting with document-aligned data, as these two models originally rely on sentence embeddings computed as aggregations over their single word embeddings plus sentence alignments. However, some preliminary studies show that these models do not work well in practice after replacing the very strong bilingual signal coded in parallel sentences with the noisy bilingual signal given by document alignments and non-parallel data.\n(Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Boyd-Graber & Blei, 2009; De Smet & Moens, 2009; Ni, Sun, Hu, & Chen, 2009; Zhang, Mei, & Zhai, 2010; Fukumasu, Eguchi, & Xing, 2012) or other similar models for latent structure induction (Haghighi, Liang, Berg-Kirkpatrick, & Klein, 2008; Daume\u0301 III & Jagarlamudi, 2011).\nWords in this setting are represented as real-valued vectors with conditional topic probability scores P (zk|wi), regardless of their actual language. Topics zk are in fact latent inter-lingual concepts discovered directly from multilingual comparable data using a multilingual topic model such as bilingual LDA. We discuss the MuPTM-based representations in more detail in sect. 3.3.\nMuPTM-based bilingual word representations induced from comparable data have demonstrated its utility in tasks such as cross-lingual semantic similarity computation and bilingual lexicon extraction (Vulic\u0301, De Smet, & Moens, 2011; Liu, Duh, & Matsumoto, 2013) and suggesting word translations in context (Vulic\u0301 & Moens, 2014). In this work, we compare the state-of-the-art MuPTM-based word representations induced from the same type of comparable corpora with BWEs learned by our new model in these two semantic tasks."}, {"heading": "3. Model Architecture", "text": "Our new bilingual model is an extension of SGNS to bilingual settings with documentaligned comparable training data. After describing SGNS and our bilingual BWE model, we provide a comparison of our new model to other baseline representations from documentaligned comparable data in terms of modeling assumptions, training complexity and final output. Finally, we give a short introduction to computing semantic similarity and building composite representations beyond the level of words, two fundamental concepts required to utilize induced BWEs in various semantic tasks, such as bilingual lexicon extraction or suggesting word translations in context."}, {"heading": "3.1 Skip-Gram with Negative Sampling (SGNS)", "text": "Our departure point is the log-linear SGNS from Mikolov et al. (2013c) as implemented in the word2vec package.2 The SGNS model learns word embeddings (WEs) in a similar way to neural language models (Bengio et al., 2003; Collobert & Weston, 2008), but without a non-linear hidden layer.\nIn the monolingual setting, we assume one language L with vocabulary V , and a corpus of words w \u2208 V , along with their contexts c \u2208 V c, where V c is the context vocabulary. Contexts for each word wn are typically neighboring words in a context window of size cs (i.e., wn\u2212cs, . . . , wn\u22121, wn+1, . . . , wn+cs), so effectively it holds V\nc \u2261 V .3 Each word type w \u2208 V is associated with a vector ~w \u2208 Rd (its pivot word representation or pivot word embedding, see fig. 2), and a vector ~wc \u2208 Rd (its context embedding). d is the dimensionality of the WE vectors, which, as a model input parameter, has to be set in advance before the training procedure commences. The entries in these vectors are latent, and treated as parameters \u03b8 to be learned by the model. In short, the idea of the\n2. https://code.google.com/p/word2vec/ 3. Testing other options for context selection such as dependency-based contexts (Levy & Goldberg, 2014a)\nis beyond the scope of this work, and it was shown that these contexts may not lead to any gains in the final WEs (Kiela & Bottou, 2014).\nskip-gram model is to scan through the corpus (which is typically unannotated, Mikolov et al., 2013a) word by word in turn (i.e., these are the pivot words), and learn from the pairs (word, context). The learning goal is to maximize the ability of predicting context words for each pivot word in the corpus. Let ob = 1 denote that the pair (w, v) is observed in the corpus and thus belongs to the training set D. The probability of (w, v) \u2208 D is defined by the softmax function:\nP (ob = 1|w, v, \u03b8) = 1 1 + exp(\u2212~w \u00b7 ~vc)\n(1)\nEach word token w in the corpus is treated in turn as the pivot and all pairs of word tokens (w,w \u00b1 1),...,(w,w \u00b1 t(cs)) are appended to D, where t(cs) is an integer sampled from a uniform distribution on {1, . . . , cs}.4 The global training objective J is then to maximize the probabilities that all pairs from D are indeed observed in the corpus:\nJ = arg max \u03b8 \u2211 (w,v)\u2208D log 1 1 + exp(\u2212~w \u00b7 ~vc) (2)\nwhere \u03b8 are the parameters of the model, that is, pivot and context word embeddings which have to be learned. One may see that this objective function has a trivial solution by setting ~w = ~vc, and ~w \u00b7 ~vc = V al, where V al is a large enough number (Goldberg & Levy, 2014). In order to prevent this trivial training scenario, the negative sampling procedure comes into the picture (Collobert & Weston, 2008; Mikolov et al., 2013c).\nIn short, the idea behind negative sampling is to present the model with a set D\u2032 of artificially created or sampled \u201cnegative pivot-context\u201d pairs (w, v\u2032), which by assumption serve as negative examples, that is, they do not occur as observed/positive (word, context) pairs in the training corpus. The model then has to adjust the parameters \u03b8 in such a way to also maximize the probability that these negative pairs will not occur in the corpus. While the interested reader may find further details about the negative sampling procedure, and the new exact objective function along with its derivation elsewhere (Levy & Goldberg, 2014b), for illustrative purposes and simplicity, here we present the approximative objective function with negative sampling by Goldberg and Levy (2014):\nJ = arg max \u03b8 \u2211 (w,v)\u2208D log 1 1 + exp(\u2212~w \u00b7 ~vc) + \u2211 (w,v\u2032)\u2208D\u2032 log 1 1 + exp(~w \u00b7 ~v\u2032c) (3)\nThe free parameters \u03b8 are updated using stochastic gradient descent and backpropagation, with learning rate typically controlled by Adagrad (Duchi, Hazan, & Singer, 2011) or with a global linearly decreasing learning rate. By optimizing the objective from eq. (3), the model incrementally pushes observed pivot WEs towards context WEs of their collocates in the corpus. In the words of distributional hypothesis - after training, words that occur in similar contexts should end up having similar word embeddings. In other words, to link the terminology of distributional hypothesis and the modeling assumptions of SGNS - words that predict similar contexts end up having similar word embeddings.\n4. The original skip-gram model utilizes dynamic window sizes, where cs denotes the maximum window size. Moreover, the model takes into account sentence boundaries in context selection, that is, it selects as context words only words occurring in the same sentence as the pivot word."}, {"heading": "3.2 Final Model - BWESG: BWE Skip-Gram", "text": "In the next step, we propose a novel method to extend SGNS to work with bilingual document-aligned comparable data. Let us assume that we possess a document-aligned comparable corpus, defined as C = {d1, d2, . . . , dN} = {(dS1 , dT1 ), (dS2 , dT2 ), . . . , (dSN , dTD)}. dj = (d S j , d T j ) denotes a pair of aligned documents in the source language LS and the target language LT respectively, and N is the number of documents in the corpus. V S and V T are vocabularies associated with languages LS and LT . The goal is to learn a shared bilingual embedding space given the data (fig. 1) and document alignments as the only bilingual signal during training. We present two strategies that, coupled with SGNS, lead to such shared bilingual spaces. An overview of the architecture for learning BWEs from document-aligned comparable data with the two strategies is given in fig. 2(a) and fig. 2(b).\n(1) Merge and Shuffle. In the first step, we merge two documents dSj and d T j\nfrom the aligned document pair dj into a single \u201cpseudo-bilingual\u201d document d \u2032 j . Following that, we randomly shuffle the newly constructed pseudo-bilingual document. A shuffle is a (random) permutation of the word tokens given in two different languages forming the pseudo-bilingual document. The pre-training shuffling step (see fig. 2(a)) assures that each word w, regardless of its actual language, obtains word collocates from both vocabularies. The idea of obtaining bilingual contexts for each pivot word in each pseudo-bilingual document will steer the final model towards constructing a shared bilingual space. Since the model depends on the alignment at the document level, in order to ensure the bilingual contexts instead of monolingual contexts, it is intuitive to assume that larger window sizes will lead to better bilingual embeddings. We test this hypothesis and the effect of window size in sect. 5.3. In another interpretation, since the model relies only on (pseudo-bilingual) document level co-occurrence, the window size parameter then just controls the amount of random data dropout, that is, the number of positive document-level training examples. The locality of SGNS is not preserved due to the shuffling procedure.\n(2) Length-Ratio Shuffle. The non-deterministic and uncontrollable nature of the merge and shuffle procedure opens up a possibility of accidentally obtaining \u201cbad shuffles\u201d that will result in sub-optimal word representations. Therefore, we also propose a deterministic strategy for building pseudo-bilingual documents suitable for bilingual training. Source and target language words are inserted into an (initially empty) pseudo-bilingual document in turn based on the ratio of document lengths, with word order preserved. Document lengths are measured in terms of word tokens, and let us denote them as mS and mT for an aligned document pair (d S j , d T j ). Let us assume, without loss of generality, that mS \u2265 mT . The procedure then proceeds as follows (if mT > mS the procedure proceeds in an analogous manner with the roles of dSj and d T j reversed):\n1. Pseudo-bilingual document d\u2032j is empty: d \u2032 j = {}. 2. Compute the ratio: R = bmSmT c. 3. Scan through aligned documents dS and dT simultaneously and (3.1) append R word\ntokens from dSj into d \u2032 j ; then (3.2) append 1 word token from d T j . Repeat steps 3.1 and 3.2 until all word tokens from dTj have been inserted into d \u2032 j .\n4. Insert remaining mS mod mT word tokens from d S j into d \u2032 j .\nUsing a simple example, assume that we have an English (EN) document {Frodo, Sam, orcs, goblins,Mordor, ring} and a Spanish (ES) document {anillo, orcos,mago}: the pseudobilingual document would be formed by inserting 1 Spanish word after 2 English words (as the length ratio is 6:3 = 2:1). The final pseudo-bilingual document is: {FrodoEN , SamEN , anilloES , orcsEN , goblinsEN , orcosES ,MordorEN , ringEN ,magoES}.\nIn another interpretation, the length-ratio shuffle strategy constructs a single permutation/shuffle of the pseudo-bilingual document controlled by the word order in two aligned documents as well as their length ratio. As before, the model relies on pseudo-bilingual document level co-occurrence, and the window size parameter controls the amount of (now\nnon-random) data dropout. A difference lies in the fact that this procedure now keeps word order intact monolingually while constructing a pseudo-bilingual document.\nThe final BWE Skip-gram (BWESG) model then relies on the monolingual variant of SGNS (or any other monolingual WE induction model) trained on these shuffled/permuted pseudo-bilingual documents (using strategies (1) or (2)).5 The model learns word embeddings for source and target language words aligned over the d shared embedding dimensions. The BWESG-based representation of word w, regardless of its actual language, is then a d-dimensional vector: ~w = [f1, . . . , fk, . . . , fd]. fk \u2208 R denotes the score for the k-th shared inter-lingual feature within the d-dimensional shared bilingual embedding space. Since all words share the embedding space, semantic similarity between words may be computed both monolingually and across languages. We will extensively use this property in our evaluation tasks."}, {"heading": "3.3 A Comparison of BWESG with Baseline Representation Models", "text": "In the following three subsections, we quickly navigate through other approaches to word representation learning from document-aligned comparable data."}, {"heading": "3.3.1 Basic-MuPTM", "text": "The early approaches (e.g., Dumais, Landauer, & Littman, 1996; Carbonell, Yang, Frederking, Brown, Geng, Lee, Frederking, E, Geng, & Yang, 1997) tried to mine topical structure from document-aligned comparable texts using a monolingual topic model (e.g., LSA or LDA) trained on pseudo-bilingual documents with the target document simply appended to its source language counterpart, and then used the discovered latent topical structure as a shared semantic space in which both words and documents from two languages may be represented in a uniform way.\nMore recent work on multilingual probabilistic topic modeling (MuPTM) (Mimno et al., 2009; De Smet & Moens, 2009; Vulic\u0301 et al., 2011) showed that word representations of higher quality may be built if a multilingual topic model such as bilingual LDA (BiLDA) is trained jointly on document-aligned comparable corpora by retaining the structure of the corpus intact (i.e., there is no need to construct pseudo-bilingual documents).\nMuPTM discovers the latent structure of the observed data in the form of K latent cross-lingual topics z1, . . . , zK which optimally describe the generation of observed data. Extracting latent cross-lingual topics actually implies learning per-document topic distributions for each document in the corpus (probability scores P (zk|dj)), and discovering language-specific representations of these topics given by per-topic word distributions in each language (probability scores P (wSi |zk) and P (wTi |zk)). Latent cross-lingual topics are in fact distributions over vocabulary words, and have their language-specific representation in each language. Per-document topic distributions and per-topic word distributions are obtained after training the topic model on multilingual data. The representation of some word w \u2208 V S (or in an analogous manner w \u2208 V T ) is then a K-dimensional vector: ~w = [P (z1|w), . . . , P (zk|w), . . . , P (zK |w)].\nWe call this representation model (RM) Basic-MuPTM. Since the number of topics, that is, the number of vector dimensions K is typically high (Dinu & Lapata, 2010; Vulic\u0301 et al.,\n5. We were also experimenting with GloVe and CBOW, but they were falling short of SGNS on average.\n2011), additional feature pruning (Reisinger & Mooney, 2010) may be employed in order to retain only the most descriptive dimensions in the MuPTM-based representation, which was shown to improve the performance on several semantic tasks (e.g., BLE or SWTC) (Vulic\u0301 & Moens, 2013a; Vulic\u0301 et al., 2015).\nA multilingual topic model is typically trained by Gibbs sampling (Geman & Geman, 1984; Steyvers & Griffiths, 2007; Vulic\u0301 et al., 2015). Similar to the SGNS/BWESG training procedure, Gibbs sampling for MuPTM/BiLDA also scans the training corpus word by word, and then cyclically updates topic assignments for each word token. However, unlike BWESG which uses only a subset of document-level training examples, Gibbs sampling for MuPTM uses all words from the source language document as well as all words from its coupled target language document to influence the topic assignment for the pivot word. The BWESG design relying on data dropout leads to decreased training times and computation costs to obtain final representations compared to Basic-MuPTM."}, {"heading": "3.3.2 Association-MuPTM", "text": "Another baseline representation is also based on the MuPTM framework, but it contains association scores P (wa|w) for each w,wa \u2208 V S\u222aV T (Vulic\u0301 & Moens, 2013a) as dimensions of real-valued word vectors. These association scores are computed as P (wa|w) = \u2211K k=1 P (wa|zk)P (zk|w) (Griffiths, Steyvers, & Tenenbaum, 2007), and the word representation is a (|V S |+|V T |)-dimensional vector: ~w = [P (wS1 |w), . . . , P (wS|V S ||w), P (w T 1 |w), . . . , P (wT|V T ||w)]. As with Basic-MuPTM, the original word representation may also be pruned post-hoc. We call this representation model Association-MuPTM. Since this approach relies on the MuPTM training plus additional |V S | \u00b7 |V T | computations to estimate association scores, the cost of obtaining Association-MuPTM representations is even higher than for BasicMuPTM, but it leads to more robust word representations for the BLE task (Vulic\u0301 & Moens, 2013a). While both Basic-MuPTM and Association-MuPTM produce high-dimensional real-valued vectors with plenty of near-zero dimensions (the number of dimensions is typically measured in thousands) which have to be pruned afterwards with the pruning parameter often set ad-hoc, BWESG produces lower-dimensional dense real-valued vectors, and no additional post-hoc feature pruning is required for BWESG."}, {"heading": "3.3.3 Traditional-PPMI", "text": "Finally, a traditional approach to building bilingual word representations in (cross-lingual) distributional semantics is to compute weighted co-occurrence scores (e.g., using PMI, TFIDF) between pivot words and their context words in a window of predefined size, plus an external bilingual lexicon to align context words/dimensions across languages (Gaussier et al., 2004; Laroche & Langlais, 2010). A weighting function (WeF), which is a standard choice in distributional semantics and yields optimal or near-optimal results over a group of semantic tasks (Bullinaria & Levy, 2007), is the smoothed positive pointwise mutual information statistic (Pantel & Lin, 2002; Turney & Pantel, 2010). Furthermore, in order to induce context words without the need for a readily available lexicon, we employ the bootstrapping procedure from (Peirsman & Pado\u0301, 2011; Vulic\u0301 & Moens, 2013b). This representation model is called Traditional-PPMI. The word representation is an R-dimensional vector: ~w = [sc1(w, c1), . . . , sck(w, ck), . . . , scR(w, cR)]. The dimensions of the vector space\nare R one-to-one word translation pairs ck = (c S k , c T k ), and sck(w, ck) is the weighted cooccurrence score of the pivot word w and the k-th context feature, where one computes the co-occurrence score using cSk if w \u2208 V S , or cTk if w \u2208 V T .\nVector dimensions ck = (c S k , c T k ) in the Traditional-PPMI representation and similar models with other WeFs are typically the most frequent and reliable translation pairs in the corpus. As opposed to BWESG, the obtained word vectors are again high-dimensional (typically thousands of dimensions) sparse real-valued vectors. In addition, traditionalPPMI is a purely local distributional model deriving distributional context knowledge from narrow context windows (typically 3-10 surrounding words, e.g., Laroche & Langlais, 2010). A bootstrapping approach (Vulic\u0301 & Moens, 2013b) which we use to induce the TraditionalPPMI representation starts from a seed lexicon of one-to-one translation pairs obtained using some other model (e.g., Basic-MuPTM or Association-MuPTM), and then gradually detects new dimensions of the shared bilingual semantic space. We refer the interested reader to the relevant literature (Vulic\u0301 & Moens, 2013b) for more details."}, {"heading": "3.4 From Word Representations to Semantic Word Similarity", "text": "Assume now that we have induced bilingual word representations, regardless of the chosen RM. Given two words wi and wj , irrespective to their actual language, we may compute the degree of their semantic similarity by applying a similarity function (SF) on their vector representations \u2212\u2192wi and \u2212\u2192wj : sim(wi, wj) = SF (\u2212\u2192wi,\u2212\u2192wj). Different choices (or rather families of) SFs are cosine, the Kullback-Leibler or the Jensen-Shannon divergence, the Hellinger distance, the Jaccard index, etc. (Lee, 1999; Cha, 2007), and different RMs typically require different SFs to produce optimal or near-optimal results over various semantic tasks. When working with word embeddings, a standard choice for SF is cosine similarity (cos) (Mikolov et al., 2013c), which is also a typical choice in traditional distributional models (Bullinaria & Levy, 2007). The similarity is then computed as follows:\nsim(wi, wj) = cos(wi, wj) = \u2212\u2192wi \u00b7 \u2212\u2192wj |\u2212\u2192wi| \u00b7 |\u2212\u2192wj |\n(4)\nOn the other hand, a good choice for SF when working with probabilistic RMs such as Basic-MuPTM and Association-MuPTM RS is the Hellinger distance (Pollard, 2001; Cha, 2007; Kazama, Saeger, Kuroda, Murata, & Torisawa, 2010), which displays excellent results in the BLE task (Vulic\u0301 & Moens, 2013a). The similarity between words wi and wj using the Hellinger distance is computed as follows:\nsim(wi, wj) = 1\u221a 2 \u221a\u221a\u221a\u221a K\u2211 i=1 (\u221a P (f \u2032k|wi)\u2212 \u221a P (f \u2032k|wj) )2 (5)\nNote that the Hellinger distance is applicable only if word representations are probability distributions, which is the case for Basic-MuPTM and Association-MuPTM. P (f \u2032k|wi) denotes the probability score for the k-th dimension (f \u2032k) in the vector representation with Basic-MuPTM or Association-MuPTM.\nFor each word wi, we can build a ranked list RL(wi) which consists of all other words wj ranked according to their respective semantic similarity scores sim(wi, wj). Additionally,\nwe label the ranked list RL(wi) that is pruned at position M as RLM (wi). Since we may retain language labels for words when training in multilingual settings (e.g., language labels are marked by different colors in fig. 2), we may compute: (1) monolingual similarity, e.g., given wi \u2208 V S , we retain only wj \u2208 V S in the ranked list (analogous for wi \u2208 V T ), (2) cross-lingual similarity (CLSS), e.g., given wi \u2208 V S , we retain only wj \u2208 V T , and (3) multilingual similarity, where we retain all words wj \u2208 V S \u222a V T . When computing CLSS for wi, the most similar word cross-lingually is called the cross-lingual nearest neighbor.\nWe will employ the models of context-insensitive CLSS at the word type level to extract bilingual lexicons from comparable document-aligned data, and to compare all representation models in the BLE task from document-aligned comparable texts in sect. 5."}, {"heading": "3.5 Context Sensitive Models of (Cross-Lingual) Semantic Similarity", "text": "Models of semantic similarity briefly discussed in sect. 3.4 provide ranked lists of semantically similar words invariably or in isolation, and they operate at the level of word types. These models do not explicitly encode different senses of words. In practice, it means that, given a sentence \u201cThe coach of his team was not satisfied with the game yesterday.\u201d, these context-insensitive models of cross-lingual semantic similarity are not able to detect that the Spanish word entrenador is more similar to the polysemous English word coach in the context of this sentence than the Spanish word autocar, although autocar is listed as the most semantically similar word to coach globally/invariably without any observed context. In another example, while the Spanish words partido, encuentro, cerilla or correspondencia are all highly similar to another ambiguous English word match when observed in isolation, given the Spanish sentence \u201dShe was unable to find a match in her pocket to light up a cigarette.\u201d, it is clear that the strength of cross-lingual semantic similarity should change in context as only cerilla exhibits a strong cross-lingual semantic similarity to match within this particular sentential context.\nThe goal now is to build BWESG-based models of cross-lingual semantic similarity in context, similar to context-aware cross-lingual semantic similarity models proposed in (Vulic\u0301 & Moens, 2014). Two key questions are: (i) How to provide BWE-based representations beyond word level to represent the context of a word token?; (ii) How to use the contextual knowledge in a context-sensitive model of semantic similarity?\nFollowing Vulic\u0301 and Moens (2014), given a word token w in context (e.g., a window of words, a sentence, a paragraph, or a document), we build its context set or rather context bag Con(w) = {cw1, . . . , cwr} by harvesting r neighboring words in the chosen context scope (e.g., the context bag may comprise all content-bearing words in the same sentence as the pivot word token, the so-called sentential context). In order to present the context Con(w) in the d-dimensional embedding space, we need to apply a model of semantic composition to learn its d-dimensional vector representation \u2212\u2212\u2212\u2212\u2212\u2192 Con(w).\nFormally, given word w, we may specify the vector representation of the context bag Con(w) as the d-dimensional vector/embedding:\n\u2212\u2212\u2212\u2212\u2212\u2192 Con(w) = \u2212\u2212\u2192cw1 ?\u2212\u2212\u2192cw2 ? . . . ?\u2212\u2212\u2192cwr (6)\nwhere \u2212\u2212\u2192cw1, . . . ,\u2212\u2212\u2192cwr are d-dimensional WEs learned from the data, and ? is a compositional vector operator such as addition, point-wise multiplication, tensor product, etc.\nA plethora of models for semantic composition have been proposed in the relevant literature, differing in their choice of vector operators, input structures and required knowledge (Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher, Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom, 2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), to name only a few. In this work, driven by the observed linear linguistic regularities in the embedding spaces (Mikolov et al., 2013d), we opt for simple addition (denoted by +) from Mitchell and Lapata (2008) as the compositional operator, due to its simplicity, the ease of applicability on bag-of-words contexts, and its relatively solid performance in various compositional tasks (Mitchell & Lapata, 2008; Milajevs et al., 2014). The d-dimensional embedding \u2212\u2212\u2212\u2212\u2212\u2192 Con(w) is then:\n\u2212\u2212\u2212\u2212\u2212\u2192 Con(w) = \u2212\u2212\u2192cw1 +\u2212\u2212\u2192cw2 + . . .+\u2212\u2212\u2192cwr (7)\nIf we use the BWESG representation model, we may compute the context-sensitive semantic similarity score sim(wi, tj , Con(wi)) between tj and wi given its context Con(wi) in the shared inter-lingual embedding space as follows:\nsim(wi, tj , Con(wi)) = SF ( \u2212\u2192 w\u2032i, \u2212\u2192 tj ) (8)\ntj \u2208 V T is any target language word, and \u2212\u2192 tj its word representation, while \u2212\u2192 w\u2032i is the new \u201ccontextualized\u201d vector representation for wi modulated by its context Con(wi), that is, its context-aware representation. Vulic\u0301 and Moens (2014) introduced a linear interpolation of two d-dimensional vectors as a plausible solution for the modulation/contextualization. The modulation of representation for wi is computed as follows:\n\u2212\u2192 w\u2032i = (1\u2212 \u03bb) \u00b7 \u2212\u2192wi + \u03bb \u00b7 \u2212\u2212\u2212\u2212\u2212\u2192 Con(wi) (9)\nwhere \u2212\u2192wi is the word embedding for wi computed at the word type level, \u2212\u2212\u2212\u2212\u2212\u2192 Con(wi) is the embedding for the context bag computed using eq. (9), and \u03bb is an interpolation parameter. Another set of similar models that can yield context-sensitive similarity computations has been proposed very recently, and has displayed very competitive results regardless of its simplicity (Melamud, Levy, & Dagan, 2015). Here, we present two best scoring contextsensitive models which we adapt to the bilingual setting:\nAdd-Melamud: sim(wi, tj , Con(wi)) = SF (wi, tj) +\n\u2211 cwi\u2208Con(wi) SF (cwi, tj)\n|C|+ 1 (10)\nMult-Melamud: sim(wi, tj , Con(wi)) = |C|+1 \u221a SF (wi, tj) \u00b7 \u220f cwi\u2208Con(wi) SF (cwi, tj) (11)\nNote that for the Mult model one has to avoid negative values, so a simple shift to an allpositives interval is required, e.g., the shifted cosine score becomes cos\u2032(x, y) = cos(x,y)+12 . Unlike the models from Vulic\u0301 and Moens (2014), these two models do not aggregate single word representations into one vector that represents the context, but compute similarity scores separately with each word from the context. For more details regarding the models, we refer the interested reader to the original paper (Melamud et al., 2015).\nWe will employ the models of context-sensitive CLSS at the word token level to compare all representation models in the task of suggesting word translations in context in sect. 6."}, {"heading": "4. Training Setup", "text": "Training Data. To induce bilingual word embeddings as well as to be directly comparable with baseline representations from prior work, we use a dataset comprising comparable Wikipedia data available in three language pairs (Vulic\u0301 & Moens, 2013b, 2014)6: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 Italian-English Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations of each other.\nFollowing prior work (Koehn & Knight, 2002; Haghighi et al., 2008; Prochasson & Fung, 2011; Vulic\u0301 & Moens, 2013b, 2014), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing steps vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to train all representation models in comparison.\nTrained BWESG Models. To test the effect of random shuffling in the merge and shuffle BWESG strategy, we have trained the BWESG model with 10 random corpora shuffles for all three training corpora. We also train BWESG with the length-ratio shuffle strategy. All parameters are set to default suggested parameters for SGNS from the word2vec package: stochastic gradient descent with a linearly decreasing global learning rate of 0.025, 25 negative samples, subsampling rate 1e\u2212 4, and 15 epochs.\nWe have varied the number of dimensions d = 100, 200, 300. We have also trained BWESG with d = 40 to be directly comparable to readily available sets of BWEs from prior work (Chandar et al., 2014; Gouws et al., 2015). Moreover, to test the effect of window size on the final results, i.e., the number of positives used for training, we have varied the maximum window size cs from 4 to 60 in steps of 4.7\nWe will make our pre-training and training code for BWESG publicly available, along with all BWESG-based bilingual word embeddings for the three language pairs at: http://liir.cs.kuleuven.be/software.php.\nA Roadmap to Experiments. In the first experiment, we quickly visually inspect the obtained lists of semantically similar words using the BWESG bilingual representation model. Following that, we compare BWESG-based models for bilingual lexicon extraction (BLE) and suggesting word translations in context (SWTC) against baseline models for the two tasks relying on the baseline multilingual representation models discussed in sect. 3.3 as well as against several benchmarking BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015). The experiments and results for the BLE task are presented in sect. 5, while the experiments and results for SWTC are presented in sect. 6.\n6. Available online: people.cs.kuleuven.be/\u223civan.vulic/software/ 7. We remind the reader that we slightly abuse terminology here, as the BWESG windows do not include\nthe locality component any more."}, {"heading": "5. Evaluation Task I: Bilingual Lexicon Extraction", "text": ""}, {"heading": "5.1 Task Description", "text": "One may employ the context-insensitive CLSS models from sect. 3.4 to extract bilingual lexicons automatically from data. By harvesting cross-lingual nearest neighbors, one is able to build a bilingual lexicon of one-to-one translation pairs (wSi , w T j ). We test the validity of our BWEs and baseline representations in the BLE task."}, {"heading": "5.2 Experimental Setup", "text": "Test Data. For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) by Vulic\u0301 and Moens (2013a, 2013b). Translation direction is ES/IT/NL\u2192 EN. The test data is available online.8\nEvaluation Metrics. Since we can build a one-to-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc1 score, that is, the number of source language (ES/IT/NL) words wSi from ground truth translation pairs for which the top ranked word cross-lingually is the correct translation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vulic\u0301 & Moens, 2013b).\nModels in Comparison. We have tested various families of similarity functions (SFs) (Lee, 1999; Cha, 2007) for each baseline word representation, and have opted for the best scoring SF for each representation. These are the CLSS/BLE models in comparison: (1) BWESG+cos. Representation model (RM): BWE Skip-Gram. SF: cosine similarity.\n8. http://people.cs.kuleuven.be/ ivan.vulic/software/\n(2) BMu+HD. RM: Basic-MuPTM. SF: Hellinger distance (Pollard, 2001; Cha, 2007). (3) AMu+HD. RM: Association-MuPTM. SF: Hellinger distance. BMu+HD and AMu+HD were the best scoring models in the work by Vulic\u0301 and Moens (2013a). (4) TPPMI+cos. RM: Traditional-PPMI. SF: cosine similarity. This combination was the best scoring BLE model in the work by Vulic\u0301 and Moens (2013b), and it currently obtains the best reported results in the BLE task for these training and test data.\nAll parameters of the baseline representation models (i.e., topic models and their settings, the number of dimensions K, the values for feature pruning, window size, weighting and similarity functions) were optimized for the same training-test combination of datasets in prior work. The settings are adopted directly from previous work (Griffiths et al., 2007; Bullinaria & Levy, 2007; Dinu & Lapata, 2010; Vulic\u0301 & Moens, 2013a, 2013b; Kiela & Clark, 2014), and we encourage the interested reader to check the details and exact parameter setup in the relevant literature."}, {"heading": "5.3 Results and Discussion", "text": ""}, {"heading": "5.3.1 Experiment 0: Qualitative Analysis and Comparison", "text": "Tab. 1 displays top 10 semantically similar words monolingually, across-languages and combined/multilingually for one ES, IT and NL word. The BWESG+cos model is able to find semantically coherent lists of words for all three directions of similarity (i.e., monolingual, cross-lingual, multilingual). In the combined (multilingual) ranked lists, words from both languages are represented as top similar words. This initial qualitative analysis already demonstrates the ability of BWESG to induce a shared bilingual embedding space using only document alignments as bilingual signals.9\nIn another brief analysis, we qualitatively compare the cross-lingual ranked lists acquired by the BWESG+cos model with the other three baseline CLSS/BLE models tailored for document-aligned data. The lists for one ES word and one IT word are presented in tab. 2. For the two example words, BWESG+cos is the only model which is able to rank the actual correct translations as nearest cross-lingual neighbors. It is already symptomatic that the word gulf, which is the correct translation for golfo, does not occur in the ranked list RL10(golfo) at all in case of the three baseline models. We will soon quantitatively confirm this initial suspicion, and demonstrate that the BWESG+cos model is superior to the three baseline models in the BLE task.\nAs an aside, tab. 2 also clearly reveals the difficulty of judging the quality of models for computing semantic similarity/relatedness solely based on the observed output of the models. The lists RL10(cebolla) and RL10(golfo) appear significantly different across all four models, yet all these lists contain words which appear semantically related to the source word. Therefore, we require a more systematic quantitative task-oriented comparison of induced word representations.\n9. We also conducted a small experiment on solving word analogies using monolingual English embedding spaces, and then we repeated the experiment with the same vocabulary and bilingual EnglishSpanish/Italian/Dutch embedding spaces. The results follow the findings from (Faruqui & Dyer, 2014), where only slight (and often insignificant) fluctuations for SGNS vectors were reported (e.g., the fluctuations are < 1% on average in our experiments) when moving from monolingual to bilingual embedding spaces. We may conclude that the linguistic regularities (Mikolov et al., 2013d) established for monolingual embedding spaces induced by SGNS also hold in bilingual embedding spaces induced by BWESG."}, {"heading": "5.3.2 Experiment I: BWESG vs Baseline Representation Models", "text": "Tab. 3 shows the first set of results on the BLE task: we report scores with two different BWESG strategies as well as with a BWESG model which does not shuffle pseudo-bilingual documents. The previous best reported Acc1 scores with baseline representations for the same training+test combination are also reported in the table. By zooming into the table multiple times, we summarize the most important findings.\nBWESG vs Baseline Representations. The results clearly reveal the superior performance of the BWESG-cos model for BLE which relies on our new framework for inducing bilingual word embeddings from document-aligned comparable data over other BLE models relying on previously used bilingual word representations from the same type of training data. The increase in Acc1 scores over the best scoring baseline models is 22.2% for ES-EN, 7% for IT-EN and 67.5% for NL-EN.\nBWESG Shuffling Strategy. Although both BWESG strategies display results that are above established baselines, there is a clear advantage to the length-ratio shuffle strategy, which displays a solid and robust performance across a variety of parameters and all three language pairs. Another advantage of that strategy is the fact that it has a deterministic outcome and does not suffer from \u201csub-optimal\u201d random shuffles. In summary, we suggest using the length-ratio shuffle strategy in future work, and along the same line we opt for that strategy in all further experiments.\nThe results also reveal that shuffling is universally useful, as BWESG without shuffling relies largely on monolingual contexts and cannot reach the performance of BWESG with shuffling. A partial remedy for the problem is to train BWESG with more documentlevel training pairs (i.e., by increasing the window size), but that leads to prohibitively expensive models, and nonetheless BWESG without shuffling with larger cs-s still falls short of BWESG with both shuffling strategies (see also fig. 3(a)-3(c)).\nWindow Size: Number of Training Pairs. The results confirm the intuition that larger window sizes, i.e., more training examples lead to better results in the BLE task. For\nall embedding dimensions d-s, BWESG+cos exhibits a superior performance for cs = 48 than for cs = 16, and the performance with cs = 48 and cs = 60 seems relatively stable: intuitively, more training pairs leads to a slightly better BLE performance, but the curve slowly flattens out (fig. 3(a)-3(c)). This finding reveals that even a coarse tuning of these parameters might lead to optimal or near-optimal scores for BLE with BWESG+cos.\nDifferences across Language Pairs. A lower increase in Acc1 scores for IT-EN is attributed to the fact that the test set for IT-EN comprises IT words with occurrence frequen-\ncies above 200 in the training data (Vulic\u0301 & Moens, 2013a), while the other two test sets comprise randomly sampled words covering all frequency spectra. As expected, all models in comparison are able to effectively utilize distributional signals for higher-frequency words, but BWESG+cos still displays the best performance, and these improvements in Acc1 scores are statistically significant (using McNemar\u2019s statistical significance test, p < 0.05).\nFurther, the lowest overall scores for all models in comparison are observed for NL-EN. We attribute it to using less training data for NL-EN when compared to ES-EN and IT-EN (i.e., training corpora for ES-EN and IT-EN are almost triple the size of training corpora for NL-EN). However, we observe that the increase obtained by BWESG+cos is even more prominent in this setting with limited training data. The lower results of TPPMI+cos compared to other two baseline models are also attributed to the overall lower quality and size of NL-EN training data, which is then reflected in a lower quality of seed lexicons necessary to start the bootstrapping procedure from Vulic\u0301 and Moens (2013b).\nComputational Complexity. BWESG trained with larger values for d and cs yields richer semantic representations, but also naturally leads to increased training times. However, due to a lightweight design of the supporting SGNS, the times are by the order of magnitude lower than the training times for Basic-MuPTM or Association-MuPTM. Typically, several hours are needed to train BWESG with d = 300 and cs \u2248 48\u2212 60, whereas it takes two to three days to train a bilingual topic model with K = 2000 on the same training set using the multi-threaded architectures on 8 Intel(R) Xeon(R) CPU E5-2667 2.90GHz processors."}, {"heading": "5.3.3 Experiment II: BWESG vs. Other BWE Induction Models", "text": "All further experiments are conducted using BWESG with the length-ratio shuffle strategy.\nAnother BWE induction that can be used in this setting with document-aligned data is the well known model from Mikolov et al. (2013b): First, two monolingual embedding spaces are induced separately in each of the two languages using a standard monolingual WE model such as SGNS. The bilingual signal is provided in the form of readily available one-to-one word translation pairs. Training is then cast as a multivariate regression problem, which implies learning a function that maps the source language vectors from the training data to their corresponding target language vectors. A standard approach (Mikolov et al., 2013b; Dinu, Lazaridou, & Baroni, 2015) is to assume a linear map.\nWe train two monolingual SGNS models on monolingual parts of our document-aligned training sets with the same parameters as with BWESG, and use the same set of translation pairs as for the TPPMI model in previous experiments to learn the mapping. Therefore, this model uses exactly the same data sources as all other models compared in sect. 5.3.2. The results with the Mikolov et al.\u2019s model are summarized in tab. 4: a comparison with the results from tab. 3 reveals a clear and prominent advantage for the BWESG model given the same data and training setup.\nAlthough the focus of this article is on the specific multilingual data type: documentaligned comparable data, we also test how well our simple and cheap yet effective BWESG framework fares against other state-of-the-art BWE induction models which also induce shared bilingual embedding spaces, but rely on stronger bilingual signals for training (i.e., parallel data, translation pairs): We compare against two benchmarking BWE induction models that rely on parallel data (Chandar et al., 2014; Gouws et al., 2015). We use a set\nof pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.10 We also train 40-dimensional embeddings from Gouws et al. (2015) (the same dimensionality as in the original paper) using the same data and setup as Gouws et al. (2015) with suggested parameters for the three language pairs.11 The comparison with these two BWE induction models is provided in tab. 5.\nTo make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2015) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligned non-parallel data produces significantly higher BLI scores for IT-EN and ES-EN with sufficiently large windows. However, the results for NL-EN with all BLI models from comparable data from tab. 3 are significantly lower than with the Gouws BWEs. We attribute it to using less (and clearly insufficient) documentaligned training data for NL-EN (see sect. 5.2)."}, {"heading": "6. Evaluation Task II: Suggesting Word Translations in Context", "text": "In another task, we test the ability of BWEs to produce context-sensitive semantic similarity modeling (see sect. 3.5), which in turn may be used to solve the task of suggesting word translations in context (SWTC) proposed recently (Vulic\u0301 & Moens, 2014). The goal now is to build BWESG-based models for SWTC given the sentential context, similar as in prior work (Vulic\u0301 & Moens, 2014). We show that our new BWESG-based SWTC models\n10. http://www.sarathchandar.in/crl.html 11. Suggestions for parameter values received through personal correspondence with the authors. The soft-\nware is available online: https://github.com/gouwsmeister/bilbowa\noutperform the best SWTC models (Vulic\u0301 & Moens, 2014), as well as other SWTC models which rely on baseline word representations discussed in sect. 3.3."}, {"heading": "6.1 Task Description", "text": "Given an occurrence of a polysemous word wi \u2208 V S and the context of that occurrence, the SWTC task is to choose the correct translation in the target language LT of that particular occurrence of wi from the given set T C(wi) = {t1, . . . , ttq}, T C(wi) \u2286 V T , of its tq possible translations/meanings. We may refer to T C(wi) as an inventory of translation candidates for wi. The task of suggesting word translations in context (SWTC) may be interpreted as ranking the tq translation candidates with respect to the observed local context Con(wi) of the occurrence of the word wi. The best scoring translation candidate according to the scores sim(wi, tj , Con(wi)) (see sect. 3.5) in the ranked list is then the correct translation for that particular occurrence of wi observing its local context Con(wi)."}, {"heading": "6.2 Experimental Setup", "text": "Test Data. We use the SWTC test set introduced recently (Vulic\u0301 & Moens, 2014). The test set comprises 15 polysemous nouns in three languages (ES, IT and NL) along with sets of their translation candidates (i.e., sets T C). For each polysemous noun, the test sets provide 24 sentences extracted from Wikipedia which illustrate different senses and translations of the pivot polysemous noun, accompanied by the annotated correct translation for each sentence. It yields 360 test sentences for each language pair (and 1080 test sentences in total). An additional set of 100 IT sentences (5 other polysemous IT nouns plus 20 sentences for each noun) is used as a development set to tune the parameter \u03bb (see sect. 3.5) for all language pairs and all models in comparison. In summary, the final aim may be formulated as follows: For each polysemous word wi in ES/IT/NL, the goal is to suggest its correct translation in English given its sentential context.\nEvaluation Metrics. Since the task is to present a list of possible translations to a SWTC model, and then let the model decide a single most likely translation given the word and its sentential context, we measure the performance again as Top 1 accuracy (Acc1).\nModels in Comparison. We have again tested various families of SFs (Lee, 1999; Cha, 2007) for each baseline word representation, and have opted for the best scoring SF for each representation. These are the SWTC models in comparison: (1) BWESG+add. RM: BWESG. SF: cosine similarity. Composition: addition. \u03bb = 1.0. The value for \u03bb suggests that only context is used to disambiguate the meaning of a polysemous word and to guess its most likely translation in context.12 (2) BMu+HD+S. RM: BasicMuPTM. SF: Hellinger distance. Composition: SmoothedFusion model from (Vulic\u0301 & Moens, 2014). \u03bb = 0.9. (3) BMu+Cue+S. RM: BasicMuPTM. SF: Cue or Association measure (Steyvers & Grif-\n12. We have also experimented with the context-sensitive CLSS models proposed by Melamud et al. (2015), but we do not report the actual scores as this model, although displaying a similar relative ranking of different representation models, was consistently outperformed by the models from (Vulic\u0301 & Moens, 2014) in our evaluation runs: \u22480.75-0.80 vs \u2248 0.60-0.65 for the models from Melamud et al. (2015).\nfiths, 2007; Vulic\u0301 & Moens, 2013a). Composition: Smoothed-Fusion. \u03bb = 0.9. The Cue similarity is tailored for probabilistic models and computed as the association score P (ti|w\u2032i) = \u2211K k=1 P (ti|zk)P (zk|w\u2032i), where zk denotes k-th latent feature, and P (zk|w\u2032i) denotes the modulated probability score obtained by smoothing the probabilistic representations of wi and its context Con(wi) (see the paper from Vulic\u0301 and Moens (2014) for a more detailed description). (4) TPPMI+add. RM: Traditional-PPMI. SF: cosine similarity. Composition: addition. \u03bb = 0.9. We have also experimented with pair-wise multiplication for composition, but these models have not displayed any improvements over TPPMI+add.\nAgain, all parameters of the baseline representation models are adopted directly from prior work (Vulic\u0301 & Moens, 2014) where they were optimized on the same training and test sets. In addition, BMu+HD+S and BMu+Cue+S also rely on the procedure of context sorting and pruning (Vulic\u0301 & Moens, 2014), where the idea is to retain only context words which are most semantically similar to the given pivot polysemous word, and then use them in computations. The procedure, however, produces significant gains only for probabilistic models (BMu+HD+S and BMu+Cue+S), and therefore, we employ it only for these models. BMu+HD+S and BMu+Cue+S with context sorting and pruning were the best scoring models in the introductory SWTC paper (Vulic\u0301 & Moens, 2014) and currently produce state-of-the-art SWTC results on these test sets.13"}, {"heading": "6.3 Results and Discussion", "text": ""}, {"heading": "6.3.1 Experiment I: BWESG vs Baseline Representation Models", "text": "As for BLE, we first investigate whether our BWESG model yields bilingual word representations which lead to better SWTC models. Tab. 6 summarizes the results on the SWTC task. NO-CONTEXT refers to the context-insensitive majority baseline (i.e., always choosing the most semantically similar translation candidate obtained by BWESG+cos at the word type level, without taking into account any context information). BWESG vs Baseline Representations. The results reveal that BWESG outperforms baseline bilingual word representations also in the SWTC task. The improvements are prominent for all reported values of parameters d and cs, and are often statistically significant even when compared to the strongest baseline (which is the fine-tuned BMu+Cue+S model with context sorting and pruning for all three language pairs from Vulic\u0301 and Moens (2014)). The increase in Acc1 scores over the strongest baseline is 12.9% for ES-EN, 11.9% for IT-EN, and 12.4% for NL-EN. The obtained results surpass previous state-of-the-art scores and are currently the best reported results on the SWTC datasets when using nonparallel data to learn semantic representations. BWESG Shuffling Strategy. Although BWESG without shuffling (due to a reduced complexity of the SWTC task compared to BLE) already displays encouraging results, there is again a clear advantage to the length-ratio shuffle strategy, which displays an excellent performance for all three language pairs. In simple words, shuffling is again useful. Dimensionality and Number of Training Pairs. Unlike in the BLE task, the highest Acc1 scores on average are obtained by using lower-dimensional word embeddings (i.e.,\n13. We omit results for the Association-MuPTM RM since SWTC models based on Association-MuPTM were consistently outperformed by SWTC models based on Basic-MuPTM across different settings.\nd = 100, similar or even higher results with d = 40, see tab. 9). The phenomenon may be attributed to the effect of semantic composition and the reduced complexity of the SWTC task compared to the BLE task. First, although enlarging the dimensionality of embeddings leads to an increased semantic expressiveness within the shared bilingual embedding space, it may be harmful when working with composition models, since the simple additive model of semantic composition may produce more erroneous dimensions when constructing higher-dimensional context embeddings out of single word embeddings. Second, due to its design, the SWTC task requires coarser-grained representations than BLE. While in the BLE task the goal is to detect a translation of a word from a vocabulary which typically spans (tens of) thousands of words, in the SWTC task the goal is to detect the most likely translation of a word given its sentential context, but from a small closed vocabulary of 2-4 possible translations from the translation inventory. Therefore, it is highly likely that even low-dimensional embeddings are sufficient to produce plausible rankings for the SWTC task, while at the same time, they are not sufficient and expressive enough to find correct translations in BLE. More training pairs (i.e., larger windows) still yield better results on average in the SWTC task. In summary, the choice of representation granularity is dependent on the actual task, which consequently leads to the conclusion that optimal values for d and cs are largely task-specific (compare also results in tab. 3 and tab. 6).\nTesting Polysemy. In order to test whether the gain in performance for BWESG+add is derived mostly from the effective handling of the easiest set of words, that is, bisemous words (polysemous words with only 2 translation candidates), we have performed an additional experiment, where we have measured Acc1 scores separately for words with 2, 3, and 4 different senses. Results indicate that the performance gain comes mostly from gains on trisemous and tetrasemous words, while the scores on bisemous words are comparable. Tab. 7 shows Acc1 over different clusters of words for ES-EN, and similar scoring patterns are observed for IT-EN and NL-EN. Differences across Language Pairs. Due to the reduced complexity of SWTC, we may also observe relatively higher results for NL-EN when compared to ES-EN and IT-EN, as opposed to their relative performance in the BLE task, where the scores for NL-EN are much lower than scores for ES-EN and IT-EN. Since SWTC is a less difficult task which requires coarse-grained representations, even limited amounts of training data may be sufficient to learn word embeddings which are useful for the specific task. This finding is in line with the recent work from Gouws and S\u00f8gaard (2015)."}, {"heading": "6.3.2 Experiment II: BWESG vs. Other BWE Induction Models", "text": "We again test other BWE induction models in the SWTC task, using the same training setup and sets of embeddings as introduced in sect. 5.3.3 for the BLE task. The representations were now plugged in a variety of context-sensitive CLSS models from sect. 3.5, and the optimization of parameters for SWTC has been conducted in the same manner as for BWESG. The results for the BWE induction model from Mikolov et al. (2013b) are summarized in tab. 8, while the results with the other two models (Chandar et al., 2014; Gouws et al., 2015) are provided in tab. 9.\nBWESG outperforms other BWE induction models in the SWTC task and further confirms its utility in cross-lingual semantic modeling. The model from Mikolov et al. (2013b) constitutes a stronger baseline: Good results in the SWTC task with this model are an interesting finding per se. While the model is not competitive with BWESG and other baseline representations models from document-aligned data in a more difficult BLE task when using noisy one-to-one translation pairs, its performance on the less complex SWTC task with a reduced search space is solid even when the model relies on the imperfect set of translation pairs to learn the mapping between two monolingual embedding spaces."}, {"heading": "6.3.3 Further Discussion", "text": "By analyzing the influence of pre-training shuffling on the results in two different evaluation tasks, we may safely establish its utility when inducing bilingual word embeddings using the BWESG model. While we have already presented two shuffling strategies in this work, one line of future work will investigate different possibilities of \u201cblending in\u201d words from two different vocabularies into pseudo-bilingual documents in a more structured and systematic manner. For instance, one approach to generating pseudo-training sentences for learning from textual and perceptual modalities has been recently introduced (Hill & Korhonen, 2014). However, it is not straightforward how to extend this approach to the generation of pseudo-bilingual training documents.\nAnother idea in the same vein is to build artificial training data of higher-quality starting from noisy comparable data by: (1) computing semantically similar words monolingually and across-languages from the noisy data, (2) retaining only highly reliable pairs of similar words using an automatic selection procedure (Vulic\u0301 & Moens, 2012), (3) building pseudobilingual documents using only reliable context word pairs. In other words, the questions is: Is it possible to choose positive training pairs more systematically to reduce the noise stemming from non-parallel data? The construction of such artificial training data and training on such data would then proceed in a bootstrapping fashion, and the model should be able to steadily reduce noise inherently present in comparable data. The idea of \u201cimproving corpus comparability\u201d was only touched upon in previous work (Li & Gaussier, 2010; Li, Gaussier, & Aizawa, 2011).\nWhile the entire framework proposed in this article is in theory completely language pair agnostic as it does not make any language pair dependent modeling assumptions, we acknowledge the fact that all three language pairs comprise languages coming from the same phylum, that is, the Indo-European language family. Future extensions also include porting the framework to other more distant language pairs that do not share the same roots nor the same alphabet (e.g., English-Chinese/Hindi/Arabic), and for which benchmarking test sets are still scarce for a variety of semantic tasks (e.g., SWTC) (Camacho-Collados, Pilehvar, & Navigli, 2015)."}, {"heading": "7. Conclusions and Future Work", "text": "We have proposed and described Bilingual Word Embeddings Skip-Gram (BWESG), a simple yet effective bilingual word representation learning model which is able to induce bilingual word embeddings solely on the basis of document-aligned comparable data. BWESG is based on the omnipresent skip-gram with negative sampling (SGNS). We have presented\ntwo ways to build pseudo-bilingual documents on which a monolingual SGNS (or any monolingual WE induction model) may be trained to produce shared bilingual embedding spaces. The BWESG model does not make any language-pair dependent assumptions nor requires language-pair specific external resources such as bilingual lexicons, predefined category/ontology knowledge or parallel data.\nWe have employed induced BWEs in two semantic tasks: (1) bilingual lexicon extraction (BLE), and (2) suggesting word translations in context (SWTC). Our new BWESG-based BLE and SWTC models outperform previous state-of-the-art models for BLE and SWTC from document-aligned comparable data and related BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015). The findings in this article follow the recently published surveys from Baroni et al. (2014), Levy et al. (2015) regarding a solid and robust performance of neural word representations/word embeddings in semantic tasks: our new BWESG-based models for BLE and SWTC significantly outscore previous state-of-theart distributional approaches on both tasks across different parameter settings. Even more encouraging is the fact that these new state-of-the-art results are attained using default parameter settings for the BWESG model as suggested in the word2vec package without any development set. Further (finer) tuning of model parameters in future work may lead to higher-quality bilingual embedding spaces.\nSeveral straightforward lines of future research have already been tackled in sect. 5 and sect. 6. For instance, the current length-ratio shuffling strategy may be replaced by a more advanced shuffling method in future work. Moreover, BWEs induced by BWESG may be used in other semantic tasks besides the ones discussed in this work, and it would be interesting to experiment with other types of context aggregation and selection beyond the bag-of-words assumption, such as dependency-based contexts (Levy & Goldberg, 2014a), or other objective functions during training in the same vein as proposed by Levy and Goldberg (2014b). Similar to the evolution in multilingual probabilistic topic modeling, another path of future work may lead to investigating bilingual models for learning BWEs which will be able to jointly learn from separate documents in aligned document pairs, without the need to construct pseudo-bilingual documents.\nA natural step in the text representation learning research is to extend the focus from single word representations to composite phrase, sentence and document representations (Hermann & Blunsom, 2013; Kalchbrenner, Grefenstette, & Blunsom, 2014; Le & Mikolov, 2014; Soyer, Stenetorp, & Aizawa, 2015). In this article, we have relied on a simple composition model based on vector addition, and have shown that this model performs excellent in the SWTC task. However, in the long run this model is not by any means sufficient to effectively capture all complex compositional phenomena in the data. Several models which aim to learn sentence and document embeddings have been proposed recently, but they critically rely on sentence-aligned parallel data. It is yet to be seen how to build structured multilingual phrase, sentence and document embeddings solely on the basis of comparable data. Such low-cost multilingual embeddings beyond the word level extracted from comparable data may find its application in a variety of tasks such as statistical machine translation (Mikolov et al., 2013b; Zou et al., 2013; Zhang, Liu, Li, Zhou, & Zong, 2014; Wu et al., 2014), semantic tasks such as multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual\ninformation retrieval (Vulic\u0301 et al., 2013; Vulic\u0301 & Moens, 2015) or cross-lingual document classification (Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).\nIn another future research path, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for learning from large unaligned multilingual datasets (Mikolov et al., 2013b; Al-Rfou, Perozzi, & Skiena, 2013). In the long run, this idea may lead to large-scale learning models from huge amounts of multilingual data without any requirement for parallel data or manually built bilingual lexicons."}], "references": [{"title": "SemEval-2014 task 10: Multilingual semantic textual similarity", "author": ["E. Agirre", "C. Banea", "C. Cardie", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo", "R. Mihalcea", "G. Rigau", "J. Wiebe"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SEMEVAL),", "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Baroni and Zamparelli,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Zamparelli", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Blacoe and Lapata,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata", "year": 2012}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Multilingual topic models for unaligned text", "author": ["J. Boyd-Graber", "D.M. Blei"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Boyd.Graber and Blei,? \\Q2009\\E", "shortCiteRegEx": "Boyd.Graber and Blei", "year": 2009}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "A framework for the construction of monolingual and cross-lingual word similarity datasets", "author": ["J. Camacho-Collados", "M.T. Pilehvar", "R. Navigli"], "venue": null, "citeRegEx": "Camacho.Collados et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "Translingual information retrieval: A comparative evaluation", "author": ["J.G. Carbonell", "J.G. Yang", "R.E. Frederking", "R.D. Brown", "Y. Geng", "D. Lee", "Y. Frederking", "R. E", "R.D. Geng", "Y. Yang"], "venue": "In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Carbonell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Carbonell et al\\.", "year": 1997}, {"title": "Comprehensive survey on distance/similarity measures between probability density functions", "author": ["Cha", "S.-H."], "venue": "International Journal of Mathematical Models and Methods in Applied Sciences, 1 (4), 300\u2013307.", "citeRegEx": "Cha and S..H.,? 2007", "shortCiteRegEx": "Cha and S..H.", "year": 2007}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["S. Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V.C. Raykar", "A. Saha"], "venue": "In Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["D. Clarke"], "venue": "Computational Linguistics, 38 (1), 41\u201371.", "citeRegEx": "Clarke,? 2012", "shortCiteRegEx": "Clarke", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML),", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graphbased projections", "author": ["D. Das", "S. Petrov"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Das and Petrov,? \\Q2011\\E", "shortCiteRegEx": "Das and Petrov", "year": 2011}, {"title": "Domain adaptation for machine translation by mining unseen words", "author": ["H. Daum\u00e9 III", "J. Jagarlamudi"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "III and Jagarlamudi,? \\Q2011\\E", "shortCiteRegEx": "III and Jagarlamudi", "year": 2011}, {"title": "Cross-language linking of news stories on the Web using interlingual topic modeling", "author": ["W. De Smet", "Moens", "M.-F"], "venue": "In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining (SWSM@CIKM),", "citeRegEx": "Smet et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Smet et al\\.", "year": 2009}, {"title": "The latent words language model", "author": ["K. Deschacht", "J. De Belder", "Moens", "M.-F"], "venue": "Computer Speech & Language,", "citeRegEx": "Deschacht et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deschacht et al\\.", "year": 2012}, {"title": "Semi-supervised semantic role labeling using the latent words language model", "author": ["K. Deschacht", "Moens", "M.-F"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Deschacht et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deschacht et al\\.", "year": 2009}, {"title": "Measuring distributional similarity in context", "author": ["G. Dinu", "M. Lapata"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Dinu and Lapata,? \\Q2010\\E", "shortCiteRegEx": "Dinu and Lapata", "year": 2010}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "author": ["G. Dinu", "A. Lazaridou", "M. Baroni"], "venue": "In ICLR Workshop Papers", "citeRegEx": "Dinu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Automatic cross-linguistic information retrieval using Latent Semantic Indexing", "author": ["S.T. Dumais", "T.K. Landauer", "M. Littman"], "venue": "In Proceedings of the SIGIR Workshop on Cross-Linguistic Information Retrieval,", "citeRegEx": "Dumais et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 1996}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, 14, 179\u2013211.", "citeRegEx": "Elman,? 1990", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Faruqui and Dyer,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer", "year": 2014}, {"title": "Symmetric correspondence topic models for multilingual text analysis", "author": ["K. Fukumasu", "K. Eguchi", "E.P. Xing"], "venue": "In Proceedings of the 25th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Fukumasu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fukumasu et al\\.", "year": 2012}, {"title": "Cross-lingual discriminative learning of sequence models with posterior regularization", "author": ["K. Ganchev", "D. Das"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Ganchev and Das,? \\Q2013\\E", "shortCiteRegEx": "Ganchev and Das", "year": 2013}, {"title": "A geometric view on bilingual lexicon extraction from comparable corpora", "author": ["\u00c9. Gaussier", "Renders", "J.-M", "I. Matveeva", "C. Goutte", "H. D\u00e9jean"], "venue": "In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Gaussier et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gaussier et al\\.", "year": 2004}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Word2vec explained: Deriving Mikolov et al.\u2019s negativesampling word-embedding method. CoRR, abs/1402.3722", "author": ["Y. Goldberg", "O. Levy"], "venue": null, "citeRegEx": "Goldberg and Levy,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy", "year": 2014}, {"title": "BilBOWA: Fast bilingual distributed representations without word alignments", "author": ["S. Gouws", "Y. Bengio", "G. Corrado"], "venue": "In ICML,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Simple task-specific bilingual word embeddings", "author": ["S. Gouws", "A. S\u00f8gaard"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Gouws and S\u00f8gaard,? \\Q2015\\E", "shortCiteRegEx": "Gouws and S\u00f8gaard", "year": 2015}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10 (23), 146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Hermann and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2013}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 2014 International Conference on Learning Representations (ICLR)", "citeRegEx": "Hermann and Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Hermann and Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what I mean", "author": ["F. Hill", "A. Korhonen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Hill and Korhonen,? \\Q2014\\E", "shortCiteRegEx": "Hill and Korhonen", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A Bayesian method for robust estimation of distributional similarities", "author": ["J. Kazama", "S.D. Saeger", "K. Kuroda", "M. Murata", "K. Torisawa"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kazama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kazama et al\\.", "year": 2010}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["D. Kiela", "L. Bottou"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kiela and Bottou,? \\Q2014\\E", "shortCiteRegEx": "Kiela and Bottou", "year": 2014}, {"title": "A systematic study of semantic vector space model parameters", "author": ["D. Kiela", "S. Clark"], "venue": "In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),", "citeRegEx": "Kiela and Clark,? \\Q2014\\E", "shortCiteRegEx": "Kiela and Clark", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["P. Koehn", "K. Knight"], "venue": "In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition (ULA),", "citeRegEx": "Koehn and Knight,? \\Q2002\\E", "shortCiteRegEx": "Koehn and Knight", "year": 2002}, {"title": "Learning bilingual word representations by marginalizing alignments", "author": ["T. Ko\u010disk\u00fd", "K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Ko\u010disk\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ko\u010disk\u00fd et al\\.", "year": 2014}, {"title": "Solutions to Plato\u2019s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Revisiting context-based projection methods for termtranslation spotting in comparable corpora", "author": ["A. Laroche", "P. Langlais"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Laroche and Langlais,? \\Q2010\\E", "shortCiteRegEx": "Laroche and Langlais", "year": 2010}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Le and Mikolov,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Word embeddings through Hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Measures of distributional similarity", "author": ["L. Lee"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 25\u201332.", "citeRegEx": "Lee,? 1999", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Dictionary-based techniques for crosslanguage information retrieval", "author": ["Levow", "G.-A", "D.W. Oard", "P. Resnik"], "venue": "Information Processing and Management,", "citeRegEx": "Levow et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Levow et al\\.", "year": 2005}, {"title": "Dependency-based word embeddings", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transactions of the ACL,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Improving corpus comparability for bilingual lexicon extraction from comparable corpora", "author": ["B. Li", "\u00c9. Gaussier"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Li and Gaussier,? \\Q2010\\E", "shortCiteRegEx": "Li and Gaussier", "year": 2010}, {"title": "Clustering comparable corpora for bilingual lexicon extraction", "author": ["B. Li", "\u00c9. Gaussier", "A. Aizawa"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Q. Liu", "H. Jiang", "S. Wei", "Ling", "Z.-H", "Y. Hu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Topic models + word alignment = a flexible framework for extracting bilingual dictionary from comparable corpus", "author": ["X. Liu", "K. Duh", "Y. Matsumoto"], "venue": "In Proceedings of the 17th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "A simple word embedding model for lexical substitution", "author": ["O. Melamud", "O. Levy", "I. Dagan"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In ICLR Workshop Papers", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": "In Proceedings of the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluating neural word representations in tensor-based compositional settings", "author": ["D. Milajevs", "D. Kartsaklis", "M. Sadrzadeh", "M. Purver"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Milajevs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Milajevs et al\\.", "year": 2014}, {"title": "Polylingual topic models", "author": ["D. Mimno", "H. Wallach", "J. Naradowsky", "D.A. Smith", "A. McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Mitchell and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "Mining multilingual topics from Wikipedia", "author": ["X. Ni", "Sun", "J.-T", "J. Hu", "Z. Chen"], "venue": "In Proceedings of the 18th International World Wide Web Conference (WWW),", "citeRegEx": "Ni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2009}, {"title": "Cross lingual text classification by mining multilingual topics from Wikipedia", "author": ["X. Ni", "Sun", "J.-T", "J. Hu", "Z. Chen"], "venue": "In Proceedings of the 4th International Conference on Web Search and Web Data Mining (WSDM),", "citeRegEx": "Ni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2011}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["S. Pad\u00f3", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pad\u00f3 and Lapata,? \\Q2009\\E", "shortCiteRegEx": "Pad\u00f3 and Lapata", "year": 2009}, {"title": "Discovering word senses from text", "author": ["P. Pantel", "D. Lin"], "venue": "In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Pantel and Lin,? \\Q2002\\E", "shortCiteRegEx": "Pantel and Lin", "year": 2002}, {"title": "Cross-lingual induction of selectional preferences with bilingual vector spaces", "author": ["Y. Peirsman", "S. Pad\u00f3"], "venue": "In Proceedings of the 11th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Peirsman and Pad\u00f3,? \\Q2010\\E", "shortCiteRegEx": "Peirsman and Pad\u00f3", "year": 2010}, {"title": "Semantic relations in bilingual lexicons", "author": ["Y. Peirsman", "S. Pad\u00f3"], "venue": "ACM Transactions on Speech and Language Processing,", "citeRegEx": "Peirsman and Pad\u00f3,? \\Q2011\\E", "shortCiteRegEx": "Peirsman and Pad\u00f3", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A User\u2019s Guide to Measure Theoretic Probability", "author": ["D. Pollard"], "venue": "Cambridge University Press.", "citeRegEx": "Pollard,? 2001", "shortCiteRegEx": "Pollard", "year": 2001}, {"title": "Rare word translation extraction from aligned comparable documents", "author": ["E. Prochasson", "P. Fung"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Prochasson and Fung,? \\Q2011\\E", "shortCiteRegEx": "Prochasson and Fung", "year": 2011}, {"title": "Automatic identification of word translations from unrelated English and German corpora", "author": ["R. Rapp"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 519\u2013526.", "citeRegEx": "Rapp,? 1999", "shortCiteRegEx": "Rapp", "year": 1999}, {"title": "A mixture model with sharing for lexical semantics", "author": ["J. Reisinger", "R.J. Mooney"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Reisinger and Mooney,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "Compositional matrix-space models of language", "author": ["S. Rudolph", "E. Giesbrecht"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Rudolph and Giesbrecht,? \\Q2010\\E", "shortCiteRegEx": "Rudolph and Giesbrecht", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "Proceedings of the International Conference on New Methods in Language Processing.", "citeRegEx": "Schmid,? 1994", "shortCiteRegEx": "Schmid", "year": 1994}, {"title": "Learning cross-lingual word embeddings via matrix co-factorization", "author": ["T. Shi", "Z. Liu", "Y. Liu", "M. Sun"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP),", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Leveraging monolingual data for crosslingual compositional word representations", "author": ["H. Soyer", "P. Stenetorp", "A. Aizawa"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations (ICLR)", "citeRegEx": "Soyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soyer et al\\.", "year": 2015}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of Latent Semantic Analysis,", "citeRegEx": "Steyvers and Griffiths,? \\Q2007\\E", "shortCiteRegEx": "Steyvers and Griffiths", "year": 2007}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["K. Stratos", "M. Collins", "D. Hsu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP),", "citeRegEx": "Stratos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "author": ["O. T\u00e4ckstr\u00f6m", "D. Das", "S. Petrov", "R. McDonald", "J. Nivre"], "venue": "Transactions of ACL,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Bilingual lexicon extraction from comparable corpora using label propagation", "author": ["A. Tamura", "T. Watanabe", "E. Sumita"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Tamura et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tamura et al\\.", "year": 2012}, {"title": "Treebank translation for cross-lingual parser induction", "author": ["J. Tiedemann", "Agi\u0107", "J. Nivre"], "venue": "In Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Tiedemann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2014}, {"title": "Modeling order in neural word embeddings at scale", "author": ["A. Trask", "D. Gilmore", "M. Russell"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Trask et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Trask et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J.P. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artifical Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Identifying word translations from comparable corpora using latent topic models", "author": ["I. Vuli\u0107", "W. De Smet", "Moens", "M.-F"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2011}, {"title": "Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora", "author": ["I. Vuli\u0107", "W. De Smet", "Moens", "M.-F"], "venue": "Information Retrieval,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications", "author": ["I. Vuli\u0107", "W. De Smet", "J. Tang", "M. Moens"], "venue": "Information Processing and Management,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Detecting highly confident word translations from comparable corpora without any prior knowledge", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2012}, {"title": "Cross-lingual semantic similarity of words as the similarity of their semantic word responses", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "A study on bootstrapping bilingual vector spaces from non-parallel data (and nothing else)", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "Probabilistic models of cross-lingual semantic similarity in context based on latent cross-lingual concepts induced from comparable data", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2014}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 38th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Improve statistical machine translation with context-sensitive bilingual semantic embedding model", "author": ["H. Wu", "D. Dong", "X. Hu", "D. Yu", "W. He", "H. Wang", "T. Liu"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora", "author": ["H. Wu", "H. Wang", "C. Zong"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Distributed word representation learning for cross-lingual dependency parsing", "author": ["M. Xiao", "Y. Guo"], "venue": "In Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Xiao and Guo,? \\Q2014\\E", "shortCiteRegEx": "Xiao and Guo", "year": 2014}, {"title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora", "author": ["D. Yarowsky", "G. Ngai"], "venue": "In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Yarowsky and Ngai,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky and Ngai", "year": 2001}, {"title": "Cross-lingual latent topic extraction", "author": ["D. Zhang", "Q. Mei", "C. Zhai"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["J. Zhang", "S. Liu", "M. Li", "M. Zhou", "C. Zong"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 35, "context": "A huge body of work in distributional semantics and word representation learning almost exclusively revolves around the distributional hypothesis (Harris, 1954) - an idea which states that similar words occur in similar contexts.", "startOffset": 146, "endOffset": 160}, {"referenceID": 25, "context": "The idea of representing words as continuous real-valued vectors dates way back to mid80s (Rumelhart, Hinton, & Williams, 1986; Elman, 1990).", "startOffset": 90, "endOffset": 140}, {"referenceID": 4, "context": "The idea met its resurgence a decade ago (Bengio et al., 2003), where a neural language model learns word embeddings as part of a neural network architecture for statistical language modeling.", "startOffset": 41, "endOffset": 62}, {"referenceID": 76, "context": "Other models inspired by skip-gram and CBOW are GloVe (Global Vectors for Word Representation) (Pennington et al., 2014), which combines local and global contexts of a word into a unified model, and a model which relies on dependency-based contexts instead of simpler word-based contexts (Levy &", "startOffset": 95, "endOffset": 120}, {"referenceID": 4, "context": "The idea met its resurgence a decade ago (Bengio et al., 2003), where a neural language model learns word embeddings as part of a neural network architecture for statistical language modeling. This work inspired other approaches that learn word embeddings within the neural-network language modeling framework (Collobert & Weston, 2008; Collobert, Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011). Word embeddings are tailored to capture semantics and encode a continuous notion of semantic similarity (as opposed to semantically poorer discrete representations), necessary to share information between words and other text units. Recently, the skip-gram and continuous bag-of-words (CBOW) model from Mikolov et al. (2013a, 2013c) revealed that the full neural-network structure is not needed at all to learn high-quality word embeddings (with extremely decreased training times compared to the full-fledged neural network models, see Mikolov et al.\u2019s (2013a) work for the full analysis of complexity of the models).", "startOffset": 42, "endOffset": 963}, {"referenceID": 32, "context": "Figure 1: A toy 3D shared bilingual embedding space from Gouws et al. (2015): While in monolingual spaces words with similar meanings should have similar representations, in bilingual spaces words in two different languages with similar meanings should have similar representations (both mono- and cross-lingually).", "startOffset": 57, "endOffset": 77}, {"referenceID": 2, "context": ", context counting plus context weighting and/or dimensionality reduction), with a slight improvement in performance with SGNS (Baroni et al., 2014; Levy et al., 2015).", "startOffset": 127, "endOffset": 167}, {"referenceID": 55, "context": ", context counting plus context weighting and/or dimensionality reduction), with a slight improvement in performance with SGNS (Baroni et al., 2014; Levy et al., 2015).", "startOffset": 127, "endOffset": 167}, {"referenceID": 2, "context": "Their utility has been validated and proven in various semantic tasks such as semantic word similarity, synonymy detection or word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014; Pennington et al., 2014).", "startOffset": 147, "endOffset": 216}, {"referenceID": 76, "context": "Their utility has been validated and proven in various semantic tasks such as semantic word similarity, synonymy detection or word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014; Pennington et al., 2014).", "startOffset": 147, "endOffset": 216}, {"referenceID": 15, "context": "Moreover, word embeddings have been proven to serve as useful unsupervised features for plenty of downstream NLP tasks such as named entity recognition, chunking, semantic role labeling, part-of-speech tagging, or selectional preferences (Turian, Ratinov, & Bengio, 2010; Collobert et al., 2011).", "startOffset": 238, "endOffset": 295}, {"referenceID": 55, "context": ", 2013c; Levy & Goldberg, 2014b), with a clear advantage on similarity tasks when compared to traditional models from distributional semantics (Levy et al., 2015) in this article we will focus on the adaptation of the skip-gram model with negative sampling (Mikolov et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 44, "context": "Bilingual word representations could serve as an useful source knowledge for problems in cross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vuli\u0107, De Smet, & Moens, 2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification (Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014; Vuli\u0107, De Smet, Tang, & Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vuli\u0107 & Moens, 2013a), or knowledge transfer and annotation projection from resource-rich to resource-poor languages for a myriad of NLP tasks (Yarowsky & Ngai, 2001; Pad\u00f3 & Lapata, 2009; Peirsman & Pad\u00f3, 2010; Das & Petrov, 2011; T\u00e4ckstr\u00f6m, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agi\u0107, & Nivre, 2014).", "startOffset": 269, "endOffset": 406}, {"referenceID": 12, "context": "Bilingual word representations could serve as an useful source knowledge for problems in cross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vuli\u0107, De Smet, & Moens, 2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification (Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014; Vuli\u0107, De Smet, Tang, & Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vuli\u0107 & Moens, 2013a), or knowledge transfer and annotation projection from resource-rich to resource-poor languages for a myriad of NLP tasks (Yarowsky & Ngai, 2001; Pad\u00f3 & Lapata, 2009; Peirsman & Pad\u00f3, 2010; Das & Petrov, 2011; T\u00e4ckstr\u00f6m, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agi\u0107, & Nivre, 2014).", "startOffset": 269, "endOffset": 406}, {"referenceID": 44, "context": "We may cluster the current work in three different groups: (1) the models that rely on hard word alignments obtained from parallel data to constrain the learning of BWEs (Klementiev et al., 2012; Zou, Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014); (2) the models that use the alignment of parallel data at the sentence level (Ko\u010disk\u00fd, Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al.", "startOffset": 170, "endOffset": 275}, {"referenceID": 12, "context": ", 2012; Zou, Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014); (2) the models that use the alignment of parallel data at the sentence level (Ko\u010disk\u00fd, Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi, Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) the models that critically require readily available bilingual lexicons (Mikolov et al.", "startOffset": 166, "endOffset": 304}, {"referenceID": 32, "context": ", 2012; Zou, Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014); (2) the models that use the alignment of parallel data at the sentence level (Ko\u010disk\u00fd, Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi, Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) the models that critically require readily available bilingual lexicons (Mikolov et al.", "startOffset": 166, "endOffset": 304}, {"referenceID": 79, "context": "Prior work on inducing bilingual word representations in the early days followed the tradition of window-based context-counting distributional models (Rapp, 1999; Gaussier, Renders, Matveeva, Goutte, & D\u00e9jean, 2004; Laroche & Langlais, 2010) and it again required a bilingual lexicon as a critical resource.", "startOffset": 150, "endOffset": 241}, {"referenceID": 12, "context": "In theory, the work from (Hermann & Blunsom, 2014b; Chandar et al., 2014) may also be extended to the same setting with document-aligned data, as these two models originally rely on sentence embeddings computed as aggregations over their single word embeddings plus sentence alignments.", "startOffset": 25, "endOffset": 73}, {"referenceID": 4, "context": "2 The SGNS model learns word embeddings (WEs) in a similar way to neural language models (Bengio et al., 2003; Collobert & Weston, 2008), but without a non-linear hidden layer.", "startOffset": 89, "endOffset": 136}, {"referenceID": 61, "context": "Our departure point is the log-linear SGNS from Mikolov et al. (2013c) as implemented in the word2vec package.", "startOffset": 48, "endOffset": 71}, {"referenceID": 31, "context": "While the interested reader may find further details about the negative sampling procedure, and the new exact objective function along with its derivation elsewhere (Levy & Goldberg, 2014b), for illustrative purposes and simplicity, here we present the approximative objective function with negative sampling by Goldberg and Levy (2014):", "startOffset": 312, "endOffset": 337}, {"referenceID": 67, "context": "More recent work on multilingual probabilistic topic modeling (MuPTM) (Mimno et al., 2009; De Smet & Moens, 2009; Vuli\u0107 et al., 2011) showed that word representations of higher quality may be built if a multilingual topic model such as bilingual LDA (BiLDA) is trained jointly on document-aligned comparable corpora by retaining the structure of the corpus intact (i.", "startOffset": 70, "endOffset": 133}, {"referenceID": 95, "context": "More recent work on multilingual probabilistic topic modeling (MuPTM) (Mimno et al., 2009; De Smet & Moens, 2009; Vuli\u0107 et al., 2011) showed that word representations of higher quality may be built if a multilingual topic model such as bilingual LDA (BiLDA) is trained jointly on document-aligned comparable corpora by retaining the structure of the corpus intact (i.", "startOffset": 70, "endOffset": 133}, {"referenceID": 97, "context": ", BLE or SWTC) (Vuli\u0107 & Moens, 2013a; Vuli\u0107 et al., 2015).", "startOffset": 15, "endOffset": 57}, {"referenceID": 97, "context": "A multilingual topic model is typically trained by Gibbs sampling (Geman & Geman, 1984; Steyvers & Griffiths, 2007; Vuli\u0107 et al., 2015).", "startOffset": 66, "endOffset": 135}, {"referenceID": 29, "context": ", using PMI, TFIDF) between pivot words and their context words in a window of predefined size, plus an external bilingual lexicon to align context words/dimensions across languages (Gaussier et al., 2004; Laroche & Langlais, 2010).", "startOffset": 182, "endOffset": 231}, {"referenceID": 51, "context": "(Lee, 1999; Cha, 2007), and different RMs typically require different SFs to produce optimal or near-optimal results over various semantic tasks.", "startOffset": 0, "endOffset": 22}, {"referenceID": 77, "context": "On the other hand, a good choice for SF when working with probabilistic RMs such as Basic-MuPTM and Association-MuPTM RS is the Hellinger distance (Pollard, 2001; Cha, 2007; Kazama, Saeger, Kuroda, Murata, & Torisawa, 2010), which displays excellent results in the BLE task (Vuli\u0107 & Moens, 2013a).", "startOffset": 147, "endOffset": 223}, {"referenceID": 13, "context": "A plethora of models for semantic composition have been proposed in the relevant literature, differing in their choice of vector operators, input structures and required knowledge (Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher, Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom, 2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), to name only a few.", "startOffset": 180, "endOffset": 408}, {"referenceID": 66, "context": ", 2013d), we opt for simple addition (denoted by +) from Mitchell and Lapata (2008) as the compositional operator, due to its simplicity, the ease of applicability on bag-of-words contexts, and its relatively solid performance in various compositional tasks (Mitchell & Lapata, 2008; Milajevs et al., 2014).", "startOffset": 258, "endOffset": 306}, {"referenceID": 13, "context": "A plethora of models for semantic composition have been proposed in the relevant literature, differing in their choice of vector operators, input structures and required knowledge (Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher, Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom, 2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), to name only a few. In this work, driven by the observed linear linguistic regularities in the embedding spaces (Mikolov et al., 2013d), we opt for simple addition (denoted by +) from Mitchell and Lapata (2008) as the compositional operator, due to its simplicity, the ease of applicability on bag-of-words contexts, and its relatively solid performance in various compositional tasks (Mitchell & Lapata, 2008; Milajevs et al.", "startOffset": 320, "endOffset": 621}, {"referenceID": 61, "context": "For more details regarding the models, we refer the interested reader to the original paper (Melamud et al., 2015).", "startOffset": 92, "endOffset": 114}, {"referenceID": 34, "context": "Following prior work (Koehn & Knight, 2002; Haghighi et al., 2008; Prochasson & Fung, 2011; Vuli\u0107 & Moens, 2013b, 2014), we retain only nouns that occur at least 5 times in the corpus.", "startOffset": 21, "endOffset": 119}, {"referenceID": 83, "context": "TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization.", "startOffset": 11, "endOffset": 25}, {"referenceID": 12, "context": "We have also trained BWESG with d = 40 to be directly comparable to readily available sets of BWEs from prior work (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 115, "endOffset": 157}, {"referenceID": 32, "context": "We have also trained BWESG with d = 40 to be directly comparable to readily available sets of BWEs from prior work (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 115, "endOffset": 157}, {"referenceID": 12, "context": "3 as well as against several benchmarking BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 63, "endOffset": 128}, {"referenceID": 32, "context": "3 as well as against several benchmarking BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 63, "endOffset": 128}, {"referenceID": 29, "context": "Since we can build a one-to-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc1 score, that is, the number of source language (ES/IT/NL) words wS i from ground truth translation pairs for which the top ranked word cross-lingually is the correct translation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vuli\u0107 & Moens, 2013b).", "startOffset": 441, "endOffset": 507}, {"referenceID": 90, "context": "Since we can build a one-to-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc1 score, that is, the number of source language (ES/IT/NL) words wS i from ground truth translation pairs for which the top ranked word cross-lingually is the correct translation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vuli\u0107 & Moens, 2013b).", "startOffset": 441, "endOffset": 507}, {"referenceID": 51, "context": "We have tested various families of similarity functions (SFs) (Lee, 1999; Cha, 2007) for each baseline word representation, and have opted for the best scoring SF for each representation.", "startOffset": 62, "endOffset": 84}, {"referenceID": 77, "context": "SF: Hellinger distance (Pollard, 2001; Cha, 2007).", "startOffset": 23, "endOffset": 49}, {"referenceID": 77, "context": "SF: Hellinger distance (Pollard, 2001; Cha, 2007). (3) AMu+HD. RM: Association-MuPTM. SF: Hellinger distance. BMu+HD and AMu+HD were the best scoring models in the work by Vuli\u0107 and Moens (2013a). (4) TPPMI+cos.", "startOffset": 24, "endOffset": 196}, {"referenceID": 77, "context": "SF: Hellinger distance (Pollard, 2001; Cha, 2007). (3) AMu+HD. RM: Association-MuPTM. SF: Hellinger distance. BMu+HD and AMu+HD were the best scoring models in the work by Vuli\u0107 and Moens (2013a). (4) TPPMI+cos. RM: Traditional-PPMI. SF: cosine similarity. This combination was the best scoring BLE model in the work by Vuli\u0107 and Moens (2013b), and it currently obtains the best reported results in the BLE task for these training and test data.", "startOffset": 24, "endOffset": 344}, {"referenceID": 79, "context": "The lower results of TPPMI+cos compared to other two baseline models are also attributed to the overall lower quality and size of NL-EN training data, which is then reflected in a lower quality of seed lexicons necessary to start the bootstrapping procedure from Vuli\u0107 and Moens (2013b). Computational Complexity.", "startOffset": 240, "endOffset": 287}, {"referenceID": 12, "context": ", parallel data, translation pairs): We compare against two benchmarking BWE induction models that rely on parallel data (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 121, "endOffset": 163}, {"referenceID": 32, "context": ", parallel data, translation pairs): We compare against two benchmarking BWE induction models that rely on parallel data (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 121, "endOffset": 163}, {"referenceID": 60, "context": "Another BWE induction that can be used in this setting with document-aligned data is the well known model from Mikolov et al. (2013b): First, two monolingual embedding spaces are induced separately in each of the two languages using a standard monolingual WE model such as SGNS.", "startOffset": 111, "endOffset": 134}, {"referenceID": 62, "context": "Table 4: BLE results: BWE induction model from Mikolov et al. (2013b) relying on SGNS.", "startOffset": 47, "endOffset": 70}, {"referenceID": 12, "context": "Table 5: BLE results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 12, "context": "Table 5: BLE results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al. (2015). d=40 as in their original work for all vectors.", "startOffset": 57, "endOffset": 100}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.", "startOffset": 71, "endOffset": 93}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.10 We also train 40-dimensional embeddings from Gouws et al. (2015) (the same dimensionality as in the original paper) using the same data and setup as Gouws et al.", "startOffset": 71, "endOffset": 188}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.10 We also train 40-dimensional embeddings from Gouws et al. (2015) (the same dimensionality as in the original paper) using the same data and setup as Gouws et al. (2015) with suggested parameters for the three language pairs.", "startOffset": 71, "endOffset": 292}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.10 We also train 40-dimensional embeddings from Gouws et al. (2015) (the same dimensionality as in the original paper) using the same data and setup as Gouws et al. (2015) with suggested parameters for the three language pairs.11 The comparison with these two BWE induction models is provided in tab. 5. To make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2015) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligned non-parallel data produces significantly higher BLI scores for IT-EN and ES-EN with sufficiently large windows.", "startOffset": 71, "endOffset": 675}, {"referenceID": 51, "context": "We have again tested various families of SFs (Lee, 1999; Cha, 2007) for each baseline word representation, and have opted for the best scoring SF for each representation.", "startOffset": 45, "endOffset": 67}, {"referenceID": 61, "context": "We have also experimented with the context-sensitive CLSS models proposed by Melamud et al. (2015), but we do not report the actual scores as this model, although displaying a similar relative ranking of different representation models, was consistently outperformed by the models from (Vuli\u0107 & Moens, 2014) in our evaluation runs: \u22480.", "startOffset": 77, "endOffset": 99}, {"referenceID": 61, "context": "We have also experimented with the context-sensitive CLSS models proposed by Melamud et al. (2015), but we do not report the actual scores as this model, although displaying a similar relative ranking of different representation models, was consistently outperformed by the models from (Vuli\u0107 & Moens, 2014) in our evaluation runs: \u22480.75-0.80 vs \u2248 0.60-0.65 for the models from Melamud et al. (2015).", "startOffset": 77, "endOffset": 400}, {"referenceID": 62, "context": "Table 8: SWTC results: BWE induction model from Mikolov et al. (2013b) relying on SGNS.", "startOffset": 48, "endOffset": 71}, {"referenceID": 12, "context": "Table 9: SWTC results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 12, "context": "Table 9: SWTC results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al. (2015). d=40 as in their original work for all vectors.", "startOffset": 58, "endOffset": 101}, {"referenceID": 33, "context": "This finding is in line with the recent work from Gouws and S\u00f8gaard (2015).", "startOffset": 50, "endOffset": 75}, {"referenceID": 12, "context": "8, while the results with the other two models (Chandar et al., 2014; Gouws et al., 2015) are provided in tab.", "startOffset": 47, "endOffset": 89}, {"referenceID": 32, "context": "8, while the results with the other two models (Chandar et al., 2014; Gouws et al., 2015) are provided in tab.", "startOffset": 47, "endOffset": 89}, {"referenceID": 60, "context": "The results for the BWE induction model from Mikolov et al. (2013b) are summarized in tab.", "startOffset": 45, "endOffset": 68}, {"referenceID": 62, "context": "The model from Mikolov et al. (2013b) constitutes a stronger baseline: Good results in the SWTC task with this model are an interesting finding per se.", "startOffset": 15, "endOffset": 38}, {"referenceID": 12, "context": "Our new BWESG-based BLE and SWTC models outperform previous state-of-the-art models for BLE and SWTC from document-aligned comparable data and related BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 172, "endOffset": 237}, {"referenceID": 32, "context": "Our new BWESG-based BLE and SWTC models outperform previous state-of-the-art models for BLE and SWTC from document-aligned comparable data and related BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 172, "endOffset": 237}, {"referenceID": 2, "context": "The findings in this article follow the recently published surveys from Baroni et al. (2014), Levy et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 2, "context": "The findings in this article follow the recently published surveys from Baroni et al. (2014), Levy et al. (2015) regarding a solid and robust performance of neural word representations/word embeddings in semantic tasks: our new BWESG-based models for BLE and SWTC significantly outscore previous state-of-theart distributional approaches on both tasks across different parameter settings.", "startOffset": 72, "endOffset": 113}, {"referenceID": 53, "context": "Moreover, BWEs induced by BWESG may be used in other semantic tasks besides the ones discussed in this work, and it would be interesting to experiment with other types of context aggregation and selection beyond the bag-of-words assumption, such as dependency-based contexts (Levy & Goldberg, 2014a), or other objective functions during training in the same vein as proposed by Levy and Goldberg (2014b). Similar to the evolution in multilingual probabilistic topic modeling, another path of future work may lead to investigating bilingual models for learning BWEs which will be able to jointly learn from separate documents in aligned document pairs, without the need to construct pseudo-bilingual documents.", "startOffset": 378, "endOffset": 404}, {"referenceID": 109, "context": "Such low-cost multilingual embeddings beyond the word level extracted from comparable data may find its application in a variety of tasks such as statistical machine translation (Mikolov et al., 2013b; Zou et al., 2013; Zhang, Liu, Li, Zhou, & Zong, 2014; Wu et al., 2014), semantic tasks such as multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual", "startOffset": 178, "endOffset": 272}, {"referenceID": 103, "context": "Such low-cost multilingual embeddings beyond the word level extracted from comparable data may find its application in a variety of tasks such as statistical machine translation (Mikolov et al., 2013b; Zou et al., 2013; Zhang, Liu, Li, Zhou, & Zong, 2014; Wu et al., 2014), semantic tasks such as multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual", "startOffset": 178, "endOffset": 272}, {"referenceID": 96, "context": "information retrieval (Vuli\u0107 et al., 2013; Vuli\u0107 & Moens, 2015) or cross-lingual document classification (Klementiev et al.", "startOffset": 22, "endOffset": 63}, {"referenceID": 44, "context": ", 2013; Vuli\u0107 & Moens, 2015) or cross-lingual document classification (Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).", "startOffset": 70, "endOffset": 143}, {"referenceID": 12, "context": ", 2013; Vuli\u0107 & Moens, 2015) or cross-lingual document classification (Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).", "startOffset": 70, "endOffset": 143}], "year": 2015, "abstractText": "We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and contextcounting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.", "creator": "TeX"}}}