{"id": "1509.08360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2015", "title": "Compressive spectral embedding: sidestepping the SVD", "abstract": "spectral compression based on the singular decomposition decomposition ( svd ) is a widely used \" preprocessing \" step in many learning tasks, typically leading to dimensionality reduction by decomposition onto a number of dominant singular ones and rescaling distinct coordinate axes ( by a predefined function of the singular value ). however, the number of such vectors required so capture problem structure grows with problem depth, and even partial svd computation task gradually staple. in this paper, we propose utilizing low - budget it compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations inside svd - based embedding. for an m times n matrix with t non - integer, its time complexity is o ( ( t + m + t ) log ( m + n ) ), and the embedding size is o ( log ( 1 + \u221e ) ), both of which are independent onto the number of singular vectors utilizing gradient we like to capture. to the best of our knowledge, this is the first work a circumvent this dependence on the number of singular vectors for orthogonal svd - based embeddings. the key to researching the svd is the reason that, for downstream inference tasks such as clustering and renaming, we are only interested in using the resulting embedding to evaluate pairwise similarity schemes derived from the euclidean blocks, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial svd tries to do. our numerical results on network datasets demonstrate empirical efficacy confronting the downstream implementations, and motivate further exploration of its application given large - scale inference tasks.", "histories": [["v1", "Mon, 28 Sep 2015 15:32:20 GMT  (27kb)", "http://arxiv.org/abs/1509.08360v1", "NIPS 2015"]], "COMMENTS": "NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dinesh ramasamy", "upamanyu madhow"], "accepted": true, "id": "1509.08360"}, "pdf": {"name": "1509.08360.pdf", "metadata": {"source": "CRF", "title": "Compressive spectral embedding: sidestepping the SVD", "authors": ["Dinesh Ramasamy"], "emails": ["dineshr@ece.ucsb.edu", "madhow@ece.ucsb.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n08 36\n0v 1\n[ st\nat .M\nL ]\n2 8\nSe p"}, {"heading": "1 Introduction", "text": "Inference tasks encountered in natural language processing, graph inference and manifold learning employ the singular value decomposition (SVD) as a first step to reduce dimensionality while retaining useful structure in the input. Such spectral embeddings go under various guises: Principle Component Analysis (PCA), Latent Semantic Indexing (natural language processing), Kernel Principal Component Analysis, commute time and diffusion embeddings of graphs, to name a few. In this paper, we present a compressive approach for accomplishing SVD-based dimensionality reduction, or embedding, without actually performing the computationally expensive SVD step.\nThe setting is as follows. The input is represented in matrix form. This matrix could represent the adjacency matrix or the Laplacian of a graph, the probability transition matrix of a random walker on the graph, a bag-of-words representation of documents, the action of a kernel on a set of l points {x(p) \u2208 Rd : p = 1, . . . ,m} (kernel PCA)[1][2] such as\nA(p, q) = e\u2212\u2016x(p)\u2212x(q)\u2016 2/2\u03b12 (or) A(p, q) = I(\u2016x(p)\u2212 x(q)\u2016 < \u03b1), 1 \u2264 p, q \u2264 l, (1)\nwhere I(\u00b7) denotes the indicator function or matrices derived from K-nearest-neighbor graphs constructed from {x(p)}. We wish to compute a transformation of the rows of this m \u00d7 n matrix A which succinctly captures the global structure of A via euclidean distances (or similarity metrics derived from the \u21132-norm, such as normalized correlations). A common approach is to com-\npute a partial SVD of A, \u2211l=k\nl=1 \u03c3lulv T l , k \u226a n, and to use it to embed the rows of A into a\nk-dimensional space using the rows of E = [f(\u03c31)u1 f(\u03c32)u2 \u00b7 \u00b7 \u00b7 f(\u03c3k)uk], for some function f(\u00b7). The embedding of the variable corresponding to the l-th row of the matrix A is the l-th row of E. For example, f(x) = x corresponds to Principal Component Analysis (PCA): the k-dimensional rows of E are projections of the n-dimensional rows of A along the first k principal components, {vl, l = 1, . . . , k}. Other important choices include f(x) = constant used to cut graphs [3] and f(x) = 1 /\u221a 1\u2212 x for commute time embedding of graphs [4]. Inference tasks such as (unsupervised) clustering and (supervised) classification are performed using \u21132-based pairwise similarity metrics on the embedded coordinates (rows of E) instead of the ambient data (rows of A).\nBeyond the obvious benefit of dimensionality reduction from n to k, embeddings derived from the leading partial-SVD can often be interpreted as denoising, since the \u201cnoise\u201d in matrices arising from real-world data manifests itself via the smaller singular vectors of A (e.g., see [5], which analyzes graph adjacency matrices). This is often cited as a motivation for choosing PCA over \u201cisotropic\u201d dimensionality reduction techniques such as random embeddings, which, under the setting of the Johnson-Lindenstrauss (JL) lemma, can also preserve structure.\nThe number of singular vectors k needed to capture the structure of an m\u00d7 n matrix grows with its size, and two bottlenecks emerge as we scale: (a) The computational effort required to extract a large number of singular vectors using conventional iterative methods such as Lanczos or simultaneous iteration or approximate algorithms like Nystrom [6], [7] and Randomized SVD [8] for computation of partial SVD becomes prohibitive (scaling as \u2126(kT ), where T is the number of non-zeros in A) (b) the resulting k-dimensional embedding becomes unwieldy for use in subsequent inference steps.\nApproach and Contributions: In this paper, we tackle these scalability bottlenecks by focusing on what embeddings are actually used for: computing \u21132-based pairwise similarity metrics typically used for supervised or unsupervised learning. For example, K-means clustering uses pairwise Euclidean distances, and SVM-based classification uses pairwise inner products. We therefore ask the following question: \u201cIs it possible to compute an embedding which captures the pairwise euclidean distances between the rows of the spectral embedding E = [f(\u03c31)u1 \u00b7 \u00b7 \u00b7 f(\u03c3k)uk], while sidestepping the computationally expensive partial SVD?\u201d We answer this question in the affirmative by presenting a compressive algorithm which directly computes a low-dimensional embedding.\nThere are two key insights that drive our algorithm: \u2022 By approximating f(\u03c3) by a low-order (L \u226a min{m,n}) polynomial, we can compute the embedding iteratively using matrix-vector products of the form Aq or ATq. \u2022 The iterations can be computed compressively: by virtue of the celebrated JL lemma, the embedding geometry is approximately captured by a small number d = O(log(m + n)) of randomly picked starting vectors.\nThe number of passes over A, AT and time complexity of the algorithm are L, L and O(L(T +m+ n) log(m + n)) respectively. These are all independent of the number of singular vectors k whose effect we wish to capture via the embedding. This is in stark contrast to embedding directly based on the partial SVD. Our algorithm lends itself to parallel implementation as a sequence of 2L matrixvector products interlaced with vector additions, run in parallel across d = O(log(m+n)) randomly chosen starting vectors. This approach significantly reduces both computational complexity and embedding dimensionality relative to partial SVD. A freely downloadable Python implementation of the proposed algorithm that exploits this inherent parallelism can be found in [9]."}, {"heading": "2 Related work", "text": "As discussed in Section 3.1, the concept of compressive measurements forms a key ingredient in our algorithm, and is based on the JL lemma [10]. The latter, which provides probabilistic guarantees on approximate preservation of the Euclidean geometry for a finite collection of points under random projections, forms the basis for many other applications, such as compressive sensing [11].\nWe now mention a few techniques for exact and approximate SVD computation, before discussing algorithms that sidestep the SVD as we do. The time complexity of the full SVD of an m \u00d7 n matrix is O(mn2) (for m > n). Partial SVDs are computed using iterative methods for eigen decompositions of symmetric matrices derived from A such as AAT and [ 0 AT ;A 0 ] [12]. The\ncomplexity of standard iterative eigensolvers such as simultaneous iteration[13] and the Lanczos method scales as \u2126(kT ) [12], where T denotes the number of non-zeros of A. The leading k singular value, vector triplets {(\u03c3l,ul,vl), l = 1, . . . , k} minimize the matrix reconstruction error under a rank k constraint: they are a solution to the optimization problem argmin\u2016A\u2212\u2211l=kl=1 \u03c3lulvTl \u20162F , where \u2016 \u00b7 \u2016F denotes the Frobenius norm. Approximate SVD algorithms strive to reduce this error while also placing constraints on the computational budget and/or the number of passes over A. A commonly employed approximate eigendecomposition algorithm is the Nystrom method [6], [7] based on random sampling of s columns of A, which has time complexity O(ksn+ s3). A number of variants of the Nystrom method for kernel matrices like (1) have been proposed in the literature. These aim to improve accuracy using preprocessing steps such as K-means clustering [14] or random projection trees [15]. Methods to reduce the complexity of the Nystrom algorithm to O(ksn+ k3)[16], [17] enable Nystrom sketches that see more columns of A. The complexity of all of these grow as \u2126(ksn). Other randomized algorithms, involving iterative computations, include the Randomized SVD [8]. Since all of these algorithms set out to recover k-leading eigenvectors (exact or otherwise), their complexity scales as \u2126(kT ). We now turn to algorithms that sidestep SVD computation. In [18], [19], vertices of a graph are embedded based on diffusion of probability mass in random walks on the graph, using the power iteration run independently on random starting vectors, and stopping \u201cprior to convergence.\u201d While this approach is specialized to probability transition matrices (unlike our general framework) and does not provide explicit control on the nature of the embedding as we do, a feature in common with the present paper is that the time complexity of the algorithm and the dimensionality of the resulting embedding are independent of the number of eigenvectors k captured by it. A parallel implementation of this algorithm was considered in [20]; similar parallelization directly applies to our algorithm. Another specific application that falls within our general framework is the commute time embedding on a graph, based on the normalized adjacency matrix and weighing function f(x) = 1/ \u221a 1\u2212 x [4], [21]. Approximate commute time embeddings have been computed using Spielman-Teng solvers [22], [23] and the JL lemma in [24]. The complexity of the latter algorithm and the dimensionality of the resulting embedding are comparable to ours, but the method is specially designed for the normalized adjacency matrix and the weighing function f(x) = 1/ \u221a 1\u2212 x. Our more general framework would, for example, provide the flexibility of suppressing small eigenvectors from contributing to the embedding (e.g, by setting f(x) = I(x > \u01eb)/ \u221a 1\u2212 x).\nThus, while randomized projections are extensively used in the embedding literature, to the best of our knowledge, the present paper is the first to develop a general compressive framework for spectral embeddings derived from the SVD. It is interesting to note that methods similar to ours have been used in a different context, to estimate the empirical distribution of eigenvalues of a large hermitian matrix [25], [26]. These methods use a polynomial approximation of indicator functions f(\u03bb) = I(a \u2264 \u03bb \u2264 b) and random projections to compute an approximate histogram of the number of eigenvectors across different bands of the spectrum: [a, b] \u2286 [\u03bbmin, \u03bbmax]."}, {"heading": "3 Algorithm", "text": "We first present the algorithm for a symmetric n \u00d7 n matrix S. Later, in Section 3.5, we show how to handle a general m \u00d7 n matrix by considering a related (m + n) \u00d7 (m + n) symmetric matrix. Let \u03bbl denote the eigenvalues of S sorted in descending order and vl their corresponding unit-norm eigenvectors (chosen to be orthogonal in case of repeated eigenvalues). For any function g(x) : R 7\u2192 R, we denote by g(S) the n \u00d7 n symmetric matrix g(S) = \u2211l=n l=1 g(\u03bbl)vlv T l . We now develop an O(n log n) algorithm to compute a d = O(log n) dimensional embedding which approximately captures pairwise euclidean distances between the rows of the embedding E = [f (\u03bb1)v1 f (\u03bb2)v2 \u00b7 \u00b7 \u00b7 f (\u03bbn)vn]. Rotations are inconsequential: We first observe that rotation of basis does not alter \u21132-based similarity metrics. Since V = [v1 \u00b7 \u00b7 \u00b7vn] satisfies V V T = V TV = In, pairwise distances between the rows of E are equal to corresponding pairwise distances between the rows of EV T =\u2211l=n\nl=1 f(\u03bbl)vlv T l = f(S). We use this observation to compute embeddings of the rows of f(S)\nrather than those of E."}, {"heading": "3.1 Compressive embedding", "text": "Suppose now that we know f(S). This constitutes an n-dimensional embedding, and similarity queries between two \u201cvertices\u201d (we refer to the variables corresponding to rows of S as vertices, as we would for matrices derived from graphs) requires O(n) operations. However, we can reduce this time to O(log n) by using the JL lemma, which informs us that pairwise distances can be approximately captured by compressive projection onto d = O(log n) dimensions. Specifically, for d > (4 + 2\u03b2) logn /( \u01eb2/2\u2212 \u01eb3/3 )\n, let \u2126 denote an n\u00d7 d matrix with i.i.d. entries drawn uniformly at random from {\u00b11/ \u221a d}. According to the JL lemma, pairwise distances between the rows of f(S)\u2126 approximate pairwise distances between the rows of f(S) with high probability. In particular, the following statement holds with probability at least 1 \u2212 n\u2212\u03b2 : (1 \u2212 \u01eb) \u2016u\u2212 v\u20162 \u2264 \u2016(u\u2212 v) \u2126\u20162 \u2264 (1 + \u01eb) \u2016u\u2212 v\u20162, for any two rows u,v of f(S). The key take-aways are that (a) we can reduce the embedding dimension to d = O(log n), since we are only interested in pairwise similarity measures, and (b) We do not need to compute f(S). We only need to compute f(S)\u2126. We now discuss how to accomplish the latter efficiently."}, {"heading": "3.2 Polynomial approximation of embedding", "text": "Direct computation of E\u2032 = f(S)\u2126 from the eigenvectors and eigenvalues of S, as f(S) =\u2211 f(\u03bbl)vlv T l would suggest, is expensive (O(n\n3)). However, we now observe that computation of \u03c8(S)\u2126 is easy when \u03c8(\u00b7) is a polynomial. In this case, \u03c8(S) = \u2211p=Lp=0 bpSp for some bp \u2208 R, so that \u03c8(S)\u2126 can be computed as a sequence of L matrix-vector products interlaced with vector additions run in parallel for each of the d columns of \u2126. Therefore, they only require LdT +O(Ldn) flops. Our strategy is to approximate E\u2032 = f(S)\u2126 by E\u0303 = f\u0303L(S)\u2126, where f\u0303L(x) is an L-th order polynomial approximation of f(x). We defer the details of computing a \u201cgood\u201d polynomial approximation to Section 3.4. For now, we assume that one such approximation f\u0303L(\u00b7) is available and give bounds on the loss in fidelity as a result of this approximation."}, {"heading": "3.3 Performance guarantees", "text": "The spectral norm of the \u201cerror matrix\u201d Z = f(S)\u2212 f\u0303(S) = \u2211r=nr=1 (f(\u03bbr)\u2212 f\u0303L(\u03bbr))vrvTr satisfies \u2016Z\u2016 = \u03b4 = maxl|f(\u03bbl)\u2212 f\u0303L(\u03bbl)| \u2264 max{|f(x)\u2212 f\u0303L(x)|}, where the spectral norm of a matrix B, denoted by \u2016B\u2016 refers to the induced \u21132-norm. For symmetric matrices, \u2016B\u2016 \u2264 \u03b1 \u21d0\u21d2 |\u03bbl| \u2264 \u03b1 \u2200l, where \u03bbl are the eigenvalues of B. Letting ip denote the unit vector along the p-th coordinate of Rn, the distance between the p, q-th rows of f\u0303(S) can be written as\n\u2016f\u0303L(S) (ip \u2212 iq)\u2016 = \u2016f(S) (ip \u2212 iq)\u2212 Z (ip \u2212 iq)\u2016 \u2264 \u2016ET (ip \u2212 iq)\u2016+ \u03b4 \u221a 2. (2)\nSimilarly, we have that \u2016f\u0303L(S) (ip \u2212 iq)\u2016 \u2265 \u2016ET (ip \u2212 iq)\u2016 \u2212 \u03b4 \u221a 2. Thus pairwise distances be-\ntween the rows of f\u0303L(S) approximate those between the rows of E. However, the distortion term \u03b4 \u221a 2 is additive and must be controlled by carefully choosing f\u0303L(\u00b7), as discussed in Section 4. Applying the JL lemma [10] to the rows of f\u0303L(S), we have that when d > O ( \u01eb\u22122 logn ) with i.i.d. entries drawn uniformly at random from {\u00b11/ \u221a d}, the embedding E\u0303 = f\u0303L(S)\u2126 captures pairwise distances between the rows of f\u0303L(S) up to a multiplicative distortion of 1\u00b1 \u01eb with high probability: \u2225\u2225\u2225E\u0303T (ip \u2212 iq) \u2225\u2225\u2225 = \u2225\u2225\u2225\u2126T f\u0303L(S) (ip \u2212 iq) \u2225\u2225\u2225 \u2264 \u221a 1 + \u01eb \u2225\u2225\u2225f\u0303L(S) (ip \u2212 iq) \u2225\u2225\u2225\nUsing (2), we can show that \u2016E\u0303T (ip \u2212 iq)\u2016 \u2264 \u221a 1 + \u01eb ( \u2016ET (ip \u2212 iq)\u2016+ \u03b4 \u221a 2 ) . Similarly, \u2016E\u0303T (ip \u2212 iq)\u2016 \u2265 \u221a 1\u2212 \u01eb ( \u2016ET (ip \u2212 iq)\u2016 \u2212 \u03b4 \u221a 2 ) . We state this result in Theorem 1.\nTheorem 1. Let f\u0303L(x) denote an L-th order polynomial such that: \u03b4 = maxl|f(\u03bbl)\u2212 f\u0303L(\u03bbl)| \u2264 max|f(x)\u2212 f\u0303L(x)| and \u2126 an n \u00d7 d matrix with entries drawn independently and uniformly at random from {\u00b11/ \u221a d}, where d is an integer satisfying d > (4 + 2\u03b2) log n / (\u01eb2/2\u2212 \u01eb3/3) . Let\ng : Rp \u2192 Rd denote the mapping from the i-th row of E = [f (\u03bb1)v1 \u00b7 \u00b7 \u00b7 f (\u03bbn)vn] to the i-th row of E\u0303 = f\u0303L(S)\u2126. The following statement is true with probability at least 1\u2212 n\u2212\u03b2:\u221a\n1\u2212 \u01eb(\u2016u\u2212 v\u2016 \u2212 \u03b4 \u221a 2) \u2264 \u2016g(u)\u2212 g(v)\u2016 \u2264 \u221a 1 + \u01eb(\u2016u\u2212 v\u2016+ \u03b4 \u221a 2)\nfor any two rows u, v of E. Furthermore, there exists an algorithm to compute each of the d = O(log n) columns of E\u0303 in O(L(T + n)) flops independent of its other columns which makes L passes over S (T is the number of non-zeros in S)."}, {"heading": "3.4 Choosing the polynomial approximation", "text": "We restrict attention to matrices which satisfy \u2016S\u2016 \u2264 1, which implies that |\u03bbl| \u2264 1. We observe that we can trivially center and scale the spectrum of any matrix to satisfy this assumption when we have the following bounds: \u03bbl \u2264 \u03c3max and \u03bbl \u2265 \u03c3min via the rescaling and centering operation given by: S\u2032 = 2S/(\u03c3max \u2212 \u03c3min) \u2212 (\u03c3max + \u03c3min) In/(\u03c3max \u2212 \u03c3min) and by modifying f(x) to f \u2032(x) = f (x (\u03c3max \u2212 \u03c3min)/2 + (\u03c3max + \u03c3min)/2). In order to compute a polynomial approximation of f(x), we need to define the notion of \u201cgood\u201d approximation. We showed in Section 3.3 that the errors introduced by the polynomial approximation can be summarized by furnishing a bound on the spectral norm of the error matrix Z = f(S) \u2212 f\u0303L(S): Since \u2016Z\u2016 = \u03b4 = maxl|f(\u03bbl) \u2212 f\u0303L(\u03bbl)|, what matters is how well we approximate the function f(\u00b7) at the eigenvalues {\u03bbl} of S. Indeed, if we know the eigenvalues, we can minimize \u2016Z\u2016 by minimizing maxl|f(\u03bbl) \u2212 f\u0303L(\u03bbl)|. This is not a particularly useful approach, since computing the eigenvalues is expensive. However, we can use our prior knowledge of the domain from which the matrix S comes from to penalize deviations from f(\u03bb) differently for different values of \u03bb. For example, if we know the distribution p(x) of the eigenvalues of S, we can minimize the average error \u2206L = \u222b 1 \u22121 p(\u03bb)|f(\u03bb) \u2212 f\u0303L(\u03bb)|2dx. In our examples, for the sake of concreteness, we assume that the eigenvalues are uniformly distributed over [\u22121, 1] and give a procedure to compute an L-th order polynomial approximation of f(x) that minimizes \u2206L = (1/2) \u222b 1 \u22121\n|f(x)\u2212 f\u0303L(x)|2dx. A numerically stable procedure to generate finite order polynomial approximations of a function over [\u22121, 1] with the objective of minimizing \u222b 1 \u22121\n|f(x) \u2212 f\u0303L(x)|2dx is via Legendre polynomials p(r, x), r = 0, 1, . . . , L. They satisfy the recursion p(r, x) = (2 \u2212 1/r)xp(r \u2212 1, x) \u2212 (1 \u2212 1/r)p(r \u2212 2, x) and are orthogonal: \u222b 1 \u22121 p(k, x)p(l, x)dx = 2I(k = l)/(2r + 1) . Therefore we set f\u0303L(x) = \u2211r=L r=0 a(r)p(r, x) where a(r) = (r + 1/2) \u222b 1 \u22121 p(r, x)f(x)dx. We give a method in Algorithm 1 that uses the Legendre recursion to compute p(r, S)\u2126, r = 0, 1, . . . , L using Ld matrix-vector products and vector additions. The coefficients a(r) are used to compute f\u0303L(S)\u2126 by adding weighted versions of p(r, S)\u2126.\nAlgorithm 1 Proposed algorithm to compute approximate d-dimensional eigenvector embedding of a n\u00d7 n symmetric matrix S (such that \u2016S\u2016 \u2264 1) using the n\u00d7 d random projection matrix \u2126.\n1: Procedure FASTEMBEDEIG(S, f(x), L, \u2126): 2: //* Compute polynomial approximation f\u0303L(x) which minimizes \u222b 1 \u22121\n|f(x) \u2212 f\u0303L(x)|2dx *// 3: for r = 0, . . . , L do 4: a(r) \u2190 (r + 1/2) \u222b x=1 x=\u22121 f(x)p(r, x)dx //* p(r, x): Order r Legendre polynomial *//\n5: Q(0) \u2190 \u2126, Q(\u22121) \u2190 0, E\u0303 \u2190 a(0)Q(0) 6: for r = 1, 2, . . . , L do 7: Q(r) \u2190 (2\u2212 1/r)SQ(r \u2212 1)\u2212 (1\u2212 1/r)Q(r \u2212 2) //* Q(r) = p(r, S)\u2126 *// 8: E\u0303 \u2190 E\u0303 + a(r)Q(r) //* E\u0303 now holds f\u0303r(S)\u2126 *//\n9: return E\u0303 //* E\u0303 = f\u0303L(S)\u2126 *//\nAs described in Section 4, if we have prior knowledge of the distribution of eigenvalues (as we do for many commonly encountered large matrices), then we can \u201cboost\u201d the performance of the generic Algorithm 1 based on the assumption of eigenvalues uniformly distributed over [\u22121, 1]."}, {"heading": "3.5 Embedding general matrices", "text": "We complete the algorithm description by generalizing to any m \u00d7 n matrix A (not necessarily symmetric) such that \u2016A\u2016 \u2264 1. The approach is to utilize Algorithm 1 to compute an approximate d-dimensional embedding of the symmetric matrix S = [0 AT ;A 0]. Let {(\u03c3l,ul,vl) : l = 1, . . . ,min{m,n}} be an SVD of A = \u2211 l \u03c3lulv T l (\u2016A\u2016 \u2264 1 \u21d0\u21d2 \u03c3l \u2264 1). Consider the following spectral mapping of the rows of A to the rows of Erow = [f(\u03c31)u1 \u00b7 \u00b7 \u00b7 f(\u03c3m)um] and the columns of A to the rows of Ecol = [f(\u03c31)v1 \u00b7 \u00b7 \u00b7 f(\u03c3n)vn]. It can be shown that the unit-norm orthogonal eigenvectors of S take the form [vl;ul] /\u221a 2 and [vl;\u2212ul] /\u221a\n2 , l = 1, . . . ,min{m,n}, and their corresponding eigenvalues are \u03c3l and \u2212\u03c3l respectively. The remaining |m \u2212 n| eigenvalues of S are equal to 0. Therefore, we call E\u0303all \u2190 FASTEMBEDEIG(S, f \u2032(x), L,\u2126) with f \u2032(x) = f(x)I(x \u2265 0) \u2212 f(\u2212x)I(x < 0) and \u2126 is an (m + n)\u00d7 d, d = O(log(m + n)) matrix (entries drawn independently and uniformly at random from {\u00b11/ \u221a d}). Let E\u0303col and E\u0303row denote the first n and last m rows of E\u0303all. From Theorem 1,\nwe know that, with overwhelming probability, pairwise distances between any two rows of E\u0303row approximates those between corresponding rows of Erow. Similarly, pairwise distances between any two rows of E\u0303col approximates those between corresponding rows of Ecol."}, {"heading": "4 Implementation considerations", "text": "We now briefly go over implementation considerations before presenting numerical results in Section 5.\nSpectral norm estimates In order to ensure that the eigenvalues of S are within [\u22121, 1] as we have assumed, we scale the matrix by its spectral norm (\u2016S\u2016 = max|\u03bbl|). To this end, we obtain a tight lower bound (and a good approximation) on the spectral norm using power iteration (20 iterates on 6 logn randomly chosen starting vectors), and then scale this up by a small factor (1.01) for our estimate (typically an upper bound) for \u2016S\u2016.\nPolynomial approximation order L: The error in approximating f(\u03bb) by f\u0303L(\u03bb), as measured by \u2206L = \u222b 1 \u22121 |f(x)\u2212 f\u0303L(x)|2dx is a non-increasing function of the polynomial order L. Reduction in \u2206L often corresponds to a reduction in \u03b4 that appears as a bound on distortion in Theorem 1. \u201cSmooth\u201d functions generally admit a lower order approximation for the same target error \u2206L, and hence yield considerable savings in algorithm complexity, which scales linearly with L.\nPolynomial approximation method: The rate at which \u03b4 decreases as we increase L depends on the function p(\u03bb) used to compute f\u0303L(\u03bb) (by minimizing \u2206L = \u222b p(\u03bb)|f(\u03bb) \u2212 f\u0303L(\u03bb)|2dx). The\nchoice p(\u03bb) \u221d 1 yields the Legendre recursion used in Algorithm 1, whereas p(\u03bb) \u221d 1/ \u221a 1\u2212 \u03bb2 corresponds to the Chebyshev recursion, which is known to result in fast convergence. We defer to future work a detailed study of the impact of alternative choices for p(\u03bb) on \u03b4.\nDenoising by cascading In large-scale problems, it may be necessary to drive the contribution from certain singular vectors to zero. In many settings, singular vectors with smaller singular values correspond to noise. The number of such singular values can scale as fast as O(min{m,n}). Therefore, when we place nulls (zeros) in f(\u03bb), it is desirable to ensure that these nulls are pronounced after we approximate f(\u03bb) by f\u0303L(\u03bb). We do this by computing ( g\u0303L/b(S) )b \u2126, where g\u0303L/b(\u03bb) is an L/bth order approximation of g(\u03bb) = f1/b(\u03bb). The small values in the polynomial approximation of f1/b(\u03bb) which correspond to f(\u03bb) = 0 (nulls which we have set) get amplified when we pass them through the xb non-linearity."}, {"heading": "5 Numerical results", "text": "While the proposed approach is particularly useful for large problems in which exact eigendecomposition is computationally infeasible, for the purpose of comparison, our results are restricted to smaller settings where the exact solution can be computed. We compute the exact partial eigendecomposition using the ARPACK library (called from MATLAB). For a given choice of weighing\nfunction f(\u03bb), the associated embedding E = [f(\u03bb1)v1 \u00b7 \u00b7 \u00b7 f(\u03bbn)vn] is compared with the compressive embedding E\u0303 returned by Algorithm 1. The latter was implemented in Python using the Scipy\u2019s sparse matrix-multiplication routines and is available for download from [9].\nWe consider two real world undirected graphs in [27] for our evaluation, and compute embeddings for the normalized adjacency matrix A\u0303 (= D\u22121/2AD\u22121/2, where D is a diagonal matrix with row sums of the adjacency matrix A; the eigenvalues of A\u0303 lie in [\u22121, 1]) for graphs. We study the accuracy of embeddings by comparing pairwise normalized correlations between i, j-th rows of E given by < E(i, :), E(j, :) >/\u2016E(i, :)\u2016\u2016E(j, :)\u2016 with those predicted by the approximate embedding < E\u0303(i, :), E\u0303(j, :) > /\u2016E\u0303(i, :)\u2016\u2016E\u0303(j, :)\u2016 (E(i, :) is short-hand for the i-th row of E). DBLP collaboration network [27] is an undirected graph on n = 317080 vertices with 1049866 edges. We compute the leading 500 eigenvectors of the normalized adjacency matrix A\u0303. The smallest of the five hundred eigenvalues is 0.98, so we set f(\u03bb) = I(\u03bb \u2265 0.98) and S = A\u0303 in Algorithm 1 and compare the resulting embedding E\u0303 with E = [v1 \u00b7 \u00b7 \u00b7 v500]. We demonstrate the dependence of the quality of the embedding E\u0303 returned by the proposed algorithm on two parameters: (i) number of random starting vectors d, which gives the dimensionality of the embedding and (ii) the boosting/cascading parameter b using this dataset.\nDependence on the number of random projections d: In Figure (1a), d ranges from 1 to 120 \u2248 9 logn and plot the 1-st, 5-th, 25-th, 50-th, 75-th, 95-th and 99-th percentile values of the deviation between the compressive normalized correlation (from the rows of E\u0303) and the corresponding exact normalized correlation (rows of E). The deviation decreases with increasing d, corresponding to \u21132-norm concentration (JL lemma), but this payoff saturates for large values of d as polynomial approximation errors start to dominate. From the 5-th and 95-th percentile curves, we see that a significant fraction (90%) of pairwise normalized correlations in E\u0303 lie within \u00b10.2 of their corresponding values in E when d = 80 \u2248 6 logn. For Figure (1a), we use L = 180 matrix-vector products for each randomly picked starting vector and set cascading parameter b = 2 for the algorithm in Section 4.\nDependence on cascading parameter b: In Section 4 we described how cascading can help suppress the contribution to the embedding E\u0303 of the eigenvectors whose eigenvalues lie in regions where we have set f(\u03bb) = 0. We illustrate the importance of this boosting procedure by comparing the quality of the embedding E\u0303 for b = 1 and b = 2 (keeping the other parameters of the algorithm in Section 4 fixed: L = 180 matrix-vector products for each of d = 80 randomly picked starting vectors). We report the results in Figure (1b) where we plot percentile values of compressive normalized correlation (from the rows of E\u0303) for different values of the exact normalized correlation (rows of E). For b = 1, the polynomial approximation of f(\u03bb) does not suppress small eigenvectors. As a result, we notice a deviation (bias) of the 50-percentile curve (green) from the ideal y = x dotted line drawn (Figure 1b left). This disappears for b = 2 (Figure 1b right).\nThe running time for our algorithm on a standard workstation was about two orders of magnitude smaller than partial SVD using off-the-shelf sparse eigensolvers (e.g., the 80 dimensional embedding of the leading 500 eigenvectors of the DBLP graph took 1 minute whereas their exact computation\ntook 105 minutes). A more detailed comparison of running times is beyond the scope of this paper, but it is clear that the promised gains in computational complexity are realized in practice.\nApplication to graph clustering for the Amazon co-purchasing network [27] : This is an undirected graph on n = 334863 vertices with 925872 edges. We illustrate the potential downstream benefits of our algorithm by applying K-means clustering on embeddings (exact and compressive) of this network. For the purpose of our comparisons, we compute the first 500 eigenvectors for A\u0303 explicitly using an exact eigensolver, and use an 80-dimensional compressive embedding E\u0303 which captures the effect of these, with f(\u03bb) = I(\u03bb \u2265 \u03bb500), where \u03bb500 is the 500th eigenvalue. We compare this against the usual spectral embedding using the first 80 eigenvectors of A\u0303: E = [v1 \u00b7 \u00b7 \u00b7 v80]. We keep the dimension fixed at 80 in the comparison because K-means complexity scales linearly with it, and quickly becomes the bottleneck. Indeed, our ability to embed a large number of eigenvectors directly into a low dimensional space (d \u2248 6 logn) has the added benefit of dimensionality reduction within the subspace of interest (in this case the largest 500 eigenvectors).\nWe consider 25 instances of K-means clustering with K = 200 throughout, reporting the median of a commonly used graph clustering score, modularity [28] (larger values translate to better clustering solutions). The median modularity for clustering based on our embedding E\u0303 is 0.87. This is significantly better than that for E, which yields median modularity of 0.835. In addition, the computational cost for E\u0303 is one-fifth that for E (1.5 minutes versus 10 minutes). When we replace the exact eigenvector embedding E with approximate eigendecomposition using Randomized SVD [8] (parameters: power iterates q = 5 and excess dimensionality l = 10), the time taken reduces from 10 minutes to 17 seconds, but this comes at the expense of inference quality: median modularity drops to 0.748. On the other hand, the median modularity increases to 0.845 when we consider exact partial SVD embedding with 120 eigenvectors. This indicates that our compressive embedding yields better clustering quality because it is able to concisely capture more eigenvectors(500 in this example, compared to 80 and 120 with conventional partial SVD). It is worth pointing out that, even for known eigenvectors, the number of dominant eigenvectors k that yields the best inference performance is often unknown a priori, and is treated as a hyper-parameter. For compressive spectral embedding E\u0303, an elegant approach for implicitly optimizing over k is to use the embedding function f(\u03bb) = I(\u03bb \u2265 c), with c as a hyper-parameter."}, {"heading": "6 Conclusion", "text": "We have shown that random projections and polynomial expansions provide a powerful approach for spectral embedding of large matrices: for an m\u00d7n matrix A, our O((T +m+n) log(m+n)) algorithm computes an O(log(m+n))-dimensional compressive embedding that provably approximates pairwise distances between points in the desired spectral embedding. Numerical results for several real-world data sets show that our method provides good approximations for embeddings based on partial SVD, while incurring much lower complexity. Moreover, our method can also approximate spectral embeddings which depend on the entire SVD, since its complexity does not depend on the number of dominant vectors whose effect we wish to model. A glimpse of this potential is provided by the example of K-means based clustering for estimating sparse-cuts of the Amazon graph, where our method yields much better performance (using graph metrics) than a partial SVD with significantly higher complexity. This motivates further investigation into applications of this approach for improving downstream inference tasks in a variety of large-scale problems."}, {"heading": "Acknowledgments", "text": "This work is supported in part by DARPA GRAPHS (BAA-12-01) and by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies."}], "references": [{"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Artificial Neural Networks ICANN\u201997, ser. Lecture Notes in Computer Science, W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, Eds. Springer Berlin Heidelberg, 1997, pp. 583\u2013588. 8", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Kernel PCA and de-noising in feature spaces", "author": ["S. Mika", "B. Sch\u00f6lkopf", "A.J. Smola", "K.-R. M\u00fcller", "M. Scholz", "G. R\u00e4tsch"], "venue": "Advances in Neural Information Processing Systems, 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "A spectral clustering approach to finding communities in graph.", "author": ["S. White", "P. Smyth"], "venue": "in SDM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Random walks on graphs", "author": ["F. G\u00f6bel", "A.A. Jagers"], "venue": "Stochastic Processes and their Applications, 1974.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1974}, {"title": "Graph spectra and the detectability of community structure in networks", "author": ["R.R. Nadakuditi", "M.E.J. Newman"], "venue": "Physical Review Letters, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 2, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Journal on Machine Learning Resources, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "SIAM Rev., 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Database-friendly random projections", "author": ["D. Achlioptas"], "venue": "Proceedings of the Twentieth ACM SIGMOD- SIGACT-SIGART Symposium on Principles of Database Systems, ser. PODS \u201901, 2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "An introduction to compressive sampling", "author": ["E. Candes", "M. Wakin"], "venue": "Signal Processing Magazine, IEEE, March 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Simultaneous iteration for the matrix eigenvalue problem", "author": ["S.F. McCormick", "T. Noe"], "venue": "Linear Algebra and its Applications, vol. 16, no. 1, pp. 43\u201356, 1977.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1977}, {"title": "Improved Nystr\u00f6m Low-rank Approximation and Error Analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "Proceedings of the 25th International Conference on Machine Learning, ser. ICML \u201908. ACM, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast Approximate Spectral Clustering", "author": ["D. Yan", "L. Huang", "M.I. Jordan"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201909. ACM, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Making Large-Scale Nystr\u00f6m Approximation Possible.", "author": ["M. Li", "J.T. Kwok", "B.-L. Lu"], "venue": "in ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Ensemble Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Power iteration clustering", "author": ["F. Lin", "W.W. Cohen"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable methods for graph-based unsupervised and semi-supervised learning", "author": ["F. Lin"], "venue": "Ph.D. dissertation, Carnegie Mellon University, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "PIC: Parallel power iteration clustering for big data", "author": ["W. Yan", "U. Brahmakshatriya", "Y. Xue", "M. Gilder", "B. Wise"], "venue": "Journal of Parallel and Distributed Computing, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Random walks on graphs: A survey", "author": ["L. Lov\u00e1sz"], "venue": "Combinatorics, Paul erdos is eighty, vol. 2, no. 1, pp. 1\u201346, 1993.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems", "author": ["D.A. Spielman", "S.-H. Teng"], "venue": "Proceedings of the Thirty-sixth Annual ACM Symposium on Theory of Computing, ser. STOC \u201904. New York, NY, USA: ACM, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems", "author": ["D. Spielman", "S. Teng"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 35, Jan. 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Graph sparsification by effective resistances", "author": ["D. Spielman", "N. Srivastava"], "venue": "SIAM Journal on Computing, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel polynomial approximations for densities of states and spectral functions", "author": ["R.N. Silver", "H. Roeder", "A.F. Voter", "J.D. Kress"], "venue": "Journal of Computational Physics, vol. 124, no. 1, pp. 115\u2013130, Mar. 1996.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient estimation of eigenvalue counts in an interval", "author": ["E. Di Napoli", "E. Polizzi", "Y. Saad"], "venue": "arXiv:1308.4275 [cs], Aug. 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Defining and evaluating network communities based on ground-truth", "author": ["J. Yang", "J. Leskovec"], "venue": "2012 IEEE 12th International Conference on Data Mining (ICDM), Dec. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Community detection in graphs", "author": ["S. Fortunato"], "venue": "Physics Reports, vol. 486, no. 3-5, Feb. 2010. 9", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": ",m} (kernel PCA)[1][2] such as A(p, q) = e 2/2\u03b12 (or) A(p, q) = I(\u2016x(p)\u2212 x(q)\u2016 < \u03b1), 1 \u2264 p, q \u2264 l, (1) where I(\u00b7) denotes the indicator function or matrices derived from K-nearest-neighbor graphs constructed from {x(p)}.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": ",m} (kernel PCA)[1][2] such as A(p, q) = e 2/2\u03b12 (or) A(p, q) = I(\u2016x(p)\u2212 x(q)\u2016 < \u03b1), 1 \u2264 p, q \u2264 l, (1) where I(\u00b7) denotes the indicator function or matrices derived from K-nearest-neighbor graphs constructed from {x(p)}.", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "Other important choices include f(x) = constant used to cut graphs [3] and f(x) = 1 /\u221a 1\u2212 x for commute time embedding of graphs [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Other important choices include f(x) = constant used to cut graphs [3] and f(x) = 1 /\u221a 1\u2212 x for commute time embedding of graphs [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": ", see [5], which analyzes graph adjacency matrices).", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "The number of singular vectors k needed to capture the structure of an m\u00d7 n matrix grows with its size, and two bottlenecks emerge as we scale: (a) The computational effort required to extract a large number of singular vectors using conventional iterative methods such as Lanczos or simultaneous iteration or approximate algorithms like Nystrom [6], [7] and Randomized SVD [8] for computation of partial SVD becomes prohibitive (scaling as \u03a9(kT ), where T is the number of non-zeros in A) (b) the resulting k-dimensional embedding becomes unwieldy for use in subsequent inference steps.", "startOffset": 346, "endOffset": 349}, {"referenceID": 6, "context": "The number of singular vectors k needed to capture the structure of an m\u00d7 n matrix grows with its size, and two bottlenecks emerge as we scale: (a) The computational effort required to extract a large number of singular vectors using conventional iterative methods such as Lanczos or simultaneous iteration or approximate algorithms like Nystrom [6], [7] and Randomized SVD [8] for computation of partial SVD becomes prohibitive (scaling as \u03a9(kT ), where T is the number of non-zeros in A) (b) the resulting k-dimensional embedding becomes unwieldy for use in subsequent inference steps.", "startOffset": 351, "endOffset": 354}, {"referenceID": 7, "context": "The number of singular vectors k needed to capture the structure of an m\u00d7 n matrix grows with its size, and two bottlenecks emerge as we scale: (a) The computational effort required to extract a large number of singular vectors using conventional iterative methods such as Lanczos or simultaneous iteration or approximate algorithms like Nystrom [6], [7] and Randomized SVD [8] for computation of partial SVD becomes prohibitive (scaling as \u03a9(kT ), where T is the number of non-zeros in A) (b) the resulting k-dimensional embedding becomes unwieldy for use in subsequent inference steps.", "startOffset": 374, "endOffset": 377}, {"referenceID": 8, "context": "1, the concept of compressive measurements forms a key ingredient in our algorithm, and is based on the JL lemma [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "The latter, which provides probabilistic guarantees on approximate preservation of the Euclidean geometry for a finite collection of points under random projections, forms the basis for many other applications, such as compressive sensing [11].", "startOffset": 239, "endOffset": 243}, {"referenceID": 10, "context": "complexity of standard iterative eigensolvers such as simultaneous iteration[13] and the Lanczos method scales as \u03a9(kT ) [12], where T denotes the number of non-zeros of A.", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "A commonly employed approximate eigendecomposition algorithm is the Nystrom method [6], [7] based on random sampling of s columns of A, which has time complexity O(ksn+ s).", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "A commonly employed approximate eigendecomposition algorithm is the Nystrom method [6], [7] based on random sampling of s columns of A, which has time complexity O(ksn+ s).", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "These aim to improve accuracy using preprocessing steps such as K-means clustering [14] or random projection trees [15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "These aim to improve accuracy using preprocessing steps such as K-means clustering [14] or random projection trees [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Methods to reduce the complexity of the Nystrom algorithm to O(ksn+ k)[16], [17] enable Nystrom sketches that see more columns of A.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Methods to reduce the complexity of the Nystrom algorithm to O(ksn+ k)[16], [17] enable Nystrom sketches that see more columns of A.", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "Other randomized algorithms, involving iterative computations, include the Randomized SVD [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "In [18], [19], vertices of a graph are embedded based on diffusion of probability mass in random walks on the graph, using the power iteration run independently on random starting vectors, and stopping \u201cprior to convergence.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [18], [19], vertices of a graph are embedded based on diffusion of probability mass in random walks on the graph, using the power iteration run independently on random starting vectors, and stopping \u201cprior to convergence.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "A parallel implementation of this algorithm was considered in [20]; similar parallelization directly applies to our algorithm.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "Another specific application that falls within our general framework is the commute time embedding on a graph, based on the normalized adjacency matrix and weighing function f(x) = 1/ \u221a 1\u2212 x [4], [21].", "startOffset": 191, "endOffset": 194}, {"referenceID": 18, "context": "Another specific application that falls within our general framework is the commute time embedding on a graph, based on the normalized adjacency matrix and weighing function f(x) = 1/ \u221a 1\u2212 x [4], [21].", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "Approximate commute time embeddings have been computed using Spielman-Teng solvers [22], [23] and the JL lemma in [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Approximate commute time embeddings have been computed using Spielman-Teng solvers [22], [23] and the JL lemma in [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "Approximate commute time embeddings have been computed using Spielman-Teng solvers [22], [23] and the JL lemma in [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "It is interesting to note that methods similar to ours have been used in a different context, to estimate the empirical distribution of eigenvalues of a large hermitian matrix [25], [26].", "startOffset": 176, "endOffset": 180}, {"referenceID": 23, "context": "It is interesting to note that methods similar to ours have been used in a different context, to estimate the empirical distribution of eigenvalues of a large hermitian matrix [25], [26].", "startOffset": 182, "endOffset": 186}, {"referenceID": 8, "context": "Applying the JL lemma [10] to the rows of f\u0303L(S), we have that when d > O ( \u01eb logn ) with i.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "We consider two real world undirected graphs in [27] for our evaluation, and compute embeddings for the normalized adjacency matrix \u00c3 (= DAD, where D is a diagonal matrix with row sums of the adjacency matrix A; the eigenvalues of \u00c3 lie in [\u22121, 1]) for graphs.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "DBLP collaboration network [27] is an undirected graph on n = 317080 vertices with 1049866 edges.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "Application to graph clustering for the Amazon co-purchasing network [27] : This is an undirected graph on n = 334863 vertices with 925872 edges.", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "We consider 25 instances of K-means clustering with K = 200 throughout, reporting the median of a commonly used graph clustering score, modularity [28] (larger values translate to better clustering solutions).", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "When we replace the exact eigenvector embedding E with approximate eigendecomposition using Randomized SVD [8] (parameters: power iterates q = 5 and excess dimensionality l = 10), the time taken reduces from 10 minutes to 17 seconds, but this comes at the expense of inference quality: median modularity drops to 0.", "startOffset": 107, "endOffset": 110}], "year": 2015, "abstractText": "Spectral embedding based on the Singular Value Decomposition (SVD) is a widely used \u201cpreprocessing\u201d step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value). However, the number of such vectors required to capture problem structure grows with problem size, and even partial SVD computation becomes a bottleneck. In this paper, we propose a low-complexity compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to SVD-based embedding. For anm\u00d7n matrix with T non-zeros, its time complexity is O ((T +m+ n) log(m+ n)), and the embedding dimension is O(log(m + n)), both of which are independent of the number of singular vectors whose effect we wish to capture. To the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general SVD-based embeddings. The key to sidestepping the SVD is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the l2-norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial SVD tries to do. Our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks.", "creator": "LaTeX with hyperref package"}}}