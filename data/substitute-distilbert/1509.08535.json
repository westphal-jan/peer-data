{"id": "1509.08535", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2015", "title": "Boolean Matrix Factorization and Noisy Completion via Message Passing", "abstract": "boolean flow analysis is common task of decomposing our binary matrix to the boolean content of two binary factors. this unsupervised data - valued approach is desirable due to its interpretability, but hard to perform due its re - hardness. to closely related problem is intermediate - rank boolean constraint completion from noisy observations. we treat these problems as maximum a posteriori inference problems, and present message passing solutions that scale linearly with the number of observations of factors. our mathematical conclusion demonstrates that message passing is able to recover low - rank boolean error, in the boundaries of theoretically possible recovery and outperform existing techniques in real - world applications, such as large - scale binary valued collaborative filtering challenges.", "histories": [["v1", "Mon, 28 Sep 2015 23:11:16 GMT  (2232kb,D)", "http://arxiv.org/abs/1509.08535v1", null], ["v2", "Fri, 9 Oct 2015 19:27:13 GMT  (2232kb,D)", "http://arxiv.org/abs/1509.08535v2", null], ["v3", "Thu, 4 Feb 2016 21:05:32 GMT  (7229kb,D)", "http://arxiv.org/abs/1509.08535v3", null]], "reviews": [], "SUBJECTS": "math.ST cs.AI cs.DM stat.ML stat.TH", "authors": ["siamak ravanbakhsh", "barnab\u00e1s p\u00f3czos", "russell greiner"], "accepted": true, "id": "1509.08535"}, "pdf": {"name": "1509.08535.pdf", "metadata": {"source": "CRF", "title": "Boolean Matrix Factorization and Completion via Message Passing", "authors": ["Siamak Ravanbakhsh", "Russell Greiner"], "emails": [], "sections": [{"heading": null, "text": "A large body of problems in machine learning, communication theory and combinatorial optimization involve the product form Z = X Y where\nZ = {Zm,n}M\u00d7N,X = {Xm,k}M\u00d7K,Y = {Yk,n}K\u00d7N\nwhere one or two components (out of three) are (partially) known and the task is to recover the unknown component(s). Here, operation corresponds to a type of matrix multiplication.\nA subset of these problems, which are most closely related to Boolean matrix factorization and matrix completion, can be expressed over the Boolean domain \u2013 i.e., Zm,n, Xm,k, Yk,n \u2208 {false, true} \u223c= {0, 1}. The two most common Boolean matrix products used in such applications are\nZ = X \u2022Y \u21d2 Zm,n = K\u2228\nk=1\nXm,k \u2227 Yk,n\nZ = X \u2217Y \u21d2 Zm,n \u2261 ( K\u2211 k=1 Xm,k \u2227 Yk,n ) mod 2\n(1a)\n(1b)\nwhere we refer to (1a) simply as Boolean product and we distinguish (1b) as exclusive-OR Boolean product. One may think of Boolean product as ordinary matrix product where the values that are larger than zero in the product matrix are set to one. Alternatively, in XOR product, the odd (even) numbers are identically set the one (zero) in the product matrix.\nWhen Z and Y are column vectors (i.e., N = 1), and Zm,n = true for all m,n, the problem of finding Y from a given X corresponds to Boolean satisfiability (a.k.a. SAT); here message passing has been able to solve hard random SAT instances close to the unsatisfiability transition (Braunstein et al., 2005; Ravanbakhsh and Greiner, 2014a). Using the\nXOR product of (1b), the problem of recovering Y becomes exclusive-OR satisfiability (a.k.a. XOR-SAT).\nLow density parity check (LDPC) codes use the same modulus algebra, with N = 1, where instead of the vector Z, we observe O, after transmitting Z through a noisy channel with known noise model pO(O | Z). Here again, message passing decoding has been able to transmit Z and recover Y (given the parity checks X) at rates close to the theoretical capacity of the communication channel (Gallager, 1962).\nLDPC codes are in turn closely related to the compressed sensing (Donoho, 2006) \u2013 so much so that successful binary LDPC codes (i.e., matrix X) have been reused for compressed sensing (Dimakis et al., 2012). In this setting, the columnvector Y is known to be `-sparse (i.e., ` non-zero values) which makes it possible to use approximate message passing (Donoho et al., 2009) to recover Y using few noisy measurements O \u2013 that is M K and similar to LDPC codes, the measurement matrix X is known.\nWhen the underlying algebra is Boolean, the compressed sensing problem reduces to the problem of (noisy) group testing, which has a longer history (Du and Hwang, 1993). The intuition is that the non-zero elements of the vector Y identify the presence or absence of a rare property (e.g., a rare disease or manufacturing defect), therefore Y is sparse. The objective is to find these non-zero elements (i.e., recover Y) by screening a few (M K) \u201csubsets\u201d of elements of Y. Each of these Y-bundles corresponds to a row of X (in (1a)), and message passing has been successfully applied in this setting as well (Atia and Saligrama, 2012; Sejdinovic and Johnson, 2010). These problems over Boolean domain are special instances of the problem of Boolean factor analysis in which both X and Y have a matrix form and neither is given. This paper presents an efficient message passing solution to this general setting."}, {"heading": "A. Boolean Factor Analysis", "text": "The umbrella term \u201cfactor analysis\u201d refers to the unsupervised methodology of expressing a set of observations in terms of unobserved factors (McDonald, 2014).1 In contrast to the closely related problems that we briefly considered in the previous section, in factor analysis, only (a partial and/or distorted version of) the matrix Z is observed, and our task is then to find low-dimensional X and Y whose product\n1While some definitions restrict factor analysis to variables over continuous domain or even probabilistic models with Gaussian priors, we take a more general view.\nar X\niv :1\n50 9.\n08 53\n5v 1\n[ m\nat h.\nST ]\n2 8\nSe p\n20 15\n2 is (approximately) Z. Here, approximate message passing techniques (inspired by their success in compressed sensing) have been successfully extended to matrix factorization, matrix calibration and robust PCA over real domain (Krzakala et al., 2013; Parker et al., 2013; Kabashima et al., 2014).\nThe Boolean factor analysis has a particularly appealing form. This is because the Boolean values simply indicate the presence of a particular factor (a.k.a. basis (Miettinen et al., 2006)/ concept / category (Belohlavek and Vychodil, 2010; Keprt and Sna\u0301sel, 2004)) in observations, where each factor is in turn a binary pattern.\nThe combinatorial representation of this problem is biclique cover problem in a bipartite graph G = (A \u222a B, E). Here a bipartite graph has two disjoint node sets A (s.t. |A| = M) and B (s.t. |B| = N) where the only edges are between these two sets \u2013 i.e., E \u2286 {(a, b) | a \u2208 A, b \u2208 B}. Here Z \u2208 {0, 1}M\u00d7N represents the incident matrix of G and the objective of factorization is to cover the edges using K bicliques (i.e., complete bipartite sub-graphs of G). Here the kth biclique is identified with a subset of A, corresponding to kth column of X, and a subset of B, corresponding to the kth row of Y.\nBoolean factorization is also closely related to the tiling problem (Stockmeyer, 1975). Despite its numerous applications (see Vaidya et al., 2007; Lu et al., 2008; Geerts et al., 2004; Zhang et al., 2010), the non-linearity of Boolean matrix product has confined most techniques to heuristics and local search methods (e.g., Keprt and Sna\u0301sel, 2004; Belohlavek et al., 2007)."}, {"heading": "I. BAYESIAN FORMULATION", "text": "This section formulates both Boolean matrix factorization and Boolean matrix completion as maximum a posteriori (MAP) inference in a probabilistic graphical model. We then proposes simple message passing procedure, whose cost (per iteration) is linear in the number of observed elements of the matrix and the Boolean rank (a.k.a. Schein rank), which is the smallest number K for which a decomposition of Z to X \u2022Y is exact (Kim, 1982). For noisy observations, Miettinen and Vreeken (2011) suggest using the minimum description length (MDL) principle to estimate K (also see Tatti et al. (2006) for alternative measures).\nHere, we also consider \u201capproximate\u201d decompositions, and assume that the value of K is given as an input. To formalize approximate decompositions for binary data, we use a communication channel, where we assume the product matrix Z is communicated through a noisy binary erasure channel (Cover and Thomas, 2012) to produce the observation O \u2208 {0, 1,null}M\u00d7N where Om,n = null, means this entry was erased in the channel. Using this erasure channel, we can model matrix completion using the same formalism that we use for low-rank factorization. We are unaware of any prior work on Boolean matrix completion. This is despite the fact that it naturally choice for collaborative filtering with binary data.\nFor simplicity, we assume each element of Z is independently transmitted (that is erased, flipped or remains intact)\nthrough the channel, meaning the following conditional probability completely defines the noise model:\npO(O | Z) = \u220f m,n pOm,n(Om,n | Zm,n) (2)\nNote that each of these conditional probabilities can be represented using six values \u2013 one value per each pair of Om,n \u2208 {0, 1,null} and Zm,n \u2208 {0, 1}. This setting allows the probability of erasure to depend on the value of m, n and Zm,n.\nThe objective is to recover X and Y from O. However, due to its degeneracy, recovering X and Y is only up to a K\u00d7K permutation matrix U \u2013 that is X \u2022Y = (X \u2022U) \u2022 (UT \u2022Y). A Bayesian approach can resolve this ambiguity by defining non-symmetric priors\npX(X) = \u220f m,k pXm,k(Xm,k)\npY(Y) = \u220f k,n pYk,n(Yk,n)\n(3a)\n(3b)\nwhere we assume separability. Now, we can express the problem of recovering X and Y as a maximum a posteriori (MAP) inference problem argX,Y max p(X,Y | O), where the posterior is\np(X,Y | O) \u221d pX(X) pY(Y) pO(O | X \u2022Y) (4)\nFinding the maximizing assignment for (4) is NPhard (Stockmeyer, 1975). Our treatment of SAT as a special case of factorization with m = 1 and fixed Y proves that even the problem of finding the Boolean matrix X (when Y and Z are given) such that Z = X \u2022Y, is NP-hard. Here we introduce a graphical model to represent the posterior and use a simplified form of Belief Propagation (BP Pearl, 1982) to approximate the MAP assignment.\nAn alternative to finding the MAP assignment is that of finding the marginal-MAP \u2013 i.e., argXm,k max p(Xm,k | O) = argXm,n max \u2211 X\\xXi,Y p(X,Y | O). While the MAP assignment seeks the optimal joint assignment to X and Y, finding the marginal-MAP corresponds to optimally estimating individual assignments for each variables, while the other variable assignments are marginalized. We also provide the message passing solution to this alternative in Appendix B. Here, we focus on the MAP estimate as we find it more useful in practice."}, {"heading": "A. The Probabilistic Graphical Model", "text": "Figure 1 shows the factor-graph (Kschischang et al., 2001) representation of the posterior (4). Here, variables are circles and squares are factors. The factor-graph is a bipartite graph, connecting each factor (i.e. function) to its relevant variables. This factor-graph has one variable Xm,k \u2208 {0, 1} for each element of X, and a variable Yk,n \u2208 {0, 1} for each element of Y. In addition to these K(M + N) variables, we have introduced K M N auxiliary variables Wm,n,k \u2208 {0, 1}. For Boolean matrix completion the number of auxiliary variables is K|E|, where E = {(m,n)|Om,n 6= null} is the set of observed elements (see section II-A).\n3\nWe use plate notation (often used with directed models) in representing this factor-graph. Here we have three plates for 1 \u2264 m \u2264 M, 1 \u2264 n \u2264 N and 1 \u2264 k \u2264 K (large transparent boxes in fig. 1). In plate notation, all variables and factors on a plate are replicated. For example, variables on the m-plate are replicated for 1 \u2264 m \u2264 M. Variables and factors located on more than one plate are replicated for all combinations of their plates. For example, since variable X is in common between m-plate and k-plate, it refers to M N binary variables \u2013 i.e., Xm,k \u2200m,n.\n1) Variables and Factors: The auxiliary variable Wm,n,k represents the Boolean product of Xm,k and Yk,n \u2013 i.e., Wm,n,k = Xm,k \u2227 Yk,n. This is achieved through M N K hard constraint factors\nfm,n,k(Xm,k, Yk,n,Wm,n,k) = I(Wm,n,k = Xm,k \u2227 Yk,n)\nwhere I(.) is the identity function on the inference semiring (see Ravanbakhsh and Greiner, 2014b). For the max-sum inference Imax-sum(true) = 0 and Imax-sum(false) = \u2212\u221e.\nLocal factors\nhm,k(Xm,k) = log(p X(Xm,k)) hk,n(Yk,n) = log(p Y(Yk,n))\nrepresent the logarithm of priors over X and Y in (4). Finally, the noise model in (4) is represented by M N factors over auxiliary variables\ngm,n({Wm,n,k}1\u2264k\u2264K) = log ( pOm,n(Om,n | \u2228 k Wm,n,k) ) .\nThe combination of the factors of type g and f, represent the term p(Om,n | \u2228K k=1Xm,k\u2227Yk,n) in (4) and the local factors h, represent the logarithm of the priors. It is easy to see that the sum of all the factors above, evaluates to the logarithm of the posterior\nlog(p(X,Y | O) = \u2211 m,k hm,k(Xm,k) + \u2211 k,n hk,n(Xk,n)\n+ \u2211 m,n gm,n({Xm,k \u2227 Yk,n}1\u2264k\u2264K)\nAlgorithm 1: message passing for Boolean matrix factorization/completion\nInput: 1) observed matrix O \u2208 {0, 1}M\u00d7N; 2) K \u2208 N; 3) priors pXm,k, p Y n,k; 4) noise model pOm,n \u2200m,n, k Output: X \u2208 {0, 1}M\u00d7K and Y \u2208 {0, 1}K\u00d7N. t := 0 init \u03a6(t)m,n,k,\u03a8 (t) m,n,k, \u03a6\u0302 (t) m,n,k, \u03a8\u0302 (t) m,n,k, \u0393\u0302 (t) m,n,k and \u0393 (t) m,n,k \u2200m,n, k while t < Tmax and not converged do\n\u03a6 (t+1) m,n,k := ( \u0393 (t) m,n,k + \u03a8\u0302 (t) m,n,k ) + \u2212 ( \u03a8\u0302 (t) m,n,k ) + \u03a8 (t+1) m,n,k := ( \u0393 (t) m,n,k + \u03a6\u0302 (t) m,n,k ) + \u2212 ( \u03a6\u0302 (t) m,n,k ) +\n\u03a6\u0302 (t+1) m,n,k := log\n( pXm,k(1)\npXm,k(0) ) + \u2211 n\u2032 6=n \u03a6 (t) m,n\u2032,k\n\u03a8\u0302 (t+1) m,n,k := log\n( pYn,k(1)\npYn,k(0)\n) + \u2211\nm\u2032 6=m\n\u03a8 (t) m\u2032,n,k\n\u0393\u0302 (t+1) m,n,k := min\n{ \u03a6\u0302\n(t) m,n,k + \u03a8\u0302 (t) m,n,k, \u03a6\u0302 (t) m,n,k, \u03a8\u0302 (t) m,n,k } \u0393 (t+1) m,n,k := min {( \u2212max\nk\u2032 6=k \u0393\u0302 (t) m,n,k\u2032 ) + ,\n\u2211 k\u2032 6=k ( \u0393\u0302 (t) m,n,k\u2032 ) + + log ( pOm,n(Om,n | 1) pOm,n(Om,n | 0) )}\n(5a)\n(5b)\n(5c)\n(5d)\n(5e)\n(5f)\nend calculate log-ratio of the posterior marginals\n\u039em,k := log\n( pXm,k(1)\npXm,k(0) ) + \u2211 n \u03a6 (t) m,n,k\n\u03a5k,n := log\n( pYk,n(1)\npYk,n(0) ) + \u2211 m \u03a8 (t) m,n,k\n(6a)\n(6b)\ncalculate X and Y\nXm,k := { 1, if \u039em,k > 0 0, otherwise\nYk,n := { 1, if \u03a5k,n > 0 0, otherwise\n(7a)\n(7b)\nreturn X,Y\nif Wm,n,k = Xm,k \u2227 Yk,n \u2200m,n, k and \u2212\u221e otherwise. Therefore maximizing the sum of these factors is equivalent to MAP inference for (4)."}, {"heading": "II. SIMPLIFIED FORM OF BELIEF PROPAGATION", "text": "Max-sum Belief Propagation (BP) is a message passing procedure for approximating the MAP assignment in a graphical model. In factor-graphs without loops, max-sum BP is simply an exact dynamic programming approach that leverages the distributive law. In loopy factor-graphs the approximations of this message passing procedure is justified by the fact\n4 that it represents the zero temperature limit to sum-product BP, which is in turn a fixed point iteration procedure whose fixed points are the local optima of the Bethe approximation to the free energy (Yedidia et al., 2000); see also (Weiss et al., 2012). For general factor-graphs, it is known that the approximate MAP solution obtained using max-sum BP is optimal within a \u201cneighborhood\u201d (Weiss and Freeman, 2001). Although few applications of max-sum BP to loopy factorgraphs have optimality guarantees (e.g., Bayati et al., 2008; Gamarnik et al., 2012), approximations using BP have been successfully applied to machine learning, coding theory and combinatorial optimization problems.\nWe apply max-sum BP to approximate the MAP assignment of the factor-graph of fig. 1. This factor-graph is very densely connected and therefore, one expects BP to oscillate or fail to converge to a good solution. However, surprisingly we see in section III that BP performs near-optimally. This can be attributed to the fact that despite dense connectivity, many of the factors have a week influence, often resulting in (close) to uniform messages (see fig. 4). Near-optimal behavior of max-sum BP in dense factor-graph is not without precedence (e.g., Decelle et al., 2011; Ravanbakhsh et al., 2014).\nThe message passing for MAP inference of (4) involves message exchange between all variables and their neighboring factors in both directions. Here, each message is a Bernoulli distribution. For example mXm,k\u2192fm,n,k(Xm,n) : {0, 1} \u2192 <2 is the message from variable node Xm,n to the factor node fm,n,k. For binary variables, it is convenient to work with the log-ratio of messages \u2013 e.g., we use \u03a6\u0302m,n,k = log (mXm,k\u2192fm,n,k (1) mXm,k\u2192fm,n,k (0) ) . In the following, capital Greek letters denote these log-ratios (see also fig. 1). For a review of max-sum BP and the detailed derivation of the simplified BP updates for this factor-graph, see appendix A. In particular, an important simplification in our derivation, is that of the messages \u0393m,n from the likelihood factors gm,n({Wm,n,k}1\u2264k\u2264K) \u2200m,n to the auxiliary variables Wm,n,k. Since each of these factors depend on K variables, a naive application of BP will have a O(2K) cost. We reduce this cost to O(K). algorithm 1 summarizes this simplified message passing algorithm.\nAt the beginning of the algorithm, t = 0, messages are initialized with some random value \u2013 e.g., using log(U) \u2212 log(1\u2212U) where U \u223c Uniform(0, 1). Using the short notation( a ) +\n= max{0, a}, at time t+ 1, the messages are updated using 1) the message values at time t, 2) the priors and 3) the noise model and observation O. The message updates of (5) are repeated until convergence or a maximum number of iterations Tmax is reached. We decide the convergence based on the maximum absolute change in one of the message types\ne.g., maxm,n,k |\u03a6(t+1)m,n,k \u2212 \u03a6 (t) m,n,k|\n? \u2264 .\nOnce the message update converges, at iteration T , we can use the values for \u03a6(T)m,n,k and \u03a8 (T) m,n,k to recover the log-ratio of the marginals p(Xm,k) and p(Yn,k). These log-ratios are denoted by \u039em,k and \u03a5k,n in (6). A positive log-ratio \u039em,k > 0 means p(Xm,k = 1) > p(Xm,k = 0) and the posterior favors Xm,k = 1. In this way the marginals are used to obtain an approximate MAP assignment to both X and Y.\nFor better convergence, we also use damping in practice. For this, one type of messages is updated to a linear combination of messages at time t and t + 1 using a damping parameter \u03bb \u2208 (0, 1]. Choosing \u03a6\u0302 and \u03a8\u0302 for this purpose, the updates of (5c,5d) become\n\u03a6\u0302 (t+1) m,n,k := (1\u2212 \u03bb)\u03a6\u0302 (t) m,n,k+ (8)\n\u03bb ( log ( pXm,k(1)\npXm,k(0) ) + \u2211 n\u2032 6=n \u03a6 (t) m,n\u2032,k ) \u03a8\u0302\n(t+1) m,n,k := (1\u2212 \u03bb)\u03a8\u0302 (t) m,n,k+\n\u03bb ( log ( pYn,k(1)\npYn,k(0)\n) + \u2211\nm\u2032 6=m\n\u03a8 (t) m\u2032,n,k\n)"}, {"heading": "A. Further Simplifications", "text": "Partial knowledge. If any of the priors, p(Xm,k) and p(Yn,k), are zero or one, it means that X and Y are partially known. The message updates of (5c,5d) will assume \u00b1\u221e values, to reflect these hard constrains. In contrast, for uniform priors, the log-ratio terms disappear.\nMatrix completion speed up. Consider the case where log (pO(Om,n|1) pO(Om,n|0) ) = 0 in (5f) \u2013 i.e., the probabilities in the nominator and denominator are equal. An important case of this happens in matrix completion, when the probability of erasure is independent of the value of Zm,n \u2013 that is pO(null | Zm,n = 0) = pO(null | Zm,n = 1) = pO(null) for all m and n.\nIt is easy to check that in such cases, \u0393m,n,k = min (( \u2212\nmaxk\u2032 6=k \u0393\u0302 (t) m,n,k ) + , \u2211 k\u2032 6=k ( \u0393\u0302 (t) m,n,k ) + ) is always zero. This further implies that \u03a6\u0302m,n,k and \u03a8\u0302m,n,k in (5c,5d) are also always zero and calculating \u0393\u0302m,n,k in (5f) is pointless. The bottom-line is that we only need to keep track of messages where this log-ratio is non-zero. Let E = {(m,n) | Om,n 6= null} denote the observed entries of O. Then in message passing updates of (5) in algorithm 1, wherever the indices m and n appear, we may restrict them to the set E .\nNoiseless channel. When pO(1 | 1) = pO(0 | 0) = 1, cOm,n evaluates to \u2212\u221e or +\u221e for Om,n = 0 and Om,n = 1 respectively. In the former case \u2013 i.e., cOm,n = \u2212\u221e \u2013 the update of (5f) is unnecessary as \u0393(t+1)m,n,k = \u2212\u221e at all time. Consequently, (5a,5b) further simplify, speeding up the message update for the noiseless Boolean matrix factorization problem.\nBelief update. Another trick to reduce the complexity of message updates is in calculating {\u03a6\u0302m,n,k}n and {\u03a8\u0302m,n,k}m in (5c,5d). We may calculate the marginals \u039em,k and \u03a5k,n using (6), and replace the (8) (i.e., the damped version of the (5c,5d)) with\n\u03a6\u0302 (t+1) m,n,k := (1\u2212 \u03bb)\u03a6\u0302 (t) m,n,k + \u03bb ( \u039e (t) m,k \u2212 \u03a6 (t) m,n,k ) \u03a8\u0302\n(t+1) m,n,k := (1\u2212 \u03bb)\u03a8\u0302 (t) m,n,k + \u03bb\n( \u03a5\n(t) k,n \u2212\u03a8 (t) m,n,k\n) (9a) (9b)\nwhere the summation over n\u2032 and m\u2032 in (5c,5d) respectively, is now performed only once (in producing the marginal) and reused.\nRecycling of the max. Finally, using one more computational trick the message passing cost is reduced to linear:\n5 bit-flip probability 0.0 0.1 0.2 0.3 0.4 0.5 -0.2 0.0 0.2 0.4 0.6 re co ns tr uc tio n er ro r NimfaMessage Passing Method\nFig. 3: Comparison of message passing and NIMFA for Boolean matrix factorization\nin (5e), the maximum of the term ( \u2212maxk\u2032 6=k \u0393\u0302(t)m,n,k ) +\nis calculated for each of K messages {\u0393m,n,k}k\u2208{1,...,K}. Here, we may calculate the \u201ctwo\u201d largest values in the set {\u0393\u0302(t)m,n,k}k only once and reuse them in the updated for all {\u0393m,n,k}k \u2013 i.e., if the largest value is \u0393\u0302(t)m,n,k\u2217 then we use the second largest value, only in producing \u0393m,n,k\u2217 .\nComputational Complexity. All of the updates in (5a,5b,5f,5e,9) have a constant computational cost. Since these are performed for K|E| messages, and the updates in (6a,6b) are O(|E|K), the complexity of message update is O(|E|K) per iteration."}, {"heading": "III. EXPERIMENTS", "text": "We evaluated the performance of message passing on random matrices and real-world data. In all experiments, message passing uses damping with \u03bb = .4, T = 200 iterations and uniform priors pXm,k(1) = p Y k,n(1) = .5 (i.e., c X m,k = cYk,n = 0).2"}, {"heading": "A. Random Matrices", "text": "Figure 3 compares the reconstruction error of message passing against state-of-the-art Boolean matrix factorization method of Zhang et al. (2007), which was implemented by NIMFA (Zitnik and Zupan, 2012). Both methods receive the correct K as input. The results are for 1000 \u00d7 1000 random matrices of rank K = 5 where X and Y were uniformly sampled from binary matrices. The reconstruction error is\nd(Z, Z\u0302) def =\n1\nMN \u2211 m,n |Zm,n \u2212 Z\u0302m,n|\nNIMFA uses an alternating optimization method to repeatedly solve a penalized non-negative matrix factorization problem, where the penalty parameters try to enforce the desired binary form.3\n2This also means that if the channel is symmetric \u2013 that is pO(1 | 1) = pO(0 | 0) > .5 \u2013 the approximate MAP reconstruction Z\u0302 does not depend on pO, and we could simply use pOm,n(1 | 1) = pOm,n(1 | 1) = c for any c > .5.\n3Both methods use the same number of iterations T = 200. For NIMFA we use the default parameters of \u03bbh = \u03bbw = 1.1.\n0 200000 400000 600000 800000\n1000000 1200000\nfre qu\nen cy\niteration 2\n0 100000 200000 300000 400000 500000 600000 700000 800000 900000 fre qu en cy\niteration 20\n6 4 2 0 2 4 6 message value\n0 200000 400000 600000 800000\n1000000 1200000\nfre qu\nen cy\niteration 200\nFig. 4: Histogram of BP messages {\u03a6\u0302(t)m,n}m,n at t \u2208 {2, 20, 200} for a random 1000\u00d7 1000 matrix factorization with K = 2.\nThe results suggests that two methods are competitive, but message passing performs slightly better at higher noise levels. Here, the experiments were repeated 10 times for each point. The small variance of message passing performance at low noise-levels is due to the multiplicity of symmetric MAP solutions, and could be resolved by performing decimation, albeit at a computational cost. We speculate that the symmetry breaking of higher noise levels help message passing choose a fixed point, which results in lower variance.\nDespite being densely connected, at lower levels of noise, BP often converges within the maximum number of iterations. The surprisingly good performance of BP in this setting is due to the fact that most factors have a week influence on many of their neighboring variables. This effectively limits the number of influential loops in the factor-graph. Figure 4 shows the histogram of factor-to-variable messages {\u03a6\u0302m,n}1\u2264mM,1\u2264n\u2264N at different iterations. It suggests that a large portion of messages are close to zero. Since these are log-ratios, the corresponding probabilities are close to uniform.\nFor the Boolean matrix completion task, and K M,N, we can lower-bound the number of observed entries \u03c1 = MN(1\u2212 pO(null)) required for recovering Z by\n\u03c1 > K(M + N\u2212 log(K) + 1) +O(log(K)). (10)\nTo derive this bound, we briefly sketch an information theoretic argument. Note that the total number of ways to define a binary matrix Z \u2208 {0, 1}M\u00d7N of rank K is 2 K(M+N)\nK! , where the nominator is the number of different X and Y matrices and K! is the irrelevant degree of freedom in choosing the permutation matrix U, such that Z = (X \u2022U) \u2022 (UT \u2022Y). The logarithm of this number, using Sterling\u2019s approximation, is the r.h.s. of (10), lower-bounding the number of bits required to recover Z, in the absence of any noise.\nIn all panels of fig. 2, each point represents the average reconstruction error for random 1000\u00d71000 Boolean matrices. For each choice of observation sparsity \u03c1 and rank K, the experiments were repeated 10 times. The figure compares the average reconstruction error of Generalized Low-Rank Models (GLRM; Udell et al., 2014) and message passing, where the red line is the information theoretic lower-bound of (10) for perfect recovery. This result suggests that message passing\n6\nis indeed near optimal for Boolean matrix completion. Note that the lower-bound also resembles the O(KNpolylog(N)) sample complexity for various real-domain matrix completion tasks (e.g., Candes and Plan, 2010; Keshavan et al., 2010).\nGLRM does not produce Boolean factors and it only tries to produce a binary product matrix Z\u0302. The choice of GLRM is because of the fact that we could not find any Boolean matrix completion method (with Boolean factors). 4 In our experiments, GLRM is optimizing the hinge loss, argX,Y min \u2211 m,n ( 1 \u2212 ( \u2211 kXm,kYk,n)(2Om,n \u2212 1) ) +\n, where(2Om,n \u2212 1) changes the domain of observations to {\u22121,+1}.\nThe better performance of message passing is despite the fact that using real-valued factors gives a higher degree of freedom. Figure 2(message-passing) shows that the transition from recoverability to non-recoverability is sharp, and the variance of the reconstruction error is always close to zero, but in a small neighborhood of the red line.5 Figure 2(right) demonstrates that the inferior performance of GLRM using real factors is not due to over-fitting as using quadratic regularization (i.e., Gaussian priors over elements of both factors) did not significantly improve its performance."}, {"heading": "B. Real-World Applications", "text": "This section evaluates message passing on two realworld applications. For application in binary image restoration/completion see Appendix B.\nMovieLens. We applied our message passing method to MovieLens-1M dataset6 as an application in collaborative filtering. This benchmark dataset contains 1 million ratings from 6000 users on 4000 movies (i.e., 1/24 of all the ratings are available). The ratings are ordinals 1-5. Here we say a user is \u201cinterested\u201d in the movie iff her rating is above the global\n4Moreover, GLRM allows different choices of loss function and uses an efficient proximal gradient method that facilitates its application to large matrices.\n5An issue that is not apparent in fig. 2, is the sparsity of Z. Here, if we generate X and Y uniformly at random, as K grows, the matrix Z = X \u2022 Y becomes all ones. To avoid this degeneracy, we choose pXm,k(Xm,k) and p Y k,n(Yk,n) so as to enforce p(Z = 1) \u2248 p(Z = 0). It is easy to check that pXm,k(1) = p Y k,n(1) = \u221a 1\u2212 K \u221a .5 produces this desirable outcome. Note that these probabilities are only used for random matrix generation and the message passing algorithm is using uniform priors pYk,n(1) = p X m,k(1) = .5\n6http://grouplens.org/datasets/movielens/\naverage of ratings. The task is to predict this single bit by observing a random subset of the available user\u00d7movie rating matrix. For this we use \u03b1 \u2208 (0, 1) portion of the 106 ratings to predict the one-bit interest level for the remaining (1 \u2212 \u03b1 portion of the) data-points. Note that here \u03c1 = \u03b124\nFor Boolean matrix completion, we threshold the training data to {0, 1} values from the start using the average rating. For ordinary matrix completion we use GLRM with ordinal hinge loss (Rennie and Srebro, 2005) and quadratic regularization to predict the ratings for the unobserved entries.7 These ratings are then thresholded by the global average rating to predict the interest level.\nTable I reports the test error of the two methods for K \u2208 {2, 20}, using \u03b1 \u2208 {.01, .05, .1, .2, .5, .95} portion of the available ratings. It is surprising that only using one bit of information per rating, message passing is competitive with GLRM that benefits from the full range of ordinal values. The results also suggest that when only few observations are available (e.g., \u03b1 = .01), message passing performs significantly better.\nDavenport et al. (2014) also report a good performance by only observing a single bit, instead of ordinal ratings for the MovieLens dataset, albeit on a small version of this dataset (with 100,000 entries) and for the dense setting of \u03b1 = .95. Similar to GLRM, they assume real factors, and according to their model, the 1-bit observation is obtained by thresholding a function of the product matrix \u2013 i.e., O = f(Z) ? \u2265 c for some function f and constant c. Using sigmoid function and c = .5, their method is closely related to GLRM with logistic loss (instead of the hinge loss, used in our experiments).\n7The results for GLRM are for the regularization parameter in {.01, .1, 1, 10}, with the best test error. However this choice did not drastically changed the results \u2013 i.e., often the test error changed by less than 2%.\n7 congress 101 102 103 104 105 106 107 0.08 0.10 0.12 0.14 0.16 0. 5 0.08 0.10 0.12 0.14 0.16 0.18 0. 2 0.0 0.1 0.2 0.3 0.4 0.5 0. 05 pr ed ic tio n er ro r f or % o bs er va tio ns real factorsBoolean factors method\nFig. 5: the prediction error using Boolean matrix completion (by message passing) versus using GLRM with hinge loss for binary matrix completion using real factors. Each panel has a different observed percentage of entries \u03c1 \u2208 {.05, .2, .5}. Here the horizontal axis identifies senator\u00d7issue matrices and the y-axis is the average error in prediction of the unobserved portion of the (yes/no) votes.\nReconstructing senate voting records. We applied our noisy completion method to predict the (yes/no) senate votes during 1989-2003 by observing a randomly selected subset of votes.8 This dataset contains 7 Boolean matrices (corresponding to voting sessions for 101st\u2212107th congress), where a small portion of entries are missing. For example the first matrix is a 634 \u00d7 103 Boolean matrix recording the vote of 102 senators on 634 topics plus the outcome (we ignore the outcome column).\nFigure 5 compares the prediction accuracy of message passing and GLRM (with hinge loss or binary predictions) for the best choice of K \u2208 {1, . . . , 10} on each of 7 matrices. GLRM is using quadratic regularization (as it improve its results) while message passing is using uniform priors. In each case we report the prediction accuracy on the unobserved entries, after observing \u03c1 \u2208 {5%, 20%, 50%} of the votes. For sparse observations (\u03c1 = .05), the message passing error is almost always half of the error when we use real factors. With larger number of observations, the methods are comparable, with GLRM performing slightly better.\nCONCLUSION & FUTURE WORK\nThis paper introduced a simple message passing technique for approximate Boolean factorization and noisy matrix completion. While having a linear time complexity, this procedure achieves near-optimal performance in Boolean matrix completion problem and favorably compares with the state-of-theart in Boolean matrix factorization. In particular, for matrix\n8The senate data was obtained from http://www.stat.columbia.edu/\u223cjakulin/ Politics/senate-data.zip prepared by Jakulin et al. (2009).\ncompletion with few entries, message passing significantly outperforms the existing methods that use real factors. This makes message passing is a useful candidate for collaborative filtering in modern applications involving large datasets of sparse Boolean observations.\nBoolean matrix factorization with modular arithmetic (see (1b)) replaces the logical OR operation with exclusive-OR, only changing one of the factor types (i.e., type g) in our graphical model. Therefore both min-sum and sum-product message passing can also be applied to this variation. The similarity of this type of Boolean factorization to LDPC codes (Gallager, 1962), suggests that one may be able to use noisy matrix completion as an efficient method of communication over a noisy channel. This is particularly interesting, as both the code and its parity checks are transferred in the same matrix. We leave this promising direction to future work."}, {"heading": "APPENDIX A DETAILED DERIVATION OF SIMPLIFIED BP MESSAGES", "text": "The sum of the factors in the factor-graph of fig. 1 is\u2211\nm,k hm,k(Xm,k) + \u2211 n,k\nhn,k(Yn,k)+\u2211 m,n,k\nfm,n,k(Xm,n,k, Ym,n,k,Wm,n,k)+\u2211 m,n gm,n({Wm,n,k}k) (11)\n= \u2211 m,n log(pX(Xm,k)) + \u2211 n,k\nlog(pY(Yk,n))+\u2211 m,n,k\nI(Wm,n,k = Xm,k \u2227 Yk,n)+\u2211 m,n log ( pOm,n(Om,n | \u2228 k Wm,n,k) )\n(12)\n= \u2211 m,n log(pX(Xm,k)) + \u2211 n,k\nlog(pY(Yk,n))+\u2211 m,n log ( pOm,n(Om,n | \u2228 k Xm,k \u2227 Yk,n) )\n(13)\n= log(p(X,Y | O)) (14)\nwhere in (12) we replaced each factor with its definition. (13) combines the two last terms of (12), which is equivalent to marginalizing out W. The final result of (14) is the logposterior of (4).\nSince the original MAP inference problem of argX,Y max p(X,Y | O) is equivalent to argX,Y max log(p(X,Y | O)), our objective is to perform max-sum inference over this factor-graph, finding an assignment that maximizes the summation of (11)\nWe perform this max-sum inference using Belief Propagation (BP). Applied to a factor-graph, BP involves message exchange between neighboring variable and factor nodes. Two most well-known variations of BP are sum-product BP for marginalization and max-product or max-sum BP for MAP inference. Here, we provide some details on algebraic manipulations that lead to the simplified form of max-sum BP message updates of (5). (A-A) obtains the updates (5c) and (5d) in our algorithm and (A-B) reviews the remaining message updates of (5)\nA. Variable-to-factor messages\nConsider the binary variable Xm,k \u2208 {0, 1} in the graphical model of fig. 1. Let mXm,k\u2192fm,n,k(Xm,k) : {0, 1} \u2192 < be the message from variable Xm,k to the factor fm,n,k in this factorgraph. Note that this message contains two assignments for Xm,k = 0 and Xm,k = 1. As we show here, in our simplified updates this message is represented by \u03a6\u0302m,n,k. In the max-sum BP, the outgoing message from any variable to a neighboring factor is the sum of all incoming messages, except for the message from the receiving factor \u2013 i.e.,\nmXm,k\u2192fm,n,k(Xm,k) (t+1) = mhm,k\u2192Xm,k(Xm,k) (t) + \u2211 n\u2032 6=n mfm,n\u2032,k\u2192Xm,k(Xm,k) (t) + c (15)\nWhat matters in BP messages is the difference between the message mXm,k\u2192fm,n,k(Xm,k) assignment for Xm,k = 1 and Xm,k = 0 (note the constant c in (15)). Therefore we can use a singleton message value that capture this difference instead of using a message over the binary domain \u2013 i.e.,\n\u03a6\u0302m,n,k = mXm,k\u2192fm,n,k(1)\u2212mXm,k\u2192fm,n,k(0) (16)\nThis is equivalent to assuming that the messages are normalized so that mXm,k\u2192fm,n,k(0) = 0. We will extensively use this normalization assumption in the following. By substituting (15) in (16) we get the simplified update of (5c)\n\u03a6\u0302 (t+1) m,n,k = ( mhm,k\u2192Xm,k(1) (t) + \u2211 n\u2032 6=n mfm,n\u2032,k\u2192Xm,k(1) (t)(1) ) \u2212 ( mhm,k\u2192Xm,k(0)\n(t) + \u2211 n\u2032 6=n mfm,n\u2032,k\u2192Xm,k(0) (t) ) = ( mhm,k\u2192Xm,k(1) (t) \u2212mhm,k\u2192Xm,k(0)(t) ) + \u2211 n\u2032 6=n ( mfm,n\u2032,k\u2192Xm,k(1)(t) \u2212mfm,n\u2032,k\u2192Xm,k(0) (t) )\n= log\n( pXm,k(1)\npXm,k(0) ) + \u2211 n\u2032 6=n \u03a6 (t) m,n\u2032,k\nand we used the fact that\n\u03a6m,n\u2032,k = mfm,n\u2032,k\u2192Xm,k(1) (t) \u2212 mfm,n\u2032,k\u2192Xm,k(0) (t)\nlog\n( pXm,k(1)\npXm,k(0)\n) = hm,k(1)\u2212 hm,k(0).\nThe messages \u03a8\u0302m,n,k from the variables Yn,k to fm,n,k is obtain similarly. The only remaining variable-to-factor messages in the factor-graph of fig. 1 are from auxiliary variables Wm,n,k to neighboring factors. However, since each variable Wm,n,k has exactly two neighboring factors, the message from Wm,n,k to any of these factors is simply the incoming message from the other factor \u2013 that is\nmWm,n,k\u2192gm,n(Wm,n,k) = mfm,n,k\u2192Wm,n,k(Wm,n,k) mgm,n\u2192Wm,n,k(Wm,n,k) = mWm,n,k\u2192fm,n,k(Wm,n,k) (17)"}, {"heading": "B. Factor-to-variable messages", "text": "The factor-graph of fig. 1 has three types of factors. We obtain the simplified messages from each of these factors to their neighboring variables in the following sections.\n1) Local factors: The local factors are {hm,k}m,k and {hn,k}n,k, each of which is only connected to a single variable. The unnormalized message, leaving these factors is identical to the factor itself. We already used the normalized messages from these local factors to neighboring variables in (17) \u2013 i.e., hm,k(1)\u2212 hm,k(0) and hn,k(1)\u2212 hn,k(0), respectively.\n2) Constraint factors: The constraint factors {fm,n,k}m,n,k ensure \u2200m,n,kWm,n,k = Xm,k\u2227Yn,k. Each of these factors has three neighboring variables. In max-sum BP the message from a factor to a neighboring variable is given by the sum of that factor and incoming messages from its neighboring variables, except for the receiving variable, max-marginalized over the\n10\ndomain of the receiving variable. Here we first calculate the messages from a constraint factor to Xm,k (or equivalently Yn,k) variables in (1). In (2) we derive the simplified messages to the auxiliary variable Wm,n,k. (1) according to max-sum BP equations the message from the factor fm,n,k to variable Xm,k is\nmfm,n,k\u2192Xm,k(Xm,k) (t+1) =\nmax Wm,n,k,Yn,k\n( fm,n,k(Xm,k,Wm,n,k, Yn,k)\n+ mYn,kfm,n,k\u2192(Yn,k) (t) + mWm,n,k\u2192fm,n,k(Wm,n,k) (t) ) For notational simplicity we temporarily use the shortened\nversion of the above\nm\u20321(X) = max W,Y f(X,W, Y ) + m2(Y ) + m3(W ) (18)\nwhere\nm1(X) = mXm,k\u2192fm,n,k(Xm,k) m\u20321(X) = mfm,n,k\u2192Xm,k(Xm,k)\nm2(Y ) = mYn,k\u2192fm,n,k(Yn,k) m\u20322(Y ) = mfm,n,k\u2192Yn,k(Yn,k)\nm3(W ) = mWm,n,k\u2192fm,n,k(Wm,n,k) m\u20323(W ) = mfm,n,k\u2192Wm,n,k(Wm,n,k),\nthat is we use m(.) to denote the incoming messages to the factor and m\u2032(.) to identify the outgoing message.\nIf the constraint f(X,Y,W ) = I(W = X \u2227 Y ) is not satisfied by an assignment to X,Y and W , it evaluates to \u2212\u221e, and therefore it does not have any effect on the outgoing message due to the max operation. Therefore we should consider the maxW,Y only over the assignments that satisfy f(.).\nHere, X can have two assignments; for X = 1, if Y = 1, then W = 1 is enforced by f(.), and if Y = 0 then W = 0. Therefore (18) for X = 1 becomes\nm\u20321(1) = max(m2(1) + m3(1),m2(0) + m3(0)) (19)\nFor X = 0, we have W = 0, regardless of Y and the update of (18) reduces to\nm\u20321(0) = max(m2(1) + m3(0),m2(0) + m3(0)} (20) = m3(0) + max{m2(0),m2(1)}\nAssuming the incoming messages are normalized such that m3(0) = m2(0) = 0 and denoting\n\u03a8\u0302m,n,k = mYn,k\u2192fm,n,k(1)\u2212mYn,k\u2192fm,n,k(0) = m2(1)\nand\n\u0393m,n,k = mWm,n,k\u2192fm,n,k(1)\u2212mWm,n,k\u2192fm,n,k(0) = m3(1)\nthe difference of (19) and (20) gives the normalized outgoing message of (5a)\n\u03a6m,n,k =m \u2032 1(1)\u2212m\u20321(0) = max(\u0393m,n,k + \u03a8\u0302m,n,k, 0)\n\u2212max(0, \u03a8\u0302m,n,k) (21)\nThe message of (5b) from the constraint fm,n,k to Yn,k is obtained in exactly the same way.\n(2) The max-sum BP message from the constraint factor fm,n,k to the auxiliary variable Wm,n,k is\nmfm,n,k\u2192Wm,n,k(Wm,n,k) (t+1) =\nmax Xm,k,Yn,k\n( fm,n,k(Xm,k,Wm,n,k, Yn,k)+\nmYn,k\u2192fm,n,k(Yn,k) (t) + mXm,k\u2192fm,n,k(Wm,n,k)\n(t) )\nHere, again we use the short notation\nm\u20323(W ) = max X,Y f(X,W, Y ) + m1(X) + m2(Y ) (22)\nand consider the outgoing message m\u2032(W ) for W = 1 and W = 0. If W = 1, we know that X = Y = 1. This is because otherwise the factor f evaluates to \u2212\u221e. This simplifies (22) to\nm\u20323(1) = m1(1) + m2(1)\nFor W = 0, either X = 0, or Y = 0 or both. This means\nm\u20323(0) = max(m1(0) + m2(1),m + 1(1) + m2(0),\nm1(0) + m2(0))\nAssuming the incoming messages were normalized, such that m2(0) = m1(0) = 0, the normalized outgoing message \u0393\u0302m,n,k = m3(1)\u2212m3(0) simplifies to\n\u0393\u0302m,n,k = m1(1) + m2(1)\u2212max(0,m1(1),m2(1)) = min(m1(1) + m2(1),m1(1),m2(1))\n= min(\u03a6\u0302m,n,k + \u03a8\u0302m,n,k, \u03a6\u0302m,n,k, \u03a8\u0302m,n,k)"}, {"heading": "C. Likelihood factors", "text": "At this point we have derived all simplified message updates of (5), except for the message \u0393m,n,k from factors gm,n to the auxiliary variables Wm,n,k ( (5f)). These factors encode the likelihood term in the factor-graph.\nThe naive form of max-sum BP for the messages leaving this factor to each of K neighboring variables {Wm,n,k}1\u2264k\u2264K is\nmgm,n\u2192Wm,n,k(Wm,n,k) (t+1) = (23)\nmax {Wm,n,`}` 6=k} ( gm,n({Wm,n,`\u2032}`\u2032)+\u2211\nk\u2032 6=k\nmWm,n,k\u2032\u2192gm,n(Wm,n,k\u2032) (t) ) However, since g(.) is a high-order factor (i.e., depends on many variables), this naive update has an exponential cost in K. Fortunately, by exploiting the special form of g(.), we can reduce this cost to linear in K.\nIn evaluating g({Wm,n,k}k) two scenarios are conceivable: 1) at least one of Wm,n,1, . . . ,Wm,n,K is non-zero \u2013\nthat is \u2228\nkWm,n,k = 1 and g(Wm,n,k) evaluates to pOm,n(Om,n | 1). 2) \u2228\nkWm,n,k = 0 and g(Wm,n,k) evaluates to pOm,n(Om,n | 0).\n11\nWe can divide the maximization of (23) into two separate maximization operations over sets of assignments depending on the conditioning above and select the maximum of the two.\nFor simplicity, let m1(W1), . . . ,mK(WK) denote mWm,n,1\u2192gm,n(Wm,n,1) (t), . . . ,mWm,n,K\u2192gm,n(Wm,n,K) (t) respectively. W.L.O.G., let us assume the objective is to calculate the outgoing message to the first variable m\u20321(W1) = mgm,n\u2192Wm,n,1(Wm,n,1)\n(t+1). Let us rewrite (23) using this notation:\nm\u20321(W1) = max W2...WK\n( gm,n({Wk}) + \u2211 k\u2032>1 mk\u2032(Wk\u2032) )\nFor W1 = 1, regardless of assignments to W2, . . . ,WK, we have \u2228 kWm,n,k = 1 and therefore the maximization above simplifies to\nm\u20321(1) = max W2...WK\n( log(pOm,n(Om,n | 1)) \u2211 k\u2032>1 mk\u2032(Wk\u2032) )\n= log(pOm,n(Om,n | 1)) + \u2211 k\u2032>1 max(mk\u2032(0),mk\u2032(1)).\nFor W1 = 0, if \u2200k\u2032>1Wk\u2032 = 0 then g({Wk}) evaluates to log(pOm,n(Om,n | 0), and otherwise it evaluates to log(pOm,n(Om,n | 1). We need to choose the maximum over these two cases. Note that in the second case we have to ensure at least one of the remaining variables is non-zero \u2013 i.e., \u2203k\u2032>1Wk\u2032 = 1. In the following update to enforce this constraint we use\nk\u2217 = argk\u2032>1 maxmk\u2032(1)\u2212mk\u2032(0) (24)\nto get\nm\u20321(0) = max ( log(pOm,n(Om,n | 0) + \u2211 k\u2032>1 mk\u2032(0) ,\nlog(pOm,n(Om,n | 1) + mk\u2217+\u2211 k\u2032>1,k\u2032 6=k\u2217 max(mk\u2032(0),mk\u2032(1))\n)\nwhere, choosing Wk\u2217 = 1 maximizes the second case (where at least one Wk\u2032 for k\u2032 > 1 is non-zero).\nAs before, let us assume that the incoming messages are normalized such that \u2200k\u2032mk\u2032(0) = 0, and therefore \u0393\u0302m,n,k\u2032 =\nmk\u2032(1). The normalized outgoing message is\n\u0393m,n,1 = m \u2032 1(1)\u2212m\u2032(0) = log(pOm,n(Om,n | 1)) + \u2211 k\u2032>1 max(0,mk\u2032(1))\u2212\nmax ( log(pOm,n(Om,n | 0), log(pOm,n(Om,n | 1) + mk\u2217 + \u2211\nk\u2032>1,k\u2032 6=k\u2217 max(0,mk\u2032(1)) ) = min ( log(pOm,n(Om,n | 1))\u2212 log(pOm,n(Om,n | 0) + \u2211 k\u2032>1 max(0,mk\u2032(1)),max(\u2212mk\u2217(1), 0) )\n= min (\u2211 k\u2032>1 max(0, \u0393\u0302 (t) m,n,k\u2032)\n+ log ( pOm,n(Om,n | 1) pOm,n(Om,n | 0) ) ,max(0,\u2212max k>1 \u0393\u0302 (t) m,n,k\u2032) ) where in the last step we used the definition of factor g and (24) that defines mk\u2217(1). This produces the simplified form of BP messages for the update (5f) in our algorithm."}, {"heading": "APPENDIX B MARGINAL-MAP", "text": "While the message passing for MAP inference approximates the \u201cjointly\u201d optimal assignment to X and Y in the Bayesian setting, the marginals p(Xm,k | O) and p(Xk,n | O) are concerned with optimal assignments to \u201cindividual\u201d Xm,k and Yk,n for each m,n and k. Here again, message passing can approximate the log-ratio of these marginals.\nWe use the function \u03c6(a) = log(1 + exp(a)) and its inverse \u03c6\u22121(b) = log(exp(b) \u2212 1) in the following updates for marginalization.\n\u03a6 (t+1) m,n,k := \u0393 (t) m,n,k + \u03a8\u0302 (t) m,n,k\u2212 log ( 1 + exp(\u03a8\u0302\n(t) m,n,k) + exp(\u03a6\u0302 (t) m,n,k) ) \u03a8\n(t+1) m,n,k := \u0393 (t) m,n,k + \u03a6\u0302 (t) m,n,k\u2212 log ( 1 + exp(\u03a6\u0302\n(t) m,n,k + exp(\u03a8\u0302 (t) m,n,k) ) \u03a6\u0302\n(t+1) m,n,k := log\n( pXm,k(1)\npXm,k(0) ) + \u2211 n\u2032 6=n \u03a6 (t) m,n\u2032,k\n\u03a8\u0302 (t+1) m,n,k := log\n( pYn,k(1)\npYn,k(0)\n) + \u2211\nm\u2032 6=m\n\u03a8 (t) m\u2032,n,k\n\u0393\u0302 (t+1) m,n,k := \u03a6\u0302 (t) m,n,k + \u03a8\u0302 (t) m,n,k\n\u0393 (t+1) m,n,k := \u2211 k\u2032 6=k \u03c6(\u0393\u0302 (t) m,n,k\u2032) + log ( pOm,n(Om,n | 1) pOm,n(Om,n | 0) ) \u2212 \u03c6 ( \u03c6\u22121\n( \u2211 k\u2032 6=k \u03c6(\u0393\u0302 (t) m,n,k\u2032) ) + log\n( pOm,n(Om,n | 1) pOm,n(Om,n | 0) ))\n12\nHere, again using (6), we can recover X and Y from the marginals. However, due to the symmetry of the set of solutions, one needs to perform decimation to obtain an assignment to X and Y. Decimation is the iterative process of running message passing then fixing the most biased variable \u2013 e.g., an Xm,k \u2208 argm,k max |\u039em,k| \u2013 after each convergence. While a simple randomized initialization of messages is often enough to break the symmetry of the solutions in max-sum inference, in the sum-product case one has to repeatedly fix a new subset of most biased variables."}, {"heading": "APPENDIX C IMAGE COMPLETION", "text": "Figure 6 is an example of completing a 1000\u00d7 1000 black and white image, here using message passing or GLRM. In fig. 6(a) we vary the number of observed pixels \u03c1 \u2208 {.01, .02, .05} with fixed K = 10 and in fig. 6(b) we vary the rank K \u2208 {2, 20, 200}, while fixing \u03c1 = .02. A visual inspection of reconstructions suggests that, since GLRM is using real factors, it can easily over-fit the observation as we increase the rank. However, the Boolean factorization, despite being expressive, does not show over-fitting behavior for larger rank values \u2013 as if the result was regularized. In fig. 6(c), we regularize both methods for K = 20: for GLRM we use Gaussian priors over both X and Y and for message passing we use sparsity inducing priors pXm,k(0) = p X m,k(0) = .9. This improves the performance of both methods. However, note that regularization does not significantly improve the results of GLRM when applied to the matrix completion task, where the underlying factors are known to be Boolean (see fig. 2(right)).\n13"}], "references": [{"title": "Boolean compressed sensing and noisy group testing", "author": ["George K Atia", "Venkatesh Saligrama"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Atia and Saligrama.,? \\Q2012\\E", "shortCiteRegEx": "Atia and Saligrama.", "year": 2012}, {"title": "Maxproduct for maximum weight matching: Convergence, correctness, and lp duality", "author": ["Mohsen Bayati", "Devavrat Shah", "Mayank Sharma"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bayati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bayati et al\\.", "year": 2008}, {"title": "Discovery of optimal factors in binary data via a novel method of matrix decomposition", "author": ["Radim Belohlavek", "Vilem Vychodil"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Belohlavek and Vychodil.,? \\Q2010\\E", "shortCiteRegEx": "Belohlavek and Vychodil.", "year": 2010}, {"title": "Fast factorization by similarity in formal concept analysis of data with fuzzy attributes", "author": ["Radim Belohlavek", "Ji\u0159\u0131\u0301 Dvo\u0159\u00e1k", "Jan Outrata"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Belohlavek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Belohlavek et al\\.", "year": 2007}, {"title": "Survey propagation: An algorithm for satisfiability", "author": ["Alfredo Braunstein", "Marc M\u00e9zard", "Riccardo Zecchina"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Braunstein et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Braunstein et al\\.", "year": 2005}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Candes", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan.", "year": 2010}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2012}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference,", "citeRegEx": "Davenport et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2014}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "author": ["Aurelien Decelle", "Florent Krzakala", "Cristopher Moore", "Lenka Zdeborov\u00e1"], "venue": "Physical Review E,", "citeRegEx": "Decelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Decelle et al\\.", "year": 2011}, {"title": "Ldpc codes for compressed sensing", "author": ["Alexandros G Dimakis", "Roxana Smarandache", "Pascal O Vontobel"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Dimakis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dimakis et al\\.", "year": 2012}, {"title": "Compressed sensing", "author": ["David L Donoho"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Message-passing algorithms for compressed sensing", "author": ["David L Donoho", "Arian Maleki", "Andrea Montanari"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Donoho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Donoho et al\\.", "year": 2009}, {"title": "Combinatorial group testing and its applications", "author": ["Ding-Zhu Du", "Frank K Hwang"], "venue": "World Scientific,", "citeRegEx": "Du and Hwang.,? \\Q1993\\E", "shortCiteRegEx": "Du and Hwang.", "year": 1993}, {"title": "Low-density parity-check codes", "author": ["Robert G Gallager"], "venue": "Information Theory, IRE Transactions on,", "citeRegEx": "Gallager.,? \\Q1962\\E", "shortCiteRegEx": "Gallager.", "year": 1962}, {"title": "Belief propagation for min-cost network flow: Convergence and correctness", "author": ["David Gamarnik", "Devavrat Shah", "Yehua Wei"], "venue": "Operations Research,", "citeRegEx": "Gamarnik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gamarnik et al\\.", "year": 2012}, {"title": "Tiling databases", "author": ["Floris Geerts", "Bart Goethals", "Taneli Mielik\u00e4inen"], "venue": "In Discovery science,", "citeRegEx": "Geerts et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Geerts et al\\.", "year": 2004}, {"title": "Analyzing the us senate in 2003: Similarities, clusters, and blocs", "author": ["Aleks Jakulin", "Wray Buntine", "Timothy M La Pira", "Holly Brasher"], "venue": "Political Analysis, page mpp006,", "citeRegEx": "Jakulin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jakulin et al\\.", "year": 2009}, {"title": "Phase transitions and sample complexity in bayes-optimal matrix factorization", "author": ["Yoshiyuki Kabashima", "Florent Krzakala", "Marc M\u00e9zard", "Ayaka Sakata", "Lenka Zdeborov\u00e1"], "venue": "arXiv preprint arXiv:1402.1298,", "citeRegEx": "Kabashima et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kabashima et al\\.", "year": 2014}, {"title": "Binary factor analysis with help of formal concepts", "author": ["Ales Keprt", "V\u00e1clav Sn\u00e1sel"], "venue": "In CLA,", "citeRegEx": "Keprt and Sn\u00e1sel.,? \\Q2004\\E", "shortCiteRegEx": "Keprt and Sn\u00e1sel.", "year": 2004}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Boolean matrix theory and applications. Monographs and textbooks in pure and applied mathematics", "author": ["K.H. Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q1982\\E", "shortCiteRegEx": "Kim.", "year": 1982}, {"title": "Phase diagram and approximate message passing for blind calibration and dictionary learning", "author": ["Florent Krzakala", "Marc M\u00e9zard", "Lenka Zdeborov\u00e1"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Krzakala et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krzakala et al\\.", "year": 2013}, {"title": "Factor graphs and the sum-product algorithm", "author": ["Frank R Kschischang", "Brendan J Frey", "Hans-Andrea Loeliger"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Optimal boolean matrix decomposition: Application to role engineering", "author": ["Haibing Lu", "Jaideep Vaidya", "Vijayalakshmi Atluri"], "venue": "In Data Engineering,", "citeRegEx": "Lu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2008}, {"title": "Factor analysis and related methods", "author": ["Roderick P McDonald"], "venue": null, "citeRegEx": "McDonald.,? \\Q2014\\E", "shortCiteRegEx": "McDonald.", "year": 2014}, {"title": "Model order selection for boolean matrix factorization", "author": ["Pauli Miettinen", "Jilles Vreeken"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Miettinen and Vreeken.,? \\Q2011\\E", "shortCiteRegEx": "Miettinen and Vreeken.", "year": 2011}, {"title": "The discrete basis problem", "author": ["Pauli Miettinen", "Taneli Mielik\u00e4inen", "Aristides Gionis", "Gautam Das", "Heikki Mannila"], "venue": "In Knowledge Discovery in Databases: PKDD", "citeRegEx": "Miettinen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Miettinen et al\\.", "year": 2006}, {"title": "Bilinear generalized approximate message passing", "author": ["Jason T Parker", "Philip Schniter", "Volkan Cevher"], "venue": "arXiv preprint arXiv:1310.2632,", "citeRegEx": "Parker et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Parker et al\\.", "year": 2013}, {"title": "Reverend bayes on inference engines: A distributed hierarchical approach", "author": ["Judea Pearl"], "venue": "In AAAI,", "citeRegEx": "Pearl.,? \\Q1982\\E", "shortCiteRegEx": "Pearl.", "year": 1982}, {"title": "Perturbed message passing for constraint satisfaction problems", "author": ["Siamak Ravanbakhsh", "Russell Greiner"], "venue": "arXiv preprint arXiv:1401.6686,", "citeRegEx": "Ravanbakhsh and Greiner.,? \\Q2014\\E", "shortCiteRegEx": "Ravanbakhsh and Greiner.", "year": 2014}, {"title": "Augmentative message passing for traveling salesman problem and graph partitioning", "author": ["Siamak Ravanbakhsh", "Reihaneh Rabbany", "Russell Greiner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ravanbakhsh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ravanbakhsh et al\\.", "year": 2014}, {"title": "Loss functions for preference levels: Regression with discrete ordered labels", "author": ["Jason DM Rennie", "Nathan Srebro"], "venue": "In Proceedings of the IJCAI multidisciplinary workshop on advances in preference handling,", "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Note on noisy group testing: asymptotic bounds and belief propagation reconstruction", "author": ["Dino Sejdinovic", "Oliver Johnson"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "Sejdinovic and Johnson.,? \\Q2010\\E", "shortCiteRegEx": "Sejdinovic and Johnson.", "year": 2010}, {"title": "The set basis problem is NP-complete", "author": ["Larry J Stockmeyer"], "venue": "IBM Thomas J. Watson Research Division,", "citeRegEx": "Stockmeyer.,? \\Q1975\\E", "shortCiteRegEx": "Stockmeyer.", "year": 1975}, {"title": "What is the dimension of your binary data", "author": ["Nikolaj Tatti", "Taneli Mielikainen", "Aristides Gionis", "Heikki Mannila"], "venue": "In Data Mining,", "citeRegEx": "Tatti et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tatti et al\\.", "year": 2006}, {"title": "Generalized low rank models", "author": ["Madeleine Udell", "Corinne Horn", "Reza Zadeh", "Stephen Boyd"], "venue": "arXiv preprint arXiv:1410.0342,", "citeRegEx": "Udell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Udell et al\\.", "year": 2014}, {"title": "The role mining problem: finding a minimal descriptive set of roles", "author": ["Jaideep Vaidya", "Vijayalakshmi Atluri", "Qi Guo"], "venue": "In Proceedings of the 12th ACM symposium on Access control models and technologies,", "citeRegEx": "Vaidya et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vaidya et al\\.", "year": 2007}, {"title": "On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs", "author": ["Yair Weiss", "William T Freeman"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Weiss and Freeman.,? \\Q2001\\E", "shortCiteRegEx": "Weiss and Freeman.", "year": 2001}, {"title": "Map estimation, linear programming and belief propagation with convex free energies", "author": ["Yair Weiss", "Chen Yanover", "Talya Meltzer"], "venue": "arXiv preprint arXiv:1206.5286,", "citeRegEx": "Weiss et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2012}, {"title": "Generalized belief propagation", "author": ["Jonathan S Yedidia", "William T Freeman", "Yair Weiss"], "venue": "In NIPS,", "citeRegEx": "Yedidia et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2000}, {"title": "Binary matrix factorization for analyzing gene expression data", "author": ["Zhong-Yuan Zhang", "Tao Li", "Chris Ding", "Xian-Wen Ren", "Xiang-Sun Zhang"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Binary matrix factorization with applications", "author": ["Zhongyuan Zhang", "Chris Ding", "Tao Li", "Xiangsun Zhang"], "venue": "In Data Mining,", "citeRegEx": "Zhang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "Nimfa: A python library for nonnegative matrix factorization", "author": ["Marinka Zitnik", "Blaz Zupan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zitnik and Zupan.,? \\Q2012\\E", "shortCiteRegEx": "Zitnik and Zupan.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "SAT); here message passing has been able to solve hard random SAT instances close to the unsatisfiability transition (Braunstein et al., 2005; Ravanbakhsh and Greiner, 2014a).", "startOffset": 117, "endOffset": 174}, {"referenceID": 13, "context": "Here again, message passing decoding has been able to transmit Z and recover Y (given the parity checks X) at rates close to the theoretical capacity of the communication channel (Gallager, 1962).", "startOffset": 179, "endOffset": 195}, {"referenceID": 10, "context": "LDPC codes are in turn closely related to the compressed sensing (Donoho, 2006) \u2013 so much so that successful binary LDPC codes (i.", "startOffset": 65, "endOffset": 79}, {"referenceID": 9, "context": ", matrix X) have been reused for compressed sensing (Dimakis et al., 2012).", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": ", ` non-zero values) which makes it possible to use approximate message passing (Donoho et al., 2009) to recover Y using few noisy measurements O \u2013 that is M K and similar to LDPC codes, the measurement matrix X is known.", "startOffset": 80, "endOffset": 101}, {"referenceID": 12, "context": "When the underlying algebra is Boolean, the compressed sensing problem reduces to the problem of (noisy) group testing, which has a longer history (Du and Hwang, 1993).", "startOffset": 147, "endOffset": 167}, {"referenceID": 0, "context": "Each of these Y-bundles corresponds to a row of X (in (1a)), and message passing has been successfully applied in this setting as well (Atia and Saligrama, 2012; Sejdinovic and Johnson, 2010).", "startOffset": 135, "endOffset": 191}, {"referenceID": 32, "context": "Each of these Y-bundles corresponds to a row of X (in (1a)), and message passing has been successfully applied in this setting as well (Atia and Saligrama, 2012; Sejdinovic and Johnson, 2010).", "startOffset": 135, "endOffset": 191}, {"referenceID": 24, "context": "The umbrella term \u201cfactor analysis\u201d refers to the unsupervised methodology of expressing a set of observations in terms of unobserved factors (McDonald, 2014).", "startOffset": 142, "endOffset": 158}, {"referenceID": 21, "context": "Here, approximate message passing techniques (inspired by their success in compressed sensing) have been successfully extended to matrix factorization, matrix calibration and robust PCA over real domain (Krzakala et al., 2013; Parker et al., 2013; Kabashima et al., 2014).", "startOffset": 203, "endOffset": 271}, {"referenceID": 27, "context": "Here, approximate message passing techniques (inspired by their success in compressed sensing) have been successfully extended to matrix factorization, matrix calibration and robust PCA over real domain (Krzakala et al., 2013; Parker et al., 2013; Kabashima et al., 2014).", "startOffset": 203, "endOffset": 271}, {"referenceID": 17, "context": "Here, approximate message passing techniques (inspired by their success in compressed sensing) have been successfully extended to matrix factorization, matrix calibration and robust PCA over real domain (Krzakala et al., 2013; Parker et al., 2013; Kabashima et al., 2014).", "startOffset": 203, "endOffset": 271}, {"referenceID": 26, "context": "basis (Miettinen et al., 2006)/ concept / category (Belohlavek and Vychodil, 2010; Keprt and Sn\u00e1sel, 2004)) in observations, where each factor is in turn a binary pattern.", "startOffset": 6, "endOffset": 30}, {"referenceID": 2, "context": ", 2006)/ concept / category (Belohlavek and Vychodil, 2010; Keprt and Sn\u00e1sel, 2004)) in observations, where each factor is in turn a binary pattern.", "startOffset": 28, "endOffset": 83}, {"referenceID": 18, "context": ", 2006)/ concept / category (Belohlavek and Vychodil, 2010; Keprt and Sn\u00e1sel, 2004)) in observations, where each factor is in turn a binary pattern.", "startOffset": 28, "endOffset": 83}, {"referenceID": 33, "context": "Boolean factorization is also closely related to the tiling problem (Stockmeyer, 1975).", "startOffset": 68, "endOffset": 86}, {"referenceID": 23, "context": "Despite its numerous applications (see Vaidya et al., 2007; Lu et al., 2008; Geerts et al., 2004; Zhang et al., 2010), the non-linearity of Boolean matrix product has confined most techniques to heuristics and local search methods (e.", "startOffset": 34, "endOffset": 117}, {"referenceID": 15, "context": "Despite its numerous applications (see Vaidya et al., 2007; Lu et al., 2008; Geerts et al., 2004; Zhang et al., 2010), the non-linearity of Boolean matrix product has confined most techniques to heuristics and local search methods (e.", "startOffset": 34, "endOffset": 117}, {"referenceID": 40, "context": "Despite its numerous applications (see Vaidya et al., 2007; Lu et al., 2008; Geerts et al., 2004; Zhang et al., 2010), the non-linearity of Boolean matrix product has confined most techniques to heuristics and local search methods (e.", "startOffset": 34, "endOffset": 117}, {"referenceID": 3, "context": ", 2010), the non-linearity of Boolean matrix product has confined most techniques to heuristics and local search methods (e.g., Keprt and Sn\u00e1sel, 2004; Belohlavek et al., 2007).", "startOffset": 121, "endOffset": 176}, {"referenceID": 20, "context": "Schein rank), which is the smallest number K for which a decomposition of Z to X \u2022Y is exact (Kim, 1982).", "startOffset": 93, "endOffset": 104}, {"referenceID": 6, "context": "To formalize approximate decompositions for binary data, we use a communication channel, where we assume the product matrix Z is communicated through a noisy binary erasure channel (Cover and Thomas, 2012) to produce the observation O \u2208 {0, 1,null}M\u00d7N where Om,n = null, means this entry was erased in the channel.", "startOffset": 181, "endOffset": 205}, {"referenceID": 19, "context": "Schein rank), which is the smallest number K for which a decomposition of Z to X \u2022Y is exact (Kim, 1982). For noisy observations, Miettinen and Vreeken (2011) suggest using the minimum description length (MDL) principle to estimate K (also see Tatti et al.", "startOffset": 94, "endOffset": 159}, {"referenceID": 19, "context": "Schein rank), which is the smallest number K for which a decomposition of Z to X \u2022Y is exact (Kim, 1982). For noisy observations, Miettinen and Vreeken (2011) suggest using the minimum description length (MDL) principle to estimate K (also see Tatti et al. (2006) for alternative measures).", "startOffset": 94, "endOffset": 264}, {"referenceID": 33, "context": "Finding the maximizing assignment for (4) is NPhard (Stockmeyer, 1975).", "startOffset": 52, "endOffset": 70}, {"referenceID": 22, "context": "The Probabilistic Graphical Model Figure 1 shows the factor-graph (Kschischang et al., 2001) representation of the posterior (4).", "startOffset": 66, "endOffset": 92}, {"referenceID": 39, "context": "that it represents the zero temperature limit to sum-product BP, which is in turn a fixed point iteration procedure whose fixed points are the local optima of the Bethe approximation to the free energy (Yedidia et al., 2000); see also (Weiss et al.", "startOffset": 202, "endOffset": 224}, {"referenceID": 38, "context": ", 2000); see also (Weiss et al., 2012).", "startOffset": 18, "endOffset": 38}, {"referenceID": 37, "context": "For general factor-graphs, it is known that the approximate MAP solution obtained using max-sum BP is optimal within a \u201cneighborhood\u201d (Weiss and Freeman, 2001).", "startOffset": 134, "endOffset": 159}, {"referenceID": 14, "context": "Although few applications of max-sum BP to loopy factorgraphs have optimality guarantees (e.g., Bayati et al., 2008; Gamarnik et al., 2012), approximations using BP have been successfully applied to machine learning, coding theory and combinatorial optimization problems.", "startOffset": 89, "endOffset": 139}, {"referenceID": 30, "context": "Near-optimal behavior of max-sum BP in dense factor-graph is not without precedence (e.g., Decelle et al., 2011; Ravanbakhsh et al., 2014).", "startOffset": 84, "endOffset": 138}, {"referenceID": 42, "context": "(2007), which was implemented by NIMFA (Zitnik and Zupan, 2012).", "startOffset": 39, "endOffset": 63}, {"referenceID": 40, "context": "Random Matrices Figure 3 compares the reconstruction error of message passing against state-of-the-art Boolean matrix factorization method of Zhang et al. (2007), which was implemented by NIMFA (Zitnik and Zupan, 2012).", "startOffset": 142, "endOffset": 162}, {"referenceID": 35, "context": "The figure compares the average reconstruction error of Generalized Low-Rank Models (GLRM; Udell et al., 2014) and message passing, where the red line is the information theoretic lower-bound of (10) for perfect recovery.", "startOffset": 84, "endOffset": 110}, {"referenceID": 19, "context": "Note that the lower-bound also resembles the O(KNpolylog(N)) sample complexity for various real-domain matrix completion tasks (e.g., Candes and Plan, 2010; Keshavan et al., 2010).", "startOffset": 127, "endOffset": 179}, {"referenceID": 31, "context": "For ordinary matrix completion we use GLRM with ordinal hinge loss (Rennie and Srebro, 2005) and quadratic regularization to predict the ratings for the unobserved entries.", "startOffset": 67, "endOffset": 92}, {"referenceID": 7, "context": "Davenport et al. (2014) also report a good performance by only observing a single bit, instead of ordinal ratings for the MovieLens dataset, albeit on a small version of this dataset (with 100,000 entries) and for the dense setting of \u03b1 = .", "startOffset": 0, "endOffset": 24}], "year": 2017, "abstractText": "Boolean factor analysis is the task of decomposing a Binary matrix to the Boolean product of two binary factors. This unsupervised data-analysis approach is desirable due to its interpretability, but hard to perform due its NP-hardness. A closely related problem is low-rank Boolean matrix completion from noisy observations. We treat these problems as maximum a posteriori inference problems, and present message passing solutions that scale linearly with the number of observations and factors. Our empirical study demonstrates that message passing is able to recover low-rank Boolean matrices, in the boundaries of theoretically possible recovery and outperform existing techniques in real-world applications, such as large-scale binary valued collaborative filtering tasks. A large body of problems in machine learning, communication theory and combinatorial optimization involve the product form Z = X Y where Z = {Zm,n},X = {Xm,k},Y = {Yk,n} where one or two components (out of three) are (partially) known and the task is to recover the unknown component(s). Here, operation corresponds to a type of matrix multiplication. A subset of these problems, which are most closely related to Boolean matrix factorization and matrix completion, can be expressed over the Boolean domain \u2013 i.e., Zm,n, Xm,k, Yk,n \u2208 {false, true} \u223c= {0, 1}. The two most common Boolean matrix products used in such applications are Z = X \u2022Y \u21d2 Zm,n = K \u2228", "creator": "LaTeX with hyperref package"}}}