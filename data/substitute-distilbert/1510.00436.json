{"id": "1510.00436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2015", "title": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and G\\'omez-Rodr\\'iguez (2015) on Dependency Length Minimization", "abstract": "we illustrate recent outcomes ( liu et al., 2015 ; ferrer - i - cancho | g \\'omez - rodr \\'iguez, 2015 ) of our work toward empirical evidence promoting dependency length minimization across languages ( futrell et al., 2015 ). first, we acknowledge error in failing to acknowledge liu ( 2008 )'s previous statements on corpora into 20 languages exceeding similar weights. a correction will appear unlike pnas. afterwards, we argue that contemporary work provides novel, strong evidence for dependency length minimization as a universal quantitative property of languages, beyond this previous work, because it provides baselines which focus imply word order fairness. accordingly, we argue that our choices of baselines were appropriate because they control for alternative theories.", "histories": [["v1", "Thu, 1 Oct 2015 22:09:34 GMT  (213kb,D)", "http://arxiv.org/abs/1510.00436v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["richard futrell", "kyle mahowald", "edward gibson"], "accepted": false, "id": "1510.00436"}, "pdf": {"name": "1510.00436.pdf", "metadata": {"source": "CRF", "title": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and Go\u0301mez-Rodr\u0301\u0131guez (2015) on Dependency Length Minimization", "authors": ["Richard Futrell", "Kyle Mahowald", "Edward Gibson"], "emails": [], "sections": [{"heading": null, "text": "In recent work, we addressed the question of whether dependency length\u2014 the distance between syntactically related words in natural language sentences\u2014 is shorter than one would expect under random baselines (Futrell et al., 2015). This idea has linguistic relevance because if one hypothesizes a universal pressure to minimize dependency length, one can explain a variety of universal properties of languages, including many of the word-order universals noted by Greenberg (1963). Evidence that language users perfer word orders with shorter dependency length than chance supports this hypothesis, known as the dependency length minimization (DLM) hypothesis. The DLM hypothesis is theoretically attractive because it is motivated by general human information processing constraints: minimizing dependency length minimizes the online memory load for human sentence parsing and generation.\nar X\niv :1\n51 0.\n00 43\n6v 1\n[ cs\n.C L\nTwo recent articles have raised important criticisms of our work (Liu et al., 2015; Ferrer-i-Cancho & Go\u0301mez-Rodr\u0301\u0131guez, 2015)."}, {"heading": "1 Random Trees and Random Word Orders", "text": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article.\nNevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis. Liu (2008) uses a \u201crandom tree\u201d baseline, comparing dependency length in attested dependency trees to dependency length in random ordered trees with the same numbers of nodes. For example, the dependency length of a sentence with a tree such as in Figure 1 is compared to the dependency length induced by random ordered trees as in Figure 2. The baseline trees do not share any syntactic structure with the attested trees they are compared to, beyond their length. In contrast, Gildea & Temperley (2010) and Futrell et al. (2015) use \u201crandom word order\u201d baselines, keeping the syntactic dependency structure of attested sentences constant and investigating random word orders given that syntactic structure, subject to a number of linguistic constraints. For example, dependency length for a sentence such as in Figure 1 is compared to dependency length in a sentence with different word order but the same (unordered) dependency tree structure, as in Figure 3. Attested dependency length is shorter than both the random tree and random word order baselines.\nOur finding that attested dependency length is shorter than random word order baselines shows that, given a syntactic structure, language users and language grammars tend to prefer the word order that minimizes dependency\nJohn threw out the trash\n1 1\n3\n1\nTotal dependency length = 6\nTotal dependency length = 8\nA B C D E\n2\n4 3\n2\nTotal dependency length = 11\nA B C D E\n1\n3 2\n1\nTotal dependency length = 7\nFigure 2: Some random trees based on the sentence in Figure 1 according to the Liu (2008) random tree baseline.\nJohn threw the trash out\n1 2\n1\n3\nTotal dependency length = 7\nFigure 3: A random permutation of the sentence in Figure 1 according to a random word order baseline, specifically the head-fixed projective baseline in Futrell et al. (2015). This baseline permutes sister nodes while maintaining head direction.\nlength. This finding supports the DLM hypothesis and provides direct evidence for a specific mechanism (word order preferences) by which dependency length minimization is accomplished.\nOn the other hand, the finding that attested dependency length is shorter than the random tree baselines supports the DLM hypothesis in a more general form and is consistent with many possible mechanisms that shorten dependency length, including non-syntactic mechanisms. For example, it is consistent with the idea that languages might disprefer structures which inevitably create long dependencies, such as high arity trees. It is also consistent with the hypothesis that language users prefer sentences with structures that create long dependencies, and might structure discourse to avoid such sentences. For example, the sentence (1) \u201cA man who was wearing a hat arrived\u201d has a long dependency between the subject \u201cman\u201d and the verb \u201carrived\u201d because the relative clause \u201cwho was wearing a hat\u201d intervenes between them. Language users might prefer to instead say (2) \u201cA man arrived\u201d, avoiding the relative clause between the subject and the verb, and perhaps mentioning the information about the hat in another sentence later in discourse, or perhaps dropping it altogether. Though language users are ultimately achieving the same or similar communicative goals in saying sentence (1) and sentence (2), they are doing so by expressing different propositional content in each sentence. The mechanisms by which dependency length minimization is accomplished in comparison to a random tree baseline are thus highly general: in addition to word order preferences, languages might have tree structure preferences; and language users might strategically choose what content to express, in addition to what word order to use, in\norder to avoid long dependencies. In summary, comparing to random tree baselines can show DLM as a result of many mechanisms, including the content that people choose to express and/or the word orders they use in sentences. So the finding that attested dependency length is shorter than this baseline supports an influence of DLM on discourse structure or syntactic structure or both. Comparing to the random word order baseline, on the other hand, shows specifically that the word orders that people prefer, given the content they choose to express, are those that minimize dependency length. That is, it shows unambiguously that DLM as a pressure affects syntactic structure and word order in particular. Because our findings are only compatible with dependencylength-minimizing preferences in word order, we believe they provide novel, strong evidence for the DLM hypothesis as it pertains to syntax. Our claim is that, all else being equal, language users prefer linearizations with short dependency length. Only the comparison to a random word order baseline supports this claim unambiguously. So we see this work as a complement of Liu (2008) and related work, strengthening the body of evidence for the DLM hypothesis, rather than a repetition.\nThe difference between random tree baselines and random word order baselines can also explain some discrepancies between our work and previous findings. For example, we find relatively long dependency lengths for head-final languages such as Japanese and Turkish, whereas Hiranuma (1999) finds that dependency length in Japanese is highly optimized. Hiranuma (1999)\u2019s finding is specifically that Japanese speakers drop verbal arguments to achieve dependency length minimization, trusting that the language comprehender will be able to infer the missing arguments from discourse context. Our finding is that, given the set of words and the dependency tree that Japanese speakers want to express, they choose orders with longer dependency length than, say, English speakers. (This finding remains unexplained.)"}, {"heading": "2 Projective Baselines", "text": "The second major issue raised in both Liu et al. (2015) and Ferrer-i-Cancho & Go\u0301mez-Rodr\u0301\u0131guez (2015) is our choice of baselines for comparison. We use projective linearizations, meaning that when a dependency tree is drawn over a linearized sentence, none of the arcs of the tree cross. We also use\nlinearizations incorporating other factors that might conceivably influence word order: a pressure for fixed word order, and a pressure for consistency in head direction. These three factors\u2014projectivity, head direction consistency, and fixed word order\u2014all have the effect of reducing dependency length, and so it has been argued for the first two that they need not be considered separate factors, but rather the result of DLM. Ferrer-i-Cancho and Go\u0301mezRodr\u0301\u0131guez (2015) argue that our use of these baselines is redundant for this reason.\nWe believe comparison to these baselines provides stronger evidence for DLM than comparison only to a fully nonprojective baseline, because it shows that the phenomenon of short dependencies must be explained even if independent factors affecting word order are assumed. Since DLM can explain the phenomena attributed to these other factors, the most parsimonious theory seems to be that DLM is the only factor influencing word order. But we can only make this argument after showing that the shortness of dependencies persists as a phenomenon even after controlling for these other hypothetical factors. For example, suppose we had found that attested dependency length was not shorter than the projective random baselines1. One would be left with the question of why, if DLM is the main factor influencing language structure, German speakers pass up opportunities to minimize dependency length. Then one could argue that DLM is not a good explanation for projectivity, since word orders are not minimized for dependency length beyond what is needed to establish projectivity, which itself might have independent motivations (such as enabling polynomial-time parsing). Since we found that dependency length is shorter than this baseline in many languages, this line of argumentation is no longer available.\nFor the sake of completeness, we provide a comparison of attested dependency lengths with dependency lengths in random nonprojective linearizations in Figure 4. For this baseline, the dependency tree is linearized by shuffling nodes at random. The baselines from Futrell et al. (2015) are also shown. The figure shows that dependency length is much shorter than the nonprojective baseline, and that the projective baselines are much more conservative than the nonprojective baseline. We felt that including the nonprojective baselines in the original paper would be redundant, since Ferreri-Cancho (2006) showed that projective trees on average have shorter de-\n1Which would not have been surprising given previous work: Gildea & Temperley (2010) found much weaker minimization in German than in English.\npendency length than nonprojective trees, and Kuhlmann (2013) (among others) showed that natural language dependency trees are overwhelmingly projective.\nWe also want to stress that, contra Ferrer-i-Cancho & Go\u0301mez-Rodr\u0301\u0131guez (2015), controlling for these possible alternative factors affecting word order does not imply that we are accepting traditional nativist or Universal Grammar-based hypotheses. These factors have possible functional explanations, just as DLM does. Fixed word order can be motivated by efficient communication of relation types; consistent head direcion can be motivated by compression of grammars; and projectivity can be motivated by the time complexity of parsing, where parsing to projective trees is cubic-time but parsing to fully nonprojective trees is NP-hard. In general, we aimed to include the most conservative reasonable baselines."}, {"heading": "3 Other Issues", "text": "Liu et al. (2015) also raise a number of more specific criticisms. They claim that the uniformity of genres of the text in our corpora could be a confounding factor. The criticism is valid: It is true that our corpora were primarily (but not entirely) written text from newspapers and novels. Nevertheless, we would find it surprising if DLM universally influenced novels and newspapers but not language use in general. We welcome any work which controls for this possible issue.\nFinally, Liu et al. (2015) also note that in our original paper we state that head-final languages appear to have longer dependencies than more head-initial or head-medial languages, but we do not provide statistical tests of this claim. We intended this remark not as a main claim of the paper, but as a conjecture intended to draw attention to the wide variation between languages in their dependency length, and possible typological implications of that variation. Working out the correct statistical methodology and gathering the right data to make this a strong empirical claim would require another whole paper. The question of variation in dependency length has also been a major focus of Liu\u2019s research. We feel that explaining this variation is the most interesting direction for future dependency length research, and we hope to join our present critics in future investigations of this phenomenon."}], "references": [{"title": "Why do syntactic links not cross", "author": ["R. Ferrer-i-Cancho"], "venue": "Europhysics Letters,", "citeRegEx": "Ferrer.i.Cancho,? \\Q2006\\E", "shortCiteRegEx": "Ferrer.i.Cancho", "year": 2006}, {"title": "Liberating language research from dogmas of the 20th century", "author": ["R. Ferrer-i-Cancho", "C. G\u00f3mez-Rod\u0155\u0131guez"], "venue": null, "citeRegEx": "Ferrer.i.Cancho and G\u00f3mez.Rod\u0155\u0131guez,? \\Q2015\\E", "shortCiteRegEx": "Ferrer.i.Cancho and G\u00f3mez.Rod\u0155\u0131guez", "year": 2015}, {"title": "Large-scale evidence of dependency length minimization in 37 languages", "author": ["R. Futrell", "K. Mahowald", "E. Gibson"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Futrell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Futrell et al\\.", "year": 2015}, {"title": "Do grammars minimize dependency length", "author": ["D. Gildea", "D. Temperley"], "venue": "Cognitive Science,", "citeRegEx": "Gildea and Temperley,? \\Q2010\\E", "shortCiteRegEx": "Gildea and Temperley", "year": 2010}, {"title": "Some universals of grammar with particular reference to the order of meaningful elements", "author": ["J. Greenberg"], "venue": "In J. Greenberg (ed.), Universals of Language,", "citeRegEx": "Greenberg,? \\Q1963\\E", "shortCiteRegEx": "Greenberg", "year": 1963}, {"title": "Syntactic difficulty in English and Japanese: A textual study", "author": ["S. Hiranuma"], "venue": "UCL Working Papers in Linguistics,", "citeRegEx": "Hiranuma,? \\Q1999\\E", "shortCiteRegEx": "Hiranuma", "year": 1999}, {"title": "Mildly non-projective dependency grammar", "author": ["M. Kuhlmann"], "venue": "Computational Linguistics,", "citeRegEx": "Kuhlmann,? \\Q2013\\E", "shortCiteRegEx": "Kuhlmann", "year": 2013}, {"title": "Dependency distance as a metric of language comprehension difficulty", "author": ["H. Liu"], "venue": "Journal of Cognitive Science,", "citeRegEx": "Liu,? \\Q2008\\E", "shortCiteRegEx": "Liu", "year": 2008}, {"title": "Dependency length minimization: Puzzles and Promises", "author": ["H. Liu", "C. Xu", "J. Liang"], "venue": "arXiv, 1509.04393", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and G\u00f3mez-Rod\u0155\u0131guez (2015) on Dependency Length Minimization", "startOffset": 12, "endOffset": 38}, {"referenceID": 0, "context": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and G\u00f3mez-Rod\u0155\u0131guez (2015) on Dependency Length Minimization", "startOffset": 42, "endOffset": 85}, {"referenceID": 2, "context": "G\u00f3mez-Rod\u0155\u0131guez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015).", "startOffset": 108, "endOffset": 130}, {"referenceID": 2, "context": "G\u00f3mez-Rod\u0155\u0131guez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)\u2019s previous work on corpora of 20 languages with similar aims.", "startOffset": 109, "endOffset": 197}, {"referenceID": 2, "context": "In recent work, we addressed the question of whether dependency length\u2014 the distance between syntactically related words in natural language sentences\u2014 is shorter than one would expect under random baselines (Futrell et al., 2015).", "startOffset": 208, "endOffset": 230}, {"referenceID": 2, "context": "In recent work, we addressed the question of whether dependency length\u2014 the distance between syntactically related words in natural language sentences\u2014 is shorter than one would expect under random baselines (Futrell et al., 2015). This idea has linguistic relevance because if one hypothesizes a universal pressure to minimize dependency length, one can explain a variety of universal properties of languages, including many of the word-order universals noted by Greenberg (1963). Evidence that language users perfer word orders with shorter dependency length than chance supports this hypothesis, known as the dependency length minimization (DLM) hypothesis.", "startOffset": 209, "endOffset": 481}, {"referenceID": 8, "context": "Two recent articles have raised important criticisms of our work (Liu et al., 2015; Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez, 2015).", "startOffset": 65, "endOffset": 124}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims.", "startOffset": 7, "endOffset": 25}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees.", "startOffset": 7, "endOffset": 146}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article.", "startOffset": 7, "endOffset": 763}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis.", "startOffset": 7, "endOffset": 901}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis. Liu (2008) uses a \u201crandom tree\u201d baseline, comparing dependency length in attested dependency trees to dependency length in random ordered trees with the same numbers of nodes.", "startOffset": 7, "endOffset": 1026}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis. Liu (2008) uses a \u201crandom tree\u201d baseline, comparing dependency length in attested dependency trees to dependency length in random ordered trees with the same numbers of nodes. For example, the dependency length of a sentence with a tree such as in Figure 1 is compared to the dependency length induced by random ordered trees as in Figure 2. The baseline trees do not share any syntactic structure with the attested trees they are compared to, beyond their length. In contrast, Gildea & Temperley (2010) and Futrell et al.", "startOffset": 7, "endOffset": 1519}, {"referenceID": 2, "context": "In contrast, Gildea & Temperley (2010) and Futrell et al. (2015) use \u201crandom word order\u201d baselines, keeping the syntactic dependency structure of attested sentences constant and investigating random word orders given that syntactic structure, subject to a number of linguistic constraints.", "startOffset": 43, "endOffset": 65}, {"referenceID": 7, "context": "Figure 2: Some random trees based on the sentence in Figure 1 according to the Liu (2008) random tree baseline.", "startOffset": 79, "endOffset": 90}, {"referenceID": 2, "context": "Figure 3: A random permutation of the sentence in Figure 1 according to a random word order baseline, specifically the head-fixed projective baseline in Futrell et al. (2015). This baseline permutes sister nodes while maintaining head direction.", "startOffset": 153, "endOffset": 175}, {"referenceID": 6, "context": "So we see this work as a complement of Liu (2008) and related work, strengthening the body of evidence for the DLM hypothesis, rather than a repetition.", "startOffset": 39, "endOffset": 50}, {"referenceID": 5, "context": "For example, we find relatively long dependency lengths for head-final languages such as Japanese and Turkish, whereas Hiranuma (1999) finds that dependency length in Japanese is highly optimized.", "startOffset": 119, "endOffset": 135}, {"referenceID": 5, "context": "For example, we find relatively long dependency lengths for head-final languages such as Japanese and Turkish, whereas Hiranuma (1999) finds that dependency length in Japanese is highly optimized. Hiranuma (1999)\u2019s finding is specifically that Japanese speakers drop verbal arguments to achieve dependency length minimization, trusting that the language comprehender will be able to infer the missing arguments from discourse context.", "startOffset": 119, "endOffset": 213}, {"referenceID": 6, "context": "The second major issue raised in both Liu et al. (2015) and Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez (2015) is our choice of baselines for comparison.", "startOffset": 38, "endOffset": 56}, {"referenceID": 0, "context": "(2015) and Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez (2015) is our choice of baselines for comparison.", "startOffset": 11, "endOffset": 52}, {"referenceID": 0, "context": "Ferrer-i-Cancho and G\u00f3mezRod\u0155\u0131guez (2015) argue that our use of these baselines is redundant for this reason.", "startOffset": 0, "endOffset": 42}, {"referenceID": 0, "context": "Ferrer-i-Cancho and G\u00f3mezRod\u0155\u0131guez (2015) argue that our use of these baselines is redundant for this reason. We believe comparison to these baselines provides stronger evidence for DLM than comparison only to a fully nonprojective baseline, because it shows that the phenomenon of short dependencies must be explained even if independent factors affecting word order are assumed. Since DLM can explain the phenomena attributed to these other factors, the most parsimonious theory seems to be that DLM is the only factor influencing word order. But we can only make this argument after showing that the shortness of dependencies persists as a phenomenon even after controlling for these other hypothetical factors. For example, suppose we had found that attested dependency length was not shorter than the projective random baselines. One would be left with the question of why, if DLM is the main factor influencing language structure, German speakers pass up opportunities to minimize dependency length. Then one could argue that DLM is not a good explanation for projectivity, since word orders are not minimized for dependency length beyond what is needed to establish projectivity, which itself might have independent motivations (such as enabling polynomial-time parsing). Since we found that dependency length is shorter than this baseline in many languages, this line of argumentation is no longer available. For the sake of completeness, we provide a comparison of attested dependency lengths with dependency lengths in random nonprojective linearizations in Figure 4. For this baseline, the dependency tree is linearized by shuffling nodes at random. The baselines from Futrell et al. (2015) are also shown.", "startOffset": 0, "endOffset": 1702}, {"referenceID": 0, "context": "Ferrer-i-Cancho and G\u00f3mezRod\u0155\u0131guez (2015) argue that our use of these baselines is redundant for this reason. We believe comparison to these baselines provides stronger evidence for DLM than comparison only to a fully nonprojective baseline, because it shows that the phenomenon of short dependencies must be explained even if independent factors affecting word order are assumed. Since DLM can explain the phenomena attributed to these other factors, the most parsimonious theory seems to be that DLM is the only factor influencing word order. But we can only make this argument after showing that the shortness of dependencies persists as a phenomenon even after controlling for these other hypothetical factors. For example, suppose we had found that attested dependency length was not shorter than the projective random baselines. One would be left with the question of why, if DLM is the main factor influencing language structure, German speakers pass up opportunities to minimize dependency length. Then one could argue that DLM is not a good explanation for projectivity, since word orders are not minimized for dependency length beyond what is needed to establish projectivity, which itself might have independent motivations (such as enabling polynomial-time parsing). Since we found that dependency length is shorter than this baseline in many languages, this line of argumentation is no longer available. For the sake of completeness, we provide a comparison of attested dependency lengths with dependency lengths in random nonprojective linearizations in Figure 4. For this baseline, the dependency tree is linearized by shuffling nodes at random. The baselines from Futrell et al. (2015) are also shown. The figure shows that dependency length is much shorter than the nonprojective baseline, and that the projective baselines are much more conservative than the nonprojective baseline. We felt that including the nonprojective baselines in the original paper would be redundant, since Ferreri-Cancho (2006) showed that projective trees on average have shorter de-", "startOffset": 0, "endOffset": 2022}, {"referenceID": 5, "context": "pendency length than nonprojective trees, and Kuhlmann (2013) (among others) showed that natural language dependency trees are overwhelmingly projective.", "startOffset": 46, "endOffset": 62}, {"referenceID": 0, "context": "We also want to stress that, contra Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez (2015), controlling for these possible alternative factors affecting word order does not imply that we are accepting traditional nativist or Universal Grammar-based hypotheses.", "startOffset": 36, "endOffset": 77}], "year": 2015, "abstractText": "We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and G\u00f3mez-Rod\u0155\u0131guez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)\u2019s previous work on corpora of 20 languages with similar aims. A correction will appear in PNAS. Nevertheless, we argue that our work provides novel, strong evidence for dependency length minimization as a universal quantitative property of languages, beyond this previous work, because it provides baselines which focus on word order preferences. Second, we argue that our choices of baselines were appropriate because they control for alternative theories. In recent work, we addressed the question of whether dependency length\u2014 the distance between syntactically related words in natural language sentences\u2014 is shorter than one would expect under random baselines (Futrell et al., 2015). This idea has linguistic relevance because if one hypothesizes a universal pressure to minimize dependency length, one can explain a variety of universal properties of languages, including many of the word-order universals noted by Greenberg (1963). Evidence that language users perfer word orders with shorter dependency length than chance supports this hypothesis, known as the dependency length minimization (DLM) hypothesis. The DLM hypothesis is theoretically attractive because it is motivated by general human information processing constraints: minimizing dependency length minimizes the online memory load for human sentence parsing and generation. 1 ar X iv :1 51 0. 00 43 6v 1 [ cs .C L ] 1 O ct 2 01 5 Two recent articles have raised important criticisms of our work (Liu et al., 2015; Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez, 2015). 1 Random Trees and Random Word Orders First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis. Liu (2008) uses a \u201crandom tree\u201d baseline, comparing dependency length in attested dependency trees to dependency length in random ordered trees with the same numbers of nodes. For example, the dependency length of a sentence with a tree such as in Figure 1 is compared to the dependency length induced by random ordered trees as in Figure 2. The baseline trees do not share any syntactic structure with the attested trees they are compared to, beyond their length. In contrast, Gildea & Temperley (2010) and Futrell et al. (2015) use \u201crandom word order\u201d baselines, keeping the syntactic dependency structure of attested sentences constant and investigating random word orders given that syntactic structure, subject to a number of linguistic constraints. For example, dependency length for a sentence such as in Figure 1 is compared to dependency length in a sentence with different word order but the same (unordered) dependency tree structure, as in Figure 3. Attested dependency length is shorter than both the random tree and random word order baselines. Our finding that attested dependency length is shorter than random word order baselines shows that, given a syntactic structure, language users and language grammars tend to prefer the word order that minimizes dependency", "creator": "LaTeX with hyperref package"}}}