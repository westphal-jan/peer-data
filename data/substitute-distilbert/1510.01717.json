{"id": "1510.01717", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "Language Segmentation", "abstract": "language segmentation consists towards finding the boundaries where one process ends. another language begins in a text written in nearer than certain language. isolation is important for all natural language processing tasks. the problem aims be solved by training language models on language data. however, in the case consider narrow - or no - category languages, this is problematic. writers regularly investigate whether unsupervised methods perform better than traditional learning when it is difficult or impossible to train supervised functions. a wider focus is given to difficult texts, gr. e. texts that are potentially short ( often sentence ), containing abbreviations, low - resource languages and non - standard language. i compare separate approaches : supervised n - gram language models, unsupervised clustering and weakly supervised n - gram language model induction. i devised the weakly supervised approach in order to deal with difficult text specifically. in making simultaneously test the approach, i compiled a small corpus of different manuscript types, ranging from one - sentence texts to texts of about 420 words. the weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. the results look promising, but there is room for improvement on a more thorough investigation should be undertaken.", "histories": [["v1", "Tue, 6 Oct 2015 19:35:23 GMT  (584kb)", "http://arxiv.org/abs/1510.01717v1", "Master Thesis"]], "COMMENTS": "Master Thesis", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david alfter"], "accepted": false, "id": "1510.01717"}, "pdf": {"name": "1510.01717.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "U T\nF II C  D H\nLanguage Segmentation\nAuthor: David A\nSupervisors: Prof. Dr. Caroline S\nDr. Sven N\nAugust 18, 2015"}, {"heading": "Erkl\u00e4rung zur Masterarbeit", "text": "Hiermit erkl\u00e4re ich, dass ich die Masterarbeit selbstst\u00e4ndig verfasst und keine anderen als die angegebenenellen und Hilfsmiel benutzt und die aus fremdenellen direkt oder indirekt \u00fcbernommenen Gedanken als solche kenntlich gemacht habe.\nDie Arbeit habe ich bisher keinem anderen Pr\u00fcfungsamt in gleicher oder vergleichbarer Form vorgelegt. Sie wurde bisher nicht ver\u00f6ffentlicht.\nDatum Unterschri\ni\nAbstract\nLanguage segmentation consists in finding the boundaries where one language ends and another language begins in a text wrien in more than one language. is is important for all natural language processing tasks.\ne problem can be solved by training language models on language data. However, in the case of low- or no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform beer than supervised methods when it is difficult or impossible to train supervised approaches.\nA special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language.\nI compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words.\ne weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. e results look promising, but there is room for improvement and a more thorough investigation should be undertaken.\nii\nAnowledgements My thanks go to professor Caroline Sporleder for sharing her knowledge with me, for her inspiring ideas and for agreeing to supervise my Bachelor\u2019s and Master\u2019s esis despite her busy schedule. It was also thanks to the topic she suggested for my Bachelor\u2019s esis that I met J\u00fcrgen Knauth and later was able to get a research assistant position at the SeNeReKo project, collaborating closely with J\u00fcrgen.\nWhich brings me to the next person on the list. I would like to thank J\u00fcrgen Knauth for the wonderful collaboration, for his patience, for his contagious enthusiasm, and all the interesting conversations in passing that always lasted longer than intended.\nI would like to thank Stephan Faber for his insightful comments when I couldn\u2019t see the wood for the trees, for his patience and optimism, for pushing me to go further and to persevere.\nI would also like to thank Julian Vaudroz for accompanying me throughout the degree program. We both didn\u2019t know what we were in for when we started, but we persevered and it paid off. It wouldn\u2019t have been the same without you.\nFinally, I would like to thank all the people that volunteered to proofread my thesis and all the people that helped me during the writing of this thesis. Unfortunately, I cannot list everyone. You know who you are!\niii"}, {"heading": "List of Figures", "text": "1 Out-of-place metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 Simple text illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3 Initial model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Initial model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 5 Model update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 7 New model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 8 Multiple model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 14 9 Updating relevant model . . . . . . . . . . . . . . . . . . . . . . . . . . 14 10 Multiple model evaluation 2 . . . . . . . . . . . . . . . . . . . . . . . . 14 11 New model creation 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 12 Problematic text sample . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 13 Finding the most similar models . . . . . . . . . . . . . . . . . . . . . . 16 14 Merging most similar models . . . . . . . . . . . . . . . . . . . . . . . . 16 15 Word-Model assignment . . . . . . . . . . . . . . . . . . . . . . . . . . 16 16 Clustering preprocessor . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 17 WEKA: Cluster visualization . . . . . . . . . . . . . . . . . . . . . . . . 28 18 ELKI: Cluster visualization . . . . . . . . . . . . . . . . . . . . . . . . . 29 19 Language model: Distribution 1 . . . . . . . . . . . . . . . . . . . . . . 35 20 Language Model: Distribution 2 . . . . . . . . . . . . . . . . . . . . . . 35 21 Language model: Distribution 3 . . . . . . . . . . . . . . . . . . . . . . 36 22 Alternating language structure . . . . . . . . . . . . . . . . . . . . . . . 54\niv"}, {"heading": "List of Tables", "text": "1 Training data: Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2 Unambiguous encoding: distances . . . . . . . . . . . . . . . . . . . . . 27 3 Simplified encoding: distances . . . . . . . . . . . . . . . . . . . . . . . 27 4 N-Gram language model results: Latin script . . . . . . . . . . . . . . . 38 5 N-Gram language model results: Mixed script . . . . . . . . . . . . . . 39 6 N-Gram language model results: Pali data . . . . . . . . . . . . . . . . . 40 7 N-Gram language model results: Twier data . . . . . . . . . . . . . . . 41 8 Textcat results: Latin script . . . . . . . . . . . . . . . . . . . . . . . . . 42 9 Textcat results: Mixed script . . . . . . . . . . . . . . . . . . . . . . . . 43 10 Textcat results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . . . 44 11 Textcat results: Twier data . . . . . . . . . . . . . . . . . . . . . . . . 45 12 Clustering results: Latin script . . . . . . . . . . . . . . . . . . . . . . . 46 13 Clustering results: Mixed script . . . . . . . . . . . . . . . . . . . . . . 47 14 Clustering results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . . 48 15 Clustering results: Twier data . . . . . . . . . . . . . . . . . . . . . . . 49 16 Induction results: Latin script . . . . . . . . . . . . . . . . . . . . . . . . 50 17 Induction results: Mixed script . . . . . . . . . . . . . . . . . . . . . . . 51 18 Induction results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . . 52 19 Induction results: Twier data . . . . . . . . . . . . . . . . . . . . . . . 53 20 \u2018Twier 3\u2019: Textcat versus Gold clustering . . . . . . . . . . . . . . . . 58 21 \u2018Twier 4\u2019: Textcat versus Gold clustering . . . . . . . . . . . . . . . . 58\nv"}, {"heading": "List of Algorithms", "text": "1 N-gram numerical encoding . . . . . . . . . . . . . . . . . . . . . . . . 26 2 Model induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3 Initial model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4 Max model and max score . . . . . . . . . . . . . . . . . . . . . . . . . 32 5 Model merger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6 Distributional Similarity Calculation . . . . . . . . . . . . . . . . . . . . 36\nvi"}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Related work 2", "text": "2.1 N-Grams and rank order statistics . . . . . . . . . . . . . . . . . . . . . 2 2.2 N-Grams and maximum likelihood estimator . . . . . . . . . . . . . . . 3 2.3 Trigrams and short words . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.4 N-Grams and clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.5 Inclusion detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.6 Clustering and speech . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.7 Monolingual training data . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.8 Predictive suffix trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3 eory 9 3.1 Supervised language model . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.1.1 N-Gram models . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.1.2 Formal definition . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1.3 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2 Unsupervised clustering . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.3 Weakly supervised language model induction . . . . . . . . . . . . . . 12"}, {"heading": "4 Experimental setup 18", "text": "4.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.2 Supervised language model . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2.2 Training phase . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2.3 Application of the approach . . . . . . . . . . . . . . . . . . . . 21 4.2.4 Textcat and language segmentation . . . . . . . . . . . . . . . . 21\n4.3 Unsupervised clustering . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.3.1 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.3.2 Defining features . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.3.3 Mapping features to a common scale . . . . . . . . . . . . . . . 25 4.3.4 e problem of unambiguous encoding . . . . . . . . . . . . . . 26 4.3.5 e clusterer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.3.6 Evaluating clusterings . . . . . . . . . . . . . . . . . . . . . . . 29 4.4 Weakly supervised language model induction . . . . . . . . . . . . . . 31 4.4.1 Distributional similarity . . . . . . . . . . . . . . . . . . . . . . 34 4.4.2 Evaluating results . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.4.3 Estimating the parameters . . . . . . . . . . . . . . . . . . . . . 37\nvii"}, {"heading": "5 Results 38", "text": "5.1 N-Gram language model . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.2 Textcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.3 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.4 Language model induction . . . . . . . . . . . . . . . . . . . . . . . . . 50"}, {"heading": "6 Discussion 54", "text": "6.1 N-Gram language models . . . . . . . . . . . . . . . . . . . . . . . . . . 54 6.2 Textcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 6.3 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 6.4 Language model induction . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.5 Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64"}, {"heading": "7 Conclusion 65", "text": ""}, {"heading": "8 Appendix 72", "text": "8.1 Development data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n8.1.1 Latin script data . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.1.2 Mixed script data . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.1.3 Twier data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 8.1.4 Pali dictionary data . . . . . . . . . . . . . . . . . . . . . . . . . 73\n8.2 Test data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 8.2.1 Latin script data . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 8.2.2 Mixed script data . . . . . . . . . . . . . . . . . . . . . . . . . . 75 8.2.3 Twier data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 8.2.4 Pali dictionary data . . . . . . . . . . . . . . . . . . . . . . . . . 78 8.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 8.3.1 N-Gram Language Models . . . . . . . . . . . . . . . . . . . . . 80 8.3.2 Textcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 8.3.3 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 8.3.4 Language Model Induction . . . . . . . . . . . . . . . . . . . . . 112\nviii"}, {"heading": "1 Introduction", "text": "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014). Indeed, using \u201ctraditional\u201dmonolingual natural language processing components on mixed language data leads to miserable results (Jain and Bhat, 2014). Even if the results are not terrible, language identification and segmentation can improve the overall results. For example, by identifying foreign language inclusions in an otherwise monolingual text, parser accuracy can be increased (Alex et al., 2007).\nOne important point that has to be borne in mind is the difference between language identification and language segmentation. Language identification is concerned with recognizing the language at hand. It is possible to use language identification for language segmentation. Indeed, by identifying the languages in a text, the segmentation is implicitly obtained. Language segmentation on the other hand is only concerned with identifying language boundaries. No claims about the languages involved are made.\nAer giving an overview over related work and different approaches that can be taken for language segmentation, I will present the theory behind supervised methods as well as unsupervised methods. Finally, I will introduce a weakly supervised method for language segmentation that I developed.\nAer the theoretical part, I will present experiments done with the different approaches, comparing their effectiveness on the task of language segmentation on different text types. A special focus will be given to difficult text types, such as short texts, texts containing under-resourced languages or texts containing a lot of abbreviations or other non-standard features.\nA big advantage of unsupervised methods is language independence. If the approach used does not rely on language-specific details, the approach is more flexible as no language resources have to be adapted for the method to work on other languages. ese advantages might be especially useful for under-resourced languages. When there is no or insufficient data available to train a supervised language model, an unsupervised approach might yield beer results.\nAnother advantage is that unsupervised methods do not require prior training. ey are not dependent on training data and thus cannot be skewed by the data. Indeed, supervised approaches that are trained on data are qualitatively tied to their training data; different training data will, in all probability, yield different models.\nis thesis aims at answering the question whether unsupervised language segmentation approaches work beer on difficult text types than supervised language approaches.\n1"}, {"heading": "2 Related work", "text": ""}, {"heading": "2.1 N-Grams and rank order statistics", "text": "Cavnar and Trenkle (1994) use an n-gram language model for language identification purposes. eir program \u2018Textcat\u2019 is intended to classify documents by language. e system calculates n-grams for 1 6 n 6 5 from training data and orders the n-grams according to inverse frequency, i.e. from the most frequent n-grams to the most infrequent n-grams. e numerical frequency data is then discarded and only inherently present.\nDuring training, the program calculates an n-gram profile consisting of these ngram lists for each category (i.e. language to classify).\nNew data is classified by first calculating the n-gram profile and then comparing the profile to existing profiles. e category with the lowest difference score is taken as the category for the document.\ne score they use for classification is called out-of-place metric. For each n-gram in the document n-gram profile, the corresponding n-gram in the category profile is looked up and the absolute difference of ranks is taken as score. e sum is calculated over all n-grams. More formally, the out-of-place metricmoop is calculated as:\nmoop = n\u2211\ni=1\n(|r(xi, d)\u2212 r(xi, c)|) (1)\nWith n the number of n-grams in the document profile, xi the i-th n-gram, r(xi, d) the rank of the i-th n-gram in the document profile, r(xi, c) the rank of the i-th n-gram in the category profile.\nFigure 1 illustrates the out-of-place metric.\nCategory profile Document profile\nScore\n2\nIn figure 1, the document profile has \u2018ER\u2019 as most frequent n-gram, at rank 1, followed by \u2018ING\u2019 at rank 2, etc. e category profile does not contain the n-gram \u2018ER\u2019; in that case, an arbitrary fixed maximum value is assigned. e category profile contains the n-gram \u2018ING\u2019 at rank 2, the same rank as in the document profile; the difference is 0. e category profile contains the n-gram \u2018AT\u2019 at rank 1, while in the document profile, it occurs at rank 3. e absolute difference is 2. e out-of-place metric consists of the sum of all scores thus calculated.\nCavnar and Trenkle (1994) collected 3713 Usenet texts with a cultural theme in different languages. ey filtered out non-monolingual texts and texts that had no useful content for language classification. In the end, they had 3478 articles ranging from a single line of text to 50 KB of text.\neir results indicated that length had no significant impact on the classification, contrary to what they thought. Also, they found that training the system with 400 n-grams yielded the best result with a precision of 99.8%.\ney also showed that their approach could be used for subject classification of texts in the same language with reasonable precision. is finding indicates that language and domain are linked to a certain degree."}, {"heading": "2.2 N-Grams and maximum likelihood estimator", "text": "Dunning (1994) also uses an n-gram language model for language identification purposes. e program calculates n-grams and their frequencies from the training data and estimates the probability P of a given string using the Maximum Likelihood Estimator (MLE) with Laplace add-one smoothing.More formally:\nP (wi|w1, . . . , wi\u22121) = C(w1, . . . , wi) + 1\nC(w1, . . . , wi\u22121) + |V | (2)\nwith C(w1, . . . , Ci) the number of times the n-gram w1, . . . , wi occurred, C(w1, . . . , Ci\u22121) the number of times the (n\u2212 1)-gram w1, . . . , wi\u22121 occurred and |V | the size of the vocabulary.\nFor a string S, the string is decomposed into n-grams and the log probability lk is calculated as:\nlk = \u2211\nw1,...,wk\u2208S\nC(w1, . . . , wk) logP (wk|w1, . . . , wk\u22121) (3)\nwhere k is the order of the n-gram (k = n) used. In order to test the system, Dunning (1994) uses a specially constructed test corpus from a bilingual parallel translated English-Spanish corpus containing English and Spanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and 100 texts varying from 10 to 500 bytes for the test set.\n3\ne results indicate that bigram models perform beer for shorter strings and less training data while trigram models work beer for larger strings and more training data.\nDunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words. e system implemented by Dunning (1994) can classify strings of 10 characters in length \u201cmoderately well\u201d, while strings of 50 characters or more are classified \u201cvery well\u201d. Accuracies given vary from 92% for 20 bytes of training data to 99.9% for 500 bytes of text."}, {"heading": "2.3 Trigrams and short words", "text": "Grefenstee (1995) compares trigrams versus short words for language identification. Short words are oen function words that are typical for and highly frequent in a given language.\ne trigram language guesser was trained on one million characters of text in 10 languages: Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese, Spanish and Swedish. From the same texts, all words with 5 or less characters were counted for the short-word-strategy.\ne results indicate that the trigram approach works beer for small text fragments of up to 15 words, while for any text longer than 15 words, both methods work equally well with reported accuracies of up to 100% in the 11-15 word range."}, {"heading": "2.4 N-Grams and clustering", "text": "Gao et al. (2001) present a system that augments n-gram language models with clustering techniques. ey cluster words by similarity and use these clusters in order to overcome the data sparsity problem.\nIn traditional cluster-based n-gram models, the probability P (wi) is defined as the product of the probability of a word given a cluster ci and the probability of the cluster ci given the preceding clusters. For a trigram model, the probability P (wi) of a word wi is calculated as\nP (wi|wi\u22122wi\u22121) = P (wi|ci)\u00d7 P (ci|ci\u22122ci\u22121) (4)\ne probability of a word given a cluster is calculated as\nP (wi|ci) = C(wi)\nC(ci) (5)\nwith C(wi) the count of the word wi and C(ci) the count of the cluster ci.\n4\ne probability of a cluster given the preceding clusters is calculated using the Maximum Likelihood Estimator\nP (ci|ci\u22122ci\u22121) = C(ci\u22122ci\u22121ci)\nC(ci\u22122ci\u22121) (6)\nGao et al. (2001) derive from this three ways of using clusters to augment language models: predictive clustering (7), conditional clustering (8) and combined clustering (9).\nP (wi|wi\u22122wi\u22121) = P (ci|wi\u22122wi\u22121)\u00d7 P (wi|wi\u22122wi\u22121ci) (7)\nP (wi|wi\u22122wi\u22121) = P (wi|ci\u22122ci\u22121) (8)\nP (wi|wi\u22122wi\u22121) = P (ci|ci\u22122ci\u22121)\u00d7 P (wi|ci\u22122ci\u22121ci) (9)\nSimilarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models. In addition to Gao et al. (2001), they also use information about the subject-verb and verb-object relations of the sentence.\ney show that their model, using clustering, subject-verb information, verb-object information, and the Porter stemmer outperforms a traditional trigram model.\nCarter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)). Carter (1994) shows that the subdivision into smaller clusters increases the accuracy of bigram language models, but not trigram models."}, {"heading": "2.5 Inclusion detection", "text": "Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts. For the language pair German-English, inclusions are detected using a German and an English lexicon as first resource. If a word is found only in the English lexicon, it is tagged as unambiguously English. If the word is found in neither lexicon, a web search is conducted, restricting the search options to either German or English and counting the number of results. If the German search yields more results, the word is tagged as German, otherwise as English inclusion. If a word is found in both lexicons, a postprocessing module resolves the ambiguity.\nAlex is mainly concerned with the improvement of parsing results by inclusion detection. For example in (Alex et al., 2007) they report an increase in F-Score of 4.3\n5\nby using inclusion detection when parsing a German text with a parser trained on the TIGER corpus (Brants et al., 2002)."}, {"heading": "2.6 Clustering and spee", "text": "In the area of clustering and spoken language identification, Yin et al. (2007) present a hierarchical clusterer for spoken language. ey cluster 10 languages1 using prosodic features and Mel Frequency Cepstral Coefficients (MFCC). MFCC vectors are a way of representing acoustic signals (Logan et al., 2000). e signal is first divided into smaller \u2018frames\u2019, each frame is passed through the discrete Fourier transform and only the logarithm of the amplitude spectrum is retained (Logan et al., 2000). e spectrum is then projected onto the \u2018Mel frequency scale\u2019, a scale that maps actual pitch to perceived pitch, \u201cas apparently the human auditory system does not perceive pitch in a linear manner\u201d (Logan et al., 2000). Finally, a discrete cosine transform is applied to the spectrum to get the MFCC representations of the original signal (Logan et al., 2000).\nYin et al. (2007) show that their hierarchical clusterer outperforms traditionalAcoustic Gaussian Mixture Model systems.\nAs spoken language will not be further investigated in this thesis, I will not dive deeper into the maer at this point."}, {"heading": "2.7 Monolingual training data", "text": "Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text.\nYamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.\nKing and Abney (2013) use weakly supervised methods to label the languages of words. ey consider the task as sequence labeling task. ey have limited themselves to bilingual documents with a single language boundary and the task consists\n1e authors do not explicitly list the languages clustered, except for two-leer abbreviations which seem to correspond to ISO 639-1. e languages under investigation could have been Vietnamese, German, Farsi, French, Japanese, Spanish, Korean, English, Tamil, and \u2018ma\u2019, though it is impossible to tell.\n2http://www.un.org/en/documents/udhr/\n6\nin discriminating between English and non-English text. ey found that a Conditional Random Field model augmented with Generalized Expectation criteria worked best, yielding accuracies of 88% with as lile as 10 words used for training.\nLui et al. (2014) consider the task as multi-label classification task. ey represent a document as an n-gram distribution of byte sequences in a bag-of-words manner. ey report F-scores of 0.957 and 0.959. ey note that similar languages will pose problems when trying to identify a language, and solve this problem by identifying a set of languages that most probably are correct instead of a single language.\nOne problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014)."}, {"heading": "2.8 Predictive suffix trees", "text": "Seldin et al. (2001) propose a system for automatic unsupervised language segmentation and protein sequence segmentation. eir system uses Variable Memory Markov (VMM) sources, an alternative to Hidden Markov Models (HMM) implemented as Predictive Suffix Trees (PST).\nWhereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to \u201csolve many applications with notable success\u201d (Begleiter et al., 2004). In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004). us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004).\nere is no single VMM algorithm, but rather a family of related algorithms. One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996). A PST is a tree over an alphabet \u03a3, with each node either having 0 (leaf nodes) or |\u03a3| children (non-terminal nodes) (Ron et al., 1996). Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996). Each edge is labeled by a symbol s \u2208 \u03a3 and the probability for the next symbol being s (Ron et al., 1996).\nBy modifying the Predictive Suffix Tree (PST) algorithm using the Minimum Description Length (MDL) principle, Seldin et al. (2001) end up with a non-parametric self-regulating algorithm. e MDL principle avoids overfi\ue03cing of the model by favoring low complexity over goodness-of-fit (Gr\u00fcnwald, 2007).\ney embed the algorithm in a deterministic annealing (DA) procedure to refine the results. Finally, they use the Blahut-Arimoto algorithm, a rate-distortion function, until convergence of the system.\nFor the language segmentation task, they use 150000 leers of text, 30000 from each of the following languages: English, German, French, Italian, transliterated Russian. ey used continuous language fragments of approximately 100 leers, yielding a\n7\nsynthetic multilingual text that switches language approximately every two sentences. One important point that they note is that \u201ctoo short segments do not enable reliable discrimination between different models\u201d. erefore, they disallow switching models aer every word.\ney report very good results on the language segmentation task (and on the protein segmentation task). Aer 2000-3000 iterations of the Blahut-Arimoto algorithm, the correct number of languages is identified and the segmentation is accurate up to a few leers.\n8\n3 eory"}, {"heading": "3.1 Supervised language model", "text": ""}, {"heading": "3.1.1 N-Gram models", "text": "Among supervised languagemodels, n-grammodels are very popular (Gao et al., 2001). An n-gram is a slice from the original string (Cavnar and Trenkle, 1994). ese slices can be contiguous or not. Non-contiguous n-grams are also called skip-grams (Guthrie et al., 2006). In skip-grams, an additional parameter k indicates the maximum distance that is allowed between units. In this parlance, contiguous n-grams can be regarded as 0-skip-n-grams (Guthrie et al., 2006).\ne following example demonstrates the difference between (traditional) n-grams and skip-grams. Given the following sentence:\nTh i s i s a sample s en t en c e . We can construct, for example, the following word k-skip-n-grams:\n(0-skip-)2-grams: is is, is a, a sample, sample sentence 2-skip-2-grams: is is, is a, is sample, is a, is sample, is sentence, a sample, a sentence, sample sentence (0-skip-)3-grams:is is a, is a sample, a sample sentence 2-skip-3-grams:is is a, is is sample, is is sentence, is a sample, is a sentence, is sample sentence, is a sample, is a sentence, is sample sentence, a sample sentence\ne results for 2-skip-2-grams does not include the skip-gram \u201cis sentence\u201d, as the distance in words between these two words is 3, higher than the allowed k of 2. As can be seen from this example, the number of skip-grams ismore than two times higher than the number of contiguous n-grams, and this trend continues the more skips are allowed (Guthrie et al., 2006). Skip-grams, unlike n-grams, do not incur the problem of data sparseness with an increase of n.\nInstead of using words as unit for n-gram decompositions, we can also choose characters. Each word is then decomposed into sequences of n characters. For example, the word\nmodel can be decomposed into the 2-grams: mo, de, el. Oen, the word to decompose is padded with start and end tags in order to improve the model (Cavnar and Trenkle, 1994). If we pad the word with <w> and </w>, the 2-gram decomposition yields: <w>m, mo, de, el, l </w>. e use of paddings allows themodel to capture details about character distribution with regard to the start and end of words (Cavnar and Trenkle, 1994). For example, in English the leer \u2018y\u2019 occurs more oen at the end of words than\n9\nat the beginning of words, while the leer \u2018w\u2019 occurs mainly at the beginning of words (Taylor, 2015). A non-padding model cannot capture this distinction, while a padding model can.\nOne advantage of n-gram models is that the decomposition of a string into smaller units reduces the impact of typing errors (Cavnar and Trenkle, 1994). Indeed, a typing error only affects a limited number of units (Cavnar and Trenkle, 1994). Due to this property, n-gram models have been shown to be able to deal well with noisy text (Cavnar and Trenkle, 1994)."}, {"heading": "3.1.2 Formal definition", "text": "Traditional n-gram languagemodels predict the next wordwi given the previouswords w1, . . . , wi\u22121. is prediction uses the conditional probability P (wi|w1, . . . , wi\u22121). Instead of using the entire historyw1, . . . , wi\u22121, the probability is approximated by using only the n previous words wi\u2212n+1, . . . , wi\u22121.\nP (wi|w1, . . . , wi\u22121) = P (wi|wi\u2212n+1, . . . , wi\u22121) (10)\ne probability can be estimated using theMaximum Likelihood Estimation (MLE):\nP (wi|wi\u2212n+1, . . . , wi\u22121) = C(wi\u2212n+1, . . . , wi)\nC(wi\u2212n+1, . . . , wi\u22121) (11)\nWhere C(wi\u2212n+1, . . . , wi) represents the number of times the n-gram sequence wi\u2212n+1, . . . , wi occurred in the training corpus andC(wi\u2212n+1, . . . , wi\u22121) represents the number of times the (n\u2212 1)-gram sequence wi\u2212n+1, . . . , wi\u22121 was seen in the training corpus."}, {"heading": "3.1.3 Smoothing", "text": "e problem with MLE is that sequences not seen during training will have a probability of zero. In order to avoid this problem, different smoothing techniques can be used (Chen and Goodman, 1996). e simplest smoothing technique is additive (Laplace) smoothing (Chen and Goodman, 1996). Let V be the vocabulary size (i.e. the total number of unique words in the test corpus). e smoothed probability PLaplace becomes:\nPLaplace(wi|wi\u2212n+1, . . . , wi\u22121) = C(wi\u2212n+1, . . . , wi) + \u03bb\nC(wi\u2212n+1, . . . , wi\u22121) + \u03bbV (12)\nWith \u03bb the smoothing factor. If we choose \u03bb = 1, we speak of \u201cadd one\u201d smoothing (Jurafsky and Martin, 2000). In practice, \u03bb < 1 is oen chosen (Manning and Sch\u00fctze, 1999).\n10\nAn important estimation is theGood-Turing estimation (Chen andGoodman, 1996). While not directly a smoothing method, it estimates the frequency of a given observation with\nc\u2217 = (c+ 1) Nc+1 Nc\n(13)\nwhere c is the number of times the observationwasmade,Nc is the number of times the frequency c was observed and Nc+1 the frequency of the frequency c + 1. us, instead of using the actual count c, the count is taken to be c\u2217 (Chen and Goodman, 1996).\nAnother way to avoid assigning probabilities of zero to unseen sequences is by using back-off models. ere are linear and non-linear back-off models. In non-linear back-offmodels, if the original n-gram probability falls below a certain threshold value, the probability is estimated by the next lowest n-gram model. Katz\u2019s back-off model (Katz, 1987) for instance calculates probability Pbo using the formula:\nPbo =\n{ dwi\u2212n+1,...,wi C(wi\u2212n+1,...,wi) C(wi\u2212n+1,...,wi\u22121)\nif C(wi\u2212n+1, . . . , wi) > k \u03b1wi\u2212n+1,...,wi\u22121Pbo(wi|wi\u2212n+2, . . . , wi\u22121) otherwise\n(14)\nWith d and \u03b1 as smoothing parameters. e parameter k is oen chosen k = 0. is means that if the probability given a high-order n-gram model is zero, we back off to the next lowest model. For tri-gram models, the formula becomes:\nPbo(wi|wi\u22122, wi\u22121) =  P (wi|wi\u22122, wi\u22121) if C(wi\u22122, wi\u22121) > 0 \u03b11P (wi|wi\u22121) if C(wi\u22122, wi\u22121) = 0 and C(wi\u22121, wi) > 0 \u03b12P (wi) otherwise\n(15) In contrast, linear back-offmodels use an interpolated probability estimate by combiningmultiple probability estimates andweighting each estimate. e probabilityPLI for a tri-gram model is:\nPLI(wi|wi\u22122, wi\u22121) = \u03bb3P (wi|wi\u22122, wi\u22121) + \u03bb2P (wi|wi\u22121) + \u03bb1P (wi) (16) with \u2211 \u03bbi = 1"}, {"heading": "3.2 Unsupervised clustering", "text": "Clustering consists in the grouping of objects based on their mutual similarity (Biemann, 2006). Objects to be clustered are typically represented as feature vectors (Biemann, 2006); from the original objects, a feature representation is calculated and used for further processing.\n11\nClustering can be partitional or hierarchical (Yin et al., 2007). Partitional clustering divides the initial objects into separate groups in one step, whereas hierarchical clustering builds a hierarchy of objects by first grouping the most similar objects together and then clustering the next level hierarchy with regard to the existing clusters (Yin et al., 2007).\ne clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006). e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999). ere are different metrics available. A frequently chosen metric is the cosine similarity that calculates the distance between two vectors, i.e. the angle between them (Biemann, 2006).\nIn order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999). Features can be quantitative (e.g. word length) or qualitative (e.g. word starts with a capital leer) (Jain et al., 1999).\nMost clustering algorithms, e.g. k-means, need the number of clusters to generate (Jain et al., 1999). e question how to best choose this key number has been addressed in-depth by Dubes (1987).\nClustering can be so or hard. When hard-clustering, an object can belong to one class only, while in so-clustering, an object can belong to one or more classes, sometimes with different probabilities (Jain et al., 1999)."}, {"heading": "3.3 Weakly supervised language model induction", "text": "e main idea behind language model induction is that by inducing language models from the text itself, the models are highly specialized but the approach is generally more flexible since genre or text specific issues do not arise.\nis approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set. However, the realization differs greatly. Whereas Seldin et al. (2001) use predictive suffix trees, I use n-gram language models.\ne intuition is to learn the language models from the text itself, in an iterative manner. Suppose we have a document as follows where wi represents the word at position i in the text. Suppose the text contains two languages, marked in red and blue.\nw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 \u2026\nWe then evaluate the second word using the first language model. If the language model score is high enough, we update the language model with the second word.\nIf the score is below a certain threshold, the existing languagemodel does notmodel the word well enough and a new model is created.\n13\nWhen there is more than one language model, each word is evaluated by every language model, and the highest scoring model is updated, or a new model is created if no language model models the word well enough.\n14\ne last example shows that it is not necessarily the case that exactly one language model is created per language; it oen is the case that many language models are created for one language.\nAt the beginning, the models are not very reliable, as they only have a few words as basis, but the more text is analyzed, the more reliable the models become.\nHowever, the approach is problematic in that the text structure itself influences the language models created. If the text starts with a foreign language inclusion, as illustrated in figure 12, the initial model might be too frail to recognize the following words as being a different language, updating the first model with the second and third word and so on. us, the approach would fail at recognizing the foreign language inclusion.\nw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 \u2026\nFigure 12: Problematic text sample\nIf we were to start from the end of the text and work towards the beginning, the probability of having a relatively robust language model for the \u2018blue\u2019 language would be high, and so, it would theoretically be easier to recognize the first word as not being \u2018blue\u2019.\nerefore, one induction step involves one forward generation and one backwards generation. is yields two sets, the set of models from the forward generation F = {f1, f2, . . . , fn} and the set from the backwards generation B = {b1, b2, . . . , bm}. en, from the two sets of models, the most similar models are selected. For this, every model from F is compared to every model fromB, as figure 13 shows. e most similar models are then merged, as illustrated in figure 14. Indeed, if both the forward and backwards generation yielded a similar language model, it is probable that the model is correct.\nEven so, both forward and backwards generation can not guarantee ideal results, there is the option to run the generation from a random position. is random induction picks a random position in the text and runs one induction step from that position, meaning one forward and one backwards generation. Finally, the most similar models are merged as for the general generation.\n15\nis only yields one probable language model, therefore the induction is repeated with the difference that all probable models are taken into consideration as well. For each word, if a probable model models the word well enough, no newmodel is created, otherwise a new model is created.\nAt the end of the induction loop, the set of probable models P is examined. As long as there are two models that have a similarity score below a certain threshold, the two most similar models are merged.\nFinally, aer the language models have been induced, another pass is made over the text and each word is assigned to the language model which yields the highest score for that word, resulting in a word-to-model assignment as illustrated in figure 15.\nI have made the approach parametric with parameters being:\n\u2022 Induction iterations: Number of induction iterations\n\u2022 Random iterations: Number of random iterations\n16\n\u2022 Forward/Backwards threshold: reshold for forward/backwards merging\n\u2022 Silver threshold: reshold for P model merging\nese parameters can be adapted, in the hope that some parameter configurations will work beer on certain data sets than other configurations. Since the approach has parameters that have to be learned from a development set, the approach is said to be weakly supervised; the development set is not used to train any language specifics, only for the estimation of the parameters of the approach.\n17"}, {"heading": "4 Experimental setup", "text": "In this chapter I present experiments done using the approaches delineated in the previous section in order to find out whether there are approaches that work beer on certain types of text.\ne central hypothesis is that unsupervised language segmentation approaches are more successful on difficult data. Difficult data is data for which there is not enough data to train a language model or data which contains a lot of non-standard language such as abbreviations.\nFirst, I present the data used to test the language segmentation systems and elaborate on the different aspects that had to be considered for the data compilation.\nI then present two supervised language segmentation experiments using n-gram language models and Textcat.\nFor unsupervised language segmentation, I will first present experiments using clustering algorithms before presenting experiments using language model induction."}, {"heading": "4.1 Data", "text": "In order to test the different language segmentation approaches, I compiled different sets of test data. As I want to focus on short texts, most texts from the test corpus are rather small, sometimes consisting of only one sentence. However, in order to test the general applicability of the approach, the test corpus also contains larger text samples.\ne test corpus can be subdivided into different sub-corpora:\n\u2022 Latin-based: Texts consisting of languages using Latin-based scripts, such as German, English, Finnish or Italian\n\u2022 Mixed script: Texts consisting of languages using Latin-based scripts and languages using non-Latin-based scripts\n\u2022 Twier data: Short texts taken from Twier\n\u2022 Pali dictionary data: Unstructured texts containing many different language inclusions such as Vedic Sanskrit, Sanskrit, Indogermanic reconstructions, Old Bulgarian, Lithuanian, Greek, Latin, Old Irish, many abbreviations and references to text passages\nAs every outcome has to bemanually checked, the test corpus is rather small. Every category consists of five texts. Each texts consists of two or three languages with the exception of the Pali dictionary data that oen contains inclusions frommany different languages in the etymological explanations.\nFor each text, I also created a gold standard version with the expected clusters. In some cases it is not clear how to cluster certain objects. In that case, I use a clustering\n18\nthat makes sense to me, but this need not mean that it is the correct or only possible clustering.\nFor the parameter estimation of the language model induction approach, I also compiled a set of development data. All texts can be found in the appendix under 8.1 and 8.2."}, {"heading": "4.2 Supervised language model", "text": ""}, {"heading": "4.2.1 Implementation", "text": "For the supervised language segmentation method, I implemented an n-gram language model as described by Dunning (1994). e n-gram language model is implemented as a character trigram model with non-linear back-off to bigram and unigram models. e conditional probability P is calculated using the formula:\nP (wi|wi\u22122, wi\u22121) =  \u03b11 C(wi\u22122,wi\u22121,wi) C(wi\u22122,wi\u22121) if C(wi\u22122, wi\u22121, wi) > 0 \u03b12 C(wi\u22121,wi) C(wi\u22121) if C(wi\u22121, wi) > 0 \u03b13 C(wi) V\nif C(wi) > 0 \u03b14 1 V+W+X otherwise\n(17)\nwith \u03b11 = 0.7, \u03b12 = 0.2, \u03b13 = 0.09, \u03b14 = 0.01, V the number of unigrams,W the number of bigrams and X the number of trigrams.\nEach word is padded by two different start symbols and two different end symbols. e joint probability for a word w of length n is calculated as\nP (w) = 1\u2211n\ni=2 | logP (wi|wi\u22122, wi\u22121)| (18)\nIn the denominator, I use the log probability instead of the probability to increase numerical stability. Indeed, multiplying very small numbers can lead to the result being approximated as zero by the computer when the numbers become too small to be represented as normalized number (Goldberg, 1991). Using the sum of logarithms avoids this problem and is less computationally expensive (B\u00fcrgisser et al., 1997).\nAs the logarithm of a number approaching zero tends to infinity, rare observations get a higher score than frequent observations. As such, the denominator can be seen as a scale of rarity, with a higher score corresponding to a rarer word. By taking the inverse of this scale, we get a score corresponding to the \u201ccommonness\u201d (\u2248 frequency) of a word."}, {"heading": "4.2.2 Training phase", "text": "First, models are trained on training data in the relevant languages. I have not included the languages from the Pali dictionary data, as there are too many different languages\n19\nand there are typically only small inclusions of different languages in a dictionary entry; as such, it would not have made sense to train a language model just to recognize a single word. Another reason for not using the Pali dictionary data languages is that sometimes it is not possible to find data for a language, e.g. Old Bulgarian or reconstructed Indogermanic. In some cases, it would have been conceivable to train models on similar languages, but again, the effort of training a model is disproportionately high compared to the (uncertain) result of recognizing a single inclusion. Instead, an additional catch-all languagemodel is used to capture words that do not seem to belong to a trained model.\ne training data consists ofWikipedia dumps from the months June and July 2015; a dump is a copy of the whole encyclopedia for a given language. Due to the difference in size of theWikipedia of the different languages, I choose the full dump for languages with less than 3 GB of compressed data and limited the amount of data to maximally 3 GB of compressed data.\ne Wikipedia data was processed using the Wikipedia Extractor3 version 2.8 in order to extract the textual content from the article pages. Indeed, theWikipedia pages are wrien using the MediaWiki Markup Language4. While this markup is useful for meta-data annotation and cross-referencing, the encoded information is superfluous for language model training and has to be removed before training a model on the data. Table 1 shows the size of the training data per language aer text extraction.\n3http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 4https://www.mediawiki.org/wiki/Help:Formatting\n20\nAs the test data only contains transliterated Amharic text, theWikipedia data, written in the Ge\u2019ez script, had to be transliterated. e text was transliterated according to the EAE transliteration scheme by the Encyclopaedia Aethiopica.\nAs the test data contains transliterated Greek, the Greek data was used once as-is and once transliterated according to the ELOT (Hellenic Organization for Standardization) transliteration scheme for Modern monotonic Greek.\nIt should be borne inmind that the training data influences the quality and accuracy of the model. Furthermore, a model might work well on certain text types and less well on other text types. It is not possible to train a perfect, universal model."}, {"heading": "4.2.3 Application of the approa", "text": "In the second step, an input text is segmented into words. en, each word is evaluated by each language model and the model with the highest score is assigned as the word\u2019s language model.\ne approach taken consists in classifying words as either belonging to a trained language model or to the additional, catch-all model other, which simply means that the word could not be assigned to a trained model class."}, {"heading": "4.2.4 Textcat and language segmentation", "text": "I also tested how well Textcat is suited to the task of language segmentation. e approach is similar to the n-gram approach, with the exception that I do not train any models and rely on Textcat\u2019s classifier for language prediction.\nIn the first step, an input text is segmented into words. en, each word is passed to Textcat and the guess made by Textcat is taken as the word\u2019s language."}, {"heading": "4.3 Unsupervised clustering", "text": "In order to test the efficiency of clustering algorithms on the task of language segmentation, I looked at various algorithms readily available throughWEKA, \u201ca collection of machine learning algorithms for data mining tasks\u201d by the University of Waikato in New Zealand (Hall et al., 2009) and the Environment for Developing KDD-Applications Supported by Index-Structures (ELKI), \u201can open source data mining soware [\u2026] with an emphasis on unsupervised methods in cluster analysis and outlier detection\u201d by the Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen (Achtert et al., 2013). I also looked at JavaML, \u201ca collection of machine learning and data mining algorithms\u201d (Abeel et al., 2009), in order to integrate clusterers into my own code framework. JavaML offers different clustering algorithms and also offers access toWEKA\u2019s clustering algorithms. In contrast to WEKA and ELKI, which can be used in stand-alone mode, JavaML is meant\n21\nto be integrated into bigger programs and provides an application programming interface (API) that allows the provided algorithms to be accessed in a programmatic way, i.e. from inside a program."}, {"heading": "4.3.1 Preprocessing", "text": "However, in order for the clustering algorithms to work, the document to segment has to be preprocessed in a number of ways, as shown in figure 16.\nFirst of all, the document has to be read in by the program. is step is straightforward.\n22\ne document then has to be tokenized. Tokenization is not trivial and depends on the definition of a \u2018word\u2019. For this task I have used a whitespace tokenizer that defines a word as a continuous sequence of character literals separated by one or more whitespace characters. While it can be objected that for scripts that don\u2019t use whitespace to separate words, such as Chinese, tokenization fails, this is not too big a concern. Indeed, if a continuous block of Chinese characters is treated as one word, it is likely to be clustered separately due to the different in \u201dword\u201d length and the different character set. If, however, a document contains two scripts that do not separate words by whitespace, the approach totally fails. It is beyond the scope of this thesis, and possibly of any thesis, to implement a universal tokenizer that works regardless of language without prior knowledge about the languages at hand.\nEach token is then normalized. Normalization of a non-Latin-based input (e.g. Arabic or Cyrillic script) returns the input without modification. Otherwise, the following modifications are made, if applicable:\n\u2022 remove leading and trailing whitespace\n\u2022 remove punctuation\n\u2022 remove control characters\nControl characters are defined as the set ( [ ] ) \\\nPunctuation is defined as the set . , \u201d \u2019 : ; ! ? \u2212\ne token is then stripped of XML-like tags, if applicable. e following example illustrates this step. Let us assume we have the following token:\n<word i d =\u201d1 \u201d lemma=\u201d go \u201d> goes </word> e token is replaced by the text content of the node, thus the resulting token is \u2018goes\u2019.\nIf, aer all these modifications, the token corresponds to the empty string, we continue with the next token. Otherwise, the token is passed on to the feature extraction module. e algorithm terminates when all tokens have been consumed."}, {"heading": "4.3.2 Defining features", "text": "e final step consists in defining features by which to cluster and implementing feature extractors that build the feature vectors from the input. Since the features are to be language independent, using features such as \u2018occurs in an English lexicon\u2019 cannot be used. e following features were devised:\n23\n1. word length: the length of the word in characters\n2. X tail bigrams: bigrams calculated from the end of the word\n3. Y tail trigrams: trigrams calculated from the end of the word\n4. X first bigrams: bigrams calculcated from the beginning of the word\n5. Y first trigrams: trigrams calculated from the beginning of the word\n6. latin basic: is the word latin basic?\n7. latin extended: is the word latin extended?\n8. capitalized: is the word capitalized?\n9. contains non-word: does the word contain a non-word?\n10. is non-word: is the word a non-word?\n11. number of latin leers: number of latin leers\n12. number of non-latin leers: number of non-latin leers\n13. vowel ratio: number of vowels divided by the word length\n14. basic latin leer ratio: number of latin leers divided by the word length\n15. max consonant cluster: the longest consonant cluster size in characters\n16. is digit: is the word a digit?\n17. is ideographic: is the word ideographic?\n18. directionality: what directionality does first character of the word have?\n19. is BMP codepoint: does the word contain non-BMP characters?\n20. general type: what is the general type of the first character of the word?\ne last two features are based on the Java Character class. is class provides methods to check for specific implementation-based properties of characters.\nWhile most features are rather self-explanatory, a few require further explanation. For the n-grams, the number of n-grams is restricted so as to keep the resulting vectors the same size. is is important because the clustering algorithm considers one data column as one feature, and having vectors of different length would disrupt this precondition. Implementing the comparison of vectors of different lengths, or rather\n24\nor vectors containing vectors as features would have been possible, but rather timeconsuming. If a word is too short to generate the required number of n-grams, only the possible n-grams are generated and all other positions filled with 0.\ne \u2018latin\u2019 features check whether the word consists only of the basic latin leers A-Z and a-z (\u2018basic\u2019) while the \u2018extended\u2019 feature also covers leers derived from the latin leers (e.g. \u00eb, \u00e7, \u1e43, \u00f1).\nNon-words are defined as anything not consisting of leers, such as punctuation marks or digits.\nDirectionality indicates which direction a character should be wrien. While the actual list is much more exhaustive, this property basically indicates whether the character is wrien from le to right or from right to le. 5\nBMP stands for Basic Multilingual Plane and refers to an encoding unit known as plane, which consists of 216 = 65536 codepoints (i.e. encoding slots for characters) (e Unicode Consortium, 2014). e BMP is the first plane, covering the codepoints U+0000 to U+FFFF (e Unicode Consortium, 2014). While it is not important to understand the technical details fully, it is interesting to note that most characters are covered by the BMP, including Chinese, Japanese and Korean characters (e Unicode Consortium, 2014). e next plane, called Supplementary Multilingual Plane or Plane 1 contains historic scripts such as Egyptian hieroglyphs and cuneiform scripts, but also musical notation, game symbols and various other scripts and symbols (e Unicode Consortium, 2014). ere are 17 planes in total (e Unicode Consortium, 2014).\ne last feature in the list, General Type is also an implementation-related property. Type can be, for example5, END_PUNCTUATION, LETTER_NUMBER or MATH_SYMBOL. ese constants are represented as numbers internally, which are taken as feature for the clustering algorithm."}, {"heading": "4.3.3 Mapping features to a common scale", "text": "As JavaML requires numerical features, all features were mapped to numerical scales:\n\u2022 Binary features were mapped to 0 (false) and 1 (true)\n\u2022 Ternary features were mapped to 0 (false), 1 (true) and 99 (not applicable)\n\u2022 Numerical features were represented as themselves, either as whole numbers (e.g. word length) or as floating point numbers (e.g. vowel ratio)\n\u2022 Java specific features (18,20) take the underlying numerical value as feature\n\u2022 N-grams were encoded numerically using algorithm 1\n5e full list can be found under the documentation of the Java Character class hp://docs.oracle.com/javase/7/docs/api/java/lang/Character.html\n25\nAlgorithm 1 N-gram numerical encoding 1: function (word) 2: sum \u2190 0 3: for character in word do 4: value \u2190code-point of character 5: sum \u2190 sum+ value 6: end for 7: return sum 8: end function\nWhile algorithm 1 does not encode n-grams in an unambiguous way (\u201cen\u201d and \u201cne\u201d are both encoded as 211), it provides a sufficiently good encoding."}, {"heading": "4.3.4 e problem of unambiguous encoding", "text": "I have tried using unambiguous encodings. e main problem with unambiguous encoding is that the notion of \u201cdistance\u201d is distorted. e idea behind the unambiguous encoding is that each \u201cword\u201d (i.e. string of characters) is encoded numerically so that no two \u201cwords\u201d are represented as the same number. Besides the encoding of each separate character, the position of the character inside the string also has to be encoded. A possible encoding e for a string w1w2w3 could be\new1w2w3 = n(w1) + x \u2217 n(w2) + y \u2217 n(w3) (19)\nwith wi the character of the string at position i, n(wi) the numerical encoding of the character wi and x and y parameters. If |A| is the alphabet size of the alphabet A in which the word is encoded, the following constraints must be true for the encoding to be unambiguous:\nx \u2265 |A| (20)\ny \u2265 |A|2 (21)\nIf we take for example the English alphabet with 26 lowercase and 26 uppercase leers, not counting punctuation, digits and other characters, it has to be true that x \u2265 52 and y \u2265 2704. e problem is that we cannot know in advance what size the alphabet will be. If we have English and German texts, the size can be estimated around 60. However, if we have English, Russian and Arabic text, the size drastically increases. We could choose any two very big numbers, but if we want to guarantee our encoding to be unambiguous, we run the risk of ending up with numbers too big to be represented efficiently.\n26\nIn this encoding scheme, distance is skewed: changes to the first character result in linear distance. \u2018man\u2019 and \u2018nan\u2019 have a distance of 1, because \u2018m\u2019 and \u2018n\u2019 have a distance of 1. \u2018man\u2019 and \u2018lan\u2019 have a distance of 2, etc. Changes to the second character are multiplied by x. \u2018man\u2019 and \u2018men\u2019 have a distance of x \u2217 (distance(a, e)) = 4 \u2217 x. Changes to the third character are scaled by y. For any sufficiently big x and y, the distances are too skewed to be used for automatic cluster analysis. Let us consider the following example with only two characters for simplicity. For this example, let us assume x = 1373.\nIt should be apparent from table 2 that the notion of \u201cdistance\u201d is distorted. In comparison, table 3 shows the encoding achieved with algorithm 1.\nWhile this encoding is not unambiguous, it is considered sufficiently good for our purposes."}, {"heading": "4.3.5 e clusterer", "text": "Most clustering algorithms such as k-means need to be passed the number of clusters to generate. As we want to work as flexibly as possible, I ignored all algorithms that need the number of clusters before clustering. In contrast, the x-means algorithm (Pelleg and Moore, 2000) estimates the number of clusters to generate itself. is algorithm has been chosen to perform the language clustering tasks.\nWhile WEKA and ELKI offer a graphical user interface and various graphical representations of the results, the output is not easily interpretable. Indeed, we can get a visualization of a clustering operation as shown in figures 17 (WEKA) and 18 (ELKI). However, all data points have to be manually checked by either clicking each point\n27\nin order to get additional information about that data point (WEKA) or by hovering over the data points aer having selected the Object Label Tooltip option (ELKI). Figure 18 shows the information for the lowest orange rectangle data point in the ELKI visualization.\n28\nerefore, I have decided to embed the x-means clustering algorithm into a custom framework. Originally part of the WEKA algorithms, the x-means algorithm has been integrated into a Java program via the JavaML library. e framework takes an input file, constructs the aforementioned feature vectors from the input, performs normalization, passes the calculated feature vectors to the clustering algorithm and displays the results in a text-based easily interpretable manner.\nPreliminary analyses have shown that the first clustering result oen is not discriminating enough. Hence, I perform a first clustering analysis, followed by a second clustering analysis on the clusters obtained from the first analysis."}, {"heading": "4.3.6 Evaluating clusterings", "text": "e clustering results are evaluated using four common similarity measures used in evaluating the accuracy of clustering algorithms. esemethods are based on counting\n29\npairs (Wagner and Wagner, 2007). Let us consider the clustering C = {C1, . . . , Ck}. C is a set of non-empty disjoint clusters C1, . . . , Ck. Let us consider the reference clustering C \u2032 = {C1, . . . , Cl}. We define the following sets.\n\u2022 S11: set of pairs that are in the same cluster in C and C \u2032\n\u2022 S00: set of pairs that are in different clusters in C and C \u2032\n\u2022 S10: set of pairs that are in the same cluster in C and in different clusters in C \u2032\n\u2022 S01: set of pairs that are in different clusters in C and in the same cluster in C \u2032\nLet nij = |Sij|, with i, j \u2208 {0, 1} be the size of a given set Sij . e Rand Index is defined as\nRI = n11 + n00\nn11 + n10 + n01 + n00 (22)\ne Rand Index measures the accuracy of the clustering given a reference partition (Wagner and Wagner, 2007). However, it is criticized for being highly dependent on the number of clusters (Wagner and Wagner, 2007).\ne Jaccard Index measures the similarity of sets. It is similar to the Rand Index, but it disregards S00, the set of pairs that are clustered into different clusters in C and C \u2032 (Wagner and Wagner, 2007). It is calculated as\nJ = n11\nn11 + n10 + n01 (23)\ne Fowlkes-Mallows Index measures precision. It is calculated as\nFM = n11\u221a\n(n11 + n10)(n11 + n01) (24)\ne Fowlkes-Mallows Index has the undesired property of yielding high values when the number of clusters is small (Wagner and Wagner, 2007).\nFinally, I will indicate the F-Score. According toManning et al. (2008), in the context of clustering evaluation the F(\u03b2) score is defined as\nF (\u03b2) = (\u03b22 + 1) \u2217 P \u2217R\n(\u03b22)P +R (25)\nwith precision P and recall R defined as\nP = n11\nn11 + n10 (26)\n30\nR = n11\nn11 + n01 (27)\nBy varying \u03b2, it is possible to give more weight to either precision (\u03b2 < 0) or recall (\u03b2 > 1) (Manning et al., 2008). As I value recall higher than precision, I will indicate F1 (\u03b2 = 1) and F5 (\u03b2 = 5) scores. Indeed, I want to penalize the algorithm for clustering together pairs that are separate in the gold standard while not penalizing the algorithm for spliing pairs that are together in the gold standard.\nAll measures of similarity fall between [0, 1]with 0 being most dissimilar and 1 being identical. As there is no ultimate measure and all measures of similarity have their drawbacks (Wagner and Wagner, 2007), all measures will be indicated in the results section."}, {"heading": "4.4 Weakly supervised language model induction", "text": "e language model induction approach works in two stages. In the first stage, n-gram language models are induced from the text. In the second stage, the text is mapped to the induced models. e algorithm for the language model induction is as follows:\nAlgorithm 2Model induction 1: IM 2: for word in words do 3: modelAndScore \u2190 MS(word) 4: score \u2190 modelAndScore.score 5: if score < threshold then 6: model \u2190 M(word) 7: models.add(model) 8: else 9: maxModel \u2190 modelAndScore.model 10: maxModel.update(word) 11: end if 12: end for\nFirst of all, an initial languagemodel is created. For eachword, themaximummodel and maximum score is calculated. ese values correspond to the language model that yielded the highest probability for the word in question, and the associated probability. If the score falls below a threshold t (i.e. none of the existing language models model the word well enough), a new language model is created on the basis of the word and added to the list of language models. Otherwise, the top scoring language model is updated with the word in question.\n31\nAs the text structure itself influences the quality of the induced models, the language model induction is run i times (i 6 1), with one iteration consisting of two induction steps, once forward and once backward, and j times from a random position (j 6 0). e initial model creation thus either picks the first word of the text (as shown in algorithm 3 line 2), or the last word of the text, or a random word.\nAlgorithm 3 Initial model creation 1: function IM 2: word \u2190 words.first 3: model \u2190 createModel(word) 4: models.add(model) 5: end function\nAlgorithm 4Max model and max score 1: function MS(word) 2: maxScore \u2190 0 3: maxModel \u2190 none 4: for model in models do 5: score \u2190 model.probability(word) 6: if score > maxScore then 7: maxScore \u2190 score 8: maxModel \u2190 model 9: end if 10: end for 11: returnmaxModel,maxScore 12: end function\nAlgorithm 4 returns both the max model and the max score wrapped as a custom object. e individual values can then be read as necessary.\nAer the models have been induced, the most similar models are merged based on distributional similarity. Distributional similarity is calculated as explained below. is merging step only merges one model from the forward induction group with one model from the backward induction group. e resulting model is added to the set of probable (\u201csilver\u201d ) models.\nMerging is performed according to algorithm 5. e merging algorithm only retains the common set of unigrams from both models, and all resulting bi- and trigrams, excluding any bi- and trigrams that contain character that occur only in one of the models. e values for the resulting language model are calculated according to one of four different merge modes.\ne merge modes are:\n32\nAlgorithm 5Model merger 1: function (model1,model2,mode) 2: merged \u2190 \u2205 3: for unigram u1 inmodel1.unigrams do 4: for unigram u2 inmodel2.unigrams do 5: if u1 = u2 then 6: v1 \u2190 f(u1) \u25c3 f(u1) is the frequency of u1 7: v2 \u2190 f(u2) 8: value \u2190 mode(v1, v2) 9: unigram \u2190 u1 \u25c3 or u2, since both are equal 10: merged \u2190 (unigram, value) 11: else 12: exclude \u2190 u1 13: exclude \u2190 u2 14: end if 15: end for 16: end for 17: for all bigrams b inmodel1 andmodel2 do 18: if not exclude contains any char in b then 19: v1 \u2190 f(b,model1) or 0 \u25c3 frequency of b inmodel1 20: \u25c3 or 0 if it does not exist 21: v2 \u2190 f(b,model2) or 0 22: value \u2190 mode(v1, v2) 23: merged \u2190 (b, value) 24: end if 25: end for 26: for all trigrams t inmodel1 andmodel2 do 27: if not exclude contains any char in t then 28: v1 \u2190 f(t,model1) or 0 29: v2 \u2190 f(t,model2) or 0 30: value \u2190 mode(v1, v2) 31: merged \u2190 (t, value) 32: end if 33: end for 34: returnmerged 35: end function\n33\n\u2022 MAX: use the maximum value (max(v1, v2))\n\u2022 MIN: use the minimum value (min(v1, v2))\n\u2022 MEAN: use the mean value (v1+v2 2 )\n\u2022 ADD: use the sum of the values (v1 + v2)\nIf the random iteration count j > 0, a random word is chosen and the induction is run once forward and once backward starting from this position. en, the most similar models from each set are merged and added to the set of probable models. It should be noted that seing the parameter j > 0 will make the algorithm nondeterministic.\ne model induction is then repeated while the iteration count i has not been reached or until no more models are induced, with the difference that for each word, each probable model is first consulted. If any of the probable models yields a score higher than the threshold value t, it is assumed that the word is already well represented by one of the probable models and no models are induced for this word. If the score falls below the threshold value t, induction is run as described.\nAt the end of the induction loop, all probablemodels are checked against each other. While there are two models that have a similarity below the silver threshold value s, the two models are merged and added to the set of very probable (\u201cgold\u201d ) models.\nIf the set of probable models is not empty aer this merging step, all remaining probable models are added to the set of very probable models.\nIn the second stage, the text is segmented according to the induced \u201cgold\u201d models. For each word, the language model with the highest probability for the word is chosen as that word\u2019s hypothetical language model."}, {"heading": "4.4.1 Distributional similarity", "text": "Suppose we have three models with the distributions of leers as shown in figures 19, 20 and 216. Similarity could be calculated based on the occurrence of unigrams/leers alone, i.e. ifmodel1 contains the leer \u2018a\u2019 andmodel2 also contains the leer \u2018a\u2019, their similarity increases by 1.\nHowever, if we calculate similarity in such a way, all three models are equally similar to each other, as each of the leers occurs at least once in each model. Yet, it should be clear that models 1 and 2 are very similar to each other while model 3 is dissimilar.\nerefore, in order to include the distribution of leers in the similarity measure, similarity is calculated as shown in algorithm 6.\n6\ue049efigures shown are used for illustration purposes only and do not necessarily reflect real language models.\n34\n35\na b c d e f g h i 0\n2\n4\n6\nwith f(c) returning the frequency of the character c. e number 2 in (2\u2212q) in line 10 can be explained as follows: q expresses the dissimilarity of the models with regard to a unigram distribution with 0 6 q 6 1, hence (1 \u2212 q) expresses the similarity. To this, we add 1, as we increase similarity by 1 due to the match; we augment the simple increase of 1 by the similarity of the distribution.\n36"}, {"heading": "4.4.2 Evaluating results", "text": "e results of this approach can be interpreted as clusters, where each language model represents one cluster core and all words assigned to that model making up that cluster. Evaluation will hence be analogous to the evaluation of the clustering approach."}, {"heading": "4.4.3 Estimating the parameters", "text": "As the language model induction can be controlled by parameters, we have to find a combination of parameters thatworkswell for our task. e parameters i, j and \u201cmerge mode\u201d have been estimated on the development set. e development set contains similar documents to those in the test set. e development set can be found in the appendix.\nIt has been found that the parameter combination i = 4, j = 2, ADD yields good results across the development set. Hence, these values have been used for the test set evaluation.\n37"}, {"heading": "5 Results", "text": "\u2018Baseline\u2019 indicates the measurement where all words have been thrown into one cluster, measured against the gold standard. For \u2018Baseline 2\u2019, every word has been put into its own cluster and this clustering is evaluated against the gold standard. e column \u2018F1\u2019 stands for the F1 score and the \u2018F5\u2019 column stands for the F5 score.\nIf any of the \u2018runs\u2019 yields a higher score than any of the baseline values, the maximum score is indicated in bold. If a field contains \u2018n/a\u2019, this means that the value could not be calculated for whatever reason (most oen a division by zero would have occurred)."}, {"heading": "5.1 N-Gram language model", "text": "38\n39\n40\n41"}, {"heading": "5.2 Textcat", "text": "42\n43\n44\n45"}, {"heading": "5.3 Clustering", "text": "e first run indicates the value aer one clustering step, and the second run indicates the value aer applying the clustering algorithm to the results of the first run.\n46\n47\n48\n49"}, {"heading": "5.4 Language model induction", "text": "In addition to highlighting results that outperform the baseline values, the following tables have been color coded. Results that outperform the clustering algorithm are indicated in red and results that outperform both the clustering algorithm and the ngram language model are indicated in blue.7\n7Results that outperform only the n-gram language model would have been indicated in green, but\nthere is no score that outperforms only the n-gram language model.\n50\n51\n52\n53"}, {"heading": "6 Discussion", "text": "e work by Seldin et al. (2001) is similar to the work presented here. ey propose an unsupervised language (and protein sequence) segmentation approach that yields accurate segmentations. While their work looks promising, it also has its drawbacks. eir method requires longer monolingual text fragments and a sizable amount of text. Furthermore, they disallow switching languagemodels aer each word. is presumption will fail to detect single-word inclusions and structures as shown in figure 22, where the language alternates aer each word.\nw1 w2 w3 w4 w5 w6 w7 \u2026\nFigure 22: Alternating language structure\nWhile this structure looks very artificial, such a structure is found, for instance, in the fi\ue039h Pali dictionary text, in the passage \u201cPacati, [Ved. pacati, Igd. *peq\u01d4\u014d, Av. pac-;\u201d. In this case, \u2018red\u2019 corresponds to Pali, \u2018blue\u2019 to (abbreviations in) English and \u2018green\u2019 to reconstructed Indo-european."}, {"heading": "6.1 N-Gram language models", "text": "e trained n-gram languagemodel approachworks well on the Latin script data, managing to single out the German inclusion from the English\u2013German text (even though it is classified as \u201cother\u201d instead of German).\nFor German\u2013Finnish\u2013Turkish, English\u2013French, English\u2013Transliterated Greek and Italian\u2013German, the separation of the main languages involved is good, although there appear to be some problems when words contain non-word characters such as quotes or parentheses.\nSome puzzling misclassifications happen in the English\u2013Transliterated Greek case: ag\u00e1pe is considered English and \u00e9ros is considered Transliterated Amharic.\nIn the Italian\u2013German text, the Italian language leads to a rather important Spanish cluster due to the relatedness of the two Romance languages.\nOn the mixed script data set, the results are more diverse. Greek\u2013Russian, English\u2013Spanish\u2013Arabic andUkrainian\u2013Russian are segmentedwell, with English\u2013 Spanish\u2013Arabic having Spanish split into Spanish, French and Italian due to the relatedness of the languages.\nIn contrast, the segmentation of English\u2013Greek did not work well at all. Of the two Greek words \u1f00\u03b3\u03ac\u03c0\u03b7 and \u1f14\u03c1\u03c9\u03c2, \u1f00\u03b3\u03ac\u03c0\u03b7 was considered French and \u1f14\u03c1\u03c9\u03c2 was considered Russian. It must be noted, though, that these words bear polytonic diacritics, whereas the model was trained on monotonic Greek.\n54\nAlso, the segmentation of English\u2013Chinese did not work well. is is probably due to the way the model was trained. Chinese script is wrien without whitespace characters between words, and the correct segmentation of a text wrien in Chinese requires in-depth knowledge of the language. Some words are wrien with only one character, but others are composed of two or more characters, with the meaning oen being non-compositional; the meaning of a two-character word is different from the sum of the meaning of the two characters. Sometimes, more than one segmentation would be possible and the context decides on which segmentation is correct. In other cases, more than one segmentation might be correct. is problem occurs with all scripts that are wrien without whitespace.\nAs with the simplified assumption in the tokenization of whitespace-scripts, where I consider a word to be a character sequence delineated by whitespace, I have treated each character as a word. Adapting the method to Chinese and similar scripts would have been possible, but would have introduced the need for large amounts of external linguistic knowledge. Indeed, every possible non-whitespace-script would have to be considered, and each of the tokenizers would be language dependent, i.e. a tokenizer for Chinese would not work on Korean or Japanese.\ne supervised approach did not work well on the Pali dictionary data. While English words could be isolated somewhat successfully, the rest of the data proved difficult to segment. As an example, let us look at the first Pali text. e English cluster contains almost only English words, but not all, the \u201cother\u201d cluster contains mainly marked up words, and the rest is seemingly haphazardly distributed among the other models.\nPali 1: abbha\n\u2022 (AR) ., 134., 289.\n\u2022 (DE) Miln), imber, dark), Miln\n\u2022 (EL) (=, (abbha\u014b\n\u2022 (EN) water, mountain, of, free, (used, or, like, referred, (also, A, is, cloudy, clouds, later, a, froth, 1, summit, thundering, by, mass, Pv, Oir, obscure, scum, that, water]., thick, As, from, It, is, at, as, the, in, clouds, things, also\n\u2022 (ES) (dense, f., sense, expl, rajo\n\u2022 (FI) 239., rain;, Lat., Vin, perhaps, SnA\n\u2022 (FR) cloud, Dh, adj., point, cloud, Dhs, A), rain, VvA, DhsA, list\n\u2022 (IT) \\\u201ddark, &, ambha, 3, 1, 317, J, sunshine, cp., abhra, [Vedic, (megho\n55\n\u2022 (PL) 487, =, S, 295, <br, moon\u2013, 249\n\u2022 (RU) 348, 53\n\u2022 (TR) viz., ambu, Vv\n\u2022 (TrAM) 687, PvA, (\u00b0sama, 101, (n\u012bl\u00b0, (cp., 64;, (nt.), 581, m., Sn, 1064;\n\u2022 (TrEL) , Gr., Sk., Idg., to, pabbata, nt.\n\u2022 (UK) 12)., 273, 617, 348)., 250;, 251)., 382).\n\u2022 (other) <b> \u2013sa\u014bvil\u0101pa </b>, <b> \u2013mua </b>, <smallcaps> vi. </smallcaps>, (mahiy\u0101, <smallcaps> iv. </smallcaps>, cloud\\\u201d;, <b> R\u0101hu </b>, <b> abbh\u0101 </b>, <b> abbha\u014b, <superscript> 9 </superscript>, marajo </b>, abbh\u0101mua, val\u0101haka);, <smallcaps> i. </smallcaps>, <b> abbh\u0101maa </b>, val\u0101haka\u2013sikhara, <superscript> s. </superscript>, <smallcaps> ii. </smallcaps>, <b> dh\u016b-, storm\u2013 cloud, /><b> \u2013k\u016b\u1e6da </b>, thunder\u2013cloud);, <at> a)fro\\\\s </at>, <b> \u2013pa\u1e6dala </b>, <at>o)/mbros</at>, n\u012bla\u2013megha, <superscript>1</superscript>, *m\u030abhro, \\\u201ddull\\\u201d;, acch\u0101desi);, mahik\u0101</b>, <b> \u2013ghana </b>\nOn the Twier data, the supervised approach achieved passable results. While the numbers look great, the actual segmentations do not. For Twier 1, too many clusters were generated, for Twier 2 and 3, the recognition of French words worked somewhat, also recognizing English words as French and French words as English. For Twier 4, the Polish inclusion was isolated but recognized as \u201cother\u201d, together with \u201cstrawberries\u201d. e recognition of transliterated Amharic worked satisfactorily, yielding \u2018naw\u2019 to the Polish model.\nAs the number of language models increases, so does the risk of misclassification. As can be seen, we already have quite some misclassification with only 15 language models. For example, in our data, the English preposition \u2018to\u2019 is oen erroneously classified as \u2018transliterated Greek\u2019. e Greek particle \u03c4\u03bf \u2018to\u2019 can be either the neuter singular accusative or nominative definite article \u2018the\u2019, the masculine singular accusative or nominative definite article \u2018the\u2019 or the 3rd person neuter singular nominative/accusative weak pronoun \u2018it\u2019, and as such is rather frequent in the language. is is especially problematic with the transliterated Greek language model, which tends to misclassify the English preposition \u2018to\u2019 as transliterated Greek.\nA quick corpus study using the Corpus of Modern Greek8 and the Corpus of Contemporary American English9 reveals that the frequency per million words for the Greek particle \u03c4\u03bf is 22666, while the English preposition \u2018to\u2019 has a frequency per million words of 25193. eir relative frequencies are very close together, and it might\n8http://web-corpora.net/GreekCorpus/ 9http://corpus.byu.edu/coca/\n56\njust have happened that the training data used in this work contained more Greek \u2018to\u2019s than English \u2018to\u2019s, leading to this misclassification.\nOther reasons for misclassification include relatedness of the modeled languages as in the case of Germanic or Romance language families. Also, the text types used for training and the text types used for testing play an important role, as well as the amount of training data.\nFor n-gram language models, the quality of the model is dependent on the texts used for training and the texts used in evaluation. It is probable that a different training set would have yielded different results. is is also the problem with the supervised approach; it is necessary to have language data for training and the trained models reflect the training data to some extent."}, {"heading": "6.2 Textcat", "text": "Textcat works well on monolingual texts. However, it fails on multilingual texts and does not workwell on short fragments of text, such as single words. Many of the words are tagged as unknown, and if a language has been identified, the language guess oen is not correct. Hence, Textcat cannot be used for language segmentation purposes.\nIndeed, Textcat fails to exceed the baseline values except for two cases: \u2018Twier 3\u2019 and \u2018Twier 4\u2019 yield beer values than the baseline values. However, upon closer inspection, it is clear that the numerical index values do not give a reliable picture of the quality of the clustering.\nIndeed, while the clustering of \u2018Twier 3\u2019 is not nonsensical, it is not very good, failing to extract the French insertion \u2018breuvages\u2019. e Rand Index also only shows a slightly beer value than the baseline values. It seems that the outstanding score for \u2018Twier 4\u2019 is achieved because both the clustering by Textcat and the gold standard have the same number of clusters.\nTables 20 and 21 show the clusterings side by side. Clearly, Textcat performed poorly despite the high numerical index values. A closer inspection of all the Textcat results shows that Textcat performs poorly at the task of language segmentation; oen, a word cannot be assigned a language and thus is added to the cluster of \u2018unknown\u2019 language words. For the words where a language has been identified, it most oen is not the correct language. While language identification is not necessary for the task of language segmentation, it helps to understand why Textcat failed at the task of language segmentation.\n57"}, {"heading": "6.3 Clustering", "text": "e clustering results are more difficult to interpret. Oen, the first distinction made seems to be based on case, i.e. words that begin with a capital leer versus words that are all lowercase leers. e second run on the \u2018mixed script: English \u2013 Greek\u2019 data shows that the first cluster from the first run has been separated into a cluster with words that begin with a capital leer and two clusters with words that don\u2019t begin with a capital leer.\nEnglish\u2013Greek: First run: First cluster\n\u2022 \u201cintimate, \u201cwithout, Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether, affection, ancient, another.\u201d, appreciation, aspires, araction, araction.\u201d, becomes, benevolence., biblical, brotherly, chapter,\u201d, charity;, children, children., contemplation, content, continues, contributes, definition:, described, existence;, explained, express, feeling, feelings, finding, further, holding, initially, inspired, knowledge, marriage., necessary, non-corporeal, passage, passion.\u201d, philosophers, physical, platonic, refined, relationships, returned, self-benefit)., sensually, spiritual, subject, suggesting, through, throughout, transcendence., unconditional, understanding, without, youthful\n58\nEnglish\u2013Greek: Second run: Splitting of first cluster\n\u2022 affection, ancient, another.\u201d, aspires, becomes, biblical, chapter,\u201d, charity;, children, children., content, definition:, feeling, feelings, finding, holding, marriage., necessary, passage, passion.\u201d, platonic, refined, returned, subject, through, without\n\u2022 Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether\n\u2022 \u201cintimate, appreciation, araction, araction.\u201d, benevolence., brotherly, contemplation, continues, contributes, described, existence;, explained, express, further, initially, inspired, knowledge, non-corporeal, philosophers, physical, relationships, self-benefit)., sensually, spiritual, suggesting, throughout, transcendence., unconditional, understanding, youthful\nAnother important distinction seems to be the length of words. Indeed, the results oen show clusters that clearly are based on the length of the contained words. e first run on the \u2018latin script: German \u2013 Italian\u2019 data shows that short words have been singled out into the first cluster.\nItalian\u2013German: First run: First cluster\n\u2022 (il, E, So, a, ad, da, di, e, es, ha, i, il, in, la, le, lo, ma, ne, se, si, un, va, zu\ne clustering works well when the scripts involved are dissimilar, as in the case of the English\u2013Chinese text, where the Chinese characters were isolated aer the first run, and also the English\u2013Spanish\u2013Arabic example, where the Arabic part was completely isolated in the first run.\ne closer the scripts become, the less well clear cut the results are. For Greek\u2013 Russian, the results are acceptable, with one mixed cluster. However, the number of clusters is too high for the number of languages involved and the separation is only achieved aer two consecutive clusterings.\ne clustering of closer scripts, such as Ukrainian\u2013Russian does not work well. e clusters, with the exception of the cluster containing the datum \u20189\u201413\u2019 are all impure, consisting of Ukrainian and Russian words. e second run also fails at improving the clustering.\nFinally, clustering of latin based scripts does not perform well unless diacritics are involved and the diacritics form the most salient distinction. Word containing leers with diacritics are then generally separated from words containing no diacritics, as in the German\u2013Finnish-Turkish example. e first run generates a cluster for numbers, two clusters with diacritics and one cluster without diacritics.\n59\nProbably for this reason, the clustering of TransliteratedGreek\u2013English andGreek\u2013 English worked surprisingly well. In both cases, the first run managed to separate the (transliterated) Greek parts from the English words. However, unaccented Greek words such as Agape, erotas or eros were clustered with English.\nEnglish\u2013Transliterated Greek: First run: Transliterated Greek cluster\n\u2022 ag\u00e1pe, phil\u00eda, storg\u0113., \u00e9ros\nEnglish\u2013Greek: First run: Greek cluster\n\u2022 (\u1f00\u03b3\u03ac\u03c0\u03b7, (\u1f14\u03c1\u03c9\u03c2, Ag\u00e1pe, ag\u00e1p\u0113), \u00c9ros, \u00e9r\u014ds), \u2013\ne problem is that when there are other salient distinguishing features besides diacritics, the result is less good, as can be seen on the Pali data."}, {"heading": "Pali: abhijjhitar: Second run", "text": "\u2022 abhijjhita, abhijjh\u0101tar, covets, function], med., one, who, \u00b0itar), \u00b0itar, \u00b0\u0101tar).\n\u2022 (T., A, M\n\u2022 =, l., v.\n\u2022 <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, ag., fr., in\n\u2022 265, 287\n\u2022 [n.\nIn some cases, the clustering fails at the task of language segmentation, as in the case of the various English\u2013French texts and the English\u2013German example with the German inclusion. We can thus say that the surface structure or morphology, or in other words the basis from which we can extract features, is not sufficient to deduce relevant information about \u2018language\u2019.\nWhen there are more than two languages that are to be separated, the clustering also does not work well. Indeed, the most dissimilar objects are separated first. In the case of English\u2013Spanish\u2013Arabic, the Arabic part is separated first, as well as words with diacritics, while English and Spanish words without diacritics are thrown together. Subsequent runs show no improvement of the clustering concerning the separation of English and Spanish.\nIn the case of German\u2013Finnish\u2013Turkish, the clustering algorithm seems to cluster out Turkish first, followed by Finnish. e results are however much less clear-cut than for English\u2013Spanish\u2013Arabic.\n60"}, {"heading": "6.4 Language model induction", "text": "e languagemodel induction does not seem towork verywell on the Latin script data. ere are almost only impure clusters, containing more than one language. However, the approach consistently outperforms the clustering approach when we look at the F5 score. For the English\u2013French data set, the clustering approach even outperforms the n-gram language model approach. Indeed, the French words are relatively well separated from the English text, with the exception of \u2018sucr\u00e9\u2019, which is still thrown together with English words."}, {"heading": "Latin script: English\u2013Fren", "text": "\u2022 both, \u201cso\u201d, in, English, although, their, is, is, the, opposite, of, \u201crough\u201d, or, is, the, opposite, of, sweet, only, for, wines, (otherwise, is\n\u2022 mou, :, mou, but\n\u2022 doux,\n\u2022 Doux, (rugueux), Doux\n\u2022 while\n\u2022 \u201chard\u201d., used).,\n\u2022 translate, as, meaning, very, different., \u201dcoarse\u201d, can, also, mean, almost,sucr\u00e9,\nIn contrast, the approach works well on the mixed script data. Indeed, we achieve a good separation of the languages by script. However, when there are also Latin based scripts, we encounter the same problems as mentioned above with rather modest results. For example, for the English\u2013Greek text, the approach separates out the Greek character words but it fails to separate transliterated Greek and English. Also, for the English\u2013Spanish\u2013Arabic text, Arabic is separated out, but English and Spanish are not separated well.\nOne interesting observation can be made in the case of the English\u2013Chinese text. e Chinese characters have been isolated, but the Pinyin transcription is thrown together with the Chinese characters. Based on the prior observations, this is rather unexpected. is raises the question of whether Pinyin ought to be clustered out, or clustered together with English or Chinese.\nAgain, the languagemodel induction approach outperforms the clustering approach, and also the n-gram language model approach in the case of the English\u2013Greek text.\nOn the larger Pali dictionary entries, the language model induction approach yields acceptable results. On the shorter Pali dictionary entries, the languagemodel induction approach yields good results.\n61\ne quite low performance must be blamed on the data. Indeed, the Pali dictionary data contain various problematic characters such as \u2018comma/dot and whitespace\u2019 as one character. On such characters, whitespace tokenization fails, yielding big chunks of nonsense tokens. For example, the fourth Pali dictionary entry was split into five chunks (while it might not be displayed as such, all commata and all dots are in fact not followed by whitespace, the whitespace is part of the character,10 hence whitespace tokenization fails).\nPali: g\u016bhan\u0101: Chunks\n\u2022 G\u016bhan\u0101\uff0c\uff08f.\uff09\n\u2022 [abstr\uff0efr\uff0eg\u016bhati]=g\u016bhan\u0101\n\u2022\uff08q\uff0ev.\uff09\n\u2022 Pug\uff0e19\uff0eCp\uff0epari\u00b0\uff0e\uff08Page\n\u2022 253\uff09\nFurthermore, the data contains markup, abbreviations, references, typing mistakes and signs such as <-> that are difficult to assign to a language.\nOn the Twier data, the language model induction approach works rather well. For example, on the first text, separation is not perfect with the Greek cluster still containing some English words.\nTwitter 1: English\u2013Greek\n\u2022 BUSINESS, EXCELLENCE.\n\u2022 \u039c\u03cc\u03bb\u03b9\u03c2, \u03c8\u03ae\u03c6\u03b9\u03c3\u03b1, \u03b1\u03c5\u03c4\u03ae, \u03c4\u03b7, \u03bb\u03cd\u03c3\u03b7, Internet, of, \u03c3\u03c4\u03bf, \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc\n\u2022 ings, IT\nFor the third and fourth text, the approachmanages to single out the other-language inclusions, but not exclusively. Both times, there is one additional item in the cluster (the relevant clusters are marked in red).\n10e comma has the Unicode codepoint U+FF0C (FULLWIDTH COMMA) and the dot has the Unicode codepoint U+FF0E (FULLWIDTH FULL STOP)\n62\nTwitter 3: Fren\u2013English\n\u2022 #FWWC2015\n\u2022 breuvages, go\n\u2022 Food, Edmonton, to, for, the\n\u2022 in, waiting, #bilingualism\n\u2022 and, are, ready, just, fans\nTwitter 4: English\u2013Polish\n\u2022 comes, from, with, two, crates, of, strawberries, jackets, omg\n\u2022 my, dad, poland, and, adidas\n\u2022 back, \u017cubr\u00f3wka\ne approach exceeded expectations on the second and fi\ue039h Twier text. On the second text, the \u2018French\u2019 cluster does not only contain the French words \u2018Demain\u2019 and \u2018par\u2019, but also the French way of notating time \u201818h\u2019.\nTwitter 2: Fren\u2013English\n\u2022 Keynote, \u201ce, collective, of, science-publish, or, perish;, it, all, that, counts?\u201d\n\u2022 Demain, 18h, par\n\u2022 #dhiha6, David\n\u2022 @dhiparis, dynamics, is\nOn the fi\ue039h text, an almost perfect result was achieved, with only one additional subdivision of the \u2018English\u2019 cluster.\nTwitter 5: Transliterated Amharic\u2013English\n\u2022 (coffee\n\u2022 bread). is, our\n\u2022 Buna, dabo, naw\n63\nIt seems that the language model approach does not work very well on longer texts, especially on longer texts in Latin-based scripts, with the chosen parameter set; still, the approach outperforms the clustering approach and achieves scores in the vicinity of the scores achieved with the supervised trained n-gram language model approach. On mixed script texts, the approach consistently outperforms the clustering approach and we also reach scores in the vicinity of the scores achieved with the supervised trained n-gram language model approach.\nMoreover, on short texts, the approach works rather well. We succeed in outperforming the supervised trained n-gram language model approach on a number of texts, and we achieve scores close to the scores achieved with the supervised trained n-gram language model approach.\nAlthough the language model induction approach tends to generate too many clusters, it also generally succeeds at separating the languages involved."}, {"heading": "6.5 Scores", "text": "Of the scores I used for evaluation purposes, it seems that a combination of a high Rand Index and a high F5 score indicate a good language segmentation. A high F5 score alone is not significant. For example, the clustering algorithm achieves an F5 score of 0.7215 on \u2018Twier 3\u2019. is score looks good, but the Rand Index score is at 0.4571, and the segmentation is not good.\nTwitter 3: Cluster analysis\n\u2022 Edmonton, Food\n\u2022 go, in, to\n\u2022 and, are, breuvages, fans, for, just, ready, the, waiting Similarly, a high Rand Index score alone is not significant. For example, the clustering algorithm achieves a Rand Index score of 0.6738 on the \u2018Pali 2\u2019 text, but the F5 score is at 0.3825 and the clustering is not good.\nPali 2: Cluster analysis\n\u2022 abhijjhita, abhijjh\u0101tar, covets, function], med., one, who, \u00b0itar), \u00b0itar, \u00b0\u0101tar).\n\u2022 (T., <smallcaps>i.</smallcaps>, <smallcaps>v.</smallcaps>, =, A, M, ag., fr., in, l., v.\n\u2022 265, 287\n\u2022 [n.\n64"}, {"heading": "7 Conclusion", "text": "In this thesis, I have asked the question of whether unsupervised approaches to language segmentation perform beer on short and difficult texts than supervised approaches by overcoming some of the difficulties associatedwith supervised approaches, such as the need for (enough and adequate)11 training data, the language-specificity of the language model or the inflexibility of trained language models when it comes to spelling variation and abbreviations, unless the training data also contained spelling variation and abbreviations.\nI have given an overview over related work, presenting supervised approaches that have been used in monolingual language identification and the amelioration of such approaches through unsupervised approaches such as clustering.\nUnfortunately, the body of literature covering the topic of language segmentation is sparse. e work by Yin et al. (2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wrien language. As I concentrated on wrien language, their work was not conducive to this thesis.\nIn contrast, Seldin et al. (2001) present a work that looks promising. ey present a system that finds language borders in a text with great accuracy using unsupervised algorithms. However, they restrict their algorithm in such a way that switching language models aer each word is disallowed. us, they are unable to detect singleword inclusions and cannot handle situationswhere the language switches everyword, as has been shown to occur in the test data used in section 4.\nAnother major drawback of the approach is that it also needs longer fragments of monolingual text and an overall longer text. Hence, their approach would not work well on short texts, if at all.\nNext, I have presented the theoretical foundations of a supervised n-gram language model approach and an unsupervised clustering approach. Finally, I have introduced a weakly supervised n-gram language model inducing approach devised by myself. All of these approaches can be used for language segmentation. In order to test how well the different approaches perform on different text types, I have performed experiments.\nSection 4 presents the experiments made. I have first compiled a small corpus of texts ranging from longer texts with clearly separated languages to one-sentence Twier messages containing foreign language inclusions. I have also included a set of dictionary entries from the Pali dictionary by the Pali Text Society. Indeed, these entries contain a lot of different languages and abbreviations, and (unfortunately) are not consistently formaed.\nI have then presented my implementations of the supervised and weakly super11e question of what is to be considered \u2018enough\u2019 or \u2018adequate\u2019 is another point of contention; the\ndata always influences the resulting models.\n65\nvised approaches and the choice of the unsupervised clustering algorithms. en, I have presented the results of their application to the data.\nIt can be said that the supervised approach works reasonably well. e drawbacks are that the approach needs training data to train the models on. e problems of the training data and its influence on the models have been raised more than once.\ne supervised approach failed for non-whitespace scripts. emodels would have to be adapted for non-whitespace scripts, introducing more complexity. Also, the training and test texts would have to be split in meaningful ways, introducing the need for a vast array of language-specific text spliers, should the approach work on a wide range of languages.\ne unsupervised approach generally succeeded in separating languages by script when different scripts were involved. Other than that, it seems that the chosen morphological features, or possibly morphological features in general, are insufficient for the algorithm to separate languages effectively.\ne weakly supervised approach worked well on short texts and on difficult short texts, but less well on long texts, while still outperforming the clustering approach on long texts. e approach consistently outperforms the clustering approach and reaches scores in the vicinity of the scores achieved by the supervised approach, even surpassing the supervised approach in some cases. ese results are promising, but more thorough investigations have to be undertaken.\nIn conclusion, it can be said that some unsupervised (or weakly supervised) approaches can perform beer on the task of language segmentation on difficult and short texts. e presented weakly supervised approach does not only outperform the unsupervised clustering approach, it also achieves scores comparable to the scores achieved with the supervised approach.\nFuture work could concentrate on the reduction of the number of generated clusters, ideally geing down to one cluster per language; it would also be thinkable to prevent overly frequent language model switching by taking a word\u2019s context into account. Finally, the parameters could conceivably be adapted automatically. With an increased interest in the area of multilingual text processing lately, the emergence and evolution of the texts themselves will influence the direction of the work in that direction.\n\u201cIl est venu le temps des cath\u00e9drales le monde est entr\u00e9 dans un nouveau mill\u00e9naire\nL\u2019homme a voulu monter vers les \u00e9toiles \u00e9crire son histoire dans le verre ou dans la pierre\u201d\n\u2014 Gringoire\n66"}, {"heading": "8 Appendix", "text": ""}, {"heading": "8.1 Development data", "text": ""}, {"heading": "8.1.1 Latin script data", "text": "Karl Marx anses som en af de fire klassiske sociologer. Marx er epokeg\u00f8rende for den historiske videnskab. Og Marx spillede en vigtig rolle for den samtidige og eerf\u00f8lgende arbejderbev\u00e6gelse.\n1891, nach einer Tuberkuloseerkrankung Hopes, er\u00f6ffnete das Ehepaar ein modernes Lungensanatorium in Nordrach im Schwarzwald, das sie bis 1893 gemeinsam \u00fchrten. 1895 wurde die Ehe geschieden.\nSources: hps://da.wikipedia.org/wiki/Karl_Marx hps://de.wikipedia.org/wiki/Hope_Bridges_Adams_Lehmann"}, {"heading": "8.1.2 Mixed script data", "text": "Capitalism is an economic system and a mode of production in which trade, industries, and the means of production are largely or entirely privately owned. Private firms and proprietorships usually operate in order to generate profit, but may operate as private nonprofit organizations.\n\u0627\u06cc\u062f\u0648\u0644\u0648 .\u062f\u0634 \u0631\u0648\u0647\u0634\u0645 \u0646\u06cc\u0646\u0644 \u0645\u0633\u0627 \u0647\u0628 \u0627\u06cc\u0646\u062f \u0631\u062f \u06cc\u0644\u0648 \u062f\u0648\u0628 \u0641\u0648\u0646\u0627\u06cc\u0644\u0648\u0627 \u0686\u06cc\u0644\u06cc\u0627 \u0631\u06cc\u0645\u062f\u0627\u0644\u0648 \u0648\u0627 \u06cc\u0644\u0635\u0627 \u0645\u0627\u0646 \u062a\u0633\u0627 \u0631\u06cc\u0645\u062f\u0627\u0644\u0648 \u0641\u0641\u062e\u0645 \u0647\u06a9 \u062f\u0646\u062f\u0631\u06a9\u06cc\u0645 \u0628\u0627\u0637\u062e \u0627\u06cc\u062f\u0648\u0644\u0648 \u0627\u0631 \u0648\u0627 \u06a9\u0633\u06cc\u0631\u0628\u0645\u06cc\u0633 \u0631\u062f \u0647\u0641\u0631\u0645 \u0647\u062f\u0627\u0648\u0646\u0627\u062e \u06a9\u06cc \u0631\u062f \u060c\u0633\u06cc\u0631\u0627\u067e \u0646\u0648\u0645\u06a9 \u0632\u0627 \u0644\u0628\u0642 \u0644\u0627\u0633 \u06a9\u06cc \u06cc\u0646\u0639\u06cc \u06f1\u06f8\u06f7\u06f0 \u0644\u0627\u0633 \u0631\u062f \u0647\u06a9 \u062f\u0648\u0628 \u0641\u0648\u0646\u0627\u06cc\u0644\u0648\u0627 \u0647\u062f\u0627\u0648\u0646\u0627\u062e \u062f\u0646\u0632\u0631\u0641 \u0634\u0634 \u0632\u0627 \u062f\u0646\u0632\u0631\u0641 \u0646\u06cc\u0645\u0648\u0633 \u06a9\u06cc \u0634\u0631\u062f\u067e .\u062f\u06cc\u062f\u0631\u06af \u062f\u0644\u0648\u062a\u0645 \u062f\u0645\u0627\u0653 \u0631\u062f \u06a9\u0633\u0641\u0648\u0646 \u0621\u0627\u06cc\u0644\u0648\u0627 \u0645\u0627\u0646 \u0647\u0628 \u06cc\u06af\u0631\u0632\u0628 \u0631\u0647\u0634 \u062a\u0631\u0648\u0635 \u0647\u0628 \u0627\u0647\u062f\u0639\u0628 \u06cc\u0644\u0648 \u062f\u0648\u0628\u0646 \u0634\u06cc\u0628 \u06cc\u06a9\u0631\u0647\u0634 \u0646\u0627\u0645\u0632 \u0646\u0627\u0653 \u0631\u062f \u0647\u06a9 \u0627\u06af\u0644\u0648 \u062f\u0648\u0631 \u0644\u062d\u0627\u0633 \u0631\u062f \u0631\u06a9\u0641\u062a \u0632\u0631\u0637 \u0648 \u0627\u0647\u06cc\u0646\u0627\u0645\u0644\u0627 \u0647\u0628 \u0631\u0645\u0639 \u062a\u062f\u0645 \u0645\u0627\u0645\u062a \u0631\u062f \u0646\u06cc\u0646\u0644 \u062a\u0647\u062c \u0646\u06cc\u0645\u0647 \u0647\u0628\u0648 \u062f\u0648\u0628 \u06cc\u0646\u0627\u0645\u0644\u0627 \u06a9\u0634\u0632\u067e \u06a9\u06cc \u0631\u062a\u062e\u062f \u0634\u0631\u062f\u0627\u0645 \u0648 \u06cc\u0636\u0627\u06cc\u0631 \u0645\u0644\u0639\u0645 \u0648 \u0644\u0627\u0631\u0628\u06cc\u0644 \u06cc\u0627\u0648\u0698\u0631\u0648\u0628 \u0647\u062f\u0631\u062e \u0631\u062f \u06cc\u0644\u0648 \u062a\u0634\u0627\u062f \u06cc\u0646\u0627\u0634\u062e\u0631\u062f \u0644\u0627\u0644\u062f\u062a\u0633\u0627 \u0647\u0648\u0642 \u0648 \u062f\u0648\u0628 \u06cc\u0628\u0648\u062e \u062f\u0631\u06af\u0627\u0634 \u0646\u0627\u062a\u0633\u0631\u06cc\u0628\u062f \u0631\u062f \u0627\u06cc\u062f\u0648\u0644\u0648 .\u062a\u0633\u06cc\u0631\u06af\u0646\u06cc\u0645 \u0636\u0627\u0645\u063a\u0627 \u0647\u062f\u06cc\u062f \u0647\u0628 \u062f\u0648\u0628 \u0646\u0627\u0653 \u062f\u0648\u0644\u0648\u0645 \u0633\u06a9\u0631\u0627\u0645 \u0647\u06a9 \u06cc\u0646\u0627\u0645\u0644\u0627 .\u062f\u0648\u0628 \u06cc\u0630\u0648\u0645 \u06cc\u0627\u0647\u0686\u0628 \u0644\u0627\u062d \u0646\u06cc\u0639\nSources: hps://en.wikipedia.org/wiki/Capitalism hps://fa.wikipedia.org/wiki/\u0646\u06cc\u0646\u0644_\u0631\u06cc\u0645\u06cc\u062f\u0627\u0644\u0648"}, {"heading": "8.1.3 Twitter data", "text": "Twitter 1 \u00bbFallo ergo sum\u00ab: On being wrong."}, {"heading": "Source:", "text": "Roland Hieber (daniel_bohrer). \u201c\u00bbFallo ergo sum\u00ab: On being wrong.\u201d. 26 July 2015, 16:47. Tweet.\n72\nTwitter 2 Music for Airports > le piano en libre-acc\u00e8s dans l\u2019a\u00e9roport Charles-deGaulles"}, {"heading": "Source:", "text": "Yannick Rochat (yrochat). \u201cMusic for Airports > le piano en libre-acc\u00e8s dans l\u2019a\u00e9roport Charles-de-Gaulles\u201d. 26 July 2015, 18:12. Tweet."}, {"heading": "8.1.4 Pali dictionary data", "text": "All entries have been taken from the Pali Text Society\u2019s Pali-English dictionary (T. W. Rhys Davids, William Stede, editors, e Pali Text Society\u2019s Pali\u2013English dictionary. Chipstead: Pali Text Society, 1921\u20135). 8 parts [738 pp.].)\nHambho Hambho\uff0c\uff08indecl.\uff09[ha\u1e41+bho] a particle expressing surprise or haughtiness J.I\uff0c184\uff0c494\uff0eSee also ambho\uff0e\uff08Page 729\uff09\nUssada Ussada\uff0c[most likely to ud + syad\uff1bsee ussanna]\uff1athis word is beset with difficulties\uff0cthe phrase sa-ussada is applied in all kinds of meanings\uff0cevidently the result of an original application & meaning having become obliterated\uff0esa\u00b0 is taken as *sapta\uff08seven\uff09as well as *sava\uff08being\uff09\uff0cussada as prominence\uff0cprotuberance\uff0c fulness\uff0carrogance\uff0eemeanings may be tabulated as follows\uff1a\uff081\uff09prominence\uff08cp\uff0e Sk\uff0eutsedha\uff09\uff0cused in characterisation of the Nirayas\uff0cas\u201cprojecting\uff0cprominent hells\u201d\uff0cussadaniray\u0101\uff08but see also below 4\uff09J\uff0eI\uff0c174\uff1bIV\uff0c3\uff0c422\uff08palla\u1e45ka\u1e41\uff0c v\uff0el\uff0ecaturass\u1ea1\u1e41\uff0cwith four corners\uff09\uff1bV\uff0c266\uff0e\u2013 adj\uff0eprominent A\uff0e13\uff08tejussadehi ariyamaggadhammehi\uff0cor as below 4?\uff09\uff0e\u2013 2\uff0eprotuberance\uff0cbump\uff0cswelling J\uff0eIV\uff0c188\uff1balso in phrase saussada having 7 protuberances\uff0ca qualification of the Mah\u0101purisa D\uff0eIII\uff0c151\uff08viz\uff0eon both hands\uff0cfeet\uff0cshoulders\uff0cand on his back\uff09\uff0e \u2013 3\uff0erubbing in\uff0canointing\uff0cointment\uff1badj\uff0eanointed with\uff08-\u00b0\uff09\uff0cin candan\u00b0 J\uff0eIII\uff0c 139\uff1bIV\uff0c60\uff1b\uff0e1\uff0c267\uff1bVv 537\uff1bDhA\uff0eI\uff0c28\uff1bVvA\uff0e237\uff0e\u2013 4\uff0ea crowd adj\uff0efull of\uff08-\u00b0\uff09in phrase saussada crowded with\uff08human beings\uff09D\uff0eI\uff0c87\uff08cp\uff0eDA\uff0eI\uff0c 245\uff1aaneka-saa-sam\u0101ki\u1e47\u1e47a\uff1bbut in same sense BSk\uff0esapt-otsada Divy 620\uff0c621\uff09\uff1bPv IV\uff0e18\uff08of Niraya = full of beings\uff0cexpld\uff0eby saehi ussanna upar\u00fbpari nicita PvA\uff0e 221\uff0e\u2013 5\uff0equalification\uff0ccharacteristic\uff0cmark\uff0ca\ue03cribute\uff0cin catussada\u201chaving the four qualifications\uff08of a good village\uff09\u201dJ\uff0eIV\uff0c309\uff08viz\uff0eplenty of people\uff0ccorn\uff0c wood and water C\uff0e\uff09\uff0ee phrase is evidently shaped aer D\uff0eI\uff0c87\uff08under 4\uff09\uff0eAs \u201cpreponderant quality\uff0ccharacteristic\u201dwe find ussada used at Vism\uff0e103\uff08cf\uff0eAsl\uff0e 267\uff09in combns\uff0elobh\u00b0\uff0cdos\u00b0\uff0cmoh\u00b0\uff0calobh\u00b0 etc\uff0e\uff08quoted from the\u201cUssadakiana\u201d\uff09\uff0c and similarly at VvA\uff0e19 in Dhammap\u0101la\u2019s definition of manussa\uff08lobh\u2019\u0101d\u012bhi alobh\u2019 \u0101d\u012bhi sahitassa manassa ussannat\u0101ya manuss\u0101\uff09\uff0cviz\uff0esa\u0101 manussa-j\u0101tik\u0101 tesu lobh\u2019 \u2039-\u203a \u0101dayo alobh\u2019\u0101dayo ca ussad\u0101\uff0e\u2013 6\uff0e\uff08metaph\uff0e\uff09self-elevation\uff0carrogance\uff0cconceit\uff0c haughtiness Vin\uff0eI\uff0c3\uff1bSn\uff0e515\uff0c624\uff08an\u00b0 = ta\u1e47h\u0101-ussada-abh\u0101vena SnA 467\uff09\uff0c783\n73\n\uff08expld\uff0eby Nd1 72 under formula saussada\uff1bi\uff0ee\uff0eshowing 7 bad qualities\uff0cviz\uff0er\u0101ga\uff0c dosa\uff0cmoha etc\uff0e\uff09\uff0c855\uff0e\u2013 See also uss\u0101dana\uff0cuss\u0101deti etc\uff0e\uff08Page 157\uff09"}, {"heading": "8.2 Test data", "text": ""}, {"heading": "8.2.1 Latin script data", "text": "English - German e German word Nabelschau means \u201dnavel-gazing\u201d or \u201dstaring at your navel\u201d. But in this case, it doesn\u2019t refer to anyone else\u2019s belly buon \u2013 just your own."}, {"heading": "Source:", "text": "Glass, Nicole (2015): \u201dGerman Missions in the United States - Word of the Week\u201d. Germany.info.\nEnglish - Fren doux, mou : both translate as \u201dso\u201d in English, although theirmeaning is very different. Doux is the opposite of \u201drough\u201d or \u201dcoarse\u201d (rugueux), while mou is the opposite of \u201dhard\u201d. Doux can also mean sweet, but almost only for wines (otherwise sucr\u00e9 is used)."}, {"heading": "Source:", "text": "Maciamo, (2015): \u201dFrench words and nuances that don\u2019t exist in English\u201d. Eupedia.\nEnglish - Transliterated Greek e Greek language distinguishes at least four different ways as to how the word love is used. Ancient Greek has four distinct words for love: ag\u00e1pe, \u00e9ros, phil\u00eda, and storg\u0113. However, as with other languages, it has been historically difficult to separate the meanings of these words when used outside of their respective contexts. Nonetheless, the senses in which these words were generally used are as follows.\nSource: hps://en.wikipedia.org/wiki/Greek_words_for_love\nItalian - German Milano ne custodisce l\u2019esempio pi\u00f9 struggente: quel Cenacolo che il vinciano affresc\u00f2 con amore, cura e rivoluzionaria psicologia (il Giuda non viene privato dell\u2019aureola, ma si condanna da solo, con la consapevolezza del peccato) cominci\u00f2 subito ad autodistruggersi, con un cancro che solo un lunghissimo restauro ha di recente arginato.\nKaum eine Woche vergeht, in der es keine neue Studie, Umfrage oder Warnung zumema Fachkr\u00e4emangel in Deutschland gibt.\nCerto, lo faceva per definire le idee, ma anche perch\u00e9 consapevole che le intuizioni sono periture, che la vita stessa va caurata in qualche modo.\n74\nDabei mehren sich letzter Zeit auch Stimmen, die Entwarnung geben. So kam j\u00fcngst eine Studie des Stierverbands \u00fcr die Deutsche Wissenscha zu dem Ergebnis, dass \u201dein allgemeiner Fachkr\u00e4emangel in den MINT-Berufen eher nicht mehr\u201d drohe.\nCome anche i riccioli del Baista richiamano il movimento delle acque, moto che poi Leonardo studier\u00e0 pi\u00f9 approfonditamente a Venezia, nelle ricerche sui bacini in chiave di difesa anti-Turchi. E si vada alla bellissima Annunciazione, con un occhio aento alle ali dell\u2019angelo: la delicatezza delle punte all\u2019ins\u00f9 che cosa sono se non il barbaglio di un sogno che lo ossessionava da anni, ovvero quello di volare?\nIst das seit Jahren angemahnte Szenario vom drohenden Fachkr\u00e4emangel bei Ingenieuren und Naturwissenschalern also nur ein Mythos?"}, {"heading": "Source:", "text": "Stalinski, Sandra (2015): \u201dIngenieure: Mythos Fachkr\u00e4emangel?\u201d. tagesschau.de. Scorranese, Roberta (2015): \u201dNelle grandi opere il racconto sofferto della natura mortale\u201d. Archiviostorico.corriere.it.\nGerman - Finnish - Turkish Der Sommer ist die w\u00e4rmste der vier Jahreszeiten in der gem\u00e4\u00dfigten und arktischen Klimazone. Je nachdem, ob er gerade auf der Nord- oder S\u00fcdhalbkugel herrscht, spricht man vom Nord- oder S\u00fcdsommer. Der Nordsommer findet gleichzeitig mit dem S\u00fcdwinter sta.\nKes\u00e4 eli suvi on vuodenaika kev\u00e4\u00e4n ja syksyn v\u00e4liss\u00e4. Kes\u00e4 on vuodenajoista l\u00e4mpimin, koska maapallo on silloin kallistunut niin, e\u00e4 aurinko s\u00e4teilee maan pinnalle jyrkemm\u00e4ss\u00e4 kulmassa kuin muina vuodenaikoina. Pohjoisella pallonpuoliskolla kes\u00e4kuukausiksi lasketaan tavallisesti kes\u00e4-. hein\u00e4- ja elokuu, etel\u00e4isell\u00e4 pallonpuoliskolla joulu-, tammi- ja helmikuu.\nYaz, en s\u0131cak mevsimdir. Kuzey Yar\u0131m K\u00fcre\u2019de en uzun g\u00fcnler yazda ger\u00e7ekle\u015fir. D\u00fcnya \u0131s\u0131y\u0131 depo ei\u011fi i\u00e7in en s\u0131cak g\u00fcnler genellikle yakla\u015f\u0131k iki ay sonra ortaya \u00e7\u0131kar. S\u0131cak g\u00fcnler Kuzey Yar\u0131m K\u00fcre\u2019de 21 Haziran ile 22 Eyl\u00fcl aras\u0131nda, G\u00fcney Yar\u0131m K\u00fcre\u2019de ise 22 Aral\u0131k ile 21 Mart aras\u0131ndad\u0131r.\nSource: h\ue03cps://fi.wikipedia.org/wiki/Kes\u00e4 hps://de.wikipedia.org/wiki/Sommer hps://tr.wikipedia.org/wiki/Yaz"}, {"heading": "8.2.2 Mixed script data", "text": "Greek - Russian \u0397 \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1 \u03b5\u03af\u03bd\u03b1\u03b9 \u03bc\u03af\u03b1 \u03b1\u03c0\u03cc \u03c4\u03b9\u03c2 \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ad\u03c2 \u03b3\u03bb\u03ce\u03c3\u03c3\u03b5\u03c2. \u0391\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03af \u03c4\u03bf \u03bc\u03bf\u03bd\u03b1\u03b4\u03b9\u03ba\u03cc \u03bc\u03ad\u03bb\u03bf\u03c2 \u03b5\u03bd\u03cc\u03c2 \u03b1\u03bd\u03b5\u03be\u03ac\u03c1\u03c4\u03b7\u03c4\u03bf\u03c5 \u03ba\u03bb\u03ac\u03b4\u03bf\u03c5 \u03c4\u03b7\u03c2 \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ae\u03c2 \u03bf\u03b9\u03ba\u03bf\u03b3\u03ad\u03bd\u03b5\u03b9\u03b1\u03c2 \u03b3\u03bb\u03c9\u03c3\u03c3\u03ce\u03bd. \u0391\u03bd\u03ae\u03ba\u03b5\u03b9 \u03b5\u03c0\u03af\u03c3\u03b7\u03c2 \u03c3\u03c4\u03bf\u03bd \u03b2\u03b1\u03bb\u03ba\u03b1\u03bd\u03b9\u03ba\u03cc \u03b3\u03bb\u03c9\u03c3\u03c3\u03b9\u03ba\u03cc \u03b4\u03b5\u03c3\u03bc\u03cc. \u03a3\u03c4\u03b7\u03bd \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03b3\u03c1\u03b1\u03c0\u03c4\u03ac \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03b1 \u03b1\u03c0\u03cc \u03c4\u03bf\u03bd 15\u03bf \u03b1\u03b9\u03ce\u03bd\u03b1 \u03c0.\u03a7. \u03bc\u03ad\u03c7\u03c1\u03b9 \u03c3\u03ae\u03bc\u03b5\u03c1\u03b1.\n\u041d\u0430 \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435 \u043d\u0430 \u0432\u0441\u0435\u0445 \u044d\u0442\u0430\u043f\u0430\u0445 \u0435\u0433\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f \u0431\u044b\u043b\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u0430 \u0431\u043e\u0433\u0430\u0442\u0435\u0439\u0448\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430. \u0412 \u0420\u0438\u043c\u0441\u043a\u043e\u0439 \u0438\u043c\u043f\u0435\u0440\u0438\u0438 \u0437\u043d\u0430\u043d\u0438\u0435 \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c \u043e\u0431\u044f\u0437\u0430-\n75\n\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u0441\u044f\u043a\u043e\u0433\u043e \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430. \u0412 \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0445 \u0437\u0430\u0438\u043c\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0439, \u0430 \u0432 \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u043c \u2014\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u0445 \u0438 \u0440\u043e\u043c\u0430\u043d\u0441\u043a\u0438\u0445 \u0441\u043b\u043e\u0432. \u0412 \u043d\u043e\u0432\u043e\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u0440\u0435\u0432\u043d\u0435\u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a \u0441\u0442\u0430\u043b (\u043d\u0430\u0440\u044f\u0434\u0443 \u0441 \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u043c) \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043d\u043e\u0432\u044b\u0445 \u043d\u0430\u0443\u0447\u043d\u044b\u0445 \u0438 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u0440\u043c\u0438\u043d\u043e\u0432 (\u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u0430\u044f \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0430\u044f \u043b\u0435\u043a\u0441\u0438\u043a\u0430). \u0412 \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u043f\u0440\u043e\u043d\u0438\u043a\u0430\u043b\u0438 \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0434\u0432\u0443\u043c\u044f \u043f\u0443\u0442\u044f\u043c\u0438\u2014\u0447\u0435\u0440\u0435\u0437 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0443\u044e \u043b\u0435\u043a\u0441\u0438\u043a\u0443 \u0438 \u0447\u0435\u0440\u0435\u0437 \u0446\u0435\u0440\u043a\u043e\u0432\u043d\u043e\u0441\u043b\u0430\u0432\u044f\u043d\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a.\nSource: hps://el.wikipedia.org/wiki/\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae_\u03b3\u03bb\u03ce\u03c3\u03c3\u03b1 hps://ru.wikipedia.org/wiki/\u0413\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0439_\u044f\u0437\u044b\u043a\nEnglish - Greek - Transliterated Greek Ag\u00e1pe (\u1f00\u03b3\u03ac\u03c0\u03b7 ag\u00e1p\u0113) means \u201dlove: esp. brotherly love, charity; the love of God for man and of man for God.\u201d Agape is used in the biblical passage known as the \u201dlove chapter,\u201d 1 Corinthians 13, and is described there and throughout the New Testament as brotherly love, affection, good will, love, and benevolence. Whether the love given is returned or not, the person continues to love (even without any self-benefit). Agape is also used in ancient texts to denote feelings for one\u2019s children and the feelings for a spouse, and it was also used to refer to a love feast. It can also be described as the feeling of being content or holding one in high regard. Agape is used by Christians to express the unconditional love of God for his children. is type of love was further explained by omas Aquinas as \u201dto will the good of another.\u201d\n\u00c9ros (\u1f14\u03c1\u03c9\u03c2 \u00e9r\u014ds) means \u201dlove, mostly of the sexual passion.\u201d e Modern Greek word \u201derotas\u201d means \u201dintimate love.\u201d It can also apply to dating relationships as well as marriage. Plato refined his own definition: Although eros is initially felt for a person, with contemplation it becomes an appreciation of the beauty within that person, or even becomes appreciation of beauty itself. Plato does not talk of physical araction as a necessary part of love, hence the use of the word platonic to mean, \u201dwithout physical araction.\u201d\nIn the Symposium, the most famous ancient work on the subject, Plato has Socrates argue that eros helps the soul recall knowledge of beauty, and contributes to an understanding of spiritual truth, the ideal \u201dForm\u201d of youthful beauty that leads us humans to feel erotic desire \u2013 thus suggesting that even that sensually based love aspires to the non-corporeal, spiritual plane of existence; that is, finding its truth, just like finding any truth, leads to transcendence. Lovers and philosophers are all inspired to seek truth through the means of eros.\nSource: hps://en.wikipedia.org/wiki/Greek_words_for_love\n76\nEnglish - Spanish - Arabic A black ribbon is a symbol of remembrance or mourning. Wearing or displaying a black ribbon has been used for POW/MIA remembrance, mourning tragedies or as a political statement.\nEl cresp\u00f3n negro o lazo negro es un s\u00edmbolo utilizado por personas, estados, sociedades y organizaciones, representando un sentimiento pol\u00edtico-social en se\u00f1al de duelo.\n\u0646\u0645 \u0648 \u062a\u0627\u0645\u0644\u0643 \u0629\u064a\u0627\u0644 \u0629\u062c\u0627\u062d\u0644\u0627 \u0646\u0648\u062f \u0629\u062f\u062d\u0627\u0648 \u0629\u0631\u0638\u0646\u0628 \u0627\u0647\u062a\u0644\u0627\u0633\u0631 \u0644\u0642\u0646\u062a \u0646\u0627\u0654 \u064a\u063a\u0628\u0646\u064a \u0629\u0645\u0627\u0644\u0639\u0644\u0627 \u0646\u0627\u0654\u0641 \u0627\u0645\u0648\u0645\u0639\u0648 \u0646\u064a\u0639\u0645 \u0621\u064a\u0634 \u0646\u0639 \u0631\u0628\u0639\u064a \u064a\u0630\u0644\u0627 \u0645\u0633\u0631\u0644\u0627 \u064a\u0646\u0639\u064a \u0632\u0645\u0631\u0644\u0627 \u0645\u0647 \u062a\u0627\u0645\u0627\u0644\u0639\u0644\u0627 \u0645\u062f\u062e\u062a\u0633\u0627 \u0646\u0645 \u0631\u062b\u0643\u0627\u0654 \u0646\u0643\u0644\u0648 \u062a\u0627\u0645\u0627\u0644\u0639\u0644\u0627 \u0627\u0648\u0645\u062f\u062e\u062a\u0633\u0627\u0654 \u0642\u064a\u0631\u063a\u0627\u0654\u0644\u0627\u0648 \u0646\u064a\u064a\u0631\u0635\u0645\u0644\u0627 \u0621\u0627\u0645\u062f\u0642 \u0646\u0627\u0654 \u0641\u0648\u0631\u0639\u0645\u0644\u0627\nSource:\nhps://es.wikipedia.org/?title=Lazo_negro hps://en.wikipedia.org/wiki/Black_ribbon hps://ar.wikipedia.org/wiki/\u0632\u0645\u0631\nEnglish - Chinese - (Pinyin) e Chinese word for \u201dcrisis\u201d (simplified Chinese: \u5371 \u673a; traditional Chinese: \u5371\u6a5f; pinyin: w\u0113ij\u012b) is frequently invoked in Western motivational speaking because the word is composed of two Chinese characters that can represent \u201ddanger\u201d and \u201dopportunity\u201d. Some linguists have criticized this usage because the component pronounced j\u012b (simplified Chinese: \u673a; traditional Chinese: \u6a5f) has other meanings besides \u201dopportunity\u201d. In Chinese tradition, certain numbers are believed by some to be auspicious (\u5409\u5229) or inauspicious (\u4e0d\u5229) based on the Chinese word that the number name sounds similar to. e numbers 0, 6, 8, and 9 are believed to have auspicious meanings because their names sound similar to words that have positive meanings. Source: hps://en.wikipedia.org/w/index.php?title=Chinese_word_for_\u201dcrisis\u201d\nUkrainian - Russian \u0412\u0456\u0434\u0434\u0430\u0432\u043d\u0430 \u043d\u0430 \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457 \u0423\u043a\u0440\u0430\u0457\u043d\u0438 \u0456\u0441\u043d\u0443\u0432\u0430\u043b\u0438 \u0434\u0435\u0440\u0436\u0430\u0432\u0438 \u0441\u043a\u0456\u0444\u0456\u0432, \u0441\u0430\u0440\u043c\u0430\u0442\u0456\u0432, \u0433\u043e\u0442\u0456\u0432 \u0442\u0430 \u0456\u043d\u0448\u0438\u0445 \u043d\u0430\u0440\u043e\u0434\u0456\u0432, \u0430\u043b\u0435 \u0432\u0456\u0434\u043f\u0440\u0430\u0432\u043d\u0438\u043c \u043f\u0443\u043d\u043a\u0442\u043e\u043c \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0457 \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u043e\u0441\u0442\u0456 \u0439 \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0438 \u0432\u0432\u0430\u0436\u0430\u0454\u0442\u044c\u0441\u044f \u041a\u0438\u0457\u0432\u0441\u044c\u043a\u0430 \u0420\u0443\u0441\u044c 9\u201413 \u0441\u0442\u043e\u043b\u0456\u0442\u0442\u044f. \u041d\u0430 \u044e\u0433\u0435 \u043e\u043c\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0432\u043e\u0434\u0430\u043c\u0438 \u0427\u0451\u0440\u043d\u043e\u0433\u043e \u0438 \u0410\u0437\u043e\u0432\u0441\u043a\u043e\u0433\u043e \u043c\u043e\u0440\u0435\u0439. \u0418\u043c\u0435\u0435\u0442 \u0441\u0443\u0445\u043e\u043f\u0443\u0442\u043d\u0443\u044e \u0433\u0440\u0430\u043d\u0438\u0446\u0443 \u0441 \u0420\u043e\u0441\u0441\u0438\u0435\u0439, \u0411\u0435\u043b\u043e\u0440\u0443\u0441\u0441\u0438\u0435\u0439, \u041f\u043e\u043b\u044c\u0448\u0435\u0439, \u0421\u043b\u043e\u0432\u0430\u043a\u0438\u0435\u0439, \u0412\u0435\u043d\u0433\u0440\u0438\u0435\u0439, \u0420\u0443\u043c\u044b\u043d\u0438\u0435\u0439 \u0438 \u041c\u043e\u043b\u0434\u0430\u0432\u0438\u0435\u0439. Source: hps://uk.wikipedia.org/wiki/\u0423\u043a\u0440\u0430\u0457\u043d\u0430 Surgut-safari.ru, (2015): \u201d\u0421\u0442\u0440\u0430\u043d\u044b - Safari Tour\u201d.\n77"}, {"heading": "8.2.3 Twitter data", "text": "Tweet 1: Greek \u2013 English \u039c\u03cc\u03bb\u03b9\u03c2 \u03c8\u03ae\u03c6\u03b9\u03c3\u03b1 \u03b1\u03c5\u03c4\u03ae \u03c4\u03b7 \u03bb\u03cd\u03c3\u03b7 Internet of ings, \u03c3\u03c4\u03bf \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc BUSINESS IT EXCELLENCE."}, {"heading": "Source:", "text": "GaloTyri. \u201d\u039c\u03cc\u03bb\u03b9\u03c2 \u03c8\u03ae\u03c6\u03b9\u03c3\u03b1 \u03b1\u03c5\u03c4\u03ae \u03c4\u03b7 \u03bb\u03cd\u03c3\u03b7 Internet of ings, \u03c3\u03c4\u03bf \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc BUSINESS IT EXCELLENCE.\u201d. 19 June 2015, 12:06. Tweet\nTweet 2: English \u2013 Fren Demain #dhiha6 Keynote 18h @dhiparis \u201ce collective dynamics of science-publish or perish; is it all that counts?\u201d par David @chavalarias"}, {"heading": "Source:", "text": "Claudine Moulin (ClaudineMoulin). \u201dDemain #dhiha6 Keynote 18h @dhiparis \u201de collective dynamics of science-publish or perish; is it all that counts?\u201d par David @chavalarias\u201d. 10 June 2015, 17:35. Tweet.\nTweet 3: English \u2013 Fren Food and breuvages in Edmonton are ready to go, just waiting for the fans #FWWC2015 #bilingualism"}, {"heading": "Source:", "text": "HBS (HBS_Tweets). \u201dFood and breuvages in Edmonton are ready to go, just waiting for the fans #FWWC2015 #bilingualism\u201d. 6 June 2015, 23:29. Tweet.\nTweet 4: English \u2013 Polish my dad comes back from poland with two crates of strawberries, \u017cubr\u00f3wka and adidas jackets omg"}, {"heading": "Source:", "text": "katarzyne (wifeyriddim). \u201dmy dad comes back from poland with two crates of strawberries, \u017cubr\u00f3wka and adidas jackets omg\u201d. 8 June 2015, 08:49. Tweet.\nTweet 5: Transliterated Amharic \u2013 English Buna dabo naw (coffee is our bread)."}, {"heading": "Source:", "text": "eCodeswitcher. \u201dBuna dabo naw (coffee is our bread).\u201d. 9 June 2015, 02:12. Tweet."}, {"heading": "8.2.4 Pali dictionary data", "text": "All entries have been taken from the Pali Text Society\u2019s Pali-English dictionary (T. W. Rhys Davids, William Stede, editors, e Pali Text Society\u2019s Pali\u2013English dictionary. Chipstead: Pali Text Society, 1921\u20135. 8 parts [738 pp.].)\nabbha (nt.) [Vedic abhra nt. & later Sk. abhra m. \u201ddark cloud\u201d; Idg. *m\u030abhro, cp. Gr. <at>a)fro\\\\s</at> scum, froth, Lat. imber rain; also Sk. ambha water, Gr. <at>o)/mbros</at> rain, Oir ambu water]. A (dense & dark) cloud, a cloudy mass A <smallcaps>ii.</smallcaps> 53 = Vin <smallcaps>ii.</smallcaps> 295 = Miln 273 in\n78\nlist of to things that obscure moon\u2013 & sunshine, viz. <b>abbha\u014b mahik\u0101</b> (mahiy\u0101 A) <b>dh\u016b- marajo</b> (megho Miln), <b>R\u0101hu</b> . is list is referred to at SnA 487 & VvA 134. S <smallcaps>i.</smallcaps> 101 (\u00b0sama pabbata a mountain like a thunder\u2013cloud); J <smallcaps>vi.</smallcaps> 581 (abbha\u014b rajo acch\u0101desi); Pv <smallcaps>iv.</smallcaps> 3 <superscript>9</superscript> (n\u012bl\u00b0 = n\u012bla\u2013megha PvA 251). As f. <b>abbh\u0101</b> at Dhs 617 & DhsA 317 (used in sense of adj. \u201ddull\u201d; DhsA expl <superscript>s.</superscript> by val\u0101haka); perhaps also in <b>abbh\u0101maa</b> . <br /><b>\u2013k\u016b\u1e6da</b> the point or summit of a storm\u2013cloud  1, 1064; J <smallcaps>vi.</smallcaps> 249, 250; Vv 1 <superscript>1</superscript> (= val\u0101haka\u2013sikhara VvA 12). <b>\u2013ghana</b> a mass of clouds, a thick cloud It 64; Sn 348 (cp. SnA 348). <b>\u2013pa\u1e6dala</b> a mass of clouds DhsA 239. <b>\u2013mua</b> free from clouds Sn 687 (also as abbh\u0101mua Dh 382). <b>\u2013sa\u014bvil\u0101pa</b> thundering S <smallcaps>iv.</smallcaps> 289.\nabhijjhitar [n. ag. fr. abhijjhita in med. function] one who covets M <smallcaps>i.</smallcaps> 287 (T. abhijjh\u0101tar, v. l. \u00b0itar) = A <smallcaps>v.</smallcaps> 265 (T. \u00b0itar, v. l. \u00b0\u0101tar).\najja Ajja\uff0c& Ajj\u0101\uff08adv.\uff09[Vedic adya & ady\u0101\uff0ca + dy\u0101\uff0ca\u00b0 being base of demonstr. pron. \uff08see a3\uff09and dy\u0101 an old Loc. of dyaus\uff08see diva\uff09\uff0cthus\u201con this day\u201d] to-day\uff0cnow Sn.75\uff0c153\uff0c158\uff0c970\uff0c998\uff1bDh.326\uff1bJ.I\uff0c279\uff1bIII\uff0c425\uff08read bahuta\u1e41 ajj\u0101\uff1bnot with Kern\uff0cToev. s. v. as\u201cfood\u201d\uff09\uff1bPv.I\uff0c117\uff08= id\u0101ni PvA.59\uff09\uff1bPvA.6\uff0c 23\uff1bMhvs 15\uff0c64. \u2039-\u203a Freq. in phrase ajjatagge\uff08= ajjato + agge\uff08?\uff09or ajja-tagge\uff0c see agga3\uff09from this day onward\uff0chenceforth Vin.I\uff0c18\uff1bD.I\uff0c85\uff1bDA.I\uff0c235. \u2013k\u0101la\u1e41\uff08adv.\uff09this morning J.VI\uff0c180\uff1b\u2013divasa the present day Mhvs 32\uff0c23. \uff08Page 10\uff09\ng\u016bhan\u0101 G\u016bhan\u0101\uff0c\uff08f.\uff09[abstr\uff0efr\uff0eg\u016bhati]=g\u016bhan\u0101\uff08q\uff0ev.\uff09Pug\uff0e19\uff0eCp\uff0e pari\u00b0\uff0e\uff08Page 253\uff09\npacati Pacati\uff0c[Ved\uff0epacati\uff0cIdg\uff0e*peq\u01d4\u014d\uff0cAv\uff0epac-\uff1bObulg\uff0epeka to fry\uff0croast\uff0c Lith\uff0ckep\u016b bake\uff0cGr\uff0ep\u03adssw cook\uff0cp\u03adpwn ripe] to cook\uff0cboil\uff0croast Vin\uff0eIV\uff0c264\uff1b fig\uff0etorment in purgatory\uff08trs\uff0eand intrs\uff0e\uff09\uff1aNiraye pacitv\u0101 aer roasting in N\uff0eS\uff0e II\uff0c225\uff0cPvA\uff0e10\uff0c14\uff0e\u2013 ppr\uff0epacanto tormenting\uff0cGen\uff0epacato\uff08+Caus\uff0e p\u0101cayato\uff09D\uff0eI\uff0c52\uff08expld at DA\uff0eI\uff0c159\uff0cwhere read pacato for paccato\uff0cby pare da\u1e47\u1e0dena p\u012b\u1e37entassa\uff09\uff0e\u2013 pp\uff0epakka\uff08q\uff0ev\uff0e\uff09\uff0e\u2039-\u203a Caus\uff0epac\u0101peti & p\u0101ceti\uff08q\uff0ev\uff0e\uff09\uff0e \u2013 Pass\uff0epaccati to be roasted or tormented\uff08q\uff0ev\uff0e\uff09\uff0e\uff08Page 382\uff09\n79"}, {"heading": "8.3 Results", "text": ""}, {"heading": "8.3.1 N-Gram Language Models", "text": "For the n-gram language model approach, the identified language is indicated in parentheses. e language abbreviations are:\nAbbreviation Language\nAR Arabic DE German EL Greek EN English ES Spanish FI Finnish FR French IT Italian PL Polish RU Russian UK Ukrainian TR Turkish TrAM Transliterated Amharic TrEL Transliterated Greek ZH Chinese\nData: Latin script: German \u2013 English\n\u2022 (EN) own., belly, refer, buon, But, it, or, your, at, in, \u201dstaring, anyone, doesn\u2019t, else\u2019s, word, this\n\u2022 (FI) \u2013\n\u2022 (FR) case, just, means, navel\u201d.\n\u2022 (TrAM) e\n\u2022 (TrEL) to, German\n\u2022 (other) Nabelschau, \u201dnavel-gazing\u201d\n80\nData: Latin script: German \u2013 Finnish \u2013 Turkish \u2022 (DE) ob, oder, Sommer, und, Nord-, arktischen, der, Der, dem, gem\u00e4\u00dfigten, mit, er, S\u00fcdsommer., spricht, Jahreszeiten, S\u00fcdwinter, herrscht, w\u00e4rmste, vom, die, sta., nachdem, auf\n\u2022 (EN) ist, Nordsommer, Mart, in\n\u2022 (ES) en, depo\n\u2022 (FI) joulu-, kev\u00e4\u00e4n, suvi, on, eli, vuodenajoista, syksyn, koska, kes\u00e4-., kuin, Pohjoisella, man, helmikuu., tammi-, l\u00e4mpimin, hein\u00e4-, niin, maapallo, maan, pinnalle, Kes\u00e4, s\u00e4teilee, tavallisesti, vuodenaika, kallistunut, lasketaan, muina, ei\u011fi, jyrkemm\u00e4ss\u00e4, elokuu, v\u00e4liss\u00e4., e\u00e4, etel\u00e4isell\u00e4, silloin, ja, kulmassa\n\u2022 (FR) vier, Je\n\u2022 (PL) aurinko\n\u2022 (RU) 22, 21\n\u2022 (TR) yakla\u015f\u0131k, ortaya, genellikle, Eyl\u00fcl, S\u0131cak, \u00e7\u0131kar., Yaz, sonra, aras\u0131nda, Kuzey, G\u00fcney, Aral\u0131k, gerade, \u0131s\u0131y\u0131, ger\u00e7ekle\u015fir., K\u00fcre\u2019de, g\u00fcnler, i\u00e7in, findet, mevsimdir., aras\u0131ndad\u0131r., Haziran, iki, yazda, uzun, ise, ay, s\u0131cak, ile, Yar\u0131m, D\u00fcnya\n\u2022 (TrAM) Der\n\u2022 (other) Klimazone., gleichzeitig,kes\u00e4kuukausiksi, vuodenaikoina., pallonpuoliskolla,S\u00fcdhalbkugel\nData: Latin script: English \u2013 French \u2022 (EL) \u201dcoarse\u201d\n\u2022 (EN) but, both, for, while, wines, almost, sweet, of, although, only, is, \u201drough\u201d, used)., or, as, meaning, the, in, translate, \u201dhard\u201d., their, English, also, different., very\n\u2022 (ES) can\n\u2022 (FI) mean\n\u2022 (FR) opposite, Doux, doux, sucr\u00e9, :\n\u2022 (RU) \u201dso\u201d\n\u2022 (TrEL) mou\n\u2022 (other) (otherwise, (rugueux)\n81\nData: Latin script: English \u2013 Transliterated Greek\n\u2022 (EN) for, meanings, least, used, been, distinct, love, of, were, are, when, ag\u00e1pe, these, how, and, Greek, word, used., outside, ways, different, other, follows., words, respective, generally, However, is, with, it, at, as, historically, the, in, which, their\n\u2022 (ES) has, separate\n\u2022 (FR) language, senses, Ancient, languages, difficult, four\n\u2022 (IT) contexts.\n\u2022 (TrAM) \u00e9ros, e, love:\n\u2022 (TrEL) to, storg\u0113., phil\u00eda\n\u2022 (other) Nonetheless, distinguishes\nData: Latin script: Italian \u2013 German\n\u2022 (DE) drohe., geben., allgemeiner, Studie, j\u00fcngst, \u00fcr, Ergebnis, keine, kam, drohenden, oder, und, letzter, neue, Mythos?, Deutschland, Ist, sich, der, vergeht, studier\u00e0, Dabei, Studie, den, dem, auch, Entwarnung, dass, nur, eher, nicht, gibt., Umfrage, Woche, eine, Kaum, Jahren, bei, mehren, Stimmen, Deutsche, das, zum, mehr\u201d, angemahnte, \u201dein, Zeit, ein, So, vom, zu, die, seit, Warnung, Wissenscha\n\u2022 (EL) affresc\u00f2\n\u2022 (EN) moto, aento, a, in, ad, also\n\u2022 (ES) custodisce, cura, subito, Certo, Giuda, lo, del, difesa, con, definire, restauro, se, modo., la, arginato., recente, vada, movimento, Leonardo, Szenario, quel, cominci\u00f2\n\u2022 (FI) va, si, Baista, ema\n\u2022 (FR) l\u2019esempio, non, des, acque, perch\u00e9, un, es, le, sui, condanna\n\u2022 (IT) solo, faceva, caurata, chiave, peccato), periture, (il, delicatezza, cancro, privato, bellissima, anni, bacini, ovvero, delle, sogno, di, barbaglio, ma, qualche, e, amore, ricerche, Come, per, richiamano, ne, intuizioni, punte, occhio, struggente:, nelle, vita, riccioli, solo, che, volare?, sono, alla, alle, anche, Cenacolo, quello, cosa, ali, viene, il, psicologia, vinciano, Venezia\n82\n\u2022 (PL) i\n\u2022 (TR) ha, pi\u00f9, da\n\u2022 (TrAM) Milano, E\n\u2022 (TrEL) poi, idee, stessa\n\u2022 (other)MINT-Berufen, Fachkr\u00e4emangel, dell\u2019angelo:, consapevole, anti-Turchi., Annunciazione, lunghissimo, consapevolezza, ossessionava, dell\u2019aureola, approfonditamente, autodistruggersi, rivoluzionaria, Stierverbands, all\u2019ins\u00f9, Naturwissenschalern, Ingenieuren\nData: Mixed script: Greek \u2013 Russian\n\u2022 (EL) \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03b1, \u03b2\u03b1\u03bb\u03ba\u03b1\u03bd\u03b9\u03ba\u03cc, \u03b1\u03c0\u03cc, \u03c4\u03bf, \u03b1\u03b9\u03ce\u03bd\u03b1, \u0391\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03af, \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae, \u03bc\u03af\u03b1, \u03b5\u03c0\u03af\u03c3\u03b7\u03c2, \u03c3\u03c4\u03bf\u03bd, \u03b3\u03bb\u03c9\u03c3\u03c3\u03b9\u03ba\u03cc, \u03b3\u03bb\u03c9\u03c3\u03c3\u03ce\u03bd., \u03b5\u03af\u03bd\u03b1\u03b9, \u03a3\u03c4\u03b7\u03bd, \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5, \u03bc\u03ad\u03bb\u03bf\u03c2, \u03b1\u03bd\u03b5\u03be\u03ac\u03c1\u03c4\u03b7\u03c4\u03bf\u03c5, \u03c4\u03b9\u03c2, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b5\u03c2., 15\u03bf, \u0391\u03bd\u03ae\u03ba\u03b5\u03b9, \u03b3\u03c1\u03b1\u03c0\u03c4\u03ac, \u03c0.\u03a7., \u03c3\u03ae\u03bc\u03b5\u03c1\u03b1., \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03ba\u03bb\u03ac\u03b4\u03bf\u03c5, \u03bf\u03b9\u03ba\u03bf\u03b3\u03ad\u03bd\u03b5\u03b9\u03b1\u03c2, \u03c4\u03bf\u03bd, \u03c4\u03b7\u03c2, \u03b4\u03b5\u03c3\u03bc\u03cc., \u03bc\u03ad\u03c7\u03c1\u03b9, \u03bc\u03bf\u03bd\u03b1\u03b4\u03b9\u03ba\u03cc, \u03b5\u03bd\u03cc\u03c2\n\u2022 (RU) \u0441\u043b\u043e\u0432., \u0441, \u0431\u043e\u0433\u0430\u0442\u0435\u0439\u0448\u0430\u044f, \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e, \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430., \u044d\u0442\u0430\u043f\u0430\u0445, \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435, \u0437\u043d\u0430\u043d\u0438\u0435, \u043d\u0430\u0443\u0447\u043d\u044b\u0445, \u043b\u0435\u043a\u0441\u0438\u043a\u0430)., \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u0430\u044f, \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445, \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c, \u0441\u0442\u0430\u043b, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u0445, \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f, \u0441\u043b\u043e\u0432\u0430, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u043c, \u0432\u0441\u0435\u0445, \u2014, \u0412, \u0440\u043e\u043c\u0430\u043d\u0441\u043a\u0438\u0445, \u043d\u043e\u0432\u044b\u0445, \u0420\u0438\u043c\u0441\u043a\u043e\u0439, \u0438, \u043f\u0440\u043e\u043d\u0438\u043a\u0430\u043b\u0438, \u0432, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0435, \u0442\u0435\u0440\u043c\u0438\u043d\u043e\u0432, \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0445, \u043d\u043e\u0432\u043e\u0435, \u0440\u0443\u0441\u0441\u043a\u0438\u0439, \u0438\u043c\u043f\u0435\u0440\u0438\u0438, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u043e\u043c, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430., \u0441\u043e\u0437\u0434\u0430\u043d\u0430, \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f, \u043f\u0443\u0442\u044f\u043c\u0438, \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c, \u044f\u0437\u044b\u043a., \u044f\u0437\u044b\u043a, (\u0442\u0430\u043a, \u0435\u0433\u043e, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e, \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c, \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c, \u0432\u0440\u0435\u043c\u044f, \u0434\u0432\u0443\u043c\u044f, \u0431\u044b\u043b\u0430, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e, \u0431\u043e\u043b\u044c\u0448\u043e\u0435, \u044f\u0437\u044b\u043a\u0435, \u044f\u0437\u044b\u043a\u0430\n\u2022 (TrAM) \u0397\n\u2022 (UK) \u043b\u0435\u043a\u0441\u0438\u043a\u0443, (\u043d\u0430\u0440\u044f\u0434\u0443, \u0447\u0435\u0440\u0435\u0437, \u0432\u0441\u044f\u043a\u043e\u0433\u043e, \u0430, \u041d\u0430, \u0434\u043b\u044f, \u043d\u0430\n\u2022 (other) \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ad\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ae\u03c2,\u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u043c), \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0443\u044e, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0430\u044f, \u0446\u0435\u0440\u043a\u043e\u0432\u043d\u043e\u0441\u043b\u0430\u0432\u044f\u043d\u0441\u043a\u0438\u0439, \u0437\u0430\u0438\u043c\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0439, \u0434\u0440\u0435\u0432\u043d\u0435\u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0439\nData: Mixed script: English \u2013 Greek\n\u2022 (DE) Symposium, Modern, being, felt\n\u2022 (EL) \u201dForm\u201d\n83\n\u2022 (EN) sensually, platonic, for, holding, existence;, refined, its, explained, araction, of, (even, are, spiritual, given, refer, Agape, beauty, or, araction.\u201d, like, without, not, further, will, own, love, knowledge, will, one\u2019s, most, use, express, is, another.\u201d, e, leads, truth, suggesting, dating, relationships, inspired, \u201dlove, mostly, hence, definition:, regard., appreciation, a, ideal, us, helps, seek, Ag\u00e1pe, plane, recall, feeling, within, returned, chapter,\u201d, based, described, apply, physical, Although, good, by, used, love, God.\u201d, children., his, any, charity;, Socrates, be, work, throughout, and, that, Greek, even, word, ag\u00e1p\u0113), love.\u201d, known, biblical, feelings, does, famous, In, subject, becomes, one, understanding, children, \u201dlove, through, beauty, well, It, was, initially, feast., finding, itself., 13, all, \u201dwithout, feel, with, is, it, thus, New, as, the, brotherly, in, is, an, there, God, youthful, necessary, high, Lovers, also, Whether\n\u2022 (ES) person, Aquinas, esp., continues, has, omas, truth, can, erotic, sexual, desire\n\u2022 (FI) on, \u2013, man, mean\n\u2022 (FR) (\u1f00\u03b3\u03ac\u03c0\u03b7, spouse, not, ancient, marriage., soul, person, content, Christians, Testament, \u00c9ros, just, part, type, passage, means, humans, passion.\u201d, aspires, contemplation, contributes, argue, affection\n\u2022 (IT) texts, 1, \u201dintimate, Plato, \u201dto\n\u2022 (RU) (\u1f14\u03c1\u03c9\u03c2\n\u2022 (TR) talk\n\u2022 (TrAM) \u00e9r\u014ds), \u201dlove:\n\u2022 (TrEL) \u201derotas\u201d, denote, eros., to, eros\n\u2022 (other) non-corporeal, Corinthians, self-benefit)., benevolence., unconditional, philosophers, transcendence.\nData: Mixed script: English \u2013 Spanish \u2013 Arabic\n\u2022 (AR) ,\u062a\u0627\u0645\u0644\u0643 ,\u0646\u0645 ,\u0621\u0627\u0645\u062f\u0642 ,\u0629\u062f\u062d\u0627\u0648 ,\u062a\u0627\u0645\u0627\u0644\u0639\u0644\u0627 ,\u0629\u0645\u0627\u0644\u0639\u0644\u0627 ,\u064a\u063a\u0628\u0646\u064a ,\u0648 ,\u0627\u0648\u0645\u062f\u062e\u062a\u0633\u0627\u0654 ,\u0631\u062b\u0643\u0627\u0654 ,\u0629\u064a\u0627\u0644 ,\u0646\u0648\u062f ,\u0646\u0627\u0654\u0641 ,\u0645\u0633\u0631\u0644\u0627 ,\u064a\u0630\u0644\u0627 ,\u0627\u0647\u062a\u0644\u0627\u0633\u0631 ,\u0646\u064a\u064a\u0631\u0635\u0645\u0644\u0627 \u0629\u062c\u0627\u062d\u0644\u0627 ,\u0646\u064a\u0639\u0645 ,\u0642\u064a\u0631\u063a\u0627\u0654\u0644\u0627\u0648 ,\u0627\u0645\u0648\u0645\u0639\u0648 ,\u0644\u0642\u0646\u062a ,\u0641\u0648\u0631\u0639\u0645\u0644\u0627 ,\u0629\u0631\u0638\u0646\u0628 ,\u0646\u0639 ,\u0631\u0628\u0639\u064a ,\u0645\u062f\u062e\u062a\u0633\u0627 ,\u064a\u0646\u0639\u064a ,\u0621\u064a\u0634 ,\u0646\u0627\u0654 ,\u0645\u0647 ,\u0632\u0645\u0631\u0644\u0627 ,\u0646\u0643\u0644\u0648\n\u2022 (EN) for, used, been, displaying, of, ribbon, black, or, mourning., statement., tragedies, is, political, a, Wearing, as, mourning\n\u2022 (ES) por, has, cresp\u00f3n, sociedades, personas, sentimiento, representando, estados, de, El, se\u00f1al, lazo, s\u00edmbolo, en, utilizado, y\n84\n\u2022 (FR) remembrance, remembrance, un, es\n\u2022 (IT) negro, duelo., POW/MIA\n\u2022 (TrAM)\n\u2022 (TrEL) symbol, o\n\u2022 (other) pol\u00edtico-social, organizaciones\nData: Mixed script: English \u2013 Chinese \u2022 (DE)\u673a;, Chinese:, Western\n\u2022 (EL)\u6a5f)\n\u2022 (EN) Some, for, meanings, by, of, are, 8, positive, speaking, be, composed, or, meanings., tradition, number, and, that, sound, linguists, word, some, this, other, In, have, invoked, criticized, 6, because, e, believed, words, numbers, sounds, frequently, is, pronounced, besides, traditional, the, in, represent, two, motivational, usage, their, based\n\u2022 (ES)\u5371\u6a5f;, has,\u5371\u673a;, can, Chinese, \u201dcrisis\u201d, similar\n\u2022 (FI) on\n\u2022 (FR) (\u5409\u5229), component, \u201ddanger\u201d, characters, (\u4e0d\u5229), certain, j\u012b\n\u2022 (PL) pinyin:\n\u2022 (RU) 0, 9, w\u0113ij\u012b)\n\u2022 (TrEL) to, to., names, name\n\u2022 (other) inauspicious, \u201dopportunity\u201d., (simplified, auspicious\nData: Mixed script: Ukrainian \u2013 Russian \u2022 (RU) \u041f\u043e\u043b\u044c\u0448\u0435\u0439, \u0420\u0443\u043c\u044b\u043d\u0438\u0435\u0439, \u0412\u0435\u043d\u0433\u0440\u0438\u0435\u0439, \u044e\u0433\u0435, \u0433\u0440\u0430\u043d\u0438\u0446\u0443, \u0441, \u043e\u043c\u044b\u0432\u0430\u0435\u0442\u0441\u044f, \u0418\u043c\u0435\u0435\u0442, 9 \u201413, \u041c\u043e\u043b\u0434\u0430\u0432\u0438\u0435\u0439., \u0410\u0437\u043e\u0432\u0441\u043a\u043e\u0433\u043e, \u0432\u043e\u0434\u0430\u043c\u0438, \u0420\u043e\u0441\u0441\u0438\u0435\u0439, \u0427\u0451\u0440\u043d\u043e\u0433\u043e, \u0420\u0443\u0441\u044c, \u0438, \u043f\u0443\u043d\u043a\u0442\u043e\u043c, \u0421\u043b\u043e\u0432\u0430\u043a\u0438\u0435\u0439\n\u2022 (TrAM) \u0439\n\u2022 (UK) \u0434\u0435\u0440\u0436\u0430\u0432\u0438, \u0441\u043a\u0456\u0444\u0456\u0432, \u0423\u043a\u0440\u0430\u0457\u043d\u0438, \u043d\u0430\u0440\u043e\u0434\u0456\u0432, \u041d\u0430, \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u043e\u0441\u0442\u0456, \u0432\u0432\u0430\u0436\u0430\u0454\u0442\u044c\u0441\u044f, \u0432\u0456\u0434\u043f\u0440\u0430\u0432\u043d\u0438\u043c, \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457, \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0457, \u0433\u043e\u0442\u0456\u0432, \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0438, \u0430\u043b\u0435, \u0441\u0430\u0440\u043c\u0430\u0442\u0456\u0432, \u0456\u0441\u043d\u0443\u0432\u0430\u043b\u0438, \u0441\u0442\u043e\u043b\u0456\u0442\u0442\u044f., \u041a\u0438\u0457\u0432\u0441\u044c\u043a\u0430, \u043d\u0430, \u0412\u0456\u0434\u0434\u0430\u0432\u043d\u0430, \u0456\u043d\u0448\u0438\u0445, \u0442\u0430, \u043c\u043e\u0440\u0435\u0439.\n\u2022 (other) \u0441\u0443\u0445\u043e\u043f\u0443\u0442\u043d\u0443\u044e, \u0411\u0435\u043b\u043e\u0440\u0443\u0441\u0441\u0438\u0435\u0439\n85\nData: Pali: abbha\n\u2022 (AR) ., 134., 289.\n\u2022 (DE) Miln), imber, dark), Miln\n\u2022 (EL) (=, (abbha\u014b\n\u2022 (EN) water, mountain, of, free, (used, or, like, referred, (also, A, is, cloudy, clouds, later, a, froth, 1, summit, thundering, by, mass, Pv, Oir, obscure, scum, that, water]., thick, As, from, It, is, at, as, the, in, clouds, things, also\n\u2022 (ES) (dense, f., sense, expl, rajo\n\u2022 (FI) 239., rain;, Lat., Vin, perhaps, SnA\n\u2022 (FR) cloud, Dh, adj., point, cloud, Dhs, A), rain, VvA, DhsA, list\n\u2022 (IT) \\\u201ddark, &, ambha, 3, 1, 317, J, sunshine, cp., abhra, [Vedic, (megho\n\u2022 (PL) 487, =, S, 295, <br, moon\u2013, 249\n\u2022 (RU) 348, 53\n\u2022 (TR) viz., ambu, Vv\n\u2022 (TrAM) 687, PvA, (\u00b0sama, 101, (n\u012bl\u00b0, (cp., 64;, (nt.), 581, m., Sn, 1064;\n\u2022 (TrEL) , Gr., Sk., Idg., to, pabbata, nt.\n\u2022 (UK) 12)., 273, 617, 348)., 250;, 251)., 382).\n\u2022 (other) <b> \u2013sa\u014bvil\u0101pa </b>, <b> \u2013mua </b>, <smallcaps> vi. </smallcaps>, (mahiy\u0101, <smallcaps> iv. </smallcaps>, cloud\\\u201d;, <b> R\u0101hu </b>, <b> abbh\u0101 </b>, <b> abbha\u014b, <superscript> 9 </superscript>, marajo </b>, abbh\u0101mua, val\u0101haka);, <smallcaps> i. </smallcaps>, <b> abbh\u0101maa </b>, val\u0101haka\u2013sikhara, <superscript> s. </superscript>, <smallcaps> ii. </smallcaps>, <b> dh\u016b-, storm\u2013 cloud, /><b> \u2013k\u016b\u1e6da </b>, thunder\u2013cloud);, <at>a)fro\\\\s</at>, <b>\u2013pa\u1e6dala</b>, <at>o)/mbros</at>, n\u012bla\u2013megha, <superscript>1</superscript>, *m\u030abhro, \\\u201ddull\\\u201d;, acch\u0101desi);, mahik\u0101</b>, <b> \u2013ghana </b>\n86\nData: Pali: abhijjhitar\n\u2022 (DE) v.\n\u2022 (EN) A, one, in, who, covets, med., function]\n\u2022 (IT) ag., M, fr.\n\u2022 (PL) 287, =\n\u2022 (RU) 265\n\u2022 (TrAM) l., [n.\n\u2022 (TrEL) (T.\n\u2022 (other) <smallcaps> v. </smallcaps>, abhijjh\u0101tar, abhijjhita, \u00b0\u0101tar)., <smallcaps> i. </smallcaps>, \u00b0itar, \u00b0itar)\nData: Pali: ajja\n\u2022 (DE)\uff08see, v., being, Ajj\u0101\n\u2022 (EN) of, or, and, not, present, Freq., day, this,\u201con, from, ady\u0101\uff0ca, with, as, the, morning, in, day\u201d], an\n\u2022 (ES) bahuta\u1e41,\n\u2022 (FI) 32\uff0c23., ajjato\n\u2022 (FR) Loc., dyaus, 15\uff0c64., dy\u0101, pron.\n\u2022 (IT) [Vedic, Mhvs, &, \u2013divasa\n\u2022 (PL)\uff08=, +, demonstr., s.\n\u2022 (RU) III\uff0c425, agge\uff08?\uff09\n\u2022 (TR) old, adya, 10\uff09, id\u0101ni\n\u2022 (TrAM) \u2039-\u203a\n\u2022 (TrEL) phrase, base\n\u2022 (UK) a3\uff09\n\u2022 (other) onward\uff0chenceforth, ajj\u0101\uff1b, DA.I\uff0c235.,\uff08adv.\uff09, J.I\uff0c279\uff1b, D.I\uff0c85\uff1b, ajja-tagge\uff0csee, Sn.75\uff0c153\uff0c158\uff0c970\uff0c998\uff1b, J.VI\uff0c180\uff1b, PvA.6\uff0c23\uff1b, \u2013k\u0101la\u1e41, diva\uff09\uff0cthus, PvA.59\uff09\uff1b, agga3\uff09, Kern\uff0cToev., Pv.I\uff0c117, Dh.326\uff1b, ajjatagge, \uff08read,\uff08Page, Vin.I\uff0c18\uff1b, dy\u0101\uff0ca\u00b0, Ajja\uff0c&, to-day\uff0cnow,\u201cfood\u201d\uff09\uff1b\n87\nData: Pali: g\u016bhan\u0101\n\u2022 (ES) 253\uff09\n\u2022 (other) [abstr\uff0efr\uff0eg\u016bhati]=g\u016bhan\u0101, Pug\uff0e19\uff0eCp\uff0epari\u00b0\uff0e\uff08Page, \uff08q\uff0ev.\uff09, G\u016bhan\u0101\uff0c\uff08f.\uff09\nData: Pali: pacati\n\u2022 (EL) 382\uff09\n\u2022 (EN) for, aer, roasting, read, roasted, be, or, at, tormented, in\n\u2022 (FR) pare, D\uff0eI\uff0c52\n\u2022 (IT) &, pacato, purgatory\n\u2022 (TrAM) p\u0101ceti, ripe]\n\u2022 (TrEL) to, da\u1e47\u1e0dena\n\u2022 (other) bake\uff0cGr\uff0ep\u03adssw,\uff08+Caus\uff0ep\u0101cayato\uff09,\uff08q\uff0ev\uff0e\uff09\uff0e\uff08Page, DA\uff0eI\uff0c159\uff0cwhere, Caus\uff0epac\u0101peti, intrs\uff0e\uff09\uff1aNiraye, pacitv\u0101, Pass\uff0epaccati,\uff08trs\uff0eand, tormenting\uff0c Gen\uff0epacato, p\u012b\u1e37entassa\uff09\uff0e\u2013, fig\uff0etorment, cook\uff0cp\u03adpwn, Pacati\uff0c[Ved\uff0epacati\uff0c Idg\uff0e*peq\u01d4\u014d\uff0cAv\uff0epac-\uff1b, paccato\uff0cby, ppr\uff0epacanto, cook\uff0cboil\uff0croast, fry\uff0c roast\uff0cLith\uff0ckep\u016b,\uff08q\uff0ev\uff0e\uff09\uff0e\u2013,\uff08expld, Vin\uff0eIV\uff0c264\uff1b, Obulg\uff0epeka, pp\uff0e pakka,\uff08q\uff0ev\uff0e\uff09\uff0e\u2039-\u203a, N\uff0eS\uff0eII\uff0c225\uff0cPvA\uff0e10\uff0c14\uff0e\u2013\nData: Twier 1 (Greek\u2013English)\n\u2022 (DE) Internet\n\u2022 (EL) \u03c3\u03c4\u03bf, \u03c4\u03b7, \u03b1\u03c5\u03c4\u03ae, \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc, \u03bb\u03cd\u03c3\u03b7, \u03c8\u03ae\u03c6\u03b9\u03c3\u03b1\n\u2022 (EN) of, IT, ings\n\u2022 (ES) BUSINESS\n\u2022 (TrAM) \u039c\u03cc\u03bb\u03b9\u03c2\n\u2022 (other) EXCELLENCE.\n88\nData: Twier 2 (French\u2013English)\n\u2022 (EN) David, \u201de, is, it, perish;, or, collective, Demain, counts?\u201d, that, of, dynamics, all\n\u2022 (FI) 18h\n\u2022 (FR) par, Keynote\n\u2022 (other) #dhiha6, @dhiparis, science-publish\nData: Twier 3 (French\u2013English)\n\u2022 (EN) for, Food, waiting, the, in, ready, and, are\n\u2022 (ES) go\n\u2022 (FI) Edmonton\n\u2022 (FR) just, breuvages, fans\n\u2022 (TrEL) to\n\u2022 (other) #bilingualism, #FWWC2015\nData: Twier 4 (English\u2013Polish)\n\u2022 (EN) with, back, from, comes, crates, and, poland, two, of, jackets\n\u2022 (ES) dad, adidas\n\u2022 (TrAM) my\n\u2022 (TrEL) omg\n\u2022 (other) \u017cubr\u00f3wka, strawberries\nData: Twier 5 (Transliterated Amharic\u2013English)\n\u2022 (EN) is, bread).\n\u2022 (FR) our\n\u2022 (IT) (coffee\n\u2022 (PL) naw\n\u2022 (TrAM) Buna, dabo\n89"}, {"heading": "8.3.2 Textcat", "text": "For Textcat, the identified language is indicated in parentheses. As Textcat returns unknown for many words, I merely indicate the non-unknown categories to save space and write rest to indicate that all other words of the text have been classified as unknown. e language abbreviations are:\nAbbreviation Language\nDA Danish DE German EL Greek EN English ES Spanish FI Finnish FR French HU Hungarian ID Indonesian IT Italian LT Lithuanian LV Latvian NL Dutch PT Portuguese RU Russian TH ai ZH Chinese\nData: Latin script: German \u2013 English\n\u2022 (HU) \u201cnavel-gazing\u201d\n\u2022 (ZH) Nabelschau\n\u2022 (unknown) rest\nData: Latin script: German \u2013 Finnish \u2013 Turkish\n\u2022 (DA) S\u00fcdsommer., genellikle,\n\u2022 (DE) Jahreszeiten, arktischen,\n\u2022 (FI) vuodenajoista, kallistunut, tavallisesti,\n90\n\u2022 (ZH) gem\u00e4\u00dfigten, Klimazone., S\u00fcdhalbkugel, Nordsommer, gleichzeitig, vuodenaika, jyrkemm\u00e4ss\u00e4, vuodenaikoina., Pohjoisella, pallonpuoliskolla, kes\u00e4kuukausiksi, etel\u00e4isell\u00e4, mevsimdir., ger\u00e7ekle\u015fir., aras\u0131ndad\u0131r.,\n\u2022 (unknown) rest\nData: Latin script: English \u2013 French\n\u2022 (HU) different.,\n\u2022 (ZH) (rugueux),(otherwise,\n\u2022 (unknown) rest\nData: Latin script: English \u2013 Transliterated Greek\n\u2022 (EN) historically, respective,\n\u2022 (LT) languages,\n\u2022 (ZH) distinguishes, Nonetheless,\n\u2022 (unknown) rest\nData: Latin script: Italian \u2013 German\n\u2022 (DE) allgemeiner, angemahnte,\n\u2022 (ES) delicatezza,\n\u2022 (HU) bellissima,\n\u2022 (IT) dell\u2019aureola, consapevole, richiamano, anti-Turchi., ossessionava,\n\u2022 (NL) Ingenieuren,\n\u2022 (PT) approfonditamente,\n\u2022 (ZH) custodisce, struggente:, rivoluzionaria, psicologia, consapevolezza, autodistruggersi, lunghissimo, Fachkr\u00e4emangel, Deutschland, intuizioni, Entwarnung, Stierverbands, Wissenscha, MINT-Berufen, Annunciazione, dell\u2019angelo:, Naturwissenschalern,\n\u2022 (unknown) rest\n91\nData: Mixed script: Greek \u2013 Russian\n\u2022 (EL) \u03b1\u03bd\u03b5\u03be\u03ac\u03c1\u03c4\u03b7\u03c4\u03bf\u03c5, \u03bf\u03b9\u03ba\u03bf\u03b3\u03ad\u03bd\u03b5\u03b9\u03b1\u03c2,\n\u2022 (RU) \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f, \u0431\u043e\u0433\u0430\u0442\u0435\u0439\u0448\u0430\u044f, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430., \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e, \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c, \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e, \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e, \u0437\u0430\u0438\u043c\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0439, \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435, \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c, \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445, \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u0430\u044f, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0430\u044f,\n\u2022 (TH) \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u043c),\n\u2022 (ZH) \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ad\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ae\u03c2, \u0434\u0440\u0435\u0432\u043d\u0435\u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0439, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0443\u044e, \u0446\u0435\u0440\u043a\u043e\u0432\u043d\u043e\u0441\u043b\u0430\u0432\u044f\u043d\u0441\u043a\u0438\u0439,\n\u2022 (unknown) rest\nData: Mixed script: English \u2013 Greek\n\u2022 (DA) definition:, understanding,\n\u2022 (EN) affection, unconditional, suggesting,\n\u2022 (FR) relationships, contemplation, appreciation, araction, araction.\u201d, transcendence.,\n\u2022 (HU) benevolence., self-benefit).,\n\u2022 (IT) non-corporeal,\n\u2022 (PT) contributes,\n\u2022 (ZH) Corinthians, throughout, Christians, Symposium, existence;, philosophers,\n\u2022 (unknown) rest\nData: Mixed script: English \u2013 Spanish \u2013 Arabic\n\u2022 (ES) sociedades, organizaciones, sentimiento, pol\u00edtico-social,\n\u2022 (FR) remembrance, remembrance, statement.,\n\u2022 (ID) displaying,\n\u2022 (PT) representando,\n\u2022 (unknown) rest\n92\nData: Mixed script: English \u2013 Chinese\n\u2022 (EN) traditional, motivational, pronounced, tradition\u201e\n\u2022 (FR) characters,\n\u2022 (ZH) simplified, frequently, \u201dopportunity\u201d., criticized, auspicious, inauspicious,\n\u2022 (unknown) rest\nData: Mixed script: Ukrainian \u2013 Russian\n\u2022 (RU) \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u043e\u0441\u0442\u0456, \u0421\u043b\u043e\u0432\u0430\u043a\u0438\u0435\u0439, \u041c\u043e\u043b\u0434\u0430\u0432\u0438\u0435\u0439.,\n\u2022 (TH) \u0432\u0432\u0430\u0436\u0430\u0454\u0442\u044c\u0441\u044f,\n\u2022 (ZH) \u0432\u0456\u0434\u043f\u0440\u0430\u0432\u043d\u0438\u043c, \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0457, \u0441\u0443\u0445\u043e\u043f\u0443\u0442\u043d\u0443\u044e, \u0411\u0435\u043b\u043e\u0440\u0443\u0441\u0441\u0438\u0435\u0439,\n\u2022 (unknown) rest\nData: Pali: abbha\n\u2022 (DA) storm\u2013cloud, thundering,\n\u2022 (HU) marajo</b>, n\u012bla\u2013megha, val\u0101haka\u2013sikhara,\n\u2022 (ZH) <at> a)fro\\\\</at>, <at> o)/mbros </at>, <smallcaps> ii. </smallcaps>, mahik\u0101</b>, <b> R\u0101hu </b>, <smallcaps> i. </smallcaps>, thunder\u2013cloud);, <smallcaps> vi. </smallcaps>, acch\u0101desi);, <smallcaps> iv. </smallcaps>, <superscript> 9 </superscript>, <b> abbh\u0101 </b>, <superscript> s. </superscript>, val\u0101haka);, <b> abbh\u0101maa </b>, /><b> \u2013k\u016b\u1e6da </b>, <superscript> 1 </superscript>, <b> \u2013ghana </b>, <b> \u2013pa\u1e6dala </b>, <b> \u2013mua </b>, abbh\u0101mua, <b> \u2013sa\u014bvil\u0101pa </b>\n\u2022 (unknown) rest\nData: Pali: abhijjhitar\n\u2022 (ZH) abhijjhita, <smallcaps> i. </smallcaps>, abhijjh\u0101tar, <smallcaps> v. </smallcaps>,\n\u2022 (unknown) rest\n93\nData: Pali: ajja\n\u2022 (ZH) diva\uff09\uff0cthus, to-day\uff0cnow, Sn.75\uff0c153\uff0c158\uff0c970\uff0c998\uff1b, Kern\uff0cToev., ajja-tagge\uff0csee, onward\uff0chenceforth,\n\u2022 (unknown) rest\nData: Pali: g\u016bhan\u0101\n\u2022 (ZH) G\u016bhan\u0101\uff0c\uff08f.\uff09, [abstr\uff0efr\uff0eg\u016bhati]han\u0101, Pug\uff0e19\uff0eCp\uff0epari\u00b0\uff0e\uff08Page,\n\u2022 (unknown) rest\nData: Pali: pacati\n\u2022 (ZH) fig\uff0etorment, Pacati\uff0c[Ved\uff0epacati\uff0cIdg\uff0e*peq\u01d4\u014d\uff0cAv\uff0epac-\uff1b, Obulg\uff0epeka, fry\uff0croast\uff0cLith\uff0ckep\u016b, bake\uff0cGr\uff0ep\u03adssw, cook\uff0cp\u03adpwn, cook\uff0cboil\uff0croast, Vin\uff0e IV\uff0c264\uff1b, intrs\uff0e\uff09\uff1aNiraye, N\uff0eS\uff0eII\uff0c225\uff0cPvA\uff0e10\uff0c14\uff0e\u2013, ppr\uff0epacanto, tormenting\uff0cGen\uff0epacato,\uff08+Caus\uff0ep\u0101cayato\uff09, DA\uff0eI\uff0c159\uff0cwhere, paccato\uff0cby, p\u012b\u1e37entassa\uff09\uff0e\u2013,\uff08q\uff0ev\uff0e\uff09\uff0e\u2039-\u203a, Caus\uff0epac\u0101peti, Pass\uff0epaccati,\uff08q\uff0ev\uff0e\uff09\uff0e\uff08Page,\n\u2022 (unknown) rest\nData: Twier 1 (Greek\u2013English)\n\u2022 (ZH) \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc, EXCELLENCE.,\n\u2022 (unknown) rest\nData: Twier 2 (French\u2013English)\n\u2022 (IT) collective,\n\u2022 (ZH) science-publish,\n\u2022 (unknown) rest\nData: Twier 3 (French\u2013English)\n\u2022 (ZH) #bilingualism,\n\u2022 (unknown) rest\n94\nData: Twier 4 (English\u2013Polish)\n\u2022 (LV) strawberries,\n\u2022 (unknown) rest\nData: Twier 5 (Transliterated Amharic\u2013English)\n\u2022 (unknown) rest"}, {"heading": "8.3.3 Clustering", "text": "Clustering the different data sets produced the following clusters. e second run uses the clusters from the first run and possibly subdivides each cluster into two or more clusters.\nData: Latin script: German \u2013 English"}, {"heading": "First run", "text": "\u2022 \u201cnavel-gazing\u201d, doesn\u2019t, else\u2019s\n\u2022 \u201cstaring, But, German, Nabelschau, anyone, belly, buon, case, just, means, navel\u201d., own., refer, this, word, your\n\u2022 at, in, it, or, to\n\u2022 \u2013, e\nSecond run\n\u2022 doesn\u2019t, else\u2019s\n\u2022 \u201cnavel-gazing\u201d\n\u2022 \u201cstaring, But, German, Nabelschau, belly, case, means, navel\u201d., refer, this\n\u2022 anyone, buon, just, own., word, your\n\u2022 it, or, to\n\u2022 at, in\n\u2022 \u2013, e\n95\nData: Latin script: German \u2013 Finnish \u2013 Turkish"}, {"heading": "First run", "text": "\u2022 D\u00fcnya, G\u00fcney, K\u00fcre\u2019de, S\u00fcdhalbkugel, S\u00fcdsommer., S\u00fcdwinter, S\u0131cak, aras\u0131nda, gem\u00e4\u00dfigten, g\u00fcnler, i\u00e7in, kes\u00e4kuukausiksi, l\u00e4mpimin, s\u00e4teilee, s\u0131cak, w\u00e4rmste, \u00e7\u0131kar., Der\n\u2022 Aral\u0131k, Eyl\u00fcl, Kes\u00e4, Yar\u0131m, aras\u0131ndad\u0131r., etel\u00e4isell\u00e4, ei\u011fi, e\u00e4, ger\u00e7ekle\u015fir., hein\u00e4, jyrkemm\u00e4ss\u00e4, kes\u00e4-., kev\u00e4\u00e4n, v\u00e4liss\u00e4., yakla\u015f\u0131k, \u0131s\u0131y\u0131\n\u2022 21, 22\n\u2022 Der, Haziran, Jahreszeiten, Je, Klimazone., Kuzey, Mart, Nord-, Nordsommer, Pohjoisella, Sommer, Yaz, arktischen, auf, aurinko, ay, dem, depo, der, die, eli, elokuu, en, er, findet, genellikle, gerade, gleichzeitig, helmikuu., herrscht, iki, ile, in, ise, ist, ja, joulu-, kallistunut, koska, kuin, kulmassa, lasketaan, maan, maapallo, man, mevsimdir., mit, muina, nachdem, niin, ob, oder, on, ortaya, pallonpuoliskolla, pinnalle, silloin, sonra, spricht, sta., suvi, syksyn, tammi-, tavallisesti, und, uzun, vier, vom, vuodenaika, vuodenaikoina., vuodenajoista, yazda"}, {"heading": "Second run", "text": "\u2022 S\u00fcdhalbkugel, S\u00fcdsommer., S\u00fcdwinter, aras\u0131nda, gem\u00e4\u00dfigten, kes\u00e4kuukausiksi, l\u00e4mpimin, s\u00e4teilee, w\u00e4rmste\n\u2022 D\u00fcnya, G\u00fcney, K\u00fcre\u2019de, S\u0131cak, g\u00fcnler, i\u00e7in, s\u0131cak, \u00e7\u0131kar., Der\n\u2022 aras\u0131ndad\u0131r., etel\u00e4isell\u00e4, ei\u011fi, e\u00e4, ger\u00e7ekle\u015fir., hein\u00e4-, jyrkemm\u00e4ss\u00e4, kes\u00e4-., kev\u00e4\u00e4n, v\u00e4liss\u00e4., yakla\u015f\u0131k, \u0131s\u0131y\u0131\n\u2022 Aral\u0131k, Eyl\u00fcl, Yar\u0131m\n\u2022 Kes\u00e4\n\u2022 22\n\u2022 21\n\u2022 Der, Haziran, Jahreszeiten, Klimazone., Kuzey, Mart, Nord-, Nordsommer, Pohjoisella, Sommer, Yaz,\n96\n\u2022 arktischen, auf, aurinko, dem, depo, der, die, eli, elokuu, findet, genellikle, gerade, gleichzeitig, helmikuu., herrscht, iki, ile, ise, ist, joulu-, kallistunut, koska, kuin, kulmassa, lasketaan, maan, maapallo, man, mevsimdir., mit, muina, nachdem, niin, oder, ortaya, pallonpuoliskolla, pinnalle, silloin, sonra, spricht, sta., suvi, syksyn, tammi-, tavallisesti, und, uzun, vier, vom, vuodenaika, vuodenaikoina., vuodenajoista, yazda\n\u2022 Je, ay, en, er, in, ja, ob, on\nData: Latin script: English \u2013 French"}, {"heading": "First run", "text": "\u2022 \u201dcoarse\u201d, \u201dhard\u201d., \u201drough\u201d, \u201dso\u201d, (otherwise, (rugueux), Doux, English, almost, also, although, both, but, can, different., doux, for, mean, meaning, mou, only, opposite, sucr\u00e9, sweet, the, their, translate, used)., very, while, wines\n\u2022 is, or\n\u2022 as, in, of\nSecond run\n\u2022 Doux, English,\n\u2022 \u201ccoarse\u201d, (otherwise, (rugueux), almost, although, different., meaning, opposite, translate\n\u2022 \u201chard\u201d., \u201drough\u201d, \u201dso\u201d, also, both, but, can, doux, for, mean, mou, only, sucr\u00e9, sweet, the, their, used)., very, while, wines\n\u2022 or\n\u2022 is\n\u2022 in\n\u2022 of\n\u2022 as\n97\nData: Latin script: English \u2013 Transliterated Greek\nFirst run\n\u2022 e\n\u2022 ag\u00e1pe, phil\u00eda, storg\u0113., \u00e9ros,\n\u2022 Ancient, However, Nonetheless, contexts., different, difficult, distinct, distinguishes, follows., generally, historically, language, languages, meanings, outside, respective, senses, separate, which, words\n\u2022 Greek, and, are, as, at, been, for, four, has, how, in, is, it, least, love, love:, of, other, the, their, these, to, used, used., ways, were, when, with, word\nSecond run\n\u2022 e\n\u2022 phil\u00eda, storg\u0113.\n\u2022 ag\u00e1pe, \u00e9ros,\n\u2022 Ancient, However, Nonetheless, contexts., different, difficult, distinct, distinguishes, follows., generally, historically, meanings, respective\n\u2022 words\n\u2022 language, languages, outside, senses, separate, which\n\u2022 and, are, as, at, been, for, four, has, how, in, is, it, least, love, love:, of, other, the, their, these, to, used, used., ways, were, when, with, word\n\u2022 Greek\nData: Latin script: German \u2013 Italian"}, {"heading": "First run", "text": "\u2022 (il, E, So, a, ad, da, di, e, es, ha, i, il, in, la, le, lo, ma, ne, se, si, un, va, zu\n98\n\u2022 \u201cein , Annunciazione, Baista, Cenacolo, Certo, Come, Dabei, Deutsche, Deutschland, Entwarnung, Ergebnis, Giuda, Ingenieuren, Ist, Jahren, Kaum, Leonardo, MINT-Berufen, Mythos?, Naturwissenschalern, Stierverbands, Stimmen, Studie, Studie, Szenario, ema, Umfrage, Venezia, Warnung, Wissenscha, Woche, Zeit, acque, ali, alla, alle, allgemeiner, also, amore, anche, angemahnte, anni, anti-Turchi., approfonditamente, arginato., aento, auch, autodistruggersi, bacini, barbaglio, bei, bellissima, cancro, caurata, che, chiave, con, condanna, consapevole, consapevolezza, cosa, cura, custodisce, das, dass, definire, del, delicatezza, delle, dem, den, der, des, die, difesa, drohe., drohenden, eher, ein, eine, faceva, geben., gibt., idee, intuizioni, kam, keine, letzter, lunghissimo, mehr\u201d, mehren, modo., moto, movimento, nelle, neue, nicht, non, nur, occhio, oder, ossessionava, ovvero, peccato), per, periture, poi, privato, psicologia, punte, qualche, quel, quello, recente, restauro, riccioli, ricerche, richiamano, rivoluzionaria, seit, sich, sogno, solo, solo, sono, stessa, struggente:, subito, sui, und, vada, vergeht, viene, vinciano, vita, volare?, vom, zum\n\u2022 all\u2019ins\u00f9, dell\u2019angelo:, dell\u2019aureola, l\u2019esempio, Milano\n\u2022 Fachkr\u00e4emangel, affresc\u00f2, cominci\u00f2, \u00fcr, j\u00fcngst, perch\u00e9, pi\u00f9, studier\u00e0"}, {"heading": "Second run", "text": "\u2022 a, e, i\n\u2022 E\n\u2022 So\n\u2022 (il, ad, da, di, es, ha, il, in, la, le, lo, ma, ne, se, si, un, va, zu\n\u2022 Annunciazione, Baista, Cenacolo, Certo, Come, Dabei, Deutsche, Deutschland, Entwarnung, Ergebnis, Giuda, Ingenieuren, Ist, Jahren, Kaum, Leonardo, MINTBerufen, Mythos?, Naturwissenschalern, Stierverbands, Stimmen, Studie, Studie, Szenario, ema, Umfrage, Venezia, Warnung, Wissenscha, Woche, Zeit\n\u2022 \u201cein, acque, ali, alla, alle, allgemeiner, also, amore, anche, angemahnte, anni, anti-Turchi., approfonditamente, arginato., aento, auch, autodistruggersi, bacini, barbaglio, bei, bellissima, cancro, caurata, che, chiave, con, condanna, consapevole, consapevolezza, cosa, cura, custodisce, das, dass, definire, del, delicatezza, delle, dem, den, der, des, die, difesa, drohe., drohenden, eher, ein, eine, faceva, geben., gibt., idee, intuizioni, kam, keine, letzter, lunghissimo, mehr\u201d, mehren, modo., moto, movimento, nelle, neue, nicht, non, nur, occhio, oder, ossessionava, ovvero, peccato), per, periture, poi, privato, psicologia,\n99\npunte, qualche, quel, quello, recente, restauro, riccioli, ricerche, richiamano, rivoluzionaria, seit, sich, sogno, solo, solo, sono, stessa, struggente:, subito, sui, und, vada, vergeht, viene, vinciano, vita, volare?, vom, zum\n\u2022 all\u2019ins\u00f9, dell\u2019angelo:, dell\u2019aureola, l\u2019esempio, Milano\n\u2022 Fachkr\u00e4emangel\n\u2022 affresc\u00f2, cominci\u00f2, j\u00fcngst, perch\u00e9, studier\u00e0\n\u2022 \u00fcr\n\u2022 pi\u00f9\nData: Mixed script: Greek \u2013 Russian\nFirst run\n\u2022 15\u03bf,\u2014, \u0397\n\u2022 \u03c4\u03bf, \u0412, \u041d\u0430, \u0430, \u0432, \u0438, \u043d\u0430, \u0441\n\u2022 (\u043d\u0430\u0440\u044f\u0434\u0443, (\u0442\u0430\u043a, \u03b3\u03bb\u03c9\u03c3\u03c3\u03ce\u03bd., \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b5\u03c2., \u03b4\u03b5\u03c3\u03bc\u03cc., \u03c0.\u03a7., \u03c3\u03ae\u03bc\u03b5\u03c1\u03b1., \u0437\u0430\u0438\u043c\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0439, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u043c), \u043b\u0435\u043a\u0441\u0438\u043a\u0430)., \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430., \u0441\u043b\u043e\u0432., \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430., \u044f\u0437\u044b\u043a.\n\u2022 \u0391\u03bd\u03ae\u03ba\u03b5\u03b9, \u0391\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03af, \u03a3\u03c4\u03b7\u03bd, \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5, \u03b1\u03b9\u03ce\u03bd\u03b1, \u03b1\u03bd\u03b5\u03be\u03ac\u03c1\u03c4\u03b7\u03c4\u03bf\u03c5, \u03b1\u03c0\u03cc, \u03b2\u03b1\u03bb\u03ba\u03b1\u03bd\u03b9\u03ba\u03cc, \u03b3\u03bb\u03c9\u03c3\u03c3\u03b9\u03ba\u03cc, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03b3\u03c1\u03b1\u03c0\u03c4\u03ac, \u03b5\u03af\u03bd\u03b1\u03b9, \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae, \u03b5\u03bd\u03cc\u03c2, \u03b5\u03c0\u03af\u03c3\u03b7\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ad\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ae\u03c2, \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03b1, \u03ba\u03bb\u03ac\u03b4\u03bf\u03c5, \u03bc\u03ad\u03bb\u03bf\u03c2, \u03bc\u03ad\u03c7\u03c1\u03b9, \u03bc\u03af\u03b1, \u03bc\u03bf\u03bd\u03b1\u03b4\u03b9\u03ba\u03cc, \u03bf\u03b9\u03ba\u03bf\u03b3\u03ad\u03bd\u03b5\u03b9\u03b1\u03c2, \u03c3\u03c4\u03bf\u03bd, \u03c4\u03b7\u03c2, \u03c4\u03b9\u03c2, \u03c4\u03bf\u03bd, \u0420\u0438\u043c\u0441\u043a\u043e\u0439, \u0431\u043e\u0433\u0430\u0442\u0435\u0439\u0448\u0430\u044f, \u0431\u043e\u043b\u044c\u0448\u043e\u0435, \u0431\u044b\u043b\u0430, \u0432\u0440\u0435\u043c\u044f, \u0432\u0441\u0435\u0445, \u0432\u0441\u044f\u043a\u043e\u0433\u043e, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0435, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0445, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u043c, \u0434\u0432\u0443\u043c\u044f, \u0434\u043b\u044f, \u0434\u0440\u0435\u0432\u043d\u0435\u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0439, \u0435\u0433\u043e, \u0437\u043d\u0430\u043d\u0438\u0435, \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435, \u0438\u043c\u043f\u0435\u0440\u0438\u0438, \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u0445, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u043e\u043c, \u043b\u0435\u043a\u0441\u0438\u043a\u0443, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0430\u044f, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0443\u044e, \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u0430\u044f, \u043d\u0430\u0443\u0447\u043d\u044b\u0445, \u043d\u043e\u0432\u043e\u0435, \u043d\u043e\u0432\u044b\u0445, \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e, \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c, \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c, \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442, \u043f\u0440\u043e\u043d\u0438\u043a\u0430\u043b\u0438, \u043f\u0443\u0442\u044f\u043c\u0438, \u0440\u043e\u043c\u0430\u043d\u0441\u043a\u0438\u0445, \u0440\u0443\u0441\u0441\u043a\u0438\u0439, \u0441\u043b\u043e\u0432\u0430, \u0441\u043e\u0437\u0434\u0430\u043d\u0430, \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f, \u0441\u0442\u0430\u043b, \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f, \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c, \u0442\u0435\u0440\u043c\u0438\u043d\u043e\u0432, \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445, \u0446\u0435\u0440\u043a\u043e\u0432\u043d\u043e\u0441\u043b\u0430\u0432\u044f\u043d\u0441\u043a\u0438\u0439, \u0447\u0435\u0440\u0435\u0437, \u044d\u0442\u0430\u043f\u0430\u0445, \u044f\u0437\u044b\u043a, \u044f\u0437\u044b\u043a\u0430, \u044f\u0437\u044b\u043a\u0435\nSecond run\n\u2022 15\u03bf\n\u2022 \u2014\n\u2022 \u0397\n100\n\u2022 \u0430, \u0432, \u0438, \u0441\n\u2022 \u0412\n\u2022 \u03c4\u03bf, \u041d\u0430, \u043d\u0430\n\u2022 (\u043d\u0430\u0440\u044f\u0434\u0443, (\u0442\u0430\u043a\n\u2022 \u03b3\u03bb\u03c9\u03c3\u03c3\u03ce\u03bd., \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b5\u03c2., \u03b4\u03b5\u03c3\u03bc\u03cc., \u03c0.\u03a7., \u03c3\u03ae\u03bc\u03b5\u03c1\u03b1., \u0437\u0430\u0438\u043c\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0439, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u043c), \u043b\u0435\u043a\u0441\u0438\u043a\u0430)., \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430., \u0441\u043b\u043e\u0432., \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430., \u044f\u0437\u044b\u043a.\n\u2022 \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5, \u03b1\u03b9\u03ce\u03bd\u03b1, \u03b1\u03bd\u03b5\u03be\u03ac\u03c1\u03c4\u03b7\u03c4\u03bf\u03c5, \u03b1\u03c0\u03cc, \u03b2\u03b1\u03bb\u03ba\u03b1\u03bd\u03b9\u03ba\u03cc, \u03b3\u03bb\u03c9\u03c3\u03c3\u03b9\u03ba\u03cc, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03b3\u03c1\u03b1\u03c0\u03c4\u03ac, \u03b5\u03af\u03bd\u03b1\u03b9, \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae, \u03b5\u03bd\u03cc\u03c2, \u03b5\u03c0\u03af\u03c3\u03b7\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ad\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ae\u03c2, \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03b1, \u03ba\u03bb\u03ac\u03b4\u03bf\u03c5, \u03bc\u03ad\u03bb\u03bf\u03c2, \u03bc\u03ad\u03c7\u03c1\u03b9, \u03bc\u03af\u03b1, \u03bc\u03bf\u03bd\u03b1\u03b4\u03b9\u03ba\u03cc, \u03bf\u03b9\u03ba\u03bf\u03b3\u03ad\u03bd\u03b5\u03b9\u03b1\u03c2, \u03c3\u03c4\u03bf\u03bd, \u03c4\u03b7\u03c2, \u03c4\u03b9\u03c2, \u03c4\u03bf\u03bd\n\u2022 \u0391\u03bd\u03ae\u03ba\u03b5\u03b9, \u0391\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03af, \u03a3\u03c4\u03b7\u03bd\n\u2022 \u0431\u043e\u0433\u0430\u0442\u0435\u0439\u0448\u0430\u044f, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0435, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0445, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u043c, \u0434\u0440\u0435\u0432\u043d\u0435\u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0439, \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435, \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u0445, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u043e\u043c, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0430\u044f, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0443\u044e, \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u0430\u044f, \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e, \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c, \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c, \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442, \u043f\u0440\u043e\u043d\u0438\u043a\u0430\u043b\u0438, \u0440\u043e\u043c\u0430\u043d\u0441\u043a\u0438\u0445, \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f, \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f, \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c, \u0442\u0435\u0440\u043c\u0438\u043d\u043e\u0432, \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445, \u0446\u0435\u0440\u043a\u043e\u0432\u043d\u043e\u0441\u043b\u0430\u0432\u044f\u043d\u0441\u043a\u0438\u0439\n\u2022 \u0420\u0438\u043c\u0441\u043a\u043e\u0439, \u0431\u043e\u043b\u044c\u0448\u043e\u0435, \u0431\u044b\u043b\u0430, \u0432\u0440\u0435\u043c\u044f, \u0432\u0441\u0435\u0445, \u0432\u0441\u044f\u043a\u043e\u0433\u043e, \u0434\u0432\u0443\u043c\u044f, \u0434\u043b\u044f, \u0435\u0433\u043e, \u0437\u043d\u0430\u043d\u0438\u0435, \u0438\u043c\u043f\u0435\u0440\u0438\u0438, \u043b\u0435\u043a\u0441\u0438\u043a\u0443, \u043d\u0430\u0443\u0447\u043d\u044b\u0445, \u043d\u043e\u0432\u043e\u0435, \u043d\u043e\u0432\u044b\u0445, \u043f\u0443\u0442\u044f\u043c\u0438, \u0440\u0443\u0441\u0441\u043a\u0438\u0439, \u0441\u043b\u043e\u0432\u0430, \u0441\u043e\u0437\u0434\u0430\u043d\u0430, \u0441\u0442\u0430\u043b, \u0447\u0435\u0440\u0435\u0437, \u044d\u0442\u0430\u043f\u0430\u0445, \u044f\u0437\u044b\u043a, \u044f\u0437\u044b\u043a\u0430, \u044f\u0437\u044b\u043a\u0435\nData: Mixed script: English \u2013 Greek"}, {"heading": "First run", "text": "\u2022 \u201cintimate, \u201cwithout, Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether, affection, ancient, another.\u201d, appreciation, aspires, araction, araction.\u201d, becomes, benevolence., biblical, brotherly, chapter,\u201d, charity;, children, children., contemplation, content, continues, contributes, definition:, described, existence;, explained, express, feeling, feelings, finding, further, holding, initially, inspired, knowledge, marriage., necessary, non-corporeal, passage, passion.\u201d, philosophers, physical, platonic, refined, relationships, returned, self-benefit)., sensually, spiritual, subject, suggesting, through, throughout, transcendence., unconditional, understanding, without, youthful\n\u2022 (\u1f00\u03b3\u03ac\u03c0\u03b7, (\u1f14\u03c1\u03c9\u03c2, Ag\u00e1pe, ag\u00e1p\u0113), \u00c9ros, \u00e9r\u014ds), \u2013\n101\n\u2022 \u201cForm\u201d, \u201cerotas\u201d, \u201clove, \u201clove, \u201clove:, (even, Agape, Greek, Lovers, Modern, Plato, is, omas, also, apply, argue, based, beauty, beauty, being, dating, denote, desire, does, eros, eros., erotic, even, famous, feast., feel, felt, given, good, helps, hence, high, humans, ideal, itself., just, known, leads, like, love, love, love.\u201d, mean, means, most, mostly, one\u2019s, part, person, person, plane, recall, refer, regard., seek, sexual, soul, spouse, talk, texts, that, there, thus, truth, truth, type, used, well, will, will, with, within, word, work\n\u2022 \u201cto, 1, 13, God, God.\u201d, In, It, New, e, a, all, an, and, any, are, as, be, by, can, esp., for, has, his, in, is, is, it, its, man, not, not, of, on, one, or, own, the, to, us, use, was"}, {"heading": "Second run", "text": "\u2022 affection, ancient, another.\u201d, aspires, becomes, biblical, chapter,\u201d, charity;, children, children., content, definition:, feeling, feelings, finding, holding, marriage., necessary, passage, passion.\u201d, platonic, refined, returned, subject, through, without\n\u2022 Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether\n\u2022 \u201cintimate, appreciation, araction, araction.\u201d, benevolence., brotherly, contemplation, continues, contributes, described, existence;, explained, express, further, initially, inspired, knowledge, non-corporeal, philosophers, physical, relationships, self-benefit)., sensually, spiritual, suggesting, throughout, transcendence., unconditional, understanding, youthful\n\u2022 Ag\u00e1pe, ag\u00e1p\u0113), \u00c9ros, \u00e9r\u014ds)\n\u2022 (\u1f00\u03b3\u03ac\u03c0\u03b7, (\u1f14\u03c1\u03c9\u03c2\n\u2022 \u2013\n\u2022 \u201cerotas\u201d, beauty, beauty, dating, denote, desire, erotic, famous, humans, itself., mostly, person, person, recall, regard., sexual, spouse, within\n\u2022 \u201cForm\u201d, Agape, Greek, Lovers, Modern, Plato, is, omas, based, being, feast., hence, ideal, leads, means, plane, refer, there\n\u2022 apply, felt, helps, high, just, known, most, part, talk, texts, that, thus, truth, truth, type, well, will, will, with, word, work\n\u2022 \u201clove, \u201clove, \u201clove:, (even, also, argue, does, eros, eros., even, feel, given, good, like, love, love, love.\u201d, mean, one\u2019s, seek, soul, used\n102\n\u2022 1, 13, In, It\n\u2022 \u201cto, a, an, as, be, by, in, is, is, it, of, on, or, to, us\n\u2022 God, God.\u201d, New, e, all, and, any, esp., its, own, the\n\u2022 are, can, for, has, his, man, not, not, one, use, was\nData: Mixed script: English \u2013 Spanish \u2013 Arabic"}, {"heading": "First run", "text": "\u2022 El, POW/MIA, Wearing, a, as, been, black, de, displaying, duelo., en, es, estados, for, has, is, lazo, mourning, mourning., negro, o, of, or, organizaciones, personas, political, por, remembrance, remembrance, representando, ribbon, sentimiento, sociedades, statement., symbol, tragedies, un, used, utilizado, y\n\u2022 cresp\u00f3n, pol\u00edtico-social, se\u00f1al, s\u00edmbolo\n\u2022 A\n\u2022 \u060c\u0627\u0647\u062a\u0644\u0627\u0633\u0631 \u060c\u0646\u0648\u062f \u060c\u0644\u0642\u0646\u062a \u060c\u0629\u0631\u0638\u0646\u0628 \u060c\u0641\u0648\u0631\u0639\u0645\u0644\u0627 \u060c\u0646\u064a\u064a\u0631\u0635\u0645\u0644\u0627 \u060c\u0629\u0645\u0627\u0644\u0639\u0644\u0627 \u060c\u062a\u0627\u0645\u0627\u0644\u0639\u0644\u0627 \u060c\u0632\u0645\u0631\u0644\u0627 \u060c\u0645\u0633\u0631\u0644\u0627 \u060c\u064a\u0630\u0644\u0627 \u060c\u0629\u062c\u0627\u062d\u0644\u0627 \u060c\u0645\u062f\u062e\u062a\u0633\u0627 \u060c\u0646\u0627\u0654 \u060c\u0631\u062b\u0643\u0627\u0654 \u060c\u0627\u0648\u0645\u062f\u062e\u062a\u0633\u0627\u0654 \u064a\u063a\u0628\u0646\u064a \u060c\u064a\u0646\u0639\u064a \u060c\u0631\u0628\u0639\u064a \u060c\u0646\u0643\u0644\u0648 \u060c\u0627\u0645\u0648\u0645\u0639\u0648 \u060c\u0642\u064a\u0631\u063a\u0627\u0654\u0644\u0627\u0648 \u060c\u0629\u062f\u062d\u0627\u0648 \u060c\u0648 \u060c\u0645\u0647 \u060c\u0646\u0645 \u060c\u0646\u064a\u0639\u0645 \u060c\u0629\u064a\u0627\u0644 \u060c\u062a\u0627\u0645\u0644\u0643 \u060c\u0621\u0627\u0645\u062f\u0642 \u060c\u0646\u0627\u0654\u0641 \u060c\u0646\u0639 \u060c\u0621\u064a\u0634"}, {"heading": "Second run", "text": "\u2022 a, o, y\n\u2022 El, as, de, en, es, is, of, or, un\n\u2022 Wearing, been, black, displaying, duelo., estados, for, has, lazo, mourning, mourning., negro, organizaciones, personas, political, por, remembrance, remembrance, representando, ribbon, sentimiento, sociedades, statement., symbol, tragedies, used, utilizado\n\u2022 POW/MIA\n\u2022 pol\u00edtico-social, s\u00edmbolo\n\u2022 cresp\u00f3n, se\u00f1al\n\u2022 A\n\u2022 \u0627\u0645\u0648\u0645\u0639\u0648 \u060c\u0642\u064a\u0631\u063a\u0627\u0654\u0644\u0627\u0648 \u060c\u0627\u0647\u062a\u0644\u0627\u0633\u0631 \u060c\u0641\u0648\u0631\u0639\u0645\u0644\u0627 \u060c\u0646\u064a\u064a\u0631\u0635\u0645\u0644\u0627 \u060c\u0629\u0645\u0627\u0644\u0639\u0644\u0627 \u060c\u062a\u0627\u0645\u0627\u0644\u0639\u0644\u0627 \u060c\u0629\u062c\u0627\u062d\u0644\u0627 \u060c\u0645\u062f\u062e\u062a\u0633\u0627 \u060c\u0627\u0648\u0645\u062f\u062e\u062a\u0633\u0627\u0654\n\u2022 \u064a\u063a\u0628\u0646\u064a \u060c\u064a\u0646\u0639\u064a \u060c\u0631\u0628\u0639\u064a \u060c\u0646\u0643\u0644\u0648 \u060c\u0629\u062f\u062d\u0627\u0648 \u060c\u0646\u064a\u0639\u0645 \u060c\u0629\u064a\u0627\u0644 \u060c\u062a\u0627\u0645\u0644\u0643 \u060c\u0621\u0627\u0645\u062f\u0642 \u060c\u0646\u0627\u0654\u0641 \u060c\u0621\u064a\u0634 \u060c\u0646\u0648\u062f \u060c\u0644\u0642\u0646\u062a \u060c\u0629\u0631\u0638\u0646\u0628 \u060c\u0632\u0645\u0631\u0644\u0627 \u060c\u0645\u0633\u0631\u0644\u0627 \u060c\u064a\u0630\u0644\u0627 \u060c\u0631\u062b\u0643\u0627\u0654\n103\n\u2022 \u0645\u0647 \u060c\u0646\u0645 \u060c\u0646\u0639 \u060c\u0646\u0627\u0654\n\u2022 \u0648\nData: Mixed script: English \u2013 Chinese"}, {"heading": "First run", "text": "\u2022 \u201ccrisis\u201d, \u201cdanger\u201d, \u201copportunity\u201d., (simplified, Chinese, Chinese:, Western, auspicious, because, believed, besides, certain, characters, component, composed, criticized, frequently, inauspicious, invoked, linguists, meanings, meanings., motivational, number, numbers, pinyin:, positive, pronounced, represent, similar, sounds, speaking, tradition, traditional, w\u0113ij\u012b)\n\u2022 (\u4e0d\u5229), (\u5409\u5229),\u5371\u673a;,\u5371\u6a5f;,\u673a;,\u6a5f)\n\u2022 0, 6, 8, 9\n\u2022 In, Some, e, and, are, based, be, by, can, for, has, have, in, is, j\u012b, name, names, of, on, or, other, some, sound, that, the, their, this, to, to., two, usage, word, words\nSecond run\n\u2022 Chinese, Chinese:\n\u2022 Western\n\u2022 \u201ccrisis\u201d, \u201cdanger\u201d, \u201copportunity\u201d., (simplified, auspicious, because, believed, besides, certain, characters, component, composed, criticized, frequently, inauspicious, invoked, linguists, meanings, meanings., motivational, number, numbers, pinyin:, positive, pronounced, represent, similar, sounds, speaking, tradition, traditional, w\u0113ij\u012b)\n\u2022 (\u4e0d\u5229), (\u5409\u5229)\n\u2022 \u5371\u673a;,\u5371\u6a5f;\n\u2022 \u673a;,\u6a5f)\n\u2022 6, 8, 9\n\u2022 0,\n\u2022 Some, e, and, are, based, can, for, has, have, name, names, other, some, sound, that, the, their, this, two, usage, word, words\n104\n\u2022 In, be, by, in, is, of, on, or, to, to.\n\u2022 j\u012b\nData: Mixed script: Ukrainian \u2013 Russian\nFirst run\n\u2022 9\u201413\n\u2022 \u0411\u0435\u043b\u043e\u0440\u0443\u0441\u0441\u0438\u0435\u0439, \u0412\u0435\u043d\u0433\u0440\u0438\u0435\u0439, \u041c\u043e\u043b\u0434\u0430\u0432\u0438\u0435\u0439., \u041f\u043e\u043b\u044c\u0448\u0435\u0439, \u0420\u043e\u0441\u0441\u0438\u0435\u0439, \u0421\u043b\u043e\u0432\u0430\u043a\u0438\u0435\u0439, \u043c\u043e\u0440\u0435\u0439., \u043d\u0430\u0440\u043e\u0434\u0456\u0432, \u0441\u0430\u0440\u043c\u0430\u0442\u0456\u0432, \u0441\u043a\u0456\u0444\u0456\u0432, \u0441\u0442\u043e\u043b\u0456\u0442\u0442\u044f.\n\u2022 \u0410\u0437\u043e\u0432\u0441\u043a\u043e\u0433\u043e, \u0412\u0456\u0434\u0434\u0430\u0432\u043d\u0430, \u041a\u0438\u0457\u0432\u0441\u044c\u043a\u0430, \u0420\u0443\u043c\u044b\u043d\u0438\u0435\u0439, \u0423\u043a\u0440\u0430\u0457\u043d\u0438, , \u0427\u0451\u0440\u043d\u043e\u0433\u043e, \u0432\u0432\u0430\u0436\u0430\u0454\u0442\u044c\u0441\u044f, \u0432\u0456\u0434\u043f\u0440\u0430\u0432\u043d\u0438\u043c, , \u0433\u0440\u0430\u043d\u0438\u0446\u0443, \u0434\u0435\u0440\u0436\u0430\u0432\u0438, \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u043e\u0441\u0442\u0456, , \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0438, \u043e\u043c\u044b\u0432\u0430\u0435\u0442\u0441\u044f, \u043f\u0443\u043d\u043a\u0442\u043e\u043c, \u0441\u0443\u0445\u043e\u043f\u0443\u0442\u043d\u0443\u044e, \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457, \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0457, \u0456\u0441\u043d\u0443\u0432\u0430\u043b\u0438,\n\u2022 \u0418\u043c\u0435\u0435\u0442, \u041d\u0430, \u0420\u0443\u0441\u044c, \u0430\u043b\u0435, \u0432\u043e\u0434\u0430\u043c\u0438, \u0433\u043e\u0442\u0456\u0432, \u0438, \u0439, \u043d\u0430, \u0441, \u0442\u0430, \u044e\u0433\u0435, \u0456\u043d\u0448\u0438\u0445\nSecond run\n\u2022 9\u201413\n\u2022 \u043c\u043e\u0440\u0435\u0439., \u043d\u0430\u0440\u043e\u0434\u0456\u0432, \u0441\u0430\u0440\u043c\u0430\u0442\u0456\u0432, \u0441\u043a\u0456\u0444\u0456\u0432, \u0441\u0442\u043e\u043b\u0456\u0442\u0442\u044f.\n\u2022 \u0411\u0435\u043b\u043e\u0440\u0443\u0441\u0441\u0438\u0435\u0439, \u0412\u0435\u043d\u0433\u0440\u0438\u0435\u0439, \u041c\u043e\u043b\u0434\u0430\u0432\u0438\u0435\u0439., \u041f\u043e\u043b\u044c\u0448\u0435\u0439, \u0420\u043e\u0441\u0441\u0438\u0435\u0439, \u0421\u043b\u043e\u0432\u0430\u043a\u0438\u0435\u0439,\n\u2022 \u0410\u0437\u043e\u0432\u0441\u043a\u043e\u0433\u043e, \u0412\u0456\u0434\u0434\u0430\u0432\u043d\u0430, \u041a\u0438\u0457\u0432\u0441\u044c\u043a\u0430, \u0420\u0443\u043c\u044b\u043d\u0438\u0435\u0439, \u0423\u043a\u0440\u0430\u0457\u043d\u0438, \u0427\u0451\u0440\u043d\u043e\u0433\u043e, \u0433\u0440\u0430\u043d\u0438\u0446\u0443, \u0434\u0435\u0440\u0436\u0430\u0432\u0438, \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0438, \u043f\u0443\u043d\u043a\u0442\u043e\u043c, \u0456\u0441\u043d\u0443\u0432\u0430\u043b\u0438\n\u2022 \u0432\u0432\u0430\u0436\u0430\u0454\u0442\u044c\u0441\u044f, \u0432\u0456\u0434\u043f\u0440\u0430\u0432\u043d\u0438\u043c, \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u043e\u0441\u0442\u0456, \u043e\u043c\u044b\u0432\u0430\u0435\u0442\u0441\u044f, \u0441\u0443\u0445\u043e\u043f\u0443\u0442\u043d\u0443\u044e, \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457, \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0457\n\u2022 \u0438, \u0439, \u0441\n\u2022 \u041d\u0430, \u043d\u0430, \u0442\u0430\n\u2022 \u0430\u043b\u0435, \u0432\u043e\u0434\u0430\u043c\u0438, \u0433\u043e\u0442\u0456\u0432, \u044e\u0433\u0435, \u0456\u043d\u0448\u0438\u0445\n\u2022 \u0418\u043c\u0435\u0435\u0442, \u0420\u0443\u0441\u044c\n105\nData: Pali: abbha"}, {"heading": "First run", "text": "\u2022 (also, (cp., (dense, (megho, (used, (\u00b0sama, 1, 1, 101, 1064;, 12)., 134., 239., 249, 250;, 251)., 273, 289., 295, 3, 317, 348, 348)., 382)., 487, 53, 581, 617, 64;, 687, <at> a)fro\\\\s </at>, <at> o)/mbros </at>, <smallcaps> i. </smallcaps>, <smallcaps> ii. </smallcaps>, <smallcaps> iv. </smallcaps>, <smallcaps> vi. </smallcaps>, <superscript> 1 </superscript>, <superscript> 9 </superscript>, <superscript> s. </superscript>, A, A), As, Dh, Dhs, DhsA, Gr., Idg., It, J, Lat., Miln, Miln), Oir, Pv, PvA, S, Sk., Sn, SnA, , is, Vin, Vv, VvA, [Vedic, a, abhra, adj., also, ambha, ambu, as, at, by, cloud, cloud, cloud\\\u201d;, clouds, clouds, cloudy, cp., dark), expl, f., free, from, froth, imber, in, is, later, like, list, m., marajo</b>, mass, moon\u2013, mountain, nt., obscure, of, or, pabbata, perhaps, point, rain, rain;, rajo, referred, scum, sense, storm\u2013cloud, summit, sunshine, that, the, thick, things, thunder\u2013cloud);, thundering, to, viz., water, water].\n\u2022 &, (=, <b>\u2013ghana</b>, <b>\u2013mua</b>, <br, =, \\\u201ddark, \\\u201ddull\\\u201d;\n\u2022 (abbha\u014b, (mahiy\u0101, (n\u012bl\u00b0, <b> \u2013sa\u014bvil\u0101pa </b>, <b> R\u0101hu </b>, <b> abbh\u0101 </b>, <b> abbh\u0101maa </b>, abbh\u0101mua, acch\u0101desi);, mahik\u0101 </b>, n\u012bla\u2013megha, val\u0101haka);, val\u0101haka\u2013 sikhara\n\u2022 *m\u030abhrocite /><b>\u2013k\u016b\u1e6da</b>, <b>\u2013pa\u1e6dala</b>, <b>abbha\u014b, <b>dh\u016b-, (nt.)"}, {"heading": "Second run", "text": "\u2022 (cp., Dhs, DhsA, Idg., Lat., Miln, Miln), Oir, PvA, SnA, is, Vin, VvA, [Vedic, as, at, by, cp., in, is, nt., of, or, to\n\u2022 (also, (dense, (megho, (used, (\u00b0sama, <at> a)fro\\\\s </at>, <at> o)/mbros </at>, <smallcaps> ii. </smallcaps>, <smallcaps> iv. </smallcaps>, <smallcaps> vi. </smallcaps>, abhra, adj., also, ambha, ambu, cloud, cloud, cloud\\\u201d;, clouds, clouds, cloudy, dark), expl, free, from, froth, imber, later, like, list, marajo </b>, mass, moon\u2013, mountain, obscure, pabbata, perhaps, point, rain, rain;, rajo, referred, scum, sense, storm\u2013 cloud, summit, sunshine, that, the, thick, things, thunder\u2013 cloud);, thundering, viz., water, water].\n\u2022 1, 1, 101, 1064;, 12)., 134., 239., 249, 250;, 251)., 273, 289., 295, 3, 317, 348, 348)., 382)., 487, 53, 581, 617, 64;, 687, <superscript> 1 </superscript>, <superscript> 9 </superscript>\n\u2022 <smallcaps> i. </smallcaps>, <superscript> s. </superscript>, A, A), As, Dh, Gr., It, J, Pv, S, Sk., Sn, , Vv, a, f., m.\n106\n\u2022 <b> \u2013ghana </b>, <b> \u2013mua </b>, <br, \\\u201ddark, \\\u201ddull\\\u201d;\n\u2022 &, (=, =\n\u2022 (abbha\u014b, (mahiy\u0101, (n\u012bl\u00b0, <b> R\u0101hu </b>, <b> abbh\u0101 </b>, n\u012bla\u2013megha\n\u2022 <b> \u2013sa\u014bvil\u0101pa </b>, <b> abbh\u0101maa </b>, abbh\u0101mua, acch\u0101desi);, mahik\u0101 </b>, val\u0101haka);, val\u0101haka\u2013sikhara\n\u2022 *m\u030abhro, /><b> \u2013k\u016b\u1e6da </b>, <b> \u2013pa\u1e6dala </b>, <b> abbha\u014b, <b> dh\u016b-\n\u2022 (nt.)\nData: Pali: abhijjhitar"}, {"heading": "First run", "text": "\u2022 abhijjhita, abhijjh\u0101tar, covets, function], med., one, who, \u00b0itar), \u00b0itar, \u00b0\u0101tar).\n\u2022 (T., <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, =, A, M, ag., fr., in, l., v.\n\u2022 265, 287\n\u2022 [n."}, {"heading": "Second run", "text": "\u2022 abhijjhita, abhijjh\u0101tar, covets, function], med., one, who, \u00b0itar), \u00b0itar, \u00b0\u0101tar).\n\u2022 (T., A, M\n\u2022 =, l., v.\n\u2022 <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, ag., fr., in\n\u2022 265, 287\n\u2022 [n.\n107\nData: Pali: ajja"}, {"heading": "First run", "text": "\u2022 \u2013divasa, Freq., Loc., [Vedic, adya, ajjatagge, ajjato, an, and, as, base, being, day, demonstr., dyaus, from, in, morning, not, of, old, or, phrase, present, pron., the, this, with\n\u2022 &, +, Mhvs, s., v.\n\u2022 \u2013k\u0101la\u1e41, 10\uff09, 15\uff0c64., 32\uff0c23., Ajj\u0101, D.I\uff0c85\uff1b, DA.I\uff0c235., Dh.326\uff1b, III\uff0c425, J.I\uff0c 279\uff1b, J.VI\uff0c180\uff1b, Kern\uff0cToev., Pv.I\uff0c117, PvA.59\uff09\uff1b, PvA.6\uff0c23\uff1b, Sn.75\uff0c153\uff0c 158\uff0c970\uff0c998\uff1b, Vin.I\uff0c18\uff1b, a3\uff09, ady\u0101\uff0ca, agga3\uff09, agge\uff08?\uff09, ajja-tagge\uff0csee, ajj\u0101\uff1b, bahuta\u1e41, day\u201d], diva\uff09\uff0cthus, dy\u0101, dy\u0101\uff0ca\u00b0, id\u0101ni, onward\uff0chenceforth, to-day\uff0cnow,\u201cfood\u201d\uff09\uff1b,\u201con, \u2039-\u203a, Ajja\uff0c&,\uff08=,\uff08Page,\uff08adv.\uff09,\uff08read,\uff08see"}, {"heading": "Second run", "text": "\u2022 an, as, in, of, or\n\u2022 Freq., Loc., [Vedic\n\u2022 \u2013divasa, adya, ajjatagge, ajjato, and, base, being, day, demonstr., dyaus, from, morning, not, old, phrase, present, pron., the, this, with\n\u2022 &\n\u2022 +\n\u2022 Mhvs\n\u2022 s., v.\n\u2022\u201con, \u2039-\u203a, Ajja\uff0c&,\uff08=,\uff08Page,\uff08adv.\uff09,\uff08read,\uff08see\n\u2022 \u2013k\u0101la\u1e41, 10\uff09, 15\uff0c64., 32\uff0c23., Ajj\u0101, D.I\uff0c85\uff1b, DA.I\uff0c235., Dh.326\uff1b, III\uff0c425, J.I\uff0c279\uff1b, J.VI\uff0c180\uff1b, Kern\uff0cToev., Pv.I\uff0c117, PvA.6\uff0c23\uff1b, Sn.75\uff0c153\uff0c158\uff0c 970\uff0c998\uff1b, Vin.I\uff0c18\uff1b, a3\uff09, agga3\uff09, ajja-tagge\uff0csee, ajj\u0101\uff1b, bahuta\u1e41, day\u201d ], diva\uff09\uff0cthus, dy\u0101, id\u0101ni, onward\uff0chenceforth, to-day\uff0cnow\n\u2022 PvA.59\uff09\uff1b, ady\u0101\uff0ca, agge\uff08?\uff09, dy\u0101\uff0ca\u00b0,\u201cfood\u201d\uff09\uff1b\n108\nData: Pali: g\u016bhan\u0101"}, {"heading": "First run", "text": "\u2022 253\uff09, Pug\uff0e19\uff0eCp\uff0epari\u00b0\uff0e\uff08Page, [abstr\uff0efr\uff0eg\u016bhati]=g\u016bhan\u0101, G\u016bhan\u0101\uff0c\uff08f.\uff09 ,\uff08q\uff0ev.\uff09"}, {"heading": "Second run", "text": "\u2022 253\uff09, Pug\uff0e19\uff0eCp\uff0epari\u00b0\uff0e\uff08Page, [abstr\uff0efr\uff0eg\u016bhati]=g\u016bhan\u0101, G\u016bhan\u0101\uff0c\uff08f.\uff09 ,\uff08q\uff0ev.\uff09\nData: Pali: pacati"}, {"heading": "First run", "text": "\u2022 382\uff09, Caus\uff0epac\u0101peti, DA\uff0eI\uff0c159\uff0cwhere, Obulg\uff0epeka, Pass\uff0epaccati, Vin\uff0e IV\uff0c264\uff1b, bake\uff0cGr\uff0ep\u03adssw, cook\uff0cboil\uff0croast, cook\uff0cp\u03adpwn, da\u1e47\u1e0dena, fig\uff0e torment, fry\uff0croast\uff0cLith\uff0ckep\u016b, intrs\uff0e\uff09\uff1aNiraye, paccato\uff0cby, ppr\uff0epacanto, pp\uff0epakka, p\u012b\u1e37entassa\uff09\uff0e\u2013, tormenting\uff0cGen\uff0epacato\n\u2022 D\uff0eI\uff0c52, N\uff0eS\uff0eII\uff0c225\uff0cPvA\uff0e10\uff0c14\uff0e\u2013, Pacati\uff0c[Ved\uff0epacati\uff0cIdg\uff0e*peq\u01d4\u014d\uff0c Av\uff0epac-\uff1b,\uff08+Caus\uff0ep\u0101cayato\uff09,\uff08expld,\uff08q\uff0ev\uff0e\uff09\uff0e\u2013,\uff08q\uff0ev\uff0e\uff09\uff0e\u2039-\u203a,\uff08q\uff0e v\uff0e\uff09\uff0e\uff08Page,\uff08trs\uff0eand\n\u2022 aer, at, be, for, in, or, pacato, pare, purgatory, read, ripe], roasted, roasting, to, tormented\n\u2022 &, pacitv\u0101, p\u0101ceti"}, {"heading": "Second run", "text": "\u2022 Caus\uff0epac\u0101peti, DA\uff0eI\uff0c159\uff0cwhere, Obulg\uff0epeka, Pass\uff0epaccati, Vin\uff0eIV\uff0c264\uff1b , bake\uff0cGr\uff0ep\u03adssw, cook\uff0cboil\uff0croast, cook\uff0cp\u03adpwn, da\u1e47\u1e0dena, fig\uff0etorment, fry\uff0c roast\uff0cLith\uff0ckep\u016b, intrs\uff0e\uff09\uff1aNiraye, paccato\uff0cby, ppr\uff0epacanto, p\u012b\u1e37entassa\uff09\uff0e\u2013, tormenting\uff0cGen\uff0epacato\n\u2022 382\uff09, pp\uff0epakka\n\u2022 D\uff0eI\uff0c52, N\uff0eS\uff0eII\uff0c225\uff0cPvA\uff0e10\uff0c14\uff0e\u2013,\uff08q\uff0ev\uff0e\uff09\uff0e\u2013\n\u2022 Pacati\uff0c[Ved\uff0epacati\uff0cIdg\uff0e*peq\u01d4\u014d\uff0cAv\uff0epac-\uff1b,\uff08+Caus\uff0ep\u0101cayato\uff09,\uff08expld, \uff08q\uff0ev\uff0e\uff09\uff0e\u2039-\u203a,\uff08q\uff0ev\uff0e\uff09\uff0e\uff08Page,\uff08trs\uff0eand\n109\n\u2022 for, pacato, pare, read, ripe]\n\u2022 aer, purgatory, roasted, roasting, tormented\n\u2022 or, to\n\u2022 at, be, in\n\u2022 &\n\u2022 pacitv\u0101, p\u0101ceti\nData: Twier 1 (Greek\u2013English)"}, {"heading": "First run", "text": "\u2022 \u03b1\u03c5\u03c4\u03ae, \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc, \u03bb\u03cd\u03c3\u03b7, \u03c3\u03c4\u03bf, \u03c4\u03b7, \u03c8\u03ae\u03c6\u03b9\u03c3\u03b1, \u039c\u03cc\u03bb\u03b9\u03c2\n\u2022 BUSINESS, EXCELLENCE., IT, Internet, ings, of\nSecond run\n\u2022 \u039c\u03cc\u03bb\u03b9\u03c2\n\u2022 \u03b1\u03c5\u03c4\u03ae, \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc, \u03bb\u03cd\u03c3\u03b7, \u03c3\u03c4\u03bf, \u03c4\u03b7, \u03c8\u03ae\u03c6\u03b9\u03c3\u03b1\n\u2022 IT, of\n\u2022 Internet, ings,\n\u2022 BUSINESS, EXCELLENCE.\nData: Twier 2 (French\u2013English)"}, {"heading": "First run", "text": "\u2022 \u201ce, 18h, @dhiparis, David, Demain, Keynote, all, collective, counts?\u201d, dynamics, par, perish;, science-publish, that\n\u2022 is, it, of, or\n110"}, {"heading": "Second run", "text": "\u2022 \u201ce, @dhiparis, David, Demain, Keynote, all, collective, counts?\u201d, dynamics, par, perish;, science-publish, that\n\u2022 18h\n\u2022 is, it, or\n\u2022 of\nData: Twier 3 (French\u2013English)\nFirst run\n\u2022 Edmonton, Food\n\u2022 go, in, to\n\u2022 and, are, breuvages, fans, for, just, ready, the, waiting\nSecond run\n\u2022 Edmonton, Food\n\u2022 to\n\u2022 go, in\n\u2022 for, just\n\u2022 and, are, breuvages, fans, ready, the, waiting\nData: Twier 4 (English\u2013Polish)\nFirst run\n\u2022 \u017cubr\u00f3wka, my\n\u2022 adidas, and, back, comes, crates, dad, from, jackets, of, omg, poland, strawberries, two, with\n111\nSecond run\n\u2022 \u017cubr\u00f3wka, my\n\u2022 adidas, comes, dad, of\n\u2022 and, back, crates, from, jackets, omg, poland, strawberries, two, with\nData: Twier 5 (Transliterated Amharic\u2013English)\nFirst run\n\u2022 Buna\n\u2022 (coffee, bread)., dabo, is, naw, our\nSecond run\n\u2022 Buna\n\u2022 our\n\u2022 (coffee, bread)., dabo, is, naw"}, {"heading": "8.3.4 Language Model Induction", "text": "For all language model induction tasks, the threshold value t has been set t = 0.02 and the silver threshold value s has been set s = 0.1. e other parameters have been set to \u201cmaximum iteration count\u201d i = 4, \u201cmaximum random iteration count\u201d j = 2 and \u201cmerge mode ADD\u201d.\nData: Latin script: German\u2013English\n\u2022 e, German, word, Nabelschau, means, or, \u201cstaring, at, your, But, in, this, it, doesn\u2019t, refer, to, anyone, else\u2019s, buon, just, your, own.,\n\u2022 \u2013\n\u2022 \u201cnavel-gazing\u201d, navel\u201d., case, belly\n112\nData: Latin script: German\u2013Finnish\u2013Turkish\n\u2022 die, in, und, Klimazone., Je, ob, auf, S\u00fcdhalbkugel, vom, eli, on, vuodenaika, ja, on, vuodenajoista, koska, maapallo, on, silloin, kallistunut, aurinko, maan, pinnalle, kulmassa, muina, vuodenaikoina., Pohjoisella, pallonpuoliskolla, lasketaan, tavallisesti, ja, elokuu, etel\u00e4isell\u00e4, pallonpuoliskolla, joulu-, ja, helmikuu., en, s\u0131cak, en, yazda, D\u00fcnya, depo, en, s\u0131cak, yakla\u015f\u0131k, ay, sonra, ortaya, S\u0131cak, Haziran, Eyl\u00fcl, ise, Aral\u0131k, aras\u0131ndad\u0131r.\n\u2022 Der, ist, w\u00e4rmste, der, vier, Jahreszeiten, der, arktischen, nachdem, er, der, Nord-, oder, herrscht, spricht, Nord-, oder, Der, findet, mit, S\u00fcdwinter, sta., suvi, l\u00e4mpimin, niin, e\u00e4, s\u00e4teilee, hein\u00e4-, Yaz, mevsimdir., K\u00fcre\u2019de, K\u00fcre\u2019de, 21, 22, aras\u0131nda, K\u00fcre\u2019de, 22, 21, Mart\n\u2022 gem\u00e4\u00dfigten, gerade, gleichzeitig, kuin, Kuzey, uzun, g\u00fcnler, ger\u00e7ekle\u015fir., ei\u011fi, i\u00e7in, g\u00fcnler, genellikle, iki, g\u00fcnler, Kuzey, ile, ile\n\u2022 Sommer, man, S\u00fcdsommer., Nordsommer, dem, Kes\u00e4, kev\u00e4\u00e4n, syksyn, v\u00e4liss\u00e4., Kes\u00e4, jyrkemm\u00e4ss\u00e4, kes\u00e4kuukausiksi, kes\u00e4-., tammi-, Yar\u0131m, \u0131s\u0131y\u0131, \u00e7\u0131kar., Yar\u0131m, G\u00fcney, Yar\u0131m\nData: Latin script: English\u2013French\n\u2022 both, \u201cso\u201d, in, English, although, their, is, is, the, opposite, of, \u201crough\u201d, or, is, the, opposite, of, sweet, only, for, wines, (otherwise, is\n\u2022 mou, :, mou, but\n\u2022 doux,\n\u2022 Doux, (rugueux), Doux\n\u2022 while\n\u2022 \u201chard\u201d., used).,\n\u2022 translate, as, meaning, very, different., \u201dcoarse\u201d, can, also, mean, almost,sucr\u00e9,\nData: Latin script: English\u2013Transliterated Greek\n\u2022 at, least, ways, as, to, is, has, phil\u00eda, and, storg\u0113., as, has, historically, difficult, to, which, generally, as\n113\n\u2022 e, language, distinguishes, different, the, Ancient, distinct, with, languages, it, been, separate, the, meanings, these, used, outside, their, respective, the, senses, in, these, used\n\u2022 Greek, how, word, Greek, ag\u00e1pe, \u00e9ros, However, other, when, were, are\n\u2022 four, love, used., four, words, for, love:, of, words, of, contexts., Nonetheless, words, follows.\nData: Latin script: Italian\u2013German\n\u2022 affresc\u00f2, privato, Studie, definire, periture,Stierverbands,Wissenscha,studier\u00e0, difesa, ovvero, Szenario, Naturwissenschalern\n\u2022 dell\u2019aureola, da, del, di, der, zum, modo., dem, den, drohe., Come, vom\n\u2022 custodisce, quel, es, oder, per, le, idee, stessa, des, dass, delle, E, se, Ist, das, seit\n\u2022 pi\u00f9, Cenacolo, vinciano, rivoluzionaria, Giuda, condanna, con, peccato), cominci\u00f2, con, cancro, faceva, intuizioni, vita, va, Dabei, Ergebnis, in, i, riccioli, poi, pi\u00f9, bacini, in, Annunciazione, con, ali, la, cosa, barbaglio, anni, bei,\n\u2022 ne, struggente:, che, amore, e, non, viene, ma, consapevolezza, ad, che, ha, recente, Kaum, eine, Woche, vergeht, keine, neue, Umfrage, Warnung, ema, Fachkr\u00e4emangel, Deutschland, Certo, ma, anche, consapevole, che, qualche, mehren, letzter, Zeit, Stimmen, Entwarnung, geben., kam, j\u00fcngst, eine, Deutsche, \u201dein, allgemeiner, Fachkr\u00e4emangel, eher, mehr\u201d, anche, Baista, che, Leonardo, approfonditamente, a, Venezia, nelle, vada, alla, aento, alle, dell\u2019angelo:, delicatezza, punte, che, non, che, volare?, Jahren, angemahnte, drohenden, Fachkr\u00e4emangel, Ingenieuren, ein\n\u2022 Milano, l\u2019esempio, psicologia, (il, subito, autodistruggersi, solo, lunghissimo, So, il, movimento, moto, sui, si, bellissima, occhio, all\u2019ins\u00f9, sono, sogno, lo, ossessionava, quello, und, also, Mythos?\n\u2022 un, \u00fcr, MINT-Berufen\n\u2022 cura, restauro, arginato., gibt., perch\u00e9, caurata, sich, auch, zu, nicht, richiamano, acque, ricerche, chiave, anti-Turchi., nur\n114\nData: Mixed script: Greek\u2013Russian\n\u2022 \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03b5\u03af\u03bd\u03b1\u03b9, \u03bc\u03af\u03b1, \u03b1\u03c0\u03cc, \u03c4\u03b9\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ad\u03c2, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b5\u03c2., \u0391\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03af, \u03c4\u03bf, \u03bc\u03bf\u03bd\u03b1\u03b4\u03b9\u03ba\u03cc, \u03bc\u03ad\u03bb\u03bf\u03c2, \u03b5\u03bd\u03cc\u03c2, \u03b1\u03bd\u03b5\u03be\u03ac\u03c1\u03c4\u03b7\u03c4\u03bf\u03c5, \u03ba\u03bb\u03ac\u03b4\u03bf\u03c5, \u03c4\u03b7\u03c2, \u03b9\u03bd\u03b4\u03bf\u03b5\u03c5\u03c1\u03c9\u03c0\u03b1\u03ca\u03ba\u03ae\u03c2, \u03bf\u03b9\u03ba\u03bf\u03b3\u03ad\u03bd\u03b5\u03b9\u03b1\u03c2, \u03b3\u03bb\u03c9\u03c3\u03c3\u03ce\u03bd., \u0391\u03bd\u03ae\u03ba\u03b5\u03b9, \u03b5\u03c0\u03af\u03c3\u03b7\u03c2, \u03c3\u03c4\u03bf\u03bd, \u03b2\u03b1\u03bb\u03ba\u03b1\u03bd\u03b9\u03ba\u03cc, \u03b3\u03bb\u03c9\u03c3\u03c3\u03b9\u03ba\u03cc, \u03b4\u03b5\u03c3\u03bc\u03cc., \u03a3\u03c4\u03b7\u03bd, \u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae, \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1, \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5, \u03b3\u03c1\u03b1\u03c0\u03c4\u03ac, \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03b1, \u03b1\u03c0\u03cc, \u03c4\u03bf\u03bd, 15\u03bf, \u03b1\u03b9\u03ce\u03bd\u03b1, \u03bc\u03ad\u03c7\u03c1\u03b9, \u03c3\u03ae\u03bc\u03b5\u03c1\u03b1.\n\u2022 \u041d\u0430, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u043c, \u043d\u0430, \u0432\u0441\u0435\u0445, \u0435\u0433\u043e, \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f, \u0431\u044b\u043b\u0430, \u0441\u043e\u0437\u0434\u0430\u043d\u0430, \u0431\u043e\u0433\u0430\u0442\u0435\u0439\u0448\u0430\u044f, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e, \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c, \u0432\u0441\u044f\u043a\u043e\u0433\u043e, \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e, \u0431\u043e\u043b\u044c\u0448\u043e\u0435, \u0437\u0430\u0438\u043c\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0439, \u0430, \u0432, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u043e\u043c, \u043d\u043e\u0432\u043e\u0435, \u0432\u0440\u0435\u043c\u044f, (\u043d\u0430\u0440\u044f\u0434\u0443, \u043d\u043e\u0432\u044b\u0445, \u043d\u0430\u0443\u0447\u043d\u044b\u0445, \u0442\u0435\u0440\u043c\u0438\u043d\u043e\u0432, \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u0430\u044f, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0430\u044f, \u0441\u043b\u043e\u0432\u0430, \u0432, \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c, \u0434\u0432\u0443\u043c\u044f, \u0447\u0435\u0440\u0435\u0437\n\u2022 \u0397, \u03c0.\u03a7.,\u044f\u0437\u044b\u043a\u0435, \u044d\u0442\u0430\u043f\u0430\u0445, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430., \u0412, \u0420\u0438\u043c\u0441\u043a\u043e\u0439, \u0438\u043c\u043f\u0435\u0440\u0438\u0438, \u0437\u043d\u0430\u043d\u0438\u0435, \u044f\u0437\u044b\u043a\u0430, \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c, \u0434\u043b\u044f, \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430., \u0412, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u043e\u043c, \u044f\u0437\u044b\u043a\u0435, \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0445,\u2014, \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u0445, \u0438, \u0440\u043e\u043c\u0430\u043d\u0441\u043a\u0438\u0445, \u0441\u043b\u043e\u0432., \u0412, \u0434\u0440\u0435\u0432\u043d\u0435\u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0439, \u044f\u0437\u044b\u043a, \u0441\u0442\u0430\u043b, \u0441, \u043b\u0430\u0442\u0438\u043d\u0441\u043a\u0438\u043c), \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c, \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f, \u0438, \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445, (\u0442\u0430\u043a, \u043b\u0435\u043a\u0441\u0438\u043a\u0430)., \u0412, \u0440\u0443\u0441\u0441\u043a\u0438\u0439, \u044f\u0437\u044b\u043a, \u0433\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0435, \u043f\u0440\u043e\u043d\u0438\u043a\u0430\u043b\u0438, \u043f\u0443\u0442\u044f\u043c\u0438, \u2014, \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u0443\u044e, \u043b\u0435\u043a\u0441\u0438\u043a\u0443, \u0438, \u0446\u0435\u0440\u043a\u043e\u0432\u043d\u043e\u0441\u043b\u0430\u0432\u044f\u043d\u0441\u043a\u0438\u0439, \u044f\u0437\u044b\u043a.\nData: Mixed script: English\u2013Greek\n\u2022 is, biblical, is, will, is, without, self-benefit)., is, feelings, feelings, it, be, feeling, being, high, is, by, his, is, by, will, mostly, sexual, \u201dintimate, well, refined, his, definition:, is, initially, felt, with, it, beauty, within, beauty, itself., use, \u201dwithout, helps, soul, beauty, spiritual, youthful, beauty, feel, suggesting, sensually, spiritual, finding, its, like, finding, all, seek\n\u2022 (\u1f00\u03b3\u03ac\u03c0\u03b7, (\u1f14\u03c1\u03c9\u03c2\n\u2022 Ag\u00e1pe, \u201dlove:, brotherly, love, love, of, God, for, of, for, in, known, \u201dlove, 1, 13, throughout, New, brotherly, love, affection, good, love, love, given, or, not, person, continues, love, (even, in, for, one\u2019s, for, spouse, refer, love, of, content, or, holding, one, in, unconditional, love, of, God, for, of, love, \u201dto, good, of, \u00c9ros, \u201dlove, of, e, Modern, Greek, word, love.\u201d, own, Although, eros, for, person, contemplation, becomes, of, person, or, even, becomes, of, not, of, of, love, of, word, mean, In, Symposium, work, on, subject, eros, knowledge, of, of, \u201dForm\u201d, of, erotic, \u2013, even, love, non-corporeal, of, is, Lovers, philosophers, through, of,\n\u2022 ag\u00e1p\u0113), means, esp., charity;, the, man, and, man, God.\u201d, Agape, used, the, passage, as, the, chapter,\u201d, Corinthians, and, described, there, and, the, Testament, as, and, benevolence., Whether, the, returned, the, to, any, Agape, also, used, ancient, texts, to, denote, children, and, the, a, and, was, also, used, to, to, a, feast., It, can, also, described, as, the, regard., Agape, used, Christians, to, express,\n115\nthe, children., type, was, further, explained, omas, Aquinas, as, the, another.\u201d, \u00e9r\u014ds), means, the, passion.\u201d, \u201derotas\u201d, means, It, can, also, apply, to, dating, relationships, as, as, marriage., Plato, a, an, appreciation, the, that, appreciation, Plato, does, talk, physical, araction, as, a, necessary, part, hence, the, the, platonic, to, physical, araction.\u201d, the, the, most, famous, ancient, the, Plato, has, Socrates, argue, that, the, recall, and, contributes, to, an, understanding, truth, the, ideal, that, leads, us, humans, to, desire, thus, that, that, based, aspires, to, the, plane, existence;, that, truth, just, any, truth, leads, to, transcendence., and, are, inspired, to, truth, the, means, eros.\nData: Mixed script: English\u2013Spanish\u2013Arabic \u2022 \u060c\u0646\u0648\u062f \u060c\u0629\u062f\u062d\u0627\u0648 \u060c\u0629\u0631\u0638\u0646\u0628 \u060c\u0627\u0647\u062a\u0644\u0627\u0633\u0631 \u060c\u0644\u0642\u0646\u062a \u060c\u0646\u0627\u0654 \u060c\u064a\u063a\u0628\u0646\u064a \u060c\u0629\u0645\u0627\u0644\u0639\u0644\u0627 \u060c\u0646\u0627\u0654\u0641 \u060c\u0627\u0645\u0648\u0645\u0639\u0648 \u060c\u0646\u064a\u0639\u0645 \u060c\u0621\u064a\u0634 \u060c\u0646\u0639 \u060c\u0631\u0628\u0639\u064a \u060c\u064a\u0630\u0644\u0627 \u060c\u0645\u0633\u0631\u0644\u0627 \u060c\u064a\u0646\u0639\u064a \u060c\u0632\u0645\u0631\u0644\u0627\n\u060c\u0645\u062f\u062e\u062a\u0633\u0627 \u060c\u0646\u0645 \u060c\u0631\u062b\u0643\u0627\u0654 \u060c\u0646\u0643\u0644\u0648 \u060c\u062a\u0627\u0645\u0627\u0644\u0639\u0644\u0627 \u060c\u0627\u0648\u0645\u062f\u062e\u062a\u0633\u0627\u0654 \u060c\u0642\u064a\u0631\u063a\u0627\u0654\u0644\u0627\u0648 \u060c\u0646\u064a\u064a\u0631\u0635\u0645\u0644\u0627 \u060c\u0621\u0627\u0645\u062f\u0642 \u060c\u0646\u0627\u0654 \u060c\u0641\u0648\u0631\u0639\u0645\u0644\u0627 \u060c\u0646\u0645 \u060c\u0648 \u060c\u062a\u0627\u0645\u0644\u0643 \u060c\u0629\u064a\u0627\u0644 \u060c\u0629\u062c\u0627\u062d\u0644\u0627 \u0645\u0647 \u060c\u062a\u0627\u0645\u0627\u0644\u0639\u0644\u0627\n\u2022 ribbon, symbol, mourning., ribbon, mourning, El, un, y, un, en\n\u2022 black, is, a, of, remembrance, or, Wearing, or, displaying, a, black, has, been, used, for, remembrance, tragedies, or, as, a, political, statement., cresp\u00f3n, negro, o, lazo, negro, es, s\u00edmbolo, utilizado, por, personas, estados, sociedades, organizaciones, representando, sentimiento, pol\u00edtico-social, se\u00f1al, de, duelo.\n\u2022 A, POW/MIA\nData: Mixed script: English\u2013Chinese \u2022 e, Chinese, (simplified, traditional, Chinese:, invoked, motivational, speaking, because, the, composed, characters, that, represent, linguists, have, criticized, this, usage, because, the, component, (simplified, Chinese:, traditional, Chinese:, has, other, besides, Chinese, certain, some, be, based, the, Chinese, that, the, e, numbers, believed, have, because, their, similar, words, that, have, positive\n\u2022 (\u4e0d\u5229)\n\u2022 Western, can, and, Some, meanings, In, are, number, name, and, are, meanings, names, meanings.\n\u2022 0, 6, 8, 9\n\u2022 \u201dcrisis\u201d, is, auspicious, inauspicious, sounds, sound\n\u2022 for, pinyin:, frequently, in, word, of, two, \u201ddanger\u201d, \u201dopportunity\u201d., pronounced, tradition, by, or, on, word, to., to\n\u2022 \u5371\u673a;,\u5371\u6a5f;, w\u0113ij\u012b), j\u012b,\u673a;,\u6a5f), (\u5409\u5229)\n116\nData: Mixed script: Ukrainian\u2013Russian\n\u2022 \u0439, \u0420\u0443\u0441\u044c, \u043c\u043e\u0440\u0435\u0439., \u0420\u043e\u0441\u0441\u0438\u0435\u0439, \u0411\u0435\u043b\u043e\u0440\u0443\u0441\u0441\u0438\u0435\u0439, \u041f\u043e\u043b\u044c\u0448\u0435\u0439, \u0421\u043b\u043e\u0432\u0430\u043a\u0438\u0435\u0439, \u0412\u0435\u043d\u0433\u0440\u0438\u0435\u0439, \u0420\u0443\u043c\u044b\u043d\u0438\u0435\u0439\n\u2022 \u0430\u043b\u0435, 9\u201413, \u044e\u0433\u0435, \u0418\u043c\u0435\u0435\u0442, \u041c\u043e\u043b\u0434\u0430\u0432\u0438\u0435\u0439.\n\u2022 \u0456\u0441\u043d\u0443\u0432\u0430\u043b\u0438, \u0456\u043d\u0448\u0438\u0445\n\u2022 \u0427\u0451\u0440\u043d\u043e\u0433\u043e, \u0410\u0437\u043e\u0432\u0441\u043a\u043e\u0433\u043e, \u0433\u0440\u0430\u043d\u0438\u0446\u0443\n\u2022 \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0438\n\u2022 \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457, \u0423\u043a\u0440\u0430\u0457\u043d\u0438, \u043f\u0443\u043d\u043a\u0442\u043e\u043c, \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0457, \u0438, \u0441\u0443\u0445\u043e\u043f\u0443\u0442\u043d\u0443\u044e, \u0438\n\u2022 \u0412\u0456\u0434\u0434\u0430\u0432\u043d\u0430, \u043d\u0430, \u0434\u0435\u0440\u0436\u0430\u0432\u0438, \u0441\u043a\u0456\u0444\u0456\u0432, \u0441\u0430\u0440\u043c\u0430\u0442\u0456\u0432, \u0433\u043e\u0442\u0456\u0432, \u043d\u0430\u0440\u043e\u0434\u0456\u0432, \u0432\u0456\u0434\u043f\u0440\u0430\u0432\u043d\u0438\u043c, \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u043e\u0441\u0442\u0456, \u041d\u0430, \u0432\u043e\u0434\u0430\u043c\u0438\n\u2022 \u0442\u0430, \u0432\u0432\u0430\u0436\u0430\u0454\u0442\u044c\u0441\u044f, \u041a\u0438\u0457\u0432\u0441\u044c\u043a\u0430, \u0441\u0442\u043e\u043b\u0456\u0442\u0442\u044f., \u043e\u043c\u044b\u0432\u0430\u0435\u0442\u0441\u044f, \u0441\nData: Pali: abbha\n\u2022 (nt.), nt., Sk., \\\u201ddark, Idg., cp., Gr., Lat., Sk., water, Gr., water]., dark), at, SnA, S, at, It, Sn, (cp., SnA, Sn, S\n\u2022 &, A, A), ., J, 251)., 1, 1064;, 249, 250;, 12)., 64;, 348)., 382).\n\u2022 viz., 134., 101, 581, f., 289.\n\u2022 53, 295, 273, 487, 3, 617, 317, 348, 239., 687\n\u2022 cloud\\\u201d;, also, cloud, cloudy, <smallcaps> ii. </smallcaps>, =, list, is, <smallcaps> i. </smallcaps>, (\u00b0sama, <smallcaps> vi. </smallcaps>, (abbha\u014b, <smallcaps> iv. </smallcaps>, (n\u012bl\u00b0, As, Dhs, DhsA, (used, (=, clouds, cloud, (also, as\n\u2022 m., adj.\n\u2022 abhra, (mahiy\u0101, VvA, acch\u0101desi);, Pv, PvA, \\\u201ddull\\\u201d;, val\u0101haka);, Vv, val\u0101haka\u2013 sikhara\n\u2022 <at>a)fro\\\\s</at>, froth, of, <superscript> 9 </superscript>, <superscript> s. </superscript>, <superscript> 1 </superscript>\n\u2022 later, scum, rain;, ambha, rain, a, Miln, (megho, Miln), n\u012bla\u2013megha, sense, expl, , Dh\n117\n\u2022 *m\u030abhro, <at>o)/mbros</at>, ambu, mass, to, obscure, moon\u2013, <b>abbha\u014b, mahik\u0101 </b>, <b>dh\u016b-, marajo</b>, <b>R\u0101hu</b>, pabbata, rajo, <b>abbh\u0101</b>, by, perhaps, <b>abbh\u0101maa</b>, <br, /><b>\u2013k\u016b\u1e6da</b>, or, summit, storm\u2013cloud, <b>\u2013ghana</b>, <b>\u2013pa\u1e6dala</b>, mass, <b>\u2013mua</b>, from, abbh\u0101mua, <b> \u2013sa\u014bvil\u0101pa</b>,\n\u2022 [Vedic, imber, Oir, (dense, Vin, in, things, that, sunshine, is, referred, mountain, like, thunder\u2013cloud);, the, point, thick, free, thundering\nData: Pali: abhijjhitar\n\u2022 <smallcaps>i.</smallcaps>, v., l., <smallcaps>v.</smallcaps>\n\u2022 abhijjhita, abhijjh\u0101tar, \u00b0itar), \u00b0itar, \u00b0\u0101tar).,\n\u2022 [n., ag., fr., med., M, 287, (T., =, A, 265\n\u2022 in, function], one, who, covets\nData: Pali: ajja\n\u2022 Ajja\uff0c&, Ajj\u0101,\uff08adv.\uff09, base, a3\uff09, diva\uff09\uff0cthus, Dh.326\uff1b, ajj\u0101\uff1b, v., PvA.59\uff09\uff1b, PvA.6\uff0c23\uff1b, phrase, ajjatagge, ajjato, agge\uff08?\uff09, ajja-tagge\uff0csee, agga3\uff09,\uff08adv.\uff09 , the, 32\uff0c23.,\uff08Page\n\u2022 \u2039-\u203a, \u2013k\u0101la\u1e41\n\u2022 [Vedic, &, +, being,\uff08see,\uff08see,\uff08read, as,\uff08=, Mhvs,\uff08=, +, Mhvs\n\u2022 of, of,\u201con,\u201cfood\u201d\uff09\uff1b\n\u2022 and, an, old, not\n\u2022 adya, ady\u0101\uff0ca, dy\u0101\uff0ca\u00b0, dy\u0101, dyaus, day\u201d], to-day\uff0cnow, bahuta\u1e41, with, day, \u2013divasa, day\n\u2022 demonstr., pron., Loc., this, Kern\uff0cToev., s., Freq., or, from, this, onward\uff0chenceforth, this, morning, present\n\u2022 Sn.75\uff0c153\uff0c158\uff0c970\uff0c998\uff1b, J.I\uff0c279\uff1b, III\uff0c425, Pv.I\uff0c117, id\u0101ni, 15\uff0c64., in, Vin.I\uff0c18\uff1b, D.I\uff0c85\uff1b, DA.I\uff0c235., J.VI\uff0c180\uff1b, 10\uff09\n118\nData: Pali: g\u016bhan\u0101 \u2022 Pug\uff0e19\uff0eCp\uff0epari\u00b0\uff0e\uff08Page\n\u2022 G\u016bhan\u0101\uff0c\uff08f.\uff09, [abstr\uff0efr\uff0eg\u016bhati]=g\u016bhan\u0101\n\u2022 253\uff09,\uff08q\uff0ev.\uff09\nData: Pali: pacati \u2022 Vin\uff0eIV\uff0c264\uff1b, N\uff0eS\uff0eII\uff0c225\uff0cPvA\uff0e10\uff0c14\uff0e\u2013, D\uff0eI\uff0c52\n\u2022 DA\uff0eI\uff0c159\uff0cwhere, 382\uff09\n\u2022 in\n\u2022 at, &\n\u2022 cook\uff0cp\u03adpwn, cook\uff0cboil\uff0croast\n\u2022 Pacati\uff0c[Ved\uff0epacati\uff0cIdg\uff0e*peq\u01d4\u014d\uff0cAv\uff0epac-\uff1b, Obulg\uff0epeka, to, fry\uff0croast\uff0c Lith\uff0ckep\u016b, ripe], to, fig\uff0etorment, purgatory,\uff08trs\uff0eand, pacitv\u0101, aer, roasting, ppr\uff0epacanto, tormenting\uff0cGen\uff0epacato,\uff08+Caus\uff0ep\u0101cayato\uff09, read, pacato, for, paccato\uff0cby, pare, pp\uff0epakka, Caus\uff0epac\u0101peti, p\u0101ceti, Pass\uff0epaccati, to, roasted, or, tormented\n\u2022 bake\uff0cGr\uff0ep\u03adssw, intrs\uff0e\uff09\uff1aNiraye,\uff08expld, da\u1e47\u1e0dena, p\u012b\u1e37entassa\uff09\uff0e\u2013,\uff08q\uff0ev\uff0e\uff09\uff0e \u2039-\u203a,\uff08q\uff0ev\uff0e\uff09\uff0e\u2013, be,\uff08q\uff0ev\uff0e\uff09\uff0e\uff08Page"}, {"heading": "Normalized data", "text": "\u2022 pacati, peka, p\u03adssw, p\u03adpwn, pacitv\u0101, ppr., pacanto, Gen., pacato, (+Caus., p\u0101cayato), pacato, paccato, pare, p\u012b\u1e37entassa)., pp., pakka, Caus., pac\u0101peti, Pass., paccati\n\u2022 *peq\u01d4\u014d, bake\n\u2022 pac-;, 264;, 52, &, 382)\n\u2022 10,14.\u2013, 159, \u2013, <->, \u2013\n\u2022 fry, Niraye, I, I, by\n\u2022 Av., Obulg., Gr., (trs., D., DA., (q.v.)., (q.v.)., (q.v.).\n\u2022 [Ved., to, roast, kep\u016b, cook, ripe], to, cook, roast, torment, purgatory, and, aer, roasting, tormenting, (expld, at, where, read, for, da\u1e47\u1e0dena, p\u0101ceti, to, be, roasted, or, tormented, (Page\n\u2022 Pacati, Idg., Lith, boil, Vin.IV, fig., in, intrs.):, in, N.S.II,225,PvA.\n119\nData: Twier 1 (Greek\u2013English)\n\u2022 BUSINESS, EXCELLENCE.\n\u2022 \u039c\u03cc\u03bb\u03b9\u03c2, \u03c8\u03ae\u03c6\u03b9\u03c3\u03b1, \u03b1\u03c5\u03c4\u03ae, \u03c4\u03b7, \u03bb\u03cd\u03c3\u03b7, Internet, of, \u03c3\u03c4\u03bf, \u03b4\u03b9\u03b1\u03b3\u03c9\u03bd\u03b9\u03c3\u03bc\u03cc\n\u2022 ings, IT\nData: Twier 2 (French\u2013English)\n\u2022 Keynote, \u201ce, collective, of, science-publish, or, perish;, it, all, that, counts?\u201d\n\u2022 Demain, 18h, par\n\u2022 #dhiha6, David\n\u2022 @dhiparis, dynamics, is\nData: Twier 3 (French\u2013English)\n\u2022 #FWWC2015\n\u2022 breuvages, go,\n\u2022 Food, Edmonton, to, for, the\n\u2022 in, waiting, #bilingualism\n\u2022 and, are, ready, just, fans\nData: Twier 4 (English\u2013Polish)\n\u2022 comes, from, with, two, crates, of, strawberries, jackets, omg\n\u2022 my, dad, poland, and, adidas\n\u2022 back, \u017cubr\u00f3wka\nData: Twier 5 (Transliterated Amharic\u2013English)\n\u2022 (coffee\n\u2022 bread). is, our\n\u2022 Buna, dabo, naw\n120"}], "references": [{"title": "Java-ML: A Machine Learning Library", "author": ["T. Abeel", "Y.V. de Peer", "Y. Saeys"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Abeel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abeel et al\\.", "year": 2009}, {"title": "Interactive data mining with 3D-parallel-coordinate-trees", "author": ["E. Achtert", "H. Kriegel", "E. Schubert", "A. Zimek"], "venue": "In Proceedings of the ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Achtert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Achtert et al\\.", "year": 2013}, {"title": "An unsupervised system for identifying English inclusions in German text", "author": ["B. Alex"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Alex,? \\Q2005\\E", "shortCiteRegEx": "Alex", "year": 2005}, {"title": "Integrating language knowledge resources to extend the English inclusion classifier to a new language", "author": ["B. Alex"], "venue": "In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC). European Language Resources Association", "citeRegEx": "Alex,? \\Q2006\\E", "shortCiteRegEx": "Alex", "year": 2006}, {"title": "Automatic detection of English inclusions in mixed-lingual data with an application to parsing", "author": ["B. Alex"], "venue": "PhD thesis,", "citeRegEx": "Alex,? \\Q2007\\E", "shortCiteRegEx": "Alex", "year": 2007}, {"title": "Using Foreign Inclusion Detection to Improve Parsing Performance", "author": ["B. Alex", "A. Dubey", "F. Keller"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Alex et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Alex et al\\.", "year": 2007}, {"title": "Zum Erkennen von Anglizismen im Deutschen: der Vergleich von einer automatisierten mit einer manuellen Erhebung", "author": ["B. Alex", "A. Onysko"], "venue": "Strategien der Integration und Isolation nicht-nativer Einheiten und Strukturen,", "citeRegEx": "Alex and Onysko,? \\Q2010\\E", "shortCiteRegEx": "Alex and Onysko", "year": 2010}, {"title": "On prediction using variable order Markov models", "author": ["R. Begleiter", "R. El-Yaniv", "G. Yona"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Begleiter et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Begleiter et al\\.", "year": 2004}, {"title": "Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the first workshop on graph based methods for natural language processing, pages 73\u201380", "author": ["C. Biemann"], "venue": null, "citeRegEx": "Biemann,? \\Q2006\\E", "shortCiteRegEx": "Biemann", "year": 2006}, {"title": "\ue049e TIGER treebank", "author": ["S. Brants", "S. Dipper", "S. Hansen", "W. Lezius", "G. Smith"], "venue": "In Proceedings of the workshop on treebanks and linguistic theories,", "citeRegEx": "Brants et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "Algebraic complexity theory, volume", "author": ["P. B\u00fcrgisser", "M. Clausen", "M.A. Shokrollahi"], "venue": null, "citeRegEx": "B\u00fcrgisser et al\\.,? \\Q1997\\E", "shortCiteRegEx": "B\u00fcrgisser et al\\.", "year": 1997}, {"title": "Improving language models by clustering training sentences", "author": ["D. Carter"], "venue": "In Proceedings of the fourth conference on Applied natural language processing,", "citeRegEx": "Carter,? \\Q1994\\E", "shortCiteRegEx": "Carter", "year": 1994}, {"title": "N-gram-based text categorization", "author": ["W.B. Cavnar", "J.M. Trenkle"], "venue": "In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,", "citeRegEx": "Cavnar and Trenkle,? \\Q1994\\E", "shortCiteRegEx": "Cavnar and Trenkle", "year": 1994}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "In Proceedings of the 34th annual meeting on Association for Computational Linguistics,", "citeRegEx": "Chen and Goodman,? \\Q1996\\E", "shortCiteRegEx": "Chen and Goodman", "year": 1996}, {"title": "Clustering Methods for Improving Language Models", "author": ["E. Dreyfuss", "I. Goodfellow", "P. Baumstarck"], "venue": null, "citeRegEx": "Dreyfuss et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dreyfuss et al\\.", "year": 2007}, {"title": "How many clusters are best?-an experiment", "author": ["R.C. Dubes"], "venue": "Pa\ue03cern Recognition,", "citeRegEx": "Dubes,? \\Q1987\\E", "shortCiteRegEx": "Dubes", "year": 1987}, {"title": "Statistical Identification of Language", "author": ["T. Dunning"], "venue": "Computing Research Laboratory,", "citeRegEx": "Dunning,? \\Q1994\\E", "shortCiteRegEx": "Dunning", "year": 1994}, {"title": "Good-turing smoothing without tears", "author": ["W. Gale", "G. Sampson"], "venue": "Journal of \ue048antitative Linguistics,", "citeRegEx": "Gale and Sampson,? \\Q1995\\E", "shortCiteRegEx": "Gale and Sampson", "year": 1995}, {"title": "\ue049e use of clustering techniques for language modeling\u2013application to Asian languages", "author": ["J. Gao", "J. Goodman", "J Miao"], "venue": "International Journal of Computational Linguistics and Chinese Language Processing,", "citeRegEx": "Gao et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2001}, {"title": "What every computer scientist should know about floating-point arithmetic", "author": ["D. Goldberg"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Goldberg,? \\Q1991\\E", "shortCiteRegEx": "Goldberg", "year": 1991}, {"title": "Language model size reduction by pruning and clustering", "author": ["J. Goodman", "J. Gao"], "venue": "In INTERSPEECH,", "citeRegEx": "Goodman and Gao,? \\Q2000\\E", "shortCiteRegEx": "Goodman and Gao", "year": 2000}, {"title": "A bit of progress in language modeling", "author": ["J.T. Goodman"], "venue": "Computer Speech and Language,", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Comparing two language identification schemes", "author": ["G. Grefenste\ue03ce"], "venue": "In Proceedings of the 3rd International conference on Statistical Analysis of Textual Data. JADT", "citeRegEx": "Grefenste\ue03ce,? \\Q1995\\E", "shortCiteRegEx": "Grefenste\ue03ce", "year": 1995}, {"title": "\ue049e minimum description length principle", "author": ["P.D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald", "year": 2007}, {"title": "A closer look at skip-gram modelling", "author": ["D. Guthrie", "B. Allison", "W. Liu", "L. Guthrie", "Y. Wilks"], "venue": "In Proceedings of the 5th international Conference on Language Resources and Evaluation", "citeRegEx": "Guthrie et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guthrie et al\\.", "year": 2006}, {"title": "\ue049e WEKA Data Mining So\ue039ware: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Wi\ue03cen"], "venue": "SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "Jain et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1999}, {"title": "Language Identification in Code-Switching Scenario", "author": ["N. Jain", "R.A. Bhat"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "Jain and Bhat,? \\Q2014\\E", "shortCiteRegEx": "Jain and Bhat", "year": 2014}, {"title": "Speech and language processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition", "author": ["D. Jurafsky", "J.H. Martin"], "venue": "Pearson Education India,", "citeRegEx": "Jurafsky and Martin,? \\Q2000\\E", "shortCiteRegEx": "Jurafsky and Martin", "year": 2000}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions", "citeRegEx": "Katz,? \\Q1987\\E", "shortCiteRegEx": "Katz", "year": 1987}, {"title": "Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods", "author": ["B. King", "S.P. Abney"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies,", "citeRegEx": "King and Abney,? \\Q2013\\E", "shortCiteRegEx": "King and Abney", "year": 2013}, {"title": "Language clustering with word co-occurrence networks based on parallel texts", "author": ["H. Liu", "J. Cong"], "venue": "Chinese Science Bulletin,", "citeRegEx": "Liu and Cong,? \\Q2013\\E", "shortCiteRegEx": "Liu and Cong", "year": 2013}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["B Logan"], "venue": "In Proceedings of the 1st International Symposium onMusic Information Retrieval (ISMIR)", "citeRegEx": "Logan,? \\Q2000\\E", "shortCiteRegEx": "Logan", "year": 2000}, {"title": "Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics, 2:27\u201340", "author": ["M. Lui", "J.H. Lau", "T. Baldwin"], "venue": null, "citeRegEx": "Lui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2014}, {"title": "Introduction to information retrieval, volume 1", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Foundations of statistical natural language processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "Novelty detection in learning systems", "author": ["S. Marsland"], "venue": "Neural computing surveys,", "citeRegEx": "Marsland,? \\Q2003\\E", "shortCiteRegEx": "Marsland", "year": 2003}, {"title": "TweetSafa: Tweet language identification", "author": ["I. Mendizabal", "J. Carandell", "D. Horowitz"], "venue": "TweetLID @ SEPLN", "citeRegEx": "Mendizabal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mendizabal et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On structuring probabilistic dependences in stochastic language modelling", "author": ["H. Ney", "U. Essen", "R. Kneser"], "venue": "Computer Speech & Language,", "citeRegEx": "Ney et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ney et al\\.", "year": 1994}, {"title": "X-means: Extending K-means with Efficient Estimation of the Number of Clusters", "author": ["D. Pelleg", "A.W. Moore"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML", "citeRegEx": "Pelleg and Moore,? \\Q2000\\E", "shortCiteRegEx": "Pelleg and Moore", "year": 2000}, {"title": "Distributional clustering of english words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Twi\ue03cer Language Identification using Rational Kernels and its potential application to Sociolinguistics", "author": ["J. Porta"], "venue": "TweetLID @ SEPLN", "citeRegEx": "Porta,? \\Q2014\\E", "shortCiteRegEx": "Porta", "year": 2014}, {"title": "Parallel Algorithms for Unsupervised Tagging", "author": ["S. Ravi", "S. Vassilivitskii", "V. Rastogi"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Ravi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2014}, {"title": "\ue049e power of amnesia: Learning probabilistic automata with variable memory length", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine learning,", "citeRegEx": "Ron et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Ron et al\\.", "year": 1996}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor", "J.C. Pla"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Unsupervised sequence segmentation by a mixture of switching variable memory Markov sources", "author": ["Y. Seldin", "G. Bejerano", "N. Tishby"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML),", "citeRegEx": "Seldin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2001}, {"title": "Overview for the First Shared Task on Language Identification in Code-Switched Data", "author": ["T. Solorio", "E. Blair", "S. Maharjan", "S. Bethard", "M. Diab", "M. Gohneim", "A. Hawwari", "F. AlGhamdi", "J. Hirschberg", "A Chang"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "Solorio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solorio et al\\.", "year": 2014}, {"title": "Graphing the distribution of English le\ue03cers towards the beginning, middle or end of words. http://www.prooffreader.com/2014/05/ graphing-distribution-of-english.html", "author": ["D. Taylor"], "venue": null, "citeRegEx": "Taylor,? \\Q2015\\E", "shortCiteRegEx": "Taylor", "year": 2015}, {"title": "Distributed word clustering for large scale classbased language modeling in machine translation", "author": ["J. Uszkoreit", "T. Brants"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Uszkoreit and Brants,? \\Q2008\\E", "shortCiteRegEx": "Uszkoreit and Brants", "year": 2008}, {"title": "Comparing clusterings: an overview. Universit\u00e4t Karlsruhe, Fakult\u00e4t f\u00fcr Informatik Karlsruhe", "author": ["S. Wagner", "D. Wagner"], "venue": null, "citeRegEx": "Wagner and Wagner,? \\Q2007\\E", "shortCiteRegEx": "Wagner and Wagner", "year": 2007}, {"title": "Text segmentation by language using minimum description length. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969\u2013978", "author": ["H. Yamaguchi", "K. Tanaka-Ishii"], "venue": null, "citeRegEx": "Yamaguchi and Tanaka.Ishii,? \\Q2012\\E", "shortCiteRegEx": "Yamaguchi and Tanaka.Ishii", "year": 2012}, {"title": "Hierarchical language identification based on automatic language clustering", "author": ["B. Yin", "E. Ambikairajah", "F. Chen"], "venue": "In INTERSPEECH,", "citeRegEx": "Yin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2007}, {"title": "Language model based on word clustering", "author": ["L. Yuan"], "venue": "In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation,", "citeRegEx": "Yuan,? \\Q2006\\E", "shortCiteRegEx": "Yuan", "year": 2006}, {"title": "Overview of TweetLID: Tweet language identification", "author": ["A. Zubiaga", "I. San Vicente", "P. Gamallo", "J.R. Pichel", "I. Alegria", "N. Aranberri", "A. Ezeiza", "V. Fresno"], "venue": "SEPLN", "citeRegEx": "Zubiaga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014).", "startOffset": 178, "endOffset": 221}, {"referenceID": 54, "context": "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014).", "startOffset": 178, "endOffset": 221}, {"referenceID": 27, "context": "Indeed, using \u201ctraditional\u201dmonolingual natural language processing components on mixed language data leads to miserable results (Jain and Bhat, 2014).", "startOffset": 128, "endOffset": 149}, {"referenceID": 5, "context": "For example, by identifying foreign language inclusions in an otherwise monolingual text, parser accuracy can be increased (Alex et al., 2007).", "startOffset": 123, "endOffset": 142}, {"referenceID": 12, "context": "1 N-Grams and rank order statistics Cavnar and Trenkle (1994) use an n-gram language model for language identification purposes.", "startOffset": 36, "endOffset": 62}, {"referenceID": 12, "context": "Cavnar and Trenkle (1994) collected 3713 Usenet texts with a cultural theme in different languages.", "startOffset": 0, "endOffset": 26}, {"referenceID": 16, "context": "2 N-Grams and maximum likelihood estimator Dunning (1994) also uses an n-gram language model for language identification purposes.", "startOffset": 43, "endOffset": 58}, {"referenceID": 16, "context": "In order to test the system, Dunning (1994) uses a specially constructed test corpus from a bilingual parallel translated English-Spanish corpus containing English and Spanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and 100 texts varying from 10 to 500 bytes for the test set.", "startOffset": 29, "endOffset": 44}, {"referenceID": 15, "context": "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words.", "startOffset": 0, "endOffset": 15}, {"referenceID": 12, "context": "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words.", "startOffset": 26, "endOffset": 52}, {"referenceID": 12, "context": "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words. \ue049e system implemented by Dunning (1994) can classify strings of 10 characters in length \u201cmoderately well\u201d, while strings of 50 characters or more are classified \u201cvery well\u201d.", "startOffset": 26, "endOffset": 245}, {"referenceID": 22, "context": "3 Trigrams and short words Grefenste\ue03ce (1995) compares trigrams versus short words for language identification.", "startOffset": 27, "endOffset": 46}, {"referenceID": 18, "context": "4 N-Grams and clustering Gao et al. (2001) present a system that augments n-gram language models with clustering techniques.", "startOffset": 25, "endOffset": 43}, {"referenceID": 13, "context": "P (wi|wi\u22122wi\u22121) = P (ci|ci\u22122ci\u22121)\u00d7 P (wi|ci\u22122ci\u22121ci) (9) Similarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models.", "startOffset": 68, "endOffset": 91}, {"referenceID": 13, "context": "P (wi|wi\u22122wi\u22121) = P (ci|ci\u22122ci\u22121)\u00d7 P (wi|ci\u22122ci\u22121ci) (9) Similarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models. In addition to Gao et al. (2001), they also use information about the subject-verb and verb-object relations of the sentence.", "startOffset": 68, "endOffset": 218}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al.", "startOffset": 0, "endOffset": 252}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al.", "startOffset": 0, "endOffset": 326}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)).", "startOffset": 0, "endOffset": 348}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)). Carter (1994) shows that the subdivision into smaller clusters increases the accuracy of bigram language models, but not trigram models.", "startOffset": 0, "endOffset": 364}, {"referenceID": 5, "context": "For example in (Alex et al., 2007) they report an increase in F-Score of 4.", "startOffset": 15, "endOffset": 34}, {"referenceID": 2, "context": "5 Inclusion detection Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts.", "startOffset": 31, "endOffset": 85}, {"referenceID": 2, "context": "5 Inclusion detection Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts.", "startOffset": 31, "endOffset": 109}, {"referenceID": 9, "context": "by using inclusion detection when parsing a German text with a parser trained on the TIGER corpus (Brants et al., 2002).", "startOffset": 98, "endOffset": 119}, {"referenceID": 51, "context": "6 Clustering and spee\ue03b In the area of clustering and spoken language identification, Yin et al. (2007) present a hierarchical clusterer for spoken language.", "startOffset": 85, "endOffset": 103}, {"referenceID": 32, "context": "MFCC vectors are a way of representing acoustic signals (Logan et al., 2000). \ue049e signal is first divided into smaller \u2018frames\u2019, each frame is passed through the discrete Fourier transform and only the logarithm of the amplitude spectrum is retained (Logan et al., 2000). \ue049e spectrum is then projected onto the \u2018Mel frequency scale\u2019, a scale that maps actual pitch to perceived pitch, \u201cas apparently the human auditory system does not perceive pitch in a linear manner\u201d (Logan et al., 2000). Finally, a discrete cosine transform is applied to the spectrum to get the MFCC representations of the original signal (Logan et al., 2000). Yin et al. (2007) show that their hierarchical clusterer outperforms traditionalAcoustic Gaussian Mixture Model systems.", "startOffset": 57, "endOffset": 650}, {"referenceID": 49, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al.", "startOffset": 28, "endOffset": 62}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text.", "startOffset": 63, "endOffset": 107}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language.", "startOffset": 63, "endOffset": 259}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al.", "startOffset": 63, "endOffset": 659}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.", "startOffset": 63, "endOffset": 752}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.", "startOffset": 63, "endOffset": 832}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation. King and Abney (2013) use weakly supervised methods to label the languages of words.", "startOffset": 63, "endOffset": 930}, {"referenceID": 30, "context": "One problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 33, "context": "One problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 32, "context": "Lui et al. (2014) consider the task as multi-label classification task.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Whereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to \u201csolve many applications with notable success\u201d (Begleiter et al., 2004).", "startOffset": 261, "endOffset": 285}, {"referenceID": 7, "context": "In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004).", "startOffset": 181, "endOffset": 205}, {"referenceID": 7, "context": "\ue049us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004).", "startOffset": 94, "endOffset": 118}, {"referenceID": 44, "context": "One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996).", "startOffset": 63, "endOffset": 81}, {"referenceID": 44, "context": "A PST is a tree over an alphabet \u03a3, with each node either having 0 (leaf nodes) or |\u03a3| children (non-terminal nodes) (Ron et al., 1996).", "startOffset": 117, "endOffset": 135}, {"referenceID": 44, "context": "Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996).", "startOffset": 79, "endOffset": 97}, {"referenceID": 44, "context": "Each edge is labeled by a symbol s \u2208 \u03a3 and the probability for the next symbol being s (Ron et al., 1996).", "startOffset": 87, "endOffset": 105}, {"referenceID": 23, "context": "\ue049e MDL principle avoids overfi\ue03cing of the model by favoring low complexity over goodness-of-fit (Gr\u00fcnwald, 2007).", "startOffset": 96, "endOffset": 112}, {"referenceID": 43, "context": "8 Predictive suffix trees Seldin et al. (2001) propose a system for automatic unsupervised language segmentation and protein sequence segmentation.", "startOffset": 26, "endOffset": 47}, {"referenceID": 7, "context": "Whereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to \u201csolve many applications with notable success\u201d (Begleiter et al., 2004). In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004). \ue049us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004). \ue049ere is no single VMM algorithm, but rather a family of related algorithms. One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996). A PST is a tree over an alphabet \u03a3, with each node either having 0 (leaf nodes) or |\u03a3| children (non-terminal nodes) (Ron et al., 1996). Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996). Each edge is labeled by a symbol s \u2208 \u03a3 and the probability for the next symbol being s (Ron et al., 1996). By modifying the Predictive Suffix Tree (PST) algorithm using the Minimum Description Length (MDL) principle, Seldin et al. (2001) end up with a non-parametric self-regulating algorithm.", "startOffset": 262, "endOffset": 1247}, {"referenceID": 18, "context": "1 N-Gram models Among supervised languagemodels, n-grammodels are very popular (Gao et al., 2001).", "startOffset": 79, "endOffset": 97}, {"referenceID": 12, "context": "An n-gram is a slice from the original string (Cavnar and Trenkle, 1994).", "startOffset": 46, "endOffset": 72}, {"referenceID": 24, "context": "Non-contiguous n-grams are also called skip-grams (Guthrie et al., 2006).", "startOffset": 50, "endOffset": 72}, {"referenceID": 24, "context": "In this parlance, contiguous n-grams can be regarded as 0-skip-n-grams (Guthrie et al., 2006).", "startOffset": 71, "endOffset": 93}, {"referenceID": 24, "context": "As can be seen from this example, the number of skip-grams ismore than two times higher than the number of contiguous n-grams, and this trend continues the more skips are allowed (Guthrie et al., 2006).", "startOffset": 179, "endOffset": 201}, {"referenceID": 12, "context": "O\ue039en, the word to decompose is padded with start and end tags in order to improve the model (Cavnar and Trenkle, 1994).", "startOffset": 92, "endOffset": 118}, {"referenceID": 12, "context": "\ue049e use of paddings allows themodel to capture details about character distribution with regard to the start and end of words (Cavnar and Trenkle, 1994).", "startOffset": 125, "endOffset": 151}, {"referenceID": 48, "context": "at the beginning of words, while the le\ue03cer \u2018w\u2019 occurs mainly at the beginning of words (Taylor, 2015).", "startOffset": 87, "endOffset": 101}, {"referenceID": 12, "context": "One advantage of n-gram models is that the decomposition of a string into smaller units reduces the impact of typing errors (Cavnar and Trenkle, 1994).", "startOffset": 124, "endOffset": 150}, {"referenceID": 12, "context": "Indeed, a typing error only affects a limited number of units (Cavnar and Trenkle, 1994).", "startOffset": 62, "endOffset": 88}, {"referenceID": 12, "context": "Due to this property, n-gram models have been shown to be able to deal well with noisy text (Cavnar and Trenkle, 1994).", "startOffset": 92, "endOffset": 118}, {"referenceID": 13, "context": "In order to avoid this problem, different smoothing techniques can be used (Chen and Goodman, 1996).", "startOffset": 75, "endOffset": 99}, {"referenceID": 13, "context": "\ue049e simplest smoothing technique is additive (Laplace) smoothing (Chen and Goodman, 1996).", "startOffset": 64, "endOffset": 88}, {"referenceID": 28, "context": "If we choose \u03bb = 1, we speak of \u201cadd one\u201d smoothing (Jurafsky and Martin, 2000).", "startOffset": 52, "endOffset": 79}, {"referenceID": 35, "context": "In practice, \u03bb < 1 is o\ue039en chosen (Manning and Sch\u00fctze, 1999).", "startOffset": 34, "endOffset": 61}, {"referenceID": 13, "context": "\ue049us, instead of using the actual count c, the count is taken to be c\u2217 (Chen and Goodman, 1996).", "startOffset": 70, "endOffset": 94}, {"referenceID": 29, "context": "Katz\u2019s back-off model (Katz, 1987) for instance calculates probability Pbo using the formula:", "startOffset": 22, "endOffset": 34}, {"referenceID": 8, "context": "2 Unsupervised clustering Clustering consists in the grouping of objects based on their mutual similarity (Biemann, 2006).", "startOffset": 106, "endOffset": 121}, {"referenceID": 8, "context": "Objects to be clustered are typically represented as feature vectors (Biemann, 2006); from the original objects, a feature representation is calculated and used for further processing.", "startOffset": 69, "endOffset": 84}, {"referenceID": 52, "context": "Clustering can be partitional or hierarchical (Yin et al., 2007).", "startOffset": 46, "endOffset": 64}, {"referenceID": 52, "context": "Partitional clustering divides the initial objects into separate groups in one step, whereas hierarchical clustering builds a hierarchy of objects by first grouping the most similar objects together and then clustering the next level hierarchy with regard to the existing clusters (Yin et al., 2007).", "startOffset": 281, "endOffset": 299}, {"referenceID": 8, "context": "\ue049e clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006).", "startOffset": 108, "endOffset": 123}, {"referenceID": 26, "context": "\ue049e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999).", "startOffset": 117, "endOffset": 136}, {"referenceID": 8, "context": "the angle between them (Biemann, 2006).", "startOffset": 23, "endOffset": 38}, {"referenceID": 26, "context": "In order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999).", "startOffset": 115, "endOffset": 134}, {"referenceID": 26, "context": "word starts with a capital le\ue03cer) (Jain et al., 1999).", "startOffset": 34, "endOffset": 53}, {"referenceID": 26, "context": "k-means, need the number of clusters to generate (Jain et al., 1999).", "startOffset": 49, "endOffset": 68}, {"referenceID": 26, "context": "When hard-clustering, an object can belong to one class only, while in so\ue039-clustering, an object can belong to one or more classes, sometimes with different probabilities (Jain et al., 1999).", "startOffset": 171, "endOffset": 190}, {"referenceID": 8, "context": "\ue049e clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006). \ue049e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999). \ue049ere are different metrics available. A frequently chosen metric is the cosine similarity that calculates the distance between two vectors, i.e. the angle between them (Biemann, 2006). In order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999). Features can be quantitative (e.g. word length) or qualitative (e.g. word starts with a capital le\ue03cer) (Jain et al., 1999). Most clustering algorithms, e.g. k-means, need the number of clusters to generate (Jain et al., 1999). \ue049e question how to best choose this key number has been addressed in-depth by Dubes (1987). Clustering can be so\ue039 or hard.", "startOffset": 109, "endOffset": 902}, {"referenceID": 46, "context": "\ue049is approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set.", "startOffset": 52, "endOffset": 73}, {"referenceID": 46, "context": "\ue049is approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set. However, the realization differs greatly. Whereas Seldin et al. (2001) use predictive suffix trees, I use n-gram language models.", "startOffset": 52, "endOffset": 189}, {"referenceID": 16, "context": "1 Implementation For the supervised language segmentation method, I implemented an n-gram language model as described by Dunning (1994). \ue049e n-gram language model is implemented as a character trigram model with non-linear back-off to bigram and unigram models.", "startOffset": 121, "endOffset": 136}, {"referenceID": 19, "context": "Indeed, multiplying very small numbers can lead to the result being approximated as zero by the computer when the numbers become too small to be represented as normalized number (Goldberg, 1991).", "startOffset": 178, "endOffset": 194}, {"referenceID": 10, "context": "Using the sum of logarithms avoids this problem and is less computationally expensive (B\u00fcrgisser et al., 1997).", "startOffset": 86, "endOffset": 110}, {"referenceID": 25, "context": "3 Unsupervised clustering In order to test the efficiency of clustering algorithms on the task of language segmentation, I looked at various algorithms readily available throughWEKA, \u201ca collection of machine learning algorithms for data mining tasks\u201d by the University of Waikato in New Zealand (Hall et al., 2009) and the Environment for Developing KDD-Applications Supported by Index-Structures (ELKI), \u201can open source data mining so\ue039ware [.", "startOffset": 295, "endOffset": 314}, {"referenceID": 1, "context": "] with an emphasis on unsupervised methods in cluster analysis and outlier detection\u201d by the Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen (Achtert et al., 2013).", "startOffset": 132, "endOffset": 154}, {"referenceID": 0, "context": "I also looked at JavaML, \u201ca collection of machine learning and data mining algorithms\u201d (Abeel et al., 2009), in order to integrate clusterers into my own code framework.", "startOffset": 87, "endOffset": 107}, {"referenceID": 40, "context": "In contrast, the x-means algorithm (Pelleg and Moore, 2000) estimates the number of clusters to generate itself.", "startOffset": 35, "endOffset": 59}, {"referenceID": 50, "context": "pairs (Wagner and Wagner, 2007).", "startOffset": 6, "endOffset": 31}, {"referenceID": 50, "context": "\ue049e Rand Index measures the accuracy of the clustering given a reference partition (Wagner and Wagner, 2007).", "startOffset": 82, "endOffset": 107}, {"referenceID": 50, "context": "However, it is criticized for being highly dependent on the number of clusters (Wagner and Wagner, 2007).", "startOffset": 79, "endOffset": 104}, {"referenceID": 50, "context": "It is similar to the Rand Index, but it disregards S00, the set of pairs that are clustered into different clusters in C and C \u2032 (Wagner and Wagner, 2007).", "startOffset": 129, "endOffset": 154}, {"referenceID": 50, "context": "\ue049e Fowlkes-Mallows Index has the undesired property of yielding high values when the number of clusters is small (Wagner and Wagner, 2007).", "startOffset": 113, "endOffset": 138}, {"referenceID": 34, "context": "According toManning et al. (2008), in the context of clustering evaluation the F(\u03b2) score is defined as", "startOffset": 12, "endOffset": 34}, {"referenceID": 34, "context": "By varying \u03b2, it is possible to give more weight to either precision (\u03b2 < 0) or recall (\u03b2 > 1) (Manning et al., 2008).", "startOffset": 95, "endOffset": 117}, {"referenceID": 50, "context": "As there is no ultimate measure and all measures of similarity have their drawbacks (Wagner and Wagner, 2007), all measures will be indicated in the results section.", "startOffset": 84, "endOffset": 109}, {"referenceID": 46, "context": "\ue049e work by Seldin et al. (2001) is similar to the work presented here.", "startOffset": 11, "endOffset": 32}, {"referenceID": 51, "context": "\ue049e work by Yin et al. (2007) and the work by Seldin et al.", "startOffset": 11, "endOffset": 29}, {"referenceID": 46, "context": "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis.", "startOffset": 23, "endOffset": 44}, {"referenceID": 46, "context": "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wri\ue03cen language.", "startOffset": 23, "endOffset": 108}, {"referenceID": 46, "context": "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wri\ue03cen language. As I concentrated on wri\ue03cen language, their work was not conducive to this thesis. In contrast, Seldin et al. (2001) present a work that looks promising.", "startOffset": 23, "endOffset": 336}], "year": 2015, "abstractText": "Language segmentation consists in finding the boundaries where one language ends and another language begins in a text wri\ue03cen in more than one language. \ue049is is important for all natural language processing tasks. \ue049e problem can be solved by training language models on language data. However, in the case of lowor no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform be\ue03cer than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. \ue049e weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. \ue049e results look promising, but there is room for improvement and a more thorough investigation should be undertaken.", "creator": " XeTeX output 2015.08.28:1436"}}}