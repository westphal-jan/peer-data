{"id": "1510.02824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "On the Complexity of Inner Product Similarity Join", "abstract": "a number of tasks in classification, information retrieval, recommendation systems, and record linkage reduce to the core problem of inner product similarity join ( ips join ) : identifying dimensions of vectors in column collection that have a visibly large inner product. ips join is well understood when vectors are normalized and any approximation of interior products even allowed. however, constructing general case matching vectors potentially have any length appears much more challenging. recently, new upper bounds based on autonomous locality - oriented hashing ( alsh ) and asymmetric embeddings are emerged, but little has been explored on the lower bound side. in further paper we initiate a systematic study of inner product similarity join, showing new lower and upper bounds. our main results exist :", "histories": [["v1", "Fri, 9 Oct 2015 21:10:12 GMT  (247kb,D)", "https://arxiv.org/abs/1510.02824v1", null], ["v2", "Tue, 20 Oct 2015 09:45:18 GMT  (104kb,D)", "http://arxiv.org/abs/1510.02824v2", null], ["v3", "Thu, 7 Apr 2016 11:36:25 GMT  (166kb,D)", "http://arxiv.org/abs/1510.02824v3", "in Proc. 35th ACM Symposium on Principles of Database Systems, 2016"]], "reviews": [], "SUBJECTS": "cs.DS cs.DB cs.LG", "authors": ["thomas d ahle", "rasmus pagh", "ilya razenshteyn", "francesco silvestri"], "accepted": false, "id": "1510.02824"}, "pdf": {"name": "1510.02824.pdf", "metadata": {"source": "CRF", "title": "On the Complexity of Inner Product Similarity Join", "authors": ["Thomas D. Ahle", "Rasmus Pagh", "Ilya Razenshteyn", "Francesco Silvestri"], "emails": ["thdy@itu.dk", "pagh@itu.dk", "ilyaraz@mit.edu", "fras@itu.dk"], "sections": [{"heading": null, "text": "\u2022 Approximation hardness of IPS join in subquadratic time, assuming the strong exponential time hypothesis.\n\u2022 New upper and lower bounds for (A)LSH-based algorithms. In particular, we show that asymmetry can be avoided by relaxing the LSH definition to only consider the collision probability of distinct elements.\n\u2022 A new indexing method for IPS based on linear sketches, implying that our hardness results are not far from being tight.\nOur technical contributions include new asymmetric embeddings that may be of independent interest. At the conceptual level we strive to provide greater clarity, for example by distinguishing among signed and unsigned variants of IPS join and shedding new light on the effect of asymmetry."}, {"heading": "1. INTRODUCTION", "text": "This paper is concerned with inner product similarity join (IPS join) where, given two sets P,Q \u2286 Rd, the task is to find for each point q \u2208 Q at least one pair1 (p, q) \u2208 P \u00d7Q where the inner product (or its absolute value) is larger than a given threshold s. Our results apply also to the problem where for each q \u2208 Q we seek \u2217The research leading to these results has received funding from the European Research Council under the European Union\u2019s Seventh Framework Programme (FP7/2007-2013) / ERC grant agreement no. 614331. 1Since our focus is on lower bounds, we do not consider the more general problem of finding all such pairs. Also note that from an upper bound side, it is common to limit the number of occurrences of each tuple in a join result to a given number k.\nthe vector p \u2208 P that maximizes the inner product, a search problem known in literature as maximum inner product search (MIPS) [43, 45]."}, {"heading": "Motivation", "text": "Similarity joins have been widely studied in the database and information retrieval communities as a mechanism for linking noisy or incomplete data. Considerable progress, in theory and practice, has been made to address metric spaces where the triangle inequality can be used to prune the search space (see e.g. [6, 61]). In particular, it is now known that in many cases it is possible to improve upon the quadratic time complexity of a naive algorithm that explicitly considers all pairs of tuples. The most prominent technique used to achieve provably subquadratic running time is locality-sensitive hashing (LSH) [25, 24]. In the database community the similarity join problem was originally motivated by applications in data cleaning [17, 10]. However, since then it has become clear that similarity join is relevant for a range of other data processing applications such as clustering, semi-supervised learning, query refinement, and collaborative filtering (see e.g. [44] for references and further examples). We refer to the recent book by Augsten and Bo\u0308hlen [11] for more background on similarity join algorithms in database systems.\nInner product is an important measure of similarity between real vectors, particularly in information retrieval and machine learning contexts [31, 48], but not captured by techniques for metric similarity joins such as [26, 61]. Teflioudi et al. [50] studied the IPS join problem motivated by applications in recommender systems based on latent-factor models. In this setting, a user and the available items are represented as vectors and the preference of a user for an item is given by the inner product of the two associated vectors. Other examples of applications for IPS join are object detection [23] and multi-class prediction [22, 28]. IPS join also captures the so-called maximum kernel search, a general machine learning approach with applications such as image matching and finding similar protein/DNA sequences [20]."}, {"heading": "Challenges of IPS join", "text": "Large inner products do not correspond to close vectors in any metric on the vector space, so metric space techniques cannot directly be used. In fact, there are reasons\nar X\niv :1\n51 0.\n02 82\n4v 3\n[ cs\n.D S]\n7 A\npr 2\n01 6\nto believe that inner product similarity may be inherently more difficult than other kinds of similarity search: Williams [56, 4] has shown that a truly subquadratic exact algorithm for IPS join would contradict the Strong Exponential Time Hypothesis, an important conjecture in computational complexity. On the upper bound side new reductions of (special cases of) approximate IPS join to fast matrix multiplication have appeared [51, 29], resulting in truly subquadratic algorithms even with approximation factors asymptotically close to 1. However, the approach of reducing to fast matrix multiplication does not seem to lead to practical algorithms, since fast matrix multiplication algorithms are currently not competitive on realistic input sizes. From a theoretical viewpoint it is of interest to determine how far this kind of technique might take us by extending lower bounds for exact IPS join to the approximate case.\nAnother approach to IPS join would be to use LSH, which has shown its utility in practice. The difficulty is that inner products do not admit locality-sensitive hashing as defined by Indyk and Motwani [45, Theorem 1]. Recently there has been progress on asymmetric LSH methods for inner products, resulting in subquadratic IPS join algorithms in many settings. The idea is to consider collisions between two different hash functions, using one hash function for query vectors and another hash function for data vectors [45, 46, 39]. However, existing ALSH methods give very weak guarantees in situations where inner products are small relative to the lengths of vectors. It is therefore highly relevant to determine the possibilities and limitations of this approach."}, {"heading": "Problem definitions", "text": "We are interested in two variants of IPS join that slightly differ in the formulation of the objective function. For notational simplicity, we omit the term IPS and we simply refer to IPS join as join. Let s > 0 be a given value. The first variant is the signed join, where the goal is to find at least one pair (p, q) \u2208 P \u00d7Q for each point q \u2208 Q with pT q \u2265 s. The second variant is the unsigned join which finds, for each point q \u2208 Q, at least one pair (p, q) \u2208 P\u00d7Q where |pT q| \u2265 s. We observe that the unsigned version can be solved with the signed one by computing the join between P and Q and between P and \u2212Q, and then returning only pairs where the absolute inner products are larger than s. Signed join is of interest when searching for similar or preferred items with a positive correlation, like in recommender systems. On the other hand, unsigned join can be used when studying relations among phenomena where even a large negative correlation is of interest. We note that previous works do not make the distinction between the signed and unsigned versions since they focus on settings where there are no negative dot products.\nOur focus is on approximate algorithms for signed and unsigned joins. Indeed, approximate algorithms allow us to overcome, at least in some cases, the curse of dimensionality without significantly affecting the final results. Approximate signed joins are defined as follows.\nDefinition 1 (Approximate signed join). Given two point sets P , Q and values 0 < c < 1 and s > 0, the signed (cs, s) join returns, for each q \u2208 Q, at least one pair (p, q) \u2208 P \u00d7Q with pT q \u2265 cs if there exists p\u2032 \u2208 P such that p\u2032T q \u2265 s. No guarantee is provided for q \u2208 Q where there is no p\u2032 \u2208 P with p\u2032T q \u2265 s.\nThe unsigned (cs, s) join is defined analogously by taking the absolute value of dot products. Indexing versions of signed/unsigned exact/approximate joins can be defined in a similar way. For example, the signed (cs, s) search is defined as follows: given a set P \u2282 Rd of n vectors, construct a data structure that efficiently returns a vector p \u2208 P such that pT q > cs for any given query vector q \u2208 Rd, under the promise that there is a point p\u2032 \u2208 P such that pT q \u2265 s (a similar definition holds for the unsigned case).\nAs already mentioned, LSH is often used for solving similarity joins. In this paper, we use the following definition of asymmetric LSH based on the definition in [45].\nDefinition 2 (Asymmetric LSH). Let Up denote the data domain and Uq the query domain. Consider a family H of pairs of hash functions h = (hp(\u00b7), hq(\u00b7)). Then H is said (s, cs, P1, P2)-asymmetric LSH for a similarity function sim if for any p \u2208 Up and q \u2208 Uq we have:\n1. if sim(p, q) \u2265 s then PrH[hp(p) = hq(q)] \u2265 P1;\n2. if sim(p, q) < cs then PrH[hp(p) = hq(q)] \u2264 P2.\nWhen hp(\u00b7) = hq(\u00b7), we get the traditional (symmetric) LSH definition. The \u03c1 value of an (asymmetric) LSH is defined as usual with \u03c1 = logP1/ logP2 [6]. Two vectors p \u2208 Up and q \u2208 Uq are said to collide under a hash function from H if hp(p) = hq(q)."}, {"heading": "1.1 Overview of results", "text": ""}, {"heading": "Hardness results", "text": "The first results of the paper are conditional lower bounds for approximate signed and unsigned IPS join that rely on a conjecture about the Orthogonal Vectors Problem (OVP). This problem consists in determining if two sets A,B \u2286 {0, 1}d, each one with n vectors, contain x \u2208 A and y \u2208 B such that xT y = 0. It is conjectured that there cannot exist an algorithm that solves OVP in O ( n2\u2212 ) time as soon as d = \u03c9 (log n), for any given constant > 0. Indeed, such an algorithm would imply that the Strong Exponential Time Hypothesis (SETH) is true [56].\nMany recent interesting hardness results rely on reductions from OVP, however we believe ours is the first example of using the conjecture to show the conditional hardness for an approximate problem. In particular we show the following result:\nTheorem 1. Let \u03b1 > 0 be given and consider sets of vectors P,Q with |Q| = n, |P | = n\u03b1. Suppose there\nexists a constant > 0 and an algorithm with running time at most dO(1)n1+\u03b1\u2212 , when d and n are sufficiently large and for all s > 0, for at least one of the following IPS join problems:\n1. Signed (cs, s) join of P,Q \u2286 {\u22121, 1}d with c > 0.\n2. Unsigned (cs, s) join of P,Q \u2286 {\u22121, 1}d with c = e\u2212o( \u221a logn/ log logn).\n3. Unsigned (cs, s) join of P,Q \u2286 {0, 1}d with c = 1\u2212 o(1)."}, {"heading": "Then the OVP conjecture is false.", "text": "Discussion. For the search problem, theorem 1 implies that, assuming the OVP conjecture, there does not exist a data structure for signed/unsigned (cs, s) inner product search with (nd)O(1) construction time and n1\u2212 dO(1) query time, for constant > 0. This follows by considering a join instance with \u03b1 constant small enough that we can build the data structure on P in time o(nd). We can then query over all the points of Q in time n1+\u03b1(1\u2212 )dO(1), contradicting the OVP conjecture. Our result can be seen as an explanation of why all LSH schemes for IPS have failed to provide sub-linear query times for small s. As the theorem however does not cover the case where c is very small, e.g. n\u2212\u03b4 for unsigned {\u22121, 1}d, we show in section 4 that for such approximation requirements, there are indeed useful data structures.\nWe stress that the hardness result holds for algorithms solving signed/unsigned (cs, s) joins for any c in the specified range and all s > 0. It is possible to show complete relations between hard values of c, s and d, but for the sake of clearnes, we have prefered to optimize the largest range of hard c\u2019s. For intuition we can say, that the exact instances of (cs, s) joins that are hard, turn out to be the ones where s/d and cs/d are very small, that is when we have to distinguish nearly orthogonal vectors from very nearly orthogonal vectors. If we inspect the proofs of Theorem 1 and Lemma 3, we see that for unsigned join in {\u22121, 1}d, the hard case has cs/d around n1/ log logn. Similarly for {0, 1}d join, cs ends up at just barely \u03c9(1), while the d is as high as no(1). It is interesting to note for {0, 1} that if cs had been slightly lower, at O(1), we could have solved the OVP problem\nexact in subquadratic time using an n ( no(1)\nO(1)\n) = n1+o(1)\nalgorithm. It is interesting to compare our conditional lower bound to the recent upper bounds by Karppa et al. [29], who get sub-quadratic running time for unsigned join of normalized vectors in {\u22121, 1}d, when log(s/d)/ log(cs/d) is a constant smaller than 1.2 Our next Theorem 2 shows that we cannot hope to do much better than this, though it does not completely close the gap. A hardness result for log(s/d)/ log(cs/d) = 1\u2212 o(1) is still an interesting open problem. However, while the algorithm of Karppa 2More precisely they need log(s/d)/ log(cs/d) < 2/\u03c9, where \u03c9 is the matrix multiplication constant. Note that the d term is due to normalization.\net al. works even for dimension n1/3, our bound only requires the dimension to be slightly larger than polylog, so it may well be that their algorithm is optimal, while another algorithm with a higher dependency on the dimension matches our bound from above.\nTheorem 2. Let \u03b1 > 0 be given and consider sets of vectors P,Q with |Q| = n, |P | = n\u03b1. Suppose there exists a constant > 0 and an algorithm with running time at most dO(1)n1+\u03b1\u2212 , when d and n are sufficiently large and for all s > 0, for at least one of the following IPS join problems:\n1. Unsigned (cs, s) over {\u22121, 1}d where log(s/d)log(cs/d) = 1\u2212 o(1/ \u221a log n)\n2. Unsigned (cs, s) over {0, 1}d where log(s/d)log(cs/d) = 1\u2212 o(1/ log n)"}, {"heading": "Then the OVP conjecture is false.", "text": "The {\u22121, 1}d case seems to be harder than the {0, 1}d case. In fact Valiant [51] reduces the general case of P,Q \u2286 Rd to the case P,Q \u2286 {\u22121, 1}d using the Charikar hyperplane LSH [15]. Another piece of evidence is that we can achive runtime n1+ log(s/d) log(cs/d) using LSH for {0, 1}d, but it is not known to be possible for {\u22121, 1}d. Furthermore there appears to be some hope for even better data dependent LSH, as we show in section 4.2. The {0, 1}d case is particularly interesting, as it is occurs often in practice, for example when the vectors represent sets. A better understanding of the upper and lower bounds for this case is a nice open problem. For an ellaborate comparison of the different upper and lower bounds, see Table 1. Techniques. From a technical point of view, the proof uses a number of different algebraic techniques to expand the gap between orthogonal and non-orthogonal vectors from the OVP problem. For the {\u22121, 1} we use an enhanced, deterministic version of the \u201cChebyshev embedding\u201d [51], while for the interesting {0, 1} part, we initiate a study of embeddings for restricted alphabets.\nInner product LSH lower bounds In the second part of the paper we focus on LSH functions for signed and unsigned IPS. We investigate the gap between the collision probability P1 of vectors with inner product (or absolute inner product) larger than s and the collision probability P2 of vectors with inner product (or absolute inner product) smaller than cs. As a special case, we get the impossibility result in [39, 45], that there cannot exist an asymmetric LSH for unbounded query vectors. Specifically we get the following theorem:\nTheorem 3. Consider an (s, cs, P1, P2)-asymmetric LSH for signed IPS when data and query domains are d-dimensional balls with unit radius and radius U respectively. Then, the following upper bounds on P1 \u2212 P2 apply:\n1. if d \u2265 1 and s \u2264 min{cU,U/(4 \u221a d}, we have P1\u2212P2 = O ( 1/log(d log1/c(U/s)) ) for signed and\nunsigned IPS;\n2. if d \u2265 2 and s \u2264 U/(2d), we have P1 \u2212 P2 = O (1/log(dU/(s(1\u2212 c)))) for signed IPS;\n3. if d > \u0398 ( U5/(c2s5) ) and s \u2264 U/8, we have P1 \u2212\nP2 = O (\u221a s/U ) for signed and unsigned IPS.\nIt follows that, for any given dimension d, there cannot exist an asymmetric LSH when the query domain is unbounded.\nDiscussion. The upper bounds for P1 \u2212 P2 translate into lower bounds for the \u03c1 factor, as soon as P2 is fixed. To the best of our knowledge, this is the first lower bound on \u03c1 that holds for asymmetric LSH. Indeed, previous results [37, 40] have investigated lower bounds for symmetric LSH and it is not clear if they can be extended to the asymmetric case. Techniques. The starting point of our proof is the same as in [39]: Use a collision matrix given by two sequences of data and query vectors that force the gap to be small. The proof in [39] then applies an asymptotic analysis of the margin complexity of this matrix [49], and it shows that for any given value of P1 \u2212 P2 there are sufficiently large data and query domains for which the gap must be smaller. Unfortunately, due to their analysis, an upper bound on the gap for a given radius U of the query domain is not possible, and so the result does not rule out very large gaps for small domains. Our method also highlights a dependency of the gap on the dimension, which is missing in [39]. In addition, our proof holds for d = 1 and only uses purely combinatorial arguments."}, {"heading": "IPS upper bounds", "text": "In the third part we provide some insights on the upper bound side. We first show that it is possible to improve the asymmetric LSH in [39, 46] by just plugging the best known data structure for Approximate Near Neighbor\nfor `2 on a sphere [9] into the reduction in [39, 12]. With data/query points in the unit ball, this LSH reaches \u03c1 = (1\u2212 s)/(1 + (1\u2212 2c)s). In the {0, 1} domain, this LSH improves upon the state of the art [46] for some ranges of c and s.\nThen we show how to circumvent the impossibility results in [39, 45] by showing that there exists a symmetric LSH when the data and query space coincide by allowing the bounds on collision probability to not hold when the data and query vectors are identical.\nWe conclude by describing a data structure based on the linear sketches for `p in [5] for unsigned (cs, s) search: for any given 0 < \u03ba \u2264 1/2, the data structure yields a c = 1/n\u03ba approximation with O\u0303 ( dn2\u22122/\u03ba ) construction\ntime and O\u0303 ( dn1\u22122/\u03ba ) query time. Theorem 1 suggests that we cannot substantially improve the approximation with similar performance.\nThe last data structure allows us to reach truly subquadratic time for c = 1/n\u03ba for the unsigned version in the {0, 1} and {\u22121, 1} domains for all value s. We note that the result in [29] also reaches subquadtratic time for the {\u22121, 1} case. However, it exploits fast matrix multiplication, whereas our data structure does not."}, {"heading": "1.2 Previous work", "text": ""}, {"heading": "Similarity join", "text": "Similarity join problems have been extensively studied in the database literature (e.g. [17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.g. [14, 21, 59]), and knowledge discovery (e.g. [3, 13, 54, 60, 62]). Most of the literature considers algorithms for particular metrics (where the task is to join tuples that are near according to the metric), or particular application areas (e.g. near-duplicate detection). A distinction is made between methods that approximate distances in the sense that we only care about distances up to some factor c > 1, and methods that consider exact distances. Known exact methods do not guarantee subquadratic running time. It was recently shown how approximate LSH-based similarity join can be made I/O-efficient [41]."}, {"heading": "IPS join", "text": "The inner product similarity for the case of normalized vectors is known as \u201ccosine similarity\u201d and it is well understood [16, 33, 43]. While the general case where vectors may have any length appears theoretically challenging, practically efficient indexes for unsigned search were proposed in [43, 30], based on tree data structures combined with a branch-and-bound space partitioning technique similar to k-d trees, and in [12] based on principal component axes trees. For document term vectors Low and Zheng [35] showed that unsigned search can be sped up using matrix compression ideas. However, as many similarity search problems, the exact version considered in these papers suffers from the curse of dimensionality [55].\nThe efficiency of approximate IPS approaches based on LSH is studied in [45, 39]. These papers show that a traditional LSH does exist when the data domain is the unit ball and the query domain is the unit sphere, while it does not exist when both domains are the unit ball (the claim automatically applies to any radius by suitably normalizing vectors). On the other hand an asymmetric LSH exists in this case, but it cannot be extended to the unbounded domain Rd. An asymmetric LSH for binary inner product is proposed in [46]. The unsigned version is equivalent to the signed one when the vectors are non-negative."}, {"heading": "Algebraic techniques", "text": "Finally, recent breakthroughs have been made on the (unsigned) join problem in the approximate case as well as the exact. Valiant [51] showed how to reduce the problem to matrix multiplication, when cs \u2248 O( \u221a n) and s \u2248 O(n), significantly improving on the asymptotic time complexity of approaches based on LSH. Recently this technique was improved by Karppa et al. [29], who also generalized the sub-quadratic running time to the case when log(s)/ log(cs) is small. In another surprising development Alman and Williams [4] showed that for d = O (log n) dimensions, truly subquadratic algorithms for the exact IPS join problem on binary vectors is possible. Their algorithm is based on an algebraic technique (probabilistic polynomials) and tools from circuit complexity."}, {"heading": "2. HARDNESS OF IPS JOIN", "text": "We first provide an overview of OVP and of the associated conjecture in next Section 2.1. Then, in Section 2.2, we prove Theorem 1 by describing some reductions from the OVP to signed/unsigned joins."}, {"heading": "2.1 Preliminaries", "text": "The Orthogonal Vectors Problem (OVP) is defined as follows:\nDefinition 3 (OVP). Given two sets P and Q, each one containing n vectors in {0, 1}d, detect if there exist vectors p \u2208 P and q \u2208 Q such that pT q = 0. OVP derives its hardness from the Strong Exponential Time Hypothesis (Williams [56]), but could potentially\nbe true even if SETH is not. We will therefore assume the following plausible conjecture:3\nConjecture 1 (OVP, [56]). For every constant > 0, there is no algorithm for OVP with |P | = |Q| = n and dimension d = \u03c9(log n) running in O(n2\u2212 ) time.\nThe conjecture does not hold for d = O(log n): recently Abboud et al. [1] have proposed an algorithm for OVP running in time n2\u22121/O(\u03b3 log 2 \u03b3), when d = \u03b3 log n. Thus, in order to disprove OVP, an algorithm must be strongly subquadratic when d = \u03b3 log n for all constant \u03b3 > 0.\nThe OVP conjecture, as usually stated, concerns the case where the two sets have equal size. However in order to eventually show hardness for data structures, we consider the following generalization of OVP, which follows directly from the original:\nLemma 1 (Generalized OVP). Suppose that there exist constants > 0 and \u03b1 > 0, and an algorithm such that for d = \u03c9 (log n) the algorithm solves OVP for P,Q \u2286 {0, 1}d where |P | = n\u03b1 and |Q| = n in time O(n1+\u03b1\u2212 ). Then OVP is false.\nProof. Without loss of generality assume \u03b1 \u2264 1 (otherwise is enough to invert the role of P and Q). Suppose we have an algorithm running in time O(n1+\u03b1\u2212 ) for some > 0. Take a normal OVP instance with |P | = |Q| = n. Split P into chunks Pi of size n\u03b1 and run the OVP algorithm on all pairs (Pi, Q). By our assumption this takes time n1\u2212\u03b1O(n1+\u03b1\u2212 ) = O(n2\u2212 ), contradicting OVP."}, {"heading": "2.2 Reductions from OVP", "text": "In this section we prove Theorem 1, about hardness of approximate joins. We will do this by showing the existence of certain efficient \u2018gap embeddings\u2019 that make orthogonality discoverable with joins. We need the following definition:\nDefinition 4 (Gap Embedding). An unsigned (d1, d2, cs, s)-gap embedding into the domain A is a pair of functions (f, g) : {0, 1}d1 \u2192 Ad\u20322 , where d\u20322 \u2264 d2, A \u2286 R, and for any x, y \u2208 {0, 1}d1 :\n|f(x)T g(y)| \u2265 s when xT y = 0 |f(x)T g(y)| \u2264 cs when xT y \u2265 1\nA \u2018signed embedding\u2019 is analogous, but without the absolute value symbols. We further require that the functions f and g can be evaluated in time polynomial to d2.\nGap embeddings connect to the join problem, by the following technical lemma:\nLemma 2. Suppose there exist a join algorithm for (un)signed (cs, s)-join over A and a family of (un)signed (d, 2o(d), cs, s)-gap embeddings into A, for all d large enough.\n3 We will use the name, OVP, for the problem as well as the conjecture. Sorry about that.\n\u2022 For given constants \u03b1 \u2265 0, and > 0, the algorithm has running time dO(1)n1+\u03b1\u2212 when |Q| = n and |P | = n\u03b1 for all n and d large enough.\n\u2022 The embedding has can be evaluated in time dO(1)2 . Then OVP can be solved in n1+\u03b1\u2212 time, and the conjecture is false.\nProof. First notice that for any function d2 = 2 o(d) we can take d = \u03c9(logn) growing slowly enough that d2 = n\no(1). To see this, assume d2(d) = 2\nf(d) where f(d) = o(d). Then we have f(d(n)) = o(d(n)) = o(1)d(n) that is f(d(n)) = f \u2032(n)d(n) for some f \u2032(n) = o(1). Now take d(n) = logn\u221a f \u2032(n) = \u03c9(logn) and we get d2(d(n)) = 2f \u2032(n)d(n) = 2 \u221a f \u2032(n) logn = no(1) as desired.\nHence, there is a family of (d(n), d2(n), cs, s)-gap embeddings for all n, where d(n) = \u03c9(logn) and d2(n) = no(1). By the generalized OVP lemma, for large enough n, we can thus take a hard OVP instance with |Q| = n, |P | = n\u03b1 and dimension d(n). Apply the coresponding gap embedding, (f, g), to the instance, such that the maximum inner product between f(P ), g(Q) is at least s if the OVP instance has an orthogonal pair and \u2264 cs otherwise. Now run the algorithm for (un)signed (cs, s) join on (f(P ), g(Q)), which produces the orthogonal pair, if it exists.\nIt remains to show that the running time of the above procedure is O(n1+\u03b1\u2212 \u2032 ) for some \u2032 > 0. But this is easy, since by assumption, performing the embedding takes time n1+o(1), and nunning the algorithm on vectors of dimension no(1) takes time n1+\u03b1\u2212 +o(1). So letting \u2032 = /2 suffices.\nThe last ingredient we need to show Theorem 1 is a suitable family of embeddings to use with Lemma 2:\nLemma 3. We can construct the following gap embeddings:\n1. A signed (d, 4d\u2212 4, 0, 4)-embedding into {\u22121, 1}. 2. An unsigned (d, (9d)q, (2d)q, (2d)qeq/ \u221a d/2)-embedding\ninto {\u22121, 1}, for any q \u2208 N+, d > 1.\n3. An unsigned (d, k2d/k, k\u22121, k)-embedding into {0, 1}, for any integer 1 \u2264 k \u2264 d.\nProof. We will use the following notation in our constructions: Let x y be the concatenation of vectors x and y;4 Let xn mean x concatenated with itself n times;5 And let x y mean the vectorial representation of the outer product xyT . Tensoring is interesting because of the following folklore property: (x1 x2)T (y1 y2) = trace (x1x T 2 ) T (y1y T 2 ) = trace x2(x T 1 y1)y T 2 = (x T 1 y1)(x T 2 y2). 4 for concatenation and for tensoring stresses their dual relationship with + and \u00d7 on the inner products in the embedded space. We note however that in general, it is only safe to commute \u2019es and \u2019es in an embedding (f, g), when both f and g are commuted equally. 5 If we wanted to further stress the duality between construction and embedding, we could define ~n to be the all 1 vector of length n. Then ~n x would stand for repeating x n times.\n(Embedding 1) The signed embedding is a simple coordinate wise construction:\nf\u0302(0) := ( 1,\u22121,\u22121) g\u0302(0) := ( 1, 1,\u22121) f\u0302(1) := ( 1, 1, 1) g\u0302(1) := (\u22121,\u22121,\u22121)\nsuch that f\u0302(1)T g\u0302(1) = \u22123 and f\u0302(0)T g\u0302(1) = f\u0302(1)T g\u0302(0) = f\u0302(0)T g\u0302(0) = 1. This, on its own, gives a (d, 3d, d\u2212 4, d) embedding, as non orthogonal vectors need to have at least one (1,1) at some position.\nWe can then translate all the inner products by \u2212(d\u2212 4):\nf(x) := f\u0302(x1) \u00b7 \u00b7 \u00b7 f\u0302(xn) 1d\u22124 g(x) := g\u0302(x1) \u00b7 \u00b7 \u00b7 g\u0302(xn) (\u22121)d\u22124\nwhich gives the (d, 4d\u2212 4, 0, 4) embedding we wanted. Note that the magnitudes of non orthogonal vectors may be large (\u22124d+ 4), but we do not care about those for signed embeddings.\n(Embedding 2) We recall the recursive definition of the qth order Chebyshev polynomial of first kind, with q \u2265 0 (see, e.g., [2] page 782):\nT0(x) = 1\nT1(x) = x Tq(x) = 2xTq\u22121(x)\u2212 Tq\u22122(x)\nThe polynomials have the following properties [51]:\n|Tq(x)| \u2264 1 when |x| \u2264 1\n|Tq(1 + )| \u2265 eq \u221a when 0 < < 1/2\nWe use the same coordinate wise transformation as in the signed embedding, but instead of translating by a negative value, we translate by adding d+ 2 ones, giving a (d, 4d+ 2, 2d\u2212 2, 2d+ 2) unsigned embedding. Let the vectors created this way be called x and y.\nOn top of that, we would like to construct an embedding for the polynomial Tq(u/2d), where Tq is the qth order Chebyshev polynomial of the first kind. However since this will not in general be interger, there is no hope for constructing it using {\u22121, 1}.\nLuckily it turns out we can construct an embedding for bqTq(u/b) for any integers b and q. Let (fq, gq) be the qth embedding of this type, defined by:\nf0(x), g0(y) := 1, 1\nf1(x), g1(y) := x, y\nfq(x) := (x fq\u22121(x)) 2 fq\u22122(x)\n(2d)2\ngq(y) := (y gq\u22121(y)) 2 (\u2212gq\u22122(y))(2d)\n2\nWe make the following observations:\n\u2022 If x and y are {\u22121, 1} vectors, then so are fq(x) and gq(y).\n\u2022 The inner product of the embedded vectors, fq(x)T gq(x)\nis a function of the original inner product:\nf0(x) T g0(y) = 1\nf1(x) T g1(y) = x T y\nfq(x) T gq(y) = 2x T y fq\u22121(x) T gq\u22121(y)\n\u2212 (2d)2fq\u22122(x)T gq\u22122(y)\nIndeed it may be verified from the recursive definition of Tq that fq(x) T gq(y) = (2d) nTq(x\nT y/2d) as wanted.\n\u2022 Let dq be the dimension of fq(x) and gq(y). Then we have:\nd0 = 1 d1 = 4d\u2212 4 dq = 2(4d\u2212 4)dq\u22121 + (2d)2dq\u22122\nIt can be verified that dq \u2264 (9d)q for any q \u2265 0 and d \u2265 8. Interestingly the (2d)2 concatenations don\u2019t increase dq significantly, while d\n2+ for any > 0 would have killed the simple exponential dependency.\n\u2022 Finally, with dynamic programming, we can compute the embeddings in linear time in the output dimension. This follows from induction over q.\nPutting the above observations together, we have for any integer q \u2265 0 a (d, (9d)q, (2d)q, (2d)qTq(1 + 1/d)) embedding. By the aforementioned properties of the Chebyshev polynomials, we have the desired embedding. We note that the Chebyshev embedding proposed by Valiant [51] can provide similar results; however, our construction is deterministic, while Valiant\u2019s is randomized.\n(Embedding 3) The third embedding maps into {0, 1}. The difficulty here is that without \u22121, we cannot express subtraction as in the previous argument. It turns out however, that we can construct the following polynomial:\n(1\u2212 x1y1)(1\u2212 x2y2) \u00b7 \u00b7 \u00b7 (1\u2212 xdyd)\nsince {0, 1} is closed under tensoring and\n1\u2212 xiyi = (1\u2212 xi, 1)T (yi, 1\u2212 yi)\nwhere 1 \u2212 xi, yi and 1 \u2212 yi are both in {0, 1}. The polynomial has the property of being 1 exactly when the two vectors are orthogonal and 0 otherwise.\nHowever we cannot use it directly with Lemma 2, as it blows up the dimension too much, d2 = 2\nd1 . Instead we \u201cchop up\u201d the polynomial in k chunks and take their sum:\nk\u22121\u2211 i=0 d/k\u220f j=1 (1\u2212 xik/d+jyik/d+j)\nThis uses just d2 = k2 d/k dimensions, which is more manageble. If k does not divide d, we can let the last \u201cchop\u201d of the polynomial be shorter than d/k, which only\nhas the effect of making the output dimension slightly smaller.\nFinally we get the gap s = k and cs = k\u22121. The later follows because for non orthogonal vectors, at least one chunk has a (1 \u2212 xiyi) terms which evaluates to zero. We thus have a (d, k2d/k, k\u2212 1, k)-embedding into {0, 1}. The explicit construction is thus:\nf(x) :=\nk\u22121\ni=0\nd/k\nj=1\n(1\u2212 xik/d+j , 1)\ng(x) :=\nk\u22121\ni=0\nd/k\nj=1\n(yik/d+j , 1\u2212 yik/d+j)\nAnd the running time is linear in the output dimension.\nFinally we parametize and prove Theorem 1. Proof. (Theorem 1) We first prove the bounds parametrized\nby c. To get the strongest possible results, we want to get c as small as possible, while keeping the dimension bounded by n\u03b4 for some \u03b4 > 0.\n1. The first embedding is already on the form (d, 2o(d), 0, 4), showing theorem 1 for signed (0, 4) join, and thus any c > 0.\n2. In the second embedding we can take q to be any function in o (\nd log d\n) , giving us a family of (d, 2o(d),\n2o(d), 2o(d)e o ( \u221a d log d ) ). Thus by lemma 2, and for a small enough d = \u03c9(logn) in OVP, we get that\neven c \u2264 e\u2212o ( \u221a log n log log n ) \u2264 1/polylog(n) is hard.\n3. Finally for the third embedding, we can pick any k = \u03c9(1) and less than d, to get a family of (d, 2o(d), k \u2212 1, k) embeddings. Again picking d = \u03c9(log n) small enough in OVP, we have by lemma 2 that c \u2264 (k \u2212 1)/k = 1 \u2212 1/k = 1 \u2212 o(1) is hard. Notice that this means any c not bounded away from 1 is hard.\nFor the bounds parametrized by log(s)/ log(cs), we need to tweak our families slightly differently. This in turn allows for hardness for shorter vectors than used in the previous results.\nProof. (Theorem 2) For embedding 1 the result follows directly as log(s/d)/ log(cs/d)\u2192\n0 as c\u2192 0. We have to remember that the results in theorem 1 are stated in terms of normalized s. For embedding 2 we calculate:\nlog(s/d2) log(cs/d2) = q log(2/9) + q/\n\u221a d\u2212 log 2\nq log(2/9)\n= 1\u2212 1 log(9/2) \u221a d + log 2 q log(9/2)\n= 1\u2212 o ( 1/ \u221a log n )\nWhere in the last step we have taken q = \u221a d and d = \u03c9(log n) as by the OVP conjecture. It is important to notice that we could have taken q much larger, and still satisfied lemma 2. However that wouldn\u2019t have improved the result, except by more quickly vanishing second order asymptitic terms. What we instead gain from having d2 = (9d) \u221a d is that, as one can verify going through the lemma, we show hardness for any join algorithm running in time d o( log d log log2 d ) n1+\u03b1\u2212 . That is, the hardness holds even for algorithms with a much higher dependency on the dimension than polynomial.\nSimilarly, we calculate for embedding 3:\nlog(s/d2)\nlog(cs/d2) =\nlog k k2d/k log k\u22121 k2d/k\n= 1\u2212 k log(1 + 1/(k \u2212 1)) d+ k log(1 + 1/(k \u2212 1))\n= 1\u2212 1/d+O(1/(kd)) = 1\u2212 o(1/ log n)\nWhere we have taken k = d and d = \u03c9(log n) as by the OVP conjecture.\nTaking k = d means that d2 is only 2d."}, {"heading": "3. LIMITATIONS OF LSH FOR IPS", "text": "We provide an upper bound on the gap between P1 and P2 for an (s, cs, P1, P2)-asymmetric LSH for signed/unsigned IPS. For the sake of simplicity we assume the data and query domains to be the d-dimensional balls of radius 1 and U \u2265 1, respectively. The bound holds for a fixed set of data vectors, so it applies also to data dependent LSH [9]. A consequence of our result is that there cannot exist an asymmetric LSH for any dimension d \u2265 1 when the set of query vectors is unbounded, getting a result similar to that of [39], which however requires even the data space to be unbounded and d \u2265 2.\nWe firsts show in Lemma 4 that the gap P1\u2212P2 can be expressed as a function of the length h of two sequences of query and data vectors with suitable collision properties. Then we provide the proof of the aforementioned Theorem 3, where we derive some of such sequences and then apply the lemma.\nLemma 4. Suppose that there exists a sequence of data vectors P = {p0, . . . , pn\u22121} and a sequence of query vectors Q = {q0, . . . , qn\u22121} such that qTi pj \u2265 s if j \u2265 i and qTi pj \u2264 cs otherwise (resp., |qTi pj | \u2265 s if j \u2265 i and |qTi pj | \u2264 cs otherwise) . Then any (s, cs, P1, P2)asymmetric LSH for signed IPS (resp., unsigned IPS) must satisfy P1 \u2212 P2 \u2264 1/(8 log n).\nProof. For the sake of simplicity we assume that n = 2` \u2212 1 for some ` \u2265 1; the assumption can be removed by introducing floor and ceiling operations in the proof. Let H denote an (s, cs, P1, P2)-asymmetric\nLSH family of hash functions, and let h be a function in H. The following argument works for signed and unsigned IPS.\nConsider the n \u00d7 n grid representing the collisions between Q\u00d7 P , that is, a node (i, j) denotes the querydata vectors qi and pj . We say that a node (i, j), with 0 \u2264 i, j < n, collides under h if vectors qi and pj collide under h. By definition of asymmetric LSH, all nodes with j \u2265 i must collide with probability at least P1, while the remaining nodes collide with probability at most P2. We use lower triangle to refer to the part of the grid with j \u2265 i and P1-nodes to refer to the nodes within it; we refer to the remaining nodes as P2-nodes.\nWe partition the lower triangle into squares of exponentially increasing side as shown in Figure 1. Specifically, we split the lower triangle into squares Gr,s for every r and s with 0 \u2264 r < log(n + 1) = ` and 0 \u2264 s < (n + 1)/2r+1 = 2`\u2212r\u22121, where Gr,s includes all nodes in the square of side 2r and top-left node ((2s+ 1)2r \u2212 1, (2s + 1)2r \u2212 1). For a given square Gr,s, we define the left squares (resp., top squares) to be the set of squares that are on the left (resp., top) of Gr,s. We note that the left squares (resp., top squares) contain 2r\u2212i\u22121 squares of side 2i for any 0 \u2264 i < r and all P1-nodes with s2\nr+1 \u2264 i, j < (2s + 1)2r \u2212 1 (resp., (2s+ 1)2r \u2212 1 < i, j \u2264 (s+ 1)2r+1 \u2212 2) .\nWe define the mass mi,j of a node (i, j) to be the collision probability, under H, of qi and pj . We split the mass of a P1-node into three contributions called shared mass, partially shared mass, and proper mass, all defined below. Consider each P1-node (i, j) and each function h \u2208 H where (i, j) collides. Let Gr,s be the square containing (i, j) and let Kh,i,j denote the set of P1-nodes (i\n\u2032, j\u2032) on the left side of the same row or on the top of the same column of (i, j) (i.e., i\u2032 = i and i \u2264 j\u2032 < j, or j\u2032 = j and i < i\u2032 \u2264 j) and with the same hash value of (i, j) under h (i.e., h(i) = h(j) = h(i\u2032) = h(j\u2032)). Clearly all nodes in Kh,i,j collide under h. For the given node (i, j), we classify h as follows (see Figure 1 for an example):\n\u2022 (i, j)-shared function. Kh,i,j contains at least a node (i, j\u2032) in a left square, and at least a node (i\u2032, j) in a top square.\n\u2022 (i, j)-partially shared function. Function h is not in case 1 and Kh,i,j contains at least a node node (i, j\u2032) with j\u2032 < j, and at least a node (i\u2032, j) with i\u2032 > i. That is, Kh,i,j contains only nodes in Gr,s and in the left blocks, or only nodes in Gr,s and in the top blocks.\n\u2022 (i, j)-proper function. Kh,i,j contains no points (i, j\u2032) for any i \u2264 j\u2032 < j or contains no points (i\u2032, j) for any i < i\u2032 \u2264 j. That is, Kh,i,j cannot contain at the same time a point in a left square and a point in a top square. Function h is said row (resp., column) proper if there are no nodes in the same row (resp., column). We break ties arbitrary but consistently if Kh,i,j is empty.\nThe shared mass msi,j is the sum of probabilities of all\n(i, j)-shared functions. The partially shared mass mpsi,j is the sum of probabilities of all (i, j)-partially shared functions. The proper mass mpi,j is the sum of probabilities of all (i, j)-proper functions (the row/column proper mass includes only row/column proper functions). We have mi,j = m p i,j + m ps i,j + m s i,j . The mass Mr,s of a square Gr,s is the sum of the masses of all its nodes, while the proper mass Mpr,s is the sum of proper masses of all its nodes. The sum of row proper masses of all nodes in a row is at most one since a function h is row proper for at most one node in a row. Similarly, the sum of column proper masses of all nodes in a column is at most one. Therefore, we have that \u2211 r,sM p r,s \u2264 2n.\nWe now show that \u2211\n(i,j)\u2208Gr,s m s i,j \u2264 22rP2 for every\nGr,s. Consider a node (i, j) in a given Gr,s. For each (i, j)-shared function h there is a P2-node colliding under h: indeed, Kh,i,j contains nodes (i, j\n\u2032) in the left blocks and (i\u2032, j) in the top blocks with h(i) = h(j) = h(i\u2032) = h(j\u2032) (i.e., s2r+1 \u2264 j\u2032 < (2s+1)2r\u22121 and (2s+1)2r\u22121 < i\u2032 \u2264 (s+1)2r+1\u22122); then node (i\u2032, j\u2032) is a P2-node since i\u2032 > j\u2032 and collides under h. By considering all nodes in Gr,s, we get that all the P2-nodes that collide in a shared function are in the square of side 2r\u22121 and bottom-right node in ((2s+1)2r, (2s+1)2r\u22122). Since these P2-nodes have total mass at most 22rP2, the claim follows.\nWe now prove that \u2211\n(i,j)\u2208Gr,s m ps i,j \u2264 2r+1Mpr,s. A\n(i, j)-partially shared function is (i\u2032, j) or (i, j\u2032)-proper for some i\u2032 < i and j\u2032 > j, otherwise there would be a node in left blocks and a node in top blocks that collide with (i, j) under h, implying that h cannot be partially shared. Since an (i, j)-proper function is partially shared for at most 2r+1 nodes in Gr,s, we get\u2211\n(i,j)\u2208Gr,s\nmpsi,j \u2264 2 r+1 \u2211 (i,j)\u2208Gr,s mpi,j = 2 r+1Mpr,s.\nBy the above two bounds, we get Mr,s \u2264 \u2211\n(i,j)\u2208Gr,s\nmpi,j+m ps i,j+m s i,j \u2264 (2r+1+1)Mpr,s+22rP2.\nSince Mr,s \u2265 22rP1 we get Mpr,s \u2265 (2r\u22121 \u2212 1)(P1 \u2212 P2).\nBy summing among all squares, we get\n2n \u2265 `\u22121\u2211 r=0 2`\u2212r\u22121\u22121\u2211 s=0 Mpr,s > (P1 \u2212 P2) n log n 4\nfrom which the claim follows.\nWe are now ready to prove Theorem 3.\nProof. (Theorem 3) The upper bounds to P1 \u2212 P2 in the different cases follow by applying Lemma 4 with different sequences of query and data vectors. We anticipate that in all three cases the gap P1\u2212P2 becomes 0 if the query ball is unbounded (i.e., U = +\u221e), and hence there cannot exist an asymmetric LSH with P1 > P2.\nFirst case. We now show that there exist data and query sequences of length n = \u0398 (md), with m = \u0398 ( log1/c(U/s) ) , for signed and unsigned IPS in d \u2265 1\ndimensions if s = O ( U/ \u221a d )\n. Note that m \u2265 1 since we assume s \u2264 cU . As a warm-up, we start with d = 1. Let Q = {qi,\u2200 0 \u2264 i < n} and P = {pj ,\u2200 0 \u2264 j < n} with\nqi = Uc i, pj = s/(Uc j). (1)\nLet pj \u2208 P and qi \u2208 Q. We get pTj qi = ci\u2212js: if j \u2265 i then pTj qi \u2265 s and pTj qi \u2264 cs otherwise. Data and query vectors are respectively contained in the unit ball and in\nthe ball of radius U since i, j < \u0398 ( log1/c(U/s) ) . Being\nthe sequences P and Q of length m, the claim follows. Let now d \u2265 2 and assume for the sake of simplicity d = 2d\u2032 (the general case just requires some more tedious computations). Consider the following sequences Qk = {qi,k,\u2200 0 \u2264 i < m} and Pk = {pj,k,\u2200 0 \u2264 j < m} for each 0 \u2264 k < d\u2032, where qi,k and pj,k are d-dimensional vectors defined as follows. Denote with qi,k[t] the t-th coordinate of qi,k, for 0 \u2264 t < d (similarly for pj,k). For vector qi,k we have: qi,k[2k] = Uc\ni; qi,k[2t + 1] = 2s for each k \u2264 t < d\u2032; remaining positions are set to 0. For vector pi,k we have: pi,k[2k] = s/(Uc\ni); pi,k[2k \u2212 1] = 1/2 (only when k > 0); remaining positions are set to 0. Intuitively, these data and query sequences follow by constructing the 1-dimensional sequences in\nEquation 1 on d\u2032 orthogonal dimensions and then by suitably translating each sequence. As an example, for d = 6 we get:\nqi,0 = (Uc i, 2s, 0, 2s, 0, 2s) pj,0 = (s/(Uc j), 0, 0, 0, 0, 0);\nqi,1 = (0, 0, Uc i, 2s, 0, 2s) pj,1 = (0, 1/2, s/(Uc j), 0, 0, 0);\nqi,2 = (0, 0, 0, 0, Uc i, 2s) pj,2 = (0, 0, 0, 1/2, s/(Uc j), 0).\nThe query and data sequences Q = {Q0, . . . Qd\u2032\u22121} and P = {P0, . . . Pd\u2032\u22121} satisfy the hypothesis of Lemma 4. Indeed, it can be verified that: pTj,`qi,` = sc i\u2212j and thus pTj,`qi,` \u2265 s if j \u2265 i and pTj,`qi,` \u2264 cs otherwise; pTj,`\u2032qi,` = 0 if `\n\u2032 < `; pTj,`\u2032qi,` \u2265 s if `\u2032 > `. Further, when s \u2264 U/(2 \u221a d) data and query vectors are contained in balls with radius 1 and U respectively, with the exception of vectors qi,k for 0 \u2264 i < 1/(2 log(1/c)) and 0 \u2264 k < d\u2032 which are contained in a ball of radius 2U . However, these query vectors and the respective data vectors can be removed from the above sequences without affecting the asymptotic length. We thus get two sequences of\nlength n = (m\u2212 1/(2 log(1/c)))d\u2032 = \u0398 ( d log1/c(U/s) ) ,\nthe claim follows. Since all inner products are non negative, the upper bound on P1 \u2212 P2 holds for signed and unsigned IPS.\nSecond case. Longer query and data sequences, with length n = \u0398 (md) for m = \u0398 (\u221a U/(s(1\u2212 c)) ) , can\nbe constructed for signed IPS when d \u2265 2. . We start considering the case d = 2. Let Q = {qi,\u2200 0 \u2264 i < m} and P = {pj ,\u2200 0 \u2264 j < m} with\nqi = (\u221a sU(1\u2212 (1\u2212 c)i), \u221a sU(1\u2212 c) ) ,\npj =\n(\u221a s\nU , j\n\u221a s(1\u2212 c)\nU\n) .\n(2)\nWe observe that these sequences are similar to the one used in [39]. We have pTj qi = s(1\u2212 c)(j \u2212 i) + s: then, pTj qi \u2265 s if j \u2265 i and pTj qi \u2264 cs otherwise. If s \u2264 U/2, data and query vectors are within balls of radius respectively 1 and U .\nLet now d \u2265 2 and assume for the sake of simplicity d = 2d\u2032 (the general case just requires some more tedious computations). Consider the following sequences Qk = {qi,k,\u2200 0 \u2264 i < m} and Pk = {pj,k,\u2200 0 \u2264 j < m} for each 0 \u2264 k < d\u2032, where qi,k and pj,k are d-dimensional vectors defined as follows. For vector qi,k we have: qi,k[2k] = \u221a sU(1\u2212 (1\u2212 c)i); qi,k[2k + 1] = \u221a sU(1\u2212 c); qi,k[2t] = \u221a Us for each k < t < d\u2032; remaining positions\nare set to 0. For vector pi,k we have: pi,k[2k] = \u221a s/U ;\npi,k[2k] = j \u221a s(1\u2212 c)/U ; remaining positions are set to 0. We observe that the two sequences follow by constructing the 2-dimensional sequences in Equation 2 on d\u2032 orthogonal planes and then suitably translate. Then, it follows that data and query sequences P = {P0, . . . Pd\u2032\u22121} and Q = {Q0, . . . Qd\u2032\u22121} satisfy the hypothesis of Lemma 4 and they are respectively contained in balls or radius one and U respectively if s \u2264 U/(2d).\nBeing m = nd\u2032 = O ( d \u221a U/(s(1\u2212 c)) ) and the claim\nfollows. We observe that the above sequences may generate large negative inner products and then they cannot be used for unsigned IPS.\nThird case. Finally, we provide an upper bound on P1 \u2212 P2 for signed and unsigned IPS that holds for d \u2265 \u0398 ( U5/(c2s5) ) by providing data and query sequences of length n = 2 \u221a U/(8s). Suppose there exists a family Z of 2n\u2212 1 vectors such that |zTi zj | \u2264 and (1\u2212 ) \u2264 zTi zi \u2264 (1 + ) for any zi 6= zj , for = c/(2 log2 n). It can be shown with the Johnson-Lindenstrauss lemma that such a family exists when d = \u2126 ( \u22122 log n ) (for an analysis see e.g. [42]). For notational convenience, we denote the vectors in Z as follows: zb0 , zb0,b1 , . . . , zb0,b1,...,blog n\u22121 for each possible value b0, . . . , blogn\u22121 \u2208 {0, 1}. Let bi,` denote the `-th bit of the binary representation of i and with b\u0304i,` its negation, where we assume ` = 0 to be the most significant bit. Let Q = {qi,\u2200 0 \u2264 i < n} and P = {pj ,\u2200 0 \u2264 j < n} with\nqi = \u221a 2sU logn\u22121\u2211 `=0 b\u0304i,`zbi,0,...bi,`\u22121,b\u0304i,`\npj = \u221a 2s/U logn\u22121\u2211 `=0 bj,`zbj,0,...bj,`\u22121,bj,`\nSince the inner product of two distinct vectors in Z is in the range [\u2212 , ], we have that pTj qi can be upper bounded as\npTj qi \u2264 2s(log 2 n\u2212 log n)+\n+ 2s logn\u22121\u2211 `=0 bj,`b\u0304i,`zbj,0,...bj,`\u22121,bj,`zbi,0,...bi,`\u22121,b\u0304i,`\nSuppose i > j. Then there exists a bit position `\u2032 such that bi,`\u2032 = 1, bj,`\u2032 = 0 and bi,` = bj,` for all ` < `\n\u2032. We get bj,`b\u0304i,` = 0 for all ` \u2264 `\u2032 and zbj,0,...bj,`\u22121,bj,` 6= zbi,0,...bi,`,b\u0304i,` for all ` > ` \u2032. It then follows that pTj qi <\n2s log2 n when i > j. On the other hand we get that pTj qi can be lower bounded as\npTj qi \u2265\u2212 2s(log 2 n\u2212 log n)+\n+ 2s logn\u22121\u2211 `=0 bj,`b\u0304i,`zbj,0,...bj,`\u22121,bj,`zbi,0,...bi,`\u22121,b\u0304i,`\nSuppose i \u2264 j. Then there exists an index `\u2032 such that bj,`\u2032 = 1, bi,`\u2032 = 0 and bj,` = bi,` for all ` < `\n\u2032. We get bj,`\u2032 b\u0304i,`\u2032 = 1 and zbj,0,...bj,`\u2032\u22121,bj,`\u2032 = zbi,0,...bi,`\u2032\u22121,b\u0304i,`\u2032 . It then follows that pTj qi \u2265 \u2212 2s log 2 n + 2s. When\nd = \u2126 ( (log5 n)/c2 ) , we can set = c/(2 log2 n). From the above lower and upper bounds, it then follows that pTj qi \u2264 cs if j < i and pTj qi \u2265 s if j \u2265 i and hence the sequences Q and P satisfy the hypothesis of Lemmma 4. Finally, we observe that the data and query vectors in P and Q are respectively contained in balls of radius 1 and U : each qi (resp., pj) is given by the sum of at most\nlog n vectors whose norm is not larger than \u221a 2sU(1 + )\n(resp., \u221a 2s/U(1 + )); being n = 2 \u221a U/(8s) the claim follows."}, {"heading": "4. UPPER BOUNDS", "text": "This section contains three observations with implications for IPS join and its indexing version. We first notice in Section 4.1 that by plugging the best known LSH for `2 distance on a sphere [9] into a reduction presented in [12, 39], we get a data structure based on LSH for signed MIPS with search time exponent \u03c1 = (1\u2212 s)/(1 + (1\u2212 2c)s).\nThen, in Section 4.2, we show how to circumvent the results in [39, 45] showing that symmetric LSH is not possible when the data and query domains coincide (while an asymmetric LSH does exist). We use an slightly modified definition of LSH that disregards the collision probability of 1 for pairs of identical vectors, and assume that vectors are represented with finite precision. The LSH construction uses explicit incoherent matrices built using Reed-Solomon codes [38] to implement a symmetric version of the reduction in [12, 39].\nFinally, in Section 4.3 we solve unsigned (cs, s) join using linear sketches for `p norms from [5]. Given \u03ba \u2265 2 we obtain a approximation factor c \u2265 1/n1/\u03ba using O\u0303 ( dn2\u22122/\u03ba ) time. Although this trade-off is not that strong, it is not far from the conditional lower bound in Theorem 1."}, {"heading": "4.1 Asymmetric LSH for signed IPS", "text": "We assume the data and query domains to be ddimensional balls with respective radius 1 and U . Vectors are embedded into a (d+2)-dimensional unit sphere using the asymmetric map as in [39]: a data vector p is\nmapped to (p, \u221a\n1\u2212 ||p||2, 0), while a query q is mapped to (q/U, 0, \u221a 1\u2212 ||q||2/U2). This transformation just scales the inner product by a factor U , and hence signed inner product search can be seen as an instance of ANN in `2 with distance threshold r = \u221a 2(1\u2212 s/U) and ap-\nproximation c\u2032 = \u221a\n(1\u2212 cs/U)/(1\u2212 s/U). The latter can be solved in space O(n1+\u03c1 + dn) and query time O(n\u03c1) using the LSH construction of [9]. We get the following \u03c1 value (for the LSH gap as well as for the exponent of the running time):\n\u03c1 = 1\n2c\u20322 \u2212 1 = 1\u2212 s/U 1 + (1\u2212 2c)s/U . (3)\nIn Figure 2, we plot the \u03c1 values of three LSH constructions: the one proposed here (with U = 1), the one from [39], and the one from [46]. The latter works only for binary vectors. We point out that our bound is always stronger than the one from [39] and sometimes stronger than the one from [46], despite that the latter is tailored for binary vectors. The latter conclusion is somewhat surprising, since the data structure we obtain works for non-binary vectors as well.\nWe point out that in practice one may want to use a recent LSH family from [7] that\u2014both in theory and in\npractice\u2014is superior to the hyperplane LSH from [16] used in [39]."}, {"heading": "4.2 Symmetric LSH for almost all vectors", "text": "Neyshabur and Srebro [39] show that an asymmetric view on LSH for signed IPS is required. Indeed they show that a symmetric LSH for signed IPS does not exist when data and query domains are balls of the same radius, while an asymmetric LSH does exist. (On the other hand, when the data domain is a ball of given radius U and the query domain is a sphere of same radius, a symmetric LSH does exist.) In this section we show that even when data and query spaces coincide a nontrivial symmetric LSH does exist if we disregard the trivial collision probability of 1 when data and query vectors are identical.\nWe first show how to reduce signed IPS to the case where data and query vectors lie on a unit sphere. The reduction is deterministic and maintains inner products up to an additive error \u03b5 for all vectors x, y with x 6= y. We then plug in any Euclidean LSH for ANN on the sphere, for example the one from [9]. This reduction treats data and query vectors identically, unlike the one from [39], and thus we are able to obtain a symmetric LSH.\nAssume that all the coordinates of all the data and queries are encoded as k-bit numbers and that the data and query vectors are in the unit ball. The idea is the following. There are at most N = 2O(dk) possible data vectors and queries. Imagine a collection of N unit vectors v1, . . . , vN such that for every i 6= j one has |vTi vj | \u2264 \u03b5. Then, it is easy to check that a map of a vector p to f(p) = (p, \u221a 1\u2212 \u2016p\u20162 \u00b7vp) maps a vector from a unit ball to a unit sphere and, moreover, for p 6= q one has |f(p)T f(q)\u2212 pT q| \u2264 \u03b5.\nWhat remains is to construct such a collection of vectors vi. Moreover, our collection of vectors must be explicit in a strong sense: we should be able to compute vu given a vector u (after interpreting it as an dk-bit string). Such a constructions are well known, e.g., in [38] it is shown how to build such vectors using Reed-Solomon codes. The resulting dimension is O ( \u03b5\u22122 logN ) = O ( kd/\u03b52 ) [32, 38].\nAfter performing such a reduction we can apply any state-of-the-art LSH (or data structure for ANN) for `2 norm on a sphere, e.g. from [9, 7], with distance threshold r2 = 2(1\u2212s+ ), approximation factor c\u20322 = (1\u2212cs\u2212 )/r2. If \u03b5 is sufficiently small we get a \u03c1 value close to the one in (3). The final result is therefore a symmetric LSH for symmetric domains that does not provide any collision bound for all pairs (q, p) with q = p since the guarantees on the inner product fail for these pairs. This LSH can used for solving signed (cs, s) IPS as a traditional LSH [6], although it is required an initial step that verifies whether a query vector is in the input set and, if this is the case, returns the vector q itself if qT q \u2265 s."}, {"heading": "4.3 Unsigned IPS via linear sketches", "text": "In this section we propose a linear sketch for unsigned\nc-MIPS, that can be used for solving unsigned (cs, s) join. The unsigned c-MIPS is defined as follows: given a set P \u2282 Rd of n vectors, construct a data structure that efficiently returns, for a given query vector q, a vector p \u2208 P where |pT q| \u2265 c(p\u2032T q), where p\u2032 is the vector in P with maximum absolute inner product with q. The unsigned (cs, s) join between sets P and Q can be computed by constructing a data structure for unsigned c-MIPS for vectors in P and then performs a query for each vector in Q.\nOf independent interest, we notice that unsigned cMIPS can be solved by a data structure for unsigned (cs, s) search. Let D be a data structure for unsigned (cs, s) search on the data set P , and suppose we are given a query q and the promise that there exists p\u2032 \u2208 P such that p\u2032T q > \u03b3. Then, unsigned c-MIPS can be solved by performing on D the queries q/ci for any 0 \u2264 i \u2264 dlog1/c(s/\u03b3)e. Intuitively, we are scaling up the query q until the largest inner product becomes larger than the threshold s. We notice that \u03b3 can be also considered as the smallest inner product that can be stored according to the numerical precision of the machine.\nOur data structure requires O\u0303 ( dn2\u22122/\u03ba ) construction\ntime and O\u0303 ( dn1\u22122/\u03ba ) query time and provide a c \u2265 1/n1/\u03ba approximation with high probability, for any \u03ba \u2265 2. This gives an algorithm for unsigned (cs, s) join on two sets of size n requiring time O\u0303 ( dn2\u22122/\u03ba ) . As shown in Theorem 1, we are unlikely to significant improve further the approximation factor if the OVP conjecture is true.\nFirst, suppose we are only interested in approximating the value of maxp |qtp| and not to find the corresponding vector. Then, the problem is equivalent to estimating \u2016Aq\u2016\u221e, where A is an n \u00d7 d matrix, whose rows are data vectors. This problem can be tackled using linear sketches (for an overview see [57, 8]). More specifically, we use the following result from [5]: for every 2 \u2264 \u03ba \u2264 \u221e there exists a distribution over O\u0303(n1\u22122/\u03ba)\u00d7 n matrices \u03a0 such that for every x \u2208 Rn one has:\nPr \u03a0\n[(1\u2212 c)\u2016x\u2016\u03ba \u2264 \u2016\u03a0x\u2016\u221e \u2264 (1 + c)\u2016x\u2016\u03ba] \u2265 0.99\nfor a suitable constant 0 < c < 1. Thus, to build a data structure for computing \u2016Aq\u2016\u221e, we sample a matrix \u03a0 according to the aforementioned result in [5] and com-\npute the O\u0303 ( n1\u22122/\u03ba ) \u00d7 d matrix As = \u03a0A. Then, for ev-\nery query q, we compute \u2016Asq\u2016\u221e in time O\u0303 ( d \u00b7 n1\u22122/\u03ba ) ,\nwhich is a O ( n1/\u03ba ) -approximation to \u2016Aq\u2016\u221e with probability at least 0.99. Note that we can reduce the probability of error from 0.01 to \u03b4 > 0 as usual, by building O(log(1/\u03b4)) independent copies of the above data structure and reporting the median estimate.\nWe now consider the recovery of the vector that almost maximizes |ptq|. We recover the index of the desired vector bit by bit. That is, for every bit index 0 \u2264 i < log n, we consider every binary sequence b of length i and build a data structure for the dataset containing only the vectors in P for which the binary representations of their indexes have prefix b. Although the number of data structures is n, the total required space is still O\u0303 ( dn1\u22122/\u03ba ) since each vector appears in only log n data structures. The claim stated at the beginning follows."}, {"heading": "5. CONCLUSION", "text": "This paper has investigated different aspects of the complexity of approximate similarity join with inner product. In particular, we have related the hardness of this problem to the OVP conjecture. Under some assumptions on c and s, the proposed conditional lower bounds rule out algorithms for signed/unsigned (cs, s) IPS join running in n2\u2212 time, for a constant > 0, unless the OVP conjecture is false. Nevertheless, the data structures in section 4 show that it still possible to reach weak subquadratic time, and even truly subquadratic time for small values of the approximation factor.\nThe hardness of signed/unsigned IPS holds even for weak approximation factors when the vector domain is {\u22121, 1}d. Indeed, the result holds if c \u2265 0 for signed join, and if c \u2265 e\u2212o( \u221a logn/ log logn) for unsigned join. When c < 1/n\u2126(1), the data structure for unsigned IPS in section 4.3 reaches strongly subquadratic time and this gives evidence that the constraint on c of the hardness result cannot be significantly relaxed. On the other hand, when vectors are in the {0, 1}d domain, a\nstronger assumption is required on the approximation factor. In this case, the conditional lower bound holds for c = 1\u2212 o(1) and hence it does not rule out an algorithm running in n2\u2212 time for a constant approximation factor. We believe that a different approach is required to show the hardness of IPS for constant approximation: indeed, the proposed reduction from OVP to IPS in the {0, 1}d domain strongly relies on the ability to distinguish inner products smaller than k \u2212 1 and larger than k for some k = \u03c9(1), implying c \u2265 1 \u2212 o(1). From an upper bound point of view, the LSH proposed in section 4.1 improves upon the state of the art [46] based on minwise hashing for different values (e.g., when s \u2265 d/3 and c \u2265 0.83); however, it still gives weak subquadratic algorithm for signed/unsigned IPS with constant c. An interesting open question is therefore to assess if strongly subquadratic time is possible when the approximation is constant (or smaller) in the {0, 1}d domain."}, {"heading": "6. REFERENCES", "text": "[1] A. Abboud, R. Williams, and H. Yu. More\napplications of the polynomial method to algorithm design. In Proc. 26th ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 218\u2013230, 2015.\n[2] M. Abramowitz and I. A. Stegun. Handbook of mathematical functions: with formulas, graphs, and mathematical tables. Courier Corporation, 1964.\n[3] P. Achlioptas, B. Scho\u0308lkopf, and K. Borgwardt. Two-locus association mapping in subquadratic time. In Proc. 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 726\u2013734. ACM, 2011.\n[4] J. Alman and R. Williams. Probabilistic polynomials and hamming nearest neighbors. In Proc. 56th IEEE Symposium on Foundations of Computer Science (FOCS), 2015.\n[5] A. Andoni. High frequency moments via max-stability. Unpublished manuscript, 2012.\n[6] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Commun. ACM, 51(1):117\u2013122, 2008.\n[7] A. Andoni, P. Indyk, M. Kapralov, T. Laarhoven, I. Razenshteyn, and L. Schmidt. Practical and optimal LSH for angular distance. In Proc. 28th Conference on Neural Information Processing Systems (NIPS), 2015.\n[8] A. Andoni, R. Krauthgamer, and I. P. Razenshteyn. Sketching and embedding are equivalent for norms. In Proc. 47th ACM on Symposium on Theory of Computing, (STOC), pages 479\u2013488, 2015.\n[9] A. Andoni and I. Razenshteyn. Optimal data-dependent hashing for approximate near neighbors. In Proc. 47th Symposium on Theory of Computing (STOC), pages 793\u2013801, 2015.\n[10] A. Arasu, V. Ganti, and R. Kaushik. Efficient exact set-similarity joins. In Proc. International\nConference on Very Large Data Bases (VLDB), pages 918\u2013929, 2006.\n[11] N. Augsten and M. H. Bo\u0308hlen. Similarity joins in relational database systems. Synthesis Lectures on Data Management, 5(5):1\u2013124, 2013.\n[12] Y. Bachrach, Y. Finkelstein, R. Gilad-Bachrach, L. Katzir, N. Koenigstein, N. Nice, and U. Paquet. Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces. In Proc. 8th ACM Conference on Recommender Systems, pages 257\u2013264, 2014.\n[13] B. Bahmani, A. Goel, and R. Shinde. Efficient distributed locality sensitive hashing. In Proc. ACM International Conference on Information and Knowledge Management (CIKM), pages 2174\u20132178, 2012.\n[14] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all pairs similarity search. In Proc. International Conference on World Wide Web (WWW), pages 131\u2013140, 2007.\n[15] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proc. 34 ACM Symposium on Theory of computing (STOC), pages 380\u2013388. ACM, 2002.\n[16] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proc. 34th ACM Symposium on Theory of Computing (STOC), pages 380\u2013388, 2002.\n[17] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive operator for similarity joins in data cleaning. In Proc.22nd International Conference on Data Engineering (ICDE), page 5, 2006.\n[18] Y. Chen and J. M. Patel. Efficient evaluation of all-nearest-neighbor queries. In Proc. International Conference on Data Engineering (ICDE), pages 1056\u20131065, 2007.\n[19] E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk, R. Motwani, J. D. Ullman, and C. Yang. Finding interesting associations without support pruning. IEEE Trans. Knowl. Data Eng., 13(1):64\u201378, 2001.\n[20] R. R. Curtin, A. G. Gray, and P. Ram. Fast exact max-kernel search. In Proc. 13th SIAM International Conference on Data Mining (SDM), pages 1\u20139, 2013.\n[21] A. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: scalable online collaborative filtering. In Proc. International Conference on World Wide Web (WWW), pages 271\u2013280, 2007.\n[22] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijayanarasimhan, and J. Yagnik. Fast, accurate detection of 100,000 object classes on a single machine. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, Washington, DC, USA, 2013.\n[23] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,, 32(9):1627\u20131645, 2010.\n[24] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. In Proc. 25th International Conference on Very Large Data Bases (VLDB), pages 518\u2013529, 1999.\n[25] S. Har-Peled, P. Indyk, and R. Motwani. Approximate nearest neighbor: Towards removing the curse of dimensionality. Theory of computing, 8(1):321\u2013350, 2012.\n[26] E. H. Jacox and H. Samet. Metric space similarity joins. ACM Transactions on Database Systems (TODS), 33(2):7, 2008.\n[27] Y. Jiang, D. Deng, J. Wang, G. Li, and J. Feng. Efficient parallel partition-based algorithms for similarity search and join with edit distance constraints. In Proc. Joint EDBT/ICDT Workshops, pages 341\u2013348. ACM, 2013.\n[28] T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane training of structural svms. Mach. Learn., 77(1):27\u201359, 2009.\n[29] M. Karppa, P. Kaski, and J. Kohonen. A faster subquadratic algorithm for finding outlier correlations. In Proc. 27th ACM-SIAM Symposium on Discrete Algorithms (SODA16), 2016.\n[30] N. Koenigstein, P. Ram, and Y. Shavitt. Efficient retrieval of recommendations in a matrix factorization framework. In Proc. 21st ACM International Conference on Information and Knowledge Management (CIKM), pages 535\u2013544, 2012.\n[31] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337, Aug. 2009.\n[32] K. G. Larsen and J. Nelson. The johnson-lindenstrauss lemma is optimal for linear dimensionality reduction. CoRR, abs/1411.2404, 2014.\n[33] D. Lee, J. Park, J. Shim, and S.-g. Lee. An efficient similarity join algorithm with cosine similarity predicate. In Database and Expert Systems Applications, pages 422\u2013436. Springer, 2010.\n[34] G. Li, D. Deng, J. Wang, and J. Feng. Pass-join: A partition-based method for similarity joins. Proc. VLDB Endowment, 5(3):253\u2013264, 2011.\n[35] Y. Low and A. X. Zheng. Fast top-k similarity queries via matrix compression. In Proc. ACM International Conference on Information and Knowledge Management (CIKM)KM, pages 2070\u20132074. ACM, 2012.\n[36] J. Lu, C. Lin, W. Wang, C. Li, and H. Wang. String similarity measures and joins with synonyms. In Proc. 2013 ACM SIGMOD International Conference on Management of Data, pages 373\u2013384, 2013.\n[37] R. Motwani, A. Naor, and R. Panigrahi. Lower bounds on locality sensitive hashing. In Proc. 22nd Symposium on Computational Geometry (SoCS), pages 154\u2013157, 2006.\n[38] J. Nelson, H. L. Nguyen, and D. P. Woodruff. On deterministic sketching and streaming for sparse recovery and norm estimation. Linear Algebra and its Applications, 441(0):152 \u2013 167, 2014.\n[39] B. Neyshabur and N. Srebro. On symmetric and asymmetric lshs for inner product search. In Proc. 32nd International Conference on Machine Learning (ICML), 2015.\n[40] R. O\u2019Donnell, Y. Wu, and Y. Zhou. Optimal lower bounds for locality-sensitive hashing (except when q is tiny). ACM Trans. Comput. Theory, 6(1):5:1\u20135:13, 2014.\n[41] R. Pagh, N. Pham, F. Silvestri, and M. Sto\u0308ckel. I/O-efficient similarity join. In Proc. 23rd European Symposium on Algorithms (ESA), pages 941\u2013952, 2015.\n[42] R. Pagh, F. Silvestri, J. Sivertsen, and M. Skala. Approximate furthest neighbor in high dimensions. In Proc. 8th International Conference on Similarity Search and Applications (SISAP), volume 9371 of LNCS, pages 3\u201314, 2015.\n[43] P. Ram and A. G. Gray. Maximum inner-product search using cone trees. In Proc. 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 931\u2013939, 2012.\n[44] V. Satuluri and S. Parthasarathy. Bayesian locality sensitive hashing for fast similarity search. Proc. VLDB Endowment, 5(5):430\u2013441, 2012.\n[45] A. Shrivastava and P. Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Proc. 27th Conference on Neural Information Processing Systems (NIPS), pages 2321\u20132329, 2014.\n[46] A. Shrivastava and P. Li. Asymmetric minwise hashing for indexing binary inner products and set containment. In Proc. 24th International Conference on World Wide Web (WWW), pages 981\u2013991, 2015.\n[47] Y. N. Silva, W. G. Aref, and M. H. Ali. The similarity join database operator. In Proc. International Conference on Data Engineering (ICDE), pages 892\u2013903. IEEE, 2010.\n[48] N. Srebro, J. D. M. Rennie, and T. S. Jaakola. Maximum-margin matrix factorization. In Advances in Neural Information Processing Systems 17, pages 1329\u20131336. MIT Press, 2005.\n[49] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proc. 18th Conference on Learning Theory COLT, volume 3559 of LNCS, pages 545\u2013560, 2005.\n[50] C. Teflioudi, R. Gemulla, and O. Mykytiuk. Lemp: Fast retrieval of large entries in a matrix product. In Proc. ACM SIGMOD International Conference on Management of Data, pages 107\u2013122. ACM, 2015.\n[51] G. Valiant. Finding correlations in subquadratic time, with applications to learning parities and the closest pair problem. J. ACM, 62(2):13:1\u201313:45,\n2015. [52] J. Wang, G. Li, and J. Fe. Fast-join: An efficient\nmethod for fuzzy token matching based string similarity join. In Proc. International Conference on Data Engineering (ICDE), pages 458\u2013469. IEEE, 2011.\n[53] J. Wang, G. Li, and J. Feng. Can we beat the prefix filtering?: an adaptive framework for similarity join and search. In Proc. ACM SIGMOD International Conference on Management of Data, pages 85\u201396. ACM, 2012.\n[54] Y. Wang, A. Metwally, and S. Parthasarathy. Scalable all-pairs similarity search in metric spaces. In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 829\u2013837, 2013.\n[55] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces. In Proc. 24rd International Conference on Very Large Data Bases (VLDB), pages 194\u2013205, 1998.\n[56] R. Williams. A new algorithm for optimal 2-constraint satisfaction and its implications. Theoretical Computer Science, 348(2):357\u2013365, 2005.\n[57] D. P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in Theoretical Computer Science, 10:1\u2013157, 2014.\n[58] C. Xia, H. Lu, B. C. Ooi, and J. Hu. Gorder: an efficient method for knn join processing. In Proc. VLDB, pages 756\u2013767, 2004.\n[59] C. Xiao, W. Wang, X. Lin, and J. X. Yu. Efficient similarity joins for near duplicate detection. In Proc. International Conference on World Wide Web (WWW), pages 131\u2013140, 2008.\n[60] R. B. Zadeh and A. Goel. Dimension independent similarity computation. The Journal of Machine Learning Research, 14(1):1605\u20131626, 2013.\n[61] P. Zezula, G. Amato, V. Dohnal, and M. Batko. Similarity search: the metric space approach, volume 32. Springer Science & Business Media, 2006.\n[62] X. Zhang, F. Zou, and W. Wang. Fastanova: an efficient algorithm for genome-wide association study. In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 821\u2013829. ACM, 2008."}], "references": [{"title": "More applications of the polynomial method to algorithm design", "author": ["A. Abboud", "R. Williams", "H. Yu"], "venue": "In Proc. 26th ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Handbook of mathematical functions: with formulas, graphs, and mathematical tables", "author": ["M. Abramowitz", "I.A. Stegun"], "venue": "Courier Corporation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1964}, {"title": "Two-locus association mapping in subquadratic time", "author": ["P. Achlioptas", "B. Sch\u00f6lkopf", "K. Borgwardt"], "venue": "In Proc. 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Probabilistic polynomials and hamming nearest neighbors", "author": ["J. Alman", "R. Williams"], "venue": "In Proc. 56th IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "High frequency moments via max-stability", "author": ["A. Andoni"], "venue": "Unpublished manuscript,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Practical and optimal LSH for angular distance", "author": ["A. Andoni", "P. Indyk", "M. Kapralov", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"], "venue": "In Proc. 28th Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Sketching and embedding are equivalent for norms", "author": ["A. Andoni", "R. Krauthgamer", "I.P. Razenshteyn"], "venue": "In Proc. 47th ACM on Symposium on Theory of Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Optimal data-dependent hashing for approximate near neighbors", "author": ["A. Andoni", "I. Razenshteyn"], "venue": "In Proc. 47th Symposium on Theory of Computing (STOC),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Efficient exact set-similarity joins", "author": ["A. Arasu", "V. Ganti", "R. Kaushik"], "venue": "In Proc. International  Conference on Very Large Data Bases (VLDB),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Similarity joins in relational database systems", "author": ["N. Augsten", "M.H. B\u00f6hlen"], "venue": "Synthesis Lectures on Data Management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces", "author": ["Y. Bachrach", "Y. Finkelstein", "R. Gilad-Bachrach", "L. Katzir", "N. Koenigstein", "N. Nice", "U. Paquet"], "venue": "In Proc. 8th ACM Conference on Recommender Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Efficient distributed locality sensitive hashing", "author": ["B. Bahmani", "A. Goel", "R. Shinde"], "venue": "In Proc. ACM International Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Scaling up all pairs similarity search", "author": ["R.J. Bayardo", "Y. Ma", "R. Srikant"], "venue": "In Proc. International Conference on World Wide Web (WWW),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "In Proc. 34 ACM Symposium on Theory of computing (STOC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "In Proc. 34th ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "A primitive operator for similarity joins in data cleaning", "author": ["S. Chaudhuri", "V. Ganti", "R. Kaushik"], "venue": "In Proc.22nd International Conference on Data Engineering (ICDE),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Efficient evaluation of all-nearest-neighbor queries", "author": ["Y. Chen", "J.M. Patel"], "venue": "In Proc. International Conference on Data Engineering (ICDE),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Finding interesting associations without support pruning", "author": ["E. Cohen", "M. Datar", "S. Fujiwara", "A. Gionis", "P. Indyk", "R. Motwani", "J.D. Ullman", "C. Yang"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Fast exact max-kernel search", "author": ["R.R. Curtin", "A.G. Gray", "P. Ram"], "venue": "In Proc. 13th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "In Proc. International Conference on World Wide Web (WWW),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["T. Dean", "M. Ruzon", "M. Segal", "J. Shlens", "S. Vijayanarasimhan", "J. Yagnik"], "venue": "In Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE  Transactions on Pattern Analysis and Machine Intelligence,,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proc. 25th International Conference on Very Large Data Bases (VLDB),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Approximate nearest neighbor: Towards removing the curse of dimensionality", "author": ["S. Har-Peled", "P. Indyk", "R. Motwani"], "venue": "Theory of computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Metric space similarity joins", "author": ["E.H. Jacox", "H. Samet"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Efficient parallel partition-based algorithms for similarity search and join with edit distance constraints", "author": ["Y. Jiang", "D. Deng", "J. Wang", "G. Li", "J. Feng"], "venue": "In Proc. Joint EDBT/ICDT Workshops,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Mach. Learn.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "A faster subquadratic algorithm for finding outlier correlations", "author": ["M. Karppa", "P. Kaski", "J. Kohonen"], "venue": "In Proc. 27th ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Efficient retrieval of recommendations in a matrix factorization framework", "author": ["N. Koenigstein", "P. Ram", "Y. Shavitt"], "venue": "In Proc. 21st ACM International Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "The johnson-lindenstrauss lemma is optimal for linear dimensionality reduction", "author": ["K.G. Larsen", "J. Nelson"], "venue": "CoRR, abs/1411.2404,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "An efficient similarity join algorithm with cosine similarity predicate", "author": ["D. Lee", "J. Park", "J. Shim", "S.-g. Lee"], "venue": "In Database and Expert Systems Applications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Pass-join: A partition-based method for similarity joins", "author": ["G. Li", "D. Deng", "J. Wang", "J. Feng"], "venue": "Proc. VLDB Endowment,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Fast top-k similarity queries via matrix compression", "author": ["Y. Low", "A.X. Zheng"], "venue": "In Proc. ACM International Conference on Information and Knowledge Management (CIKM)KM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "String similarity measures and joins with synonyms", "author": ["J. Lu", "C. Lin", "W. Wang", "C. Li", "H. Wang"], "venue": "In Proc. 2013 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Lower bounds on locality sensitive hashing", "author": ["R. Motwani", "A. Naor", "R. Panigrahi"], "venue": "In Proc. 22nd Symposium on Computational Geometry (SoCS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "On deterministic sketching and streaming for sparse recovery and norm estimation", "author": ["J. Nelson", "H.L. Nguyen", "D.P. Woodruff"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "On symmetric and asymmetric lshs for inner product search", "author": ["B. Neyshabur", "N. Srebro"], "venue": "In Proc. 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Optimal lower bounds for locality-sensitive hashing (except when q is tiny)", "author": ["R. O\u2019Donnell", "Y. Wu", "Y. Zhou"], "venue": "ACM Trans. Comput. Theory,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "I/O-efficient similarity join", "author": ["R. Pagh", "N. Pham", "F. Silvestri", "M. St\u00f6ckel"], "venue": "In Proc. 23rd European Symposium on Algorithms (ESA),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Approximate furthest neighbor in high dimensions", "author": ["R. Pagh", "F. Silvestri", "J. Sivertsen", "M. Skala"], "venue": "In Proc. 8th International Conference on Similarity Search and Applications (SISAP),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Maximum inner-product search using cone trees", "author": ["P. Ram", "A.G. Gray"], "venue": "In Proc. 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Bayesian locality sensitive hashing for fast similarity search", "author": ["V. Satuluri", "S. Parthasarathy"], "venue": "Proc. VLDB Endowment,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)", "author": ["A. Shrivastava", "P. Li"], "venue": "In Proc. 27th Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Asymmetric minwise hashing for indexing binary inner products and set containment", "author": ["A. Shrivastava", "P. Li"], "venue": "In Proc. 24th International Conference on World Wide Web (WWW),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "The similarity join database operator", "author": ["Y.N. Silva", "W.G. Aref", "M.H. Ali"], "venue": "In Proc. International Conference on Data Engineering (ICDE),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2005}, {"title": "Rank, trace-norm and max-norm", "author": ["N. Srebro", "A. Shraibman"], "venue": "In Proc. 18th Conference on Learning Theory COLT, volume 3559 of LNCS,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2005}, {"title": "Lemp: Fast retrieval of large entries in a matrix product", "author": ["C. Teflioudi", "R. Gemulla", "O. Mykytiuk"], "venue": "In Proc. ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Finding correlations in subquadratic time, with applications to learning parities and the closest pair problem", "author": ["G. Valiant"], "venue": "J. ACM,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Fast-join: An efficient method for fuzzy token matching based string similarity join", "author": ["J. Wang", "G. Li", "J. Fe"], "venue": "In Proc. International Conference on Data Engineering (ICDE),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2011}, {"title": "Can we beat the prefix filtering?: an adaptive framework for similarity join and search", "author": ["J. Wang", "G. Li", "J. Feng"], "venue": "In Proc. ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2012}, {"title": "Scalable all-pairs similarity search in metric spaces", "author": ["Y. Wang", "A. Metwally", "S. Parthasarathy"], "venue": "In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces", "author": ["R. Weber", "H.-J. Schek", "S. Blott"], "venue": "In Proc. 24rd International Conference on Very Large Data Bases (VLDB),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1998}, {"title": "A new algorithm for optimal 2-constraint satisfaction and its implications", "author": ["R. Williams"], "venue": "Theoretical Computer Science,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2005}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["D.P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Gorder: an efficient method for knn join processing", "author": ["C. Xia", "H. Lu", "B.C. Ooi", "J. Hu"], "venue": "In Proc. VLDB,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2004}, {"title": "Efficient similarity joins for near duplicate detection", "author": ["C. Xiao", "W. Wang", "X. Lin", "J.X. Yu"], "venue": "In Proc. International Conference on World Wide Web (WWW),", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2008}, {"title": "Dimension independent similarity computation", "author": ["R.B. Zadeh", "A. Goel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2013}, {"title": "Similarity search: the metric space approach, volume 32", "author": ["P. Zezula", "G. Amato", "V. Dohnal", "M. Batko"], "venue": "Springer Science & Business Media,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2006}, {"title": "Fastanova: an efficient algorithm for genome-wide association study", "author": ["X. Zhang", "F. Zou", "W. Wang"], "venue": "In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2008}], "referenceMentions": [{"referenceID": 42, "context": "the vector p \u2208 P that maximizes the inner product, a search problem known in literature as maximum inner product search (MIPS) [43, 45].", "startOffset": 127, "endOffset": 135}, {"referenceID": 44, "context": "the vector p \u2208 P that maximizes the inner product, a search problem known in literature as maximum inner product search (MIPS) [43, 45].", "startOffset": 127, "endOffset": 135}, {"referenceID": 5, "context": "[6, 61]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 60, "context": "[6, 61]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "The most prominent technique used to achieve provably subquadratic running time is locality-sensitive hashing (LSH) [25, 24].", "startOffset": 116, "endOffset": 124}, {"referenceID": 23, "context": "The most prominent technique used to achieve provably subquadratic running time is locality-sensitive hashing (LSH) [25, 24].", "startOffset": 116, "endOffset": 124}, {"referenceID": 16, "context": "In the database community the similarity join problem was originally motivated by applications in data cleaning [17, 10].", "startOffset": 112, "endOffset": 120}, {"referenceID": 9, "context": "In the database community the similarity join problem was originally motivated by applications in data cleaning [17, 10].", "startOffset": 112, "endOffset": 120}, {"referenceID": 43, "context": "[44] for references and further examples).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We refer to the recent book by Augsten and B\u00f6hlen [11] for more background on similarity join algorithms in database systems.", "startOffset": 50, "endOffset": 54}, {"referenceID": 30, "context": "Inner product is an important measure of similarity between real vectors, particularly in information retrieval and machine learning contexts [31, 48], but not captured by techniques for metric similarity joins such as [26, 61].", "startOffset": 142, "endOffset": 150}, {"referenceID": 47, "context": "Inner product is an important measure of similarity between real vectors, particularly in information retrieval and machine learning contexts [31, 48], but not captured by techniques for metric similarity joins such as [26, 61].", "startOffset": 142, "endOffset": 150}, {"referenceID": 25, "context": "Inner product is an important measure of similarity between real vectors, particularly in information retrieval and machine learning contexts [31, 48], but not captured by techniques for metric similarity joins such as [26, 61].", "startOffset": 219, "endOffset": 227}, {"referenceID": 60, "context": "Inner product is an important measure of similarity between real vectors, particularly in information retrieval and machine learning contexts [31, 48], but not captured by techniques for metric similarity joins such as [26, 61].", "startOffset": 219, "endOffset": 227}, {"referenceID": 49, "context": "[50] studied the IPS join problem motivated by applications in recommender systems based on latent-factor models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Other examples of applications for IPS join are object detection [23] and multi-class prediction [22, 28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "Other examples of applications for IPS join are object detection [23] and multi-class prediction [22, 28].", "startOffset": 97, "endOffset": 105}, {"referenceID": 27, "context": "Other examples of applications for IPS join are object detection [23] and multi-class prediction [22, 28].", "startOffset": 97, "endOffset": 105}, {"referenceID": 19, "context": "IPS join also captures the so-called maximum kernel search, a general machine learning approach with applications such as image matching and finding similar protein/DNA sequences [20].", "startOffset": 179, "endOffset": 183}, {"referenceID": 55, "context": "to believe that inner product similarity may be inherently more difficult than other kinds of similarity search: Williams [56, 4] has shown that a truly subquadratic exact algorithm for IPS join would contradict the Strong Exponential Time Hypothesis, an important conjecture in computational complexity.", "startOffset": 122, "endOffset": 129}, {"referenceID": 3, "context": "to believe that inner product similarity may be inherently more difficult than other kinds of similarity search: Williams [56, 4] has shown that a truly subquadratic exact algorithm for IPS join would contradict the Strong Exponential Time Hypothesis, an important conjecture in computational complexity.", "startOffset": 122, "endOffset": 129}, {"referenceID": 50, "context": "On the upper bound side new reductions of (special cases of) approximate IPS join to fast matrix multiplication have appeared [51, 29], resulting in truly subquadratic algorithms even with approximation factors asymptotically close to 1.", "startOffset": 126, "endOffset": 134}, {"referenceID": 28, "context": "On the upper bound side new reductions of (special cases of) approximate IPS join to fast matrix multiplication have appeared [51, 29], resulting in truly subquadratic algorithms even with approximation factors asymptotically close to 1.", "startOffset": 126, "endOffset": 134}, {"referenceID": 44, "context": "The idea is to consider collisions between two different hash functions, using one hash function for query vectors and another hash function for data vectors [45, 46, 39].", "startOffset": 158, "endOffset": 170}, {"referenceID": 45, "context": "The idea is to consider collisions between two different hash functions, using one hash function for query vectors and another hash function for data vectors [45, 46, 39].", "startOffset": 158, "endOffset": 170}, {"referenceID": 38, "context": "The idea is to consider collisions between two different hash functions, using one hash function for query vectors and another hash function for data vectors [45, 46, 39].", "startOffset": 158, "endOffset": 170}, {"referenceID": 44, "context": "In this paper, we use the following definition of asymmetric LSH based on the definition in [45].", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "The \u03c1 value of an (asymmetric) LSH is defined as usual with \u03c1 = logP1/ logP2 [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 55, "context": "Indeed, such an algorithm would imply that the Strong Exponential Time Hypothesis (SETH) is true [56].", "startOffset": 97, "endOffset": 101}, {"referenceID": 28, "context": "[29], who get sub-quadratic running time for unsigned join of normalized vectors in {\u22121, 1}, when log(s/d)/ log(cs/d) is a constant smaller than 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "In fact Valiant [51] reduces the general case of P,Q \u2286 R to the case P,Q \u2286 {\u22121, 1} using the Charikar hyperplane LSH [15].", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "In fact Valiant [51] reduces the general case of P,Q \u2286 R to the case P,Q \u2286 {\u22121, 1} using the Charikar hyperplane LSH [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 50, "context": "For the {\u22121, 1} we use an enhanced, deterministic version of the \u201cChebyshev embedding\u201d [51], while for the interesting {0, 1} part, we initiate a study of embeddings for restricted alphabets.", "startOffset": 87, "endOffset": 91}, {"referenceID": 38, "context": "As a special case, we get the impossibility result in [39, 45], that there cannot exist an asymmetric LSH for unbounded query vectors.", "startOffset": 54, "endOffset": 62}, {"referenceID": 44, "context": "As a special case, we get the impossibility result in [39, 45], that there cannot exist an asymmetric LSH for unbounded query vectors.", "startOffset": 54, "endOffset": 62}, {"referenceID": 28, "context": "Unsigned (cs, s) over {\u22121, 1} c \u2265 e \u221a log n log log n ) c < n\u2212 [29] log(s/d)", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "log(cs/d) \u2265 1\u2212 o( 1 logn ) [29] log(s/d) log(cs/d) = 1\u2212 [29]", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "log(cs/d) \u2265 1\u2212 o( 1 logn ) [29] log(s/d) log(cs/d) = 1\u2212 [29]", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "The upper bounds cited to [29] use fast matrix multiplication, whereas the rest don\u2019t and are usable as data structures.", "startOffset": 26, "endOffset": 30}, {"referenceID": 36, "context": "Indeed, previous results [37, 40] have investigated lower bounds for symmetric LSH and it is not clear if they can be extended to the asymmetric case.", "startOffset": 25, "endOffset": 33}, {"referenceID": 39, "context": "Indeed, previous results [37, 40] have investigated lower bounds for symmetric LSH and it is not clear if they can be extended to the asymmetric case.", "startOffset": 25, "endOffset": 33}, {"referenceID": 38, "context": "The starting point of our proof is the same as in [39]: Use a collision matrix given by two sequences of data and query vectors that force the gap to be small.", "startOffset": 50, "endOffset": 54}, {"referenceID": 38, "context": "The proof in [39] then applies an asymptotic analysis of the margin complexity of this matrix [49], and it shows that for any given value of P1 \u2212 P2 there are sufficiently large data and query domains for which the gap must be smaller.", "startOffset": 13, "endOffset": 17}, {"referenceID": 48, "context": "The proof in [39] then applies an asymptotic analysis of the margin complexity of this matrix [49], and it shows that for any given value of P1 \u2212 P2 there are sufficiently large data and query domains for which the gap must be smaller.", "startOffset": 94, "endOffset": 98}, {"referenceID": 38, "context": "Our method also highlights a dependency of the gap on the dimension, which is missing in [39].", "startOffset": 89, "endOffset": 93}, {"referenceID": 38, "context": "We first show that it is possible to improve the asymmetric LSH in [39, 46] by just plugging the best known data structure for Approximate Near Neighbor for `2 on a sphere [9] into the reduction in [39, 12].", "startOffset": 67, "endOffset": 75}, {"referenceID": 45, "context": "We first show that it is possible to improve the asymmetric LSH in [39, 46] by just plugging the best known data structure for Approximate Near Neighbor for `2 on a sphere [9] into the reduction in [39, 12].", "startOffset": 67, "endOffset": 75}, {"referenceID": 8, "context": "We first show that it is possible to improve the asymmetric LSH in [39, 46] by just plugging the best known data structure for Approximate Near Neighbor for `2 on a sphere [9] into the reduction in [39, 12].", "startOffset": 172, "endOffset": 175}, {"referenceID": 38, "context": "We first show that it is possible to improve the asymmetric LSH in [39, 46] by just plugging the best known data structure for Approximate Near Neighbor for `2 on a sphere [9] into the reduction in [39, 12].", "startOffset": 198, "endOffset": 206}, {"referenceID": 11, "context": "We first show that it is possible to improve the asymmetric LSH in [39, 46] by just plugging the best known data structure for Approximate Near Neighbor for `2 on a sphere [9] into the reduction in [39, 12].", "startOffset": 198, "endOffset": 206}, {"referenceID": 45, "context": "In the {0, 1} domain, this LSH improves upon the state of the art [46] for some ranges of c and s.", "startOffset": 66, "endOffset": 70}, {"referenceID": 38, "context": "Then we show how to circumvent the impossibility results in [39, 45] by showing that there exists a symmetric LSH when the data and query space coincide by allowing the bounds on collision probability to not hold when the data and query vectors are identical.", "startOffset": 60, "endOffset": 68}, {"referenceID": 44, "context": "Then we show how to circumvent the impossibility results in [39, 45] by showing that there exists a symmetric LSH when the data and query space coincide by allowing the bounds on collision probability to not hold when the data and query vectors are identical.", "startOffset": 60, "endOffset": 68}, {"referenceID": 4, "context": "We conclude by describing a data structure based on the linear sketches for `p in [5] for unsigned (cs, s) search: for any given 0 < \u03ba \u2264 1/2, the data structure yields a c = 1/n approximation with \u00d5 ( dn2\u22122/\u03ba ) construction", "startOffset": 82, "endOffset": 85}, {"referenceID": 28, "context": "We note that the result in [29] also reaches subquadtratic time for the {\u22121, 1} case.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 17, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 18, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 25, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 26, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 33, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 35, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 46, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 51, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 52, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 57, "context": "[17, 18, 19, 26, 27, 34, 36, 47, 52, 53, 58]), as well as in information retrieval (e.", "startOffset": 0, "endOffset": 44}, {"referenceID": 13, "context": "[14, 21, 59]), and knowledge discovery (e.", "startOffset": 0, "endOffset": 12}, {"referenceID": 20, "context": "[14, 21, 59]), and knowledge discovery (e.", "startOffset": 0, "endOffset": 12}, {"referenceID": 58, "context": "[14, 21, 59]), and knowledge discovery (e.", "startOffset": 0, "endOffset": 12}, {"referenceID": 2, "context": "[3, 13, 54, 60, 62]).", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "[3, 13, 54, 60, 62]).", "startOffset": 0, "endOffset": 19}, {"referenceID": 53, "context": "[3, 13, 54, 60, 62]).", "startOffset": 0, "endOffset": 19}, {"referenceID": 59, "context": "[3, 13, 54, 60, 62]).", "startOffset": 0, "endOffset": 19}, {"referenceID": 61, "context": "[3, 13, 54, 60, 62]).", "startOffset": 0, "endOffset": 19}, {"referenceID": 40, "context": "It was recently shown how approximate LSH-based similarity join can be made I/O-efficient [41].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "The inner product similarity for the case of normalized vectors is known as \u201ccosine similarity\u201d and it is well understood [16, 33, 43].", "startOffset": 122, "endOffset": 134}, {"referenceID": 32, "context": "The inner product similarity for the case of normalized vectors is known as \u201ccosine similarity\u201d and it is well understood [16, 33, 43].", "startOffset": 122, "endOffset": 134}, {"referenceID": 42, "context": "The inner product similarity for the case of normalized vectors is known as \u201ccosine similarity\u201d and it is well understood [16, 33, 43].", "startOffset": 122, "endOffset": 134}, {"referenceID": 42, "context": "While the general case where vectors may have any length appears theoretically challenging, practically efficient indexes for unsigned search were proposed in [43, 30], based on tree data structures combined with a branch-and-bound space partitioning technique similar to k-d trees, and in [12] based on principal component axes trees.", "startOffset": 159, "endOffset": 167}, {"referenceID": 29, "context": "While the general case where vectors may have any length appears theoretically challenging, practically efficient indexes for unsigned search were proposed in [43, 30], based on tree data structures combined with a branch-and-bound space partitioning technique similar to k-d trees, and in [12] based on principal component axes trees.", "startOffset": 159, "endOffset": 167}, {"referenceID": 11, "context": "While the general case where vectors may have any length appears theoretically challenging, practically efficient indexes for unsigned search were proposed in [43, 30], based on tree data structures combined with a branch-and-bound space partitioning technique similar to k-d trees, and in [12] based on principal component axes trees.", "startOffset": 290, "endOffset": 294}, {"referenceID": 34, "context": "For document term vectors Low and Zheng [35] showed that unsigned search can be sped up using matrix compression ideas.", "startOffset": 40, "endOffset": 44}, {"referenceID": 54, "context": "However, as many similarity search problems, the exact version considered in these papers suffers from the curse of dimensionality [55].", "startOffset": 131, "endOffset": 135}, {"referenceID": 44, "context": "The efficiency of approximate IPS approaches based on LSH is studied in [45, 39].", "startOffset": 72, "endOffset": 80}, {"referenceID": 38, "context": "The efficiency of approximate IPS approaches based on LSH is studied in [45, 39].", "startOffset": 72, "endOffset": 80}, {"referenceID": 45, "context": "An asymmetric LSH for binary inner product is proposed in [46].", "startOffset": 58, "endOffset": 62}, {"referenceID": 50, "context": "Valiant [51] showed how to reduce the problem to matrix multiplication, when cs \u2248 O( \u221a n) and s \u2248 O(n), significantly improving on the asymptotic time complexity of approaches based on LSH.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "[29], who also generalized the sub-quadratic running time to the case when log(s)/ log(cs) is small.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "In another surprising development Alman and Williams [4] showed that for d = O (log n) dimensions, truly subquadratic algorithms for the exact IPS join problem on binary vectors is possible.", "startOffset": 53, "endOffset": 56}, {"referenceID": 55, "context": "OVP derives its hardness from the Strong Exponential Time Hypothesis (Williams [56]), but could potentially be true even if SETH is not.", "startOffset": 79, "endOffset": 83}, {"referenceID": 55, "context": "Conjecture 1 (OVP, [56]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "[1] have proposed an algorithm for OVP running in time n2\u22121/O(\u03b3 log 2 , when d = \u03b3 log n.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": ", [2] page 782):", "startOffset": 2, "endOffset": 5}, {"referenceID": 50, "context": "The polynomials have the following properties [51]:", "startOffset": 46, "endOffset": 50}, {"referenceID": 50, "context": "We note that the Chebyshev embedding proposed by Valiant [51] can provide similar results; however, our construction is deterministic, while Valiant\u2019s is randomized.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "The bound holds for a fixed set of data vectors, so it applies also to data dependent LSH [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 38, "context": "A consequence of our result is that there cannot exist an asymmetric LSH for any dimension d \u2265 1 when the set of query vectors is unbounded, getting a result similar to that of [39], which however requires even the data space to be unbounded and d \u2265 2.", "startOffset": 177, "endOffset": 181}, {"referenceID": 38, "context": "We observe that these sequences are similar to the one used in [39].", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "[42]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "1 that by plugging the best known LSH for `2 distance on a sphere [9] into a reduction presented in [12, 39], we get a data structure based on LSH for signed MIPS with search time exponent \u03c1 = (1\u2212 s)/(1 + (1\u2212 2c)s).", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "1 that by plugging the best known LSH for `2 distance on a sphere [9] into a reduction presented in [12, 39], we get a data structure based on LSH for signed MIPS with search time exponent \u03c1 = (1\u2212 s)/(1 + (1\u2212 2c)s).", "startOffset": 100, "endOffset": 108}, {"referenceID": 38, "context": "1 that by plugging the best known LSH for `2 distance on a sphere [9] into a reduction presented in [12, 39], we get a data structure based on LSH for signed MIPS with search time exponent \u03c1 = (1\u2212 s)/(1 + (1\u2212 2c)s).", "startOffset": 100, "endOffset": 108}, {"referenceID": 38, "context": "2, we show how to circumvent the results in [39, 45] showing that symmetric LSH is not possible when the data and query domains coincide (while an asymmetric LSH does exist).", "startOffset": 44, "endOffset": 52}, {"referenceID": 44, "context": "2, we show how to circumvent the results in [39, 45] showing that symmetric LSH is not possible when the data and query domains coincide (while an asymmetric LSH does exist).", "startOffset": 44, "endOffset": 52}, {"referenceID": 37, "context": "The LSH construction uses explicit incoherent matrices built using Reed-Solomon codes [38] to implement a symmetric version of the reduction in [12, 39].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "The LSH construction uses explicit incoherent matrices built using Reed-Solomon codes [38] to implement a symmetric version of the reduction in [12, 39].", "startOffset": 144, "endOffset": 152}, {"referenceID": 38, "context": "The LSH construction uses explicit incoherent matrices built using Reed-Solomon codes [38] to implement a symmetric version of the reduction in [12, 39].", "startOffset": 144, "endOffset": 152}, {"referenceID": 4, "context": "3 we solve unsigned (cs, s) join using linear sketches for `p norms from [5].", "startOffset": 73, "endOffset": 76}, {"referenceID": 38, "context": "Vectors are embedded into a (d+2)-dimensional unit sphere using the asymmetric map as in [39]: a data vector p is", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "The latter can be solved in space O(n + dn) and query time O(n) using the LSH construction of [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 38, "context": "In Figure 2, we plot the \u03c1 values of three LSH constructions: the one proposed here (with U = 1), the one from [39], and the one from [46].", "startOffset": 111, "endOffset": 115}, {"referenceID": 45, "context": "In Figure 2, we plot the \u03c1 values of three LSH constructions: the one proposed here (with U = 1), the one from [39], and the one from [46].", "startOffset": 134, "endOffset": 138}, {"referenceID": 38, "context": "We point out that our bound is always stronger than the one from [39] and sometimes stronger than the one from [46], despite that the latter is tailored for binary vectors.", "startOffset": 65, "endOffset": 69}, {"referenceID": 45, "context": "We point out that our bound is always stronger than the one from [39] and sometimes stronger than the one from [46], despite that the latter is tailored for binary vectors.", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "We point out that in practice one may want to use a recent LSH family from [7] that\u2014both in theory and in practice\u2014is superior to the hyperplane LSH from [16] used in [39].", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "We point out that in practice one may want to use a recent LSH family from [7] that\u2014both in theory and in practice\u2014is superior to the hyperplane LSH from [16] used in [39].", "startOffset": 154, "endOffset": 158}, {"referenceID": 38, "context": "We point out that in practice one may want to use a recent LSH family from [7] that\u2014both in theory and in practice\u2014is superior to the hyperplane LSH from [16] used in [39].", "startOffset": 167, "endOffset": 171}, {"referenceID": 38, "context": "Neyshabur and Srebro [39] show that an asymmetric view on LSH for signed IPS is required.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "We then plug in any Euclidean LSH for ANN on the sphere, for example the one from [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 38, "context": "This reduction treats data and query vectors identically, unlike the one from [39], and thus we are able to obtain a symmetric LSH.", "startOffset": 78, "endOffset": 82}, {"referenceID": 37, "context": ", in [38] it is shown how to build such vectors using Reed-Solomon codes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "The resulting dimension is O ( \u03b5\u22122 logN ) = O ( kd/\u03b5 ) [32, 38].", "startOffset": 55, "endOffset": 63}, {"referenceID": 37, "context": "The resulting dimension is O ( \u03b5\u22122 logN ) = O ( kd/\u03b5 ) [32, 38].", "startOffset": 55, "endOffset": 63}, {"referenceID": 8, "context": "from [9, 7], with distance threshold r = 2(1\u2212s+ ), approximation factor c\u20322 = (1\u2212cs\u2212 )/r.", "startOffset": 5, "endOffset": 11}, {"referenceID": 6, "context": "from [9, 7], with distance threshold r = 2(1\u2212s+ ), approximation factor c\u20322 = (1\u2212cs\u2212 )/r.", "startOffset": 5, "endOffset": 11}, {"referenceID": 5, "context": "This LSH can used for solving signed (cs, s) IPS as a traditional LSH [6], although it is required an initial step that verifies whether a query vector is in the input set and, if this is the case, returns the vector q itself if q q \u2265 s.", "startOffset": 70, "endOffset": 73}, {"referenceID": 38, "context": "Figure 2: Our \u03c1 value (DATA-DEP) compared to that of [39] (SIMP) and the binary data only of [46] (MH-ALSH).", "startOffset": 53, "endOffset": 57}, {"referenceID": 45, "context": "Figure 2: Our \u03c1 value (DATA-DEP) compared to that of [39] (SIMP) and the binary data only of [46] (MH-ALSH).", "startOffset": 93, "endOffset": 97}, {"referenceID": 56, "context": "This problem can be tackled using linear sketches (for an overview see [57, 8]).", "startOffset": 71, "endOffset": 78}, {"referenceID": 7, "context": "This problem can be tackled using linear sketches (for an overview see [57, 8]).", "startOffset": 71, "endOffset": 78}, {"referenceID": 4, "context": "More specifically, we use the following result from [5]: for every 2 \u2264 \u03ba \u2264 \u221e there exists a distribution over \u00d5(n1\u22122/\u03ba)\u00d7 n matrices \u03a0 such that for every x \u2208 R one has:", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "Thus, to build a data structure for computing \u2016Aq\u2016\u221e, we sample a matrix \u03a0 according to the aforementioned result in [5] and com-", "startOffset": 116, "endOffset": 119}, {"referenceID": 45, "context": "1 improves upon the state of the art [46] based on minwise hashing for different values (e.", "startOffset": 37, "endOffset": 41}], "year": 2016, "abstractText": "A number of tasks in classification, information retrieval, recommendation systems, and record linkage reduce to the core problem of inner product similarity join (IPS join): identifying pairs of vectors in a collection that have a sufficiently large inner product. IPS join is well understood when vectors are normalized and some approximation of inner products is allowed. However, the general case where vectors may have any length appears much more challenging. Recently, new upper bounds based on asymmetric locality-sensitive hashing (ALSH) and asymmetric embeddings have emerged, but little has been known on the lower bound side. In this paper we initiate a systematic study of inner product similarity join, showing new lower and upper bounds. Our main results are: \u2022 Approximation hardness of IPS join in subquadratic time, assuming the strong exponential time hypothesis. \u2022 New upper and lower bounds for (A)LSH-based algorithms. In particular, we show that asymmetry can be avoided by relaxing the LSH definition to only consider the collision probability of distinct elements. \u2022 A new indexing method for IPS based on linear sketches, implying that our hardness results are not far from", "creator": "LaTeX with hyperref package"}}}