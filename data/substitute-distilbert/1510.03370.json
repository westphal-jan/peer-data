{"id": "1510.03370", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2015", "title": "Asymptotic Logical Uncertainty and The Benford Test", "abstract": "we give an algorithm handler which assigns skinner to logical events. for any simple consecutive sequence of sentences whose truth - values appear indistinguishable from a biased coin that outputs \" true \" with probability p, we have that the sequence assigns probabilities that skinner assigns to these sentences converges to p.", "histories": [["v1", "Mon, 12 Oct 2015 17:14:44 GMT  (96kb)", "http://arxiv.org/abs/1510.03370v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["scott garrabrant", "siddharth bhaskar", "abram demski", "joanna garrabrant", "george koleszarik", "evan lloyd"], "accepted": false, "id": "1510.03370"}, "pdf": {"name": "1510.03370.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Scott Garrabrant", "Siddharth Bhaskar", "Abram Demski", "Joanna Garrabrant"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n03 37\n0v 1\n[ cs\n.L G\n] 1\n2 O\nct 2\n01 5"}, {"heading": "1 Introduction", "text": "Let \u03c61, \u03c62, . . . be a simple enumeration of all sentences in first order logic over ZFC. The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that \u03c6N is true [1, 2, 3, 4].\n1 This notion of probability does not refer to random variables. It refers to the degree of uncertainty that one might have about logical sentences whose truth-values have not been calculated.\nMuch work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11]. In this case, M(N) is not computable, and can easily be 1 for all provable \u03c6 and 0 for all disprovable \u03c6, so all of the work is in figuring out how M should behave when \u03c6 is independent of ZFC.\nIn this paper, we take a different approach, which we call asymptotic logical uncertainty. We require that M(N) be computable and have runtime bounded by some function of N .\nWe propose as a baseline that any method of quickly assigning probabilities should be able to pass a test we call the Benford test. Consider the infinite sequence of sentences {\u03c6sn} given by \u03c6sn = \u201cThe first digit of Research supported by the Machine Intelligence Research Institute (intelligence.org). Technical Report 2015\u201311.\n1The problem has also been studied in the case where we don\u2019t require computability even in the limit [5, 6, 7]. The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].\n3 \u2191n 3 is a 1.\u201d We say that M passes the Benford test if\nlim n\u2192\u221e\nM(sn) = log10(2) \u2248 .30103,\nas prescribed by Benford\u2019s law. More generally, we say that M passes the generalized Benford test if it converges to the correct probability on any similar infinite sequences whose truth values appear indistinguishable from independent flips of a biased coin. We then give an algorithm AL,T which passes the generalized Benford test.\nLogical uncertainty is one aspect of the problem of combining probability and logic, of which statistical relational learning is another [12]. Statistical relational learning addresses the problem of representing probabilistic models with logical structure, including regularities such as repeated entities and other complexities such as uncertainty about the number of entities. In contrast, logical uncertainty deals with uncertainty about logic. As Paul Christiano put it: \u201cany realistic agent is necessarily uncertain not only about its environment or about the future, but also about the logically necessary consequences of its beliefs.\u201d [1]"}, {"heading": "2 The Benford Test", "text": "Benford\u2019s law states that in naturally occurring numbers, the leading digit d \u2208 {1, . . . , 9} of that number in base 10 occurs with probability log10(1 + 1 d ). Many mathematical sequences have been shown to have frequencies of first digits that satisfy Benford\u2019s law [13]. In particular, the frequencies of the first digits of powers of 3 provably satisfy Benford\u2019s law.\nThe function 3 \u2191n k is defined by 3 \u21911 k = 3k, 3 \u2191n 1 = 3, and 3 \u2191n k = 3 \u2191n\u22121 (3 \u2191n (k \u2212 1)). Throughout the paper, let T (N) be an increasing time complexity function in the range of N \u2264 T (N) \u2264 3 \u2191k N for some fixed k, and let R(N) = T (N)N4 logT (N).\nConsider the sequence 3 \u2191n 3. Clearly this sequence only contains powers of 3. We might hypothesize that the frequencies of the first digits in this sequence also satisfy Benford\u2019s law. However, 3 \u2191n 3 is very large, and first digit of 3 \u2191n 3 is probably very difficult to\ncompute. It is unlikely that the first digit of 3 \u21913 3 will ever be known.\nIf asked to quickly assign a probability to the sentence \u03c6sn = \u201cThe first digit of 3 \u2191n 3 is a 1,\u201d for some n > 2, the only reasonable answer would be log10(2) \u2248 .30103. Note that \u03c6sn is either true or false; there are no random variables. The probability here represents a reasonable guess in the absence of enough time or resources to compute 3 \u2191n 3.\nDefinition 2.1. Let M be a Turing machine which on input N runs in time O(R(N)) and outputs a probability M(N), which represents the probability assigned to \u03c6N . We say that M passes the Benford test if\nlim n\u2192\u221e M(sn) = log10(2),\nwhere \u03c6sn = \u201cThe first digit of 3 \u2191n 3 is a 1.\u201d\nIt is easy to pass the Benford test by hard-coding in the probability. It is more difficult to pass the Benford test in a natural way. That the best probability to assign to \u03c6sn is log10(2) depends not only on the fact that the frequency with which \u03c6sn is true tends toward log10(2), but also on the fact that the sequence of truth-values of \u03c6sn contains no patterns that can be used to quickly compute a better probability on some subsequence. We therefore assume that this sequence of truth-values is indistinguishable from a sequence produced by a coin that outputs \u201ctrue\u201d with probability log10(2). Formally, we are assuming that S = {sn|n \u2208 N} is an irreducible pattern with probability log10(2), as defined in the next section."}, {"heading": "3 Irreducible Patterns", "text": "Fix a universal Turing machine U and an encoding scheme for machines, and let U(M,x) denote running the machine U to simulate M with input x.\nDefinition 3.1. 2 Let S \u2286 N be an infinite subset of natural numbers such that \u03c6N is provable or disprovable for all N \u2208 S, and there exists a Turing machine Z such that U(Z,N) runs in time T (N) and accepts N if and only if N \u2208 S.\nWe say that S is an irreducible pattern with probability p if there exists a constant c such that for every positive integer m \u2265 3 and every Turing machine W expressible in k(W ) bits, if\nS\u2032 = {N \u2208 S | U(W,N) accepts in time T (N)} 2We tailored this definition of irreducible pattern to our needs. The theory of algorithmic randomness may offer alternatives. However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1\n2 [14, 15, 16]. We believe that any reasonable definition inspired by algorithmic randomness would imply Definition 3.1.\nhas at least m elements and r(m,W ) is the probability that \u03c6N is provable when N is chosen uniformly at random from the first m elements of S\u2032, we have\n|r(m,W ) \u2212 p|< ck(W ) \u221a log logm\u221a m .\nThe intuition behind the formula is that the observed frequency r(m,W ) for any sequence S\u2032 we select should not stray far from p. The right hand side of the inequality needs to shrink slowly enough that a true random process would stay within it with probability 1 (given choice of c sufficiently large to accommodate initial variation). The law of the iterated logarithm gives such a formula, which is also tight in the sense that we cannot replace it with a formula which diminishes more quickly as a function of m.\nProposition 3.2. If we replace provability in Definition 3.1 with a random process, such that for each N \u2208 S the sentence \u03c6N is independently called \u201cprovable\u201d with probability p, then S would almost surely be an irreducible pattern with probability p.\nProof. Fix a Turing machine W . By the law of the iterated logarithm, there exists a constant c1 such that\nlim sup m\u2192\u221e |mr(m,W )\u2212mp|\u221a m log logm = c1\nalmost surely. Therefore\nsup m |mr(m,W ) \u2212mp|\u221a m log logm < \u221e\nalmost surely. We will use \u03a6(W ) as a shorthand for this supremum. For any \u03b5 > 0, there therefore exists a c2 such that P(\u03a6(W ) > c2) \u2264 \u03b5.\nWe now show that P(\u03a6(W ) > 2c2 + 1) \u2264 \u03b52. By the chain rule for probabilities, it suffices to show that P((\u03a6(W ) > 2c2 + 1)|(\u03a6(W ) > c2)) \u2264 \u03b5. Assume \u03a6(W ) > c2, and Let m1 be the first m such that\n|mr(m,W ) \u2212mp|\u221a m log logm > c2.\nIt suffices to show that the probability that there exists an m2 with\n|m2r(m2,W )\u2212m2p|\u221a m2 log logm2 \u2212 |m1r(m1,W )\u2212m1p|\u221a m1 log logm1 > c2\nis at most \u03b5. Observe that\n|m2r(m2,W )\u2212m2p|\u221a m2 log logm2 \u2212 |m1r(m1,W )\u2212m1p|\u221a m1 log logm1 \u2264 |m2r(m2,W )\u2212m1r(m1,W )\u2212 (m2 \u2212m1)p|\u221a (m2 \u2212m1) log log(m2 \u2212m1) ,\nand that the probability there exists an m2 with\n|m2r(m2,W )\u2212m1r(m1,W )\u2212 (m2 \u2212m1)p| \u221a\n(m2 \u2212m1) log log(m2 \u2212m1) > c2\nis the same as the probability that \u03a6(W ) > c2, which is at most \u03b5.\nWe have thus shown that for every \u03b5, there exists a constant c3 = c2 + 1 such that the probability that \u03a6(W ) \u2265 2\u2113c3 is at most \u03b52 \u2113\n. Partition the set of all Turing machines into sets W1,W2, . . . , such that W\u2113 contains all Turing machines expressed in at least 2\u2113 but fewer than 2\u2113+1 bits. The probability that a Turing machine W in W\u2113 violates\n|r(m,W )\u2212 p|< c3k(W ) \u221a log logm\u221a m , (\u22c6)\nfor any m \u2265 3 is at most \u03b52\u2113 . The number of Turing machines in W\u2113 is at most 22 \u2113+1\n, so the probability that there is any W \u2208 W\u2113 and m \u2265 3 which violate (\u22c6) is at most \u03b52 \u2113 22 \u2113+1\n. Therefore, the probability that there is any Turing machine W and m \u2265 3 which violate (\u22c6) is at most\n\u2211 \u2113\u2208N \u03b52 \u2113 22 \u2113+1 = \u2211 \u2113\u2208N (4\u03b5)2 \u2113 .\nFor small enough \u03b5 this goes to 0, so for large enough c3, the probability that (\u22c6) holds for all W and m goes to 1. Therefore, with probability 1, there exists a c such that\n|r(m,W ) \u2212 p|< ck(W ) \u221a log logm\u221a m ,\nfor all m and W .\nWe now use the concept of irreducible patterns to generalize the Benford test.\nDefinition 3.3. Let M be a Turing machine which on input N runs in time O(R(N)) and outputs a probability M(N), which represents the probability assigned to \u03c6N . We say that M passes the generalized Benford test if\nlim N\u2192\u221e N\u2208S M(N) = p,\nwhenever S is an irreducible pattern with probability p.\nNote that if we conjecture that the S from Definition 2.1 is an irreducible pattern with probability log10(2), then any M which passes the generalized Benford test also passes the Benford test."}, {"heading": "4 A Learning Algorithm", "text": "We now introduce an algorithm AL,T that passes the generalized Benford test (see Algorithm 1).\nLet L be the Turing machine which accepts on input N if ZFC proves \u03c6N , rejects on input N if ZFC\nAlgorithm 1 AL,T (N)\n1: P = 0 2: M = N 3: for j = 0, . . . , N do 4: MY = 0 5: for Y a Turing machine expressible in KY <\nlogN bits do 6: MX = N 7: forX a Turing machine expressible in KX <\nlogN bits do 8: if U(X,N) and U(Y,N) both accept in\ntime T (N) then 9: A = 0\n10: R = 0 11: i = 1 12: while i \u2264 N do 13: if U(X, i) and U(Y, i) both accept\nin time T (i) then 14: if U(L, i) accepts in time\nT (N) then"}, {"heading": "15: A = A+ 1", "text": "16: else if U(L, i) rejects in time\nT (N) then 17: R = R+ 1 18: else 19: i = N 20: i = i+ 1 21: F = A/(A+R) 22: Q = A+R 23: if max ( KX , |F\u2212 j N |\u221aQ\nKY \u221a log logQ\n)\n< MX\nthen\n24: MX = max ( KX , |F\u2212 j\nN | \u221a Q\nKY \u221a log logQ\n)\n25: if MX > MY then 26: MY = MX 27: if MY < M then 28: M = MY 29: P = j/N\n30: return P\ndisproves \u03c6N , and otherwise does not halt. For convenience, in Algorithm 1, we define log q = 1 for q < 2.\nLet TM(N) be the set of all Turing machines X expressible in at most logN bits such that U(X,N) accepts in time at most T (N). The encoding of Turing machines must be prefix-free, which in particular means that no Turing machine is encoded in 0 bits. Let JN denote the set of rational numbers of the form j\nN with\nj = 0, . . . , N . For X and Y Turing machines, let K(X) be the number of bits necessary to encode X . Let S\u2032(X,Y ) be the subset of natural numbers i which are accepted by both U(X, i) and U(Y, i) in time at most T (i). Let QN (X,Y ) be the greatest number less than or equal to N such that for every s in the first QN (X,Y ) elements of S\u2032, U(L, s) halts in time T (N). Let FN (X,Y ) be the proportion of the first QN (X,Y ) elements of S\n\u2032 which L accepts. Let\nBN (X,Y, P )\n= max\n(\nK(X), |FN (X,Y )\u2212 P |\n\u221a\nQN (X,Y )\nK(Y ) \u221a\nlog logQN (X,Y )\n)\n.\nLemma 4.1. The output of AL,T on input N is in\nargmin P\u2208JN max Y \u2208TM(N) min X\u2208TM(N) BN (X,Y, P ).\nProof. The algorithm has three for loops, the outer ranging over j = 0, . . .N and the inner two ranging over Y and X respectively, both restricted to Turing machines expressible in logN bits. The condition on line 8 means that X and Y effectively range over all Turing machines in TM(N), and P = j\nN ranges over\nJN . The inner while loop will increment the variables A or R a total of exactly QN(X,Y ) times. Thus, Q is set to QN(X,Y ) in line 22. Similarly, F is sent to FN (X,Y ) in line 21. Clearly KX and KY are K(X) and K(Y ) respectively. Therefore, the expression on lines 23 and 24 is BN (X,Y, P ).\nConsidering the for loops from inner to outer, we minimize this quantity in X , maximize it in Y , and find P of the form j/N minimizing the whole quantity. The P returned is therefore a minimizer of\nmax Y \u2208TM(N) min X\u2208TM(N) BN(X,Y, P ).\nThe code is not optimized for computational efficiency. The following proposition is just to ensure that the runtime is not far off from T (N).\nProposition 4.2. The runtime of AL,T (N) is in O(R(N)) = O(T (N)N4 logT (N))).\nProof. Simulating U on any input for T time steps can be done in time cT logT for some fixed constant c [17]. The bulk of the runtime comes from simulating Turing machines on lines 8, 13, 14, and 16. Each of these lines takes at most cT (N) logT (N) time, and we enter each of these lines at most N4 times. Therefore, the program runs in time O(T (N)N4 logT (N))."}, {"heading": "5 Passing the Generalized Benford Test", "text": "We are now ready to show that AL,T passes the generalized Benford test. The proof will use the following two lemmas.\nLemma 5.1. Let S be an irreducible pattern with probability p, and let Z be a Turing machine such that U(Z,N) accepts in time T (N) if and only if N \u2208 S.\nThere exists a constant C such that if N \u2208 S, then there exists a P \u2208 JN such that\nmax Y \u2208TM(N) BN (Z, Y, P ) < C.\nProof. Let P = \u230apN\u230b N\n. From the definition of irreducible pattern, we have that there exists c such that for all Y ,\n|FN (Z, Y )\u2212 p|< cK(Y )\n\u221a\nlog logQN (Z, Y ) \u221a\nQN(Z, Y ) .\nClearly,\n|P \u2212 p|\u2264 1 N \u2264 1 QN(Z, Y ) \u2264 1\u221a QN (Z, Y )\n\u2264 K(Z)K(Y ) \u221a log logQN(Z, Y ) \u221a\nQN (Z, Y ) .\nSetting C = K(Z) + c, we get\n|FN (Z, Y )\u2212 P | \u2264 |FN (Z, Y )\u2212 p|+|P \u2212 p|\n< CK(Y )\n\u221a\nlog logQN (Z, Y ) \u221a\nQN(Z, Y ) ,\nso |FN (Z, Y )\u2212 P | \u221a QN(Z, Y )\nK(Y ) \u221a log logQN (Z, Y ) < C.\nClearly, K(Z) < C, so BN (Z, Y, P ) > C for all Y . Therefore,\nmax Y \u2208TM(N) BN (Z, Y, P ) < C.\nLemma 5.2. Let S be an irreducible pattern with probability p, and let Z be a Turing machine such that U(Z,N) accepts in time T (N) if and only if N \u2208 S.\nFor all C, for all \u03b5 > 0, for all N sufficiently large, for all P \u2208 JN , if N \u2208 S, and\nmin X\u2208TM(N) BN (X,Z, P ) < C,\nthen |P \u2212 p|< \u03b5.\nProof. Fix a C and a \u03b5 > 0. It suffices to show that for all N sufficiently large, if N \u2208 S and |P \u2212 p|\u2265 \u03b5, then for all X \u2208 TM(N), we have BN (X,Z, P ) \u2265 C.\nObserve that since BN (X,Z, P ) \u2265 K(X), this claim trivially holds when K(X) \u2265 C. Therefore we only have to check the claim for the finitely many Turing machines expressible in fewer than C bits.\nFix an arbitrary X . Since S is an irreducible pattern, there exists a c such that\n|FN (X,Z)\u2212 p|< cK(Z)\n\u221a\nlog logQN (X,Z) \u221a\nQN (X,Z) .\nWe may assume that S\u2032(X,Z) is infinite, since otherwise if we take N \u2208 S large enough, X /\u2208 TM(N). Thus, by taking N sufficiently large, we can get QN (X,Z) sufficiently large, and in particular satisfy\n\u221a\nQN (X,Z)\nK(Z) \u221a log logQN(X,Z) \u03b5 \u2265 C + c.\nTake N \u2208 S large enough that this holds for each X \u2208 TM(N) with K(X) < C, and assume |P \u2212 p|\u2265 \u03b5. By the triangle inequality, we have\n|FN (X,Z)\u2212 P |\u2265 |P \u2212 p|\u2212|FN (X,Z)\u2212 p|\n\u2265 \u03b5\u2212 cK(Z) \u221a log logQN (X,Z) \u221a\nQN (X,Z) .\nTherefore\nBN (X,Z, P )\n\u2265\n( \u03b5\u2212 cK(Z) \u221a\nlog logQN (X,Z)\u221a QN (X,Z)\n)\n\u221a\nQN (X,Z)\nK(Z) \u221a\nlog logQN(X,Z)\n=\n\u221a\nQN(X,Z)\nK(Z) \u221a log logQN (X,Z) \u03b5\u2212 c \u2265 C,\nwhich proves the claim.\nTheorem 5.3. AL,T passes the generalized Benford test.\nProof. Let S be an irreducible pattern with probability p. We must show that\nlim N\u2192\u221e N\u2208S AL,T (N) = p.\nLet Z be a Turing machine such that U(Z,N) accepts in time T (N) if and only if N \u2208 S.\nBy considering the case when X = Z, Lemma 5.1 implies that there exists a constant C such that for all N sufficiently large, there exists a P \u2208 JN such that\nmax Y \u2208TM(N) min X\u2208TM(N) BN (X,Y, P ) < C.\nSimilarly, using this value of C, and considering the case where Y = Z, Lemma 5.2 implies that for all \u03b5 > 0, for all N sufficiently large, for all P \u2208 JN if N \u2208 S, and\nmax Y \u2208TM(N) min X\u2208TM(N) BN (X,Y, P ) < C,\nthen |P \u2212 p|\u2264 \u03b5. Combining these, we get that for all \u03b5 > 0, for all N sufficiently large, if N \u2208 S and if P is in argmin P\u2208JN max Y \u2208TM(N) min X\u2208TM(N) BN (X,Y, P ),\nthen |P \u2212 p|\u2264 \u03b5. Thus, by Lemma 4.1, we get that for all \u03b5 > 0, for all N sufficiently large, if N \u2208 S, then |AL,T (N) \u2212 p|\u2264 \u03b5, so\nlim N\u2192\u221e N\u2208S AL,T (N) = p."}, {"heading": "6 Final Remarks", "text": "Definition 6.1. Given a sentence \u03c8, consider the infinite sequence of integers {s\u03c8n} given by \u03c6s\u03c8\n0\n= \u03c8 and\n\u03c6 s \u03c8 n+1 = \u00ac\u00ac\u03c6 s \u03c8 n . If a machine M satisfies\nlim n\u2192\u221e\nM(s\u03c8n) = p,\nwe say that M converges to p on \u03c8.\nCorollary 6.2. If \u03c8 is provable, then AL,T converges to 1 on \u03c8. If \u03c8 is disprovable, then AL,T converges to 0 on \u03c8.\nProof. If \u03c8 is provable, then {s\u03c8n} is an irreducible pattern with probably 1. If \u03c8 is disprovable, then {s\u03c8n} is an irreducible pattern with probably 0.\nIf \u03c8 is neither provable nor disprovable, then it is not clear whether or not AL,T even converges on \u03c8.\nQuestion 6.3. Does there exist a machine M such that M passes the generalized Benford test, and for each sentence \u03c8, there exists a P (\u03c8) such that M converges to P (\u03c8) on \u03c8?\nDefinition 6.4. A function P from logical sentences to [0, 1] is called coherent if it satisfies the following three properties:\n1. P (\u03c6) = 1 for all provable \u03c6,\n2. P (\u03c6) = 0 for all disprovable \u03c6, and\n3. P (\u03c6) = P (\u03c6 \u2227 \u03c8) + P (\u03c6 \u2227 \u00ac\u03c8) for all \u03c6 and \u03c8. Coherent functions correspond to probability distributions on the space of complete extensions of a given theory.\nQuestion 6.5. Does there exist a machine M and a coherent function P such that M passes the generalized Benford test, and for each sentence \u03c8, M converges to P (\u03c8) on \u03c8?"}], "references": [{"title": "Non-Omniscience, Probabilistic Inference, and Metamathematics", "author": ["Paul Christiano"], "venue": "Tech. rep. 2014\u20133. Berkeley, CA: Machine Intelligence Research Institute,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Logical Prior Probability", "author": ["Abram Demski"], "venue": "Artificial General Intelligence. 5th International Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Reasoning with Limited Resources and Assigning Probabilities to Arithmetical Statements", "author": ["Haim Gaifman"], "venue": "Synthese", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Concerning Measures in First Order Calculi", "author": ["Haim Gaifman"], "venue": "In: Israel Journal of Mathematics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1964}, {"title": "Probabilities on Sentences in an Expressive Logic", "author": ["Marcus Hutter"], "venue": "Journal of Applied Logic", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Assigning probabilities to logical formulas", "author": ["Dana Scott", "Peter Krauss"], "venue": "Studies in Logic and the Foundations of Mathematics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1966}, {"title": "Measures in Boolean algebras", "author": ["Alfred Horn", "Alfred Tarski"], "venue": "In: Transactions of the American Mathematical Society", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1948}, {"title": "Measures on Boolean algebras", "author": ["J.L. Kelley"], "venue": "Pacific Journal of Mathematics", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1959}, {"title": "An algebraic characterization of measure algebras", "author": ["Dorothy Maharam"], "venue": "Annals of Mathematics", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1947}, {"title": "Questions of Reasoning Under Logical Uncertainty", "author": ["Nate Soares", "Benja Fallenstein"], "venue": "Tech. rep. 2015\u2013 1. Berkeley, CA: Machine Intelligence Research Institute,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Introduction to statistical relational learning", "author": ["Lise Getoor"], "venue": "MIT press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Explaining the uneven distribution of numbers in nature: the laws of Benford and Zipf", "author": ["L. Pietronero"], "venue": "Physica A: Statistical Mechanics and its Applications", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "On the notion of infinite pseudorandom sequences", "author": ["Ker-I Ko"], "venue": "Theoretical Computer Science", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "The definition of random sequences", "author": ["Per Martin-L\u00f6f"], "venue": "Information and Control", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1966}, {"title": "Algorithmic randomness and complexity", "author": ["Rodney G. Downey", "Denis R. Hirschfeldt"], "venue": "Springer Science & Business Media,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Two-tape simulation of multitape Turing machines", "author": ["F.C. Hennie", "R.E. Stearns"], "venue": "Journal of the ACM", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1966}], "referenceMentions": [{"referenceID": 0, "context": "The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that \u03c6N is true [1, 2, 3, 4].", "startOffset": 157, "endOffset": 169}, {"referenceID": 1, "context": "The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that \u03c6N is true [1, 2, 3, 4].", "startOffset": 157, "endOffset": 169}, {"referenceID": 2, "context": "The goal of logical uncertainty is to construct an algorithm M which on input N outputs a probability M(N), which represents the probability that \u03c6N is true [1, 2, 3, 4].", "startOffset": 157, "endOffset": 169}, {"referenceID": 0, "context": "Much work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11].", "startOffset": 179, "endOffset": 189}, {"referenceID": 1, "context": "Much work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11].", "startOffset": 179, "endOffset": 189}, {"referenceID": 9, "context": "Much work has been done on a related problem where M on input N outputs an infinite sequence of numbers and M(N) is defined to be the limit of the sequence output by M on input N [1, 2, 11].", "startOffset": 179, "endOffset": 189}, {"referenceID": 3, "context": "The problem has also been studied in the case where we don\u2019t require computability even in the limit [5, 6, 7].", "startOffset": 101, "endOffset": 110}, {"referenceID": 4, "context": "The problem has also been studied in the case where we don\u2019t require computability even in the limit [5, 6, 7].", "startOffset": 101, "endOffset": 110}, {"referenceID": 5, "context": "The problem has also been studied in the case where we don\u2019t require computability even in the limit [5, 6, 7].", "startOffset": 101, "endOffset": 110}, {"referenceID": 6, "context": "The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].", "startOffset": 77, "endOffset": 87}, {"referenceID": 7, "context": "The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].", "startOffset": 77, "endOffset": 87}, {"referenceID": 8, "context": "The problem was first studied in the context of measures on Boolean algebras [8, 9, 10].", "startOffset": 77, "endOffset": 87}, {"referenceID": 10, "context": "Logical uncertainty is one aspect of the problem of combining probability and logic, of which statistical relational learning is another [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "\u201d [1]", "startOffset": 2, "endOffset": 5}, {"referenceID": 11, "context": "Many mathematical sequences have been shown to have frequencies of first digits that satisfy Benford\u2019s law [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1 2 [14, 15, 16].", "startOffset": 111, "endOffset": 123}, {"referenceID": 13, "context": "However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1 2 [14, 15, 16].", "startOffset": 111, "endOffset": 123}, {"referenceID": 14, "context": "However, algorithmic randomness generally considers all computable tests and focuses on the case where p = 1 2 [14, 15, 16].", "startOffset": 111, "endOffset": 123}, {"referenceID": 15, "context": "Simulating U on any input for T time steps can be done in time cT logT for some fixed constant c [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "A function P from logical sentences to [0, 1] is called coherent if it satisfies the following three properties:", "startOffset": 39, "endOffset": 45}], "year": 2015, "abstractText": "We give an algorithm AL,T which assigns probabilities to logical sentences. For any simple infinite sequence {\u03c6sn} of sentences whose truthvalues appear indistinguishable from a biased coin that outputs \u201ctrue\u201d with probability p, we have limn\u2192\u221e AL,T (sn) = p.", "creator": "LaTeX with hyperref package"}}}