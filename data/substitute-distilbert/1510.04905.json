{"id": "1510.04905", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2015", "title": "Robust Partially-Compressed Least-Squares", "abstract": "randomized matrix compression techniques, using as the johnson - lindenstrauss transform, have emerged as an expansive and practical way for solving large - scale problems efficiently. adopting a focus on computational efficiency, however, forsaking solutions by information success becomes the trade - off. in this paper, we investigate compressed large - squares problems and propose new models and algorithms that address the processes of error and noise introduced by compression. while maintaining noise efficiency, diffusion models provide robust solutions where are more accurate - - relative to solutions of uncompressed under - squares - - than those of classical compressed variants. we introduce tools from robust optimization together - a form of partial compression to preserve the error - time trade - offs of compressed least - squares solvers. we evolve an efficient solution algorithm for our robust partially - compressed ( rpc ) model based on a reduction to a one - dimensional sum. we also derive the first approximation error bounds for partially - compressed least - squares solutions. empirical results comparing numerous algorithms suggest that tighter and more compressed solutions are effectively insulated at aggressive randomized transforms.", "histories": [["v1", "Fri, 16 Oct 2015 14:59:04 GMT  (450kb,D)", "http://arxiv.org/abs/1510.04905v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["stephen becker", "ban kawas", "marek petrik"], "accepted": true, "id": "1510.04905"}, "pdf": {"name": "1510.04905.pdf", "metadata": {"source": "CRF", "title": "Robust Partially-Compressed Least-Squares", "authors": ["Stephen Becker", "Ban Kawas", "Marek Petrik", "Karthikeyan N. Ramamurthy"], "emails": [], "sections": [{"heading": null, "text": "Randomized matrix compression techniques, such as the Johnson-Lindenstrauss transform, have emerged as an effective and practical way for solving large-scale problems efficiently. With a focus on computational efficiency, however, forsaking solutions quality and accuracy becomes the trade-off. In this paper, we investigate compressed least-squares problems and propose new models and algorithms that address the issue of error and noise introduced by compression. While maintaining computational efficiency, our models provide robust solutions that are more accurate\u2014relative to solutions of uncompressed least-squares\u2014than those of classical compressed variants. We introduce tools from robust optimization together with a form of partial compression to improve the error-time trade-offs of compressed least-squares solvers. We develop an efficient solution algorithm for our Robust Partially-Compressed (RPC) model based on a reduction to a one-dimensional search. We also derive the first approximation error bounds for Partially-Compressed least-squares solutions. Empirical results comparing numerous alternatives suggest that robust and partially compressed solutions are effectively insulated against aggressive randomized transforms."}, {"heading": "1 Introduction", "text": "Random projection is a simple and effective dimensionality reduction technique that enables significant speedups in solving large-scale machine learning problems [6, 10]. It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11]. The focus of this paper will be on the latter. We consider the following canonical least-squares estimator, with A \u2208 RM\u00d7N :\nxLS def = argmin\nx\n1 2 \u2016Ax\u2212 b\u20162 = (ATA)\u22121AT b (1)\nar X\niv :1\n51 0.\n04 90\n5v 1\n[ st\nat .M\nL ]\nwhere \u2016\u00b7\u2016, for vectors, denotes the Euclidean norm throughout the paper, and A has the full column rank. We assume that M N and refer to xLS as the solution to the uncompressed problem.\nWhen M is very large, solving the least-squares problem in (1) can be time-consuming and computationally expensive. To gain the necessary speedups, random projections are used. The standard approach to doing so proceeds as follows [7]. First, we construct a compression matrix \u03a6 \u2208 Rm\u00d7M from a random distribution such that E [ \u03a6T\u03a6 ] = I and m M . Then, we solve the fully compressed problem:\nxCLS def = argmin\nx\n1 2 \u2016\u03a6 (Ax\u2212 b)\u20162 (2)\nNumerous efficient methods for constructing the compression matrix \u03a6 have been developed; surveys are provided in [3, 7, 10]. We describe and use several common methods for constructing \u03a6 in Section 5.\nWhen m in \u03a6 is small, the fully compressed least-squares problem in (2) is much easier to solve than the uncompressed least-squares in (1). However, as demonstrated in our numerical results, compression can introduce significant errors to the solution xCLS when compared to the uncompressed solution, xLS (see Fig. 1, explained in details in Section 5) and one is forced to consider the trade-off between accuracy and efficiency. As our main contribution, we propose and analyze two new models that address this issue and provide a desirable trade-off; enabling robust solutions while preserving small computational complexity.\nDemonstrated in Fig. 1\u2014which is based on a real dataset from the National Health Interview Survey from 1992\u2014by simply applying a form of regularization or by applying robust optimization techniques to explicitly model errors introduced by compression, we already observe increased\naccuracy of solutions compared to the classical compressed solution, xCLS. Inspired by these results, our main model alleviates the effects of compression on solutions\u2019 quality\u2014while maintaining the computational benefits of compressed least-squares\u2014by applying the compression on only the computationally intensive terms of ordinary least-squares. Our new model is the following partiallycompressed least-squares estimator:\nmin x\n1 2 \u2016\u03a6Ax\u20162 \u2212 bTAx (3)\nand its solution is given by:\nxPCLS def = (AT\u03a6T\u03a6A)\u22121ATb . (4)\nAgain, notice that only the computationally expensive parts of the ordinary least-squares estimator, which involve inverting ATA, are compressed. Also notice, in comparison, that the objective function of the fully compressed least-squares estimator is 12 \u2016\u03a6Ax\u2016 2 \u2212 bT \u03a6T\u03a6Ax.\nWhile not the focus of this paper\u2014since our goal here is to introduce our new estimator in (3)\u2014it is important to note that the prediction error \u2016AxPCLS\u2212AxLS\u2016 of the partially compressed solution, xPCLS, is not always smaller than the error of the fully compressed one, xCLS. Section 4 describes when and why this is the case. Currently, we are investigating this topic in more details and have derived a new model that combines both solutions and outperforms both individually.\nObserving that our new estimator still introduces prediction errors when compared to the original uncompressed solution xLS, we have derived our second model, the robust partially-compressed least-squares estimator (RPC). Described in Section 2, RPC explicitly models errors introduced by compression and is closely related to robust least-squares regression [8]. Leveraging robust optimization techniques makes it possible to reduce the solution error without excessively increasing computational complexity and is a data-driven approach that has been widely used in the last two decades [2]. In our numerical results, we have observed a similar effect to that when applying robust optimization to the fully compressed least-squares solution; increased accuracy and reduction in error.\nWhile RPC can be formulated as a second-order conic program (SOCP), generic off-the-shelf SOCP solvers may be slow for large problems. Therefore, as one of our contributions, we have developed a fast algorithm based on a one-dimensional search that can be faster than CPLEX by over an order of magnitude. Using this fast algorithm, described in Section 3, the RPC model is asymptotically just as efficient as the non-robust model. Table 1 puts our results in context of prior related work and all model variants we consider are presented in Table 3.\nOur empirical results, discussed in Section 5, show that both partially-compressed and robust partially-compressed solutions can outperform models that use full compression in terms of quality of solutions. We also show that compressed variants are more computationally efficient than ordinary least-squares, especially as dimensions grow."}, {"heading": "2 Robust Partially-Compressed Least-Squares", "text": "In this section, we describe how to incorporate robustness in our Partially-Compressed least-squares model (3). As described above, our objective is to enhance solutions\u2019 quality and increase robustness against noise and errors introduced by compression. One way of improving robustness is to use ridge regression, which when applied to our model (3), we obtain the following formulation:\nmin x\n1 2 \u2016\u03a6Ax\u20162 \u2212 bTAx+ \u00b5\u2016x\u20162, (5)\nfor some regularization parameter \u00b5. One caveat of using ridge regression is that it does not capture the error structure introduced by compression, which differs significantly from that present in the data of the original uncompressed ordinary least-squares problem. Robust optimization [2], however, enables us to do exactly that and allows us to explicity model the error structure. The following is our Robust Partially-Compressed (RPC) estimator:\nxRPC = argmin x max \u2016\u2206P\u2016F\u2264\u03c1\n1 2 \u2016(P + \u2206P )x\u20162 \u2212 bTAx (6)\nwhere P = \u03a6A and \u2206P is a matrix variable of size m\u00d7N . The general formulation of the problem allows for a more targeted model of the noise that captures the fact that \u2016\u03a6Ax\u2016 is a random variable while bTAx is not. That is, the uncertainty is restricted to the data matrix P alone since the partial compression does not introduce any noise in the right-hand side.\nWithout compression, it is worth noting that applying robust optimization techniques to the ordinary least-squares problem yields the same solution as applying ridge regression with a datadependent parameter [8]. As we will show, this is not the case in our setting, as robust partiallycompressed least-squares does not reduce to ridge regression. Empirically, we have also seen that robust partially-compressed least-squares is more likely to yield better results than ridge regression and has more intuition built behind it.\nAll of the above, motivated us to focus more on our RPC (6) model and to derive a corresponding efficient solution algorithm. In what follows, we show that RPC (6) can be formulated as a secondorder conic program (SOCP) that can be solved via off-the-shelf solvers such as CPLEX, and in Section 3, we propose an alternative way to solving RPC and derive an efficient solution algorithm based on a reduction to one-dimensional search."}, {"heading": "2.1 SOCP Formulation of RPC", "text": "While the inner optimization in (6) is a non-convex optimization problem, we show in the following lemma that there exists a closed-form solution.\nLemma 1. The inner maximization in (6) can be reformulated for any x as:\nmax \u2016\u2206P\u2016\u2264\u03c1\n\u2016(P + \u2206P )x\u20162 = (\u2016Px\u2016+ \u03c1\u2016x\u2016)2 . (7)"}, {"heading": "In addition, the maximal value is achieved for \u2206P = \u03c1\u2016Px\u2016\u2016x\u2016Pxx", "text": "T.\nProof. The objective function can be upper-bounded using the triangle inequality:\nmax \u2016\u2206P\u2016\u2264\u03c1 \u2016(P + \u2206P )x\u20162 \u2264 max \u2016\u2206P\u2016\u2264\u03c1 (\u2016Px\u2016+ \u2016\u2206Px\u2016)2\n\u2264 (\u2016Px\u2016+ \u03c1\u2016x\u2016)2 .\nTo show that this bound is tight, consider \u2206P = \u03c1\u2016Px\u2016\u2016x\u2016Pxx T. It can be readily seen that \u2016\u2206P\u2016F = \u03c1. Then by algebraic manipulation:\nmax \u2016\u2206P\u2016\u2264\u03c1\n\u2016(P + \u2206P )x\u20162 \u2265 \u2016(P + \u2206P )x\u20162\n= (\u2016Px\u2016+ \u03c1\u2016x\u2016)2 .\nUsing Lemma 1, the robust partially-compressed estimator xRPC is the optimal solution to:\nmin x\n1 2 (\u2016Px\u2016+ \u03c1\u2016x\u2016)2 \u2212 bTAx . (8)\nWe now analyze the structure of the optimal solution and point to connections and differences in comparison to results from ridge regression.\nTheorem 1. The optimal solution xRPC to (8) must satisfy:\nxRPC = 1\n\u03b1+ \u03c1 \u03b2 (\u03b1\u22121 PT P + \u03c1 \u03b2\u22121 I)\u22121AT b , (9)\nsuch that \u03b1 = \u2016PxRPC\u2016 and \u03b2 = \u2016xRPC\u2016, or xRPC = 0 if ATb = 0.\nProof. The theorem follows the first-order optimality conditions. The function (8) is everywhere convex, and differentiable everywhere except at x = 0. We can show the solution x = 0 is only optimal if ATb = 0. The objective at x = 0 is 0. If ATb 6= 0, then for sufficiently small t > 0, the point tATb gives a strictly negative objective (since t2 = o(t) as t\u2192 0), hence x = 0 is not optimal. If x 6= 0, the following first-order conditions are necessary and sufficient:\n0 = (\u2016Px\u2016+ \u03c1 \u2016x\u2016) ( PTPx\n\u2016Px\u2016 + \u03c1\nx\n\u2016x\u2016\n) \u2212ATb,\nfrom where we derive (9). The theorem follows directly from setting \u03b1 and \u03b2 to the required values.\nTheorem 1 shows that the optimal solution to the robust partially-compressed least-squares problem is structurally similar to a ridge regression solution. The two main differences are that there are two parameters, \u03b1 and \u03b2, and these parameters are data-dependent. When setting \u03c1 to 1\u2014which is what we have done in our empirical study, one advantage over ridge regression would be that there is no need to fine-tune the regularization parameter, \u00b5, and one can rely on only data-driven parameters \u03b1 and \u03b2. Even when there is a need to fine-tune the free parameter \u03c1 in RPC\u2014which we have not done in our results and simply sat \u03c1 to be equal to 1\u2014\u03c1 has a structural meaning associated with it; \u03c1 is the size of the uncertainty set in (6) and (7) and one can quickly build an intuition behind how to set its value, which is not the case for the regularization parameter \u00b5. In a current investigation, which is out of the scope of this paper, we are building connections between \u03c1 and the compression dimension m, which will enable us to appropriately set \u03c1 as a function of m.\nNote that Theorem 1 does not provide a method to calculate xRPC, since \u03b1 and \u03b2 depend on xRPC. However, given that (8) is a convex optimization problem, we are able to reformulate it as the following second-order conic program (SOCP) in standard form:\nmin x,t,u,z\n1 2 z \u2212 bTAx s.t. \u2016Px\u2016 \u2264 t , \u03c1 \u2016x\u2016 \u2264 u , \u2225\u2225\u2225\u2225t+ uz \u2212 14 \u2225\u2225\u2225\u2225 \u2264 z + 14 . (10) The last constraint in this program translates to z \u2265 (t+ u)2.\nWhile efficient polynomial-time algorithms exists for solving SOCP problems, they are typically significantly slower than solving least-squares. Therefore, to achieve practical speedup, we need to derive a more efficient algorithm. In fact we propose a reduction to a one-dimensional optimization problem in Section 3."}, {"heading": "3 Efficient Computation of RPC", "text": "As Section 2 shows, the RPC optimization (8) represents a convex second-order conic problem. However, simply using convex or SOCP solvers is too slow when applied to large problems. In this section, we describe a faster approach based on a reduction to a one-dimensional search problem.\nAlgorithm 1: Efficient Algorithm for Solving RPC\nInput: A, b, \u03a6, P = \u03a6A, \u03c1 Output: x\n1 U \u03a3V T \u2190 SVD(P ); 2 \u03c4 \u2190 \u03c1 \u2016b\u20162/2 ; // Initialization\n// Solve x\u2190 argminx h\u03c4k (x) 3 while | \u2016\u03a3y\u2016\u03b3k \u2212 1| \u2264 do 4 \u03b3k \u2190 arg min\u03b3 \u03c6(\u03b3) = \u2211N i=1\nb\u03042i\n(\u03b3\u03c32i+\u03c1) 2 \u2212 1 yk \u2190 1\u03c4 V T(PTP + \u03b3kI) \u22121ATb ;\n// When \u03c4 = \u03c4? then \u03b1 = \u2016\u03a3y\u2016 5 \u03c4k+1 \u2190 \u03c4k\u2016\u03a3yk\u2016 \u03b3k ;\n// Recover the solution\n6 \u03b1\u2190 \u03c4 1+\u03c1\u03b3? ; // Using: \u03b1+ \u03c1 \u03b2 = \u03c4 7 \u03b2 \u2190 \u03c4\u2212\u03b1 \u03c1 ; 8 x\u2190 1 \u03b2 V y ;\nFirst, re-formulate the optimization problem (8) as:\nmin x,t\n1 2 t2 \u2212 bTAx s.t. \u2016Px\u2016+ \u03c1\u2016x\u2016 \u2264 t (11)\nOur goal is to derive and then solve the dual problem. The Lagrangian of (11) is\nL(x, t, \u03c4) = 1 2 t2 \u2212 bTAx+ \u03c4 (\u2016Px\u2016+ \u03c1\u2016x\u2016 \u2212 t)\nSince strong duality conditions hold, we solve the one-dimensional dual maximization problem max\u03c4\u22650 g(\u03c4) where g(\u03c4) is given as\nmin t\n( 1\n2 t2 \u2212 \u03c4t\n) + min\nx \u03c4 (\u2016Px\u2016+ \u03c1\u2016x\u2016)\u2212 bTAx\n= \u22121 2 \u03c42 + min x \u03c4 (\u2016Px\u2016+ \u03c1\u2016x\u2016)\u2212 bTAx\ufe38 \ufe37\ufe37 \ufe38\nh\u03c4 (x)\n. (12)\nThe second equality follows since \u2016Px\u2016 + \u03c1\u2016x\u2016 = t = \u03c4 for the optimal primal and dual solution. Observe that h\u03c4 (x) is positive homogeneous in x and therefore:\nmin x h\u03c4 (x) =  \u2212\u221e \u03c4 < \u03c4? (Case 1) 0 \u03c4 = \u03c4? (Case 2)\n0 \u03c4 > \u03c4? (Case 3)\n(13)\nwhere \u03c4? \u2265 0 is the optimal dual value.\nIntuitively, to solve for the optimal solution, we need to find the maximal value of \u03c4 such that h\u03c4 (x) = 0. Appendix A derives the approach that is summarized in Algorithm 1. Observe that the function h\u03c4 (x) is convex. The main idea is to reduce the optimization to a single-dimensional\nminimization and solve it using Newton method. We also use the SVD decomposition of P to make the search more efficient so that only a single O(N3) step is needed.\nIn terms of the computational complexity, Algorithm 1 requires O(mN2 +N3) operations to compute the singular value decomposition and to multiply PTP . All operations inside of the loop are dominated by O(N3). The number of iteration that is needed depends on the desired precision. Table 2 compares the asymptotic computational complexity of the proposed robust partial compression with the complexity of computing the full least-squares solution."}, {"heading": "4 Approximation Error Bounds", "text": "Compressing the least-squares problem can significantly speed up the computation, but it is also important to analyze the quality of the solution of the compressed solution. Such analysis is known for the fully compressed least-squares problem (e.g. [11]) and in this section, we derive bounds for the partially-compressed least-squares regression.\nFirst, the following simple analysis elucidates the relative trade-offs in computing full or partial projection solutions. Let x? be the solution to the full least-squares problem (3) and z? = b\u2212Ax? be the residual. Recall that ATz? = 0. Now when xCLS is the solution to (2), then:\nxCLS = (A T\u03a6T\u03a6A)\u22121AT\u03a6T\u03a6b\n= x? + (AT\u03a6T\u03a6A)\u22121AT\u03a6T\u03a6z?\nOn the other hand, the solution xPCLS to (4) satisfies:\nxPCLS = (A T\u03a6T\u03a6A)\u22121ATb = (AT\u03a6T\u03a6A)\u22121ATAx?\nThe error in xCLS is additive and is a function of the remainder z ?. The error in xPCLS is, on the other hand, multiplicative and is independent of z?. As a result, a small z? will favor the standard fully compressed least-squares formulation, and a large z? will favor the new partial compressed one.\nWe will now show that, in the sense of the following definition, the residual of the optimal solution of the partial projection problem is close to the residual true solution of the least-squares problem.\nDefinition 1 ( -optimal solution). We say that a solution x\u0302 is -optimal if it satisfies\n\u2016A(x\u0302\u2212 xLS)\u2016 \u2016AxLS\u2016 \u2264 , \u2208 (0, 1) (14)\nwhere xLS is an optimal solution of the original high-dimensional system (1).\nFor sub-Gaussian and ROS sketches, we can show that results in [11] can be extended to bound approximation errors for partially-compressed least-squares based on the definition of -optimal above. These results are nearly independent of the number of rows M in the data matrix (except for how these affect \u2016AxLS\u2016). The main guarantees for unconstrained least-squares are given in the following theorem that provides an exponential tail bound:\nTheorem 2 (Approximation Guarantee). Given a normalized sketching matrix \u03a6 \u2208 Rm\u00d7M , and universal constants c0, c \u2032 0, c1, c2, the sketched solution xPCLS (4) is -optimal (14) with probability at least 1\u2212 c1 exp(\u2212c2m 2), for any tolerance parameter \u2208 (0, 1), when the sketch or compression size m is bounded below by\n(i) m > c0 rank(A) 2 , if \u03a6 is a scaled sub-Gaussian sketch\n(ii) m > c\u20320 rank(A) 2 log 4(N), if \u03a6 is a scaled randomized orthogonal systems (ROS) sketch\nSee Appendix B for the proof.\nBy \u201cscaled\u201d sketch, we mean E(\u03a6T\u03a6) = I, since for partial compression, scaling \u03a6 does affect the answer, unlike full compression. For example, in the Gaussian case, we draw the entries of \u03a6 from N (0, 1m ) instead of N (0, 1)."}, {"heading": "5 Empirical Results", "text": "Our focus in this section is on the improvement of the solution error in comparison with the noncompressed least squares solution and the improvement over regular full compression. We also investigate the computational speed of the algorithms and show that partial compression is just as fast as full compression (and hence sometimes faster than standard least-squares), and that robust partial compression is only roughly twice as slow (and asymptotically it is the same cost).\nTable 3 summarizes the methods that we consider for the empirical evaluation. All methods were implemented in MATLAB with some of the random projection code implemented in C and multithreaded using the pthread library; all experiments were run on the same computer. For solving ordinary least squares, we consider two methods: 1) directly use Matlab\u2019s least-square solver A\\b (which is based on LAPACK), and 2) solving the normal equations: (ATA)\\b. The latter method is significantly faster but less numerically stable for ill-conditioned matrices.\nFor completeness, we compare with (ridge-)regularized and robustified versions of the standard compressed LS problem (2). The robust version is solved following the algorithm outlined in [8] since this can be treated as a robust ordinary least squares problem.\nWe first investigate accuracy using a data set from the National Health Interview Survey from 1992, containing 44085 rows and only 9 columns; since it is highly overcomplete and contains potentially sensitive data, it is a good candidate for sketching. To test over this, we do 100 realizations of 5000 randomly chosen training data and 10000 testing data, and for each realization draw 50 random Walsh-Hadamard sketches with m = 10N . The residual on the testing data (median over all 5000 realizations) is shown in Fig. 1. For robust variants, we set \u00b5 to be 5 times the minimum eigenvalue of AT\u03a6T\u03a6A, and for robust variants we set \u03c1 = 1.\nThe figure presents the results similar to a CDF or a \u201cperformance profile\u201d as used in benchmarking software; a smaller area above the curve indicates better performance. A point such as (0.5, 1.02) means that on half the simulations, the method achieved a residual within a factor of 1.02 of the least-square residual.\nThere are two clear observations from the figure: partial compression gives lower residuals than full compression, and the regularized and robust variants may do slightly worse in the lower-left (i.e., more bias) but better in the worst-case upper-right (i.e., less variance). Put another way, the robust and regularized versions have stronger tail bounds than the standard versions. We also see a slight benefit of robustness over regularization, though the effect depends on how \u00b5 and \u03c1 are chosen.\nThe next dataset represents a set of synthetically generated matrices used in [1]. The authors consider three distinct types of matrix based on their coherence: 1) incoherent, 2) semi-coherent, and 3) coherent. Figure 3 shows the results on a 30000\u00d7750 incoherent matrix for all three types of matrix compression. The data matrix has a condition number 104. To compare multiple methods, we use the same compression matrix to evaluate each methods. The accuracy of a solution x\u0302 is defined with respect to the residual of xLS, namely (\u2016Ax\u0302\u2212 b\u2016/\u2016AxLS \u2212 b\u2016)\u2212 1.\nThe results in Figure 3 illustrate that the partial compression significantly improves on the residual of the solution in comparison with using full compression. In terms of the timing results, the\ncomputation time is dominated by the random projection. As a result, only the fastest projection based on the count sketch significantly outperforms ordinary least squares. Figure 4 shows the same results for a larger matrix with dimensions 40000 \u00d7 2000. Now that the dimensions are larger, the Walsh-Hadamard sketch is also faster than ordinary least-squares. Overall the results show that the partial compression method can significantly improve on the accuracy in comparison with full compression while significantly reducing the computation time with respect to ordinary least-squares.\nThe timing results on the semi-coherent matrix from [1] are essentially the same as for the incoherent one, hence we do not show them. However, Figure 5 shows that partial compression combined with the count sketch has a significantly greater error. This is not unexpected since the semi-incoherent matrix is sparse (and in fact parts of it are nearly diagonal) and the count sketch will preserve most of this sparsity, though we lack a theory to explain it precisely. Note that our approximation guarantees were only shown for the Walsh-Hadamard and Gaussian sketch. The semi-coherence of the matrix does not affect Walsh-Hadamard and Gaussian sketches, in agreement with our theorem.\nFigure 6 shows the breakdown of timing for the individual parts of each of the algorithms that we consider. The compression method used the counting sketch in all compressed methods with the exception of blendenpik (mex) which used the Walsh-Hadamard random matrix vis the sprial WHT package, and both blendenpik versions are set to use low-accuracy for the LSQR step. The matrix A is 5 \u00b7 104 \u00d7 500 random matrix with condition number 106.\nFinally, Fig. 2 investigates the regime when m is very small. The partial and fully compressed solutions greatly deteriorate as m\u2192 N , whereas the robust and regularized versions still give more useful solutions."}, {"heading": "6 Conclusion", "text": "We introduced partially-compressed and robust partially-compressed least-squares linear regression models. The models reduce the error introduced by random projection, or sketching, while retaining the computational improvements of matrix compression. The robust model specifically captures the uncertainty introduced by the partial compression, unlike ordinary ridge regression with the partially compressed model. Our experimental results indicate that the robust partial compression model out-performs both partially-compressed model (with or without regularization) as well as the fully compressed model. Partial compression alone can also significantly improve the solution quality over full compression.\nWhile the partially-compressed least-squares retains the same computational complexity as full compression, the robust approach introduces an additional difficulty in solving the convex opti-\nmization problem. By introducing an algorithm based on one-dimensional parameter search, even the robust partially-compressed least-squares can be faster than ordinary least-squares."}, {"heading": "A Minimization of h\u03c4 in Algorithm 1", "text": "In this section, we describe how to efficiently minimize h\u03c4 (x). Recall that h\u03c4 is defined in (12). Now consider the problem of computing argminh\u03c4 (x) for a fixed \u03c4 . In case 2 of (13), there exists a solution x 6= 0 and therefore the function is differentiable and the optimality conditions read\n\u03c4\n( PTP\n\u03b1 + \u03c1\nI\n\u03b2\n) x = ATb, \u03b1 = \u2016Px\u2016, \u03b2 = \u2016x\u2016 . (15)\nThe optimality conditions are scale invariant for x 6= 0 and therefore we can construct a solution such that \u03b2 = \u2016x\u2016 = 1.\nLet V DV T = PTP be an eigenvalue decomposition of PTP , i.e., Dii = di = \u03c3 2 i are the squared singular values of P , and V are the right singular vectors of P = U\u03a3V T. We make the changeof-variables to y = V Tx (hence \u2016y\u2016 = \u2016x\u2016) and define b = V TATb, which gives an equation for y which is separable if \u03b1 is fixed. We thus need to solve\n\u03c4(\u03b3D + \u03c1I)y = b (16)\n1 = \u03b2 = \u2016y\u2016 (17) 1/\u03b3 = \u03b1 = \u2016\u03a3y\u2016 (18)\nSince di \u2265 0 the solution of (16) is unique for a given \u03b3. Therefore, the equations (16)-(18) are satisfied if and only if there exists a \u03b3 such that the solution to (16) satisfies both (17) and (18).\nWe use Newton\u2019s method to compute \u03b3 that satisfies (16). Define\n\u03c6(\u03b3) = \u03c4\u22122 N\u2211 i=1\nb 2\ni\n(\u03b3\u03c32i + \u03c1) 2 \u2212 1\nso (16) and (17) are satisfied if \u03c6(\u03b3) = 0 for \u03b3 \u2265 0. We note that\n\u03c6\u2032(\u03b3) = \u22122\u03c4\u22122 N\u2211 i=1\n\u03c32i b 2 i\n(\u03b3\u03c32i + \u03c1) 3\nwhich is always negative when \u03b3 \u2265 0, hence \u03c6 is monotonic and we are guaranteed that there is a unique root (i.e., it is analogous to convex minimization) in the region \u03b3 \u2265 0. We can apply any variant of safe-guarded Newton style methods to solve for the root.\nLet x = V Ty for y the optimal solution of the Newton method optimization. We now check if (18) is satisfied for this particular value of \u03b3 \u2261 \u03b1\u22121 to determine which case of (13) we are in. If (18) is satisfied and h\u03c4 (x) = 0 that means that we are in Case 2 and \u03c4 = \u03c4\n?. That is, the complementary slackness conditions are satisfied and the minimum of h\u03c4 (x) is 0. If, on the other hand, h\u03c4 (x) < 0 then we are in Case 1 and scaling x yields an arbitrarily small value. Finally, if y does not satisfy (18), then the optimal solution is y = 0 and we are in Case 3. Note that h\u03c4 (x) is not differentiable at x = 0.\nFinally, if we are in Case 2, then x is a scaled optimal solution. To recover the optimal solution, we use t = \u03c4 to appropriately scale x. Specifically, since we took \u03b2 = 1 and worked with \u03b3 \u2261 \u03b1\u22121, this was equivalent to working with \u03b3 = \u03b2/\u03b1 so we can recover the properly scaled \u03b2? = \u03b1?\u03b3 and hence \u03b1? = (1 + \u03c1\u03b3)\u22121\u03c4?."}, {"heading": "B Proof of Theorem 2", "text": "Proof. The proof uses the stochastic arguments of [11] directly, and modifies their deterministic argument (Lemma 1). For brevity, write x\u0302 = xPCLS and x\n? = xLS. From the optimality of x\u0302 to the partial-compressed least squares problem (3), we have:\n\u2016\u03a6Ax\u0302\u20162 \u2264 \u2016\u03a6Ax\u20162 + 2\u3008A (x\u0302\u2212 x), b\u3009. (19)\nfor all x, and in particular x = x?. From the optimality of x? to equation (1), the gradient at x? is zero so we have \u3008Ax,Ax? \u2212 b\u3009 = 0 for any x, and hence, using x = x\u0302\u2212 x?, re-arranging this gives\n\u3008A(x\u0302\u2212 x?), b\u3009 = \u3008A(x\u0302\u2212 x?), Ax?\u3009 (20)\nThus\n1 2 \u2016\u03a6A (x\u0302\u2212 x?)\u20162\n= 1 2 \u2016\u03a6Ax\u0302\u20162 + 1 2 \u2016\u03a6Ax?\u20162 \u2212 \u3008\u03a6Ax\u0302,\u03a6Ax?\u3009 \u2264 \u2016\u03a6Ax?\u20162 + \u3008A (x\u0302\u2212 x?), b\u3009 \u2212 \u3008\u03a6Ax\u0302,\u03a6Ax?\u3009 = \u3008A (x\u0302\u2212 x?), b\u3009 \u2212 \u3008\u03a6A (x\u0302\u2212 x?),\u03a6Ax?\u3009 = \u3008A (x\u0302\u2212 x?), (I \u2212 \u03a6T\u03a6)Ax?\u3009\nwhere the first inequality follows from (19) and the final equality follows from (20). Normalizing both sides of the last inequality appropriately, we obtain:\n1\n2\n\u2016\u03a6A (x\u0302\u2212 x?)\u20162 \u2016A (x\u0302\u2212 x?)\u20162\ufe38 \ufe37\ufe37 \ufe38 U1 \u2016A (x\u0302\u2212 x?)\u2016\n\u2264 \u2016Ax?\u2016 \u2329\nA (x\u0302\u2212 x?) \u2016A (x\u0302\u2212 x?)\u2016 , (I \u2212 \u03a6T\u03a6) Ax ? \u2016Ax?\u2016 \u232a \ufe38 \ufe37\ufe37 \ufe38\nU2\nTo complete the proof, we need to show that 2 U2U1 is bounded above by \u2208 (0, 1) for both the sub-Gaussian sketch and the ROS sketch. Define Z1(A) = infv\u2208range(A), \u2016v\u2016=1 \u2016\u03a6v\u20162 and\nZ2(A) = sup v\u2208range(A), \u2016v\u2016=1 \u2223\u2223\u2329u, (\u03a6T\u03a6\u2212 I) v\u232a\u2223\u2223 , where u is any fixed vector of norm 1. Then U2/U1 \u2264 Z2/Z1.\nTaking the scaling of \u03a6 into account, then Z2/Z1 < if: (a) \u03a6 is a scaled sub-Gaussian sketch and condition (i) of the theorem holds, since we apply Lemmas 2 and 3 of [11]; or (b) \u03a6 is a scaled ROS sketch and condition (ii) of the theorem holds, since we apply Lemmas 4 and 5 of [11]."}], "references": [{"title": "Blendenpik: Supercharging lapack\u2019s least-squares solver", "author": ["Haim Avron", "Petar Maymounkov", "Sivan Toledo"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Robust optimization", "author": ["Aharon Ben-Tal", "Laurent El Ghaoui", "Arkadi Nemirovski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Near-optimal coresets for least-squares regression", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "IEEE transactions on information theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Random projections for k-means clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Petros Drineas"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Experiments with random projection", "author": ["Sanjoy Dasgupta"], "venue": "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Robust solutions to least-squares problems with uncertain data", "author": ["Laurent El Ghaoui", "Herv\u00e9 Lebret"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Random projection for high dimensional data clustering: A cluster ensemble approach", "author": ["Xiaoli Zhang Fern", "Carla E Brodley"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W Mahoney"], "venue": "arXiv preprint arXiv:1104.5557,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Randomized sketches of convex programs with sharp guarantees", "author": ["Mert Pilanci", "Martin J. Wainwright"], "venue": "CoRR, abs/1404.7203,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Clustering by random projections", "author": ["Thierry Urruty", "Chabane Djeraba", "Dan A Simovici"], "venue": "In Advances in Data Mining. Theoretical Aspects and Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "Random projection is a simple and effective dimensionality reduction technique that enables significant speedups in solving large-scale machine learning problems [6, 10].", "startOffset": 162, "endOffset": 169}, {"referenceID": 9, "context": "Random projection is a simple and effective dimensionality reduction technique that enables significant speedups in solving large-scale machine learning problems [6, 10].", "startOffset": 162, "endOffset": 169}, {"referenceID": 10, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 62, "endOffset": 70}, {"referenceID": 3, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 83, "endOffset": 93}, {"referenceID": 8, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 83, "endOffset": 93}, {"referenceID": 11, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 83, "endOffset": 93}, {"referenceID": 6, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 122, "endOffset": 129}, {"referenceID": 10, "context": "It has been successfully used, for example, in classification [11, 13], clustering [4, 9, 12], and least-squares problems [7, 11].", "startOffset": 122, "endOffset": 129}, {"referenceID": 6, "context": "The standard approach to doing so proceeds as follows [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "Then, we solve the fully compressed problem: xCLS def = argmin x 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 (2) Numerous efficient methods for constructing the compression matrix \u03a6 have been developed; surveys are provided in [3, 7, 10].", "startOffset": 199, "endOffset": 209}, {"referenceID": 6, "context": "Then, we solve the fully compressed problem: xCLS def = argmin x 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 (2) Numerous efficient methods for constructing the compression matrix \u03a6 have been developed; surveys are provided in [3, 7, 10].", "startOffset": 199, "endOffset": 209}, {"referenceID": 9, "context": "Then, we solve the fully compressed problem: xCLS def = argmin x 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 (2) Numerous efficient methods for constructing the compression matrix \u03a6 have been developed; surveys are provided in [3, 7, 10].", "startOffset": 199, "endOffset": 209}, {"referenceID": 7, "context": "Described in Section 2, RPC explicitly models errors introduced by compression and is closely related to robust least-squares regression [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "Leveraging robust optimization techniques makes it possible to reduce the solution error without excessively increasing computational complexity and is a data-driven approach that has been widely used in the last two decades [2].", "startOffset": 225, "endOffset": 228}, {"referenceID": 4, "context": ", [5] [8] Partial Compression new: (3) new: (5) new: (6) Full Compression e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [5] [8] Partial Compression new: (3) new: (5) new: (6) Full Compression e.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": ", [7] e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", [5] new (but algo.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Robust optimization [2], however, enables us to do exactly that and allows us to explicity model the error structure.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "Without compression, it is worth noting that applying robust optimization techniques to the ordinary least-squares problem yields the same solution as applying ridge regression with a datadependent parameter [8].", "startOffset": 208, "endOffset": 211}, {"referenceID": 10, "context": "[11]) and in this section, we derive bounds for the partially-compressed least-squares regression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For sub-Gaussian and ROS sketches, we can show that results in [11] can be extended to bound approximation errors for partially-compressed least-squares based on the definition of -optimal above.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": ", full compression) minx 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 Regularized Compressed LS minx 1 2 \u2016\u03a6 (Ax\u2212 b)\u2016 + \u03bc 2 \u2016x\u2016 Robust Compressed LS minx max\u2016[\u2206P,\u2206B]\u2016F\u2264\u03c1 1 2 \u2016(\u03a6A+ \u2206P )x\u2212 (\u03a6 + \u2206B)b)\u2016 Partial Compressed LS minx 1 2 \u2016\u03a6Ax\u2016 \u2212 bAx Regularized Partial Compressed LS minx 1 2 \u2016\u03a6Ax\u2016 \u2212 bAx+ \u03bc 2 \u2016x\u2016 Robust Partial Compressed LS minx max\u2016\u2206P\u2016F\u2264\u03c1 1 2 \u2016(\u03a6A+ \u2206P )x\u2016 \u2212 bAx BLENDENPIK minx 1 2 \u2016Ax\u2212 b\u2016 via preconditioned LSQR [1] Table 3: Methods used in the empirical comparison", "startOffset": 396, "endOffset": 399}, {"referenceID": 7, "context": "The robust version is solved following the algorithm outlined in [8] since this can be treated as a robust ordinary least squares problem.", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "The next dataset represents a set of synthetically generated matrices used in [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "The timing results on the semi-coherent matrix from [1] are essentially the same as for the incoherent one, hence we do not show them.", "startOffset": 52, "endOffset": 55}], "year": 2015, "abstractText": "Randomized matrix compression techniques, such as the Johnson-Lindenstrauss transform, have emerged as an effective and practical way for solving large-scale problems efficiently. With a focus on computational efficiency, however, forsaking solutions quality and accuracy becomes the trade-off. In this paper, we investigate compressed least-squares problems and propose new models and algorithms that address the issue of error and noise introduced by compression. While maintaining computational efficiency, our models provide robust solutions that are more accurate\u2014relative to solutions of uncompressed least-squares\u2014than those of classical compressed variants. We introduce tools from robust optimization together with a form of partial compression to improve the error-time trade-offs of compressed least-squares solvers. We develop an efficient solution algorithm for our Robust Partially-Compressed (RPC) model based on a reduction to a one-dimensional search. We also derive the first approximation error bounds for Partially-Compressed least-squares solutions. Empirical results comparing numerous alternatives suggest that robust and partially compressed solutions are effectively insulated against aggressive randomized transforms.", "creator": "LaTeX with hyperref package"}}}