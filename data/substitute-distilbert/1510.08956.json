{"id": "1510.08956", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Principal Differences Analysis: Interpretable Characterization of Differences between Distributions", "abstract": "we introduce principal differences analysis ( eta ) for approximate differences between high - dimensional distributions. the method operates by measuring the projection that maximizes the wasserstein divergence between the largest univariate populations. relying on the cramer - briggs device, it requires no assumptions about the form of the underlying distributions, nor the nature of their pressure - group differences. a sparse variant of the method is introduced to identify features responsible for such differences. there provide algorithms for both the original minimax formulation as well as enabling semidefinite relaxation. in addition to deriving some convergence results, we illustrate how the approach possible be applied to identify differences between cell populations in the somatosensory phenomenon and disorders partly manifested by single lobe gal - seq. our broader framework extends beyond the specific domains of wasserstein divergence.", "histories": [["v1", "Fri, 30 Oct 2015 03:06:00 GMT  (293kb,D)", "http://arxiv.org/abs/1510.08956v1", "Advances in Neural Information Processing Systems 28 (NIPS 2015)"]], "COMMENTS": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["jonas mueller", "tommi s jaakkola"], "accepted": true, "id": "1510.08956"}, "pdf": {"name": "1510.08956.pdf", "metadata": {"source": "CRF", "title": "Principal Differences Analysis: Interpretable Characterization of Differences between Distributions", "authors": ["Jonas Mueller", "Tommi Jaakkola"], "emails": ["jonasmueller@csail.mit.edu", "tommi@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Understanding differences between populations is a common task across disciplines, from biomedical data analysis to demographic or textual analysis. For example, in biomedical analysis, a set of variables (features) such as genes may be profiled under different conditions (e.g. cell types, disease variants), resulting in two or more populations to compare. The hope of this analysis is to answer whether or not the populations differ and, if so, which variables or relationships contribute most to this difference. In many cases of interest, the comparison may be challenging primarily for three reasons: 1) the number of variables profiled may be large, 2) populations are represented by finite, unpaired, high-dimensional sets of samples, and 3) information may be lacking about the nature of possible differences (exploratory analysis).\nWe will focus on the comparison of two high dimensional populations. Therefore, given two unpaired i.i.d. sets of samples Xpnq \u201c xp1q, . . . , xpnq \u201e PX and Ypmq \u201c yp1q, . . . , ypmq \u201e PY , the goal is to answer the following two questions about the underlying multivariate random variables X,Y P Rd: (Q1) Is PX \u201c PY ? (Q2) If not, what is the minimal subset of features S \u010e t1, . . . , du such that the marginal distributions differ PXS \u2030 PYS while PXSC \u00ab PYSC for the complement? A finer version of (Q2) may additionally be posed which asks how much each feature contributes to the overall difference between the two probability distributions (with respect to the given scale on which the variables are measured).\nMany two-sample analyses have focused on characterizing limited differences such as mean shifts [1, 2]. More general differences beyond the mean of each feature remain of interest, however, including variance/covariance of demographic statistics such as income. It is also undesirable to restrict the analysis to specific parametric differences, especially in exploratory analysis where the nature of the underlying distributions may be unknown. In the univariate case, a number of nonparametric tests of equality of distributions are available with accompanying concentration results [3]. Popular examples of such divergences (also referred to as probability metrics) include: f -divergences\nar X\niv :1\n51 0.\n08 95\n6v 1\n[ st\nat .M\nL ]\n3 0\nO ct\n(Kullback-Leibler, Hellinger, total-variation, etc.), the Kolmogorov distance, or the Wasserstein metric [4]. Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].\nIn this work, we propose the principal differences analysis (PDA) framework which circumvents the curse of dimensionality through explicit reduction back to the univariate case. Given a pre-specified statistical divergence D which measures the difference between univariate probability distributions, PDA seeks to find a projection \u03b2 which maximizesDp\u03b2TX,\u03b2TY q subject to the constraints ||\u03b2||2 \u010f 1, \u03b21 \u011b 0 (to avoid underspecification). This reduction is justified by the Cramer-Wold device, which ensures that PX \u2030 PY if and only if there exists a direction along which the univariate linearly projected distributions differ [9, 10, 11]. Assuming D is a positive definite divergence (meaning it is nonzero between any two distinct univariate distributions), the projection vector produced by PDA can thus capture arbitrary types of differences between high-dimensional PX and PY . Furthermore, the approach can be straightforwardly modified to address (Q2) by introducing a sparsity penalty on \u03b2 and examining the features with nonzero weight in the resulting optimal projection. The resulting comparison pertains to marginal distributions up to the sparsity level. We refer to this approach as sparse differences analysis or SPARDA."}, {"heading": "2 Related Work", "text": "The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1]. We limit our discussion to projection-based methods which, as a family of methods, are closest to our approach. For multivariate two-class data, the most widely adopted methods include (sparse) linear discriminant analysis (LDA) [2] and the logistic lasso [12]. While interpretable, these methods seek specific differences (e.g., covariance-rescaled average differences) or operate under stringent assumptions (e.g., log-linear model). In contrast, SPARDA (with a positive-definite divergence) aims to find features that characterize a priori unspecified differences between general multivariate distributions.\nPerhaps most similar to our general approach is Direction-Projection-Permutation (DiProPerm) procedure of Wei et al. [5], in which the data is first projected along the normal to the separating hyperplane (found using linear SVM, distance weighted discrimination, or the centroid method) followed by a univariate two-sample test on the projected data. The projections could also be chosen at random [1]. In contrast to our approach, the choice of the projection in such methods is not optimized for the test statistics. We note that by restricting the divergence measure in our technique, methods such as the (sparse) linear support vector machine [13] could be viewed as special cases. The divergence in this case would measure the margin between projected univariate distributions. While suitable for finding well-separated projected populations, it may fail to uncover more general differences between possibly multi-modal projected populations."}, {"heading": "3 General Framework for Principal Differences Analysis", "text": "For a given divergence measure D between two univariate random variables, we find the projection p\u03b2 that solves\nmax \u03b2PB,||\u03b2||0\u010fk\nDp\u03b2T pXpnq, \u03b2T pY pmqq (\n(1)\nwhere B :\u201c t\u03b2 P Rd : ||\u03b2||2 \u010f 1, \u03b21 \u011b 0u is the feasible set, ||\u03b2||0 \u010f k is the sparsity constraint, and \u03b2T pXpnq denotes the observed random variable that follows the empirical distribution of n samples of \u03b2TX . Instead of imposing a hard cardinality constraint ||\u03b2||0 \u010f k, we may instead penalize by adding a penalty term1 \u00b4\u03bb||\u03b2||0 or its natural relaxation, the `1 shrinkage used in Lasso [12], sparse LDA [2], and sparse PCA [14, 15]. Sparsity in our setting explicitly restricts the comparison to the marginal distributions over features with non-zero coefficients. We can evaluate the null hypothesis PX \u201c PY (or its sparse variant over marginals) using permutation testing (cf. [5, 16]) with statistic Dpp\u03b2T pXpnq, p\u03b2T pY pmqq.\n1In practice, shrinkage parameter \u03bb (or explicit cardinality constraint k) may be chosen via cross-validation by maximizing the divergence between held-out samples.\nThe divergence D plays a key role in our analysis. If D is defined in terms of density functions as in f -divergence, one can use univariate kernel density estimation to approximate projected pdfs with additional tuning of the bandwidth hyperparameter. For a suitably chosen kernel (e.g. Gaussian), the unregularized PDA objective (without shrinkage) is a smooth function of \u03b2, and thus amenable to the projected gradient method (or its accelerated variants [17, 18]). In contrast, when D is defined over the cdfs along the projected direction \u2013 e.g. the Kolmogorov or Wasserstein distance that we focus on in this paper \u2013 the objective is nondifferentiable due to the discrete jumps in the empirical cdf. We specifically address the combinatorial problem implied by the Wasserstein distance. Moreover, since the divergence assesses general differences between distributions, Equation (1) is typically a non-concave optimization. To this end, we develop a semi-definite relaxation for use with the Wasserstein distance."}, {"heading": "4 PDA using the Wasserstein Distance", "text": "In the remainder of the paper, we focus on the squared L2 Wasserstein distance (a.k.a. Kantorovich, Mallows, Dudley, or earth-mover distance), defined as\nDpX,Y q \u201c min PXY EPXY ||X \u00b4 Y ||2 s.t. pX,Y q \u201e PXY , X \u201e PX , Y \u201e PY (2)\nwhere the minimization is over all joint distributions over pX,Y q with given marginals PX and PY . Intuitively interpreted as the amount of work required to transform one distribution into the other, D provides a natural dissimilarity measure between populations that integrates both the fraction of individuals which are different and the magnitude of these differences. While component analysis based on the Wasserstein distance has been limited to [19], this divergence has been successfully used in many other applications [20]. In the univariate case, (2) may be analytically expressed as the L2 distance between quantile functions. We can thus efficiently compute empirical projected Wasserstein distances by sorting X and Y samples along the projection direction to obtain quantile estimates.\nUsing the Wasserstein distance, the empirical objective in Equation (1) between unpaired sampled populations txp1q, . . . , xpnqu and typ1q, . . . , ypmqu can be shown to be\nmax \u03b2PB\n||\u03b2||0\u010fk\n\"\nmin MPM,\nn \u00ff\ni\u201c1\nm \u00ff j\u201c1 p\u03b2Txpiq \u00b4 \u03b2T ypjqq2Mij * \u201c max \u03b2PB\n||\u03b2||0\u010fk\n\"\nmin MPM\n\u03b2TWM\u03b2\n*\n(3)\nwhere M is the set of all n \u02c6 m nonnegative matching matrices with fixed row sums \u201c 1{n and column sums \u201c 1{m (see [20] for details), WM :\u201c \u0159\ni,jrZij b ZijsMij , and Zij :\u201c xpiq \u00b4 ypjq. If we omitted (fixed) the inner minimization over the matching matrices and set \u03bb \u201c 0, the solution of (3) would be simply the largest eigenvector of WM . Similarly, for the sparse variant without minizing over M , the problem would be solvable as sparse PCA [14, 15, 21]. The actual maxmin problem in (3) is more complex and non-concave with respect to \u03b2. We propose a two-step procedure similar to \u201ctighten after relax\u201d framework used to attain minimax-optimal rates in sparse PCA [21]. First, we first solve a convex relaxation of the problem and subsequently run a steepest ascent method (initialized at the global optimum of the relaxation) to greedily improve the current solution with respect to the original nonconvex problem whenever the relaxation is not tight.\nFinally, we emphasize that PDA (and SPARDA) not only computationally resembles (sparse) PCA, but the latter is actually a special case of the former in the Gaussian, paired-sample-differences setting. This connection is made explicit by considering the two-class problem with paired samples pxpiq, ypiqq where X,Y follow two multivariate Gaussian distributions. Here, the largest principal component of the (uncentered) differences xpiq \u00b4 ypiq is in fact equivalent to the direction which maximizes the projected Wasserstein difference between the distribution of X \u00b4 Y and a delta distribution at 0."}, {"heading": "4.1 Semidefinite Relaxation", "text": "The SPARDA problem may be expressed in terms of d\u02c6 d symmetric matrices B as max B min MPM tr pWMBq\nsubject to trpBq \u201c 1, B \u013e 0, ||B||0 \u010f k2, rankpBq \u201c 1 (4)\nwhere the correspondence between (3) and (4) comes from writingB \u201c \u03b2b\u03b2 (note that any solution of (3) will have unit norm). When k \u201c d, i.e., we impose no sparsity constraint as in PDA, we can relax by simply dropping the rank-constraint. The objective is then a supremum of linear functions of B and the resulting semidefinite problem is concave over a convex set and may be written as:\nmax BPBr min MPM\ntr pWMBq (5)\nwhere Br is the convex set of positive semidefinite d \u02c6 d matrices with trace = 1. If B\u02da P Rd\u02c6d denotes the global optimum of this relaxation and rankpB\u02daq \u201c 1, then the best projection for PDA is simply the dominant eigenvector ofB\u02da and the relaxation is tight. Otherwise, we can truncateB\u02da as in [14], treating the dominant eigenvector as an approximate solution to the original problem (3).\nTo obtain a relaxation for the sparse version where k \u0103 d (SPARDA), we follow [14] closely. BecauseB \u201c \u03b2b\u03b2 implies ||B||0 \u010f k2, we can thus get an equivalent cardinality constrained problem by incorporating this nonconvex constraint into (4). Since trpBq \u201c 1 and ||B||F \u201c ||\u03b2||22 \u201c 1, a convex relaxation of the squared `0 constraint is given by ||B||1 \u010f k. By selecting \u03bb as the optimal Lagrange multiplier for this `1 constraint, we can obtain an equivalent penalized reformulation parameterized by \u03bb rather than k [14]. The sparse semidefinite relaxation is thus the following concave problem\nmax BPBr min MPM\ntr pWMBq \u00b4 \u03bb||B||1 (\n(6)\nWhile the relaxation bears strong resemblance to DSPCA relaxation for sparse PCA, the inner maximization over matchings prevents direct application of general semidefinite programming solvers. Let MpBq denote the matching that minimizes tr pWMBq for a given B. Standard projected subgradient ascent could be applied to solve (6), where at the tth iterate the (matrix-valued) subgradient would be given by WMpBptqq. However, this approach requires us to maintain large n\u02c6m matrices. Instead, we resort to the dual (cf. [22, 23])\nmin MPM\ntr pWMBq \u201c 1\nm max u,v\nn \u00ff\ni\u201c1\nm \u00ff j\u201c1 mint0, trprZij b ZijsBq \u00b4 ui \u00b4 vju ` 1 n n \u00ff i\u201c1 ui ` 1 m m \u00ff j\u201c1 vj\nthat enables us to alternatingly maximize (6) over B P Br, u P Rn, and v P Rm without requiring matching matrices nor their cumbersome row/column constraints. While u and v can be solved in closed form for each fixed B (via sorting), we describe a simple sub-gradient approach that works better in practice.\nRELAX Algorithm: Solves the dualized semidefinite relaxation of SPARDA (6). Returns the largest eigenvector of the solution to (6) as the desired projection direction for SPARDA.\nInput: d-dimensional data xp1q, . . . , xpnq and yp1q, . . . , ypmq (with n \u011b m) Parameters: \u03bb \u011b 0 controls the amount of regularization, \u03b3 \u0105 0 is the step-size used for B updates, \u03b7 \u0105 0 is the step-size used for updates of dual variables u and v, T is the maximum number of iterations without improvement in cost after which algorithm terminates.\n1: Initialize \u03b2p0q \u00d0 \u201d?\nd d , . . . , ? d d \u0131\n, Bp0q \u00d0 \u03b2p0q b \u03b2p0q P Br, up0q \u00d0 0n\u02c61, vp0q \u00d0 0m\u02c61 2: While the number of iterations since last improvement in objective function is less than T : 3: Bu\u00d0 r1{n, . . . , 1{ns P Rn, Bv \u00d0 r1{m, . . . , 1{ms P Rm, BB \u00d0 0d\u02c6d 4: For i, j P t1, . . . , nu \u02c6 t1, . . . ,mu: 5: Zij \u00d0 xpiq \u00b4 ypjq\n6: If trprZij b ZijsBptqq \u00b4 uptqi \u00b4 v ptq j \u0103 0 :\n7: Bui \u00d0 Bui \u00b4 1{m , Bvj \u00d0 Bvj \u00b4 1{m , BB \u00d0 BB ` Zij b Zij {m 8: End For 9: upt`1q \u00d0 uptq ` \u03b7 \u00a8 Bu and vpt`1q \u00d0 vptq ` \u03b7 \u00a8 Bv\n10: Bpt`1q \u00d0 Projection \u00b4 Bptq ` \u03b3||BB||F \u00a8 BB ; \u03bb, \u03b3{||BB||F \u00af Output: p\u03b2relax P Rd defined as the largest eigenvector (based on corresponding eigenvalue\u2019s magnitude) of the matrix Bpt \u02daq which attained the best objective value over all iterations.\nProjection Algorithm: Projects matrix onto positive semidefinite cone of unit-trace matrices Br (the feasible set in our relaxation). Step 4 applies soft-thresholding proximal operator for sparsity.\nInput: B P Rd\u02c6d Parameters: \u03bb \u011b 0 controls the amount of regularization, \u03b4 \u201c \u03b3{||BB||F \u011b 0 is the actual step-size used in the B-update. 1: Q\u039bQT \u00d0 eigendecomposition of B\n2: w\u02da \u00d0 arg min ||w \u00b4 diagp\u039bq||22 : w P r0, 1sd, ||w||1 \u201c 1 (\n(Quadratic program)\n3: rB \u00d0 Q \u00a8 diagtw\u02da1 , . . . , w\u02dad u \u00a8QT\n4: If \u03bb \u0105 0: For r, s P t1, . . . , du2 : rBr,s \u00d0 signp rBr,sq \u00a8maxt0, | rBr,s| \u00b4 \u03b4\u03bbu Output: rB P Br\nThe RELAX algorithm (boxed) is a projected subgradient method with supergradients computed in Steps 3 - 8. For scaling to large samples, one may alternatively employ incremental supergradient directions [24] where Step 4 would be replaced by drawing random pi, jq pairs. After each subgradient step, projection back into the feasible set Br is done via a quadratic program involving the current solution\u2019s eigenvalues. In SPARDA, sparsity is encouraged via the soft-thresholding proximal map corresponding to the `1 penalty. The overall form of our iterations matches subgradient-proximal updates (4.14)-(4.15) in [24]. By the convergence analysis in \u00a74.2 of [24], the RELAX algorithm (as well as its incremental variant) is guaranteed to approach the optimal solution of the dual which also solves (6), provided we employ sufficiently large T and small step-sizes. In practice, fast and accurate convergence is attained by: (a) renormalizing the B-subgradient (Step 10) to ensure balanced updates of the unit-norm constrained B, (b) using diminishing learning rates which are initially set larger for the unconstrained dual variables (or even taking multiple subgradient steps in the dual variables per each update of B)."}, {"heading": "4.2 Tightening after relaxation", "text": "It is unreasonable to expect that our semidefinite relaxation is always tight. Therefore, we can sometimes further refine the projection p\u03b2relax obtained by the RELAX algorithm by using it as a starting point in the original non-convex optimization. We introduce a sparsity constrained tightening procedure for applying projected gradient ascent for the original nonconvex objective Jp\u03b2q \u201c minMPM \u03b2TWM\u03b2 where \u03b2 is now forced to lie in BXSk and Sk :\u201c t\u03b2 P Rd : ||\u03b2||0 \u010f ku. The sparsity level k is fixed based on the relaxed solution (k \u201c ||p\u03b2relax||0). After initializing \u03b2p0q \u201c p\u03b2relax P Rd, the tightening procedure iterates steps in the gradient direction of J followed by straightforward projections into the unit half-ball B and the set Sk (accomplished by greedily truncating all entries of \u03b2 to zero besides the largest k in magnitude).\nLet Mp\u03b2q again denote the matching matrix chosen in response to \u03b2. J fails to be differentiable at the r\u03b2 where Mpr\u03b2q is not unique. This occurs, e.g., if two samples have identical projections under r\u03b2. While this situation becomes increasingly likely as n,m\u00d18, J interestingly becomes smoother overall (assuming the distributions admit density functions). For all other \u03b2: Mp\u03b21q \u201cMp\u03b2q where \u03b21 lies in a small neighborhood around \u03b2 and J admits a well-defined gradient 2WMp\u03b2q\u03b2. In practice, we find that the tightening always approaches a local optimum of J with a diminishing stepsize. We note that, for a given projection, we can efficiently calculate gradients without recourse to matrices Mp\u03b2q or WMp\u03b2q by sorting \u03b2ptq T xp1q, . . . , \u03b2ptq T xpnq and \u03b2ptq T yp1q, . . . , \u03b2ptq T ypmq. The gradient is directly derivable from expression (3) where the nonzeroMij are determined by appropriately matching empirical quantiles (represented by sorted indices) since the univariate Wasserstein distance is simply the L2 distance between quantile functions [20]. Additional computation can be saved by employing insertion sort which runs in nearly linear time for almost sorted points (in iteration t\u00b4 1, the points have been sorted along the \u03b2pt\u00b41q direction and their sorting in direction \u03b2ptq is likely similar under small step-size). Thus the tightening procedure is much more efficient than the RELAX algorithm (respective runtimes are Opdn log nq vs. Opd3n2q per iteration).\nWe require the combined steps for good performance. The projection found by the tightening algorithm heavily depends on the starting point \u03b2p0q, finding only the closest local optimum (as in Figure 1a). It is thus important that \u03b2p0q is already a good solution, as can be produced by our RELAX algorithm. Additionally, we note that as first-order methods, both the RELAX and tightening algorithms are amendable to a number of (sub)gradient-acceleration schemes (e.g. momentum techniques, adaptive learning rates, or FISTA and other variants of Nesterov\u2019s method [18, 17, 25])."}, {"heading": "4.3 Properties of semidefinite relaxation", "text": "We conclude the algorithmic discussion by highlighting basic conditions under which our PDA relaxation is tight. Assuming n,m \u00d1 8, each of (i)-(iii) implies that the B\u02da which maximizes (5) is nearly rank one, or equivalently B\u02da \u00ab r\u03b2 b r\u03b2 (see Supplementary Information \u00a7S4 for intuition). Thus, the tightening procedure initialized at r\u03b2 will produce a global maximum of the PDA objective.\n(i) There exists direction in which the projected Wasserstein distance between X and Y is nearly as large as the overall Wasserstein distance in Rd. This occurs for example if ||ErXs \u00b4 ErY s||2 is large while both ||CovpXq||F and ||CovpY q||F are small (the distributions need not be Gaussian).\n(ii) X \u201e Np\u00b5X ,\u03a3Xq and Y \u201e Np\u00b5Y ,\u03a3Y q with \u00b5X \u2030 \u00b5Y and \u03a3X \u00ab \u03a3Y . (iii) X \u201e Np\u00b5X ,\u03a3Xq and Y \u201e Np\u00b5Y ,\u03a3Y q with \u00b5X \u201c \u00b5Y where the underlying covariance\nstructure is such that arg maxBPBr ||pB1{2\u03a3XB1{2q1{2 \u00b4 pB1{2\u03a3YB1{2q1{2||2F is nearly rank 1. For example, if the primary difference between covariances is a shift in the marginal variance of some features, i.e. \u03a3Y \u00ab V \u00a8 \u03a3X where V is a diagonal matrix."}, {"heading": "5 Theoretical Results", "text": "In this section, we characterize statistical properties of an empirical divergence-maximizing projection p\u03b2 :\u201c arg max\n\u03b2PB Dp\u03b2T pXpnq, \u03b2T pY pnqq, although we note that the algorithms may not succeed\nin finding such a global maximum for severely nonconvex problems. Throughout, D denotes the squared L2 Wasserstein distance between univariate distributions, C represents universal constants that change from line to line. All proofs are relegated to the Supplementary Information \u00a7S3. We make the following simplifying assumptions: (A1) n \u201c m (A2) X,Y admit continuous density functions (A3) X,Y are compactly supported with nonzero density in the Euclidean ball of radius R. Our theory can be generalized beyond (A1)-(A3) to obtain similar (but complex) statements through careful treatment of the distributions\u2019 tails and zero-density regions where cdfs are flat.\nTheorem 1. Suppose there exists direction \u03b2\u02da P B such that Dp\u03b2\u02daTX,\u03b2\u02daTY q \u011b \u2206. Then:\nDpp\u03b2T pXpnq, p\u03b2T pY pnqq \u0105 \u2206\u00b4 with probability greater than 1\u00b4 4 exp \u02c6 \u00b4 n 2\n16R4\n\u02d9\nTheorem 1 gives basic concentration results for the projections used in empirical applications our method. To relate distributional differences between X,Y in the ambient d-dimensional space with their estimated divergence along the univariate linear representation chosen by PDA, we turn to Theorems 2 and 3. Finally, Theorem 4 provides sparsistency guarantees for SPARDA in the case where X,Y exhibit large differences over a certain feature subset (of known cardinality).\nTheorem 2. If X and Y are identically distributed in Rd, then: Dpp\u03b2T pXpnq, p\u03b2T pY pnqq \u0103 with probability greater than\n1\u00b4 C1 \u02c6 1` R 2\n\u02d9d\nexp\n\u02c6\n\u00b4C2 R4 n 2 \u02d9\nTo measure the difference between the untransformed random variables X,Y P Rd, we define the following metric between distributions on Rd which is parameterized by a \u011b 0 (cf. [11]):\nTapX,Y q :\u201c |Prp|X1| \u010f a, . . . , |Xd| \u010f aq \u00b4 Prp|Y1| \u010f a, . . . , |Yd| \u010f aq| (7)\nIn addition to (A1)-(A3), we assume the following for the next two theorems: (A4) Y has subGaussian tails, meaning cdf FY satisfies: 1 \u00b4 FY pyq \u010f Cy expp\u00b4y\n2{2q, (A5) ErXs \u201c ErY s \u201c 0 (note that mean differences can trivially be captured by linear projections, so these are not the differences of interest in the following theorems), (A6) Var(X`) = 1 for ` \u201c 1, . . . , d Theorem 3. Suppose D a \u011b 0 s.t. TapX,Y q \u0105 h pgp\u2206qq where h pgp\u2206qq :\u201c mint\u22061,\u22062u with\n\u22061 :\u201c pa` dqdpgp\u2206q ` dq ` expp\u00b4a2{2q ` \u03c8 exp \u00b4 \u00b41{p ? 2\u03c8q \u00af\n(8)\n\u22062 :\u201c ` gp\u2206q ` expp\u00b4a2{2q \u02d8 \u00a8 d (9)\n\u03c8 :\u201c ||CovpXq||1, gp\u2206q :\u201c \u22064 \u00a8 p1` \u03a6q\u00b44, and \u03a6 :\u201c sup\u03b1PB supy |f\u03b1TY pyq| ( with f\u03b1TY pyq defined as the density of the projection of Y in the \u03b1 direction. Then: Dpp\u03b2T pXpnq, p\u03b2T pY pnqq \u0105 C\u2206\u00b4 (10) with probability greater than 1\u00b4 C1 exp ` \u00b4C2R4n 2 \u02d8 Theorem 4. Define C as in (10). Suppose there exists feature subset S \u0102 t1, . . . , du s.t. |S| \u201c k, T pXS , YSq \u011b h pg p pd` 1q{Cqq, and remaining marginal distributions XSC , YSC are identical. Then:\np\u03b2pkq :\u201c arg max \u03b2PB tDp\u03b2T pXpnq, \u03b2T pY pnqq : ||\u03b2||0 \u010f ku\nsatisfies p\u03b2pkqi \u2030 0 and p\u03b2 pkq j \u201c 0 @ i P S, j P SC with probability greater than\n1\u00b4 C1 \u02c6 1` R 2\n\u02d9d\u00b4k\nexp\n\u02c6\n\u00b4C2 R4 n 2 \u02d9"}, {"heading": "6 Experiments", "text": "Figure 1a illustrates the cost function of PDA pertaining to two 3-dimensional distributions (see details in Supplementary Information \u00a7S1). In this example, the point of convergence p\u03b2 of the tightening method after random initialization (in green) is significantly inferior to the solution produced by the RELAX algorithm (in red). It is therefore important to use RELAX before tightening as we advise.\nThe synthetic MADELON dataset used in the NIPS 2003 feature selection challenge consists of points (n \u201c m \u201c 1000, d \u201c 500) which have 5 features scattered on the vertices of a fivedimensional hypercube (so that interactions between features must be considered in order to distinguish the two classes), 15 features that are noisy linear combinations of the original five, and 480 useless features [26]. While the focus of the challenge was on extracting features useful to classifiers, we direct our attention toward more interpretable models. Figure 1b demonstrates how well SPARDA (red), the top sparse principal component (black) [27], sparse LDA (green) [2], and the logistic lasso (blue) [12] are able to identify the 20 relevant features over different settings of their respective regularization parameters (which determine the cardinality of the vector returned by each method). The red asterisk indicates the SPARDA result with \u03bb automatically selected via our crossvalidation procedure (without information of the underlying features\u2019 importance), and the black asterisk indicates the best reported result in the challenge [26].\nThe restrictive assumptions in logistic regression and linear discriminant analysis are not satisfied in this complex dataset resulting in poor performance. Despite being class-agnostic, PCA was successfully utilized by numerous challenge participants [26], and we find that the sparse PCA performs on par with logistic regression and LDA. Although the lasso fairly efficiently picks out 5 relevant features, it struggles to identify the rest due to severe multi-colinearity. Similarly, the challengewinning Bayesian SVM with Automatic Relevance Determination [26] only selects 8 of the 20 relevant features. In many applications, the goal is to thoroughly characterize the set of differences rather than select a subset of features that maintains predictive accuracy. SPARDA is better suited for this alternative objective. Many settings of \u03bb return 14 of the relevant features with zero false positives. If \u03bb is chosen automatically through cross-validation, the projection returned by SPARDA contains 46 nonzero elements of which 17 correspond to relevant features.\nFigure 1c depicts (average) p-values produced by SPARDA (red), PDA (purple), the overall Wasserstein distance in Rd (black), Maximum Mean Discrepancy [8] (green), and DiProPerm [5] (blue) in two-sample synthetically controlled problems where PX \u2030 PY and the underlying differences have varying degrees of sparsity. Here, d indicates the overall number of features included of which only the first 3 are relevant (see Supplementary Information \u00a7S1 for details). As we evaluate the significance of each method\u2019s statistic via permutation testing, all the tests are guaranteed to exactly control Type I error [16], and we thus only compare their respective power in determining PX \u2030 PY setting. The figure demonstrates clear superiority of SPARDA which leverages the underlying sparsity to maintain high power even with the increasing overall dimensionality. Even when all the features differ (when d \u201c 3), SPARDA matches the power of methods that consider the full space despite only selecting a single direction (which cannot be based on mean-differences as there are none in this controlled data). This experiment also demonstrate that the unregularized PDA retains greater power than DiProPerm, a similar projection-based method [5].\nRecent technological advances allow complete transcriptome profiling in thousands of individual cells with the goal of fine molecular characterization of cell populations (beyond the crude averagetissue-level expression measure that is currently standard) [28]. We apply SPARDA to expression measurements of 10,305 genes profiled in 1,691 single cells from the somatosensory cortex and 1,314 hippocampus cells sampled from the brains of juvenile mice [29]. The resulting p\u03b2 identifies many previously characterized subtype-specific genes and is in many respects more informative than the results of standard differential expression methods (see Supplementary Information \u00a7S2 for details). Finally, we also apply SPARDA to normalized data with mean-zero & unit-variance marginals in order to explicitly restrict our search to genes whose relationship with other genes\u2019 expression is different between hippocampus and cortex cells. This analysis reveals many genes known to be heavily involved in signaling, regulating important processes, and other forms of functional interaction between genes (see Supplementary Information \u00a7S2.1 for details). These types of important changes cannot be detected by standard differential expression analyses which consider each gene in isolation or require gene-sets to be explicitly identified as features [28]."}, {"heading": "7 Conclusion", "text": "This paper introduces the overall principal differences methodology and demonstrates its numerous practical benefits of this approach. While we focused on algorithms for PDA & SPARDA tailored to the Wasserstein distance, different divergences may be better suited for certain applications.\nFurther theoretical investigation of the SPARDA framework is of interest, particularly in the highdimensional d \u201c Opnq setting. Here, rich theory has been derived for compressed sensing and sparse PCA by leveraging ideas such as restricted isometry or spiked covariance [15]. A natural question is then which analogous properties of PX ,PY theoretically guarantee the strong empirical performance of SPARDA observed in our high-dimensional applications. Finally, we also envision extensions of the methods presented here which employ multiple projections in succession, or adapt the approach to non-pairwise comparison of multiple populations."}, {"heading": "Acknowledgements", "text": "This research was supported by NIH Grant T32HG004947."}, {"heading": "S1 Details of simulation study 10", "text": ""}, {"heading": "S2 Single cell gene expression in cortex vs. hippocampus 11", "text": "S2.1 Identifying genes whose interactions differ between cortex vs. hippocampus cells 11"}, {"heading": "S3 Proofs and Auxiliary Lemmas 15", "text": "S3.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 S3.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 S3.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 S3.4 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "S4 Derivation of semidefinite relaxation properties 17", "text": "List of Figures\nS1 Enriched biological processes in annotations of top SPARDA genes . . . . . . . . 13\nS2 Cellular Snca expression in somatosensory cortex vs. hippocampus . . . . . . . . . 14\nList of Tables\nS1 Top genes found by SPARDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nS2 Top SPARDA genes after marginal normalization . . . . . . . . . . . . . . . . . . 12"}, {"heading": "S1 Details of simulation study", "text": "To generate the cost function depicted in Figure 1a, we draw n \u201c m \u201c 1000 points from mean-zero 3-dimensional Gaussian distributions with the following respective covariance matrices:\n\u03a3X \u201c \u00ab 1 0.2 0.4 0.2 1 0 0.4 0 1 ff\n\u03a3Y \u201c \u00ab 1 \u00b40.9 0 \u00b40.9 1 0\n0 0 1\nff\nDue to the large sample sizes, the empirical distributions accurately represent the underlying populations, and thus the projection produced by the tightening procedure (in green) is significantly inferior to the projection produced by the RELAX algorithm (in red) in terms of actual divergence captured. Note that only dimensions 2 and 3 of the projection-space are plotted in the figure since \u03b21 \u201c b 1 \u00b4 \u0159d `\u201c2 \u03b2 2 ` is fixed for the unit-norm projections of interest.\nNext, we detail the process by which the data are generated for the two-sample problems depicted in Figure 1c. We set the features of the underlying X and Y to mean-zero multivariate Gaussian distributions in blocks of 3, where within each block, (common) covariance parameters are sampled from the Wishart(I3\u02c63) distribution with 3 degrees of freedom. Only for the first block of 3 features do we sample a separate covariance matrix for X and a separate covariance matrix for Y , so all differences between the two distributions lie in the first 3 features. To generate a dataset with d \u201c 3 \u02c6 `, we simply concatenate ` of our blocks together (always including the first block with the underlying difference) and draw n \u201c m \u201c 100 points from each class. We generate 20 datasets by increasing ` (so the largest d \u201c 60), and repeat this entire experiment 10 times reporting the average p-values in Figure 1c.\nEach p-value is obtain by randomly permuting the class labels and recomputing the test statistic 100 times (where we use the same permutations between all datasets). In SPARDA, regularization parameter \u03bb is re-selected using our cross-validation technique in each permutation. The overall Wasserstein distance in the ambient space is computed by solving a transportation problem [20], and we note the similarity between this statistic and the cross-match test [6]. A popular kernel method for testing high-dimensional distribution equality, the mean map discrepancy, is computed using the Gaussian kernel with bandwidth parameter chosen by the \u201cmedian trick\u201d [8] (this is very similar to the energy test of [7]). Finally, we also compute the DiProPerm statistic, employing the the DWD-t variant recommended for testing general equality of distributions [5]."}, {"heading": "S2 Single cell gene expression in cortex vs. hippocampus", "text": "Playing critical roles in the brain, the somatosensory cortex (linked to the senses) and hippocampal region (linked to memory regulation and spatial coding) contain a diversity of cell types [29]. It is thus of great interest to identify how cell populations in these regions diverge in developing brains, a question we address by applying SPARDA to single cell RNA-seq data from these regions. Following [36], we represent gene expression by log-transformed FPKM computed from the sequencing read counts2, so values are directly comparable between genes. Because expression measurements from individual cells are poorer in quality than transcriptome profiles obtained in aggregate across tissue samples (due to a drastically reduced amount of available RNA), it is important to filter out poorly measured genes and we retain a set of 10,305 genes that are measured with sufficient accuracy for informative analysis [36].\nTable S1 and Figure S1 demonstrate that SPARDA discovers many interesting genes which are already known to play important functional roles in these regions of the brain. For comparison, we also run LIMMA, a standard method for differential expression analysis which tests for marginal mean-differences on a gene-by-gene basis [37]. Ordering the significant genes under LIMMA by magnitude of their mean expression difference, we find that 3 of the top 10 genes identified by SPARDA also appear in this top 10 list (Crym, Spink8, Neurod6), demonstrating SPARDA\u2019s implicit attraction toward large first-order differences over more nuanced changes in practice. Because only few genes can feasibly be considered for subsequent experimentation in these studies, a good tool for differential expression analysis must rank the most relevant genes very highly in order for researchers to take note.\nOne particularly relevant gene in this data is Snca, a presynaptic signaling and membrane trafficking gene whose defects are implicated in both Parkinson and Alzheimer\u2019s disease [38, 39]. While Snca is ranked 11th highest under SPARDA, it only ranks 349 according to LIMMA p-values and 95 based on absolute mean-shift. Figure S2 shows that the primary change in Snca expression between the cell types is not a shift in the distributions, but rather the movement of a large fraction of low (1-2.5 log-FPKM) expression cells into the high-expression (\u0105 2.5 log-FPKM) regime. As this type of change does not match the restrictive assumptions of LIMMA\u2019s t-test, the method fails to highly-rank this gene while the Wasserstein distance employed by SPARDA is perfectly suited for measuring this sort of effect.\nS2.1 Identifying genes whose interactions differ between cortex vs. hippocampus cells\nAfter restricting our analysis to only the top 500 genes with largest average expression (since genes playing important roles in interactions must be highly expressed), we normalize each gene\u2019s expression values to have mean zero and unit variance within in the cells of each class. Subsequent application of SPARDA reveals that most of the genes corresponding to the ten greatest values of the resulting p\u03b2 are known to play important roles in in signaling and regulation (see Table S2).\n2available in NCBI\u2019s Gene Expression Omnibus (under accession GSE60361)\nGENE WEIGHT DESCRIPTION\nCck 0.0593 Primary distinguishing gene between distinct interneuron classes identified in the cortex and hippocampus [40]\nNeurod6 0.0583 General regulator of nervous system development whose induced mutation displays different effects in neocortex vs. the hippocampal region [41]\nStmn3 0.0573 Up-expressed in hippocampus of patients with depressive disorders [42] Plp1 0.0570 An oligodendrocyte- and myelin-related gene which exhibits cortical differential expression in schizophrenia [43] Crym 0.0550 Plays a role in neuronal specification [44] Spink8 0.0536 Serine protease inhibitor specific to hippocampal pyramidal cells [29] Gap43 0.0511 Encodes plasticity protein important for axonal regeneration and neural growth Cryab 0.0500 Stress induction leads to reduced expression in the mouse hippocampus [45] Mal 0.0494 Regulates dendritic morphology and is expressed at lower levels in cortex than in hippocampus [46] Tspan13 0.0488 Membrane protein which mediates signal transduction events in\ncell development, activation, growth and motility\nTable S1: Genes with the greatest weight in the projection p\u03b2 produced by SPARDA analysis of the mouse brain single cell RNA-seq data. Where not cited, the description of the genes are taken from the standard ontology annotations.\nGENE WEIGHT DESCRIPTION\nThy1 0.1245 Plays a role in cell-cell & cell-ligand interactions during synaptogenesis and other processes in the brain\nVsnl1 0.1245 Modulates intracellular signaling pathways of the central nervous system Stmn3 0.1222 Stathmins form important protein complex with tubulins\nStmn2 0.1188 Note: Tubulins Tubb3 and Tubb2 are ranked 20th and 25th by weight in p\u03b2 Tmem59 0.1176 Fundamental regulator of neural cell differentiation. Knock out in the hippocampus results in drastic expression changes of many other genes [47] Basp1 0.1171 Transcriptional cofactor which can divert the differentiation of cells to a neuronal-like morphology [48] Snhg1 0.1166 Unclassified non-coding RNA gene Mllt11 0.1145 Promoter of neurodifferentiation and axonal/dendritic maintenance [49] Uchl1 0.1137 Loss of function leads to profound degeneration of motor neurons [50]. Cck 0.1131 Targets pyramidal neurons and enables neocortical plasticity allowing\nfor example the auditory cortex to detect light stimuli [51, 52]\nTable S2: Genes with the greatest weight in the projection p\u03b2 produced by SPARDA analysis of the marginally normalized expression data.\nFigure S1: Biological process terms most significantly enriched in the annotations of the top 100 genes identified by SPARDA.\n0. 0\n0. 5\n1. 0\n1. 5\n2. 0\n2. 5\n3. 0\nS nc\na ex\npr es\nsi on\nCortex Hippocampus\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\u25cf \u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf \u25cf\nFigure S2: Distribution of Snca expression across cells of the somatosensory cortex and hippocampus."}, {"heading": "S3 Proofs and Auxiliary Lemmas", "text": "Throughout this section, we use C to denote absolute constants whose value may change from line to line. F is defined the cdf of a random variable, and the corresponding quantile function is F\u00b41ppq :\u201c inftx : F pxq \u011b pu. Note our assumptions (A1)-(A3) ensure the quantile function equals the unique inverse of any projected cdf. Hat notation is used to represent the empirical versions of all quantities, and recall that D denotes the squared Wasserstein distance.\nS3.1 Proof of Theorem 1\nProof. Since p\u03b2 maximizes the empirical divergence, we have:\nPrpDpp\u03b2T pXpnq, p\u03b2T pY pnqq \u0105 \u2206\u00b4 q\n\u011bPrpDp\u03b2\u02daT pXpnq, \u03b2\u02daT pY pnqq \u0105 \u2206\u00b4 q\n\u011bPrpDp\u03b2\u02daT pXpnq, \u03b2\u02daTXq `Dp\u03b2\u02daT pY pnq, \u03b2\u02daTY q \u010f q\n\u011b1\u00b4 4 exp \u02c6 \u00b4 n 2\n16R4\n\u02d9\napplying Lemma 1 and the union bound.\nLemma 1. For bounded univariate random variable Z P r\u00b4R,Rs with nonzero continuous density in this region, we have\nDp pZpnq, Zq \u0105\nwith probability at most 2 exp \u00b4 \u00b4 n 2\n8R4\n\u00af\nProof. On the real line, the (squared) Wasserstein distance is given by:\nDp pZpnq, Zq \u201c \u017c 1\n0\n\u00b4\npF\u00b41Z ppq \u00b4 F \u00b41 Z ppq\n\u00af2\ndp\n\u201c 4R2 \u017c 1\n0\n\u02dc\npF\u00b41Z ppq \u00b4 F \u00b41 Z ppq\n2R\n\u00b82\ndp where\n\u02c7\n\u02c7\n\u02c7\n\u02c7\n\u02c7\npF\u00b41Z ppq \u00b4 F \u00b41 Z ppq\n2R\n\u02c7\n\u02c7\n\u02c7\n\u02c7\n\u02c7\n\u010f 1 for each p P p0, 1q\n\u010f 4R2 \u017c 1\n0\n\u02c7\n\u02c7\n\u02c7\n\u02c7\n\u02c7\npF\u00b41Z ppq \u00b4 F \u00b41 Z ppq\n2R\n\u02c7\n\u02c7\n\u02c7\n\u02c7\n\u02c7\ndp\n\u201c 2R \u017c 1\n0\n\u02c7\n\u02c7\n\u02c7 pF\u00b41Z ppq \u00b4 F \u00b41 Z ppq\n\u02c7\n\u02c7 \u02c7 dp\n\u201c 2R \u017c 8\n\u00b48\n\u02c7\n\u02c7\n\u02c7 pFZpzq \u00b4 FZpzq \u02c7 \u02c7 \u02c7 dz by the equivalence of the (empirical) quantile function and inverse (empirical) cdf\n\u010f 4R2 \u00a8 sup z\n\u02c7\n\u02c7\n\u02c7 pFZpzq \u00b4 FZpzq \u02c7 \u02c7 \u02c7\n\u010f with probability \u011b 1\u00b4 2 exp \u02c6 \u00b4 n 2\n8R4\n\u02d9\nby the Dvoretzky-Kiefer-Wolfowitz inequality [3].\nS3.2 Proof of Theorem 2\nProof. We first construct a fine grid of points t\u03b11, . . . , \u03b1Su which form an ( {R2)-net covering the surface of the unit ball in Rd. When PX \u201c PY , the Cramer-Wold device [9] implies that for any point in our grid: Dp\u03b1Ts X,\u03b1Ts Y q \u201c 0. A result analogous to Theorem 1 implies Dp\u03b1Ts pXpnq, \u03b1Ts pY pnqq \u0105 with probability \u0103 C1 exp ` \u00b4C2R4n 2 \u02d8 .\nSubsequently, we apply the union bound over the finite set of all grid points. The total number of\npoints under consideration is the covering number of the unit-sphere which is \u00b4 1` 2R 2\n\u00afd\n. Thus,\nthe probability that Dp\u03b1Ts pXpnq, \u03b1Ts pY pnqq \u0103 simultaneously for all points in the grid is at least\nC1\n\u02c6\n1` 2R 2\n\u02d9d\nexp\n\u02c6\n\u00b4C2 R4 n 2 \u02d9\nBy construction, there must exist grid point \u03b10 such that ||p\u03b2 \u00b4 \u03b10||2 \u0103 {R2. By Lemma 2\nDpp\u03b2T pXpnq, p\u03b2T pY pnqq \u010f Dp\u03b10T pXpnq, \u03b10T pY pnqq ` C\nthus completing the proof.\nLemma 2. For \u03b1, \u03b2 P B such that ||\u03b1\u00b4 \u03b2||2 \u0103 , we have:\n|Dp\u03b1T pXpnq, \u03b1T pY pnqq \u00b4Dp\u03b2T pXpnq, \u03b2T pY pnqq| \u010f C R2 (11)\nProof. We assume that the \u03b1-projected divergence is larger than the \u03b2-projected divergence, and write:\nDp\u03b2T pXpnq, \u03b2T pY pnqq \u201c min MPM,\nn \u00ff\ni\u201c1\nm \u00ff j\u201c1 p\u03b2Txpiq \u00b4 \u03b2T ypjqq2Mij\nrecalling that M is the set of matching matrices defined in the main text. LetMp\u03b2q denote the matrix which is used in the computation of the \u03b2-projected empirical Wasserstein distance (the minimizer of the righthand side of the above expression). Thus, we can express (11) as:\nn \u00ff\ni\u201c1\nm \u00ff j\u201c1 p\u03b1Txpiq \u00b4 \u03b1T ypjqq2Mp\u03b1qij \u00b4 n \u00ff i\u201c1 m \u00ff j\u201c1 p\u03b2Txpiq \u00b4 \u03b2T ypjqq2Mp\u03b2qij\n\u010f n \u00ff\ni\u201c1\nm \u00ff j\u201c1 p\u03b1Txpiq \u00b4 \u03b1T ypjqq2Mp\u03b2qij \u00b4 n \u00ff i\u201c1 m \u00ff j\u201c1 p\u03b2Txpiq \u00b4 \u03b2T ypjqq2Mp\u03b2qij\n\u010f n \u00ff\ni\u201c1\nm \u00ff\nj\u201c1\n\u201d p\u03b1T pxpiq \u00b4 ypjqqq2 \u00b4 p\u03b2T pxpiq \u00b4 ypjqqq2 \u0131 Mp\u03b2qij\n\u201c n \u00ff\ni\u201c1\nm \u00ff\nj\u201c1\n\u201d p\u03b1\u00b4 \u03b2qT pxpiq \u00b4 ypjqq \u00a8 p\u03b1` \u03b2qT pxpiq \u00b4 ypjqq \u0131 Mp\u03b2qij\n\u010f n \u00ff\ni\u201c1\nm \u00ff j\u201c1 C R2Mp\u03b2qij \u201c C R2\nS3.3 Proof of Theorem 3\nProof. Our proof relies primarily on a quantitative form of the Cramer-Wold result presented in [11]. We adapt Theorem 3.1 [11] in its contrapositive form: If D a \u011b 0 such that TapX,Y q \u0105 hpgp\u2206qq, then D\u03b2 P B such that\nsup zPR\n\u02c7\n\u02c7\n\u02c7\n\u02c7\nPr ` \u03b2TX \u010f z \u02d8 \u00b4 Pr ` \u03b2TY \u010f z \u02d8\n\u02c7\n\u02c7\n\u02c7\n\u02c7\n\u0105 gpC\u2206q (12)\nSubsequently we leverage a number of well-characterized relationships between different probability metrics (cf. [4]) to lower bound the projected (squared) Wasserstein distance (of the underlying random variables).\nLetting K\u03b2 denote the projected Kolmogorov distance in (12), we have that the \u03b2-projected Le\u0301vydistance, L\u03b2 satisfies:\nK\u03b2 \u010f p1` \u03a6qL\u03b2 where \u03a6 :\u201c sup \u03b1PB sup y |f\u03b1TY pyq|\n(\n(13)\nand f\u03b1TY pyq is the density of the projection of Y in the \u03b1 direction. In turn the projected Le\u0301vy L\u03b2 is bounded above by the Prokhorov metric which itself is bounded above by the square root of the \u03b2-projected Wasserstein distance. Following the chain of inequalities, we obtain: Dp\u03b2TX,\u03b2TY q \u011b C\u2206, to which we can apply Theorem 1 to obtain the desired probabilistic bound on the empirical projected divergence.\nS3.4 Proof of Theorem 4\nProof. Theorem 2 implies that with high probability, any \u03b2SC P Rd\u00b4k has Dp\u03b2TSC pX pnq SC , \u03b2TSC pY pnq SC q \u0103 , while Theorem 3 specifies the probability that there exists \u03b2S P Rk such that Dpp\u03b2TS pX pnq S , p\u03b2TS pY pnq S q \u0105 d \u00a8 .\nA bound for the probability that both theorems\u2019 conclusions hold may be obtained by the union bound. When this is the case, it is clear that the optimal k-sparse p\u03b2 P Rd must obey the sparsity pattern specified in the statement of Theorem 4. To see this, consider any \u03b2 P B with \u03b2j \u2030 0 for some j P SC and note that it is always possible to produce a strictly superior projection by setting \u03b2j \u201c 0 and distributing the additional weight |\u03b2j | among the features in S in an optimal manner."}, {"heading": "S4 Derivation of semidefinite relaxation properties", "text": "Here, we provide some intuitive arguments for the conclusions in \u00a74.3, regarding some conditions under which our semidefinite relaxation is nearly tight.\nCondition (i) derives from the fact that (5) has rank one solution when the objective is approximately linear in B.\n(ii) and (iii) are derived by noting that (5) is the Wasserstein distance between random variables B1{2X and B1{2Y where AX follows a NpA\u00b5X , A\u03a3XAT q distribution when X is Gaussian. Furthermore, the Wasserstein distance between (multivariate) Gaussian distributions can be analytically written as\nW pX,Y q \u201c ||\u00b5X \u00b4 \u00b5Y ||22 ` ||\u03a3 1{2 X \u00b4 \u03a3 1{2 Y || 2 F\nAdditional References for the Supplementary Information [30] Levina E, Bickel P (2001) The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics.\nICCV 2: 251\u2013256. [31] Rosenbaum PR (2005) An exact distribution-free test comparing two multivariate distributions based on\nadjacency. Journal of the Royal Statistical Society Series B 67: 515\u2013530. [32] Gretton A, Borgwardt KM, Rasch MJ, Scholkopf B, Smola A (2012) A Kernel Two-Sample Test. The\nJournal of Machine Learning Research 13: 723\u2013773. [33] Szekely G, Rizzo M (2004) Testing for equal distributions in high dimension. InterStat 5. [34] Wei S, Lee C, Wichers L, Marron JS (2015) Direction-Projection-Permutation for High Dimensional Hy-\npothesis Tests. Journal of Computational and Graphical Statistics . [35] Zeisel A, Munoz-Manchado AB, Codeluppi S, Lonnerberg P, La Manno G, et al. (2015) Cell types in the\nmouse cortex and hippocampus revealed by single-cell RNA-seq. Science 347: 1138\u20131142. [36] Trapnell C, Cacchiarelli D, Grimsby J, Pokharel P, Li S, et al. (2014) The dynamics and regulators of cell\nfate decisions are revealed by pseudotemporal ordering of single cells. Nature Biotechnology 32: 381\u2013386. [37] Ritchie M, Phipson B, Wu D, Hu Y, Law CW, et al. (2015) limma powers differential expression analyses for\nRNA-sequencing and microarray studies. Nucleic Acids Research 43: e47. [38] Lesage S, Brice A (2009) Parkinson\u2019s disease: from monogenic forms to genetic susceptibility factors. Hu-\nman Molecular Genetics 18: R48\u2013R59. [39] Linnertz C, Lutz MW, Ervin JF, Allen J, Miller NR, et al. (2014) The genetic contributions of SNCA and\nLRRK2 genes to Lewy Body pathology in Alzheimer\u2019s disease. Human molecular genetics 23: 4814\u20134821. [40] Jasnow AM, Ressler KJ, Hammack SE, Chhatwal JP, Rainnie DG (2009) Distinct subtypes of cholecys-\ntokinin (CCK)-containing interneurons of the basolateral amygdala identified using a CCK promoter-specific lentivirus. Journal of neurophysiology 101: 1494\u20131506.\n[41] Bormuth I, Yan K, Yonemasu T, Gummert M, Zhang M, et al. (2013) Neuronal Basic HelixLoopHelix Proteins Neurod2/6 Regulate Cortical Commissure Formation before Midline Interactions. Journal of Neuroscience 33: 641\u2013651.\n[42] Oh DH, Park YC, Kim SH (2010) Increased glycogen synthase kinase-3beta mRNA level in the hippocampus of patients with major depression: a study using the stanley neuropathology consortium integrative database. Psychiatry investigation 7: 202\u2013207.\n[43] Wu JQ, Wang X, Beveridge NJ, Tooney PA, Scott RJ, et al. (2012) Transcriptome Sequencing Revealed Significant Alteration of Cortical Promoter Usage and Splicing in Schizophrenia. PLoS ONE 7: e36351.\n[44] Molyneaux BJ, Arlotta P, Menezes JRL, Macklis JD (2007) Neuronal subtype specification in the cerebral cortex. Nat Rev Neurosci 8: 427\u2013437.\n[45] Hagemann TL, Jobe EM, Messing A (2012) Genetic Ablation of Nrf2/Antioxidant Response Pathway in Alexander Disease Mice Reduces Hippocampal Gliosis but Does Not Impact Survival. PLoS ONE 7: e37304.\n[46] Shiota J, Ishikawa M, Sakagami H, Tsuda M, Baraban JM, et al. (2006) Developmental expression of the SRF co-activator MAL in brain: role in regulating dendritic morphology. Journal of Neurochemistry 98: 1778\u20131788.\n[47] Zhang L, Ju X, Cheng Y, Guo X, Wen T (2011) Identifying Tmem59 related gene regulatory network of mouse neural stem cell from a compendium of expression profiles. BMC Systems Biology 5: 152.\n[48] Goodfellow S, Rebello M, Toska E, Zeef L, Rudd S, et al. (2011) WT1 and its transcriptional cofactor BASP1 redirect the differentiation pathway of an established blood cell line. Biochemical Journal 435: 113\u2013125.\n[49] Lederer CW, Torrisi A, Pantelidou M, Santama N, Cavallaro S (2007) Pathways and genes differentially expressed in the motor cortex of patients with sporadic amyotrophic lateral sclerosis. BMC genomics 8: 26.\n[50] Jara JH, Genc\u0327 B, Cox GA, Bohn MC, Roos RP, et al. (2015) Corticospinal Motor Neurons Are Susceptible to Increased ER Stress and Display Profound Degeneration in the Absence of UCHL1 Function. Cerebral Cortex .\n[51] Li X, Yu K, Zhang Z, Sun W, Yang Z, et al. (2014) Cholecystokinin from the entorhinal cortex enables neural plasticity in the auditory cortex. Cell Res 24: 307\u2013330.\n[52] Gallopin T, Geoffroy H, Rossier J, Lambolez B (2006) Cortical sources of CRF, NKB, and CCK and their effects on pyramidal cells in the neocortex. Cerebral cortex (New York, NY : 1991) 16: 1440\u20131452.\n[53] van der Vaart AW, Wellner JA (1996) Weak Convergence and Empirical Processes. Springer. [54] Cramer H, Wold H (1936) Some Theorems on Distribution Functions. Journal of the London Mathematical\nSociety 11: 290\u2013294. [55] Jirak M (2011) On the maximum of covariance estimators. Journal of Multivariate Analysis 102: 1032\u2013\n1046. [56] Gibbs AL, Su FE (2002) On Choosing and Bounding Probability Metrics. International Statistical Review\n70: 419\u2013435."}], "references": [{"title": "A More Powerful Two-Sample Test in High Dimensions using Random Projection", "author": ["M Lopes", "L Jacob", "M Wainwright"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "On Choosing and Bounding Probability Metrics", "author": ["AL Gibbs", "FE Su"], "venue": "International Statistical Review", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Direction-Projection-Permutation for High Dimensional Hypothesis Tests", "author": ["S Wei", "C Lee", "L Wichers", "JS Marron"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "An exact distribution-free test comparing two multivariate distributions based on adjacency", "author": ["PR Rosenbaum"], "venue": "Journal of the Royal Statistical Society Series B", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Testing for equal distributions in high dimension", "author": ["G Szekely", "M Rizzo"], "venue": "InterStat", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A (2012) A Kernel Two-Sample Test", "author": ["A Gretton", "KM Borgwardt", "MJ Rasch", "B Scholkopf", "Smola"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "A sharp form of the Cramer\u2013Wold theorem", "author": ["JA Cuesta-Albertos", "R Fraiman", "T Ransford"], "venue": "Journal of Theoretical Probability", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "On the maximum of covariance estimators", "author": ["M Jirak"], "venue": "Journal of Multivariate Analysis", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B : 267\u2013288", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Feature Selection via Concave Minimization and Support", "author": ["PS Bradley", "OL Mangasarian"], "venue": "Vector Machines", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A D\u2019Aspremont", "L El Ghaoui", "MI Jordan", "GR Lanckriet"], "venue": "SIAM Review : 434\u2013448", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["AA Amini", "MJ Wainwright"], "venue": "The Annals of Statistics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses. Spring-Verlag", "author": ["P Good"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J Duchi", "E Hazan", "Y Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Optimization Algorithms in Machine Learning", "author": ["SJ Wright"], "venue": "NIPS Tutorial", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Nonnegative Matrix Factorization with Earth Mover\u2019s Distance Metric for Image Analysis", "author": ["R Sandler", "M Lindenbaum"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics", "author": ["E Levina", "P Bickel"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time", "author": ["Z Wang", "H Lu", "H Liu"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Network Optimization: Continuous and Discrete Models", "author": ["DP Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Dual coordinate step methods for linear network flow problems", "author": ["DP Bertsekas", "J Eckstein"], "venue": "Mathematical Programming", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. In: Optimization for Machine Learning", "author": ["DP Bertsekas"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems", "author": ["A Beck", "M Teboulle"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Feature Extraction: Foundations and Applications", "author": ["I Guyon", "S Gunn", "M Nikravesh", "LA Zadeh"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Sparse Principal Component Analysis", "author": ["H Zou", "T Hastie", "R Tibshirani"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "The details in the distributions: why and how to study phenotypic variability", "author": ["KA Geiler-Samerotte", "CR Bauer", "S Li", "N Ziv", "D Gresham"], "venue": "Current opinion in biotechnology", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics", "author": ["E Levina", "P Bickel"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "An exact distribution-free test comparing two multivariate distributions based on adjacency", "author": ["PR Rosenbaum"], "venue": "Journal of the Royal Statistical Society Series B", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "A (2012) A Kernel Two-Sample Test", "author": ["A Gretton", "KM Borgwardt", "MJ Rasch", "B Scholkopf", "Smola"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Testing for equal distributions in high dimension", "author": ["G Szekely", "M Rizzo"], "venue": "InterStat", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Direction-Projection-Permutation for High Dimensional Hypothesis Tests", "author": ["S Wei", "C Lee", "L Wichers", "JS Marron"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq", "author": ["A Zeisel", "AB Munoz-Manchado", "S Codeluppi", "P Lonnerberg", "G La Manno"], "venue": "Science", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells", "author": ["C Trapnell", "D Cacchiarelli", "J Grimsby", "P Pokharel", "S Li"], "venue": "Nature Biotechnology", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "limma powers differential expression analyses for RNA-sequencing and microarray studies", "author": ["M Ritchie", "B Phipson", "D Wu", "Y Hu", "CW Law"], "venue": "Nucleic Acids Research", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Parkinson\u2019s disease: from monogenic forms to genetic susceptibility factors", "author": ["S Lesage", "A Brice"], "venue": "Human Molecular Genetics", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "The genetic contributions of SNCA and LRRK2 genes to Lewy Body pathology in Alzheimer\u2019s disease", "author": ["C Linnertz", "MW Lutz", "JF Ervin", "J Allen", "NR Miller"], "venue": "Human molecular genetics", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Distinct subtypes of cholecystokinin (CCK)-containing interneurons of the basolateral amygdala identified using a CCK promoter-specific lentivirus", "author": ["AM Jasnow", "KJ Ressler", "SE Hammack", "JP Chhatwal", "DG Rainnie"], "venue": "Journal of neurophysiology", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Neuronal Basic HelixLoopHelix Proteins Neurod2/6 Regulate Cortical Commissure Formation before Midline Interactions", "author": ["I Bormuth", "K Yan", "T Yonemasu", "M Gummert", "M Zhang"], "venue": "Journal of Neuroscience", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Increased glycogen synthase kinase-3beta mRNA level in the hippocampus of patients with major depression: a study using the stanley neuropathology consortium integrative database", "author": ["DH Oh", "YC Park", "SH Kim"], "venue": "Psychiatry investigation", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Transcriptome Sequencing Revealed Significant Alteration of Cortical Promoter Usage and Splicing in Schizophrenia", "author": ["JQ Wu", "X Wang", "NJ Beveridge", "PA Tooney", "RJ Scott"], "venue": "PLoS ONE", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Neuronal subtype specification in the cerebral cortex", "author": ["BJ Molyneaux", "P Arlotta", "JRL Menezes", "JD Macklis"], "venue": "Nat Rev Neurosci", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Genetic Ablation of Nrf2/Antioxidant Response Pathway in Alexander Disease Mice Reduces Hippocampal Gliosis but Does Not Impact Survival", "author": ["TL Hagemann", "EM Jobe", "A Messing"], "venue": "PLoS ONE", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Developmental expression of the SRF co-activator MAL in brain: role in regulating dendritic morphology", "author": ["J Shiota", "M Ishikawa", "H Sakagami", "M Tsuda", "JM Baraban"], "venue": "Journal of Neurochemistry", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "Identifying Tmem59 related gene regulatory network of mouse neural stem cell from a compendium of expression profiles", "author": ["L Zhang", "X Ju", "Y Cheng", "X Guo", "T Wen"], "venue": "BMC Systems Biology", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "WT1 and its transcriptional cofactor BASP1 redirect the differentiation pathway of an established blood cell line", "author": ["S Goodfellow", "M Rebello", "E Toska", "L Zeef", "S Rudd"], "venue": "Biochemical Journal", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Pathways and genes differentially expressed in the motor cortex of patients with sporadic amyotrophic lateral sclerosis", "author": ["CW Lederer", "A Torrisi", "M Pantelidou", "N Santama", "S Cavallaro"], "venue": "BMC genomics", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2007}, {"title": "Corticospinal Motor Neurons Are Susceptible to Increased ER Stress and Display Profound Degeneration in the Absence of UCHL1 Function. Cerebral Cortex", "author": ["JH Jara", "B Gen\u00e7", "GA Cox", "MC Bohn", "RP Roos"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Cholecystokinin from the entorhinal cortex enables neural plasticity in the auditory cortex", "author": ["X Li", "K Yu", "Z Zhang", "W Sun", "Z Yang"], "venue": "Cell Res", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Cortical sources of CRF, NKB, and CCK and their effects on pyramidal cells in the neocortex. Cerebral cortex", "author": ["T Gallopin", "H Geoffroy", "J Rossier", "B Lambolez"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2006}, {"title": "On the maximum of covariance estimators", "author": ["M Jirak"], "venue": "Journal of Multivariate Analysis", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2011}, {"title": "On Choosing and Bounding Probability Metrics", "author": ["AL Gibbs", "FE Su"], "venue": "International Statistical Review", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Many two-sample analyses have focused on characterizing limited differences such as mean shifts [1, 2].", "startOffset": 96, "endOffset": 102}, {"referenceID": 1, "context": "), the Kolmogorov distance, or the Wasserstein metric [4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 3, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 4, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 5, "context": "Unfortunately, this simplicity vanishes as the dimensionality d grows, and complex test-statistics have been designed to address some of the difficulties that appear in high-dimensional settings [5, 6, 7, 8].", "startOffset": 195, "endOffset": 207}, {"referenceID": 6, "context": "This reduction is justified by the Cramer-Wold device, which ensures that PX \u2030 PY if and only if there exists a direction along which the univariate linearly projected distributions differ [9, 10, 11].", "startOffset": 189, "endOffset": 200}, {"referenceID": 7, "context": "This reduction is justified by the Cramer-Wold device, which ensures that PX \u2030 PY if and only if there exists a direction along which the univariate linearly projected distributions differ [9, 10, 11].", "startOffset": 189, "endOffset": 200}, {"referenceID": 8, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 9, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 2, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 0, "context": "2 Related Work The problem of characterizing differences between populations, including feature selection, has received a great deal of study [2, 12, 13, 5, 1].", "startOffset": 142, "endOffset": 159}, {"referenceID": 8, "context": "For multivariate two-class data, the most widely adopted methods include (sparse) linear discriminant analysis (LDA) [2] and the logistic lasso [12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "[5], in which the data is first projected along the normal to the separating hyperplane (found using linear SVM, distance weighted discrimination, or the centroid method) followed by a univariate two-sample test on the projected data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The projections could also be chosen at random [1].", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "We note that by restricting the divergence measure in our technique, methods such as the (sparse) linear support vector machine [13] could be viewed as special cases.", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "Instead of imposing a hard cardinality constraint ||\u03b2||0 \u010f k, we may instead penalize by adding a penalty term1  \u0301\u03bb||\u03b2||0 or its natural relaxation, the `1 shrinkage used in Lasso [12], sparse LDA [2], and sparse PCA [14, 15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "Instead of imposing a hard cardinality constraint ||\u03b2||0 \u010f k, we may instead penalize by adding a penalty term1  \u0301\u03bb||\u03b2||0 or its natural relaxation, the `1 shrinkage used in Lasso [12], sparse LDA [2], and sparse PCA [14, 15].", "startOffset": 217, "endOffset": 225}, {"referenceID": 11, "context": "Instead of imposing a hard cardinality constraint ||\u03b2||0 \u010f k, we may instead penalize by adding a penalty term1  \u0301\u03bb||\u03b2||0 or its natural relaxation, the `1 shrinkage used in Lasso [12], sparse LDA [2], and sparse PCA [14, 15].", "startOffset": 217, "endOffset": 225}, {"referenceID": 2, "context": "[5, 16]) with statistic Dpp \u03b2 p Xpnq, p \u03b2 p Y pmqq.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[5, 16]) with statistic Dpp \u03b2 p Xpnq, p \u03b2 p Y pmqq.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "Gaussian), the unregularized PDA objective (without shrinkage) is a smooth function of \u03b2, and thus amenable to the projected gradient method (or its accelerated variants [17, 18]).", "startOffset": 170, "endOffset": 178}, {"referenceID": 14, "context": "Gaussian), the unregularized PDA objective (without shrinkage) is a smooth function of \u03b2, and thus amenable to the projected gradient method (or its accelerated variants [17, 18]).", "startOffset": 170, "endOffset": 178}, {"referenceID": 15, "context": "While component analysis based on the Wasserstein distance has been limited to [19], this divergence has been successfully used in many other applications [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "While component analysis based on the Wasserstein distance has been limited to [19], this divergence has been successfully used in many other applications [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 16, "context": "(3) where M is the set of all n \u02c6 m nonnegative matching matrices with fixed row sums \u201c 1{n and column sums \u201c 1{m (see [20] for details), WM :\u201c \u0159 i,jrZij b ZijsMij , and Zij :\u201c xpiq  \u0301 ypjq.", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "Similarly, for the sparse variant without minizing over M , the problem would be solvable as sparse PCA [14, 15, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 11, "context": "Similarly, for the sparse variant without minizing over M , the problem would be solvable as sparse PCA [14, 15, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 17, "context": "Similarly, for the sparse variant without minizing over M , the problem would be solvable as sparse PCA [14, 15, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 17, "context": "We propose a two-step procedure similar to \u201ctighten after relax\u201d framework used to attain minimax-optimal rates in sparse PCA [21].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "Otherwise, we can truncateB \u030a as in [14], treating the dominant eigenvector as an approximate solution to the original problem (3).", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "To obtain a relaxation for the sparse version where k \u0103 d (SPARDA), we follow [14] closely.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "By selecting \u03bb as the optimal Lagrange multiplier for this `1 constraint, we can obtain an equivalent penalized reformulation parameterized by \u03bb rather than k [14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "[22, 23]) min MPM tr pWMBq \u201c 1 m max u,v n \u00ff", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[22, 23]) min MPM tr pWMBq \u201c 1 m max u,v n \u00ff", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "For scaling to large samples, one may alternatively employ incremental supergradient directions [24] where Step 4 would be replaced by drawing random pi, jq pairs.", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "15) in [24].", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "2 of [24], the RELAX algorithm (as well as its incremental variant) is guaranteed to approach the optimal solution of the dual which also solves (6), provided we employ sufficiently large T and small step-sizes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "The gradient is directly derivable from expression (3) where the nonzeroMij are determined by appropriately matching empirical quantiles (represented by sorted indices) since the univariate Wasserstein distance is simply the L2 distance between quantile functions [20].", "startOffset": 264, "endOffset": 268}, {"referenceID": 14, "context": "momentum techniques, adaptive learning rates, or FISTA and other variants of Nesterov\u2019s method [18, 17, 25]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 13, "context": "momentum techniques, adaptive learning rates, or FISTA and other variants of Nesterov\u2019s method [18, 17, 25]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 21, "context": "momentum techniques, adaptive learning rates, or FISTA and other variants of Nesterov\u2019s method [18, 17, 25]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 7, "context": "[11]): TapX,Y q :\u201c |Prp|X1| \u010f a, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The synthetic MADELON dataset used in the NIPS 2003 feature selection challenge consists of points (n \u201c m \u201c 1000, d \u201c 500) which have 5 features scattered on the vertices of a fivedimensional hypercube (so that interactions between features must be considered in order to distinguish the two classes), 15 features that are noisy linear combinations of the original five, and 480 useless features [26].", "startOffset": 396, "endOffset": 400}, {"referenceID": 23, "context": "Figure 1b demonstrates how well SPARDA (red), the top sparse principal component (black) [27], sparse LDA (green) [2], and the logistic lasso (blue) [12] are able to identify the 20 relevant features over different settings of their respective regularization parameters (which determine the cardinality of the vector returned by each method).", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Figure 1b demonstrates how well SPARDA (red), the top sparse principal component (black) [27], sparse LDA (green) [2], and the logistic lasso (blue) [12] are able to identify the 20 relevant features over different settings of their respective regularization parameters (which determine the cardinality of the vector returned by each method).", "startOffset": 149, "endOffset": 153}, {"referenceID": 22, "context": "The red asterisk indicates the SPARDA result with \u03bb automatically selected via our crossvalidation procedure (without information of the underlying features\u2019 importance), and the black asterisk indicates the best reported result in the challenge [26].", "startOffset": 246, "endOffset": 250}, {"referenceID": 22, "context": "Despite being class-agnostic, PCA was successfully utilized by numerous challenge participants [26], and we find that the sparse PCA performs on par with logistic regression and LDA.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "Similarly, the challengewinning Bayesian SVM with Automatic Relevance Determination [26] only selects 8 of the 20 relevant features.", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "Figure 1c depicts (average) p-values produced by SPARDA (red), PDA (purple), the overall Wasserstein distance in R (black), Maximum Mean Discrepancy [8] (green), and DiProPerm [5] (blue) in two-sample synthetically controlled problems where PX \u2030 PY and the underlying differences have varying degrees of sparsity.", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Figure 1c depicts (average) p-values produced by SPARDA (red), PDA (purple), the overall Wasserstein distance in R (black), Maximum Mean Discrepancy [8] (green), and DiProPerm [5] (blue) in two-sample synthetically controlled problems where PX \u2030 PY and the underlying differences have varying degrees of sparsity.", "startOffset": 176, "endOffset": 179}, {"referenceID": 12, "context": "As we evaluate the significance of each method\u2019s statistic via permutation testing, all the tests are guaranteed to exactly control Type I error [16], and we thus only compare their respective power in determining PX \u2030 PY setting.", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "This experiment also demonstrate that the unregularized PDA retains greater power than DiProPerm, a similar projection-based method [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 24, "context": "Recent technological advances allow complete transcriptome profiling in thousands of individual cells with the goal of fine molecular characterization of cell populations (beyond the crude averagetissue-level expression measure that is currently standard) [28].", "startOffset": 256, "endOffset": 260}, {"referenceID": 24, "context": "These types of important changes cannot be detected by standard differential expression analyses which consider each gene in isolation or require gene-sets to be explicitly identified as features [28].", "startOffset": 196, "endOffset": 200}, {"referenceID": 11, "context": "Here, rich theory has been derived for compressed sensing and sparse PCA by leveraging ideas such as restricted isometry or spiked covariance [15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "References [1] Lopes M, Jacob L, Wainwright M (2011) A More Powerful Two-Sample Test in High Dimensions using Random Projection.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[4] Gibbs AL, Su FE (2002) On Choosing and Bounding Probability Metrics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[5] Wei S, Lee C, Wichers L, Marron JS (2015) Direction-Projection-Permutation for High Dimensional Hypothesis Tests.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[6] Rosenbaum PR (2005) An exact distribution-free test comparing two multivariate distributions based on adjacency.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Szekely G, Rizzo M (2004) Testing for equal distributions in high dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] Gretton A, Borgwardt KM, Rasch MJ, Scholkopf B, Smola A (2012) A Kernel Two-Sample Test.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[10] Cuesta-Albertos JA, Fraiman R, Ransford T (2007) A sharp form of the Cramer\u2013Wold theorem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[11] Jirak M (2011) On the maximum of covariance estimators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[12] Tibshirani R (1996) Regression shrinkage and selection via the lasso.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[13] Bradley PS, Mangasarian OL (1998) Feature Selection via Concave Minimization and Support Vector Machines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[14] D\u2019Aspremont A, El Ghaoui L, Jordan MI, Lanckriet GR (2007) A direct formulation for sparse PCA using semidefinite programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[15] Amini AA, Wainwright MJ (2009) High-dimensional analysis of semidefinite relaxations for sparse principal components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[16] Good P (1994) Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[17] Duchi J, Hazan E, Singer Y (2011) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[18] Wright SJ (2010) Optimization Algorithms in Machine Learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[19] Sandler R, Lindenbaum M (2011) Nonnegative Matrix Factorization with Earth Mover\u2019s Distance Metric for Image Analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[20] Levina E, Bickel P (2001) The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[21] Wang Z, Lu H, Liu H (2014) Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[22] Bertsekas DP (1998) Network Optimization: Continuous and Discrete Models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[23] Bertsekas DP, Eckstein J (1988) Dual coordinate step methods for linear network flow problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24] Bertsekas DP (2011) Incremental gradient, subgradient, and proximal methods for convex optimization: A survey.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[25] Beck A, Teboulle M (2009) A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[26] Guyon I, Gunn S, Nikravesh M, Zadeh LA (2006) Feature Extraction: Foundations and Applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27] Zou H, Hastie T, Tibshirani R (2005) Sparse Principal Component Analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] Geiler-Samerotte KA, Bauer CR, Li S, Ziv N, Gresham D, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The overall Wasserstein distance in the ambient space is computed by solving a transportation problem [20], and we note the similarity between this statistic and the cross-match test [6].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The overall Wasserstein distance in the ambient space is computed by solving a transportation problem [20], and we note the similarity between this statistic and the cross-match test [6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "A popular kernel method for testing high-dimensional distribution equality, the mean map discrepancy, is computed using the Gaussian kernel with bandwidth parameter chosen by the \u201cmedian trick\u201d [8] (this is very similar to the energy test of [7]).", "startOffset": 194, "endOffset": 197}, {"referenceID": 4, "context": "A popular kernel method for testing high-dimensional distribution equality, the mean map discrepancy, is computed using the Gaussian kernel with bandwidth parameter chosen by the \u201cmedian trick\u201d [8] (this is very similar to the energy test of [7]).", "startOffset": 242, "endOffset": 245}, {"referenceID": 2, "context": "Finally, we also compute the DiProPerm statistic, employing the the DWD-t variant recommended for testing general equality of distributions [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 31, "context": "Following [36], we represent gene expression by log-transformed FPKM computed from the sequencing read counts2, so values are directly comparable between genes.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "Because expression measurements from individual cells are poorer in quality than transcriptome profiles obtained in aggregate across tissue samples (due to a drastically reduced amount of available RNA), it is important to filter out poorly measured genes and we retain a set of 10,305 genes that are measured with sufficient accuracy for informative analysis [36].", "startOffset": 360, "endOffset": 364}, {"referenceID": 32, "context": "For comparison, we also run LIMMA, a standard method for differential expression analysis which tests for marginal mean-differences on a gene-by-gene basis [37].", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "One particularly relevant gene in this data is Snca, a presynaptic signaling and membrane trafficking gene whose defects are implicated in both Parkinson and Alzheimer\u2019s disease [38, 39].", "startOffset": 178, "endOffset": 186}, {"referenceID": 34, "context": "One particularly relevant gene in this data is Snca, a presynaptic signaling and membrane trafficking gene whose defects are implicated in both Parkinson and Alzheimer\u2019s disease [38, 39].", "startOffset": 178, "endOffset": 186}, {"referenceID": 35, "context": "0593 Primary distinguishing gene between distinct interneuron classes identified in the cortex and hippocampus [40] Neurod6 0.", "startOffset": 111, "endOffset": 115}, {"referenceID": 36, "context": "the hippocampal region [41] Stmn3 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "0573 Up-expressed in hippocampus of patients with depressive disorders [42] Plp1 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 38, "context": "0570 An oligodendrocyte- and myelin-related gene which exhibits cortical differential expression in schizophrenia [43] Crym 0.", "startOffset": 114, "endOffset": 118}, {"referenceID": 39, "context": "0550 Plays a role in neuronal specification [44] Spink8 0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "0500 Stress induction leads to reduced expression in the mouse hippocampus [45] Mal 0.", "startOffset": 75, "endOffset": 79}, {"referenceID": 41, "context": "0494 Regulates dendritic morphology and is expressed at lower levels in cortex than in hippocampus [46] Tspan13 0.", "startOffset": 99, "endOffset": 103}, {"referenceID": 42, "context": "Knock out in the hippocampus results in drastic expression changes of many other genes [47] Basp1 0.", "startOffset": 87, "endOffset": 91}, {"referenceID": 43, "context": "1171 Transcriptional cofactor which can divert the differentiation of cells to a neuronal-like morphology [48] Snhg1 0.", "startOffset": 106, "endOffset": 110}, {"referenceID": 44, "context": "1145 Promoter of neurodifferentiation and axonal/dendritic maintenance [49] Uchl1 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 45, "context": "1137 Loss of function leads to profound degeneration of motor neurons [50].", "startOffset": 70, "endOffset": 74}, {"referenceID": 46, "context": "1131 Targets pyramidal neurons and enables neocortical plasticity allowing for example the auditory cortex to detect light stimuli [51, 52] Table S2: Genes with the greatest weight in the projection p \u03b2 produced by SPARDA analysis of the marginally normalized expression data.", "startOffset": 131, "endOffset": 139}, {"referenceID": 47, "context": "1131 Targets pyramidal neurons and enables neocortical plasticity allowing for example the auditory cortex to detect light stimuli [51, 52] Table S2: Genes with the greatest weight in the projection p \u03b2 produced by SPARDA analysis of the marginally normalized expression data.", "startOffset": 131, "endOffset": 139}, {"referenceID": 7, "context": "Our proof relies primarily on a quantitative form of the Cramer-Wold result presented in [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "1 [11] in its contrapositive form: If D a \u011b 0 such that TapX,Y q \u0105 hpgp\u2206qq, then D\u03b2 P B such that sup zPR \u02c7 \u02c7 \u02c7 \u02c7 Pr ` \u03b2X \u010f z \u0306 \u0301 Pr ` \u03b2Y \u010f z \u0306 \u02c7 \u02c7 \u02c7 \u02c7 \u0105 gpC\u2206q (12) Subsequently we leverage a number of well-characterized relationships between different probability metrics (cf.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "[4]) to lower bound the projected (squared) Wasserstein distance (of the underlying random variables).", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Additional References for the Supplementary Information [30] Levina E, Bickel P (2001) The Earth Mover\u2019s distance is the Mallows distance: some insights from statistics.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "[31] Rosenbaum PR (2005) An exact distribution-free test comparing two multivariate distributions based on adjacency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[32] Gretton A, Borgwardt KM, Rasch MJ, Scholkopf B, Smola A (2012) A Kernel Two-Sample Test.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[33] Szekely G, Rizzo M (2004) Testing for equal distributions in high dimension.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[34] Wei S, Lee C, Wichers L, Marron JS (2015) Direction-Projection-Permutation for High Dimensional Hypothesis Tests.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[35] Zeisel A, Munoz-Manchado AB, Codeluppi S, Lonnerberg P, La Manno G, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[36] Trapnell C, Cacchiarelli D, Grimsby J, Pokharel P, Li S, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[37] Ritchie M, Phipson B, Wu D, Hu Y, Law CW, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[38] Lesage S, Brice A (2009) Parkinson\u2019s disease: from monogenic forms to genetic susceptibility factors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[39] Linnertz C, Lutz MW, Ervin JF, Allen J, Miller NR, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[40] Jasnow AM, Ressler KJ, Hammack SE, Chhatwal JP, Rainnie DG (2009) Distinct subtypes of cholecystokinin (CCK)-containing interneurons of the basolateral amygdala identified using a CCK promoter-specific lentivirus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[41] Bormuth I, Yan K, Yonemasu T, Gummert M, Zhang M, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[42] Oh DH, Park YC, Kim SH (2010) Increased glycogen synthase kinase-3beta mRNA level in the hippocampus of patients with major depression: a study using the stanley neuropathology consortium integrative database.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[43] Wu JQ, Wang X, Beveridge NJ, Tooney PA, Scott RJ, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[44] Molyneaux BJ, Arlotta P, Menezes JRL, Macklis JD (2007) Neuronal subtype specification in the cerebral cortex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[45] Hagemann TL, Jobe EM, Messing A (2012) Genetic Ablation of Nrf2/Antioxidant Response Pathway in Alexander Disease Mice Reduces Hippocampal Gliosis but Does Not Impact Survival.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[46] Shiota J, Ishikawa M, Sakagami H, Tsuda M, Baraban JM, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[47] Zhang L, Ju X, Cheng Y, Guo X, Wen T (2011) Identifying Tmem59 related gene regulatory network of mouse neural stem cell from a compendium of expression profiles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[48] Goodfellow S, Rebello M, Toska E, Zeef L, Rudd S, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[49] Lederer CW, Torrisi A, Pantelidou M, Santama N, Cavallaro S (2007) Pathways and genes differentially expressed in the motor cortex of patients with sporadic amyotrophic lateral sclerosis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[50] Jara JH, Gen\u00e7 B, Cox GA, Bohn MC, Roos RP, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[51] Li X, Yu K, Zhang Z, Sun W, Yang Z, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[52] Gallopin T, Geoffroy H, Rossier J, Lambolez B (2006) Cortical sources of CRF, NKB, and CCK and their effects on pyramidal cells in the neocortex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[55] Jirak M (2011) On the maximum of covariance estimators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[56] Gibbs AL, Su FE (2002) On Choosing and Bounding Probability Metrics.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.", "creator": "LaTeX with hyperref package"}}}