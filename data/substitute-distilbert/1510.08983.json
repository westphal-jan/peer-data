{"id": "1510.08983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "abstract": "in this paper, we extend the deep long short - term memory ( rr ) recurrent neural networks by introducing gated direct connections crossing memory cells in adjacent layers. these direct links, called highway connections, enable unimpeded information flow across different layers may thus alleviate the gradient error problem when building deeper lstms. we can introduce the latency - controlled linking lstms ( blstms ) which can exploit the whole history while keeping the latency under control. efficient algorithms successfully proposed to train these novel neurons using both frame and sequence discriminative criteria. experiments representing the ami distant speech recognition ( ict ) task indicate that we can deliver deeper lstms and achieve better algorithm fighting continuous interference with highway lstms ( hlstms ). our novel model obtains $ 01. 9 / 47. 16 \\ % $ wer on ami ( sdm ) dev and eval channels, outperforming all previous works. ai beats the exact dnn and dlstm specifications with $ 15. 7 \\ % $ ` $ 5. 3 \\ % $ their improvement respectively.", "histories": [["v1", "Fri, 30 Oct 2015 06:40:14 GMT  (375kb,D)", "https://arxiv.org/abs/1510.08983v1", null], ["v2", "Mon, 11 Jan 2016 09:48:01 GMT  (616kb,D)", "http://arxiv.org/abs/1510.08983v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG", "authors": ["yu zhang", "guoguo chen", "dong yu", "kaisheng yao", "sanjeev khudanpur", "james glass"], "accepted": false, "id": "1510.08983"}, "pdf": {"name": "1510.08983.pdf", "metadata": {"source": "CRF", "title": "HIGHWAY LONG SHORT-TERM MEMORY RNNS FOR DISTANT SPEECH RECOGNITION", "authors": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yao", "Sanjeev Khudanpur", "James Glass"], "emails": ["yzhang87@mit.edu,", "glass@mit.edu,", "guoguo@jhu.edu,", "khudanpur@jhu.edu,", "Kaisheng.YAO}@microsoft.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 Highway LSTM, CNTK, LSTM, Sequence Training"}, {"heading": "1. INTRODUCTION", "text": "Recently the deep neural network (DNN)-based acoustic models (AMs) greatly improved automatic speech recognition (ASR) accuracy on many tasks [1, 2, 3, 4]. Further improvements were reported by using more advanced models such as convolutional neural networks (CNNs) [5] and long short-term memory (LSTM) recurrent neural networks (RNNs) [6, 7, 8].\nAlthough these new techniques help to decrease the word error rate (WER) on distant speech recognition (DSR) [9], DSR remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques [10, 11, 12] and multi-pass decoding schemes.\nIn this paper, we explore more advanced back-end techniques for DSR. It is reported [8] that deep LSTM (DLSTM) RNNs help improve generalization and often outperform single-layer LSTM RNNs. However, DLSTM RNNs are harder to train and slower to converge. In this paper, we extend DLSTM RNNs by introducing a gated direct connection between memory cells of adjacent layers. These direct links, called highway connections, provide a path for information to flow between layers more directly without decay. It alleviates the gradient vanishing problem and enables DLSTM RNNs training with virtually arbitrary depth. Here, we refer to an LSTM RNN with highway connections as HLSTM RNN.\n\u2217Part of the work reported here was carried out during the 2015 Jelinek Memorial Summer Workshop on Speech and Language Technologies at the University of Washington, Seattle, and was supported by Johns Hopkins University via NSF Grant No IIS 1005411, and gifts from Google, Microsoft Research, Amazon, Mitsubishi Electric, and MERL.\nTo further improve the performance, we also introduce the latency-controlled bidirectional LSTM (LC-BLSTM) RNNs. In the LC-BLSTM RNNs, the past history is fully exploited similar to that in the unidirectional LSTM RNNs. However, unlike the standard BLSTM RNNs which can start model evaluation only after seeing the whole utterance, the LC-BLSTM RNNs only look ahead for a fixed number of frames which limits the latency. The LC-BLSTM can be much more efficiently trained than the standard BLSTM without performance loss. It also trains and decodes faster than context-sensitive-chunk BLSTMs [13] which can only access limited past and future context.\nOur study is conducted on the AMI single distant microphone (SDM) setup. We compare the standard (B)LSTM RNNs and highway (B)LSTM RNNs with 3 and 8 layers. We show that the highway (B)LSTM RNNs with dropout applied to the highway connection significantly outperform the standard (B)LSTM RNNs. The high way connection helps to train deeper networks better and the highway (B)LSTM RNNs seem to benefit more from sequence discriminative training. Overall, our proposed model decreased WER by 15.7% over DNNs, 14.4% over CNNs [14], and 5.3% over DLSTM RNNs relatively. To our best knowledge, the 43.9/47.7% WER we achieved on the AMI (SDM) dev and eval sets is the best results reported on this task.1.\nThe rest of the paper is organized as follows. In Section 2 we briefly discuss related work. In Section 3 we introduce standard DLSTM RNNs, BLSTM RNNs, and the highway (B)LSTM RNNs. In Section 4 we describe LC-BLSTM and the way to train such models efficiently with both frame and sequence discriminative criteria. We summarize the experimental setup in Section 5 and report experimental results in Section 6. Conclusions are reiterated in Section 7."}, {"heading": "2. RELATED WORK", "text": "After developing the highway LSTMs independently we noticed that similar work has been done in [16, 17, 18]. All of these works share the same idea of adding gated linear connections between different layers. The highway networks proposed in [16] adaptively carry some dimensions of the input directly to the output so that information can flow across layers much more easily. However, their formulation is different from ours and their focus is DNN. The work in [17] share the same idea and model structure. [18] is more general and uses a generic form. However, their task is on text e.g. machine translation while our focus is distant speech recognition. In addition, we used dropout as the way to control the highway connections which turns out to be critical for DSR.\n1The tools and scripts used to produce the results reported in this paper are publicly available as part of the CNTK toolkit[15], and anyone with access to the data should be able to reproduce our results.\nar X\niv :1\n51 0.\n08 98\n3v 2\n[ cs\n.N E\n] 1\n1 Ja\nn 20\n16"}, {"heading": "3. HIGHWAY LONG SHORT-TERM MEMORY RNNS", "text": ""}, {"heading": "3.1. Long short-term memory RNNs", "text": "The LSTM RNN was initially proposed in [19] to solve the gradient diminishing problem in RNNs. It introduces a linear dependence between ct, the memory cell state at time t, and ct\u22121, the same cell\u2019s state at t \u2212 1. Nonlinear gates are introduced to control the information flow. The operation of the network follows the equations\nit = \u03c3(Wxixt +Wmiht\u22121 +Wcict\u22121 + bi) (1) ft = \u03c3(Wxfxt +Wmfht\u22121 +Wcfct\u22121 + bf ) (2) ct = ft ct\u22121 + it tanh(Wxcxt +Wmcmt\u22121 + bc) (3) ot = \u03c3(Wxoxt +Wmoht\u22121 +Wcoct + bo) (4) mt = ot tanh(ct) (5)\niteratively from t = 1 to T , where \u03c3()\u0307 is the logistic sigmoid function, it, ft,ot, ct and mt are vectors to represent values at time t of the input gate, forget gate, output gate, cell activation, and cell output activation respectively. denotes element-wise product of vectors. W\u2217 are the weight matrices connecting different gates, and b\u2217 are the corresponding bias vectors. All the matrices are full except the matrices Wci,Wcf ,Wco from the cell to gate vector which is diagonal."}, {"heading": "3.2. Deep LSTM RNNs", "text": "Deep LSTM RNNs are formed by stacking multiple layers of LSTM cells. Specifically, the output of the lower layer LSTM cells ylt is fed to the upper layer as input xl+1t . Although each LSTM layer is deep in time since it can be unrolled in time to become a feedforward neural network in which each layer shares the same weights, deep LSTM RNNs still outperform single-layer LSTM RNNs significantly. It is conjectured [8] that DLSTM RNNs can make better use of parameters by distributing them over the space through multiple layers. Note that in the conventional DLSTM RNNs the interaction between cells in different layers must go through the output-input connection."}, {"heading": "3.3. HLSTM RNNs", "text": "The Highway LSTM (HLSTM) RNN proposed in this paper is illustrated in Figure 1. It has a direct gated connection (in the red block) between the memory cells clt in the lower layer l and the memory cells cl+1t in the upper layer l + 1. The carry gate controls how much information can flow from the lower-layer cells directly to the upper-layer cells. The gate function at layer l + 1 at time t is\nd (l+1) t = \u03c3(b (l+1) d +W l+1 xd x (l+1) t +w l+1 cd c (l+1) t\u22121 +w (l+1) ld c l t), (6)\nwhere b(l+1)d is a bias term, W (l+1) xd is the weight matrix connecting the carry gate to the input of this layer. w(L+1)cd is a weight vector from the carry gate to the past cell state in the current layer. w(L+1)ld is a weight vector connecting the carry gate to the lower layer memory cell. d(l+1) is the carry gate activation vectors at layer l + 1.\nUsing the carry gate, an HLSTM RNN computes the cell state at layer (l + 1) according to\ncl+1t = d (l+1) t c l t + f (l+1) t c (l+1) t\u22121\n+ i (l+1) t tanh(W (l+1) xc x (l+1) t +W (l+1) hc m (l+1) t\u22121 + bc), (7)\nwhile all other equations are the same as that in the standard LSTM RNNs as described in Eq. (1),(2),(4), and (5).\nThus, depending on the output of the carry gates, the highway connection can smoothly vary its behavior between that of a plain LSTM layer or simply passes its cell memory from previous layer. The highway connection between cells in different layers makes influence from cells in one layer to the other more direct and can alleviate the gradient vanishing problem when training deeper LSTM RNNs."}, {"heading": "3.4. Bidirectional Highway LSTM RNNs", "text": "The unidirectional LSTM RNNs we described above can only exploit past history. In speech recognition, however, future contexts also carry information and should be utilized to further enhance acoustic models. The bidirectional RNNs take advantage of both\npast and future contexts by processing the data in both directions with two separate hidden layers. It is shown in [6, 7, 13] that bidirectional LSTN RNNs can indeed improve the speech recognition results. In this study, we also extend the HLSTM RNNs from unidirection to bidirection. Note that the backward layer follows the same equations used in the forward layer except that t \u2212 1 is replaced by t+ 1 to exploit future frames and the model operates from t = T to 1. The output of the forward and backward layers are concatenated to form the input to the next layer."}, {"heading": "4. EFFICIENT NETWORK TRAINING", "text": "Nowadays, GPUs are widely used in deep learning by leveraging massive parallel computations via mini-batch based training. For unidirectional RNN models, to better utilize the parallelization power of the GPU card, in [15], multiple sequences (e.g., 40) are often packed into the same mini-batch. Truncated BPTT is usually performed for parameters updating, therefore, only a small segment (e.g., 20 frames) of each sequence has to be packed into the minibatch. However, when applied to sequence level training (BLSTM or sequence training), GPU\u2019s limited memory restricts the number of sequences that can be packed into a mini-batch, especially for LVCSR tasks with long training sequences and large model sizes. One alternative way to speed up is using asynchronous SGD based on a GPU/CPU farm [20]. In this section, we are more focused on fully utilizing the parallelization power of a single GPU Card. The algorithms proposed here can also be applied to a multi-GPU setup."}, {"heading": "4.1. Latency-controlled bi-directional model training", "text": "To speed up the training of bi-direcctional RNNs, the Contextsensitive-chunk BPTT (CSC-BPTT) is proposed in [13]. In this method, a sequence is firstly split into chunks of fixed length Nc. Then Nl past frames and Nr future frames are concatenated before and after each chunk as the left and right context, respectively. The appended frames are only used to provide context information and do not generate error signals during training. Since each trunk can be independently drawn and trained, they can be stacked to form large minibatches to speed up training.\nUnfortunately, the model trained with CSC-BPTT is no longer the true bidirectional RNN since the history it can exploit is limited by the left and right context concatenated to the chunk. It also introduces additional computation cost during decoding since both the left and right contexts need to be recomputed for each chunk.\nTo solve the problems in the CSC-BPTT we propose the latencycontrolled bi-directional RNNs. Different from the CSC-BPTT, in our new model we carry the whole past history while still using a truncated future context. Instead of concatenating and computing Nl left contextual frames for each chunk we directly carry over the left contextual information from the previous chunk of the same utterance. For every chunk, both training and decoding computational cost is reduced by a factor of Nl\nNl+Nc+Nr . Moreover, loading the\nhistory from previous mini-batch instead of a fixed contextual windows makes the context exact when compared to the uni-directional model. Note that the standard BLSTM RNNs come with significant latency since the model can only be evaluated after seeing the whole utterance. In the latency-controlled BLSTM RNNs the latency is limited to Nr which can be set by the users. In our experiments, we process 40 utterances in parallel which is 10 times faster than processing the whole utterances without performance loss. Compared to the CSC BPTT our approach is 1.5 times faster and often leads to better accuracy.\nTo increase the number of sequences that can be fit into a mini-batch, we propose a two-forward-pass solution for sequence discriminative training on top of recurrent neural networks. The basic idea is pretty straightforward. For sequence training of recurrent models, we use the same mini-batch packaging method as that in the cross-entropy training case, i.e., we pack multiple sequences (e.g., 40) into the same mini-batch, each with a small chunk (e.g., 20 frames). Then in the first forward pass, we collect log-likelihood of frames in the mini-batch, and put those into a pool, without updating the model. We do this until we have collected the log-likelihood for a certain number of sequences. At this point, we are able to compute the error signal for each of the sequence in the pool. We then roll back the mini-batches that we have just computed error signals for, and start the second forward pass, this time we update the model using the error signals from the pool. With this two-forward-pass solution, we are able to pack far more sequences in the same mini-batch, e.g., 40\u223c60, thus leading to much faster training."}, {"heading": "5. EXPERIMENT SETUP", "text": ""}, {"heading": "5.1. Corpus", "text": "We evaluated our models on the AMI meeting corpus [21]. The AMI corpus comprises around 100 hours of meeting recordings, recorded in instrumented meeting rooms. Multiple microphones were used, including individual headset microphones (IHM), lapel microphones, and one or more microphone arrays. In this work, we use the single distant microphone (SDM) condition for our experiments. Our systems are trained and tested using the split recommended in the corpus release: a training set of 80 hours, a development set and a test set each of 9 hours. For our training, we use all the segments provided by the corpus, including those with overlapped speech. Our models are evaluated on the evaluation set only. NIST\u2019s asclite tool [22] is used for scoring."}, {"heading": "5.2. System description", "text": "Kaldi [23] is used for feature extraction, early stage triphone training as well as decoding. A maximum likelihood acoustic training recipe is used to trains a GMM-HMM triphone system. Forced alignment is performed on the training data by this triphone system to generate labels for further neural network training.\nThe Computational Network Toolkit (CNTK) [15] is used for neural network training. We start off by training a 6-layer DNN, with 2, 048 sigmoid units per layer. 40-dimensional filterbank features, together with their corresponding delta and delta-delta features are used as raw feature vectors. For our DNN training we concatenated 15 frames of raw feature vectors, which leads to a dimension of 1, 800. This DNN again is used to force align the training data to generate labels for further LSTM training.\nOur (H)LSTM models, unless explicitly stated otherwise, are added with a projection layer on top of each layer\u2019s output, as proposed in [8], and are trained with 80-dimensional log Mel filterbank (FBANK) features. For LSTMP models, each hidden layer consists of 1024 memory cells together with a 512-node projection layer. For the BLSTMP models, each hidden layer consists of 1024 memory cells (512 for forward and 512 for backward) with a 300-node projection layer. Their highway companions share the same network structure, except the additional highway connections.\nAll models are randomly initialized without either generative or discriminative pretraining [24]. A validation set is used to control\nthe learning rate which will be halved when no gain is observed. To train the unidirectional model, the truncated back-propagationthrough-time (BPTT) [25] is used to update the model parameters. Each BPTT segment contains 20 frames and process 40 utterances simultaneously. To train the latency-controlled bidirectional model, we set Nc = 22 and Nr = 21 and also process 40 utterances simultaneously. A start learning rate of 0.2 per minibatch is used and then the learning rate scheduler takes action. For frame level cross-entropy training, L2 constraint regularization [26] is used. For sequence training, L2 constraint regularization is also applied whenever it is used in the corresponding cross-entropy trained model. We use a fixed per sample learning rate of 1e \u2212 5 for DNN sequence training, and 2e\u2212 6 for LSTM sequence training."}, {"heading": "6. RESULTS", "text": "The performance of various models are evaluated using word error rate (WER) in percent below. All the experiments are conducted on AMI SDM1 eval set, if not specified otherwise. Since we do not exclude the overlapping speech segments during model training, in addition to results on the full eval set, we also show results on a subset that only contains the non-overlapping speech segments as [14]."}, {"heading": "6.1. 3-layer Highway (B)LSTMP", "text": "Table 1 gives WER performance of the 3-layer LSTMP and BLSTMP RNNs, as well as their highway versions. The performance of the DNN network is also listed for comparison. From the table, it\u2019s clear that the highway version of the LSTM RNNs consistently outperform their non-highway companions, though with a small margin."}, {"heading": "6.2. Highway (B)LSTMP with dropout", "text": "Dropout is applied to the highway connection to control its flow: a high dropout rate essentially turns off the highway connection, and a small dropout rate, on the other hand, keeps the connection alive. In our experiments, for early training stages, we use a small dropout rate of 0.1, and increase it to 0.8 after 5 epochs of training. Performance of highway (B)LSTMP networks with dropout is shown in Table 2, as we can see, dropout helps to further bring down the WER for highway networks.\nWhen a network goes deeper, the training usually becomes difficult. Table 3 compares the performance of shallow and deep networks. From the table we can see that for a normal LSTMP network, when it goes from 3 layers to 8 layers, the recognition performance degrades dramatically. For the highway network, however, the WER only increase a little bit. The table suggests that the highway connection between LSTM layers allows the network to go much deeper than the normal LSTM networks."}, {"heading": "6.4. Highway LSTMP with sequence training", "text": "We perform sequence discriminative training for the networks discussed in Section 4.2. Detailed results are shown in Table 4. The table suggests that introducing the highway connection between LSTMP layers is beneficial to sequence discriminative training. For example, without the highway connection, sequence training on top of the 3-layer LSTMP network brings WER from 50.7 down to 49.3, a relative improvement of only 3% on this particular task. After introducing the highway connection and dropout, the improvement is from 50.4 to 47.7, 5% relatively. The relative improvement is even larger on the non-overlapping segment subset, which is roughly 7%. The results suggest that sequence training is beneficial from both highway connection and a deeper structure."}, {"heading": "7. CONCLUSION", "text": "We presented a novel highway LSTM network, and applied it to a far-field speech recognition task. Experimental results suggest that this type of network consistently outperforms the normal (B)LSTMP networks, especially when dropout is applied to the highway connection to control the connection\u2019s on/off state. Further experiments also suggest that the highway connection allows the network to go much deeper, and to get larger benefit from sequence discriminative training.\n[1] G. E. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\u201d IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.\n[2] F. Seide, G. Li, and D. Yu, \u201cConversational speech transcription using context-dependent deep neural networks,\u201d in Proc. Annual Conference of International Speech Communication Association (INTERSPEECH), 2011, pp. 437\u2013440.\n[3] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, no. 6, pp. 82\u201397, 2012.\n[4] M. Seltzer, D. Yu, and Y. Q. Wang, \u201cAn investigation of deep neural networks for noise robust speech recognition,\u201d in Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.\n[5] P. Swietojanski, A. Ghoshal, and S. Renals, \u201cConvolutional neural networks for distant speech recognition,\u201d Signal Processing Letters, IEEE, vol. 21, no. 9, pp. 1120\u20131124, September 2014.\n[6] A. Graves, A. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.\n[7] A. Graves, N. Jaitly, and A. Mohamed, \u201cHybrid speech recognition with deep bidirectional LSTM,\u201d in Proc. IEEE Workshop on Automfatic Speech Recognition and Understanding (ASRU), 2013, pp. 273\u2013278.\n[8] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Fifteenth Annual Conference of the International Speech Communication Association, 2014.\n[9] P. Swietojanski, A. Ghoshal, and S. Renals, \u201cHybrid acoustic models for distant and multichannel large vocabulary speech recognition,\u201d in ASRU, 2013.\n[10] K. Kumatani, J. W. McDonough, and B. Raj, \u201cMicrophone array processing for distant speech recognition: From closetalking microphones to far-field sensors.\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 127\u2013140, 2012.\n[11] T. Hain, L. Burget, J. Dines, P. N. Garner, F. Grzl, A. E. Hannani, M. Huijbregts, M. Karafit, M. Lincoln, and V. Wan, \u201cTranscribing meetings with the amida systems.\u201d IEEE Transactions on Audio, Speech & Language Processing, vol. 20, no. 2, pp. 486\u2013498, 2012.\n[12] A. Stolcke, \u201cMaking the most from multiple microphones in meeting recognition,\u201d in ICASSP, 2011.\n[13] K. Chen, Z.-J. Yan, and Q. Huo, \u201cTraining deep bidirectional lstm acoustic model for lvcsr by a context-sensitive-chunk bptt approach,\u201d in Interspeech, 2015.\n[14] P. Swietojanski, A. Ghoshal, and S. Renals, \u201cConvolutional neural networks for distant speech recognition,\u201d IEEE Singal Processing Letters, vol. 21, no. 9, pp. 1120\u20131124, 2014.\n[15] D. Yu, A. Eversole, M. Seltzer, K. Yao, B. Guenter, O. Kuchaiev, Y. Zhang, F. Seide, G. Chen, H. Wang, J. Droppo,\nA. Agarwal, C. Basoglu, M. Padmilac, A. Kamenev, V. Ivanov, S. Cyphers, H. Parthasarathi, B. Mitra, Z. Huang, G. Zweig, C. Rossbach, J. Currey, J. Gao, A. May, B. Peng, A. Stolcke, M. Slaney, and X. Huang, \u201cAn introduction to computational networks and the computational network toolkit,\u201d Microsoft Technical Report, 2014.\n[16] R. Srivastava, K. Greff, and J. Schmidhuber, \u201cHighway networks,\u201d 2015. [Online]. Available: http://arxiv.org/abs/ 1505.00387\n[17] K. Yao, T. Cohn, K. Vylomova, K. Duh, and C. Dyer, \u201cDepth-gated lstm,\u201d 2015. [Online]. Available: http://arxiv. org/abs/1508.03790\n[18] N. Kalchbrenner, I. Danihelka, and A. Graves, \u201cGrid long short-term memory,\u201d 2015. [Online]. Available: http: //arXiv.org/abs/1507.01526\n[19] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, p. 17351438, 1997.\n[20] G. Heigold, E. McDermott, V. Vanhoucke, A. Senior, and M. Bacchiani, \u201cAsynchronous stochastic optimization for sequence training of deep neural networks,\u201d in ICASSP, 2014.\n[21] J. Carletta, \u201cunleashing the killer corpus: experiences in creating the multi-everything ami meeting corpus,\u201d Language Resources & Evaluation Journal, vol. 41, no. 2, pp. 181\u2013190, 2007.\n[22] J. Fiscus, J. Ajot, N. Radde, and C. Laprun, \u201cMultiple dimension levenshtein edit distance calculations for evaluating asr systems during simultaneous speech,\u201d in LREC, 2006.\n[23] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motl\u0131\u0301c\u030cek, Y. Qian, P. Schwarz, J. Silovsky\u0301, G. Stemmer, and K. Vesely\u0301, \u201cThe Kaldi speech recognition toolkit,\u201d in ASRU, 2011.\n[24] F. Seide, G. Li, X. Chen, and D. Yu, \u201cFeature engineering in context-dependent deep neural networks for conversational speech transcription,\u201d in Proc. IEEE Workshop on Automfatic Speech Recognition and Understanding (ASRU), 2011, pp. 24\u2013 29.\n[25] R. Williams and J. Peng, \u201cAn efficient gradient-based algorithm for online training of recurrent network trajectories,\u201d Neural Computation, vol. 2, p. 490501, 1990.\n[26] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d 2012. [Online]. Available: http://arxiv.org/abs/1207.0580"}], "references": [{"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH), 2011, pp. 437\u2013440.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "An investigation of deep neural networks for noise robust speech recognition", "author": ["M. Seltzer", "D. Yu", "Y.Q. Wang"], "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Signal Processing Letters, IEEE, vol. 21, no. 9, pp. 1120\u20131124, September 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A. Mohamed"], "venue": "Proc. IEEE Workshop on Automfatic Speech Recognition and Understanding (ASRU), 2013, pp. 273\u2013278.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "ASRU, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Microphone array processing for distant speech recognition: From closetalking microphones to far-field sensors.", "author": ["K. Kumatani", "J.W. McDonough", "B. Raj"], "venue": "IEEE Signal Process. Mag., vol. 29,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Transcribing meetings with the amida systems.", "author": ["T. Hain", "L. Burget", "J. Dines", "P.N. Garner", "F. Grzl", "A.E. Hannani", "M. Huijbregts", "M. Karafit", "M. Lincoln", "V. Wan"], "venue": "IEEE Transactions on Audio, Speech & Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Making the most from multiple microphones in meeting recognition", "author": ["A. Stolcke"], "venue": "ICASSP, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Training deep bidirectional lstm acoustic model for lvcsr by a context-sensitive-chunk bptt approach", "author": ["K. Chen", "Z.-J. Yan", "Q. Huo"], "venue": "Interspeech, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "IEEE Singal Processing Letters, vol. 21, no. 9, pp. 1120\u20131124, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide", "G. Chen", "H. Wang", "J. Droppo", "A. Agarwal", "C. Basoglu", "M. Padmilac", "A. Kamenev", "V. Ivanov", "S. Cyphers", "H. Parthasarathi", "B. Mitra", "Z. Huang", "G. Zweig", "C. Rossbach", "J. Currey", "J. Gao", "A. May", "B. Peng", "A. Stolcke", "M. Slaney", "X. Huang"], "venue": "Microsoft Technical Report, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway networks", "author": ["R. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "2015. [Online]. Available: http://arxiv.org/abs/ 1505.00387", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth-gated lstm", "author": ["K. Yao", "T. Cohn", "K. Vylomova", "K. Duh", "C. Dyer"], "venue": "2015. [Online]. Available: http://arxiv. org/abs/1508.03790", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "2015. [Online]. Available: http: //arXiv.org/abs/1507.01526", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, p. 17351438, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Asynchronous stochastic optimization for sequence training of deep neural networks", "author": ["G. Heigold", "E. McDermott", "V. Vanhoucke", "A. Senior", "M. Bacchiani"], "venue": "ICASSP, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "unleashing the killer corpus: experiences in creating the multi-everything ami meeting corpus", "author": ["J. Carletta"], "venue": "Language Resources & Evaluation Journal, vol. 41, no. 2, pp. 181\u2013190, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiple dimension levenshtein edit distance calculations for evaluating asr systems during simultaneous speech", "author": ["J. Fiscus", "J. Ajot", "N. Radde", "C. Laprun"], "venue": "LREC, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131\u0301\u010dek", "Y. Qian", "P. Schwarz", "J. Silovsk\u00fd", "G. Stemmer", "K. Vesel\u00fd"], "venue": "ASRU, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "Proc. IEEE Workshop on Automfatic Speech Recognition and Understanding (ASRU), 2011, pp. 24\u2013 29.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient gradient-based algorithm for online training of recurrent network trajectories", "author": ["R. Williams", "J. Peng"], "venue": "Neural Computation, vol. 2, p. 490501, 1990.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1990}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "2012. [Online]. Available: http://arxiv.org/abs/1207.0580", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recently the deep neural network (DNN)-based acoustic models (AMs) greatly improved automatic speech recognition (ASR) accuracy on many tasks [1, 2, 3, 4].", "startOffset": 142, "endOffset": 154}, {"referenceID": 1, "context": "Recently the deep neural network (DNN)-based acoustic models (AMs) greatly improved automatic speech recognition (ASR) accuracy on many tasks [1, 2, 3, 4].", "startOffset": 142, "endOffset": 154}, {"referenceID": 2, "context": "Recently the deep neural network (DNN)-based acoustic models (AMs) greatly improved automatic speech recognition (ASR) accuracy on many tasks [1, 2, 3, 4].", "startOffset": 142, "endOffset": 154}, {"referenceID": 3, "context": "Recently the deep neural network (DNN)-based acoustic models (AMs) greatly improved automatic speech recognition (ASR) accuracy on many tasks [1, 2, 3, 4].", "startOffset": 142, "endOffset": 154}, {"referenceID": 4, "context": "Further improvements were reported by using more advanced models such as convolutional neural networks (CNNs) [5] and long short-term memory (LSTM) recurrent neural networks (RNNs) [6, 7, 8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 5, "context": "Further improvements were reported by using more advanced models such as convolutional neural networks (CNNs) [5] and long short-term memory (LSTM) recurrent neural networks (RNNs) [6, 7, 8].", "startOffset": 181, "endOffset": 190}, {"referenceID": 6, "context": "Further improvements were reported by using more advanced models such as convolutional neural networks (CNNs) [5] and long short-term memory (LSTM) recurrent neural networks (RNNs) [6, 7, 8].", "startOffset": 181, "endOffset": 190}, {"referenceID": 7, "context": "Further improvements were reported by using more advanced models such as convolutional neural networks (CNNs) [5] and long short-term memory (LSTM) recurrent neural networks (RNNs) [6, 7, 8].", "startOffset": 181, "endOffset": 190}, {"referenceID": 8, "context": "Although these new techniques help to decrease the word error rate (WER) on distant speech recognition (DSR) [9], DSR remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques [10, 11, 12] and multi-pass decoding schemes.", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "Although these new techniques help to decrease the word error rate (WER) on distant speech recognition (DSR) [9], DSR remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques [10, 11, 12] and multi-pass decoding schemes.", "startOffset": 260, "endOffset": 272}, {"referenceID": 10, "context": "Although these new techniques help to decrease the word error rate (WER) on distant speech recognition (DSR) [9], DSR remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques [10, 11, 12] and multi-pass decoding schemes.", "startOffset": 260, "endOffset": 272}, {"referenceID": 11, "context": "Although these new techniques help to decrease the word error rate (WER) on distant speech recognition (DSR) [9], DSR remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques [10, 11, 12] and multi-pass decoding schemes.", "startOffset": 260, "endOffset": 272}, {"referenceID": 7, "context": "It is reported [8] that deep LSTM (DLSTM) RNNs help improve generalization and often outperform single-layer LSTM RNNs.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "It also trains and decodes faster than context-sensitive-chunk BLSTMs [13] which can only access limited past and future context.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "4% over CNNs [14], and 5.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "After developing the highway LSTMs independently we noticed that similar work has been done in [16, 17, 18].", "startOffset": 95, "endOffset": 107}, {"referenceID": 16, "context": "After developing the highway LSTMs independently we noticed that similar work has been done in [16, 17, 18].", "startOffset": 95, "endOffset": 107}, {"referenceID": 17, "context": "After developing the highway LSTMs independently we noticed that similar work has been done in [16, 17, 18].", "startOffset": 95, "endOffset": 107}, {"referenceID": 15, "context": "The highway networks proposed in [16] adaptively carry some dimensions of the input directly to the output so that information can flow across layers much more easily.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "The work in [17] share the same idea and model structure.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "[18] is more general and uses a generic form.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "1The tools and scripts used to produce the results reported in this paper are publicly available as part of the CNTK toolkit[15], and anyone with access to the data should be able to reproduce our results.", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "The LSTM RNN was initially proposed in [19] to solve the gradient diminishing problem in RNNs.", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "It is conjectured [8] that DLSTM RNNs can make better use of parameters by distributing them over the space through multiple layers.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "It is shown in [6, 7, 13] that bidirectional LSTN RNNs can indeed improve the speech recognition results.", "startOffset": 15, "endOffset": 25}, {"referenceID": 6, "context": "It is shown in [6, 7, 13] that bidirectional LSTN RNNs can indeed improve the speech recognition results.", "startOffset": 15, "endOffset": 25}, {"referenceID": 12, "context": "It is shown in [6, 7, 13] that bidirectional LSTN RNNs can indeed improve the speech recognition results.", "startOffset": 15, "endOffset": 25}, {"referenceID": 14, "context": "For unidirectional RNN models, to better utilize the parallelization power of the GPU card, in [15], multiple sequences (e.", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "One alternative way to speed up is using asynchronous SGD based on a GPU/CPU farm [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "To speed up the training of bi-direcctional RNNs, the Contextsensitive-chunk BPTT (CSC-BPTT) is proposed in [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "We evaluated our models on the AMI meeting corpus [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "NIST\u2019s asclite tool [22] is used for scoring.", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "Kaldi [23] is used for feature extraction, early stage triphone training as well as decoding.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "The Computational Network Toolkit (CNTK) [15] is used for neural network training.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "Our (H)LSTM models, unless explicitly stated otherwise, are added with a projection layer on top of each layer\u2019s output, as proposed in [8], and are trained with 80-dimensional log Mel filterbank (FBANK) features.", "startOffset": 136, "endOffset": 139}, {"referenceID": 23, "context": "All models are randomly initialized without either generative or discriminative pretraining [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "To train the unidirectional model, the truncated back-propagationthrough-time (BPTT) [25] is used to update the model parameters.", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "For frame level cross-entropy training, L2 constraint regularization [26] is used.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "Since we do not exclude the overlapping speech segments during model training, in addition to results on the full eval set, we also show results on a subset that only contains the non-overlapping speech segments as [14].", "startOffset": 215, "endOffset": 219}], "year": 2016, "abstractText": "In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains 43.9/47.7% WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with 15.7% and 5.3% relative improvement respectively.", "creator": "LaTeX with hyperref package"}}}