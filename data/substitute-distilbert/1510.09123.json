{"id": "1510.09123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Subsampling in Smoothed Range Spaces", "abstract": "we consider smoothed versions of geometric geometric pairs, so an element of either tangent set ( e. g. a point ) can vanish contained in a segment with a non - binary value in $ [ 0, 1 ] $. similar notions have been considered for subdivisions ; we extend them to more general types of ranges. we then consider approximations of these discrete spaces through $ \\ varepsilon $ - fields and $ \\ varepsilon $ - samples ( aka $ \\ varepsilon $ - approximations ). mathematical characterize when consistency bounds for $ \\ varepsilon $ - samples on kernels can go extended to these more general smoothed range spaces. we also create new generalizations for $ \\ basic $ - nets to correct range spaces and show when results in binary range spaces progressively carry over to these smoothed ones.", "histories": [["v1", "Fri, 30 Oct 2015 15:27:31 GMT  (349kb,D)", "http://arxiv.org/abs/1510.09123v1", "This is the full version of the paper which appeared in ALT 2015. 16 pages, 3 figures. In Algorithmic Learning Theory, pp. 224-238. Springer International Publishing, 2015"]], "COMMENTS": "This is the full version of the paper which appeared in ALT 2015. 16 pages, 3 figures. In Algorithmic Learning Theory, pp. 224-238. Springer International Publishing, 2015", "reviews": [], "SUBJECTS": "cs.CG cs.LG", "authors": ["jeff m phillips", "yan zheng"], "accepted": false, "id": "1510.09123"}, "pdf": {"name": "1510.09123.pdf", "metadata": {"source": "CRF", "title": "Subsampling in Smoothed Range Spaces", "authors": ["Jeff M. Phillips", "Yan Zheng"], "emails": ["jeffp@cs.utah.edu", "yanzheng@cs.utah.edu"], "sections": [{"heading": "1 Introduction", "text": "This paper considers traditional sample complexity problems but adapted to when the range space (or function space) smoothes out its boundary. This is important in various scenarios where either the data points or the measuring function is noisy. Similar problems have been considered in specific contexts of functions classes with a [0, 1] range or kernel density estimates. We extend and generalize various of these results, motivated by scenarios like the following.\n(S1) Consider maintaining a random sample of noisy spatial data points (say twitter users with geocoordinates), and we want this sample to include a witness to every large enough event. However, because the data coordinates are noisy we use a kernel density estimate to represent the density. And moreover, we do not want to consider regions with a single or constant number of data points which only occurred due to random variations. In this scenario, how many samples do we need to maintain?\n(S2) Next consider a large approximate (say high-dimensional image feature [1]) dataset, where we want to build a linear classifier. Because the features are approximate (say due to feature hashing techniques), we model the classifier boundary to be randomly shifted using Gaussian noise. How many samples from this dataset do we need to obtain a desired generalization bound?\n(S3) Finally, consider one of these scenarios in which we are trying to create an informative subset of the enormous full dataset, but have the opportunity to do so in ways more intelligent than randomly sampling. On such a reduced dataset one may want to train several types of classifiers, or to estimate the density of various subsets. Can we generate a smaller dataset compared to what would be required by random sampling?\nThe traditional way to study related sample complexity problems is through range spaces (a ground setX , and family of subsets A) and their associated dimension (e.g., VC-dimension [26]). We focus on a smooth extension of range spaces defined on a geometric ground set. Specifically, consider the ground set P to be a subset of points in Rd, and let A describe subsets defined by some geometric objects, for instance a halfspace or a ball. Points p \u2208 Rd that are inside the object (e.g., halfspace or ball) are typically assigned a value 1, and those outside a value 0. In our smoothed setting points near the boundary are given a value between 0 and 1, instead of discretely switching from 0 to 1.\nar X\niv :1\n51 0.\n09 12\n3v 1\n[ cs\n.C G\n] 3\n0 O\nct 2\n01 5\nIn learning theory these smooth range spaces can be characterized by more general notions called P - dimension [23] (or Pseudo dimension) or V -dimension [25] (or \u201cfat\u201d versions of these [2]) and can be used to learn real-valued functions for regression or density estimation, respectively.\nIn geometry and data structures, these smoothed range spaces are of interest in studying noisy data. Our work extends some recent work [13, 22] which examines a special case of our setting that maps to kernel density estimates, and matches or improves on related bounds for non-smoothed versions.\nMain contributions. We next summarize the main contributions in this paper.\n\u2022 We define a general class of smoothed range spaces (Sec 3.1), with application to density estimation and noisy agnostic learning, and we show that these can inherit sample complexity results from linked non-smooth range spaces (Corollary 4.1).\n\u2022 We define an (\u03b5, \u03c4)-net for a smoothed range space (Sec 3.3). We show how this can inherit sampling complexity bounds from linked non-smooth range spaces (Theorem 4.2), and we relate this to nonagnostic density estimation and hitting set problems.\n\u2022 We provide discrepancy-based bounds and constructions for \u03b5-samples on smooth range spaces requiring significantly fewer points than uniform sampling approaches (Theorems 6.2 and 6.4), and also smaller than discrepancy-based bounds on the linked binary range spaces. These are useful for batched active learning, where a prespecified batch of (not uniform at random) samples can be then asked for labels to be used for learning."}, {"heading": "2 Definitions and Background", "text": "Recall that we will focus on geometric range spaces (P,A) where the ground set P \u2282 Rd and the family of ranges A are defined by geometric objects. It is common to approximate a range space in one of two ways, as an \u03b5-sample (aka \u03b5-approximation) or an \u03b5-net. An \u03b5-sample for a range space (P,A) is a subset Q \u2282 P such that\nmax A\u2208A \u2223\u2223\u2223\u2223 |A \u2229 P ||P | \u2212 |Q \u2229A||Q| \u2223\u2223\u2223\u2223 \u2264 \u03b5.\nAn \u03b5-net of a range space (P,A) is a subset Q \u2282 P such that\nfor all A \u2208 A such that |P \u2229A| |P | \u2265 \u03b5 then A \u2229Q 6= \u2205.\nGiven a range space (P,A) where |P | = m, then \u03c0A(m) describes the maximum number of possible distinct subsets of P defined by some A \u2208 A. If we can bound, \u03c0A(m) \u2264 Cm\u03bd for absolute constant C, then (P,A) is said to have shatter dimension \u03bd. For instance the shatter dimension of H halfspaces in Rd is d, and for B balls in Rd is d + 1. For a range space with shatter dimension \u03bd, a random sample of size O((1/\u03b52)(\u03bd + log(1/\u03b4))) is an \u03b5-sample with probability at least 1 \u2212 \u03b4 [26, 15], and a random sample of size O((\u03bd/\u03b5) log(1/\u03b5\u03b4)) is an \u03b5-net with probability at least 1\u2212 \u03b4 [12, 19].\nAn \u03b5-sample Q is sufficient for agnostic learning with generalization error \u03b5, where the best classifier might misclassify some points. An \u03b5-net Q is sufficient for non-agnostic learning with generalization error \u03b5, where the best classifier is assumed to have no error on P .\nThe size bounds can be made deterministic and slightly improved for certain cases. An \u03b5-sampleQ can be made of size O(1/\u03b52\u03bd/(\u03bd+1)) [16] and this bound can be no smaller [17] in the general case. For balls B in Rd which have shatter-dimension \u03bd = d+1, this can be improved toO(1/\u03b52d/(d+1) logd/(d+1)(1/\u03b5)) [4, 17],\nand the best known lower bound is \u2126(1/\u03b52d/(d+1)). For axis-aligned rectangles R in Rd which have shatterdimension \u03bd = 2d, this can be improved to O((1/\u03b5) logd+1/2(1/\u03b5)) [14].\nFor \u03b5-nets, the general bound of O((\u03bd/\u03b5) log(1/\u03b5)) can also be made deterministic [16], and for halfspaces in R4 the size must be at least \u2126((1/\u03b5) log(1/\u03b5)) [20]. But for halfspaces in R3 the size can be O(1/\u03b5) [18, 11], which is tight. By a simple lifting, this also applies for balls in R2. For other range spaces, such as axis-aligned rectangles in R2, the size bound is \u0398((1/\u03b5) log log(1/\u03b5)) [3, 20]."}, {"heading": "2.1 Kernels", "text": "A kernel is a bivariate similarity function K : Rd \u00d7 Rd \u2192 R+, which can be normalized so K(x, x) = 1 (which we assume through this paper). Examples include ball kernels (K(x, p) = {1 if \u2016x \u2212 p\u2016 \u2264 1 and 0 otherwise}), triangle kernels (K(x, p) = max{0, 1 \u2212 \u2016x \u2212 p\u2016}), Epanechnikov kernels (K(x, p) = max{0, 1\u2212 \u2016x\u2212 p\u20162}), and Gaussian kernels (K(x, p) = exp(\u2212\u2016x\u2212 p\u20162), which is reproducing). In this paper we focus on symmetric, shift invariant kernels which depend only on z = \u2016x\u2212 p\u2016, and can be written as a single parameter function K(x, p) = k(z); these can be parameterized by a single bandwidth (or just width) parameter w so K(x, p) = kw(\u2016x\u2212 p\u2016/w).\nGiven a point set P \u2282 Rd and a kernel, a kernel density estimate KDEP is the convolution of that point set with K. For any x \u2208 Rd we define KDEP (x) = 1|P | \u2211 p\u2208P K(x, p).\nA kernel range space [13, 22] (P,K) is an extension of the combinatorial concept of a range space (P,A) (or to distinguish it we refer to the classic notion as a binary range space). It is defined by a point set P \u2282 Rd and a kernel K. An element Kx of K is a kernel K(x, \u00b7) applied at point x \u2208 Rd; it assigns a value in [0, 1] to each point p \u2208 P as K(x, p). If we use a ball kernel, then each value is exactly {0, 1} and we recover exactly the notion of a binary range space for geometric ranges defined by balls.\nThe notion of an \u03b5-kernel sample [13] extends the definition of \u03b5-sample. It is a subset Q \u2282 P such that\nmax x\u2208Rd\n|KDEP (x)\u2212 KDEQ(x)| \u2264 \u03b5.\nA binary range space (P,A) is linked to a kernel range space (P,K) if the set {p \u2208 P | K(x, p) \u2265 \u03c4} is equal to P \u2229 A for some A \u2208 A, for any threshold value \u03c4 . [13] showed that an \u03b5-sample of a linked range space (P,A) is also an \u03b5-kernel sample of a corresponding kernel range space (P,K). Since all range spaces defined by symmetric, shift-invariant kernels are linked to range spaces defined by balls, they inherit all \u03b5-sample bounds, including that random samples of size O((1/\u03b52)(d + log(1/\u03b4)) provide an \u03b5kernel sample with probability at least 1\u2212 \u03b4. Then [22] showed that these bounds can be improved through discrepancy-based methods to O(((1/\u03b5) \u221a log(1/\u03b5\u03b4))2d/(d+2)), which is O((1/\u03b5) \u221a log(1/\u03b5\u03b4)) in R2.\nA more general concept has been studied in learning theory on real-valued functions, where a function f as a member of a function class F describes a mapping from Rd to [0, 1] (or more generally R). A kernel range space where the linked binary range space has bounded shatter-dimension \u03bd is said to have bounded V-dimension [25] (see [2]) of \u03bd. Given a ground set X , then for (X,F) this describes the largest subset Y of X which can be shattered in the following sense. Choose any value s \u2208 [0, 1] for all points y \u2208 Y , and then for each subset of Z \u2282 Y there exists a function f \u2208 F so f(y) > s if y \u2208 Z and f(y) < s if y /\u2208 Z. The best sample complexity bounds for ensuring Q is an \u03b5-sample of P based on V-dimension are derived from a more general sort of dimension (called a P-dimension [23] where in the shattering definition, each y may have a distinct s(y) value) requires |Q| = O((1/\u03b52)(\u03bd+log(1/\u03b4))) [15]. As we will see, these V-dimension based results are also general enough to apply to the to-be-defined smooth range spaces."}, {"heading": "3 New Definitions", "text": "In this paper we extend the notion of a kernel range spaces to other smoothed range spaces that are \u201clinked\u201d with common range spaces, e.g., halfspaces. These inherent the construction bounds through the linking result of [13], and we show cases where these bounds can also be improved. We also extend the notion of \u03b5-nets to kernels and smoothed range spaces, and showing linking results for these as well."}, {"heading": "3.1 Smoothed Range Spaces", "text": "Here we will define the primary smoothed combinatorial object we will examine, starting with halfspaces, and then generalizing. Let Hw denote the family of smoothed halfspaces with width parameter w, and let (P,Hw) be the associated smoothed range space where P \u2282 Rd. Given a point p \u2208 P , then smoothed halfspace h \u2208 Hw maps p to a value vh(p) \u2208 [0, 1] (rather than the traditional {0, 1} in a binary range space).\nWe first describe a specific mapping to the function value vh(p) that will be sufficient for the development of most of our techniques. Let F be the (d \u2212 1)-flat defining the boundary of halfspace h. Given a point p \u2208 Rd, let pF = arg minq\u2208F \u2016p\u2212 q\u2016 describe the point on F closest to p. Now we define\nvh,w(p) =  1 p \u2208 h and \u2016p\u2212 pF \u2016 \u2265 w 1 2 + 1 2 \u2016p\u2212pF \u2016 w p \u2208 h and \u2016p\u2212 pF \u2016 < w 1 2 \u2212 1 2 \u2016p\u2212pF \u2016\nw p /\u2208 h and \u2016p\u2212 pF \u2016 < w 0 p /\u2208 h and \u2016p\u2212 pF \u2016 \u2265 w.\nThese points within a slab of width 2w surrounding F can take on a value between 0 and 1, where points outside of this slab revert back to the binary values of either 0 or 1.\nWe can make this more general using a shift-invariant kernel k(\u2016p\u2212x\u2016) = K(p, x), where kw(\u2016p\u2212x\u2016) = k(\u2016p\u2212 x\u2016/w) allows us to parameterize by w. Define vh,w(p) as follows.\nvh,w(p) = { 1\u2212 12kw(\u2016p\u2212 pF \u2016) p \u2208 h 1 2kw(\u2016p\u2212 pF \u2016) p /\u2208 h.\nFor brevity, we will omit the w and just use vh(p) when clear. These definitions are equivalent when using the triangle kernel. But for instance we could also use a Epanechnikov kernel or Gaussian kernel. Although the Gaussian kernel does not satisfy the restriction that only points in the width 2w slab take non {0, 1} values, we can use techniques from [22] to extend to this case as well. This is illustrated in Figure 1. Another property held by this definition which we will exploit is that the slope \u03c2 of these kernels is bounded by \u03c2 = O(1/w) = c/w, for some constant c; the constant c = 1/2 for triangle and Gaussian, and c = 1 for Epanechnikov.\nFinally, we can further generalize this by replacing the flat F at the boundary of h with a polynomial surface G. The point pG = arg minq\u2208G \u2016p\u2212 q\u2016 replaces pF in the above definitions. Then the slab of width 2w is replaced with a curved volume in Rd; see Figure 1. For instance, if G defines a circle in Rd, then vh defines a disc of value 1, then an annulus of width 2w where the function value decreases to 0. Alternatively, if G is a single point, then we essentially recover the kernel range space, except that the maximum height is 1/2 instead of 1. We will prove the key structural results for polynomial curves in Section 5, but otherwise focus on halfspaces to keep the discussion cleaner. The most challenging elements of our results are all contained in the case with F as a (d\u2212 1)-flat."}, {"heading": "3.2 \u03b5-Sample in a Smoothed Range Space", "text": "It will be convenient to extend the notion of a kernel density estimate to these smoothed range space. A smoothed density estimate SDEP is defined for any h \u2208 Hw as\nSDEP (h) = 1 |P | \u2211 p\u2208P vh(p).\nAn \u03b5-sample Q of a smoothed range space (P,Hw) is a subset Q \u2282 P such that\nmax h\u2208Hw\n|SDEP (h)\u2212 SDEQ(h)| \u2264 \u03b5.\nGiven such an \u03b5-sample Q, we can then consider a subset H\u0304w of Hw with bounded integral (perhaps restricted to some domain like a unit cube that contains all of the data P ). If we can learn the smooth range h\u0302 = arg maxh\u2208H\u0304w SDEQ(h), then we know SDEP (h\n\u2217) \u2212 SDEQ(h\u0302) \u2264 \u03b5, where h\u2217 = arg maxh\u2208H\u0304w SDEP (h), since SDEQ(h\u0302) \u2265 SDEQ(h\u2217) \u2265 SDEP (h\u2217) \u2212 \u03b5. Thus, such a set Q allows us to learn these more general density estimates with generalization error \u03b5.\nWe can also learn smoothed classifiers, like scenario (S2) in the introduction, with generalization error \u03b5, by giving points in the negative class a weight of \u22121; this requires separate (\u03b5/2)-samples for the negative and positive classes."}, {"heading": "3.3 (\u03b5, \u03c4)-Net in a Smoothed Range Space", "text": "We now generalize the definition of an \u03b5-net. Recall that it is a subset Q \u2282 P such that Q \u201chits\u201d all large enough ranges (|P \u2229 A|/|P | \u2265 \u03b5). However, the notion of \u201chitting\u201d is now less well-defined since a point q \u2208 Qmay be in a range but with value very close to 0; if a smoothed range space is defined with a Gaussian or other kernel with infinite support, any point q will have a non-zero value for all ranges! Hence, we need to introduce another parameter \u03c4 \u2208 (0, \u03b5), to make the notion of hitting more interesting in this case.\nA subset Q \u2282 P is an (\u03b5, \u03c4)-net of smoothed range space (P,Hw) if for any smoothed range h \u2208 Hw such that SDEP (h) \u2265 \u03b5, then there exists a point q \u2208 Q such that vh(q) \u2265 \u03c4 .\nThe notion of \u03b5-net is closely related to that of hitting sets. A hitting set of a binary range space (P,A) is a subset Q \u2282 P so every A \u2208 A (not just the large enough ones) contains some q \u2208 Q. To extend these notions to the smoothed setting, we again need an extra parameter \u03c4 \u2208 (0, \u03b5), and also need to only consider large enough smoothed ranges, since there are now an infinite number even if P is finite. A subset Q \u2282 P is an (\u03b5, \u03c4)-hitting set of smoothed range space (P,Hw) if for any h \u2208 Hw such that SDEP (h) \u2265 \u03b5, then SDEQ(h) \u2265 \u03c4 .\nIn the binary range space setting, an \u03b5-netQ of a range space (P,A) is sufficient to learn the best classifier on P with generalization error \u03b5 in the non-agnostic learning setting, that is assuming a perfect classifier\nexists on P from A. In the density estimation setting, there is not a notion of a perfect classifier, but if we assume some other properties of the data, the (\u03b5, \u03c4)-net will be sufficient to recover them. For instance, consider (like scenario (S1) in the introduction) that P is a discrete distribution so for some \u201cevent\u201d points p \u2208 P , there is at least an \u03b5-fraction of the probability distribution describing P at p (e.g., there are more than \u03b5|P | points very close to p). In this setting, we can recover the location of these points since they will have probability at least \u03c4 in the (\u03b5, \u03c4)-net Q."}, {"heading": "4 Linking and Properties of (\u03b5, \u03c4)-Nets", "text": "First we establish some basic connections between \u03b5-sample, (\u03b5, \u03c4)-net, and (\u03b5, \u03c4)-hitting set in smoothed range spaces. In binary range spaces an \u03b5-sample Q is also an \u03b5-net, and a hitting set is also an \u03b5-net; we show a similar result here up to the covering constant \u03c4 .\nLemma 4.1. For a smoothed range space (P,Hw) and 0 < \u03c4 < \u03b5 < 1, an (\u03b5, \u03c4)-hitting set Q is also an (\u03b5, \u03c4)-net of (P,Hw).\nProof. The (\u03b5, \u03c4)-hitting set property establishes for all h \u2208 Hw with SDEP (h) \u2265 \u03b5, then also SDEQ(h) \u2265 \u03c4 . Since SDEQ(h) = 1|Q| \u2211 q\u2208Q vh(q) is the average over all points q \u2208 Q, then it implies that at least one point also satisfies vh(q) \u2265 \u03c4 . Thus Q is also an (\u03b5, \u03c4)-net.\nIn the other direction an (\u03b5, \u03c4)-net is not necessarily an (\u03b5, \u03c4)-hitting set since the (\u03b5, \u03c4)-netQmay satisfy a smoothed range h \u2208 Hw with a single point q \u2208 Q such that vh(q) \u2265 \u03c4 , but all others q\u2032 \u2208 Q \\ {q} having vh(q \u2032) \u03c4 , and thus SDEQ(h) < \u03c4 .\nTheorem 4.1. For 0 < \u03c4 < \u03b5 < 1, an (\u03b5 \u2212 \u03c4)-sample Q in smoothed range space (P,Hw) is an (\u03b5, \u03c4)hitting set in (P,Hw), and thus also an (\u03b5, \u03c4)-net of (P,Hw).\nProof. Since Q is the (\u03b5 \u2212 \u03c4)-sample in the smoothed range space, for any smoothed range h \u2208 Hw we have |SDEP (h)\u2212 SDEQ(h)| \u2264 \u03b5\u2212 \u03c4 . We consider the upper and lower bound separately.\nIf SDEP (h) \u2265 \u03b5, when SDEP (h) \u2265 SDEQ(h), we have\nSDEQ(h) \u2265 SDEP (h)\u2212 (\u03b5\u2212 \u03c4) \u2265 \u03b5\u2212 (\u03b5\u2212 \u03c4) = \u03c4.\nAnd more simply when SDEQ(h) \u2265 SDEP (h) and SDEP (h) \u2265 \u03b5 \u2265 \u03c4 , then SDEQ(h) \u2265 \u03c4 . Thus in both situations, Q is an (\u03b5, \u03c4)-hitting set of (P,Hw). And then by Lemma 4.1 Q is also an (\u03b5, \u03c4)-net of (P,Hw)."}, {"heading": "4.1 Relations between Smoothed Range Spaces and Linked Binary Range Spaces", "text": "Consider a smoothed range space (P,Hw), and for one smoothed range h \u2208 Hw, examine the range boundary F (e.g. a (d \u2212 1)-flat, or polynomial surface) along with a symmetric, shift invariant kernel K that describes vh. The superlevel set (vh)\u03c4 is all points x \u2208 Rd such that vh(x) \u2265 \u03c4 . Then recall a smoothed range space (P,Hw) is linked to a binary range space (P,A) if every set {p \u2208 P | vh(p) \u2265 \u03c4} for any h \u2208 Hw and any \u03c4 > 0, is exactly the same as some range A \u2229 P for A \u2208 A. For smoothed range spaces defined by halfspaces, then the linked binary range space is also defined by halfspaces. For smoothed range spaces defined by points, mapping to kernel range spaces, then the linked binary range spaces are defined by balls.\nJoshi et al. [13] established that given a kernel range space (P,K), a linked binary range space (P,A), and an \u03b5-sampleQ of (P,A), thenQ is also an \u03b5-kernel sample of (P,K). An inspection of the proof reveals the same property holds directly for smoothed range spaces, as the only structural property needed is that\nall points p \u2208 P , as well as all points q \u2208 Q, can be sorted in decreasing function value K(p, x), where x is the center of the kernel. For smoothed range space, this can be replaced with sorting by vh(p).\nCorollary 4.1 ([13]). Consider a smoothed range space (P,Hw), a linked binary range space (P,A), and an \u03b5-sample Q of (P,A) with \u03b5 \u2208 (0, 1). Then Q is an \u03b5-sample of (P,Hw).\nWe now establish a similar relationship to (\u03b5, \u03c4)-nets of smoothed range spaces from (\u03b5 \u2212 \u03c4)-nets of linked binary range spaces.\nTheorem 4.2. Consider a smoothed range space (P,Hw), a linked binary range space (P,A), and an (\u03b5\u2212 \u03c4)-net Q of (P,A) for 0 < \u03c4 < \u03b5 < 1. Then Q is an (\u03b5, \u03c4)-net of (P,Hw).\nProof. Let |P | = n. Then since Q is an (\u03b5\u2212 \u03c4)-net of (P,A), for any range A \u2208 A, if |P \u2229A| \u2265 (\u03b5\u2212 \u03c4)n, then Q \u2229A 6= \u2205.\nSuppose h \u2208 Hw has SDEP (h) \u2265 \u03b5 and we want to establish that SDEQ(h) \u2265 \u03c4 . Let A \u2208 A be the range such that (\u03b5 \u2212 \u03c4)n points with largest vh(pi) values are exactly the points in A. We now partition P into three parts (1) let P1 be the (\u03b5\u2212 \u03c4)n\u2212 1 points with largest vh values, (2) let y be the point in P with (\u03b5 \u2212 \u03c4)nth largest vh value, and (3) let P2 be the remaining n \u2212 n(\u03b5 \u2212 \u03c4) points. Thus for every p1 \u2208 P1 and every p2 \u2208 P2 we have vh(p2) \u2264 vh(y) \u2264 vh(p1) \u2264 1.\nNow using our assumption n \u00b7 SDEP (h) \u2265 n\u03b5 we can decompose the sum n \u00b7 SDEP (h) = \u2211 p1\u2208P1 vh(p1) + vh(y) + \u2211 p2\u2208P2 vh(p2) \u2265 n\u03b5,\nand hence using upper bounds vh(p1) \u2264 1 and vh(p2) \u2264 vh(y), vh(y) \u2265 n\u03b5\u2212 \u2211 p1\u2208P1 vh(p1)\u2212 \u2211 p2\u2208P2 vh(p2)\n\u2265 n\u03b5\u2212 (n(\u03b5\u2212 \u03c4)\u2212 1) \u00b7 1\u2212 (n\u2212 n(\u03b5\u2212 \u03c4))vh(y).\nSolving for vh(y) we obtain\nvh(y) \u2265 n\u03c4 + 1 n\u2212 n(\u03b5\u2212 \u03c4) + 1 \u2265 n\u03c4 n\u2212 n(\u03b5\u2212 \u03c4) \u2265 n\u03c4 n = \u03c4.\nSince (P,A) is linked to (P,Hw), there exists a range A \u2208 A that includes precisely P1 \u222a y (or more points with the same vh(y) value as y). Because Q is an (\u03b5 \u2212 \u03c4)-net of (P,A), Q contains at least one of these points, lets call it q. Since all of these points have function value vh(p) \u2265 vh(y) \u2265 \u03c4 , then vh(q) \u2265 \u03c4 . Hence Q is also an (\u03b5, \u03c4)-net of (P,Hw), as desired.\nThis implies that if \u03c4 \u2264 c\u03b5 for any constant c < 1, then creating an (\u03b5, \u03c4)-net of a smoothed range space, with a known linked binary range space, reduces to computing an \u03b5-net for the linked binary range space. For instance any linked binary range space with shatter-dimension \u03bd has an \u03b5-net of size O(\u03bd\u03b5 log 1 \u03b5 ), including halfspaces in Rd with \u03bd = d and balls in Rd with \u03bd = d+1; hence there exists (\u03b5, \u03b5/2)-nets of the same size. For halfspaces in R2 or R3 (linked to smoothed halfspaces) and balls in R2 (linked to kernels), the size can be reduced to O(1/\u03b5) [18, 11, 24]."}, {"heading": "5 Min-Cost Matchings within Cubes", "text": "Before we proceed with our construction for smaller \u03b5-samples for smoothed range spaces, we need to prepare some structural results about min-cost matchings. Following some basic ideas from [22], these matchings will be used for discrepancy bounds on smoothed range spaces in Section 6.\nIn particular, we analyze some properties of the interaction of a min-cost matching M and some basic shapes ([22] considered only balls). Let P \u2282 Rd be a set of 2n points. A matchingM(P ) is a decomposition of P into n pairs {pi, qi} where pi, qi \u2208 P and each pi (and qi) is in exactly one pair. A min-cost matching is the matchingM that minimizes cost1(M,P ) = \u2211n i=1 \u2016pi\u2212qi\u2016. The min-cost matching can be computed in O(n3) time by [9] (using an extension of the Hungarian algorithm from the bipartite case). In R2 it can be calculated in O(n3/2 log5 n) time [27].\nFollowing [22], again we will base our analysis on a result of [5] which says that if P \u2282 [0, 1]d (a unit cube) then for d a constant, costd(M,P ) = \u2211n i=1 \u2016pi \u2212 qi\u2016d = O(1), where M is the min-cost matching. We make no attempt to optimize constants, and assume d is constant. One simple consequence, is that ifP is contained in a d-dimensional cube of side length `, then costd(M,P ) =\u2211n i=1 \u2016pi \u2212 qi\u2016d = O(`d). We are now interested in interactions with a matching M for P in a d-dimensional cube of side length `\nC`,d (call this shape an (`, d)-cube), and more general objects; in particular Cw a (w, d)-cube and, Sw a slab of width 2w, both restricted to be within C`,d. Now for such an object Ow (which will either be Cw or Sw) and an edge {p, q} where line segment pq intersects Ow define point pB (resp. qB) as the point on segment pq inside Ow closest to p (resp. q). Note if p (resp. q) is inside O then pB = p (resp. qB = q), otherwise it is on the boundary of Ow. For instance, see C20w in Figure 2.\nDefine the length of a matching M restricted to an object Ow \u2282 Rd as\n\u03c1(Ow,M) = \u2211\n(q,p)\u2208M\nmin { (2w)d, \u2016pB \u2212 qB\u2016d } .\nNote this differs from a similar definition by [22] since that case did not need to consider when both p and q were both outside of Ow, and did not need the min{(2w)d, . . .} term because all objects had diameter 2.\nLemma 5.1. Let P \u2282 C`,d, where d is constant, andM be its min-cost matching. For any (w, d)-cube Cw \u2282 C`,d we have \u03c1(Cw,M) = O(wd).\nProof. We cannot simply apply the result of [5] since we do not restrict that P \u2282 Cw. We need to consider cases where either p or q or both are outside of Cw. As such, we have three types of edges we consider, based on a cube C20w of side length 20w and with center the same as Cw.\n(T1) Both endpoints are within C20w of edge length at most \u221a d20w.\n(T2) One endpoint is in Cw, the other is outside C20w.\n(T3) Both endpoints are outside C20w.\nFor all (T1) edges, the result of Bern and Eppstein can directly bound their contribution to \u03c1(Cw,M) as O(wd) (scale to a unit cube, and rescale). For all (T2) edges, we can also bound their contribution to \u03c1(Cw,M) as O(wd), by extending an analysis of [22] when both Cw and C20w are similarly proportioned balls. This analysis shows there are O(1) such edges.\nWe now consider the case of (T3) edges, restricting to those that also intersect Cw. We argue there can be at most O(1) of them. In particular consider two such edges {p, q} and {p\u2032, q\u2032}, and their mappings to the boundary of C20w as pB, qB, p\u2032B, q \u2032 B; see Figure 2. If \u2016pB \u2212 p\u2032B\u2016 \u2264 10w and \u2016qB \u2212 q\u2032B\u2016 \u2264 10w, then we argue next that this cannot be part of a min-cost matching since \u2016p\u2212 p\u2032\u2016+ \u2016q\u2212 q\u2032\u2016 < \u2016p\u2212 q\u2016+ \u2016p\u2032\u2212 q\u2032\u2016, and it would be better to swap the pairing. Then it follows from the straight-forward net argument below that there can be at most O(1) such pairs.\nWe first observe that \u2016pB \u2212 p\u2032B\u2016+ \u2016qB \u2212 q\u2032B\u2016 \u2264 10w+ 10w < 20w+ 20w \u2264 \u2016pB \u2212 qB\u2016+ \u2016p\u2032B \u2212 q\u2032B\u2016. Now we can obtain our desired inequality using that \u2016p \u2212 q\u2016 = \u2016p \u2212 pB\u2016 + \u2016pB \u2212 qB\u2016 + \u2016qB \u2212 q\u2016 (and\nsimilar for \u2016p\u2032 \u2212 q\u2032\u2016) and that \u2016p\u2212 p\u2032\u2016 \u2264 \u2016p\u2212 pB\u2016+ \u2016pB \u2212 p\u2032B\u2016+ \u2016p\u2032B \u2212 p\u2032\u2016 by triangle inequality (and similar for \u2016q \u2212 q\u2032\u2016).\nNext we describe the net argument that there can be at most O(d2 \u00b7 22d) = O(1) such pairs with \u2016pB \u2212 p\u2032B\u2016 > 10w and \u2016qB \u2212 q\u2032B\u2016 > 10w. First place a 5w-net Nf on each (d\u2212 1)-dimensional face f of C20w so that any point x \u2208 f is within 5w of some point \u03b7 \u2208 Nf . We can construct Nf of size O(2d) with a simple grid. Then let N = \u22c3 f Nf as the union of the nets on each face; its size is O(d \u00b7 2d). Now for any point p /\u2208 C20w let \u03b7(p) = arg min\u03b7\u2208N \u2016pB \u2212 \u03b7\u2016 be the closest point in N to pB . If two points p and p\u2032 have \u03b7(p) = \u03b7(p\u2032) then \u2016p\u2212 p\u2032\u2016 \u2264 10w. Hence there can be at most O((d \u00b7 2d)2) edges with {p, q} mapping to unique \u03b7(p) and \u03b7(q) if no other edge {p\u2032, q\u2032} has \u2016pB \u2212 p\u2032B\u2016 \u2264 10w and \u2016qB \u2212 q\u2032B\u2016 \u2264 10w.\nConcluding, there can be at most O(d2 \u00b7 22d) = O(1) edges in M of type (T3), and the sum of their contribution to \u03c1(Cw,M) is at most O(wd), completing the proof.\nLemma 5.2. Let P \u2282 C`,d, where d is constant, and let M be its min-cost matching. For any width 2w slab Sw restricted to C`,d we have \u03c1(Sw,M) = O(`d\u22121w).\nProof. We can cover the slab Sw with O((`/w)d\u22121) (w, d)-cubes. To make this concrete, we cover C`,d with d`/wed cubes on a regular grid. Then in at least one basis direction (the one closest to orthogonal to the normal of F ) any column of cubes can intersect Sw in at most 4 cubes. Since there are d`/wed\u22121 such columns, the bound holds. Let Cw be the set of these cubes covering Sw.\nRestricted to any one such cube Cw, the contribution of those edges to \u03c1(Sw,M) is at most O(wd) by Lemma 5.1. Now we need to argue that we can just sum the effect of all covering cubes. The concern is that an edge goes through many cubes, only contributing a small amount to each \u03c1(Cw,M) term, but when the total length is taken to the dth power it is much more. However, since each edge\u2019s contribution is capped at (2w)2, we can say that if any edge goes through more than O(1) cubes, its length must be at least w, and its contribution in one such cube is already \u2126(w), so we can simply inflate the effect of each cube towards \u03c1(Sw,M) by a constant.\nIn particular, consider any edge pq that has p \u2208 Cw. Each cube has 3d \u2212 1 neighboring cubes, including through vertex incidence. Thus if edge pq passes through more than 3d cubes, q must be in a cube that is not one of Cw\u2019s neighbors. Thus it must have length at least w; and hence its length in at least one cube Cw must be at least w/3d, with its contribution to \u03c1(Cw,M) > wd/(3d 2 ). Thus we can multiply the effect of each edge in \u03c1(Cw,M) by 3d 2 2d = O(1) and be sure it is at least as large as the effect of that edge in \u03c1(Sw,M). Hence\n\u03c1(Sw,M) \u2264 3d 2 2d \u2211 Cw\u2208Cw \u03c1(Cw,M) \u2264 O(1) \u2211 Cw\u2208Cw O(wd)\n= O((`/w)d\u22121) \u00b7O(wd) = O(`d\u22121w).\nWe can apply the same decomposition as used to prove Lemma 5.2 to also prove a result for aw-expanded volume Gw around a degree g polynomial surface G. A degree g polynomial surface can intersect a line at most g times, so for some C`,d the expanded surface Gw \u2229 C`,d can be intersected by O(g(`/w)d\u22121) (w, d)-cubes. Hence we can achieve the following bound.\nCorollary 5.1. Let P \u2282 C`,d, where d is constant, and let M be its min-cost matching. For any volume Gw defined by a polynomial surface of degree g expanded by a width w, restricted to C`,d we have \u03c1(Gw,M) = O(g`d\u22121w)."}, {"heading": "6 Constructing \u03b5-Samples for Smoothed Range Spaces", "text": "In this section we build on the ideas from [22] and the new min-cost matching results in Section 5 to produce new discrepancy-based \u03b5-sample bounds for smoothed range spaces. The basic construction is as follows. We create a min-cost matching M on P , then for each pair (p, q) \u2208 M , we retain one of the two points at random, halving the point set. We repeat this until we reach our desired size. This should not be unfamiliar to readers familiar with discrepancy-based techniques for creating \u03b5-samples of binary range spaces [17, 6]. In that literature similar methods exist for creating matchings \u201cwith low-crossing number\u201d. Each such matching formulation is specific to the particular combinatorial range space one is concerned with. However, in the case of smoothed range spaces, we show that the min-cost matching approach is a universal algorithm. It means that an \u03b5-sampleQ for one smoothed range space (P,Hw) is also an \u03b5-sample for any other smoothed range space (P,H\u2032w), perhaps up to some constant factors. We also show how these bounds can sometimes improve upon \u03b5-sample bounds derived from linked range spaces; herein the parameter w will play a critical role."}, {"heading": "6.1 Discrepancy for Smoothed Halfspaces", "text": "To simplify arguments, we first consider P \u2282 R2 extending to Rd in Section 6.5. Let \u03c7 : P \u2192 {\u22121,+1} be a coloring of P , and define the discrepancy of (P,Hw) with coloring \u03c7\nas disc\u03c7(P,Hw) = maxh\u2208Hw | \u2211\np\u2208P \u03c7(p)vh(p)|. Restricted to one smoothed range h \u2208 Hw this is disc\u03c7(P, h) = | \u2211 p\u2208P \u03c7(p)vh(p)|. We construct a coloring \u03c7 using the min-cost matching M of P ; for each {pi, qi} \u2208 M we randomly select one of pi or qi to have \u03c7(pi) = +1, and the other \u03c7(qi) = \u22121. We next establish bounds on the discrepancy of this coloring for a \u03c2-bounded smoothed range space (P,Hw), i.e., where the gradient of vh is bounded by \u03c2 \u2264 c1/w for a constant c1 (see Section 3.1).\nFor any smoothed range h \u2208 Hw, we can now define a random variableXj = \u03c7(pj)vh(pj)+\u03c7(qj)vh(qj) for each pair {pj , qj} in the matching M . This allows us to rewrite disc\u03c7(P, h) = | \u2211 j Xj |. We can also define a variable \u2206j = 2|vh(pj) \u2212 vh(qj)| such that Xj \u2208 {\u2212\u2206j/2,\u2206j/2}. Now following the key insight from [22] we can bound \u2211 j \u2206 2 j using results from Section 5, which shows up in the following Chernoff bound from [8]: Let {X1, X2, . . .} be independent random variables with E[Xj ] = 0 and Xj = {\u2212\u2206j/2,\u2206j/2} then\nPr [ disc\u03c7(P, h) \u2265 \u03b1 ] = Pr [\u2223\u2223\u2223\u2211 j Xj \u2223\u2223\u2223 \u2265 \u03b1] \u2264 2 exp( \u22122\u03b12\u2211 j \u2206 2 j ) . (6.1)\nLemma 6.1. Assume P \u2282 R2 is contained in some cube C`,2 and with min-cost matching M defining \u03c7, and consider a \u03c2-bounded smoothed halfspace h \u2208 Hw associated with slab Sw. Let \u03c1(Sw,M) \u2264 c2(`w)\nfor constant c2 (see definition of \u03c1 in Section 5). Then Pr [ disc\u03c7(P, h) > C \u221a ` w log(2/\u03b4) ] \u2264 \u03b4 for any \u03b4 > 0 and constant C = c1 \u221a 2c2.\nProof. Using the gradient of vh is at most \u03c2 = c1/w and |vh(pj) \u2212 vh(qj)| \u2264 \u03c2 max{2w, \u2016pj \u2212 qj\u2016} we have \u2211\nj \u22062j = \u2211 j 4(vh(pj)\u2212 vh(qj))2 \u2264 4\u03c22\u03c1(Sw,M) \u2264 4c21/w2 \u00b7 c2`w = 4c21c2`/w,\nwhere the second inequality follows by Lemma 5.2 which shows that \u03c1(Sw,M) = \u2211\nj max{(2w)2, \u2016pj \u2212 qj\u20162} \u2264 c2(`w).\nWe now study the random variable disc\u03c7(P, h) = | \u2211\niXi| for a single h \u2208 Hw. Invoking (6.1) we can bound Pr[disc\u03c7(P, h) > \u03b1] \u2264 2 exp(\u2212\u03b12/(2c21c2`/w)). Setting C = c1 \u221a 2c2 and \u03b1 = C \u221a ` w log(2/\u03b4)\nreveals Pr [ disc\u03c7(P, h) > C \u221a ` w log(2/\u03b4) ] \u2264 \u03b4."}, {"heading": "6.2 From a Single Smoothed Halfspace to a Smoothed Range Space", "text": "The above theorems imply small discrepancy for a single smoothed halfspace h \u2208 Hw, but this does not yet imply small discrepancy disc\u03c7(P,Hw), for all choices of smoothed halfspaces simultaneously. And in a smoothed range space, the family Hw is not finite, since even if the same set of points have vh(p) = 1, vh(p) = 0, or are in the slab Sw, infinitesimal changes of h will change SDEP (h). So in order to bound disc\u03c7(P,Hw), we will show that there are polynomial in n number of smoothed halfspaces that need to be considered, and then apply a union bound across this set.\nTheorem 6.1. For P \u2282 R2 of size n, for Hw, and value \u03a8(n, \u03b4) = O (\u221a ` w log n \u03b4 ) for \u03b4 > 0, we can\nchoose a coloring \u03c7 such that Pr[disc\u03c7(P,Hw) > \u03a8(n, \u03b4)] \u2264 \u03b4.\nProof. We define a net of smoothed halfspaces S\u03b1 \u2282 Hw where any smoothed halfspace h \u2208 Hw assigns a value vh(p) to a point p \u2208 P , then there always exists a smoothed halfspace s \u2208 S\u03b1 such that \u2200p\u2208P |vh(p)\u2212 vs(p)| \u2264 \u03b1\u03c2 . Since there are only |P | = n points, the difference \u2211 p\u2208P |vh(p) \u2212 vs(p)| is no more than n\u03b1\u03c2 . By setting \u03b1 = 1/n\u03c2 we can ensure that disc\u03c7(P, h) < disc\u03c7(P, s) + 1. Thus if all s \u2208 S\u03b1 have small discrepancy, then all smoothed halfspaces in Hw have small discrepancy.\nWe now describe a construction of S\u03b1 (illustrated in Figure 3) of size at most O(n4) and then apply the union bound in Lemma 6.1 to only increase the discrepancy in that bound by a \u221a log n factor. First consider the halfspace with boundary passing through each pair of points p, p\u2032 \u2208 P . For each such halfspace, and for each point (p or p\u2032) it passes through, consider 4w/\u03b1 rotations around that point (wlog p). Make the increment of the rotation such that the closest point p\u2032F on the rotated boundary F increases a distance \u2016p\u2032 \u2212 p\u2032F \u2016 of \u03b1/2 in each next rotation. That is, the projection distance \u2016p\u2032 \u2212 p\u2032F \u2016 on each rotation around p is a distance of \u03b1/2, \u03b1, 3\u03b1/2, ..., 2w; this is repeated in each direction. Now, for each rotated halfspace, consider 4w/\u03b1 translations in the direction normal to the halfspace. There are 2w/\u03b1 translations in the normal direction, and its opposite, at increments of \u03b1/2 (e.g., \u03b1/2, \u03b1, 3\u03b1/2, ... 2w).\nSince \u03b1 = 1/n\u03c2 and w = O(1/\u03c2), then 4w/\u03b1 = O(n). Thus the size of S\u03b1 is O(n4): for each of O(n2) pairs, there are O(n) rotations and for each rotations there are O(n) translations.\nWe now show for any h \u2208 Hw how to map to the smoothed halfspace in s \u2208 S\u03b1 such that for all p \u2208 P that |vh(p)\u2212 vs(p)| \u2264 \u03b1\u03c2 . First consider all points P \u2229 Sw, where Sw is the slab defined by h. If the slab is empty then the closest two points p, p\u2032 \u2208 P would generate one translation and rotation s \u2208 S\u03b1 that moved both of them out of the slab, causing all of the same values vh(pi) = vs(pi) \u2208 {0, 1}. Otherwise, for any point p in the slab, there exists some rotation moving pF by at most \u03b1/2 and another rotation moving pF by at most \u03b1/2 resulting in |vh(p)\u2212vs(p)| \u2264 \u2016p\u2212pF \u2016\u00b7\u03c2 \u2264 (\u03b1/2+\u03b1/2) \u00b7\u03c2 = \u03b1\u03c2 . However, we need to ensure this holds for all points simultaneously. The translations affect \u2016p \u2212 pF \u2016 for all points the same (at most \u03b1/2), but the rotations can affect further away points by more. Thus, we choose the two points p, p\u2032 \u2208 S\u03b1 that maximize \u2016pF \u2212 p\u2032F \u2016, and consider the closest rotation of h to one of the smoothed halfspaces s \u2208 S\u03b1 that they generate. The rotation will affect all other points less than it will those two, and thus at most \u03b1/2, as desired.\nFinally we set the probability of failure in Lemma 6.1 as \u03b4\u2032 = \u2126(\u03b4/|S\u03b1|) for each smoothed halfspace. This implies that for \u03a8(n, \u03b4) = C \u221a\n` w log(2/\u03b4 \u2032) = O (\u221a ` w log n \u03b4 ) , the Pr[disc\u03c7(P,Hw) > \u03a8(n, \u03b4)] \u2264\n\u03b4."}, {"heading": "6.3 \u03b5-Samples for Smoothed Halfspaces", "text": "To transform this discrepancy algorithm to \u03b5-samples, let f(n) = disc\u03c7(P,Hw)/n be the value of \u03b5 in the \u03b5-samples generated by a single coloring of a set of size n. Solving for n in terms of \u03b5, the sample size\nis s(\u03b5) = O(1\u03b5 \u221a ` w log ` w\u03b5\u03b4 ). We can then apply the MergeReduce framework [7]; iteratively apply this random coloring in O(log n) rounds on disjoint subsets of size O(s(\u03b5)). Using a generalized analysis (c.f., Theorem 3.1 in [21]), we have the same \u03b5-sample size bound.\nTheorem 6.2. For P \u2282 C`,2 \u2282 R2, with probability at least 1\u2212\u03b4, we can construct an \u03b5-sample of (P,Hw) of size O ( 1 \u03b5 \u221a ` w log ` w\u03b5\u03b4 ) .\nTo see that these bounds make rough sense, consider a random point set P in a unit square (so ` = 1). Then setting w = 1/n will yield roughly O(1) points in the slab (and should roughly revert to the nonsmoothed setting); this leads to disc\u03c7(P,Hw) = O( \u221a n \u221a log(n/\u03b4)) and an \u03b5-sample of sizeO((1/\u03b52) \u221a\nlog(1/\u03b5\u03b4)), basically the random sampling bound. But settingw = \u03b5 so about \u03b5n points are in the slab (the same amount of error we allow in an \u03b5-sample) yields disc\u03c7(P,Hw) = O((1/ \u221a \u03b5n) \u00b7 \u221a log(n/\u03b4)) and the size of the \u03b5-\nsample to be O(1\u03b5 \u221a\nlog(1/\u03b5\u03b4)), which is a large improvement over O(1/\u03b54/3), and the best bound known for non-smoothed range spaces [17]."}, {"heading": "6.4 Adaptive Bounds for Non-Uniform Distributed Data", "text": "However, the assumption that P \u2282 C`,2 (although not uncommon [17]) can be restrictive. In this section, we attempt to relax this assumption. We do not see how to completely remove some such assumption using our suite of techniques since it could be all of the data lies very close to a line l, and then a halfspace boundary similar to that line l will have all of the points within the slab. In this case, we should not expect much better than with binary range spaces unless we make w much larger than the average deviation of points from the line l.\nHowever, we can do better, if the data is \u201cwell-clustered\u201d. That is, consider partitioning the data into subsets P1, P2, . . . , Pk so that each Pi is contained in an (`k, 2)-cube. Then we can replace ` in the previous bound with a value \u03a6k = max{k \u2212 1, k \u00b7 `k}. In particular, let \u03a6 = mink\u22651 max \u03a6k. We can then bound the contribution of each (`k, 2) cube towards \u03c1(Sw,M) as O(`w) using Lemma 5.2, and the sum of them as \u03a6k = max{k \u2212 1, k \u00b7 `k} since there will at most k \u2212 1 edges between these k boxes. In Lemma 6.1 this yields Pr [ disc\u03c7(P, h) = O( \u221a (\u03a6/w) log(1/\u03b4)) ] \u2264 \u03b4, and eventually with probability 1\u2212 \u03b4 an \u03b5-sample of\nsize O ( (1/\u03b5) \u221a (\u03a6/w) log(1/\u03b5\u03b4) ) in place of Theorem 6.2.\nTheorem 6.3. Consider a partition of P = \u22c3 i Pi for P \u2282 R2, so each Pi is in a (`k, 2)-cube, and letting \u03a6k = max{k \u2212 1, k \u00b7 `k} and \u03a6 = mink\u22651 \u03a6k. Then with probability at least 1 \u2212 \u03b4, we can construct an \u03b5-sample of (P,Hw) of size O ( 1 \u03b5 \u221a \u03a6 w log \u03a6 w\u03b5\u03b4 ) .\nWe can compute a 2 \u221a\n2-approximation to \u03a6 in O(nk2max) time, where kmax is the largest value of k we consider (kmax = log n may be a good choice). Our algorithm will only use axis-aligned cubes which is a \u221a 2-approximation to more generally allowing rotated cubes to fit each Pi. We simply run the k-clustering of [10] using the L\u221e metric. That is, we start with an arbitrary point p \u2208 P to place in a set W1; this represents the center of the smallest (`, 2)-cube that fits all data. Then we inductively, choose pk = arg maxp\u2208P minw\u2208Wk\u22121 \u2016w \u2212 p\u2016\u221e, and create Wk = Wk\u22121 \u222a pk. At any stage `k = 2 \u00b7maxp\u2208P minw\u2208Wk\u22121 \u2016w \u2212 p\u2016\u221e, and \u03a6k = max{k \u2212 1, `k \u00b7 k}.\nNon-linear clusters. We can also observe a slightly tighter bound. If there are k (`k, 2)-cubes, but they are not all near a single line, then they cannot all contribute to the discrepancy. Given a partition P = \u22c2 i Pi where each Pi is in a (`k, 2)-cube, let \u03bak describe the maximum number of these cubes that a single slab Sw can intersect. Then we can use \u03a6\u0304k = max{k \u2212 1, \u03bak \u00b7 `k} in place of \u03a6k. However, it is less clear the best way to construct an approximation to \u03a6\u0304k and \u03a6\u0304 = mink\u22651 \u03a6\u0304k.\nAs another thought experiment, consider all of the points are in C`,2. We can now decompose this square into w2 smaller squares, each of side length `/w. Any slab Sw can only pass through O(w) smaller squares; thus \u03a6\u0304w2 = min{w \u2212 1, O(w) \u00b7 `/w} = O(`). So we recover the original non-adaptive bound.\nOne may wonder if this can be improved if many of thew2 squares are empty. If there areO(1) non-empty squares, then \u03a6 already captures this improved bound. If there is still a slab Sw pass through \u2126(w) squares, then this \u03a6\u0304 bound again does not improve over the non-adaptive one. However if there are \u0398(w) non-empty squares, and no slab Sw passes through more than O(1) of them (e.g., they are all on the boundary of C`,2), then \u03a6\u0304 improves the bound over \u03a6 by a factor of w. Thus the \u03a6\u0304 approach can improve the bound in certain settings."}, {"heading": "6.5 Generalization to d Dimensions", "text": "We now extend from R2 to Rd for d > 2. Using results from Section 5 we implicitly get a bound on \u2211\nj \u2206 d j , but the Chernoff bound we use requires a bound on \u2211\nj \u2206 2 j . As in [22], we can attain a weaker bound using\nJensen\u2019s inequality over at most n terms\u2211 j 1 n \u22062j d/2 \u2264\u2211 j 1 n ( \u22062j )d/2 so \u2211 j \u22062j \u2264 n1\u22122/d \u2211 j \u2206dj 2/d . (6.2) Replacing this bound and using \u03c1(Sw,M) \u2264 O(`d\u22121w) in Lemma 6.1 and considering \u03c2 = c1/w for some constant c1 results:\nLemma 6.2. Assume P \u2282 Rd is contained in some cube C`,d and with min-cost matching M , and consider a \u03c2-bounded smoothed halfspace h \u2208 Hw associated with slab Sw. Let \u03c1(Sw,M) \u2264 c2(`d\u22121w) for constant c2. Then Pr [ disc\u03c7(P, h) > Cn 1/2\u22121/d(`/w)1\u22121/d \u221a log(2/\u03b4) ] \u2264 \u03b4 for any \u03b4 > 0, where C = \u221a 2c1(c2) 1/d\nis a constant.\nProof. Using the gradient of vh is at most \u03c2 = c1/w and |vh(pj) \u2212 vh(qj)| \u2264 \u03c2 max{2w, \u2016pj \u2212 qj\u2016} we have\u2211\nj \u2206dj = \u2211 j 2d(vh(pj)\u2212 vh(qj))d \u2264 2d\u03c2d\u03c1(Sw,M) \u2264 2dcd1/wd \u00b7 c2`d\u22121w = 2dcd1c2(`/w)d\u22121,\nwhere second inequality follows by Lemma 5.2 that \u03c1(Sw,M) = \u2211\nj max{(2w)d, \u2016pj\u2212qj\u2016d} \u2264 c2(`d\u22121w). Hence, by Jensen\u2019s inequality (i.e. (6.2))\u2211\nj\n\u22062j \u2264 n1\u22122/d(2dcd1c2(`/w)d\u22121)2/d = n1\u22122/d4c21(c2)2/d(`/w)2(d\u22121)/d.\nWe now study the random variable disc\u03c7(P, h) = | \u2211\niXi| for a single h \u2208 Hw. Invoking (6.1) we can bound\nPr[disc\u03c7(P, h) > \u03b1] \u2264 2 exp(\u2212\u03b12/n1\u22122/d2c21(c2)2/d(`/w)2(d\u22121)/d.\nSetting C = \u221a 2c1(c2) 1/d and\n\u03b1 = Cn1/2\u22121/d(`/w)1\u22121/d \u221a log(2/\u03b4)\nreveals Pr [ disc\u03c7(P, h) > Cn 1/2\u22121/d(`/w)1\u22121/d \u221a log(2/\u03b4) ] \u2264 \u03b4.\nFor all choices of smoothed halfspaces, applying the union bound, the discrepancy is increased by a\u221a log n factor, with the following probabilistic guarantee,\nPr[disc\u03c7(P,Hw) > Cn 1/2\u22121/d(`/w)1\u22121/d \u221a log(n/\u03b4)] \u2264 \u03b4.\nUltimately, we can extend Theorem 6.2 to the following.\nTheorem 6.4. For P \u2282 C`,d \u2282 Rd, where d is constant, with probability at least 1\u2212 \u03b4, we can construct an \u03b5-sample of (P,Hw) of size O ( (`/w)2(d\u22121)/(d+2) \u00b7 ( 1 \u03b5 \u221a log `w\u03b5\u03b4 )2d/(d+2)) .\nIf the data is \u201cwell-clustered\u201d in high dimension, we can get a similar adaptive bounds as Theorem 6.3. Theorem 6.5. Consider a partition of P = \u22c3 i Pi for P \u2282 Rd, so each Pi is in a (`k, d)-cube, and letting \u03a6k = max{k \u2212 1, k \u00b7 `k} and \u03a6 = mink\u22651 \u03a6k. Then with probability at least 1 \u2212 \u03b4, we can construct an\n\u03b5-sample of (P,Hw) of size O ( (\u03a6/w)2(d\u22121)/(d+2) \u00b7 ( 1 \u03b5 \u221a log \u03a6w\u03b5\u03b4 )2d/(d+2)) .\nNote these results address scenario (S3) from the introduction where we want to find a small set (the \u03b5-sample) so that it could be much smaller than the d/\u03b52 random sampling bound, and allows generalization error O(\u03b5) for agnostic learning as described in Section 3.2. When `/w (or \u03a6/w) is constant, the exponents on 1/\u03b5 are also better than those for binary ranges spaces (see Section 2)."}], "references": [{"title": "Picodes: Learning a compact code for novel-category recognition", "author": ["A.F. Alessandro Bergamo", "Lorenzo Torresani"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Scale-sensitive dimensions, uniform convergence, and learnability", "author": ["N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler"], "venue": "Journal of ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Small size \u03b5-nets for axis-parallel rectangles and boxes", "author": ["B. Aronov", "E. Ezra", "M. Sharir"], "venue": "Siam Journal of Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Irregularities of distribution I", "author": ["J. Beck"], "venue": "Acta Mathematica,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1987}, {"title": "Worst-case bounds for subadditive geometric graphs", "author": ["M. Bern", "D. Eppstein"], "venue": "SOCG,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "The Discrepancy Method", "author": ["B. Chazelle"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "On linear-time deterministic algorithms for optimization problems in fixed dimensions", "author": ["B. Chazelle", "J. Matousek"], "venue": "J. Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Concentration of Measure for the Analysis of Randomized Algorithms", "author": ["D.P. Dubhashi", "A. Panconesi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Paths, trees, and flowers", "author": ["J. Edmonds"], "venue": "Canadian Journal of Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1965}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["T.F. Gonzalez"], "venue": "Theoretical Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Smorodinksy. \u03b5-nets for halfspaces revisited", "author": ["S. Har-Peled", "H. Kaplan", "M. Sharir"], "venue": "Technical report,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "epsilon-nets and simplex range queries", "author": ["D. Haussler", "E. Welzl"], "venue": "Disc. & Comp. Geom.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Comparing distributions and shapes using the kernel distance", "author": ["S. Joshi", "R.V. Kommaraju", "J.M. Phillips", "S. Venkatasubramanian"], "venue": "SOCG,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "On range searching in the group model and combinatorial discrepancy", "author": ["K.G. Larsen"], "venue": "In FOCS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Improved bounds on the samples complexity of learning", "author": ["Y. Li", "P.M. Long", "A. Srinivasan"], "venue": "J. Comp. and Sys. Sci.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Tight upper bounds for the discrepancy of halfspaces", "author": ["J. Matousek"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Geometric Discrepancy", "author": ["J. Matousek"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "How to net a lot with little: Small \u03b5-nets for disks and halfspaces", "author": ["J. Matousek", "R. Seidel", "E. Welzl"], "venue": "SOCG,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}, {"title": "Combinatorial geometry. Wiley-Interscience series in discrete mathematics and optimization", "author": ["J. Pach", "P.K. Agarwal"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Tight lower bounds for the size of epsilon-nets", "author": ["J. Pach", "G. Tardos"], "venue": "Journal of American Mathematical Society,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Algorithms for \u03b5-approximations of terrains", "author": ["J.M. Phillips"], "venue": "In Automata, Languages and Programming,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "eps-samples for kernels", "author": ["J.M. Phillips"], "venue": "SODA,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Emperical Processes: Theory and Applications", "author": ["D. Pollard"], "venue": "NSF-CBMS REgional Confernece Series in Probability and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "New existence proofs \u03b5-nets", "author": ["E. Pyrga", "S. Ray"], "venue": "In SOCG,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Inductive principles of the search for empirical dependencies", "author": ["V. Vapnik"], "venue": "In COLT,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1989}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory of Probability and its Applications,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1971}, {"title": "A divide-and-conquer algorithm for min-cost perfect matching in the plane", "author": ["K.R. Varadarajan"], "venue": "In FOCS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "a point) can be contained in a range with a non-binary value in [0, 1].", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "Similar problems have been considered in specific contexts of functions classes with a [0, 1] range or kernel density estimates.", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "(S2) Next consider a large approximate (say high-dimensional image feature [1]) dataset, where we want to build a linear classifier.", "startOffset": 75, "endOffset": 78}, {"referenceID": 25, "context": ", VC-dimension [26]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In learning theory these smooth range spaces can be characterized by more general notions called P dimension [23] (or Pseudo dimension) or V -dimension [25] (or \u201cfat\u201d versions of these [2]) and can be used to learn real-valued functions for regression or density estimation, respectively.", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "In learning theory these smooth range spaces can be characterized by more general notions called P dimension [23] (or Pseudo dimension) or V -dimension [25] (or \u201cfat\u201d versions of these [2]) and can be used to learn real-valued functions for regression or density estimation, respectively.", "startOffset": 152, "endOffset": 156}, {"referenceID": 1, "context": "In learning theory these smooth range spaces can be characterized by more general notions called P dimension [23] (or Pseudo dimension) or V -dimension [25] (or \u201cfat\u201d versions of these [2]) and can be used to learn real-valued functions for regression or density estimation, respectively.", "startOffset": 185, "endOffset": 188}, {"referenceID": 12, "context": "Our work extends some recent work [13, 22] which examines a special case of our setting that maps to kernel density estimates, and matches or improves on related bounds for non-smoothed versions.", "startOffset": 34, "endOffset": 42}, {"referenceID": 21, "context": "Our work extends some recent work [13, 22] which examines a special case of our setting that maps to kernel density estimates, and matches or improves on related bounds for non-smoothed versions.", "startOffset": 34, "endOffset": 42}, {"referenceID": 25, "context": "For a range space with shatter dimension \u03bd, a random sample of size O((1/\u03b52)(\u03bd + log(1/\u03b4))) is an \u03b5-sample with probability at least 1 \u2212 \u03b4 [26, 15], and a random sample of size O((\u03bd/\u03b5) log(1/\u03b5\u03b4)) is an \u03b5-net with probability at least 1\u2212 \u03b4 [12, 19].", "startOffset": 139, "endOffset": 147}, {"referenceID": 14, "context": "For a range space with shatter dimension \u03bd, a random sample of size O((1/\u03b52)(\u03bd + log(1/\u03b4))) is an \u03b5-sample with probability at least 1 \u2212 \u03b4 [26, 15], and a random sample of size O((\u03bd/\u03b5) log(1/\u03b5\u03b4)) is an \u03b5-net with probability at least 1\u2212 \u03b4 [12, 19].", "startOffset": 139, "endOffset": 147}, {"referenceID": 11, "context": "For a range space with shatter dimension \u03bd, a random sample of size O((1/\u03b52)(\u03bd + log(1/\u03b4))) is an \u03b5-sample with probability at least 1 \u2212 \u03b4 [26, 15], and a random sample of size O((\u03bd/\u03b5) log(1/\u03b5\u03b4)) is an \u03b5-net with probability at least 1\u2212 \u03b4 [12, 19].", "startOffset": 239, "endOffset": 247}, {"referenceID": 18, "context": "For a range space with shatter dimension \u03bd, a random sample of size O((1/\u03b52)(\u03bd + log(1/\u03b4))) is an \u03b5-sample with probability at least 1 \u2212 \u03b4 [26, 15], and a random sample of size O((\u03bd/\u03b5) log(1/\u03b5\u03b4)) is an \u03b5-net with probability at least 1\u2212 \u03b4 [12, 19].", "startOffset": 239, "endOffset": 247}, {"referenceID": 15, "context": "An \u03b5-sampleQ can be made of size O(1/\u03b52\u03bd/(\u03bd+1)) [16] and this bound can be no smaller [17] in the general case.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "An \u03b5-sampleQ can be made of size O(1/\u03b52\u03bd/(\u03bd+1)) [16] and this bound can be no smaller [17] in the general case.", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "For balls B in Rd which have shatter-dimension \u03bd = d+1, this can be improved toO(1/\u03b52d/(d+1) log(1/\u03b5)) [4, 17],", "startOffset": 103, "endOffset": 110}, {"referenceID": 16, "context": "For balls B in Rd which have shatter-dimension \u03bd = d+1, this can be improved toO(1/\u03b52d/(d+1) log(1/\u03b5)) [4, 17],", "startOffset": 103, "endOffset": 110}, {"referenceID": 13, "context": "For axis-aligned rectangles R in Rd which have shatterdimension \u03bd = 2d, this can be improved to O((1/\u03b5) log(1/\u03b5)) [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "For \u03b5-nets, the general bound of O((\u03bd/\u03b5) log(1/\u03b5)) can also be made deterministic [16], and for halfspaces in R4 the size must be at least \u03a9((1/\u03b5) log(1/\u03b5)) [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "For \u03b5-nets, the general bound of O((\u03bd/\u03b5) log(1/\u03b5)) can also be made deterministic [16], and for halfspaces in R4 the size must be at least \u03a9((1/\u03b5) log(1/\u03b5)) [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "But for halfspaces in R3 the size can be O(1/\u03b5) [18, 11], which is tight.", "startOffset": 48, "endOffset": 56}, {"referenceID": 10, "context": "But for halfspaces in R3 the size can be O(1/\u03b5) [18, 11], which is tight.", "startOffset": 48, "endOffset": 56}, {"referenceID": 2, "context": "For other range spaces, such as axis-aligned rectangles in R2, the size bound is \u0398((1/\u03b5) log log(1/\u03b5)) [3, 20].", "startOffset": 103, "endOffset": 110}, {"referenceID": 19, "context": "For other range spaces, such as axis-aligned rectangles in R2, the size bound is \u0398((1/\u03b5) log log(1/\u03b5)) [3, 20].", "startOffset": 103, "endOffset": 110}, {"referenceID": 12, "context": "A kernel range space [13, 22] (P,K) is an extension of the combinatorial concept of a range space (P,A) (or to distinguish it we refer to the classic notion as a binary range space).", "startOffset": 21, "endOffset": 29}, {"referenceID": 21, "context": "A kernel range space [13, 22] (P,K) is an extension of the combinatorial concept of a range space (P,A) (or to distinguish it we refer to the classic notion as a binary range space).", "startOffset": 21, "endOffset": 29}, {"referenceID": 0, "context": "An element Kx of K is a kernel K(x, \u00b7) applied at point x \u2208 Rd; it assigns a value in [0, 1] to each point p \u2208 P as K(x, p).", "startOffset": 86, "endOffset": 92}, {"referenceID": 12, "context": "The notion of an \u03b5-kernel sample [13] extends the definition of \u03b5-sample.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "[13] showed that an \u03b5-sample of a linked range space (P,A) is also an \u03b5-kernel sample of a corresponding kernel range space (P,K).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Then [22] showed that these bounds can be improved through discrepancy-based methods to O(((1/\u03b5) \u221a log(1/\u03b5\u03b4))2d/(d+2)), which is O((1/\u03b5) \u221a log(1/\u03b5\u03b4)) in R2.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "A more general concept has been studied in learning theory on real-valued functions, where a function f as a member of a function class F describes a mapping from Rd to [0, 1] (or more generally R).", "startOffset": 169, "endOffset": 175}, {"referenceID": 24, "context": "A kernel range space where the linked binary range space has bounded shatter-dimension \u03bd is said to have bounded V-dimension [25] (see [2]) of \u03bd.", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "A kernel range space where the linked binary range space has bounded shatter-dimension \u03bd is said to have bounded V-dimension [25] (see [2]) of \u03bd.", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "Choose any value s \u2208 [0, 1] for all points y \u2208 Y , and then for each subset of Z \u2282 Y there exists a function f \u2208 F so f(y) > s if y \u2208 Z and f(y) < s if y / \u2208 Z.", "startOffset": 21, "endOffset": 27}, {"referenceID": 22, "context": "The best sample complexity bounds for ensuring Q is an \u03b5-sample of P based on V-dimension are derived from a more general sort of dimension (called a P-dimension [23] where in the shattering definition, each y may have a distinct s(y) value) requires |Q| = O((1/\u03b52)(\u03bd+log(1/\u03b4))) [15].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "The best sample complexity bounds for ensuring Q is an \u03b5-sample of P based on V-dimension are derived from a more general sort of dimension (called a P-dimension [23] where in the shattering definition, each y may have a distinct s(y) value) requires |Q| = O((1/\u03b52)(\u03bd+log(1/\u03b4))) [15].", "startOffset": 279, "endOffset": 283}, {"referenceID": 12, "context": "These inherent the construction bounds through the linking result of [13], and we show cases where these bounds can also be improved.", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "Given a point p \u2208 P , then smoothed halfspace h \u2208 Hw maps p to a value vh(p) \u2208 [0, 1] (rather than the traditional {0, 1} in a binary range space).", "startOffset": 79, "endOffset": 85}, {"referenceID": 21, "context": "Although the Gaussian kernel does not satisfy the restriction that only points in the width 2w slab take non {0, 1} values, we can use techniques from [22] to extend to this case as well.", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "[13] established that given a kernel range space (P,K), a linked binary range space (P,A), and an \u03b5-sampleQ of (P,A), thenQ is also an \u03b5-kernel sample of (P,K).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "1 ([13]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "For halfspaces in R2 or R3 (linked to smoothed halfspaces) and balls in R2 (linked to kernels), the size can be reduced to O(1/\u03b5) [18, 11, 24].", "startOffset": 130, "endOffset": 142}, {"referenceID": 10, "context": "For halfspaces in R2 or R3 (linked to smoothed halfspaces) and balls in R2 (linked to kernels), the size can be reduced to O(1/\u03b5) [18, 11, 24].", "startOffset": 130, "endOffset": 142}, {"referenceID": 23, "context": "For halfspaces in R2 or R3 (linked to smoothed halfspaces) and balls in R2 (linked to kernels), the size can be reduced to O(1/\u03b5) [18, 11, 24].", "startOffset": 130, "endOffset": 142}, {"referenceID": 21, "context": "Following some basic ideas from [22], these matchings will be used for discrepancy bounds on smoothed range spaces in Section 6.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "In particular, we analyze some properties of the interaction of a min-cost matching M and some basic shapes ([22] considered only balls).", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "The min-cost matching can be computed in O(n3) time by [9] (using an extension of the Hungarian algorithm from the bipartite case).", "startOffset": 55, "endOffset": 58}, {"referenceID": 26, "context": "In R2 it can be calculated in O(n3/2 log n) time [27].", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "Following [22], again we will base our analysis on a result of [5] which says that if P \u2282 [0, 1]d (a unit cube) then for d a constant, costd(M,P ) = \u2211n i=1 \u2016pi \u2212 qi\u2016 = O(1), where M is the min-cost matching.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Following [22], again we will base our analysis on a result of [5] which says that if P \u2282 [0, 1]d (a unit cube) then for d a constant, costd(M,P ) = \u2211n i=1 \u2016pi \u2212 qi\u2016 = O(1), where M is the min-cost matching.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "Following [22], again we will base our analysis on a result of [5] which says that if P \u2282 [0, 1]d (a unit cube) then for d a constant, costd(M,P ) = \u2211n i=1 \u2016pi \u2212 qi\u2016 = O(1), where M is the min-cost matching.", "startOffset": 90, "endOffset": 96}, {"referenceID": 21, "context": "Note this differs from a similar definition by [22] since that case did not need to consider when both p and q were both outside of Ow, and did not need the min{(2w)d, .", "startOffset": 47, "endOffset": 51}, {"referenceID": 4, "context": "We cannot simply apply the result of [5] since we do not restrict that P \u2282 Cw.", "startOffset": 37, "endOffset": 40}, {"referenceID": 21, "context": "For all (T2) edges, we can also bound their contribution to \u03c1(Cw,M) as O(wd), by extending an analysis of [22] when both Cw and C20w are similarly proportioned balls.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "In this section we build on the ideas from [22] and the new min-cost matching results in Section 5 to produce new discrepancy-based \u03b5-sample bounds for smoothed range spaces.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "This should not be unfamiliar to readers familiar with discrepancy-based techniques for creating \u03b5-samples of binary range spaces [17, 6].", "startOffset": 130, "endOffset": 137}, {"referenceID": 5, "context": "This should not be unfamiliar to readers familiar with discrepancy-based techniques for creating \u03b5-samples of binary range spaces [17, 6].", "startOffset": 130, "endOffset": 137}, {"referenceID": 21, "context": "Now following the key insight from [22] we can bound \u2211 j \u2206 2 j using results from Section 5, which shows up in the following Chernoff bound from [8]: Let {X1, X2, .", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "Now following the key insight from [22] we can bound \u2211 j \u2206 2 j using results from Section 5, which shows up in the following Chernoff bound from [8]: Let {X1, X2, .", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "We can then apply the MergeReduce framework [7]; iteratively apply this random coloring in O(log n) rounds on disjoint subsets of size O(s(\u03b5)).", "startOffset": 44, "endOffset": 47}, {"referenceID": 20, "context": "1 in [21]), we have the same \u03b5-sample size bound.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "But settingw = \u03b5 so about \u03b5n points are in the slab (the same amount of error we allow in an \u03b5-sample) yields disc\u03c7(P,Hw) = O((1/ \u221a \u03b5n) \u00b7 \u221a log(n/\u03b4)) and the size of the \u03b5sample to be O(1\u03b5 \u221a log(1/\u03b5\u03b4)), which is a large improvement over O(1/\u03b54/3), and the best bound known for non-smoothed range spaces [17].", "startOffset": 303, "endOffset": 307}, {"referenceID": 16, "context": "However, the assumption that P \u2282 C`,2 (although not uncommon [17]) can be restrictive.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "We simply run the k-clustering of [10] using the L\u221e metric.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "As in [22], we can attain a weaker bound using Jensen\u2019s inequality over at most n terms \uf8eb\uf8ed\u2211", "startOffset": 6, "endOffset": 10}], "year": 2015, "abstractText": "We consider smoothed versions of geometric range spaces, so an element of the ground set (e.g. a point) can be contained in a range with a non-binary value in [0, 1]. Similar notions have been considered for kernels; we extend them to more general types of ranges. We then consider approximations of these range spaces through \u03b5-nets and \u03b5-samples (aka \u03b5-approximations). We characterize when size bounds for \u03b5-samples on kernels can be extended to these more general smoothed range spaces. We also describe new generalizations for \u03b5-nets to these range spaces and show when results from binary range spaces can carry over to these smoothed ones.", "creator": "TeX"}}}