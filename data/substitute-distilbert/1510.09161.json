{"id": "1510.09161", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Streaming, Distributed Variational Inference for Bayesian Nonparametrics", "abstract": "this paper presents a methodology for creating hierarchical, distributed inference algorithms for weighted nonparametric ( bnp ) models. in the proposed framework, processing nodes receive a sequence of data minibatches, compute temporal variational posterior for each, and make asynchronous streaming updates to a mixed model. as contrast to previous algorithms, the proposed framework utilized truly streaming, distributed, asynchronous, learning - rate - free, and truncation - elastic. the earliest challenge concerns developing the framework, arising from the fact that bnp models don't impose yet inherent ordering on their components, is involving proper correspondence between minibatch and central bnp posterior components before performing each update. to address this, the paper seeks a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. the paper came with an application of the methodology to the underlying mixture trees, with experimental results demonstrating highly robust potential and performance.", "histories": [["v1", "Fri, 30 Oct 2015 17:04:33 GMT  (751kb,D)", "http://arxiv.org/abs/1510.09161v1", "This paper was presented at NIPS 2015. Please use the following BibTeX citation: @inproceedings{Campbell15_NIPS, Author = {Trevor Campbell and Julian Straub and John W. {Fisher III} and Jonathan P. How}, Title = {Streaming, Distributed Variational Inference for Bayesian Nonparametrics}, Booktitle = {Advances in Neural Information Processing Systems (NIPS)}, Year = {2015}}"]], "COMMENTS": "This paper was presented at NIPS 2015. Please use the following BibTeX citation: @inproceedings{Campbell15_NIPS, Author = {Trevor Campbell and Julian Straub and John W. {Fisher III} and Jonathan P. How}, Title = {Streaming, Distributed Variational Inference for Bayesian Nonparametrics}, Booktitle = {Advances in Neural Information Processing Systems (NIPS)}, Year = {2015}}", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["trevor campbell", "julian straub", "john w fisher iii", "jonathan p how"], "accepted": true, "id": "1510.09161"}, "pdf": {"name": "1510.09161.pdf", "metadata": {"source": "CRF", "title": "Streaming, Distributed Variational Inference for Bayesian Nonparametrics", "authors": ["Trevor Campbell", "Julian Straub", "John W. Fisher"], "emails": ["jstraub@csail.", "fisher@csail.", "jhow@}mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Bayesian nonparametric (BNP) stochastic processes are streaming priors \u2013 their unique feature is that they specify, in a probabilistic sense, that the complexity of a latent model should grow as the amount of observed data increases. This property captures common sense in many data analysis problems \u2013 for example, one would expect to encounter far more topics in a document corpus after reading 106 documents than after reading 10 \u2013 and becomes crucial in settings with unbounded, persistent streams of data. While their fixed, parametric cousins can be used to infer model complexity for datasets with known magnitude a priori [1, 2], such priors are silent with respect to notions of model complexity growth in streaming data settings.\nBayesian nonparametrics are also naturally suited to parallelization of data processing, due to the exchangeability, and thus conditional independence, they often exhibit via de Finetti\u2019s theorem. For example, labels from the Chinese Restaurant process [3] are rendered i.i.d. by conditioning on the underlying Dirichlet process (DP) random measure, and feature assignments from the Indian Buffet process [4] are rendered i.i.d. by conditioning on the underlying beta process (BP) random measure.\nGiven these properties, one might expect there to be a wealth of inference algorithms for BNPs that address the challenges associated with parallelization and streaming. However, previous work has only addressed these two settings in concert for parametric models [5, 6], and only recently has each been addressed individually for BNPs. In the streaming setting, [7] and [8] developed streaming inference for DP mixture models using sequential variational approximation. Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5]. Outside of variational approaches, which are the focus of the present paper, there exist exact parallelized MCMC methods for BNPs [14, 15]; the tradeoff in using such methods is that they provide samples from the posterior rather than the distribution itself, and results regarding assessing\nar X\niv :1\n51 0.\n09 16\n1v 1\n[ cs\n.L G\n] 3\n0 O\nct 2\nconvergence remain limited. Sequential particle filters for inference have also been developed [16], but these suffer issues with particle degeneracy and exponential forgetting.\nThe main challenge posed by the streaming, distributed setting for BNPs is the combinatorial problem of component identification. Most BNP models contain some notion of a countably infinite set of latent \u201ccomponents\u201d (e.g. clusters in a DP mixture model), and do not impose an inherent ordering on the components. Thus, in order to combine information about the components from multiple processors, the correspondence between components must first be found. Brute force search is intractable even for moderately sized models \u2013 there are ( K1+K2 K1 ) possible correspondences for two sets of components of sizes K1 and K2. Furthermore, there does not yet exist a method to evaluate the quality of a component correspondence for BNP models. This issue has been studied before in the MCMC literature, where it is known as the \u201clabel switching problem\u201d, but past solution techniques are generally model-specific and restricted to use on very simple mixture models [17, 18].\nThis paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric models. In the proposed framework (shown for a single node A in Figure 1), processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model using a mapping obtained from a component identification optimization. The key contributions of this work are as follows. First, we develop a minibatch posterior decomposition that motivates a learning-rate-free streaming, distributed framework suitable for Bayesian nonparametrics. Then, we derive the component identification optimization problem by maximizing the probability of a component matching. We show that the BNP prior regularizes model complexity in the optimization; an interesting side effect of this is that regardless of whether the minibatch variational inference scheme is truncated, the proposed algorithm is truncation-free. Finally, we provide an efficiently computable regularization bound for the Dirichlet process prior based on Jensen\u2019s inequality1. The paper concludes with applications of the methodology to the DP mixture model, with experimental results demonstrating the scalability and performance of the method in practice."}, {"heading": "2 Streaming, distributed Bayesian nonparametric inference", "text": "The proposed framework, motivated by a posterior decomposition that will be discussed in Section 2.1, involves a collection of processing nodes with asynchronous access to a central variational posterior approximation (shown for a single node in Figure 1). Data is provided to each processing node as a sequence of minibatches. When a processing node receives a minibatch of data, it obtains the central posterior (Figure 1a), and using it as a prior, computes a minibatch variational posterior approximation (Figure 1b). When minibatch inference is complete, the node then performs component identification between the minibatch posterior and the current central posterior, accounting for possible modifications made by other processing nodes (Figure 1c). Finally, it merges the minibatch posterior into the central variational posterior (Figure 1d).\nIn the following sections, we use the DP mixture [3] as a guiding example for the technical development of the inference framework. However, it is emphasized that the material in this paper generalizes to many other BNP models, such as the hierarchical DP (HDP) topic model [19], BP latent feature model [20], and Pitman-Yor (PY) mixture [21] (see the supplement for further details).\n1Regularization bounds for other popular BNP priors may be found in the supplement."}, {"heading": "2.1 Posterior decomposition", "text": "Consider a DP mixture model [3], with cluster parameters \u03b8, assignments z, and observed data y. For each asynchronous update made by each processing node, the dataset is split into three subsets y = yo \u222a yi \u222a ym for analysis. When the processing node receives a minibatch of data ym, it queries the central processing node for the original posterior p(\u03b8, zo|yo), which will be used as the prior for minibatch inference. Once inference is complete, it again queries the central processing node for the intermediate posterior p(\u03b8, zo, zi|yo, yi) which accounts for asynchronous updates from other processing nodes since minibatch inference began. Each subset yr, r \u2208 {o, i,m}, has Nr observations {yrj}Nrj=1, and each variable zrj \u2208 N assigns yrj to cluster parameter \u03b8zrj . Given the independence of \u03b8 and z in the prior, and the conditional independence of the data given the latent parameters, Bayes\u2019 rule yields the following decomposition of the posterior of \u03b8 and z given y,\nUpdated Central Posterior\ufe37 \ufe38\ufe38 \ufe37 p(\u03b8, z|y)\u221d p(zi, zm|zo)\np(zi|zo)p(zm|zo) \u00b7\nOriginal Posterior\ufe37 \ufe38\ufe38 \ufe37 p(\u03b8, zo|yo)\u22121 \u00b7 Minibatch Posterior\ufe37 \ufe38\ufe38 \ufe37 p(\u03b8, zm, zo|ym, yo) \u00b7 Intermediate Posterior\ufe37 \ufe38\ufe38 \ufe37 p(\u03b8, zi, zo|yi, yo). (1)\nThis decomposition suggests a simple streaming, distributed, asynchronous update rule for a processing node: first, obtain the current central posterior density p(\u03b8, zo|yo), and using it as a prior, compute the minibatch posterior p(\u03b8, zm, zo|yo, ym); and then update the central posterior density by using (1) with the current central posterior density p(\u03b8, zi, zo|yi, yo). However, there are two issues preventing the direct application of the decomposition rule (1):\nUnknown component correspondence: Since it is generally intractable to find the minibatch posteriors p(\u03b8, zm, zo|yo, ym) exactly, approximate methods are required. Further, as (1) requires the multiplication of densities, sampling-based methods are difficult to use, suggesting a variational approach. Typical mean-field variational techniques introduce an artificial ordering of the parameters in the posterior, thereby breaking symmetry that is crucial to combining posteriors correctly using density multiplication [6]. The use of (1) with mean-field variational approximations thus requires first solving a component identification problem.\nUnknown model size: While previous posterior merging procedures required a 1-to-1 matching between the components of the minibatch posterior and central posterior [5, 6], Bayesian nonparametric posteriors break this assumption. Indeed, the datasets yo, yi, and ym from the same nonparametric mixture model can be generated by the same, disjoint, or an overlapping set of cluster parameters. In other words, the global number of unique posterior components cannot be determined until the component identification problem is solved and the minibatch posterior is merged."}, {"heading": "2.2 Variational component identification", "text": "Suppose we have the following mean-field exponential family prior and approximate variational posterior densities in the minibatch decomposition (1),\np(\u03b8k) = h(\u03b8k)e \u03b7T0 T (\u03b8k)\u2212A(\u03b70) \u2200k \u2208 N\np(\u03b8, zo|yo) ' qo(\u03b8, zo) = \u03b6o(zo) Ko\u220f k=1 h(\u03b8k)e \u03b7TokT (\u03b8k)\u2212A(\u03b7ok)\np(\u03b8, zm, zo|ym, yo) ' qm(\u03b8, zm, zo) = \u03b6m(zm)\u03b6o(zo) Km\u220f k=1 h(\u03b8k)e \u03b7TmkT (\u03b8k)\u2212A(\u03b7mk) (2)\np(\u03b8, zi, zo|yi, yo) ' qi(\u03b8, zi, zo) = \u03b6i(zi)\u03b6o(zo) Ki\u220f k=1 h(\u03b8k)e \u03b7TikT (\u03b8k)\u2212A(\u03b7ik),\nwhere \u03b6r(\u00b7), r \u2208 {o, i,m} are products of categorical distributions for the cluster labels zr, and the goal is to use the posterior decomposition (1) to find the updated posterior approximation\np(\u03b8, z|y) ' q(\u03b8, z) = \u03b6(z) K\u220f k=1 h(\u03b8k)e \u03b7Tk T (\u03b8k)\u2212A(\u03b7k). (3)\nAs mentioned in the previous section, the artificial ordering of components causes the na\u0131\u0308ve application of (1) with variational approximations to fail, as disparate components from the approximate posteriors may be merged erroneously. This is demonstrated in Figure 3a, which shows results from a synthetic experiment (described in Section 4) ignoring component identification. As the number of parallel threads increases, more matching mistakes are made, leading to decreasing model quality.\nTo address this, first note that there is no issue with the first Ko components of qm and qi; these can be merged directly since they each correspond to the Ko components of qo. Thus, the component identification problem reduces to finding the correspondence between the last K \u2032m = Km \u2212 Ko components of the minibatch posterior and the last K \u2032i = Ki \u2212Ko components of the intermediate posterior. For notational simplicity (and without loss of generality), fix the component ordering of the intermediate posterior qi, and define \u03c3 : [Km] \u2192 [Ki +K \u2032m] to be the 1-to-1 mapping from minibatch posterior component k to updated central posterior component \u03c3(k), where [K] := {1, . . . ,K}. The fact that the first Ko components have no ordering ambiguity can be expressed as \u03c3(k) = k \u2200k \u2208 [Ko]. Note that the maximum number of components after merging is Ki + K \u2032m, since each of the lastK \u2032m components in the minibatch posterior may correspond to new components in the intermediate posterior. After substituting the three variational approximations (2) into (1), the goal of the component identification optimization is to find the 1-to-1 mapping \u03c3? that yields the largest updated posterior normalizing constant, i.e. matches components with similar densities,\n\u03c3? \u2190 argmax \u03c3 \u2211 z \u222b \u03b8 p(zi, zm|zo) p(zi|zo)p(zm|zo) qo(\u03b8, zo) \u22121q\u03c3m(\u03b8, zm, zo)qi(\u03b8, zi, zo)\ns.t. q\u03c3m(\u03b8, zm) = \u03b6 \u03c3 m(zm) Km\u220f k=1 h(\u03b8\u03c3(k))e \u03b7TmkT (\u03b8\u03c3(k))\u2212A(\u03b7mk)\n\u03c3(k) = k, \u2200k \u2208 [Ko] , \u03c3 1-to-1\n(4)\nwhere \u03b6\u03c3m(zm) is the distribution such that P\u03b6\u03c3m(zmj = \u03c3(k)) = P\u03b6m(zmj = k). Taking the logarithm of the objective and exploiting the mean-field decoupling allows the separation of the objective into a sum of two terms: one expressing the quality of the matching between components (the integral over \u03b8), and one that regularizes the final model size (the sum over z). While the first term is available in closed form, the second is in general not. Therefore, using the concavity of the logarithm function, Jensen\u2019s inequality yields a lower bound that can be used in place of the intractable original objective, resulting in the final component identification optimization:\n\u03c3? \u2190 argmax \u03c3\nKi+K \u2032 m\u2211\nk=1\nA (\u03b7\u0303\u03c3k ) + E\u03c3\u03b6 [log p(zi, zm, zo)]\ns.t. \u03b7\u0303\u03c3k = \u03b7\u0303ik + \u03b7\u0303 \u03c3 mk \u2212 \u03b7\u0303ok\n\u03c3(k) = k \u2200k \u2208 [Ko] , \u03c3 1-to-1.\n(5)\nA more detailed derivation of the optimization may be found in the supplement. E\u03c3\u03b6 denotes expectation under the distribution \u03b6o(zo)\u03b6i(zi)\u03b6\u03c3m(zm), and\n\u03b7\u0303rk = { \u03b7rk k \u2264 Kr \u03b70 k > Kr \u2200r \u2208 {o, i,m}, \u03b7\u0303\u03c3mk = { \u03b7m\u03c3\u22121(k) k \u2208 \u03c3 ([Km]) \u03b70 k /\u2208 \u03c3 ([Km]) , (6)\nwhere \u03c3 ([Km]) denotes the range of the mapping \u03c3. The definitions in (6) ensure that the prior \u03b70 is used whenever a posterior r \u2208 {i,m, o} does not contain a particular component k. The intuition for the optimization (5) is that it combines finding component correspondences with high similarity (via the log-partition function) with a regularization term2 on the final updated posterior model size.\nDespite its motivation from the Dirichlet process mixture, the component identification optimization (5) is not specific to this model. Indeed, the derivation did not rely on any properties specific to the Dirichlet process mixture; the optimization applies to any Bayesian nonparametric model with a set of \u201ccomponents\u201d \u03b8, and a set of combinatorial \u201cindicators\u201d z. For example, the optimization applies to the hierarchical Dirichlet process topic model [10] with topic word distributions \u03b8 and local-toglobal topic correspondences z, and to the beta process latent feature model [4] with features \u03b8 and\n2This is equivalent to the KL-divergence regularization \u2212KL [ \u03b6o(zo)\u03b6i(zi)\u03b6 \u03c3 m(zm) \u2223\u2223\u2223\u2223\u2223\u2223 p(zi, zm, zo)].\nbinary assignment vectors z. The form of the objective in the component identification optimization (5) reflects this generality. In order to apply the proposed streaming, distributed method to a particular model, one simply needs a black-box variational inference algorithm that computes posteriors of the form (2), and a way to compute or bound the expectation in the objective of (5)."}, {"heading": "2.3 Updating the central posterior", "text": "To update the central posterior, the node first locks it and solves for \u03c3? via (5). Locking prevents other nodes from solving (5) or modifying the central posterior, but does not prevent other nodes from reading the central posterior, obtaining minibatches, or performing inference; the synthetic experiment in Section 4 shows that this does not incur a significant time penalty in practice. Then the processing node transmits \u03c3? and its minibatch variational posterior to the central processing node where the product decomposition (1) is used to find the updated central variational posterior q in (3), with parameters\nK = max { Ki, max\nk\u2208[Km] \u03c3?(k)\n} , \u03b6(z) = \u03b6i(zi)\u03b6o(zo)\u03b6 \u03c3? m (zm), \u03b7k = \u03b7\u0303ik + \u03b7\u0303 \u03c3? mk \u2212 \u03b7\u0303ok. (7)\nFinally, the node unlocks the central posterior, and the next processing node to receive a new minibatch will use the above K, \u03b6(z), and \u03b7k from the central node as their Ko, \u03b6o(zo), and \u03b7ok."}, {"heading": "3 Application to the Dirichlet process mixture model", "text": "The expectation in the objective of (5) is typically intractable to compute in closed-form; therefore, a suitable lower bound may be used in its place. This section presents such a bound for the Dirichlet process, and discusses the application of the proposed inference framework to the Dirichlet process mixture model using the developed bound. Crucially, the lower bound decomposes such that the optimization (5) becomes a maximum-weight bipartite matching problem. Such problems are solvable in polynomial time [22] by the Hungarian algorithm, leading to a tractable component identification step in the proposed streaming, distributed framework."}, {"heading": "3.1 Regularization lower bound", "text": "For the Dirichlet process with concentration parameter \u03b1 > 0, p(zi, zm, zo) is the Exchangeable Partition Probability Function (EPPF) [23]\np(zi, zm, zo) \u221d \u03b1|K|\u22121 \u220f k\u2208K (nk \u2212 1)!, (8)\nwhere nk is the amount of data assigned to cluster k, and K is the set of labels of nonempty clusters. Given that the variational distribution \u03b6r(zr), r \u2208 {i,m, o} is a product of independent categorical distributions \u03b6r(zr) = \u220fNr j=1 \u220fKr k=1 \u03c0 1[zrj=k] rjk , Jensen\u2019s inequality may be used to bound the regularization in (5) below (see the supplement for further details) by\nE\u03c3\u03b6 [log p(zi, zm, zo)] \u2265 Ki+K\n\u2032 m\u2211\nk=1\n( 1\u2212 es\u0303 \u03c3 k ) log\u03b1+ log \u0393 ( max { 2, t\u0303\u03c3k }) + C\ns\u0303\u03c3k = s\u0303ik + s\u0303 \u03c3 mk + s\u0303ok, t\u0303 \u03c3 k = t\u0303ik + t\u0303 \u03c3 mk + t\u0303ok,\n(9)\nwhere C is a constant with respect to the component mapping \u03c3, and\ns\u0303rk =\n{ \u2211Nr j=1 log(1\u2212\u03c0rjk) k\u2264Kr\n0 k>Kr \u2200r\u2208{o,i,m} t\u0303rk =\n{ \u2211Nr j=1 \u03c0rjk k\u2264Kr\n0 k>Kr \u2200r\u2208{o,i,m}\ns\u0303\u03c3mk =\n{ \u2211Nm j=1 log(1\u2212\u03c0mj\u03c3\u22121(k)) k\u2208\u03c3([Km])\n0 k/\u2208\u03c3([Km]) t\u0303\u03c3mk =\n{ \u2211Nm j=1 \u03c0mj\u03c3\u22121(k) k\u2208\u03c3([Km])\n0 k/\u2208\u03c3([Km]) .\n(10)\nNote that the bound (9) allows incremental updates: after finding the optimal mapping \u03c3?, the central update (7) can be augmented by updating the values of sk and tk on the central node to\nsk \u2190 s\u0303ik + s\u0303\u03c3 ? mk + s\u0303ok, tk \u2190 t\u0303ik + t\u0303\u03c3 ? mk + t\u0303ok. (11)\nAs withK, \u03b7k, and \u03b6 from (7), after performing the regularization statistics update (11), a processing node that receives a new minibatch will use the above sk and tk as their sok and tok, respectively.\nFigure 2 demonstrates the behavior of the lower bound in a synthetic experiment with N = 100 datapoints for various DP concentration parameter values \u03b1 \u2208 [ 10\u22123, 103 ] . The true regularization\nlogE\u03b6 [p(z)] was computed by sample approximation with 104 samples. In Figure 2a, the number of clusters K was varied, with symmetric categorical label weights set to 1K . This figure demonstrates two important phenomena. First, the bound increases as K \u2192 0; in other words, it gives preference to fewer, larger clusters, which is the typical BNP \u201crich get richer\u201d property. Second, the behavior of the bound as K \u2192 N depends on the concentration parameter \u03b1 \u2013 as \u03b1 increases, more clusters are preferred. In Figure 2b, the number of clusters K was fixed to 10, and the categorical label weights were sampled from a symmetric Dirichlet distribution with parameter \u03b3 \u2208 [ 10\u22123, 103 ] . This figure demonstrates that the bound does not degrade significantly with high labelling uncertainty, and is nearly exact for low labelling uncertainty. Overall, Figure 2a demonstrates that the proposed lower bound exhibits similar behaviors to the true regularization, supporting its use in the optimization (5)."}, {"heading": "3.2 Solving the component identification optimization", "text": "Given that both the regularization (9) and component matching score in the objective (5) decompose as a sum of terms for each k \u2208 [Ki +K \u2032m], the objective can be rewritten using a matrix of matching scores R \u2208 R(Ki+K \u2032 m)\u00d7(Ki+K \u2032 m) and selector variables X \u2208 {0, 1}(Ki+K \u2032 m)\u00d7(Ki+K \u2032 m). Setting Xkj = 1 indicates that component k in the minibatch posterior is matched to component j in the intermediate posterior (i.e. \u03c3(k) = j), providing a score Rkj defined using (6) and (10) as\nRkj = A (\u03b7\u0303ij+ \u03b7\u0303mk \u2212 \u03b7\u0303oj)+ ( 1\u2212 es\u0303ij+s\u0303mk+s\u0303oj ) log\u03b1+log \u0393 ( max { 2, t\u0303ij + t\u0303mk + t\u0303oj }) . (12)\nThe optimization (5) can be rewritten in terms of X and R as\nX? \u2190 argmax X\ntr [ XTR ] s.t. X1 = 1, XT1 = 1, Xkk = 1,\u2200k \u2208 [Ko]\nX \u2208 {0, 1}(Ki+K \u2032 m)\u00d7(Ki+K \u2032 m), 1 = [1, . . . , 1] T .\n(13)\nThe first two constraints express the 1-to-1 property of \u03c3(\u00b7). The constraint Xkk = 1\u2200k \u2208 [Ko] fixes the upperKo\u00d7Ko block of X to I (due to the fact that the firstKo components are matched directly), and the off-diagonal blocks to 0. Denoting X\u2032, R\u2032 to be the lower right (K \u2032i +K \u2032 m)\u00d7 (K \u2032i +K \u2032m) blocks of X, R, the remaining optimization problem is a linear assignment problem on X\u2032 with cost matrix \u2212R\u2032, which can be solved using the Hungarian algorithm3. Note that if Km = Ko or Ki = Ko, this implies that no matching problem needs to be solved \u2013 the first Ko components of the minibatch posterior are matched directly, and the last K \u2032m are set as new components. In practical implementation of the framework, new clusters are typically discovered at a diminishing rate as more data are observed, so the number of matching problems that are solved likewise tapers off. The final optimal component mapping \u03c3? is found by finding the nonzero elements of X?:\n\u03c3?(k)\u2190 argmax j X?kj \u2200k \u2208 [Km] . (14)\n3For the experiments in this work, we used the implementation at github.com/hrldcpr/hungarian."}, {"heading": "4 Experiments", "text": "In this section, the proposed inference framework is evaluated on the DP Gaussian mixture with a normal-inverse-Wishart (NIW) prior. We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24]. Priors were set by hand and all methods were initialized randomly. Methods that use multiple passes through the data (e.g. moVB, SVI) were allowed to do so. moVB was allowed to make birth/death moves, while SVI/Batch had fixed truncations. All experiments were performed on a computer with 24 CPU cores and 12GiB of RAM.\nSynthetic: This dataset consisted of 100,000 2-dimensional vectors generated from a Gaussian mixture model with 100 clusters and a NIW(\u00b50, \u03ba0,\u03a80, \u03bd0) prior with \u00b50 = 0, \u03ba0 = 10\u22123, \u03a80 = I , and \u03bd0 = 4. The algorithms were given the true NIW prior, DP concentration \u03b1 = 5, and minibatches of size 50. SDA-DP minibatch inference was truncated toK = 50 components, and all other algorithms were truncated to K = 200 components. Figure 3 shows the results from the experiment over 30 trials, which illustrate a number of important properties of SDA-DP. First and foremost, ignoring the component identification problem leads to decreasing model quality with increasing number of parallel threads, since more matching mistakes are made (Figure 3a). Second, if component identification is properly accounted for using the proposed optimization, increasing the number of parallel threads reduces execution time, but does not affect the final model quality (Figure 3b). Third, SDA-DP (with 40 threads) converges to the same final test log likelihood as the comparison algorithms in significantly reduced time (Figure 3c). Fourth, each component identification optimization typically takes \u223c 10\u22125 seconds, and thus matching accounts for less than a millisecond of total computation and does not affect the overall computation time significantly (Figure 3d). Fifth, the majority of the component matching problems are solved within the first 80 minibatch updates (out of a total of 2,000) \u2013 afterwards, the true clusters have all been discovered and the processing nodes contribute to those clusters rather than creating new ones, as per the discussion at the end of Section 3.2 (Figure 3e). Finally, increased parallelization can be advantageous in discovering the correct number of clusters; with only one thread, mistakes made early on are built upon and persist, whereas with more threads there are more component identification problems solved, and thus more chances to discover the correct clusters (Figure 3f).\nAirplane Trajectories: This dataset consisted of \u223c3,000,000 automatic dependent surveillance broadcast (ADS-B) messages collected from planes across the United States during the period 2013- 03-22 01:30:00UTC to 2013-03-28 12:00:00UTC. The messages were connected based on plane call sign and time stamp, and erroneous trajectories were filtered based on reasonable spatial/temporal bounds, yielding 15,022 trajectories with 1,000 held out for testing. The latitude/longitude points in each trajectory were fit via linear regression, and the 3-dimensional parameter vectors were clustered. Data was split into minibatches of size 100, and SDA-DP used 16 parallel threads.\nMNIST Digits [25]: This dataset consisted of 70,000 28 \u00d7 28 images of hand-written digits, with 10,000 held out for testing. The images were reduced to 20 dimensions with PCA prior to clustering. Data was split into minibatches of size 500, and SDA-DP used 48 parallel threads.\nSUN Images [26]: This dataset consisted of 108,755 images from 397 scene categories, with 8,755 held out for testing. The images were reduced to 20 dimensions with PCA prior to clustering. Data was split into minibatches of size 500, and SDA-DP used 48 parallel threads.\nFigure 4 shows the results from the experiments on the three real datasets. From a qualitative standpoint, SDA-DP discovers sensible clusters in the data, as demonstrated in Figures 4a\u20134c. However, an important quantitative result is highlighted by Table 4d: the larger a dataset is, the more the benefits of parallelism provided by SDA-DP become apparent. SDA-DP consistently provides a model quality that is competitive with the other algorithms, but requires orders of magnitude less computation time, corroborating similar findings on the synthetic dataset."}, {"heading": "5 Conclusions", "text": "This paper presented a streaming, distributed, asynchronous inference algorithm for Bayesian nonparametric models, with a focus on the combinatorial problem of matching minibatch posterior components to central posterior components during asynchronous updates. The main contributions are a component identification optimization based on a minibatch posterior decomposition, a tractable bound on the objective for the Dirichlet process mixture, and experiments demonstrating the performance of the methodology on large-scale datasets. While the present work focused on the DP mixture as a guiding example, it is not limited to this model \u2013 exploring the application of the proposed methodology to other BNP models is a potential area for future research."}, {"heading": "Acknowledgments", "text": "This work was supported by the Office of Naval Research under ONR MURI grant N000141110688."}], "references": [{"title": "Bayesian Analysis of Finite Mixture Distributions", "author": ["Agostino Nobile"], "venue": "PhD thesis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "A simple example of Dirichlet process mixture inconsistency for the number of components", "author": ["Jeffrey W. Miller", "Matthew T. Harrison"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Dirichlet processes. In Encyclopedia of Machine Learning", "author": ["Yee Whye Teh"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["Thomas L. Griffiths", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Streaming variational Bayes", "author": ["Tamara Broderick", "Nicholas Boyd", "Andre Wibisono", "Ashia C. Wilson", "Michael I. Jordan"], "venue": "In Advances in Neural Information Procesing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Approximate decentralized Bayesian inference", "author": ["Trevor Campbell", "Jonathan P. How"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Online learning of nonparametric mixture models via sequential variational approximation", "author": ["Dahua Lin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A sequential algorithm for fast fitting of Dirichlet process mixture models", "author": ["Xiaole Zhang", "David J. Nott", "Christopher Yau", "Ajay Jasra"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Stochastic variational inference", "author": ["Matt Hoffman", "David Blei", "Chong Wang", "John Paisley"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online variational inference for the hierarchical Dirichlet process", "author": ["Chong Wang", "John Paisley", "David M. Blei"], "venue": "In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Truly nonparametric online variational inference for hierarchical Dirichlet processes", "author": ["Michael Bryant", "Erik Sudderth"], "venue": "In Advances in Neural Information Proecssing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Truncation-free stochastic variational inference for Bayesian nonparametric models", "author": ["Chong Wang", "David Blei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Memoized online variational inference for Dirichlet process mixture models", "author": ["Michael Hughes", "Erik Sudderth"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Parallel sampling of DP mixture models using sub-clusters splits", "author": ["Jason Chang", "John Fisher III"], "venue": "In Advances in Neural Information Procesing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Asymptotically exact, embarassingly parallel MCMC", "author": ["Willie Neiswanger", "Chong Wang", "Eric P. Xing"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Particle learning for general mixtures", "author": ["Carlos M. Carvalho", "Hedibert F. Lopes", "Nicholas G. Polson", "Matt A. Taddy"], "venue": "Bayesian Analysis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Dealing with label switching in mixture models", "author": ["Matthew Stephens"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling", "author": ["Ajay Jasra", "Chris Holmes", "David Stephens"], "venue": "Statistical Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Hierarchical Dirichlet processes", "author": ["Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Accelerated sampling for the Indian buffet process", "author": ["Finale Doshi-Velez", "Zoubin Ghahramani"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Parallel Markov chain Monte Carlo for Pitman-Yor mixture models", "author": ["Avinava Dubey", "Sinead Williamson", "Eric Xing"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Theoretical improvements in algorithmic efficiency for network flow problems", "author": ["Jack Edmonds", "Richard Karp"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1972}, {"title": "Exchangeable and partially exchangeable random partitions", "author": ["Jim Pitman"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["David M. Blei", "Michael I. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "While their fixed, parametric cousins can be used to infer model complexity for datasets with known magnitude a priori [1, 2], such priors are silent with respect to notions of model complexity growth in streaming data settings.", "startOffset": 119, "endOffset": 125}, {"referenceID": 1, "context": "While their fixed, parametric cousins can be used to infer model complexity for datasets with known magnitude a priori [1, 2], such priors are silent with respect to notions of model complexity growth in streaming data settings.", "startOffset": 119, "endOffset": 125}, {"referenceID": 2, "context": "For example, labels from the Chinese Restaurant process [3] are rendered i.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "by conditioning on the underlying Dirichlet process (DP) random measure, and feature assignments from the Indian Buffet process [4] are rendered i.", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "However, previous work has only addressed these two settings in concert for parametric models [5, 6], and only recently has each been addressed individually for BNPs.", "startOffset": 94, "endOffset": 100}, {"referenceID": 5, "context": "However, previous work has only addressed these two settings in concert for parametric models [5, 6], and only recently has each been addressed individually for BNPs.", "startOffset": 94, "endOffset": 100}, {"referenceID": 6, "context": "In the streaming setting, [7] and [8] developed streaming inference for DP mixture models using sequential variational approximation.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "In the streaming setting, [7] and [8] developed streaming inference for DP mixture models using sequential variational approximation.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 10, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 11, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 12, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 4, "context": "Stochastic variational inference [9] and related methods [10\u201313] are often considered streaming algorithms, but their performance depends on the choice of a learning rate and on the dataset having known, fixed size a priori [5].", "startOffset": 224, "endOffset": 227}, {"referenceID": 13, "context": "Outside of variational approaches, which are the focus of the present paper, there exist exact parallelized MCMC methods for BNPs [14, 15]; the tradeoff in using such methods is that they provide samples from the posterior rather than the distribution itself, and results regarding assessing", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "Outside of variational approaches, which are the focus of the present paper, there exist exact parallelized MCMC methods for BNPs [14, 15]; the tradeoff in using such methods is that they provide samples from the posterior rather than the distribution itself, and results regarding assessing", "startOffset": 130, "endOffset": 138}, {"referenceID": 15, "context": "Sequential particle filters for inference have also been developed [16], but these suffer issues with particle degeneracy and exponential forgetting.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "This issue has been studied before in the MCMC literature, where it is known as the \u201clabel switching problem\u201d, but past solution techniques are generally model-specific and restricted to use on very simple mixture models [17, 18].", "startOffset": 221, "endOffset": 229}, {"referenceID": 17, "context": "This issue has been studied before in the MCMC literature, where it is known as the \u201clabel switching problem\u201d, but past solution techniques are generally model-specific and restricted to use on very simple mixture models [17, 18].", "startOffset": 221, "endOffset": 229}, {"referenceID": 2, "context": "In the following sections, we use the DP mixture [3] as a guiding example for the technical development of the inference framework.", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "However, it is emphasized that the material in this paper generalizes to many other BNP models, such as the hierarchical DP (HDP) topic model [19], BP latent feature model [20], and Pitman-Yor (PY) mixture [21] (see the supplement for further details).", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "However, it is emphasized that the material in this paper generalizes to many other BNP models, such as the hierarchical DP (HDP) topic model [19], BP latent feature model [20], and Pitman-Yor (PY) mixture [21] (see the supplement for further details).", "startOffset": 172, "endOffset": 176}, {"referenceID": 20, "context": "However, it is emphasized that the material in this paper generalizes to many other BNP models, such as the hierarchical DP (HDP) topic model [19], BP latent feature model [20], and Pitman-Yor (PY) mixture [21] (see the supplement for further details).", "startOffset": 206, "endOffset": 210}, {"referenceID": 2, "context": "Consider a DP mixture model [3], with cluster parameters \u03b8, assignments z, and observed data y.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Typical mean-field variational techniques introduce an artificial ordering of the parameters in the posterior, thereby breaking symmetry that is crucial to combining posteriors correctly using density multiplication [6].", "startOffset": 216, "endOffset": 219}, {"referenceID": 4, "context": "Unknown model size: While previous posterior merging procedures required a 1-to-1 matching between the components of the minibatch posterior and central posterior [5, 6], Bayesian nonparametric posteriors break this assumption.", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "Unknown model size: While previous posterior merging procedures required a 1-to-1 matching between the components of the minibatch posterior and central posterior [5, 6], Bayesian nonparametric posteriors break this assumption.", "startOffset": 163, "endOffset": 169}, {"referenceID": 9, "context": "For example, the optimization applies to the hierarchical Dirichlet process topic model [10] with topic word distributions \u03b8 and local-toglobal topic correspondences z, and to the beta process latent feature model [4] with features \u03b8 and This is equivalent to the KL-divergence regularization \u2212KL [ \u03b6o(zo)\u03b6i(zi)\u03b6 \u03c3 m(zm) \u2223\u2223\u2223\u2223\u2223\u2223 p(zi, zm, zo)].", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "For example, the optimization applies to the hierarchical Dirichlet process topic model [10] with topic word distributions \u03b8 and local-toglobal topic correspondences z, and to the beta process latent feature model [4] with features \u03b8 and This is equivalent to the KL-divergence regularization \u2212KL [ \u03b6o(zo)\u03b6i(zi)\u03b6 \u03c3 m(zm) \u2223\u2223\u2223\u2223\u2223\u2223 p(zi, zm, zo)].", "startOffset": 214, "endOffset": 217}, {"referenceID": 21, "context": "Such problems are solvable in polynomial time [22] by the Hungarian algorithm, leading to a tractable component identification step in the proposed streaming, distributed framework.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "1 Regularization lower bound For the Dirichlet process with concentration parameter \u03b1 > 0, p(zi, zm, zo) is the Exchangeable Partition Probability Function (EPPF) [23] p(zi, zm, zo) \u221d \u03b1|K|\u22121 \u220f", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 198, "endOffset": 202}, {"referenceID": 8, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 250, "endOffset": 253}, {"referenceID": 6, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 330, "endOffset": 333}, {"referenceID": 13, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 431, "endOffset": 435}, {"referenceID": 23, "context": "We compare the streaming, distributed procedure coupled with standard variational inference [24] (SDA-DP) to five state-of-the-art inference algorithms: memoized online variational inference (moVB) [13], stochastic online variational inference (SVI) [9] with learning rate (t+10)\u2212 1 2 , sequential variational approximation (SVA) [7] with cluster creation threshold 10\u22121 and prune/merge threshold 10\u22123, subcluster splits MCMC (SC) [14], and batch variational inference (Batch) [24].", "startOffset": 477, "endOffset": 481}], "year": 2015, "abstractText": "This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.", "creator": "LaTeX with hyperref package"}}}