{"id": "1511.02841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Symmetries and control in generative neural nets", "abstract": "we study generative nets which can control and orient observations, after being trained on real - life datasets. in order to zoom - in on an object, some spatial, color and index attributes are grouped by classifiers and attention cues. plugging these symmetry statistics in the generative layers of auto - classifiers - encoders ( ace ) appears to appear the leading direct way to simultaneously : i ) generate new equations with arbitrary attributes, from a given class, ii ) form the true \" style \", i. e., the low - dimensional latent manifold represent the \" essence \" of geometric data, through superfluous attributes often factored out, and iii ) organically control, i. e., move or modify objects within given observations. we demonstrate overall sharp improvement while orthogonal generative structures of shallow ace with spatial symmetry statistics, on simultaneously distorted mnist and index datasets.", "histories": [["v1", "Mon, 9 Nov 2015 20:49:03 GMT  (1842kb,D)", "https://arxiv.org/abs/1511.02841v1", null], ["v2", "Mon, 16 Nov 2015 17:49:51 GMT  (1097kb,D)", "http://arxiv.org/abs/1511.02841v2", null], ["v3", "Fri, 8 Apr 2016 21:38:31 GMT  (1097kb,D)", "http://arxiv.org/abs/1511.02841v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["galin georgiev"], "accepted": false, "id": "1511.02841"}, "pdf": {"name": "1511.02841.pdf", "metadata": {"source": "CRF", "title": "SYMMETRIES AND CONTROL IN GENERATIVE NEURAL NETS", "authors": ["Galin Georgiev"], "emails": ["galin.georgiev@gammadynamics.com"], "sections": [{"heading": "1 INTRODUCTION", "text": ""}, {"heading": "1.1 GENERATIVITY AND CONTROL.", "text": "Generating plausible but unseen previously observations appears, at least chronologically, to have been one of the hardest challenges for artificial neural nets. A generative net can \u201cdream-up\u201d new observations {x\u0302\u03bd}, each a vector in a high-dimensional space RN , by sampling from a white noise probability density p(z). This model density resides on a preferably low-dimensional space of latent variables z = {z(\u03ba)}Nlat\u03ba=1 . In order to create plausible new observations, the latent manifold has to encode the complexity of the set of P training observations {x\u00b5}P\u00b5=1 \u2282 RN . Generativity has a lot more to it than \u201cdreaming-up\u201d new random observations. It is at the heart of the control skills of a neural net. Visual biological nets, for example, capture existential motor information like location/shape and other attributes of an object and can act on it by moving or modifying it deterministically. Asking for this data compression to be as compact and low-dimensional as possible is therefore not only a general minimalist requirement. Learning and mastering control is a gradual process, which naturally starts by seeking and exploring only a few degrees of freedom.\nMoreover, the ability to modify an object implies an ability to first and foremost reconstruct it, with various degrees of precision. Not unlike human creativity, a fully generative net has to balance out and minimize terms with non-compatible objectives: a) a generative error term, which is responsible for converting random noise into plausible data, on the one hand, and b) a reconstruction error term which is responsible for meticulous reconstruction of existing objects, on the other."}, {"heading": "1.2 LEARNING FROM REAL-LIFE DATA.", "text": "From the recent crop of generative nets, section 2, only one appears to offer this desirable reconstruction via a low-dimensional latent manifold: the variational auto-encoders (VAE) Kingma & Welling (2014), Rezende et al. (2014). Their subset called Gibbs machines, has also far-reaching roots into information geometry and thermodynamics, which come in very handy. They perform well on idealized visual data sets like MNIST LeCun et al. (1998). Unfortunately, like the other generative nets, they do not cope well with more realistic images, when objects are spatially varied or, if there is heavy clutter in the background. These traits are simulated in the rotated-translated-scaled (RTS)\nar X\niv :1\n51 1.\n02 84\n1v 3\n[ cs\n.C V\n] 8\nA pr\n2 01\n6\nMNIST and translated-cluttered (TC) MNIST, Appendix B. We highlight the shortcomings of basic generative nets on Figure 1, for the simplest case of one-dimensional latent manifold per class. While simulating wonderfully on the original MNIST (top-left), even with Nlat = 1, the net fails miserably to learn the distorted data: The randomly \u201cdreamed-up\u201d samples {x\u0302\u03bd} are blurred and not plausible (top-right and bottom). Low latent dimensionality is not the culprit: latent manifolds with dimensions Nlat \u2265 100 do not yield much better results.\nFor the real-life CIFAR10 dataset, Krizhevsky (2009), the latent two-dimensional1 manifold of the class of horses, produced by the same architecture, is on the left of Figure 3. The training dataset has horses of different colors, facing both left and right, so the latent manifold tends to produce two-headed vague shapes of different colors."}, {"heading": "1.3 \u201cA HORSE, A HORSE! MY KINGDOM FOR A HORSE!\u201d 2", "text": "In order to get the horses back, we invoke the Gibbs thermodynamic framework. It allows adding non-energy attributes to the sampling distribution and modifying them, randomly or deterministically. These symmetry statistics, like location, size, angle, color etc, are factored-out at the start and factored back-in at the end. The auto-classifier-encoder (ACE) net with symmetry statistics was suggested in Georgiev (2015) and detailed in section 4 here. The latent manifolds it produces, for the above three MNIST datasets, are on Figure 2: With distortions and clutter factored out, the quotient one-dimensional latent manifold is clear and legible. The factorization is via transformations from the affine group Aff(2,R), which plays the role of the gauge group in field theory. The spatial symmetry statistics are the transformations parameters, computed via another optimizer net. The CIFAR10 horse class manifold, generated by ACE with spatial symmetry statistics, is on the right of Figure 3. We have horse-like creatures, which morph into giraffes as one moves up the grid!\nThe first successful application of Lie algebra symmetries to neural nets was in Simard et al. (2000). The recent crop of spatial attention nets Jadeberg et al. (2015), Gregor et al. (2015), Sermanet et al.\n1The color scheme \u201cappropriates\u201d at least one latent dimension, hence the need for more dimensions. 2Shakespeare (1592)\n(2014), Ba et al. (2014) optimize spatial symmetry statistics, corresponding to a given object inside an observation. An efficient calculation of symmetry statistics, for multiple objects, requires a classifier. Hence, generation and reconstruction on real-life datasets lead to an auto-encoder/classifier combo like ACE. Supplementing auto-encoders with affine transforms was first proposed in Hinton et al. (2011), where spatial symmetry statistics were referred to as \u201ccapsules\u201d. As suggested there, hundreds and thousands of capsules can in principle be attached to feature maps. Current attention\nnets produce one set of symmetry statistics per object (inside an observation). Incorporating convolutional feature maps in the encoder, and sampling from symmetry statistics at various depths, is yet to be engineered well for deep generative nets, see open problems 1, 2, section 5. Results from a shallow convolutional ACE are on Figure 4.\nSpatial symmetry statistics are vital in biological nets and can in principle be traced down experimentally. Feedback loops for attention data, vaguely reminiscent of Figure 5, have been identified between higher- and lower-level visual areas of the brain, Sherman (2005), Buffalo et al. (2010).\nFor colored images, one also needs the color symmetry statistics, forming a semigroup of nonnegative 3x3 matrices in the stochastic group3 S(3,R). As shown on the right of Figure 4, they help subdue the background color, and perhaps, more. In particle physics parlance, three-dimensional color images are described by chromodynamics with a minimum gauge group Aff(3,R)\u00d7 S(3,R). The rest of the paper is organized as follows: section 2 briefly overviews recent generative nets and details VAE-s objective function; section 3 outlines the theoretical framework of generative nets with control, highlighting the connections with information geometry and thermodynamics; section 4 presents the enhanced ACE architecture; the Appendices offer implementation and dataset details."}, {"heading": "2 GENERATIVE NETS AND THE LATENT MANIFOLD.", "text": "Latent manifold learning was pioneered for modern nets in Rifai et al. (2012). When a latent sample z\u03bd is chosen from a model density p(z), a generative net decodes it into a simulated observation x\u0302\u03bd , from a corresponding model density q(x\u0302). There are two scenarios:\na) the net has reconstruction capabilities, hence q(x) can in theory be evaluated on the training and testing observations {x\u00b5}. The objective is to minimize the so-called cross-entropy or negative log-likelihood, i.e., the expectation E(\u2212 log q(x))r(x), where E()r() is an expectation with respect to the empirical density r(). Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al. (2014), and v) Sohl-Dickstein et al. (2015). Except\n3The subgroup of matrices \u2208 GL(3,R), with entries in each row adding up to one, Poole (1995).\nfor NICE, the log-likelihood can not be exactly evaluated in practice, and is hence approximated. The first two models proxy q(x) with a certain conditional density q(x|x\u0303) and a Markov chain for the corrupted data x\u0303. The variational auto-encoders proxy the negative log-likelihood by a variational upper bound U(\u2212 log q(x)). Method v) conjures up a forward diffusion process from q(x) to p(z) and uses the backward diffusion process to \u201cdream-up\u201d new observations {x\u0302\u03bd}. b) the net has no reconstruction capabilities, hence one has to resort to an interpolation q(x\u0302) \u2192 q\u0302(x), in order to evaluate q() on the training and testing observations {x\u00b5}. The objective is to minimize directly or indirectly the negative log-likelihood E(\u2212 log q\u0302(x))r(x). Recent such model is the generative adversarial network (GAN) Goodfellow et al. (2014). It minimizes indirectly the above negative log-likelihood by combining a generative and a discriminative net, the latter tasked with distinguishing between the \u201cdreamed-up\u201d observations {x\u0302\u03bd} and training observations {x\u00b5}. Of these models, only the variational auto-encoders and the generative adversarial networks are designed to handle a low-dimensional latent manifold. As argued in sub-section 1.1, reconstruction, i.e. scenario a), is an indispensable part of the control skill set, hence we are left with the variational auto-encoder approach. As all generative nets, variational auto-encoders work in two regimes:\n\u2022 creative regime, with no data clamped onto the net and sampling from p(z), and \u2022 non-creative regime, with the training or testing observations {x\u00b5} fed to the input layer\nof the net. Variational auto-encoders sample in this regime from a different closed-form conditional posterior model density p(z|x\u00b5).\nIn order to do reconstruction, variational auto-encoders also introduce a conditional model reconstruction density prec(x\u00b5|z). In non-creative regime, the reconstruction error at the output layer of the net is the expectation E(\u2212 log prec(x\u00b5|z))p(z|x\u00b5). In the creative regime, we have a joint model density p(x\u00b5, z) := prec(x\u00b5|z)p(z). The data model density q(x\u00b5) is the implied marginal:\nq(x\u00b5) = \u222b p(x\u00b5, z)dz = p(x\u00b5, z)\nq(z|x\u00b5) , (2.1)\nfor some implied posterior conditional density q(z|x\u00b5) which is generally intractable, q(z|x\u00b5) 6= p(z|x\u00b5). The full decomposition of our minimization target - the negative log-likelihood \u2212 log q(x\u00b5) - is easily derived via the Bayes rules, Georgiev (2015), section 3:\n\u2212 log q(x\u00b5) = E(\u2212 log prec(x\u00b5|z))p(z|x\u00b5)\ufe38 \ufe37\ufe37 \ufe38 reconstruction error +D(p(z|x\u00b5)||p(z))\ufe38 \ufe37\ufe37 \ufe38 generative error \u2212D(p(z|x\u00b5)||q(z|x\u00b5))\ufe38 \ufe37\ufe37 \ufe38 variational error , (2.2)\nwhere D(||) is the Kullback-Leibler divergence. The reconstruction error measures the negative likelihood of getting x\u00b5 back, after the transformations and randomness inside the net. The generative error is the divergence between the generative densities in the non-creative and creative regimes. The variational error is an approximation error: it is the price variational auto-encoders pay for having a tractable generative density p(z|x\u00b5) in the non-creative regime. It is hard to compute, although some strides have been made, Rezende & Mohamed (2015). For the Gibbs machines discussed below, it was conjectured that this error can be made arbitrary small, Georgiev (2015)."}, {"heading": "3 THE THEORY. CONNECTIONS WITH INFORMATION GEOMETRY AND THERMODYNAMICS.", "text": "A theoretical framework for universal nets was recently outlined in Georgiev (2015). Some of the constructs there, like the ACE architecture, appeared optional and driven solely by requirements for universality. We summarize and generalize the framework in the current context and argue that the ACE architecture, or its variations, are indispensable for generative reconstructive nets.\n1. Information geometry and Gibbs machines: the minimization of the generative error in (2.2) leads to sampling from Gibbs a.k.a. exponential class of densities. It follows from the probabilistic or variational Pythagorean theorem, Chentsov (1968), which underlies modern estimation theory, and is pervasive in information geometry, Amari & Nagaoka\n(2000). In the case of Laplacian 4 generative densities, and conditionally independent latent variables z = {z(\u03ba)}Nlat\u03ba=1 , one has:\np(z|x\u00b5) \u223c e \u2212\n\u2211Nlat \u03ba=1 p (\u03ba) \u00b5 |z\n(\u03ba)\u2212m(\u03ba)\u00b5 |, (3.1)\nwhere the means {m(\u03ba)\u00b5 } are symmetry statistics, the absolute value terms are sufficient statistics and the inverse scale momenta {p(\u03ba)\u00b5 } are Lagrange multipliers, computed so as to satisfy given expectations of the sufficient statistics. The Gibbs density class leads to:\n2. Thermodynamics and more symmetry statistics: The Gibbs class is also central in thermodynamics because it is maximum-entropy class and allows to add fluctuating attributes, other than energy. These additions are not cosmetic and fundamentally alter the dynamics of the canonical distribution, Landau & Lifshitz (1980), section 35. They can be any attributes: i) spatial attributes, as in the example below; ii) color attributes, as introduced in subsection 1.3, and others. For multiple objects, one needs specialized nets and a classifier to optimize them. This leads to:\n3. Auto-classifiers-encoder (ACE) architecture, section 4: Since classification labels are already needed above, the latent manifold is better learned: i) via supervised reconstruction, and ii) with symmetry statistics used by decoder. This leads to:\n4. Control: With symmetry statistics in the generative layer, the net can organically move or modify the respective attributes of the objects, either deterministically or randomly. The ACE architecture ensures that the modifications stay within a given class.\nExample: An important special case in visual recognition are the spatial symmetry statistics, which describe the location, size, stance etc of an object. For a simple gray two-dimensional image x\u00b5 on N pixels e.g., two of its spatial symmetry statistics are the coordinates (h\u00b5, v\u00b5) of its center of mass, where the \u201dmass\u201d of a pixel is its intensity. Assuming independence, one can embed a translational invariance in the net, multiplying (3.1) by the spatial symmetry statistics (SSS) conditional density:\npSSS(z|x\u00b5) \u223c e \u2212p(h)\u00b5 |z (h)\u2212h\u00b5|\u2212p(v)\u00b5 |z (v)\u2212v\u00b5|, (3.2)\nwhere z(h), z(v) are two new zero-mean latent random variables, responsible respectively for horizontal and vertical translation. If (h,v) are the vectors of horizontal and vertical pixel coordinates, the image is centered at the input layer via the transform (h,v)\u2192 (h\u2212 h\u00b5,v\u2212 v\u00b5). This transformation is inverted, before reconstruction error is computed.\nWhen rescaled and normalized, (3.2) is the quantum mechanical probability density of a free particle, in imaginary space/time and Planck constant } = 1. Furthermore, for every observation x\u00b5, there could be multiple or infinitely many latents {z(\u03ba)\u00b5 }L\u03ba=1, L \u2264 \u221e, and x\u00b5 is merely a draw from a probability density prec(x\u00b5|z). In a quantum statistics interpretation, latents are microscopic quantum variables, while observables like pixels, are macroscopic aggregates. Observations represent partial equilibria of independent small parts of the expanded (by a factor of L) data set."}, {"heading": "4 ACE WITH SYMMETRY STATISTICS.", "text": "The ACE architecture with symmetry statistics is on Figure 5. As in the basic ACE, training is supervised i.e. labels are used in the auto-encoder and every class has a dedicated decoder, with unimodal sampling in the generative layer of each class. The sampling during testing is instead from a mixture of densities, with mixture weights {\u03c9\u00b5,c}NCc=1 for the \u00b5-th observation, for class c, produced by the classifier. The posterior densitiy from section 2 becomes5:\np(z|x\u00b5) = NC\u2211 c=1 \u03c9\u00b5,cp(z|x\u00b5, c). (4.1)\n4 The Laplacian density is not in the exponential class, but is a sum of two exponential densities which are in the exponential class in their respective domains.\n5Using a similar mixture of posterior densities, but different architecturally conditional VAEs, were proposed in the context of semi-supervised learning in Kingma et al. (2014).\nAfter interim symmetry statistics are computed in box 0.3 on Figure 5, they are used to transform the input (box 0.4), before it is sent for reconstruction and classification. The inverse transformation is applied right before the calculation of reconstruction error.\nPlugging the symmetry statistics in the latent layers allows to deterministically control the reconstructed observations. Alternatively, sampling randomly from the symmetry statistics, organically \u201caugments\u201d the training set. External augmentation is known to improve significantly a net\u2019s classification performance Ciresan et al. (2012), Krizhevsky et al. (2012). This in turn improves the quality of the symmetry statistics and creates a virtuous feedback cycle."}, {"heading": "5 OPEN PROBLEMS.", "text": "1. Test experimentally deep convolutional ACE-s, with (shared) feature maps, both in the classifier and the encoder. From feature maps at various depths, produce corresponding generative latent variables. Add symmetry statistics to latent variables at various depths. 2. Produce separate symmetry statistics for separate feature maps in generative nets, in the spirit of Hinton et al. (2011)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We appreciate discussions with Nikola Toshev, Stefan Petrov and their help with CIFAR10."}, {"heading": "B DISTORTED MNIST.", "text": "The two distorted MNIST datasets replicate Jadeberg et al. (2015), Appendix A.3, although different random seeds and implementation details may cause differences. The rotated-translated-scaled (RTS) MNIST is on 42x42 canvas with random +/- 45\u25e6 rotations, +/- 7 pixels translations and 1.2/0.7 scaling. The translated-cluttered (TC) MNIST has the original image randomly translated across a 60x60 canvas, with 6 clutter pieces of size 6x6, extracted randomly from randomly picked other images and added randomly to the background."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "In ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric", "Yosinski", "Jason"], "venue": "In ICML,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "A backward progression of attentional effects in the ventral stream", "author": ["E.A. Buffalo", "P. Fries", "R. Landman", "H. Liang", "R. Desimone"], "venue": "Proc. Nat. Acad. Sci.,", "citeRegEx": "Buffalo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buffalo et al\\.", "year": 2010}, {"title": "Nonsymmetrical distance between probability distributions, entropy and the theorem of Pythagoras", "author": ["N.N. Chentsov"], "venue": "Mathematical notes of the Academy of Sciences of the USSR,", "citeRegEx": "Chentsov,? \\Q1968\\E", "shortCiteRegEx": "Chentsov", "year": 1968}, {"title": "Multi-column deep neural networks for image classification", "author": ["Ciresan", "Dan", "Meier", "Ueli", "Schmidhuber", "Juergen"], "venue": null, "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily", "Chintala", "Soumith", "Szlam", "Arthur", "Fergus", "Rob"], "venue": null, "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "NICE: Non-linear independent components estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Towards universal neural nets: Gibbs machines and ACE", "author": ["Georgiev", "Galin"], "venue": null, "citeRegEx": "Georgiev and Galin.,? \\Q2015\\E", "shortCiteRegEx": "Georgiev and Galin.", "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Transforming auto-encoders", "author": ["Hinton", "Geoffrey", "Krizhevsky", "Alex", "S. Wang"], "venue": "In ICANN,", "citeRegEx": "Hinton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Durk P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Durk P", "Rezende", "Danilo J", "Mohamed", "Shakir", "Welling", "Max"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Statistical Physics, Part 1, 3rd edition", "author": ["L.D. Landau", "E.M. Lifshitz"], "venue": null, "citeRegEx": "Landau and Lifshitz,? \\Q1980\\E", "shortCiteRegEx": "Landau and Lifshitz", "year": 1980}, {"title": "The stochastic group", "author": ["Poole", "David"], "venue": "American Mathematical Monthly,", "citeRegEx": "Poole and David.,? \\Q1995\\E", "shortCiteRegEx": "Poole and David.", "year": 1995}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In JMLR,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Thalamic relays and cortical functioning", "author": ["Sherman", "Murray"], "venue": "Progress in Brain Research,", "citeRegEx": "Sherman and Murray.,? \\Q2005\\E", "shortCiteRegEx": "Sherman and Murray.", "year": 2005}, {"title": "Transformation invariance in pattern recognition: Tangent distance and propagation", "author": ["P. Simard", "Cun", "Y. Le", "J. Denker", "B. Victorri"], "venue": "Int. J. Imag. Syst. Tech.,", "citeRegEx": "Simard et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2000}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Sohl-Dickstein", "Jascha", "Weiss", "Eric", "Maheswaranathan", "Niru", "Ganguli", "Surya"], "venue": "In ICML,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Optimizer is Adam, Kingma & Ba (2015), stochastic gradient descent back-propagation, learning rate = 0.0015 for MNIST and 0.0005 for CIFAR10, decay = 50 epochs, batch size = 250. We used only one standard set of hyper-parameters per dataset and have not done hyper-parameter optimizations. Convolutional weights are initialized uniformly in (\u22121, 1) and normalized by square root of the product", "author": ["Theano platform", "Bastien"], "venue": null, "citeRegEx": "platform and Bastien,? \\Q2012\\E", "shortCiteRegEx": "platform and Bastien", "year": 2012}, {"title": "2015), produces six affine spatial symmetry statistics (box 0.2 in Figure 5). This net has 2 convolutional hidden layers, with 20 5x5 filters each, with 2x2 max-poolings between layers, and a fully-connected layer of size 50. Figure 3: Layer sizes 3072-2048-2048(2x10)-(2048x10)-(2048x10)-(3072x10) for the auto-encoder branch, same classifier as in Fig 2. The symmetry statistics net has 2 convolutional hidden layers, with 32-64 3x3 filters", "author": ["Jadeberg"], "venue": null, "citeRegEx": "Jadeberg,? \\Q2048\\E", "shortCiteRegEx": "Jadeberg", "year": 2048}], "referenceMentions": [{"referenceID": 18, "context": "From the recent crop of generative nets, section 2, only one appears to offer this desirable reconstruction via a low-dimensional latent manifold: the variational auto-encoders (VAE) Kingma & Welling (2014), Rezende et al. (2014). Their subset called Gibbs machines, has also far-reaching roots into information geometry and thermodynamics, which come in very handy.", "startOffset": 208, "endOffset": 230}, {"referenceID": 18, "context": "From the recent crop of generative nets, section 2, only one appears to offer this desirable reconstruction via a low-dimensional latent manifold: the variational auto-encoders (VAE) Kingma & Welling (2014), Rezende et al. (2014). Their subset called Gibbs machines, has also far-reaching roots into information geometry and thermodynamics, which come in very handy. They perform well on idealized visual data sets like MNIST LeCun et al. (1998). Unfortunately, like the other generative nets, they do not cope well with more realistic images, when objects are spatially varied or, if there is heavy clutter in the background.", "startOffset": 208, "endOffset": 446}, {"referenceID": 21, "context": "We have horse-like creatures, which morph into giraffes as one moves up the grid! The first successful application of Lie algebra symmetries to neural nets was in Simard et al. (2000). The recent crop of spatial attention nets Jadeberg et al.", "startOffset": 163, "endOffset": 184}, {"referenceID": 21, "context": "We have horse-like creatures, which morph into giraffes as one moves up the grid! The first successful application of Lie algebra symmetries to neural nets was in Simard et al. (2000). The recent crop of spatial attention nets Jadeberg et al. (2015), Gregor et al.", "startOffset": 163, "endOffset": 250}, {"referenceID": 9, "context": "(2015), Gregor et al. (2015), Sermanet et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 23, "context": "(2014), or Figure 3 (d) in Sohl-Dickstein et al. (2015). The improvement in Denton et al.", "startOffset": 27, "endOffset": 56}, {"referenceID": 6, "context": "The improvement in Denton et al. (2015) is due to so-called Laplacian pyramids, and can be overlayed on any core generative model.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": "(2014), Ba et al. (2014) optimize spatial symmetry statistics, corresponding to a given object inside an observation.", "startOffset": 8, "endOffset": 25}, {"referenceID": 0, "context": "(2014), Ba et al. (2014) optimize spatial symmetry statistics, corresponding to a given object inside an observation. An efficient calculation of symmetry statistics, for multiple objects, requires a classifier. Hence, generation and reconstruction on real-life datasets lead to an auto-encoder/classifier combo like ACE. Supplementing auto-encoders with affine transforms was first proposed in Hinton et al. (2011), where spatial symmetry statistics were referred to as \u201ccapsules\u201d.", "startOffset": 8, "endOffset": 416}, {"referenceID": 3, "context": "Feedback loops for attention data, vaguely reminiscent of Figure 5, have been identified between higher- and lower-level visual areas of the brain, Sherman (2005), Buffalo et al. (2010). For colored images, one also needs the color symmetry statistics, forming a semigroup of nonnegative 3x3 matrices in the stochastic group3 S(3,R).", "startOffset": 164, "endOffset": 186}, {"referenceID": 17, "context": "Latent manifold learning was pioneered for modern nets in Rifai et al. (2012). When a latent sample z\u03bd is chosen from a model density p(z), a generative net decodes it into a simulated observation x\u0302\u03bd , from a corresponding model density q(x\u0302).", "startOffset": 58, "endOffset": 78}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al.", "startOffset": 103, "endOffset": 191}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al. (2014), and v) Sohl-Dickstein et al.", "startOffset": 103, "endOffset": 323}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al. (2014), and v) Sohl-Dickstein et al. (2015). Except The subgroup of matrices \u2208 GL(3,R), with entries in each row adding up to one, Poole (1995).", "startOffset": 103, "endOffset": 360}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al. (2014), and v) Sohl-Dickstein et al. (2015). Except The subgroup of matrices \u2208 GL(3,R), with entries in each row adding up to one, Poole (1995).", "startOffset": 103, "endOffset": 460}, {"referenceID": 4, "context": "It follows from the probabilistic or variational Pythagorean theorem, Chentsov (1968), which underlies modern estimation theory, and is pervasive in information geometry, Amari & Nagaoka", "startOffset": 70, "endOffset": 86}, {"referenceID": 11, "context": "Using a similar mixture of posterior densities, but different architecturally conditional VAEs, were proposed in the context of semi-supervised learning in Kingma et al. (2014).", "startOffset": 156, "endOffset": 177}, {"referenceID": 5, "context": "External augmentation is known to improve significantly a net\u2019s classification performance Ciresan et al. (2012), Krizhevsky et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 5, "context": "External augmentation is known to improve significantly a net\u2019s classification performance Ciresan et al. (2012), Krizhevsky et al. (2012). This in turn improves the quality of the symmetry statistics and creates a virtuous feedback cycle.", "startOffset": 91, "endOffset": 139}, {"referenceID": 10, "context": "Produce separate symmetry statistics for separate feature maps in generative nets, in the spirit of Hinton et al. (2011).", "startOffset": 100, "endOffset": 121}], "year": 2016, "abstractText": "We study generative nets which can control and modify observations, after being trained on real-life datasets. In order to zoom-in on an object, some spatial, color and other attributes are learned by classifiers in specialized attention nets. In fieldtheoretical terms, these learned symmetry statistics form the gauge group of the data set. Plugging them in the generative layers of auto-classifiers-encoders (ACE) appears to be the most direct way to simultaneously: i) generate new observations with arbitrary attributes, from a given class; ii) describe the low-dimensional manifold encoding the \u201cessence\u201d of the data, after superfluous attributes are factored out; and iii) organically control, i.e., move or modify objects within given observations. We demonstrate the sharp improvement of the generative qualities of shallow ACE, with added spatial and color symmetry statistics, on the distorted MNIST and CIFAR10 datasets.", "creator": "LaTeX with hyperref package"}}}