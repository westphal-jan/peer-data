{"id": "1511.02954", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Reducing the Training Time of Neural Networks by Partitioning", "abstract": "this information presents a new method for pre - training neural networks that can decrease up total training time for a neural network while maintaining it final performance, which motivates its use onboard deep neural networks. by replacing the training task in multiple training subtasks with sub - models, which can be performed independently and in parallel, it is detected that the size behind the component - models reduces almost quadratically with the number of subtasks created, quickly scaling down the sub - models used for the pre - training. the sub - models are easily merged to provide a pre - trained initial computation of weights for the original model. the proposed method is split of the other sections of the training, such as architecture of agile neural network, training method, and objective, making it compatible with also wide range as nonlinear phenomena. the speedup can loss your performance is validated experimentally on mnist and on cifar10 data sets, also showing that even realizing the subtasks exactly can decrease the training time. moreover, we show that larger models further present higher speedups and skepticism about the benefits of the method in distributed learning systems.", "histories": [["v1", "Tue, 10 Nov 2015 01:20:51 GMT  (77kb)", "http://arxiv.org/abs/1511.02954v1", "Figure 2b has lower quality due to file size constraints"], ["v2", "Sun, 3 Jan 2016 17:18:06 GMT  (79kb)", "http://arxiv.org/abs/1511.02954v2", "Figure 2b has lower quality due to file size constraints"]], "COMMENTS": "Figure 2b has lower quality due to file size constraints", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["conrado s miranda", "fernando j von zuben"], "accepted": false, "id": "1511.02954"}, "pdf": {"name": "1511.02954.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["contact@conradomiranda.com,", "vonzuben@dca.fee.unicamp.br"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n02 95\n4v 1\n[ cs\n.N E\n] 1\n0 N\nov 2\n01 5"}, {"heading": "1 INTRODUCTION", "text": "Research in deep learning using neural networks has increased significantly over the last years. This occurred due to the ability of deep neural networks to achieve higher performance when compared to other methods on problems with a large amount of data (Bengio et al., 2015) and advances in computing power, such as the use of graphic processing units (GPUs) (Raina et al., 2009).\nDespite the advances obtained by using GPUs for training deep neural networks, this step still can take a lot of time, which affects negatively both research and industry as new methods take longer to be tested and deployed. Some researches have focused on speeding up deep neural networks in general, including proposals based on hardware, such as using limited numerical precision (Gupta et al., 2015), which could increase the number of computing units on the hardware, and software, such as using Fourier transform to compute a convolution (Vasilache et al., 2014) and developing a base library for deep neural networks (Chetlur et al., 2014). In this paper, we focus on existing research interested in decreasing the training time, as these approaches are closer to the proposed method. However, we highlight that these improvements are not mutually exclusive and can be used together.\nKrizhevsky (2014) proposed a mixture of data and model parallelism over GPUs in a single machine based on the type of the layer, exploiting their particularities for increased speed. Essentially, the convolutional layers exploit data parallelism, since they are the most computing intensive part of the neural network, and the fully-connected layers exploit model parallelism, since they have most of the parameters and may not fit in a single GPU. This leads to a significant speedup in comparison to other existing methods for training convolutional neural networks over a GPU cluster.\nDistBelief (Dean et al., 2012) is another framework to speedup the training of large neural network by exploiting parallelism, but it focuses on clusters of computers. The data parallelism is exploited by dividing the data set in shards that are fed to different groups of computers, where each group replicates the full model and are synchronized through a parameter server. For the model parallelism, a locally connected neural network is used in order to reduce the communication among machines that jointly represent one replica of the model. Since the model is locally connected, only the edges\nthat cross the machine boundary need to be transmitted. This framework is extended in Coates et al. (2013) to use GPUs as computing units.\nThese methods of model parallelization to handle large neural networks require communication between the multiple computing units, which characterizes an overhead in the learning process. Moreover, even if a single GPU can hold the full model, all computations of a layer may not be performed in parallel, leading to an increase in test and training time. Therefore, using smaller models can speedup the task, but smaller models may not be able to achieve the same performance attainable by larger models.\nHowever, the learning process does not have to adjust the full model in all iterations. Pre-training methods exploit this characteristic by providing adjusted, instead of fully random, initial conditions for the training to continue with the full model.\nA well known example of this is unsupervised pre-training (Erhan et al., 2010), where the network is greedly trained layer-wise to reconstruct its input. Since only the parameters of the current layer have to be learned, this pre-training requires less memory and processing time to fit the model. After the initial iterations of pre-training, the full model is built stacking the layers on top of each other and the training continues, performing a fine-tuning of the parameters found. However, two important limitations of unsupervised pre-training are that it requires a generative model of the data to be known, which can be different from the desired task, and that the layers must be learnt sequentially.\nIn this paper, we introduce a new method of partitioning the network to perform a pre-training. Instead of partitioning layer-wise, the proposed method partitions the neural network in smaller neural networks whose tasks are equivalent to the original training task, avoiding the need to create substitute tasks such as generative costs, for pre-training. Moreover, the subtasks are independent of each other, which allows them to be learnt in parallel without communication. Another advantage of the proposed method is that, since the task is kept the same, the learning algorithm used to adjust the parameters can also be the same for all stages. Therefore, it can be viewed as a higher level method and is compatible with existing training strategies. After the proposed pre-training is complete, the obtained smaller neural networks are merged and used as initial condition for the original neural network.\nThe new method is also straightforward to implement and decreases the number of parameters of the subtasks quadratically in the number of subtasks created, thus being characterized as a highly scalable approach. We perform two experiments, one with MNIST and another with CIFAR10 data sets, to show that the training time required is indeed reduced while the generalization capability may not be affected by the new method. Furthermore, the experiments also show that the method can be used to speedup the training even when the subtasks are performed sequentially, that larger models have higher speedups, and that the sub-models learn different representations for the data, which is advantageous for the performance when these sub-models are merged.\nThis paper is organized as follows. Section 2 provides the motivation, description and main advantages of the method proposed. Section 3 describes the experiments performed to test the new method and discusses the obtained results. Finally, Section 4 provides a summary of the findings and future research directions."}, {"heading": "2 PARTITIONING NEURAL NETWORKS", "text": "This section is divided in three parts. Section 2.1 further clarifies the problem which is being solved and how the solution is related to existing methods in the literature. Section 2.2 describes the method itself, and Section 2.3 analyzes the possible benefits achievable by the method, being some of them confirmed in the experiments performed."}, {"heading": "2.1 MOTIVATION", "text": "Large neural networks are able to achieve better performance than smaller ones, but are considerably more expensive to learn and use. They may require special methods for training if they do not fit in a single computing unit (Dean et al., 2012) and can be used after trained to provide guidance to improve smaller networks (Hinton et al., 2015). Therefore, even if a large neural network will not be\nused for the desired task due to its high computational cost, training large networks is still important for guiding the improvement of smaller ones.\nAnother common solution to improve performance achievable by smaller neural networks is through ensembles (Hansen & Salamon, 1990), where predictions of multiple neural networks are combined to provide more accurate outputs. It is important to highlight that ensembles only work because the multiple models provide diverse predictions for the same data (Perrone & Cooper, 1992). Therefore, the improvement in performance relies on the distinct behavior of the predictors.\nSince the performance achieved by a neural network may depend on its initialization, there has been a search for good initialization methods (Glorot & Bengio, 2010; Sutskever et al., 2013). Nonetheless, neural networks seem to be able to achieve good and diverse local minima or saddle points (Dauphin et al., 2014; Choromanska et al., 2014; Pascanu et al., 2014), so they can easily be used as components of ensembles to improve performance. In the experiment with the MNIST data set, we will show that the sub-models in fact achieve diverse performance even when they are small, which indicates that they learn non-redundant features and provides variety in the features provided as initial condition for the merged model.\nBut if we view an ensemble of trained neural networks as a single large neural network with a constraint of no connection between layers of different base networks, it raises the question of whether it is possible to use smaller networks to properly initialize larger ones. In the initial learning phase, isolated sets of parameters are going to be adjusted independently, thus avoiding the necessity of using methods devoted to train the full network with all adjustable parameters."}, {"heading": "2.2 PARTITIONING METHOD", "text": "Consider a neural network described by a directed graph from the input to the output that performs some computation as information flows through the graph. The case of undirected neural networks will be discussed later, and it will be shown that they do not modify the algorithm very much.\nThe proposed partitioning method first divides the neurons in the large neural network in disjoint sets, except for neurons in the inputs and outputs layers, which must be present in all sets. In this partition, one filter of a convolutional layer corresponds to an atomic unit, since all the computed activations share the same parameters, and any layer that has internal parameters or whose activation depends on the individual input values instead of their aggregate, such as normalization layers (Krizhevsky et al., 2012), must be replaced by multiple similar, parallel layers.\nFor each set of neurons, the only sources and sinks in the vertex-induced subgraph must be the input and output neurons of the full neural network, respectively. Therefore, the following holds: 1) each subset defines a complete flow of information from the input to the output; 2) each vertex-induced subgraph defines a valid smaller neural network; 3) the original cost function can be learnt on each subgraph; 4) every neuron of the full neural network is allocated to exactly one of the smaller networks, except for the input and output neurons; and 5) every parameter is allocated to at most one smaller network, except for the parameters of the output neurons.\nAs an example, consider the neural network on the left of Figure 1, which will be separated into two sub-models. The neurons are grouped in the sets S1 = X\u222aL11\u222aL21\u222aY andS2 = X\u222aL12\u222aL22\u222aY , which satisfy the conditions imposed before. When defining the vertex-induced subgraphs, which\nare shown on the right of Figure 1, the connections between L11 and L22 and between L12 and L21 are not allocated to any subgraph. The extension for larger number of sub-models is straightforward.\nSince the subgraphs are neural networks by themselves and have the same input and output as the original neural network, the next step in the method is to train these smaller models independently in the original task. Moreover, since there is no communication among these networks, they can be trained in parallel. Once done, almost all the weights and biases can be copied from the sub-models to the original model directly, except for the output parameters, while setting to zero all parameters associated with edges that were not trained.\nAs each sub-model learns to predict the output by themselves, simply copying the weights would directly sum the contributions of each sub-model, besides each one providing different values for the biases. While the sum of contributions would not be a problem for classification problems, since the softmax is scale-independent, it is a problem for regressions tasks. Therefore, to normalize the contributions, each weight to the output is divided by the number K of subtasks created and the output bias is given by the mean of the learnt biases of all K sub-models, that is,\nby =\n\u2211K i=1 b y i\nK , W\ny ij =\nW\u0302 y ij\nK , (1)\nwhere byi , i = 1, . . . ,K, are the output biases learned by each sub-model. This change on the parameters is equivalent to computing the mean of the sub-models on linear outputs or the geometric mean on softmax outputs, which are the common methods to compute an ensemble prediction.\nAn alternative to changing the weights to the output layer, which would be necessary in the case of undirected neural networks, since changing the weights towards the output affect the activation of the hidden units from the input, is to sum the biases and change the input/output activation function. Instead of scaling the weights and biases for the output layer, we just have to scale the input of the functions that describe how the data is generated, as higher activations will occur. Since the internal parts of the neural network were trained independently, they can be connected as described before.\nOnce the parameters of the full model are adjusted based on those from the sub-models, the finetunning can occur by training the full pre-trained model."}, {"heading": "2.3 ANALYSIS OF THE METHOD", "text": "From the description of the partitioning method, it is clear that each sub-model of the network will have less parameters to train. After the partition, there are three possibilities for changing the number of parameters: 1) keep the same number as the original network, which happens to the output biases; 2) reduce linearly, which happens to all other biases and input and output weights, since the number of neurons reduces linearly and the input and output are copied; and 3) reduce quadratically, which happens to the weights between internal neurons.\nSince most of the parameters in deep neural networks are concentrated in connections between internal layers, we can expect an almost quadratic reduction in the number of parameters in relation to the number of partitions. This indicates that even a distributed, large neural network can quickly become small enough to fit in a single machine, which completely eliminates the communication cost between nodes during the initial training phase. Moreover, it is possible to further partition a network that is already small enough to fit a single machine, which reduces the number of operations required for the training and may further speedup the process.\nThis not only decreases the communication overhead but can also reduce the number of machines required during the initial phase of the training. Consider, for instance, a large neural network that requires four machines connected to be represented. Due to the space overhead for loading values from other computers, it might be possible that partitioning the network in two sub-models, each with about a quarter the original size, allows one of the reduced models to fit in a single machine. So, besides not requiring communication between the original machines to train the reduced neural networks, only two of the four original machines are required for this training step, one for each sub-model of the full model, supposing that these sub-models are going to be trained in parallel.\nFurthermore, we conjecture that the pre-training should not affect the final performance of the neural network if the partitions have different initial conditions and are flexible enough. As discussed in Section 2.1, the improved performance achieved by an ensemble relies on each of its models\nproducing diverse predictions. Moreover, these diverse predictions may be achieved by distinct representations of the data in the internal nodes of the neural network.\nIn this case, after merging the sub-models, new lower-level features for the data are available to each neuron that was learned in one of the sub-models, which allows them to exploit these new features to provide better outputs. Since the new connections have zero value, the network does not have to revert a bad initial value for the parameters and can directly take advantage of the promising representations. Moreover, since the representations were also learned from the data, they should provide useful features and the performance should not be affected by the smaller parametrization during the pre-training."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "A GeForce GTX 760 was used in these experiments and they were conducted on neural networks small enough to fit the memory of the GPU, including the parameters, intermediary values and data batch, but large enough to prevent all the layer-wise processing to be done completely in parallel. This avoids overheads due to communication between nodes that may be specific to the method used to perform model parallelism, while allowing the proposed method to show its improvements.\nWe present two experiments to evaluate the method proposed in this paper. The first uses a small network to classify digits on MNIST (LeCun et al., 1998) and focuses on analysing the effects of the number of partitions created on the training time and performance. The second uses a larger network to classify the images on CIFAR10 (Krizhevsky, 2009) and considers different number of epochs for the pre-training."}, {"heading": "3.1 MNIST", "text": "For this task, the neural network was composed of three layers, all with ReLU activation. The first two layers are convolutions with 20 and 50 filters, respectively, of size 5x5, both followed by maxpooling of size 2x2, while the last layer is fully connected and composed of 500 hidden units with dropout probability of 0.5. The learning was performed by gradient descent with learning rate of 0.1 and momentum of 0.9, which were selected using the validation set to provide the best performance for the full model without pre-training, and minibatches of 500 samples. When the parameters of the sub-models are copied to the full neural network, the accumulated momentums of each training task are also copied to the respective sub-set of the full network and adjusted like the associated parameters. Hence, the accumulated momentums for the weights and biases for the output layer are computed using Eq. (1) and the other accumulated momentums are copied directly.\nThe neural network was partitioned in 2, 5 and 10 sub-models with the same number of neurons in each layer, besides the baseline of 1 sub-model, which is the standard neural network model. Each sub-model was trained for 100 epochs on the full data set, followed by another 100 epochs on the full model with the merged parameters. The performance of each sub-model was averaged to obtain the results during pre-training, as they are considered independent models and do not belong to an ensemble. We also compared the performance of merging the sub-models before any pre-training, which is equivalent to setting the initial values for the inter-model weights to zero. However, since this approach achieved slightly worse results than the baseline, the results are not shown.\nTests with different numbers of sub-models used the same initialization for the weights, avoiding fluctuation of the results due to random number generation. A total of 50 runs with different initialization were performed and the averages of the runs are reported.\nTable 1 presents the reduction in the number of parameters and computing time of each pre-training epoch with varying number of sub-models. As expected by the analysis in Section 2.3, the number of parameters is reduced almost quadratically with the number of sub-models, significantly decreasing the memory requirements for their storage. The speedup, on the other hand, presents a diminishing return effect. Although training on different machines would be faster for all values of K, training on a single machine would only be faster for K = 2, which provides an argument for partitioning neural networks even when they fit on memory, as already discussed in Section 2.3. This reduction occurs because the neural network is relatively small already, so reducing it by a large amount leaves unused cores of the GPU during training.\nAs anticipated in Section 2.1, it is important that the sub-models learn different representations and predictions for the data, as otherwise there would be redundant information provided by the initialization. To measure the diversity, we evaluated the probability given by each sub-model to the correct class on each sample in the test set just before merging the sub-models and the resulting images are shown in Figure 2. It is clear that in both cases the sub-models indeed have different predictions for the data, with more diverse values when smaller sub-models are considered, and the case for 5 sub-models, not shown, presenting an intermediary diversity. This result helps to lessen the concern about the size of sub-models used and provides support that different initializations for each sub-model may be enough to provide the required diversity. However, another important factor to take advantage of the sub-models is present in Figure 3, which shows that the sub-models have not converged before the merge, since this could have reduced the diversity present in the sub-models.\nFigure 3 shows the mean loss and classification error for the test set. It is clear that the pre-training method showed faster convergence to the same performance than the full training, providing evidence for the conjecture in Section 2.3 that the proposed method should not affect the performance of the network. The moment when the models are merged is clearly marked by the step-like change\nin the curves. One noticeable result present in this image is that using 2 or 5 sub-models provided similar performance in similar time scales, although 5 sub-models have faster iterations. This occurs because, although the 5 smaller models are able to learn diverse representations, these representations do not have the same quality level as the ones learned with 2 sub-models due to the flexibility of the sub-models. When the 5 sub-models are merged, the learning algorithm has to adjust the pre-trained features more, requiring more time on the full model. This also explains the curve of classification error for 10 sub-models, as these models have even less flexibility and a large part of the learning has to be performed after the merge occurred.\nIt is important to highlight that the speedup achieved was relatively small due to the size of the network. On larger networks, the benefit of partitioning the neural network during the first part of the training should be higher, as will be shown in the next section."}, {"heading": "3.2 CIFAR10", "text": "The neural network and parameters used for these experiments are the same used by Krizhevsky et al. (2012) in the CUDA-ConvNet library1, which is composed of two convolutional layers with ReLU activation, each one followed by normalization and pooling layers. After that, two locally connected layers with ReLU activation and a fully connected layer complete the network. The details of the network and parameters can be found in the source code of the library.\nThe neural network has more than 2.1 million parameters and we fix the number of partitions to 2 and vary the iteration in which the sub-models are merged. Since the datapath in the original network presents normalization layers, it is not possible to apply the pre-training algorithm directly, since these layers would require communication between the sub-models. To solve this issue, as discussed in Section 2.2, we duplicate each normalization layer and connect them to different subsets of the pooling layer, so that a valid partition of the vertices of the graph can be performed.\nWe compare the performances and training times of the full model, which works as the baseline, the full model with duplicated normalization layers, and pre-trained models with merge at different epochs, all trained with the same parameter setting to optimize the performance of the baseline. This allows us to evaluate the difference in behavior by just using the proposed pre-training method in an existing training schedule for the full model. Like in the MNIST test, the accumulated momentums of each sub-model are copied with their corresponding weights when merging the sub-models.\nTable 2 shows the results for each one of the models considered. Note that duplicating the normalization layers and dividing inputs among them, named \u201cDuplicated\u201d in the table, decreases the\n1https://github.com/akrizhevsky/cuda-convnet2\ntraining time because there are less inputs being considered at each normalization unit, reducing the time required to compute the normalized data.\nFor merge epoch up to 400, there is no noticeable trend in the performance of the classifier, with fluctuations occurring due to the stochastic optimization. On the other hand, the new pre-training method reduced the training time significantly even when serial training of the partitions is considered, with higher improvements assuming parallel training. Since the training schedule performs 520 iterations, the higher error for the last two merges can be explained by the optimization algorithm not having enough time to adjust the zero weights created by the merge.\nEach training iteration over the merged model takes about 3.3 seconds while iterations over one partition takes 1 second, characterizing a speedup of 3.3 times. When compared against the speedup of 2.1 times obtained on the MNIST network, which was about 5 times smaller, this provides evidence for the conjecture in Section 3.1 that larger networks benefit more from the pre-training. Therefore, even larger neural networks should have higher improvements on their training time, since more computations are saved and less computing power is idle during the training of the sub-models."}, {"heading": "4 CONCLUSION", "text": "In this paper, we introduced a method for pre-training a neural network by partitioning it into smaller neural networks and training them on the original learning task. The size of the sub-models reduces almost quadratically with the number of sub-models created, which allows larger neural networks to save more computational resources during pre-training.\nBy design, the method decreases the training time by creating training subtasks that can be parallelized and may reduce the communication overhead present in model parallelism. It may also decrease the number of computing units required during the pre-training due to the quadratic reduction on the number of parameters being learned.\nTwo experiments, on MNIST and on CIFAR10, with neural networks that fit in a single GPU, confirmed that the training time of the full model can be decreased without affecting the performance. The experiments also show that the proposed method may be able to improve training speed even if the training of the sub-models is performed serially and that larger models may experience higher speedups, with a speedup on the pre-training iterations of 2.1 for a 3-layer model with 430k parameters and a speedup of 3.3 for a 4-layer model with 2.1M parameters when creating 2 sub-models.\nSince the proposed method relies on the different sub-models finding diverse representations for the data that can be exploited once they are merged, it is plausible to worry about the size of the submodels created, as smaller models have less flexibility and might learn similar functions. However, the MNIST experiment shows that indeed the sub-models learn diverse representations for the data, with the pairwise diversity being higher when we increase the number of sub-models.\nFuture research should experiment with deep neural networks using both data and model parallelism to evaluate the gains obtained by reducing the models. The time spent training the neural network can be decomposed mainly in three parts, all of which may be affected by the proposed method: 1) the time to compute the gradients and adjust the parameters, which we showed that can achieve large improvement; 2) the time to communicate updates between units with different parts of the data, which is proportional to the number of parameters and should get an almost quadratic speedup; and 3) the time to communicate activations between computers when using model parallelism, which can be decreased or even avoided by using smaller models. These reductions in training time and the possibility of reducing the number of computers required during pre-training, as discussed in Section 2.3, make the proposed method appealing to handle large models.\nAnother direction is the evaluation of a hierarchical training method, in which the neural network is decomposed recursively and the pre-training is performed at each level, such that the proposed method characterizes one level of recursion. This could provide additional benefits to very large models by allowing smaller sub-models to be trained without merging all of them at the same time."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank CNPq for the financial support."}], "references": [{"title": "cuDNN: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": null, "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "The loss surface of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Deep learning with COTS HPC systems", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Why does unsupervised pretraining help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Manzagol", "P. Vincent", "S. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence,", "citeRegEx": "Hansen and Salamon,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon", "year": 1990}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y.N. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "When networks disagree: Ensemble methods for hybrid neural networks", "author": ["M.P. Perrone", "L.N. Cooper"], "venue": "World Scientific,", "citeRegEx": "Perrone and Cooper,? \\Q1992\\E", "shortCiteRegEx": "Perrone and Cooper", "year": 1992}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["R. Raina", "A. Madhavan", "A.Y. Ng"], "venue": "In Proceedings of the 26th annual International Conference on Machine Learning,", "citeRegEx": "Raina et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2009}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": null, "citeRegEx": "Vasilache et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vasilache et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": ", 2015) and advances in computing power, such as the use of graphic processing units (GPUs) (Raina et al., 2009).", "startOffset": 92, "endOffset": 112}, {"referenceID": 7, "context": "Some researches have focused on speeding up deep neural networks in general, including proposals based on hardware, such as using limited numerical precision (Gupta et al., 2015), which could increase the number of computing units on the hardware, and software, such as using Fourier transform to compute a convolution (Vasilache et al.", "startOffset": 158, "endOffset": 178}, {"referenceID": 18, "context": ", 2015), which could increase the number of computing units on the hardware, and software, such as using Fourier transform to compute a convolution (Vasilache et al., 2014) and developing a base library for deep neural networks (Chetlur et al.", "startOffset": 148, "endOffset": 172}, {"referenceID": 0, "context": ", 2014) and developing a base library for deep neural networks (Chetlur et al., 2014).", "startOffset": 63, "endOffset": 85}, {"referenceID": 4, "context": "DistBelief (Dean et al., 2012) is another framework to speedup the training of large neural network by exploiting parallelism, but it focuses on clusters of computers.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": ", 2014) and developing a base library for deep neural networks (Chetlur et al., 2014). In this paper, we focus on existing research interested in decreasing the training time, as these approaches are closer to the proposed method. However, we highlight that these improvements are not mutually exclusive and can be used together. Krizhevsky (2014) proposed a mixture of data and model parallelism over GPUs in a single machine based on the type of the layer, exploiting their particularities for increased speed.", "startOffset": 64, "endOffset": 348}, {"referenceID": 5, "context": "A well known example of this is unsupervised pre-training (Erhan et al., 2010), where the network is greedly trained layer-wise to reconstruct its input.", "startOffset": 58, "endOffset": 78}, {"referenceID": 2, "context": "This framework is extended in Coates et al. (2013) to use GPUs as computing units.", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": "They may require special methods for training if they do not fit in a single computing unit (Dean et al., 2012) and can be used after trained to provide guidance to improve smaller networks (Hinton et al.", "startOffset": 92, "endOffset": 111}, {"referenceID": 9, "context": ", 2012) and can be used after trained to provide guidance to improve smaller networks (Hinton et al., 2015).", "startOffset": 86, "endOffset": 107}, {"referenceID": 17, "context": "Since the performance achieved by a neural network may depend on its initialization, there has been a search for good initialization methods (Glorot & Bengio, 2010; Sutskever et al., 2013).", "startOffset": 141, "endOffset": 188}, {"referenceID": 3, "context": "Nonetheless, neural networks seem to be able to achieve good and diverse local minima or saddle points (Dauphin et al., 2014; Choromanska et al., 2014; Pascanu et al., 2014), so they can easily be used as components of ensembles to improve performance.", "startOffset": 103, "endOffset": 173}, {"referenceID": 1, "context": "Nonetheless, neural networks seem to be able to achieve good and diverse local minima or saddle points (Dauphin et al., 2014; Choromanska et al., 2014; Pascanu et al., 2014), so they can easily be used as components of ensembles to improve performance.", "startOffset": 103, "endOffset": 173}, {"referenceID": 14, "context": "Nonetheless, neural networks seem to be able to achieve good and diverse local minima or saddle points (Dauphin et al., 2014; Choromanska et al., 2014; Pascanu et al., 2014), so they can easily be used as components of ensembles to improve performance.", "startOffset": 103, "endOffset": 173}, {"referenceID": 12, "context": "In this partition, one filter of a convolutional layer corresponds to an atomic unit, since all the computed activations share the same parameters, and any layer that has internal parameters or whose activation depends on the individual input values instead of their aggregate, such as normalization layers (Krizhevsky et al., 2012), must be replaced by multiple similar, parallel layers.", "startOffset": 307, "endOffset": 332}, {"referenceID": 13, "context": "The first uses a small network to classify digits on MNIST (LeCun et al., 1998) and focuses on analysing the effects of the number of partitions created on the training time and performance.", "startOffset": 59, "endOffset": 79}, {"referenceID": 10, "context": "The second uses a larger network to classify the images on CIFAR10 (Krizhevsky, 2009) and considers different number of epochs for the pre-training.", "startOffset": 67, "endOffset": 85}, {"referenceID": 10, "context": "The neural network and parameters used for these experiments are the same used by Krizhevsky et al. (2012) in the CUDA-ConvNet library1, which is composed of two convolutional layers with ReLU activation, each one followed by normalization and pooling layers.", "startOffset": 82, "endOffset": 107}], "year": 2017, "abstractText": "This paper presents a new method for pre-training neural networks that can decrease the total training time for a neural network while maintaining the final performance, which motivates its use on deep neural networks. By partitioning the training task in multiple training subtasks with sub-models, which can be performed independently and in parallel, it is shown that the size of the sub-models reduces almost quadratically with the number of subtasks created, quickly scaling down the sub-models used for the pre-training. The sub-models are then merged to provide a pre-trained initial set of weights for the original model. The proposed method is independent of the other aspects of the training, such as architecture of the neural network, training method, and objective, making it compatible with a wide range of existing approaches. The speedup without loss of performance is validated experimentally on MNIST and on CIFAR10 data sets, also showing that even performing the subtasks sequentially can decrease the training time. Moreover, we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems.", "creator": "LaTeX with hyperref package"}}}