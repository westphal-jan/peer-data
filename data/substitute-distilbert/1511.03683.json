{"id": "1511.03683", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Generative Concatenative Nets Jointly Learn to Write and Classify Reviews", "abstract": "we present a character - weighted recurrent neural network that generates consistently and coherent text given auxiliary information such as a sentiment or topic. using a simple input replication strategy, we preserve the signal of auxiliary input across wider sequence intervals than having feasibly be trained by feedback - propagation through time. our main results center on eliminating large corpus of 1. 5 grams beer reviews from beeradvocate. entering generative mode, our network produces reviews deemed command, tailored to a star rating or item category. thus same model can also run through reverse, performing outcomes with surprising validity. performance of the reverse model provides much straightforward way to determine what the generative model knows without experimenting too heavily on subjective analysis. given a review, the model can accurately judge the corresponding relation and infer the beer'\u03b1 category ( ipa, stout, etc. ). we exploit this capability, referencing perceived sentiment and class membership as it character in a review is processed. quantitative and qualitative empirical evaluations demonstrate that the picture captures meaning and learns nonlinear dynamics in sentence, such at the effect of negation on objects, despite possessing no syntax priori process of words. because the model operates at the character level, it handles misspellings, slang, and large vocabularies without any machinery explicitly dedicated to the purpose.", "histories": [["v1", "Wed, 11 Nov 2015 21:16:59 GMT  (1020kb,D)", "http://arxiv.org/abs/1511.03683v1", null], ["v2", "Mon, 16 Nov 2015 10:27:27 GMT  (1080kb,D)", "http://arxiv.org/abs/1511.03683v2", null], ["v3", "Wed, 18 Nov 2015 08:20:05 GMT  (1080kb,D)", "http://arxiv.org/abs/1511.03683v3", null], ["v4", "Fri, 20 Nov 2015 19:17:07 GMT  (1072kb,D)", "http://arxiv.org/abs/1511.03683v4", null], ["v5", "Thu, 7 Apr 2016 07:08:42 GMT  (1211kb,D)", "http://arxiv.org/abs/1511.03683v5", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zachary c lipton", "sharad vikram", "julian mcauley"], "accepted": false, "id": "1511.03683"}, "pdf": {"name": "1511.03683.pdf", "metadata": {"source": "CRF", "title": "CHARACTER-LEVEL GENERATIVE TEXT MODELS", "authors": ["Zachary C. Lipton", "Sharad Vikram", "Julian McAuley"], "emails": ["zlipton@cs.ucsd.edu", "svikram@cs.ucsd.edu", "jmcauley@cs.ucsd.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Our work is motivated by an interest in product recommendation. Currently, recommender systems assist users in navigating an unprecedented selections of items, personalizing services to a diverse set of users with distinct individual tastes. Typical approaches surface items that a customer is likely to purchase or rate highly, providing a basic set of primitives for building functioning internet applications. Our goal is to create richer user experiences, not only recommending products but generating descriptive text. For example, engaged users may wish to know precisely their impression of an item is expected to be, not simply whether the item will warrant a thumbs up or thumbs down. Consumer reviews can address this issue to some extent, but large volumes of reviews are difficult to sift through, especially if a user is interested in some niche aspect. Our fundamental goal is to resolve this issue by building systems that can both generate contextually appropriate descriptions, and infer items from abstract descriptions.\n\u2217Author website: http://zacklipton.com \u2020Author website: http://www.sharadvikram.com \u2021Author website: http://cseweb.ucsd.edu/\u223cjmcauley/\nar X\niv :1\n51 1.\n03 68\n3v 1\n[ cs\n.C L\n] 1\n1 N\nCharacter-level Recurrent Neural Networks (RNNs) have a remarkable ability to generate coherent text (Sutskever et al., 2011), appearing to hallucinate passages that plausibly resemble a training corpus. In contrast to word-level models, they do not suffer from computational costs that scale with the size of the input or output vocabularies. This property is alluring, as product reviews draw upon an enormous vocabulary. Our work focuses on reviews scraped from Beer Advocate (McAuley and Leskovec, 2013). This corpus contains over 60,000 distinct product names alone, in addition to standard vocabulary, slang, jargon, punctuation, and misspellings.\nCharacter-level LSTMs powerfully demonstrate the ability of RNNs to model sequences on multiple time scales simultaneously, i.e., they learn to form words, to form sentences, to generate paragraphs of appropriate length, etc. To our knowledge, all previous character-level generative models are unsupervised. However, our goal is to generate character-level text in a supervised fashion, conditioning upon auxiliary input such as an item\u2019s rating or category12. Such conditioning of sequential output has been performed successfully with word-level models, for tasks including machine translation (Sutskever et al., 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al., 2014). However, despite the aforementioned virtues of character-level models, no prior work, to our knowledge, has successfully trained them in such a supervised fashion.\nMost supervised approaches to word-level generative text models follow the encoder-decoder approach popularized by Sutskever et al. (2014). Some auxiliary input, which might be a sentence or an image, is encoded by an encoder model as a fixed-length vector. This vector becomes the initial input to a decoder model, which then outputs at each sequence step a probability distribution predicting the next word. During training, weights are updated to give high likelihood to the sequences encountered in the training data. When generating output, words are sampled from each predicted distribution and passed as input at the subsequent sequence step. This approach successfully produces coherent and relevant sentences, but is generally limited to generating sentences (e.g. typically less than 10 words in length), as the model gradually \u2018forgets\u2019 the auxiliary input.\nHowever, to model longer passages of text (such as reviews), and to do so at the character level, we must produce much longer sequences than seem practically trainable with an encoder-decoder approach. To overcome these challenges, we present an alternative modeling strategy. At each sequence step t, we concatenate the auxiliary input vector xaux with the character representation x (t) char , using the resulting vector x\n\u2032(t) to train an otherwise standard generative RNN model. It might seem redundant to replicate xaux at each sequence step, but by providing it, we eliminate pressure on the model to memorize it. Instead, all computation can focus on modeling the text and its interaction with the auxiliary input.\n1We use auxiliary input to differentiate the \u201ccontext\u201d input from the character representation passed in at each sequence step.\n2By supervised, we mean the output sequence depends upon some auxiliary input.\nIn this paper, we implement the concatenated input model, demonstrating its efficacy at both review generation and traditional supervised learning tasks. In generative mode, our model produces convincing reviews, tailored to a star rating and category. This generative model can also run in reverse, performing classification with surprising accuracy (Figure 1). The purpose of this model is to generate text, but we find that classification accuracy of the reverse model provides an objective way to assess what the model has learned. An empirical evaluation shows that our model can accurately classify previously unseen reviews as positive or negative and determine which of 5 beer categories applies, despite operating at the character level and not being optimized directly to minimize classification error. Our exploratory analysis also reveals that the model implicitly learns a large vocabulary and can effectively model nonlinear dynamics, like the effect of negation. Plotting the inferred rating as each character is encountered for many sentences (Figure 1) shows that the model can infer ratings quickly and anticipate words after reading particularly informative characters."}, {"heading": "2 THE BEER ADVOCATE DATASET", "text": "We focus on data scraped from Beer Advocate as originally collected and described by McAuley and Leskovec (2013). Beer Advocate is a large online review community boasting 1,586,614 reviews of 66,051 distinct items composed by 33,387 users. Each review is accompanied by a number of numerical ratings, corresponding to \u201cappearance\u201d, \u201caroma\u201d, \u201cpalate\u201d, \u201ctaste\u201d, and also the user\u2019s \u201coverall\u201d impression. The reviews are also annotated with the item\u2019s category. For our experiments on ratings-based generation and classification, we select 250,000 reviews for training, focusing on the most active users and popular items. For our experiments focusing on generating reviews conditioned on item category, we select a subset of 150,000 reviews, 30,000 each from 5 of the top categories, namely \u201cAmerican IPA\u201d, \u201cRussian Imperial Stout\u201d, \u201cAmerican Porter\u201d, \u201cFruit/Vegetable Beer\u201d, and \u201cAmerican Adjunct Lager\u201d. From both datasets, we hold out 10% of reviews for testing."}, {"heading": "3 RECURRENT NEURAL NETWORK METHODOLOGY", "text": "Recurrent neural networks extend the capabilities of feed-forward networks to handle sequential data. Inputs x(1), ...,x(T ) are passed to the network one by one. At each step t, the network updates its hidden state as a function of both the current input and the previous step\u2019s hidden state, outputting a prediction y\u0302(t). In this paper, we use RNNs containing long short term memory (LSTM) cells introduced by Hochreiter and Schmidhuber (1997) with forget gates introduced in Gers et al. (2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al., 1994). In short, each memory cell has an internal state s in which activation is preserved along a self-connected recurrent edge. Each cell also contains three sigmoidal gating units for input (i), output (o), and forget (f ) that respectively determine when to let activation into the internal state, when to pass activation to the rest of the network, and when to flush the cell\u2019s hidden state. The output of each LSTM layer is another sequence, allowing us to stack several layers of LSTMs as in Graves (2013). At step t, each LSTM layer h(t)l receives input from the previous layer h (t) l\u22121 at the same sequence step and the same layer at the previous time step h(t\u22121)l . Formally, for a layer hl the equations to calculate the forward pass through an LSTM layer are\ng (t) l = \u03c6(W gxh(t)l\u22121 +W ghh(t\u22121)l + b g l )\ni (t) l = \u03c3(W ixh(t)l\u22121 +W ihh(t\u22121)l + b i l)\nf (t) l = \u03c3(W fxh(t)l\u22121 +W fhh(t\u22121)l + b f l )\no (t) l = \u03c3(W oxh(t)l\u22121 +W ohh(t\u22121)l + b o l )\ns (t) l = g (t) l i (i) l + s (t\u22121) l f (t) l )\nh (t) l = \u03c6(s (t) l ) o (t) l .\nHere, \u03c3 denotes an element-wise sigmoid function, \u03c6 an element-wise tanh, and is an elementwise product. While a thorough treatment of the LSTM is beyond the scope of this paper, we refer to our review of the literature (Lipton et al., 2015) for a gentler unpacking of the material."}, {"heading": "3.1 GENERATIVE RECURRENT NEURAL NETWORKS", "text": "Before introducing our contributions, we review the generative RNN model of Sutskever et al. (2011; 2014) on which we build. A generative RNN is trained to predict the next token in a sequence, i.e. y\u0302t = x(t+1), given all inputs to that point (x1, ...,xt). Thus the input and output strings are equivalent but for a one token shift (Figure 2a). The output layer is fully connected with softmax activation, ensuring that outputs specify a distribution. Cross entropy is the loss function during training.\nOnce trained, the model is run in generative mode by sampling stochastically from the distribution output at each sequence step, given some starting token and state. Passing the sampled output as the subsequent input, we generate another output conditioned on the first prediction, and can continue in this manner to produce arbitrarily long sequences. The sampling can be done directly according to softmax outputs, but it is also common to sharpen the distribution by setting a temperature \u2264 1, analogous to the so-named parameter in a Boltzmann distribution. Applied to text, generative models trained in this fashion produce surprisingly coherent passages that appear to plausibly come from a\nthe same distribution as the training corpus. They can also be used to continue passages given some starting tokens."}, {"heading": "3.2 CONCATENATED INPUT RECURRENT NEURAL NETWORKS", "text": "Our goal is to generate text in a supervised fashion, conditioned on an auxiliary input xaux. This has been done at the word-level with encoder-decoder models (Figure 2b), in which the auxiliary input is encoded and passed as the initial state to a decoder, which then must preserve this input signal across many sequence steps (Sutskever et al., 2014; Karpathy and Fei-Fei, 2014). Such models have successfully produced image captions (typically less than 10 tokens long), but seem impractical for generating full reviews at the character level because signal from xaux must survive for hundreds of sequence steps.\nWe take inspiration from an analogy to human text generation. Consider that given a topic and told to speak at length, a human might be apt to meander and ramble. But given a subject to stare at, it is far easier to remain focused. The value of re-iterating high-level material is borne out in one study, Surber and Schroeder (2007), which showed that repetitive subject headings in textbooks resulted in faster learning, less rereading and more accurate answers to high-level questions.\nThus we propose a simple architecture in which input xaux is concatenated with the character representation xchar. Given this new input x\u2032 (t) = [xchar;xaux] we can train the model precisely as with the standard generative RNN (Figure 2c). At train time, xaux is a feature of the training set. At predict time, we fix some xaux, concatenating it with each character sampled from y\u0302(t). One might reasonably note that this replicated input information is redundant. However, since it is fixed over the course of the review, we see no reason to require the model to transmit this signal across hundreds of time steps. By replicating xaux at each input, we free the model to focus on learning the complex interaction between input and language, rather than input memorization."}, {"heading": "3.3 WEIGHT TRANSPLANTATION", "text": "Models with even modestly sized auxiliary input representations are considerably harder to train than a typical unsupervised character model. To overcome this problem, we first train a character model to convergence. Then we transplant these weights into a concatenated input model, initializing the extra weights (between the input layer and the first hidden layer) to zero. Zero initialization is not problematic here because symmetry in the hidden layers is already broken. Thus we guarantee that the model will achieve a strictly lower loss than a character model, saving (days of) repeated training. This scheme bears some resemblance to the pre-training common in the computer vision community (Yosinski et al., 2014). Here, instead of new output weights, we train new input weights."}, {"heading": "3.4 RUNNING THE MODEL IN REVERSE", "text": "Many common document classification models, like tf-idf logistic regression, maximize the likelihood of the training labels given the text. Given our generative model, we can then produce a predictor by reversing the order of inference, that is by maximizing the likelihood of the text, given a classification. The relationship between these two tasks (P (xaux|Review) and P (Review|xaux)) follows from Bayes\u2019 rule. That is, our model predicts the conditional probability of an entire review given a rating P (Review|xaux). The normalizing term can be disregarded in determining the most probable rating and when the classes are balanced, as they are in our test cases, the prior also vanishes from the decision rule leaving P (xaux|Review) \u221d P (Review|xaux)."}, {"heading": "4 EXPERIMENTS", "text": "All experiments are executed with a custom recurrent neural network library written in Python, using Theano for GPU acceleration. Our networks use 2 hidden layers with 1024 nodes per layer. During training, examples are processed in mini-batches and we update weights with RMSprop (Tieleman and Hinton, 2012). To assemble batches, we concatenate all reviews in the training set together, delimiting them with (<STR>) and (<EOS>) tokens. We split this string into batches of 256 and again split the batch into segments with sequence length 200. Furthermore, LSTM state is\npreserved through batches during training, and to combat exploding gradients, we clip the elements of each gradient at \u00b1 5. To train the concatenated inputs we found that it was faster to first train an unsupervised character-level generative RNN to convergence. Then we use weight transplantation from the unsupervised net to initialize our concatenated-input RNNs. We implement two nets in this fashion, one using the star rating scaled to [-1, 1] as xaux, and a second using a one-hot encoding of 5 beer categories as xaux."}, {"heading": "4.1 GENERATING TEXT", "text": "Running the concatenated input RNN in generative mode and conditioning upon a 5 star rating, we produce a decidedly positive review:\n<STR>Poured from a 12oz bottle into a pint glass. A: Pours a deep brown color with a thin tan head. The aroma is of coffee, chocolate, and coffee. The taste is of roasted malts, coffee, chocolate, and coffee. The finish is slightly sweet and smooth with a light bitterness and a light bitterness that lingers on the palate. The finish is slightly bitter and dry. Mouthfeel is medium bodied with a good amount of carbonation. The alcohol is well hidden. Drinkability is good. I could drink this all day long. I would love to try this one again and again. <EOS>\nConditioning on the \u201cFruit / Vegetable Beer\u201d category, the model generates a commensurately botanical review. This review is particularly interesting as user \u201cMikeygrootia\u201d does not exist in the dataset.\n<STR>Thanks to Mikeygrootia for the opportunity to try this one. A: Poured a nice deep copper with a one finger head that disappears quickly. Some lacing. S: A very strong smelling beer. Some corn and grain, some apple and lemon peel. Taste: A very sweet berry flavor with a little bit of a spice to it. I am not sure what to expect from this beer. This stuff is a good summer beer. I could drink this all day long. Not a bad one for me to recommend this beer.<EOS>\nFor more examples of generated text, please see Appendix A and Appendix B."}, {"heading": "4.2 PREDICTING SENTIMENT AND CATEGORY ONE CHARACTER AT A TIME", "text": "In addition to running the model to generate output, we can also take example sentences from unseen reviews and plot the rating which gives the sentence maximum likelihood as each character is encountered (Figure 3). Please see Appendix C and Appendix D for more examples of these graphs. To verify that the argmax over many settings of the rating is reasonable, we also plot the log likelihood of sentences (after the final character is processed) using evenly spaced, granular settings of the rating (1.0, 1.01, etc.). These plots show that the log likelihood tends to be smooth and monotonic for sentences with unambiguous sentiment. For less obvious sentences, they are smooth with a peak in the middle of the ratings scale (Figure 4). We also find that the model understands nonlinear dynamics of negation and can handle simple spelling mistakes, as seen in Appendix E and Appendix D."}, {"heading": "4.3 CLASSIFICATION RESULTS", "text": "While our motivation is to produce a character-level general model, running in reverse-fashion as a classifier proved an effective way to objectively gauge what the model knows. To investigate this capability more thoroughly, we compared it to a word-level tf-idf n-gram multinomial logistic regression (LR) model, using the top 10 thousand n-grams. Our model achieves a classification accuracy of 83.0% while LR achieves 93.4% accuracy (Figure 5). Both models make the majority of their mistakes confusing stouts and porters, which is not surprising because a stout is considered to be a sub-type of porter. If we collapse these two into one category the RNN achieves 91.6% accuracy while LR achieves 96.5%. While the reverse model is not competitive with a state of the art classifier, it was trained at the character level and was not optimized to minimize classification error or with attention to generalization error. In this light, the results are sufficient to warrant a deeper exploration of this capability. Please see Appendix F for detailed classification results."}, {"heading": "5 RELATED WORK", "text": "The prospect of capturing meaning in character-level text has long captivated neural network researchers. In the seminal work, \u201cFinding Structure in Time\u201d, Elman (1990) speculated, \u201cone can ask whether the notion \u2018word\u2019 (or something which maps on to this concept) could emerge as a consequence of learning the sequential structure of letter sequences that form words and sentences (but in which word boundaries are not marked).\u201d In this work, an \u2018Elman RNN\u2019 was trained with 5 input nodes, 5 output nodes, and a single hidden layer of 20 nodes, each of which had a corresponding context unit to predict the next character in a sequence. At each step, the network received a binary encoding (not one-hot) of a character and tried to predict the next character\u2019s binary encoding. Elman plots the error of the net character by character, showing that it is typically high at the onset of words, but decreasing as it becomes clear what each word is. While these nets do not possess the size or capabilities of large modern LSTM networks trained on GPUs, this work lays the foundation for much of our research. Subsequently, in 2011, Sutskever et al. (2011) introduced the model of text generation on which we build. In that paper, the authors generate text resembling Wikipedia articles and New York Times articles. They sanity check the model by showing that it can perform a debagging task in which it unscrambles bag-of-words representations of sentences by determin-\ning which unscrambling has the highest likelihood. Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al., 1989; 1998). Demonstrating success on both English and Chinese language datasets, their models achieve high accuracy on a number of classification tasks.\nRelated works generating sequences in a supervised fashion follow the pattern of Sutskever et al. (2014) which uses a word-level encoder-decoder RNN to map sequences onto sequences. Their system for machine translation demonstrated that a recurrent neural network can compete with state of the art machine translation systems absent any hard-coded notion of language (beyond that of words). Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014)."}, {"heading": "5.1 KEY DIFFERENCES AND CONTRIBUTIONS", "text": "RNNs have been used previously to generate text at the character level. And they have been used to generate text in a supervised fashion at the word-level. However, to our knowledge, this is the first work to demonstrate that the a recurrent neural network can generate relevant text at the character level. Further while Sutskever et al. (2011) demonstrates the use of a character level RNN as a scoring mechanism, this is the first paper to use such a scoring mechanism to infer labels in an unsupervised fashion, simultaneously learning to generate text and to perform tasks like regression and multiclass classification with high accuracy. Our work is not the first to demonstrate a characterlevel classifier, as Zhang and LeCun (2015) offered such an approach. However, while their model is strictly discriminative, our model\u2019s main purpose is to generate text, a capability not present in their approach. Further, while we present a preliminary exploration of ways that our generative model can be used as a classifier, we do not train it directly to minimize classification error or generalization error, rather using the classifier interpretation to validate that the generative model is in fact modeling the auxiliary information meaningfully."}, {"heading": "6 DISCUSSION", "text": "In this work, we demonstrate the first character-level recurrent neural network to generate relevant text conditioned on auxiliary input. This work is also the first work, to our knowledge, to generate coherent product reviews conditioned upon data such as rating and item category. Our quantitative and qualitative analysis shows that our model can accurately perform sentiment analysis and model item category. While this capability is intriguing, much work remains to investigate if such an approach can be competitive against state of the art word-level classifiers. The model learns nonlinear dynamics of negation, and appears to respond intelligently to a wide vocabulary despite lacking any a priori notion of words.\nWe believe that this is only beginning of this line of research. Next steps include extending our work to the more complex domain of individual items and users. Given users with extensive historical feedback in a review community and a set of frequently reviewed items, we\u2019d like to take a previously unseen (user, item) pair and generate a review that plausibly reflects the user\u2019s tastes and writing style as well as the item\u2019s attributes. We also imagine an architecture by which our con-\ncatenated input network could be paired with a neural network encoder, to leverage the strengths of both the encoder-decoder approach and our approach. Details of this proposed model are included in Appendix G."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "Zachary C. Lipton\u2019s research is funded by the UCSD Division of Biomedical Informatics, via an NIH/NLM training grant (T15LM011271). Sharad Vikram\u2019s research was supported in part by NSF grant CNS-1446912. We would also like to acknowledge the NVIDIA Corporation, whose hardware donation program furnished us with a Tesla K40 GPU, making our research possible."}, {"heading": "A REVIEWS GENERATED BY ITEM CATEGORY", "text": "In the following examples, we generate reviews conditioned on the item category. We show one example for each for American IPA, American Adjunct Lager, Russian Imperial Stout, American Porter, and Fruit/Vegetable Beer. All reviews in this section are generated with a temperature of .5\nA.1 AMERICAN IPA\n<STR>A - Pours a clear golden color with a slight haze. Poured a two finger head that dissipates quickly leaving no lacing. S - Sweet malt and some floral hops. Mouthfeel is quite thin and fizzy. The alcohol is hidden very well with the hops and the carbonation and a high drinkability. The beer is a really good brew that I would definitely buy again if I had another.<EOS>\nA.2 AMERICAN ADJUNCT LAGER\n<STR>Clear yellow gold color with some head. Lots of bubbles rising to the top. Smell is a little sweet, but mostly corn and grains. The taste is sweet and tart, with a light malt backbone. The finish is a bit grainy and a bit slick on the palate with a slight sourness to it. It was a little too sweet for me to drink and a case of it here and they seem to have the problem with this one. I don\u2019t know why I would drink it again. It is better than the style, but it doesn\u2019t stand out at all, and the beer is not in a good way. I have no idea why I have some of the styles that is that I mean it\u2019s been a case of this. Mild and carbonated for a beer that is very good for a hot day.<EOS>\nA.3 RUSSIAN IMPERIAL STOUT\n<STR>The taste is also very sweet with a strong chocolate flavor that starts out the most noticeable as well. The mouthfeel is very smooth and creamy. The carbonation is a little thin, the mouthfeel is pretty good. Lots of carbonation levels and a pretty good drinkability for the style. This is a good thing though still lived up to the big brewery in the way it was the first beer I had so far. I was able to try it again in the future. It is a great stout, and I could have drank one all night. <EOS>\nA.4 AMERICAN PORTER\n<STR> A very dark brown with a thin layer of brown head. Smells of bitter chocolate, coffee, and bourbon. The taste was sweet with a light chocolate and coffee flavor. The taste is mostly chocolate and coffee, with a soft and sweet malt flavor that is overwhelmed by a subtle porter sweetness. The mouthfeel is full bodied with medium carbonation. The finish is slightly sour and dry and the finish is slightly bitter. Drinkability is OK in the mouth. The taste of this beer really isn\u2019t so much to this beer, but it is not that bad, and it\u2019s not the best American porter I\u2019ve ever had. I would like to see more than one of these in a sitting. <EOS>\nA.5 FRUIT/VEGETABLE BEER"}, {"heading": "B REVIEWS GENERATED BY STAR RATING", "text": "Below, we show examples of the concatenated input network\u2019s ability to generate decidedly positive reviews when conditioned on high star ratings and decidedly negative reviews when conditioned on low ratings. Because the star rating is simply a scalar input to the network, we can also generate\nreviews conditioned on the more extreme 0 star and 10 star ratings, even though neither truly exist in the dataset. We present examples of reviews generated by the model and conditioned upon 0, 1, 5, and 10 star ratings. For each we show an example each for three settings of the temperature (.1, .5, 1.0).\nB.1 0 STAR REVIEWS\nThe following full reviews are generated conditioned on a 0 star rating for varying settings of the temperature.\nWith a temperature of .1\n<STR>Pours a clear golden color with a small white head that dissipates quickly. No lacing to speak of. Smell: Sweet malt and some hops. The taste is a little sweet with a slight bitterness that is not very present. The mouthfeel is a bit thin and watery. It is a bit too sweet for my taste. I would not recommend this beer. I would not recommend this beer.<EOS>\nWith a temperature of .5:\n<STR>This beer poured a crisp white head with little retention. The aroma had a strong aroma of corn and malt. The taste was weak, watery, and watery. The taste was a bit off. I was expecting a little more balance and this would be great for a while. The mouthfeel was ok and this was a bit bitter and distracting at the same time. It was almost a bit too strong for me. Not a bad beer, but not great.<EOS>\nWith a temperature of 1.0\n<STR>Bottle: Poured a bright cloudy gold color with a slight, white head. It smelled like an old sauce and a weak hops aroma. I had to split this and just had the problem when I wasn\u2019t distinguished. II##22377* stamped on. Fortmucky upfront start to bottom of Chimay wheat beer, borderline cloying with the stuff moments. I can taste the \u201dpleasant\u201d later flavor this is more chocolate than just cookie. Mild, thick and has light carbonation. There is a sweet like character in the mix, and like a stout. The lactose is less than. Carob is conceing the whole tongue and it doesnt exactly live up to ebsses through the glass it immediately. Aperitially somehow short lived in sour worth shine, with his high of good, maroon brew fall sunshine too. Too bad this beer wouldn\u2019t be past its prime. I would not mind this on ice crack. It\u2019s very thin and unripe with not excessive taste for me.<EOS>\nB.2 1 STAR REVIEWS\nWith a temperature of 0.1\n<STR>Poured from a 12 oz. bottle into a pint glass. Appearance: Pours a clear amber color with a small white head that dissipates quickly. The aroma is of caramel malts and some hops. The taste is of sweet malt, caramel, and some hops. The mouthfeel is light and crisp. The drinkability is average for the style. I would not recommend this beer.<EOS>\nWith a temperature of 0.5\n<STR>A: Poured a hazy orange color with a thin white head which settled down to a thin coat of foam around the glass. The aroma was a little funky, maybe a little corn. It had a light malty sweetness to it, with a touch of caramel. The taste was a bit of a letdown after the pour. The taste was a little sweet with a light malt backbone. The mouthfeel was smooth. It had a light body and good carbonation. It was a tad drinkable but the alcohol was not really noticeable on the palate. This is a high ABV beer, but I would not recommend this one again.<EOS>\nWith a temperature of 1.0\n<STR>First off, I had two hate it. Poured a murky dark brown that is, for the memor I dig. It poured with very good head retention and lacing that stuck around for the duration of the glass. The aroma was chocolate, a little bit of chocolate and a musty coffee quality. I think it wasnt so horrible. It was too shabby and enjoyable. I just noticed alcohol but missed with this beer, high IBUs come from.<EOS>\nB.3 5 STAR REVIEWS\nWith a temperature of 0.1\n<STR>Poured from a 12 oz. bottle into a pint glass. Appearance: Pours a clear golden color with a thick frothy white head that slowly fades to lacing. The aroma is great. It has a sweet caramel malt aroma with a slight hop aroma. The taste is a little sweet with a slight hop bitterness that lingers in the aftertaste. The mouthfeel is medium bodied with a good amount of carbonation. This is a very drinkable beer. I would have no problem drinking a few of these in a sitting. I would definitely drink this again in the future.<EOS>\nWith a temperature of 0.5\n<STR>A - Poured a midnight black with a thick frothy tan head that left lacing on the glass. S- Smell is a wonderful virtual malt aroma with a touch of coffee and chocolate. Taste is chocolate and coffee with a hint of chocolate and coffee. Its a little thin for a witbier at all. M- This beer has a medium mouthfeel with a slight amount of carbonation. D - Very drinkable beer. I could drink this all night long. I will definitely be buying a few more in the cellar.<EOS>\nWith a temperature of 1.0\n<STR>First off, I have batch whose pure vertical esopetime trade! The forgot \u201dHomaching copper\u201d IPA would be a popular beer fixed in Pounds VE! Poured into Duvel tulip. APV of caramel with a small light tan head that cakes off on the glass now and lays down to a thin layer of residence that leaves behind dots of lace as it settles. Several left behind with remarkable introducing curtains. Aroma is enveloping massive but incredibly were redolent \u2013 refreshing. Juicy Belgian yeast esters - bursts of lime, lemon pith, etc... all right up front. Spiciness is in a glorious essence. Really man\u2019s Belgian pale and wild yeast funk too. Basically the taste\u2019s aroma is mostly grapefruit juice and vegetably accents. hints of oak, cranberries and light hints of yeast funk. A faint funkiness in there as well, with all malts. And smooth and a bit muddy too. Finish is fresh and both easy to glass, you know the glass who dies on that sweetness itself, but you still think you what effect you get - it makes the least busy-love color? Stone has perfect! Mouthfeel is smooth and clean with an earthy smoothness that lack the nice sharp bite of the malt and certain to exquisitely surface from the malt. Exceptional imperial stout. Low to drinkability, in an awesome winter warmer, especially when a bit of age doesn\u2019t work for me. A 20% beer is only great by my someone worry but a beer that would go well with something along with my friend. Nicely done in any mark. Superb.<EOS>\nB.4 10 STAR REVIEWS\nWith a temperature of 0.1\n<STR>Poured from a 12 oz. bottle into a pint glass. Appearance: Pours a dark brown color with a thick tan head that lasts forever and leaves a nice lacing on the glass. The aroma is of chocolate, coffee, chocolate, and a hint of chocolate. The taste is a nice blend of coffee, chocolate, and coffee. The mouthfeel is smooth\nand creamy with a nice creamy feel to it. This is a very good beer that I would definitely have again. I would love to have this again and again. I wish I had more of this stuff and see how this is the best beer I\u2019ve ever had. I could drink this all day and enjoy it all the time. I could drink this all day and drink all day. I could drink this all day and drink all day and savor the other three of these and this is a staple in my fridge.<EOS>\nWith a temperature of 0.5\n<STR>A- Amber brown with a small white head that has decent retention and leaves a nice lacing on the glass. Aroma is strong of citrus hops and a mild sweet malt aroma. The taste is so smooth and crisp tasting with the hops throughout the taste. The malt is more citrus than I expect to be sure this is a hop bomb with the alcohol in this beer that makes it easy to drink and smooth. The alcohol is noticeable and delicious. This is a great beer, and a great take on the style and one of the best in the world. I could drink this all night. I guess the alcohol is evident in the flavor and the flavors are bright and crisp and drinks smoother and is so easy to drink. I wish I had more! This is the best beer that I\u2019ve ever had in awhile. I would definitely order it again and again.<EOS>\nWith a temperature of 1.0\n<STR>A: Pours vibrant amber with a nice, hugs foamy head. Awesome light brown sheen that looks like it. successful excellent! Want down thoroughly IPA indeed. Soft with pine and lime juice (since you sat down!) come through at most. Also kicking bach down from something dampeness. Take through the enjoyment of this is wonderful for sure. Delicious. Is this rank? Now that it full body\u2019s imperessively. Espresso-strock, this beer is that fantastic. You will nip sip that OK. I drink one beer yet! I can\u2019t remember the 2008 version I might even paybe local!<EOS>"}, {"heading": "C CATEGORY TRACKING OVER SENTENCES", "text": "Here we show several representative examples demonstrating the ability of the concatenated input network (run in reverse) to recognize the category of the beer at the precise point in the sentence when it becomes clear. At first the probabilities are all close to .2 reflecting the uniform prior. By the end of the sentences the distribution conditioned on the input is considerably less entropic."}, {"heading": "D SENTIMENT TRACKING OVER SENTENCES", "text": "As each character in a review is encountered, we can plot the rating (with a granularity of 100 evenly spaced settings between 1 star and 5) which gives the review highest likelihood. Thus we can tell not only the sentiment of the rating, but the precise word, and even character at which this sentiment became clear."}, {"heading": "E NONLINEAR DYNAMICS OF NEGATION", "text": ""}, {"heading": "F CLASSIFICATION RESULTS", "text": "We trained a category RNN, where the auxiliary input to the network is a one-hot encoding of one of five beer categories. Inferring the class probability via the conditional likelihoods of the review, we can use the model in reverse to predict the category of beer described in a review.\nUsing a balanced test set of 5000 reviews, we evaluated the classification performance of the category RNN against two multinomial regression classifiers, one trained on the top 10,000 n-grams from the training set, and the other trained on tf-idf transfromed n-grams. The confusion matrices for these experiments can be seen in Table 1, Table 2, and Table 3."}, {"heading": "G A PROSPECTIVE ENCODER CONCATENATION NETWORK", "text": "In this paper, we introduced and demonstrated the efficacy of a simple technique for incorporating auxiliary information xaux in a generative RNN by concatenating it with with the character representation x(t)char at each sequence step. However, sometimes we don\u2019t simply want to generate given a representation xaux, but to learn a representation of xaux. For example, to generate a characterlevel caption given an image, we might want to jointly learn a convolutional neural network to encode the image, and a generative RNN to output a caption.\nTo accomplish this task at the character level, we propose the following network architecture and hypothesize that is will provide the benefits of learning an encoding while preserving our ability to generate long passages at the character level. At train time the xaux is fed to an encoder, whose output is then passed as auxiliary information to a concatenated input network. At prediction time, for any input, the encoding is calculated once, after which the inference problem is identical to that of our demonstrated concatenated input network."}, {"heading": "H LEARNING CURVES", "text": "For each task, we train two unsupervised character-level donor RNNs so that we may harvest the weights for transplantation into the concatenated input networks. We train separate donor networks for the two tasks (rating and category modeling) because each is trained on a different subset of the data (the beer set is selected for class balance among the 5 categories and is thus smaller). These networks are trained until convergence (Figure 10). After transplantation, we train the concatenated input RNNs with high learning rates, to induce the weights for auxiliary information to grow quickly. We suspect this results in the initial spike in loss seen in Figure 10a and Figure 10b, after which the loss quickly decreases."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2014}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Zachary C. Lipton", "John Berkowitz", "Charles Elkan"], "venue": "arXiv preprint arXiv:1506.00019,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan Yuille"], "venue": "arXiv preprint arXiv:1412.6632,", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews", "author": ["Julian John McAuley", "Jure Leskovec"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "McAuley and Leskovec.,? \\Q2013\\E", "shortCiteRegEx": "McAuley and Leskovec.", "year": 2013}, {"title": "Effect of Prior Domain Knowledge and Headings on Processing of Informative Text", "author": ["John R. Surber", "Mark Schroeder"], "venue": "Contemporary Educational Psychology,", "citeRegEx": "Surber and Schroeder.,? \\Q2007\\E", "shortCiteRegEx": "Surber and Schroeder.", "year": 2007}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lecture 6.5- RMSprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey E. Hinton"], "venue": "https://www.youtube.com/watch?v= LGA-gRkLEsI,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": "arXiv preprint arXiv:1412.4729,", "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun"], "venue": "arXiv preprint arXiv:1502.01710,", "citeRegEx": "Zhang and LeCun.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Character-level Recurrent Neural Networks (RNNs) have a remarkable ability to generate coherent text (Sutskever et al., 2011), appearing to hallucinate passages that plausibly resemble a training corpus.", "startOffset": 101, "endOffset": 125}, {"referenceID": 10, "context": "Our work focuses on reviews scraped from Beer Advocate (McAuley and Leskovec, 2013).", "startOffset": 55, "endOffset": 83}, {"referenceID": 13, "context": "Such conditioning of sequential output has been performed successfully with word-level models, for tasks including machine translation (Sutskever et al., 2014), image captioning (Vinyals et al.", "startOffset": 135, "endOffset": 159}, {"referenceID": 16, "context": ", 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al.", "startOffset": 26, "endOffset": 94}, {"referenceID": 5, "context": ", 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al.", "startOffset": 26, "endOffset": 94}, {"referenceID": 9, "context": ", 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al.", "startOffset": 26, "endOffset": 94}, {"referenceID": 15, "context": ", 2014), and even video captioning (Venugopalan et al., 2014).", "startOffset": 35, "endOffset": 61}, {"referenceID": 5, "context": ", 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al., 2014). However, despite the aforementioned virtues of character-level models, no prior work, to our knowledge, has successfully trained them in such a supervised fashion. Most supervised approaches to word-level generative text models follow the encoder-decoder approach popularized by Sutskever et al. (2014). Some auxiliary input, which might be a sentence or an image, is encoded by an encoder model as a fixed-length vector.", "startOffset": 8, "endOffset": 412}, {"referenceID": 10, "context": "We focus on data scraped from Beer Advocate as originally collected and described by McAuley and Leskovec (2013). Beer Advocate is a large online review community boasting 1,586,614 reviews of 66,051 distinct items composed by 33,387 users.", "startOffset": 85, "endOffset": 113}, {"referenceID": 0, "context": "(2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al., 1994).", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": "In this paper, we use RNNs containing long short term memory (LSTM) cells introduced by Hochreiter and Schmidhuber (1997) with forget gates introduced in Gers et al.", "startOffset": 88, "endOffset": 122}, {"referenceID": 1, "context": "In this paper, we use RNNs containing long short term memory (LSTM) cells introduced by Hochreiter and Schmidhuber (1997) with forget gates introduced in Gers et al. (2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al.", "startOffset": 154, "endOffset": 173}, {"referenceID": 0, "context": "(2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al., 1994). In short, each memory cell has an internal state s in which activation is preserved along a self-connected recurrent edge. Each cell also contains three sigmoidal gating units for input (i), output (o), and forget (f ) that respectively determine when to let activation into the internal state, when to pass activation to the rest of the network, and when to flush the cell\u2019s hidden state. The output of each LSTM layer is another sequence, allowing us to stack several layers of LSTMs as in Graves (2013). At step t, each LSTM layer h l receives input from the previous layer h (t) l\u22121 at the same sequence step and the same layer at the previous time step h l .", "startOffset": 146, "endOffset": 674}, {"referenceID": 8, "context": "While a thorough treatment of the LSTM is beyond the scope of this paper, we refer to our review of the literature (Lipton et al., 2015) for a gentler unpacking of the material.", "startOffset": 115, "endOffset": 136}, {"referenceID": 13, "context": "This has been done at the word-level with encoder-decoder models (Figure 2b), in which the auxiliary input is encoded and passed as the initial state to a decoder, which then must preserve this input signal across many sequence steps (Sutskever et al., 2014; Karpathy and Fei-Fei, 2014).", "startOffset": 234, "endOffset": 286}, {"referenceID": 5, "context": "This has been done at the word-level with encoder-decoder models (Figure 2b), in which the auxiliary input is encoded and passed as the initial state to a decoder, which then must preserve this input signal across many sequence steps (Sutskever et al., 2014; Karpathy and Fei-Fei, 2014).", "startOffset": 234, "endOffset": 286}, {"referenceID": 5, "context": ", 2014; Karpathy and Fei-Fei, 2014). Such models have successfully produced image captions (typically less than 10 tokens long), but seem impractical for generating full reviews at the character level because signal from xaux must survive for hundreds of sequence steps. We take inspiration from an analogy to human text generation. Consider that given a topic and told to speak at length, a human might be apt to meander and ramble. But given a subject to stare at, it is far easier to remain focused. The value of re-iterating high-level material is borne out in one study, Surber and Schroeder (2007), which showed that repetitive subject headings in textbooks resulted in faster learning, less rereading and more accurate answers to high-level questions.", "startOffset": 8, "endOffset": 604}, {"referenceID": 17, "context": "This scheme bears some resemblance to the pre-training common in the computer vision community (Yosinski et al., 2014).", "startOffset": 95, "endOffset": 118}, {"referenceID": 14, "context": "During training, examples are processed in mini-batches and we update weights with RMSprop (Tieleman and Hinton, 2012).", "startOffset": 91, "endOffset": 118}, {"referenceID": 1, "context": "In the seminal work, \u201cFinding Structure in Time\u201d, Elman (1990) speculated, \u201cone can ask whether the notion \u2018word\u2019 (or something which maps on to this concept) could emerge as a consequence of learning the sequential structure of letter sequences that form words and sentences (but in which word boundaries are not marked).", "startOffset": 50, "endOffset": 63}, {"referenceID": 1, "context": "In the seminal work, \u201cFinding Structure in Time\u201d, Elman (1990) speculated, \u201cone can ask whether the notion \u2018word\u2019 (or something which maps on to this concept) could emerge as a consequence of learning the sequential structure of letter sequences that form words and sentences (but in which word boundaries are not marked).\u201d In this work, an \u2018Elman RNN\u2019 was trained with 5 input nodes, 5 output nodes, and a single hidden layer of 20 nodes, each of which had a corresponding context unit to predict the next character in a sequence. At each step, the network received a binary encoding (not one-hot) of a character and tried to predict the next character\u2019s binary encoding. Elman plots the error of the net character by character, showing that it is typically high at the onset of words, but decreasing as it becomes clear what each word is. While these nets do not possess the size or capabilities of large modern LSTM networks trained on GPUs, this work lays the foundation for much of our research. Subsequently, in 2011, Sutskever et al. (2011) introduced the model of text generation on which we build.", "startOffset": 50, "endOffset": 1048}, {"referenceID": 6, "context": "Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al., 1989; 1998).", "startOffset": 165, "endOffset": 191}, {"referenceID": 9, "context": "Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014).", "startOffset": 137, "endOffset": 205}, {"referenceID": 16, "context": "Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014).", "startOffset": 137, "endOffset": 205}, {"referenceID": 5, "context": "Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014).", "startOffset": 137, "endOffset": 205}, {"referenceID": 11, "context": "Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 5, "context": "Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al., 1989; 1998). Demonstrating success on both English and Chinese language datasets, their models achieve high accuracy on a number of classification tasks. Related works generating sequences in a supervised fashion follow the pattern of Sutskever et al. (2014) which uses a word-level encoder-decoder RNN to map sequences onto sequences.", "startOffset": 166, "endOffset": 439}, {"referenceID": 12, "context": "Further while Sutskever et al. (2011) demonstrates the use of a character level RNN as a scoring mechanism, this is the first paper to use such a scoring mechanism to infer labels in an unsupervised fashion, simultaneously learning to generate text and to perform tasks like regression and multiclass classification with high accuracy.", "startOffset": 14, "endOffset": 38}, {"referenceID": 12, "context": "Further while Sutskever et al. (2011) demonstrates the use of a character level RNN as a scoring mechanism, this is the first paper to use such a scoring mechanism to infer labels in an unsupervised fashion, simultaneously learning to generate text and to perform tasks like regression and multiclass classification with high accuracy. Our work is not the first to demonstrate a characterlevel classifier, as Zhang and LeCun (2015) offered such an approach.", "startOffset": 14, "endOffset": 432}], "year": 2017, "abstractText": "We present a character-level recurrent neural network that generates relevant and coherent text given auxiliary information such as a sentiment or topic. Using a simple input replication strategy, we preserve the signal of auxiliary input across wider sequence intervals than can feasibly be trained by back-propagation through time. Our main results center on a large corpus of 1.5 million beer reviews from BeerAdvocate. In generative mode, our network produces reviews on command, tailored to a star rating or item category. The generative model can also run in reverse, performing classification with surprising accuracy. Performance of the reverse model provides a straightforward way to determine what the generative model knows without relying too heavily on subjective analysis. Given a review, the model can accurately determine the corresponding rating and infer the beer\u2019s category (IPA, Stout, etc.). We exploit this capability, tracking perceived sentiment and class membership as each character in a review is processed. Quantitative and qualitative empirical evaluations demonstrate that the model captures meaning and learns nonlinear dynamics in text, such as the effect of negation on sentiment, despite possessing no a priori notion of words. Because the model operates at the character level, it handles misspellings, slang, and large vocabularies without any machinery explicitly dedicated to the purpose.", "creator": "LaTeX with hyperref package"}}}