{"id": "1511.05607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Identifying the Absorption Bump with Deep Learning", "abstract": "the pervasive interstellar dust grains provide significant insights with help us understand the patterns and evolution of the stars, planetary systems, and the galaxies, could could potentially stimulate people to the secret of the origin of life. one of the most effective way readers analyze the dusts is researching their interaction and interference on observable background clouds. the small extinction curves and spectral features carry the information of the size isotope composition of dusts. among the features, the broad 2175 narrow absorption bump is one of the most significant spectroscopic interstellar extinction feature. traditionally, statistical methods are applied to calculate the existence of absorption bump. these methods require heavy preprocessing assuming the co - existence of other reference features ; alleviate the influence from the noises. taking this paper, we apply deep learning techniques to detect the broad absorption slide. we demonstrate the key metric for training the selected models and their results. the success of deep learning enhancement method makes engineers to generalize a common methodology for the broader science discovery market. we present our on - going work to build the deepdis statistics for such kind of applications.", "histories": [["v1", "Tue, 17 Nov 2015 22:27:05 GMT  (733kb,D)", "https://arxiv.org/abs/1511.05607v1", null], ["v2", "Fri, 20 Nov 2015 14:20:46 GMT  (733kb,D)", "http://arxiv.org/abs/1511.05607v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["min li", "sudeep gaddam", "xiaolin li", "yinan zhao", "jingzhe ma", "jian ge"], "accepted": false, "id": "1511.05607"}, "pdf": {"name": "1511.05607.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Min Li", "Sudeep Gaddam", "Xiaolin Li", "Yinan Zhao", "Jingzhe Ma", "Jian Ge"], "emails": ["sudeepgaddam}@ufl.edu,", "andyli@ece.ufl.edu", "jingzhema}@ufl.edu,", "jge@astro.ufl.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "The pervasive interstellar dust contains footprints of how the cosmos has been evolving. Dust grains come from condensation of heavy elements produced by stars. They are considered the primary reaction sites for molecules to form and essentially the source for all the H2 in the interstellar medium (Gould & Salpeter (1963); Hollenbach & Salpeter (1971)). Dust grains also play an important role in controlling the gas-phase metal abundances and the thermodynamic evolution of the interstellar medium. Besides, coagulation of interstellar dust grains in a protostellar disk, along with their catalyzed complex organic molecules, eventually leads to planets. Dust is thought to harbor the secrets of planetary formation, and even life.\nResearch on dust and gas is conducted by analyzing the reddening and extinction effects on the spectra of background light sources and re-radiated emission. In the Local Universe, it is feasible to compare stellar spectra and explore the extinction curves to investigate the dust grains. However, more luminous background light sources, such as quasars, are necessary to probe the dust and gas ingredients in the galaxies at larger distances.\nAmong the research topics on cosmic dust grains, the 2175 A\u030a broad absorption bump 1 stands out with significant values. Although the precise characteristics have not yet been established, the 2175 A\u030a broad absorption bump is believed to be tightly bounded with some types of aromatic carbonaceous materials (Draine (2003); Li & Greenberg (2003)). The most promising carriers of the bump\n1We use the 2175 A\u030a broad absorption bump and absorption bump for short in the rest of this paper.\nar X\niv :1\n51 1.\n05 60\n7v 2\n[ cs\n.C V\n] 2\n0 N\nare the Polycyclic Aromatic Hydrocarbon (PAH) molecules. PAHs are recognized as the most abundant organic molecules in our Milk Way and other neighboring galaxies (Peeters et al. (2003)), and are believed to be the building blocks of organic life (Bernstein et al. (2002); Caro et al. (2002)).\nThe traditional way of discovering the absorption bump mainly depends on statistic techniques (Jiang et al. (2011)). It generally involves three steps. First, a composite quasar spectrum is constructed using median combining (Jiang et al. (2010a;b)). The composite spectrum is then reddened and used to fit every spectrum from the candidate absorption bump spectra with a parameterized absorption profile. Finally, by applying constrains on peak position, bump width and bump height, a large portion of spectra without bump features are filtered out. In order to further exclude the false positives caused by some noises, a simulation technique (Jiang et al. (2010a;b)) is applied to determine the detection significance. Among the steps, curve fitting occupies considerable amount of time. In order to convolve the absorption bump with the composite spectrum, extra information is needed to determine the absorption redshift. This significantly constrains the available observations due to the lack of this extra information in some cases. While the composite quasar spectrum could potentially be reused for other related computations, the fitting procedure evolves iterative error minimization to get the best bump profile using curve fitting method, and is required for each and every new observation. The fitting is restricted to relatively smooth continuum of the quasar spectrum. However, the complicated types of emission and absorption in the candidate spectra could potentially disturb the fitting process, reducing the effectiveness of the method. All these issues are tightly coupled with the method itself, and cannot be easily resolved.\nIn this paper, we propose to apply a Deep Learning based method to detect the absorption bump and try to alleviate the aforementioned issues. Deep Learning is recently recognized with its ability to automatically extract high level features and accurately recognize or classify the target objects (Hinton & Salakhutdinov (2006); LeCun et al. (2015)). Deep Learning is most effective when the model is trained with sufficient data and the features are complex. For the absorption bump application, a normal observed spectrum consists of over 4,000 raw features. Each of them is a flux value at a certain wavelength. Due to the complex cosmos environment, the relation space among the features could be complicated. We also observe that in the traditional method, the curving fitting process is actually generating various spectra with bumps. These two facts make Deep Learning model a natural fit for such an application.\nFor the rest of this paper, we present the details of our Deep Learning based method for the absorption bump detection. Specifically, we first give more background information about the problem and the traditional method. We then show how we generate the raw training and testing data sets. Next, we present how we transform the raw data into different formats for models including fully connected neural networks and two convolutional neural networks. The details and the results are also reported. Finally, we present a generalization from our methodology to simplify and improve the efficiency for similar astronomy problems, and also resembling problems in other science fields. We discuss the workflow and the design details of our ongoing work, the DeepDis system, which aims to provide such services."}, {"heading": "2 TRADITIONAL METHOD FOR DISCOVERING ABSORPTION BUMP", "text": "Previous research (Fitzpatrick & Massa (1990)) showed that the extinction curve and the variation of the intrinsic spectral slopes of a quasar can be represented by a parameterized linear component. A Drude component can be added to approximate the possible absorption bump. Overall, the representation for an extinction spectrum with an absorption bump can be expressed as:\nA(\u03bb) = c1 + c2x+ c3D(x, x0, \u03b3) (1)\n, where \u03bb is the wavelength, and x = \u03bb\u22121. The component D(x, x0, \u03b3) is a Drude profile, and its definition is:\nD(x, x0, \u03b3) = x2\n(x2 \u2212 x20)2 + x2\u03b32 (2)\n, where x0 and \u03b3 is the peak position and full width at half maximum (FWHM) of the Drude profile, respectively. The strength can be measured by the area of the bump, i.e., Abump = \u03c0c3/(2\u03b3).\nThe most popular method for discovering the 2175 A\u030a absorption bump is rule-based filtering after fitting. The main idea is to get the best fitted parameters for each observed quasar spectrum, following Equation 1 and Equation 2. The whole procedure consists of three major steps. First, obtaining the median quasar composite spectrum. The basic assumption is that the quasars under similar condition will have similar observed spectrum. A median quasar composite spectrum is regarded as the base spectrum. It is created by combining the spectra from a set of observed quasars at their rest frame (e.g., Fitzpatrick & Massa (1990); Schneider et al. (2010)). Second, curve fitting based on method such as Chi-Square Testing. The median quasar spectrum is first reddened to the quasar\u2019s emission redshift, and an absorption bump profile is convolved at the absorption redshift. The absorption redshift is obtained by referring to some unique absorption lines, such as the Mg II absorption lines. The parameters in Equation 1 and Equation 2 are selected to minimize the fitting error. With the parameters, the last step is conducted by applying a set of rules to filter out low confidence candidates. One fitting example is shown in Figure 1, where the black curve is the observed spectrum, the green curve is the reddened normal extinction spectrum continuum, and the red line is the continuum with absorption bump.\nThere are a couple of issues in this method. First, hard dependency on Mg II absorption lines. The absorption redshift is obtained by referring to the Mg II absorption lines. The Mg II absorption lines are identified using some extra step, and is itself a research topic (Quider et al. (2011); Zhu & Me\u0301nard (2013)). However, not all observations contain this absorption feature, due to different emission and absorption redshift ranges. Second, the error minimizing curve fitting method is not effective in some cases, e.g., broader Fe emission. This sometimes requires human identification. Third, the fitting procedure for each observation is time consuming. Multiple iterations of error minimizing steps are required to produce the best profile."}, {"heading": "3 IDENTIFYING THE ABSORPTION BUMP WITH DEEP LEARNING", "text": "For the several aforementioned drawbacks of the traditional method, we propose a Deep Learning based method that could alleviate these issues and provide more flexible usage. Specifically, the proposed method exhibits the following merits.\n\u2022 Large data set empowers accurate representation learning. The curve fitting step in the traditional method is repeatedly generating training samples. By carefully designing the generation, we are able to train an accurate model for the absorption bump detection.\n\u2022 No hard dependency on Mg II absorption lines. When generating the samples, we are in control of selecting the emission and absorption redshifts. After the model is trained, no extra information is needed for new observations.\n\u2022 Easy to use and share. With a trained model, each prediction on newly added observations is just one feed forward process. Trained models could also be easily shared among researchers. Beyond trained models, simulation parameters and simulated data sets are all sharing-friendly.\n\u2022 Easy to extend. Extensions of already trained models happen at two levels. First, additional training samples could be included to cover more cases. Second, multiple targets could also be added based on the same feature set. These extensions could be achieved conveniently by fine tuning of a trained model.\nIn order to validate the feasibility, we present our work on applying Deep Learning based method for the absorption bump detection in the following subsections. Specifically, we show the details of training data generation, and our experience on model selection and data transformation."}, {"heading": "3.1 RAW DATA GENERATION", "text": "We applied two methods to generate the data set. The first method follows the corresponding part from the curve fitting procedure. We refer to Berk et al. (2001) and obtain the SDSS data release 7 composite quasar spectrum as the intrinsic spectrum. We then change the emission and absorption redshifts and add the absorption bump profile according to Equation 2. We choose a typical bump profile: x0 = 4.59, \u03b3 = 1.0 and Abump = 2.0.\nIn order to increase the diversity and closely simulate the real-world observations, we apply the second method. We selected a subset of the spectra from the SDSS data release 7 according to the Mg II catalog from Zhu & Me\u0301nard (2013). We follow the fitting method described in Jiang et al. (2010a; 2011), and convolve them with the absorption bump profile same as the above one. These generated samples are extremely close to real-world cases. Note that we refer to existing Mg II catalog to generate data. However, no dependency is required for predicting with new observations.\nA total of 30K samples are generated with the two methods described above. Half of them contains the absorption bump and half does not. Each generated sample is stored in a file following the <wavelength, flux> pattern. There are around 4600 pairs in each file. We divide the data into two parts, 22K as the training data set and 8K as the testing data set."}, {"heading": "3.2 CASE ONE: FULLY CONNECTED NEURAL NETWORK", "text": "The first straightforward choice for the model is the fully connected neural network. Its effectiveness in capturing the non-linearity relationships among the input features has been tested through the past decades. In order to feed the training data into the fully connected neural network, we extracted the flux value as a vector, and use this vector to train the model. We perform this data transformation based on the observation that the absorption bump is not related to the absolute wavelength value due to redshifts, but rather the neighboring flux value relationship. In addition, the wavelength interval between two consecutive wavelength-flux pair is relative stable in the data (both simulated and realworld). Due to the data collection mechanism, the number of pairs in one sample could be slightly different. We padded the samples with zeros to round up to a total of 4761 flux values.\nWe changed the configurations of the fully connected neural network and vary the number of hidden layers and the number of neurons per layer. The output layer is softmax with two classes, with or without absorption bump. The initial learning rate is set to 0.01 and step decreasing police is applied. The testing results after converged training are shown in Table 1. The performance for one hidden\nlayer is significantly worse than multiple-hidden-layer neural networks, due to its incapability to capture the non-linearity. The best performance is observed in a two-hidden-layer network with 400 neurons per layer. Its total number of trainable parameters is around 2 million."}, {"heading": "3.3 CASE TWO: IMAGE AND CONVOLUTIONAL NEURAL NETWORK", "text": "A quasar spectrum is plotted as the black curve in Figure 1 when inspected by researchers. This inspires us to transform raw data into images. We first plot all the training and testing samples. The coverage span for the wavelength is 8000 A\u030a and around 50 for the flux. The raw image drawn can be as large as hundreds of KB. The images are down scaled to 256*256 gray images and used for training and testing. Convolutional Neural Network (CNN) (LeCun & Bengio (1995)) recently has proved its effectiveness in image related recognition and classification, due to its capability of parameter sharing and capturing localized patterns. We adapted two popular CNN based models, the AlexNet (Krizhevsky et al. (2012)) and the GoogLeNet (Szegedy et al. (2014)). The results we get for modified AlexNet is 98.52% and for GoogLeNet is 98.55%.\n1 (a)\n(b)\n(c)\n(d)\n(e)\nFigure 2: Visualization for AlexNet. (a) shows the filters for the first convolutional layer. (b) and (d) plot two test images with absorption bump, marked with red dashed circle. (c) and (e) are the last convolutional layer feature maps for the two test images. The maps with effective activation are marked with red dashed rectangles. Better viewed in electronic version.\nIn order to understand the learned representation, we select the trained AlexNet model and visualize the first convolutional layer filters and also the last convolutional layer feature maps, as shown in Figure 2. The first convolutional layer filters are typical edge detection filters. The effective activation area in the feature maps (Figure 2c and 2e, marked with red dash rectangles) corresponds to the absorption bump locations in the test images. This proves that the representation is sensitive to the absorption bump. We also reconstructed the input images from a randomly generated image with respect to the given classes (with or without bump) using the gradient ascend method. The reconstructed images are shown in Figure3. The image for with bump clearly shows that the model is sensitive to the absorption bump in the input spectrum."}, {"heading": "3.4 CASE THREE: MATRIX AND CONVOLUTIONAL NEURAL NETWORK", "text": "The 256*256 image plus CNN model improves the accuracy. However, the input image is too sparse considering the large white background. In order to condense the input as well as empower the CNN model, we perform another data transformation. This transformation is also based on the assumption used for fully connected neural network. Different from the previous one, where the input is a 1D vector, we pad and fold the vector into a 69*69 matrix. These folded matrices are fed into the CNN models. This transformation makes it possible for filters in the convolutional layer to capture the localized information with neighboring flux values. We use different configurations of the CNN and\npost the best results we find in Table 2. We fix the CNN models with two fully connected layers after the convolutional layers, and alter the number of convolutional layers, the number of filters in each convolutional layer, and the kernel size. The best model we get achieves 99.404% test accuracy, and is a model with 4 convolutional layers, each with 50 filters, and the filter size is 5*5, 3*3, 3*3, 3*3 for the convolutional layers, respectively. The trainable parameters is around 900,000, instead of several or tens of millions in the previous two cases."}, {"heading": "3.5 DISCUSSION", "text": "In previous sections, we present three kinds of models and the corresponding data transformations. We select some of the trained models and plot their ROC curves, shown in Figure 4, and the corresponding AUCs are given in Table 3. The ROC curves for the selected models are consistent with the previously reported testing accuracy. For this absorption bump detection application, we could improve the sensitivity of the trained models by slightly decreasing the decision threshold. This could result in slightly more false positive signals, but is more confident in capturing the potential absorption bump events we are interested in.\n0.0 0.1 0.2 0.3 0.4\nfalse positive rate\n0.6\n0.7\n0.8\n0.9\n1.0\ntr u e p\no si\nti v e r\na te\nalexnet googlenet convnet2 convnet3 convnet4_1\nconvnet4_2\nconvnet4_3\nfc2\nFigure 4: ROC curves for various models\nTable 3: AUC of the ROC curves\nType AUC AlexNet 0.99915 GoogLeNet 0.99898 ConvNet2 0.99802 ConvNet3 0.99873 ConvNet4-1 0.99943 ConvNet4-2 0.99947 ConvNet4-3 0.99875\nFC2 0.99455\nBased on our experience with absorption bump detection application, there are a couple of things worth notice for data preparation and model selection. When generating raw data, in order to eliminate the bias in the trained model, carefully designed criteria should be followed to cover the cases as much as possible. Another necessary step before feeding the data into training is shuffling. The raw data are generated in a systematical manner. Using non-shuffled data, localized characteristics in the training batches could impede the convergence of the model. It could also cause diverged performance on training and testing data set. We have experienced these problems at early stage. The accuracy for the best model we get from non-shuffled data is as low as 88%.\nFrom the presented details, we believe that Deep Learning based method for absorption bump detection is effective. With a trained model, Mg II information is not required to filter new observations for candidates. The specifications for data generation and model configuration, and the trained model are easy to share and reuse. We observe from our experience that the data generation could also be modeled into a MapReduce-like programming model, significantly relieving users from heavy programming, but still benefiting from the power of distributed computing. In the following section, we present our generalization and current work on providing such a Deep Learning based framework for similar problems."}, {"heading": "4 GENERALIZATION: BOOSTING SCIENCE DISCOVERY WITH DEEP LEARNING", "text": ""}, {"heading": "4.1 SCIENCE DISCOVERY PROBLEM", "text": "Similar as the absorption bump detection, a category of problems in other science field involves the detection of a certain phenomenon among the background events. We call them the Science Discovery Problem. These discoveries are building blocks for further scientific research. Scientists rely on them to propose ideas, validate hypotheses, and prove theories. A typical science discovery problem consists of two main components, the feature set, F = {fi|0 < i <= N}, and the relation among the features, R. A science discovery problem can be expressed as:\nD = {F|F \u223c Rtarget(P,C),Rtarget \u2282 R} (3)\nD is the targeted discoveries, P and C are parameters and constants. F is one event in F. Rtarget defines the relations among the features that depict the targeted phenomenon. Rtarget and its complementary set Rctarget compose R.\nWith the F, R, Rtarget, scientists aim to pick the discoveries out of the observations. The mainstream method to perform the detection is rule-based filtering. The filtering rules are constructed upon raw features or extracted high level features. High level feature extraction can be done by numerically combining raw features, or by some fitting method, similar as the absorption bump application. While scientists are concerned with the accuracy of the approximation theories and continue improving their effectiveness, there are some issues with such methods.\nFeature Engineering. Feature engineering is a time-consuming procedure. It also depends on a lot of experience and numerous trial-and-error experiments. The complex and mysterious nature of the science discovery problems also increase the difficulty of extracting meaningful high level features to effectively capture the target event characteristics. Even with a carefully developed feature extraction technique with excellent theoretical explanation, real-world data could also offset the expected effectiveness considering the existence of various noise and uncertainty during the data collection phase.\nExtra Dependency. In some science discovery cases, in order to get a better approximated representation for the targeted event, extra effort is required to obtain additional information. For example, the approximated absorption bump representation requires emission and absorption redshifts. While the emission redshift is included in the public data set, the absorption redshift is calculated based on Mg II absorption lines. Similar requirements occur in other science fields. Two potential drawbacks in such scenarios impede the science exploration. First, these extra dependencies incur considerable amount of time and efforts. Second, they may not co-exist with the target discovery, thus, restricting the exploration scope due to incomplete data sets.\nSharing and Collaboration. Information exchange and collaboration are two significant factors to speed up the science discoveries. The theories and findings are well shared by researchers, but heavy data processing are required to generate results with existing methods for each new observation. Even though distributed computing technologies are pervasive and performance-ascendant, crossdiscipline knowledge requirement impedes its wide adoption."}, {"heading": "4.2 DEEP LEARNING BASED METHOD", "text": "Inspired by the advance and practical effectiveness of Deep Learning, we propose a data-driven, simulation-based method for science discovery problems. The proposed method is rooted in three observations. First, Deep Learning has proven itself with practical success on classification and recognition problems. Second, Deep Learning is more powerful when the relations among features are complicated. This is often the case in science discovery problems. Finally, Deep Learning requires a large amount of training data to be effective. Despite of the fact that real-world data might not be adequate and they are sometimes biased (i.e., the targeted discoveries are normally rare compared with the background events), we observe that it is relatively easy to simulate enough eligible training samples with desired information embedded in. The control over the training data sets ensures us to accurately capture the targeted phenomena. This both solves the biased training issue and satisfies Deep Learning requirement for large-scale training samples.\nThe workflow of the proposed method is shown in Figure 5. There are three main steps: data preparation, model training, and model service. During data preparation, two operations are performed: data generation (1) and data transformation (2). The approximation theory Rtarget and Rctarget, the constant variables C, and a set of range parameters P are used to generate enough labeled raw data. The raw data can be represented by a series of key-value pairs. The data transformation phase maps the raw data sets into desired formats for training, e.g., images or multi-dimensional matrix. The transformed data sets are used in model training step to train a pre-defined model (3). After the converge criteria are satisfied, the trained model will be published and ready for use (4). In model service step, the trained model is used to give decisions for real-world data (5). The real-world data set go through similar transformations as the simulated raw data sets. In science discovery problems, various phenomena could be detected upon the same feature set. This inspires us to introduce the fine tuning and transfer learning techniques into the workflow (6). Specifically, additional data could be generated to further tune an already trained model. By substituting some layers in the previous model and apply the fine tuning using extra data sets, a new model could be trained to identify the new phenomenon.\nThe architecture of the proposed DeepDis system is shown in Figure 6. There are four layers in DeepDis, namely user interface layer, control layer, runtime layer, and compute and storage resource layer. From the beginning of the design, several main considerations are planted in the gene of DeepDis. The first one is flexibility and easy-to-use. The second one is harnessing the power of various computing and storage resources. The third one is sharing. We show how we emphasize on these aspects in the following descriptions.\nUser Interface Layer. Two kinds of information are required from the users. The first part is the data generation function and parameters. Users are requested to implement their own generation functions. The function will take a dictionary of parameters, including constants and range parameters, and output all possible key-value pairs constrained by the ranges. It resembles the flat map function in many programming languages. Data specifications also contains instructions on how to\ntransform raw data sets into ready-to-process data format. The second part is the model configuration. The specifications are passed to DeepDis by json/prototxt. Only these configurations are required from the users. This makes DeepDis easy-to-use even for scientists with less knowledge about distributed computing or Deep Learning.\nControl Layer. This layer is responsible for understanding users requests and translate them into computing tasks. Specifically, DeepDis has a controller and three coordinators, Data Coordinator, Model Coordinator and Service Coordinator, to fulfill this. The controller divides the requests into several inter-dependent tasks and use DAG to schedule. The coordinators are responsible for the designated tasks: preparing, dividing, and distributing them to the underlying runtime. This layer also opens pluggable and extensible interfaces for advanced users to provide new data format, transformation functions, etc.\nRuntime Layer and Compute and Storage Resources Layer. DeepDis integrates state-of-the-art distributed in-memory computing framework, Spark (Zaharia et al. (2012)), and the Caffe (Jia et al. (2014)) Deep Learning toolbox to efficiently execute the data generation and model training tasks. Spark will be extended to incorporate adaptations from the control layer and to collaborate with Caffe for distributed model training. For the Compute and Storage Resources Layer, different types of resources will be utilized to achieve efficient data processing, model training, and sharing."}, {"heading": "5 CONCLUSION", "text": "In this paper, we presented a Deep Learning based method for detecting the absorption bump. We discussed in detail about how we generated and transformed the training data according to the selected models. For different models, we presented the results. In order to get a sense of what the models have learned, we chose the CNN based AlexNet model and provided the visualizations of filters, feature maps and reconstructed maximum activation input images. The resulted images prove that the model is identifying the absorption bump, rather than any other background noise. With the success in applying Deep Learning based method for the absorption bump application, we generalized the methodology to the broad science discovery problems, where Deep Learning models can be effectively trained upon sufficient amount of data. We also showed our ongoing work in building a specialized system, DeepDis, to support the proposed method. Distributed data processing techniques are used to automatically handle data generation, data transformation, model training and sharing. DeepDis is designed to provide science discovery as a service for researchers without too much knowledge about distributed computing and Deep Learning. With DeepDis, we hope to boost the science discovery process."}], "references": [{"title": "Side group addition to the polycyclic aromatic hydrocarbon coronene by ultraviolet photolysis in cosmic ice analogs", "author": ["Bernstein", "Max P", "Elsila", "Jamie E", "Dworkin", "Jason P", "Sandford", "Scott A", "Allamandola", "Louis J", "Zare", "Richard N"], "venue": "The Astrophysical Journal,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Amino acids from ultraviolet irradiation of interstellar ice analogues", "author": ["Caro", "GM Munoz", "UJ Meierhenrich", "WA Schutte", "B Barbier", "Segovia", "A Arcones", "H Rosenbauer", "Thiemann", "WH-P", "A Brack", "Greenberg", "JM"], "venue": null, "citeRegEx": "Caro et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Caro et al\\.", "year": 2002}, {"title": "Interstellar dust grains", "author": ["Draine", "BT"], "venue": "Annu. Rev. Astron. Astrophys,", "citeRegEx": "Draine and BT.,? \\Q2003\\E", "shortCiteRegEx": "Draine and BT.", "year": 2003}, {"title": "An analysis of the shapes of ultraviolet extinction curves. iii-an atlas of ultraviolet extinction curves", "author": ["Fitzpatrick", "Edward L", "Massa", "Derck"], "venue": "The Astrophysical Journal Supplement Series,", "citeRegEx": "Fitzpatrick et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fitzpatrick et al\\.", "year": 1990}, {"title": "The interstellar abundance of the hydrogen molecule", "author": ["Gould", "Robert J", "Salpeter", "Edwin E"], "venue": "i. basic processes. The Astrophysical Journal,", "citeRegEx": "Gould et al\\.,? \\Q1963\\E", "shortCiteRegEx": "Gould et al\\.", "year": 1963}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Surface recombination of hydrogen molecules", "author": ["Hollenbach", "David", "Salpeter", "EE"], "venue": "The Astrophysical Journal,", "citeRegEx": "Hollenbach et al\\.,? \\Q1971\\E", "shortCiteRegEx": "Hollenbach et al\\.", "year": 1971}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "A Dusty Mg II Absorber Associated with the Quasar SDSS J003545", "author": ["P Jiang", "J Ge", "JX Prochaska", "VP Kulkarni", "HL Lu", "Zhou", "HY"], "venue": "The Astrophysical Journal,", "citeRegEx": "Jiang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2010}, {"title": "High Dust Depletion in two Intervening Quasar Absorption Line Systems with the 2175 \u00c5 Extinction Bump at z \u0303 1.4", "author": ["Jiang", "Peng", "Ge", "Jian", "Prochaska", "J Xavier", "Wang", "Junfeng", "Zhou", "Hongyan", "Tinggui"], "venue": "The Astrophysical Journal,", "citeRegEx": "Jiang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2010}, {"title": "Toward Detecting the 2175 \u00c5 Dust Feature Associated with Strong High-redshift Mg II Absorption Lines", "author": ["Jiang", "Peng", "Ge", "Jian", "Zhou", "Hongyan", "Wang", "Junxian", "Tinggui"], "venue": "The Astrophysical Journal,", "citeRegEx": "Jiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Yann", "Bengio", "Yoshua"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "In dust we trust: an overview of observations and theories of interstellar dust", "author": ["Li", "Aigen", "Greenberg", "J Mayo"], "venue": "In Solid State Astrochemistry,", "citeRegEx": "Li et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Li et al\\.", "year": 2003}, {"title": "The unidentified infrared features after iso", "author": ["Peeters", "Els", "LJ Allamandola", "DM Hudgins", "S Hony", "Tielens", "AGGM"], "venue": "arXiv preprint astro-ph/0312184,", "citeRegEx": "Peeters et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Peeters et al\\.", "year": 2003}, {"title": "The pittsburgh sloan digital sky survey mg ii quasar absorptionline survey catalog", "author": ["Quider", "Anna M", "Nestor", "Daniel B", "Turnshek", "David A", "Rao", "Sandhya M", "Monier", "Eric M", "Weyant", "Anja N", "Busche", "Joseph R"], "venue": "The Astronomical Journal,", "citeRegEx": "Quider et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Quider et al\\.", "year": 2011}, {"title": "The sloan digital sky survey quasar catalog", "author": ["Schneider", "Donald P", "Richards", "Gordon T", "Hall", "Patrick B", "Strauss", "Michael A", "Anderson", "Scott F", "Boroson", "Todd A", "Ross", "Nicholas P", "Shen", "Yue", "WN Brandt", "Fan", "Xiaohui"], "venue": "v. seventh data release. The Astronomical Journal,", "citeRegEx": "Schneider et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 2010}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing", "author": ["Zaharia", "Matei", "Chowdhury", "Mosharaf", "Das", "Tathagata", "Dave", "Ankur", "Ma", "Justin", "McCauley", "Murphy", "Franklin", "Michael J", "Shenker", "Scott", "Stoica", "Ion"], "venue": "In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,", "citeRegEx": "Zaharia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2012}, {"title": "The jhu-sdss metal absorption line catalog: redshift evolution and properties of mg ii absorbers", "author": ["Zhu", "Guangtun", "M\u00e9nard", "Brice"], "venue": "The Astrophysical Journal,", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "PAHs are recognized as the most abundant organic molecules in our Milk Way and other neighboring galaxies (Peeters et al. (2003)), and are believed to be the building blocks of organic life (Bernstein et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 0, "context": "(2003)), and are believed to be the building blocks of organic life (Bernstein et al. (2002); Caro et al.", "startOffset": 69, "endOffset": 93}, {"referenceID": 0, "context": "(2003)), and are believed to be the building blocks of organic life (Bernstein et al. (2002); Caro et al. (2002)).", "startOffset": 69, "endOffset": 113}, {"referenceID": 0, "context": "(2003)), and are believed to be the building blocks of organic life (Bernstein et al. (2002); Caro et al. (2002)). The traditional way of discovering the absorption bump mainly depends on statistic techniques (Jiang et al. (2011)).", "startOffset": 69, "endOffset": 230}, {"referenceID": 0, "context": "(2003)), and are believed to be the building blocks of organic life (Bernstein et al. (2002); Caro et al. (2002)). The traditional way of discovering the absorption bump mainly depends on statistic techniques (Jiang et al. (2011)). It generally involves three steps. First, a composite quasar spectrum is constructed using median combining (Jiang et al. (2010a;b)). The composite spectrum is then reddened and used to fit every spectrum from the candidate absorption bump spectra with a parameterized absorption profile. Finally, by applying constrains on peak position, bump width and bump height, a large portion of spectra without bump features are filtered out. In order to further exclude the false positives caused by some noises, a simulation technique (Jiang et al. (2010a;b)) is applied to determine the detection significance. Among the steps, curve fitting occupies considerable amount of time. In order to convolve the absorption bump with the composite spectrum, extra information is needed to determine the absorption redshift. This significantly constrains the available observations due to the lack of this extra information in some cases. While the composite quasar spectrum could potentially be reused for other related computations, the fitting procedure evolves iterative error minimization to get the best bump profile using curve fitting method, and is required for each and every new observation. The fitting is restricted to relatively smooth continuum of the quasar spectrum. However, the complicated types of emission and absorption in the candidate spectra could potentially disturb the fitting process, reducing the effectiveness of the method. All these issues are tightly coupled with the method itself, and cannot be easily resolved. In this paper, we propose to apply a Deep Learning based method to detect the absorption bump and try to alleviate the aforementioned issues. Deep Learning is recently recognized with its ability to automatically extract high level features and accurately recognize or classify the target objects (Hinton & Salakhutdinov (2006); LeCun et al.", "startOffset": 69, "endOffset": 2093}, {"referenceID": 0, "context": "(2003)), and are believed to be the building blocks of organic life (Bernstein et al. (2002); Caro et al. (2002)). The traditional way of discovering the absorption bump mainly depends on statistic techniques (Jiang et al. (2011)). It generally involves three steps. First, a composite quasar spectrum is constructed using median combining (Jiang et al. (2010a;b)). The composite spectrum is then reddened and used to fit every spectrum from the candidate absorption bump spectra with a parameterized absorption profile. Finally, by applying constrains on peak position, bump width and bump height, a large portion of spectra without bump features are filtered out. In order to further exclude the false positives caused by some noises, a simulation technique (Jiang et al. (2010a;b)) is applied to determine the detection significance. Among the steps, curve fitting occupies considerable amount of time. In order to convolve the absorption bump with the composite spectrum, extra information is needed to determine the absorption redshift. This significantly constrains the available observations due to the lack of this extra information in some cases. While the composite quasar spectrum could potentially be reused for other related computations, the fitting procedure evolves iterative error minimization to get the best bump profile using curve fitting method, and is required for each and every new observation. The fitting is restricted to relatively smooth continuum of the quasar spectrum. However, the complicated types of emission and absorption in the candidate spectra could potentially disturb the fitting process, reducing the effectiveness of the method. All these issues are tightly coupled with the method itself, and cannot be easily resolved. In this paper, we propose to apply a Deep Learning based method to detect the absorption bump and try to alleviate the aforementioned issues. Deep Learning is recently recognized with its ability to automatically extract high level features and accurately recognize or classify the target objects (Hinton & Salakhutdinov (2006); LeCun et al. (2015)).", "startOffset": 69, "endOffset": 2114}, {"referenceID": 15, "context": ", Fitzpatrick & Massa (1990); Schneider et al. (2010)).", "startOffset": 30, "endOffset": 54}, {"referenceID": 15, "context": "The Mg II absorption lines are identified using some extra step, and is itself a research topic (Quider et al. (2011); Zhu & M\u00e9nard (2013)).", "startOffset": 97, "endOffset": 118}, {"referenceID": 15, "context": "The Mg II absorption lines are identified using some extra step, and is itself a research topic (Quider et al. (2011); Zhu & M\u00e9nard (2013)).", "startOffset": 97, "endOffset": 139}, {"referenceID": 11, "context": "We adapted two popular CNN based models, the AlexNet (Krizhevsky et al. (2012)) and the GoogLeNet (Szegedy et al.", "startOffset": 54, "endOffset": 79}, {"referenceID": 11, "context": "We adapted two popular CNN based models, the AlexNet (Krizhevsky et al. (2012)) and the GoogLeNet (Szegedy et al. (2014)).", "startOffset": 54, "endOffset": 121}, {"referenceID": 17, "context": "DeepDis integrates state-of-the-art distributed in-memory computing framework, Spark (Zaharia et al. (2012)), and the Caffe (Jia et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 7, "context": "(2012)), and the Caffe (Jia et al. (2014)) Deep Learning toolbox to efficiently execute the data generation and model training tasks.", "startOffset": 24, "endOffset": 42}], "year": 2015, "abstractText": "The pervasive interstellar dust grains provide significant insights to understand the formation and evolution of the stars, planetary systems, and the galaxies, and may harbor the building blocks of life. One of the most effective way to analyze the dust is via their interaction with the light from background sources. The observed extinction curves and spectral features carry the size and composition information of dust. The broad absorption bump at 2175 \u00c5 is the most prominent feature in the extinction curves. Traditionally, statistical methods are applied to detect the existence of the absorption bump. These methods require heavy preprocessing and the co-existence of other reference features to alleviate the influence from the noises. In this paper, we apply Deep Learning techniques to detect the broad absorption bump. We demonstrate the key steps for training the selected models and their results. The success of Deep Learning based method inspires us to generalize a common methodology for broader science discovery problems. We present our on-going work to build the DeepDis system for such kind of applications.", "creator": "LaTeX with hyperref package"}}}