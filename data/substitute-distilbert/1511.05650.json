{"id": "1511.05650", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models", "abstract": "normalized convergence populations ( nrms ) provide a useful class of discrete random measures that are often used as priors for bayesian nonparametric models. dirichlet process is a well - known example of nrms. most of posterior inference networks for nrm mixture models rely on mcmc methods assuming networks are easy to implement and slower convergence is well studied. first, mcmc often suffers from slow convergence when the acceptance rate is low. tree - based inference is an expensive comparative posterior inference method, where bayesian clustered clustering ( bhc ) or incremental finite integrated clustering ( ibhc ) have been coined for dp or nrm mixture ( nrmm ) models, respectively. although ibhc is a promising method for posterior inference analyzing nrmm models due to its efficiency and similarity to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. in other paper, we find a hybrid inference algorithm for nrmm users, particularly considers the merits in both mcmc and ibhc. graphs built by ibhc outlines partitions of data, which guides metropolis - hastings procedure to employ appropriate proposals. inheriting robust nature principle mcmc, specifically tree - guided mcmc ( tgmcmc ) is guaranteed to converge, and enjoys the fast continuation thanks to the tailored proposals guided by trees. experiments on continuous analytics and real - world datasets demonstrate the benefit of our method.", "histories": [["v1", "Wed, 18 Nov 2015 03:16:27 GMT  (795kb,D)", "http://arxiv.org/abs/1511.05650v1", "12 pages, 10 figures, NIPS-2015"]], "COMMENTS": "12 pages, 10 figures, NIPS-2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["juho lee", "seungjin choi"], "accepted": true, "id": "1511.05650"}, "pdf": {"name": "1511.05650.pdf", "metadata": {"source": "CRF", "title": "Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models", "authors": ["Juho Lee"], "emails": ["stonecold@postech.ac.kr", "seungjin@postech.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4]. NRM mixture (NRMM) model [5] is a representative example where NRM is used as a prior for mixture models. Recently NRMs were extended to dependent NRMs (DNRMs) [6, 7] to model data where exchangeability fails. The posterior analysis for NRM mixture (NRMM) models has been developed [8, 9], yielding simple MCMC methods [10]. As in DP mixture (DPM) models [11], there are two paradigms in the MCMC algorithms for NRMM models: (1) marginal samplers and (2) slice samplers. The marginal samplers simulate the posterior distributions of partitions and cluster parameters given data (or just partitions given data provided that conjugate priors are assumed) by marginalizing out the random measures. The marginal samplers include the Gibbs sampler [10], and the split-merge sampler [12], although it was not formally extended to NRMM models. The slice sampler [13] maintains random measures and explicitly samples the weights and atoms of the random measures. The term \u201dslice\u201d comes from the auxiliary slice variables used to control the number of atoms to be used. The slice sampler is known to mix faster than the marginal Gibbs sampler when applied to complicated DNRM mixture models where the evaluation of marginal distribution is costly [7].\nar X\niv :1\n51 1.\n05 65\n0v 1\n[ st\nat .M\nL ]\n1 8\nN ov\nThe main drawback of MCMC methods for NRMM models is their poor scalability, due to the nature of MCMC methods. Moreover, since the marginal Gibbs sampler and slice sampler iteratively sample the cluster assignment variable for a single data point at a time, they easily get stuck in local optima. Split-merge sampler may resolve the local optima problem to some extent, but is still problematic for large-scale datasets since the samples proposed by split or merge procedures are rarely accepted. Recently, a deterministic alternative to MCMC algorithms for NRM (or DNRM) mixture models were proposed [14], extending Bayesian hierarchical clustering (BHC) [15] which was developed as a tree-based inference for DP mixture models. The algorithm, referred to as incremental BHC (IBHC) [14] builds binary trees that reflects the hierarchical cluster structures of datasets by evaluating the approximate marginal likelihood of NRMM models, and is well suited for the incremental inferences for large-scale or streaming datasets. The key idea of IBHC is to consider only exponentially many posterior samples (which are represented as binary trees), instead of drawing indefinite number of samples as in MCMC methods. However, IBHC depends on the heuristics that chooses the best trees after the multiple trials, and thus is not guaranteed to converge to the true posterior distributions.\nIn this paper, we propose a novel MCMC algorithm that elegantly combines IBHC and MCMC methods for NRMM models. Our algorithm, called the tree-guided MCMC, utilizes the trees built from IBHC to proposes a good quality posterior samples efficiently. The trees contain useful information such as dissimilarities between clusters, so the errors in cluster assignments may be detected and corrected with less efforts. Moreover, designed as a MCMC methods, our algorithm is guaranteed to converge to the true posterior, which was not possible for IBHC. We demonstrate the efficiency and accuracy of our algorithm by comparing it to existing MCMC algorithms."}, {"heading": "2 Background", "text": "Throughout this paper we use the following notations. Denote by [n] = {1, . . . , n} a set of indices and by X = {xi | i \u2208 [n]} a dataset. A partition \u03a0[n] of [n] is a set of disjoint nonempty subsets of [n] whose union is [n]. Cluster c is an entry of \u03a0[n], i.e., c \u2208 \u03a0[n]. Data points in cluster c is denoted by Xc = {xi | i \u2208 c} for c \u2208 \u03a0n. For the sake of simplicity, we often use i to represent a singleton {i} for i \u2208 [n]. In this section, we briefly review NRMM models, existing posterior inference methods such as MCMC and IBHC."}, {"heading": "2.1 Normalized random measure mixture models", "text": "Let \u00b5 be a homogeneous completely random measure (CRM) on measure space (\u0398,F) with Le\u0301vy intensity \u03c1 and base measure H , written as \u00b5 \u223c CRM(\u03c1H). We also assume that,\u222b \u221e\n0\n\u03c1(dw) =\u221e, \u222b \u221e\n0\n(1\u2212 e\u2212w)\u03c1(dw) <\u221e, (1)\nso that \u00b5 has infinitely many atoms and the total mass \u00b5(\u0398) is finite: \u00b5 = \u2211\u221e j=1 wj\u03b4\u03b8\u2217j , \u00b5(\u0398) =\u2211\u221e\nj=1 wj < \u221e. A NRM is then formed by normalizing \u00b5 by its total mass \u00b5(\u0398). For each index i \u2208 [n], we draw the corresponding atoms from NRM, \u03b8i|\u00b5 \u223c \u00b5/\u00b5(\u0398). Since \u00b5 is discrete, the set {\u03b8i|i \u2208 [n]} naturally form a partition of [n] with respect to the assigned atoms. We write the partition as a set of sets \u03a0[n] whose elements are non-empty and non-overlapping subsets of [n], and the union of the elements is [n]. We index the elements (clusters) of \u03a0[n] with the symbol c, and denote the unique atom assigned to c as \u03b8c. Summarizing the set {\u03b8i|i \u2208 [n]} as (\u03a0[n], {\u03b8c|c \u2208 \u03a0[n]}), the posterior random measure is written as follows: Theorem 1. ([9]) Let (\u03a0[n], {\u03b8c|c \u2208 \u03a0[n]}) be samples drawn from \u00b5/\u00b5(\u0398) where \u00b5 \u223c CRM(\u03c1H). With an auxiliary variable u \u223c Gamma(n, \u00b5(\u0398)), the posterior random measure is written as\n\u00b5|u+ \u2211 c\u2208\u03a0[n] wc\u03b4\u03b8c , (2)\nwhere\n\u03c1u(dw) := e \u2212uw\u03c1(dw), \u00b5|u \u223c CRM(\u03c1uH), P (dwc) \u221d w|c|c \u03c1u(dwc). (3)\nMoreover, the marginal distribution is written as\nP (\u03a0[n], {d\u03b8c|c \u2208 \u03a0[n]}, du) = un\u22121e\u2212\u03c8\u03c1(u)du\n\u0393(n)\n\u220f c\u2208\u03a0[n] \u03ba\u03c1(|c|, u)H(d\u03b8c), (4)\nwhere\n\u03c8\u03c1(u) := \u222b \u221e 0 (1\u2212 e\u2212uw)\u03c1(dw), \u03ba\u03c1(|c|, u) := \u222b \u221e 0 w|c|\u03c1u(dw). (5)\nUsing (4), the predictive distribution for the novel atom \u03b8 is written as\nP (d\u03b8|{\u03b8i}, u) \u221d \u03ba\u03c1(1, u)H(d\u03b8) + \u2211 c\u2208\u03a0[n] \u03ba\u03c1(|c|+ 1, u) \u03ba\u03c1(|c|, u) \u03b4\u03b8c(d\u03b8). (6)\nThe most general CRM may be used is the generalized Gamma [3], with Le\u0301vy intensity \u03c1(dw) = \u03b1\u03c3\n\u0393(1\u2212\u03c3)w \u2212\u03c3\u22121e\u2212wdw. In NRMM models, the observed datasetX is assusmed to be generated from a likelihood P (dx|\u03b8) with parameters {\u03b8i} drawn from NRM. We focus on the conjugate case where H is conjugate to P (dx|\u03b8), so that the integral P (dXc) := \u222b \u0398 H(d\u03b8) \u220f i\u2208c P (dxi|\u03b8) is tractable."}, {"heading": "2.2 MCMC Inference for NRMM models", "text": "The goal of posterior inference for NRMM models is to compute the posterior P (\u03a0[n], {d\u03b8c}, du|X) with the marginal likelihood P (dX).\nMarginal Gibbs Sampler: marginal Gibbs sampler is basesd on the predictive distribution (6). At each iteration, cluster assignments for each data point is sampled, where xi may join an existing cluster c with probability proportional to \u03ba\u03c1(|c|+1,u)\u03ba\u03c1(|c|,u) P (dxi|Xc), or create a novel cluster with probability proportional to \u03ba\u03c1(1, u)P (dxi).\nSlice sampler: instead of marginalizing out \u00b5, slice sampler explicitly sample the atoms and weights {wj , \u03b8\u2217j } of \u00b5. Since maintaining infinitely many atoms is infeasible, slice variables {si} are introduced for each data point, and atoms with masses larger than a threshold (usually set as mini\u2208[n] si) are kept and remaining atoms are added on the fly as the threshold changes. At each iteration, xi is assigned to the jth atom with probability 1[si < wj ]P (dxi|\u03b8\u2217j ). Split-merge sampler: both marginal Gibbs and slice sampler alter a single cluster assignment at a time, so are prone to the local optima. Split-merge sampler, originally developed for DPM, is a marginal sampler that is based on (6). At each iteration, instead of changing individual cluster assignments, split-merge sampler splits or merges clusters to propose a new partition. The split or merged partition is proposed by a procedure called the restricted Gibbs sampling, which is Gibbs sampling restricted to the clusters to split or merge. The proposed partitions are accepted or rejected according to Metropolis-Hastings schemes. Split-merge samplers are reported to mix better than marginal Gibbs sampler."}, {"heading": "2.3 IBHC Inference for NRMM models", "text": "Bayesian hierarchical clustering (BHC, [15]) is a probabilistic model-based agglomerative clustering, where the marginal likelihood of DPM is evaluated to measure the dissimilarity between nodes. Like the traditional agglomerative clustering algorithms, BHC repeatedly merges the pair of nodes with the smallest dissimilarities, and builds binary trees embedding the hierarchical cluster structure of datasets. BHC defines the generative probability of binary trees which is maximized during the construction of the tree, and the generative probability provides a lower bound on the marginal likelihood of DPM. For this reason, BHC is considered to be a posterior inference algorithm for DPM. Incremental BHC (IBHC, [14]) is an extension of BHC to (dependent) NRMM models. Like BHC is a deterministic posterior inference algorithm for DPM, IBHC serves as a deterministic posterior inference algorithms for NRMM models. Unlike the original BHC that greedily builds trees, IBHC sequentially insert data points into trees, yielding scalable algorithm that is well suited for online inference. We first explain the generative model of trees, and then explain the sequential algorithm of IBHC.\nIBHC aims to maximize the joint probability of the data X and the auxiliary variable u:\nP (dX, du) = un\u22121e\u2212\u03c8\u03c1(u)du\n\u0393(n)\n\u2211 \u03a0[n] \u220f c\u2208\u03a0[n] \u03ba\u03c1(|c|, u)P (dXc) (7)\nLet tc be a binary tree whose leaf nodes consist of the indices in c. Let l(c) and r(c) denote the left and right child of the set c in tree, and thus the corresponding trees are denoted by tl(c) and tr(c). The generative probability of trees is described with the potential function [14], which is the unnormalized reformulation of the original definition [15]. The potential function of the data Xc given the tree tc is recursively defined as follows:\n\u03c6(Xc|hc) := \u03ba\u03c1(|c|, u)P (dXc), \u03c6(Xc|tc) = \u03c6(Xc|hc) + \u03c6(Xl(c)|tl(c))\u03c6(Xr(c)|tr(c)). (8) Here, hc is the hypothesis that Xc was generated from a single cluster. The first therm \u03c6(Xc|hc) is proportional to the probability that hc is true, and came from the term inside the product of (7). The second term is proportional to the probability that Xc was generated from more than two clusters embedded in the subtrees tl(c) and tr(c). The posterior probability of hc is then computed as\nP (hc|Xc, tc) = 1\n1 + d(l(c), r(c)) , where d(l(c), r(c)) := \u03c6(Xl(c)|tl(c))\u03c6(Xr(c)|tr(c)) \u03c6(Xc|hc) . (9)\nd(\u00b7, \u00b7) is defined to be the dissimilarity between l(c) and r(c). In the greedy construction, the pair of nodes with smallest d(\u00b7, \u00b7) are merged at each iteration. When the minimum dissimilarity exceeds one (P (hc|Xc, tc) < 0.5), hc is concluded to be false and the construction stops. This is an important mechanism of BHC (and IBHC) that naturally selects the proper number of clusters. In the perspective of the posterior inference, this stopping corresponds to selecting the MAP partition that maximizes P (\u03a0[n]|X,u). If the tree is built and the potential function is computed for the entire dataset X , a lower bound on the joint likelihood (7) is obtained [15, 14]:\nun\u22121e\u2212\u03c8\u03c1(u)du \u0393(n) \u03c6(X|t[n]) \u2264 P (dX, du). (10)\nNow we explain the sequential tree construction of IBHC. IBHC constructs a tree in an incremental manner by inserting a new data point into an appropriate position of the existing tree, without computing dissimilarities between every pair of nodes. The procedure, which comprises three steps, is elucidated in Fig. 1.\nStep 1 (left): Given {x1, . . . , xi\u22121}, suppose that trees are built by IBHC, yielding to a partition \u03a0[i\u22121]. When a new data point xi arrives, this step assigns xi to a tree tc\u0302, which has the smallest distance, i.e., c\u0302 = arg minc\u2208\u03a0[i\u22121] d(i, c), or create a new tree ti if d(i, c\u0302) > 1.\nStep 2 (middle): Suppose that the tree chosen in Step 1 is tc. Then Step 2 determines an appropriate position of xi when it is inserted into the tree tc, and this is done by the procedure SeqInsert(c, i). SeqInsert(c, i) chooses the position of i among three cases (Fig. 1). Case 1 elucidates an option where xi is placed on the top of the tree tc. Case 2 and 3 show options where xi is added as a sibling of the subtree tl(c) or tr(c), respectively. Among these three cases, the one with the highest potential function \u03c6(Xc\u222ai|tc\u222ai) is selected, which can easily be done by comparing d(l(c), r(c)), d(l(c), i) and d(r(c), i) [14]. If d(l(c), r(c)) is the smallest, then Case 1 is selected and the insertion terminates. Otherwise, if d(l(c), i) is the smallest, xi is inserted into tl(c) and SeqInsert(l(c), i) is recursively executed. The same procedure is applied to the case where d(r(c), i) is smallest.\nStep 3 (right): After Step 1 and 2 are applied, the potential functions of tc\u222ai should be computed again, starting from the subtree of tc to which xi is inserted, to the root tc\u222ai. During this procedure, updated d(\u00b7, \u00b7) values may exceed 1. In such a case, we split the tree at the level where d(\u00b7, \u00b7) > 1, and re-insert all the split nodes.\nAfter having inserted all the data points in X , the auxiliary variable u and hyperparameters for \u03c1(dw) are resampled, and the tree is reconstructed. This procedure is repeated several times and the trees with the highest potential functions are chosen as an output."}, {"heading": "3 Main results: A tree-guided MCMC procedure", "text": "IBHC should reconstruct trees from the ground whenever u and hyperparameters are resampled, and this is obviously time consuming, and more importantly, converge is not guaranteed. Instead of completely reconstructing trees, we propose to refine the parts of existing trees with MCMC. Our algorithm, called tree-guided MCMC (tgMCMC), is a combination of deterministic tree-based inference and MCMC, where the trees constructed via IBHC guides MCMC to propose good-quality samples. tgMCMC initialize a chain with a single run of IBHC. Given a current partition \u03a0[n] and trees {tc | c \u2208 \u03a0[n]}, tgMCMC proposes a novel partition \u03a0\u2217[n] by global and local moves. Global moves split or merges clusters to propose \u03a0\u2217[n], and local moves alters cluster assignments of individual data points via Gibbs sampling. We first explain the two key operations used to modify tree structures, and then explain global and local moves. More details on the algorithm can be found in the supplementary material."}, {"heading": "3.1 Key operations", "text": "SampleSub(c, p): given a tree tc, draw a subtree tc\u2032 with probability \u221d d(l(c\u2032), r(c\u2032)) + . is added for leaf nodes whose d(\u00b7, \u00b7) = 0, and set to the maximum d(\u00b7, \u00b7) among all subtrees of tc. The drawn subtree is likely to contain errors to be corrected by splitting. The probability of drawing tc\u2032 is multiplied to p, where p is usually set to transition probabilities.\nStocInsert(S, c, p): a stochastic version of IBHC. c may be inserted to c\u2032 \u2208 S via SeqInsert(c\u2032, c) with probability d\n\u22121(c\u2032,c) 1+ \u2211 c\u2032\u2208S d \u22121(c\u2032,c) , or may just be put into S (create a new cluster\nin S) with probability 11+\u2211c\u2032\u2208S d\u22121(c\u2032,c) . If c is inserted via SeqInsert, the potential functions are updated accordingly, but the trees are not split even if the update dissimilarities exceed 1. As in SampleSub, the probability is multiplied to p."}, {"heading": "3.2 Global moves", "text": "The global moves of tgMCMC are tree-guided analogy to split-merge sampling. In split-merge sampling, a pair of data points are randomly selected, and split partition is proposed if they belong to the same cluster, or merged partition is proposed otherwise. Instead, tgMCMC finds the clusters that are highly likely to be split or merged using the dissimilarities between trees, which goes as follows in detail. First, we randomly pick a tree tc in uniform. Then, we compute d(c, c\u2032) for\nc\u2032 \u2208 \u03a0[n]\\c, and put c\u2032 in a set M with probability (1 + d(c, c\u2032))\u22121 (the probability of merging c and c\u2032). The transition probability q(\u03a0\u2217[n]|\u03a0[n]) up to this step is 1|\u03a0[n]| \u220f c\u2032 d(c,c\u2032)1[c \u2032 /\u2208M]\n1+d(c,c\u2032) . The set M contains candidate clusters to merge with c. If M is empty, which means that there are no candidates to merge with c, we propose \u03a0\u2217[n] by splitting c. Otherwise, we propose \u03a0 \u2217 [n] by merging c and clusters in M .\nSplit case: we start splitting by drawing a subtree tc? by SampleSub(c, q(\u03a0\u2217[n]|\u03a0[n])) 1. Then we split c? to S = {l(c?), r(c?)}, destroy all the parents of tc? and collect the split trees into a set Q (Fig. 2, top). Then we reconstruct the tree by StocInsert(S, c\u2032, q(\u03a0\u2217[n]|\u03a0[n])) for all c\u2032 \u2208 Q. After the reconstruction, S has at least two clusters since we split S = {l(c?), r(c?)} before insertion. The split partition to propose is \u03a0\u2217[n] = (\u03a0[n]\\c) \u222a S. The reverse transition probability q(\u03a0[n]|\u03a0\u2217[n]) is computed as follows. To obtain \u03a0[n] from \u03a0\u2217[n], we must merge the clusters in S to c. For this, we should pick a cluster c\u2032 \u2208 S, and put other clusters in S\\c into M . Since we can pick any c\u2032 at first, the reverse transition probability is computed as a sum of all those possibilities:\nq(\u03a0[n]|\u03a0\u2217[n]) = \u2211 c\u2032\u2208S 1 |\u03a0\u2217[n]| \u220f\nc\u2032\u2032\u2208\u03a0\u2217 [n] \\c\u2032\nd(c\u2032, c\u2032\u2032)1[c \u2032\u2032 /\u2208S]\n1 + d(c\u2032, c\u2032\u2032) , (11)\nMerge case: suppose that we have M = {c1, . . . , cm} 2. The merged partition to propose is given as \u03a0\u2217[n] = (\u03a0[n]\\M) \u222a cm+1, where cm+1 = \u22c3m i=1 cm. We construct the corresponding binary tree as a cascading tree, where we put c1, . . . cm on top of c in order (Fig. 2, bottom). To compute the reverse transition probability q(\u03a0[n]|\u03a0\u2217[n]), we should compute the probability of splitting cm+1 back into c1, . . . , cm. For this, we should first choose cm+1 and put nothing into the set M to provoke splitting. q(\u03a0[n]|\u03a0\u2217[n]) up to this step is 1|\u03a0\u2217 [n] | \u220f c\u2032 d(cm+1,c \u2032) 1+d(cm+1,c\u2032) . Then, we should sample the parent of c (the subtree connecting c and c1) via SampleSub(cm+1, q(\u03a0[n]|\u03a0\u2217[n])), and this would result in S = {c, c1} and Q = {c2, . . . , cm}. Finally, we insert ci \u2208 Q into S via StocInsert(S, ci, q(\u03a0[n]|\u03a0\u2217[n])) for i = 2, . . . ,m, where we select each c(i) to create a new cluster in S. Corresponding update to q(\u03a0[n]|\u03a0\u2217[n]) by StocInsert is,\nq(\u03a0[n]|\u03a0\u2217[n])\u2190 q(\u03a0[n]|\u03a0\u2217[n]) \u00b7 m\u220f i=2\n1 1 + d\u22121(c, ci) + \u2211i\u22121 j=1 d \u22121(cj , ci) . (12)\nOnce we\u2019ve proposed \u03a0\u2217[n] and computed both q(\u03a0 \u2217 [n]|\u03a0[n]) and q(\u03a0[n]|\u03a0\u2217[n]), \u03a0\u2217[n] is accepted with probability min{1, r} where r = p(dX,du,\u03a0[n]\u2217)q(\u03a0[n]|\u03a0 \u2217 [n])\np(dX,du,\u03a0[n])q(\u03a0 \u2217 [n] |\u03a0[n]) .\nErgodicity of the global moves: to show that the global moves are ergodic, it is enough to show that we can move an arbitrary point i from its current cluster c to any other cluster c\u2032 in finite step. This can easily be done by a single split and merge moves, so the global moves are ergodic.\nTime complexity of the global moves: the time complexity of StocInsert(S, c, p) is O(|S|+h), where h is a height of the tree to insert c. The total time complexity of split proposal is mainly determined by the time to execute StocInsert(S, c, p). This procedure is usually efficient, especially when the trees are well balanced. The time complexity to propose merged partition isO(|\u03a0[n]|+M)."}, {"heading": "3.3 Local moves", "text": "In local moves, we resample cluster assignments of individual data points via Gibbs sampling. If a leaf node i is moved from c to c\u2032, we detach i from tc and run SeqInsert(c\u2032, i) 3. Here, instead of running Gibbs sampling for all data points, we run Gibbs sampling for a subset of data points S, which is formed as follows. For each c \u2208 \u03a0[n], we draw a subtree tc\u2032 by SampleSub. Then, we\n1Here, we restrict SampleSub to sample non-leaf nodes, since leaf nodes cannot be split. 2We assume that clusters are given their own indices (such as hash values) so that they can be ordered. 3We do not split even if the update dissimilarity exceed one, as in StocInsert.\ndraw a subtree of tc\u2032 again by SampleSub. We repeat this subsampling for D times, and put the leaf nodes of the final subtree into S. Smaller D would result in more data points to resample, so we can control the tradeoff between iteration time and mixing rates.\nCycling: at each iteration of tgMCMC, we cycle the global moves and local moves, as in split-merge sampling. We first run the global moves for G times, and run a single sweep of local moves. Setting G = 20 and D = 2 were the moderate choice for all data we\u2019ve tested."}, {"heading": "4 Experiments", "text": "In this section, we compare marginal Gibbs sampler (Gibbs), split-merge sampler (SM) and tgMCMC on synthetic and real datasets."}, {"heading": "4.1 Toy dataset", "text": "We first compared the samplers on simple toy dataset that has 1,300 two-dimensional points with 13 clusters, sampled from the mixture of Gaussians with predefined means and covariances. Since the partition found by IBHC is almost perfect for this simple data, instead of initializing with IBHC, we initialized the binary tree (and partition) as follows. As in IBHC, we sequentially inserted data points into existing trees with a random order. However, instead of inserting them via SeqInsert, we just put data points on top of existing trees, so that no splitting would occur. tgMCMC was initialized with the tree constructed from this procedure, and Gibbs and SM were initialized with corresponding partition. We assumed the Gaussian-likelihood and Gaussian-Wishart base measure,\nH(d\u00b5, d\u039b) = N (d\u00b5|m, (r\u039b)\u22121)W(d\u039b|\u03a8\u22121, \u03bd), (13)\nwhere r = 0.1, \u03bd = d+6, d is the dimensionality,m is the sample mean and \u03a8 = \u03a3/(10\u00b7det(\u03a3))1/d (\u03a3 is the sample covariance). We compared the samplers using both DP and NGGP priors. For tgMCMC, we fixed the number of global moves G = 20 and the parameter for local moves D = 2, except for the cases where we controlled them explicitly. All the samplers were run for 10 seconds, and repeated 10 times. We compared the joint log-likelihood log p(dX,\u03a0[n], du) of samples and the effective sample size (ESS) of the number of clusters found. For SM and tgMCMC, we compared the average log value of the acceptance ratio r. The results are summarized in Fig. 3. As shown in\nthe log-likelihood trace plot, tgMCMC quickly converged to the ground truth solution for both DP and NGGP cases. Also, tgMCMC mixed better than other two samplers in terms of ESS. Comparing the average log r values of SM and tgMCMC, we can see that the partitions proposed by tgMCMC is more often accepted. We also controlled the parameter G and D; as expected, higher G resulted in faster convergence. However, smaller D (more data points involved in local moves) did not necessarily mean faster convergence."}, {"heading": "4.2 Large-scale synthetic dataset", "text": "We also compared the three samplers on larger dataset containing 10,000 points, which we will call as 10K dataset, generated from six-dimensional mixture of Gaussians with labels drawn from PY(3, 0.8). We used the same base measure and initialization with those of the toy datasets, and used the NGGP prior, We ran the samplers for 1,000 seconds and repeated 10 times. Gibbs and SM were too slow, so the number of samples produced in 1,000 seconds were too small. Hence, we also compared Gibbs sub and SM sub, where we uniformly sampled the subset of data points and ran Gibbs sweep only for those sampled points. We controlled the subset size to make their running time similar to that of tgMCMC. The results are summarized in Fig. 4. Again, tgMCMC outperformed other samplers both in terms of the log-likelihoods and ESS. Interestingly, SM was even worse than Gibbs, since most of the samples proposed by split or merge proposal were rejected. Gibbs sub and SM sub were better than Gibbs and SM, but still failed to reach the best state found by tgMCMC."}, {"heading": "4.3 NIPS corpus", "text": "We also compared the samplers on NIPS corpus4, containing 1,500 documents with 12,419 words. We used the multinomial likelihood and symmetric Dirichlet base measure Dir(0.1), used NGGP prior, and initialized the samplers with normal IBHC. As for the 10K dataset, we compared Gibbs sub and SM sub along. We ran the samplers for 10,000 seconds and repeated 10 times. The results are summarized in Fig. 5. tgMCMC outperformed other samplers in terms of the loglikelihood; all the other samplers were trapped in local optima and failed to reach the states found by tgMCMC. However, ESS for tgMCMC were the lowest, meaning the poor mixing rates. We still argue that tgMCMC is a better option for this dataset, since we think that finding the better log-likelihood states is more important than mixing rates."}, {"heading": "5 Conclusion", "text": "In this paper we have presented a novel inference algorithm for NRMM models. Our sampler, called tgMCMC, utilized the binary trees constructed by IBHC to propose good quality samples. tgMCMC explored the space of partitions via global and local moves which were guided by the potential functions of trees. tgMCMC was demonstrated to be outperform existing samplers in both synthetic and real world datasets.\nAcknowledgments: This work was supported by the IT R&D Program of MSIP/IITP (B010115-0307, Machine Learning Center), National Research Foundation (NRF) of Korea (NRF2013R1A2A2A01067464), and IITP-MSRA Creative ICT/SW Research Project.\n4https://archive.ics.uci.edu/ml/datasets/Bag+of+Words\nSupplementary Material"}, {"heading": "A Key operations", "text": "We first explain two key operations used for tgMCMC.\nSampleSub(c, p): given a tree tc, SampleSub(c, p) aims to draw a subtree tc\u2032 of tc. We want tc\u2032 to have high dissimilarity between its subtrees, so we draw tc\u2032 with probability proportional to d(l(c)c\u2032, r(c)c\u2032) + , where is the maximum dissimilarity between subtrees. is added for leaf nodes, whose dissimilarity between subtrees are defined to be zero. See Figure 6.\nAs depicted in Figure 6, the probability of drawing the subtree tci is computed as pi. If we draw tci as a result, corresponding probability pi is multiplied into p; p \u2190 p \u00b7 pi. This procedure is needed to compute the forward transition probability. If SampleSub takes another argument as SampleSub(c, c\u2032, p), we don\u2019t do the actual sampling but compute the probability of drawing subtree tc\u2032 from tc and multiply the probability to p. In above example, SampleSub(c, ci, p) does p\u2190 p\u00b7pi. This operation is needed to compute the reverse transition probability.\nStocInsert(S, c, p): given a set of clusters S (and corresponding trees), StocInsert inserts tc into one of the trees tc\u2032(c\u2032 \u2208 S), or just put c in S without insertion. If tc is inserted into tc\u2032 , the insertion is done by SeqInsert(c, c\u2032). See Figure 7 for example. The set S contains three clusters, and tc may be inserted into ci \u2208 S via SeqInsert(ci, c) with probability pi, or c may just be put into S without insertion with probability p\u2217. In the original IBHC algorithm, tc is inserted into the tree ci with smallest d(c, ci), or put into S without insertion if the minimum d(c, ci) exceeds one. StocInsert is a probabilistic operation of this procedure, where the probability pi is proportional to d\u22121(c, ci). As emphasized in the paper, the trees are not split after the update of the dissimilarities of inserted trees; this is to ensure reversibility and make the computation of\ntransition probability easier. Even if some dissimilarity exceeds one after the insertion, we expect StocInsert operation to find those nodes needed to be split. After the insertion, corresponding probability is multiplied to p, as in StocInsert operation. If StocInsert takes another arguments as StocInsert(S, c, c\u2032, p), we don\u2019t do insertion but compute the probability of inserting c into c\u2032 \u2208 S and multiply it to p. In above example, StocInsert(S, c, ci, p) multiplies p \u2190 p \u00b7 pi. StocInsert(S, c,\u2205, p) multiplies the probability of creating a new tree with c, p\u2190 p \u00b7 p\u2217."}, {"heading": "B Global moves", "text": "In this section, we explain global moves for our tgMCMC.\nB.1 Selection between splitting and merging\nIn global moves, we randomly choose between splitting and merging operations. The basic principle is this; pick a tree, and find candidate trees that can be merged with the picked tree. If there is any candidate, invoke merge operation, and invoke split operation otherwise.\nAs an example, suppose that our current partition is \u03a0[n] = {ci}6i=1, with 6 clusters. We first uniformly pick a cluster among them, and initialize the forward transition probability q(\u03a0\u2217[n]|\u03a0[n]) = 1/6. Suppose that c1 is selected. Then, for {ci}5i=2, we put ci into a set M with probability (1 + d(c1, ci))\n\u22121; note that this probability is equal to p(hc1\u222aci |Xc1\u222aci), the probability of merging c1 and ci. The splitting is invoked if M contains no cluster, and merging is invoked otherwise.\nB.2 Splitting\nSuppose that M contains no cluster. The forward transition probability up to this is\nq(\u03a0\u2217[n]|\u03a0[n]) = 1\n6 6\u220f i=2 d(c1, ci) 1 + d(c1, ci) . (14)\nNow suppose that c1 looks like in Figure 8. The split proposal \u03a0\u2217[n] is then proposed according to the following procedure. First, a subtree c? of c1 is sampled via SampleSub procedure. Then, the\ntree is cut at c?, collecting S = {l(c)c?, r(c)c?1} and remaining split nodes Q = {c7, c8}. Nodes in Q are inserted into S via StocInsert. Since (l(c)c?, r(c)c?1) were initialized to be split in S, the resulting partition must have more than two clusters. During SampleSub and StocInsert, every intermediate transition probabilities are multiplied into q(\u03a0\u2217[n]|\u03a0[n]). The final partition \u03a0\u2217[n] to propose is {c(i)}6i=2 \u222a {c9, c10}.\nB.3 Merging\nSuppose that M contains three clusters; M = {c2, c3, c4}. The forward transition probability up to this is\nq(\u03a0\u2217[n]|\u03a0[n]) = 1\n6 4\u220f i=2\n1\n1 + d(c1, ci) 6\u220f i=5 d(c1, ci) 1 + d(c1, ci) . (15)\nIn partition space, there is only one way to merge c1 and M = {c2, c3, c4} into a single cluster c9 = c1 \u222a c2 \u222a c3 \u222a c4, so no transition probability is multiplied. However, there can be many trees representing c9, since we can merge the nodes in any order or topology, in terms of tree. Hence, we fix the merged tree to be a cascaded tree, merged in order of indices of nodes; see Figure 9. As we wrote in our paper, we assume that each nodes are given unique indices (such as hash values) so\nthat they can be sorted in order. In this example, the indices for c1 and c2 are 1 and 2, respectively. Then, we build a cascaded tree, merged in order c1, c2, c3, c4. The resulting merged partition is then \u03a0\u2217[n] = {c5, c6, c9}.\nB.4 Computing reverse transition probability for splitting\nNow we explain how to compute the reverse transition probability q(\u03a0[n]|\u03a0\u2217[n]) for splitting case. We start from the illustrative partition \u03a0\u2217[n] in subsection B.2. Starting from \u03a0 \u2217 [n], we must merge c9 and c10 back into c1 = c9 \u222a c10 to retrieve \u03a0[n]. For this, there are two possibilities: we first pick c9 and collect M = {c10}, or pick c10 and collect M = {c9}. Hence, the reverse transition probability is computed as\nq(\u03a0[n]|\u03a0\u2217[n]) = 1\n7 ( 5\u2211 i=1 d(c9, ci) 1 + d(c9, ci) +\n1\n1 + d(c9, c10)\n)\n+ 1\n7 ( 5\u2211 i=1 d(c10, ci) 1 + d(c10, ci) +\n1\n1 + d(c10, c9)\n) . (16)\nAs we explained in subsection B.3, no more reverse transition probabilities are needed since there is only one way to merge nodes back into c1 in partition space.\nB.5 Computing reverse transition probability for merging\nWe explain how to compute the reverse transition probability for merging case, starting from the illustrative partition in subsection B.3, \u03a0\u2217[n] = {c5, c6, c9}. See Figure 9. To retrieve \u03a0[n] = {ci}6i=1, we should split c9 into c1, c2, c3, c4. For this, we should first invoke split operation for c9; pick c9 and collect M = \u2205. The reverse transition probability up to this is\nq(\u03a0[n]|\u03a0\u2217[n]) = 1\n3\n( d(c9, c5)\n1 + d(c9, c5) +\nd(c9, c6)\n1 + d(c9, c6)\n) . (17)\nNow, we should split c9 to retrieve the original partition. The achieve this, we should first pick direct parent of c1 and c2 (c7 in Figure 9) via SampleSub. The reverse transition probability for this is computed by SampleSub(c9, c7, q(\u03a0[n]|\u03a0\u2217[n])), and we have S = {c1, c2} and Q = {c3, c4}. To get the original partition, c3 and c4 should be put into S without insertion, and the reverse transition probability for this is computed by StocInsert(S, c3,\u2205, q(\u03a0[n]|\u03a0\u2217[n])) and StocInsert(S, c4,\u2205, q(\u03a0[n]|\u03a0\u2217[n]))."}, {"heading": "C Local moves", "text": "Local moves resample cluster assignments of individual data points (leaf nodes) with Gibbs sampling. However, instead of running Gibbs sampling for full data, we select a random subset S and run Gibbs sampling for data points in S. S is constructed as follows. Given a partition \u03a0[n], for each c \u2208 \u03a0[n], we sample its subtree via SampleSub. Then we sample a subtree of drawn subtree again with SampleSub. This procedure is repeated for D times, where D is a parameter set by users. Figure 10 depicts the subsampling procedure for c \u2208 \u03a0[n] with D = 2. As a result of Figure 10, the\nleaf nodes {i, j, k} are added to S. After having collected S for all c \u2208 \u03a0[n], we run Gibbs sampling\nfor those leaf nodes in S, and if a leaf node i should be moved from c to another c\u2032, we insert i into c\u2032 with SeqInsert(c\u2032, i). We can control the tradeoff between mixing rate and speed by controlling D; higher D means more elements in S, and thus more data points are moved by Gibbs sampling."}], "references": [{"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1973}, {"title": "Hierarchical mixture modeling with normalized inverse- Gaussian priors", "author": ["A. Lijoi", "R.H. Mena", "I. Pr\u00fcnster"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Generalized Gamma measures and shot-noise Cox processes", "author": ["A. Brix"], "venue": "Advances in Applied Probability,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Controlling the reinforcement in Bayesian nonparametric mixture models", "author": ["A. Lijoi", "R.H. Mena", "I. Pr\u00fcnster"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Distriubtional results for means of normalized random measures with independent increments", "author": ["E. Regazzini", "A. Lijoi", "I. Pr\u00fcnster"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Dependent hierarchical normalized random measures for dynamic topic modeling", "author": ["C. Chen", "N. Ding", "W. Buntine"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Dependent normalized random measures", "author": ["C. Chen", "V. Rao", "W. Buntine", "Y.W. Teh"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Bayesian Poisson process partition calculus with an application to Bayesian L\u00e9vy moving averages", "author": ["L.F. James"], "venue": "The Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Posterior analysis for normalized random measures with independent increments", "author": ["L.F. James", "A. Lijoi", "I. Pr\u00fcnster"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "MCMC for normalized random measure mixture models", "author": ["S. Favaro", "Y.W. Teh"], "venue": "Statistical Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems", "author": ["C.E. Antoniak"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1974}, {"title": "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model", "author": ["S. Jain", "R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Posterior simulation of normalized random measure mixtures", "author": ["J.E. Griffin", "S.G. Walkera"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Incremental tree-based inference with dependent normalized random measures", "author": ["J. Lee", "S. Choi"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), Reykjavik,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Bayesian hierarchical clustering", "author": ["K.A. Heller", "Z. Ghahrahmani"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 3, "context": "Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 4, "context": "NRM mixture (NRMM) model [5] is a representative example where NRM is used as a prior for mixture models.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "Recently NRMs were extended to dependent NRMs (DNRMs) [6, 7] to model data where exchangeability fails.", "startOffset": 54, "endOffset": 60}, {"referenceID": 6, "context": "Recently NRMs were extended to dependent NRMs (DNRMs) [6, 7] to model data where exchangeability fails.", "startOffset": 54, "endOffset": 60}, {"referenceID": 7, "context": "The posterior analysis for NRM mixture (NRMM) models has been developed [8, 9], yielding simple MCMC methods [10].", "startOffset": 72, "endOffset": 78}, {"referenceID": 8, "context": "The posterior analysis for NRM mixture (NRMM) models has been developed [8, 9], yielding simple MCMC methods [10].", "startOffset": 72, "endOffset": 78}, {"referenceID": 9, "context": "The posterior analysis for NRM mixture (NRMM) models has been developed [8, 9], yielding simple MCMC methods [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "As in DP mixture (DPM) models [11], there are two paradigms in the MCMC algorithms for NRMM models: (1) marginal samplers and (2) slice samplers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "The marginal samplers include the Gibbs sampler [10], and the split-merge sampler [12], although it was not formally extended to NRMM models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "The marginal samplers include the Gibbs sampler [10], and the split-merge sampler [12], although it was not formally extended to NRMM models.", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "The slice sampler [13] maintains random measures and explicitly samples the weights and atoms of the random measures.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "The slice sampler is known to mix faster than the marginal Gibbs sampler when applied to complicated DNRM mixture models where the evaluation of marginal distribution is costly [7].", "startOffset": 177, "endOffset": 180}, {"referenceID": 13, "context": "Recently, a deterministic alternative to MCMC algorithms for NRM (or DNRM) mixture models were proposed [14], extending Bayesian hierarchical clustering (BHC) [15] which was developed as a tree-based inference for DP mixture models.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Recently, a deterministic alternative to MCMC algorithms for NRM (or DNRM) mixture models were proposed [14], extending Bayesian hierarchical clustering (BHC) [15] which was developed as a tree-based inference for DP mixture models.", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "The algorithm, referred to as incremental BHC (IBHC) [14] builds binary trees that reflects the hierarchical cluster structures of datasets by evaluating the approximate marginal likelihood of NRMM models, and is well suited for the incremental inferences for large-scale or streaming datasets.", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "([9]) Let (\u03a0[n], {\u03b8c|c \u2208 \u03a0[n]}) be samples drawn from \u03bc/\u03bc(\u0398) where \u03bc \u223c CRM(\u03c1H).", "startOffset": 1, "endOffset": 4}, {"referenceID": 2, "context": "The most general CRM may be used is the generalized Gamma [3], with L\u00e9vy intensity \u03c1(dw) = \u03b1\u03c3 \u0393(1\u2212\u03c3)w \u2212\u03c3\u22121e\u2212wdw.", "startOffset": 58, "endOffset": 61}, {"referenceID": 14, "context": "Bayesian hierarchical clustering (BHC, [15]) is a probabilistic model-based agglomerative clustering, where the marginal likelihood of DPM is evaluated to measure the dissimilarity between nodes.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Incremental BHC (IBHC, [14]) is an extension of BHC to (dependent) NRMM models.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "The generative probability of trees is described with the potential function [14], which is the unnormalized reformulation of the original definition [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "The generative probability of trees is described with the potential function [14], which is the unnormalized reformulation of the original definition [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 14, "context": "If the tree is built and the potential function is computed for the entire dataset X , a lower bound on the joint likelihood (7) is obtained [15, 14]: un\u22121e\u2212\u03c8\u03c1(u)du \u0393(n) \u03c6(X|t[n]) \u2264 P (dX, du).", "startOffset": 141, "endOffset": 149}, {"referenceID": 13, "context": "If the tree is built and the potential function is computed for the entire dataset X , a lower bound on the joint likelihood (7) is obtained [15, 14]: un\u22121e\u2212\u03c8\u03c1(u)du \u0393(n) \u03c6(X|t[n]) \u2264 P (dX, du).", "startOffset": 141, "endOffset": 149}, {"referenceID": 13, "context": "Among these three cases, the one with the highest potential function \u03c6(Xc\u222ai|tc\u222ai) is selected, which can easily be done by comparing d(l(c), r(c)), d(l(c), i) and d(r(c), i) [14].", "startOffset": 174, "endOffset": 178}], "year": 2015, "abstractText": "Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and realworld datasets demonstrate the benefit of our method.", "creator": "LaTeX with hyperref package"}}}