{"id": "1511.06246", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Gaussian Mixture Embeddings for Multiple Word Prototypes", "abstract": "recently, word representation has been increasingly focused on calculating its excellent competence in representing the word semantics. previous works mainly range from the problem of the phenomenon. to address this problem, most of previous models represent words as multiple functional neurons. however, it cannot reflect the perceived relations between words by reflecting words as points in the embedded space. in this paper, we propose the gaussian mixture skip - gram ( ff ) model to encompass the gaussian mixture embeddings from words based on skip - bucket framework. each word can be characterized as a gaussian mixture only in the embedded space, and each gaussian component represents a hidden sense. since the number per senses varies from word to word, we further describe the dynamic gmsg ( d - gmsg ) model by adaptively increasing the minimum number of words during training. experiments on four benchmarks show the effectiveness of our proposed model.", "histories": [["v1", "Thu, 19 Nov 2015 16:46:49 GMT  (702kb,D)", "http://arxiv.org/abs/1511.06246v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinchi chen", "xipeng qiu", "jingxiang jiang", "xuanjing huang"], "accepted": false, "id": "1511.06246"}, "pdf": {"name": "1511.06246.pdf", "metadata": {"source": "CRF", "title": "Gaussian Mixture Embeddings for Multiple Word Prototypes", "authors": ["Xinchi Chen", "Xipeng Qiu", "Jingxiang Jiang", "Xuanjing Huang"], "emails": ["xinchichen13@fudan.edu.cn", "xpqiu@fudan.edu.cn", "jxjiang14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "Introduction", "text": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b). By representing a word in the embedded space, it could address the problem of curse of dimensionality and capture syntactic and semantic properties. In the embedded space, words that have similar syntactic and semantic roles are also close with each other. Thus, distributed word representation is applied to a abundant natural language processing (NLP) tasks (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2011; 2013).\nMost of previous models map a word as a single point vector in the embedded space, which surfer from polysemy problems. Concretely, many words have different senses in different contextual surroundings. For instance, word \u201capple\u201d could mean a kind of fruit when the contextual information implies that the topic of the text is about food. At the meanwhile, the word \u201capple\u201d could represent the Apple Inc. when the context is about information technology. To\n\u2217 Corresponding author.\naddress this problem, previous works have gained great success in representing multiple word prototypes. Reisinger and Mooney (2010) constructs multiple high-dimensional vectors for each word. Huang et al. (2012) learns multiple dense embeddings for each word using global document context. Neelakantan et al. (2014) proposed multi-sense skip-gram (MSSG) to learn word sense embeddings with online word sense discrimination. Most of previous works are trying to use multiple points to represent the multiple senses of words, which lead to a drawback. It cannot reflect the rich relations between words by simply representing words as points in the embedded space. For instance, we cannot infer that the word \u201cfruit\u201d is the hypernym of the word \u201capple\u201d by representing these two words as two point vectors in the embedded space.\nIn this paper, we propose the Gaussian mixture skip-gram (GMSG) model inspired by Vilnis and McCallum (2014), who maps a word as a gaussian distribution instead of a point vector in the embedded space. GMSG model represents each word as a gaussian mixture distribution in the embedded space. Each gaussian component can be regarded as a sense of a word. Figure 1 gives an illustration of Gaussian mixture embeddings. In this way, much richer relations can be reflected via the relation of two gaussian distributions. For instance, if the word \u201cfruit\u201d has larger variance than the word \u201capple\u201d, it could show that the word \u201cfruit\u201d is the hypernym of the word \u201capple\u201d. In addition, using different distance metrics, the relations between words vary. The pair (apple, pear) might be much closer when we use Euclidean distance, while the pair (apple, fruit) might be much\nar X\niv :1\n51 1.\n06 24\n6v 1\n[ cs\n.C L\n] 1\n9 N\nov 2\n01 5\ncloser when we measure them using KL divergence. Further, since the number of senses varies from word to word, we propose the Dynamic GMSG (D-GMSG) model to handle the varying sense number. D-GMSG automatically increases the sense numbers of words during training. Experiments on four benchmarks show the effectiveness of our proposed models."}, {"heading": "Skip-Gram", "text": "In the skip-gram model (Figure 2a) (Mikolov et al. 2013c), each word w is represented as a distributed vector ew \u2208W, where W \u2208 R|V |\u2217d is the word embedding matrix for all words in word vocabulary V . d is the dimensionality of the word embeddings. Correspondingly, each context word cw also has a distributed representation vector e\u0302cw \u2208 C, where C \u2208 R|V |\u2217d is another distinguished space.\nThe skip-gram aims to maximize the probability of the co-occurrence of a word w and its context word cw, which can be formalized as:\nv(w|cw) = \u220f\nu\u2208{w} \u22c3 NEG(w) p(u|cw), (1)\nwhere function NEG(\u00b7) returns a set of negative sampling context words and p(u|cw) can be formalized as:\np(u|cw) = [\u03c3(e>w e\u0302u)]1{u=cw}[1\u2212 \u03c3(e>w e\u0302u)]1{u6=cw}, (2)\nwhere 1{\u00b7} is a indicator function and \u03c3(\u00b7) is the sigmoid function.\nGiven a training corpus D which insists of (word, context) pairs (w, cw) \u2208 D, the goal of skipgram model is to maximize the objective function:\nJ(\u03b8) = \u2211\n(w,cw)\u2208D\nlogv(w|cw), (3)\nwhere \u03b8 is the parameter set of the model. Concretely, the NEG(w) function samples negative contextual words for current word w drawn on the distribution:\nP (w) \u221d Punigram(w)sr, (4)\nwhere Punigram(w) is a unigram distribution of words and sr is a hyper-parameter."}, {"heading": "Gaussian Skip-Gram Model", "text": "Although the skip-gram model is extremely efficient and the learned word embeddings have greet properties on the syntactic and semantic roles, it cannot give the asymmetric distance between words.\nSkip-gram model defines the representation of a word as a vector, and defines the similarity of two words w1 and w2 using cosine distance of vectors ew1 and ew2 . Unlike the definition of skip-gram model, in this paper, we argue that a word can be regarded as a function. The similarity between two function f and g can be formalized as:\nsim(f, g) = \u222b x\u2208Rn f(x)g(x)dx (5)\nSpecifically, when we choose the gaussian distribution as the function,\nf(x) = N (x;\u00b5\u03b1,\u03a3\u03b1), (6) g(x) = N (x;\u00b5\u03b2 ,\u03a3\u03b2), (7)\nthe similarity between two gaussian distributions f and g can be formalized as:\nsim(f, g) = \u222b x\u2208Rn N (x;\u00b5\u03b1,\u03a3\u03b1)N (x;\u00b5\u03b2 ,\u03a3\u03b2)dx (8)\n= N (0;\u00b5\u03b1 \u2212 \u00b5\u03b1,\u03a3\u03b2 + \u03a3\u03b2), (9)\nwhere \u00b5\u03b1, \u00b5\u03b2 and \u03a3\u03b1, \u03a3\u03b2 are the mean and covariance matrix of word f and word g respectively.\nIn this way, we can use KL divergence to measure the distance of two words which are represented as distributions. Since KL divergence is not symmetric metric, Gaussian skip-gram (GSG) model (Figure 2b) (Vilnis and McCallum 2014) could measure the distance between words asymmetrically."}, {"heading": "Gaussian Mixture Skip-Gram Model", "text": "Although GSG model seems work well, it cannot handle the problem of polysemy phenomenon of words. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model (Figure 2c). GMSG regard each word as a gaussian mixture distribution. Each component of the gaussian mixture distribution of a word can be regarded as a sense of the word. The senses are automatically learned during training using the information of occurrences of words and their context. Besides handling the polysemy problem, GMSG also captures the richer relations between words. As shown in Figure 1, it is tricky to tell whose distance is smaller between word pairs (apple, fruit) and (apple, pear). On the one hand, word \u201capple\u201d seems much closer with word \u201cfruit\u201d, in a manner, since apple is a kind of fruit while \u201cpear\u201d and \u201capple\u201d are just syntagmatic relation. On the other hand, word \u201capple\u201d seems much closer with word \u201cpear\u201d, since they are in the same level of semantic granularity. Actually, in different distance metrics, the relations between words varies. The pair (apple, pear) might be much closer when we use Euclidean distance, while the pair (apple, fruit) might be much closer when we measure them using KL divergence. However, whatever the relationship between them are, their representations are learned and fixed.\nFormally, we define the distributions of two words as f and g:\nf(x) = N (x;\u03c6\u03b1, \u00b5\u03b1,\u03a3\u03b1), (10) g(x) = N (x;\u03c6\u03b2 , \u00b5\u03b2 ,\u03a3\u03b2), (11)\nwhere \u03c6\u03b1 and \u03c6\u03b2 are the parameters of multinomial distributions.\nThe similarity of two distributions can be formalized as:\nsim(f, g) = \u222b x\u2208Rn N (x;\u03c6\u03b1, \u00b5\u03b1,\u03a3\u03b1)N (x;\u03c6\u03b2 , \u00b5\u03b2 ,\u03a3\u03b2)dx\n(12)\n= \u222b x\u2208Rn [\u2211 i N (x|z = i;\u00b5\u03b1,\u03a3\u03b1)M(z = i;\u03c6\u03b1) ] \u00d7\n(13)\u2211 j N (x|z = j;\u00b5\u03b2 ,\u03a3\u03b2)M(z = i;\u03c6\u03b2)  dx (14) =\n\u222b x\u2208Rn [\u2211 i N (x;\u00b5\u03b1i,\u03a3\u03b1i)\u03c6\u03b1i) ] \u00d7 (15)\u2211\nj\nN (x;\u00b5\u03b2j ,\u03a3\u03b2j)\u03c6\u03b2j)  dx (16) =\n\u2211 i \u2211 j \u03c6\u03b1i\u03c6\u03b2j \u222b x\u2208Rn N (x;\u00b5\u03b1i,\u03a3\u03b1i)N (x;\u00b5\u03b2j ,\u03a3\u03b2j)dx\n(17) = \u2211 i \u2211 j \u03c6\u03b1i\u03c6\u03b2jN (0;\u00b5\u03b1i \u2212 \u00b5\u03b2j ,\u03a3\u03b1i + \u03a3\u03b2j), (18)\nwhereM represents multinomial distribution. Algorithm 1 shows the details, where \u03c6, \u00b5 and \u03a3 are pa-\nrameters of word representations in word space. \u03c6\u0302, \u00b5\u0302 and \u03a3\u0302 are parameters of word representations in context space."}, {"heading": "Dynamic Gaussian Mixture Skip-Gram Model", "text": "Although we could represent polysemy of words by using gaussian mixture distribution, there is still a short that\nshould be pointed out. Actually, the number of word senses varies from word to word. To dynamically increasing the numbers of gaussian components of words, we propose the Dynamic Gaussian Mixture Skip-Gram (D-GMSG) model (Figure 2d). The number of senses for a word is unknown and is automatically learned during training.\nAt the beginning, each word is assigned a random gaussian distribution. During training, a new gaussian component will be generated when the similarity of the word and its context is less than \u03b3, where \u03b3 is a hyper-parameter of D-GMSG model.\nConcretely, consider the wordw and its context c(w). The similarity of them is defined as:\ns(w, c(w)) = 1 |c(w)| \u2211 c\u2208c(w) sim(fw, fc). (19)\nFor wordw, assuming thatw already has k Gaussian components, it is represented as\nfw = N (\u00b7;\u03c6w, \u00b5w,\u03a3w), \u03c6w = {\u03c6wi}ki=1, \u00b5w = {\u00b5wi}ki=1, \u03a3w = {\u03a3wi}ki=1.\n(20)\nWhen s(w, c(w)) < \u03b3, we generate a new random gaussian component N (\u00b7;\u00b5wk+1 ,\u03a3wk+1) for word w, and fw is\nInput : Training corpus: w1, w2, . . . , wT ; Sense number: K; Dimensionality of mean: d; Context window size: N ; Max-margin: \u03ba.\nInitialize: Multinomial parameter: \u03c6, \u03c6\u0302 \u2208 R|V |\u00d7K ; Means of Gaussian mixture distribution: \u00b5, \u00b5\u0302 \u2208 R|V |\u00d7d; Covariances of Gaussian mixture distribution: \u03a3, \u03a3\u0302 \u2208 R|V |\u00d7d\u00d7d; for w = w1 \u00b7 \u00b7 \u00b7wT do\nnw \u223c {1, . . . , N} c(w) = {wt\u2212nw , . . . , wt\u22121, wt+1, . . . , wt+nw} for c in c(w) do\nfor c\u0302 in NEG(c\u0302) do l = \u03ba\u2212 sim(fw, fc) + sim(fw, fc\u0302); if l > 0 then\nAccumulate gradient for \u03c6w, \u00b5w, \u03a3w; Gradient update on \u03c6\u0302c, \u03c6\u0302c\u0302, \u00b5\u0302c, \u00b5\u0302c\u0302, \u03a3\u0302c, \u03a3\u0302c\u0302;\nend Gradient update for L2 normalization term of \u03c6\u0302c, \u03c6\u0302c\u0302, \u00b5\u0302c, \u00b5\u0302c\u0302, \u03a3\u0302c, \u03a3\u0302c\u0302;\nend Gradient update on \u03c6w, \u00b5w, \u03a3w; Gradient update for L2 normalization term of \u03c6w, \u00b5w, \u03a3w;\nend end Output: \u03c6, \u00b5 and \u03a3\nAlgorithm 1: Training algorithm of GMSG model using max-margin criterion.\nthen updated as\nf\u2217w = N (\u00b7;\u03c6\u2217w, \u00b5\u2217w,\u03a3\u2217w), \u03c6\u2217w = {(1\u2212 \u03be)\u03c6wi}ki=1 \u2295 \u03be, \u00b5\u2217w = {\u00b5wi}ki=1 \u2295 \u00b5wk+1 , \u03a3\u2217w = {\u03a3wi}ki=1 \u2295 \u03a3wk+1 ,\n(21)\nwhere mixture coefficient \u03be is a hyper-parameter and operator \u2295 is an set union operation."}, {"heading": "Relation with Skip-Gram", "text": "In this section, we would like to introduce the relation between GMSG model and prevalent skip-gram model.\nSkip-gram model (Mikolov et al. 2013c) is a well known model for learning word embeddings for its efficiency and effectiveness. Skip-gram defines the similarity of word w and its context c as:\nsim(w, c) = 1\n1 + exp(\u2212e>w e\u0302c) (22)\nWhen the covariances of all words and context words \u03a3w = \u03a3\u0302c = 1 2I are fixed, \u00b5fw = ew, \u00b5fc = e\u0302c and sense\nnumber K = 1, the similarity of GMSG model can be formalized as:\nsim(fw, fc) = N (0;\u00b5fw \u2212 \u00b5fc ,\u03a3fw + \u03a3fc) = N (0; ew \u2212 e\u0302c,\u03a3w + \u03a3\u0302c) = N (0; ew \u2212 e\u0302c, I) \u221d exp((ew \u2212 e\u0302c)>(ew \u2212 e\u0302c)).\n(23)\nThe definition of similarity of skip-gram is a function of dot product of ew and e\u0302c, while it is a function of Euclidian distance of ew and e\u0302c. In a manner, skip-gram model is a related model of GMSG model conditioned on fixed \u03a3w = \u03a3\u0302c = 1 2I, \u00b5fw = ew, \u00b5fc = e\u0302c and K = 1."}, {"heading": "Training", "text": "The similarity function of the skip-gram model is dot product of vectors, which could perform a binary classifier to tell the positive and negative (word, context) pairs. In this paper, we use a more complex similarity function, which defines a absolute value for each positive and negative pair rather than a relative relation. Thus, we minimise a different max-margin based loss function L(\u03b8) following (Joachims 2002) and (Weston, Bengio, and Usunier 2011):\nL(\u03b8) = 1\nZ \u2211 (w,cw)\u2208D \u2211 c\u2212\u2208NEG(w) l(\u03b8) + 1 2 \u03bb\u2016\u03b8\u201622,\nl(\u03b8) = max{0, \u03ba\u2212 sim(fw, fcw ) + sim(fw, fc\u2212)}, (24)\nwhere \u03b8 is the parameter set of our model and the margin \u03ba is a hyper-parameter. Z = \u2211 (w,cw)\u2208D |NEG(w)| is a normalization term. Conventionally, we add a L2 regularization term for all parameters, weighted by a hyper-parameter \u03bb.\nSince the objective function is not differentiable due to the hinge loss, we use the sub-gradient method (Duchi, Hazan, and Singer 2011). Thus, the subgradient of Eq. 24 is:\n\u2202L(\u03b8)\n\u2202\u03b8 =\n1\nZ \u2211 (w,cw)\u2208D \u2211 c\u2212\u2208NEG(w) \u2202l(\u03b8) \u2202\u03b8 + \u03bb\u03b8,\n\u2202l(\u03b8) \u2202\u03b8 = \u2212\u2202sim(fw, fcw) \u2202\u03b8 + \u2202sim(fw, fc\u2212) \u2202\u03b8 .\n(25)\nIn addition, the covariance matrices need to be kept positive definite. Following Vilnis and McCallum (2014), we use diagonal covariance matrices with a hard constraint that the eigenvalues % of the covariance matrices lie within the interval [m,M ]."}, {"heading": "Experiments", "text": "To evaluate our proposed methods, we learn the word representation using the Wikipedia corpus1. We experiment on four different benchmarks: WordSim-353, Rel-122, MC and SCWS. Only SCWS provides the contextual information.\nWordSim-353 WordSim-3532 (Finkelstein et al. 2001) consists of 353 pairs of words and their similarity scores.\n1http://mattmahoney.net/dc/enwik9.zip 2http://www.cs.technion.ac.il/\u02dcgabr/resour\nces/data/wordsim353/wordsim353.html\nSampling rate sr = 3/4 Context window size N = 5 Dimensionality of mean d = 50 Initial learning rate \u03b1 = 0.025 Margin \u03ba = 0.5 Regularization \u03bb = 10\u22128 Min count mc = 5 Number of learning iteration iter = 5 Sense Number for GMSG K = 3 Weight of new Gaussian component \u03be = 0.2 Threshold for generating a new Gaussian component \u03b3 = 0.02\nTable 1: Hyper-parameter settings.\nRel-122 Rel-1223 (Szumlanski, Gomez, and Sims 2013) contains 122 pairs of nouns and compiled them into a new set of relatedness norms.\nMC MC4 (Miller and Charles 1991) contains 30 pairs of nouns that vary from high to low semantic similarity.\nSCWS SCWS5 (Huang et al. 2012) consists of 2003 word pairs and their contextual information. Concretely, the dataset consists of 1328 noun-noun, 97 adjective-adjective, 30 noun-adjective, 9 verb-adjective, 399 verb-verb, 140 verb-noun and 241 same-word pairs."}, {"heading": "Hyper-parameters", "text": "The hyper-parameter settings are listed in the Table 1. In this paper, we evaluate our models conditioned on the dimensionality of means d = 50, and we use diagonal covariance matrices for experiments. We remove all the word with occurrences less than 5 (Min count)."}, {"heading": "Model Selection", "text": "Figure 3 shows the performances using different sense number for words. According to the results, sense number sn = 3 for GMSG model is a good trade off between efficiency and model performance."}, {"heading": "Word Similarity and Polysemy Phenomenon", "text": "To evaluate our proposed models, we experiment on four benchmarks, which can be divided into two kinds. Datasets (Sim-353, Rel-122 and MC) only provide the word pairs and their similarity scores, while SCWS dataset additionally provides the contexts of word pairs. It is natural way to tackle the polysemy problem using contextual information, which means a word in different contextual surroundings might have different senses.\nTable 2 shows the results on Sim-353, Rel-122 and MC datasets, which shows that our models have excellent performance on word similarity tasks. Table 3 shows the results on\n3http://www.cs.ucf.edu/\u02dcseansz/rel-122/ 4http://www.cs.cmu.edu/\u02dcmfaruqui/word-sim\n/EN-MC-30.txt 5http://www.socher.org/index.php/Main/Impr ovingWordRepresentationsViaGlobalContextAndM ultipleWordPrototypes"}, {"heading": "Methods WordSim-353 Rel-122 MC", "text": "SCWS dataset, which shows that our models perform well on polysemy phenomenon.\nIn this paper, we define the similarity of two words w and w\u2032 as:\nAvgSim(fw, fw\u2032) = sim(fw, fw\u2032) = \u2211 i \u2211 j \u03c6wi\u03c6w\u2032jsim(fwi, fw\u2032j), (26)\nwhere fw and fw\u2032 are the distribution representations of the corresponding words. Table 2 gives the results on the three benchmarks. The size of word representation of all the previous models are chosen to be 50 in this paper.\nTo tackle the polysemy problem, we incorporate the contextual information. In this paper, we define the similarity of two words (w, w\u2032) with their contexts (c(w), c(w\u2032)) as:\nMaxSimC(fw, fw\u2032) = sim(fwk, fw\u2032k\u2032) (27)\nwhere k = arg maxi P (i|w, c(w)) and k\u2032 = arg maxj P (j|w\u2032, c(w\u2032)). Here, P (i|w, c(w)) gives the probability of the i-th sense of the current word w will take\nconditioned on the specific contextual surrounding c(w), where P (i|w, c(w)) is defined as:\nP (i|w, c(w)) = 1\n|c(w)| \u2211\nc\u2208c(w) \u03c6wisim(fwi, fc)\u2211 j 1 |c(w)| \u2211 c\u2032\u2208c(w) \u03c6wjsim(fwj , fc\u2032) . (28)\nTable 3 gives the results on the SCWS benchmark. Figure 4 gives the distribution of sense numbers of words using logarithmic scale, which is trained by D-GMSG model. The vocabulary size is 71084 here. As shown in Figure 4, majority of words have only one sense, and the number of words decreases progressively with the increase of word sense number."}, {"heading": "Related Work", "text": "Recently, it has attracted lots of interests to learn word representation. Much previous works focus on learning word as a point vector in the embedded space. Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a\nlot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman.\nHowever, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination. These models perform word sense discrimination by clustering context of words. Liu et al. (2015) discriminates word sense by introducing latent topic model to globally cluster the words into different topics. Liu, Qiu, and Huang (2015) further extended this work to model the complicated interactions of word embedding and its corresponding topic embedding by incorporating the tensor method.\nAlmost previous works are trying to use multiple points to represent the multiple senses of words. However, it cannot reflect the rich relations between words by simply representing words as points in the embedded space. Vilnis and McCallum (2014) represented a word as a gaussian distribution. Gaussian mixture skip-gram (GMSG) model represents a word as a gaussian mixture distribution. Each sense of a word can be regarded as a gaussian component of the word. GMSG model gives different relations of words under different distance metrics, such as cosine distance, dot product, Euclidian distance KL divergence, etc."}, {"heading": "Conclusions and Further Works", "text": "In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to map a word as a density in the embedded space. A word is represented as a gaussian mixture distribution whose components can be regarded as the senses of the word. GMSG could reflect the rich relations of words when using different distance metrics. Since the number of word senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model to adaptively increase the sense numbers of words during training.\nActually, a word can be regarded as any function including gaussian mixture distribution. In the further, we would like to investigate the properties of other functions for word representations and try to figure out the nature of the word semantic."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research 3:1137\u20131155. Bengio, Y.; Schwenk, H.; Sen\u00e9cal, J.-S.; Morin, F.; and Gauvain, J.-L. 2006. Neural probabilistic language models. In", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of ICML.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12:2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "Proceedings of the 10th international conference on World Wide Web, 406\u2013 414. ACM.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E. Huang", "R. Socher", "C. Manning", "A. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 873\u2013882. Jeju Island, Korea: As-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "Proc. of SIGKDD.", "citeRegEx": "Joachims,? 2002", "shortCiteRegEx": "Joachims", "year": 2002}, {"title": "Topical word embeddings", "author": ["Y. Liu", "Z. Liu", "T.-S. Chua", "M. Sun"], "venue": "AAAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning contextsensitive word embeddings with neural tensor skip-gram model", "author": ["P. Liu", "X. Qiu", "X. Huang"], "venue": "Proceedings of IJCAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": "Language and cognitive processes 6(1):1\u201328.", "citeRegEx": "Miller and Charles,? 1991", "shortCiteRegEx": "Miller and Charles", "year": 1991}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of ICML.", "citeRegEx": "Mnih and Hinton,? 2007", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of the international workshop on artificial intelligence and statistics, 246\u2013252. Citeseer.", "citeRegEx": "Morin and Bengio,? 2005", "shortCiteRegEx": "Morin and Bengio", "year": 2005}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["J. Reisinger", "R.J. Mooney"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 109\u2013117. Association for Computational", "citeRegEx": "Reisinger and Mooney,? 2010", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the ACL conference. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A new set of norms for semantic relatedness measures", "author": ["S.R. Szumlanski", "F. Gomez", "V.K. Sims"], "venue": "ACL (2), 890\u2013895.", "citeRegEx": "Szumlanski et al\\.,? 2013", "shortCiteRegEx": "Szumlanski et al\\.", "year": 2013}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["F. Tian", "H. Dai", "J. Bian", "B. Gao", "R. Zhang", "E. Chen", "T.-Y. Liu"], "venue": "Proceedings of COLING, 151\u2013160.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, 384\u2013394. Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Word representations via gaussian embedding", "author": ["L. Vilnis", "A. McCallum"], "venue": "arXiv preprint arXiv:1412.6623.", "citeRegEx": "Vilnis and McCallum,? 2014", "shortCiteRegEx": "Vilnis and McCallum", "year": 2014}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "IJCAI, volume 11, 2764\u20132770.", "citeRegEx": "Weston et al\\.,? 2011", "shortCiteRegEx": "Weston et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 15, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 14, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 9, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 17, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 5, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 10, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 1, "context": "Thus, distributed word representation is applied to a abundant natural language processing (NLP) tasks (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2011; 2013).", "startOffset": 103, "endOffset": 179}, {"referenceID": 2, "context": "Thus, distributed word representation is applied to a abundant natural language processing (NLP) tasks (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2011; 2013).", "startOffset": 103, "endOffset": 179}, {"referenceID": 18, "context": "Thus, distributed word representation is applied to a abundant natural language processing (NLP) tasks (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2011; 2013).", "startOffset": 103, "endOffset": 179}, {"referenceID": 15, "context": "Reisinger and Mooney (2010) constructs multiple high-dimensional vectors for each word.", "startOffset": 0, "endOffset": 28}, {"referenceID": 5, "context": "Huang et al. (2012) learns multiple dense embeddings for each word using global document context.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Huang et al. (2012) learns multiple dense embeddings for each word using global document context. Neelakantan et al. (2014) proposed multi-sense skip-gram (MSSG) to learn word sense embeddings with online word sense discrimination.", "startOffset": 0, "endOffset": 124}, {"referenceID": 23, "context": "In this paper, we propose the Gaussian mixture skip-gram (GMSG) model inspired by Vilnis and McCallum (2014), who maps a word as a gaussian distribution instead of a point vector in the embedded space.", "startOffset": 82, "endOffset": 109}, {"referenceID": 12, "context": "In the skip-gram model (Figure 2a) (Mikolov et al. 2013c), each word w is represented as a distributed vector ew \u2208W, where W \u2208 R|V |\u2217d is the word embedding matrix for all words in word vocabulary V .", "startOffset": 35, "endOffset": 57}, {"referenceID": 23, "context": "Since KL divergence is not symmetric metric, Gaussian skip-gram (GSG) model (Figure 2b) (Vilnis and McCallum 2014) could measure the distance between words asymmetrically.", "startOffset": 88, "endOffset": 114}, {"referenceID": 12, "context": "Skip-gram model (Mikolov et al. 2013c) is a well known model for learning word embeddings for its efficiency and effectiveness.", "startOffset": 16, "endOffset": 38}, {"referenceID": 6, "context": "Thus, we minimise a different max-margin based loss function L(\u03b8) following (Joachims 2002) and (Weston, Bengio, and Usunier 2011):", "startOffset": 76, "endOffset": 91}, {"referenceID": 23, "context": "Following Vilnis and McCallum (2014), we use diagonal covariance matrices with a hard constraint that the eigenvalues % of the covariance matrices lie within the interval [m,M ].", "startOffset": 10, "endOffset": 37}, {"referenceID": 4, "context": "WordSim-353 WordSim-3532 (Finkelstein et al. 2001) consists of 353 pairs of words and their similarity scores.", "startOffset": 25, "endOffset": 50}, {"referenceID": 13, "context": "MC MC4 (Miller and Charles 1991) contains 30 pairs of nouns that vary from high to low semantic similarity.", "startOffset": 7, "endOffset": 32}, {"referenceID": 5, "context": "SCWS SCWS5 (Huang et al. 2012) consists of 2003 word pairs and their contextual information.", "startOffset": 11, "endOffset": 30}, {"referenceID": 16, "context": "MSSG (Neelakantan et al. 2014) model sets the same sense number for each word.", "startOffset": 5, "endOffset": 30}, {"referenceID": 23, "context": "GSG (Vilnis and McCallum 2014) has several variations.", "startOffset": 4, "endOffset": 30}, {"referenceID": 17, "context": "Pruned TFIDF (Reisinger and Mooney 2010) uses spare, high-dimensional word representations.", "startOffset": 13, "endOffset": 40}, {"referenceID": 1, "context": "C&W (Collobert and Weston 2008) is a language model.", "startOffset": 4, "endOffset": 31}, {"referenceID": 5, "context": "Huang (Huang et al. 2012) is a neural network model for learning multi-representations per word.", "startOffset": 6, "endOffset": 25}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource.", "startOffset": 0, "endOffset": 142}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation.", "startOffset": 0, "endOffset": 251}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors.", "startOffset": 0, "endOffset": 907}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context.", "startOffset": 0, "endOffset": 1003}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model.", "startOffset": 0, "endOffset": 1146}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination.", "startOffset": 0, "endOffset": 1302}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination. These models perform word sense discrimination by clustering context of words. Liu et al. (2015) discriminates word sense by introducing latent topic model to globally cluster the words into different topics.", "startOffset": 0, "endOffset": 1507}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination. These models perform word sense discrimination by clustering context of words. Liu et al. (2015) discriminates word sense by introducing latent topic model to globally cluster the words into different topics. Liu, Qiu, and Huang (2015) further extended this work to model the complicated interactions of word embedding and its corresponding topic embedding by incorporating the tensor method.", "startOffset": 0, "endOffset": 1646}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination. These models perform word sense discrimination by clustering context of words. Liu et al. (2015) discriminates word sense by introducing latent topic model to globally cluster the words into different topics. Liu, Qiu, and Huang (2015) further extended this work to model the complicated interactions of word embedding and its corresponding topic embedding by incorporating the tensor method. Almost previous works are trying to use multiple points to represent the multiple senses of words. However, it cannot reflect the rich relations between words by simply representing words as points in the embedded space. Vilnis and McCallum (2014) represented a word as a gaussian distribution.", "startOffset": 0, "endOffset": 2051}], "year": 2015, "abstractText": "Recently, word representation has been increasingly focused on for its excellent properties in representing the word semantics. Previous works mainly suffer from the problem of polysemy phenomenon. To address this problem, most of previous models represent words as multiple distributed vectors. However, it cannot reflect the rich relations between words by representing words as points in the embedded space. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to learn the Gaussian mixture embeddings for words based on skip-gram framework. Each word can be regarded as a gaussian mixture distribution in the embedded space, and each gaussian component represents a word sense. Since the number of senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense number of words during training. Experiments on four benchmarks show the effectiveness of our proposed model.", "creator": "TeX"}}}