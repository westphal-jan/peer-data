{"id": "1511.06281", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Density Modeling of Images using a Generalized Normalization Transformation", "abstract": "we introduce a parametric nonlinear transformation that is well - valued viewing different data from natural images. after a linear transformation of the data, each reaction produces normalized by a pooled activity measure, computed by exponentiating a weighted sum of sampling and exponentiated output and an additive correction. we optimize the parameters of whole transformation ( linear transform, exponents, weights, constant ) over a system of natural images, directly minimizing the negentropy of the responses. we find that the whole transformation successfully gaussianizes the data, achieving a significantly smaller mutual information between transformed components than previous trends including ica and radial gamma. the transformation performs differentiable of returns only gradually inverted, and thus induces our dynamic model integrating images. we show that samples of this model are visually similar to samples of natural image patches. we also demonstrate the use of the model as a prior density in removing additive noise. finally, we estimate that the transformation approaches be cascaded, with each layer optimized ( unsupervised ) using the harmonic gaussianization objective, to capture additional probabilistic structure.", "histories": [["v1", "Thu, 19 Nov 2015 17:52:01 GMT  (1810kb,D)", "http://arxiv.org/abs/1511.06281v1", "under review as a conference paper at ICLR 2016"], ["v2", "Thu, 7 Jan 2016 22:05:15 GMT  (1812kb,D)", "http://arxiv.org/abs/1511.06281v2", "under review as a conference paper at ICLR 2016"], ["v3", "Sun, 17 Jan 2016 03:14:40 GMT  (1830kb,D)", "http://arxiv.org/abs/1511.06281v3", "under review as a conference paper at ICLR 2016"], ["v4", "Mon, 29 Feb 2016 21:07:30 GMT  (1812kb,D)", "http://arxiv.org/abs/1511.06281v4", "published as a conference paper at ICLR 2016"]], "COMMENTS": "under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["johannes ball\\'e", "valero laparra", "eero p simoncelli"], "accepted": true, "id": "1511.06281"}, "pdf": {"name": "1511.06281.pdf", "metadata": {"source": "CRF", "title": "GENERALIZED NORMALIZATION TRANSFORMATION", "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "emails": ["johannes.balle@nyu.edu", "valero@nyu.edu", "eero.simoncelli@nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "The learning of representations for classification of complex patterns has witnessed an impressive series of successes in recent years. But these results have relied heavily on large labeled data sets, leaving open the question of whether such representations can be learned directly from observed examples, without supervision. Density estimation, in which one captures the properties of observed data by choosing a probability density that best describes them, is the mother of all unsupervised learning problems. The direct approach to this problem is to fit a density model, either drawn from a parametric family, or composed as a nonparametric superposition of kernels, to the data. An indirect alternative, which can offer access to different families of densities, and in some cases an easier optimization problem, is to seek an invertible and differentiable parametric function y = g(x;\u03b8) that best maps the data onto a fixed target density py(y). The inverse image of this target density then provides a density model for the input space.\nMany unsupervised learning methods may be interpreted in this context. As a simple example, consider principal component analysis (PCA; Jolliffe, 2002): we might fix py as a multivariate standard normal and think of PCA as either a linear whitening transformation, or as a density model px describing the data as a normal distribution with arbitrary covariance. Independent component analysis (ICA; Cardoso, 2003) can be cast in the same framework: In this case, the data x is modeled as a linear combination of independent sources. We may fix g to be linear and py = \u220f i pyi to be a product of independent marginal densities of unknown form. Alternatively, we can apply nonparametric nonlinearities to the marginals of the linearly transformed data so as to Gaussianize them (i.e., histogram equalization). For this combined ICA-marginal-Gaussianization (ICA-MG) operation, py is again standard normal, and the transformation is a composition of a linear transform\n\u2217EPS is also affiliated with the Courant Institute of Mathematical Sciences, New York; VL is also affiliated with the University of Val\u00e8ncia, Spain.\nar X\niv :1\n51 1.\n06 28\n1v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nand marginal nonlinearities. Another model that aims for the same outcome is radial Gaussianization (RG; Lyu & Simoncelli, 2009b), in which g is the composition of a linear transformation and a radial (as opposed to marginal) Gaussianizing nonlinearity. The induced density model is the family of elliptically symmetric distributions.\nThe notion of optimizing a transformation so as to achieve desired statistical properties at the output is central to theories of efficient sensory coding in neurobiology (Barlow, 1961; Ruderman, 1994; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001), and also lends itself naturally to the design of cascaded representations such as deep neural networks. Specifically, variants of ICA-MG transformations have been applied in iterative cascades to learn densities (Friedman et al., 1984; Chen & Gopinath, 2000; Laparra et al., 2011). Each stage seeks a linear transformation that produces the \u201cleast Gaussian\u201d marginal directions, and then Gaussianizes these using nonparametric scalar nonlinear transformations. In principle, this series of transformations can be shown to converge for any data density. However, the generality of these models is also their weakness: implementing the marginal nonlinearities in a non-parametric way makes the model prone to error and requires large amounts of data. In addition, since the nonlinearities operate only on marginals, convergence can be slow, requiring a lengthy sequence of transformations (i.e., a very deep network).\nTo address these shortcomings, we develop a joint transformation that is highly effective in Gaussianizing local patches of natural images. The transformation is a generalization of divisive normalization, a form of local gain control first introduced as a means of modeling nonlinear properties of cortical neurons (Heeger, 1992), in which linear responses are divided by pooled responses of their rectified neighbors. Simple forms of divisive normalization have been shown to offer improvements in recognition recognition performance of deep neural networks (Jarrett et al., 2009). Variants of divisive normalization have been found to reduce dependencies when applied to natural images or sounds and to produce approximately Gaussian responses (Ruderman, 1994; Schwartz & Simoncelli, 2001; Lyu, 2010; Malo & Laparra, 2010). But the Gaussianity of these representations has not been carefully optimized, and typical forms of normalization do not succeed in capturing all forms of dependency found in natural images (Lyu & Simoncelli, 2009a).\nIn this article, we define a generalized divisive normalization (GDN) transform that includes as special cases both ICA-MG and RG, but avoids nonparametric transformations entirely. We solve for the parameters of the transform by optimizing an unsupervised learning objective for the nonGaussianity of the transformed data. The transformation is continuous and differentiable, and we present an effective method of inverting it. We demonstrate that the resulting GDN transform provides a significantly better model for natural images than either ICA-MG or RG. Specifically, we show that GDN provides a better fit to the pairwise statistics of local filter responses, that it generates more natural samples of image patches, and that it produces better results when used as a prior for image processing problems such as denoising. Finally, we show that a two-stage cascade of GDN transformations offers additional improvements in capturing image statistics, paving the way for its use as a general tool for unsupervised learning of deep networks."}, {"heading": "2 PARAMETRIC GAUSSIANIZATION", "text": "Given a parametric family of transformations y = g(x;\u03b8), we wish to select parameters \u03b8 so as to transform the input vector x into a standard normal random vector (i.e., zero mean, identity covariance matrix). For a differentiable transformation, the input and output densities are related by:\npx(x) = \u2223\u2223\u2223\u2223\u2202g(x;\u03b8)\u2202x \u2223\u2223\u2223\u2223 py(g(x;\u03b8)), (1)\nwhere | \u00b7 | denotes the absolute value of the matrix determinant. If we fix py to be a standard normal distribution (denoted N ), the shape of px is determined solely by the transformation. Thus, g induces a density model on x, specified by the parameters \u03b8.\nWe can achieve this by minimizing the Kullback\u2013Leibler (KL) divergence between the output density and the standard normal, known as the negentropy, J . This is defined as an expected value over py , which we can rewrite as an expectation over px (see appendix):\nJ(py) = Ey ( log py(y)\u2212 logN (y) ) = Ex ( log px(x)\u2212 log \u2223\u2223\u2223\u2202g(x;\u03b8)\u2202x \u2223\u2223\u2223\u2212 logN (g(x;\u03b8))). (2)\nDifferentiating with respect to the parameter vector \u03b8 yields:\n\u2202J(py)\n\u2202\u03b8 = Ex \u2212\u2211 ij [ \u2202g(x,\u03b8) \u2202x ]\u2212T ij \u22022gi(x,\u03b8) \u2202xj\u2202\u03b8 + \u2211 i gi(x,\u03b8) \u2202gi(x,\u03b8) \u2202\u03b8  , (3) where the expectation can be evaluated by summing over data samples, allowing the model to be fit using stochastic gradient descent. It can be shown that this optimization is equivalent to maximizing the log likelihood of the induced density model.\nNote that, while optimization is feasible, measuring success in terms of the actual KL divergence in Eq. (2) is difficult in practice, as it requires evaluating the density px. Instead, we can monitor the difference in negentropy between the input and output densities:\n\u2206J \u2261 J(py)\u2212 J(px) = Ex ( 1 2 \u2225\u2225y\u2225\u22252 2 \u2212 log \u2223\u2223\u2223 \u2202y\u2202x \u2223\u2223\u2223\u2212 12\u2225\u2225x\u2225\u222522). (4) This quantity provides a measure of how much more Gaussian the data become as a result of the transformation g(x;\u03b8)."}, {"heading": "3 DIVISIVE NORMALIZATION TRANSFORMATIONS", "text": "Divisive normalization, a form of gain control in which responses are divided by pooled activity of neighbors, has become a standard model for describing the nonlinear properties of sensory neurons (Carandini & Heeger, 2012). A commonly used form for this transformation is:\nyi = \u03b3 x\u03b1i \u03b2\u03b1 + \u2211 j x \u03b1 j ,\nwhere \u03b8 = {\u03b1, \u03b2, \u03b3} are parameters. Loosely speaking, the transformation adjusts responses to lie within a desired operating range, while maintaining their relative values. A weighted form of normalization (with exponents fixed at \u03b1 = 2) was introduced in (Schwartz & Simoncelli, 2001), and shown to produce approximately Gaussian responses with greatly reduced dependencies. The weights were optimized over a collection of photographic images so as to maximize the likelihood of responses under a Gaussian model. Normalization has also been derived as an inference method for a Gaussian scale mixture (GSM) model for wavelet coefficients of natural images (Wainwright & Simoncelli, 2000). This model factorizes local groups of coefficients into a Gaussian vector and a positive-valued scalar. In a specific instance of the model, the optimal estimator for the Gaussian vector (after decorrelation) can be shown to be a modified form of divisive normalization that uses a weighted L2-norm (Lyu & Simoncelli, 2008):\nyi = xi( \u03b22 + \u2211 j \u03b3jx 2 j ) 1 2 .\nHowever, the above instances of divisive normalization have only been shown to be effective when applied to local groups of filter responses. In what follows, we introduce a more general form, with better Gaussianization capabilities that extend to to more distant responses (as well as those arising from distinct filters)."}, {"heading": "3.1 PROPOSED GENERALIZED DIVISIVE NORMALIZATION (GDN) TRANSFORM", "text": "Here, we define a nonlinear transformation suitable for Gaussianizing photographic image data. Specifically, we define a vector-valued parametric transformation as a composition of a linear transformation followed by a generalized form of divisive normalization:\ny = g(x;\u03b8) s.t. z = Hx (5)\nand yi = zi( \u03b2i + \u2211 j \u03b3ij |zj |\u03b1ij )\u03b5i . The full parameter vector \u03b8 consists of the vectors \u03b2 and \u03b5, as well as the matricesH , \u03b1, and \u03b3, for a total of 2N + 3N2 parameters (where N is the dimensionality of the input space). We refer to this transformation as generalized divisive normalization (GDN), since it provides generalizes several previously-developed models:\n\u2022 Choosing \u03b5i \u2261 1, \u03b1ii \u2261 1, and \u03b3 the identity matrix yields the classic form of the divisive normalization transformation (Carandini & Heeger, 2012), with the exponents set to 1.\n\u2022 Choosing matrix \u03b3 to be diagonal eliminates the cross terms in the normalization pool, and the model is then a particular form of ICA-MG, or the first iteration of the Gaussianization algorithms described in Chen & Gopinath (2000) or Laparra et al. (2011): a linear \u201cunmixing\u201d transform, followed by a pointwise, Gaussianizing nonlinearity.\n\u2022 Choosing \u03b1ij \u2261 2 and setting all elements of \u03b2, \u03b5, and \u03b3 equal, the transformation assumes a radial form:\ny = z( \u03b2 + \u03b3 \u2211 j z 2 j )\u03b5 = z\u2016z\u20162 g2(\u2016z\u20162) where g2(r) = r/(\u03b2 + \u03b3r2)\u03b5 is a scalar-valued transformation on the radial component of z, ensuring that the normalization operation preserves the vector direction of z. If, in addition, H is a whitening transformation such as ZCA (Bell & Sejnowski, 1997), the overall transformation is a form of RG Lyu & Simoncelli (2009b).\n\u2022 More generally, if we allow exponents \u03b1ij \u2261 p, the induced distribution is anLp-symmetric distribution, a family which has been shown to capture statistical properties of natural images (Sinz & Bethge, 2010). The corresponding transformation on the Lp-radius is given by gp(r) = r/(\u03b2 + \u03b3rp)\u03b5."}, {"heading": "3.2 WELL-DEFINEDNESS AND INVERTIBILITY", "text": "For the density function in Eq. (1) to be well defined, we require the transformation in Eq. (5) to be continuous and invertible. For the linear portion of the transformation, we need only ensure that the matrixH is non-singular. For the normalization portion, consider the partial derivatives:\n\u2202yi \u2202zk\n= \u03b4ik( \u03b2i + \u2211 j \u03b3ij |zj |\u03b1ij )\u03b5i \u2212 \u03b1ik\u03b3ik\u03b5izi|zk|\u03b1ik\u22121 sgn(zk)( \u03b2i + \u2211 j \u03b3ij |zj |\u03b1ij )\u03b5i+1 (6) To ensure continuity, we require all partial derivatives to be finite for all z \u2208 RN . More specifically, we require all exponents in Eq. (6) to be non-negative, as well as the parenthesized expression in the denominator to be positive.\nIt can be shown that the normalization part of the transformation is invertible if the Jacobian matrix containing the partial derivatives in Eq. (6) is positive definite everywhere (see appendix). In all practical cases, we observed this to be the case, but expressing this precisely as a condition on the parameters is difficult. A necessary (but generally not sufficient) condition for invertibility can be established as follows. First, note that, as the denominator is positive, each vector z is mapped to a vector y in the same orthant. The cardinal axes of z are mapped to themselves, and for this one-dimensional mapping to be continuous and invertible, it must be monotonic. Along the cardinal axes, the following bound holds:\n|yi| = |zi|( \u03b2i + \u03b3ii|zi|\u03b1ij )\u03b5i \u2264 |zi|\u03b3\u03b5iii |zi|\u03b1ii\u03b5i = \u03b3\u2212\u03b5iii |zi|1\u2212\u03b1ii\u03b5i .\nFor the magnitude of yi to grow monotonically with |zi|, the exponent 1\u2212 \u03b1ii\u03b5i must be positive. In summary, the constraints we enforce during the optimization are \u03b1ij \u2265 1, \u03b2i > 0, \u03b3ij \u2265 0, and 0 \u2264 \u03b5i \u2264 \u03b1\u22121ii . We initialize the parameters such that \u2202y \u2202z is positive definite everywhere (for example, by letting \u03b3 be diagonal, such that the Jacobian is diagonal, the transformation is separable, and the necessary constraint on the cardinal axes becomes sufficient). Suppose that during the course of optimization, the matrix should cease being positive definite. Following a continuous path, the matrix must then become singular at some point, because going from positive definite to indefinite or negative definite would require at least one of the eigenvalues to change signs. However, the optimization objective heavily penalizes singularity: The term \u2212 log |\u2202y\u2202x | in the objective (which separates into \u2212 log |\u2202y\u2202z | and \u2212 log | \u2202z \u2202x |) grows out of bounds as the determinant of the Jacobian approaches zero. Therefore, given a sensible initialization and a sufficiently small step size, it is highly unlikely that the Jacobian should cease to be positive definite during the optimization.\nFinally, we find that the GDN transformation can be efficiently inverted using a fixed point iteration:\nz (0) i = sgn(yi) ( \u03b3\u03b5iii |yi| ) 1 1\u2212\u03b1ii\u03b5i\nz (n+1) i = ( \u03b2i + \u2211 j \u03b3ij \u2223\u2223z(n)j \u2223\u2223\u03b1ij)\u03b5iyi.\nOther types of iterations based on matrix inversion have been proposed for this purpose in the literature (Malo et al., 2006; Lyu & Simoncelli, 2008). However, they can only be applied to special cases of the generalized form of normalization we introduce here."}, {"heading": "4 EXPERIMENTS", "text": "The model was optimized to capture the distribution of image data using stochastic descent of the gradient expressed in Eq. (3). We then conducted a series of experiments to assess the validity of the fitted model for natural images."}, {"heading": "4.1 JOINT DENSITY OF PAIRS OF WAVELET COEFFICIENTS", "text": "We first examined the pairwise statistics of model responses, both for our GDN model, as well as the ICA model and the RG model. First, we computed the responses of an oriented filter (specifically, we used a subband of the steerable pyramid (Simoncelli & Freeman, 1995)) for images taken from the van Hateren dataset (van Hateren & van der Schaaf, 1998) and extracted pairs of coefficients at different spatial offsets up to d = 1024. We then transformed these two-dimensional datasets using ICA, RG, and GDN. Figure 1 (modelled after figure 4 of Lyu & Simoncelli (2009b)) shows the mutual information in the transformed data (note that, in our case, mutual information is related to the negentropy by an additive constant.) For small distances, a linear ICA transformation reduces some of the dependencies in the raw data. However, for large distances, the correlation between the coefficients is negligible compared to their nonlinear dependencies, and the mutual information of the ICA-transformed data is identical to that of the raw data. An elliptically symmetric model (RG) is good for modeling the relations when the distance is small. However, the fit gets worse as the distance increases. As pointed out in (Lyu & Simoncelli, 2009b), it can even cause an increase the mutual information relative to that of the raw data, as seen in the right hand side of the plot. GDN, however, is well suited to model the dependencies at all separations, yielding a nearly constant level of mutual information.\nIn figure 2, we compare histogram estimates of the joint wavelet coefficient densities against modelfitted densities for selected spatial offsets. The GDN fits are quite accurate, particularly in the center of the plot, where the bulk of the data lie. Note that GDN is able to capture elliptically symmetric distributions just as well as distributions which are closer to being marginally independent, whereas RG and ICA each fail in one of these cases, respectively."}, {"heading": "4.2 JOINT DENSITY OVER IMAGE PATCHES", "text": "We also estimated the model parameters on vectorized image patches of 16 \u00d7 16 pixels. We used the stochastic optimization algorithm ADAM to facilitate the optimization (Kingma & Ba, 2014) and somewhat reduced the complexity of the model by enforcing \u03b1 to be constant along its columns. We also fitted versions of our model where the normalization part is constrained to marginal transformations (ICA-MG) or radial transformations (RG). For higher dimensional data, it is difficult to visualize the densities themselves, so we use several other measures to evaluate the effectiveness of the model:\nNegentropy reduction. As an overall metric of model fit, we evaluated the negentropy difference \u2206J given in (4) on the full GDN model, as well as the marginal and radial models model (ICA-MG and RG, respectively). We find that ICA-MG and RG reduce negentropy by 2.04 and 2.11 nats, respectively, whereas GDN reduces it by 2.43 nats.\nMarginal/radial distributions of transformed data. If the transformed data is multivariate standard normal, its marginals should be standard normal, as well, and the radial component should be Chi distributed with degree 256. Figure 3 shows these distributions, in comparison to those of ICAMG and RG. As expected from (Lyu & Simoncelli, 2009a), RG fails to Gaussianize the marginals,\nICA-MG\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n2e-0 9\n2 e-\n0 9\n2e08\n2e -0\n8\n1e -0\n7\n1e07\n8e07\n8e -0\n7\n6e -0\n6\n6e06\n5e05\n5e -0\n5\n0.0 003\n0. 00\n03\n0. 00\n2\n0.0 02\n0. 02\n0.1\n1\n7\nd=2\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n6e-06 6e -0 6\n6e -0\n6 6e-06\n5e-05 5e -0 5\n5e -0 5 5e-05\n0.0003 0. 00 03\n0.00030. 00 03\n0.002 0. 00 2\n0. 00 2 0.002\n0.02\n0. 1 1 7\nd=16\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n6e-06 6e -0 6\n6e -0\n6 6e-06\n5e-05 5e -0 5\n5e -0 5 5e-05\n0.0003 0. 00 03\n0. 00\n03 0.0003\n0.002 0. 00 2\n0. 00 2 0.002\n0. 02\n0.1\n1\n7\nd=1024\nRG\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n3e -1\n0\n3e -1\n0\n2e -0\n9\n2e -0\n9\n2e -0\n8\n2e -0\n8\n1e -0\n7\n1e -0\n7\n8e -0\n7\n8e -0\n7\n6e -0\n6\n6e -0\n6\n5e -0\n5\n5e -0\n5\n0. 00\n03\n0.0 00\n3\n0. 00\n2\n0. 02\n0. 1\n1 7\nd=2\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n5e -0\n5\n5e -0\n5\n0.0003\n0. 00\n03\n0. 00\n03\n0.0003\n0.0020. 02\n0. 1\n1\n7\nd=16\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n5e-05\n5e -0\n5\n5e -0\n5 5e-05\n0.0003\n0. 00\n2\n0. 02\n0. 1\n1\n7\nd=1024\nGDN\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n2e -0\n9\n2e -0\n9\n2e -0\n8\n2e -0\n8\n1e -0\n7\n1e -0\n7\n8e -0\n7\n8e -0\n7\n6e -0\n6 6e -0\n6\n5e -0\n5\n5e -0\n5\n0. 00\n03\n0. 00\n03\n0. 00\n2\n0.0 2\n0. 1 1\n7\nd=2\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n5e-05 5e -0 5\n5e -0\n5\n5e-05\n0.0003 0. 00 03\n0. 00\n03\n0.0003\n0.002 0.\n02 0. 1\n1\n7\nd=16\n1.0 0.5 0.0 0.5 1.0 1.0\n0.5\n0.0\n0.5\n1.0\n5e-05 5e -0 5\n5e -0 5 5e-05\n0.0003 0. 00 03\n0. 00\n03\n0.0003\n0. 00\n2\n0.002\n0. 02\n0. 1\n1\n7\nd=1024\nFigure 2: Contour plots of joint wavelet coefficient densities. Each row corresponds to a model arising from a different transformation (ICA-MG, RG, GDN). Each column corresponds to a pair of coefficients spatially separated by distance d (pixels). Gray: contour lines of histogram density estimate. Black: contour lines of densities induced by best-fitting transformations. As distance increases, the empirical density between the coefficients transitions from elliptical but correlated to separable. The RG density captures the former, and the ICA density captures the latter. Only the GDN density has sufficient flexibility to capture the full range of behaviors.\nand ICA-MG fails to transform the radial component into a Chi distribution. GDN comes close to achieving both goals.\nSampling. The density model induced by a transformation can also be visualized by examining samples, computed by sampling from a standard normal distribution and then passing these samples through the inverse transformation. Figure 4 compares sets of 25 image patches drawn from the GDN model, the ICA-MG model, and randomly selected from a database of images. GDN notably captures two features of natural images: First, a substantial fraction of the samples are constant or nearly so (as in the natural images, which include patches of sky or untextured surfaces). Second, in the cases with more activity, the samples contain sparse \u201corganic\u201d structures (although they are not as sparse or structured as those drawn from the natural images). In comparison, the samples from the ICA-MG model are more jumbled, and filled with random mixtures of oriented elements.\nDenoising. Another test of a probability model comes from using it as a prior in a Bayesian estimation problem. The most basic example is that of removing additive Gaussian noise. For GDN, we use the empirical Bayes solution of Miyasawa (1961); Raphan & Simoncelli (2011), which expresses the least-squares optimal solution directly as a function of the distribution of the noisy data:\nx\u0302 = x\u0303+ \u03c32\u2207 log px\u0303(x\u0303), (7) where x\u0303 is the noisy observation, px\u0303 is the density of the noisy data, \u03c32 is the noise variance, and x\u0302 is the optimal estimate. Although the GDN model was developed for modeling the distribution of\nclean image data, we use it here to estimate the distribution of the noisy image data. We find that, since the noisy density is a Gaussian-smoothed version of the original density, the model fits the data well (results not shown).\nFor comparison, we implemented two denoising methods that operate on orthogonal wavelet coefficients, one assuming a marginal model (Figueiredo & Nowak, 2001), and the other a spherically symmetric Gaussian scale mixture (GSM) model (Portilla et al., 2003). Since the GDN model is applied to 16\u00d7 16 patches of pixels and is restricted to a complete (i.e., square matrix) linear transformation, we restrict the wavelet transform employed in the other two models to be orthogonal, and to include three scales. We also report numerical scores: the Peak Signal to Noise ratio (PSNR), and Structural Similarity Index (SSIM; Wang et al., 2004). Fig. 5 shows the denoising results. Both marginal and spherical models produce results with strong artifacts resembling the basis functions of the respective linear transform. The GDN solution has artifacts that are less perceptually noticeable, while leaving a larger amount of background noise."}, {"heading": "4.3 TWO-STAGE CASCADED MODEL", "text": "In the previous section, we show that the GDN transformation works well on local patches of images. However, this cannot capture statistical dependencies across larger spatial distances (i.e., across adjacent patches). One way of achieving this is to cascade Gaussianizing transformations (Chen & Gopinath, 2000; Laparra et al., 2011). In previous implementations of such cascades, each stage of the transformation consists of a linear transformation (to rotate the previous responses, exposing additional non-Gaussian directions) and a Gaussianizing nonlinear transformation applied to the marginals. We\u2019ve implemented a cascade based on GDN that benefits from two innovations. First, by jointly Gaussianizing groups of coefficients (rather than transforming each one independently), GDN achieves a much more significant reduction in negentropy than MG (see Figure 1), thereby reducing the total number of stages that would be needed to fully Gaussianize the data. Second, we replace the ICA rotations with convolutional ICA (CICA; Ball\u00e9 & Simoncelli, 2014). This is a better solution than either partitioning the image into non-overlapping blocks \u2013 causing block artifacts \u2013\nor simply increasing the size of the transformation, which would require a much larger number of parameters as opposed to convolutional \u201cweight sharing\u201d (LeCun et al., 1990).\nThe central question that determines effectiveness of a multi-layer model based on the above ingredients is whether the parametric form of the normalization is suitable for Gaussianizing the densities arising from previous layers. According to our preliminary results, this seems to be the case. We constructed a two-stage model, consisting of the transformations CICA\u2013GDN\u2013CICA\u2013GDN. The first CICA instance implements a complete, invertible linear transformation with a set of 256 convolutional filters of support 48 \u00d7 48, with each filter response subsampled by a factor of 16 (both horizontally and vertically). The output thus consists of 256 reduced-resolution feature maps. The subsequent GDN operation then acts on the 256-vectors of responses across maps. Thus, the responses of the first CICA\u2013GDN stage are Gaussianized across maps, but not across spatial locations. The second-stage CICA instance is applied to vectors of first-stage responses across maps within a 9\u00d7 9 spatial neighborhood \u2013 thus seeking new non-Gaussian directions across spatial locations and across maps. Histogram estimates of the marginals of these directions are shown in figure 6. The distributions are qualitatively similar to those found for the first stage CICA operating on image pixels, although their heavy-tailedness is less pronounced. The figure also shows histograms of the second-stage GDN marginals, indicating that the new directions have been effectively Gaussianized."}, {"heading": "5 CONCLUSION", "text": "We\u2019ve introduced a new probability model for natural images, implicitly defined in terms of an invertible nonlinear transformation that is optimized so as to Gaussianize the data. This transformation is formed as the composition of a linear operation and a generalized form of divisive normalization, a local gain control operation commonly used to model response properties of sensory neurons. We developed an efficient algorithm for fitting the parameters of this transformation, minimizing the KL divergence of the distribution of transformed data against a Gaussian target.\nOur method arises as a natural combination of concepts drawn from two different research endeavors. The first aims to explain the architecture and functional properties of biological sensory systems as arising from principles of coding efficiency (Barlow, 1961; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001). A common theme in these studies is the idea that the hierarchical organization of the system acts to transform the raw sensory inputs into more compact, and statistically factorized, representations. Divisive normalization has been proposed as a transformation that participates in this process. The form we propose here is highly effective: the transformed data are significantly closer to Gaussian than data transformed by either marginal or radial Gaussianization procedures, and the induced density is thus a more accurate description of the data.\nThe second area of endeavor arises from the statistics literature on projection pursuit, and its use in problems of density estimation (Friedman et al., 1984). More recent examples include marginal and radial Gaussianization (Chen & Gopinath, 2000; Lyu & Simoncelli, 2009a; Laparra et al., 2011). Our preliminary experiments indicate that the fusion of a generalized variant of the normalization computation with the iterated Gaussianization architecture is feasible, both in terms of computation and statistical validity. The resulting architecture is a new type of a deep neural network that offers a promising platform for unsupervised learning of probabilistic structures from data."}, {"heading": "6 APPENDIX", "text": ""}, {"heading": "6.1 NEGENTROPY", "text": "To see that the negentropy J of the transformed data y can be written as an expectation over the original data, consider a change of variables:\nJ(py) = Ey ( log py(y)\u2212 logN (y) )\n= \u222b py(y) ( log py(y)\u2212 logN (y) ) dy\n= \u222b px(x) \u2223\u2223\u2223\u2202y \u2202x \u2223\u2223\u2223\u22121(log(px(x)\u2223\u2223\u2223\u2202y \u2202x \u2223\u2223\u2223\u22121)\u2212 logN (y))\u2223\u2223\u2223\u2202y \u2202x \u2223\u2223\u2223 dx = Ex ( log px(x)\u2212 log \u2223\u2223\u2223\u2202y \u2202x \u2223\u2223\u2223\u2212 logN (y))"}, {"heading": "6.2 INVERTIBILITY", "text": "Here, we show that a transformation g : x 7\u2192 y is invertible if it is continuous and its Jacobian g\u2032 : x 7\u2192 \u2202y\u2202x positive definite everywhere. g is invertible if and only if any two nonidentical inputs xa, xb are mapped to nonidentical outputs ya, yb, and vice versa:\n\u2200xa,xb : xa 6= xb \u21d4 ya 6= yb.\nFirst, we can write the inequality of the two right-hand side vectors as\n\u2203u : u>\u2206y 6= 0,\nwhere \u2206y is their difference. Second, we can compute \u2206y by integrating the Jacobian along a straight line Lab between xa and xb:\n\u2206y = \u222b Lab g\u2032(x) dx.\nWriting the integral as a Riemann limit, invertibility can be stated as:\nxa 6= xb \u21d4 \u2203u : u>\u2206y = lim T\u2192\u221e T\u22121\u2211 t=0 u>g\u2032 ( xa + t \u2206x T )\u2206x T 6= 0.\nIf \u2206x 6= 0 (the left-hand inequality is true) and g\u2032 is positive definite everywhere, all terms in the sum can be made positive by choosing u = \u2206xT . Hence, for g\n\u2032 positive definite, the right-hand inequality follows from the left. Since g is a function, the converse is trivially met."}, {"heading": "ACKNOWLEDGMENTS", "text": "JB and EPS were supported by the Howard Hughes Medical Institute. VL was supported by APOSTD/2014/095 Generalitat Valenciana grant (Spain)."}], "references": [{"title": "Learning sparse filterbank transforms with convolutional ICA", "author": ["Ball\u00e9", "Johannes", "Simoncelli", "Eero P"], "venue": "In 2014 IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2014}, {"title": "Possible principles underlying the transformations of sensory messages", "author": ["Barlow", "Horace B"], "venue": "In Sensory Communication,", "citeRegEx": "Barlow and B.,? \\Q1961\\E", "shortCiteRegEx": "Barlow and B.", "year": 1961}, {"title": "The independent components of natural scenes are edge filters", "author": ["Bell", "Anthony J", "Sejnowski", "Terrence J"], "venue": "Vision Research,", "citeRegEx": "Bell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bell et al\\.", "year": 1997}, {"title": "Normalization as a canonical neural computation", "author": ["Carandini", "Matteo", "Heeger", "David J"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "Carandini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Carandini et al\\.", "year": 2012}, {"title": "Dependence, correlation and Gaussianity in independent component analysis", "author": ["Cardoso", "Jean-Fran\u00e7ois"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cardoso and Jean.Fran\u00e7ois.,? \\Q2003\\E", "shortCiteRegEx": "Cardoso and Jean.Fran\u00e7ois.", "year": 2003}, {"title": "Wavelet-based image estimation: an empirical bayes approach using Jeffrey\u2019s noninformative prior", "author": ["M.A.T. Figueiredo", "R.D. Nowak"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Figueiredo and Nowak,? \\Q2001\\E", "shortCiteRegEx": "Figueiredo and Nowak", "year": 2001}, {"title": "Projection pursuit density estimation", "author": ["Friedman", "Jerome H", "Stuetzle", "Werner", "Schroeder", "Anne"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Friedman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1984}, {"title": "Normalization of cell responses in cat striate cortex", "author": ["Heeger", "David J"], "venue": "Visual Neuroscience,", "citeRegEx": "Heeger and J.,? \\Q1992\\E", "shortCiteRegEx": "Heeger and J.", "year": 1992}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Adam: A method for stochastic optimization. ArXiv eprints", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "San Diego,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Iterative Gaussianization: From ICA to random rotations", "author": ["Laparra", "Valero", "Camps-Valls", "Gustavo", "Malo", "Jes\u00fas"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Laparra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laparra et al\\.", "year": 2011}, {"title": "Handwritten zip code recognition with multilayer networks", "author": ["LeCun", "Yann", "O. Matan", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel", "H.S. Baird"], "venue": "In Proceedings, 10th International Conference on Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Divisive normalization: Justification and effectiveness as efficient coding transform", "author": ["Lyu", "Siwei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lyu and Siwei.,? \\Q2010\\E", "shortCiteRegEx": "Lyu and Siwei.", "year": 2010}, {"title": "Nonlinear image representation using divisive normalization", "author": ["Lyu", "Siwei", "Simoncelli", "Eero P"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lyu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2008}, {"title": "Modeling multiscale subbands of photographic images with fields of Gaussian scale mixtures", "author": ["Lyu", "Siwei", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lyu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2008}, {"title": "Nonlinear extraction of independent components of natural images using radial Gaussianization", "author": ["Lyu", "Siwei", "Simoncelli", "Eero P"], "venue": "Neural Computation,", "citeRegEx": "Lyu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2009}, {"title": "Psychophysically tuned divisive normalization approximately factorizes the PDF of natural images", "author": ["Malo", "Jes\u00fas", "Laparra", "Valero"], "venue": "Neural Computation,", "citeRegEx": "Malo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Malo et al\\.", "year": 2010}, {"title": "Non-linear image representation for efficient perceptual coding", "author": ["Malo", "Jes\u00fas", "I. Epifanio", "R. Navarro", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Malo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Malo et al\\.", "year": 2006}, {"title": "An empirical bayes estimator of the mean of a normal population", "author": ["K. Miyasawa"], "venue": "Bulletin de l\u2019Institut international de Statistique,", "citeRegEx": "Miyasawa,? \\Q1961\\E", "shortCiteRegEx": "Miyasawa", "year": 1961}, {"title": "Image denoising using scale mixtures of Gaussians in the wavelet domain", "author": ["Portilla", "Javier", "Strela", "Vasily", "Wainwright", "Martin J", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Portilla et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Portilla et al\\.", "year": 2003}, {"title": "Least squares estimation without priors or supervision", "author": ["Raphan", "Martin", "Simoncelli", "Eero P"], "venue": "Neural Computation,", "citeRegEx": "Raphan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Raphan et al\\.", "year": 2011}, {"title": "Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents", "author": ["F. Rieke", "D.A. Bodnar", "W. Bialek"], "venue": "Proceedings of the Royal Society of London B: Biological Sciences,", "citeRegEx": "Rieke et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Rieke et al\\.", "year": 1995}, {"title": "The statistics of natural images. Network: Computation", "author": ["Ruderman", "Daniel L"], "venue": "Neural Systems,", "citeRegEx": "Ruderman and L.,? \\Q1994\\E", "shortCiteRegEx": "Ruderman and L.", "year": 1994}, {"title": "Natural signal statistics and sensory gain control", "author": ["Schwartz", "Odelia", "Simoncelli", "Eero P"], "venue": "Nature Neuroscience,", "citeRegEx": "Schwartz et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2001}, {"title": "The steerable pyramid: A flexible architecture for multi-scale derivative computation", "author": ["Simoncelli", "Eero P", "Freeman", "William T"], "venue": "IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "Simoncelli et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Simoncelli et al\\.", "year": 1995}, {"title": "Lp-nested symmetric distributions", "author": ["Sinz", "Fabian", "Bethge", "Matthias"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sinz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sinz et al\\.", "year": 2010}, {"title": "Independent component filters of natural images compared with simple cells in primary visual cortex", "author": ["J.H. van Hateren", "A. van der Schaaf"], "venue": "Proceedings of the Royal Society of London B: Biological Sciences,", "citeRegEx": "Hateren and Schaaf,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Schaaf", "year": 1998}, {"title": "Scale mixtures of Gaussians and the statistics of natural images", "author": ["Wainwright", "Martin J", "Simoncelli", "Eero P"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wainwright et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2000}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan Conrad", "H.R. Sheikh", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 21, "context": "The notion of optimizing a transformation so as to achieve desired statistical properties at the output is central to theories of efficient sensory coding in neurobiology (Barlow, 1961; Ruderman, 1994; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001), and also lends itself naturally to the design of cascaded representations such as deep neural networks.", "startOffset": 171, "endOffset": 274}, {"referenceID": 6, "context": "Specifically, variants of ICA-MG transformations have been applied in iterative cascades to learn densities (Friedman et al., 1984; Chen & Gopinath, 2000; Laparra et al., 2011).", "startOffset": 108, "endOffset": 176}, {"referenceID": 10, "context": "Specifically, variants of ICA-MG transformations have been applied in iterative cascades to learn densities (Friedman et al., 1984; Chen & Gopinath, 2000; Laparra et al., 2011).", "startOffset": 108, "endOffset": 176}, {"referenceID": 8, "context": "Simple forms of divisive normalization have been shown to offer improvements in recognition recognition performance of deep neural networks (Jarrett et al., 2009).", "startOffset": 140, "endOffset": 162}, {"referenceID": 10, "context": "\u2022 Choosing matrix \u03b3 to be diagonal eliminates the cross terms in the normalization pool, and the model is then a particular form of ICA-MG, or the first iteration of the Gaussianization algorithms described in Chen & Gopinath (2000) or Laparra et al. (2011): a linear \u201cunmixing\u201d transform, followed by a pointwise, Gaussianizing nonlinearity.", "startOffset": 236, "endOffset": 258}, {"referenceID": 10, "context": "\u2022 Choosing matrix \u03b3 to be diagonal eliminates the cross terms in the normalization pool, and the model is then a particular form of ICA-MG, or the first iteration of the Gaussianization algorithms described in Chen & Gopinath (2000) or Laparra et al. (2011): a linear \u201cunmixing\u201d transform, followed by a pointwise, Gaussianizing nonlinearity. \u2022 Choosing \u03b1ij \u2261 2 and setting all elements of \u03b2, \u03b5, and \u03b3 equal, the transformation assumes a radial form: y = z ( \u03b2 + \u03b3 \u2211 j z 2 j )\u03b5 = z \u2016z\u20162 g2(\u2016z\u20162) where g2(r) = r/(\u03b2 + \u03b3r) is a scalar-valued transformation on the radial component of z, ensuring that the normalization operation preserves the vector direction of z. If, in addition, H is a whitening transformation such as ZCA (Bell & Sejnowski, 1997), the overall transformation is a form of RG Lyu & Simoncelli (2009b). \u2022 More generally, if we allow exponents \u03b1ij \u2261 p, the induced distribution is anLp-symmetric distribution, a family which has been shown to capture statistical properties of natural images (Sinz & Bethge, 2010).", "startOffset": 236, "endOffset": 819}, {"referenceID": 17, "context": "Other types of iterations based on matrix inversion have been proposed for this purpose in the literature (Malo et al., 2006; Lyu & Simoncelli, 2008).", "startOffset": 106, "endOffset": 149}, {"referenceID": 18, "context": "For GDN, we use the empirical Bayes solution of Miyasawa (1961); Raphan & Simoncelli (2011), which expresses the least-squares optimal solution directly as a function of the distribution of the noisy data: x\u0302 = x\u0303+ \u03c3\u2207 log px\u0303(x\u0303), (7) where x\u0303 is the noisy observation, px\u0303 is the density of the noisy data, \u03c3 is the noise variance, and x\u0302 is the optimal estimate.", "startOffset": 48, "endOffset": 64}, {"referenceID": 18, "context": "For GDN, we use the empirical Bayes solution of Miyasawa (1961); Raphan & Simoncelli (2011), which expresses the least-squares optimal solution directly as a function of the distribution of the noisy data: x\u0302 = x\u0303+ \u03c3\u2207 log px\u0303(x\u0303), (7) where x\u0303 is the noisy observation, px\u0303 is the density of the noisy data, \u03c3 is the noise variance, and x\u0302 is the optimal estimate.", "startOffset": 48, "endOffset": 92}, {"referenceID": 19, "context": "For comparison, we implemented two denoising methods that operate on orthogonal wavelet coefficients, one assuming a marginal model (Figueiredo & Nowak, 2001), and the other a spherically symmetric Gaussian scale mixture (GSM) model (Portilla et al., 2003).", "startOffset": 233, "endOffset": 256}, {"referenceID": 28, "context": "We also report numerical scores: the Peak Signal to Noise ratio (PSNR), and Structural Similarity Index (SSIM; Wang et al., 2004).", "startOffset": 104, "endOffset": 129}, {"referenceID": 10, "context": "One way of achieving this is to cascade Gaussianizing transformations (Chen & Gopinath, 2000; Laparra et al., 2011).", "startOffset": 70, "endOffset": 115}, {"referenceID": 11, "context": "or simply increasing the size of the transformation, which would require a much larger number of parameters as opposed to convolutional \u201cweight sharing\u201d (LeCun et al., 1990).", "startOffset": 153, "endOffset": 173}, {"referenceID": 21, "context": "The first aims to explain the architecture and functional properties of biological sensory systems as arising from principles of coding efficiency (Barlow, 1961; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001).", "startOffset": 147, "endOffset": 234}, {"referenceID": 6, "context": "The second area of endeavor arises from the statistics literature on projection pursuit, and its use in problems of density estimation (Friedman et al., 1984).", "startOffset": 135, "endOffset": 158}, {"referenceID": 10, "context": "More recent examples include marginal and radial Gaussianization (Chen & Gopinath, 2000; Lyu & Simoncelli, 2009a; Laparra et al., 2011).", "startOffset": 65, "endOffset": 135}], "year": 2017, "abstractText": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. After a linear transformation of the data, each component is normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and an additive constant. We optimize the parameters of this transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. We find that the optimized transformation successfully Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than previous methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We also demonstrate the use of the model as a prior density in removing additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized (unsupervised) using the same Gaussianization objective, to capture additional probabilistic structure.", "creator": "LaTeX with hyperref package"}}}