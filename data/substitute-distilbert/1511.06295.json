{"id": "1511.06295", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Policy Distillation", "abstract": "policies for complex visual tasks see been successfully learned with deep formal learning, often patented approach called deep q - networks ( dqn ), but relatively large ( competence - specific ) networks and extensive training are needed yet achieve good performance. in this work, we present a competing method called incentive distillation that tools be used to extract the policy of a reinforcement learning system and train a new network to performs at the expert level but being dramatically smaller and more efficient. furthermore, the same method can be used to consolidate multiple task - specific policies into a single policy. we demonstrate these claims using the atari domain interface show that the multi - task distilled agent outperforms the single - task teachers as richly as a jointly - instructed dqn agent.", "histories": [["v1", "Thu, 19 Nov 2015 18:38:47 GMT  (371kb,D)", "http://arxiv.org/abs/1511.06295v1", "Submitted to ICLR 2016"], ["v2", "Thu, 7 Jan 2016 18:43:03 GMT  (578kb,D)", "http://arxiv.org/abs/1511.06295v2", "Submitted to ICLR 2016"]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrei a rusu", "sergio gomez colmenarejo", "caglar gulcehre", "guillaume desjardins", "james kirkpatrick", "razvan pascanu", "volodymyr mnih", "koray kavukcuoglu", "raia hadsell"], "accepted": true, "id": "1511.06295"}, "pdf": {"name": "1511.06295.pdf", "metadata": {"source": "CRF", "title": "POLICY DISTILLATION", "authors": ["Andrei A. Rusu", "Sergio G\u00f3mez Colmenarejo", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell"], "emails": ["raia}@google.com,", "gulcehrc@iro.umontreal.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recently, advances in deep reinforcement learning have shown that policies can be encoded through end-to-end learning from reward signals, and that these pixel-to-action policies can deliver superhuman performance on many challenging tasks (Mnih et al., 2015). The deep Q-network (DQN) algorithm interacts with an environment, receiving observations and rewards throughout a lengthy training process. At each step, an agent chooses the action that maximizes its predicted cumulative reward, and a convolutional network is trained to approximate the optimal action-value function based on these observations, rewards, and actions. The DQN algorithm requires long training times to train a network with a relatively large capacity on a single task.\nIn this paper, we introduce policy distillation for transferring one or more action policies from Qnetworks to an untrained network. The method has multiple advantages: it can compress the model by as much as 15 times without incurring any degradation in performance; it can be applied to combine multiple expert policies into a single multi-task policy that can outperform the original experts; and finally it can be applied as a real-time, online learning process that continually distills the best policy to date from the Q-network to the target network, thus efficiently tracking the evolving Q-learning policy. The contribution of this work is to describe and discuss the policy distillation approach and to demonstrate results on (a) single game distillation, (b) single game distillation with highly compressed models, (c) multi-game distillation, and (d) online distillation.\nDistillation was first presented as an efficient means for supervised model compression (Bucila et al., 2006), and it has since been extended to the problem of creating a single network from an ensemble model (Hinton et al., 2014). It also shows merit as an optimization method that acts to stabilize learning over large datasets or in dynamic domains (Shalev-Shwartz, 2014). It uses supervised regression to train a target network to produce the same output distribution as the original network, often using a less peaked, or \u2018softened\u2019 target distribution. We show that distillation can also be used in the context of reinforcement learning (RL), a significant discovery that belies the commonly held belief that supervised learning cannot generalize to sequential prediction tasks (Barto and Dietterich, 2004).\nTo understand why distillation is difficult in a deep RL setting, consider the DQN algorithm. DQN trains a neural network to map observations to Q-values: estimates of the cumulative, time-\n\u2217While interning at Google DeepMind. Other affiliation: Universite\u0301 de Montre\u0301al, Montre\u0301al, Canada.\nar X\niv :1\n51 1.\n06 29\n5v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\ndiscounted reward for each possible action. Thus, unlike the supervised learning regime where the network outputs are class probabilities, in a Q-network the outputs can be blurred and nondiscriminative (where multiple actions have similar consequences) and at important times sharp and absolute (e.g., when the action choice can prevent loss of the game). Moreover, the outputs are realvalued and unbounded, tied to the scale of the rewards in the game. These factors make it necessary to adapt and reinterpret the original distillation approach for the RL setting."}, {"heading": "2 PREVIOUS WORK", "text": "This work is related to three different research areas: model compression using distillation, deep reinforcement learning, and multi-task learning. The concept of model compression through training a student network using the outputs of a teacher network was first suggested by Bucila et al. (2006), who proposed it as a means of compressing a large ensemble model into a single network. In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique. Hinton et al. (2014) introduced the term distillation and suggested raising the temperature of the softmax distribution in order to transfer knowledge about all the output classes from teacher to student network. Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015). These algorithms are not concerned with reinforcement learning or policies, nor are they addressing multi-task learning through distillation of single-task learners.\nIn reinforcement learning, Guo et al. (2014) has achieved superhuman Atari scores on some games through a process of UCT training that builds a search tree using the ATARI emulator (Kocsis and Szepesva\u0301ri, 2006) followed by transfer of the learned policy to a neural network via either regression or classification. This is related to our approach but differs importantly in that the teacher model (UCT) is a planning algorithm with access to the true state from the game emulator, thus allowing the policy to be developed as a deterministic MDP. In our approach, the teacher model is DQN, which only uses the pixel observations and rewards from the games. Moreover, we show that in a pure RL setting one has to consider different loss functions than regression or classification.\nMulti-task learning (Caruana, 1997) is often described as a method for improving generalization performance by leveraging a fairly limited number of similar tasks as a shared source of inductive bias. Typically, such tasks need to be defined on the same input support. Another review (Caruana, 1998) notes that naive multi-task learning of arbitrary tasks competing for the same resources (in our case network parameters) can very quickly lead to sub-optimal results. Although Atari games share a common input modality, their images are very diverse and do not share a common statistical basis (as opposed to natural images), making multi-task learning much more difficult. We show that model compression and distillation can alleviate such issues."}, {"heading": "3 APPROACH", "text": "Before describing policy distillation, we will first give a brief review of deep Q-learning, since DQN serves as both the baseline for performance comparisons as well as the teacher for the policy distillation. Note that the proposed method is not tied to DQN and can be applied to models trained using other RL algorithms. After the DQN summary, we will describe policy distillation for single and multiple tasks."}, {"heading": "3.1 DEEP Q-LEARNING", "text": "DQN is a state-of-the-art model-free approach to reinforcement learning using deep networks, in environments with discrete action choices, which has achieved super-human performance on a large collection of diverse Atari 2600 games (Mnih et al., 2015). In deep Q-learning, a neural network is optimized to predict the average discounted future return of each possible action given a small number of consecutive observations. The action with the highest predicted return is cho-\nsen by the agent. Thus, given an environment E whose interface at timestep i comprises actions ai \u2208 A = {1, ...,K}, observations xi \u2208 Rd, and rewards ri \u2208 R, we define a sequence st = x1, a1, x2, a2, ..., at\u22121, xt and a future return at time t with discount \u03b3: Rt = \u2211T t\u2032=t \u03b3\nt\u2032\u2212trt. The Q function gives the maximum expected return after seeing sequence s and choosing action a: Q\u2217(s, a) = max\u03c0 E[Rt|st = s, at = a, \u03c0], where \u03c0 is an action policy, a mapping from sequences to actions. In order to train a convolutional neural net to approximate Q\u2217(s, a), DQN minimizes the following loss, using samples (s, a, r, s\u2032) drawn from a replay memory:\nLi(\u03b8i) = E(s,a,r,s\u2032)\u223cU(D) [( r + \u03b3max\na\u2032 Q(s\u2032, a\u2032; \u03b8\u2212i )\u2212Q(s, a; \u03b8i)\n)2] .\nThe use of a replay memory to decorrelate samples is a critical element of DQN, as is the use of a target network, an older version of the parameters (\u03b8\u2212i ). Both mechanisms help to stabilize learning."}, {"heading": "3.2 SINGLE-GAME POLICY DISTILLATION", "text": "In the simplest case, illustrated in Figure 1(a), distillation is a method to transfer knowledge from a single teacher model T to a student model S. The distillation targets from a classification network are typically obtained by passing the weighted sums of the last network layer through a softmax function, yielding output probabilities that are often close to a one-hot distribution. In order to transfer more of the knowledge of the network, the teacher outputs can be softened by passing the network output through a relaxed (higher temperature) softmax than the one that was used for training. For a selected temperature \u03c4 , the new teacher outputs are thus given by softmax(q T\n\u03c4 ), where qT is the vector of Q-values of T . These can be learned by S using regression.\nIn the case of transferring a Q-function rather than a classifier, however, predicting Q-values of all actions given the observation is a difficult regression task. For one, the scale of the Q-values may be hard to learn because it is not bounded and can be quite unstable. Further, it is computationally challenging in general to compute the action values of a fixed policy because it implies solving the Q-value evaluation problem. On the other hand, training S to predict only the single best action is also problematic, since there may be multiple actions with similar Q-values.\nTo test this intuition we consider three methods of policy distillation from T to S. In all cases we assume that the teacher T has been used to generate a dataset DT = {(si,qi)}Ni=0, where each sample consists of a short observation sequence si and a vector qi of unnormalized Q-values with one value per action. The first method uses only the highest valued action from the teacher, ai,best = max(qi), and the student model is trained with a negative log likelihood loss (NLL) to predict the same action:\nLNLL(DT , \u03b8S) = \u2212 |D|\u2211 i=1 logP (ai = ai,best|xi, \u03b8S)\nIn the second case, we train using a mean-squared-error loss (MSE). The advantage of this objective is that it preserves the full set of action-values in the resulting student model. In this loss, qT and qS are the vectors of Q-values from the teacher and student networks respectively.\nLMSE(DT , \u03b8S) = |D|\u2211 i=1 ||qTi \u2212 qSi ||22.\nIn the third case, we adopt the distillation setup of Hinton et al. (2014) and use the Kullback-Leibler divergence (KL) with temperature \u03c4 :\nLKL(DT , \u03b8S) = |D|\u2211 i=1 softmax( qTi \u03c4 ) ln softmax( qTi \u03c4 ) softmax(qSi )\nIn the traditional classification setting, the output distribution of qT is very peaked, so softening the distribution by raising the temperature of the softmax allows more of the secondary knowledge to\nbe transferred to the student. In the case of policy distillation, however, the outputs of the teacher are not a distribution, rather they are the expected future discounted reward of each possible action. Rather than soften these targets, we expect that we may need to make them sharper."}, {"heading": "3.3 MULTI-TASK POLICY DISTILLATION", "text": "The approach for multi-task policy distillation, illustrated in Figure 1(b), is straightforward. We use n DQN single-game experts, each trained separately. These agents produce inputs and targets, just as with single-game distillation, and the data is stored in separate memory buffers. The distillation agent then learns from the n data stores sequentially, switching to a different one every episode. Since different tasks often have different action sets, a separate output layer (called the controller layer) is trained for each task and the id of the task is used to switch to the correct output during both training and evaluation. We also experiment with both the KL and NLL distillation loss functions for multi-task learning.\nAn important contribution of this work is to compare the performance of multi-task DQN agents with multi-task distillation agents. For multi-task DQN, the approach is similar to single-game learning: the network is optimized to predict the average discounted return of each possible action given a small number of consecutive observations. Separate replay memory buffers are maintained for each task, and training is evenly interleaved between all tasks. The game label is used to switch between different output layers as in multi-task DQN, thus enabling a different output layer, or controller, for each game. With this architecture in place, the multi-task DQN loss function remains identical to single-task learning.\nEven with the separate controllers, multi-game DQN learning is extremely challenging for Atari games and DQN generally fails to reach full single-game performance on the games. We believe this is due to interference between the different policies, different reward scaling, and the inherent instability of learning value functions.\nPolicy distillation may offer a means of combining multiple policies into a single network without the damaging interference and scaling problems. Since policies are compressed and refined during the distillation process, we surmise that they may also be more effectively combined into a single network. Also, policies are inherently lower variance than value functions, which should help performance and stability (Greensmith et al., 2004)."}, {"heading": "4 RESULTS AND DISCUSSION", "text": "A brief overview of the training and evaluation setup is given below; complete details are in Appendix A."}, {"heading": "4.1 TRAINING AND EVALUATION", "text": "Single task policy distillation is a process of data generation by the teacher network (a trained DQN agent) and supervised training by the student network, as illustrated in Figure 1(a). For each game we trained a separate DQN agent, as reported in Mnih et al. (2015). Each agent was subsequently fixed (no Q-learning) and used as a teacher for a single student policy network. The DQN teacher\u2019s outputs (Q-values for all actions) alongside the inputs (images) were held in a buffer. We employed a similar training procedure for multi-task policy distillation, as shown in Figure 1(b).\nThe network used to train the DQN agents is described in (Mnih et al., 2015). The same network was used for the student, except for the compression experiments which scaled down the number of units in each layer. A larger network (four times more parameters, with an additional fully connected layer) was used to train on multi-task distillation with 10 games. The multi-task networks had a separate MLP output (controller) layer for each task. See AppendixA for full details of training procedure and networks.\nBecause many Atari games are highly deterministic, a learner could potentially memorize and reproduce action sequences from a few starting points. To rigorously test the generalization capability of both DQN teachers and distilled agents we followed the evaluation techniques introduced by Nair et al. (2015) and adopted by subsequent research (van Hasselt et al.), in which professional human expert play was used to generate starting states for each game. Any points accumulated by the human expert until that time were discarded, thus the agent scores are not directly comparable to null-op evaluations previously reported. Since the agent is not in control of the distribution over starting states, nor do we generate any training data using human trajectories, we assert that high scores imply good levels of generalization.\nTen popular Atari games were selected and fixed before starting this research. These particular games were chosen in order to sample the diverse levels of DQN performance seen on the full collection, from super-human play (e.g. Breakout, Space Invaders) to below human level (Q*bert, Ms.Pacman)."}, {"heading": "4.2 SINGLE-GAME POLICY DISTILLATION RESULTS", "text": "In this section we show that the Kullback-Leibler (KL) cost function leads to the best-performing student agents, and that these distilled agents outperform their DQN teachers on most games. Table 1 compares the effectiveness of different policy distillation cost functions in terms of generalization performance on four Atari games, while keeping the same network architecture as DQN. Only four games were used for this experiment in order to establish parameters for the loss functions which are then fixed across other experiments (which use ten games). Note that the evaluation uses human starting points to robustly test generalization (see Section 4.1).\nStudents trained with a MSE loss performed worse than KL or NLL, even though we are successfully minimizing the squared error. This is not surprising considering that greedy action choices can be made based on very small differences in Q-values, which receive low weight in the MSE cost. Such situations are not uncommon in Atari games. Mean discounted future returns are very similar in a large number of states when coupled with control at a fine temporal resolution or very sparse rewards. This is an intrinsic property of Q-functions, which, coupled with residual errors of nonlinear function approximation during DQN training, make MSE a poor choice of loss function for policy distillation.\nAt the other end of the spectrum, using the NLL loss assumes that a single action choice is correct at any point in time, which is not wrong in principle, since any optimal policy is always deterministic if rewards are not stochastic. However, without an optimal teacher, minimizing the NLL could amplify the noise inherent in the teacher\u2019s learning process.\nPassing Q-values through a softmax function with a temperature parameter and minimizing the KL divergence cost strikes a convenient balance between these two extremes. We determine that a low temperature \u03c4 = 0.01 is best suited for transferring knowledge from teachers to students. Given the\nperformance of the KL loss, we did not experiment with other possibilities, such as combining the NLL and MSE criteria."}, {"heading": "4.3 POLICY DISTILLATION WITH MODEL COMPRESSION", "text": "In the single game setting, we also explore model compression through distillation. DQN networks are relatively large, in part due to optimization problems such as local minima that are alleviated by overcomplete models. It is also due to Q-learning, which comprises many consecutive steps of value iteration and policy improvement, thus requiring that the same deep network must represent a whole sequence of different policies before convergence. In practice DQN benefits considerably from increased network capacity, but it is likely that the final policy does not require all, or indeed, most of this capacity.\nWe evaluate single-game distilled agents and DQN teachers using 10 different Atari games, using student networks that were significantly smaller (25%, 7%, and 4% of the DQN network parameters). The distilled agents which are four times smaller than DQN (Dist-KL-net1, 428,000 parameters) actually outperform DQN, as shown in Figure 2. Distilled agents with 15 times fewer parameters perform on par with their DQN teachers. Even the smallest distilled agent (Dist-KLnet3, 62,000 parameters) achieves a mean of 84%. Details of the networks are given in Appendix A.\nThese results suggest that DQN could benefit from a reduced capacity model or regularization. However, it has been found that training a smaller DQN agent results in considerably lower performance across games (Mnih et al., 2013). We speculate that training a larger network accelerates the policy iteration cycle (Sutton and Barto, 1998) of DQN. Intuitively, once DQN performs actions resulting in a high empirical return, it is essential that the values of the novel trajectory are quickly estimated. A learner with limited capacity can be very inefficient at exploiting such potential, because high returns are often present in a minor fraction of its interactions with the environment. Hence, strong regularization could hinder the discovery of better policies with DQN."}, {"heading": "4.4 MULTI-GAME POLICY DISTILLATION RESULTS", "text": "We train a multi-task DQN agent using the standard DQN algorithm applied to interleaved experience from three games (Multi-DQN), and compare it against distilled agents (Multi-Dist) which were trained using targets from three different single-game DQN teachers (see Figure 3). All three agents are using an identical multi-controller architecture of comparable size to a single teacher network. About 90% of parameters are shared, with only 3 small MLP \u201ccontrollers\u201d on top which are task specific and allow for different action sets between different games.\nThe multi-task DQN agent learns the three tasks to 83.5% of single-task DQN performance (see Figure 3). In contrast, both distilled agents perform better than their DQN teachers, with mean scores of 105.1% for Multi- Dist-NLL and 116.9% for Multi-Dist-KL. There is a ceiling effect\non Freeway and Pong, since the single-task DQN teachers are virtually optimal, but we do see a considerable improvement on Q*bert, with as much as 50% higher scores for Multi-Dist-KL.\nWe use the same approach to distill 10 Atari games into a single student network that is four times larger than a single DQN. As can be seen from Table 2, this is quite successful, with three of the games achieving much higher scores than the teacher and an overall relative performance of 89.3%. We don\u2019t offer a comparison to a jointly trained, 10 game DQN agent, as was done for the three game set, because in our preliminary experiments DQN failed to reach higher-than-chance performance on most of the games. This highlights the challenges of multi-task reinforcement learning and supports our findings on the three game set (Figure 3).\nA visual interpretation of the representation learned by the multi-task distillation is given in Figure 4, where t-SNE embeddings of network activations from 10 different games are plotted with distinct colors. The embedding on the left suggests that statistics of low level representations (layer 1) are markedly game-specific. The second embedding characterizes shared representations at the final network layer. While activations are still game-specific, we observe higher within-game variance of representations, which probably reflect output statistics."}, {"heading": "4.5 ONLINE POLICY DISTILLATION RESULTS", "text": "As a final contribution, we investigated online policy distillation, where the student must track the DQN teacher during Q-learning. It was not obvious whether this effort would be successful, since it has been observed that the DQN policy changes dramatically throughout training as different parts of the game are explored and mastered. To test this, DQN was trained normally and the network was periodically saved if it attained a new high score. This network was then used by the distillation learner until updated by a higher DQN score. The results of this experiment are shown in Figure 5. The learning curves show the high-variance DQN score, the best-so-far score, and the score reached\nQbert\nby two distillation agents initialized with different seeds. The distilled agent is much more stable than the DQN teacher and achieves similar or equal performance on all games (see Appendix C for additional examples of online distillation)."}, {"heading": "5 DISCUSSION", "text": "We have proposed an approach for policy distillation that can be used to extract and transfer action policies from deep Q-networks, yielding significant compression and generalization advantages over the original DQN training. This is of interest both theoretically and practically, since very small models can be deployed, reused, or combined in various ways. One possible application of policy distillation is to build agents that are capable of massive multi-task performance. Multi-task learning is difficult for DQN, but distillation provides a means of transferring and merging policies online while the dedicated single- task DQN agents continue to learn. We have demonstrated and tested the building blocks of such a process: distillation to much smaller networks, multi-task distillation, and online distillation during Q-learning. Another future area of research is in designing RL optimization techniques that use distillation to stabilize and accelerate learning."}, {"heading": "A EXPERIMENTAL DETAILS", "text": "Policy Distillation Training Procedure Online data collection during policy distillation was performed under similar conditions to agent evaluation in Mnih et al. (2015). The DQN agent plays a random number of null-ops (up to 30) to initialize the episode, then acts greedily with respect to its Q-function, except for 5% of actions, which are chosen uniformly at random. Episodes can last up to 30 minutes of real-time play, or 108,000 frames. The small percentage of random actions leads to diverse game trajectories, which improves coverage of a game\u2019s state space.\nWe recorded the DQN teacher\u2019s outputs (Q-values for all valid actions) and inputs (emulator frames) into a replay memory with a capacity of 10 hours of real-time gameplay (540,000 control steps at 15Hz). At the end of each new hour of teacher gameplay added to the replay memory we performed 10,000 minibatch updates on the student network. We used the RmsProp (Tieleman and Hinton, 2012) variation of minibatch stochastic gradient descent to train student networks. Results were robust for primary learning rates between 1.0e\u22124 and 1.0e\u22123, with maximum learning rates between 1.0e\u22123 and 1.0e\u22121. We chose hyper-parameters using preliminary experiments on 4 games. The reported results consumed 500 hours of teacher gameplay to train each student. Using modern GPUs we can refresh the replay memory and train the students much faster than real-time, with typical convergence in a few days. With multi-task students we used separate replay memories for each game, with the same capacity of 10 hours, and the respective DQN teachers took turns adding data. After one hour of gameplay the student is trained with 10,000 minibatch updates (each minibatch is drawn from a randomly chosen single game memory). The same 500 hour budget of gameplay was used for all but the largest network, which used 34,000 hours of experience over 10 games.\nDistillation Targets Using DQN outputs we have defined three types of training targets that correspond to the three distillation loss functions discussed in Section3. First, the teacher\u2019s Q-values for all actions were used directly as supervised targets; thus, training the student consisted of minimizing the mean squared error (MSE) between the student\u2019s and teacher\u2019s outputs for each input state. Second, we used only the teacher\u2019s highest valued action as a one-hot target. Naturally, we minimized the negative log likelihood (NLL) loss. Finally, we passed Q-values through a softmax function with a small temperature (\u03c4 = 0.01). The resulting probabilities were used as targets by minimizing the Kullback-Leibler (KL) divergence between these \u201csharpened\u201d action probabilities and the corresponding output distribution predicted by the student policy. We went on to use the KL cost function for a majority of reported experiments, with a fixed hyper-parameter value. This choice was based on experiments described in subsection 4.2 which were performed on 4 out of the 10 games considered.\nNetwork Architectures Details of the architectures used by DQN and single-task distilled agents are given in table A1. Rectifier non-linearities were added between each two consecutive layers. We used one unit for each valid action in the output layer, which was linear. A final softmax operation was performed when distilling with NLL and KL loss functions.\nTable A1: Network architectures and parameter counts of models used for single-task compression experiments.\nAgent Input Conv. 1 Conv. 2 Conv. 3 F.C. 1 Output Parameters\nTeacher (DQN) 4 32 64 64 512 up to 18 1,693,362\nDist-KL-net1 4 16 32 32 256 up to 18 427,874\nDist-KL-net2 4 16 16 16 128 up to 18 113,346\nDist-KL-net3 4 16 16 16 64 up to 18 61,954\nFor compression experiments we scaled down the number of units in each layer without changing the basic architecture. The vast majority of saved parameters were in the fully connected layer on top of the convolutional stack.\nThe distinct characteristic of all multi-task experiments was the use of different MLP \u201ccontroller\u201d networks for each game, on top of shared representations. The specific details of these architectures are given in table A2. All results reported on 3 games used identical models of similar size with a single DQN teacher. A network 4 times larger than a teacher was trained using multi-task distillation on 10 games.\nTable A2: Network architectures and parameter counts of models used for multi-task distillation experiments.\nAgent Input Conv. 1 Conv. 2 Conv. 3 F.C. 1 F.C. 2 Output Parameters\nOne Teacher (DQN) 4 32 64 64 512 n/a up to 18 1,693,362\nMulti-DQN/Dist (3 games) 4 32 64 64 512 128 (x3) up to 18 (x3) 1,882,668\nMulti-Dist-KL (10 games) 4 64 64 64 1500 128 (x10) up to 18 (x10) 6,756,721\nAgent Evaluation Professional human expert play was used to generate starting states for each game by sampling 100 random positions which occurred in the first 20% of each episode\u2019s length. Agents are allowed to act for 30 minutes of real-time gameplay, or 108,000 frames, and they use a high value of equal to 5%. We also do not compute a generalization score until the agent\u2019s training process has ended.\nEvaluating the performance of a multi-task agent is not trivial. Since each game has a different reward structure and somewhat arbitrary choice of reward scale, it is meaningless to compute an arithmetic mean of scores across games. Therefore, DQN generalization scores (published previously (van Hasselt et al.; Nair et al., 2015)) are taken as a reference point and student scores are reported as a relative percentage. This way, performance on multiple games can be measured using the geometric mean (Fleming and Wallace, 1986)."}, {"heading": "B SUPPORTING TABLES FOR POLICY DISTILLATION FIGURES", "text": "Table B1: Performance of single-task compressed networks on 10 Atari games. Best relative scores are outlined in bold.\nDQN Dist-KL-net1 Dist-KL-net2 Dist-KL-net3\nscore score % DQN score % DQN score % DQN Beamrider 8672.4 7552.8 87.1 7393.3 85.3 6521.2 75.2 Breakout 303.9 321.0 105.6 298.2 98.1 238.8 78.6 Enduro 475.6 677.9 142.5 672.2 141.3 556.7 117.1 Freeway 25.8 26.7 103.5 26.7 103.5 26.7 103.5 Ms.Pacman 763.5 782.5 102.5 659.9 86.4 734.3 96.2 Pong 16.2 16.3 100.6 16.8 103.7 15.7 96.9 Q*bert 4589.8 5947.3 129.6 5994.0 130.6 4952.3 107.9 Riverraid 4065.3 4442.7 109.3 4175.3 102.7 3417.9 84.1 Seaquest 2793.3 3986.6 142.7 4567.1 163.5 3838.3 137.4 Space Invaders 1449.7 1140.0 78.6 716.1 49.4 302.3 20.9 Geometric Mean 108.3 101.7 83.9\nTable B2: Performance of multi-task distilled agents on 3 Atari games. Best relative scores are outlined in bold.\nDQN Multi-DQN Multi-Dist-NLL Multi-Dist-KL\nscore score % DQN score % DQN score % DQN Freeway 25.8 23.3 90.3 26.5 102.7 26.3 102.0 Pong 16.2 12.0 74.1 14.8 91.4 16.9 104.4 Q*bert 4589.8 3987.3 86.9 5678.0 123.7 6890.3 150.1 Geometric Mean 83.5 105.1 116.9"}, {"heading": "C ADDITIONAL RESULTS USING ONLINE POLICY DISTILLATION", "text": "0.0 0.2 0.4 0.6 0.8 1.0\nsteps 1e8\n20\n10\n0\n10\n20\ng a m\ne s\nco re\nDQN Best-DQN Online-Dist-seed=1 Online-Dist-seed=2\nPong\n0.0 0.2 0.4 0.6 0.8 1.0\nsteps 1e8\n0\n50\n100\n150\n200\n250\n300\n350\ng a m\ne s\nco re\nDQN Best-DQN Online-Dist-seed=1 Online-Dist-seed=2\nBreakout\nFigure C1: Online Policy Distillation during DQN learning (pale blue). The current best DQN policy to date (green) is distilled into a new network during DQN training. Showing online distillation experiments on 2 games, with 2 initial random seeds and the same learning rate across all runs."}], "references": [{"title": "A dozen tricks with multitask learning", "author": ["Rich Caruana"], "venue": null, "citeRegEx": "Caruana.,? \\Q1998\\E", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Transferring knowledge from a rnn to a dnn", "author": ["William Chan", "Nan Rosemary Ke", "Ian Lane"], "venue": "arXiv preprint arXiv:1504.01483,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "How not to lie with statistics: The correct way to summarize benchmark results", "author": ["Philip J. Fleming", "John J. Wallace"], "venue": "Commun. ACM,", "citeRegEx": "Fleming and Wallace.,? \\Q1986\\E", "shortCiteRegEx": "Fleming and Wallace.", "year": 1986}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L. Bartlett", "Jonathan Baxter"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Deep learning for realtime atari game play using offline monte-carlo tree search planning", "author": ["Xiaoxiao Guo", "Satinder P. Singh", "Honglak Lee", "Richard L. Lewis", "Xiaoshi Wang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "Deep Learning and Representation Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In Machine Learning: ECML", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "Learning small-size dnn with output-distribution-based criteria", "author": ["Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong"], "venue": "In Proc. Interspeech,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Structure compilation: trading structure for features", "author": ["Percy Liang", "Hal Daum III", "Dan Klein"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Liang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Improving supervised learning by adapting the problem to the learner", "author": ["Joshua Menke", "Tony Martinez"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Menke and Martinez.,? \\Q2009\\E", "shortCiteRegEx": "Menke and Martinez.", "year": 2009}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller"], "venue": "Deep Learning Workshop,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": "CoRR, abs/1507.04296,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "SelfieBoost: A Boosting Algorithm for Deep Learning", "author": ["S. Shalev-Shwartz"], "venue": "ArXiv e-prints,", "citeRegEx": "Shalev.Shwartz.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2014}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Knowledge transfer pre-training", "author": ["Zhiyuan Tang", "Dong Wang", "Yiqiao Pan", "Zhiyong Zhang"], "venue": "arXiv preprint arXiv:1506.02256,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Recurrent neural network training with dark knowledge transfer", "author": ["Dong Wang", "Chao Liu", "Zhiyuan Tang", "Zhiyong Zhang", "Mengyuan Zhao"], "venue": "arXiv preprint arXiv:1505.04630,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Recently, advances in deep reinforcement learning have shown that policies can be encoded through end-to-end learning from reward signals, and that these pixel-to-action policies can deliver superhuman performance on many challenging tasks (Mnih et al., 2015).", "startOffset": 240, "endOffset": 259}, {"referenceID": 5, "context": ", 2006), and it has since been extended to the problem of creating a single network from an ensemble model (Hinton et al., 2014).", "startOffset": 107, "endOffset": 128}, {"referenceID": 14, "context": "It also shows merit as an optimization method that acts to stabilize learning over large datasets or in dynamic domains (Shalev-Shwartz, 2014).", "startOffset": 120, "endOffset": 142}, {"referenceID": 7, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 13, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 1, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 19, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 16, "context": "Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015).", "startOffset": 192, "endOffset": 287}, {"referenceID": 6, "context": "(2014) has achieved superhuman Atari scores on some games through a process of UCT training that builds a search tree using the ATARI emulator (Kocsis and Szepesv\u00e1ri, 2006) followed by transfer of the learned policy to a neural network via either regression or classification.", "startOffset": 143, "endOffset": 172}, {"referenceID": 0, "context": "Another review (Caruana, 1998) notes that naive multi-task learning of arbitrary tasks competing for the same resources (in our case network parameters) can very quickly lead to sub-optimal results.", "startOffset": 15, "endOffset": 30}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network.", "startOffset": 37, "endOffset": 52}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique.", "startOffset": 37, "endOffset": 220}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique.", "startOffset": 37, "endOffset": 373}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique. Hinton et al. (2014) introduced the term distillation and suggested raising the temperature of the softmax distribution in order to transfer knowledge about all the output classes from teacher to student network.", "startOffset": 37, "endOffset": 445}, {"referenceID": 0, "context": "In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic regression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique. Hinton et al. (2014) introduced the term distillation and suggested raising the temperature of the softmax distribution in order to transfer knowledge about all the output classes from teacher to student network. Distillation has since been applied in various ways: from larger to smaller networks, from smaller to larger networks, from feedforward to recurrent networks, and from wide to narrow networks (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015). These algorithms are not concerned with reinforcement learning or policies, nor are they addressing multi-task learning through distillation of single-task learners. In reinforcement learning, Guo et al. (2014) has achieved superhuman Atari scores on some games through a process of UCT training that builds a search tree using the ATARI emulator (Kocsis and Szepesv\u00e1ri, 2006) followed by transfer of the learned policy to a neural network via either regression or classification.", "startOffset": 37, "endOffset": 1137}, {"referenceID": 11, "context": "DQN is a state-of-the-art model-free approach to reinforcement learning using deep networks, in environments with discrete action choices, which has achieved super-human performance on a large collection of diverse Atari 2600 games (Mnih et al., 2015).", "startOffset": 232, "endOffset": 251}, {"referenceID": 5, "context": "In the third case, we adopt the distillation setup of Hinton et al. (2014) and use the Kullback-Leibler divergence (KL) with temperature \u03c4 :", "startOffset": 54, "endOffset": 75}, {"referenceID": 3, "context": "Also, policies are inherently lower variance than value functions, which should help performance and stability (Greensmith et al., 2004).", "startOffset": 111, "endOffset": 136}, {"referenceID": 10, "context": "For each game we trained a separate DQN agent, as reported in Mnih et al. (2015). Each agent was subsequently fixed (no Q-learning) and used as a teacher for a single student policy network.", "startOffset": 62, "endOffset": 81}, {"referenceID": 11, "context": "The network used to train the DQN agents is described in (Mnih et al., 2015).", "startOffset": 57, "endOffset": 76}, {"referenceID": 10, "context": "The network used to train the DQN agents is described in (Mnih et al., 2015). The same network was used for the student, except for the compression experiments which scaled down the number of units in each layer. A larger network (four times more parameters, with an additional fully connected layer) was used to train on multi-task distillation with 10 games. The multi-task networks had a separate MLP output (controller) layer for each task. See AppendixA for full details of training procedure and networks. Because many Atari games are highly deterministic, a learner could potentially memorize and reproduce action sequences from a few starting points. To rigorously test the generalization capability of both DQN teachers and distilled agents we followed the evaluation techniques introduced by Nair et al. (2015) and adopted by subsequent research (van Hasselt et al.", "startOffset": 58, "endOffset": 821}, {"referenceID": 10, "context": "However, it has been found that training a smaller DQN agent results in considerably lower performance across games (Mnih et al., 2013).", "startOffset": 116, "endOffset": 135}, {"referenceID": 15, "context": "We speculate that training a larger network accelerates the policy iteration cycle (Sutton and Barto, 1998) of DQN.", "startOffset": 83, "endOffset": 107}], "year": 2015, "abstractText": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.", "creator": "LaTeX with hyperref package"}}}