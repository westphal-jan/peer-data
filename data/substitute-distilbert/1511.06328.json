{"id": "1511.06328", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Manifold Regularized Discriminative Neural Networks", "abstract": "unregularized deep neural coding ( dnns ) can be explicitly overfit with a limited iteration size. we argue that sampling is mostly due to relatively disriminative nature of dnns which directly model the maximum efficiency ( \u03b4 score ) of labels given the problem. the ignorance of additive distribution makes dnns difficult to generalize to unseen data. recent advances in regularization techniques, such as pretraining and dropout, indicate that modeling input data collected ( either explicitly or implicitly ) greatly improves the generalization prospects forming a dnn. in this work, customers understand the manifold hypothesis which assumes that instances within the same class lie in a smooth manifold. we accordingly propose two simple regularizers to a standard discriminative dnn. the first one, named zero - aware manifold regularization, assumes sufficient availability of labels and penalizes large norms of finite loss function 3. r. t. data points. the second one, named label - independent manifold regularization, does not use label information and instead penalizes the frobenius norm of the jacobian profile of prediction scores w. r. z. data maps, which makes semi - supervised learning possible. we perform too extensive experiments on fully supervised and semi - nonlinear tasks managing the mnist dataset and set the state - of - the - art results about it.", "histories": [["v1", "Thu, 19 Nov 2015 19:46:39 GMT  (498kb,D)", "https://arxiv.org/abs/1511.06328v1", "In submission to ICLR 2016"], ["v2", "Thu, 3 Dec 2015 17:11:25 GMT  (498kb,D)", "http://arxiv.org/abs/1511.06328v2", "In submission to ICLR 2016"], ["v3", "Thu, 7 Jan 2016 22:05:56 GMT  (676kb,D)", "http://arxiv.org/abs/1511.06328v3", "In submission to ICLR 2016"]], "COMMENTS": "In submission to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "zhongfei zhang"], "accepted": false, "id": "1511.06328"}, "pdf": {"name": "1511.06328.pdf", "metadata": {"source": "CRF", "title": "MANIFOLD REGULARIZED DISCRIMINATIVE NEURAL NETWORKS", "authors": ["Shuangfei Zhai", "Zhongfei (Mark) Zhang"], "emails": ["szhai2@binghamton.edu,", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Discriminative models are widely used for supervised machine learning tasks. In a probabilistic setting, they directly model the conditional probability of labels given the input P (y|x) without considering the data distribution P (x). Feed-forward neural networks are discriminative in nature, with which one can easily parameterize the conditional probability P (y|x). However, it is well known that unregularized DNNs trained in this fashion (with MLE, for instance) suffers from serious overfitting problems with a limited sample size.\nIn the past decade, we have witnessed several effective regularization techniques that help DNNs better generalize to unobserved data, most noticeably pretraining Hinton & Salakhutdinov (2006); Bengio et al. (2007) and dropout Srivastava et al. (2014). In pretraining, several layers of either RBMs or regularized autoencoders are trained on unlabeled data. One then uses the weights of the stacked deep RBM/autoencoder as the initialization weights for a discriminative DNN, which is further trained on labeled dataset by adding an appropriate classifier layer on top. It is shown that pretrained weights perform very well as an implicit regularization as they prevent the model to get stuck in poor local optimums. The reason for its success is that with the pretrained RMB/autoencoder, the disriminative model gains (implicit) prior knowledge about the data distribution P (x), thus is able to generalize better to unobserved data. Dropout works by randomly masking out part of both the input and hidden units during the discriminative training with SGD. Konda et al. (2015) shows that the effectiveness of dropout can be understood by considering it as a kind of data augmentation Simard et al. (2003). From this perspective, dropout can be considered as implicitly modeling the data manifold by sampling from it.\nAnother motivation of our work is the recent findings of Szegedy et al. (2014), where the authors show that the prediction of a trained discriminative DNN could be greatly changed by adding to the\nar X\niv :1\n51 1.\n06 32\n8v 3\n[ cs\n.L G\n] 7\nJ an\n2 01\n6\ninput a certain small perturbation that is indistinguishable by human eyes. This shows that even the state-of-the-art discriminative DNNs fail to generalize to what seems to be trivial examples. As a remedy, Goodfellow et al. (2015) proposes a way to generate adversarial examples (neighbors of sample data points that increase the loss function significantly) as the additional training data.\nIn this paper, we propose two simple regularizers to the standard discriminative DNNs: LabelAware Manifold Regularization (LAMR) and Label-Independent Manifold Regularization (LIMR). Our assumption is that data points within each class should lie in a smooth manifold. If we move from a data point x by a tiny step , our prediction should remain unchanged, as x+ is very likely to still lie in the same manifold. In the label-aware version, we encourage the loss function to be flat around observations, which amounts to minimizing the norm of the loss function w.r.t. data points. In the label-independent version, we instead encourage the prediction scores to be flat, which can be computed without lablels. We then accordingly minimize the Frobenius norm of the Jacobian matrix of the prediction socres w.r.t. inputs. In addition, we propose stochastic approximations of the regularizers, which avoids explicitly computing the gradients and Jacobians. We perform extensive experiments on both fully labeled and semi-supervised versions of the MNIST dataset, and set state-of-the-art results."}, {"heading": "2 MODEL", "text": "Consider a multi-way classification problem with data {(x1, y1), ..., (xN l , yN l)}, where xi \u2208 Rd and yi \u2208 {1, ...,K} are the observation and label for the i-th instance, respectively;N l is the number of labeled instances. We also assume a set of Nu unlabeled instances {xu1 , ..., xuNu} available. A discriminative model minimizes an objective function in the following form:\n1\nN l N l\u2211 i=1\n`(yi, f(x l i; \u03b8))\ufe38 \ufe37\ufe37 \ufe38\nJ(\u03b8)\n+R(\u03b8), (1)\nwhere f(xli) \u2208 RK computes the predicted score of xli belonging to each of the K class; ` is a loss function indicating the compatibility between the ground truth yi and the prediction f(xli); R(\u03b8) is the regularization term. For example, f(x; \u03b8) could be the conditional probability p(y|x; \u03b8) represented by a DNN with a softmax layer, and `(\u00b7) is accordingly the cross entropy. Alternatively, as in Tang (2013), f(x; \u03b8) could be the output of a DNN with a linear output layer, and ` is the one-vs-rest squared hinge loss. We use the later form for all the models in this paper. The focus of this paper is on R(\u03b8), and we now propose two versions of regularization depending upon the availability of labels."}, {"heading": "2.1 LABEL-AWARE MANIFOLD REGULARIZATION", "text": "In this version, we consider the fully supervised setting where all data points are labeled. According to the manifold assumption, data points within each class lie in a smooth manifold. Without any prior knowledge about the shape of the data manifold, we assume that moving any observation by a tiny step should still keep it in the same manifold. As a result, the loss function to be flat around each observation. In other words, the derivative of the loss function w.r.t. to input\u2207x(`(y, f(x; \u03b8))) should be small. This directly leads us to a regularization in the form:\nR1(\u03b8) = \u03bb\nN l N l\u2211 i=1 \u2016\u2207xli`(yi, f(x l i; \u03b8))\u201622, (2)\nwhere \u03bb > 0 is the regularization strength factor. We call R1(\u03b8) Label-Aware Manifold Regularization (LAMR)."}, {"heading": "2.2 LABEL-INDEPENDENT MANIFOLD REGULARIZATION", "text": "LAMR assumes the availability of fully labeled data, which prohibits its application to semisupervised settings where large portions of data are unlabeled. To this end, we propose a slightly\ndifferent view on the regularization. The idea is that we can consider f(x; \u03b8) as a mapping from the data manifold to the label manifold embedded in RK . Following a similar reasoning, we then encourage f(x; \u03b8), instead of the loss function, to be flat when we move the input along the data manifold. While the very same idea has been proposed in Gu & Rigazio (2014) as deep contractive network (DCN), we further extend it by observing that this regularization also applies to unlabeled data. This makes this view particularly interesting as we are now able to better explore the data distribution with the help of unlabeled data. As a result, we minimize the Frobenius norm of the Jacobian matrix as follows:\nR2(\u03b8) = \u03bb\nN l N l\u2211 i=1 \u2016\u2207xlif(x l i; \u03b8)\u201622 + \u03b2 Nu Nu\u2211 i=1 \u2016\u2207xui f(x u i ; \u03b8)\u201622. (3)\nHere we have slightly overloaded the notation by letting \u2207xf(x; \u03b8) denote the Jacobian matrix of f(x; \u03b8) w.r.t. x. We also allow different regularization strengths \u03bb and \u03b2 on labeled and unlabeled data. We call R2(\u03b8) Label-Independent Manifold Regularization (LIMR). Note that LIMR is, different from LAMR, also loss independent. To see this, consider f(xi; \u03b8) as the output of a DNN with a linear output layer, LIMR would be the same no matter if we use the squared loss or hinge loss, but LAMR would be different for the two cases. This difference is verified in our experiments where we observe that LAMR achieves slightly lower test error on the fully supervised task. On the other hand, LAMR and LIMR are also closely related, simply by noting that minimizing LIMR implicitly minimizes LAMR, as R1(\u03b8)\u2192 0 when R2(\u03b8)\u2192 0."}, {"heading": "2.3 STOCHASTIC APPROXIMATION", "text": "Although it is possible to express R1(\u03b8) and R2(\u03b8) in a closed form for feed-forward neural networks, the resulting objective function could be complicated for DNNs and structured architectures such as CNNs. This makes computing the gradient w.r.t. model parameters inefficient. To address this issue, we take a simple stochastic approximation of the regularization terms. Let g : Rd 7\u2192 R be a differentiable function, and \u2208 N (0, \u03c32) be a draw from an isotropic Gaussian distribution with variance \u03c32; then g(x+ ) \u2248 g(x) + T\u2207xg(x), (4) according to first order Taylor expansion. As a result, we have:\nE [g(x+ )\u2212 g(x)]2 \u2248 E [ T\u2207xg(x)]2 = \u03c32\u2016\u2207xg(x)\u201622, (5)\nwhere in the last step we have marginalized out the isotropic Gaussian random variable. With this trick, we approximate the regularization terms for LAMR and LIMR as follows:\nR\u03031(\u03b8) = \u03bb\nN l N l\u2211 i=1 E [`(yi, f(x l i + ; \u03b8))\u2212 `(yi, f(xli; \u03b8))]2\nR\u03032(\u03b8) = \u03bb\nN l N l\u2211 i=1 E \u2016f(xli + ; \u03b8)\u2212 f(xli; \u03b8)\u201622 + \u03b2 Nu Nu\u2211 i=1 E \u2016f(xui + ; \u03b8)\u2212 f(xui ; \u03b8)\u201622, (6)\nwhere we have absorbed the scaling factor \u03c32. In practice, we simulate the expectations in R\u03031 and R\u03032 by randomly sampling the Gaussian noise at each iteration of stochastic gradient descent (SGD), in the same way as Vincent et al. (2010).\nIn theory, the stochastic approximation is more accurate when using a small \u03c3. However, we find that it is beneficial to use a relatively large \u03c3 in practice, which actually allows the regularization to explore points from the data manifold that are relatively far from the observations."}, {"heading": "2.4 CONNECTION WITH EXISTING REGULARIZERS", "text": "Feature Noising Bishop (1995):One long existing trick for training neural networks is randomly adding noise to data during training. Marginally, its objective function can be expressed 1 N l \u2211N l i=1 E \u2208Pnoise [`(yi, f(x l i + ; \u03b8))], where Pnoise is the additive noise distribution to generate samples around xli. Feature noising thus implicitly explores the data manifold with Pnoise(x l i),\nin a way that is similar to LAMR. However, it does not stress the flatness of the loss function as LAMR does. To see this, consider the simplest case where Pnoise = N (0, \u03c32noiseI), we can then marginalize the noise by using the second order Taylor expansion as:\n1\nN l N l\u2211 i=1\n`(yi, f(x l i; \u03b8))\ufe38 \ufe37\ufe37 \ufe38\nJ(\u03b8)\n+ \u03c32noise N l N l\u2211 i=1\ntr(Hi(\u03b8))\ufe38 \ufe37\ufe37 \ufe38 R(\u03b8) , (7)\nwhere Hi(\u03b8) = \u22072xli`(yi, f(x l i; \u03b8) is the Hessian matrix of the loss function at x l i. We see that the regularization term now encourages the trace of Hessian matrix to be small around the observations, which corresponds to functions with low curvature. This is different from LAMR, as it is that the gradient is large under even though the trace of the Hessian matrix is small. Also, note that one variant of Dropout Srivastava et al. (2014) when mask out noise is only applied to the input layer can be analyzed in a similar way, and the readers are encouraged to see Wager et al. (2013) for detailed discussions.\nManifold Tangent Classifier (MTC) Rifai et al. (2011a): MTC is a semi-supervised classifier that considers the data manifold, which builds upon the Contractive Autoencoders (CAE) Rifai et al. (2011c;b). The idea is that one first trains a stacked CAE on all the data (with or without label) as in pretraining, from which the tangent bundles Bx around each observation is then computed and stored. The corresponding regularization takes the form:\nRmtc(\u03b8) = \u03bb\nN l N l\u2211 i=1 \u2211 ui\u2208Bxl [uTi \u2207xl`(yi, xli; \u03b8)]2. (8)\nIn Equation 8, the gradient of loss function is encouraged to be orthogonal to the tangent vectors ui. The connection with LAMR can be observed if we replace ui with a random vector drawn from a Gaussian distribution N (0, \u03c32I), and replace the inner sum with expectation over the random vector:\n\u03bb\nN l N l\u2211 i=1 Eui [u T i \u2207xl`(yi, xli; \u03b8)]2 = \u03bb\u03c32 N l N l\u2211 i=1 \u2016\u2207xl`(yi, xli; \u03b8)\u201622 \u221d R1(\u03b8). (9)\nWe see that forcing the gradient to be orthogonal to any random direction instead of the tangent vectors reduces MTC to LAMR. This highlights our assumption about the data manifold: if an observation is moved by a small random step, it is very likely to still fall into the same manifold. MTC on the other hand, explicitly characterizes the shape of the manifold with a dedicated model (CAE). As a result, LAMR puts a stronger regularization by enforcing \u2207xl`(yi, xli; \u03b8) to be small, which also implicitly minimizes Equation 8. MTC is also able to perform semi-supervised learning, as unlabeled data could help better characterize the data manifold. LIMR takes a similar idea, but instead encourages the flatness of the prediction score function on the unlabeled data points. This also enables one to directly incorporate the regularization with the supervised objective, which makes the two-step pretraining fashion unnecessary.\nDeep Contractive Network (MTC) Gu & Rigazio (2014): In the purely supervised setting, DCN is marginally equivalent to LIMR. However, there are two notable differences. First, we extend LIMR to the semi-supervised learning setting. Second, our stochastic approximation allows LIRM to be applied to deep neural networks with complicated structures (with convolutional layers, for example) easily. Moreover, as shown in the experiments, the stochastic approximation with a large noise allows us to explore a larger area of the input space, which puts more regularization on the flatness of the target function.\nAdversarial Training Goodfellow et al. (2015): Recently in Szegedy et al. (2014), the authors point out that the state-of-the-art DNNs can misclassify examples that are generated by adding to the training examples with certain small perturbations (adversarial examples). To address this issue, Goodfellow et al. (2015) propose a strategy to generate adversarial examples and add them to the training set along training. LAMR and LIMR are able to alleviate the very same problem from a different angle: reducing the chance that adversarial examples occur by restricting the loss function to be flat around the observations."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 SUPERVISED LEARNING", "text": "In the first set of experiments, we use the MNIST dataset to validate the regularization effects in the supervised learning setting. MNIST consists of 50000 training, 10000 validation and 10000 testing images of handwritten digits, each with shape 28x28. W train a three layer feed-forward neural network with size 500 \u00d7 500 \u00d7 500 \u00d7 10, where 10 is the size of outputs. We use ReLU as the nonlinear activation function for each of the three hidden layers, and a linear output layer (whose output corresponds to f(xi; \u03b8)) together with the one-vs-rest squared hinge loss as in Tang (2013). Parameters are initialized following Glorot & Bengio (2010) and optimized with Adadelta Zeiler (2012) using a batch size of 100. We set the standard deviation \u03c3 as 0.5, and vary the regularization strength \u03bb in Equation 6 for both LAMR and LIMR (\u03b2 = 0 in the fully supervised task), respectively. We show the results in Table 1. We see that without regularization (\u03bb = 0), this three layer network achieves a test error rate of 1.64. For both LAMR and LIMR, the error rate is significantly reduced and gradually changes as we vary the regularization strength. Overall, we see that the two regularizers achieve similar best results and are both pretty robust w.r.t. \u03bb within a certain range.\nWe additionally visualize the filters of the first layer learned by two models in Figure 1. The left panel corresponds to \u03bb = 0 where no regularization is used; the right panel corresponds to LIMR with \u03c3 = 0.5 and \u03bb = 0.7. LIMR learns sharp filters that mostly correspond to pen strokes, which are very similar to those learned by a regularized autoencoder (eg., Rifai et al. (2011c)).\nTo investigate the property of the stochastic approximation, we have also tried using different values for \u03c3 from the set {\u2192 0, 0.1, 0.5, 1} to demonstrate its effect on the regularization quality. Here we\ndenote\u2192 0 as using the explicit expression of gradients Gu & Rigazio (2014) as regularization, as it corresponds to the case where the variance of noise approximates zero. We show the test error rate for different \u03c3s for LAMR in Figure 2. We see that all the three values of \u03c3 are able to help reduce the error rate by varying the regularization strength \u03bb, and \u03c3 = 0.5 achieves the lowest test error rate among the three. In particular, note that taking \u03c3 = 0.5 actually outperforms directly using the gradients as regularization explicitly. This confirms our speculation that a relatively large \u03c3 allows the stochastic approximation to explore areas that are further from the observed data points; and by minimizing the difference between the loss of the clean input and the noisy input, the curvature of the target function is further encouraged to be flat.\nTo compare with the existing methods, we also train this three layer neural network with LAMR and LIMR on 60000 examples by combining the training and validation set and report the test error rate in Table 2. LAMR sets the state-of-the art result of 0.74 on this permutation invariant data set (which does not consider the 2D structure of image); LIMR also achieves a result that is close to the previous state-of-the-art. As a direct comparison, we also try applying feature noising with the same type of noise as used in LAMR and LIMR, which is outperformed by both. Note that we did not try to optimize the architecture of the network at all. In fact, our models have the fewest number of parameters among all the competitors, so it is still possible to reduce the error rate by trying different model sizes.\nWe also consider applying our regularizer to CNNs. CNNs are particularly suitable for modeling image data due to the sharing of convolutional filters across different locations within an image. We use an architecture consisting of a convolutional layer of shape 200\u00d7 9\u00d7 9, a max pooling layer of shape 2\u00d7 2, another convolutional layer of shape 200\u00d7 3\u00d7 3, another max pooling layer of shape\n2 \u00d7 2, a fully connected layer of size 500 and finally a linear output layer with 10 units together with squared hinge loss on top. As in the fully connected network, we also use ReLU as activation function, a \u03c3 value of 0.5 and a \u03bb value of 0.1. We train the model with the LAMR, and report the results in Table 3. We see that an unregularized CNN already has very low error rates, LAMR is still able to reduce it further. When training on the full 60000 labeled dataset, we achieve a test error that is comparable with the state-of-the-art result in Goodfellow et al. (2013)."}, {"heading": "3.2 SEMI-SUPERVISED LEARNING WITH LIMR", "text": "We now consider the semi-supervised learning setting where we only use a small portion of the labels from the training set. We use the same fully connected three layer network as in Section 3.1. We follow the experiment settings as in Rifai et al. (2011a); Kingma et al. (2014), where the 50000 training set is further split into one labeled set and one unlabeled set. We use the validation set the select \u03bb and \u03b2, and found that \u03bb = \u03b2 = 15 works well for all the four settings.\nWe report the results in Table 4. We see that LIMR consistently boosts the performance of the classifier with the aid of unlabeled data. Compared with the state-of-the-art models, except for the case with only 100 labels, LIMR is comparable or better than pretraining based semi-supervised learning CAE and MTC. The deep generative model proposed by Kingma et al. (2014) works the best with very few labels in general, but it is beat by LIMR in the case of 3000 labels. An error rate of 1.88 also sets the new state-of-the-art result on this sub-task.\nAs a qualitative justification, we also visualize the first layer filters learned with different label numbers in Figure 3. We see that even with a few labels, LIMR is still able to learn sharp filters. There is also a clear correlation between the visual quality of the filters and the number of labels. Especially, the filters learned with 3000 labels are almost indistinguishable from those learned with all the 50000 labels in Figure 1. This strongly indicates the power of semi-supervised learning, as\nwell as that LIMR is able to take advantage of both labels and unlabeled data simultaneously and learn meaningful representations and classifiers at the same time.\nWe also consider two more challenging benchmarks, CIFAR10 and SVHN. CIFAR10 consists of 60000 32\u00d732\u00d73 images in 10 object categories, with 50000 examples as training set and 10000 as testing set. The subset of the SVHN dataset we use consists of images of the size also 32\u00d7 32\u00d7 3, with 70000 for training, 26000 for testing and 100000 unlabeled examples. As CIFAR10 does not come with an unlabeled set, we use the entire CIFAR100 dataset for this purpose. Note that although CIFAR10 and CIFAR100 have input images of the same size, they have two different label sets. For both the two benchmarks, we use a CNN with architecture 64\u00d75\u00d75 64\u00d73\u00d73 64\u00d73\u00d7, with 2\u00d72 max pooling following each convolution layer. We plot the training and testing error curves in Figure 4 under three settings: no regularization, supervised LIMR regularization and semi-supervised LIMR regularization. We see that for both the two datasets, applying LIMR significantly reduces overfitting (larger training error and lower testing error). When incorporated with unlabeled data, LIMR is able to further reduce the testing error. Interestingly, this works despite that CIFAR10 and CIFAR100 have different label sets."}, {"heading": "4 CONCLUSION", "text": "We have proposed two regularizers that make use of the data manifold to guide the learning of a discriminative neural network. By encouraging the flatness of either the loss function or the prediction scores along the data manifold, we are able to significantly improve a DNN\u2019s generalization ability. Moreover, our label independent manifold regularization allows one to incorporate unlabeled data directly with the supervised learning task and effectively performs semi-supervised learning. We have validated our proposed regularizers on the MNIST, CIFAR10 and SVHN datasets and demonstrated their effectiveness."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Bishop", "Chris M"], "venue": "Neural computation,", "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Gu", "Shixiang", "Rigazio", "Luca"], "venue": "arXiv preprint arXiv:1412.5068,", "citeRegEx": "Gu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Dropout as data augmentation", "author": ["Konda", "Kishore", "Bouthillier", "Xavier", "Memisevic", "Roland", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1506.08700,", "citeRegEx": "Konda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2015}, {"title": "The manifold tangent classifier", "author": ["Rifai", "Salah", "Dauphin", "Yann N", "Vincent", "Pascal", "Bengio", "Yoshua", "Muller", "Xavier"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Higher order contractive auto-encoder", "author": ["Rifai", "Salah", "Mesnil", "Gr\u00e9goire", "Vincent", "Pascal", "Muller", "Xavier", "Bengio", "Yoshua", "Dauphin", "Yann", "Glorot"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Simard", "Patrice Y", "Steinkraus", "Dave", "Platt", "John C"], "venue": "In null,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Deep learning using linear support vector machines", "author": ["Tang", "Yichuan"], "venue": "arXiv preprint arXiv:1306.0239,", "citeRegEx": "Tang and Yichuan.,? \\Q2013\\E", "shortCiteRegEx": "Tang and Yichuan.", "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Dropout training as adaptive regularization", "author": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "In the past decade, we have witnessed several effective regularization techniques that help DNNs better generalize to unobserved data, most noticeably pretraining Hinton & Salakhutdinov (2006); Bengio et al. (2007) and dropout Srivastava et al.", "startOffset": 194, "endOffset": 215}, {"referenceID": 0, "context": "In the past decade, we have witnessed several effective regularization techniques that help DNNs better generalize to unobserved data, most noticeably pretraining Hinton & Salakhutdinov (2006); Bengio et al. (2007) and dropout Srivastava et al. (2014). In pretraining, several layers of either RBMs or regularized autoencoders are trained on unlabeled data.", "startOffset": 194, "endOffset": 252}, {"referenceID": 0, "context": "In the past decade, we have witnessed several effective regularization techniques that help DNNs better generalize to unobserved data, most noticeably pretraining Hinton & Salakhutdinov (2006); Bengio et al. (2007) and dropout Srivastava et al. (2014). In pretraining, several layers of either RBMs or regularized autoencoders are trained on unlabeled data. One then uses the weights of the stacked deep RBM/autoencoder as the initialization weights for a discriminative DNN, which is further trained on labeled dataset by adding an appropriate classifier layer on top. It is shown that pretrained weights perform very well as an implicit regularization as they prevent the model to get stuck in poor local optimums. The reason for its success is that with the pretrained RMB/autoencoder, the disriminative model gains (implicit) prior knowledge about the data distribution P (x), thus is able to generalize better to unobserved data. Dropout works by randomly masking out part of both the input and hidden units during the discriminative training with SGD. Konda et al. (2015) shows that the effectiveness of dropout can be understood by considering it as a kind of data augmentation Simard et al.", "startOffset": 194, "endOffset": 1078}, {"referenceID": 0, "context": "In the past decade, we have witnessed several effective regularization techniques that help DNNs better generalize to unobserved data, most noticeably pretraining Hinton & Salakhutdinov (2006); Bengio et al. (2007) and dropout Srivastava et al. (2014). In pretraining, several layers of either RBMs or regularized autoencoders are trained on unlabeled data. One then uses the weights of the stacked deep RBM/autoencoder as the initialization weights for a discriminative DNN, which is further trained on labeled dataset by adding an appropriate classifier layer on top. It is shown that pretrained weights perform very well as an implicit regularization as they prevent the model to get stuck in poor local optimums. The reason for its success is that with the pretrained RMB/autoencoder, the disriminative model gains (implicit) prior knowledge about the data distribution P (x), thus is able to generalize better to unobserved data. Dropout works by randomly masking out part of both the input and hidden units during the discriminative training with SGD. Konda et al. (2015) shows that the effectiveness of dropout can be understood by considering it as a kind of data augmentation Simard et al. (2003). From this perspective, dropout can be considered as implicitly modeling the data manifold by sampling from it.", "startOffset": 194, "endOffset": 1206}, {"referenceID": 0, "context": "In the past decade, we have witnessed several effective regularization techniques that help DNNs better generalize to unobserved data, most noticeably pretraining Hinton & Salakhutdinov (2006); Bengio et al. (2007) and dropout Srivastava et al. (2014). In pretraining, several layers of either RBMs or regularized autoencoders are trained on unlabeled data. One then uses the weights of the stacked deep RBM/autoencoder as the initialization weights for a discriminative DNN, which is further trained on labeled dataset by adding an appropriate classifier layer on top. It is shown that pretrained weights perform very well as an implicit regularization as they prevent the model to get stuck in poor local optimums. The reason for its success is that with the pretrained RMB/autoencoder, the disriminative model gains (implicit) prior knowledge about the data distribution P (x), thus is able to generalize better to unobserved data. Dropout works by randomly masking out part of both the input and hidden units during the discriminative training with SGD. Konda et al. (2015) shows that the effectiveness of dropout can be understood by considering it as a kind of data augmentation Simard et al. (2003). From this perspective, dropout can be considered as implicitly modeling the data manifold by sampling from it. Another motivation of our work is the recent findings of Szegedy et al. (2014), where the authors show that the prediction of a trained discriminative DNN could be greatly changed by adding to the", "startOffset": 194, "endOffset": 1397}, {"referenceID": 3, "context": "As a remedy, Goodfellow et al. (2015) proposes a way to generate adversarial examples (neighbors of sample data points that increase the loss function significantly) as the additional training data.", "startOffset": 13, "endOffset": 38}, {"referenceID": 15, "context": "In practice, we simulate the expectations in R\u0303 and R\u0303 by randomly sampling the Gaussian noise at each iteration of stochastic gradient descent (SGD), in the same way as Vincent et al. (2010). In theory, the stochastic approximation is more accurate when using a small \u03c3.", "startOffset": 170, "endOffset": 192}, {"referenceID": 9, "context": "Also, note that one variant of Dropout Srivastava et al. (2014) when mask out noise is only applied to the input layer can be analyzed in a similar way, and the readers are encouraged to see Wager et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "Also, note that one variant of Dropout Srivastava et al. (2014) when mask out noise is only applied to the input layer can be analyzed in a similar way, and the readers are encouraged to see Wager et al. (2013) for detailed discussions.", "startOffset": 39, "endOffset": 211}, {"referenceID": 8, "context": "Manifold Tangent Classifier (MTC) Rifai et al. (2011a): MTC is a semi-supervised classifier that considers the data manifold, which builds upon the Contractive Autoencoders (CAE) Rifai et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 3, "context": "Adversarial Training Goodfellow et al. (2015): Recently in Szegedy et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 3, "context": "Adversarial Training Goodfellow et al. (2015): Recently in Szegedy et al. (2014), the authors point out that the state-of-the-art DNNs can misclassify examples that are generated by adding to the training examples with certain small perturbations (adversarial examples).", "startOffset": 21, "endOffset": 81}, {"referenceID": 3, "context": "Adversarial Training Goodfellow et al. (2015): Recently in Szegedy et al. (2014), the authors point out that the state-of-the-art DNNs can misclassify examples that are generated by adding to the training examples with certain small perturbations (adversarial examples). To address this issue, Goodfellow et al. (2015) propose a strategy to generate adversarial examples and add them to the training set along training.", "startOffset": 21, "endOffset": 319}, {"referenceID": 8, "context": ", Rifai et al. (2011c)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 8, "context": "Method Test error rate 8192\u00d7 8192 NN + Dropout Srivastava et al. (2014) 0.", "startOffset": 47, "endOffset": 72}, {"referenceID": 8, "context": "Method Test error rate 8192\u00d7 8192 NN + Dropout Srivastava et al. (2014) 0.95 500\u00d7 500\u00d7 2000 DBN + Dropout finetuning Srivastava et al. (2014) 0.", "startOffset": 47, "endOffset": 142}, {"referenceID": 8, "context": "Method Test error rate 8192\u00d7 8192 NN + Dropout Srivastava et al. (2014) 0.95 500\u00d7 500\u00d7 2000 DBN + Dropout finetuning Srivastava et al. (2014) 0.92 500\u00d7 500\u00d7 2000 DBM + Dropout finetuning Srivastava et al. (2014) 0.", "startOffset": 47, "endOffset": 212}, {"referenceID": 7, "context": "79 2000\u00d7 2000 Contractive Autoenconder Rifai et al. (2011b) 1.", "startOffset": 39, "endOffset": 60}, {"referenceID": 7, "context": "79 2000\u00d7 2000 Contractive Autoenconder Rifai et al. (2011b) 1.04 2000\u00d7 2000 Manifold Tangent Classifier Rifai et al. (2011a) 0.", "startOffset": 39, "endOffset": 125}, {"referenceID": 3, "context": "81 1600\u00d7 1600 Maxout NN + Adversarial Training Goodfellow et al. (2015) 0.", "startOffset": 47, "endOffset": 72}, {"referenceID": 3, "context": "When training on the full 60000 labeled dataset, we achieve a test error that is comparable with the state-of-the-art result in Goodfellow et al. (2013).", "startOffset": 128, "endOffset": 153}, {"referenceID": 3, "context": "Table 3: Error rates of CNN trained with LAMR, compared with state of the art model Goodfellow et al. (2013).", "startOffset": 84, "endOffset": 109}, {"referenceID": 7, "context": "We follow the experiment settings as in Rifai et al. (2011a); Kingma et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 6, "context": "(2011a); Kingma et al. (2014), where the 50000 training set is further split into one labeled set and one unlabeled set.", "startOffset": 9, "endOffset": 30}, {"referenceID": 6, "context": "(2011a); Kingma et al. (2014), where the 50000 training set is further split into one labeled set and one unlabeled set. We use the validation set the select \u03bb and \u03b2, and found that \u03bb = \u03b2 = 15 works well for all the four settings. We report the results in Table 4. We see that LIMR consistently boosts the performance of the classifier with the aid of unlabeled data. Compared with the state-of-the-art models, except for the case with only 100 labels, LIMR is comparable or better than pretraining based semi-supervised learning CAE and MTC. The deep generative model proposed by Kingma et al. (2014) works the best with very few labels in general, but it is beat by LIMR in the case of 3000 labels.", "startOffset": 9, "endOffset": 602}, {"referenceID": 6, "context": "# labels NN NN + LIMR CAE MTC Kingma et al. (2014) 100 32.", "startOffset": 30, "endOffset": 51}], "year": 2016, "abstractText": "Unregularized deep neural networks (DNNs) can be easily overfit with a limited sample size. We argue that this is mostly due to the disriminative nature of DNNs which directly model the conditional probability (or score) of labels given the input. The ignorance of input distribution makes DNNs difficult to generalize to unseen data. Recent advances in regularization techniques, such as pretraining and dropout, indicate that modeling input data distribution (either explicitly or implicitly) greatly improves the generalization ability of a DNN. In this work, we explore the manifold hypothesis which assumes that instances within the same class lie in a smooth manifold. We accordingly propose two simple regularizers to a standard discriminative DNN. The first one, named Label-Aware Manifold Regularization, assumes the availability of labels and penalizes large norms of the loss function w.r.t. data points. The second one, named Label-Independent Manifold Regularization, does not use label information and instead penalizes the Frobenius norm of the Jacobian matrix of prediction scores w.r.t. data points, which makes semi-supervised learning possible. We perform extensive control experiments on fully supervised and semi-supervised tasks using the MNIST, CIFAR10 and SVHN datasets and achieve excellent results.", "creator": "LaTeX with hyperref package"}}}