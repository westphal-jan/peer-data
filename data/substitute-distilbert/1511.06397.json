{"id": "1511.06397", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Compressing Word Embeddings", "abstract": "recent methods for learning vector space representations of words have succeeded continually capturing fine - grained semantic matrix syntactic regularities beside vector arithmetic. however, these vector space representations ( created through large - scale text analysis ) are typically stored verbatim, since their internal structure becomes fuzzy. using encoding - analogy validation to confirm the level of detail stored in compressed re - representations of the same vector space, the trade - offs between the reduction in memory usage and expressiveness are investigated. a simple scheme is studied that can reduce the memory footprint of a state - of - the - animation embedding by each factor of 10, with only secondary impact over performance. then, using the same'bit budget ', a binary ( approximate ) implementation of the same space is also applied, revealing the aim of creating an equivalent representation with intermediate interpretability.", "histories": [["v1", "Thu, 19 Nov 2015 21:42:47 GMT  (562kb)", "http://arxiv.org/abs/1511.06397v1", "10 pages, 6 figures, ICLR-2016"], ["v2", "Mon, 16 May 2016 17:19:51 GMT  (15kb)", "http://arxiv.org/abs/1511.06397v2", "10 pages, 0 figures, submitted to ICONIP-2016. Previous experimental results were submitted to ICLR-2016, but the paper has been significantly updated, since a new experimental set-up worked much better"]], "COMMENTS": "10 pages, 6 figures, ICLR-2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["martin", "rews"], "accepted": false, "id": "1511.06397"}, "pdf": {"name": "1511.06397.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["martin@redcatlabs.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 39\n7v 1\n[ cs\n.C L\n] 1\n9 N\nov 2\n01 5"}, {"heading": "1 INTRODUCTION", "text": "Distributed representations of words have been shown to benefit NLP tasks like parsing, named entity recognition, and sentiment analysis, as well as being used as the raw material for other deep learning tasks.\nSurprisingly, these word vector embeddings can be derived directly from raw, unannotated corpora. Once created, the vector embedding itself can be expressed simply as a list of vocabulary words (of size V ), and a matrix of size V \u00d7 d, where d is the dimensionality of the embedding space.\nIn this work, a 300 dimensional GloVe embedding (Pennington et al., 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al. (2013)."}, {"heading": "2 APPROACHES TO COMPRESSION", "text": "The GloVe embeddings are produced by minimising the error in predicting the word co-occurrence matrix Xi,j from the word vectors associated with words i and j 1 :\nJ = V\u2211\ni,j=1\nf(Xij)(w T i w\u0303j + bi + b\u0303j \u2212 logXij) 2\nThis formulation means that there is no ordering of the components of the vectors. In addition, simple linear dependencies have already been eliminated (since if there were linear dependencies, the overall error would be higher than if the terms were independently allowed to fit more features of the target Xij ).\n1f() is a function that (i) vanishes at zero, to eliminate problems of Xij being zero in the log() term, and (ii) levels off once Xij is \u2018large\u2019, so that frequent words don\u2019t dominate the error calculation."}, {"heading": "2.1 REAL-VALUED ENCODINGS", "text": "Since the embeddings arrive as a vector of real numbers, it is natural to see whether these can be compressed into a representation with the same dimensionality, except quantised."}, {"heading": "2.1.1 COMPRESSION 1 : THROW AWAY DATA", "text": "The most simple approach is to simply set a fixed proportion of the data in the embedding to zero.\nA slightly more sophisticated version is to select a threshold value, and set every value in a given vector representation that lies below that value to zero. The compression here is slightly offset due to the need to store an \u2018is-zero\u2019 mask, along with the remaining values."}, {"heading": "2.1.2 COMPRESSION 2 : LINEAR QUANTISATION", "text": "A straightforward approach to quanitising the embedding in n levels is to construct a linear scale of n steps between the maximum and minimum values of each element (columns in the embedding matrix).\nA simple variant, used here due to its symmetry, is to first normalize the embedding column-wise, so that each element is \u223c N(0, 1), and then encode the value using (i) 1 bit for the sign, and (ii) an index representing which of the n/2 steps in the range [0,max |x|] to use.\nTo approximately counter the non-uniform distribution of the values in the elements, a simple nonlinearity was also experimented with, whereby |x|\u03b1 was quantised, where \u03b1 was an experimentallyderived constant (a value of \u223c 0.4 was found to be reasonably robust)."}, {"heading": "2.1.3 COMPRESSION 3 : ADAPTIVE LEVEL ENCODING", "text": "Observing the significant non-normality of the distributions of each element (see Figure 1), an adaptive quantisation approach was devised.\nFor a given number of quantisation levels n and an element e \u223c E, create a set of quantisation levels eq that minimises: \u2211\ne\u2208E\nmin q\n(e \u2212 eq) 2\nThis process can be performed iteratively, starting with the eq placed uniformly within the values of E sorted numerically. Gradient descent can then be performed on the error term, and updates made to the eq made until a stable configuration emerges. Note that early iterations may give rise to large gradients in the eq (since the function to be minimised has lots of sharp discontinuities in it).\nThe above optimisation is done for each element/column of the embedding independently, and a list of the respective levels of eq is stored, in addition to the index within each list for each element of the representation."}, {"heading": "2.2 BINARY ENCODINGS", "text": "Even though the methods in the previous section can reduce the bit-count required for an embedding significantly, they still impose an interdependence between the bits representing an individual element. Working with the \u2018bit budget\u2019 suggested by the previous encodings, \u2018true\u2019 binary representations of the same embedding were sought, following the intuition that a good encoding should have minimal pre-ordained structure."}, {"heading": "2.2.1 COMPRESSION 4 : QUASI AUTO-ENCODING", "text": "Assuming a binary representation B of the baseline embedding E can be found, a natural first step is to aim for a linear relationship : E = BW, for some weight matrix W.\nWhile it seems reasonable to want to jointly optimise B and W, the size of B (which will be a multiple of the size of E) makes this impractical.\nIn order to make this more tractable, a secondary mapping is set up, whereby B\u2217 is generated from the respective rows of E through a trainable network. This is similar to auto-encoding learning (see, for instance, Harpur & Prager (1996)), except that discovering B (which is the pure binary embedding that B\u2217 is tending towards) is the end-goal, rather than an intermediate step. Note also that the structure or complexity of the network transformation NN in the generative B\u2217 = NN(E) step is not an important factor.\nAfter testing a number of different network configurations, the structure described in Table 1 was chosen, and it includes the following features :\n1. Addition of noise : A constant, additive noise term was found to aid training greatly (see also Bengio et al. (2013)).\n2. Exclusion zone : In order to promote \u2018binarisation\u2019 after the sigmoid function, a parameterised function was included that imposed an \u2018exclusion zone\u2019 around zero, so that the network was forced to sample values further away from the origin.\n3. Sigmoid function : This was chosen to be a standard sigmoid function during training, but a hard step function during testing.\n4. \u2018Dynamic frog-boiling\u2019 : Since an optimisation of both the network weights and the binarisation B was being performed, the objective function had two components : the first being the L2 error of E\u2217 vs E, and the second being the \u2018binary-ness\u2019 of the generated embedding B\n\u2217, which was captured by measuring its variance elementwise. These two components were weighted relative to each other by a factor \u03bb. However, this balancing factor \u03bb (which often arises in these situations) is difficult to select before training commences, and it may only be after several hours of training that a bad initial choice is revealed. The method used here is to monitor the L2 error of E\u2217 vs E, and, when it is within an acceptable bound, increase the value of \u03bb incrementally (starting from zero). This method dynamically calibrates the difficulty of the task according to progress already made, allowing the network to find a reasonable starting point before the slightly-harderto-solve objective function is triggered."}, {"heading": "2.2.2 COMPRESSION 5 : GREEDY BINARY PCA", "text": "A PCA-like approach to producing a binary embedding was investigated (for another approach, see Gregor & LeCun (2010)).\nStarting with the original embedding E0 = E, an iterative search is made for a vector wn that minimised :\n\u2211\ne\u2208En\nmin (|e\u2212wn|2, |e|2),\nthat is to say, the vector wn is the vector that minimises the total remaining energy in En+1 if the choices available are either to subtract it from a particular embedding row, or not.\nEach iteration of this produces a vector wn, a binary representation of whether that vector should be used for any particular word (which becomes the last column of Bn), and a new residual embedding En+1.\nUltimately, aggregating these representations and vectors into matrices gives a sequence of representations : BnWn \u2192 E."}, {"heading": "3 EXPERIMENTS", "text": "A 300-dimensional embedding (created from a 6 billion token corpus built from a combination of Wikipedia 2014 and Gigaword 5) was used as the GloVe baseline for experimentation 2.\nThe word analogy task used here as the compression performance measure was as described in Mikolov et al. (2013), and the provided dataset contains 19,544 such questions, divided into semantic and syntactic subsets. The semantic questions are typically analogies about people or places, like \u201dAthens is to Greece as Berlin is to ?\u201d, whereas the syntactic questions are typically analogies about verb tenses or forms of adjectives, for example \u201ddance is to dancing as fly is to ?\u201d.\nTo correctly answer a question, the model must uniquely identify the missing term, with only an exact correspondence counted as a correct match. This matching is done on a cosine-similarity basis, where the answer to the question \u201da is to b as c is to ?\u201d is found by searching for the word d whose representation wd is closest to wb \u2212 wa + wc according to cosine similarity (i.e. the word where the dot-product is a maximum compared to every other word in the vocabulary). As might be expected, small distortions in the embedding space can cause large drops in performance on this exact matching task.\n2Available for download at http://www-nlp.stanford.edu/projects/glove/\nIn Figure 2, the performance of a selection of 300-dimensional full-resolution embeddings is shown (as given in (Pennington et al., 2014)). The GloVe performance was cross-checked using the downloaded embedding and the dataset derived from the word analogy task."}, {"heading": "3.1 BASIC METHODS", "text": "Also in Figure 2 are the results of the \u2018throw data away\u2019 methods, either by zeroing out columns, or thresholding (circles and triangles respectively). In addition, the hexagon marks the performance of a 100-dimensional embedding trained on the same corpus, so that it can be compared to a 300- dimensional embedding with 200 of those dimensions artificially zeroed out."}, {"heading": "3.2 QUANTISATION ENCODINGS : LINEAR, NEAR-LINEAR AND ADAPTIVE", "text": "The results of the quantisation methods are illustrated in Figure 3, where linear, simple non-linear and adaptively chosen quantisation grids were used (down triangle, up triangle and circle respectively). The numbers associated with each label correspond to the number of levels used.\nNoteably, the method \u2018ada.8\u2019 achieves a performance that is only 0.58% less than GloVe, while using only 3-bits resolution per element."}, {"heading": "3.3 BINARY ENCODINGS", "text": ""}, {"heading": "3.3.1 QUASI AUTO-ENCODING TRAINING", "text": "Figure 4 shows the performance during training of the \u2018quasi auto-encoding\u2019 method. As can be seen, the L2 error is continually being brought down to a predetermined acceptable level, while the parameter \u03bb is slowly increased to encourage the variance of B\u2217 to increase (which can also be seen, although it is clearly reluctant to improve all the way to the ultimate goal which is close to 0.25 on the right-hand axis)."}, {"heading": "3.3.2 GREEDY BINARY PCA TRAINING", "text": "Figure 5 shows the performance as the greedy binary error reduction operations take place. The reduction in overall error is clear, although the error being taken out at each step becomes very small. Interestingly, the number of \u2018on\u2019 bits in each successive additional representation in Bn, tends to a very stable limit (discussed below)."}, {"heading": "3.3.3 BINARY ENCODING RESULTS", "text": "The results of the binarisation methods are illustrated in Figure 6, where \u2018NN\u223c3\u2019 is a 1024-bit binary embedding (approximately 3.4-bits per original embedding dimension), and the \u2018pca.x\u2019 are the embeddings produced by the binary PCA method outlined earlier. The error (over the original GloVe embedding) of \u2018pca.3\u2019 is 0.81%, which is directly comparable to the earlier \u2018ada.8\u2019 method, which scored 0.58% (i.e. the adaptive levels method was superior). Moreover, the \u2018pca.8\u2019 method requires the additional transmission of a (900\u00d7300) matrix, whereas the \u2018ada.8\u2019 method needs only a lookup table of size (8\u00d7 300)."}, {"heading": "3.3.4 LEARNED BINARY REPRESENTATION USED \u2018RAW\u2019", "text": "The foregoing results on binary representations was directed towards finding B such that E could be approximately reconstructed. These representations also require the transmission of a mapping matrix W from B to E\u2217.\nCompared to the memory saving achieved by using a pure binary representation of the dictionary, the overhead of including the matrix is small. On the other hand, one of the intuitions for wanting a binary representation is that it could provide better interpretability of the embedding itself.\nGiven the relative difficulty of obtaining these binary embeddings (compared to quantisation methods), only limited experimentation was possible. However, one simple test of interpretability is whether the binary representation B could be used as a vector embedding of the language on a stand-alone basis.\nSo, given the two binary embeddings produced by (i) quasi auto-encoding and (ii) binary PCA, the same cosine-similarity tests were performed as in the other experiments (simply using the values \u20180\u2019 and \u20181\u2019 in each place in the new embedding). These results are given in Table 2, and are discussed below."}, {"heading": "4 DISCUSSION", "text": ""}, {"heading": "4.1 LEVEL QUANTISATION VS. MORE \u2018SOPHISTICATED\u2019 METHODS", "text": "The level-quantisation approaches to compression work extremely well, and are relatively simple to implement. However, they do not accomplish the goal of learning about the underlying embedding through an efficient compression algorithm.\nIn an ideal world, the binary representations would have been more readily learned by gradient descent. However, a (wide) variety of \u2018tricks\u2019 had to be implemented in order to prevent the network from converging on solutions that avoided the goal of producing a good binary representation. This is partly due to the overall optimisation being under-constrained : A 300-element vector space is being fitted by a 1024-dimensional one (albeit the larger space is ultimately constrained to be binaryvalued). Some of this overfitting was solved through the addition of an additive noise layer, however that then lead to training times increasing."}, {"heading": "4.2 BINARY PCA OBSERVATIONS", "text": "The original motivation for searching for optimal \u2018space-folding\u2019 directions was to see whether those directions would have a revealing linguistic interpretation. Disappointingly, the process was found to be simply slicing and dicing the embedding into an ever-smaller noise sphere around the origin.\nRecognising the space-folding operation as simply a way of compressing distributions in \u2018on themselves\u2019 also explains why the number of \u2018on\u2019 bits in the binary PCA representations becomes so stable. Once the distribution of the values in an embedding dimension has become stable, the proportion of the distribution that gets folded-in at each iteration becomes constant.\nThere is a curious bump in the energy improvement in Figure 5 at iteration \u223c 300. This probably represents the initial round of modifications to each vector element finishing, and the next round of subdivision commencing (complete with the initial step of removing common offsets, which can also be observed at the very start of training). A similar effect occurs at iteration \u223c 600.\nOne proof-point that the binary PCA provides is that \u2013 using approximately the same \u2018bit budget\u2019 as the quantisation approaches \u2013 there is a binary representation that can be mapped purely linearly onto the original embedding with high fidelity."}, {"heading": "4.3 BINARY ENCODINGS FOR INTERPRETABILITY", "text": "Although the binary PCA approach proves that a binarised linear encoding is possible, the PCA process has little incentive to capture the semantic or syntatic structure in the embedding. As demonstrated by the results in Table 2, the standard mantra that auto-encodings learn useful representations is borne out. In fact, it is quite reassuring that the binary representation (which is just a by-product of fitting to an existing embedding) is itself a useful representation.\nEven if the raw binary representation isn\u2019t as useful as hoped on a standalone basis, one small consolation is that a binary-valued encoding enables matrix multiplication to be simply a process of conditional additions."}, {"heading": "5 CONCLUSION", "text": "The techniques described in this paper can be applied to any embedding, since nothing specific to GloVe has been used 3.\nIdeally, the search for a good compression method would have lead to a representation with a good level of interpretability. As an analogue, compare JPEG encoding (which is lossy in ways that the brain is less sensitive to), to zip encoding (which is generically optimal, without giving much insight into the nature of the data being compressed).\nHowever, in terms of the goal of compressing a representation to within a given \u2018bit budget\u2019 (notably 3-bits per vector dimension with <1% error over the baseline), the adaptive quantisation method outline here is certainly effective : It is relatively simple to implement, each element of the resulting representation remains independent, and it is possible to reconstruct the full embedding on-the-fly from the small look-up table that is required in addition to the level-index representation."}, {"heading": "ACKNOWLEDGMENTS", "text": "The author thanks DC Frontiers, a Singapore-based company that has created the data-centric service \u2018Handshakes\u2019 (http://www.handshakes.com.sg/), for their willingness to support this ongoing research. DC Frontiers is the recipient of a Technology Enterprise Commercialisation Scheme grant from SPRING Singapore, under which this work took place.\n3And, given the way the GloVe embedding is constructed, it may be particularly resistant to compression."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron C"], "venue": "CoRR, abs/1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Learning fast approximations of sparse coding", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Gregor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2010}, {"title": "Development of low entropy coding in a recurrent network. Network: computation", "author": ["Harpur", "George F", "Prager", "Richard W"], "venue": "in neural systems,", "citeRegEx": "Harpur et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Harpur et al\\.", "year": 1996}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "In this work, a 300 dimensional GloVe embedding (Pennington et al., 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 3, "context": ", 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al. (2013). 2 APPROACHES TO COMPRESSION The GloVe embeddings are produced by minimising the error in predicting the word co-occurrence matrix Xi,j from the word vectors associated with words i and j 1 :", "startOffset": 237, "endOffset": 259}, {"referenceID": 0, "context": "Addition of noise : A constant, additive noise term was found to aid training greatly (see also Bengio et al. (2013)).", "startOffset": 96, "endOffset": 117}, {"referenceID": 3, "context": "The word analogy task used here as the compression performance measure was as described in Mikolov et al. (2013), and the provided dataset contains 19,544 such questions, divided into semantic and syntactic subsets.", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "In Figure 2, the performance of a selection of 300-dimensional full-resolution embeddings is shown (as given in (Pennington et al., 2014)).", "startOffset": 112, "endOffset": 137}], "year": 2017, "abstractText": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. However, these vector space representations (created through large-scale text analysis) are typically stored verbatim, since their internal structure is opaque. Using word-analogy tests to monitor the level of detail stored in compressed rerepresentations of the same vector space, the trade-offs between the reduction in memory usage and expressiveness are investigated. A simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10, with only minimal impact on performance. Then, using the same \u2018bit budget\u2019, a binary (approximate) factorisation of the same space is also explored, with the aim of creating an equivalent representation with better interpretability.", "creator": "LaTeX with hyperref package"}}}