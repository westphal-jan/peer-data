{"id": "1511.06488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Resiliency of Deep Neural Networks under Quantization", "abstract": "the complexity of deep neural network simulations for hardware implementation can be much lowered by testing the word - length of weights and signals. direct quantization of value - point weights, however, has not show good performance when the number of bits assigned is small. retraining of quantized networks has been developed to relieve it problem. in this system, statistical effects of retraining are controlled for a feedforward deep neural junction ( ffdnn ) and bounded convolutional neural network ( cnn ). the network complexity is controlled to know their effects on meaningful improvement of quantized approaches by retraining. the complexity of the ffdnn is controlled by transforming the unit size in each hidden layer as the number of layers, while that of the server is done by modifying the feature map configuration. we wonder if the performance gap between the floating - point using the retrain - based ternary ( + 1, 0, - 1 ) deepest neural networks exists with a fair amount in'complexity limited'networks, but the discrepancy almost vanishes in specifically complex networks whose capability is limited by network training data, rather than by limiting number of connections. this research shows that highly complex models have the capability of absorbing the effects of severe load reductions through retraining, sometimes connection limited networks are less resilient.", "histories": [["v1", "Fri, 20 Nov 2015 04:55:46 GMT  (699kb,D)", "http://arxiv.org/abs/1511.06488v1", null], ["v2", "Wed, 25 Nov 2015 12:59:38 GMT  (919kb,D)", "http://arxiv.org/abs/1511.06488v2", null], ["v3", "Thu, 7 Jan 2016 13:50:22 GMT  (591kb,D)", "http://arxiv.org/abs/1511.06488v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["wonyong sung", "sungho shin", "kyuyeon hwang"], "accepted": false, "id": "1511.06488"}, "pdf": {"name": "1511.06488.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wonyong Sung", "Sungho Shin", "Kyuyeon Hwang"], "emails": ["wysung@snu.ac.kr", "shshin@dsp.snu.ac.kr", "kyuyeon.hwang@gmail.com"], "sections": [{"heading": null, "text": "The complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals. Direct quantization of floating-point weights, however, does not show good performance when the number of bits assigned is small. Retraining of quantized networks has been developed to relieve this problem. In this work, the effects of retraining are analyzed for a feedforward deep neural network (FFDNN) and a convolutional neural network (CNN). The network complexity is controlled to know their effects on the resiliency of quantized networks by retraining. The complexity of the FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of the CNN is done by modifying the feature map configuration. We find that the performance gap between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks exists with a fair amount in complexity limited\u2019 networks, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections. This research shows that highly complex DNNs have the capability of absorbing the effects of severe weight quantization through retraining, but connection limited networks are less resilient."}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks begin to find many real-time applications, such as speech recognition, autonomous driving, gesture recognition, and robotic control (Sak et al. (2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)). Although most of deep neural networks are implemented using GPUs (Graphics Processing Units) in these days, their implementation in hardware can give many benefits in terms of power consumption and system size (Ovtcharov et al. (2015)). FPGA based implementation examples of CNN show more than 10 times advantage in power consumption (Ovtcharov et al. (2015)).\nNeural network algorithms employ many multiply and add (MAC) operations that mimic the operations of biological neurons, which suggests that reconfigurable hardware arrays that contain quite homogeneous hardware blocks, such as MAC units, can give very efficient solution to real-time neural network system design. Our previous works show that the precision required for implementing FFDNN or CNN needs not be very high, especially when the quantized networks are trained again to learn the effects of lowered precision. In the fixed-point optimization examples shown in Hwang & Sung (2014); Anwar et al. (2015), neural networks with ternary weights showed quite good performance which is close to that of floating-point arithmetic, but not always.\nIn this work, we try to know if retraining can recover the performance of FFDNN and CNN under quantization with only ternary (+1, 0, -1) levels or 3 bits (+3, +2, +1, 0, -1, -2, -3) for weight representation. For this study, the network complexity is changed to analyze their effects on the performance gap between floating-point and low-precision retrained fixed-point deep neural networks.\nar X\niv :1\n51 1.\n06 48\n8v 1\n[ cs\n.L G\n] 2\n0 N\nov 2\nWe conduct our experiments with a feed-forward deep neural network (FFDNN) for phoneme recognition and a convolutional neural network (CNN) for image classification. To control the network size, not only the number of units in each layer but also the number of hidden layers are varied in the FFDNN. For the CNN, the number of feature maps for each layer and the number of layers are both changed. The FFDNN uses the TIMIT corpus and the CNN employs the CIFAR-10 dataset. This analysis intends to find an insight to the knowledge representation capability of highly quantized networks, and also provides a guideline to efficient network size and word-length design for hardware implementation of deep neural networks."}, {"heading": "2 RELATED WORK", "text": "Fixed-point implementation of signal processing algorithms has long been of interest for VLSI based design of multimedia and communication systems. Some of early works used statistical modeling of quantization noise for application to linear digital filters. The simulation-based word-length optimization method utilized simulation tools to evaluate the fixed-point performance of a system, by which non-linear algorithms can be optimized (Sung & Kum (1995)). Ternary (+1, 0, -1) coefficients based digital filters were used to eliminate multiplications at the cost of higher quantization noise. The implementation of adaptive filters with ternary weights were developed, but it demanded oversampling to remove the quantization effects (Hussain et al. (2007)).\nFixed-point neural network design also has been studied with the same purpose of reducing the hardware implementation cost (Moerland & Fiesler (1997)). In Holt & Baker (1991), back propagation simulation with 16-bit integer arithmetic was conducted for several problems, such as NetTalk, Parity, Protein and so on. The authors conducted the experiments while changing the number of hidden units, which was, however, relatively small numbers. The integer simulations showed quite good results for NetTalk and Parity, but not for Protein benchmarks. With direct quantization of trained weights, this work also confirmed satisfactory operation of neural networks with 8-bit precision. An implementation with ternary weights were reported for neural network design with optical fiber networks (Fiesler et al. (1990)). In this ternary network design, the authors employed retraining after direct quantization to improve the performance of a shallow network.\nRecently, fixed-point design of deep neural networks is revisited, and FFDNN and CNN with ternary weights show quite good performance that are very close to the floating-point results. The ternary weight based FFDNN and CNN are used for VLSI and FPGA based implementations, by which the algorithms can be operated with only on-chip memory consuming very low power (Kim et al. (2014)). Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al. (2015)). A network restructuring technique using singular value decomposition technique is also studied (Xue et al. (2013); Rigamonti et al. (2013))."}, {"heading": "3 FIXED-POINT FFDNN AND CNN DESIGN", "text": "In this section, the design of FFDNN and CNN with varying network complexity and the fixed-point optimization procedure are explained."}, {"heading": "3.1 FFDNN AND CNN DESIGN", "text": "A feedforward deep neural network with multiple hidden layers are depicted in Figure 1. Each layer k has a signal vector yk, which is propagated to the next layer by multiplying the weight matrix Wk+1, adding biases bk+1, and applying the activation function \u03c6k+1(\u00b7) as follows:\nyk+1 = \u03c6k+1 ( Wk+1yk + bk+1 ) . (1)\nOne of the most popular activation functions is the rectified linear unit defined as\nRelu(x) = max(0, x). (2)\nIn this work, an FFDNN for phoneme recognition is used. The reference DNN has four hidden layers. Each of the hidden layers has Nh units; the value of Nh is changed to control the complexity of the network. The number of hidden layers is also reduced. We conduct experiments with the Nh size of 32, 64, 128, 256, 512, and 1024. The input layer of the network has 1,353 units to accept 11 frames of a Fourier-transform-based filter-bank with 40 coefficients (+energy) distributed on a mel-scale, together with their first and second temporal derivatives. The output layer consists of 61 softmax units which correspond to 61 target phoneme labels. Phoneme recognition experiments were performed on the TIMIT corpus. The standard 462 speaker set with all SA records removed was used for training, and a separate development set of 50 speaker was used for early stopping. Results are reported for the 24-speaker core test set. The network was trained with backpropagation algorithm with 128 mini-batch size. Initial learning rate was 10\u22125 and it was decreased until 10\u22127 during the training. Momentum was 0.9 and RMSProp was adopted for weights update (Tieleman & Hinton (2012)). The dropout technique was employed with 0.2 dropout rate in each layer.\nThe CNN used is for CIFAR-10 dataset. It contains a training set of 50,000 and a test set of 10,000 32\u00d732 RGB color images representing airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships and trucks. We divided the training set to 40,000 images for training and 10,000 images for validation. This CNN has 3 convolution and pooling layers and a fully connected hidden layer with 64 units, and the output has 10 softmax units as shown in Figure 2. We control the number of feature maps in each convolution layer. The reference size has 32-32-64 feature maps as used in Krizhevskey (2014). We didn\u2019t preform any preprocessing and data augmentation such as ZCA whitening and global contrast normalization. To know the effects of network size variation, the number of feature maps is reduced or increased. The configurations of the feature maps used for the experiments are 8-8-16, 16-16-32, 32-32-64, 64-64-128, 96-96-192, and 128-128-256. The number of feature map layers is also changed, resulting in 32-32-64, 32-64, and 64 map configurations. Note that the fully connected layer is not changed. The network was trained with a backpropagation algorithm with 128 mini-batch size. Initial learning rate was 0.001 and it was decreased to 10\u22128 during the training procedure. Momentum was 0.8 and RMSProp was applied for weights update."}, {"heading": "3.2 FIXED-POINT OPTIMIZATION OF DNNS", "text": "Reducing the word-length of weights brings several advantages in hardware based implementation of neural networks. First, it lowers the arithmetic precision, and thereby reduces the number of gates needed for multipliers. Second, the size of memory for storing weights is minimized, which would be a big advantage when keeping them on a chip, instead of external DRAM or NAND flash memory. Note that FFDNNs and recurrent neural networks demand a very large number of weights. Third, the reduced arithmetic precision or minimization of off-chip memory accesses leads to low power consumption. However, we need to concern the quantization effects that degrades the system performance when reducing the word-length.\nDirect quantization converts a floating-point value to the closest integer number, which is conventionally used in signal processing system design. However, direct quantization usually demands more than 8 bits, and does not show good performance when the number of bits is small. In fixedpoint deep neural network design, retraining of quantized weights shows quite good performance.\nThe fixed-point DNN algorithm design consists of three steps: floating-point training, direct quantization, and retraining of weights. The floating-point training procedure can be any of the state of the art techniques, which may include unsupervised learning and dropout. Note that fixed-point optimization needs to be based on the best performing floating-point weights. Thus, the floating-point weight optimization may need to be conducted several times with different initializations, and this step consumes the most of the time. After the floating-point training, direct quantization is followed.\nFor direct quantization, uniform quantization function is employed and the function Q(\u00b7) is defined as follows :\nQ(w) = sgn(w) \u00b7\u2206 \u00b7min (\u230a |(w)|\n\u2206 + 0.5\n\u230b , M \u2212 1\n2\n) (3)\nwhere sgn(\u00b7) is a sign function, \u2206 is a quantization step size, and M represents the number of quantization levels. Note that M needs to be an odd number since the weight values can be positive or negative. When M is 7, the weights are represented by -3\u00b7\u2206, -2\u00b7\u2206, -1\u00b7\u2206, 0, +1\u00b7\u2206, +2\u00b7\u2206, +3\u00b7\u2206,which can be represented in 3 bits. The quantization step size \u2206 is determined to minimize the L2 error, E, depicted as follows.\nE = 1\n2 N\u2211 i=1 ( Q(wi)\u2212 wi )2 (4)\nwhere N is the number of weights in each weight group, wi is the i-th weight value represented in floating-point. This process needs some iteration, but does not take much time.\nFor network retraining, we maintain both floating-point and quantized weights because the amount of weight updates in each training step is much smaller than the quantization step size \u2206. The forward and backward propagation is conducted using quantized weights, but the weight update is applied to the floating-point weights and newly quantized values are generated at each iteration. This retraining procedure usually converges quickly and does not take much time when compared to the floating-point training."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 ANALYSIS OF DIRECT QUANTIZATION", "text": "The performance of the FFDNN and the CNN with directly quantized weights is analyzed while varying the number of units in each layer or the number of feature maps, respectively. In this analysis, the quantization is performed on each weight group, which is illustrated in Figure1 and 2, to know the sensitivity of word-length reduction. In this sub-section, we try to analyze the effects of direct quantization.\nThe quantized weight can be represented as follows,\nwqi = wi + w d i (5)\nwhere wdi is the distortion of each weight due to quantization. In the direct quantization, we can assume that the distortion wdi is not dependent each other.\nConsider a computation procedure for a unit in a hidden layer, the signal from the previous layer is summed up after multiplication with the weights as illustrated in Figure3. We can also assemble a model for distortion, which is shown in Figure3. In the distortion model, since wdi is independent each other, we can assume that the effects of the summed distortion is reduced according to the random process theory. This analysis means that the effects of the distortion is reduced when the number of units in the anterior layer increases, but slowly.\nFigure4 illustrates the performance of the FFDNN with floating-point arithmetic, 2-bit direct quantization of all the weights, and 2-bit direct quantization only on the weight group \u2019In-h1\u2019, \u2019h1-h2\u2019, and \u2019h4-out\u2019. Consider the quantization performance of the \u2019In-h1\u2019 layer, the phone-error rate is higher than the floating-point result with an almost constant amount, about 10%. Note that the number of input to the \u2019In-h1\u2019 layer is fixed, 1353, regardless of the hidden unit size. Thus, the amount of distortion delivered to each unit of the hidden layer 1 can be considered unchanged. Figure4 also shows the quantization performance on \u2019h1-h2\u2019, \u2019h3-h4\u2019, and \u2019h4-out\u2019 layers, which informs the trend of reduced gap to the floating-point performance as the network size increases. This can be explained by the sum of increased number of independent distortions when the network size grows. The performance of all 2-bit quantization also shows the similar trend of reduced gap to the floating-point performance. But, apparently, the performance of 2-bit directly quantized networks are not satisfactory.\nA similar analysis is conducted to the CNN with direct quantization when the number of feature maps increases or decreases. In the CNN, the number of input to each output is determined by the number of input feature maps and the kernel size. For example, at the first layer C1, the number of input signal for computing one output is only 75 (=3\u00d725) regardless of the network size, where the input map size is always 3 and the kernel size is 25. However, at the second layer C2, the number of input feature maps increases as the network size grows. When the feature map of 32-32-64 is considered, the number of input for the C2 layer grows to 800 (=32\u00d725). Thus, we can expect a reduced distortion as the number of feature maps increases.\nFigure5 shows the performance of direct quantization with 2, 4, 6, and 8-bit precision when the network complexity varies. In the FFDNN, 6 bit direct quantization seems enough when the size of the network is larger than 128. But, small FFDNNs demand 8 bits for near floating-point performance. The CNN in Figure5 also shows the similar trend. The direct quantization requires about 6 bits when the feature map configuration is 16-16-32 or larger."}, {"heading": "4.2 EFFECTS OF RETRAINING ON QUANTIZED NETWORKS", "text": "In order to make the DNN to learn the effects of quantization, we conduct retraining on the directly quantized networks using the same data used for floating-point training. The fixed-point performance of the FFDNN is shown in Figure6 when the number of hidden units in each layer varies. The performance of direct 2 bits (ternary levels), direct 3 bits (7-levels), retrain-based 2 bits, and retrain-based 3 bits are compared with the floating-point simulation. We can find that the performance gap between the floating-point and the retrain-based fixed-point networks converges very fast as the network size grows. Although the performance gap between the direct and the floating-point networks also converges, the rate of convergence is significantly different. In this figure, the performance of the floating-point network almost saturates when the network size is about 1024. Note that the TIMIT corpus that is used for training has only 3 hours of data. Thus, the network with 1024 hidden units can be considered in the \u2019training-data limited region\u2019. Here, the gap between the floating-point and fixed-point networks almost vanishes when the network is in the \u2019training-data limited region\u2019. However, when the network size is limited, such as 32, 64, 128, or 256, there is some performance gap between the floating-point and highly quantized networks even if retraining on the quantized networks is performed.\nThe similar experiments are conducted for the CNN with varying feature map sizes and the results are shown in Figure7. The configuration of the feature maps used for the experiments are 8-8-16, 16-16-32, 32-32-64, 64-64-128, 96-96-192, and 128-128-256. The size of the fully connected layer is not changed. In this figure, the floating-point and the fixed-point performances with retraining also converges very fast as the number of feature maps increases. The floating-point performance saturates when the feature map size is 128-128-256, and the gap is less than 1% when comparing the floating-point and the retrain-based 2-bit networks. However, also, there is some performance gap\nwhen the number of feature maps is reduced. This suggests that a fairly high performance feature extraction can be designed even using very low-precision weights if the number of feature maps can be increased."}, {"heading": "4.3 FIXED-POINT PERFORMANCES WHEN VARYING THE DEPTH", "text": "It is well known that increasing the depth usually results in positive effects on the performance of a DNN (Yu et al. (2012a)). The network complexity of a DNN is changed by increasing or reducing the number of hidden layers or feature map levels. The result of fixed-point and floating-point performances when varying the number of hidden layers for the FFDNN is summarized in Table 1. This table shows that both the floating-point and the fixed-point performances of the FFDNN increase when adding hidden layers from 0 to 4. The performance gap between the floating-point and the fixed-point networks shrinks as the number of levels increases.\nThe network complexity of the CNN is also varied by reducing the level of feature maps as shown in Table 2. As expected, the performance of both the floating-point and retrain-based low-precision networks degrades as the number of levels is reduced. The performance gap between them is very small with 7-level quantization for all feature map levels.\nThese results for the FFDNN and the CNN with varied number of levels also show that the effects of quantization can be much reduced by retraining when the network contains some redundant complexity."}, {"heading": "5 DISCUSSION", "text": "In this study, we control the network size by changing the number of units in the hidden layers, the number of feature maps, or the number of levels. At any case, reduced complexity lowers the resiliency to quantization. We are conducting similar experiments to the recurrent neural networks that are known to be more sensitive to quantization. This work seems to be directly related to several network optimization methods, such as pruning, fault tolerance, and decomposition. In the pruning, retraining of weights is conducted after zeroing small valued weights. The effects of pruning, fault tolerance, and network decomposition efficiency would be dependent on the redundant representation capability of DNNs.\nThis study can be applied to hardware efficient DNN design. For design with limited hardware resources, when the size of the reference DNN is relatively small, it is advised to employ a very low-precision arithmetic and, instead, increasing the network complexity as much as the hardware capacity allows. But, when the DNNs are in the performance saturation region, increasing the arithmetic precision gains slightly more because growing the \u2019alread-big\u2019 network size brings almost no performance advantages."}, {"heading": "6 CONCLUSION", "text": "We analyze the performance of fixed-point deep neural networks, an FFDNN for phoneme recognition and a CNN for image classification, while not only changing the arithmetic precision but also varying their network complexity. The low-precision networks for this analysis are obtained by using the retrain based quantization methods, and the network complexity is controlled by changing the configurations of the hidden layers or feature maps. The performance gap between the floatingpoint and the fixed-point neural networks with ternary weights (+1, 0, -1) almost vanishes when the DNNs are in the performance saturation region for the given training data. However, when the complexity of DNNs are reduced, by lowering either the number of units, feature maps, or hidden layers, the performance gap between them increases. In other words, a large size network that may contain redundant representation capability for the given training data does not hurt by the lowered precision, but a very compact network does."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grants funded by the Ministry of Education, Science and Technology (MEST), Republic of Korea (No. 2012R1A2A2A06047297)."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Anwar", "Sajid", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["Chen", "Chenyi", "Seff", "Ari", "Kornhauser", "Alain", "Xiao", "Jianxiong"], "venue": "arXiv preprint arXiv:1505.00256,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Robust control of robot arms via quasi sliding modes and neural networks", "author": ["Corradini", "Maria Letizia", "Giantomassi", "Andrea", "Ippoliti", "Gianluca", "Longhi", "Sauro", "Orlando", "Giuseppe"], "venue": "In Advances and Applications in Sliding Mode Control systems,", "citeRegEx": "Corradini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Corradini et al\\.", "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "arXiv preprint arXiv:1511.00363,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Weight discretization paradigm for optical neural networks", "author": ["Fiesler", "Emile", "Choudry", "Amar", "Caulfield", "H John"], "venue": "In The Hague\u201990,", "citeRegEx": "Fiesler et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fiesler et al\\.", "year": 1990}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": null, "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Back propagation simulations using limited precision calculations", "author": ["Holt", "Jordan L", "Baker", "Thomas E"], "venue": "In Neural Networks,", "citeRegEx": "Holt et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Holt et al\\.", "year": 1991}, {"title": "Short word-length lms filtering", "author": ["Hussain", "B Zahir M"], "venue": "In Signal Processing and Its Applications,", "citeRegEx": "Hussain and M,? \\Q2007\\E", "shortCiteRegEx": "Hussain and M", "year": 2007}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "Hwang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2014}, {"title": "Human computer interface using hand gesture recognition based on neural network", "author": ["Jalab", "Hamid A", "Omer", "Herman"], "venue": "In Information Technology: Towards New Smart World (NSITNSW),", "citeRegEx": "Jalab et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jalab et al\\.", "year": 2015}, {"title": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks", "author": ["Kim", "Jonghong", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Neural network adaptations to hardware implementations", "author": ["Moerland", "Perry", "Fiesler", "Emile"], "venue": "Technical report, IDIAP,", "citeRegEx": "Moerland et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Moerland et al\\.", "year": 1997}, {"title": "Accelerating deep convolutional neural networks using specialized hardware", "author": ["Ovtcharov", "Kalin", "Ruwase", "Olatunji", "Kim", "Joo-Young", "Fowers", "Jeremy", "Strauss", "Karin", "Chung", "Eric S"], "venue": "Microsoft Research Whitepaper,", "citeRegEx": "Ovtcharov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ovtcharov et al\\.", "year": 2015}, {"title": "Learning separable filters", "author": ["Rigamonti", "Roberto", "Sironi", "Amos", "Lepetit", "Vincent", "Fua", "Pascal"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rigamonti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rigamonti et al\\.", "year": 2013}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Sak", "Ha\u015fim", "Senior", "Andrew", "Rao", "Kanishka", "Beaufays", "Fran\u00e7oise"], "venue": "arXiv preprint arXiv:1507.06947,", "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Simulation-based word-length optimization method for fixed-point digital signal processing systems", "author": ["Sung", "Wonyong", "Kum", "Ki-II"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Sung et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Sung et al\\.", "year": 1995}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Xue", "Jian", "Li", "Jinyu", "Gong", "Yifan"], "venue": "In INTERSPEECH,", "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}, {"title": "More data + deeper model = better accuracy", "author": ["Yu", "Dong", "Deng", "Alex Acero", "Dahl", "George", "Seide", "Frank", "Li", "Gang"], "venue": "In keynote at International Workshop on Statistical Machine Learning for Speech Processing,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Yu", "Dong", "Seide", "Frank", "Li", "Gang", "Deng"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "Deep neural networks begin to find many real-time applications, such as speech recognition, autonomous driving, gesture recognition, and robotic control (Sak et al. (2015); Chen et al.", "startOffset": 154, "endOffset": 172}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al.", "startOffset": 8, "endOffset": 48}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)).", "startOffset": 8, "endOffset": 73}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)). Although most of deep neural networks are implemented using GPUs (Graphics Processing Units) in these days, their implementation in hardware can give many benefits in terms of power consumption and system size (Ovtcharov et al. (2015)).", "startOffset": 8, "endOffset": 310}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)). Although most of deep neural networks are implemented using GPUs (Graphics Processing Units) in these days, their implementation in hardware can give many benefits in terms of power consumption and system size (Ovtcharov et al. (2015)). FPGA based implementation examples of CNN show more than 10 times advantage in power consumption (Ovtcharov et al. (2015)).", "startOffset": 8, "endOffset": 434}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)). Although most of deep neural networks are implemented using GPUs (Graphics Processing Units) in these days, their implementation in hardware can give many benefits in terms of power consumption and system size (Ovtcharov et al. (2015)). FPGA based implementation examples of CNN show more than 10 times advantage in power consumption (Ovtcharov et al. (2015)). Neural network algorithms employ many multiply and add (MAC) operations that mimic the operations of biological neurons, which suggests that reconfigurable hardware arrays that contain quite homogeneous hardware blocks, such as MAC units, can give very efficient solution to real-time neural network system design. Our previous works show that the precision required for implementing FFDNN or CNN needs not be very high, especially when the quantized networks are trained again to learn the effects of lowered precision. In the fixed-point optimization examples shown in Hwang & Sung (2014); Anwar et al.", "startOffset": 8, "endOffset": 1027}, {"referenceID": 0, "context": "In the fixed-point optimization examples shown in Hwang & Sung (2014); Anwar et al. (2015), neural networks with ternary weights showed quite good performance which is close to that of floating-point arithmetic, but not always.", "startOffset": 71, "endOffset": 91}, {"referenceID": 3, "context": "An implementation with ternary weights were reported for neural network design with optical fiber networks (Fiesler et al. (1990)).", "startOffset": 108, "endOffset": 130}, {"referenceID": 3, "context": "An implementation with ternary weights were reported for neural network design with optical fiber networks (Fiesler et al. (1990)). In this ternary network design, the authors employed retraining after direct quantization to improve the performance of a shallow network. Recently, fixed-point design of deep neural networks is revisited, and FFDNN and CNN with ternary weights show quite good performance that are very close to the floating-point results. The ternary weight based FFDNN and CNN are used for VLSI and FPGA based implementations, by which the algorithms can be operated with only on-chip memory consuming very low power (Kim et al. (2014)).", "startOffset": 108, "endOffset": 654}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)).", "startOffset": 64, "endOffset": 90}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al.", "startOffset": 64, "endOffset": 332}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al. (2015)).", "startOffset": 64, "endOffset": 351}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al. (2015)). A network restructuring technique using singular value decomposition technique is also studied (Xue et al. (2013); Rigamonti et al.", "startOffset": 64, "endOffset": 467}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al. (2015)). A network restructuring technique using singular value decomposition technique is also studied (Xue et al. (2013); Rigamonti et al. (2013)).", "startOffset": 64, "endOffset": 492}, {"referenceID": 18, "context": "It is well known that increasing the depth usually results in positive effects on the performance of a DNN (Yu et al. (2012a)).", "startOffset": 108, "endOffset": 126}], "year": 2017, "abstractText": "The complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals. Direct quantization of floating-point weights, however, does not show good performance when the number of bits assigned is small. Retraining of quantized networks has been developed to relieve this problem. In this work, the effects of retraining are analyzed for a feedforward deep neural network (FFDNN) and a convolutional neural network (CNN). The network complexity is controlled to know their effects on the resiliency of quantized networks by retraining. The complexity of the FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of the CNN is done by modifying the feature map configuration. We find that the performance gap between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks exists with a fair amount in complexity limited\u2019 networks, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections. This research shows that highly complex DNNs have the capability of absorbing the effects of severe weight quantization through retraining, but connection limited networks are less resilient.", "creator": "LaTeX with hyperref package"}}}