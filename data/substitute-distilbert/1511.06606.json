{"id": "1511.06606", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Data Representation and Compression Using Linear-Programming Approximations", "abstract": "we propose ` dracula ', a new semantics for unsupervised feature selection, sequential data such as wikipedia. dracula learns textual dictionary of $ n $ - grams that efficiently compresses a noisy corpus and recursively constructing its own dictionary ; in effect, dracula is a ` deep'extension of compressive feature knowledge. it implies solving a binary linear program that may be relaxed to a linear program. both theories exhibit considerable structure, their solution paths are well behaved, and we learn parameters which control the depth and diversity in the dictionary. we may discuss how to derive features from the compressed documents and show that while certain incomplete grammar models are invariant between the structure of the compressed dictionary, this structure may be used to regularize learning. properties are presented that demonstrate the efficacy of dracula's features.", "histories": [["v1", "Fri, 20 Nov 2015 14:21:44 GMT  (56kb,D)", "https://arxiv.org/abs/1511.06606v1", null], ["v2", "Tue, 19 Jan 2016 22:37:58 GMT  (60kb,D)", "http://arxiv.org/abs/1511.06606v2", null], ["v3", "Tue, 23 Feb 2016 23:13:18 GMT  (60kb,D)", "http://arxiv.org/abs/1511.06606v3", null], ["v4", "Mon, 2 May 2016 11:06:31 GMT  (190kb,D)", "http://arxiv.org/abs/1511.06606v4", null], ["v5", "Tue, 3 May 2016 01:02:04 GMT  (190kb,D)", "http://arxiv.org/abs/1511.06606v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hristo s paskov", "john c mitchell", "trevor j hastie"], "accepted": true, "id": "1511.06606"}, "pdf": {"name": "1511.06606.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hristo S. Paskov", "John C. Mitchell"], "emails": ["hpaskov@stanford.edu", "jcm@stanford.edu", "hastie@stanford.edu"], "sections": [{"heading": null, "text": "We propose \u2018Dracula\u2019, a new framework for unsupervised feature selection from sequential data such as text. Dracula learns a dictionary of n-grams that efficiently compresses a given corpus and recursively compresses its own dictionary; in effect, Dracula is a \u2018deep\u2019 extension of Compressive Feature Learning. It requires solving a binary linear program that may be relaxed to a linear program. Both problems exhibit considerable structure, their solution paths are well behaved, and we identify parameters which control the depth and diversity of the dictionary. We also discuss how to derive features from the compressed documents and show that while certain unregularized linear models are invariant to the structure of the compressed dictionary, this structure may be used to regularize learning. Experiments are presented that demonstrate the efficacy of Dracula\u2019s features."}, {"heading": "1 INTRODUCTION", "text": "At the core of any successful machine learning problem is a good feature representation that highlights salient properties in the data and acts as an effective interface to the statistical model used for inference. This paper focuses on using classical ideas from compression to derive useful feature representations for sequential data such as text. The basic tenets of compression abound in machine learning: the minimum description length principle can be used to justify regularization as well as various model selection criteria (Gabrilovich & Markovitch (2004)), while for unsupervised problems deep autoencoders (Salakhutdinov (2009)) and the classical K-means algorithm both seek a parsimonious description of data. Meanwhile, off-the-shelf compressors, such as LZ-77 (Ziv & Lempel (1977)), have been successfully applied to natural language problems as kernels that compute pairwise document similarities (Bratko et al. (2006)).\nWe propose a new framework, Dracula, so called because it simultaneously finds a useful data representation and compression using linear-programming approximations of the criterion that motivates dictionary-based compressors like LZ-77 (Ziv & Lempel (1977)). Dracula finds an explicit feature representation for the documents in a corpus by learning a dictionary of n-grams that is used to losslessly compress the corpus. It then recursively compresses the dictionary. This recursion makes Dracula a deep extension of Compressive Feature Learning (CFL) (Paskov et al. (2013)) that can find exponentially smaller representations and promotes similar n-grams to enter the dictionary. As noted in (Paskov et al. (2013)), feature representations derived from off-the-shelf compressors are inferior because the algorithms used are sensitive to document order; both Dracula and CFL are invariant to document order.\nOur framework is expressed as a binary linear program (BLP) that can viewed as a linear program (LP) over a sufficiently constrained polyhedron or relaxed to an LP by relaxing the integrality constraints. This is a notable departure from traditional deep learners (Salakhutdinov (2009); Socher & Manning (2013); LeCun et al. (2006)), which are formulated as non-convex, non-linear optimization problems. This structure makes it possible to analyze Dracula in view of well known techniques from convex analysis (e.g. the KKT conditions), polyhedral combinatorics, and graph theory. For example, we show that Dracula is easily parameterized to control the depth and diversity of its dictionary and that its solutions are well behaved as its parameters vary.\nar X\niv :1\n51 1.\n06 60\n6v 5\n[ cs\n.L G\n] 3\nM ay\n2 01\n6\nThis paper introduces Dracula in section 2 and discusses some of its problem structure and computational properties, including its NP-Completeness. Section 3 uses Dracula\u2019s polyhedral interpretation to explore the compressed representations it finds as its storage cost model varies. It also discusses how to extract features directly from a compression and how to integrate dictionary structure into the features. Finally, section 4 provides empirical evidence that deep compression finds hierarchical structure in data that is useful for learning and compression, and section 5 concludes."}, {"heading": "2 DRACULA", "text": "This section introduces Dracula by showing how to extend CFL to a deep architecture that compresses its own dictionary elements. We also show how to interpret any Dracula solution as a directed acyclic graph (DAG) that makes precise the notion of depth and provides useful statistical insights. Finally, we prove that Dracula is NP-Complete and discuss linear relaxation schemes.\nNotation Throughout this paper \u03a3 is a fixed finite alphabet and C = {D1, . . . , DN} is a fixed document corpus with each Dk \u2208 C a string Dk = ck1 . . . ckj of characters cki \u2208 \u03a3. An n-gram is any substring of some Dk and S is the set of all n-grams in the document corpus, including the original documents. For any s \u2208 S a pointer p is a triple p = (s, l \u2208 {1, . . . , |s|}, z \u2208 S) indicating that z = sl . . . sl+|z|\u22121. We say that p uses z at location l in s. Let P be the set of all valid pointers and for any P \u2282 P we use P (s) = {p \u2208 P |p = (s, l, z)} to select pointers whose first element is s, e.g. P = \u222as\u2208SP(s). Moreover, P uses z \u2208 S if there is some p \u2208 P using z, and P reconstructs s \u2208 S if every location in s is covered by at least one pointer, i.e. \u222a(s,l,v)\u2208P (s){l, . . . , l + |v| \u2212 1} = {1, . . . , |s|}. Conceptually, s is recovered from P by iterating through the (s, l, v) \u2208 P and \u201dpasting\u201d a copy of v into location l of a blank string. It will be helpful to define PC = \u222as\u2208CP(s) to be the set of pointers that can only be used to reconstruct the corpus."}, {"heading": "2.1 CFL", "text": "CFL represents document corpus C by storing a dictionary S \u2282 S, a set of n-grams, along with a pointer set P \u2282 PC that only uses dictionary n-grams and losslessly reconstructs each of the documents in C. Importantly, CFL stores the dictionary directly in plaintext. The overall representation is chosen to minimize its total storage cost for a given storage cost model that specifies ds, the cost of including n-gram s \u2208 S in the dictionary, as well as cp, the cost of including pointer p \u2208 PC in the pointer set. Selecting an optimal CFL representation may thus be expressed as\nminimize S\u2282S,P\u2282PC \u2211 p\u2208P cp + \u2211 s\u2208S ds subject to P reconstructs Dk \u2200Dk \u2208 C; P only uses s \u2208 S. (1)\nThis optimization problem naturally decomposes into subproblems by observing that when the dictionary is fixed, selecting the optimal pointer set decouples into |C| separate problems of optimally reconstructing each corpus document. We thus define the reconstruction module for document Dk \u2208 C, which takes as input a dictionary S and outputs the minimum cost of reconstructing Dk with pointers that only use strings in S. Note that specific pointers and dictionary strings can be disallowed by setting their respective costs to\u221e. For example setting ds = \u221e for all s \u2208 S longer than a certain length limits the size of dictionary n-grams. Of course, in practice, any variables with infinite costs are simply disregarded.\nThe reconstruction module can be expressed as a BLP by associating with every pointer p \u2208 P(Dk) a binary indicator variable wp \u2208 {0, 1} whereby wp = 1 indicates that p is included in the optimal pointer set for Dk. We similarly use binary variables ts \u2208 {0, 1} to indicate that s \u2208 S is included in the dictionary. Since there is a one-to-one correspondence between pointer sets (dictionaries) and w \u2208 {0, 1}|P(Dk)| (t \u2208 {0, 1}|S |), the vector storing the wp (ts), we will directly refer to these vectors as pointer sets (dictionaries). Lossless reconstruction is encoded by the constraint XDkw \u2265 1 where XDk \u2208 {0, 1}|Dk|\u00d7|P(Dk)| is a binary matrix indicating the indices of Dk that each pointer can reconstruct. In particular, for every p = (Dk, l, z) \u2208 P(Dk), column XDkp is all zeros except for a contiguous sequence of 1\u2019s in indices l, . . . , l + |z| \u2212 1. Control of which pointers may be used (based on the dictionary) is achieved by the constraint w \u2264 V Dkt where V Dk \u2208 {0, 1}|P(Dk)|\u00d7|S| contains a row for every pointer indicating the string it uses. In particular,\nfor every p = (Dk, l, z), V Dkp,z = 1 is the only non-zero entry in the row pertaining to p. The BLP may now be expressed as\nRDk(t; c) = minimize w\u2208{0,1}|P(Dk)| \u2211 p\u2208P(Dk) wpcp subject to XDkw \u2265 1; w \u2264 V Dkt. (2)\nThe optimization problem corresponding to an optimal CFL representation may now be written as a BLP by sharing the dictionary variable t among the reconstruction modules for all documents in C:\nminimize t\u2208{0,1}|S| \u2211 Dk\u2208C RDk(t, c) + \u2211 s\u2208S tsds (3)"}, {"heading": "2.2 ADDING DEPTH WITH DRACULA", "text": "The simplicity of CFL\u2019s dictionary storage scheme is a fundamental shortcoming that is demonstrated by the string aa . . . a consisting of the character a replicated 22n times. Let the cost of using any pointer be cp = 1 and the cost of storing any dictionary n-gram be its length, i.e. ds = |s|. The best CFL can do is to store a single dictionary element of length 2n and repeat it 2n times, incurring a total storage cost of 2n+1. In contrast, a \u201cdeep\u201d compression scheme that recursively compresses its own dictionary by allowing dictionary strings to be represented using pointers attains exponential space savings relative to CFL. In particular, the deep scheme constructs dictionary strings of length 2, 4, . . . , 22n\u22121 recursively and incurs a total storage cost of 4n 1.\nDracula extends CFL precisely in this hierarchical manner by allowing dictionary strings to be expressed as a combination of characters and pointers from shorter dictionary strings. CFL thus corresponds to a shallow special case of Dracula which only uses characters to reconstruct dictionary n-grams. This depth allows Dracula to leverage similarities among the dictionary strings to obtain further compression of the data. It also establishes a hierarchy among dictionary strings that allows us to interpret Dracula\u2019s representations as a directed acyclic graph (DAG) that makes precise the notion of representation depth.\nFormally, a Dracula compression (compression for brevity) of corpus C is a triple D = (S \u2282 S, P \u2282 PC , P\u0302 \u2282 P) consisting of dictionary, a pointer set P that reconstructs the documents in C, and a pointer set P\u0302 that reconstructs every dictionary string in S. As with CFL, any pointers in P may only use strings in S. However, a pointer p \u2208 P\u0302 reconstructing a dictionary string s \u2208 S is valid if it uses a unigram (irrespective of whether the unigram is in S) or a proper substring of s that is in S. This is necessary because unigrams take on the special role of characters for dictionary strings. They are the atomic units of any dictionary, so the character set \u03a3 is assumed to be globally known for dictionary reconstruction. In contrast, document pointers are not allowed to use characters and may only use a unigram if it is present in S; this ensures that all strings used to reconstruct the corpus are included in the dictionary for use as features.\nFinding an optimal Dracula representation may also be expressed as a BLP through simple modifications of CFL\u2019s objective function. In essence, the potential dictionary strings in S are treated like documents that only need to be reconstructed if they are used by some pointer. We extend the storage cost model to specify costs cp for all pointers p \u2208 PC used for document reconstruction as well as costs c\u0302p for all pointers p \u2208 P used for dictionary reconstruction. In keeping with the aformentioned restrictions we assume that c\u0302p = \u221e if p = (s, 1, s) illegally tries to use s to reconstruct s and s is not a unigram. The dictionary cost ds is now interpreted as the \u201coverhead\u201d cost of including s \u2208 S in the dictionary without regard to how it is reconstructed; CFL uses the ds to also encode the cost of storing s in plaintext (e.g. reconstructing it only with characters). Finally, we introduce dictionary reconstruction modules as analogs to the (document) reconstruction modules for dictionary strings: the reconstruction module for s \u2208 S takes as input a dictionary and outputs the cheapest valid reconstruction of s if s needs to be reconstructed. This can be written as the BLP\nR\u0302s(t; c\u0302) = minimize w\u2208{0,1}|P(s)| \u2211 p\u2208P(s) wpc\u0302p subject to Xsw \u2265 ts1; w \u2264 V\u0302 st. (4)\n1Note that the recursive model is allowed to use pointers in the dictionary and therefore selects from a larger pointer set than CFL. Care must be taken to ensure that the comparison is fair since the \u201csize\u201d of a compression is determing by the storage cost model and we could \u201ccheat\u201d by setting all dictionary pointer costs to 0. Setting all pointer costs to 1 ensures fairness.\nHere Xs is analogously defined as in equation (4) and V\u0302 s is analogous to V s in equation (4) except that it does not contain any rows for unigram pointers. With this setup in mind, the optimization problem corresponding to an optimal Dracula representation may be written as the BLP\nminimize t\u2208{0,1}|S| \u2211 Dk\u2208C RDk(t, c) + \u2211 s\u2208S [ tsds + R\u0302s(t; c\u0302) ] (5)\nFinally, any compression can be interpreted graphically as, and is equivalent to, a DAG whose vertices correspond to members of \u03a3, S, or C and whose labeled edge set is determined by the pointers: for every (s, l, z) \u2208 P or P\u0302 there is a directed edge from z to s with label l. Note that D defines a multi-graph since there may be multiple edges between nodes. Figure 1 shows the graph corresponding to a simple compression. As this graph encodes all of the information stored by D, and vice versa, we will at times treat D directly as a graph. Since D has no cycles, we can organize its vertices into layers akin to those formed by deep neural networks and with connections determined by the pointer set: layer 0 consists only of characters (i.e. there is a node for every character in \u03a3), layer 1 consists of all dictionary n-grams constructed solely from characters, higher levels pertain to longer dictionary n-grams, and the highest level consists of the document corpus C. While there are multiple ways to organize the intermediate layers, a simple stratification is obtained by placing s \u2208 S into layer i only if P\u0302 (s) uses a string in layer i\u22121 and no strings in layers i+1, . . . . We note that our architecture differs from most conventional deep learning architectures which tend to focus on pairwise layer connections \u2013 we allow arbitrary connections to higher layers."}, {"heading": "2.3 COMPUTATIONAL HARDNESS AND RELAXATION", "text": "The document and dictionary reconstruction modules RDk/R\u0302s are the basic building blocks of Dracula; when dictionary t is fixed, solving equation (5) is tantamount to solving the reconstruction modules separately. The discussion in the Appendix section A.1 shows that for a fixed binary t, solving RDk or R\u0302s is easy because of the structure of the constraint matrices X\nDk/Xs. In fact, this problem is equivalent to a min-cost flow problem. Similarly, if the pointer sets are known for each document or dictionary string then it is easy to find the corresponding dictionary t by checking which strings are used (in linear time relative to the number of pointers). One would hope that the easiness of Dracula\u2019s subproblems leads to an easy overall learning problem. However, learning the dictionary and pointer sets simultaneously makes this problem hard: Dracula is NPComplete. In particular, it requires solving a binary LP (which are NP-Complete in general) and it generalizes CFL which is itself NP-Complete (Paskov et al. (2014)) (see section 3.1.1 for how to restrict representations to be shallow).\nWe thus turn to solving Dracula approximately via its LP relaxation. This is obtained by replacing all binary constraints in equations (2),(4),(5) with interval constraints [0, 1]. We let QC denote this LP\u2019s constraint polyhedron and note that it is a subset of the unit hypercube. Importantly, we may also interpret the original problem in equation (5) as an LP over a polyhedron Q whose vertices are always binary and hence always has binary basic solutions. Here Q2 is the convex hull of all (binary) Dracula solutions andQ \u2282 QC ; all valid Dracula solutions may be obtained from the linear relaxation. In fact, the Chva\u0301tal-Gomory theorem (Schrijver (2003)) shows that we may \u201cprune\u201d\n2Note that unlike QC , this polyhedron is likely to be difficult to describe succinctly unless P = NP .\nQC into Q by adding additional constraints. We describe additional constraints in the Appendix section A.1.1 that leverage insights from suffix trees to pruneQC into a tighter approximationQ\u2032C \u2282 QC of Q. Remarkably, when applied to natural language data, these constraints allowed Gurobi (Gurobi Optimization (2015)) to quickly find optimal binary solutions. While we did not use these binary solutions in our learning experiments, they warrant further investigation.\nAs the pointer and dictionary costs vary, the resulting problems will vary in difficulty as measured by the gap between the objectives of the LP and binary solutions. When the costs force either t or the wDk/ws to be binary, our earlier reasoning shows that the entire solution will lie on a binary vertex of QC that is necessarily optimal for the corresponding BLP and the gap will be 0. This reasoning also shows how to round any continuous solution into a binary one by leveraging the easiness of the individual subproblems. First set all non-zero entries in t to 1, then reconstruct the documents and dictionary using this dictionary to yield binary pointers, and finally find the minimum cost dictionary based on which strings are used in the pointers."}, {"heading": "3 LEARNING WITH COMPRESSED FEATURES", "text": "This section explores the feature representations and compressions that can be obtained from Dracula. Central to our discussion is the observation of section 2.3 that all compressions obtained from Dracula are the vertices of a polyhedron. Each of these vertices can be obtained as the optimal compression for an appropriate storage cost model3, so we take a dual perspective in which we vary the storage costs to characterize which vertices exist and how they relate to one another. The first part of this section shows how to \u201cwalk\u201d around the surface of Dracula\u2019s polyhedron and it highlights some \u201clandmark\u201d compressions that are encountered, including ones that lead to classical bag-of-n-grams features. Our discussion applies to both, the binary and relaxed, versions of Dracula since the former can viewed as an LP over a polyhedron Q with only binary vertices. The second part of this section shows how to incorporate dictionary structure into features via a dictionary diffusion process.\nWe derive features from a compression in a bag-of-n-grams (BoN) manner by counting the number of pointers that use each dictionary string or character. It will be useful to explicitly distinguish between strings and characters when computing our representations and we will use squares brackets to denote the character inside a unigram, e.g. [c] . Recall that given a compression D = (S, P, P\u0302 ), a unigram pointer in P (used to reconstruct a document) is interpreted as a string whereas a unigram pointer in P\u0302 is interpreted as a character. We refer to any z \u2208 S \u222a \u03a3 as a feature and associate with every document Dk \u2208 C or dictionary string s \u2208 S a BoN feature vector xDk , xs \u2208 Z|S|+|\u03a3|+ , respectively. Entry xDkz counts the number of pointers that use z to reconstruct Dk, i.e. x Dk z = | {p \u2208 P (s)| p = (Dk, l, z)} |, and will necessarily have xDkz = 0 for all z \u2208 \u03a3. Dictionary strings are treated analogously with the caveat that if p = (s, l, z) \u2208 P\u0302 uses a unigram, p counts towards the character entry xDk[z] , not x Dk z ."}, {"heading": "3.1 DRACULA\u2019S SOLUTION PATH", "text": "Exploring Dracula\u2019s compressions is tantamount to varying the dictionary and pointer costs supplied to Dracula. When these costs can be expressed as continuous functions of a parameter \u03bb \u2208 [0, 1], i.e. \u2200s \u2208 S, p \u2208 PC , p\u0302 \u2208 P the cost functions ds(\u03bb), cp(\u03bb), c\u0302p\u0302(\u03bb) are continuous, the optimal solution sets vary in a predictable manner around the surface of Dracula\u2019s constraint polyhedron Q or the polyhedron of its relaxation QC . We use F (Q) to denote the set of faces of polyhedron Q (including Q), and take the dimension of a face to be the dimension of its affine hull. We prove the following theorem in the Appendix section A.3:\nTheorem 1. Let Q \u2282 Rd be a bounded polyhedron with nonempty interior and b : [0, 1] \u2192 Rd a continuous function. Then for someN \u2208 Z+\u222a{\u221e} there exists a countable partition \u0393 = {\u03b3i}Ni=0 of [0, 1] with corresponding faces Fi \u2208 F (Q) satisfying Fi 6= Fi+1 and Fi\u2229Fi+1 6= \u2205. For all \u03b1 \u2208 \u03b3i, the solution set of the LP constrained by Q and using cost vector b(\u03b1) is Fi = arg minx\u2208Q x\nT b(\u03b1). Moreover, Fi never has the same dimension as Fi+1 and the boundary between \u03b3i, \u03b3i+1 is )[ iff dimFi < dimFi+1 and ]( otherwise.\n3The storage costs pertaining to each vertex form a polyhedral cone, see (Ziegler (1995)) for details.\nThis theorem generalizes the notion of a continuous solution path typically seen in the context of regularization (e.g. the Lasso) to the LP setting where unique solutions are piecewise constant and transitions occur by going through values of \u03bb for which the solution set is not unique. For instance, suppose that vertex v0 is uniquely optimal for some \u03bb0 \u2208 [0, 1), another vertex v1 is uniquely optimal for a \u03bb0 < \u03bb1 \u2264 1, and no other vertices are optimal in (\u03bb0, \u03bb1). Then Theorem 1 shows that v0 and v1 must be connected by a face (typically an edge) and there must be some \u03bb \u2208 (\u03bb0, \u03bb1) for which this face is optimal. As such, varying Dracula\u2019s cost function continuously ensures that the solution set for the binary or relaxed problem will not suddenly \u201cjump\u201d from one vertex to the next; it must go through an intermediary connecting face. This behavior is depicted in Figure 2 on a nonlinear projection of Dracula\u2019s constraint polyhedron for the string \u201cxaxabxabxacxac\u201d.\nIt is worthwhile to note that determining the exact value of \u03bb for which the face connecting v0 and v1 is optimal is unrealistic in practice, so transitions may appear abrupt. While it is possible to smooth this behavior by adding a strongly convex term to the objective (e.g. an L2 penalty), the important insight provided by this theorem is that the trajectory of the solution path depends entirely on the combinatorial structure of Q or QC . This structure is characterized by the face lattice4 of the polyhedron and it shows which vertices are connected via edges, 2-faces, . . . , facets. It limits, for example, the set of vertices reachable from v0 when the costs vary continuously and ensure that transitions take place only along edges 5. This predictable behavior is desirable when fine tuning the compression for a learning task, akin to how one might tune the regularization parameter of a Lasso, and it is not possible to show in general for non-convex functions.\n4We leave it as an open problem to analytically characterize Dracula\u2019s face lattice. 5Restricting transitions only to edges is possible with probability 1 by adding a small amount of Gaussian\nnoise to c.\nWe now provide a simple linear cost scheme that has globally predictable effects on the dictionary. For all s \u2208 S, p \u2208 PC , p\u0302 \u2208 P we set ds = \u03c4 , cp = 1, c\u0302p\u0302 = \u03b1\u03bb if p\u0302 uses as unigram (i.e. is a character), and c\u0302p\u0302 = \u03bb otherwise. We constrain \u03c4, \u03bb \u2265 0 and \u03b1 \u2208 [0, 1]. In words, all document pointer costs are 1, all dictionary costs \u03c4 , and dictionary pointer costs are \u03bb if they use a string and \u03b1\u03bb if they use a character. The effects these parameters have on the compression may be understood by varying a single parameter and holding all others constant:\nVarying \u03c4 controls the minimum frequency with which s \u2208 S must be used before it enters the dictionary; if few pointers use s it is cheaper to construct s \u201cin place\u201d using shorter n-grams. Long n-grams appear less frequently so \u2191 \u03c4 biases the dictionary towards shorter n-grams. Varying \u03bb has a similar effect to \u03c4 in that it becomes more expensive to construct s as \u03bb increases, so the overall cost of dictionary membership increases. The effect is more nuanced, however, since the manner in which s is constructed also matters; s is more likely to enter the dictionary if it shares long substrings with existing dictionary strings. This suggests a kind of grouping effect whereby groups of strings that share many substrings are likely to enter together.\nVarying \u03b1 controls the Dracula\u2019s propensity to use characters in place of pointers in the dictionary and thereby directly modulates dictionary depth. When \u03b1 < 1K for K = 2, 3, . . . , all dictionary n-grams of length at most K are constructed entirely from characters."}, {"heading": "3.1.1 LANDMARKS ON DRACULA\u2019S POLYHEDRON", "text": "While Dracula\u2019s representations are typically deep and space saving, it is important to note that valid Dracula solutions include all of CFL\u2019s solutions as well as a set of fully redundant representations that use as many pointers as possible. The BoN features computed from these \u201cspace maximizing\u201d compressions yield the traditional BoN features containing all n-grams up to a maximum length K. A cost scheme that includes all pointers using all n-grams up to length K is obtained by setting all costs to be negative, except for ts = \u221e for all s \u2208 S where |s| > K (to disallow these strings). The optimal compression then includes all pointers with negative cost and each document position is reconstructed K times. Moreover, it is possible to restrict representations to be valid CFL solutions by disallowing all non-unigram pointers for dictionary reconstruction, i.e. by setting c\u0302p =\u221e if p is not a single character string."}, {"heading": "3.2 DICTIONARY DIFFUSION", "text": "We now discuss how to incorporate dictionary information from a compression D = (S, P, P\u0302 ) into the BoN features for each corpus document. It will be convenient to store the BoN feature vectors xDk for each document as rows in a feature matrix X \u2208 Z|C|\u00d7(|S|+|\u03a3|) and the BoN feature vectors xs for each dictionary string as rows in a feature matrix G \u2208 Z(|S|+|\u03a3|)\u00d7(|S|+|\u03a3|). We also include rows of all 0\u2019s for every character in \u03a3 to make G a square matrix for mathematical convenience. Graphically, this procedure transforms D into a simpler DAG, DR, by collapsing all multi-edges into single edges and labeling the resulting edges with an appropriate xsz . For any two features s, z, we say that s is higher (lower) order than z if it is a successor (predecessor) of z in D.\nOnce our feature extraction process throws away positional information in the pointers higher order features capture more information than their lower order constituents since the presence of an s \u2208 S formed by concatenating features z1 . . . zm indicates the order in which the zi appear and not just that they appear. Conversely, since each zi appears in the same locations as s (and typically many others), we can obtain better estimates for coefficients associated with zi than for the coefficient of s. If the learning problem does not require the information specified by s we pay an unnecessary cost in variance by using this feature over the more frequent zi.\nIn view of this reasoning, feature matrix X captures the highest order information about the documents but overlooks the features\u2019 lower order n-grams (that are indirectly used to reconstruct documents). This latter information is provided by the dictionary\u2019s structure inG and can be incorporated by a graph diffusion process that propagates the counts of s in each document to its constituent zi, which propagate these counts to the lower order features used to construct them, and so on. This process stops once we reach the characters comprising s since they are atomic. We can express this information flow in terms of G by noting that the product GTxDk = \u2211 s\u2208S\u222a\u03a3 x Dk s x\ns spreads xDks to each of the zi used to reconstruct s by multiplying xDks with x s zi , the number of times each zi\nis directly used in s. Graphically, node s in DR sends xDks units of flow to each parent zi, and this flow is modulated in proportion to xszi , the strength of the edge connecting zi to s. Performing this procedure a second time, i.e. multiplying GT (GTxDk), further spreads xDks x s zi to the features used to reconstruct zi, modulated in proportion to their usage. Iterating this procedure defines a new feature matrix X\u0302 = XH where H = I + \u2211\u221e n=1G n spreads the top level xDk to the entire graph6.\nWe can interpret the effect of the dictionary diffusion process in view of two equivalent regularized learning problems that learn coefficients \u03b2, \u03b7 \u2208 R|S\u222a\u03a3| for every feature in S \u222a \u03a3 by solving\nminimize \u03b2\u2208R|S\u222a\u03a3| L(X\u0302\u03b2) + \u03bbR(\u03b2)\n\u2261minimize \u03b7\u2208R|S\u222a\u03a3|\nL(X\u03b7) + \u03bbR ((I \u2212G)\u03b7) . (6)\nWe assume that L is a convex loss (that may implicitly encode any labels), R is a convex regularization penalty that attains its minimum at \u03b2 = 0, and that a minimizer \u03b2\u2217 exists. Note that adding an unpenalized offset does not affect our analysis. The two problems are equivalent because H is defined in terms of a convergent Neumann series and, in particular, H = (I \u2212 G)\u22121 is invertible. We may switch from one problem to the other by setting \u03b2 = H\u22121\u03b7 or \u03b7 = H\u03b2.\nWhen \u03bb = 0 the two problems reduce to estimating \u03b2/\u03b7 for unregularized models that only differ in the features they use, X\u0302 or X respectively. The equivalence of the problems shows, however, that using X\u0302 in place of X has no effect on the models as their predictions are always the same. Indeed, if \u03b2\u2217 is optimal for the first problem then \u03b7\u2217 = H\u03b2\u2217 is optimal for the second and for any z \u2208 R|S\u222a\u03a3|, the predictions zT \u03b7\u2217 = (zTH)\u03b2\u2217 are the same. Unregularized linear models \u2013 including generalized linear models \u2013 are therefore invariant to the dictionary reconstruction scheme and only depend on the document feature counts xDk , i.e. how documents are reconstructed.\nWhen \u03bb > 0, using X\u0302 in place of X results in a kind of graph Laplacian regularizer that encourages \u03b7s to be close to \u03b7Txs. One interpretation of this is effect is that \u03b7s acts a \u201clabel\u201d for s: we use its feature representation to make a prediction for what \u03b7s should be and penalize the model for any deviations. A complementary line of reasoning uses the collapsed DAG DR to show that (6) favors lower order features. Associated with every node s \u2208 S \u222a \u03a3 is a flow \u03b7s and node z sends \u03b7z units of flow to each of its children s. This flow is attenuated (or amplified) by xsz , the strength of the edge connecting z to s. In turn, s adds its incoming flows and sends out \u03b7s units of flow to its children; each document\u2019s prediction is given by the sum of its incoming flows. Here R acts a kind of \u201cflow conservation\u201d penalty that penalizes nodes for sending out a different amount of flow than they receive and the lowest order nodes (characters) are penalized for any flow. From this viewpoint it follows that the model prefers to disrupt the flow conservation of lower order nodes whenever they sufficiently decrease the loss since they influence the largest number documents. Higher order nodes influence fewer documents than their lower order constituents and act as high frequency components."}, {"heading": "4 EXPERIMENTS", "text": "This section presents experiments comparing traditional BoN features with features derived from Dracula and CFL. Our primary goal is investigate whether deep compression can provide better features for learning than shallow compression or the traditional \u201cfully redundant\u201d BoN representation (using all n-grams up to a maximum length). Since any of these representations can be obtained from Dracula using an appropriate cost scheme, positive evidence for the deep compression implies Dracula is uncovering hierarchical structure which is simultaneously useful for compression and learning. We also provide a measure of compressed size that counts the number of pointers used by each representation, i.e. the result of evaluating each compression with a \u201ccommon sense\u201d space objective where all costs are 1. We use Top to indicate BoN features counting only document pointers (X in previous section), Flat for dictionary diffusion features (i.e. X\u0302), CFL for BoN features from CFL, and All for traditional BoN features using all n-grams considered by Dracula.\n6This sum converges because G corresponds to a finite DAG so it can be permuted to a strictly lower triangular matrix so that lim\nn\u2192\u221e Gn = 0. See Appendix section A.2 for weighted variations.\nWe used Gurobi (Gurobi Optimization (2015)) to solve the refined LP relaxation of Dracula for all of our experiments. While Gurobi can solve impressively large LP\u2019s, encoding Dracula for a generalpurpose solver is inefficient and limited the scale of our experiments. Dedicated algorithms that utilize problem structure, such as the network flow interpretation of the reconstruction modules, are the subject of a follow-up paper and will allow Dracula to scale to large-scale datasets. We limited our parameter tuning to the dictionary pointer cost \u03bb (discussed in the solution path section) as this had the largest effect on performance. Experiments were performed with \u03c4 = 0, \u03b1 = 1, a maximum n-gram length, and only on n-grams that appear at least twice in each corpus.\nProtein Data We ran Dracula using 7-grams and \u03bb = 1 on 131 protein sequences that are labeled with the kingdom and phylum of their organism of origin (pro). Bacterial proteins (73) dominate this dataset, 68 of which evenly come from Actinobacteria (A) and Fermicutes (F). The first 5 singular values (SV\u2019s) of the Top features show a clear separation from the remaining SV\u2019s and Figure 3 plots the proteins when represented by their 4th and 5th principle components. They are labeled by kingdom and, in more interesting cases, by phylum. Note the clear separation of the kingdoms, the two main bacterial phyla, and the cluster of plants separated from the other eukaryotes. Table 1 shows the average accuracy of two binary classification tasks in which bacteria are positive and we hold out either phylum A or F, along with other randomly sampled phyla for negative cases, as a testing set. We compare All features to Top features from Dracula and CFL using an `2-regularized SVM with C = 1. Since there are many more features than training examples we plot the effect of using the top K principle components of each feature matrix. Flat features did not help and performance strictly decreased if we limited the n-gram length for All features, indicating that long n-grams contain essential information. Both compression criteria perform well, but using a deep dictionary seems to help as Dracula\u2019s profile is more stable than CFL\u2019s.\nStylometry We extracted 100 sentences from each of the training and testing splits of the Reuters dataset (Liu) for 10 authors, i.e. 2, 000 total sentences, and replaced their words with part-of-speech tags. The goal of this task is to predict the author of a given set of writing samples (that all come from the same author). We make predictions by representing each author by the centroid of her 100 training sentences, averaging together the unknown writing samples, and reporting the nearest author centroid to the sample centroid. We ran Dracula on this representation with 10-grams and normalized centroids by their `1 norm and features by their standard deviation. Table 2 compares the performance of All features to Top features derived from various \u03bb\u2019s for various testing sentence sample sizes. We report the average of 1, 000 trials, where each trial tested every author once and\nrandomly selected a set of sample sentences from the testing split sentences. As in the protein data, neither Flat nor shorter n-gram features helped, indicating that higher order features contain vital information. CFL with \u03bb = 20 strictly dominated every other CFL representation and is the only one included for brevity. Dracula with \u03bb = 10 or \u03bb = 20 shows a clear separation from the other schemes, indicating that the deep compression finds useful structure.\nSentiment Prediction We use a dataset of 10, 662 movie review sentences (Pang & Lee (2005)) labeled as having positive or negative sentiment. Bigrams achieve state-of-the-art accuracy on this dataset and unigrams perform nearly as well (Wang & Manning (2012)), so enough information is stored in low order n-grams that the variance from longer n-grams hurts prediction. We ran Dracula using 5-grams to highlight the utility of Flat features, which focus the classifier onto lower order features. Following (Wang & Manning (2012)), Table 3 compares the 10-fold CV accuracy of a multinomial na\u0131\u0308ve-Bayes (NB) classifier using Top or Flat features with one using all n-grams up to a maximum length. The dictionary diffusion process successfully highlights relevant low order features and allows the Flat representation to be competitive with bigrams (the expected best performer). The table also plots the mean n-gram length (MNL) used by document pointers as a function of \u03bb. The MNL decreases as \u03bb increases and this eventually pushes the Top features to behave like a mix of bigrams and unigrams. Finally, we also show the performance of `2 or `1- regularized support vector machines for which we tuned the regularization parameter to minimize CV error (to avoid issues with parameter tuning). It is known that NB performs surprisingly well relative to SVMs on a variety of sentiment prediction tasks, so the dropoff in performance is expected. Both SVMs achieve their best accuracy with bigrams; the regularizers are unable to fully remove the spurious features introduced by using overly long n-grams. In contrast, Flat achieves its best performance with larger MNLs which suggests that Dracula performs a different kind of feature selection than is possible with direct `1/`2 regularization. Moreover, tuning \u03bb combines feature selection with NB or any kind of classifier, irrespective of whether it natively performs feature selection."}, {"heading": "5 CONCLUSION", "text": "We have introduced a novel dictionary-based compression framework for feature selection from sequential data such as text. Dracula extends CFL, which finds a shallow dictionary of n-grams with which to compress a document corpus, by applying the compression recursively to the dictionary. It thereby learns a deep representation of the dictionary n-grams and document corpus. Experiments\nwith biological, stylometric, and natural language data confirm the usefulness of features derived from Dracula, suggesting that deep compression uncovers relevant structure in the data.\nA variety of extensions are possible, the most immediate of which is the design of an algorithm that takes advantage of the problem structure in Dracula. We have identified the basic subproblems comprising Dracula, as well as essential structure in these subproblems, that can be leveraged to scale the compression to large datasets. Ultimately, we hope to use Dracula to explore large and fundamental datasets, such as the human genome, and to investigate the kinds of structures it uncovers."}, {"heading": "ACKNOWLEDGEMENTS", "text": "Dedicated to Ivan i Kalinka Hand ievi (Ivan and Kalinka Handjievi). Funding provided by the Air Force Office of Scientific Research and the National Science Foundation."}, {"heading": "A APPENDIX", "text": "A.1 RECONSTRUCTION MODULES\nThe reconstruction modules RDk/R\u0302s are the basic building blocks of Dracula; when t is fixed solving (5) is tantamount to solving the reconstruction modules separately. These simple BLPs have a number of properties that result in computational savings because of the structure of the constraint matrix XDk/Xs. In order to simplify notation we define\nTs(t, v; b, V ) = minimize w\u2208{0,1}|P(s)| \u2211 p\u2208P(s) wpbp subject to Xsw \u2265 v1; w \u2264 V t. (7)\nUsing TDk(t, 1; c, V Dk) = RDk(t; c) and Ts(t, ts; c\u0302, V\u0302 s) = R\u0302s(t; c\u0302) results in the document or dictionary reconstruction modules. Now note that every column in Xs is all zeros except for a contiguous sequence of ones so that Xs is an interval matrix and therefore totally unimodular (TUM). Define T cs to be the LP relaxation of Ts obtained by replacing the integrality constraints:\nT cs (t, v; b, V ) = minimize w\u2208[0,1]|P(s)| \u2211 p\u2208P(s) wpbp subject to Xsw \u2265 v1; w \u2264 V t. (8)\nAside from X , the remaining constraints on w are bound constraints. It follows from (Bertsimas & Weismantel (2005)) that T cs is an LP over a integral polyhedron so we may conclude that Proposition 1. If the arguments t, v are integral, then all basic solutions of T cs (t, v; b, V ) are binary.\nIndeed, a simple dynamic program discussed in (Paskov et al. (2013)) solves Ts efficiently.\nOur second property reformulates T cs by transforming the constraint matrix X s into a simpler form. The resulting matrix has at most 2 non-zero entries per column instead of up to |s| non-zero entries per column in Xs. This form is more efficient to work with when solving the LP and it shows that T cs is equivalent to a min-cost flow problem over an appropriately defined graph. Define Q \u2208 {0,\u00b11}|s|\u00d7|s| be the full rank lower triangular matrix with entries Qsii = \u2212Qs(i+1)i = 1 and 0 elsewhere (and Qs|s||s| = 1). The interval structure of X\ns implies that column i of Zs = QsXs is all zeros except for Zsij = \u2212Zsik = 1 where j is the first row in which Xsij = 1 and k > j is the first row in which Xsik = 0 after the sequences of ones (if such a k exists). By introducing non-negative slack variables for theXsw \u2265 v1 constraint, i.e. writingXsw = v1+\u03be, and noting thatQs1 = e1, where e1 is all zeros except for a 1 as its first entry, we arrive at:\nT cs (t, v; b, V ) = minimize w,\u03be \u2211 p\u2208P(s) wpbp\nsubject to Zsw \u2212Qs\u03be = ve1, 0 \u2264 w \u2264 V t, 0 \u2264 \u03be.\n(9)\nThe matrix \u03a8 = [Zs| \u2212Qs] has special structure since every column has at most one 1 and at most one \u22121. This allows us to interpret \u03a8 as the incidence matrix of a directed graph if we add source and sink nodes with which to fill all columns out so that they have exactly one 1 and one \u22121. T cs may then be interpreted as a min-cost flow problem.\nA.1.1 POLYHEDRAL REFINEMENT\nWe now show how to tighten Dracula\u2019s LP relaxation by adding additional constraints to QC to shrink it closer to Q. If every time we see a string s it is followed by the character \u03b1 (in a given corpus), the strings s and s\u03b1 belong to the same equivalence class; the presence of s\u03b1 conveys the same information as the presence of s. Importantly, the theory of suffix trees shows that all substrings\nof a document corpus can be grouped into at most 2n\u22121 equivalence classes (Gusfield (1997)) where n is the word count of the corpus. We always take equivalence classes to be inclusion-wise maximal sets and say that equivalence class \u03b5 \u2282 S appears at a location if any (i.e. all) of its members appear at that location. We prove the following theorem below. This theorem verifies common sense and implies that, when the pointer costs do not favor any particular string in \u03b5, adding the constraint\u2211 s\u2208\u03b5 ts \u2264 1 to the LP relaxation to tighten QC will not remove any binary solutions. Theorem 2. Let \u2126 denote the set of all equivalence classes in corpus C and suppose that all costs are non-negative and \u2200\u03b5 \u2208 \u2126,\u2200z \u2208 S,\u2200s, x \u2208 \u03b5, the dictionary costs ds = dx are equal, the pointer costs czp = c z q (c\u0302 z p = c\u0302 z q) are equal when p = (l, s) and q = (l, x), and c s p = c x q ( c\u0302 s p = c\u0302 x q ) whenever pointers p = q = (l, h) refer to the same location and use the same string (or character) h. Then there is an optimal compression D = (S, P P\u0302 ) in which S contains at most one member of \u03b5.\nProof. Suppose that the conditions for theorem 1 hold, let \u03b5 be an equivalence class, let D = (S, P, P\u0302 ) be an optimal compression, and suppose for the sake of contradiction that s1, s2 \u2208 \u03b5 are both included in the optimal dictionary. Without loss of generality we assume that |s1| < |s2|. Consider first document pointer p which uses s1 for document Dk. By assumption there is another pointer q which uses s2 in the same location and cDkp = c Dk q so we are indifferent in our choice. We thereby may replace all document pointers that use s1 with equivalent ones that use s2 without changing the objective value.\nConsider next the usage of s1 to construct higher order dictionary elements. We must be careful here since if some dictionary element s3 is in the optimal dictionary S and can be expressed as s3 = zs1 for some string z then we may not use s2 in place of s1 since it would lead to a different dictionary string. The key step here is to realize that s3 must belong to the same equivalence class as string zs2 and we can use zs2 in place of s3 in all documents. If s3 is itself used to construct higher order dictionary elements, we can apply the same argument for s2 to zs2 in an inductive manner. Eventually, since our text is finite, we will reach the highest order strings in the dictionary, none of whose equivalence class peers construct any other dictionary n-grams. Our earlier argument shows that we can simply take the longest of the highest order n-grams that belong to the same equivalence class. Going back to s3, we note that our assumptions imply that the cost of constructing zs2 is identical to the cost of constructing s3 so we may safely replace s3 with zs2. The only remaining place where s1 may be used now is to construct s2. However, our assumptions imply that the cost of constructing s1 \u201cin place\u201d when constructing s2 is the same. By eliminating s1 we therefore never can do worse, and we may strictly improve the objective if ts1 > 0 or s1 is used to construct s2 and its pointer cost is non-zero. QED.\nA.2 WEIGHTED DIFFUSION\nWhen G is generated from the relaxation of Dracula and t \u2208 (0, 1]|S| are the dictionary coefficients, any s \u2208 S with ts < 1 will have Gsz \u2264 ts\u2200z \u2208 S. In order to prevent overly attenuating the diffusion we may wish to normalize row s in G by t\u22121s for consistency. We note that a variety of other weightings are also possible to different effects. For example, weighting G by a scalar \u03c1 \u2265 0 attenuates or enhances the entire diffusion process and mitigates or enhances the effect of features the farther away they are from directly constructing any feature directly used in the documents.\nA.3 PROOF OF PATH THEOREM\nThe fundamental theorem of linear programming states that for any c \u2208 Rd, S(c,Q) \u2261 arg minx\u2208Q x\nT c(\u03b1) \u2208 F (Q) since Q has non-empty interior and is therefore non-empty. We will use a construction known as the normal fan of Q, denoted by N (Q), that partitions Rd into a finite set of polyhedral cones pertaining to (linear) objectives for which each face in F (Q) is the solution set. We begin with some helpful definitions.\nA partition P \u2282 2X of a setX is any collection of sets satisfying \u22c3 p\u2208P p = X and \u2200p, q \u2208 P p 6= q implies p\u2229q = \u2205. The relative interior of a convex setX \u2282 Rd, denoted by relintX , is the interior of X with respect to its affine hull. Formally, relintX = {x \u2208 X | \u2203\u03b5 > 0, B(x, \u03b5) \u2229 affX \u2282 X}. The following definition is taken from (Lu & Robinson (2008)): A fan is a finite set of nonempty polyhedral convex cones in Rd, N = {N1, N2, . . . , Nm}, satisfying:\n1. any nonempty face of any cone in N is also in N , 2. any nonempty intersection of any two cones in N is a face of both cones.\nThis definition leads to the following lemma, which is adapted from (Lu & Robinson (2008)): Lemma 1. Let N be a fan in Rd and S = \u22c3 N\u2208N N the union of its cones.\n1. If two cones N1, N2 \u2208 N satisfy (relintN1) \u2229N2 6= \u2205 then N1 \u2282 N2, 2. The relative interiors of the cones in N partition S, i.e. \u22c3 N\u2208N relintN = S.\nLemma 1 is subtle but important as it contains a key geometric insight that allow us to prove our theorem. Next, let Q \u2282 Rd be a bounded polyhedron with vertex set V and nonempty interior, i.e. whose affine hull is d-dimensional. For any face F \u2208 F (Q) define V (F ) = F \u2229 V to be the vertices of F and NF = { y \u2208 Rd | \u2200x \u2208 F,\u2200z \u2208 Q, yTx \u2264 yT z } to be the normal cone to F . That NF is a (pointed) polyhedral cone follows from noting that it can be equivalently expressed as a finite collection of linear constraints involving the vertices of F and Q: NF ={ y \u2208 Rd | \u2200x \u2208 V (F ),\u2200z \u2208 V, yTx \u2264 yT z } . The normal fan for Q, N (Q) = {NF }F\u2208F(Q), is defined to be the set of all normal cones for faces of Q. Noting that Q is bounded and therefore has a recession cone of {0}, the following Lemma is implied by Proposition 1 and Corollary 1 of (Lu & Robinson (2008)):\nLemma 2. Let N (Q) be the normal fan of a bounded polyhedron Q with non-empty interior in R. Then\n1. N (Q) is a fan,\n2. for any nonempty faces F1, F2 \u2208 F (Q), F1 \u2282 F2 iff NF1 \u2283 NF2 , 3. \u22c3 F\u2208F(Q) relintNF = Rd,\n4. every nonempty face F \u2208 F (Q) satisfies relintNF = { y \u2208 Rd | F = S(y,Q) } .\nWe will also makes use of the following two results. The first is implied by Theorem 2.7, Corollary 2.14, and Problem 7.1 in Ziegler (1995):\nLemma 3. Let Q \u2282 Rd be a bounded polyhedron with nonempty interior, F \u2208 F (Q), and NF the normal cone to F . Then dimF + dimNF = d.\nThe second Lemma states a kind of neighborliness for the cones in N (Q): Lemma 4. Let Q \u2282 Rd be a bounded polyhedron with nonempty interior. For any N \u2208 N (Q) and x \u2208 relintN there exists a \u03b4 > 0 such that for any y \u2208 B(x, \u03b4) there is a N \u2032 \u2208 N (Q) with y \u2208 relintN \u2032 and N \u2282 N \u2032.\nProof. Let N \u2208 N (Q) and x \u2208 N be given. We say that N \u2032 \u2208 N (Q) occurs within \u03b4 (for \u03b4 > 0) if there is some y \u2208 B(x, \u03b4) with y \u2208 relintN \u2032. Now suppose that there is an N \u2032 \u2208 N (Q) that occurs within \u03b4 for all \u03b4 > 0. Since N \u2032 is a closed convex cone it must be that x \u2208 N \u2032 so we may conclude from Lemma 1 that N \u2282 N \u2032. Next, letM be the set of cones inN (Q) which do not contain N and suppose that for all \u03b4 > 0 there is some N \u2032 \u2208 M that occurs within \u03b4. Since |M| is finite, this is only possible if there is a cone N \u2032 \u2208 M that occurs within \u03b4 for all \u03b4 > 0. However, this leads to a contradiction since N \u2032 must contain N so the Lemma follows.\nWe are now ready to prove our main theorem which is restated below with S(c,Q) = arg minx\u2208Q x T c(\u03b1) for simplicity.\nTheorem 3. Let Q \u2282 Rd be a bounded polyhedron with nonempty interior and c : [0, 1] \u2192 Rd a continuous function. Then for some N \u2208 Z+ \u222a {\u221e} there exists a countable partition \u0393 = {\u03b3i}Ni=0 of [0, 1] with corresponding faces Fi \u2208 F (Q) satisfying Fi 6= Fi+1 and Fi \u2229 Fi+1 6= \u2205 and Fi = S(c(\u03b1), Q)\u2200\u03b1 \u2208 \u03b3i. Moreover, Fi never has the same dimension as Fi+1 and the boundary between \u03b3i, \u03b3i+1 is )[ iff dimFi < dimFi+1 and ]( otherwise.\nProof. For ease of notation let f(x) = S(c(x), Q) and for k = 0, . . . , d define \u03c9k ={ x \u2208 [0, 1] | dimNf(x) \u2265 k } to be the set of all arguments to c whose normal cone is at least kdimensional. Moreover, for any x \u2208 [0, 1] define \u03c3(x) = {y \u2208 [0, x] | \u2200z \u2208 [y, x], f(x) = f(z)} \u222a {y \u2208 [x, 1] | \u2200z \u2208 [x, y], f(x) = f(z)} to be the largest contiguous set containing x over which f remains constant and let m(x) = inf\u03c3(x) and M(x) = sup\u03c3(x) be its infinimum and supremem, respectively. The proof follows by induction on k = d, d \u2212 1, . . . , 0 with the inductive hypothesis that for some Nk \u2208 Z+ \u222a {\u221e} there exists a countable partition \u0393k = { \u03b3ki }Nk i=0\nof \u03c9k with corresponding faces F ki \u2208 F (Q) satisfying F ki = S(c(\u03b1), Q)\u2200\u03b1 \u2208 \u03b3ki . Base case (k = d): Let x \u2208 \u03c9d so that \u03c3(x) \u2282 \u03c9d. Since Nf(x) is d-dimensional, intNf(x) = relintNf(x) so continuity of c implies that \u03c3(x) is a (non-empty) open interval with m(x) < M(x). It follows that \u0393k = {\u03c3(x) | x \u2208 \u03c9d} defines a partition of \u03c9d into a set of open intervals. Each interval contains (an infinite number) of rational numbers, and we see that \u0393k is countable by assigning to each interval a rational number that it contains.\nInductive step: Let x \u2208 \u03c9k\\\u03c9k+1. There are two cases to consider. If m(x) < M(x) then (m(x),M(x)) \u2282 \u03c3(x) contains a rational number. Thus, the set \u0393ko = {\u03c3(x) | x \u2208 \u03c9k\\\u03c9k+1,m(x) < M(x)} is countable. Otherwise, if m(x) = x = M(x) then by Lemma 4 there is a \u03b4 > 0 such that if y \u2208 B(x, \u03b4) then Nf(x) \u2282 NS(y,Q). Continuity of c implies that there is a \u03b5 > 0 for which c((x \u2212 \u03b5, x + \u03b5)) \u2282 B(x, \u03b4) and hence (x \u2212 \u03b5, x + \u03b5)\\{x} \u2282 \u03c9k+1. Assigning to x any rational number in (x \u2212 \u03b5, x + \u03b5) and letting \u0393kc = {\u03c3(x) | x \u2208 \u03c9k\\\u03c9k+1,m(x) = M(x)}, we may appeal to the inductive hypothesis to conclude that \u0393kc is countable. Finally, \u0393\nk = \u0393ko \u222a \u0393kc \u222a \u0393k+1 is a finite union of countable sets and therefore countable.\nSince \u03c90 = [0, 1] we have shown that \u0393 = \u03930 is a countable partition of [0, 1] into intervals over which f is constant. Now consider two consecutive intervals \u03b3i, \u03b3i+1 \u2208 \u0393 and let M be the supremum of \u03b3i. If M /\u2208 \u03b3i then since cone NFi is closed, c(M) \u2208 NFi . Since c(M) \u2208 relintNFi+1 by assumption, it follows that NFi+1 is a proper subset of NFi and hence that Fi is a proper subset of Fi+1. Otherwise, if M \u2208 \u03b3i then the continuity of c and Lemma 4 imply that NFi is a proper subset of NFi+1 so Fi+1 is a proper subset of Fi. In either case Fi \u2229 Fi+1 6= \u2205 and Lemma 3 implies the dimensionality result of our Theorem."}], "references": [{"title": "Optimization over integers", "author": ["Bertsimas", "Dimitris", "Weismantel", "Robert"], "venue": "Athena Scientific,", "citeRegEx": "Bertsimas et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bertsimas et al\\.", "year": 2005}, {"title": "Spam filtering using statistical data compression models", "author": ["Bratko", "Andrej", "Filipi\u010d", "Bogdan", "Cormack", "Gordon V", "Lynam", "Thomas R", "Zupan", "Bla\u017e"], "venue": null, "citeRegEx": "Bratko et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bratko et al\\.", "year": 2006}, {"title": "Text categorization with many redundant features: Using aggressive feature selection to make SVMs competitive with C4.5", "author": ["Gabrilovich", "Evgeniy", "Markovitch", "Shaul"], "venue": "In ICML,", "citeRegEx": "Gabrilovich et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2004}, {"title": "Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology", "author": ["Gusfield", "Dan"], "venue": null, "citeRegEx": "Gusfield and Dan.,? \\Q1997\\E", "shortCiteRegEx": "Gusfield and Dan.", "year": 1997}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "Ranzato", "Marc\u2019Aurelio", "Huang", "Fu-Jie"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Normal fans of polyhedral convex sets", "author": ["Lu", "Shu", "Robinson", "Stephen M"], "venue": "Set-Valued Analysis,", "citeRegEx": "Lu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2008}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Compressive feature learning", "author": ["Paskov", "Hristo", "West", "Robert", "Mitchell", "John", "Hastie", "Trevor"], "venue": "In NIPS,", "citeRegEx": "Paskov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paskov et al\\.", "year": 2013}, {"title": "An efficient algorithm for large scale compressive feature learning", "author": ["Paskov", "Hristo", "Mitchell", "John", "Hastie", "Trevor"], "venue": "In AISTATS,", "citeRegEx": "Paskov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paskov et al\\.", "year": 2014}, {"title": "Learning deep generative models", "author": ["Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Salakhutdinov and Ruslan.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Ruslan.", "year": 2009}, {"title": "Combinatorial Optimization - Polyhedra and Efficiency", "author": ["A. Schrijver"], "venue": null, "citeRegEx": "Schrijver,? \\Q2003\\E", "shortCiteRegEx": "Schrijver", "year": 2003}, {"title": "Deep learning for NLP (without magic)", "author": ["Socher", "Richard", "Manning", "Christopher D"], "venue": "In Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Sida", "Manning", "Christopher D"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "A universal algorithm for sequential data", "author": ["Ziv", "Jacob", "Lempel", "Abraham"], "venue": "compression. TIT,", "citeRegEx": "Ziv et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Ziv et al\\.", "year": 1977}], "referenceMentions": [{"referenceID": 1, "context": "Meanwhile, off-the-shelf compressors, such as LZ-77 (Ziv & Lempel (1977)), have been successfully applied to natural language problems as kernels that compute pairwise document similarities (Bratko et al. (2006)).", "startOffset": 191, "endOffset": 212}, {"referenceID": 7, "context": "This recursion makes Dracula a deep extension of Compressive Feature Learning (CFL) (Paskov et al. (2013)) that can find exponentially smaller representations and promotes similar n-grams to enter the dictionary.", "startOffset": 85, "endOffset": 106}, {"referenceID": 7, "context": "This recursion makes Dracula a deep extension of Compressive Feature Learning (CFL) (Paskov et al. (2013)) that can find exponentially smaller representations and promotes similar n-grams to enter the dictionary. As noted in (Paskov et al. (2013)), feature representations derived from off-the-shelf compressors are inferior because the algorithms used are sensitive to document order; both Dracula and CFL are invariant to document order.", "startOffset": 85, "endOffset": 247}, {"referenceID": 4, "context": "This is a notable departure from traditional deep learners (Salakhutdinov (2009); Socher & Manning (2013); LeCun et al. (2006)), which are formulated as non-convex, non-linear optimization problems.", "startOffset": 107, "endOffset": 127}, {"referenceID": 7, "context": "In particular, it requires solving a binary LP (which are NP-Complete in general) and it generalizes CFL which is itself NP-Complete (Paskov et al. (2014)) (see section 3.", "startOffset": 134, "endOffset": 155}, {"referenceID": 10, "context": "In fact, the Chv\u00e1tal-Gomory theorem (Schrijver (2003)) shows that we may \u201cprune\u201d", "startOffset": 37, "endOffset": 54}], "year": 2016, "abstractText": "We propose \u2018Dracula\u2019, a new framework for unsupervised feature selection from sequential data such as text. Dracula learns a dictionary of n-grams that efficiently compresses a given corpus and recursively compresses its own dictionary; in effect, Dracula is a \u2018deep\u2019 extension of Compressive Feature Learning. It requires solving a binary linear program that may be relaxed to a linear program. Both problems exhibit considerable structure, their solution paths are well behaved, and we identify parameters which control the depth and diversity of the dictionary. We also discuss how to derive features from the compressed documents and show that while certain unregularized linear models are invariant to the structure of the compressed dictionary, this structure may be used to regularize learning. Experiments are presented that demonstrate the efficacy of Dracula\u2019s features.", "creator": "LaTeX with hyperref package"}}}