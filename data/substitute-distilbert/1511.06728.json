{"id": "1511.06728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning", "abstract": "we propose a method for hand pose mapping based on a deep regressor trained on two different kinds of input. raw depth data is fused with an intermediate language in the form of a segmentation of the hand into parts. this intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. the mapping from raw depth to segmentation texture is learned in a semi / weakly - supervised way from two sample datasets : ( i ) a synthetic dataset created through a rendering pipeline including densely labeled ground images ( pixelwise segmentations ) ; and ( ii ) a dataset collecting real images for which ground truth joint positions are constant, but false dense segmentations. loss for training on compressed images is generated from a patch - wise restoration process, which aligns structured segmentation maps with a large dictionary modeling deformation entities. the initial premise is that the domain shift between possible real real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw texture domain. structures describing the nyu dataset show essentially the raw training method decreases error vector joints over situ regression of joints from depth data by 15. 7 %.", "histories": [["v1", "Fri, 20 Nov 2015 19:19:00 GMT  (4193kb)", "http://arxiv.org/abs/1511.06728v1", "10 pages, 7 figures, 4 tables"], ["v2", "Wed, 8 Jun 2016 13:31:05 GMT  (2926kb)", "http://arxiv.org/abs/1511.06728v2", "11 pages, 7 figures, 4 tables"], ["v3", "Thu, 9 Jun 2016 06:08:54 GMT  (2926kb)", "http://arxiv.org/abs/1511.06728v3", "11 pages, 7 figures, 4 tables"], ["v4", "Fri, 15 Sep 2017 09:24:57 GMT  (2997kb,D)", "http://arxiv.org/abs/1511.06728v4", "13 pages, 10 figures, 4 tables"]], "COMMENTS": "10 pages, 7 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["natalia neverova", "christian wolf", "florian nebout", "graham taylor"], "accepted": false, "id": "1511.06728"}, "pdf": {"name": "1511.06728.pdf", "metadata": {"source": "CRF", "title": "Hand Pose Estimation through Weakly-Supervised Learning of a Rich Intermediate Representation", "authors": ["Natalia Neverova", "Christian Wolf", "Florian Nebout", "Graham Taylor"], "emails": ["christian.wolf}@liris.cnrs.fr", "2florian.nebout@awabot.com", "3gwtaylor@uoguelph.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 72\n8v 1\n[ cs\n.C V\n] 2"}, {"heading": "1. Introduction", "text": "Hand pose estimation and tracking from depth images, i.e. the estimation of joint positions of a human hand, is a first step for various applications: hand gesture recognition, human-computer interfaces (moving cursors and scrolling documents), human-object interaction in virtual reality settings and many more. While the estimation of full-body pose is now available at real-time in commercial products, at least in cooperative environments [24], the estimation of\nFigure 1: A rich intermediate representation is fused with raw input for hand joint regression. The intermediate representation is learned in a semi/weakly-supervised setting from real and synthetic data (see figure 2 for illustrations of the training procedure). The input image is from the NYU dataset [32].\nhand pose is more complex. In settings where the user is not directly placed in front of the computer, and therefore not close to the camera, the problem is inherently difficult. In this case, typically the hand occupies only a small portion of the image, and fingers and finger parts are only vaguely discernible.\nWe present a new method for the regression of hand joint positions based on joint training of several learners working on heterogeneous labels. Similar to earlier work [24, 12], we first construct an intermediate representation based on a segmentation into parts. While more recent work on body and hand pose estimation tends to perform direct regression from depth or color input to joint positions [32, 31, 3, 23], we argue that the intermediate representation is a powerful tool in the special context of training with multiple heterogeneous training sets.\nIn our setting, pose estimation is performed frame-byframe without any dynamic information. A model is learned from two training sets: (i) a (possibly small) set of real images acquired with a consumer depth sensor. Ground truth joint positions are assumed to be available, for instance obtained by multi-camera motion capture systems. (ii) a second (and possibly very large) set of synthetic training images produced from 3D models by a rendering pipeline, accompanied by dense ground truth in the form of a segmentation into hand parts. This ground truth is easy to come by, as it is usually automatically created by the same rendering pipeline. The main arguments we develop are the following:\n\u2022 an intermediate representation defined as a segmentation into parts contains rich structural and topological information, since the label space itself is structured. Labels have adjacency, topological and geometric relationships, which can be leveraged and translated into loss for weakly-supervised training;\n\u2022 a regression of joint positions is easier and more robust if the depth input is combined with a rich semantic representation like a segmentation into parts, provided that this semantic segmentation is of high fidelity (see Figure 1).\nWe will show that the intermediate representation (the segmentation into parts) is able to improve pose regression performance if it is combined with raw depth input. A key component to obtaining this improvement is getting reliable segmentations for real data. While purely supervised training on synthetic data has proven to work well for full-body pose estimation [24, 12], hand pose estimation is known to require real data captured from depth sensors for training\n[29, 27, 32] due to low input resolution and data quality. Manually annotating dense segmentations of large datasets is not an option, and estimating segmentation maps from ground truth joint positions is unreliable in the case of low quality images. We propose a weakly supervised setting in order to tackle this problem, where this intermediate representation is learned from densely labeled synthetic depth images as well as from real depth images associated with ground truth joint positions.\nThe proposed weakly supervised training method exploits the rich geometrical and topological information of the intermediate representation. During the training process, predicted segmented patches from real images are aligned with a very large dictionary of labelled patches extracted from rendered synthetic data. The novelty here lies in the fact that we do not match input patches but patches taken from the intermediate representation, which include the to-be-inferred label and its local context;\nThe paper is organized as follows. Section 2 discusses related work. Section 3 introduces the model and the weakly supervised learning setting. Section 4 gives details about the deep architectures employed in the experiments. Section 5 explains the experimental setup and provides results. Section 6 concludes."}, {"heading": "2. Related work", "text": "Compared to the study of hand-pose, a much larger body of work has focused on full-body pose estimation. We draw influence from this literature and therefore include it in our brief review. Learning \u2014 The majority of recent work on pose estima-\ntion is based on machine learning. Body part segmentation as an intermediate representation for joint estimation from depth images was successfully used in [24], where random forests were trained to perform pixel-wise classification. This was adapted to hand pose estimation in [12], where an additional pose clustering step was introduced. In [29], a random forest is learned, which performs different tasks at different levels: viewpoint clustering in higher levels, part segmentation in middle levels, and joint regression in lower levels. An explicit transfer function is learned between synthetic and real data. In follow-up work [27], a latent regression forest is learned, which automatically extracts a hierarchical and topological model of a human hand from data. Instead of pixel-wise voting, the forest is descended a single time starting from the center of mass of the hand. The traditional split nodes in RF are accompanied by division nodes, which trigger parallel descents of sub-trees for multiple entities (joints or groups of joints).\nRecent work estimates joint positions by regression with deep convolutional neural nets [32, 3, 31, 33, 10]. Typically such models have been trained to produce heat maps encoding the joint positions as spatial Gaussians, though direct regression to an encoding of joints has also been attempted. Post-processing to enforce structural constraints is based on graphical models [10, 31, 3] or inverse kinematics [32]. In [14] a deep learning framework is regularized by a bottleneck layer, enforcing the network to model underlying structure of joint positions. In a recent overview [8] the deep learning model were shown to perform the best among existing approaches, but still far from human performance. Graphical models \u2014 In [3], a graphical model is implemented with deep convolutional nets, which jointly estimate unary terms, given indications of joint types and positions, and binary terms, modelling relationships between joints. In [31], a deep full-body part detector is jointly learned with a Markov Random Field which models spatial priors. Joint learning is achieved by designing the priors as convolutions and implementing inference as forward propagation approximating a single step in a message passing algorithm. Top down methods \u2014 A different group of methods is based on top-down processes which fit 3D models to image data. [20] is based on pixelwise comparison of rendered and observed depth maps. Inverse rendering of a generative model including shading and texture is used for model fitting in [5]. In [16], ICP is employed for hand pose reconstruction and 3D fingertip localization under spatial and temporal constraints. A hybrid method [21] for real-time hand tracking uses a simple hand model consisting of a number of spheres. In [25], a person-specific and hybrid generative/discriminative model is built for a system using five RGB cameras plus a ToF camera. Correspondence \u2014 Pose estimation has been attempted by solving correspondence problems in other work. In [30], a\ncorrespondence between depth pixels and vertices of an articulated 3D model is learned. In [2], approximate directed Chamfer distances are used to align observed edge images with a synthetic dataset. Other work \u2014 Work on applications other than pose estimation share similarities with our method. Patchwise alignment of segmentations in a transductive learning setting has been performed on 3D meshes [35], matching real unlabelled shape segments to shapes from a large labelled database. A large number of candidate segments is created and the optimal segmentation is calculated solving an integer problem. Our method can be viewed as a kind of multitask learning, a topic actively pursued by the deep vision community [6, 36]. While these techniques rely on subnetworks that share parameters, our approach does not use weight sharing, instead, it co-ordinates subnetworks via a patchwise restoration process and joint error function.\nOur work bears a certain resemblance to the recently proposed N4-Fields [7]. This method, which was proposed for different applications, also combines deep networks and a patchwise nearest neighbor (NN) search. However, whereas in [7], NN-search is performed in a feature space learned by deep networks, our method performs NN-search in a patch space corresponding to semantic segmentation learned by deep networks. This part of our work bears also some similarity to the way label information is integrated in structured prediction forests [15], although no patch alignment with a dictionary is carried out there."}, {"heading": "3. Joint regression with a rich intermediate", "text": "representation\nIn our weakly-supervised setting, the training data are organized into two parts: a set of real depth images and a set of associated ground truth joint positions, and a second set of synthetic depth images with a set of associated ground truth label images. We will denote with D(j) the jth pixel in image D.\nThe synthetic images have been rendered from different 3D hand models using a rendering pipeline. As in [25], we also sample different pose parameters and hand shape parameters. Variations in viewpoints and hand poses are obtained taking into account physical and physiological constraints. In contrast to other weakly-supervised or semisupervised settings, for instance [29], we do not suppose that any ground truth data for the intermediate representation is available for the real training images. Manually labelling segmentations is extremely difficult and time consuming. Labelling a sufficiently large number of images is hardly practical.\nWe do, however, rely on ground truth for joint positions, which can be obtained in several ways: In [24], external motion capture using markers is employed. In [32], training data is acquired in a multi-view setting from three different\ndepth sensors and an articulated model is fitted offline. Our intermediate representation is a segmentation into 20 parts, illustrated in Fig. 1. 19 parts correspond to finger parts, 1 part to the palm. The background is considered to be subtracted in a preprocessing step and is not part of the segmentation process. Compared to the initial input depth image, this representation has several important advantages:\n\u2022 The part label space is characterized by strong topological properties. In contrast to other semantic segmentation problems (for instance, semantic full scene labeling), strong neighborhood relationships can be defined on the label space.\n\u2022 For a given view and pose, the part label itself carries strong geometrical information: the label alone of a given pixel is a very strong prior on the position of the pixel in 3d. This property will be exploited in our system to motivate patchwise searches in label space.\nThe intermediate representation thus provides two advantages: (i) it provides important cues for regression to the desired joint positions, and (ii) we can leverage its strong topological and geometrical properties to restore noisy part label images and to generate a loss function for training.\nThe proposed method leverages two different mappings learned on two training sets. The functional decomposition as well as the dataflow during training and testing are outlined in Fig. 2. A segmentation learner learns a mapping fs(\u00b7, \u03b8s) from raw depth data to segmentation maps, parametrized by a parameter vector \u03b8s. A regression learner learns a mapping fr(\u00b7, \u03b8r) from raw depth data combined with segmentation maps to joint positions, parametrized by a vector \u03b8r. The training procedure uses both synthetic and real data, and proceeds in three steps:\n1. First, the segmentation learner fs is pre-trained on synthetic training data in a supervised way using dense ground truth segmentations, resulting in a prediction model fs(\u00b7, \u03b8s). The parameters are learned minimizing classical negative log-likelihood (NLL). This training step is shown as blue data flow in Fig. 2.\n2. Then the prediction model for fs is fine-tuned in a subsequent step by weakly supervised training on real data, resulting in a refined prediction model fs(\u00b7, \u03b8\u2032s). This step, shown as green data flow in Fig. 2, is described in more detail in Subsection 3.1.\n3. Finally, the regression learner fr is trained on real data. It is implemented as a mapping fr(\u00b7, \u03b8r) : (D,N) \u2192 z from a full size input depth imageD and a full size segmentation map N to a joint location vector z. Parameters \u03b8r are trained classically by minimizing the L2 norm between output joint positions and ground truth joint positions. This training step is shown as red data flow in Fig. 2."}, {"heading": "3.1. Weakly supervised fine-tuning of the segmentation learner", "text": "Supervised pre-training of the segmentation learner results in a prediction model fs(\u00b7, \u03b8s), which will be fine-tuned by training on real data. Since no ground truth segmentation maps exist for this data, we generate a loss function for training based on two sources:\n\u2022 sparse information in the form of ground truth joint positions, and\n\u2022 a priori information on the local distribution of part labels on human hands through a patch-wise restoration process, which aligns noisy predictions with a large dictionary of synthetic poses.\nThe weakly supervised training procedure is shown as green data flow in Fig. 2: Each real depth image is passed through the pre-trained segmentation learner fs, resulting in a segmentation map. This noisy predicted map is restored through a restoration process fnn described further below. The quality of the restored segmentation map is estimated by comparing it to the set of ground truth joint positions for this image. In particular, for each joint, a corresponding part-label is identified, and the barycenter of the corresponding pixels in the segmentation map is calculated. A rough quality measure for a segmentation map can be given as the sum (over joints) of the L2 distances between barycenters and ground truth joint positions. The quality measure is used to determine whether the restoration process has lead to an improvement in segmentation quality, i.e. whether the barycenters of the restored map are closer to the ground truth joint positions than the barycenters of the original prediction. For images where this is the case, the segmentation learner is updated for each pixel, minimizing NLL using the labels of the restored map as artificial \u201cground truth\u201d.\nPatchwise restoration \u2014 We proceed patchwise and extract patches of size P\u00d7P from a large set of synthetic segmentation images, resulting in a dictionary of patches P={p(l)}, l\u2208{1 . . .N}. In our experiments, we used patches of size 27\u00d727 and a dictionary of 36 million patches is extracted from the training set (see Section 5). As also reported in [25], the range of poses which can occur in natural motion is extremely large, making full global matching with pose datasets difficult. This motivates our patch-based approach, which aims to match at a patch-local level rather than matching whole images.\nA given real input depth imageD is aligned with this dictionary in a patchwise process using the intermediate representation. For each pixel j, a patch qj is extracted from the segmentation produced by the learner fs, and the nearest neighbor is found by searching the dictionary P :\n\u03bd(q(j)) = arg min p(l)\u2208P dH(q (j),p(l)), (1)\nFigure 3: Organization of the regression learner fr. All functional modules are shown in red and produced feature maps are shown in blue. The grey area corresponds to masked regions on the feature maps.\nwhere dH(q,p) is the Hamming distance between the two patches q and p. The search performed in (1) can be calculated efficiently using KD-trees.\nIn a na\u0131\u0308ve setting, a restored label for pixel j could be obtained by choosing the label of the center of the retrieved patch n(j). This however leads to noisy restorations, which suggest the need for spatial context. Instead of chosing the center label of each patch only, we propose to use all of the labels in each patch. For each input pixel, the nearest neighbor results in a local window of size W\u00d7W are integrated. In particular, for a given pixel j, the assigned patches n(k) of all neighbors k are examined and the position of pixel j in each patch is calculated. A label is estimated by voting, resulting in a mapping fnn(.):\nfnn(q (j)) = argmax\nl\n\u2211\nk\u2208W\u00d7W\nI(l = n(k,j@k)) (2)\nwhere n(k)=\u03bd(q(j)) is the nearest neighbor result for pixel k, n(j,m) denotes pixel m of patch n(j), I(\u03c9)=1 if \u03c9 holds and 0 else, and the expression j@k denotes the position of of pixel j in the patch centered on neighbor k.\nThis integration bears some similarity to [7], where information of nearest neighbor searches is integrated over a local window, albeit through averaging a continuous mapping. It is also similar to the patchwise integration performed in structured prediction forests [15].\nIf real-time performance is not required, the patch alignment process in Equation 1 can be regularized with a graphical model including pairwise terms favoring consistent assignments, for instance, a Potts model or a term favoring patch assignments with consistent overlaps. Interestingly, this produces only very small gains in performance, especially given the higher computational complexity. Moreover, the gains vanish if local integration (Equation 2) is added. More information is given in Section 5 and in appendix A."}, {"heading": "4. Architectures", "text": "The structure of the segmentation learner fs is motivated by the idea of performing efficient pixelwise image segmentation preserving original resolution of the input, and is inspired by OverFeat networks which were proposed for object detection and localization [22, 19]. The learner consists of 3 convolutional layers, followed by a fully connected layer. Max pooling, which typically follows convolutional layers, results in downsampling of feature maps, destroying precise spatial information. Instead, we perform pooling over 2\u00d7 2 overlapping regions produced by shifting the feature maps obtained at the previous step by a single pixel along one or another axis. As opposed to patchwise train-\ning of pixel classification based on its local neighborhood, such an architecture is more computationally efficient, as it benefits from dense computations at earlier layers.\nThe regression learner, taking a depth image as a single input and producing 3 coordinates for a given joint, also incorporates the information provided by the segmentation learner in its training. Structurally, it resembles an inception unit [26, 17] where the output of the first convolutional layer after max pooling as passed through several parallel feature extractors capturing information at different levels of localization. The organization of this module is shown in Fig. 3. The output of the first convolutional layer c1 (followed by pooling p1) is aligned with the segmentation maps produced by the segmentation learner. From each feature map, for a given joint we extract a localized region of interest filtered by the mask of a hand part to which it belongs (or a number of hand parts which are naturally closest to this joint). These masks are calculated by performing morphological opening on the regions having the corresponding class label in the segmentation map. Once the local region\nis selected, the rest of the feature map area is set to 0. The result, along with the original feature maps is then fed to the next layer, i.e. an inception module. The rest of the training process is organized in such way that the network\u2019s capacity is split between global structure of the whole image and the local neighborhood, and a subset of inception 3 \u00d7 3 filters is learned specifically from the local area surrounding the point of interest.\nBoth learners have ReLU activation functions at each layer and employ batch normalization [9]. The regression learner during test time uses the batch normalization parameters estimated on the training data, however in the segmentation network, the batch normalization is performed across all pixels from the same image, for both training and test samples. Finally, the regression learner is trained by gradient descent using the Adam [13] update rule."}, {"heading": "5. Experimental results", "text": "We evaluated the proposed method on the NYU Hand Pose Dataset, which was published in [32] and is publicly avail-\nable 1. It comprises 70,000 images captured with a depth sensor in VGA resolution accompanied by ground truth annotations of positions of hand joints.\nTo train the segmentation learner, the synthetic training images were selected from our own dataset consisting of two subsets: (i) 170,974 synthetic training images rendered using the commercial software \u201cPoser\u201d, including ground truth; (ii) 500 labelled synthetic images plus ground truth reserved for testing.\nIn our experiments, we extract hand images of normalized metric size (taking into account depth information) and normalize them to 48\u00d748 pixels. The data is preprocessed by local contrast normalization with a kernel size of 9. For supervised training of the segmentation learner and the regression learner, the full set of 170,974 training images is used. A third of this set is used to extract patches for the patch alignment mapping fnn, giving a dictionary of 36M patches of size 27\u00d727 extracted from 56,991 images. Local integration as given in equation (2) was done of windows of size W\u00d7W = 17\u00d717.\nConvNets were implemented using the Torch7 library [4]. NN-search using KD-trees was performed using the FLANN library [18].\nSegmentation performance \u2014 To evaluate the performance of the various segmentation methods, we manually labelled 100 images from the NYU dataset and report accuracy per pixel and per class in Table 2. These 100 images\n1http://cims.nyu.edu/\u02dctompson/NYU_Hand_Pose_\nDataset.htm\nwere solely used for evaluation and never entered any training procedure.\nPurely supervised training on the synthetic dataset gives poor performance of 51.03% accuracy per pixel. We emphasize once more that training was performed on synthetic while we test on real images, thus of an unseen distribution. This domain shift is clearly a problem, as accuracy on the synthetic dataset is very high, 90.16%. Using patchwise restoration of the predicted real patches with the large dictionary of synthetic patches gives a performance increase of +3.5 percentage points per pixel and +7 percentage points per class. This corroborates our intuition that the intermediate representation carries important structural information. Integration of patch-labels over a local window with equation (2) is essential, pure NN-search without integration performs poorly.\nWe also compared the local integration of Equation (2) to potentially more powerful regularization methods by implementing a CRF-like discrete energy function with unary terms based on Hamming distances on patches and pairwise terms defined as (i) a Potts model, or alternatively, (ii) pairwise terms defined as the Hamming distance between overlapping portions of synthetic patches assigned to neighboring pixels. More information is given in appendix A. Interestingly, local patch integration outperformed the combinatorial model by a large margin.\nTable 3 shows the contribution of weakly supervised training, where sparse annotation (joint positions) are integrated into the training process of the segmentation learner\nfs. This procedure achieves an improvement of +6.15 percentage points per pixel and +7.82 percentage points per class, an essential step in learning an efficient intermediate representation.\nHand joint position estimation \u2014 Table 4 illustrates the effect of incorporating segmentation information on the regression learner performance. In the top part of the table we provide information on our own baselines, in the middle there are results of other deep learning methods reported in the literature. The error is expressed as mean distance in mm (in 2D or 3D) between predicted position of each joint and its ground truth location. In comparison to a single network regressor, the 2D mean error was improved by 15.7%. The second best model, cascade regression, was inspired by the work of [33], where the initial rough estimation of hand joints positions is then improved by zooming in\nWe should note here, that the quality of network outputs can be further improved by optimization through inverse kinematics, as it has been done, for example, in [32]. However, the focus of this work is to explore the potential of pure learning approaches with no priors enforcing structure on the output. The bottom part of the table contains non deep learning methods. In a recent work [28] on optimization of hand pose estimation formulated as an inverse kinematics problem, the authors report performance similar to [32] in terms of 2D UV-error (no error in mm provided).\nComputational complexity \u2014 All models have been trained and tested using GPUs, except the patchwise restoration process which is pure CPU code and not used at test time. Estimating the pose of a single hand takes 31 ms if the segmentation resolution is set to 24\u00d724 pixels (which includes the forward passes of both learners fs (12 ms) and fr (18 ms)) and 58 ms for 48\u00d748 segmentation outputs (40 ms for fs, corresponding to the results reported in the experiments section of the paper). Training of the segmentation network requires up to 24 hours to minimize validation error, while the regression network is trained in 20 min on a single GPU. The results were obtained using a cluster configured with 2 x E5-2620 v2 Hex-core processors, 64 GB RAM and 3 x Nvidia GTX Titan Black cards with 6GB memory per card."}, {"heading": "6. Conclusion", "text": "We presented a method for hand pose estimation based on an intermediate representation which is fused with raw depth input data. We showed that the additional structured information of this representation provides important cues for joint regression which leads to lower error. Weakly supervised learning of the mapping from depth to segmentation maps from a mixture of densely labelled synthetic data and from sparsely labelled real data is a key component of the proposed method. Weak supervision is dealt\nwith by patch-wise alignment of real data to synthetic data performed in the space of the intermediate representation, exploiting its strong geometric and topological properties."}, {"heading": "7. Acknowledgements", "text": "This work has been partly financed through the French grant Interabot, a project of type \u201cInvestissement\u2019s d\u2019Avenir / Briques Ge\u0301ne\u0301riques du Logiciel Embarque\u0301\u201d, and by the ANR project SoLStiCe (ANR-13-BS02-0002-01), a project of the grant program \u201cANR blanc\u201d. G. Taylor acknowledges the support of NSERC and CFI."}, {"heading": "A. Comparison with graphical models", "text": "In this section, we give more information on the graphical models we implemented for comparison and which where mentioned in section 5. We implemented a CRF-like discrete energy function (see Table 2). The goal of the optimization problem is to regularize the patchwise restoration process described in the main paper. Instead of choosing the nearest neighbor in patch space for each pixel as described in equation (1), a solution is searched which satisfies certain coherence conditions over spatial neighborhoods. To this end, we create a global energy function E(x) defined on a 2D-lattice corresponding to the input image to restore:\nE(x) = \u2211\ni\nu(xi) + \u03b1 \u2211\ni\u223cj\nb(xi, xj)\nwhere i\u223cj indices neighbors i and j. Each pixel i is assigned a discrete variable xi taking values between 1 and N=10, where xi = l signifies that for pixel i the l\u2212th nearest neighbor in patch space is chosen. For each pixel i, a nearest neighbor search is performed using KD-trees and a ranked list of N neighbors is kept defining the label space for this pixel. The variableN controls the degree of approximation of the model, where N=\u221e allows each pixel to be assigned every possible patch of the synthetic dictionary.\nThe unary data term u(x) guides the solution towards approximations with low error. It is defined as the Hamming distance between the original patch and the synthetic patch.\nWe tested two different pairwise terms b(xi, xj):\nPotts-like terms \u2014 a Potts model classically favors equality of labels of neighboring sites. In our setting, we favor equality of the center pixels of the two patches assigned to xi and xj .\nPatch-overlap distance \u2014 the alternative pairwise term is defined as the Hamming distance between the two synthetic patches defined by xi and xj , in particular, the distance restricted to the overlapping area.\nInference was performed through message passing using the open-GM library [1], and the hyper-parameter \u03b1 was opti-\nmized through grid-search. Interestingly, local patch integration with equation (2) outperformed the combinatorial models significantly while at the same time being much faster."}], "references": [{"title": "OpenGM: A c++ library for discrete graphical models", "author": ["T.B. Andres", "Beier", "J. Kappes"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Estimating 3D hand pose from a cluttered image", "author": ["V. Athitsos", "Z. Liu", "Y. Wu", "J. Yuan"], "venue": "CVPR. IEEE,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Articulated pose estimation by a graphical model with image dependent pairwise relations", "author": ["X. Chen", "A. Yuille"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "NIPS BigLearn Workshop,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Model-based 3d hand pose estimation from monocular video", "author": ["M. de La Gorce", "D. Fleet", "N. Paragios"], "venue": "IEEE- Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Multi-task learning of facial landmarks and expression", "author": ["T. Devries", "K. Biswaranjan", "G. Taylor"], "venue": "14th Canadian Conference on Computer and Robot Vision (CRV),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "N-Fields: Neural Network Nearest Neighbor Fields for Image Transforms", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "ACCV,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Depth-based hand pose estimation: methods, data, and challenges", "author": ["J.S.S. III", "G. Rogez", "Y. Yang", "J. Shotton", "D. Ramanan"], "venue": "ICCV,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167v3,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning human pose estimation features with convolutional networks", "author": ["A. Jain", "J. Tompson", "M. Andriluka", "G. Taylor", "C. Bregler"], "venue": "ICLR,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Hand pose estimation and hand shape classification using multi-layered randomized decision forests", "author": ["C. Keskin", "F. Kirac", "Y. Kara", "L. . Akarun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Hand pose estimation and hand shape classification using multi-layered randomized decision forests", "author": ["C. Keskin", "F. Kira\u00e7", "Y. Kara", "L. Akarun"], "venue": "ECCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J.L. Ba"], "venue": "ICLR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Hands deep in deep learning for hand pose estimation", "author": ["D. Kingma", "J.L. Ba"], "venue": "CVWW,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured labels in random forests for semantic labelling and object detection", "author": ["P. Kontschieder", "S. Bulo", "M. Pelillo", "H. Bischof"], "venue": "ICCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Modelbased hand pose estimation via spatial-temporal hand parsing and 3D fingertip localization", "author": ["H. Liang", "J. Yuan", "D. Thalmann", "Z. Zhang"], "venue": "In The Visual Computer,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D. Lowe"], "venue": "VISAPP, pages 331\u2013340,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Hand segmentation with structured convolutional learning", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "In ACCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Efficient model-based 3D tracking of hand articulations using Kinect", "author": ["I. Oikonomidis", "N. Kyriazis", "A. Argyros"], "venue": "BMVC, pages 101.1\u2013101.11,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Realtime and Robust Hand Tracking from Depth", "author": ["C. Qian", "X. Sun", "Y. Wei", "X. Tang", "J. Sun"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "ICLR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional regression forests for human pose estimation", "author": ["J. Shotton"], "venue": "CVPR, pages 3394\u20133401,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "A. Fitzgibbon", "M. Cook", "T. Sharp", "M. Finocchio", "R. Moore", "A. Kipman", "A. Blake"], "venue": "CVPR, pages 1297\u20131304,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Interactive markerless articulated hand motion tracking using RGB and depth data", "author": ["S. Sridhar", "A. Oulasvirta", "C. Theobalt"], "venue": "ICCV,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["C. Szegedy", "W. Liu", "J. Yangqing", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. R"], "venue": "shift. arXiv:1502.03167v3,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Latent regression forest: Structured estimation of 3d hand posture", "author": ["D. Tang", "H. Chang", "A. Tejani", "T.-K. Kim"], "venue": "CVPR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Opening the black box: Hierarchical sampling optimization for estimating human hand pose", "author": ["D. Tang", "J. Taylor", "K. Pushmeet", "C. Keskin", "T.-K. Kim", "J. Shotton"], "venue": "ICCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Real-time articulated hand pose estimation using semi-supervised transductive regression forests", "author": ["D. Tang", "T. Yu", "T.-K. Kim"], "venue": "ICCV,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation", "author": ["J. Taylor", "J. Shotton", "T. Sharp", "A. Fitzgibbon"], "venue": "CVPR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint training of a convolutional network and a graphical model for human pose estimation", "author": ["J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time continuous pose recovery of human hands using convolutional networks", "author": ["J. Tompson", "M. Stein", "Y. LeCun", "K. Perlin"], "venue": "SIGGRAPH,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "DeepPose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient hand pose estimation from a single depth image", "author": ["C. Xu", "L. Cheng"], "venue": "ICCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive 3D shape segmentation using sparse reconstruction", "author": ["W. Xu", "Z. Shi", "M. Xu", "K. Zhou", "J. Wang", "B. Zhou", "J. Wang", "Z. Yuan"], "venue": "Transactions on Graphics,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "Computer Vision\u2013 ECCV 2014, pages 94\u2013108. Springer,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "Experiments on the NYU dataset [32] show", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "While the estimation of full-body pose is now available at real-time in commercial products, at least in cooperative environments [24], the estimation of Figure 1: A rich intermediate representation is fused with raw input for hand joint regression.", "startOffset": 130, "endOffset": 134}, {"referenceID": 31, "context": "The input image is from the NYU dataset [32].", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "Similar to earlier work [24, 12], we first construct an intermediate representation based on a segmentation into parts.", "startOffset": 24, "endOffset": 32}, {"referenceID": 11, "context": "Similar to earlier work [24, 12], we first construct an intermediate representation based on a segmentation into parts.", "startOffset": 24, "endOffset": 32}, {"referenceID": 31, "context": "While more recent work on body and hand pose estimation tends to perform direct regression from depth or color input to joint positions [32, 31, 3, 23], we argue that the intermediate representation is a powerful tool in the special context of training with multiple heterogeneous training sets.", "startOffset": 136, "endOffset": 151}, {"referenceID": 30, "context": "While more recent work on body and hand pose estimation tends to perform direct regression from depth or color input to joint positions [32, 31, 3, 23], we argue that the intermediate representation is a powerful tool in the special context of training with multiple heterogeneous training sets.", "startOffset": 136, "endOffset": 151}, {"referenceID": 2, "context": "While more recent work on body and hand pose estimation tends to perform direct regression from depth or color input to joint positions [32, 31, 3, 23], we argue that the intermediate representation is a powerful tool in the special context of training with multiple heterogeneous training sets.", "startOffset": 136, "endOffset": 151}, {"referenceID": 22, "context": "While more recent work on body and hand pose estimation tends to perform direct regression from depth or color input to joint positions [32, 31, 3, 23], we argue that the intermediate representation is a powerful tool in the special context of training with multiple heterogeneous training sets.", "startOffset": 136, "endOffset": 151}, {"referenceID": 23, "context": "While purely supervised training on synthetic data has proven to work well for full-body pose estimation [24, 12], hand pose estimation is known to require real data captured from depth sensors for training [29, 27, 32] due to low input resolution and data quality.", "startOffset": 105, "endOffset": 113}, {"referenceID": 11, "context": "While purely supervised training on synthetic data has proven to work well for full-body pose estimation [24, 12], hand pose estimation is known to require real data captured from depth sensors for training [29, 27, 32] due to low input resolution and data quality.", "startOffset": 105, "endOffset": 113}, {"referenceID": 28, "context": "While purely supervised training on synthetic data has proven to work well for full-body pose estimation [24, 12], hand pose estimation is known to require real data captured from depth sensors for training [29, 27, 32] due to low input resolution and data quality.", "startOffset": 207, "endOffset": 219}, {"referenceID": 26, "context": "While purely supervised training on synthetic data has proven to work well for full-body pose estimation [24, 12], hand pose estimation is known to require real data captured from depth sensors for training [29, 27, 32] due to low input resolution and data quality.", "startOffset": 207, "endOffset": 219}, {"referenceID": 31, "context": "While purely supervised training on synthetic data has proven to work well for full-body pose estimation [24, 12], hand pose estimation is known to require real data captured from depth sensors for training [29, 27, 32] due to low input resolution and data quality.", "startOffset": 207, "endOffset": 219}, {"referenceID": 23, "context": "Body part segmentation as an intermediate representation for joint estimation from depth images was successfully used in [24], where random forests were trained to perform pixel-wise classification.", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "This was adapted to hand pose estimation in [12], where an additional pose clustering step was introduced.", "startOffset": 44, "endOffset": 48}, {"referenceID": 28, "context": "In [29], a random forest is learned, which performs different tasks at different levels: viewpoint clustering in higher levels, part segmentation in middle levels, and joint regression in lower levels.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In follow-up work [27], a latent regression forest is learned, which automatically extracts a hierarchical and topological model of a human hand from data.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "Recent work estimates joint positions by regression with deep convolutional neural nets [32, 3, 31, 33, 10].", "startOffset": 88, "endOffset": 107}, {"referenceID": 2, "context": "Recent work estimates joint positions by regression with deep convolutional neural nets [32, 3, 31, 33, 10].", "startOffset": 88, "endOffset": 107}, {"referenceID": 30, "context": "Recent work estimates joint positions by regression with deep convolutional neural nets [32, 3, 31, 33, 10].", "startOffset": 88, "endOffset": 107}, {"referenceID": 32, "context": "Recent work estimates joint positions by regression with deep convolutional neural nets [32, 3, 31, 33, 10].", "startOffset": 88, "endOffset": 107}, {"referenceID": 9, "context": "Recent work estimates joint positions by regression with deep convolutional neural nets [32, 3, 31, 33, 10].", "startOffset": 88, "endOffset": 107}, {"referenceID": 9, "context": "Post-processing to enforce structural constraints is based on graphical models [10, 31, 3] or inverse kinematics [32].", "startOffset": 79, "endOffset": 90}, {"referenceID": 30, "context": "Post-processing to enforce structural constraints is based on graphical models [10, 31, 3] or inverse kinematics [32].", "startOffset": 79, "endOffset": 90}, {"referenceID": 2, "context": "Post-processing to enforce structural constraints is based on graphical models [10, 31, 3] or inverse kinematics [32].", "startOffset": 79, "endOffset": 90}, {"referenceID": 31, "context": "Post-processing to enforce structural constraints is based on graphical models [10, 31, 3] or inverse kinematics [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "In [14] a deep learning framework is regularized by a bottleneck layer, enforcing the network to model underlying structure of joint positions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In a recent overview [8] the deep learning model were shown to perform the best among existing approaches, but still far from human performance.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "Graphical models \u2014 In [3], a graphical model is implemented with deep convolutional nets, which jointly estimate unary terms, given indications of joint types and positions, and binary terms, modelling relationships between joints.", "startOffset": 22, "endOffset": 25}, {"referenceID": 30, "context": "In [31], a deep full-body part detector is jointly learned with a Markov Random Field which models spatial priors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "[20] is based on pixelwise comparison of rendered and observed depth maps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Inverse rendering of a generative model including shading and texture is used for model fitting in [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "In [16], ICP is employed for hand pose reconstruction and 3D fingertip localization under spatial and temporal constraints.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "A hybrid method [21] for real-time hand tracking uses a simple hand model consisting of a number of spheres.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "In [25], a person-specific and hybrid generative/discriminative model is built for a system using five RGB cameras plus a ToF camera.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [30], a correspondence between depth pixels and vertices of an articulated 3D model is learned.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "In [2], approximate directed Chamfer distances are used to align observed edge images with a synthetic dataset.", "startOffset": 3, "endOffset": 6}, {"referenceID": 34, "context": "Patchwise alignment of segmentations in a transductive learning setting has been performed on 3D meshes [35], matching real unlabelled shape segments to shapes from a large labelled database.", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "Our method can be viewed as a kind of multitask learning, a topic actively pursued by the deep vision community [6, 36].", "startOffset": 112, "endOffset": 119}, {"referenceID": 35, "context": "Our method can be viewed as a kind of multitask learning, a topic actively pursued by the deep vision community [6, 36].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": "Our work bears a certain resemblance to the recently proposed N-Fields [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "However, whereas in [7], NN-search is performed in a feature space learned by deep networks, our method performs NN-search in a patch space corresponding to semantic segmentation learned by deep networks.", "startOffset": 20, "endOffset": 23}, {"referenceID": 14, "context": "This part of our work bears also some similarity to the way label information is integrated in structured prediction forests [15], although no patch alignment with a dictionary is carried out there.", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "As in [25], we also sample different pose parameters and hand shape parameters.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "In contrast to other weakly-supervised or semisupervised settings, for instance [29], we do not suppose that any ground truth data for the intermediate representation is available for the real training images.", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "We do, however, rely on ground truth for joint positions, which can be obtained in several ways: In [24], external motion capture using markers is employed.", "startOffset": 100, "endOffset": 104}, {"referenceID": 31, "context": "In [32], training data is acquired in a multi-view setting from three different", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "As also reported in [25], the range of poses which can occur in natural motion is extremely large, making full global matching with pose datasets difficult.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "This integration bears some similarity to [7], where information of nearest neighbor searches is integrated over a local window, albeit through averaging a continuous mapping.", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "It is also similar to the patchwise integration performed in structured prediction forests [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 21, "context": "The structure of the segmentation learner fs is motivated by the idea of performing efficient pixelwise image segmentation preserving original resolution of the input, and is inspired by OverFeat networks which were proposed for object detection and localization [22, 19].", "startOffset": 263, "endOffset": 271}, {"referenceID": 18, "context": "The structure of the segmentation learner fs is motivated by the idea of performing efficient pixelwise image segmentation preserving original resolution of the input, and is inspired by OverFeat networks which were proposed for object detection and localization [22, 19].", "startOffset": 263, "endOffset": 271}, {"referenceID": 25, "context": "Structurally, it resembles an inception unit [26, 17] where the output of the first convolutional layer after max pooling as passed through several parallel feature extractors capturing information at different levels of localization.", "startOffset": 45, "endOffset": 53}, {"referenceID": 16, "context": "Structurally, it resembles an inception unit [26, 17] where the output of the first convolutional layer after max pooling as passed through several parallel feature extractors capturing information at different levels of localization.", "startOffset": 45, "endOffset": 53}, {"referenceID": 8, "context": "Both learners have ReLU activation functions at each layer and employ batch normalization [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "Finally, the regression learner is trained by gradient descent using the Adam [13] update rule.", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": "We evaluated the proposed method on the NYU Hand Pose Dataset, which was published in [32] and is publicly avail-", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "[14] 12.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "0[8] 19.", "startOffset": 1, "endOffset": 4}, {"referenceID": 13, "context": "[14] 27.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] 30.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] 34.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] 7.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "0[14] -", "startOffset": 1, "endOffset": 5}, {"referenceID": 33, "context": "Xu and Cheng [34] 58.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "0[8] Keskin et al.", "startOffset": 1, "endOffset": 4}, {"referenceID": 10, "context": "[11] 72.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "5[8] -", "startOffset": 1, "endOffset": 4}, {"referenceID": 3, "context": "ConvNets were implemented using the Torch7 library [4].", "startOffset": 51, "endOffset": 54}, {"referenceID": 17, "context": "NN-search using KD-trees was performed using the FLANN library [18].", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "The second best model, cascade regression, was inspired by the work of [33], where the initial rough estimation of hand joints positions is then improved by zooming in We should note here, that the quality of network outputs can be further improved by optimization through inverse kinematics, as it has been done, for example, in [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "The second best model, cascade regression, was inspired by the work of [33], where the initial rough estimation of hand joints positions is then improved by zooming in We should note here, that the quality of network outputs can be further improved by optimization through inverse kinematics, as it has been done, for example, in [32].", "startOffset": 330, "endOffset": 334}, {"referenceID": 27, "context": "In a recent work [28] on optimization of hand pose estimation formulated as an inverse kinematics problem, the authors report performance similar to [32] in terms of 2D UV-error (no error in mm provided).", "startOffset": 17, "endOffset": 21}, {"referenceID": 31, "context": "In a recent work [28] on optimization of hand pose estimation formulated as an inverse kinematics problem, the authors report performance similar to [32] in terms of 2D UV-error (no error in mm provided).", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Inference was performed through message passing using the open-GM library [1], and the hyper-parameter \u03b1 was opti-", "startOffset": 74, "endOffset": 77}], "year": 2017, "abstractText": "We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset [32] show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%.", "creator": "LaTeX with hyperref package"}}}