{"id": "1511.06811", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Learning visual groups from co-occurrences in space and time", "abstract": "we propose a self - referencing framework that learns to group visual structures equally onto their rate of super - occurrence in space and time. to model thematic dependencies between the entities, we set itself a simple binary classification problem in which the goal arrives to predict if two visual primitives occur in one same wide dynamic temporal context. we apply this framework to three levels : studying patch affinities from spatial adjacency in images, learning physical affinities from temporal adjacency in videos, or learning photo affinities requiring geospatial proximity in image collections. we demonstrate that in each case the learned affinities uncover meaningful semantic chunks. from patch affinities we generate object proposals that are competitive with work - of - the - art supervised methods. from frame profiles we generate movie scene segmentations that correlate well with dvd chapter structure. finally, from geospatial affinities we learn groups that relate well to semantic place categories.", "histories": [["v1", "Sat, 21 Nov 2015 01:33:12 GMT  (2322kb,D)", "http://arxiv.org/abs/1511.06811v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["phillip isola", "daniel zoran", "dilip krishnan", "edward h adelson"], "accepted": false, "id": "1511.06811"}, "pdf": {"name": "1511.06811.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Phillip Isola", "Dilip Krishnan"], "emails": ["phillipi@berkeley.edu", "danielz@mit.edu", "dilipkay@google.com", "adelson@mit.edu"], "sections": [{"heading": null, "text": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories."}, {"heading": "1 INTRODUCTION", "text": "Clown fish live next to sea anemones, lightning is always accompanied by thunder. When looking at the world around us, we constantly notice which things go with which. These associations allow us to segment and organize the world into coherent visual representations.\nThis paper addresses how representations like \u201cobjects\u201d and \u201cscenes\u201d might be learned from natural visual experience. A large body of work has focused on learning these representations as a supervised problem (e.g., by regressing on image labels) and current object and scene classifiers are highly effective. However, in the absence of expert annotations, it remains unclear how we might uncover these representations in the first place. Do \u201cobjects\u201d fall directly out of the statistics of the environment, or are they a more subjective, human-specific construct?\nHere we probe the former hypothesis. Because the physical world is highly structured, adjacent locations are usually semantically related, whereas far apart locations are more often semantically distinct. By modeling spatial and temporal dependencies, we may therefore learn something about semantic relatedness.\nWe investigate how these dependences may be learned from unlabeled sensory input. We train a deep neural network to predict whether or not two input images or patches are likely to be found next to each other in space or time. We demonstrate that the network learns dependencies that can be used to uncover meaningful visual groups. We apply the method to generate fast and accurate object proposals that are competitive with recent supervised methods, as well as to automatic movie scene segmentation, and to the grouping of semantically related photographs.\nar X\niv :1\n51 1.\n06 81\n1v 1\n[ cs\n.L G\n] 2\n1 N\nov 2"}, {"heading": "2 RELATED WORK", "text": "The idea that perceptual groups reflect statistical structure in the environment has deep roots in the perception and cognition literature (Barlow (1985); Wilkin & Tenenbaum (1985); Tenenbaum & Witkin (1983); Lowe (2012); Rock (1983)). Barlow postulated that the brain is constantly on the lookout for events that co-occur much more often than chance, and uses these \u201csuspicious coincidences\u201d to discover underlying structure in the world (Barlow (1985)). Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al. (2013)).\nVisual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al. (2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Dolla\u0301r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Dolla\u0301r (2014), or learning to group with direct supervision Dolla\u0301r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.\nGrouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries. Both these methods require modeling generative probability distributions, which restricts their ability to scale to high-dimensional data. Our model, on the other hand, is discriminative and can be easily scaled.\nA recent line of work in representation learning has taken a similar tack, training discriminative models to predict one aspect of raw sensory data from another. This work may be termed selfsupervised learning and has a number of flavors. The common theme is exploiting spatial and/or temporal structure as supervisory signals. Mobahi et al. (2009) learn a feature embedding such that features adjacent in time are similar and features far apart in time are dissimilar. Srivastava et al. (2015) predict future frames in a video, and rely on strict temporal ordering; extension to spatial or unordered data is unclear. Wang & Gupta (2015) use a siamese triplet loss to learn a representation that can track patches through a video. They rely on training input from a separate tracking algorithm. Agrawal et al. (2015) as well as Jayaraman & Grauman (2015) regress on egomotion sig-\nnals to learn a representation. Finally, Doersch et al. (2015) learn features by predicting the relative orientation between patches within an image.\nEach of these works focus on learning good generic features, which may then be applied as pretraining for a supervised model. Our current goal is rather different. Rather than learning a vector space representation of images, we search for more explicit structure, in the form of visual groups. We show that the learned groups are semantically meaningful in and of themselves. This differs from the usual approach in feature learning, where the features are not necessarily interpretable, but are instead used as an intermediate representation on top of which further models can be trained."}, {"heading": "3 MODELING VISUAL AFFINITIES BY PREDICTING CO-OCCURRENCE", "text": "We would like to group visual primitives, A and B, based on the probability that they belong to the same semantic entity. A and B may, for example, be two image patches, in which case we would want to group them if they belong to the same visual object.\nGiven object labels, a straightforward approach to this problem would be to train a supervised classifier to predict indicator variable Q \u2208 {0, 1}, where Q = 1 iff A and B lie on the same object Manen et al. (2013). Throughout this paper, we use Q to indicate the property that A and B share the same semantic label.\nAcquiring training data for Q may require time-consuming and expensive annotation. We instead will explore an alternative strategy. Instead of training a classifier to predict Q directly, we train classifiers to predict spatial or temporal proximity, denoted by C \u2208 {0, 1}. Because the semantics of the world change slowly over space and time, we hope that C might serve as a cheap proxy for Q (c.f. Kayser et al. (2001); Wiskott & Sejnowski (2002)). The degree to which this is true is an empirical question, which we will test below. Throughout the paper, C = 1 iff A and B are nearby each other in space or time.\nFormally, we model the affinity between visual primitives A and B as\nw(A,B) = P (C = 1|A,B) + P (C = 1|B,A)\n2 . (1)\nIn other words, we model affinity as the probability that two primitives will co-occur within some context (with symmetry between the order of A and B enforced). We will then use this affinity metric to cluster primitives, in particular using spectral clustering (Section 4).\nThis affinity can be understood in a variety of ways. First, as described above, it can be seen as a proxy for what we are really after, P (Q = 1|A,B). Second, it is a measure of the spatial/temporal dependence betweenA andB. Applying Bayes\u2019 rule we can see that P (C = 1|A,B) \u221d P (A,B|C=1)P (A,B) , which factors to P (A,B|C=1)P (A)P (B) when A and B are independent. Therefore, if we sample primitives iid\nin space or time1, our affinity models how much more often A and B appear nearby each other than they would if they were independent (the rate of co-occurrences were they independent would simply be P (A)P (B)). This value is closely related to the pointwise mutual information (PMI) between A and B conditioned on C = 1. Previous work on visual grouping Isola et al. (2014) and word embeddings (Church & Hanks (1990); Levy et al. (2014)) has found PMI to be an effective measure for these tasks."}, {"heading": "3.1 PREDICTING CO-OCCURRENCES WITH A CNN", "text": "To model w(A,B) we use a Convolutional Neural Net (CNN) with a Siamese-style architecture (Figure 2, Chopra et al. (2005)), which we implement in Caffe (Jia et al. (2014)). The network has two convolutional branches, one to process A and the other to process B, with shared weights. These branches can be regarded as feature extractors. The features are then concatenated and fed to a set of fully connected layers that compare the features and try to predict C. We use a logistic loss over C and train all models with stochastic gradient descent. Our objective can be expressed as\nE(A,B, C; \u03b8) = \u22121 N N\u2211 1 Ci log(\u03c3(f(Ai,Bi; \u03b8)) + (1\u2212 Ci) log(1\u2212 \u03c3(f(Ai,Bi; \u03b8)) (2)\nwhere \u03b8 are the net parameters we optimize over (weights and biases), N is the number of training examples, \u03c3 is the logistic function, and f is a neural net. For each of our experiments,N = 500, 000 training examples, 50% of which are positive (C = 1) and 50% negative (C = 0). We examine three domains: 1) learning to group patches based on their spatial adjacency in images, 2) learning to group video frames based on their temporal adjacency in movies, and 3) learning to group photos based on their geospatial proximity.\nEach task corresponds to a different choice ofA,B, C, andQ. In each case, we analyze performance at predicting C and at predicting Q, comparing our CNN to baseline grouping cues. Each baseline corresponds to a measure of the similarity between the primitives. Similarity measures like these are commonly used in visual grouping algorithms Arbelaez et al. (2011); Faktor & Irani (2012). Full results of this analysis are given in Table 1. In all cases, our co-occurrence classifier matches or outperforms the baselines."}, {"heading": "3.2 PATCH AFFINITIES FROM CO-OCCURRENCE IN IMAGES", "text": "We first examine whether or not predicting patch co-occurrence in images results in an effective affinity measure for object-level grouping. We set A and B to be 17 \u00d7 17 pixel patches (with circular masks). The context function C is spatial adjacency. Positive examples (C = 1) are pairs of adjacent patches (with no overlap between them) and negative examples (C = 0) are pairs sampled from random locations across the dataset. We sample training patches from the Pascal VOC 2012 training set.\nWe train a CNN (two convolutional layers, two fully connected layers; Figure 2) to model P (C = 1|A,B). Figure 3 shows a t-SNE visualization of the learned affinities (van der Maaten & Hinton\n1In each of our applications we sample 50% positive (C = 1) and 50% negative (C = 0) examples. Under our logistic regression model, this results in a function monotonically related to what we would get if we had sampled iid (see appendix B.4 in King & Zeng (2001))\n(2009)). As can be seen, the network learns to associate patches with different kinds of structure such as texture, local features, and color similarities.\nTo evaluate performance, we sample 10,000 patches from the Pascal VOC 2012 validation set, 50% with C = 1 and 50% with C = 0. In Table 1 we measure the Average Precision of using several affinity measures as a binary classifier of either C or Q. In this case, we defined Q to indicate whether or not the center pixel of the two patches lies on the same labeled object instance. To testQ independently from C we create theQ test set by only sampling from patch pairs for which C = 1 (so the net cannot do well at predictingQ simply by doing well at predicting C). Our network performs well relative to the baseline affinity metrics, although color histogram similarity does reach a similar performance on predicting Q. Even though it was only trained to predict C, our method is effective at predictingQ as well, achieving an average precision (AP) of 0.80. This validates that spatial proximity, C, is a good surrogate for \u201csame object\u201d, Q. This raises the question, would we do any better if we directly trained on Q? We tested this, training a new network on 50% patches with Q = 1 and 50% with Q = 0. This net achieves higher performance on predicting Q (AP = 0.85) and lower performance at predicting C (AP = 0.92), than our net trained to predict C. Therefore, although predicting co-occurrence may be a decent proxy for predicting semantic sameness, there is still a gap in performance compared to directly training on Q. Designing better context functions, C, that narrow this gap is an important direction for future research."}, {"heading": "3.3 FRAME AFFINITIES FROM CO-OCCURRENCE IN MOVIES", "text": "Our framework can also be applied to learning temporal associations. To test this, we set A and B to be frames, cropped and down sampled to 33 \u00d7 33 pixels, from a set of 96 movies sampled from the top 100 rated movies on IMDB2. In this setting, C indicates temporal adjacency \u2013 specifically,\n2http://www.imdb.com/\ntwo frames are assigned C = 1 if they are at least 3 seconds from each other and not more than 10 seconds apart. C = 0 otherwise. Again we train a CNN to model P (C = 1|A,B) (three convolutional layers, two fully connected layers). To evaluate predicting C, we train on half the movies and test on the remaining half. Our method can learn to predict C quite effectively, reaching an Average Precision of 0.95 on the test set. How do the learned temporal associations relate to semantic visual scenes? To test this, we compared against DVD chapter annotations, setting Q to be \u201cdo these two frames occur in the same DVD chapter?\u201d We sample 10,000 frame pairs, 50% with Q = 1 and 50% with Q = 0, while holding C constant (so that good performance at predicting Q cannot be achieved simply by doing well at predicting C). Our network achieves an AP of 0.67 on this task. Similar to above, we can then see that temporal adjacency, C, is an effective surrogate for learning about semantic sameness, Q."}, {"heading": "3.4 PHOTO AFFINITIES FROM GEOSPATIAL CO-OCCURRENCE", "text": "Just as an object is a collection of associated patches, and a movie scene is a collection of associated frames, a visual place can be viewed a collection of associated photographs. Here we set A and B to be geotagged photos, cropped and down sampled to 33 \u00d7 33 pixels, and C indicates whether or not A and B are taken within 11 meters of one another (we exclude exact duplicate locations).\nUsing the same CNN architecture as for the movie frame network, we again learn P (C = 1|A,B), but for this new setting of the variables. We train on five cities selected from the MIT City Database Zhou et al. (2014) and test predicting C on a held out set of three more cities from that dataset. We also test how well the network predicts place semantics. For this, we define Q as \u201cdo these two photos belong to the same place category?\u201d We test this task on the LabelMe Outdoors dataset Liu et al. (2009) for which each photo was assigned to one of eight place categories (e.g., \u201ccoast\u201d, \u201chighway\u201d, \u201ctall building\u201d). Our network shows promising performance on this task, reaching 0.79 AP on predicting Q. HOG similarity reaches the same performance, which corroborates past findings that HOG is effective at grouping related photos (Dalal & Triggs (2005)).\nNotice that while HOG does well on associating photographs, it does not do well at associating movie frames nor image patches. On the other hand, color histogram similarity does well on associating image patches and movie frames, but fails at grouping everyday photographs \u2013 while patches on an object, or frames in a movie scene, may tend to all use a consistent color palette, tourist photos of the same location will have high color variance, due to seasonal and lighting variations. Different grouping rules will be effective at different tasks. Our learning based approach has the advantage that it automatically figures out the appropriate grouping cue for each new domain, and thereby achieves good performance on all our tasks."}, {"heading": "3.5 WHICH CUES DID THE NETWORKS LEARN TO USE?", "text": "In each domain tested above, the grouping rules may be very different. Here we study them by probing the trained networks with controlled stimuli. Similar to how a psychophysicist might experiment on human perception, we show our networks specially made stimuli. For each test, we feed the networks many pairs {A,B}, sampled from locations such that C = 1. We leaveA unaltered, but modify B in a controlled way. This allows us to test what kinds of transformations of B will change\nthe network\u2019s prediction as to wether or not C = 1 (c.f. Lenc & Vedaldi (2014)). We consider the following modifications: rotation by 90\u25e6, mirroring vertically and horizontally, removal of color (by replacing each pixel with its mean over color channels), and a darkening transformation in which we multiply each color channel by 0.5.\nResults of these tests are given in Table 2. Each number is the mean output of the network for a given test case. The left-most column provides the mean output without any transformation applied. Interestingly, the patch network is almost entirely invariant to 90\u25e6rotations and mirror flips \u2013 according to the network, sharing a common orientation does not increase the probability of two patches being nearby. On the other hand, the patch network\u2019s output depends dramatically on color similarity. The frame network behaves similarly, but shows some sensitivity to geometric transformations. On the other hand, the geo-photo network exhibits the opposite pattern of behavior: it\u2019s output is changed more when geometric transformationsare applied than when color transformations are applied. According to the geo-network, two photos may have different color compositions and still be likely to be nearby."}, {"heading": "4 FROM PREDICTING CO-OCCURRENCE TO FORMING VISUAL GROUPS", "text": "We apply the following general approach to learning visual groups:\n1. Define A, B, and C based on the domain. 2. Learn P (C|A,B) using a CNN. 3. Setup a graph in which ANi=1 and B N i=1 are nodes and edge weights are given byw(Ai,Bi)\n(Eqn. 1). Then partition the graph into visual groups using spectral clustering."}, {"heading": "4.1 FINDING OBJECTS", "text": "As demonstrated in Section 4, patches that predictably co-occur usually belong to the same object. This suggests that we can localize objects in an image by grouping patches using our co-occurrence affinity. We focus on the specific problem of \u201cobject proposals\u201d (Zitnick & Dolla\u0301r (2014); Krahnenbuhl & Koltun (2015)), where the goal is to localize all objects in an image.\nWe use the patch associations P (C = 1|A,B) defined in Section 4, trained on the Pascal VOC 2012 training set. Follow previous benchmarks, we test on the validation set. Given a test image, we sample all 17 \u00d7 17 patches at a stride of 8 pixels. We construct a graph in which each patch is a node and nodes are connected by an edge if the spatial distance between the patch centers is at least 17 pixels and no more than 33 pixels. Each patch is multiplied by a circular mask so that no two\npatches connected by an edge see any overlapping pixels (see Figure 2(right)). Each edge, indexed by i, j, is weighted by Wi,j = w(Ai,Bi)\u03b1, resulting in the affinity matrix W, where we use the value \u03b1 = 20 in our experiments.\nTo globalize the associations, we apply spectral clustering to the matrix W. First we create the Laplacian eigenmap L for W, using the 2nd through 16th eigenvectors with largest eigenvalues. Each eigenvector is scaled by \u03bb\u2212 1 2 where \u03bb is the corresponding eigenvalue. We then generate object proposals simply by applying k-means to the Laplacian eigenmap. To generate more than a few proposals, we run k-means multiple times with random restarts and for values of k from 5 to 16. Finally, we prune redundant proposals and sort proposals to achieve diversity throughout the ranking (by encouraging proposals to be made from different values of k before giving proposals from a random restart at the same value of k).\nQualitative results from our method are shown in Figure 4. In each case, we show the proposals that have best overlap with the ground truth object masks for 100 proposals. We quantitatively compare against other recent methods in Figure 5. Even though our method is not trained on labeled images, it reaches performance comparable to recent supervised methods at proposing up to 100 objects per image. Our implementation runs in about 4 seconds per image on a 2015 Macbook Pro."}, {"heading": "4.2 SEGMENTING MOVIES", "text": "Just as objects are composed of associated patches, scenes in a movie are composed of associated frames. Here we show how our learned frame affinities can be used to break a movie into coherent scenes, a problem that has received some prior attention (Chen et al. (2008); Zhai & Shah (2006)).\nTo segment a movie, we build a graph in which each frame is a node and all frames within ten seconds of one another are connected by an edge. We then weight the edges using the frameassociations P (C = 1|A,B) (Section 4), and partition the graph using spectral clustering. To evaluate, we use DVD chapter annotations as ground truth. Following a standard evaluation procedure in image boundary detection (Arbelaez et al. (2011)), we measure performance on the retrieval task of finding all ground truth boundaries. In Figure 6(right), we compare against the baseline affinity metrics from Section 4. In each case, we apply the same spectral clustering pipeline as for our method, except with edge weights given by the baseline metrics instead of by our net. It is important to properly scale each similarity metric or else spectral clustering will fail. To provide fair comparison, we sweep over a range of scale factors \u03b1, setting edge weights as exp(w(A,B)\u03b12 ), where w is the affinity measure. In Figure 6(right) we show results selecting the optimal \u03b1 for each method.\nOur approach finds more boundaries with higher precision than these baselines, except for color histogram similarity, which reaches a similar performance. Figure 6 (left) shows an example segmentation of a section of The Two Towers. The movie is displayed as a \u201cmovie barcode\u201d3 in which each frame is squished into a single column and time advances to the right. On top are the DVD chapter annotations, and on the bottom are our inferred boundaries."}, {"heading": "4.3 DISCOVERING PLACE CATEGORIES", "text": "Taking the geospatial-associations model from Section 4, we cluster photos into coherent types of places. Here we create a fully connected graph between all photos in a given collection, weight the edges with P (C = 1|A,B) and then apply spectral clustering to partition the collection. We test the purity of the clusters on LabelMe Outdoors dataset (Liu et al. (2009)). Clustering purity versus number of clusters k is given in Figure 7 (right), showing that our method is effective at discovering semantic place categories. As in our movie segmentation experiments, we select the optimal \u03b1 to scale the affinity of our method as well each baseline. Figure 7 (left) shows random sample images from each cluster after clustering into 8 categories. This clustering has 59% purity."}, {"heading": "5 CONCLUSION", "text": "We have presented a simple and general approach to learning visual groupings, which requires no pre-defined labels. Instead our framework uses co-occurrence in space or time as a supervisory signal. By doing so, we learn different clustering mechanisms for a variety of tasks. Our approach achieves competitive results on object proposal generation, even when compared to methods trained on labeled data. Additionally, we demonstrated that the same method can be used to segment movies into scenes and to uncover semantic place categories. The principles underlying the framework are quite general and may be applicable to data in other domains, when there are natural co-occurrence signals and groupings.\n3http://moviebarcode.tumblr.com/"}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank William T. Freeman, Joshua B. Tenenbaum, and Alexei A. Efros for helpful feedback and discussions. Thanks to Andrew Owens for helping collect the movie dataset. This work is supported by NSF award 1161731, Understanding Translucency, and by Shell Research."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Joao", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1505.01596,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Measuring the objectness of image windows", "author": ["Alexe", "Bogdan", "Deselaers", "Thomas", "Ferrari", "Vittorio"], "venue": null, "citeRegEx": "Alexe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alexe et al\\.", "year": 2012}, {"title": "Contour detection and hierarchical image segmentation", "author": ["Arbelaez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Arbelaez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arbelaez et al\\.", "year": 2011}, {"title": "Cerebral cortex as model builder", "author": ["Barlow", "Horace"], "venue": "Models of the visual cortex, pp", "citeRegEx": "Barlow and Horace.,? \\Q1985\\E", "shortCiteRegEx": "Barlow and Horace.", "year": 1985}, {"title": "A computational approach to edge detection", "author": ["Canny", "John"], "venue": "PAMI, (6):679\u2013698,", "citeRegEx": "Canny and John.,? \\Q1986\\E", "shortCiteRegEx": "Canny and John.", "year": 1986}, {"title": "Movie scene segmentation using background information", "author": ["Chen", "Liang-Hua", "Lai", "Yu-Chun", "Liao", "Hong-Yuan Mark"], "venue": "Pattern Recognition,", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Bing: Binarized normed gradients for objectness estimation at 300fps", "author": ["Cheng", "Ming-Ming", "Zhang", "Ziming", "Lin", "Wen-Yan", "Torr", "Philip"], "venue": "In CVPR,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Church", "Kenneth Ward", "Hanks", "Patrick"], "venue": "Computational linguistics,", "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "Histograms of oriented gradients for human detection", "author": ["Dalal", "Navneet", "Triggs", "Bill"], "venue": "In CVPR,", "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["Doersch", "Carl", "Gupta", "Abhinav", "Efros", "Alexei A"], "venue": "CoRR, abs/1505.05192,", "citeRegEx": "Doersch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2015}, {"title": "Structured forests for fast edge detection", "author": ["Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In ICCV,", "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Doll\u00e1r et al\\.", "year": 2013}, {"title": "Clustering by composition\u2013unsupervised discovery of image categories", "author": ["Faktor", "Alon", "Irani", "Michal"], "venue": "In ECCV", "citeRegEx": "Faktor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Faktor et al\\.", "year": 2012}, {"title": "Co-segmentation by composition", "author": ["Faktor", "Alon", "Irani", "Michal"], "venue": "In ICCV,", "citeRegEx": "Faktor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Faktor et al\\.", "year": 2013}, {"title": "Unsupervised statistical learning of higher-order spatial structures from visual scenes", "author": ["Fiser", "J\u00f3zsef", "Aslin", "Richard N"], "venue": "Psychological science,", "citeRegEx": "Fiser et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fiser et al\\.", "year": 2001}, {"title": "Crisp boundary detection using pointwise mutual information", "author": ["Isola", "Phillip", "Zoran", "Daniel", "Krishnan", "Dilip", "Adelson", "Edward H"], "venue": "In ECCV,", "citeRegEx": "Isola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2014}, {"title": "Learning image representations equivariant to ego-motion", "author": ["Jayaraman", "Dinesh", "Grauman", "Kristen"], "venue": "arXiv preprint arXiv:1505.02206,", "citeRegEx": "Jayaraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jayaraman et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Extracting slow subspaces from natural videos leads to complex cells", "author": ["Kayser", "Christoph", "Einh\u00e4user", "Wolfgang", "D\u00fcmmer", "Olaf", "K\u00f6nig", "Peter", "K\u00f6rding", "Konrad"], "venue": "In Artificial Neural Networks?ICANN", "citeRegEx": "Kayser et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kayser et al\\.", "year": 2001}, {"title": "Logistic regression in rare events data", "author": ["King", "Gary", "Zeng", "Langche"], "venue": "Political analysis,", "citeRegEx": "King et al\\.,? \\Q2001\\E", "shortCiteRegEx": "King et al\\.", "year": 2001}, {"title": "Geodesic object proposals", "author": ["Krahenbuhl", "Philipp", "Koltun", "Vladlen"], "venue": "In ECCV", "citeRegEx": "Krahenbuhl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Krahenbuhl et al\\.", "year": 2014}, {"title": "Learning to propose objects", "author": ["Krahnenbuhl", "Phillip", "Koltun", "Vladlen"], "venue": "In CVPR,", "citeRegEx": "Krahnenbuhl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krahnenbuhl et al\\.", "year": 2015}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["Lenc", "Karel", "Vedaldi", "Andrea"], "venue": "arXiv preprint arXiv:1411.5908,", "citeRegEx": "Lenc et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lenc et al\\.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Omer", "Goldberg", "Yoav", "Ramat-Gan", "Israel"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Nonparametric scene parsing: Label transfer via dense scene alignment", "author": ["Liu", "Ce", "Yuen", "Jenny", "Torralba", "Antonio"], "venue": "In CVPR,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Perceptual organization and visual recognition, volume", "author": ["Lowe", "David"], "venue": "Science & Business Media,", "citeRegEx": "Lowe and David.,? \\Q2012\\E", "shortCiteRegEx": "Lowe and David.", "year": 2012}, {"title": "Improving spatial support for objects via multiple segmentations", "author": ["Malisiewicz", "Tomasz", "Efros", "Alexei A"], "venue": "In BMVC,", "citeRegEx": "Malisiewicz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Malisiewicz et al\\.", "year": 2007}, {"title": "Prime object proposals with randomized prim\u2019s algorithm", "author": ["Manen", "Santiago", "Guillaumin", "Matthieu", "Gool", "Luc Van"], "venue": "In ICCV,", "citeRegEx": "Manen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Manen et al\\.", "year": 2013}, {"title": "Deep learning from temporal coherence in video", "author": ["Mobahi", "Hossein", "Collobert", "Ronan", "Weston", "Jason"], "venue": null, "citeRegEx": "Mobahi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mobahi et al\\.", "year": 2009}, {"title": "The logic of perception", "author": ["Rock", "Irvin"], "venue": null, "citeRegEx": "Rock and Irvin.,? \\Q1983\\E", "shortCiteRegEx": "Rock and Irvin.", "year": 1983}, {"title": "Statistical learning by 8-month-old infants", "author": ["Saffran", "Jenny R", "Aslin", "Richard N", "Newport", "Elissa L"], "venue": null, "citeRegEx": "Saffran et al\\.,? \\Q1926\\E", "shortCiteRegEx": "Saffran et al\\.", "year": 1926}, {"title": "Neural representations of events arise from temporal community structure", "author": ["Schapiro", "Anna C", "Rogers", "Timothy T", "Cordova", "Natalia I", "Turk-Browne", "Nicholas B", "Botvinick", "Matthew M"], "venue": "Nature Neuroscience,", "citeRegEx": "Schapiro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schapiro et al\\.", "year": 2013}, {"title": "Normalized cuts and image", "author": ["Shi", "Jianbo", "Malik", "Jitendra"], "venue": "segmentation. PAMI,", "citeRegEx": "Shi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2000}, {"title": "Discovering objects and their location in images", "author": ["Sivic", "Josef", "Russell", "Bryan C", "Efros", "Alexei", "Zisserman", "Andrew", "Freeman", "William T"], "venue": "In Computer Vision,", "citeRegEx": "Sivic et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sivic et al\\.", "year": 2005}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "On the role of structure in vision", "author": ["Tenenbaum", "Jay M", "Witkin", "AP"], "venue": "Human and machine vision,", "citeRegEx": "Tenenbaum et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 1983}, {"title": "Selective search for object recognition", "author": ["Uijlings", "Jasper RR", "van de Sande", "Koen EA", "Gevers", "Theo", "Smeulders", "Arnold WM"], "venue": null, "citeRegEx": "Uijlings et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uijlings et al\\.", "year": 2013}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2009}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Wang", "Xiaolong", "Gupta", "Abhinav"], "venue": "arXiv preprint arXiv:1505.00687,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "What is perceptual organization for", "author": ["Wilkin", "Andrew P", "Tenenbaum", "Jay M"], "venue": "From Pixels to Predicates,", "citeRegEx": "Wilkin et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Wilkin et al\\.", "year": 1985}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["Wiskott", "Laurenz", "Sejnowski", "Terrence J"], "venue": "Neural computation,", "citeRegEx": "Wiskott et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wiskott et al\\.", "year": 2002}, {"title": "Video scene segmentation using markov chain monte carlo", "author": ["Zhai", "Yun", "Shah", "Mubarak"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "Zhai et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2006}, {"title": "Recognizing City Identity via Attribute Analysis of Geo-tagged Images", "author": ["B. Zhou", "Liu", "A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["Zitnick", "C Lawrence", "Doll\u00e1r", "Piotr"], "venue": "In ECCV", "citeRegEx": "Zitnick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al.", "startOffset": 106, "endOffset": 128}, {"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al.", "startOffset": 106, "endOffset": 240}, {"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al. (2013)).", "startOffset": 106, "endOffset": 264}, {"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al. (2013)). Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al.", "startOffset": 106, "endOffset": 391}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al.", "startOffset": 126, "endOffset": 149}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al. (2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al.", "startOffset": 126, "endOffset": 170}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al. (2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al.", "startOffset": 126, "endOffset": 215}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al. (2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al.", "startOffset": 126, "endOffset": 243}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others.", "startOffset": 102, "endOffset": 122}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others.", "startOffset": 102, "endOffset": 147}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others.", "startOffset": 102, "endOffset": 176}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)).", "startOffset": 102, "endOffset": 359}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 470}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 495}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 565}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 594}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al.", "startOffset": 102, "endOffset": 770}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)).", "startOffset": 102, "endOffset": 820}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model.", "startOffset": 102, "endOffset": 842}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries.", "startOffset": 102, "endOffset": 978}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries. Both these methods require modeling generative probability distributions, which restricts their ability to scale to high-dimensional data. Our model, on the other hand, is discriminative and can be easily scaled. A recent line of work in representation learning has taken a similar tack, training discriminative models to predict one aspect of raw sensory data from another. This work may be termed selfsupervised learning and has a number of flavors. The common theme is exploiting spatial and/or temporal structure as supervisory signals. Mobahi et al. (2009) learn a feature embedding such that features adjacent in time are similar and features far apart in time are dissimilar.", "startOffset": 102, "endOffset": 1709}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries. Both these methods require modeling generative probability distributions, which restricts their ability to scale to high-dimensional data. Our model, on the other hand, is discriminative and can be easily scaled. A recent line of work in representation learning has taken a similar tack, training discriminative models to predict one aspect of raw sensory data from another. This work may be termed selfsupervised learning and has a number of flavors. The common theme is exploiting spatial and/or temporal structure as supervisory signals. Mobahi et al. (2009) learn a feature embedding such that features adjacent in time are similar and features far apart in time are dissimilar. Srivastava et al. (2015) predict future frames in a video, and rely on strict temporal ordering; extension to spatial or unordered data is unclear.", "startOffset": 102, "endOffset": 1855}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries. Both these methods require modeling generative probability distributions, which restricts their ability to scale to high-dimensional data. Our model, on the other hand, is discriminative and can be easily scaled. A recent line of work in representation learning has taken a similar tack, training discriminative models to predict one aspect of raw sensory data from another. This work may be termed selfsupervised learning and has a number of flavors. The common theme is exploiting spatial and/or temporal structure as supervisory signals. Mobahi et al. (2009) learn a feature embedding such that features adjacent in time are similar and features far apart in time are dissimilar. Srivastava et al. (2015) predict future frames in a video, and rely on strict temporal ordering; extension to spatial or unordered data is unclear. Wang & Gupta (2015) use a siamese triplet loss to learn a representation that can track patches through a video.", "startOffset": 102, "endOffset": 1998}, {"referenceID": 0, "context": "Agrawal et al. (2015) as well as Jayaraman & Grauman (2015) regress on egomotion sig-", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2015) as well as Jayaraman & Grauman (2015) regress on egomotion sig-", "startOffset": 0, "endOffset": 60}, {"referenceID": 10, "context": "Finally, Doersch et al. (2015) learn features by predicting the relative orientation between patches within an image.", "startOffset": 9, "endOffset": 31}, {"referenceID": 26, "context": "Given object labels, a straightforward approach to this problem would be to train a supervised classifier to predict indicator variable Q \u2208 {0, 1}, where Q = 1 iff A and B lie on the same object Manen et al. (2013). Throughout this paper, we use Q to indicate the property that A and B share the same semantic label.", "startOffset": 195, "endOffset": 215}, {"referenceID": 18, "context": "Kayser et al. (2001); Wiskott & Sejnowski (2002)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Kayser et al. (2001); Wiskott & Sejnowski (2002)).", "startOffset": 0, "endOffset": 49}, {"referenceID": 15, "context": "Previous work on visual grouping Isola et al. (2014) and word embeddings (Church & Hanks (1990); Levy et al.", "startOffset": 33, "endOffset": 53}, {"referenceID": 15, "context": "Previous work on visual grouping Isola et al. (2014) and word embeddings (Church & Hanks (1990); Levy et al.", "startOffset": 33, "endOffset": 96}, {"referenceID": 15, "context": "Previous work on visual grouping Isola et al. (2014) and word embeddings (Church & Hanks (1990); Levy et al. (2014)) has found PMI to be an effective measure for these tasks.", "startOffset": 33, "endOffset": 116}, {"referenceID": 7, "context": "To model w(A,B) we use a Convolutional Neural Net (CNN) with a Siamese-style architecture (Figure 2, Chopra et al. (2005)), which we implement in Caffe (Jia et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 7, "context": "To model w(A,B) we use a Convolutional Neural Net (CNN) with a Siamese-style architecture (Figure 2, Chopra et al. (2005)), which we implement in Caffe (Jia et al. (2014)).", "startOffset": 101, "endOffset": 171}, {"referenceID": 2, "context": "Similarity measures like these are commonly used in visual grouping algorithms Arbelaez et al. (2011); Faktor & Irani (2012).", "startOffset": 79, "endOffset": 102}, {"referenceID": 2, "context": "Similarity measures like these are commonly used in visual grouping algorithms Arbelaez et al. (2011); Faktor & Irani (2012). Full results of this analysis are given in Table 1.", "startOffset": 79, "endOffset": 125}, {"referenceID": 41, "context": "We train on five cities selected from the MIT City Database Zhou et al. (2014) and test predicting C on a held out set of three more cities from that dataset.", "startOffset": 60, "endOffset": 79}, {"referenceID": 24, "context": "For this, we define Q as \u201cdo these two photos belong to the same place category?\u201d We test this task on the LabelMe Outdoors dataset Liu et al. (2009) for which each photo was assigned to one of eight place categories (e.", "startOffset": 132, "endOffset": 150}, {"referenceID": 24, "context": "For this, we define Q as \u201cdo these two photos belong to the same place category?\u201d We test this task on the LabelMe Outdoors dataset Liu et al. (2009) for which each photo was assigned to one of eight place categories (e.g., \u201ccoast\u201d, \u201chighway\u201d, \u201ctall building\u201d). Our network shows promising performance on this task, reaching 0.79 AP on predicting Q. HOG similarity reaches the same performance, which corroborates past findings that HOG is effective at grouping related photos (Dalal & Triggs (2005)).", "startOffset": 132, "endOffset": 500}, {"referenceID": 5, "context": "The papers compared to are: BING (Cheng et al. (2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 5, "context": "The papers compared to are: BING (Cheng et al. (2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al.", "startOffset": 34, "endOffset": 90}, {"referenceID": 5, "context": "The papers compared to are: BING (Cheng et al. (2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al.", "startOffset": 34, "endOffset": 124}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al.", "startOffset": 91, "endOffset": 145}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al. (2013)), Sel.", "startOffset": 91, "endOffset": 184}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al. (2013)), Sel. Search (Uijlings et al. (2013)).", "startOffset": 91, "endOffset": 222}, {"referenceID": 5, "context": "Here we show how our learned frame affinities can be used to break a movie into coherent scenes, a problem that has received some prior attention (Chen et al. (2008); Zhai & Shah (2006)).", "startOffset": 147, "endOffset": 166}, {"referenceID": 5, "context": "Here we show how our learned frame affinities can be used to break a movie into coherent scenes, a problem that has received some prior attention (Chen et al. (2008); Zhai & Shah (2006)).", "startOffset": 147, "endOffset": 186}, {"referenceID": 24, "context": "Figure 7: Left: Clustering the LabelMe Outdoor dataset (Liu et al. (2009)) into 8 groups using our learned affinities.", "startOffset": 56, "endOffset": 74}, {"referenceID": 24, "context": "Figure 7: Left: Clustering the LabelMe Outdoor dataset (Liu et al. (2009)) into 8 groups using our learned affinities. Random sample images are shown from each group. Right: Photo cluster purity versus number of clusters k. Note that we trained our model on an independent dataset, the MIT City dataset (Zhou et al. (2014)).", "startOffset": 56, "endOffset": 323}, {"referenceID": 2, "context": "Following a standard evaluation procedure in image boundary detection (Arbelaez et al. (2011)), we measure performance on the retrieval task of finding all ground truth boundaries.", "startOffset": 71, "endOffset": 94}, {"referenceID": 24, "context": "We test the purity of the clusters on LabelMe Outdoors dataset (Liu et al. (2009)).", "startOffset": 64, "endOffset": 82}], "year": 2015, "abstractText": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.", "creator": "LaTeX with hyperref package"}}}