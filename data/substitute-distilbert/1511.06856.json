{"id": "1511.06856", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Data-dependent Initializations of Convolutional Neural Networks", "abstract": "convolutional neural networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. despite this, few researchers dare to train their models from scratch. most now builds on one currently s handful of typically pre - trained models, and fine - tunes significantly adapts these for specific tasks. this dates in large part due to the difficulty of properly initializing these networks from scratch. a lengthy miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. regarding this work we present a fast and simple data - dependent initialization behaviour, because sets the weights in a network mean that all units covering the network train regularly roughly the same rate, avoiding vanishing or exploding gradients. thus initialization matches the current be - of - the - art unsupervised or self - supervised pre - training methods between standard computer vision tasks, such as image classification and simultaneous detection, while being demonstrated three orders of magnitude faster. when combined with pre - training methods, such initialization significantly outperforms prior processing, covering the gap between supervised and unsupervised pre - training.", "histories": [["v1", "Sat, 21 Nov 2015 09:07:08 GMT  (1809kb,D)", "http://arxiv.org/abs/1511.06856v1", "12 pages, Under review at ICLR 2016"], ["v2", "Fri, 29 Apr 2016 03:36:16 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06856v2", "ICLR 2016"], ["v3", "Thu, 22 Sep 2016 22:14:17 GMT  (1951kb,D)", "http://arxiv.org/abs/1511.06856v3", "ICLR 2016"]], "COMMENTS": "12 pages, Under review at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["philipp kr\\\"ahenb\\\"uhl", "carl doersch", "jeff donahue", "trevor darrell"], "accepted": true, "id": "1511.06856"}, "pdf": {"name": "1511.06856.pdf", "metadata": {"source": "CRF", "title": "CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Philipp Kr\u00e4henb\u00fchl", "Carl Doersch", "Jeff Donahue", "Trevor Darrell"], "emails": ["philkr@eecs.berkeley.edu;", "jdonahue@eecs.berkeley.edu;", "trevor@eecs.berkeley.edu;", "cdoersch@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, Convolutional Neural Networks (CNNs) have improved performance across a wide variety of computer vision tasks (Szegedy et al., 2015; Simonyan & Zisserman, 2015; Girshick, 2015). Much of this improvement stems from the ability of CNNs to use large datasets better than previous methods. In fact, good performance seems to require large datasets: the best-performing methods usually begin by \u201cpre-training\u201d CNNs to solve the million-image ImageNet classification challenge (Russakovsky et al., 2015). This \u201cpre-trained\u201d representation is then \u201cfine-tuned\u201d on a smaller dataset where the target labels may be more expensive to obtain. These fine-tuning datasets generally do not fully constrain the CNN learning: different initializations can be trained until they achieve equally high training-set performance, but they will often perform very differently at test time. For example, initialization via ImageNet pre-training is known to produce a better-performing network at test time across many problems. However, little else is known about which other factors affect a CNN\u2019s generalization performance when trained on small datasets. There is a pressing need to understand these factors, first because we can potentially exploit them to improve performance on tasks where few labels are available. Second they may already be confounding our attempts to evaluate pre-training methods. A pre-trained network which extracts useful semantic information but cannot be fine-tuned for spurious reasons can be easily overlooked. Hence, this work aims to explore how to better fine-tune CNNs. We show that simple statistical properties of the network, which can be easily measured using training data, can have a significant impact on test time performance. Surprisingly, we show that controlling for these statistical properties leads to a fast and general way to improve performance when training on relatively little data.\nEmpirical evaluations have found that when transferring deep features across tasks, freezing weights of some layers during fine-tuning generally harms performance (Yosinski et al., 2014). These results suggest that, given a small dataset, it is better to adjust all of the layers a little rather than to adjust just a few layers a large amount, and so perhaps the ideal setting will adjust all of the layers the same amount. While these studies did indeed set the learning rate to be the same for all layers, somewhat counterintuitively this does not actually enforce that all layers learn at the same rate.\nar X\niv :1\n51 1.\n06 85\n6v 1\n[ cs\n.C V\n] 2\n1 N\nov 2\n01 5\nTo see this, say we have a network where there are two convolution layers separated by a ReLU. Multiplying the weights and bias term of the first layer by a scalar \u03b1 > 0, and then dividing the weights (but not bias) of the next (higher) layer by the same constant \u03b1 will result in a network which computes exactly the same function. However, note that the gradients of the two layers are not the same: they will be divided by \u03b1 for the first layer, and multiplied by \u03b1 for the second. Worse, an update of a given magnitude will have a smaller effect on the lower layer than the higher layer, simply because the lower layer\u2019s norm is now larger. Using this kind of reparameterization, it is easy to make the gradients for certain layers vanish during fine-tuning, or even to make them explode, resulting in a network that is impossible to fine-tune despite representing exactly the same function. Conversely, this sort of re-parameterization gives us a tool we can use to calibrate layer-by-layer learning to improve fine-tuning performance, provided we have an appropriate principle for making such adjustments.\nWhere can we look to find such a principle? A number of works have already suggested that statistical properties of network activations can impact network performance. Many focus on initializations which control the variance of network activations. Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al. (2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initialization for networks with tanh non-linearities, while He et al. (2015) focus on the more commonly used ReLUs. However, none of the above papers consider more general network including pooling, dropout, LRN layers (Krizhevsky et al., 2012), or DAG-structured networks (Szegedy et al., 2015). We argue that applying the network to real training data can improve these approximations and achieve better performance. Early approaches to data-driven initializations showed that whitening the activations at all layers can mitigate the vanishing gradient problem (LeCun et al., 1998), but it does not ensure all layers train at an equal rate. More recently, batch normalization (Ioffe & Szegedy, 2015) enforces that the output of each convolution and fully-connected layer are zero mean with unit variance for every batch. In practice, however, this means that the network\u2019s behavior on a single example depends on the other members of the batch, and removing this dependency at test-time relies on approximating batch statistics. The fact that these methods show improved convergence speed at training time suggests we are justified in investigating the statistics of activations. However, the main goal of our work differs in two important respects. First, these previous works pay relatively little attention to the behavior on smaller training sets, instead focusing on training speed. Second, while all above initializations require a random initialization, our approach aims to handle structured initialization, and even improve pre-trained networks."}, {"heading": "2 PRELIMINARIES", "text": "We are interested in parameterizing (and re-parameterizing) CNNs, where the output is a highly non-convex function of both the inputs and the parameters. Hence, we begin with some notation which will let us describe how a CNN\u2019s behavior will change as we alter the parameters. We focus on feed-forward networks of the form\nzk = fk(zk\u22121; \u03b8k),\nwhere zk is a vector of hidden activations of the network, and fk is a transformation with parameters \u03b8k. fk may be a linear transformation fk(z\u2032k; \u03b8k) = Wkzk\u22121 + bk, or it may be a non-linearity fk+1(zk; \u03b8k) = \u03c3k+1(z \u2032 k) such as a rectified linear unit (ReLU) \u03c3(x) = max(x, 0). Other common non-linearities include local response normalization or pooling (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan & Zisserman, 2015). However, as is common in neural networks, we assume these nonlinearities are not parametrized and kept fixed during training. Hence, \u03b8k contains only (Wk, bk) for each affine layer k.\nTo deal with spatially-structured inputs like images, most hidden activations zk \u2208 RCk\u00d7Ak\u00d7Bk are arranged in a two dimensional grid of size Ak \u00d7 Bk (for image width Ak and height Bk) with Ck channels per grid cell. We let z0 denote the input image. The final output, however, is generally not spatial, and so later layers are reduced to the form zN = RCN\u00d71\u00d71, where CN is the number\nof output units. The last of these outputs is converted into a loss with respect to some label; for classification, the approach is to convert the final output into a probability distribution over labels via a Softmax function. Learning aims to minimize the expected loss over the training dataset. Despite the non-convexity of this learning problem, backpropagation and Stochastic Gradient Descent often finds good local minima if initialized properly (LeCun et al., 1998).\nGiven an arbitrary neural network, we next aim for a good parameterization. A good parameterization should be able to learn all weights of a network equally well. We measure how well a certain weight in the network learns by how much the gradient of a loss function would change it. A large change means it learns more quickly, while a small change implies it learns more slowly. We initialize our network such that all weights in all layers learn equally fast."}, {"heading": "3 DATA-DEPENDENT INITIALIZATION", "text": "Given an N -layer neural network with loss function `(zN ), we first define C2i,j,k to be the expected norm of the gradient with respect to weights Wk(i, j) in layer k:\nC2k,i,j = Ez0\u223cD\n[( \u2202\n\u2202Wk(i, j) `(zN )\n)2] = Ez0\u223cD [( zk\u22121(j) \u2202\n\u2202zk(i) `(zN )\ufe38 \ufe37\ufe37 \ufe38\nyk(i)\n)2] , (1)\nwhere D is a set of input images (160 in all our experiments) and yk is the backpropagated error. Similar reasoning can be applied to the biases bk, but where the activations are replaced by the constant 1. To not rely on any labels during initialization, we use a random linear loss function `(zN ) = \u03b7\n>zN , where \u03b7 \u223c N (0, I) is sampled from a unit Gaussian distribution. We sample a different random loss \u03b7 for each image.\nIn order for all parameters to learn at the same \u201crate,\u201d we require the change in 1 to be proportional to the magnitude of the weights \u2016Wk\u201622 of the current layer; i.e.,\nC\u0303k,i,j = C2k,i,j \u2016Wk\u201622\n(2)\nis constant for all weights. However this is hard to enforce, because for non-linear networks the backpropagated error yk is a function of the activations zk\u22121. A change in weights that affects the activations zk\u22121 will indirectly change yk. This effect is often non-linear and hard to control or predict.\nWe thus simplify Equation (2): rather than enforce that the individual weights all learn at the same rate, we enforce that the columns of weight matrix Wk do so, i.e.:\nC\u03032k,j = 1\nN \u2211 i C\u03032k,i,j = 1 N\u2016Wk\u201622 Ez0\u223cD [ zk\u22121(j) 2\u2016yk\u20162 ] , (3)\nshould be approximately constant, where N is the number of rows of the weight matrix. As we will show in Section 4.1, all weights tend to train at roughly the same rate even though the objective does not enforce this. Looking at equation Equation (3), the relative change of a column of the weight matrix is a function of 1) the magnitude of a single activation of the bottom layer, and 2) the norm of the backpropagated gradient. The value of a single input to a layer will generally have a relatively small impact on the norm of the gradient to the entire layer. Hence, we assume zk\u22121(j) and \u2016yk\u2016 are independent, leading to the following simplification of the objective:\nC\u03032k,j \u2248 Ez0\u223cD [ zk\u22121(j)\n2 ] Ez0\u223cD [\u2016yk\u20162]\nN\u2016Wk\u201622 . (4)\nThis approximation conveniently decouples the change rate per column, which depends on zk\u22121(j)2, from the global change rate per layer, which depends on the gradient magnitude \u2016yk\u20162, allowing us to correct them in two separate steps.\nIn Section 3.1, we show how to satisfy Ez0\u223cD [ zk\u22121(i) 2 ] = ck for a layer-wise constant ck. In Section 3.2, we then adjust this layer-wise constant ck to ensure that all gradients are properly calibrated between layers, in a way that can be applied to pre-initialized networks. Finally, in Section 3.3 we present multiple data-driven weight initializations.\nAlgorithm 1 Within-layer initialization. for each affine layer k do\nInitialize weights from a zero-mean Gaussian Wk \u223c N (0, I) and biases bk = 0 Draw samples z0 \u2208 D\u0303 \u2282 D and pass them through the first k layers of the network Compute the per-channel sample mean \u00b5\u0302k(i) and variance \u03c3\u0302k(i)2 of zk(i). Rescale the weights by Wk(i, :)\u2190Wk(i, :)/\u03c3\u0302k(i) Set the bias bk(i)\u2190 \u03b2 \u2212 \u00b5\u0302k(i)/\u03c3\u0302k(i) to center activations around \u03b2.\nend for"}, {"heading": "3.1 WITHIN-LAYER WEIGHT NORMALIZATION", "text": "We aim to ensure that each channel that a layer k + 1 receives as input is similarly distributed. It is straightforward to initialize weights in affine layers such that the units have outputs following similar distributions. E.g., we could enforce that layer k activations zk(i, a, b) have Ez0\u223cD,a,b [zk(i, a, b)] = \u03b2 and Ez0\u223cD,a,b [ (zk(i, a, b)\u2212 \u03b2)2 ] = 1 simply via properly-scaled random projections, where a and b index over the 2D spatial extent of the feature map. However, we next have to contend with the nonlinearity \u03c3(.). Thankfully, most nonlinearities (such as sigmoid or ReLU) operate independently on different channels. Hence, the different channels will undergo the same transformation, and the output channels will follow the same distribution if the input channels do (though the outputs will generally not be the same distribution as the inputs). In fact, most common CNN layers that apply a homogeneous operation to uniformly-sized windows of the input with regular stride, such as local response normalization, and pooling, empirically preserve this identical distribution requirement as well, making it broadly applicable.\nWe normalize the network activations using empirical estimates of activation statistics obtained from actual data samples z0 \u223c D. In particular, for each affine layer k \u2208 {1, 2, . . . , N} in the topological ordering of the network graph, we compute the empirical mean and standard deviations for all outgoing activations and normalize the weights Wk to have unit variance and bias \u03b2. This procedure is summarized in Algorithm 1.\nThe variance of our estimate of the sample statistics falls with the size of the sample |D\u0303|. In practice, for CNN initialization, we find that on the order of just dozens of samples is typically sufficient.\nNote that this simple empirical initialization strategy guarantees affine layer activations with a particular center and scale while making no assumptions (beyond non-zero variance) about the inputs to the layer, making it robust to any exotic choice of non-linearity or other intermediate operation. This is in contrast with existing approaches designed for particular non-linearities and with architectural constraints. Extending these methods to handle operations for which they weren\u2019t designed while maintaining the desired scaling properties may be possible, but it would at least require careful thought, while our simple empirical initialization strategy generalizes to any operations and DAG architecture with no additional implementation effort.\nOn the other hand, note that for architectures which are not purely feed-forward, the assumption of identically distributed affine layer inputs may not hold. GoogLeNet (Szegedy et al., 2015), for example, concatenates layers which are computed via different operations on the same input, and hence may not be identically distributed, before feeding the result into a convolution. Our method cannot guarantee identically distributed inputs for arbitrary DAG-structured networks, so it should be applied to non-feed-forward networks with care."}, {"heading": "3.2 BETWEEN-LAYER SCALE ADJUSTMENT", "text": "Because the initialization given in Section 3.1 results in activations zk(i) with unit variance, the expected change rate C2k,i of a column i of the weight matrix Wk is constant across all columns i, under the approximation given in Equation (4). However, this does not provide any guarantee of the scaling of the change rates between layers.\nWe use an iterative procedure to obtain roughly constant parameter change rates C2k,i across all layers k (as well as all columns i within a layer), given previously-initialized weights. At each iteration we estimate the average change ratio (C\u0303k,i,j) per layer. We also estimate a global change\nAlgorithm 2 Between-layer normalization. Draw samples z0 \u2208 D\u0303 \u2282 D repeat\nCompute loss Ez0\u223cD\u0303 [`(zN )] and gradient magnitudes Ez0\u223cD\u0303 [\u2016yk\u20162] Compute the ratio C\u0303k = Ej [ C\u0303k,j ] Compute the average ratio C\u0303 = ( \u220f k Ck) 1/N\nCompute a scale correction rk = ( C\u0303/C\u0303k )\u03b1/2 with a damping factor \u03b1 < 1\nCorrect the weights and biases of layer k: bk \u2190 rkbk, Wk \u2190 rkWk Undo the scaling rk in the layer above\nuntil Convergence (roughly 10 iterations)\nratio, as the geometric mean of all layer-wise change ratios. We then scale the parameters for each layer to be closer to this global change ratio. We simultaneously undo this scaling in the layer above, such that the function that the entire network computes is unaffected. For homogeneous nonlinearities, such as ReLU, Pooling or LRN, this scaling can be undone at in the next affine layer, and for other non-linearities we insert a special scaling layer. The between-layer scale adjustment procedure is summarized in Algorithm 2. Adjusting the scale of all layers simultaneously can lead to an oscillatory behavior. To prevent this we add a small damping factor \u03b1 (usually \u03b1 = 0.25).\nWith a relatively small number of steps (we use 10), this procedure results in roughly constant initial change rates of the parameters in all layers of the network, regardless of its depth."}, {"heading": "3.3 WEIGHT INITIALIZATIONS", "text": "Until now, we used a random Gaussian initialization of the weights, but our procedure does not require this. Hence, we explored two data-driven initializations: a PCA-based initialization and a k-means based initialization. For the PCA-based initialization, we set the weights such that the layer outputs are white and decorrelated. For each layer k we record the features activations zk\u22121 of each channel c across all spatial locations and across 160 images. Then then use the first M principal components of those activations as our weight matrix W (k). For the k-means based initialization, we follow Coates & Ng (2012) and apply spherical k-means on whitened feature activations. We use the cluster centers of k-means as initial weights for our layers, such that each output unit corresponds to one centroid of k-means. k-means usually does a better job than PCA, as it captures the modes of the input data, instead of merely decorrelating it. We use both k-means and PCA on just the convolutional layers of the architecture, as we don\u2019t have enough data to estimate the required number of weights for fully connected layers."}, {"heading": "4 EVALUATION", "text": "We implement our initialization and all experiments in the open-source deep learning framework Caffe (Jia et al., 2014). To assess how easily a network can be fine-tuned with limited data, we use the classification and detection challenges in PASCAL VOC 2007 (Everingham et al., 2014), which contains 5011 images for training and 4952 for testing.\nArchitectures Most of our experiments are performed on the 8 layer CaffeNet architecture a small modification of AlexNet (Krizhevsky et al., 2012). We use the default architecture for all comparisons, except for Doersch et al. (2015) which removed groups in the convolutional layers. We also show results on the much deeper GoogLeNet (Szegedy et al., 2015) and VGG (Simonyan & Zisserman, 2015) architectures.\nImage classification The VOC image classification task is to predict the presence or absence of each of 20 object classes in an image. For this task we fine-tune all networks using a sigmoid crossentropy loss on random crops of each image. We optimize each network via Stochastic Gradient Descent (SGD) for 80,000 iterations with an initial learning rate of 0.001 which we reduce by a\nfactor of 0.5 every 10,000 iterations, batch size of 10, and momentum of 0.9. The total training takes one hour on a Titan X GPU for CaffeNet. We tried different settings for various methods, but found these setting to work best for all initializations. At test time we average 10 random crops of the image to determine the presence or absence of an object. The CNN estimates the likelihood that each object is present, which we use as a score to compute a precision-recall curve per class. We evaluate all algorithms using mean average precision (mAP) (Everingham et al., 2014).\nObject detection In addition to predicting the presence of absence of an object in a scene, object detection requires the precise localization of each object using a bounding box. We again evaluate mean average precision (Everingham et al., 2014). We fine-tune all our models using Fast R-CNN (Girshick, 2015). For a fair comparison we varied the parameters of the fine-tuning for each of the different initializations. We tried three different learning rates (0.01, 0.002 and 0.001) with a step size of 50,000 with a 0.1 step, and 150,000 training iterations. We used multi-scale training and fine-tuned all layers. All other settings were kept at their default values. Training and evaluation took roughly 8 hours in a Titan X GPU for CaffeNet."}, {"heading": "4.1 SCALING AND LEARNING ALGORITHMS", "text": "We begin our evaluation by measuring and comparing the relative change rate C\u0303k,i,j of all weights in the network (see Equation (2)) for different initializations. We estimate C\u0303k,i,j using 100 images of the VOC 2007 dataset. We compare our models to an ImageNet pretrained model of Krizhevsky et al. (2012), initialized with random Gaussian weights (with standard deviation \u03c3 = 0.01) for all layers, an unscaled k-means initialization, as well as the Gaussian initialization in Caffe (Jia et al., 2014), for which biases and standard deviations were picked per layer by hand. Figure 1a visualizes the average change rate per layer. Our initialization, as well as the ImageNet pretrained model, have similar change rates for all layers (i.e., all layers learn at the same rate), while random initializations and k-means have a drastically different change rates. Figure 1b measures the coefficient of variation of the change rate for each layer, defined as the standard deviation of the change rate, divided by their mean value. Our coefficient of variation is low throughout all layers, despite scaling the rate of change of columns of the weight matrix, instead of individual elements. Note that the low values are mirrored in the hand-tuned Caffe initialization.\nNext we show how those different initializations perform on the VOC 2007 classification task, as shown in Table 1. We train both a random Gaussian and k-means initialization using different initial scalings. Without scaling the random Gaussian initialization fares quite well, however the k-means initialization does poorly, due to the worse initial change rate as shown in Figure 1. Correcting for the within-layer scaling alone does not improve the performance much, as it worsens the betweenlayer scaling for both initializations. However in combination with the between-layer adjustment both initializations perform very well.\nBoth the between-layer and within-layer scaling could potentially be addressed by a stronger second order optimization method, such as ADAM (Kingma & Ba, 2015). ADAM is able to slightly improve on SGD for an unscaled initialization, however it does not perform as well as our scaled initialization in combination with SGD. One possible explanation for this phenomena is that ADAM does not consider the actual scale of the weights, such that different layers can still train at different relative rates.\nWe further compare with batch-normalization. While batch-norm does significantly better than standard SGD, it does not quite reach the performance of our fully scaled initialization."}, {"heading": "4.2 WEIGHT INITIALIZATION", "text": "Next we compare various initializations of the actual weight matrix. We compare our Gaussian, PCA and k-means based weights, with initializations proposed by Glorot & Bengio (2010) (commonly known as \u201cxavier\u201d), He et al. (2015), and a carefully chosen Gaussian initialization of Jia et al. (2014). We followed the suggestions of He et al. and used their initialization only for the convolutional layers, while choosing a random Gaussian initialization for the fully connected layers. We compare all methods on both classification and detection performance in Table 2.\nThe first thing to notice is that both Glorot & Bengio and He et al. perform worse than a carefully chosen random Gaussian initialization. One possibility for the drop in performance comes from the additional layers, such as Pooling or LRN used in CaffeNet. Neither Glorot & Bengio nor He et al. consider those layers but rather focus on linear layers followed by tanh or ReLU nonlinearities.\nOur initialization on the other hand has no trouble with those additional layers and substantially improves on the random Gaussian initialization."}, {"heading": "4.3 COMPARISON TO UNSUPERVISED PRE-TRAINING", "text": "We now compare our simple, properly scaled initializations to the state-of-the-art unsupervised pretraining methods on VOC 2007 classification and detection. Table 3 shows a summary of the results, including the amount of pre-training time, as well as the type of supervision used. Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model. While this information is not always readily available, it can be read from sensors and is thus \u201cfree.\u201d We believe egomotion information does not often correlate with the kind of semantic information that is required for classification or detection, and hence the egomotion pretrained model performs worse than our random baseline. Wang & Gupta (2015) supervise their pre-training using relative motion of objects in pre-selected youtube videos, as obtained by a tracker. Their model is generally quite well scaled and trains well for both classification and detection. Doersch et al. (2015) predict the relative arrangement of image patches to pre-train a model. Their model is trained the longest with\n4 weeks of training. It does well on detection, but lags behind other methods in classification. This is in part due to a poor scaling of the model, which severely limits its fine-tunability.\nInterestingly our k-means initialization is able to keep up with most unsupervised pre-training methods, despite containing very little semantic information. To analyze what information is actually captured, we sampled 100 random ImageNet images and found nearest neighbors for them from a pool of 50,000 other random ImageNet images, using the high-level feature spaces from different methods. Figure 2 shows the results. Overall, different unsupervised methods seem to focus on different attributes for matching. For example, ours appears to have some texture and material information, whereas the method of Doersch et al. (2015) seems to preserve more specific shape information.\nAs a final experiment we reinitialize all unsupervised pre-training methods to be properly scaled and compare with our initializations which use no auxiliary training beyond the proposed initializations. In particular, we take their pretrained network weights and apply the between-layer adjustment described in Section 3.2. (We do not perform local scaling as we find that the activations in these models are already scaled reasonably well locally.) The bottom three rows of Table 3 give our results for our rescaled versions of these models on the VOC classification and detection tasks. We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with. The best-performing method with auxiliary self-supervision using our rescaled features is that of Doersch et al. (2015) \u2013 in this case our rescaling improves its results on the classification task by a relative margin of 18%. This suggests that our method nicely complements existing unsupervised and self-supervised methods and could facilitate easier future exploration of this rich space of methods."}, {"heading": "4.4 DIFFERENT ARCHITECTURES", "text": "Finally we compare our initialization across different architectures, again using PASCAL 2007 classification and detection. We train both the deep architecture of Szegedy et al. (2015) and Simonyan & Zisserman (2015) using our k-means and Gaussian initializations. Unlike prior work we are able to train those models without any intermediate losses or stage-wise supervised pre-training. We simply add a sigmoid cross-entropy loss to the top of both networks. Unfortunately neither network outperformed CaffeNet in the classification tasks. GoogLeNet achieves a 50.0% and 55.0% mAP for the two initializations respectively, while 16-layer VGG performs as 53.8% and 56.5%. This might have to do with the limited amount of supervised training data available to the model at during training. The training time was 4 and 12 times slower than CaffeNet, which made them prohibitively slow for detection."}, {"heading": "4.5 IMAGENET TRAINING", "text": "We also test our data-dependent initializations on two well-known CNN architectures which have been successfully applied to the ImageNet LSVRC 1000-way classification task: CaffeNet (Jia et al., 2014) and GoogLeNet (Szegedy et al., 2015). Throughout the ImageNet training experiments we use only the local weight scaling described in Section 3.1; experiments with the between-layer adjustment described in Section 3.2 are currently in progress. We also initialize the 1000-way clas-\nsification layers to 0 in these experiments (except in our reproductions of the reference models), as we find this improves the initial learning velocity.\nCaffeNet We train instances of CaffeNet using our initializations, with the architecture and all other hyperparameters set to those used to train the reference model: learning rate 0.01 (dropped by a factor of 0.1 every 105 iterations), momentum 0.9, and batch size 256. We also train a variant of the architecture with no local response normalization (LRN) layers.\nOur CaffeNet training results are presented in Figure 3. Over the first 100,000 iterations (Figure 3, middle row), and particularly over the first 10,000 (Figure 3, top row), our initializations reduce the network\u2019s classification error on both the training and validation sets at a much faster rate than the reference initialization.\nWith the full 320,000 training iterations, all initializations achieve similar accuracy on the training and validation sets; however, in these experiments the carefully chosen reference initialization pulled non-trivially ahead of our initializations\u2019 error after the second learning rate drop to a rate of 10\u22124. We do not yet know why this occurs, or whether the difference is significant.\nOver the first 100,000 iterations, among models initialized using our method, the k-means initialization reduces the loss slightly faster than the random initialization. Interestingly, the model variant without LRN layers seems to learn just as quickly as the directly comparable network with LRNs, suggesting such normalizations may not be necessary given a well-chosen initialization.\nGoogLeNet We apply our best-performing initialization from the CaffeNet experiments\u2014kmeans\u2014to a deeper network, GoogLeNet (Szegedy et al., 2015). We use the SGD hyperparameters from the Caffe (Jia et al., 2014) GoogleNet implementation (specifically, the \u201cquick\u201d version which is trained for 2.4 million iterations), and also retrain our own instance of the model with the initialization used in the reference model (based on Glorot & Bengio (2010)).\nDue to the depth of the architecture (22 layers, compared to CaffeNet\u2019s 8) and the difficulty of propagating gradient signal to the early layers of the network, GoogLeNet includes additional \u201cauxiliary classifiers\u201d branching off from intermediate layers of the network to amplify the gradient signal to learn these early layers. To verify that networks initialized using our proposed method should have no problem backpropagating appropriately scaled gradients through all layers of arbitrarily deep networks, we also train a variant of GoogLeNet which omits the two intermediate loss towers, otherwise keeping the rest of the architecture fixed.\nOur GoogLeNet training results are presented in Figure 4. We plot only the loss of the final classifier for comparability with the single-classifier model. (Note that the models have not finished training; we will update this document when complete results are available.) The models initialized with our method learn much faster than the model using the reference initialization stategy. Furthermore, the model trained using only a single classifier learns at roughly the same rate as the original three loss tower architecture, and each iteration of training in the single classifier model is slightly faster due to the removal of layers to compute the additional losses. This result suggests that our initialization could significantly ease exploration of new, deeper CNN architectures, bypassing the need for architectural tweaks like the intermediate losses used to train GoogLeNet."}, {"heading": "5 DISCUSSION", "text": "Our method is a conceptually simple data-dependent initialization strategy for CNNs which enforces empirically identically distributed activations locally (within a layer), and roughly uniform global scaling of weight gradients across all layers of arbitrarily deep networks. Our experiments (Section 4) demonstrate that this rescaling of weights results in substantially improved CNN representations for tasks with limited labeled data (as in the PASCAL VOC classification and detection training sets), improves representations learned by existing self-supervised and unsupervised methods, and substantially accelerates the early stages of CNN training on large-scale datasets (e.g., ImageNet). We hope that our initializations will facilitate further advancement in unsupervised and self-supervised learning as well as more efficient exploration of deeper and larger CNN architectures."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The thank Alyosha Efros for his input and encouragement, without his \u201cGelato bet\u201d most of this work would not have been explored. We thank NVIDIA for their generous GPU donations."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Joao", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Learning in modular systems", "author": ["Bradley", "David M"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Bradley and M.,? \\Q2010\\E", "shortCiteRegEx": "Bradley and M.", "year": 2010}, {"title": "Learning feature representations with k-means", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Coates et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2012}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["Doersch", "Carl", "Gupta", "Abhinav", "Efros", "Alexei A"], "venue": null, "citeRegEx": "Doersch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS, pp", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross B", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In ACM Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "In Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Random walk initialization for training very deep feedforward networks", "author": ["Sussillo", "David", "Abbot", "Larry"], "venue": "ICLR,", "citeRegEx": "Sussillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sussillo et al\\.", "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Wang", "Xiaolong", "Gupta", "Abhinav"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "In fact, good performance seems to require large datasets: the best-performing methods usually begin by \u201cpre-training\u201d CNNs to solve the million-image ImageNet classification challenge (Russakovsky et al., 2015).", "startOffset": 185, "endOffset": 211}, {"referenceID": 16, "context": "Empirical evaluations have found that when transferring deep features across tasks, freezing weights of some layers during fine-tuning generally harms performance (Yosinski et al., 2014).", "startOffset": 163, "endOffset": 186}, {"referenceID": 9, "context": "However, none of the above papers consider more general network including pooling, dropout, LRN layers (Krizhevsky et al., 2012), or DAG-structured networks (Szegedy et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 10, "context": "Early approaches to data-driven initializations showed that whitening the activations at all layers can mitigate the vanishing gradient problem (LeCun et al., 1998), but it does not ensure all layers train at an equal rate.", "startOffset": 144, "endOffset": 164}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al.", "startOffset": 0, "endOffset": 273}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al. (2013); Sussillo & Abbot (2015); He et al.", "startOffset": 0, "endOffset": 293}, {"referenceID": 8, "context": "Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al. (2013); Sussillo & Abbot (2015); He et al.", "startOffset": 0, "endOffset": 318}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities.", "startOffset": 33, "endOffset": 50}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities.", "startOffset": 33, "endOffset": 66}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initialization for networks with tanh non-linearities, while He et al.", "startOffset": 33, "endOffset": 284}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initialization for networks with tanh non-linearities, while He et al.", "startOffset": 33, "endOffset": 333}, {"referenceID": 5, "context": "(2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradient problem, if the architectures are limited to linear transformations, followed by a very specific non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initialization for networks with tanh non-linearities, while He et al. (2015) focus on the more commonly used ReLUs.", "startOffset": 33, "endOffset": 421}, {"referenceID": 9, "context": "Other common non-linearities include local response normalization or pooling (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan & Zisserman, 2015).", "startOffset": 77, "endOffset": 152}, {"referenceID": 10, "context": "Despite the non-convexity of this learning problem, backpropagation and Stochastic Gradient Descent often finds good local minima if initialized properly (LeCun et al., 1998).", "startOffset": 154, "endOffset": 174}, {"referenceID": 7, "context": "We implement our initialization and all experiments in the open-source deep learning framework Caffe (Jia et al., 2014).", "startOffset": 101, "endOffset": 119}, {"referenceID": 9, "context": "Architectures Most of our experiments are performed on the 8 layer CaffeNet architecture a small modification of AlexNet (Krizhevsky et al., 2012).", "startOffset": 121, "endOffset": 146}, {"referenceID": 3, "context": "We use the default architecture for all comparisons, except for Doersch et al. (2015) which removed groups in the convolutional layers.", "startOffset": 64, "endOffset": 86}, {"referenceID": 7, "context": "01) for all layers, an unscaled k-means initialization, as well as the Gaussian initialization in Caffe (Jia et al., 2014), for which biases and standard deviations were picked per layer by hand.", "startOffset": 104, "endOffset": 122}, {"referenceID": 8, "context": "We compare our models to an ImageNet pretrained model of Krizhevsky et al. (2012), initialized with random Gaussian weights (with standard deviation \u03c3 = 0.", "startOffset": 57, "endOffset": 82}, {"referenceID": 5, "context": "4% He et al. (2015) 43.", "startOffset": 3, "endOffset": 20}, {"referenceID": 5, "context": "We compare our Gaussian, PCA and k-means based weights, with initializations proposed by Glorot & Bengio (2010) (commonly known as \u201cxavier\u201d), He et al. (2015), and a carefully chosen Gaussian initialization of Jia et al.", "startOffset": 142, "endOffset": 159}, {"referenceID": 5, "context": "We compare our Gaussian, PCA and k-means based weights, with initializations proposed by Glorot & Bengio (2010) (commonly known as \u201cxavier\u201d), He et al. (2015), and a carefully chosen Gaussian initialization of Jia et al. (2014). We followed the suggestions of He et al.", "startOffset": 142, "endOffset": 228}, {"referenceID": 0, "context": "Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model. While this information is not always readily available, it can be read from sensors and is thus \u201cfree.\u201d We believe egomotion information does not often correlate with the kind of semantic information that is required for classification or detection, and hence the egomotion pretrained model performs worse than our random baseline. Wang & Gupta (2015) supervise their pre-training using relative motion of objects in pre-selected youtube videos, as obtained by a tracker.", "startOffset": 0, "endOffset": 450}, {"referenceID": 0, "context": "Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model. While this information is not always readily available, it can be read from sensors and is thus \u201cfree.\u201d We believe egomotion information does not often correlate with the kind of semantic information that is required for classification or detection, and hence the egomotion pretrained model performs worse than our random baseline. Wang & Gupta (2015) supervise their pre-training using relative motion of objects in pre-selected youtube videos, as obtained by a tracker. Their model is generally quite well scaled and trains well for both classification and detection. Doersch et al. (2015) predict the relative arrangement of image patches to pre-train a model.", "startOffset": 0, "endOffset": 690}, {"referenceID": 3, "context": "For example, ours appears to have some texture and material information, whereas the method of Doersch et al. (2015) seems to preserve more specific shape information.", "startOffset": 95, "endOffset": 117}, {"referenceID": 0, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with.", "startOffset": 41, "endOffset": 85}, {"referenceID": 3, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with.", "startOffset": 41, "endOffset": 85}, {"referenceID": 0, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with.", "startOffset": 42, "endOffset": 170}, {"referenceID": 0, "context": "We find that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results significantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its performance, indicating it was likely relatively well-scaled globally to begin with. The best-performing method with auxiliary self-supervision using our rescaled features is that of Doersch et al. (2015) \u2013 in this case our rescaling improves its results on the classification task by a relative margin of 18%.", "startOffset": 42, "endOffset": 414}, {"referenceID": 7, "context": "We also test our data-dependent initializations on two well-known CNN architectures which have been successfully applied to the ImageNet LSVRC 1000-way classification task: CaffeNet (Jia et al., 2014) and GoogLeNet (Szegedy et al.", "startOffset": 182, "endOffset": 200}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.", "startOffset": 61, "endOffset": 83}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.", "startOffset": 61, "endOffset": 134}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.", "startOffset": 61, "endOffset": 182}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.", "startOffset": 61, "endOffset": 240}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.2% 56.8% Ours (k-means) initialization 54 seconds 56.6% 45.6% Ours + Agrawal et al. (2015) egomotion 10 hours 54.", "startOffset": 61, "endOffset": 359}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.2% 56.8% Ours (k-means) initialization 54 seconds 56.6% 45.6% Ours + Agrawal et al. (2015) egomotion 10 hours 54.2% 43.9% Ours + Wang & Gupta (2015) motion 1 week 58.", "startOffset": 61, "endOffset": 417}, {"referenceID": 0, "context": "Method Supervision Pretraining time Classification Detection Agrawal et al. (2015) egomotion 10 hours 52.9% 41.8% Wang & Gupta (2015) motion 1 week 58.4% 44.0% Doersch et al. (2015) unsupervised 4 weeks 55.3% 46.6% Krizhevsky et al. (2012) 1000 class labels 3 days 78.2% 56.8% Ours (k-means) initialization 54 seconds 56.6% 45.6% Ours + Agrawal et al. (2015) egomotion 10 hours 54.2% 43.9% Ours + Wang & Gupta (2015) motion 1 week 58.1% 44.0% Ours + Doersch et al. (2015) unsupervised 4 weeks 65.", "startOffset": 61, "endOffset": 472}, {"referenceID": 7, "context": "We use the SGD hyperparameters from the Caffe (Jia et al., 2014) GoogleNet implementation (specifically, the \u201cquick\u201d version which is trained for 2.", "startOffset": 46, "endOffset": 64}, {"referenceID": 7, "context": "We use the SGD hyperparameters from the Caffe (Jia et al., 2014) GoogleNet implementation (specifically, the \u201cquick\u201d version which is trained for 2.4 million iterations), and also retrain our own instance of the model with the initialization used in the reference model (based on Glorot & Bengio (2010)).", "startOffset": 47, "endOffset": 303}, {"referenceID": 3, "context": "(For Doersch et al. (2015) we display neighbors in fc6 feature space; the rest use the fc7 features.", "startOffset": 5, "endOffset": 27}], "year": 2015, "abstractText": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.", "creator": "LaTeX with hyperref package"}}}