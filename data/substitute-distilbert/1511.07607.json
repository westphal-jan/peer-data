{"id": "1511.07607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2015", "title": "Fine-Grain Annotation of Cricket Videos", "abstract": "the recognition through human activities addresses one of the key problems in video understanding. action recognition - challenging even for specific categories of videos, such as sports, that contain only a small set meaning actions. interestingly, sports videos are accompanied. detailed descriptions available online, which could be used to compare action annotation in a weakly - supervised animation. for the specific case than cricket videos, we address historical challenge of temporal segmentation and annotation of ctions with semantic descriptions. our solution operates of two stages. in the functional stage, the video is segmented into \" scenes \", by utilizing the scene describing information extracted through text - commentary. the second stage consists of classifying video - forms as well as the phrases using the textual description into various categories. the relevant phrases are then suitably mapped out the video - shots. the novel aspect of this work is the fine temporal scale matching which semantic information is assigned to the verb. as more result of our approach, we enable retrieval of specific actions that last only a 90 seconds, from several hours of video. this solution explores a large number of labeled moments, with no manual effort, that could be used by machine learning algorithms to learn grammatical actions.", "histories": [["v1", "Tue, 24 Nov 2015 08:34:20 GMT  (2853kb,D)", "http://arxiv.org/abs/1511.07607v1", null], ["v2", "Wed, 27 Sep 2017 10:48:11 GMT  (2935kb,D)", "http://arxiv.org/abs/1511.07607v2", "ACPR 2015"]], "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.CV", "authors": ["rahul anand sharma", "pramod sankar k", "cv jawahar"], "accepted": false, "id": "1511.07607"}, "pdf": {"name": "1511.07607.pdf", "metadata": {"source": "CRF", "title": "Fine-Grain Annotation of Cricket Videos", "authors": ["Rahul Anand Sharma", "Pramod Sankar", "C. V. Jawahar"], "emails": ["rahul.anand@research.iiit.ac.in", "pramod.kompalli@xerox.com", "jawahar@iiit.ac.in"], "sections": [{"heading": "1. Introduction", "text": "The labeling of human actions in videos is a challenging problem for computer vision systems. There are three difficult tasks that need to be solved to perform action recognition: 1) identification of the sequence of frames that involve an action performed, 2) localisation of the person performing the action and 3) recognition of the pixel information to assign a semantic label. While each of these tasks could be solved independently, there are few robust solutions for their joint inference in generic videos.\nCertain categories of videos, such as Movies, News feeds, Sports videos, etc. contain domain specific cues that could be exploited towards better understanding of the vi-\nsual content. For example, the appearance of a Basketball court [?] could help in locating and tracking players and their movements. However, current visual recognition solutions have only seen limited success towards fine-grain action classification. For example, it is difficult to automatically distinguish a \u201cforehand\u201d from a \u201chalf-volley\u201d in Tennis. Further, automatic generation of semantic descriptions is a much harder task, with only limited success in the image domain [?].\nInstead of addressing the problem using visual analysis alone, several researchers proposed to utilize relevant parallel information to build better solutions [5]. For example, the scripts available for movies provide a weak supervision to perform person [5] and action recognition [8]. Similar parallel text in sports was previously used to detect events in soccer videos, and index them for efficient retrieval [13, 14]. Gupta et al. [6] learn a graphical model from annotated baseball videos that could then be used to generate captions automatically. Their generated captions, however, are not semantically rich. Lu et al. [9] show that the weak supervision from parallel text results in superior player identification and tracking in Basketball videos.\nIn this work, we aim to label the actions of players in Cricket videos using parallel information in the form of online text-commentaries [1]. The goal is to label the video at the shot-level with the semantic descriptions of actions\n1\nar X\niv :1\n51 1.\n07 60\n7v 1\n[ cs\n.M M\n] 2\n4 N\nov 2\nand activities. Two challenges need to be addressed towards this goal. Firstly, the visual and textual modalities are not aligned, i.e. we are given a few pages of text for a four hour video with no other synchronisation information. Secondly, the text-commentaries are very descriptive, where they assume that the person reading the commentary understands the keywords being used. Bridging this semantic gap over video data is much tougher than, for example, images and object categories."}, {"heading": "1.1. Our Solution", "text": "We present a two-stage solution for the problem of finegrained Cricket video segmentation and annotation. In the first stage, the goal is to align the two modalities at a \u201cscene\u201d level. This stage consists of a joint synchronisation and segmentation of the video with the text commentary. Each scene is a meaningful event that is a few minutes long (Figure 2), and described by a small set of sentences in the commentary (Figure 3). The solution for this stage is inspired from the approach proposed in [10], and presented in Section 2.\nGiven the scene segmentation and the description for each scene, the next step is to align the individual descriptions with their corresponding visuals. At this stage, the alignment is performed between the video-shots and phrases of the text commentary. This is achieved by classifying video-shots and phrases into a known set of categories, which allows them to be mapped easily across the modalities, as described in Section 3. As an outcome of this step, we could obtain fine-grain annotation of player actions, such as those presented in Figure 1.\nOur experiments, detailed in Section 4 demonstrate that the proposed solution is sufficiently reliable to address this seemingly challenging task. The annotation of the videos allow us to build a retrieval system that can search\nacross hundreds of hours of content for specific actions that last only a few seconds. Moreover, as a consquence of this work, we generate a large set of fine-grained labelled videos, that could be used to train action recognition solutions."}, {"heading": "2. Scene Segmentation", "text": "A typical scene in a Cricket match follows the sequence of events depicted in Figure 2. A scene always begins with the bowler (similar to a pitcher in Baseball) running towards and throwing the ball at the batsman, who then plays his stroke. The events that follow vary depending on this action. Each such scene is described in the text-commentary as shown in Figure 3. The commentary consists of the event number, which is not a time-stamp; the player names, which are hard to recognise; and detailed descriptions that are hard to automatically interpret.\nIt was observed in [10] that the visual-temporal patterns of the scenes are conditioned on the outcome of the event. In other words, a 1-Run outcome is visually different from a 4-Run outcome. This can be observed from the statetransition diagrams in Figure 4. In these diagrams, the shots of the video are represented by visual categories such as ground, sky, play-area, players, etc. The sequence of the shot-categories is represented by the arrows across these states. One can observe that for a typical Four-Runs video, the number of shots and their transitions are lot more complex than that of a 1-Run video. Several shot classes such as replay are typically absent for a 1-Run scene, while a replay is expected as the third or fourth shot in a Four-Runs scene.\nWhile the visual model described above could be useful in recognizing the scene category for a given video segment, it cannot be immediately used to spot the scene in a full-length video. Conversely, the temporal segmentation of the video is not possible without using an appropriate model for the individual scenes themselves. This chicken-and-egg problem can be solved by utilizing the scene-category information from the parallel text-commentary.\nLet us say Fi represents one of theN frames and Sk represents the category of the k-th scene. The goal of the scene segmentation is to identify anchor frames Fi, Fj , which are most likely to contain the scene Sk. The optimal segmenta-\ntion of the video can be defined by the recursive function\nC(Fi, Sk) = max j\u2208[i+1,N ]\n{p(Sk | [Fi, Fj ])+C(Fj+1, Sk+1)},\nwhere p(Sk | [Fi, Fj ]) is the probability that the sequence of frames [Fi, Fj ] belong to the scene category Sk. This probability is computed by matching the learnt scene models with the sequence of features for the given frame set. The optimisation function could be solved using Dynamic Programming (DP). The optimal solution is found by backtracking the DP matrix, which provides the scene anchor points FS1 , FS2 , ..., FSK .\nWe thus obtain a temporal segmentation of the given video into its individual scenes. A typical segmentation covering five overs, is shown in Figure 5. The descriptive commentary from the parallel-text could be used to annotate the scenes, for text-based search and retrieval. In this work, we would like to further annotate the videos at a much finer temporal scale than the scenes, i.e., we would like to annotate at the shot-level. The solution towards shot-level annotation is presented in the following Section."}, {"heading": "3. Shot/Phrase Alignment", "text": "Following the scene segmentation, we obtain an alignment between minute-long clips with a paragraph of text. To perform fine-grained annotation of the video, we must segment both the video clip and the descriptive text. Firstly, the video segments at scene level are over-segmented into shots, to ensure that it is unlikely to map multiple actions into the same shot. In the case of the text, given that the commentary is free-flowing, the action descriptions need to be identified at a finer-grain than the sentence level. Hence we choose to operate at the phrase-level, by segmenting sentences at all punctuation marks. Both the video-shots and the phrases are classified into one of these categories: {Bowler Action, Batsman Action, Others}, by learning suitable classifiers for each modality. Following this, the individual phrases could be mapped to the video-shots that belong to the same category."}, {"heading": "3.1. Video-Shot Recognition", "text": "In order to ensure that the video-shots are atomic to an action/activity, we perform an over-segmentation of the video. We use a window-based shot detection scheme that works as follows. For each frame Fi, we compute its difference with every other frame Fj , where j \u2208 [i\u2212w/2, i+ w/2], for a chosen window size w, centered on Fi. If the maximum frame difference within this window is greater than a particular threshold \u03c4 , we declare Fi as a shotboundary. We choose a small value for \u03c4 to ensure oversegmentation of scenes.\nEach shot is now represented with the classical Bag of Visual Words (BoW) approach [12]. SIFT features are first computed for each frame independently, which are then clustered using the K-means clustering algorithm to build a visual vocabulary (where each cluster center corresponds to a visual word). Each frame is then represented by the normalized count of number of SIFT features assigned to each cluster (BoW histograms). The shot is represented by the average BoW histogram over all frames present in the shot. The shots are then classified into one of these classes: {Bowler Runup, Batsman Stroke, Player Close-Up, Umpire, Ground, Crowd, Animations, Miscellaneous}. The classification is performed using a multiclass Kernel-SVM.\nThe individual shot-classification results could be further refined by taking into account the temporal neighbourhood information. Given the strong structure of a Cricket match, the visuals do not change arbitrarily, but are predictable according to the sequence of events in the scenes. Such a sequence could be modelled as a Linear Chain Conditional Random Field (LC-CRF) [?]. The LC-CRF, consists of nodes corresponding to each shot, with edges connecting each node with its previous and next node, resulting in a linear chain. The goal of the CRF is to model P (y1, ..., yn|x1, ..., xn), where xi and yi are the input and output sequences respectively. The LC-CRF is posed as the\nobjective function\np(Y \u2016X) = exp( K\u2211\nk=1\nau(yk) + K\u22121\u2211 k=1 ap(yk, yk+1))/Z(X)\nHere, the unary term is given by the class-probabilities produced by the shot classifier, defined as\nau(yk) = 1\u2212 P (yk|xi).\nThe pair-wise term encodes the probability of transitioning from a class yk to yk+1 as\nap(yk, yk+1) = 1\u2212 P (yk+1|yk).\nThe function Z(X) is a normalisation factor. The transition probabilities between all pairs of classes are learnt from a training set of labelled videos. The inference of the CRF is performed using the forward-backward algorithm."}, {"heading": "3.2. Text Classification", "text": "The phrase classifier is learnt entirely automatically. We begin by crawling the web for commentaries of about 300 matches and segmenting the text into phrases. It was observed that the name of the bowler or the batsman is sometimes included in the description, for example, \u201cSachin hooks the ball to square-leg\u201d. These phrases can accordingly be labelled as belonging to the actions of the Bowler or the Batsman. From the 300 match commentaries, we obtain about 1500 phrases for bowler actions and about 6000 phrases for the batsman shot. We remove the names of the respective players and represent each phrase as a histogram of its constituent word occurrences. A Linear-SVM\nis now learnt for the bowler and batsman categories over this bag-of-words representation. Given a test phrase, the SVM provides a confidence for it to belong to either of the two classes.\nThe text classification module is evaluated using 2-fold cross validation over the 7500 phrase dataset. We obtain a recognition accuracy of 89.09% for phrases assigned to the right class of bowler or batsman action."}, {"heading": "4. Experiments", "text": "Dataset: Our dataset is collected from the YouTube channel for the Indian Premier League(IPL) tournament. The format for this tournament is 20-overs per side, which results in about 120 scenes or events for each team. The dataset consists of 16 matches, amounting to about 64 hours of footage. Four matches were groundtruthed manually at the scene and shot level, two of which are used for training and the other two for testing."}, {"heading": "4.1. Scene Segmentation", "text": "For the text-driven scene segmentation module, we use these mid-level features: {Pitch, Ground, Sky, PlayerCloseup, Scorecard}. These features are modelled using binned color histograms. Each frame is then represented by the fraction of pixels that contain each of these concepts, the scene is thus a spatio-temporal model that accumulates these scores.\nThe limitation with the DP formulation is the amount of memory available to store the DP score and indicator matrices. With our machines we are limited to performing the DP over 100K frames, which amounts to 60 scenes, or 10-overs of the match.\nThe accuracy of the scene segmentation is measured as the number of video-shots that are present in the correct scene segment. We obtain a segmentation accuracy of\n83.45%. Example segmentation results for two scenes are presented in Figure 5, one can notice that the inferred scene boundaries are very close to the groundtruth. We observe that the errors in segmentation typically occur due to events that are not modelled, such as a player injury or an extended team huddle."}, {"heading": "4.2. Shot Recognition", "text": "The accuracy of the shot recognition using various feature representations and SVM Kernels is given in Table 1. We observe that the 1000 size vocabulary works better than 300. The Linear Kernel seems to suffice to learn the decision boundary, with a best-case accuracy of 82.25%. Refining the SVM predictions with the CRF based method yields an improved accuracy of 86.54. Specifically, the accuracy of the batsman/bowler shot categories is 89.68%."}, {"heading": "4.3. Shot Annotation Results", "text": "The goal of the shot annotation module is to identify the right shot within a scene that contains the bowler and batsman actions. As the scene segmentation might contain errors, we perform a search in a shot-neighbourhood centered on the inferred scene boundary. We evaluate the accuracy of finding the right bowler and batsman shots within a neighbourhood region R of the scene boundary, which is given in Table 2. It was observed that 90% of the bowler and batsman shots were correctly identified by searching within a window of 10 shots on either side of the inferred boundary.\nOnce the shots are identified, the corresponding textual comments for bowler and batsman actions, are mapped to these video segments. A few shots that were correctly annotated are shown in Figure 6."}, {"heading": "5. Conclusions", "text": "In this paper, we present a solution that enables rich semantic annotation of Cricket videos at a fine temporal scale. Our approach circumvents technical challenges in visual recognition by utilizing information from online text-commentaries. We obtain a high annotation accuracy, as evaluated over a large video collection. The annotated videos shall be made available for the community for benchmarking, such a rich dataset is not yet available publicly. In future work, the obtained labelled datasets could be used to learn classifiers for fine-grain activity recognition and understanding."}], "references": [{"title": "Event based indexing of broadcast sports video by intermodal collaboration", "author": ["N. Babaguchi", "Y. Kawai", "T. Kitahashi"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Segmentation of intentional human gestures for sports video annotation", "author": ["G.S. Chambers", "S. Venkatesh", "G.A.W. West", "H.H. Bui"], "venue": "In Proc. MMM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Hello! My name is.", "author": ["M. Everingham", "J. Sivic", "A. Zisserman"], "venue": "Buffy\u201d \u2013 automatic naming of characters in TV video. In Proc. BMVC,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Understanding videos, constructing plots: Learning a visually grounded storyline model from annotated videos", "author": ["A. Gupta", "P. Srinivasan", "J. Shi", "L. Davis"], "venue": "In Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A hierarchical framework for generic sports video classification", "author": ["M.H. Kolekar", "S. Sengupta"], "venue": "In Proc. ACCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "In Proc. CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Learning to track and identify players from broadcast sports videos", "author": ["W.-L. Lu", "J.-A. Ting", "J. Little", "K. Murphy"], "venue": "IEEE PAMI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Text driven temporal segmentation of cricket videos", "author": ["Pramod Sankar K", "S. Pandey", "C.V. Jawahar"], "venue": "In Proc. ICVGIP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Automatically extracting highlights for tv baseball programs", "author": ["Y. Rui", "A. Gupta", "A. Acero"], "venue": "In ACM Multimedia,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In Proc. ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Live sports event detection based on broadcast video and web-casting text", "author": ["C. Xu", "J. Wang", "K. Wan", "Y. Li", "L. Duan"], "venue": "In Proc. ACM Multimedia,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Personalized retrieval of sports video", "author": ["Y. Zhang", "X. Zhang", "C. Xu", "H. Lu"], "venue": "In Proc. Intl. Workshop on Multimedia Information Retrieval,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}], "referenceMentions": [{"referenceID": 2, "context": "Instead of addressing the problem using visual analysis alone, several researchers proposed to utilize relevant parallel information to build better solutions [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "For example, the scripts available for movies provide a weak supervision to perform person [5] and action recognition [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "For example, the scripts available for movies provide a weak supervision to perform person [5] and action recognition [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "Similar parallel text in sports was previously used to detect events in soccer videos, and index them for efficient retrieval [13, 14].", "startOffset": 126, "endOffset": 134}, {"referenceID": 11, "context": "Similar parallel text in sports was previously used to detect events in soccer videos, and index them for efficient retrieval [13, 14].", "startOffset": 126, "endOffset": 134}, {"referenceID": 3, "context": "[6] learn a graphical model from annotated baseball videos that could then be used to generate captions automatically.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] show that the weak supervision from parallel text results in superior player identification and tracking in Basketball videos.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The solution for this stage is inspired from the approach proposed in [10], and presented in Section 2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "It was observed in [10] that the visual-temporal patterns of the scenes are conditioned on the outcome of the event.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "Each shot is now represented with the classical Bag of Visual Words (BoW) approach [12].", "startOffset": 83, "endOffset": 87}], "year": 2015, "abstractText": "The recognition of human activities is one of the key problems in video understanding. Action recognition is challenging even for specific categories of videos, such as sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting. For the specific case of Cricket videos, we address the challenge of temporal segmentation and annotation of actions with semantic descriptions. Our solution consists of two stages. In the first stage, the video is segmented into \u201cscenes\u201d, by utilizing the scene category information extracted from textcommentary. The second stage consists of classifying videoshots as well as the phrases in the textual description into various categories. The relevant phrases are then suitably mapped to the video-shots. The novel aspect of this work is the fine temporal scale at which semantic information is assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a large number of labelled exemplars, with no manual effort, that could be used by machine learning algorithms to learn complex actions.", "creator": "LaTeX with hyperref package"}}}