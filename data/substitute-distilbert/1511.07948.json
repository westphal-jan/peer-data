{"id": "1511.07948", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Learning Halfspaces and Neural Networks with Random Initialization", "abstract": "we study non - vanishing empirical risk minimization for learning halfspaces and neural disks. for loss functions that are $ l $ - lipschitz continuous, we present algorithms to learn halfspaces surrounding multi - layer neural networks that achieve arbitrarily small excess p $ \\ epsilon & gt ; 0 $. the time complexity is polynomial in the sample dimension $ d $ and the log size $ n $, but exponential in the ratio $ ( l / \\ epsilon ^ 2 ) \\ log ( nh / \\ epsilon ) $. these algorithms run multiple rounds of periodic initialization followed by discrete optimization steps. we further show that that the data is separable by some neural network with constant margin $ \\ gamma & hence ; 0 $, # there is a one - time machine for learning a neural network that separates the training node with margin $ \\ omega ( \\ gamma ) $. as a class, the simulator selects arbitrary generalization value $ \\ epsilon & gt ; 0 $ with $ { \\ rm log } ( d, 1 / \\ epsilon ) $ 1st and decreasing complexity. we establish the same learnability result when the labels are flipped flipped with probability $ \\ eta & lt ; 1 / 2 $.", "histories": [["v1", "Wed, 25 Nov 2015 04:41:20 GMT  (62kb,D)", "http://arxiv.org/abs/1511.07948v1", "31 pages"]], "COMMENTS": "31 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "jason d lee", "martin j wainwright", "michael i jordan"], "accepted": false, "id": "1511.07948"}, "pdf": {"name": "1511.07948.pdf", "metadata": {"source": "CRF", "title": "Learning Halfspaces and Neural Networks with Random Initialization", "authors": ["Yuchen Zhang", "Jason D. Lee", "Martin J. Wainwright", "Michael I. Jordan"], "emails": ["yuczhang@eecs.berkeley.edu", "jasondlee88@eecs.berkeley.edu", "wainwrig@eecs.berkeley.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "The learning of a halfspace is the core problem solved by many machine learning methods, including the Perceptron (Rosenblatt, 1958), the Support Vector Machine (Vapnik, 1998) and AdaBoost (Freund and Schapire, 1997). More formally, for a given input space X \u2282 Rd, a halfspace is defined by a linear mapping f(x) = \u3008w, x\u3009 from X to the real line. The sign of the function value f(x) determines if x is located on the positive side or the negative side of the halfspace. A labeled data point consists of a pair (x, y) \u2208 X \u00d7 {\u22121, 1}, and given n such pairs {(xi, yi)}ni=1, the empirical prediction error is given by\n`(f) := 1\nn n\u2211 i=1 I[\u2212yif(xi) \u2265 0]. (1)\nThe loss function in equation (1) is also called the zero-one loss. In agnostic learning, there is no hyperplane which perfectly separates the data, in which case the goal is to find a mapping f that achieves a small zero-one loss. The method of choosing a function f based on minimizing the criterion (1) is known as empirical risk minimization (ERM).\nIt is known that finding a halfspace that approximately minimizes the zero-one loss is NP-hard. In particular, Guruswami and Raghavendra (2009) show that, for any \u2208 (0, 1/2], given a set of\nar X\niv :1\n51 1.\n07 94\n8v 1\n[ cs\n.L G\n] 2\n5 N\nov 2\npairs such that the optimal zero-one loss is bounded by , it is NP-hard to find a halfspace whose zero-one loss is bounded by 1/2\u2212 . Many practical machine learning algorithms minimize convex surrogates of the zero-one loss, but the halfspaces obtained through the convex surrogate are not necessarily optimal. In fact, the result of Guruswami and Raghavendra (2009) shows that the approximation ratio of such procedures could be arbitrarily large.\nIn this paper, we study optimization problems of the form\n`(f) := 1\nn n\u2211 i=1 h(\u2212yif(xi)), (2)\nwhere the function h : R \u2192 R is L-Lipschitz continuous for some L < +\u221e, but is otherwise arbitrary (and so can be nonconvex). This family does not include the zero-one-loss (since it is not Lipschitz), but does include functions that can be used to approximate it to arbitrary accuracy with growing L. For instance, the piecewise-linear function:\nh(x) :=  0 x \u2264 \u2212 12L , 1 x \u2265 12L , Lx+ 1/2 otherwise,\n(3)\nis L-Lipschitz, and converges to the step function as the parameter L increases to infinity. Shalev-Shwartz et al. (2011) study the problem of minimizing the objective (2) with function h defined by (3), and show that under a certain cryptographic assumption, there is no poly(L)-time algorithm for (approximate) minimization. Thus, it is reasonable to assume that the Lipschitz parameter L is a constant that does not grow with the dimension d or sample size n. Moreover, when f is a linear mapping, scaling the input vector x or scaling the weight vector w is equivalent to scaling the Lipschitz constant. Thus, we assume without loss of generality that the norms of x and w are bounded by one."}, {"heading": "1.1 Our contributions", "text": "The first contribution of this paper is to present two poly(n, d)-time methods\u2014namely, Algorithm 1 and Algorithm 2\u2014for minimizing the cost function (2) for an arbitrary L-Lipschitz function h. We prove that for any given tolerance > 0, these algorithms achieve an -excess risk by running multiple rounds of random initialization followed by a constant number of optimization rounds (e.g., using an algorithm such as stochastic gradient descent). The first algorithm is based on choosing the initial vector uniformly at random from the Euclidean sphere; despite the simplicity of this scheme, it still has non-trivial guarantees. The second algorithm makes use a better initialization obtained by solving a least-squares problem, thereby leading to a stronger theoretical guarantee. Random initialization is a widely used heuristic in non-convex ERM; our analysis supports this usage but suggests that a careful theoretical treatment of the initialization step is necessary.\nOur algorithms for learning halfspaces have running time that grows polynomially in the pair (n, d), as well as a term proportional to exp((L/ 2) log(L/ )). Our next contribution is to show that under a standard complexity-theoretic assumption\u2014namely, that RP 6= NP\u2014this exponential dependence on L/ cannot be avoided. More precisely, letting h denote the piecewise linear function from equation (3) with L = 1, Proposition 1 shows that there is no algorithm achieving arbitrary excess risk > 0 in poly(n, d, 1/ ) time when RP 6= NP. Thus, the random initialization scheme is unlikely to be substantially improved.\nWe then extend our approach to the learning of multi-layer neural networks, with a detailed analysis of the family of m-layer sigmoid-activated neural networks, under the assumption that `1-norm of the incoming weights of any neuron is assumed to be bounded by a constant B. We specify a method (Algorithm 3) for training networks over this family, and in Theorem 3, we prove that its loss is at most an additive term of worse than that of the best neural network. The time complexity of the algorithm scales as poly(n, d, Cm,B,1/ ), where the constant Cm,B,1/ does not depends on the input dimension or the data size, but may depend exponentially on the triplet (m,B, 1/ ).\nDue to the exponential dependence on 1/ , this agnostic learning algorithm is too expensive to achieve a diminishing excess risk for a general data set. However, by analyzing data sets that are separable by some neural network with constant margin \u03b3 > 0, we obtain a stronger achievability result. In particular, we show in Theorem 4 that there is an efficient algorithm that correctly classifies all training points with margin \u2126(\u03b3) in polynomial time. As a consequence, the algorithm learns a neural network with generalization error bounded by using poly(d, 1/ ) training points and in poly(d, 1/ ) time. This so-called BoostNet algorithm uses the AdaBoost approach (Freund and Schapire, 1997) to construct a m-layer neural network by taking an (m \u2212 1)-layer network as a weak classifier. The shallower networks are trained by the agnostic learning algorithms that we develop in this paper. We establish the same learnability result when the labels are randomly flipped with probability \u03b7 < 1/2 (see Corollary 1). Although the time complexity of BoostNet is exponential in 1/\u03b3, we demonstrate that our achievable result is unimprovable\u2014in particular, by showing that a poly(d, 1/ , 1/\u03b3) complexity is impossible under a certain cryptographic assumption (see Proposition 2).\nFinally, we report experiments on learning parity functions with noise, which is a challenging problem in computational learning theory. We train two-layer neural networks using BoostNet, then compare them with the traditional backpropagation approach. The experiment shows that BoostNet learns the degree-5 parity function by constructing 50 hidden neurons, while the backpropagation algorithm fails to outperform random guessing."}, {"heading": "1.2 Related Work", "text": "This section is devoted to discussion of some related work so as to put our contributions into broader context."}, {"heading": "1.2.1 Learning halfspaces", "text": "The problem of learning halfspaces is an important problem in theoretical computer science. It is known that for any constant approximation ratio, the problem of approximately minimizing the zero-one loss is computationally hard (Guruswami and Raghavendra, 2009; Daniely et al., 2014). Halfspaces can be efficiently learnable if the data are drawn from certain special distributions, or if the label is corrupted by particular forms of noise. Indeed, Blum et al. (1998) and Servedio and Valiant (2001) show that if the labels are corrupted by random noise, then the halfspace can be learned in polynomial time. The same conclusion was established by Awasthi et al. (2015) when the labels are corrupted by Massart noise, and the covariates are drawn from the uniform distribution on a unit sphere. When the label noise is adversarial, the halfspace can be learned if the data distribution is isotropic log-concave and the fraction of labels being corrupted is bounded by a small quantity (Kalai et al., 2008; Klivans et al., 2009; Awasthi et al., 2014). When no assumption is made\non the noise, Kalai et al. (2008) show that if the data are drawn from the uniform distribution on a unit sphere, then there is an algorithm whose time complexity is polynomial in the input dimension, but exponential in 1/ (where is the additive error). In this same setting, Klivans and Kothari (2014) prove that the exponential dependence on 1/ is unavoidable.\nAnother line of work modifies the loss function to make it easier to minimize. Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u00b5-margin loss. The \u00b5-margin loss asserts that all points whose classification margins are smaller than \u00b5 should be marked as misclassified. Under this metric, it was shown by Ben-David and Simon (2001); Birnbaum and Shwartz (2012) that the optimal \u00b5-margin loss can be achieved in polynomial time if \u00b5 is a positive constant. Shalev-Shwartz et al. (2011) study the minimization of a continuous approximation to the zero-one loss, which is similar to our setup. They propose a kernel-based algorithm which performs as well as the best linear classifier. However, it is an improper learning method in that the classifier cannot be represented by a halfspace."}, {"heading": "1.2.2 Learning neural networks", "text": "It is known that any smooth function can be approximated by a neural network with just one hidden layer (Barron, 1993), but that training such a network is NP-hard (Blum and Rivest, 1992). In practice, people use optimization algorithms such as stochastic gradient (SG) to train neural networks. Although strong theoretical results are available for SG in the setting of convex objective functions, there are few such results in the nonconvex setting of neural networks.\nSeveral recent papers address the challenge of establishing polynomial-time learnability results for neural networks. Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations.\nSedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014). The assumption in this case is that the network weights satisfy a non-degeneracy condition; however, the algorithm is only capable of learning neural networks with one hidden layer. Our algorithm does not impose any assumption on the data distribution, and is able to learn multilayer neural networks.\nAnother approach to the problem is via the improper learning framework. The goal in this case is to find a predictor that is not a neural network, but performs as well as the best possible neural network in terms of the generalization error. Livni et al. (2014) propose a polynomial-time algorithm to learn networks whose activation function is quadratic. Zhang et al. (2015) propose an algorithm for improper learning of sigmoidal neural networks. The algorithm runs in poly(n, d, 1/ ) time if the depth of the networks is a constant and the `1-norm of the incoming weights of any node is bounded by a constant. It outputs a kernel-based classifier while our algorithm outputs a proper neural network. On the other hand, in the agnostic setting, the time complexity of our\nalgorithm depends exponentially on 1/ ."}, {"heading": "2 Preliminaries", "text": "In this section, we formalize the problem set-up and present several preliminary lemmas that are useful for the theoretical analysis. We first set up some notation so as to define a general empirical risk minimization problem. Let D be a dataset containing n points {(xi, yi)}ni=1 where xi \u2208 X \u2282 Rd and yi \u2208 {\u22121, 1}. To goal is to learn a function f : X \u2192 R so that f(xi) is as close to yi as possible. We may write the loss function as\n`(f) := n\u2211 i=1 \u03b1ih(\u2212yif(xi)). (4)\nwhere hi : R \u2192 R is a L-Lipschitz continuous function that depends on yi, and {\u03b11, . . . , \u03b1n} are non-negative importance weights that sum to 1. As concrete examples, the function h can be the piecewise linear function defined by equation (3) or the sigmoid function h(x) := 1/(1 + e\u22124Lx). Figure 1 compares the step function I[x \u2265 0] with these continuous approximations. In the following sections, we study minimizing the loss function in equation (4) when f is either a linear mapping or a multi-layer neural network.\nLet us introduce some useful shorthand notation. We use [n] to denote the set of indices {1, 2, . . . , n}. For q \u2208 [1,\u221e], let \u2016x\u2016q denote the `q-norm of vector x, given by \u2016x\u2016q := ( \u2211d j=1 x q j)\n1/q, as well as \u2016q\u2016\u221e = max\nj=1,...,d |xj |. If u is a d-dimensional vector and \u03c3 : R \u2192 R is a function, we use\n\u03c3(u) as a convenient shorthand for the vector (\u03c3(u1), . . . , \u03c3(ud)). Given a class F of real-valued functions, we define the new function class \u03c3 \u25e6 F := { \u03c3 \u25e6 f | f \u2208 F } .\nRademacher complexity bounds: Let {(x\u2032j , y\u2032j)}kj=1 be i.i.d. samples drawn from the dataset D such that probability of drawing (xi, yi) is proportional to \u03b1i. We define the sample-based loss function:\nG(f) := 1\nk k\u2211 j=1 h(\u2212y\u2032jf(x\u2032j)). (5)\nIt is straightforward to verify that E[G(f)] = `(f). For a given function class F , the Rademacher complexity of F with respect to these k samples is defined as\nRk(F) := E [\nsup f\u2208F\n1\nk k\u2211 j=1 \u03b5jf(x \u2032 j) ] , (6)\nwhere the {\u03b5j} are independent Rademacher random variables.\nLemma 1. Assume that F contains the constant zero function f(x) \u2261 0, then we have\nE [\nsup f\u2208F |G(f)\u2212 `(f)|\n] \u2264 4LRk(F).\nThis lemma shows that Rk(F) controls the distance between G(f) and `(f). For the function classes studied in this paper, we will have Rk(F) = O(1/ \u221a k). Thus, the function G(f) will be a good approximation to `(f) if the sample size is large enough. This lemma is based on a slight sharpening of the usual Ledoux-Talagrand contraction for Rademacher variables (Ledoux and Talagrand, 2013); see Appendix A for the proof.\nJohnson-Lindenstrauss lemma: The Johnson-Lindenstrauss lemma is a useful tool for dimension reduction. It gives a lower bound on an integer s such that after projecting n vectors from a high-dimensional space into an s-dimensional space, the pairwise distances between this collection of vectors will be approximately preserved.\nLemma 2. For any 0 < < 1/2 and any positive integer n, consider any positive integer s \u2265 12 log n/ 2. Let \u03c6 be a operator that projects a vector in Rd to a random s-dimensional subspace of Rd, then scales the vector by \u221a d/s. Then for any set of vectors u1, . . . , un \u2208 Rd, we have\u2223\u2223\u2223\u2016ui \u2212 uj\u201622 \u2212 \u2016\u03c6(ui)\u2212 \u03c6(uj)\u201622\u2223\u2223\u2223 \u2264 \u2016ui \u2212 uj\u201622 for every i, j \u2208 [n]. (7)\nholds with probability at least 1/n.\nSee the paper by Dasgupta and Gupta (1999) for a simple proof.\nMaurey-Barron-Jones lemma: Letting G be a subset of any Hilbert space H, the MaureyBarron-Jones lemma guarantees that any point v in the convex hull of G can be approximated by a convex combination of a small number of points of G. More precisely, we have:\nLemma 3. Consider any subset G of any Hilbert space such that \u2016g\u2016H \u2264 b for all g \u2208 G. Then for any point v is in the convex hull of G, there is a point vs in the convex hull of s points of G such that \u2016v \u2212 vs\u20162H \u2264 b2/s.\nSee the paper by Pisier (1980) for a proof. This lemma is useful in our analysis of neural network learning.\nAlgorithm 1: Learning halfspaces based on initializing from uniform sampling\nInput: Feature-label pairs {(xi, yi)}ni=1; feature dimension d; parameters T and r \u2265 1. For t = 1, 2, . . . , T :\n1. Draw a random vector u uniformly from the unit sphere of Rd.\n2. Choose wt := ru, or compute wt by a poly(n, d)-time algorithm such that \u2016wt\u20162 \u2264 r and `(wt) \u2264 `(ru).\nOutput: w\u0302 := arg minw\u2208{w1,...,wT } `(w)."}, {"heading": "3 Learning Halfspaces", "text": "In this section, we assume that the function f is a linear mapping f(x) := \u3008w, x\u3009, so that our cost function can be written as\n`(w) := n\u2211 i=1 \u03b1ih(\u2212yi\u3008w, xi\u3009). (8)\nIn this section, we present two polynomial-time algorithms to approximately minimize this cost function over certain types of `p-balls. Both algorithms run multiple rounds of random initialization followed by arbitrary optimization steps. The first algorithm initializes the weight vector by uniformly drawing from a sphere. We next present and analyze a second algorithm in which the initialization follows from the solution of a least-square problem. It attains the stronger theoretical guarantees promised in the introduction."}, {"heading": "3.1 Initialization by uniform sampling", "text": "We first analyze a very simple algorithm based on uniform sampling. Letting w\u2217 denote a minimizer of the objective function (8), by rescaling as necessary, we may assume that \u2016w\u2217\u20162 = 1. As noted earlier, by redefining the function h as necessary, we may also assume that the points {xi}ni=1 all lie inside the Eulcidean ball of radius one\u2014that is, \u2016xi\u20162 \u2264 1 for all i \u2208 [n].\nGiven this set-up, a simply way in which to estimate w\u2217 is to first draw w uniformly from the Euclidean unit sphere, and then apply an iterative scheme to minimize the the loss function with the randomly drawn vector w as the initial point. At an intuitive level, this algorithm will find the global optimum if the initial weight is drawn sufficiently close to w\u2217, so that the iterative optimization method converges to the global minimum. However, by calculating volumes of spheres in high-dimensions, it could require \u2126( 1\nd ) rounds of random sampling before drawing a vector that\nis sufficiently close to w\u2217, which makes it computationally intractable unless the dimension d is small.\nIn order to remove the exponential dependence, Algorithm 1 is based on drawing the initial vector from a sphere of radius r > 1, where the radius r should be viewed as hyper-parameter of the algorithm. One should choose a greater r for a faster algorithm, but with a less accurate solution. The following theorem characterizes the trade-off between the accuracy and the time complexity.\nAlgorithm 2: Learning halfspaces based on initializing from the solution of a least-squares problem.\nInput: Feature-label pairs {(xi, yi)}ni=1; feature dimension d; parameters k, T . For t = 1, 2, . . . , T :\n1. Sample k points {(x\u2032j , y\u2032j)}kj=1 from the dataset with respect to their importance weights. Sample a vector u uniformly from [\u22121, 1]k and let v := argmin\u2016w\u2016p\u22641 \u2211k j=1(\u3008w, x\u2032j\u3009 \u2212 uj)2. 2. Choose wt := v, or compute wt by a poly(n, d)-time algorithm such that \u2016wt\u2016p \u2264 1 and `(wt) \u2264 `(v).\nOutput: w\u0302 := arg minw\u2208{w1,...,wT } `(w).\nTheorem 1. For 0 < < 1/2 and \u03b4 > 0, let s := min{d, d12 log(n + 2)/ 2e}, r := \u221a d/s and T := d(2n + 4)(\u03c0/ )s\u22121 log(1/\u03b4)e. With probability at least 1 \u2212 \u03b4, Algorithm 1 outputs a vector \u2016w\u0302\u20162 \u2264 r which satisfies:\n`(w\u0302) \u2264 `(w\u2217) + 6 L.\nThe time complexity is bounded by poly(n(1/ 2) log2(1/ ), d, log(1/\u03b4)).\nThe proof of Theorem 1, provided in Appendix B, uses the Johnson-Lindenstrauss lemma. More specifically, suppose that we project the weight vector w and all data points x1, . . . , xn to a random subspace S and properly scale the projected vectors. The Johnson-Lindenstrauss lemma then implies that with a constant probability, the inner products \u3008w, xi\u3009 will be almost invariant after the projection\u2014that is,\n\u3008w\u2217, xi\u3009 \u2248 \u3008\u03c6(w\u2217), \u03c6(xi)\u3009 = \u3008r\u03c6(w\u2217), xi\u3009 for every i \u2208 [n],\nwhere r is the scale factor of the projection. As a consequence, the vector r\u03c6(w\u2217) will approximately minimize the loss. If we draw a vector v uniformly from the sphere of S with radius r and find that v is sufficiently close to r\u03c6(w\u2217), then we call this vector v as a successful draw. The probability of a successful draw depends on the dimension of S, independent of the original dimension d (see Lemma 2). If the draw is successful, then we use v as the initialization so that it approximately minimize the loss. Note that drawing from the unit sphere of a random subspace S is equivalent to directly drawing from the original space Rd, so that there is no algorithmic need to explicitly construct the random subspace.\nIt is worthwhile to note some important deficiencies of Algorithm 1. First, the algorithm outputs a vector satisfying the bound \u2016w\u0302\u20162 \u2264 r with r > 1, so that it is not guaranteed to lie within the Euclidean unit ball. Second, the `2-norm constraints \u2016xi\u20162 \u2264 1 and \u2016w\u2217\u20162 = 1 cannot be generalized to other norms. Third, the complexity term n(1/\n2) log2(1/ ) has a power growing with 1/ . Our second algorithm overcomes these limitations."}, {"heading": "3.2 Initialization by solving a least-square problem", "text": "We turn to a more general setting in which w\u2217 and xi are bounded by a general `q-norm for some q \u2208 [2,\u221e]. Letting p \u2208 [1, 2] denote the associated dual exponent ( i.e., such that 1/p + 1/q = 1), we assume that \u2016w\u2217\u2016p \u2264 1 and \u2016xi\u2016q \u2264 1 for every i \u2208 [n]. Note that this generalizes our previous\nset-up, which applied to the case p = q = 2. In this setting, Algorithm 2 is a procedure that outputs an approximate minimizer of the loss function. In each iteration, it draws k n points from the data set D, and then constructs a random least-squares problem based on these samples. The solution to this problem is used to initialize an optimization step.\nThe success of Algorithm 2 relies on the following observation: if we sample k points independently from the dataset, then Lemma 1 implies that the sample-based loss G(f) will be sufficiently close to the original loss `(f). Thus, it suffices to minimize the sample-based loss. Note that G(f) is uniquely determined by the k inner products \u03d5(w) := (\u3008w, xi1\u3009, . . . , \u3008w, xik\u3009). If there is a vector w satisfying \u03d5(w) = \u03d5(w\u2217), then its performance on the sample-based loss will be equivalent to that of w\u2217. As a consequence, if we draw a vector u \u2208 [\u22121, 1]k that is sufficiently close to \u03d5(w\u2217) (called a successful u), then we can approximately minimize the sample-based loss by solving \u03d5(w) = u, or alternatively by minimizing \u2016\u03d5(w)\u2212 u\u201622. The latter problem can be solved by a convex program in polynomial time. The probability of drawing a successful u only depends on k, independent of the input dimension and the sample size. This allows the time complexity to be polynomial in (n, d). The trade-off between the target excess risk and the time complexity is characterized by the following theorem. For given , \u03b4 > 0, it is based on running Algorithm 2 with the choices\nk := { d2 log d/ 2e if p = 1 d(q \u2212 1)/ 2e if p > 1,\n(9)\nand T := d5(4/ )k log(1/\u03b4)e.\nTheorem 2. For given , \u03b4 > 0, with the choices of (k, T ) given above, Algorithm 2 outputs a vector \u2016w\u0302\u2016p \u2264 1 such that\n`(w\u0302) \u2264 `(w\u2217) + 11 L with probability at least 1\u2212 \u03b4.\nThe time complexity is bounded by{ poly ( n, d(1/ 2) log(1/ ), log(1/\u03b4) )\nif p = 1 poly ( n, d, e(q/ 2) log(1/ ), log(1/\u03b4) ) if p > 1.\nSee Appendix C for the proof. Theorem 2 shows that the time complexity of the algorithm has polynomial dependence on (n, d) but exponential dependence on L/ . Shalev-Shwartz et al. (2011) proved a similar complexity bound when the function h takes the piecewise-linear form (3), but our algorithm applies to arbitrary continuous functions. We note that the result is interesting only when (L/ )2 d, since otherwise the same time complexity can be achieved by a grid search of w\u2217 within the d-dimensional unit ball."}, {"heading": "3.3 Hardness result", "text": "In Theorem 2, the time complexity has an exponential dependence on L/ . Shalev-Shwartz et al. (2011) show that the time complexity cannot be polynomial in L even for improper learning. It is natural to wonder if Algorithm 2 can be improved to have polynomial dependence on (n, d, 1/ ) given that L = 1. In this section, we provide evidence that this is unlikely to be the case.\nTo prove the hardness result, we reduce from the MAX-2-SAT problem, which is known to be NP-hard. In particular, we show that if there is an algorithm solving the minimization problem (8), then it also solves the MAX-2-SAT problem. Let us recall the MAX-2-SAT problem:\nDefinition (MAX-2-SAT). Given n literals {z1, . . . , zn} and d clauses {c1, . . . , cd}. Each clause is the conjunction of two arguments that may either be a literal or the negation of a literal \u2217. The goal is to determine the maximum number of clauses that can be simultaneously satisfied by an assignment.\nSince our interest is to prove a lower bound, it suffices to study a special case of the general minimization problem\u2014namely, one in which \u2016w\u2217\u20162 \u2264 1 and \u2016xi\u20162 \u2264 1, yi = \u22121 for any i \u2208 [n]. The following proposition shows that if h is the piecewise-linear function with L = 1, then approximately minimizing the loss function is hard. See Appendix D for the proof.\nProposition 1. Let h be the piecewise-linear function (3) with Lipschitz constant L = 1. Unless RP = NP\u2020, there is no randomized poly(n, d, 1/ )-time algorithm computing a vector w\u0302 which satisfies `(w\u0302) \u2264 `(w\u2217) + with probability at least 1/2.\nProposition 1 provides a strong evidence that learning halfspaces with respect to a continuous sigmoidal loss cannot be done in poly(n, d, 1/ ) time. We note that Hush (1999) proved a similar hardness result, but without the unit-norm constraint on w\u2217 and {xi}ni=1. The non-convex ERM problem without a unit norm constraint is notably harder than ours, so this particular hardness result does not apply to our problem setup."}, {"heading": "4 Learning Neural Networks", "text": "Let us now turn to the case in which the function f represents a neural network. Given two numbers p \u2208 (1, 2] and q \u2208 [2,\u221e) such that 1/p+1/q = 1, we assume that the input vector satisfies \u2016xi\u2016q \u2264 1 for every i \u2208 [n]. The class of m-layer neural networks is recursively defined in the following way. A one-layer neural network is a linear mapping from Rd to R, and we consider the set of mappings:\nN1 := {x\u2192 \u3008w, x\u3009 : \u2016w\u2016p \u2264 B}.\nFor m > 1, an m-layer neural network is a linear combination of (m \u2212 1)-layer neural networks activated by a sigmoid function, and so we define:\nNm := { x\u2192 d\u2211 j=1 wj\u03c3(fj(x)) : d <\u221e, fj \u2208 Nm\u22121, \u2016w\u20161 \u2264 B } .\nIn this definition, the function \u03c3 : R \u2192 [\u22121, 1] is an arbitrary 1-Lipschitz continuous function. At each hidden layer, we allow the number of neurons d to be arbitrarily large, but the per-unit `1-norm must be bounded by a constant B. This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al. (2015).\nAssuming a constant `1-norm bound might be restrictive for some applications, but without this norm constraint, the neural network class activated by any sigmoid-like or ReLU-like function is not efficiently learnable (Zhang et al., 2015, Theorem 3). On the other hand, the `1-regularization imposes sparsity on the neural network. It is observed in practice that sparse neural networks\n\u2217In the standard MAX-2-SAT setup, each clause is the disjunction of two literals. However, any disjunction clause can be reduced to three conjunction clauses. In particular, a clause z1\u2228z2 is satisfied if and only if one of the following is satisfied: z1 \u2227 z2, \u00acz1 \u2227 z2, z1 \u2227 \u00acz2. \u2020RP is the class of randomized polynomial-time algorithms.\nAlgorithm 3: Algorithm for learning neural networks\nInput: Feature-label pairs {(xi, yi)}ni=1; number of layers m; parameters k, s, T,B. For t = 1, 2, . . . , T :\n1. Sample k points {(x\u2032j , y\u2032j)}kj=1 from the dataset with respect to their importance weights. 2. Generate a neural network g \u2208 Nm in the following recursive way:\n\u2022 If m = 1, then draw a vector u uniformly from [\u2212B,B]k. Let v := arg minw\u2208Rd:\u2016w\u2016p\u2264B \u2211k j=1(\u3008w, x\u2032j\u3009 \u2212 uj)2 and return g : x\u2192 \u3008v, x\u3009. \u2022 If m > 1, then generate the (m\u2212 1)-layer networks g1, . . . , gs \u2208 Nm\u22121 using this recursive program. Draw a vector u uniformly from [\u2212B,B]k. Let\nv := arg min w\u2208Rs:\u2016w\u20161\u2264B k\u2211 j=1 ( s\u2211 l=1 wl\u03c3(gl(x \u2032 j))\u2212 uj )2 (10)\nand return g : x\u2192 \u2211s\nl=1 vl\u03c3(gl(x)).\n3. Choose ft := g, or compute ft by a poly(n, d)-time algorithm such that ft \u2208 Nm and `(ft) \u2264 `(g).\nOutput: f\u0302 := arg minf\u2208{f1,...,fT } `(f).\nare capable of learning meaningful representations such as by convolutional neural networks, for instance. Moreover, it has been argued that sparse connectivity is a natural constraint that can lead to improved performance in practice (see, e.g. Thom and Palm, 2013)."}, {"heading": "4.1 Agnostic learning", "text": "In the agnostic setting, it is not assumed there exists a neural network that separates the data. Instead, our goal is to compute a neural network f\u0302 \u2208 Nm that minimizes the loss function over the space of all given networks. Letting f\u2217 \u2208 Nm be the network that minimizes the empirical loss `, we now present and analyze a method (see Algorithm 3) that computes a network whose loss is at most worse that that of f\u2217. We first state our main guarantee for this algorithm, before providing intuition. More precisely, for any , \u03b4 > 0, the following theorem applies to Algorithm 3 with the choices:\nk := dq/ 2e, s := d1/ 2e, T := \u2308 5(4/ )k(s m\u22121)/(s\u22121) log(1/\u03b4) \u2309 . (11)\nTheorem 3. For given B \u2265 1 and , \u03b4 > 0, with the choices of (k, s, T ) given above, Algorithm 3 outputs a predictor f\u0302 \u2208 Nm such that\n`(f\u0302) \u2264 `(f\u2217) + (2m+ 9) LBm with probability at least 1\u2212 \u03b4. (12)\nThe computational complexity is bounded by poly(n, d, eq(1/ 2)m log(1/ ), log(1/\u03b4)).\nWe remark that if m = 1, then Nm is a class of linear mappings. Thus, Algorithm 2 can be viewed as a special case of Algorithm 3 for learning one-layer neural networks. See Appendix E for the proof of Theorem 3.\nThe intuition underlying Algorithm 3 is similar to that of Algorithm 2. Each iteration involves resampling k independent points from the dataset. By the Rademacher generalization bound, minimizing the sample-based loss G(f) will approximately minimize the original loss `(f). The value of G(f) is uniquely determined by the vector \u03d5(f) := (f(x\u20321), . . . , f(x \u2032 k)). As a consequence, if we draw u \u2208 [\u2212B,B]k sufficiently close to \u03d5(f\u2217), then a nearly-optimal neural network will be obtained by approximately solving \u03d5(g) \u2248 u, or equivalently \u03d5(g) \u2248 \u03d5(f\u2217).\nIn general, directly solving the equation \u03d5(g) \u2248 u would be difficult even if the vector u were known. In particular, since our class Nm is highly non-linear, solving this equation cannot be reduced to solving a convex program. On the other hand, suppose that we write f\u2217(x) =\u2211s\nl=1wl\u03c3(f \u2217 l (x)) for some functions f \u2217 l \u2208 Nm. Then the problem becomes much easier if the quantities \u03c3(f\u2217l (x \u2032 j)) are already known for every (j, l) \u2208 [k] \u00d7 [d]. With this perspective in mind, we can approximately solve the equation by minimizing\nmin w\u2208Rd:\u2016w\u20161\u2264B k\u2211 j=1 ( s\u2211 l=1 wl\u03c3(f \u2217 l (x \u2032 j))\u2212 uj )2 . (13)\nAccordingly, suppose that we draw vectors a1, . . . , as \u2208 [\u2212B,B]k such that each aj is sufficiently close to \u03d5(f\u2217j )\u2014any such draw is called successful. We may then recursively compute (m \u2212 1)- layer networks g1, . . . , gs by first solving the approximate equation \u03d5(gl) \u2248 al, and then rewriting problem (13) as\nmin w\u2208Rd:\u2016w\u20161\u2264B k\u2211 j=1 ( s\u2211 l=1 wl\u03c3(gl(x \u2032 j))\u2212 uj )2 .\nThis convex program matches the problem (10) in Algorithm 3. Note that the probability of a successful draw {a1, . . . , as} depends on the dimension s. Although there is no constraint on the dimension of Nm, the Maurey-Barron-Jones lemma (Lemma 3) asserts that it suffices to choose s = O(1/ 2) to compute an -accurate approximation. We refer the reader to Appendix E for the detailed proof."}, {"heading": "4.2 Learning with separable data", "text": "We turn to the case in which the data are separable with a positive margin. Throughout this section, we assume that the activation function of Nm is an odd function (i.e., \u03c3(\u2212x) = \u2212\u03c3(x)). We say that a given data set {(xi, yi)}ni=1 is separable with margin \u03b3, or \u03b3-separable for short, if there is a network f\u2217 \u2208 Nm such that yif\u2217(xi) \u2265 \u03b3 for each i \u2208 [n]. Given a distribution P over the space X \u00d7{\u22121, 1}, we say that it is \u03b3-separable if there is a network f\u2217 \u2208 Nm such that yf\u2217(x) \u2265 \u03b3 almost surely (with respect to P).\nAlgorithm 4 learns a neural network on the separable data. It uses the AdaBoost approach (Freund and Schapire, 1997) to construct the network, and we refer to it as the BoostNet algorithm. In each iteration, it trains a weak classifier g\u0302t \u2208 Nm\u22121 with an error rate slightly better than random guessing, then adds the weak classifier to the strong classifier to construct an m-layer network. The weaker classifier is trained by Algorithm 3 (or by Algorithm 1 or Algorithm 2 if Nm\u22121 are one-layer networks). The following theorem provides guarantees for its performance when it is run for\nT := \u230816B2 log(n+ 1)\n\u03b32\n\u2309\nAlgorithm 4: The BoostNet algorithm\nInput: Feature-label pairs {(xi, yi)}ni=1; number of layers m \u2265 2; parameters \u03b4, \u03b3, T,B. Initialize f0 = 0 and b0 = 0. For t = 1, 2, . . . , T :\n1. Define Gt(g) := \u2211n\ni=1 \u03b1t,i\u03c3(\u2212yig(xi)) where g \u2208 Nm\u22121 and \u03b1t,i := e\u2212yi\u03c3(ft\u22121(xi))\u2211n j=1 e \u2212yi\u03c3(ft\u22121(xi)) .\n2. Compute g\u0302t \u2208 Nm\u22121 by Algorithm 3 such that\nGt(g\u0302t) \u2264 inf g\u2208Nm\u22121\nGt(g) + \u03b3\n2B (14)\nwith probability at least 1\u2212 \u03b4/T \u2021. Let \u00b5t := max{\u221212 , Gt(g\u0302t)}.\n3. Set ft = ft\u22121 + 1 2 log( 1\u2212\u00b5t 1+\u00b5t )g\u0302t and bt = bt\u22121 + 1 2 \u2223\u2223\u2223log(1\u2212\u00b5t1+\u00b5t )\u2223\u2223\u2223. Output: f\u0302 := BbT fT .\niterations. The running time depends on a quantity Cm,B,1/\u03b3 that is a constant for any choice of the triple (m,B, 1/\u03b3), but with exponential dependence on 1/\u03b3.\nTheorem 4. With the above choice of T , the BoostNet algorithm achieves:\n(a) In-sample error: For any \u03b3-separable dataset {(xi, yi)}ni=1 Algorithm 4 outputs a neural network f\u0302 \u2208 Nm such that,\nyif\u0302(xi) \u2265 \u03b3\n16 for every i \u2208 [n], with probability at least 1\u2212 \u03b4.\nThe time complexity is bounded by poly(n, d, log(1/\u03b4), Cm,B,1/\u03b3).\n(b) Generalization error: Given a data set consisting of n = poly(1/ , log(1/\u03b4)) i.i.d. samples from any \u03b3-separable distribution P, Algorithm 4 outputs a network f\u0302 \u2208 Nm such that\nP [ sign(f\u0302(x)) 6= y ] \u2264 with probability at least 1\u2212 2\u03b4. (15)\nMoreover, the time complexity is bounded by poly(d, 1/ , log(1/\u03b4), Cm,B,1/\u03b3).\nSee Appendix F for the proof. The most technical work is devoted to proving part (a). The generalization bound in part (b) follows by combining part (a) with bounds on the Rademacher complexity of the network class, which then allow us to translate the in-sample error bound to generalization error in the usual way. It is worth comparing the BoostNet algorithm with the general algorithm for agnostic learning. In order to bound the generalization error by > 0, the time complexity of Algorithm 3 will be exponential in 1/ .\nThe same learnability results can be established even if the labels are randomly corrupted. Formally, for every pair (x, y) sampled from a \u03b3-separable distribution, suppose that the learning algorithm actually receives the corrupted pair (x, y\u0303), where\ny\u0303 = { y with probability 1\u2212 \u03b7, \u2212y with probability \u03b7.\n\u2021We may choose the hyper-parameters of Algorithm 3 by equation (11), with the additive error defined by \u03b3/((4m+ 10)LBm). Theorem 3 guarantees that the error bound (14) holds with high probability.\nHere the parameter \u03b7 \u2208 [0, 12) corresponds to the noise level. Since the labels are flipped, the BoostNet algorithm cannot be directly applied. However, we can use the improper learning algorithm of Zhang et al. (2015) to learn an improper classifier h\u0302 such that h\u0302(x) = y with high probability, and then apply the BoostNet algorithm taking (x, h\u0302(x)) as input. Doing so yields the following guarantee:\nCorollary 1. Assume that q = 2 and \u03b7 < 1/2. For any constant (m,B), consider the neural network class Nm activated by \u03c3(x) := erf(x)\u00a7. Given a random dataset of size n = poly(1/ , 1/\u03b4) for any \u03b3-separable distribution, there is a poly(d, 1/ , 1/\u03b4)-time algorithm that outputs a network f\u0302 \u2208 Nm such that\nP(sign(f\u0302(x)) 6= y) \u2264 with probability at least 1\u2212 \u03b4.\nSee Appendix G for the proof."}, {"heading": "4.3 Hardness result for \u03b3-separable problems", "text": "Finally, we present a hardness result showing that the dependence on 1/\u03b3 is hard to improve. Our proof relies on the hardness of standard (nonagnostic) PAC learning of the intersection of halfspaces given in Klivans et al. (2006). More precisely, consider the family of halfspace indicator functions mapping X = {\u22121, 1}d to {\u22121, 1} given by\nH = {x\u2192 sign(wTx\u2212 b\u2212 1/2) : x \u2208 {\u22121, 1}d, b \u2208 N, w \u2208 Nd, |b|+ \u2016w\u20161 \u2264 poly(d)}.\nGiven a T -tuple of functions {h1, . . . , hT } belonging to H, we define the intersection function\nh(x) = { 1 if h1(x) = \u00b7 \u00b7 \u00b7 = hT (x) = 1, \u22121 otherwise,\nwhich represents the intersection of T half-spaces. Letting HT denote the set of all such functions, for any distribution on X , we want an algorithm taking a sequence of (x, h\u2217(x)) as input where x is a sample from X and h\u2217 \u2208 HT . It should output a function h\u0302 such that P(h\u0302(x) 6= h\u2217(x)) \u2264 with probability at least 1\u2212\u03b4. If there is such an algorithm whose sample complexity and time complexity scale as poly(d, 1/ , 1/\u03b4), then we say that HT is efficiently learnable. Klivans et al. (2006) show that if T = \u0398(d\u03c1) then HT is not efficiently learnable under a certain cryptographic assumption. This hardness statement implies the hardness of learning neural networks with separable data.\nProposition 2. Assume HT is not efficiently learnable for T = \u0398(d \u03c1). Consider the class of twolayer neural networks N2 activated by the piecewise linear function \u03c3(x) := min{1,max{\u22121, x}} or the ReLU function \u03c3(x) := max{0, x}, and with the norm constraint B = 2. Consider any algorithm such that when applied to any \u03b3-separable data distribution, it is guaranteed to output a neural network f\u0302 satisfying P(sign(f\u0302(x)) 6= y) \u2264 with probability at least 1\u2212 \u03b4. Then it cannot run in poly(d, 1/ , 1/\u03b4, 1/\u03b3)-time.\nSee Appendix H for the proof.\n\u00a7The erf function can be replaced by any function \u03c3 satisfying polynomial expansion \u03c3(x) = \u2211\u221e j=0 \u03b2jx\nj , such that\u2211\u221e j=0 2 j\u03b22j\u03bb 2j < +\u221e for any finite \u03bb \u2208 R+."}, {"heading": "5 Simulation", "text": "In this section, we compare the BoostNet algorithm with the classical backpropagation method for training two-layer neural networks. The goal is to learn parity functions from noisy data \u2014 a challenging problem in computational learning theory (see, e.g. Blum et al., 2003). We construct a synthetic dataset with n = 50, 000 points. Each point (x, y) is generated as follows: first, the vector x is uniformly drawn from {\u22121, 1}d and concatenated with a constant 1 as the (d+1)-th coordinate. The label is generated as follows: for some unknown subset of p indices 1 \u2264 i1 < \u00b7 \u00b7 \u00b7 < ip \u2264 d, we set\ny =\n{ xi1xi2 . . . xip with probability 0.9 ,\n\u2212xi1xi2 . . . xip with probability 0.1 .\nThe goal is to learn a function f : Rd+1 \u2192 R such that sign(f(x)) predicts the value of y. The optimal rate is achieved by the parity function f(x) = xi1xi2 . . . xip , in which case the prediction error is 0.1. If the parity degree p > 1, the optimal rate cannot be achieved by any linear classifier.\nChoose d = 50 and p \u2208 {2, 5}. The activation function is chosen as \u03c3(x) := tanh(x). The training set, the validation set and the test set contain respectively 25K, 5K and 20K points. To train a two-layer BoostNet, we choose the hyper-parameter B = 10, and select Algorithm 3 as the subroutine to train weak classifiers with hyper-parameters (k, T ) = (10, 1). To train a classical two-layer neural network, we use the random initialization scheme of Nguyen and Widrow (1990) and the backpropagation algorithm of M\u00f8ller (1993). For both methods, the algorithm is executed for ten independent rounds to select the best solution.\nFigure 5 compares the prediction errors of BoostNet and backpropagation. Both methods generate the same two-layer network architecture, so that we compare with respect to the number of hidden nodes. Note that BoostNet constructs hidden nodes incrementally while NeuralNet trains a predefined number of neurons. Figure 5 shows that both algorithms learn the degree-2 parity function with a few hidden nodes. In contrast, BoostNet learns the degree-5 parity function with less than 50 hidden nodes, while NeuralNet\u2019s performance is no better than random guessing. This suggests that the BoostNet algorithm is less likely to be trapped in a bad local optimum in this setting."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed algorithms to learn halfspaces and neural networks with non-convex loss functions. We demonstrate that the time complexity is polynomial in the input dimension and in the sample size but exponential in the excess risk. A hardness result relating to the necessity of the exponential dependence is also presented. The algorithms perform randomized initialization followed by optimization steps. This idea coincides with the heuristics that are widely used in practice, but the theoretical analysis suggests that a careful treatment of the initialization step is necessary. We proposed the BoostNet algorithm, and showed that it can be used to learn a neural network in polynomial time when the data are separable with a constant margin. We suspect that the theoretical results of this paper are likely conservative, in that in application to real data, the algorithms can be much more efficient than the bounds might suggest.\nAcknowledgements:\nMW and YZ were partially supported by NSF grant CIF-31712-23800 from the National Science Foundation, AFOSR-FA9550-14-1-0016 grant from the Air Force Office of Scientific Research, ONR MURI grant N00014-11-1-0688 from the Office of Naval Research. MJ and YZ were partially supported by the U.S.ARL and the U.S.ARO under contract/grant number W911NF-11-1-0391. We thank Sivaraman Balakrishnan for helpful comments on an earlier draft."}, {"heading": "A Proof of Lemma 1", "text": "The following inequality always holds:\nsup f\u2208F |G(f)\u2212 `(f)| \u2264 max { sup f\u2208F {G(f)\u2212 `(f)}, sup f \u2032\u2208F {`(f \u2032)\u2212G(f \u2032)} } .\nSince F contains the constant zero function, both supf\u2208F{G(f)\u2212`(f)} and supf \u2032\u2208F{`(f \u2032)\u2212G(f \u2032)} are non-negative, which implies\nsup f\u2208F |G(f)\u2212 `(f)| \u2264 sup f\u2208F {G(f)\u2212 `(f)}+ sup f \u2032\u2208F {`(f \u2032)\u2212G(f \u2032)}.\nTo establish Lemma 1, it suffices to prove: E [\nsup f\u2208F {G(f)\u2212 `(f)}\n] \u2264 2LRk(F) and E [ sup f \u2032\u2208F {`(f \u2032)\u2212G(f \u2032)} ] \u2264 2LRk(F)\nFor the rest of the proof, we will establish the first upper bound. The second bound can be established through an identical series of steps.\nThe inequality E[supf\u2208F{G(f) \u2212 `(f)}] \u2264 2LRk(F) follows as a consequence of classical symmetrization techniques (e.g. Bartlett and Mendelson, 2003) and the Talagrand-Ledoux concentration (e.g. Ledoux and Talagrand, 2013, Corollary 3.17). However, so as to keep the paper self-contained, we provide a detailed proof here. By the definitions of `(f) and G(f), we have\nE [\nsup f\u2208F\n{ G(f)\u2212 `(f) }] = E [ sup f\u2208F {1 k k\u2211 j=1 h(\u2212y\u2032jf(x\u2032j))\u2212 E [1 k k\u2211 j=1 h(\u2212y\u2032\u2032j f(x\u2032\u2032j )) ]}] ,\nwhere (x\u2032\u2032j , y \u2032\u2032 j ) is an i.i.d. copy of (x \u2032 j , y \u2032 j). Applying Jensen\u2019s inequality yields\nE [\nsup f\u2208F\n{ G(f)\u2212 `(f) }] \u2264 E [ sup f\u2208F {1 k k\u2211 j=1 h(\u2212y\u2032jf(x\u2032j))\u2212 h(\u2212y\u2032\u2032j f(x\u2032\u2032j )) }]\n= E [\nsup f\u2208F {1 k k\u2211 j=1 \u03b5j(h(\u2212y\u2032jf(x\u2032j))\u2212 h(\u2212y\u2032\u2032j f(x\u2032\u2032j ))) }]\n\u2264 E [\nsup f\u2208F {1 k k\u2211 j=1 \u03b5jh(\u2212y\u2032jf(x\u2032j)) + sup f\u2208F 1 k k\u2211 j=1 \u03b5jh(\u2212y\u2032\u2032j f(x\u2032\u2032j )) }]\n= 2E [\nsup f\u2208F {1 k k\u2211 j=1 \u03b5jh(\u2212y\u2032jf(x\u2032j)) }] . (16)\nWe need to bound the right-hand side using the Rademacher complexity of the function class F , and we use an argument following the lecture notes of Kakade and Tewari (2008). Introducing the shorthand notation \u03d5j(x) := h(\u2212y\u2032jx), the L-Lipschitz continuity of \u03d5j implies that\nE [\nsup f\u2208F k\u2211 j=1 \u03b5j\u03d5j(f(x \u2032 j)) ] = E [ sup f,f \u2032\u2208F {\u03d51(f(x\u20321))\u2212 \u03d51(f \u2032(x\u20321)) 2 + k\u2211 j=2 \u03b5j \u03d5j(f(x \u2032 j)) + \u03d5j(f \u2032(x\u2032j)) 2 }]\n\u2264 E [\nsup f,f \u2032\u2208F {L|f(x\u20321)\u2212 f \u2032(x\u20321)| 2 + k\u2211 j=2 \u03b5j \u03d5j(f(x \u2032 j)) + \u03d5j(f \u2032(x\u2032j)) 2 }]\n= E [\nsup f,f \u2032\u2208F {Lf(x\u20321)\u2212 Lf \u2032(x\u20321) 2 + k\u2211 j=2 \u03b5j \u03d5j(f(x \u2032 j)) + \u03d5j(f \u2032(x\u2032j)) 2 }] .\nApplying Jensen\u2019s inequality implies that the right-hand side is bounded by\nRHS \u2264 1 2 E [ sup f\u2208F { Lf(x\u20321) + k\u2211 j=2 \u03b5j\u03d5j(f(x \u2032 j)) } + sup f \u2032\u2208F { \u2212 Lf(x\u20321) + k\u2211 j=2 \u03b5j\u03d5j(f \u2032(x\u2032j)) }]\n= E [\nsup f\u2208F\n{ \u03b51Lf(x \u2032 1) + k\u2211 j=2 \u03b5j\u03d5j(f(x \u2032 j)) }] .\nBy repeating this argument for j = 2, 3, . . . , k, we obtain\nE [\nsup f\u2208F k\u2211 j=1 \u03b5j\u03d5j(f(x \u2032 j)) ] \u2264 LE [ sup f\u2208F k\u2211 j=1 \u03b5jf(x \u2032 j) ] . (17)\nCombining inequalities (16) and (17), we have the desired bound."}, {"heading": "B Proof of Theorem 1", "text": "Let us study a single iteration of Algorithm 1. Recall that u is uniformly sampled from the unit sphere of Rd. Alternatively, it can be viewed as sampled by the following procedure: first draw a random s-dimensional subspace of Rd, then draw u uniformly from the unit sphere of S. Consider the n + 2 fixed points {0, w\u2217, x1, . . . , xn}. Let \u03c6(w) := r\u03a0S(w) be an operator that projects the vector w \u2208 Rd to the subspace S and scale it by r. If we choose s \u2265 12 log(n+2)\n2 and r :=\n\u221a d/s,\nthen Lemma 2 implies that with probability at least 1/(n+ 2), we are guaranteed that\n|\u2016\u03c6(w\u2217)\u201622| \u2212 \u2016w\u2217\u201622| \u2264 , \u2016\u03c6(xi)\u201622| \u2212 \u2016xi\u201622| \u2264 and |\u2016\u03c6(w\u2217)\u2212 \u03c6(xi)\u201622 \u2212 \u2016w\u2217 \u2212 xi\u201622| \u2264 |4 for any i \u2208 [n]. (18)\nConsequently, we have\n|\u3008\u03c6(w\u2217), \u03c6(xi)\u3009 \u2212 \u3008w\u2217, xi\u3009| \u2264 3 for any i \u2208 [n]. (19)\nAssume that the approximation bounds (18) and (19) hold. Then using the L-Lipschitz continuity of h, we have\n`(w\u2217) = n\u2211 i=1 \u03b1ih(\u2212yi\u3008w\u2217, xi\u3009) \u2265 \u22123 L+ k\u2211 j=1 \u03b1ih(\u2212yi\u3008\u03c6(w\u2217), \u03c6(xi)\u3009). (20)\nRecall that u is uniformly drawn from the unit sphere of S; therefore, the angle between u and \u03c6(w\u2217) is at most with probability at least ( /\u03c0)s\u22121. By inequality (18), the norm of \u03c6(w\u2217) is in [1\u2212 , 1+ ]. Hence, whenever the angle bound holds, the distance between u and \u03c6(w\u2217) is bounded by 2 . Combining this bound with inequality (20), we find that\n`(w\u2217) \u2265 \u22123 L+ n\u2211 i=1 \u03b1ih(\u2212yi\u3008u+ (\u03c6(w\u2217)\u2212 u), \u03c6(xi)\u3009).\nNote that we have the inequality\n|\u3008\u03c6(w\u2217)\u2212 u, \u03c6(xi)\u3009| \u2264 \u2016\u03c6(w\u2217)\u2212 u\u20162\u2016\u03c6(xi)\u20162 \u2264 2 (1 + ) \u2264 3 .\nCombined with the L-Lipschitz condition, we obtain the lower bound\n`(w\u2217) \u2265 \u22126 L+ k\u2211 j=1 \u03b1ih(\u2212yi\u3008u, \u03c6(xi)\u3009) = `(ru)\u2212 6 L. (21)\nThis bound holds with probability at least ( /\u03c0) s\u22121\n2n+4 . Thus, by repeating the procedure for a total\nof T \u2265 (2n+ 4)(\u03c0/ )s\u22121 log(1/\u03b4) iterations, then the best solution satisfies inequality (21) with probability at least 1 \u2212 \u03b4. The time complexity is obtained by plugging in the stated choices of (s, T )."}, {"heading": "C Proof of Theorem 2", "text": "Let us study a single iteration of Algorithm 2. Conditioning on any {(x\u2032j , y\u2032j)}kj=1, define the function G(w) := 1k \u2211k j=1 h(\u2212y\u2032j\u3008w, x\u2032j\u3009). Since h is L-Lipschitz, we have\nG(v)\u2212G(w\u2217) = 1 k k\u2211 j=1 h(\u2212y\u2032j\u3008v, x\u2032j\u3009)\u2212 h(\u2212y\u2032j\u3008w\u2217, x\u2032j\u3009) \u2264 L k k\u2211 j=1 |\u3008v \u2212 w\u2217, x\u2032j\u3009|\n\u2264 L\u221a k ( k\u2211 j=1 (\u3008v \u2212 w\u2217, x\u2032j\u3009)2 )1/2 ,\nwhere the final step follows from the Cauchy-Schwarz inequality. Let X \u2032 \u2208 Rk\u00d7d be a design matrix whose j-th row is equal to x\u2032j . Using the above bound, for any u \u2208 [\u22121, 1]k, we have\nG(v)\u2212G(w\u2217) \u2264 L\u221a k \u2016X \u2032(v \u2212 w\u2217)\u20162 \u2264 L\u221a k (\u2016X \u2032v \u2212 u\u20162 + \u2016X \u2032w\u2217 \u2212 u\u20162)\n\u2264 2L\u221a k \u2016X \u2032w\u2217 \u2212 u\u20162, (22)\nwhere the last step uses the fact that the vector v minimizes \u2016X \u2032w \u2212 u\u20162 over the set of vectors with \u2016w\u2016p \u2264 1. The right-hand side of inequality (22) is independent of v. Indeed, since X \u2032w\u2217 \u2208 [\u22121, 1]k and u is uniformly sampled from this cube, the probability that \u2016X \u2032w\u2217 \u2212 u\u2016\u221e \u2264 /2 is at least ( /4)k. This bound leads to \u2016X \u2032w\u2217 \u2212 u\u20162 \u2264 \u221a k/2. Thus, with probability at least ( /4)k, we have G(v)\u2212G(w\u2217) \u2264 L. Conditioned on the above bound holding, Lemma 1 implies that\nE [\nsup \u2016w\u2016p\u22641 \u2223\u2223`(w)\u2212G(w)\u2223\u2223] \u2264 4LRk(F), where Rk(F) is the k-sample Rademacher complexity of the function class F := {f : x\u2192 \u3008w, x\u3009 | \u2016w\u2016p \u2264 1, \u2016x\u2016q \u2264 1}. Markov\u2019s inequality implies that sup\u2016w\u2016p\u22641 \u2223\u2223`(w) \u2212 G(w)\u2223\u2223 \u2264 5LRk(F) with\nprobability at least 1/5. This event only depends on the choice of {(x\u2032j , y\u2032j)}, and conditioned on it holding, we have\n`(v) \u2264 G(v) + 5LRk(F) = G(w\u2217) + (G(v)\u2212G(w\u2217)) + 5LRk(F) \u2264 `(f\u2217) + (G(v)\u2212G(w\u2217)) + 10LRk(F).\nIt is known (e.g. Kakade et al., 2009) that the Rademacher complexity is upper bounded as\nRk(F) \u2264  \u221a 2 log d k if p = 1\u221a\nq\u22121 k if p > 1, where 1/q + 1/p = 1.\nThus, given the choice of k from equation (9) and plugging in the bound on G(v)\u2212G(w\u2217), we have `(v) \u2264 `(w\u2217) + 11 L with probability at least ( /4)k/5. Repeating this procedure for a total of T \u2265 5(4/ )k log(1/\u03b4) times guarantees that the upper bound holds with probability at least 1\u2212 \u03b4. Finally, the claimed time complexity is obtained by plugging in the stated choices of k and T ."}, {"heading": "D Proof of Proposition 1", "text": "We prove the lower bound based on a data set with \u2016xi\u20162 \u2264 1 and yi = \u22121 for each i \u2208 [n]. With this set-up, consider the following problem:\nminimize `(w) s.t. \u2016w\u20162 \u2264 1 where `(w) := 1\nn n\u2211 i=1 h(\u3008w, xi\u3009). (23)\nThe following lemma, proved in Appendix D.1, shows that for a specific 1-Lipschitz continuous function h, approximately minimizing the loss function is NP-hard.\nLemma 4. Consider the optimization problem (23) based on the function h(x) := min{0, x}. It is NP-hard to compute a vector w\u0302 \u2208 Rd such that \u2016w\u0302\u20162 \u2264 1 and `(w\u0302) \u2264 `(w\u2217) + 1(n+2)d .\nWe reduce the problem described by Lemma 4 to the problem of Theorem 1. We do so by analyzing the piecewise linear function\nh(x) :=  1 x \u2265 1/2, 0 x \u2264 \u22121/2, x+ 1/2 otherwise.\n(24)\nWith this choice of h, consider an instance of the problem underlying Lemma 4, say\nmin \u2016w\u20162\u22641\ng(w) := 1\nn n\u2211 i=1 min{0, \u3008w, xi\u3009}, (25)\nand let w\u2217 be a vector that achieves the above minimum. Our next step is to construct a new problem in dimension d+1, and argue that any approximate solution to it also leads to an approximate solution to the generic instance (25). For each i \u2208 [n], define the (d+ 1)-dimensional vector x\u2032i := (xi/ \u221a 2, 1/ \u221a 2), as well as the quantities u := \u2212ed+1/ \u221a 2\nand v := ed+1/(2 \u221a\n2). Here ed+1 represents the unit vector aligning with the (d+ 1)-th coordinate. For a (d+ 1)-dimensional weight vector w\u0303 \u2208 Rd+1, consider the objective function\n\u02dc\u0300(w\u0303) := 1 13n\n( 6nh(\u3008w\u0303, u\u3009) + 6nh(\u3008w\u0303, v\u3009) + n\u2211 i=1 h(\u3008w\u0303, x\u2032i\u3009) ) .\nIf we decompose w\u0303 as w\u0303 = (\u03b1/ \u221a 2, \u03c4/ \u221a\n2) where \u03b1 \u2208 Rd and \u03c4 \u2208 R, then these parameters satisfy \u2016\u03b1\u201622 + \u03c42 \u2264 2. We then have the equivalent expression\n\u02dc\u0300(w\u0303) = 1 13n\n( 6nh(\u2212\u03c4/2) + 6nh(\u03c4/4) + n\u2211 i=1 h(\u3008\u03b1, xi\u3009/2 + \u03c4/2) ) .\nLemma 5. Given any \u2208 (0, 126), suppose that w\u0303 = ( \u03b1\u221a 2 , \u03c4\u221a 2 ) is an approximate minimizer of the\nfunction \u02dc\u0300with additive error , Then we have\u00b6 g ( \u03b1 \u2016\u03b1\u20162 ) \u2264 g(w\u2217) + 26 . (26)\nThus, if there is a polynomial-time algorithm for minimizing function \u02dc\u0300, then there is a polynomialtime algorithm for minimizing function g. Applying the hardness result for minimization of the function g (see Lemma 4) completes the proof of the proposition.\nIt remains to prove the two lemmas, and we do so in the following two subsections.\nD.1 Proof of Lemma 4\nWe reduce MAX-2-SAT to the minimization problem. Given a MAX-2-SAT instance, we construct a loss function ` so that if any algorithm computes a vector w\u0302 satisfying\n`(w\u0302) \u2264 `(w\u2217) + 1 (2n+ 2)d , (27)\nthen the vector w\u0302 solves MAX-2-SAT. First, we construct n+ 1 vectors in Rd. Define the vector x0 := 1\u221ad1d, and for i = 1, . . . , n, the vectors xi := 1\u221a d x\u2032i, where x \u2032 i \u2208 Rd is given by\nx\u2032ij =  1 if zi appears in cj ,\n\u22121 if \u00aczi appears in cj , 0 otherwise.\nIt is straightforward to verify that that \u2016xi\u20162 \u2264 1 for any i \u2208 {0, 1, . . . , n}. We consider the following minimization problem:\n`(w) = 1\n2n+ 2 n\u2211 i=0 ( min{0, \u3008w, xi\u3009}+ min{0, \u3008w,\u2212xi\u3009} ) .\n\u00b6In making this claim, we define 0/\u20160\u20162 := 0.\nThe goal is to find a vector w\u2217 \u2208 Rd such that \u2016w\u2217\u20162 \u2264 1 and it minimizes the function `(w). Notice that for every index i, at most one of min{0, \u3008w, xi\u3009} and min{0, \u3008w,\u2212xi\u3009} is non-zero. Thus, we may write the minimization problem as\nmin \u2016w\u20162\u22641 (2n+ 2)`(w) = min \u2016w\u20162\u22641 n\u2211 i=0 ( min \u03b1i\u2208{\u22121,1} \u3008w,\u03b1ixi\u3009 ) = min \u03b1\u2208{\u22121,1}n+1 min \u2016w\u20162\u22641 n\u2211 i=0 \u3008w,\u03b1ixi\u3009\n= min \u03b1\u2208{\u22121,1}n+1 \u2212 \u2225\u2225\u2225 n\u2211 i=0 \u03b1ixi \u2225\u2225\u2225 2\n= \u2212  max \u03b1\u2208{\u22121,1}n+1 d\u2211 j=1 ( n\u2211 i=0 \u03b1ixij )21/2 . (28)\nWe claim that maximizing \u2211d j=1( \u2211n i=0 \u03b1ixij) 2 with respect to \u03b1 is equivalent to maximizing the number of satisfiable clauses. In order to prove this claim, we consider an arbitrary assignment to \u03b1 to construct a solution to the MAX-2-SAT problem. For i = 1, 2, . . . , n, let zi = true if \u03b1i = \u03b10, and let zi = false if \u03b1i = \u2212\u03b10. With this assignment, it is straightforward to verify the following: if the clause cj is satisfied, then the value of \u2211n i=0 \u03b1ixij is either 3/ \u221a d or \u22123/ \u221a d. If the clause is\nnot satisfied, then the value of the expression is either 1/ \u221a d or \u22121/ \u221a d. To summarize, we have\nd\u2211 j=1 ( n\u2211 i=0 \u03b1ixij )2 = 1 + 8\u00d7 (# of satisfied clauses) d . (29)\nThus, solving problem (28) determines the maximum number of satisfiable clauses:\n(max # of satisfied clauses) = d\n8 (( min \u2016w\u20162\u22641 (2n+ 2)`(w) )2 \u2212 1).\nBy examining equation (28) and (29), we find that the value of (2n + 2)`(w) ranges in [\u22123, 0]. Thus, the MAX-2-SAT number is exactly determined if (2n+ 2)`(w\u0302) is at most 1/d larger than the optimal value. This optimality gap is guaranteed by inequality (27), which completes the reduction.\nD.2 Proof of Lemma 5\nIn order to prove the claim, we first argue that \u03c4 \u2208 [0, 2]. If this inclusion does not hold, then it can be verified that h(\u2212\u03c4/2) + h(\u03c4/4) \u2265 613 , which implies that\n\u02dc\u0300(w\u0303) \u2265 6 13 + 1 13n n\u2211 i=1 h(\u3008\u03b1, xi\u3009/2 + \u03c4/2) \u2265 12 26 .\nHowever, the feasible assignment (\u03b1, \u03c4) = (0, 1) achieves function value 1126 , which contradicts the assumption that \u02dc\u0300(w\u0303) is -optimal. Thus, we assume henceforth \u03c4 \u2208 [0, 2], and we can rewrite \u02dc\u0300 as\n\u02dc\u0300(w\u0303) = 3|\u03c4 \u2212 1|+ 9 26 + 1 13n n\u2211 i=1 h(\u3008\u03b1, xi\u3009/2 + \u03c4/2).\nNotice that h(x+ 1/2) is lower bounded by 1 + min{0, x}. Thus, we have the lower bound\n\u02dc\u0300(w\u0303) \u2265 3|\u03c4 \u2212 1|+ 9 26 + 1 13 + 1 13n n\u2211 i=1 min{0, \u3008\u03b1, xi\u3009/2 + (\u03c4 \u2212 1)/2}\n\u2265 11 26 + g(\u03b1/2) 13 + |\u03c4 \u2212 1| 13 , (30)\nwhere the last inequality uses the 1-Lipschitz continuous of min{0, x}. Notice that the function g satisfies g(\u03b2w) = \u03b2g(w) for any scalar \u03b2. Thus, we can further lower bound the right-hand side by\ng(\u03b1/2) = \u2016\u03b1\u20162 2 \u00b7 g ( \u03b1 \u2016\u03b1\u20162 ) \u2265 1 2 g ( \u03b1 \u2016\u03b1\u20162 ) \u2212 max{0, \u2016\u03b1\u20162 \u2212 1} 2 , (31)\nwhere the last inequality uses the fact that g( \u03b1\u2016\u03b1\u20162 ) \u2265 \u22121. Note that inequality (31) holds even for \u03b1 = 0 according the definition that 0/\u20160\u20162 = 0. If the quantity \u2016\u03b1\u20162 \u2212 1 is positive, then \u2016\u03b1\u20162 > 1 and consequently \u03c4 < 1. Thus we have\n\u2016\u03b1\u20162 \u2212 1 < \u2016\u03b1\u201622 \u2212 1 \u2264 1\u2212 \u03c42 = (1 + \u03c4)(1\u2212 \u03c4) < 2(1\u2212 \u03c4).\nThus, we can lower bound g(\u03b1/2) by 12g( \u03b1 \u2016\u03b1\u20162 )\u2212|\u03c4\u22121|. Combining this bound with inequality (30), we obtain the lower bound\n`(w\u0303) \u2265 11 26 + 1 26 g ( \u03b1 \u2016\u03b1\u20162 ) . (32)\nNote that the assignment (\u03b1, \u03c4) = (w\u2217, 1) is feasible. For this assignment, it is straightforward to verify that the function value is equal to 1126 + 1 26g(w\n\u2217). Using the fact that w\u0303 is an -optimal solution, we have `(w\u0303) \u2264 1126 + 1 26g(w \u2217) + . This combining with inequality (32) implies that\ng (\n\u03b1 \u2016\u03b1\u20162\n) \u2264 g(w\u2217) + 26 , which completes the proof."}, {"heading": "E Proof of Theorem 3", "text": "Recalling the definition (6) of the Rademacher complexity, the following lemma bounds the complexity of Nm.\nLemma 6. The Rademacher complexity of Nm is bounded as Rk(Nm) \u2264 \u221a q k B m.\nSee Appendix E.1 for the proof.\nWe study a single iteration of Algorithm 3. Conditioning on any {(x\u2032j , y\u2032j)}, define the quantity G(f) := 1k \u2211k j=1 h(\u2212y\u2032jf(x\u2032j)). Since the function h is L-Lipschitz continuous, we have\nh(\u2212y\u2032jg(x\u2032j))\u2212 h(\u2212y\u2032jf\u2217(x\u2032j)) \u2264 L|g(x\u2032j)\u2212 f\u2217(x\u2032j)| for any j \u2208 [k]. (33)\nGiven any function f : Rd \u2192 R, we denote the vector (f(x\u20321), . . . , f(x\u2032k)) by \u03d5(f). For instance, \u2016\u03d5(g)\u2212 \u03d5(f\u2217)\u20162 represents the `2-norm ( \u2211k j=1 |g(x\u2032j) \u2212 f\u2217(x\u2032j)|2)1/2. Inequality (33) and the Cauchy-Schwarz inequality implies\nG(g)\u2212G(f\u2217) \u2264 L k k\u2211 j=1 |g(x\u2032j)\u2212 f\u2217(x\u2032j)| \u2264 L\u221a k \u2016\u03d5(g)\u2212 \u03d5(f\u2217)\u20162.\nWe need one further auxiliary lemma:\nLemma 7. For any fixed function f\u2217 \u2208 Nm, we have\n\u2016\u03d5(g)\u2212 \u03d5(f\u2217)\u20162 \u2264 (2m\u2212 1) \u221a kBm with probability at least pm := ( 4 )k(sm\u22121)/(s\u22121) . (34)\nSee Appendix E.2 for the proof of this claim.\nLet us now complete the proof of the theorem, using this two lemmas. Lemma 1 implies that E [\nsup f\u2208Nm \u2223\u2223`(f)\u2212G(f)\u2223\u2223] \u2264 4LRk(Nm). By Markov\u2019s inequality, we have supf\u2208Nm\n\u2223\u2223`(f)\u2212G(f)\u2223\u2223 \u2264 5LRk(Nm) with probability at least 1/5. This event only depends on the choice of {(x\u2032j , y\u2032j)}. If it is true, then we have\n`(g) \u2264 G(g) + 5LRk(Nm) = G(f\u2217) + (G(g)\u2212G(f\u2217)) + 5LRk(Nm)\n\u2264 `(f\u2217) + L\u221a k \u2016\u03d5(g)\u2212 \u03d5(f\u2217)\u20162 + 10LRk(Nm).\nBy Lemma 6, we have Rk(Nm) \u2264 \u221a q k \u220fm l=1Bl. Thus, setting k = q/ 2, substituting the bound into equation (34) and simplifying yields\n`(g) \u2264 `(f\u2217) + (2m+ 9) LBm\nwith probability at least pm/5. If we repeat the procedure for T = (5/pm) log(1/\u03b4) times, then the desired bound holds with probability at least 1\u2212 \u03b4. The time complexity is obtained by plugging in the choices of (s, k, T ).\nE.1 Proof of Lemma 6\nWe prove the claim by induction on the number of layers m. It is known (Kakade et al., 2009) that Rk(N1) \u2264 \u221a q k B. Thus, the claim holds for the base case m = 1. Now consider some m > 1, and assume that the claim holds for m\u2212 1. We then have\nRk(N1) = E [ sup f\u2208Nm 1 k k\u2211 i=1 \u03b5if(x \u2032 i) ] ,\nwhere \u03b51, . . . , \u03b5n are Rademacher variables. By the definition of Nm, we may write the expression as\nRk(N1) = E  sup f1,...,fd\u2208Nm\u22121 1 k n\u2211 i=1 \u03b5i d\u2211 j=1 wj\u03c3(fj(x \u2032 i))  = E  sup f1,...,fd\u2208Nm\u22121 1 k d\u2211 j=1 wj k\u2211 i=1 \u03b5i\u03c3(fj(x \u2032 i))  \u2264 BE [ sup\nf\u2208Nm\u22121\n1\nk k\u2211 i=1 \u03b5i\u03c3(f(x \u2032 i)) ] = BRk(\u03c3 \u25e6 Nm\u22121),\nwhere the inequality follows since \u2016w\u20161 \u2264 B. Since the function \u03c3 is 1-Lipschitz continuous, following the proof of inequality (17), we have\nRk(\u03c3 \u25e6 Nm\u22121) \u2264 Rk(Nm\u22121) \u2264 \u221a q\nn Bm,\nwhich completes the proof.\nE.2 Proof of Lemma 7\nWe prove the claim by induction on the number of layers m. If m = 1, then f\u2217 is a linear function and \u03d5(f\u2217) \u2208 [\u2212B1, B1]n. Since \u03d5(g) minimizes the `2-distance to vector u, we have\n\u2016\u03d5(g)\u2212 \u03d5(f\u2217)\u20162 \u2264 \u2016\u03d5(g)\u2212 u\u20162 + \u2016\u03d5(f\u2217)\u2212 u\u20162 \u2264 2\u2016\u03d5(f\u2217)\u2212 u\u20162. (35)\nSince u is drawn uniformly from [\u2212B,B]k, with probability at least ( 4) k we have \u2016\u03d5(f\u2217)\u2212 u\u2016\u221e \u2264 B2 , and consequently\n\u2016\u03d5(g)\u2212 \u03d5(f\u2217)\u20162 \u2264 \u221a k\u2016\u03d5(g)\u2212 \u03d5(f\u2217)\u2016\u221e \u2264 \u221a kB,\nwhich establishes the claim. For m > 1, assume that the claim holds for m \u2212 1. Recall that f\u2217/B is in the convex hull of \u03c3 \u25e6 Nm\u22121 and every function f \u2208 \u03c3 \u25e6 Nm\u22121 satisfies \u2016\u03d5(f)\u20162 \u2264 \u221a k. By the Maurey-Barron-Jones lemma (Lemma 3), there exist s functions in Nm\u22121, say f\u03031, . . . , f\u0303s, and a vector w \u2208 Rs satisfying \u2016w\u20161 \u2264 B such that \u2225\u2225\u2225 s\u2211\nj=1\nwj\u03c3(\u03d5(f\u0303j))\u2212 \u03d5(f\u2217) \u2225\u2225\u2225 2 \u2264 B \u221a k s .\nLet \u03d5(f\u0303) := \u2211s j=1wj\u03c3(\u03d5(f\u0303j)). If we chose s = \u2308 1 2 \u2309 , then we have\n\u2016\u03d5(f\u0303)\u2212 \u03d5(f\u2217)\u20162 \u2264 \u221a kB. (36)\nRecall that the function g satisfies g = \u2211s\nj=1 vj\u03c3 \u25e6 gj for g1, . . . , gs \u2208 Nm\u22121. Using the inductive hypothesis, we know that the following bound holds with probability at least psm\u22121:\n\u2016\u03c3(\u03d5(gj))\u2212 \u03c3(\u03d5(f\u0303j))\u20162 \u2264 \u2016\u03d5(gj)\u2212 \u03d5(f\u0303j)\u20162 \u2264 (2m\u2212 3) \u221a kBm\u22121 for any j \u2208 [s].\nAs a consequence, we have\u2225\u2225\u2225 s\u2211 j=1 wj\u03c3(\u03d5(gj))\u2212 s\u2211 j=1 wj\u03c3(\u03d5(f\u0303j)) \u2225\u2225\u2225 2 \u2264 s\u2211 j=1 |wj | \u00b7 \u2016\u03c3(\u03d5(gj))\u2212 \u03c3(\u03d5(f\u0303j))\u20162\n\u2264 \u2016w\u20161 \u00b7max j\u2208[s] {\u2016\u03c3(\u03d5(gj))\u2212 \u03c3(\u03d5(f\u0303j))\u20162} \u2264 (2m\u2212 3)\n\u221a k Bm. (37)\nFinally, we bound the distance between \u2211s\nj=1wj\u03c3(\u03d5(gj)) and \u03d5(g). Following the proof of inequality (35), we obtain \u2225\u2225\u2225\u03d5(g)\u2212 s\u2211\nj=1\nwj\u03c3(\u03d5(gj)) \u2225\u2225\u2225 2 \u2264 2 \u2225\u2225\u2225u\u2212 s\u2211\nj=1\nwj\u03c3(\u03d5(gj)) \u2225\u2225\u2225 2 .\nNote that \u2211s\nj=1wj\u03c3(\u03d5(gj)) \u2208 [\u2212B,B]k and u is uniformly drawn from [\u2212B,B]k. Thus, with probability at least ( 4)\nk, we have\u2225\u2225\u2225\u03d5(g)\u2212 s\u2211 j=1 wj\u03c3(\u03d5(gj)) \u2225\u2225\u2225 2 \u2264 \u221a kB. (38)\nCombining inequalities (36), (37) and (38) and using the fact that B \u2265 1, we have\u2225\u2225\u2225\u03d5(g)\u2212 \u03d5(f\u2217)\u2225\u2225\u2225 \u221e \u2264 (2m\u2212 1) \u221a kBm,\nwith probability at least\npsm\u22121 \u00b7 (\n4\n)k = (\n4\n)k( s(sm\u22121\u22121) s\u22121 +1 ) = (\n4\n)k(sm\u22121)/(s\u22121) = pm,\nwhich completes the induction."}, {"heading": "F Proof of Theorem 4", "text": "F.1 Proof of part (a) We first prove f\u0302 \u2208 Nm. Notice that f\u0302 = \u2211T\nt=1 B 2bT log(1\u2212\u00b5t1+\u00b5t )\u2206\u0302t. Thus, if\nT\u2211 t=1 B 2bT \u2223\u2223\u2223\u2223log(1\u2212 \u00b5t1 + \u00b5t ) \u2223\u2223\u2223\u2223 \u2264 B, (39)\nthen we have f\u0302 \u2208 Nm by the definition of Nm. The definition of bT makes sure that inequality (39) holds. Thus, we have proved the claim. The time complexity is obtained by plugging in the bound from Theorem 3.\nIt remains to establish the correctness of f\u0302 . We may write any function f \u2208 Nm as\nf(x) = d\u2211 j=1 wj\u03c3(fj(x)) where wj \u2265 0 for all j \u2208 [d].\nThe constraints wj \u2265 0 are always satisfiable, otherwise since \u03c3 is an odd function we may write wj\u03c3(fj(x)) as (\u2212wj)\u03c3(\u2212fj(x)) so that it satisfies the constraint. The function fj or \u2212fj belongs to the class Nm\u22121. We use the following result by Shalev-Shwartz and Singer (2010): Assume that there exists f\u2217 \u2208 Nm which separate the data with margin \u03b3. Then for any set of non-negative importance weights {\u03b1i}ni=1, there is a function f \u2208 Nm\u22121 such that \u2211n i=1 \u03b1i\u03c3(\u2212yif(xi)) \u2264 \u2212 \u03b3 B . This implies that, for every t \u2208 [T ], there is f \u2208 Nm\u22121 such that\nGt(f) = n\u2211 i=1 \u03b1t,i\u03c3(\u2212yif(xi)) \u2264 \u2212 \u03b3 B .\nHence, with probability at least 1\u2212 \u03b4, the sequence \u00b51, . . . , \u00b5T satisfies the relation\n\u00b5t = Gt(g\u0302t) \u2264 \u2212 \u03b3\n2B for every t \u2208 [T ]. (40)\nAlgorithm 4 is based on running AdaBoost for T iterations. The analysis of AdaBoost by Schapire and Singer (1999) guarantees that for any \u03b2 > 0, we have\n1\nn n\u2211 i=1 e\u2212\u03b2I[\u2212yifT (xi) \u2265 \u2212\u03b2] \u2264 1 n n\u2211 i=1 e\u2212yifT (xi) \u2264 exp ( \u2212 \u2211T t=1 \u00b5 2 t 2 ) .\nThus, the fraction of data that cannot be separated by fT with margin \u03b2 is bounded by exp(\u03b2 \u2212\u2211T t=1 \u00b5 2 t\n8B2 ). If we choose\n\u03b2 :=\n\u2211T t=1 \u00b5 2 t\n2 \u2212 log(n+ 1),\nthen this fraction is bounded by 1n+1 , meaning that all points are separated by margin \u03b2. Recall that f\u0302 is a scaled version of fT . As a consequence, all points are separated by f\u0302 with margin\nB\u03b2\nbT =\n\u2211T t=1 \u00b5 2 t \u2212 2 log(n+ 1)\n1 B \u2211T t=1 log( 1\u2212\u00b5t 1+\u00b5t ) .\nSince \u00b5t \u2265 \u22121/2, it is easy to verify that log(1\u2212\u00b5t1+\u00b5t ) \u2264 4|\u00b5t|. Using this fact and Jensen\u2019s inequality, we have\nB\u03b2\nbT \u2265\n( \u2211T\nt=1 |\u00b5t|)2/T \u2212 2 log(n+ 1) 4 B \u2211T t=1 |\u00b5t| .\nThe right-hand side is a monotonically increasing function of \u2211T\nt=1 |\u00b5t|. Plugging in the bound in (40), we find that\nB\u03b2 bT \u2265 \u03b3 2T/(4B2)\u2212 2 log(n+ 1) 2\u03b3T/B2 .\nPlugging in T = 16B 2 log(n+1) \u03b32\n, some algebra shows that the right-hand side is equal to \u03b3/16 which completes the proof.\nF.2 Proof of part (b) Consider the empirical loss function `(f) := 1n \u2211n\ni=1 h(\u2212yif(xi)), where h(t) := max{0, 1 + 16t/\u03b3}. Part (a) implies that `(f\u0302) = 0 with probability at least 1\u2212 \u03b4. Note that h is (16/\u03b3)-Lipschitz continuous; the Rademacher complexity of Nm with respect to n i.i.d. samples is bounded by \u221a q/nBm (see Lemma 6). By the classical Rademacher generalization bound (Bartlett and Mendelson, 2003, Theorem 8 and Theorem 12), if (x, y) is randomly sampled form P, then we have\nE[h(\u2212yf\u0302(x))] \u2264 `(f\u0302) + 32B m \u03b3 \u00b7 \u221a q n + \u221a 8 log(2/\u03b4) n with probabality at least 1\u2212 \u03b4.\nThus, in order to bound the generalization loss by with probability 1 \u2212 2\u03b4, it suffices to choose n = poly(1/ , log(1/\u03b4)). Since h(t) is an upper bound on the zero-one loss I[t \u2265 0], we obtain the claimed bound."}, {"heading": "G Proof of Corollary 1", "text": "The first step is to use the improper learning algorithm (Zhang et al., 2015, Algorithm 1) to learn a predictor g\u0302 that minimizes the following risk function:\n`(g) := E[\u03c6(\u2212y\u0303g(x))] where \u03c6(t) :=\n{ \u2212 2\u03b71\u22122\u03b7 + \u03b7(t+\u03b3) (1\u2212\u03b7)(1\u22122\u03b7)\u03b3 if t \u2264 \u2212\u03b3,\n\u2212 2\u03b71\u22122\u03b7 + t+\u03b3 (1\u22122\u03b7)\u03b3 if t > \u2212\u03b3.\nSince \u03b7 < 1/2, the function \u03c6 is convex and Lipschitz continuous. The activation function erf(x) satisfies the condition of (Zhang et al., 2015, Theorem 1). Thus, with sample complexity poly(1/\u03c4, log(1/\u03b4)) and time complexity poly(d, 1/\u03c4, log(1/\u03b4)), the resulting predictor g\u0302 satisfies\n`(g\u0302) \u2264 `(f\u2217) + \u03c4 with probability at least 1\u2212 \u03b4/3.\nBy the definition of y\u0303 and \u03c6, it is straightforward to verify that\n`(g) = E[(1\u2212 \u03b7)\u03c6(\u2212yg(x)) + \u03b7\u03c6(yg(x))] = E[\u03c8(\u2212yg(x))] (41)\nwhere\n\u03c8(t) :=  0 if t < \u2212\u03b3, 1 + t/\u03b3 if \u2212\u03b3 \u2264 t \u2264 \u03b3, 2 + 2\u03b7\n2\u22122\u03b7+1 (1\u2212\u03b7)(1\u22122\u03b7)\u03b3 (t\u2212 \u03b3) if t > \u03b3.\nRecall that yf\u2217(x) \u2265 \u03b3 almost surely. From the definition of \u03c8, we have `(f\u2217) = 0, so that `(g\u0302) \u2264 `(f\u2217) + \u03c4 implies `(g\u0302) \u2264 \u03c4 . Also note that \u03c8(t) upper bounds the indicator I[t \u2265 0], so that the right-hand side of equation (41) provides an upper bound on the probability P(sign(g(x)) 6= y). Consequently, defining the classifier h\u0302(x) := sign(g(x)), then we have\nP(h\u0302(x) 6= y) \u2264 `(g\u0302) \u2264 \u03c4 with probability at least 1\u2212 \u03b4/3.\nGiven the classifier h\u0302, we draw another random dataset of n points taking the form {(xi, yi)}ni=1. If \u03c4 = \u03b43n , then this dataset is equal to {(xi, h\u0302(xi))} n i=1 with probability at least 1\u2212 2\u03b4/3. Let the BoostNet algorithm take {(xi, h\u0302(xi))}ni=1 as its input. With sample size n = poly(1/ , log(1/\u03b4)), Theorem 4 implies that the algorithm learns a neural network f\u0302 such that P(sign(f\u0302(x)) 6= y) \u2264 with probability at least 1\u2212\u03b4. Plugging in the assignments of n and \u03c4 , the overall sample complexity is poly(1/ , 1/\u03b4) and the overall computation complexity is poly(d, 1/ , 1/\u03b4)."}, {"heading": "H Proof of Proposition 2", "text": "We reduce the PAC learning of intersection of T halfspaces to the problem of learning a neural network. Assume that T = \u0398(d\u03c1) for some \u03c1 > 0. We claim that for any number of pairs taking the form (x, h\u2217(x)), there is a neural network f\u2217 \u2208 N2 that separates all pairs with margin \u03b3, and moreover that the margin is bounded as \u03b3 = 1/poly(d).\nTo prove the claim, recall that h\u2217(x) = 1 if and only if h1(x) = \u00b7 \u00b7 \u00b7 = hT (x) = 1 for some h1, . . . , hT \u2208 H. For any ht, the definition of H implies that there is a (wt, bt) pair such that if ht(x) = 1 then w T t x \u2212 bt \u2212 1/2 \u2265 1/2, otherwise wTt x \u2212 bt \u2212 1/2 \u2264 \u22121/2. We consider the two possible choices of the activation function:\n\u2022 Piecewise linear function: If \u03c3(x) := min{1,max{\u22121, x}}, then let\ngt(x) := \u03c3(c(w T t x\u2212 bt \u2212 1/2) + 1),\nfor some quantity c > 0. The term inside the activation function can be written as \u3008w\u0303, x\u2032\u3009 where\nw\u0303 = (c \u221a 2d+ 2wt,\u2212c \u221a 2d+ 2(bt + 1/2), \u221a 2) and x\u2032 = ( x\u221a\n2d+ 2 , 1\u221a 2d+ 2 , 1\u221a 2 ).\nNote that \u2016x\u2032\u20162 \u2264 1, and with a sufficiently small constant c = 1/poly(d) we have \u2016w\u0303\u20162 \u2264 2. Thus, gt(x) is the output of a one-layer neural network. If ht(x) = 1, then gt(x) = 1, otherwise gt(x) \u2264 1\u2212 c/2. Now consider the two-layer neural network f(x) := c/4\u2212 T + \u2211T t=1 gt(x). If h\u2217(x) = 1, then we have gt(x) = 1 for every t \u2208 [T ] which implies f(x) = c/4. If h\u2217(x) = \u22121, then we have gt(x) \u2264 1 \u2212 c/2 for at least one t \u2208 [T ] which implies f(x) \u2264 \u2212c/4. Thus, the neural network f separates the data with margin c/4. We normalize the edge weights on the second layer to make f belong to N2. After normalization, the network still has margin 1/poly(d).\n\u2022 ReLU function: if \u03c3(x) := max{0, x}, then let gt(x) := \u03c3(\u2212c(wTt x \u2212 bt \u2212 1/2)) for some quantity c > 0. We may write the term inside the activation function as \u3008w\u0303, x\u2032\u3009 where w\u0303 = (\u2212c \u221a d+ 1wt, c \u221a d+ 1(bt + 1/2)) and x \u2032 = (x, 1)/ \u221a d+ 1. It is straightforward to verify\nthat \u2016x\u2032\u20162 \u2264 1, and with a sufficiently small c = 1/poly(d) we have \u2016w\u0303\u20162 \u2264 2. Thus, gt(x) is the output of a one-layer neural network. If ht(x) = 1, then gt(x) = 0, otherwise gt(x) \u2265 c/2. Let f(x) := c/4 \u2212 \u2211T t=1 gt(x), then this two-layer neural network separates the data with margin c/4. After normalization the network belongs to N2 and it still separates the data with margin 1/poly(d).\nTo learn the intersection of T halfspaces, we learn a neural network based on n i.i.d. points taking the form (x, h\u2217(x)). Assume that the neural network is efficiently learnable. Since there exists f\u2217 \u2208 Nm which separates the data with margin \u03b3 = 1/poly(d), we can learn a network f\u0302 in poly(d, 1/ , 1/\u03b4) sample complexity and time complexity, and satisfies P(sign(f\u0302(x)) 6= h\u2217(x)) \u2264 with probability 1\u2212 \u03b4. It contradicts with the assumption that the intersection of T halfspaces is not efficiently learnable."}], "references": [{"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M.F. Balcan", "P.M. Long"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Efficient learning of linear separators under bounded noise", "author": ["P. Awasthi", "M.-F. Balcan", "N. Haghtalab", "R. Urner"], "venue": null, "citeRegEx": "Awasthi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2015}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Barron.,? \\Q1993\\E", "shortCiteRegEx": "Barron.", "year": 1993}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bartlett.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett.", "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2003}, {"title": "Efficient learning of linear perceptrons", "author": ["S. Ben-David", "H.U. Simon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ben.David and Simon.,? \\Q2001\\E", "shortCiteRegEx": "Ben.David and Simon.", "year": 2001}, {"title": "Learning halfspaces with the zero-one loss: time-accuracy tradeoffs", "author": ["A. Birnbaum", "S.S. Shwartz"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Birnbaum and Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Birnbaum and Shwartz.", "year": 2012}, {"title": "Training a 3-node neural network is NP-complete", "author": ["A. Blum", "R.L. Rivest"], "venue": "Neural Networks,", "citeRegEx": "Blum and Rivest.,? \\Q1992\\E", "shortCiteRegEx": "Blum and Rivest.", "year": 1992}, {"title": "A polynomial-time algorithm for learning noisy linear threshold functions", "author": ["A. Blum", "A. Frieze", "R. Kannan", "S. Vempala"], "venue": null, "citeRegEx": "Blum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1998}, {"title": "Noise-tolerant learning, the parity problem, and the statistical query model", "author": ["A. Blum", "A. Kalai", "H. Wasserman"], "venue": "Journal of the ACM,", "citeRegEx": "Blum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2003}, {"title": "From average case complexity to improper learning complexity", "author": ["A. Daniely", "N. Linial", "S. Shalev-Shwartz"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Daniely et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2014}, {"title": "An elementary proof of the Johnson-Lindenstrauss lemma", "author": ["S. Dasgupta", "A. Gupta"], "venue": "International Computer Science Institute,", "citeRegEx": "Dasgupta and Gupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Hardness of learning halfspaces with noise", "author": ["V. Guruswami", "P. Raghavendra"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Guruswami and Raghavendra.,? \\Q2009\\E", "shortCiteRegEx": "Guruswami and Raghavendra.", "year": 2009}, {"title": "Training a sigmoidal node is hard", "author": ["D.R. Hush"], "venue": "Neural Computation,", "citeRegEx": "Hush.,? \\Q1999\\E", "shortCiteRegEx": "Hush.", "year": 1999}, {"title": "Generalization bounds for neural networks through tensor factorization", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Lecture note: Rademacher composition and linear prediction", "author": ["S. Kakade", "A. Tewari"], "venue": null, "citeRegEx": "Kakade and Tewari.,? \\Q2008\\E", "shortCiteRegEx": "Kakade and Tewari.", "year": 2008}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2009}, {"title": "Agnostically learning halfspaces", "author": ["A.T. Kalai", "A.R. Klivans", "Y. Mansour", "R.A. Servedio"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Kalai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2008}, {"title": "Embedding hard learning problems into gaussian space. Approximation, Randomization, and Combinatorial Optimization", "author": ["A. Klivans", "P. Kothari"], "venue": "Algorithms and Techniques,", "citeRegEx": "Klivans and Kothari.,? \\Q2014\\E", "shortCiteRegEx": "Klivans and Kothari.", "year": 2014}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": ["A.R. Klivans", "A. Sherstov"], "venue": "In 47th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Klivans and Sherstov,? \\Q2006\\E", "shortCiteRegEx": "Klivans and Sherstov", "year": 2006}, {"title": "Learning halfspaces with malicious noise", "author": ["A.R. Klivans", "P.M. Long", "R.A. Servedio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Klivans et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Klivans et al\\.", "year": 2009}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics,", "citeRegEx": "Koltchinskii and Panchenko.,? \\Q2002\\E", "shortCiteRegEx": "Koltchinskii and Panchenko.", "year": 2002}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["M. Ledoux", "M. Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ledoux and Talagrand.,? \\Q2013\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 2013}, {"title": "Norm-based capacity control in neural networks", "author": ["B. Neyshabur", "R. Tomioka", "N. Srebro"], "venue": null, "citeRegEx": "Neyshabur et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 1993}, {"title": "On the equivalence of weak learnability and linear separability", "author": ["versity", "MA Cambridge", "2001. S. Shalev-Shwartz", "Y. Singer"], "venue": null, "citeRegEx": "versity et al\\.,? \\Q2001\\E", "shortCiteRegEx": "versity et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": "The learning of a halfspace is the core problem solved by many machine learning methods, including the Perceptron (Rosenblatt, 1958), the Support Vector Machine (Vapnik, 1998) and AdaBoost (Freund and Schapire, 1997).", "startOffset": 189, "endOffset": 216}, {"referenceID": 14, "context": "In particular, Guruswami and Raghavendra (2009) show that, for any \u2208 (0, 1/2], given a set of", "startOffset": 15, "endOffset": 48}, {"referenceID": 14, "context": "In fact, the result of Guruswami and Raghavendra (2009) shows that the approximation ratio of such procedures could be arbitrarily large.", "startOffset": 23, "endOffset": 56}, {"referenceID": 13, "context": "This so-called BoostNet algorithm uses the AdaBoost approach (Freund and Schapire, 1997) to construct a m-layer neural network by taking an (m \u2212 1)-layer network as a weak classifier.", "startOffset": 61, "endOffset": 88}, {"referenceID": 14, "context": "It is known that for any constant approximation ratio, the problem of approximately minimizing the zero-one loss is computationally hard (Guruswami and Raghavendra, 2009; Daniely et al., 2014).", "startOffset": 137, "endOffset": 192}, {"referenceID": 11, "context": "It is known that for any constant approximation ratio, the problem of approximately minimizing the zero-one loss is computationally hard (Guruswami and Raghavendra, 2009; Daniely et al., 2014).", "startOffset": 137, "endOffset": 192}, {"referenceID": 19, "context": "When the label noise is adversarial, the halfspace can be learned if the data distribution is isotropic log-concave and the fraction of labels being corrupted is bounded by a small quantity (Kalai et al., 2008; Klivans et al., 2009; Awasthi et al., 2014).", "startOffset": 190, "endOffset": 254}, {"referenceID": 22, "context": "When the label noise is adversarial, the halfspace can be learned if the data distribution is isotropic log-concave and the fraction of labels being corrupted is bounded by a small quantity (Kalai et al., 2008; Klivans et al., 2009; Awasthi et al., 2014).", "startOffset": 190, "endOffset": 254}, {"referenceID": 1, "context": "When the label noise is adversarial, the halfspace can be learned if the data distribution is isotropic log-concave and the fraction of labels being corrupted is bounded by a small quantity (Kalai et al., 2008; Klivans et al., 2009; Awasthi et al., 2014).", "startOffset": 190, "endOffset": 254}, {"referenceID": 7, "context": "Indeed, Blum et al. (1998) and Servedio and Valiant (2001) show that if the labels are corrupted by random noise, then the halfspace can be learned in polynomial time.", "startOffset": 8, "endOffset": 27}, {"referenceID": 7, "context": "Indeed, Blum et al. (1998) and Servedio and Valiant (2001) show that if the labels are corrupted by random noise, then the halfspace can be learned in polynomial time.", "startOffset": 8, "endOffset": 59}, {"referenceID": 1, "context": "The same conclusion was established by Awasthi et al. (2015) when the labels are corrupted by Massart noise, and the covariates are drawn from the uniform distribution on a unit sphere.", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": "on the noise, Kalai et al. (2008) show that if the data are drawn from the uniform distribution on a unit sphere, then there is an algorithm whose time complexity is polynomial in the input dimension, but exponential in 1/ (where is the additive error).", "startOffset": 14, "endOffset": 34}, {"referenceID": 17, "context": "on the noise, Kalai et al. (2008) show that if the data are drawn from the uniform distribution on a unit sphere, then there is an algorithm whose time complexity is polynomial in the input dimension, but exponential in 1/ (where is the additive error). In this same setting, Klivans and Kothari (2014) prove that the exponential dependence on 1/ is unavoidable.", "startOffset": 14, "endOffset": 303}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss.", "startOffset": 0, "endOffset": 27}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss. The \u03bc-margin loss asserts that all points whose classification margins are smaller than \u03bc should be marked as misclassified. Under this metric, it was shown by Ben-David and Simon (2001); Birnbaum and Shwartz (2012) that the optimal \u03bc-margin loss can be achieved in polynomial time if \u03bc is a positive constant.", "startOffset": 0, "endOffset": 305}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss. The \u03bc-margin loss asserts that all points whose classification margins are smaller than \u03bc should be marked as misclassified. Under this metric, it was shown by Ben-David and Simon (2001); Birnbaum and Shwartz (2012) that the optimal \u03bc-margin loss can be achieved in polynomial time if \u03bc is a positive constant.", "startOffset": 0, "endOffset": 334}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss. The \u03bc-margin loss asserts that all points whose classification margins are smaller than \u03bc should be marked as misclassified. Under this metric, it was shown by Ben-David and Simon (2001); Birnbaum and Shwartz (2012) that the optimal \u03bc-margin loss can be achieved in polynomial time if \u03bc is a positive constant. Shalev-Shwartz et al. (2011) study the minimization of a continuous approximation to the zero-one loss, which is similar to our setup.", "startOffset": 0, "endOffset": 458}, {"referenceID": 3, "context": "2 Learning neural networks It is known that any smooth function can be approximated by a neural network with just one hidden layer (Barron, 1993), but that training such a network is NP-hard (Blum and Rivest, 1992).", "startOffset": 131, "endOffset": 145}, {"referenceID": 8, "context": "2 Learning neural networks It is known that any smooth function can be approximated by a neural network with just one hidden layer (Barron, 1993), but that training such a network is NP-hard (Blum and Rivest, 1992).", "startOffset": 191, "endOffset": 214}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known.", "startOffset": 0, "endOffset": 608}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014).", "startOffset": 0, "endOffset": 939}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014). The assumption in this case is that the network weights satisfy a non-degeneracy condition; however, the algorithm is only capable of learning neural networks with one hidden layer.", "startOffset": 0, "endOffset": 1057}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014). The assumption in this case is that the network weights satisfy a non-degeneracy condition; however, the algorithm is only capable of learning neural networks with one hidden layer. Our algorithm does not impose any assumption on the data distribution, and is able to learn multilayer neural networks. Another approach to the problem is via the improper learning framework. The goal in this case is to find a predictor that is not a neural network, but performs as well as the best possible neural network in terms of the generalization error. Livni et al. (2014) propose a polynomial-time algorithm to learn networks whose activation function is quadratic.", "startOffset": 0, "endOffset": 1622}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014). The assumption in this case is that the network weights satisfy a non-degeneracy condition; however, the algorithm is only capable of learning neural networks with one hidden layer. Our algorithm does not impose any assumption on the data distribution, and is able to learn multilayer neural networks. Another approach to the problem is via the improper learning framework. The goal in this case is to find a predictor that is not a neural network, but performs as well as the best possible neural network in terms of the generalization error. Livni et al. (2014) propose a polynomial-time algorithm to learn networks whose activation function is quadratic. Zhang et al. (2015) propose an algorithm for improper learning of sigmoidal neural networks.", "startOffset": 0, "endOffset": 1736}, {"referenceID": 24, "context": "This lemma is based on a slight sharpening of the usual Ledoux-Talagrand contraction for Rademacher variables (Ledoux and Talagrand, 2013); see Appendix A for the proof.", "startOffset": 110, "endOffset": 138}, {"referenceID": 12, "context": "See the paper by Dasgupta and Gupta (1999) for a simple proof.", "startOffset": 17, "endOffset": 43}, {"referenceID": 3, "context": "Maurey-Barron-Jones lemma: Letting G be a subset of any Hilbert space H, the MaureyBarron-Jones lemma guarantees that any point v in the convex hull of G can be approximated by a convex combination of a small number of points of G. More precisely, we have: Lemma 3. Consider any subset G of any Hilbert space such that \u2016g\u2016H \u2264 b for all g \u2208 G. Then for any point v is in the convex hull of G, there is a point vs in the convex hull of s points of G such that \u2016v \u2212 vs\u2016H \u2264 b2/s. See the paper by Pisier (1980) for a proof.", "startOffset": 7, "endOffset": 507}, {"referenceID": 15, "context": "We note that Hush (1999) proved a similar hardness result, but without the unit-norm constraint on w\u2217 and {xi}i=1.", "startOffset": 13, "endOffset": 25}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al.", "startOffset": 47, "endOffset": 63}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al.", "startOffset": 47, "endOffset": 98}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al.", "startOffset": 47, "endOffset": 129}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al. (2015). Assuming a constant `1-norm bound might be restrictive for some applications, but without this norm constraint, the neural network class activated by any sigmoid-like or ReLU-like function is not efficiently learnable (Zhang et al.", "startOffset": 47, "endOffset": 154}, {"referenceID": 13, "context": "It uses the AdaBoost approach (Freund and Schapire, 1997) to construct the network, and we refer to it as the BoostNet algorithm.", "startOffset": 30, "endOffset": 57}, {"referenceID": 22, "context": "Our proof relies on the hardness of standard (nonagnostic) PAC learning of the intersection of halfspaces given in Klivans et al. (2006). More precisely, consider the family of halfspace indicator functions mapping X = {\u22121, 1}d to {\u22121, 1} given by H = {x\u2192 sign(wx\u2212 b\u2212 1/2) : x \u2208 {\u22121, 1}, b \u2208 N, w \u2208 N, |b|+ \u2016w\u20161 \u2264 poly(d)}.", "startOffset": 115, "endOffset": 137}, {"referenceID": 22, "context": "Klivans et al. (2006) show that if T = \u0398(d\u03c1) then HT is not efficiently learnable under a certain cryptographic assumption.", "startOffset": 0, "endOffset": 22}], "year": 2015, "abstractText": "We study non-convex empirical risk minimization for learning halfspaces and neural networks. For loss functions that are L-Lipschitz continuous, we present algorithms to learn halfspaces and multi-layer neural networks that achieve arbitrarily small excess risk > 0. The time complexity is polynomial in the input dimension d and the sample size n, but exponential in the quantity (L/ ) log(L/ ). These algorithms run multiple rounds of random initialization followed by arbitrary optimization steps. We further show that if the data is separable by some neural network with constant margin \u03b3 > 0, then there is a polynomial-time algorithm for learning a neural network that separates the training data with margin \u03a9(\u03b3). As a consequence, the algorithm achieves arbitrary generalization error > 0 with poly(d, 1/ ) sample and time complexity. We establish the same learnability result when the labels are randomly flipped with probability \u03b7 < 1/2.", "creator": "LaTeX with hyperref package"}}}