{"id": "1511.08062", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Relaxed Majorization-Minimization for Non-Smooth and Non-Convex Optimization", "abstract": "we propose a new majorization - constraints ( mm ) method for non - smooth smooth non - convex programs, which is general enough to fulfill the existing mg methods. besides the local majorization condition, we only require that spatial difference between the directional product of only objective function but its surrogate function vanishes when the cycle of iterations approaches infinity, which is a very weak condition. so our method extends use every surrogate function that instead approximates the non - smooth objective function. in comparison, all the existing mm algorithm construct any surrogate function by approximating the smooth component of the objective parameters. we apply our relaxed mm methods to very robust matrix factorization ( rmf ) systems with different regularizations, where our locally majorant algorithm shows advantages with the state - of - the - art approaches for rmf. this is the first algorithm for rmf ensuring, without extra assumptions, that any limit point of the iterates is locally stationary point.", "histories": [["v1", "Wed, 25 Nov 2015 13:55:57 GMT  (939kb,D)", "http://arxiv.org/abs/1511.08062v1", "AAAI16"]], "COMMENTS": "AAAI16", "reviews": [], "SUBJECTS": "math.OC cs.LG cs.NA", "authors": ["chen xu", "zhouchen lin", "zhenyu zhao", "hongbin zha"], "accepted": true, "id": "1511.08062"}, "pdf": {"name": "1511.08062.pdf", "metadata": {"source": "CRF", "title": "Relaxed Majorization-Minimization for Non-smooth and Non-convex Optimization", "authors": ["Chen Xu", "Zhouchen Lin", "Zhenyu Zhao", "Hongbin Zha"], "emails": ["xuen@pku.edu.cn,", "zlin@pku.edu.cn,", "dwightzzy@gmail.com,", "zha@cis.pku.edu.cn"], "sections": [{"heading": "Introduction", "text": "Consider the following optimization problem:\nmin x\u2208C f(x), (1)\nwhere C is a closed convex subset in Rn and f(x) : Rn \u2192 R is a continuous function bounded below, which could be non-smooth and non-convex. Often, f(x) can be split as:\nf(x) = f\u0303(x) + f\u0302(x), (2)\nwhere f\u0303(x) is differentiable and f\u0302(x) is non-smooth1. Such an optimization problem is ubiquitous, e.g., in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho, and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al., 2014; Kong, Ding, and Huang, 2011). There have been a variety of methods to tackle problem (1). Typical methods include subdifferential (Clarke, 1990), bundle methods (Ma\u0308kela\u0308, 2002), gradient sampling (Burke, Lewis, and Overton,\n\u2217Corresponding author. Copyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1f\u0303(x) and f\u0302(x) may vanish.\nAlgorithm 1 Sketch of MM"}, {"heading": "Input: x0 \u2208 C.", "text": "1: while not converged do 2: Construct a surrogate function gk(x) of f(x) at the current iterate xk. 3: Minimize the surrogate to get the next iterate: xk+1 = argminx\u2208C gk(x). 4: k \u2190 k + 1. 5: end while\nOutput: The solution xk.\n2005), smoothing methods (Chen, 2012), and majorizationminimization (MM) (Hunter and Lange, 2004). In this paper, we focus on the MM methods."}, {"heading": "Existing MM for Non-smooth and Non-convex Optimization", "text": "MM has been successfully applied to a wide range of problems. Mairal (2013) has given a comprehensive review on MM. Conceptually, the MM methods consist of two steps (see Algorithm 1). First, construct a surrogate function gk(x) of f(x) at the current iterate xk. Second, minimize the surrogate gk(x) to update x. The choice of surrogate is crit-\nar X\niv :1\n51 1.\n08 06\n2v 1\n[ m\nat h.\nO C\n] 2\n5 N\nov 2\n01 5\nical for the efficiency of solving (1) and also the quality of solution. The most popular choice of surrogate is the class of \u201cfirst order surrogates\u201d, whose difference from the objective function is differentiable with a Lipschitz continuous gradient (Mairal, 2013). For non-smooth and non-convex objectives, to the best of our knowledge, \u201cfirst order surrogates\u201d are only used to approximate the differentiable part f\u0303(x) of the objective f(x) in (2). More precisely, denoting g\u0303k(x) as an approximation of f\u0303(x) at iteration k, the surrogate is\ngk(x) = g\u0303k(x) + f\u0302(x). (3)\nSuch a split approximation scheme has been successfully applied, e.g., to minimizing the difference of convex functions (Candes, Wakin, and Boyd, 2008) and in the proximal splitting algorithm (Attouch, Bolte, and Svaiter, 2013). In parallel to (Mairal, 2013), Razaviyayn, Hong, and Luo (2013) also showed that many popular methods for minimizing non-smooth functions could be regarded as MM methods. They proposed the block coordinate descent method, where the traditional MM could be regarded as a special case by gathering all variables in one block. Different from (Mairal, 2013), they suggested using the directional derivative to ensure the first order smoothness between the objective and the surrogate, which is weaker than the condition in (Mairal, 2013) that the difference between the objective and the surrogate should be smooth. However, Razaviyayn, Hong, and Luo (2013) only discussed the choice of surrogates by approximating f(x) as (3)."}, {"heading": "Contributions", "text": "The contributions of this paper are as follows: (a) We further relax the condition on the difference between\nthe objective and the surrogate. We only require that the directional derivative of the difference vanishes when the number of iterations approaches infinity (see (8)). Our even weaker condition ensures that the non-smooth and non-convex objective can be approximated directly. Our relaxed MM is general enough to include the existing MM methods (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013).\n(b) We also propose the conditions ensuring that the iterates produced by our relaxed MM converge to stationary points2, even for general non-smooth and non-convex objectives.\n(c) As a concrete example, we apply our relaxed MM to the robust matrix factorization (RMF) problem with different regularizations. Experimental results testify to the robustness and effectiveness of our locally majorant algorithm over the state-of-the-art algorithms for RMF. To the best of our knowledge, this is the first work that ensures convergence to stationary points without extra assumptions, as the objective and the constructed surrogate naturally fulfills the convergence conditions of our relaxed MM.\n2For convenience, we say that a sequence converges to stationary points meaning that any limit point of the sequence is a stationary point.\nTable 1 summarizes the differences between our relaxed MM and the existing MM. We can see that ours is general enough to include existing works (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013). In addition, it requires less smoothness on the difference between the objective and the surrogate and has higher approximation ability and better convergence property."}, {"heading": "Our Relaxed MM", "text": "Before introducing our relaxed MM, we recall some definitions that will be used later.\nDefinition 1. (Sufficient Descent) {f(xk)} is said to have sufficient descent on the sequence {xk} if there exists a constant \u03b1 > 0 such that:\nf(xk)\u2212 f(xk+1) \u2265 \u03b1\u2016xk \u2212 xk+1\u20162, \u2200k. (4)\nDefinition 2. (Directional Derivative (Borwein and Lewis, 2010, Chapter 6.1)) The directional derivative of function f(x) in the feasible direction d (x + d \u2208 C) is defined as:\n\u2207f(x;d) = lim inf \u03b8\u21930 f(x + \u03b8d)\u2212 f(x) \u03b8 . (5)\nDefinition 3. (Stationary Point (Razaviyayn, Hong, and Luo, 2013)) A point x\u2217 is a (minimizing) stationary point of f(x) if\u2207f(x\u2217;d) \u2265 0 for all d such that x\u2217 + d \u2208 C.\nThe surrogate function in our relaxed MM should satisfy the following three conditions:\nf(xk) = gk(xk), (6)\nf(xk+1) \u2264 gk(xk+1), (Locally Majorant) (7)\nlim k\u2192\u221e\n(\u2207f(xk;d)\u2212\u2207gk(xk;d)) = 0,\n\u2200xk + d \u2208 C. (Asymptotic Smoothness) (8)\nBy combining conditions (6) and (7), we have the nonincrement property of MM:\nf(xk+1) \u2264 gk(xk+1) \u2264 gk(xk) = f(xk). (9)\nHowever, we will show that with a careful choice of surrogate, {f(xk)} can have sufficient descent, which is stronger than non-increment and is critical for proving the convergence of our MM.\nIn the traditional MM, the global majorization condition (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013) is assumed:\nf(x) \u2264 gk(x), \u2200x \u2208 C, (Globally Majorant) (10)\nwhich also results in the non-increment of the objective function, i.e., (9). However, a globally majorant surrogate cannot approximate the object well (Fig. 1). Moreover, the step length between successive iterates may be too small. So a globally majorant surrogate is likely to produce an inferior solution and converges slower than a locally majorant one ((Mairal, 2013) and our experiments).\nCondition (8) requires very weak first order smoothness of the difference between gk(x) and f(x). It is weaker than that in (Razaviyayn, Hong, and Luo, 2013, Assumption 1.(A3)), which requires that the directional derivatives of gk(x) and f(x) are equal at every xk. Here we only require the equality when the number of iterations goes to infinity, which provides more flexibility in constructing the surrogate function. When f(\u00b7) and gk(\u00b7) are both smooth, condition (8) can be satisfied when the two hold the same gradient at xk. For non-smooth functions, condition (8) can be enforced on the differentiable part f\u0303(x) of f(x). These two cases have been discussed in the literature (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013). If f\u0303(x) vanishes, the case not yet discussed in the literature, the condition can still be fulfilled by approximating the whole objective function f(x) directly, as long as the resulted surrogate satisfies certain properties, as stated below3.\nProposition 1. Assume that \u2203 K > 0, +\u221e > \u03b3u, \u03b3l > 0, and > 0, such that\ng\u0302k(x)+\u03b3u\u2016x\u2212xk\u201622 \u2265 f(x) \u2265 g\u0302k(x)\u2212\u03b3l\u2016x\u2212xk\u201622 (11)\nholds for all k \u2265 K and x \u2208 C such that \u2016x \u2212 xk\u2016 \u2264 , where the equality holds if and only if x = xk. Then condition (8) holds for gk(x) = g\u0302k(x) + \u03b3u\u2016x\u2212 xk\u201622.\nWe make two remarks. First, for sufficiently large \u03b3u and \u03b3l the inequality (11) naturally holds. However, a larger \u03b3u leads to slower convergence. Fortunately, as we only require the inequality to hold for sufficiently large k, adaptively increasing \u03b3u from a small value is allowed and also beneficial. In many cases, the bounds of \u03b3u and \u03b3l may be deduced from the objective function. Second, the above proposition does not specify any smoothness property on either g\u0302k(x) or the difference gk(x)\u2212f(x). It is not odd to add the proximal term \u2016x \u2212 xk\u201622, which has been widely used, e.g. in proximal splitting algorithm (Attouch, Bolte, and Svaiter, 2013) and alternating direction method of multipliers (ADMM) (Lin, Liu, and Li, 2013).\nFor general non-convex and non-smooth problems, proving the convergence to a global (or local) minimum is out of reach, classical analysis focuses on converging to stationary points instead. For general MM methods, since only nonincrement property is ensured, even the convergence to stationary points cannot be guaranteed. Mairal (Mairal, 2013)\n3The proofs of the results in this paper can be found in Supplementary Materials.\nproposed using strongly convex surrogates and thus proved that the iterates of corresponding MM converge to stationary points. Here we have similar results, as stated below. Theorem 1. (Convergence) Assume that the surrogate gk(x) satisfies (6) and (7), and further is strongly convex, then the sequence {f(xk)} has sufficient descent. If gk(x) further satisfies (8) and {xk} is bounded, then the sequence {xk} converges to stationary points. Remark 1. If gk(x) = g\u0307k(x)+\u03c1/2\u2016x\u2212xk\u201622, where g\u0307k(x) is locally majorant (not necessarily convex) as (7) and \u03c1 > 0, then the strongly convex condition can be removed and the same convergence result holds.\nIn the next section, we will give a concrete example on how to construct appropriate surrogates for the RMF problem."}, {"heading": "Solving Robust Matrix Factorization by Relaxed MM", "text": "Matrix factorization is to factorize a matrix, which usually has missing values and noises, into two matrices. It is widely used for structure from motion (Tomasi and Kanade, 1992), clustering (Kong, Ding, and Huang, 2011), dictionary learning (Mairal et al., 2010), etc. Normally, people aim at minimizing the error between the given matrix and the product of two matrices at the observed entries, measured in squared `2 norm. Such models are fragile to outliers. Recently, `1- norm has been suggested to measure the error for enhancing robustness. Such models are thus called robust matrix factorization (RMF). Their formulation is as follows:\nmin U\u2208Cu,V \u2208Cv\n\u2016W (M \u2212UV T )\u20161 +Ru(U)+Rv(V ), (12)\nwhere M \u2208 Rm\u00d7n is the observed matrix and \u2016 \u00b7 \u20161 is the `1-norm, namely the sum of absolute values in a matrix. U \u2208 Rm\u00d7r and V \u2208 Rn\u00d7r are the unknown factor matrices. W is the 0-1 binary mask with the same size as M . The entry value 0 means that the corresponding entry in M is missing, and 1 otherwise. The operator is the Hadamard entry-wise product. Cu \u2286 Rm\u00d7r and Cv \u2286 Rn\u00d7r are some closed convex sets, e.g., non-negative cones or balls in some norm. Ru(U) and Rv(V ) represent some convex regularizations, e.g., `1-norm, squared Frobenius norm, or elastic net. By combining different constraints and regularizations, we can get variants of RMF, e.g., low-rank matrix recovery (Ke and Kanade, 2005), non-negative matrix factorization (NMF) (Lee and Seung, 2001), and dictionary learning (Mairal et al., 2010).\nSuppose that we have obtained (Uk, Vk) at the k-th iteration. We split (U, V ) as the sum of (Uk, Vk) and the unknown increment (\u2206U,\u2206V ):\n(U, V ) = (Uk, Vk) + (\u2206U,\u2206V ). (13)\nThen (12) can be rewritten as:\nmin \u2206U+Uk\u2208Cu,\u2206V+Vk\u2208Cv\nFk(\u2206U,\u2206V ) = \u2016W (M \u2212 (Uk+\n\u2206U)(V Tk + \u2206V ) T )\u20161 +Ru(Uk + \u2206U) +Rv(Vk + \u2206V ).\n(14)\nNow we aim at finding an increment (\u2206U,\u2206V ) such that the objective function decreases properly. However, problem (14) is not easier than the original problem (12). Inspired by MM, we try to approximate (14) with a convex surrogate. By the triangular inequality of norms, we have the following inequality:\nFk(\u2206U,\u2206V ) \u2264 \u2016W (M \u2212 UkV Tk \u2212\u2206UV Tk \u2212 Uk\u2206V T )\u20161 + \u2016W (\u2206U\u2206V T )\u20161 +Ru(Uk + \u2206U) +Rv(Vk + \u2206V ),\n(15)\nwhere the term \u2016W (\u2206U\u2206V T )\u20161 can be further approximated by \u03c1u/2\u2016U\u20162F + \u03c1v/2\u2016V \u20162F , in which \u03c1u and \u03c1v are some positive constants. Denoting\nG\u0302k(\u2206U,\u2206V ) = \u2016W (M \u2212 UkY Tk \u2212\u2206UV Tk \u2212 Uk\u2206V T )\u20161 +Ru(Uk + \u2206U) +Rv(Vk + \u2206V ), (16)\nwe have a surrogate function of Fk(\u2206U,\u2206V ) as follows:\nGk(\u2206U,\u2206V ) = G\u0302k(\u2206U,\u2206V ) + \u03c1u 2 \u2016\u2206U\u20162F + \u03c1v 2 \u2016\u2206V \u20162F ,\ns.t. \u2206U + Uk \u2208 Cu,\u2206V + Vk \u2208 Cv. (17)\nDenoting #W(i,.) and #W(.,j) as the number of observed entries in the corresponding column and row of M , respectively, and > 0 as any positive scalar, we have the following proposition.\nProposition 2. G\u0302k(\u2206U,\u2206V ) + \u03c1\u0304u/2\u2016\u2206U\u20162F + \u03c1\u0304v/2\u2016\u2206V \u20162F \u2265 Fk(\u2206U,\u2206V ) \u2265 G\u0302k(\u2206U,\u2206V ) \u2212 \u03c1\u0304u/2\u2016\u2206U\u20162F \u2212 \u03c1\u0304v/2\u2016\u2206V \u20162F holds for all possible (\u2206U,\u2206V ) and the equality holds if and only if (\u2206U,\u2206V ) = (0,0), where \u03c1\u0304u = max{#W(i,.), i = 1, . . . ,m}+ , and \u03c1\u0304v = max{#W(.,j), j = 1, . . . , n}+ .\nBy choosing \u03c1u and \u03c1v in different ways, we have two versions of relaxed MM for RMF: RMF by globally majorant MM (RMF-GMMM for short) and RMF by locally majorant MM (RMF-LMMM for short). In RMF-GMMM, \u03c1u and \u03c1v are fixed to be \u03c1\u0304u and \u03c1\u0304v in Proposition 2, respectively, throughout the iterations. In RMF-LMMM, \u03c1u and \u03c1v are instead initialized with relatively small values and then increase gradually, using the line search technique in (Beck and Teboulle, 2009) to ensure the locally majorant condition (7). \u03c1u and \u03c1v eventually reach the upper bounds \u03c1\u0304u and \u03c1\u0304v in Proposition 2, respectively. As we will show, RMFLMMM significantly outperforms RMF-GMMM in all our experiments, in both convergence speed and quality of solution.\nSince the chosen surrogate Gk naturally fulfills the conditions in Theorem 1, we have the following convergence result for RMF solved by relax MM. Theorem 2. By minimizing (17) and updating (U, V ) according to (13), the sequence {F (Uk, Vk)} has sufficient descent and the sequence {(Uk, Vk)} converges to stationary points.\nTo the best of our knowledge, this is the first convergence guarantee for variants of RMF without extra assumptions. In the following, we will give two examples of RMF in (12).\nTwo Variants of RMF\nLow Rank Matrix Recovery exploits the fact r min(m,n) to recover the intrinsic low rank data from the measurement matrix with missing data. When the error is measured by the squared Frobenius norm, many algorithms have been proposed (Buchanan and Fitzgibbon, 2005; Mitra, Sheorey, and Chellappa, 2010). For robustness, Ke and Kanade (2005) proposed to adopt the `1-norm. They minimized U and V alternatively, which could easily get stuck at non-stationary points (Bertsekas, 1999). So Eriksson and van den Hengel (2012) represented V implicitly with U and extended the Wiberg Algorithm to `1-norm. They only proved the convergence of the objective function value, not the sequence {(Uk, Vk)} itself. Moreover, they had to assume that the dependence of V on U is differentiable, which is unlikely to hold everywhere. Additionally, as it unfolds matrix U into a vector and adopt an () its memory requirement is very high, which prevents it from large scale computation. Recently, ADMM was used for matrix recovery. By assuming that the variables are bounded and convergent, Shen, Wen, and Zhang (2014) proved that any accumulation point of their algorithm is the Karush-Kuhn-Tucker (KKT) point. However, the method was only able to handle outliers with magnitudes comparable to the low rank matrix (Shen, Wen, and Zhang, 2014). Moreover, the penalty parameter was fixed, which was not easy to tune for fast convergence. Some researches (Zheng et al., 2012; Cabral et al., 2013) further extended that by adding different regularizations on (U, V ) and achieved state-of-the-art performance. As the convergence analysis in (Shen, Wen, and Zhang, 2014) cannot be directly extended, it remains unknown whether the iterates converge to KKT points. In this paper, we adopt the same formulation as (Cabral et al., 2013):\nmin U,V \u2016W (M \u2212 UV T )\u20161 + \u03bbu 2 \u2016U\u20162F + \u03bbv 2 \u2016V \u20162F , (18)\nwhere the regularizers \u2016U\u20162F and \u2016V \u20162F are for reducing the solution space (Buchanan and Fitzgibbon, 2005).\nNon-negative Matrix Factorization (NMF) has been popular since the seminal work of Lee and Seung (2001). Kong, Ding, and Huang (2011) extended the squared `2 norm to the `21-norm for robustness. Recently, Pan et al. (2014) further introduced the `1-norm to handle outliers in non-negative dictionary learning, resulting in the following model:\nmin U\u22650,V\u22650 \u2016M \u2212 UV T \u20161 + \u03bbu 2 \u2016U\u20162F + \u03bbv\u2016V \u20161, (19)\nwhere \u2016U\u20162F is added in to avoid the trivial solution and \u2016V \u20161 is to induce sparsity. All the three NMF models use multiplicative updating schemes, which only differ in the weights used. The multiplicative updating scheme is intrinsically a globally majorant MM. Assuming that the iterates converge, they proved that the limit of sequence is a stationary point. However, Gonzalez and Zhang (2005) pointed out that with such a multiplicative updating scheme is hard to reach the convergence condition even on toy data."}, {"heading": "Minimizing the Surrogate Function", "text": "Now we show how to find the minimizer of the convex Gk(\u2206U,\u2206V ) in (17). This can be easily done by using the linearized alternating direction method with parallel splitting and adaptive penalty (LADMPSAP) (Lin, Liu, and Li, 2013). LADMPSAP fits for solving the following linearly constrained separable convex programs:\nmin x1,\u00b7\u00b7\u00b7 ,xn n\u2211 j=1 fj(xj), s.t. n\u2211 j=1 Aj(xj) = b, (20)\nwhere xj and b could be either vectors or matrices, fj is a proper convex function, and Aj is a linear mapping. To apply LADMPSAP, we first introduce an auxiliary matrix E such that E = M \u2212 UkY Tk \u2212 \u2206UV Tk \u2212 Uk\u2206V T . Then minimizing Gk(\u2206U,\u2206V ) in (17) can be transformed into:\nmin E,\u2206U,\u2206V\n\u2016W E\u20161 + (\u03c1u\n2 \u2016\u2206U\u20162F +Ru(Uk + \u2206U) + \u03b4Cu(Uk + \u2206U) ) + (\u03c1v\n2 \u2016\u2206V \u20162F +Rv(Vk + \u2206V ) + \u03b4Cv (Vk + \u2206V )\n) ,\ns.t. E + \u2206UV Tk + Uk\u2206V T = M \u2212 UkY Tk ,\n(21) where the indicator function \u03b4C(x) : Rp \u2192 R is defined as:\n\u03b4C(x) = { 0, if x \u2208 C, +\u221e, otherwise. (22)\nThen problem (21) naturally fits into the model problem (20). For more details, please refer to Supplementary Materials."}, {"heading": "Experiments", "text": "In this section, we compare our relaxed MM algorithms with state-of-the-art RMF algorithms: UNuBi (Cabral et al., 2013) for low rank matrix recovery and `1-NMF (Pan et al., 2014) for robust NMF. The code of UNuBi (Cabral et al., 2013) was kindly provided by the authors. We implemented the code of `1-NMF (Pan et al., 2014) ourselves."}, {"heading": "Synthetic Data", "text": "We first conduct experiments on synthetic data. Here we set the regularization parameters \u03bbu = \u03bbv = 20/(m + n) and stop our relaxed MM algorithms when the relative change in the objective function is less than 10\u22124.\nLow Rank Matrix Recovery: We generate a data matrix M = U0V T 0 , where U0 \u2208 R500\u00d710 and V0 \u2208 R500\u00d710 are sampled i.i.d. from a Gaussian distribution N (0, 1). We additionally corrupt 40% entries of M with outliers uniformly distributed in [\u221210, 10] and choose W with 80% data missing. The positions of both outliers and missing data are chosen uniformly at random. We initialize all compared algorithms with the rank-r truncation of the singularvalue decomposition of W M . The performance is evaluated by measuring relative error with the ground truth:\n\u2016UestV Test\u2212U0V T0 \u20161/(mn), where Uest and Vest are the estimated matrices. The results are shown in Fig. 2(a), where RMF-LMMM reaches the lowest relative error in much less iterations. Non-negative Matrix Factorization: We generate a data matrix M = U0V T0 , where U0 \u2208 R500\u00d710 and V0 \u2208 R500\u00d710 are sampled i.i.d. from a uniform distribution U(0, 1). For sparsity, we further randomly set 30% entries of V as 0. We further corrupt 40% entries of M with outliers uniformly distributed in [0, 10]. All the algorithms are initialized with the same non-negative random matrix. The results are shown in Fig. 2(b), where RMF-LMMM also gets the best result with much less iterations. `1-NMF tends to be stagnant and cannot approach a high precision solution even after 5000 iterations."}, {"heading": "Real Data", "text": "In this subsection, we conduct experiment on real data. Since there is no ground truth, we measure the relative error by \u2016W (Mest \u2212M)\u20161/#W , where #W is the number of observed entries. For NMF, W becomes an all-one matrix.\nLow Rank Matrix Recovery: Tomasi and Kanade (1992) first modelled the affine rigid structure from motion as a rank-4 matrix recovery problem. Here we use the famous Oxford Dinosaur sequence 4, which consists of 36 images with a resolution of 720\u00d7 576 pixels. We pick out a portion of the raw feature points which are observed by at least 6 views (Fig. 3(a)). The observed matrix is of size 72 \u00d7 557 with a missing data ratio 79.5% and shows a band diagonal pattern. We register the image origin to the image center, (360, 288). We adopt the same initialization and parameter setting as the synthetic data above.\nFigures 3(b)-(d) show the full tracks reconstructed by all algorithms. As the dinosaur sequence is taken on a turntable,\n4http://www.robots.ox.ac.uk/\u02dcvgg/data1. html\nall the tracks are supposed to be circular. Among them, the tracks reconstructed by RMF-GMMM are the most inferior. UNuBi gives reasonably good results. However, most of the reconstructed tracks in large radii do not appear closed. Some tracks in the upper part are not reconstructed well either, including one obvious failure. In contrast, almost all the tracks reconstructed by RMF-LMMM are circular and appear closed, which are the most visually plausible. The lowest relative error also confirms the effectiveness of RMFLMMM.\nNon-negative Matrix Factorization: We test the performance of robust NMF by clustering (Kong, Ding, and Huang, 2011; Pan et al., 2014). The experiments are conducted on four benchmark datasets of face images, which includes: AT&T, UMIST, a subset of PIE 5 and a subset of AR 6. We use the first 10 images in each class for PIE and the first 13 images for AR. The descriptions of the datasets are summarized in the second row of Table 2. The evaluation metrics we use here are accuracy (ACC), normalized mutual information (NMI) and purity (PUR) (Kong, Ding, and Huang, 2011; Pan et al., 2014). We change the regularization parameter \u03bbu to 2000/(m + n) and maintain \u03bbv as 20/(m + n). The number r of clusters is equal to the number of classes in each dataset. We adopt the initializations in (Kong, Ding, and Huang, 2011). Firstly, we use the principal component analysis (PCA) to get a subspace with dimension r. Then we employ k-means on the PCA-reduced data to get the clustering results V \u2032. Finally, V is initialized as V = V \u2032 + 0.3 and U is by computing the clustering centroid for each class. We empirically terminate `1-NMF, RMF-GMMM, and RMF-LMMM after 5000, 500, and 20 iterations, respectively. The clustering results are shown in the last three rows of Table 2. We can see that RMF-LMMM achieves tremendous improvements over the two majorant algorithms across all datasets. RMF-GMMM is also better than `1-NMF. The lowest relative error in the third row shows that RMF-LMMM can always approximate the measurement matrix much better than the other two.\n5http://www.zjucadcg.cn/dengcai/Data/data. html\n6http://www2.ece.ohio-state.edu/\u02dcaleix/ ARdatabase.html"}, {"heading": "Conclusions", "text": "In this paper, we propose a weaker condition on surrogates in MM, which enables better approximation of the objective function. Our relaxed MM is general enough to include the existing MM methods. In particular, the non-smooth and non-convex objective function can be approximated directly, which is never done before. Using the RMF problems as examples, our locally majorant relaxed MM beats the stateof-the-art methods with margin, in both solution quality and convergence speed. We prove that the iterates converge to stationary points. To our best knowledge, this is the first convergence guarantee for variants of RMF without extra assumptions."}, {"heading": "Acknowledgements", "text": "Zhouchen Lin is supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), National Natural Science Foundation of China (NSFC) (grant no. 61272341 and 61231002), and Microsoft Research Asia Collaborative Research Program. Zhenyu Zhao is supported by NSFC (Grant no. 61473302). Hongbin Zha is supported by 973 Program (grant no. 2011CB302202)."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "Proofs", "text": ""}, {"heading": "Proof of Proposition 1", "text": "Consider minimizing gk(x) \u2212 f(x) in the neighbourhood \u2016x \u2212 xk\u2016 \u2264 . It reaches the local minimum 0 at x = xk. By Definition 3, we have\n\u2207gk(xk;d) \u2265 \u2207f(xk;d), \u2200 xk+d \u2208 C, \u2016d\u2016 < . (23)\nDenote lk(x) as\nlk(x) = g\u0302k(x)\u2212 \u03b3l\u2016x\u2212 xk\u201622, (24)\nSimilarly, we have\n\u2207f(xk;d) \u2265 \u2207lk(xk;d), \u2200 xk +d \u2208 C, \u2016d\u2016 < . (25)\nComparing gk(x) with lk(x), they are combined with two parts, the common part f\u0302k(x) and the continuously differentiable part \u2016x \u2212 xk\u201622. And the function gk(x) \u2212 lk(x) = (\u03b3u + \u03b3l)\u2016x \u2212 xk\u20162 achieves its global minimum at x = xk. Hence the first order optimality condition (Razaviyayn, Hong, and Luo, 2013, Assumption 1.(A3)) implies\n\u2207gk(xk;d) = \u2207lk(xk;d), \u2200 xk + d \u2208 C, \u2016d\u2016 < . (26)\nCombining (23), (25) and (26), we have\n\u2207gk(xk;d) = \u2207f(xk;d), \u2200 xk+d \u2208 C, \u2016d\u2016 < . (27)"}, {"heading": "Proof of Theorem 1", "text": "Consider the \u03c1-strongly convex surrogate gk(x). As xk+1 = arg minx\u2208C gk(x), by the definition of strongly convex function (Mairal, 2013, Lemma B.5), we have\ngk(xk)\u2212 gk(xk+1) \u2265 \u03c1\n2 \u2016xk \u2212 xk+1\u201622. (28)\nCombining with non-increment of the objective function,\nf(xk+1) \u2264 gk(xk+1) \u2264 gk(xk) = f(xk), (29)\nwe have\nf(xk)\u2212 f(xk+1) \u2265 \u03c1\n2 \u2016xk \u2212 xk+1\u20162. (30)\nConsider the surrogate gk(x) = g\u0307k(x) + \u03c1/2\u2016x\u2212 xk\u201622, we have\ngk(xk+1)\u2212 f(xk+1)\n=g\u0307k(xk+1) + \u03c1\n2 \u2016xk+1 \u2212 xk\u201622 \u2212 f(xk+1)\n\u2265\u03c1 2 \u2016xk+1 \u2212 xk\u201622,\n(31)\nwhere the inequality derived from the locally majorant g\u0307k(xk+1) \u2265 f(xk+1). Combining with (29), we can also get (30). Thus the sequence {f(xk)} has sufficient descent.\nSumming all the inequalities in (30) for k \u2265 1, we have\n+\u221e > f(x1)\u2212 f(xk)|k\u2192+\u221e \u2265 \u03c1\n2 +\u221e\u2211 k=1 \u2016xk \u2212 xk+1\u20162.\n(32)\nThen we can infer that\nlim k\u2192+\u221e\nxk+1 \u2212 xk = 0. (33)\nAs the sequence {xk} is bounded, hence has accumulation points. For any accumulation point x\u2217, there exists a subsequence {xkj} such that lim\nj\u2192\u221e xkj = x\n\u2217.\nCombining conditions (6), (7), and (9), we have\ngkj (x) \u2265 gkj (xkj+1) \u2265 f(xkj+1) \u2265 f(xkj+1) \u2265 gkj+1(xkj+1), \u2200x \u2208 C.\n(34)\nLetting j \u2192\u221e in both sides, we obtain at x = x\u2217\n\u2207g(x\u2217,d) \u2265 0, \u2200x\u2217 + d \u2208 C. (35)\nCombining with condition (8), we have\n\u2207f(x\u2217,d) \u2265 0, \u2200x\u2217 + d \u2208 C. (36)\nBy Definition 3, we can conclude that x\u2217 is a stationary point."}, {"heading": "Proof of Proposition 2", "text": "Denoting \u2206uTi and \u2206v T i as the i-th rows of \u2206U and \u2206V , respectively, we can relax\n\u2016W (\u2206U\u2206V T )\u20161\n= \u2225\u2225\u2225\u2225\u2225\u2225\u2225W \u2206u T 1 \u2206v1 . . . \u2206u T 1 \u2206vn ... . . . ... \u2206uTm\u2206v1 . . . \u2206u T m\u2206vn  \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n1\n\u2264 \u03c1\u0304u 2 \u2016\u2206U\u20162F + \u03c1\u0304v 2 \u2016\u2206V \u20162F ,\n(37)\nwhere the inequality is derived from the Cauchy-Schwartz inequality with \u03c1\u0304u = #W(i,.) + , \u2200i = 1, . . . ,m and \u03c1\u0304v = #W(.,j) + , \u2200i = 1, . . . , n. The equality holds if and only if (\u2206U,\u2206V ) = (0,0). Then we have\nG\u0302k(\u2206U,\u2206V ) + \u03c1\u0304u 2 \u2016\u2206U\u20162F + \u03c1\u0304v 2 \u2016\u2206V \u20162F\n\u2265G\u0302k(\u2206U,\u2206V ) + \u2016W (\u2206U\u2206V T )\u20161 \u2265Fk(\u2206U,\u2206V ),\n(38)\nwhere the second inequality is derived from the triangular inequality of norms. Similarly we have\nFk(\u2206U,\u2206V )\n\u2265G\u0302k(\u2206U,\u2206V )\u2212 \u2016W (\u2206U\u2206V T )\u20161\n\u2265G\u0302k(\u2206U,\u2206V )\u2212 \u03c1\u0304u 2 \u2016\u2206U\u20162F \u2212 \u03c1\u0304v 2 \u2016\u2206V \u20162F\n(39)"}, {"heading": "Proof of Theorem 2", "text": "Combining Fk(\u2206U,\u2206V ) with Gk(\u2206U,\u2206V ), we can easily get Fk((0,0)) = Gk((0,0)). Let (\u2206Uk,\u2206Vk) represent the minimizer of Gk(\u2206U,\u2206V ). By the choice of (\u03c1u, \u03c1v), both RMF-GMMM and RMF-LMMM can ensure\nGk(\u2206Uk,\u2206Vk) \u2265 Fk(\u2206Uk,\u2206Vk). (40)\nCombining Proposition 1 and 2, we can ensure the first order smoothness in the infinity\nlim k\u2192\u221e\n(\u2207Fk(0,0;Du, Dv)\u2212\u2207Gk(0,0;Du, Dv)) = 0,\n\u2200 Uk +Du \u2208 Cu, Vk +Dv \u2208 Cv. (41)\nIn addition, Gk(\u2206,\u2206V ) is strongly convex and the sequence {(Uk, Vk)} is bounded by the constraints and regularizations, according to Theorem 1, the original function sequence {F (Uk, Vk)} would have sufficient descent, and any limit point of the sequence {(Uk, Vk)} is a stationary point of the objective function F (U, V )."}, {"heading": "Minimizing the surrogate by LADMPSAP", "text": ""}, {"heading": "Sketch of LADMPSAP", "text": "LADMPSAP fits for solving the following linearly constrained separable convex programs:\nmin x1,\u00b7\u00b7\u00b7 ,xn n\u2211 j=1 fj(xj), s.t. n\u2211 j=1 Aj(xj) = b, (42)\nwhere xj and b could be either vectors or matrices, fj is a proper convex function, and Aj is a linear mapping. Very often, there are multiple blocks of variables (n \u2265 3). We denote the iteration index by superscript i. The LADMPSAP algorithm consists of the following steps (Lin, Liu, and Li, 2013):\n(a) Update xj\u2019s (j = 1, \u00b7 \u00b7 \u00b7 , n) in parallel:\nxi+1j = argmin xj\nfj(xj)+ \u03c3\n(i) j\n2 \u2225\u2225\u2225zj \u2212 xij +A\u2020j(y\u0302i)/\u03c3(i)j \u2225\u2225\u22252 (43)\n(b) Update y:\nyi+1 = yi + \u03b2(i)  n\u2211 j=1 Aj(xi+1j )\u2212 b  . (44) (c) Update \u03b2:\n\u03b2(i+1) = min(\u03b2max, \u03c1\u03b2(i)), (45)\nwhere y is the Lagrange multiplier, \u03b2(i) is the penalty parameter, \u03b2max 1 is an upper bound of \u03b2(i), \u03c3(i)j = \u03b7j\u03b2(i) with \u03b7j > n\u2016Aj\u20162 (\u2016Aj\u2016 is the operator norm of Aj), A\u2020j is the adjoint operator of Aj ,\ny\u0302i = yi + \u03b2(i)  n\u2211 j=1 Aj(xij)\u2212 b  , (46) and\n\u03c1 =\n{ \u03c10, if \u03b2(i) max ({\u221a \u03b7j \u2225\u2225zi+1j \u2212 xij\u2225\u2225}) / \u2016b\u2016 < \u03b51,\n1, otherwise, (47)\nwith \u03c10 \u2265 1 being a constant and 0 < \u03b51 1 being a threshold. The iteration terminates when the following two conditions are met:\n\u03b2k max ({\u221a \u03b7i \u2225\u2225xk+1i \u2212 xki \u2225\u2225 , i = 1, \u00b7 \u00b7 \u00b7 , n}) /\u2016b\u2016 < \u03b51,\n(48)\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 Ai(xk+1i )\u2212 b \u2225\u2225\u2225\u2225\u2225 /\u2016b\u2016 < \u03b52. (49)"}, {"heading": "The Optimization Using LADMPSAP", "text": "We aim to minimize\nmin E,\u2206U,\u2206V\n\u2016W E\u20161 + (\u03c1u\n2 \u2016\u2206U\u20162F +Ru(Uk + \u2206U) + \u03b4Cu(Uk + \u2206U) ) + (\u03c1v\n2 \u2016\u2206V \u20162F +Rv(Vk + \u2206V ) + \u03b4Cv (Vk + \u2206V )\n) ,\ns.t. E + \u2206UV Tk + Uk\u2206V T = M \u2212 UkY Tk ,\n(50)\nwhere the function \u03b4C(x) : Rp \u2192 R is define as:\n\u03b4C(x) = { 0, if x \u2208 C, +\u221e, otherwise. (51)\nwhich naturally fits into the model problem (42). According to (43), E can be updated by solving the following subproblem\nmin E \u2016W E\u20161 +\n\u03c3 (i) e\n2 \u2016E \u2212 Ei + Y\u0302 i/\u03c3(i)e \u20162F , (52)\nwhere\nY\u0302 i = Y i + \u03b2(i)(Ei + \u2206U iV Tk +Uk\u2206V iT +UkY T k \u2212M), (53) and \u03c3ie = \u03b7e\u03b2\n(i). We choose \u03b7e = 3Le + , where 3 is the number of parallelly updating variables, i.e., E, \u2206U and \u2206V ,Le denotes the squared spectral norm of the linear mapping on E, which is equal to 1, and is a small positive scalar. The solution to (52) is\nEi+1 =W S \u03c3 (i) e\n\u22121 ( Ei \u2212 Y\u0302 i/\u03c3(i)e ) + W\u0304 ( Ei \u2212 Y\u0302 i/\u03c3(i)e ) ,\n(54)\nwhere S is the shrinkage operator (Lin, Chen, and Ma, 2010): S\u03b3(x) = max(|x| \u2212 \u03b3, 0)sgn(x), (55) and W\u0304 is the complement of W .\nAlso by (43), \u2206U and \u2206V can be updated by solving:\nmin \u2206U \u03c1u 2 \u2016\u2206U\u20162F +Ru(Uk + \u2206U) + \u03b4Cu(Uk + \u2206U)\n+ \u03c3\n(i) u 2 \u2016\u2206U \u2212\u2206U i + Y\u0302 iVk/\u03c3(i)u \u20162F ,\n(56)\nmin \u2206V \u03c1v 2 \u2016\u2206V \u20162F +Rv(Vk + \u2206V ) + \u03b4Cv (Vk + \u2206V )\n+ \u03c3\n(i) v 2 \u2016\u2206V \u2212\u2206V i + Y\u0302 iTUk/(\u03c3(i)v )\u20162F ,\n(57)\nwhere \u03c3(i)u = \u03b7x\u03b2(i), \u03b7x = 3\u2016Vk\u201622 + , \u03c3v = \u03b7v\u03b2(i), and \u03b7v = 3\u2016Uk\u201622 + (\u2016\u00b7\u20162 denotes the largest singular value of a matrix).\nFor Low Rank Matrix Recovery, (56) and (57) can be solved by simply taking the derivative w.r.t \u2206U and \u2206V to be 0,\n\u2206U i+1 = ( \u2212\u03bbuUk + \u03c3(i)u \u2206U i \u2212 Y\u0302 iVk ) /(\u03bbu + \u03c3 (i) u + \u03c1u),\n(58) \u2206V i+1 = ( \u2212\u03bbvVk + \u03c3(i)v \u2206V i \u2212 Y\u0302 iTUk ) /(\u03bbv + \u03c3 (i) v + \u03c1v).\n(59) For Non-negative Matrix Factorization, (56) and (57) can be solved by projecting on to the non-negative subspace,\n\u2206U i+1 = S+0 ( (\u03c3(i)u + \u03c1u)Uk + \u03c3 (i) u \u2206U i\n\u2212 Y\u0302 iVk ) /(\u03bbu + \u03c3 (i) u + \u03c1u)\u2212 Uk, (60)\n\u2206V i+1 = S+ \u03bbv/(\u03c1v+\u03c3 (i) v )\n( Vk \u2212 \u03c3(i)v /(\u03c1v + \u03c3(i)v )(\u2212\u2206V i\n+ Y\u0302 iTUk/\u03c3 (i) v ) ) \u2212 Vk,\n(61) where S+ is the positive shrinkage operator:\nS\u03b3(x) = max(x\u2212 \u03b3, 0). (62)\nNext, we update Y as (44):\nY i+1 = Y i + \u03b2(i)(Ei+1 + \u2206U i+1V Tk\n+ Uk\u2206V (i+1)T + UkV T k \u2212M),\n(63)\nand update \u03b2 as (45):\n\u03b2(i+1) = min(\u03b2max, \u03c1\u03b2(i)), (64)\nwhere \u03c1 is defined as (47):\n\u03c1 =  \u03c10, if \u03b2(i) max (\u221a \u03b7e\u2016Ei+1 \u2212 Ei\u2016F , \u221a \u03b7u\u2016\u2206U i+1 \u2212\u2206U i\u2016F , \u221a \u03b7v\u2016\u2206V i+1 \u2212\u2206V i\u2016F ) /\u2016M \u2212 UkV Tk \u2016F < \u03b51,\n1, otherwise. (65)\nWe terminate the iteration when the following two conditions are met:\n\u03b2(i) max( \u221a \u03b7e\u2016Ei+1 \u2212 Ei\u2016F , \u221a \u03b7u\u2016\u2206U i+1 \u2212\u2206U i\u2016F ,\n\u221a \u03b7v\u2016\u2206V i+1 \u2212\u2206V i\u2016F )/\u2016M \u2212 UkV Tk \u2016F < \u03b51\n(66)\n\u2016Ei+1\u2212\u2206U i+1V Tk \u2212Uk\u2206V (i+1)T \u2016F /\u2016M\u2212UkV Tk \u2016F < \u03b52. (67) For better reference, we summarize the algorithm for minimizing Gk(\u2206U,\u2206V ) in Algorithm 2. When first executing Algorithm 2, we initialize E0 = M \u2212 U0V T0 , \u2206U0 = 0, \u2206V 0 = 0 and Y 0 = 0. In the subsequent main iterations, we adopt the warm start strategy. Namely, we initialize E0, \u2206U0,\u2206V 0 and Y 0 with their respective optimal values in last main iteration.\nAlgorithm 2 Minimizing Gk(\u2206U,\u2206V ) via LADMPSAP 1: Initialize i = 0, E0, \u2206U0, \u2206V 0, Y 0, \u03c10 > 1 and \u03b20 \u221d\n(m+ n)\u03b51. 2: while (66) or (67) is not satisfied do 3: Update E, \u2206U , and \u2206V parallelly, accordingly. 4: Update Y as (63). 5: Update \u03b2 as (64) and (65). 6: i = i+ 1. 7: end while\nOutput: The optimal (\u2206Uk,\u2206Vk)."}, {"heading": "Parameter Setting for LADMPSAP", "text": "Low Rank Matrix Recovery: we set \u03b51 = 10\u22125, \u03b52 = 10\u22124, \u03c10 = 1.5, and \u03b2max = 1010 as the default value. Non-negative Matrix Factorization: we set \u03b51 = 10\u22124, \u03b52 = 10 \u22124, \u03c10 = 3, and \u03b2max = 1010 as the default value."}], "references": [{"title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward\u2013backward splitting, and regularized Gauss\u2013 Seidel methods", "author": ["H. Attouch", "J. Bolte", "B.F. Svaiter"], "venue": "Mathematical Programming 137(1-2):91\u2013129.", "citeRegEx": "Attouch et al\\.,? 2013", "shortCiteRegEx": "Attouch et al\\.", "year": 2013}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences 2(1):183\u2013202.", "citeRegEx": "Beck and Teboulle,? 2009", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific, 2nd edition.", "citeRegEx": "Bertsekas,? 1999", "shortCiteRegEx": "Bertsekas", "year": 1999}, {"title": "Convex analysis and nonlinear optimization: theory and examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer Science & Business Media.", "citeRegEx": "Borwein and Lewis,? 2010", "shortCiteRegEx": "Borwein and Lewis", "year": 2010}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM review 51(1):34\u201381.", "citeRegEx": "Bruckstein et al\\.,? 2009", "shortCiteRegEx": "Bruckstein et al\\.", "year": 2009}, {"title": "Damped Newton algorithms for matrix factorization with missing data", "author": ["A.M. Buchanan", "A.W. Fitzgibbon"], "venue": "CVPR, volume 2, 316\u2013322. IEEE.", "citeRegEx": "Buchanan and Fitzgibbon,? 2005", "shortCiteRegEx": "Buchanan and Fitzgibbon", "year": 2005}, {"title": "A robust gradient sampling algorithm for nonsmooth, nonconvex optimization", "author": ["J.V. Burke", "A.S. Lewis", "M.L. Overton"], "venue": "SIAM Journal on Optimization 15(3):751\u2013779.", "citeRegEx": "Burke et al\\.,? 2005", "shortCiteRegEx": "Burke et al\\.", "year": 2005}, {"title": "Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition", "author": ["R. Cabral", "F.D.L. Torre", "J.P. Costeira", "A. Bernardino"], "venue": "ICCV, 2488\u20132495. IEEE.", "citeRegEx": "Cabral et al\\.,? 2013", "shortCiteRegEx": "Cabral et al\\.", "year": 2013}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications 14(5-6):877\u2013905.", "citeRegEx": "Candes et al\\.,? 2008", "shortCiteRegEx": "Candes et al\\.", "year": 2008}, {"title": "Smoothing methods for nonsmooth, nonconvex minimization", "author": ["X. Chen"], "venue": "Mathematical Programming 134(1):71\u201399.", "citeRegEx": "Chen,? 2012", "shortCiteRegEx": "Chen", "year": 2012}, {"title": "Optimization and nonsmooth analysis, volume 5", "author": ["F.H. Clarke"], "venue": "SIAM.", "citeRegEx": "Clarke,? 1990", "shortCiteRegEx": "Clarke", "year": 1990}, {"title": "Efficient computation of robust weighted low-rank matrix approximations using the L1 norm. Pattern Analysis and Machine Intelligence, IEEE Transactions on 34(9):1681\u20131690", "author": ["A. Eriksson", "A. van den Hengel"], "venue": null, "citeRegEx": "Eriksson and Hengel,? \\Q2012\\E", "shortCiteRegEx": "Eriksson and Hengel", "year": 2012}, {"title": "Accelerating the Lee-Seung algorithm for non-negative matrix factorization", "author": ["E.F. Gonzalez", "Y. Zhang"], "venue": "Dept. Comput. & Appl. Math., Rice Univ., Houston, TX, Tech. Rep. TR-05-02.", "citeRegEx": "Gonzalez and Zhang,? 2005", "shortCiteRegEx": "Gonzalez and Zhang", "year": 2005}, {"title": "A tutorial on MM algorithms", "author": ["D.R. Hunter", "K. Lange"], "venue": "The American Statistician 58(1):30\u201337.", "citeRegEx": "Hunter and Lange,? 2004", "shortCiteRegEx": "Hunter and Lange", "year": 2004}, {"title": "Robust L1-norm factorization in the presence of outliers and missing data by alternative convex programming", "author": ["Q. Ke", "T. Kanade"], "venue": "CVPR, volume 1, 739\u2013746. IEEE.", "citeRegEx": "Ke and Kanade,? 2005", "shortCiteRegEx": "Ke and Kanade", "year": 2005}, {"title": "Robust nonnegative matrix factorization using L21-norm", "author": ["D. Kong", "C. Ding", "H. Huang"], "venue": "CIKM, 673\u2013682. ACM. Lee, D. D., and Seung, H. S. 2001. Algorithms for non-negative matrix factorization. In NIPS, 556\u2013562.", "citeRegEx": "Kong et al\\.,? 2011", "shortCiteRegEx": "Kong et al\\.", "year": 2011}, {"title": "The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "Y. Ma"], "venue": "arXiv preprint arXiv:1009.5055.", "citeRegEx": "Lin et al\\.,? 2010", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning", "author": ["Z. Lin", "R. Liu", "H. Li"], "venue": "Machine Learning 1\u201339.", "citeRegEx": "Lin et al\\.,? 2013", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research 11:19\u201360.", "citeRegEx": "Mairal et al\\.,? 2010", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Optimization with first-order surrogate functions", "author": ["J. Mairal"], "venue": "arXiv preprint arXiv:1305.3120.", "citeRegEx": "Mairal,? 2013", "shortCiteRegEx": "Mairal", "year": 2013}, {"title": "Survey of bundle methods for nonsmooth optimization", "author": ["M. M\u00e4kel\u00e4"], "venue": "Optimization Methods and Software 17(1):1\u201329.", "citeRegEx": "M\u00e4kel\u00e4,? 2002", "shortCiteRegEx": "M\u00e4kel\u00e4", "year": 2002}, {"title": "Large-scale matrix factorization with missing data under additional constraints", "author": ["K. Mitra", "S. Sheorey", "R. Chellappa"], "venue": "NIPS, 1651\u20131659.", "citeRegEx": "Mitra et al\\.,? 2010", "shortCiteRegEx": "Mitra et al\\.", "year": 2010}, {"title": "Robust nonnegative dictionary learning", "author": ["Q. Pan", "D. Kong", "C. Ding", "B. Luo"], "venue": "AAAI.", "citeRegEx": "Pan et al\\.,? 2014", "shortCiteRegEx": "Pan et al\\.", "year": 2014}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo"], "venue": "SIAM Journal on Optimization 23(2):1126\u2013 1153.", "citeRegEx": "Razaviyayn et al\\.,? 2013", "shortCiteRegEx": "Razaviyayn et al\\.", "year": 2013}, {"title": "Augmented Lagrangian alternating direction method for matrix separation based on lowrank factorization", "author": ["Y. Shen", "Z. Wen", "Y. Zhang"], "venue": "Optimization Methods and Software 29(2):239\u2013 263.", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Shape and motion from image streams under orthography: a factorization method", "author": ["C. Tomasi", "T. Kanade"], "venue": "International Journal of Computer Vision 9(2):137\u2013154.", "citeRegEx": "Tomasi and Kanade,? 1992", "shortCiteRegEx": "Tomasi and Kanade", "year": 1992}, {"title": "Practical low-rank matrix approximation under robust L1-norm", "author": ["Y. Zheng", "G. Liu", "S. Sugimoto", "S. Yan", "M. Okutomi"], "venue": "CVPR, 1410\u20131417. IEEE.", "citeRegEx": "Zheng et al\\.,? 2012", "shortCiteRegEx": "Zheng et al\\.", "year": 2012}, {"title": "Proof of Theorem 1 Consider the \u03c1-strongly convex surrogate gk(x). As xk+1 = arg minx\u2208C gk(x), by the definition of strongly convex function (Mairal", "author": ["\u2207f(xk", "\u2200 xk+d \u2208 C"], "venue": "Lemma B.5),", "citeRegEx": "\u2207f.xk and C,? \\Q2013\\E", "shortCiteRegEx": "\u2207f.xk and C", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": ", in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho, and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al.", "startOffset": 16, "endOffset": 28}, {"referenceID": 14, "context": ", in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho, and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 22, "context": ", in statistics (Chen, 2012), computer vision and image processing (Bruckstein, Donoho, and Elad, 2009; Ke and Kanade, 2005), data mining and machine learning (Pan et al., 2014; Kong, Ding, and Huang, 2011).", "startOffset": 159, "endOffset": 206}, {"referenceID": 10, "context": "Typical methods include subdifferential (Clarke, 1990), bundle methods (M\u00e4kel\u00e4, 2002), gradient sampling (Burke, Lewis, and Overton,", "startOffset": 40, "endOffset": 54}, {"referenceID": 20, "context": "Typical methods include subdifferential (Clarke, 1990), bundle methods (M\u00e4kel\u00e4, 2002), gradient sampling (Burke, Lewis, and Overton,", "startOffset": 71, "endOffset": 85}, {"referenceID": 19, "context": "#1 represents globally majorant MM in (Mairal, 2013), #2 represents strongly convex MM in (Mairal, 2013), #3 represents successive MM in (Razaviyayn, Hong, and Luo, 2013) and #4 represents our relaxed MM.", "startOffset": 38, "endOffset": 52}, {"referenceID": 19, "context": "#1 represents globally majorant MM in (Mairal, 2013), #2 represents strongly convex MM in (Mairal, 2013), #3 represents successive MM in (Razaviyayn, Hong, and Luo, 2013) and #4 represents our relaxed MM.", "startOffset": 90, "endOffset": 104}, {"referenceID": 9, "context": "2005), smoothing methods (Chen, 2012), and majorizationminimization (MM) (Hunter and Lange, 2004).", "startOffset": 25, "endOffset": 37}, {"referenceID": 13, "context": "2005), smoothing methods (Chen, 2012), and majorizationminimization (MM) (Hunter and Lange, 2004).", "startOffset": 73, "endOffset": 97}, {"referenceID": 19, "context": "Mairal (2013) has given a comprehensive review on MM.", "startOffset": 0, "endOffset": 14}, {"referenceID": 19, "context": "The most popular choice of surrogate is the class of \u201cfirst order surrogates\u201d, whose difference from the objective function is differentiable with a Lipschitz continuous gradient (Mairal, 2013).", "startOffset": 179, "endOffset": 193}, {"referenceID": 19, "context": "In parallel to (Mairal, 2013), Razaviyayn, Hong, and Luo (2013) also showed that many popular methods for minimizing non-smooth functions could be regarded as MM methods.", "startOffset": 15, "endOffset": 29}, {"referenceID": 19, "context": "Different from (Mairal, 2013), they suggested using the directional derivative to ensure the first order smoothness between the objective and the surrogate, which is weaker than the condition in (Mairal, 2013) that the difference between the objective and the surrogate should be smooth.", "startOffset": 15, "endOffset": 29}, {"referenceID": 19, "context": "Different from (Mairal, 2013), they suggested using the directional derivative to ensure the first order smoothness between the objective and the surrogate, which is weaker than the condition in (Mairal, 2013) that the difference between the objective and the surrogate should be smooth.", "startOffset": 195, "endOffset": 209}, {"referenceID": 19, "context": "In parallel to (Mairal, 2013), Razaviyayn, Hong, and Luo (2013) also showed that many popular methods for minimizing non-smooth functions could be regarded as MM methods.", "startOffset": 16, "endOffset": 64}, {"referenceID": 19, "context": "In parallel to (Mairal, 2013), Razaviyayn, Hong, and Luo (2013) also showed that many popular methods for minimizing non-smooth functions could be regarded as MM methods. They proposed the block coordinate descent method, where the traditional MM could be regarded as a special case by gathering all variables in one block. Different from (Mairal, 2013), they suggested using the directional derivative to ensure the first order smoothness between the objective and the surrogate, which is weaker than the condition in (Mairal, 2013) that the difference between the objective and the surrogate should be smooth. However, Razaviyayn, Hong, and Luo (2013) only discussed the choice of surrogates by approximating f(x) as (3).", "startOffset": 16, "endOffset": 654}, {"referenceID": 19, "context": "Our relaxed MM is general enough to include the existing MM methods (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013).", "startOffset": 68, "endOffset": 115}, {"referenceID": 19, "context": "We can see that ours is general enough to include existing works (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013).", "startOffset": 65, "endOffset": 112}, {"referenceID": 19, "context": "In the traditional MM, the global majorization condition (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013) is assumed:", "startOffset": 57, "endOffset": 104}, {"referenceID": 19, "context": "So a globally majorant surrogate is likely to produce an inferior solution and converges slower than a locally majorant one ((Mairal, 2013) and our experiments).", "startOffset": 125, "endOffset": 139}, {"referenceID": 19, "context": "These two cases have been discussed in the literature (Mairal, 2013; Razaviyayn, Hong, and Luo, 2013).", "startOffset": 54, "endOffset": 101}, {"referenceID": 19, "context": "Mairal (Mairal, 2013)", "startOffset": 7, "endOffset": 21}, {"referenceID": 25, "context": "It is widely used for structure from motion (Tomasi and Kanade, 1992), clustering (Kong, Ding, and Huang, 2011), dictionary learning (Mairal et al.", "startOffset": 44, "endOffset": 69}, {"referenceID": 18, "context": "It is widely used for structure from motion (Tomasi and Kanade, 1992), clustering (Kong, Ding, and Huang, 2011), dictionary learning (Mairal et al., 2010), etc.", "startOffset": 133, "endOffset": 154}, {"referenceID": 14, "context": ", low-rank matrix recovery (Ke and Kanade, 2005), non-negative matrix factorization (NMF) (Lee and Seung, 2001), and dictionary learning (Mairal et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 18, "context": ", low-rank matrix recovery (Ke and Kanade, 2005), non-negative matrix factorization (NMF) (Lee and Seung, 2001), and dictionary learning (Mairal et al., 2010).", "startOffset": 137, "endOffset": 158}, {"referenceID": 1, "context": "In RMF-LMMM, \u03c1u and \u03c1v are instead initialized with relatively small values and then increase gradually, using the line search technique in (Beck and Teboulle, 2009) to ensure the locally majorant condition (7).", "startOffset": 140, "endOffset": 165}, {"referenceID": 5, "context": "When the error is measured by the squared Frobenius norm, many algorithms have been proposed (Buchanan and Fitzgibbon, 2005; Mitra, Sheorey, and Chellappa, 2010).", "startOffset": 93, "endOffset": 161}, {"referenceID": 2, "context": "They minimized U and V alternatively, which could easily get stuck at non-stationary points (Bertsekas, 1999).", "startOffset": 92, "endOffset": 109}, {"referenceID": 26, "context": "Some researches (Zheng et al., 2012; Cabral et al., 2013) further extended that by adding different regularizations on (U, V ) and achieved state-of-the-art performance.", "startOffset": 16, "endOffset": 57}, {"referenceID": 7, "context": "Some researches (Zheng et al., 2012; Cabral et al., 2013) further extended that by adding different regularizations on (U, V ) and achieved state-of-the-art performance.", "startOffset": 16, "endOffset": 57}, {"referenceID": 7, "context": "In this paper, we adopt the same formulation as (Cabral et al., 2013):", "startOffset": 48, "endOffset": 69}, {"referenceID": 4, "context": "When the error is measured by the squared Frobenius norm, many algorithms have been proposed (Buchanan and Fitzgibbon, 2005; Mitra, Sheorey, and Chellappa, 2010). For robustness, Ke and Kanade (2005) proposed to adopt the `1-norm.", "startOffset": 94, "endOffset": 200}, {"referenceID": 2, "context": "They minimized U and V alternatively, which could easily get stuck at non-stationary points (Bertsekas, 1999). So Eriksson and van den Hengel (2012) represented V implicitly with U and extended the Wiberg Algorithm to `1-norm.", "startOffset": 93, "endOffset": 149}, {"referenceID": 2, "context": "They minimized U and V alternatively, which could easily get stuck at non-stationary points (Bertsekas, 1999). So Eriksson and van den Hengel (2012) represented V implicitly with U and extended the Wiberg Algorithm to `1-norm. They only proved the convergence of the objective function value, not the sequence {(Uk, Vk)} itself. Moreover, they had to assume that the dependence of V on U is differentiable, which is unlikely to hold everywhere. Additionally, as it unfolds matrix U into a vector and adopt an () its memory requirement is very high, which prevents it from large scale computation. Recently, ADMM was used for matrix recovery. By assuming that the variables are bounded and convergent, Shen, Wen, and Zhang (2014) proved that any accumulation point of their algorithm is the Karush-Kuhn-Tucker (KKT) point.", "startOffset": 93, "endOffset": 729}, {"referenceID": 5, "context": "where the regularizers \u2016U\u2016F and \u2016V \u2016F are for reducing the solution space (Buchanan and Fitzgibbon, 2005).", "startOffset": 74, "endOffset": 105}, {"referenceID": 22, "context": "Recently, Pan et al. (2014) further introduced the `1-norm to handle outliers in non-negative dictionary learning, resulting in the following model:", "startOffset": 10, "endOffset": 28}, {"referenceID": 12, "context": "However, Gonzalez and Zhang (2005) pointed out that with such a multiplicative updating scheme is hard to reach the convergence condition even on toy data.", "startOffset": 9, "endOffset": 35}, {"referenceID": 7, "context": "Experiments In this section, we compare our relaxed MM algorithms with state-of-the-art RMF algorithms: UNuBi (Cabral et al., 2013) for low rank matrix recovery and `1-NMF (Pan et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 22, "context": ", 2013) for low rank matrix recovery and `1-NMF (Pan et al., 2014) for robust NMF.", "startOffset": 48, "endOffset": 66}, {"referenceID": 7, "context": "The code of UNuBi (Cabral et al., 2013) was kindly provided by the authors.", "startOffset": 18, "endOffset": 39}, {"referenceID": 22, "context": "We implemented the code of `1-NMF (Pan et al., 2014) ourselves.", "startOffset": 34, "endOffset": 52}, {"referenceID": 7, "context": "(a) The locally majorant MM, RMFLMMM, gets better solution in much less iterations than the globally majorant, RMF-GMMM, and the state-of-the-art algorithm, UNuBi (Cabral et al., 2013).", "startOffset": 163, "endOffset": 184}, {"referenceID": 22, "context": "(b) RMF-LMMM outperforms the globally majorant MM, RMF-GMMM, and the state-of-the-art algorithm (also globally majorant), `1NMF (Pan et al., 2014).", "startOffset": 128, "endOffset": 146}, {"referenceID": 25, "context": "Low Rank Matrix Recovery: Tomasi and Kanade (1992) first modelled the affine rigid structure from motion as a rank-4 matrix recovery problem.", "startOffset": 26, "endOffset": 51}, {"referenceID": 7, "context": "(b-d) Full tracks reconstructed by UNuBi (Cabral et al., 2013), RMF-GMMM, and RMF-LMMM, respectively.", "startOffset": 41, "endOffset": 62}, {"referenceID": 22, "context": "Non-negative Matrix Factorization: We test the performance of robust NMF by clustering (Kong, Ding, and Huang, 2011; Pan et al., 2014).", "startOffset": 87, "endOffset": 134}, {"referenceID": 22, "context": "The evaluation metrics we use here are accuracy (ACC), normalized mutual information (NMI) and purity (PUR) (Kong, Ding, and Huang, 2011; Pan et al., 2014).", "startOffset": 108, "endOffset": 155}], "year": 2015, "abstractText": "We propose a new majorization-minimization (MM) method for non-smooth and non-convex programs, which is general enough to include the existing MM methods. Besides the local majorization condition, we only require that the difference between the directional derivatives of the objective function and its surrogate function vanishes when the number of iterations approaches infinity, which is a very weak condition. So our method can use a surrogate function that directly approximates the non-smooth objective function. In comparison, all the existing MM methods construct the surrogate function by approximating the smooth component of the objective function. We apply our relaxed MM methods to the robust matrix factorization (RMF) problem with different regularizations, where our locally majorant algorithm shows great advantages over the state-of-the-art approaches for RMF. This is the first algorithm for RMF ensuring, without extra assumptions, that any limit point of the iterates is a stationary point. Introduction Consider the following optimization problem:", "creator": "TeX"}}}