{"id": "1511.08099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Strategic Dialogue Management via Deep Reinforcement Learning", "abstract": "artificially intelligent agencies equipped with strategic skills that can negotiate during their interactions / other natural or artificial agents are still underdeveloped. this paper describes a successful evaluation of digital reinforcement learning ( cas ) for training intelligent agents on strategic conversational skills, in a situated dialogue setting. previous theories have modelled the behaviour of strategic agents using supervised learning and passive reinforcement learning techniques, the responses using mutual representations or learning with linear function approximation. in this study, we apply agent with a high - dimensional state space to the strategic response game of republic of catan - - - where you can offer resources in exchange for others and they can also reply to offers made by regular players. our experimental results argue that the drl - based learnt policies significantly outperformed several baselines or random, rule - based, or supervised - based behaviours. the drl - based policy implies a 53 % win rate versus 3 automated players ( ` bots'), whereas a mouse defender trained on a dialogue corpus where this setting achieved only 27 %, versus the same 3 bots. this result supports the claim that drl is a promising framework for training security systems, and trusting agents with negotiation abilities.", "histories": [["v1", "Wed, 25 Nov 2015 15:48:59 GMT  (712kb,D)", "http://arxiv.org/abs/1511.08099v1", "NIPS'15 Workshop on Deep Reinforcement Learning"]], "COMMENTS": "NIPS'15 Workshop on Deep Reinforcement Learning", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["heriberto cuay\\'ahuitl", "simon keizer", "oliver lemon"], "accepted": false, "id": "1511.08099"}, "pdf": {"name": "1511.08099.pdf", "metadata": {"source": "CRF", "title": "Strategic Dialogue Management via Deep Reinforcement Learning", "authors": ["Heriberto Cuay\u00e1huitl", "Simon Keizer"], "emails": ["hc213@hw.ac.uk", "s.keizer@hw.ac.uk", "o.lemon@hw.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Artificially intelligent agents can require strategic conversational skills to negotiate during their interactions with other natural or artificial agents, e.g. \u201cA: I will give/tell you X if you give/tell me Y?, B: Okay\u201d. While typical conversations of artificial agents assume cooperative behaviour from partner conversants, strategic conversation does not assume full cooperation during the interaction between agents [2]. Throughout this paper, we will use a strategic card-trading board game to illustrate our approach. Board games with trading aspects aim not only at entertaining people, but also at training them with trading skills. Popular board games of this kind include Last Will, Settlers of Catan, and Power Grid, among others [20]. While these games can be played between humans, they can also be played between computers and humans. The trading behaviours of AI agents in computer games are usually based on carefully tuned rules [33], search algorithms such\nar X\niv :1\n51 1.\n08 09\n9v 1\n[ cs\nas Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25]. However, the application of reinforcement learning is not trivial due to the complexity of the problem, e.g. large state-action spaces exhibited in strategic conversations. On the one hand, unique situations in the interaction can be described by a large number of variables (e.g. game board and resources available) so that enumerating them would result in very large state spaces. On the other hand, the action space can also be large due to the wide range of unique negotiations (e.g. givable and receivable resources). While one can aim for optimising the interaction via compression of the search space, it is usually not clear what features to incorporate in the state representation. This is a strong motivation for applying deep reinforcement learning for dialogue management, as first proposed by (anon citation), so that the agent can simultaneously learn its feature representation and policy. In this paper, we present an application of deep reinforcement learning to learning trading dialogue for the game of Settlers of Catan.\nOur scenario for strategic conversation is the game of Settlers of Catan, where players take the role of settlers on the fictitious island of Catan\u2014see Figure 1(left). The board game consists of 19 hexes randomly connected: 3 hills, 3 mountains, 4 forests, 4 pastures, 4 fields and 1 desert. In this island, hills produce clay, mountains produce ore, pastures produce sheep, fields produce wheat, forests produce wood, and the desert produces nothing. In our setting, four players attempt to settle on the island by building settlements and cities connected by roads. To build, players need specific resource cards, for example: a road requires clay and wood; a settlement requires clay, sheep, wheat and wood; a city requires three clay cards and two wheat cards; and a development card requires clay, sheep and wheat. Each player gets points for example by building a settlement (1 point) or a city (2 points), or by obtaining victory point cards (1 point each). A game consists of a sequence of turns, and each game turn starts with the roll of a die that can make the players obtain resources (depending on the number rolled and resources on the board). The player in turn can trade resources with the bank or through dialogue with other players, and can make use of available resources to build roads, settlements or cities. This game is highly strategic because players often face decisions about when to trade, what resources to request, and what resources to give away\u2014which are influenced by what they need to build. A player can extend build-ups on locations connected to existing pieces, i.e. road, settlement or city, and all settlements and cities must be separated by at least 2 roads. The first player to win 10 victory points wins and all others lose.1\nIn this paper, we extend previous work on strategic conversation that has applied supervised or reinforcement learning in that we simultaneously learn the feature representation and dialogue policy by using Deep Reinforcement Learning (DRL). We compare our learnt policies against random, rule-based and supervised baselines, and show that the DRL-based agents perform significantly better than the baselines."}, {"heading": "2 Background", "text": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximizing a long-term reward signal [29, 30]. An RL agent is typically characterized by: (i) a finite or infinite set of states S = {si}; (ii) a finite or infinite set of actions A = {aj}; (iii) a stochastic state transition function T (s, a, s\u2032) that specifies the next state s\u2032 given the current state s and action a; (iv) a reward function R(s, a, s\u2032) that specifies the reward given to the agent for choosing action a when the environment makes a transition from state s to state s\u2032; and (v) a policy \u03c0 : S \u2192 A that defines a mapping from states to actions. The goal of an RL agent is to select actions by maximising its cumulative discounted reward defined as Q\u2217(s, a) = max\u03c0 E[rt + \u03b3rt+1 + \u03b32rt+1 + ...|st = s, at = a, \u03c0], where function Q\u2217 represents the maximum sum of rewards rt discounted by factor \u03b3 at each time step. While the RL agent takes actions with probability Pr(a|s) during training, it takes the best actions maxa Pr(a|s) at test time. To induce the Q function above we use Deep Reinforcement Learning as in [22], which approximates Q\u2217 using a multilayer convolutional neural network. The Q function of a DRL agent is parameterised as Q(s, a; \u03b8i), where \u03b8i are the parameters (weights) of the neural net at iteration i. More specifically, training a DRL agent requires a dataset of experiences D = {e1, ...eN} (also referred to as \u2018experience replay memory\u2019), where every experience is described as a tuple\n1www.catan.com/service/game-rules\net = (st, at, rt, st+1). Inducing the Q function consists in applying Q-learning updates over minibatches of experience MB = {(s, a, r, s\u2032) \u223c U(D)} drawn uniformly at random from the full dataset D.\nA Q-learning update at iteration i is thus defined as the loss function Li(\u03b8i) = EMB [ (r + \u03b3maxa\u2032 Q(s \u2032, a\u2032; \u03b8i)\u2212Q(s, a; \u03b8i))2 ] , where \u03b8i are the parameters of the neural net at\niteration i, and \u03b8i are the target parameters of the neural net at iteration i. The latter are only updated every C steps. This process is implemented in the learning algorithm Deep Q-Learning with Experience Replay described in [22]."}, {"heading": "3 Policy Learning for Strategic Interaction", "text": "Our approach for strategic interaction optimises two tasks jointly: learning to offer and learning to reply to offers. In addition, our approach learns from constrained search spaces rather than unconstrained ones, resulting in quicker learning and also in learning from only legal (allowed) decisions."}, {"heading": "3.1 Learning to offer and to reply", "text": "A strategic agent has to offer a trade to its opponent agents (or players). In the case of the game of Settlers of Catan, an example trading offer is I will give anyone sheep for clay. Several things can be observed from this simple example. First, note that this offer may include multiple givable and receivable resources. Second, note that the offer is addressed to all opponents (as opposed to one opponent in particular, which could also be possible). Third, note that not all offers are allowed at a particular point in the game \u2013 they depend on the particular state of the game and resources available to the player for trading. The goal of the agent is to learn to make legal offers that will yield the largest pay-off in the long run.\nA strategic agent also has to reply to trading offers made by an opponent. In the case of the game of Settlers of Catan, the responses can be narrowed down to (a) accepting the offer, (b) rejecting it, or (c) replying with a counteroffer (e.g. I want two sheep for one clay). Note that this set of responses is available at any point in the game once there is an offer made by any agent (or player). Similarly to the task above, the goal of the agent is to learn to choose a response that will yield the largest pay-off in the long run.\nWhile one can aim for optimising only one of the tasks above, a joint optimisation of the these two tasks equips an automatic trading agent with more completeness. To do that, given an environment state space S = {si}, trading negotiations Ao, and responses Ar, the goal of a strategic learning agent consists of inducing an optimal policy so that action selection can be defined as \u03c0\u2217(s) = arg maxQ\u2217a\u2208{Ao\u222aAr}(s, a), where the Q function is estimated as described in the previous section, Ao is the set of trading negotiations in turn, and Ar is the set of responses."}, {"heading": "3.2 Deep Learning from constrained action sets", "text": "While the behaviour of a strategic agent can be trained as described above, using deep learning with large action sets can be prohibitively expensive in terms of computation time. Our solution to this limitation consists in learning from constrained action sets rather than whole and static action sets. We distinguish two action sets, an action set Ar which contains responses to trading negotiations and remains static, and an action set Ao which contains those trading negotiations that are valid at any given point in the game (i.e. which the player is able to make due to the resources that they hold). We refer to the latter action set as A\u0304o, which contains a dynamic set |A\u0304o| \u2264 |Ao| of trading negotiations available according to the game state and available resources (e.g. the agent would not offer a particular resource if it does not have it). Thus, we reformulate the goal of a strategic learning agent as inducing an optimal policy so that action selection can be defined as \u03c0\u2217(s) = arg maxQ\u2217\na\u2208A\u0304o\u222aAr (s, a), where theQ function is still estimated as described in Section 2, A\u0304o is the constrained set of trading negotiations in turn (i.e. legal offers), and Ar is the set of responses. Note that the size of A\u0304o will vary depending on the game state."}, {"heading": "4 Experiments and Results", "text": "In this section we apply the approach above to conversational agents that learn to offer and to reply in the game of Settlers of Catan."}, {"heading": "4.1 Experimental Setting", "text": ""}, {"heading": "4.1.1 Integrated learning environment", "text": "Figure 1(left) shows our integrated learning environment. On the left-hand side, the JSettlers benchmark framework [33] receives an action (trading offer or response) and outputs the next game state and numerical reward. On the right-hand side, a Deep Reinforcement Learning (DRL) agent receives the state and reward, updates its policy during learning, and outputs an action following its learnt policy. Our integrated system is based on a multi-threaded implementation, where each player makes use of a synchronised thread. In addition, this system runs under a client-server architecture, where the learning agent acts as the \u2018server\u2019 and the game acts as the \u2018client\u2019. They communicate by exchanging messages, where the server tells the client the action to execute, and the client tells the server the game state and reward observed. Our DRL agents are based on the ConvNetJS tool [15], which implements the algorithm \u2018Deep Q-Learning with experience replay\u2019 proposed by [22]. We extended this tool to support multi-threaded and client-server processing with constrained search spaces.2"}, {"heading": "4.1.2 Characterisation of the learning agent", "text": "The state space S = {si} of our learning agent includes 160 non-binary features that describe the game board and the available resources. Table 1 describes the state variables that represent the input nodes, which we normalise to the range [0..1]. These features represent a high-dimensional state space\u2014only approachable via reinforcement learning with function approximation.\n2The code of this substantial extension with an illustrative dialogue system is available at the following link: https://github.com/cuayahuitl/SimpleDS\nThe action space A = {ai} of our learning agents includes 70 actions for offering trading negotiations3 and 3 actions4 for replying to offers from opponents. Notice that our offer actions only make use of up to two givable resources and only one receivable resource is considered.\nThe state transition function of our agents is based on the game itself using the JSettlers framework [33]. In addition, our strategic interactions were carried out at the semantic level rather than at the word level, for example: S4C is a higher-level representation of \u201cI will give you sheep for clay\u201d. Furthermore, our trained agents were active only during the selection of trading offers and reply to offers, the functionality of the rest of the game was based on the JSettlers framework.\nThe reward function of our agent is based on the game points provided by the JSettlers framework, but we make a distinction between reply actions and offer actions. This is due to the fact that we consider reply actions as high-level actions, and offer actions as lower-level ones. Our reward function is defined as:\nr = { GainedPoints\u00d7 wgp if GainedPoints > 0 TotalPoints\u00d7 wtp otherwise,\nwhereGainedPoints=points at time tminus the points at time t\u22121, and TotalPoints refers to the accumulated number of points of the trained agent during the game. We used the following weights for reply actions: {wgp = 1, wtp = 0.1}, and the following for offer actions: {wgp = 0.1, wtp = 0.01}. The model architecture consists of a fully-connected multilayer neural network with 160 nodes in the input layer (see Table 1), 50 nodes in the first hidden layer, 50 nodes in the second hidden layer, and 73 nodes (action set) in the output layer. The hidden layers use RELU (Rectified Linear Units) activation functions to normalise their weights, see [23] for details. Finally, the learning parameters are as follows: experience replay size=30K, discount factor=0.7, minimum epsilon=0.05, learning rate=0.001, and batch size=64. A comprehensive analysis comparing multiple state representations, action sets, reward functions and learning parameters is left for future work."}, {"heading": "4.2 Experimental Results", "text": "We use the following baselines to compare our trained strategic agents, where we only switch the trading offers and reply behaviours\u2014the remaining behaviour of the game remains constant and is provided by the JSettlers framework:\n3Trading negotiation actions, where C=clay, O=ore, S=sheep, W=wheat, and D = wood: C4D, C4O, C4S, C4W, CC4D, CC4O, CC4S, CC4W, CD4O, CD4S, CD4W, CO4D, CO4S, CO4W, CS4D, CS4O, CS4W, CW4D, CW4O, CW4S, D4C, D4O, D4S, D4W, DD4C, DD4O, DD4S, DD4W, O4C, O4D, O4S, O4W, OD4C, OD4S, OD4W, OO4C, OO4D, OO4S, OO4W, OS4C, OS4D, OS4W, OW4C, OW4D, OW4S, S4C, S4D, S4O, S4W, SD4C, SD4O, SD4W, SS4C, SS4D, SS4O, SS4W, SW4C, SW4D, SW4O, W4C, W4D, W4O, W4S, WD4C, WD4O, WD4S, WW4C, WW4D, WW4O, WW4S. Example trade: C4D=clay for wood.\n4Reply actions: accept, reject and counteroffer\n\u2022 Ran: This agent chooses trading negotiation offers randomly, and replies to offers from opponents also in a random fashion. Although this is a weak baseline, we use it to analyse the impact of policies trained (and tested) against random behaviour.\n\u2022 Heu: This agent chooses trading negotiation offers and replies to offers from opponents as dictated by the heuristic bots included in the JSettlers framework5, see [34, 13] for details.\n\u2022 Sup: This agent chooses trading negotiation offers using a random forest classifier [3, 14], and replies to offers from opponents using the heuristic behaviour above. This agent was trained from 32 games played between 56 different human players\u2014labelled by multiple annotators. We compute the probability distribution of a human-like trade as P (givable|evidence) = 1 Z \u220f b\u2208B Pb(givable|evidence), where givable refers to the class prediction (in our case, the\ngivable resource), evidence refers to observed features6, Pb(.|.) is the posterior distribution of the bth tree, and Z is a normalisation constant [4]. This classifier used 100 decision trees. Assuming that Y is a set of givables at a particular point in time in the game, extracting the most human-like trading offer (givable y\u2217) given collected evidence (context of the game), is defined as y\u2217 = arg maxy\u2208Y Pr(y|evidence). The classification accuracy of this statistical classifier was 65.7%\u2014according to a 10-fold cross-validation evaluation [5, 6].\nWe trained three DRL agents against random, heuristic and supervised opponents\u2014see Figure 2, which used 500K training experiences (around 2000 games each learning curve). We evaluate the learnt policies according to a cross-evaluation using the following metrics in terms of averages per game (using 10 thousand test games per comparison): win-rate, victory points, (successful) offers, total trades, pieces built, cards bought, and number of turns. Our observations of the crossevaluation, reported in Table 2, are as follows:\n1. The DRL agents acquire very competitive strategic behaviour in comparison to the other types of agents\u2014they simply win substantially more than their opponents. While random behaviour is easy to beat with over 98% win-rate, the DRL agents achieve over 50% of win-rate against heuristic opponents and over 40% against supervised opponents. These results substantially outperform the heuristic and supervised agents which achieve less than 30% of win-rate (at p < 0.05 according to a two-tailed Wilcoxon-Signed Rank Test).\n5The baseline trading agent referred to as \u2018heuristic\u2019 included the following parameters, see [13]: TRY N BEST BUILD PLANS:0, FAVOUR DEV CARDS:-5.\n6Evidence: Number of resources available, number of builds (roads, settlements and cities), and the resource received.\n2. The DRL agents outperform the baselines not just in win-rates but also in other metrics such as average victory points, pieces built and total trades. The latter is more prominent, for example, while the heuristic and supervised agents achieve between 270 to 280 trades per game, the DRL agents compared against heuristic and supervised agents achieve between 340 and 360 trades. This means that the DRL agents tend to trade more than their opponents, i.e. they accept more offered trading negotiations. These differences suggest that knowing when to accept, reject or counter offer a trading negotiation is crucial for winning.\n3. Training a DRL agent in the environment where it will be tested is better than training and testing across environments. For example, DRLheu versus heuristic behaviour is better (53.4% winrate) than DRLsup versus heuristic behaviour (50.3% win-rate). However, our results report that DRL agents trained using randomly behaving opponents are almost as good as those trained with stronger opponents. This suggests that DRL agents for strategic interaction can be also be trained without highly skilled opponents, presumably by tracking their rewards over time.\n4. The DRL agents find the supervised agent harder to beat. This is because the supervised agent is the strongest baseline, which achieves the best winning rate of the baseline agents. It can be noted that the DRL agents versus supervised behaviour make more offers and trade more than\nthe DRL agents versus heuristic behaviour. We can infer from this result that knowing when to offer and when to trade seem crucial for better winning rates.\n5. The fact that the agent with random behaviour hardly wins any games, suggests that sequential decision-making in this strategic game is far from trivial.\nIn summary, strategic dialogue agents trained with deep reinforcement learning have the potential to acquire highly competitive behaviour, not just from training against strong opponents but even from opponents with random behaviour. This result may help to reduce the resources (heuristics or labelled data) required for training future strategic agents."}, {"heading": "5 Related Work", "text": "Reinforcement learning applied to strategic interaction includes the following. [32] proposes reinforcement learning with multilayer neural networks for training an agent to play the game of Backgammon. He finds that agents trained with such an approach are able to match and even beat human performance. [26] proposes hierarchical reinforcement learning for automatic decision making on object-placing and trading actions in the game of Settlers of Catan. He incorporates builtin knowledge for learning the behaviours of the game quicker, and finds that the combination of learned and built-in knowledge is able to beat human players. [11] used reinforcement learning in non-cooperative dialogue, and focus on a small 2-player trading problem with 3 resource types, but without using any real human dialogue data. This work showed that explicit manipulation moves (e.g. \u201cI really need sheep\u201d) can be used to win when playing against adversaries who are gullible (i.e. they believe such statements) but also against adversaries who can detect manipulation and can punish the player for being manipulative [10]. More recently, [16] designed an MDP model for selecting trade offers, trained and evaluated within the full jSettlers environment (4 players, 5 resource types). In comparison to the DRL model, it had a much more restricted state-action space, leading to significant, but more modest improvements over supervised learning and hand-coded baselines.\nOther related work has been carried out in the context of automated non-cooperative dialogue systems, where an agent may act to satisfy its own goals rather than those of other participants [12]. The game-theoretic underpinnings of non-cooperative behaviour have also been investigated [1]. Such automated agents are of interest when trying to persuade, argue, or debate, or in the area of believable characters in video games and educational simulations [12, 28]. Another arena in which strategic conversational behaviour has been investigated is negotiation [35], where hiding information (and even outright lying) can be advantageous.\nRecent work on deep learning applied to games include the following. [19] train a deep convolutional network for the game of Go, but it is trained in a supervised fashion rather than trained to maximise a long-term reward as in this work. A closely related work to ours is a DRL agent for text-based games [24]. Their states are based on words, their policies are induced using gamebased rewards, and their actions are based on directions such as \u2018go east/west/south/north\u2019. Another closely related work to ours is DRL agents trained to play ATARI games [21]. Their states are based on pixels from down-sampled images, their policies make use of game-based rewards, and their actions are based on joystick movements. In contrast to these previous works which are based on navigation commands, our agents are use trading dialogue moves (e.g. \u2018I will give you ore and sheep for clay\u2019, or \u2018I accept/decline your offer\u2019), which are essential behaviours for strategic interaction.\nThis paper extends the recent work above on training strategic agents using reinforcement learning, which have either used small state-action spaces or focused on navigation commands rather than negotiation dialogue. The learning agents described in this paper use a high dimensional state representation (160 non-binary features) and a fairly large action space (73 actions) for learning strategic non-cooperative dialogue behaviour. To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see [13, 16, 9]. The comprehensive evaluation reported in the previous section is evidence to argue that deep reinforcement learning is a promising framework for training strategic interactive agents."}, {"heading": "6 Concluding Remarks", "text": "The contribution of this paper is the first application of Deep Reinforcement Learning (DRL) to optimising the behaviour of strategic conversational agents. Our learning agents are able to: (i) discover what trading negotiations to offer, (ii) discover when to accept, reject, or counteroffer; (iii) discover strategic behaviours based on constrained action sets\u2014i.e. action selection from legal actions rather than from all of them; and (iv) learn highly competitive behaviour against different types of opponents. All of this is supported by a comprehensive evaluation of three DRL agents trained against three baselines (random, heuristic and supervised), which are analysed from a crossevaluation perspective. Our experimental results report that all DRL agents substantially outperform all the baseline agents. Our results are evidence to argue that DRL is a promising framework for training the behaviour of complex strategic interactive agents.\nFuture work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features [18] and forms of learning [7, 27]. In addition, a comparison of different model architectures, training parameters and reward functions can be explored in future work. Last but not least, given that our learning agents trade at the semantic level, they can be extended with language understanding/generation abilities to communicate verbally [17, 8]."}, {"heading": "Acknowledgments", "text": "Funding from the European Research Council (ERC) project \u201cSTAC: Strategic Conversation\u201d no. 269427 is gratefully acknowledged, see http://www.irit.fr/STAC/. Funding from the ESPRC, project EP/M01553X/1 \u201cBABBLE\u201d is gratefully acknowledged, see https://sites. google.com/site/hwinteractionlab/babble."}], "references": [{"title": "Commitments, beliefs and intentions in dialogue", "author": ["N. Asher", "A. Lascarides"], "venue": "Proc. of SemDial,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Strategic conversation", "author": ["N. Asher", "A. Lascarides"], "venue": "Semantics and Pragmatics, 6(2):1\u201362,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["A. Criminisi", "J. Shotton", "E. Konukoglu"], "venue": "Foundations and Trends in Computer Graphics and Vision, 7(2-3),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to trade in strategic board games", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "IJCAI Workshop on Computer Games (IJCAI-CGW),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning trading negotiations using manually and automatically labelled data", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "International Conference on Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning for interactive systems and robots: A brief introduction. In Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication, MLIS", "author": ["H. Cuay\u00e1huitl", "M. van Otterlo", "N. Dethlefs", "L. Frommberger"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Hierarchical reinforcement learning for situated natural language generation", "author": ["N. Dethlefs", "H. Cuay\u00e1huitl"], "venue": "Natural Language Engineering, 21, 5", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning and mining human play in complex games", "author": ["M.S. Dobre", "A. Lascarides"], "venue": "IEEE Conference on Computational Intelligence and Games, CIG,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to manage risk in non-cooperative dialogues", "author": ["I. Efstathiou", "O. Lemon"], "venue": "SemDial,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning non-cooperative dialogue behaviours", "author": ["I. Efstathiou", "O. Lemon"], "venue": "SIGDIAL,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement learning of argumentation dialogue policies in negotiation", "author": ["K. Georgila", "D. Traum"], "venue": "Proc. of INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Game strategies for The Settlers of Catan", "author": ["M. Guhe", "A. Lascarides"], "venue": "2014 IEEE Conference on Computational Intelligence and Games, CIG,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "The elements of statistical learning: data mining, inference and prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer, 2 edition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "ConvNetJS: Javascript library for deep learning", "author": ["A. Karpathy"], "venue": "http://cs.stanford.edu/people/karpathy/convnetjs/,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Trade Negotiation Policies in Strategic Conversation", "author": ["S. Keizer", "H. Cuay\u00e1huitl", "O. Lemon"], "venue": "Workshop on the Semantics and Pragmatics of Dialogue: goDIAL,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive Natural Language Generation in Dialogue using Reinforcement Learning", "author": ["O. Lemon"], "venue": "Proc. SEMDIAL,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Can automated agents proficiently negotiate with humans? Commun", "author": ["R. Lin", "S. Kraus"], "venue": "ACM, 53(1), Jan.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Move evaluation in go using deep convolutional neural networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "CoRR, abs/1412.6564,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "10 great board games for traders", "author": ["M. McFarlin"], "venue": "Futures Magazine,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533, 02", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807\u2013814,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, September", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies", "author": ["A. Papangelis", "K. Georgila"], "venue": "Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGdial),", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning of strategies for settlers of catan", "author": ["M. Pfeiffer"], "venue": "International Conference on Computer Games: Artificial Intelligence, Design and Education,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Machine learning for interactive systems: Challenges and future trends", "author": ["O. Pietquin", "M. Lopez"], "venue": "Proceedings of the Workshop Affect, Compagnon Artificiel (WACAI),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "A Taxonomy of Robot Deception and its Benefits in HRI", "author": ["J. Shim", "R. Arkin"], "venue": "Proc. IEEE Systems, Man, and Cybernetics Conference,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Morgan and Claypool Publishers,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Monte-carlo tree search in settlers of catan", "author": ["I. Szita", "G. Chaslot", "P. Spronck"], "venue": "Proceedings of the 12th International Conference on Advances in Computer Games, ACG\u201909,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Temporal difference learning and TD-Gammon", "author": ["G. Tesauro"], "venue": "Commun. ACM, 38(3),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1995}, {"title": "Java Settlers: a research environment for studying multi-agent negotiation", "author": ["R. Thomas", "K.J. Hammond"], "venue": "Intelligent User Interfaces (IUI), pages 240\u2013240,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Real-time decision making for adversarial environments using a plan-based heuristic", "author": ["R.S. Thomas"], "venue": "PhD thesis, Northwestern University,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Extended abstract: Computational models of non-cooperative dialogue", "author": ["D. Traum"], "venue": "Proc. of SIGdial Workshop on Discourse and Dialogue,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "While typical conversations of artificial agents assume cooperative behaviour from partner conversants, strategic conversation does not assume full cooperation during the interaction between agents [2].", "startOffset": 198, "endOffset": 201}, {"referenceID": 19, "context": "Popular board games of this kind include Last Will, Settlers of Catan, and Power Grid, among others [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "The trading behaviours of AI agents in computer games are usually based on carefully tuned rules [33], search algorithms such", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 27, "endOffset": 34}, {"referenceID": 8, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 27, "endOffset": 34}, {"referenceID": 11, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 92, "endOffset": 100}, {"referenceID": 10, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 92, "endOffset": 100}, {"referenceID": 25, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 24, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 28, "context": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximizing a long-term reward signal [29, 30].", "startOffset": 219, "endOffset": 227}, {"referenceID": 29, "context": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximizing a long-term reward signal [29, 30].", "startOffset": 219, "endOffset": 227}, {"referenceID": 21, "context": "To induce the Q function above we use Deep Reinforcement Learning as in [22], which approximates Q\u2217 using a multilayer convolutional neural network.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "This process is implemented in the learning algorithm Deep Q-Learning with Experience Replay described in [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "(left) GUI of the board game \u201cSettlers of Catan\u201d [33].", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "On the left-hand side, the JSettlers benchmark framework [33] receives an action (trading offer or response) and outputs the next game state and numerical reward.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "Our DRL agents are based on the ConvNetJS tool [15], which implements the algorithm \u2018Deep Q-Learning with experience replay\u2019 proposed by [22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "Our DRL agents are based on the ConvNetJS tool [15], which implements the algorithm \u2018Deep Q-Learning with experience replay\u2019 proposed by [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 32, "context": "The state transition function of our agents is based on the game itself using the JSettlers framework [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "The hidden layers use RELU (Rectified Linear Units) activation functions to normalise their weights, see [23] for details.", "startOffset": 105, "endOffset": 109}, {"referenceID": 33, "context": "\u2022 Heu: This agent chooses trading negotiation offers and replies to offers from opponents as dictated by the heuristic bots included in the JSettlers framework5, see [34, 13] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 12, "context": "\u2022 Heu: This agent chooses trading negotiation offers and replies to offers from opponents as dictated by the heuristic bots included in the JSettlers framework5, see [34, 13] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 2, "context": "\u2022 Sup: This agent chooses trading negotiation offers using a random forest classifier [3, 14], and replies to offers from opponents using the heuristic behaviour above.", "startOffset": 86, "endOffset": 93}, {"referenceID": 13, "context": "\u2022 Sup: This agent chooses trading negotiation offers using a random forest classifier [3, 14], and replies to offers from opponents using the heuristic behaviour above.", "startOffset": 86, "endOffset": 93}, {"referenceID": 3, "context": ") is the posterior distribution of the bth tree, and Z is a normalisation constant [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "7%\u2014according to a 10-fold cross-validation evaluation [5, 6].", "startOffset": 54, "endOffset": 60}, {"referenceID": 5, "context": "7%\u2014according to a 10-fold cross-validation evaluation [5, 6].", "startOffset": 54, "endOffset": 60}, {"referenceID": 12, "context": "The baseline trading agent referred to as \u2018heuristic\u2019 included the following parameters, see [13]: TRY N BEST BUILD PLANS:0, FAVOUR DEV CARDS:-5.", "startOffset": 93, "endOffset": 97}, {"referenceID": 31, "context": "[32] proposes reinforcement learning with multilayer neural networks for training an agent to play the game of Backgammon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] proposes hierarchical reinforcement learning for automatic decision making on object-placing and trading actions in the game of Settlers of Catan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] used reinforcement learning in non-cooperative dialogue, and focus on a small 2-player trading problem with 3 resource types, but without using any real human dialogue data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "they believe such statements) but also against adversaries who can detect manipulation and can punish the player for being manipulative [10].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "More recently, [16] designed an MDP model for selecting trade offers, trained and evaluated within the full jSettlers environment (4 players, 5 resource types).", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "Other related work has been carried out in the context of automated non-cooperative dialogue systems, where an agent may act to satisfy its own goals rather than those of other participants [12].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "The game-theoretic underpinnings of non-cooperative behaviour have also been investigated [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "Such automated agents are of interest when trying to persuade, argue, or debate, or in the area of believable characters in video games and educational simulations [12, 28].", "startOffset": 164, "endOffset": 172}, {"referenceID": 27, "context": "Such automated agents are of interest when trying to persuade, argue, or debate, or in the area of believable characters in video games and educational simulations [12, 28].", "startOffset": 164, "endOffset": 172}, {"referenceID": 34, "context": "Another arena in which strategic conversational behaviour has been investigated is negotiation [35], where hiding information (and even outright lying) can be advantageous.", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "[19] train a deep convolutional network for the game of Go, but it is trained in a supervised fashion rather than trained to maximise a long-term reward as in this work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "A closely related work to ours is a DRL agent for text-based games [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Another closely related work to ours is DRL agents trained to play ATARI games [21].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see [13, 16, 9].", "startOffset": 118, "endOffset": 129}, {"referenceID": 15, "context": "To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see [13, 16, 9].", "startOffset": 118, "endOffset": 129}, {"referenceID": 8, "context": "To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see [13, 16, 9].", "startOffset": 118, "endOffset": 129}, {"referenceID": 17, "context": "Future work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features [18] and forms of learning [7, 27].", "startOffset": 178, "endOffset": 182}, {"referenceID": 6, "context": "Future work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features [18] and forms of learning [7, 27].", "startOffset": 205, "endOffset": 212}, {"referenceID": 26, "context": "Future work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features [18] and forms of learning [7, 27].", "startOffset": 205, "endOffset": 212}, {"referenceID": 16, "context": "Last but not least, given that our learning agents trade at the semantic level, they can be extended with language understanding/generation abilities to communicate verbally [17, 8].", "startOffset": 174, "endOffset": 181}, {"referenceID": 7, "context": "Last but not least, given that our learning agents trade at the semantic level, they can be extended with language understanding/generation abilities to communicate verbally [17, 8].", "startOffset": 174, "endOffset": 181}], "year": 2015, "abstractText": "Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan\u2014where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53% win rate versus 3 automated players (\u2018bots\u2019), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities.", "creator": "LaTeX with hyperref package"}}}