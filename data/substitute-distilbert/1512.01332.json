{"id": "1512.01332", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Q-Networks for Binary Vector Actions", "abstract": "in this paper reinforcement learning with binary vector actions was introduced. we suggest such effective architecture of the neural networks therefore approximating an action - value function with binary vector actions. the proposed architecture approximates the action - value function by reducing linear function with respect to the action vector, but is still non - linear upon regard to the state input. you show that this approximation method enables the efficient calculation of greedy action selection and softmax action selection. using simulation architecture, we suggest explicit online algorithm modelled on code - learning. the empirical results in the grid world and the blocker tree suggest that our approximation architecture would be effective from the rl problems with large discrete action sets.", "histories": [["v1", "Fri, 4 Dec 2015 07:51:48 GMT  (832kb)", "http://arxiv.org/abs/1512.01332v1", "9 pages, 5 figures, accepted for Deep Reinforcement Learning Workshop, NIPS 2015"]], "COMMENTS": "9 pages, 5 figures, accepted for Deep Reinforcement Learning Workshop, NIPS 2015", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["naoto yoshida"], "accepted": false, "id": "1512.01332"}, "pdf": {"name": "1512.01332.pdf", "metadata": {"source": "CRF", "title": "Q-Networks for Binary Vector Actions\u2217", "authors": ["Naoto Yoshida"], "emails": ["naotoyoshida@pfsl.mech.tohoku.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n01 33\n2v 1\n[ cs\n.N E\n] 4\nD ec\nIn this paper reinforcement learning with binary vector actions was investigated. We suggest an effective architecture of the neural networks for approximating an action-value function with binary vector actions. The proposed architecture approximates the action-value function by a linear function with respect to the action vector, but is still non-linear with respect to the state input. We show that this approximation method enables the efficient calculation of greedy action selection and softmax action selection. Using this architecture, we suggest an online algorithm based on Q-learning. The empirical results in the grid world and the blocker task suggest that our approximation architecture would be effective for the RL problems with large discrete action sets."}, {"heading": "1 Introduction", "text": "One of the big challenges in reinforcement learning (RL) is learning in high dimensional state-action spaces. Recent advances in deep learning technologies have enabled us to treat RL problems with the high-dimensional state space, and it achieved an impressive result in general game playing tasks (e.g. ATARI game plays) [1].\nEven though several approaches are suggested for RL with continuous actions [2][3], RL with a large action space is still problematic, especially when we treat binary vectors as representations of the actions. The difficulty is that the number of actions exponentially grows as the length of the binary vector grows. Recently, several approaches have been used to tackle this problem. Sallans & Hinton suggested an energy-based approach in which restricted Boltzmann machines [4] were adopted in the algorithm and their free energy was used as the function approximator [5]. Heess et al. followed their energy-based approach and investigated natural actor-critic algorithms with energybased policies by RBMs [6]. Although energy-based approaches are known to be effective in large discrete domains, exact action sampling is intractable due to the nonlinearity of the approximation architecture. Hence, an energy-based approach samples actions by Gibbs sampling.\nHowever, the Gibbs sampling-based action selection is computationally expensive and requires careful tuning of the parameters. Also, because of the intractability of the exact sampling of greedy actions, no Q-learning-based online off-policy RL algorithm has so far been proposed for the large discrete action domain. From this background, we treat this issue and suggest novel architecture for the off-policy RL with the a large discrete action set.\n\u2217This paper was accepted at Deep Reinforcement Learning Workshop, NIPS 2015\n1"}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Markov Decision Process and Reinforcement Learning", "text": "The value-based reinforcement learning algorithms utilize the Markov decision process (MDP) assumption. The MDP is defined by a tuple \u3008S,A, P,R\u3009. S is the state set, A is the action set, P is the transition probability P (s\u2032|s, a), where s\u2032 \u2208 S is the next state given a state-action pair (s, a). Finally R is the average reward function R(s, a) = E[r|s, a] and r is the reward sample. In the value-based RL, the action-value function Q\u03c0(s, a) is defined by\nQ\u03c0(s, a) = E\u03c0\n[ \u221e \u2211\nt=0\n\u03b3trt\n\u2223 \u2223 \u2223 s0 = s, a0 = a ] , (1)\nhere, 0 \u2264 \u03b3 < 1 is the discount factor. In value-based RL, we look for the optimal policy \u03c0\u2217 that maximizes the action-values for every state-action pair. Q-learning is an algorithm for finding the optimal policy in MDP [7], and the advantage of Q-learning is its off-policy property: the agent can directly approximate the action-value of an optimal policy \u03c0\u2217 while following the other policy \u03c0.\nAlthough Q-learning is guaranteed to approximate optimal action-values when we use the tabular functions in a discrete state-action environment [8], tabular function-based approaches become quickly inefficient for RL with large state-action spaces. Then, function approximations become necessary in such domains."}, {"heading": "2.2 Q-learning with Function Approximation", "text": "In the Q-learning algorithm with function approximations, we approximate the optimal value function by the function Q\u03b8(s, a) where \u03b8 is the parameter of the function.\nThe gradient-based update of the function Q\u03b8(s, a) calculates the gradient of the error function\nL = 1\n2 (T \u2212Q\u03b8(s, a))2, (2)\nwhere T is the target signal. Then, the gradient of the error is obtained by\n\u2202L \u2202\u03b8 = \u2212(T \u2212Q\u03b8(s, a)) \u2202Q\u03b8(s, a) \u2202\u03b8 . (3)\nThe target signal in the Q-learning is T = r + \u03b3maxa\u0302 Q\u03b8(s\u2032, a\u0302) given a transition sample {s, a, r, s\u2032}. Then the direction of the parameter update \u2206\u03b8 is given by\n\u2206\u03b8 = \u2212\u2202L \u2202\u03b8\n(4)\n= (\nr + \u03b3max a\u0302\nQ\u03b8(s \u2032, a\u0302)\u2212Q\u03b8(s, a)\n)\u2202Q\u03b8(s, a)\n\u2202\u03b8 . (5)\nThe first term of the product in the second equality is called the TD error. Using this gradient, the stochastic gradient descent or more sophisticated gradient-based algorithms are used for approximating the optimal action-value function [9][10].\nThe Q-learning-based gradient requires the max operation of Q\u03b8(s, a) given a state. In the previous research with small discrete action sets, this max operation were tractable. However, if the actions are composed of binary vectors or factored representation [5][11], the number of total actions exponentially grows and quickly become intractable."}, {"heading": "3 Proposed Method", "text": "In this study, we assume that the function approximation is done by the multi-layer perceptrons (MLPs) parameterized by \u03b8. To efficiently calculate the max operations in Q-learning with a large\n2\ndiscrete action space, we propose the network architecture of MLPs shown in Figure 1. In this architecture, the outputs of the network are composed of a continuous scalar variable \u03a8\u03b8(s) and continuous vector variable \u03c6\u03b8(s). In this study, we approximate the action-value function by the linear function with respect to the action vector:\nQ\u03b8(s, a) = \u03a8\u03b8(s) + K \u2211\ni=1\nai\u03c6 i \u03b8(s) (6)\n= \u03a8\u03b8(s) + a \u22a4\u03c6\u03b8(s) (7)\nHere, a is the action represented by the binary vector, and ai is the i-th component of the action.\nThe gradient of the function Q\u03b8(s, a) is given by\n\u2202Q\u03b8(s, a)\n\u2202\u03b8 =\n\u2202\u03a8\u03b8(s)\n\u2202\u03b8 +\nK \u2211\ni=1\nai \u2202\u03c6i\u03b8(s)\n\u2202\u03b8 , (8)\nand this is efficiently obtained by the back propagation algorithm."}, {"heading": "3.1 Sampling of the Actions", "text": "The proposed approximation architecture provides an efficient calculation of the greedy action. For actions with the one-hot representation, the greedy policy is obvious. This is\n\u03c0greedy(s) = argmax a\u2208{1,...K} Q\u03b8(s, a) (9)\n= argmax i\u2208{1,...K}\n\u03c6i\u03b8(s), (10)\nwhere \u03c6i\u03b8(s) is the i-th element of the outputs \u03c6\u03b8(s).\nFor the K-bits binary vector actions, sampling of the greedy actions with respect to the function 7 is still tractable. The i-th element of the greedy action vector is given by\n\u03c0igreedy(s) =\n{\n0 \u03c6i\u03b8(s) < 0 1 otherwise. (11)\nBecause we can efficiently sample the greedy action, the \u01eb-greedy action selection is tractable in our case. In the experiment section, we tested some variants of the \u01eb-greedy action selection.\nThe exact sampling from the softmax action selection for binary vector actions is also tractable. Substituting the equation 7 into the conventional softmax policy with the inverse temperature \u03b2 > 0\n3\ngives the equality\n\u03c0(a|s) = e \u03b2Q\u03c6(s,a) \u2211\na\u2032\u2208A e \u03b2Q\u03c6(s,a\u2032)\n(12)\n= e\u03b2\n\u2211K i=1 ai\u03c6 i \u03b8(s)\n\u2211 a\u2032\u2208A e \u03b2 \u2211 K i=1 a \u2032 i \u03c6i \u03b8 (s)\n(13)\n=\nK \u220f\ni=1\ne\u03b2ai\u03c6 i \u03b8(s)\n\u2211\na\u2032 i \u2208{0,1} e\n\u03b2a\u2032 i \u03c6i \u03b8 (s)\n(14)\n=\nK \u220f\ni=1\n\u03c0i(ai|s), (15)\nwhere \u03c0i(ai|s) is the bernoulli distribution for the i-th element of the action. The firing probability of the i-th bit of the action is given by the logistic function\n\u03c0i(ai = 1|s) = 1\n1 + e\u2212\u03b2\u03c6 i \u03b8 (s)\n. (16)\nWhen the environment is represented by the factored MDP [5][11], the action may be represented by the binary vector, which is composed of a concatenation of one-hot representation vectors (for example, the agent may have to decide one of 2 options and one of 3 options simultaneously. In this case, if the agent takes the first option and third option, an action is represented as a 5-bit vector (1, 0| 0, 0, 1)\u22a4). The greedy action for the factored environment is given by\n\u03c0jgreedy(s) = argmax i\u2208{1,...Kj} \u03c6ij\u03b8 (s), (17)\nwhere j is the index of the factored action sets, and Kj is the size of the j-th action set. Following a similar transformation of the equation 15, the softmax policy for the factored action is given as\n\u03c0(a|s) = \u220f\nj=1\n\u03c0j(a j |s), (18)\nand \u03c0j(aj |s) is the softmax function with respect to the j-th factored action set\n\u03c0j(a j i = 1|s) =\ne\u03b2\u03c6 ij \u03b8 (s)\n\u2211Kj i=1 e \u03b2\u03c6 ij \u03b8 (s)\n. (19)"}, {"heading": "4 Experiment", "text": "In the experiment, we tested our proposed architecture in several domains. In all of the experiments, we used the three-layer MLPs described in Figure 1. We also set the activation function of the hidden units using the rectifier linear units (ReLU). All weights connected with output units are sampled from the uniform distribution over [\u22120.01, 0.01], and all weights between input units and hidden units are sampled from the uniform distribution over [\u2212 \u221a 6/ \u221a Nhidden +Ninput, \u221a 6/ \u221a\nNhidden +Ninput], where Nhidden and Ninput are the number of units in the layers. The update of the parameter was done by the stochastic gradient descent with a constant step size \u03b1 = 0.01. The discount rate of the objective function in RL is also same in the all of the experiments, so we used \u03b3 = 0.95."}, {"heading": "4.1 Grid World with One-hot Representation", "text": "First, we tested our algorithm in the conventional grid world with a one-hot representation. This task is the shortest-path problem in the grid world, as suggested by Sutton & Barto [12]. The state\n4\nspace is composed of 47 discrete states and they are given by the one-hot representation. The agent has 4 discrete actions that correspond to the 4 direction moves (North, South, East, West). The action in this experiment is represented by a one-hot representation (for example, the \u201cNorth\u201d action corresponds to the vector (1, 0, 0, 0)\u22a4). The agent receives a zero reward when the agent reaches the goal, but otherwise receives a \u22121 reward. The agent was trained in an episodic manner, a single episode was terminated when the agent reached the goal or passed 800 time steps in the episode. The agents were implemented by MLPs with 50 hidden units. The \u01eb-greedy policy was used as the behavior policy. In this task, we used \u01eb = 0.1.\nThe left panel of Figure 3 is the result of the experiment. The horizontal axis represents the number of episodes, the vertical axis is the step size in the episode. The black line is the mean performance of 10 runs and the bars are standard deviations. The broken line is the optimal step size. As expected, the agent successfully obtained the optimal policy."}, {"heading": "4.2 Grid World with 4-bit Binary Vector Actions", "text": "Action Binary Vector\nNorth 1,1,0,0\nSouth 0,0,1,1\nEast 1,0,1,0\nWest 0,1,0,1\nStay otherwise\nRight panel of Figure 3 The horizontal axis represents the number of episodes, the vertical axis is the step size in the episode. The black line is the mean performance of 10 runs and the bars are standard deviations. The broken line is the optimal step size. Again, the agent successfully obtained the optimal policy through the experiment even in the binary vector action domain. This result shows that the proposed method successfully improved the behavior of the agent without any Monte-Calro based samplings of the actions, even when the representation of actions is not a one-hot representation."}, {"heading": "4.3 Grid World with Population Coding", "text": "Again, the task is the shortest path problem in the same grid world. The state representation, the reward function and termination rules of a single episode are the same as in the previous experiments. In this experiment, the action is represented by a 40-bit binary vector. And the moves of the agent\n5\nare driven according to the type of population coding. Concretely, when the environment receives a 40-bit vector, one of the four-direction moves (1: North; 2: South; 3: East; 4: West) or the stay behavior (5: Stay) occurs according to the probability\nPj = Ej\n\u22115 k=1 Ek\nj = 1, 2, 3, 4, 5, (20)\nwhere Ej are give by the action ai \u2208 {0, 1}, i = 1, . . . , 40 following the equations\nE1 =\n10 \u2211\ni=1\nai, E2 =\n20 \u2211\ni=11\nai, E3 =\n30 \u2211\ni=21\nai, E4 =\n40 \u2211\ni=31\nai (21)\nand\nE5 = max ( 10\u2212 4 \u2211\nk=1\nEk, 0 ) . (22)\nIn this experiment, because the discrete action space exponentially grows according to the length of the binary action vector, the size of the corresponding action space is huge |A| = 240 > 1012. Therefore, efficient sampling of the action is also required in this domain.\nIn this experiment, we used MLPs with 50 hidden units. We tested three types of behavior policies. The first policy is the conventional \u01eb-greedy policy. We used \u01eb = 0.3 in the task. the second policy is the bit-wise \u01eb-greedy policy. In this policy, each bit of the action element undertakes \u01ebbit-greedy exploration. More concretely, the i-th element of the action vector takes the random action (ai = 1 with probability 0.5) with probability \u01ebbit. Because we can sample the greedy actions with ease, we can explicitly take this behavior policy. We used \u01ebbit = 0.05 in this experiment. The third policy is the sofmax policy that was explained in section 3.1. We used \u03b2 = 20 in this experiment.\nFigure 4 shows the result of the experiment. The horizontal axis represents the number of episodes, the vertical axis is the step size in the episode. The solid lines are the mean performance of 10 runs and the bars are standard deviations. The broken lines are the optimal step size. The results show that all three behavior policies successfully improved the performance of the agent in the highdimensional action space. From these results, the bit-wise \u01eb-greedy policy (center) and the softmax policy (right) shows better performance than that of the conventional \u01eb-greedy policy (left). This would be because of the large exploration rate in the \u01eb-greedy policy (\u01eb = 0.3), but running with a smaller exploration rate (\u01eb \u2264 0.2) sometimes resulted in divergence of the parameters during the learning.\n6"}, {"heading": "4.4 Blocker", "text": "The blocker is the multi-agent task suggested by Sallans and Hinton [5][11]. This environment consists of a 4 \u00d7 7 grid, three agents, and two pre-programmed blockers. Agents and blockers never overlap each other in the grid. To obtain a positive reward, agents need to cooperate in this environment. The \u201cteam\u201d of agents obtain a +1 reward when any one of the three agents enters the end-zone, otherwise the team receives a \u22121 reward. The state vector is given as a 141 binary vector, composed of the positions (grid cells) of all the agents (28 bits \u00d7 3 agents), the east most positions of each blocker (28 bits \u00d7 2 blockers) and a bias bit that is always one (1 bit). Each agent can move to any of the four directions. Hence the size of the action space is 43 = 64. In this environment, the representation of the action is given as a 12-bit binary vector in which the three one-hot representation is concatenated (for example, (North, North, North) actions corresponding to the vector (1, 0, 0, 0|1, 0, 0, 0|1, 0, 0, 0)\u22a4). In each episode, the agents start at a random position in the bottom row of the grid. When one of the agents enters the end-zone or 40 time steps have passed, the episode terminates and the next episode starts after the initialization of the environment.\nIn this task, we used MLPs with 100 hidden units. The \u01eb-greedy policy was used as the behavior policy. In this task, we used \u01eb = 0.3. Also, we tested the agent-wise \u01eb-greedy policy as the behavior policy. This policy is a modified version of the \u01eb-greedy policy for actions with factored representation, and each agent follows the \u01eb-greedy policy independently. In the case of the agentwise-\u01eb-greedy policy, we used \u01eb = 0.1 for each agent.\nFigure 5 shows the results of the experiment. The horizontal axis represents the time steps, and the vertical axis represents the average reward during the last 1000 steps. The left panel is the result of the conventional \u01eb-greedy action selection, the right panel is that of the agent-wise \u01eb-greedy action selection. Both results are competitive, but in this experiment, agent-wise \u01eb-greedy agents tend to escape from the local optima."}, {"heading": "5 Discussion", "text": "In the environment with one-hot representation actions, the linear function approximation of the action-value corresponds to the bilinear function with respect to the action vector and the state vector\nQ\u03b8(s, a) = a \u22a4\u03b8s. (23)\n7\nIn this case, the parameter \u03b8 is give as a matrix. If the state is given by a one-hot representation, this approximation is identical with the table representation. As suggested in our method, the linear architecture with respect to the action enables efficient sampling of the greedy action. More recently, Mnih et al. proposed a DQN architecture [10]. In this case, we evaluate the action-values corresponding to all the discrete actions by a single forward propagation. And then the training of the approximator is done only on the output, which corresponds to the selected action. This architecture can be interpreted as a linear function approximation with respect to the actions\nQ\u03b8(s, a) = a \u22a4\u03c6\u03b8(s). (24)\nIf we construct \u03c6\u03b8(s) by some nonlinear function with high representational power such as deep neural networks, this approximation is sufficient for approximating the Q-values when actions are given by one-hot representation vectors.\nThe goal of our architecture (equation 7) is to adapt these ideas to the RL with binary vector actions. Although our function approximator is strongly restricted by the linear architecture with respect to the action, our function approximator is sufficient to represent an arbitrary deterministic policy \u03c0(s) by argmaxa Q\u03b8(s, a) even when we treat the binary vector actions, as long as we represent \u03c6\u03b8(s) by a universal function approximator."}, {"heading": "6 Conclusion", "text": "In this paper, we suggest a novel architecture of multilayer perceptrons for RL with a large discrete action set. In our architecture, the action-value function is approximated by a linear function with respect to the vector actions. This approximation method enables us to efficiently sample from the greedy policy and the softmax policy. The Q-learning-based off-policy algorithm is therefore tractable in our architecture without any Monte-Carlo approximations. We empirically tested our method in several discrete action domains, and the results supported its effectiveness. Based on these promising results, we expect to extend our approach using deep architectures in a future work."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Reinforcement learning of motor skills in high dimensions: A path integral approach", "author": ["Evangelos Theodorou", "Jonas Buchli", "Stefan Schaal"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["P Smolensky"], "venue": "In Parallel distributed processing: explorations in the microstructure of cognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1986}, {"title": "Using free energies to represent q-values in a multiagent reinforcement learning task", "author": ["Brian Sallans", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Actor-critic reinforcement learning with energy-based policies", "author": ["Nicolas Heess", "David Silver", "Yee Whye Teh"], "venue": "In EWRL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Long-Ji Lin"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Reinforcement learning with factored states and actions", "author": ["Brian Sallans", "Geoffrey E Hinton"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "ATARI game plays) [1].", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Even though several approaches are suggested for RL with continuous actions [2][3], RL with a large action space is still problematic, especially when we treat binary vectors as representations of the actions.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Even though several approaches are suggested for RL with continuous actions [2][3], RL with a large action space is still problematic, especially when we treat binary vectors as representations of the actions.", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "Sallans & Hinton suggested an energy-based approach in which restricted Boltzmann machines [4] were adopted in the algorithm and their free energy was used as the function approximator [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "Sallans & Hinton suggested an energy-based approach in which restricted Boltzmann machines [4] were adopted in the algorithm and their free energy was used as the function approximator [5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 5, "context": "followed their energy-based approach and investigated natural actor-critic algorithms with energybased policies by RBMs [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "Q-learning is an algorithm for finding the optimal policy in MDP [7], and the advantage of Q-learning is its off-policy property: the agent can directly approximate the action-value of an optimal policy \u03c0 while following the other policy \u03c0.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Using this gradient, the stochastic gradient descent or more sophisticated gradient-based algorithms are used for approximating the optimal action-value function [9][10].", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "Using this gradient, the stochastic gradient descent or more sophisticated gradient-based algorithms are used for approximating the optimal action-value function [9][10].", "startOffset": 165, "endOffset": 169}, {"referenceID": 4, "context": "However, if the actions are composed of binary vectors or factored representation [5][11], the number of total actions exponentially grows and quickly become intractable.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "However, if the actions are composed of binary vectors or factored representation [5][11], the number of total actions exponentially grows and quickly become intractable.", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "When the environment is represented by the factored MDP [5][11], the action may be represented by the binary vector, which is composed of a concatenation of one-hot representation vectors (for example, the agent may have to decide one of 2 options and one of 3 options simultaneously.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "When the environment is represented by the factored MDP [5][11], the action may be represented by the binary vector, which is composed of a concatenation of one-hot representation vectors (for example, the agent may have to decide one of 2 options and one of 3 options simultaneously.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "This task is the shortest-path problem in the grid world, as suggested by Sutton & Barto [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "4 Blocker The blocker is the multi-agent task suggested by Sallans and Hinton [5][11].", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "4 Blocker The blocker is the multi-agent task suggested by Sallans and Hinton [5][11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "proposed a DQN architecture [10].", "startOffset": 28, "endOffset": 32}], "year": 2015, "abstractText": "In this paper reinforcement learning with binary vector actions was investigated. We suggest an effective architecture of the neural networks for approximating an action-value function with binary vector actions. The proposed architecture approximates the action-value function by a linear function with respect to the action vector, but is still non-linear with respect to the state input. We show that this approximation method enables the efficient calculation of greedy action selection and softmax action selection. Using this architecture, we suggest an online algorithm based on Q-learning. The empirical results in the grid world and the blocker task suggest that our approximation architecture would be effective for the RL problems with large discrete action sets.", "creator": "LaTeX with hyperref package"}}}