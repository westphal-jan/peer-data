{"id": "1512.03953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2015", "title": "Active Distance-Based Clustering using K-medoids", "abstract": "k - medoids algorithm is a partitional, self - matched clustering algorithm which uses pairwise distances of data points and tries toward directly decompose the dataset with $ n $ points into a set including $ 7 $ disjoint clusters. however, k - medoids itself prefers all distances between data points that take not so easy to get in many examples. beside this paper, we introduce a new method which requires only a small proportion of the whole set of distances and has concrete example to estimate an upper - bound for unknown distances using the inquired ones. this algorithm makes use, the triangle inequality to calculate an upper - bound estimation of the unknown distances. our method is built upon a specialized approach to cluster objects and one choose some points actively from multiple bunch of data together acquire the distances given these prominent points from all. experimental results show that the proposed method using only a small subset of the graphs can find proper clustering on many real - world and synthetic datasets.", "histories": [["v1", "Sat, 12 Dec 2015 19:33:52 GMT  (295kb,D)", "http://arxiv.org/abs/1512.03953v1", "12 pages, 3 figures, PAKDD 2016"]], "COMMENTS": "12 pages, 3 figures, PAKDD 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehrdad ghadiri", "amin aghaee", "mahdieh soleymani baghshah"], "accepted": false, "id": "1512.03953"}, "pdf": {"name": "1512.03953.pdf", "metadata": {"source": "CRF", "title": "Active Distance-Based Clustering using K-medoids", "authors": ["Amin Aghaee", "Mehrdad Ghadiri", "Mahdieh Soleymani Baghshah", "M. Soleymani Baghshah"], "emails": ["aghaee@ce.sharif.edu", "ghadiri@ce.sharif.edu", "soleymani@sharif.edu"], "sections": [{"heading": null, "text": "Keywords: Active k-medoids, Active clustering, Distance-based clustering, Centroid-based clustering."}, {"heading": "1 Introduction", "text": "As the production of data is expanding at an astonishing rate and the era of big data is coming, organizing data via assigning items into groups is inevitable. Data clustering algorithms try to find clusters of objects in such a way that the objects in the same cluster are more similar to each other than to those in other clusters. Nowadays clustering algorithms are widely used in data mining tasks.\nThere are different categorization for clustering algorithms, e.g., these algorithms can be categorized into Density-based, Centroid-based and Distributionbased methods. In centroid-based clustering methods, each cluster is shown by a central object. This object which can be a member of the dataset denotes a prototype of the whole cluster. When these algorithms are appointed to find K clusters, they usually find K central objects and assign each element to the nearest centroid. As they go on, they attempt to decrease the energy and total error\n1 Equally Contributing Authors\nar X\niv :1\n51 2.\n03 95\n3v 1\n[ cs\n.L G\n] 1\n2 D\nec 2\n01 5\nof clusters by finding better central elements. K-medoids and K-means are the two most popular centroid-based algorithms. Although, they both partition the data into groups such that the sum of the squared distances of the data points to the nearest center to them is minimized, they have different assumptions about centroids. Indeed, the k-medoids algorithm chooses the centroids only from the data points and so these centroids are members of the whole dataset while kmeans algorithm can select the centroids from the whole input space.\nIn [3], some usages and applications of k-medoids algorithm are discussed. According to this study, in resource allocation problems, when a company wants to open some branches in a city, in such a way that the average distance from each residential block to the closest branch is intended to be minimized, the kmedoids algorithm is a proper option. Additionally, in mobile computing context, it is an issue to save communication cost when devices need to choose supernodes among each other, which should have minimum average distances to all devices and k-medoids can solve this problem. Furthermore, as reported by [3], medoid queries also arise in the sensor networks and many other fields.\nActive learning is a machine learning field that have bee attended specially in the last decade. Until now, many active methods for supervised learning that intend to select more informative samples to be labeled have been proposed. Active unsupervised methods have also been received attention recently. In an unsupervised learning manner, finding the similarities or distances of samples from each other may be difficult or infeasible. For example, the sequence similarity of proteins [23] or similarity of face images [7] which needs to be obtained from human as an oracle, may be difficult to be responded. The active version of some of the well known clustering algorithms have been recently presented in [14,26].\nIn this paper, we propose the active k-medoid algorithm that inquires a subset of pairwise distances to find the clustering of data. We use a bottomup approach to find more informative subset of the distances to be inquired. Extensive experiments on several data sets show that our algorithm usually needs a few percentage of the pairwise distances to cluster data properly.\nIn the rest of this paper, we first discuss about the works that have been done in the field of active clustering in Section 2. In Section 3, we introduce our algorithm. The result of experiments on different datasets have been presented in Section 4. At last, we discuss about some aspects of our algorithm and conclude the paper in Section 5."}, {"heading": "2 Related Work", "text": "Active learning is a machine learning paradigm that endeavors to do learning with asking labels of a few number of samples which are more important in the final result of learning. Indeed, most of supervised learning algorithms need a large amount of labeled samples and gathering these labeled samples may need unreasonable amount of time and effort. Thus, active learning tries to ask labels for more important samples where important samples may be interpreted as\nmost informative ones, most uncertain ones, or the ones that have a large effect in the results [18]. The active clustering problem has been recently received much attention. Until now, the active version of some well known clustering methods has been proposed in [14,26]. In the active clustering problem, a query is a pair of data whose similarity must be determined. The purpose of the active learning approach is reducing the number of required queries via active selection of them instead of random selection [21].\nThe existing active clustering methods can be categorized into constraintbased and distance-based ones [24] . In the most of the constraint-based methods, must-link and cannot-link constraints on pairs of data points indicating these pairs must be in the same cluster or different clusters are inquired. Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25]. In distance-based methods, the response to a query on a pair of data points is the distance of that pair according to an objective function. Distance-based methods for active clustering have been recently attended in [14,10,13,19,27,23].\nIn [14], an algorithm for active DBSCAN clustering is presented. In this algorithm, the distances that have not been queried are estimated with a lower bound. A score indicating the amount and the probability of changes in the estimated distances by asking a query is used to select queries. Moreover, an updating technique is introduced in [14] that update clustering after a query.\nIn [19,27], distance-based algorithms are presented for active spectral clustering in which a perturbation theory approach is used to select queries. A constraint-based algorithm has also been presented in [26] for active spectral clustering that uses an approach based on maximum expected error reduction to select queries.\nAn active clustering method for k-median clustering has also been proposed in [23]. This method selects some points as the landmarks and ask the distances between these landmarks and all the other data points as queries. Finally, kmedian clustering is done using these distances."}, {"heading": "3 Proposed Method", "text": "In this section, we introduce the proposed active k-medoids clustering. We assume that our algorithm intends to partition n samples into k different clusters. As mentioned above, many clustering algorithms such as K-medoids, PAM [12], and some other distance-based methods, calculate an n \u00d7 n distance matrix at first and perform the clustering algorithm on this distance matrix. We show the distance matrix by D where dij denotes the distance between the ith sample and the jth one.\nWe introduce a method to estimate unknown distances during an active clustering process. In a metric space, a satisfying and eminent upper-bound estimation for any distance metric can be obtained by the triangle inequality. For example, when we know the exact distances between dax, dxy and dyb, we can determine the upper-bound estimation for dab as:\ndab 6 dax + dxy + dyb (1)\nWe find an upper-bound estimation of the distances using the triangle inequality and the known distances asked from an oracle already. Therefore, we have\n\u2200i, j, 1 6 i, j 6 n : D(i, j) 6 De(i, j) (2)\nwhere De shows the estimated distances.\nFirst, upper-bound estimations for all distances are infinity and we update these distances by asking some of them and make better estimations for the other unknown distances using the triangle inequality and new distances taken from the oracle. The update will be done by replacing exact values for the asked distances and getting the minimum of the old and the new upper-bound estimation for unknown distances. By asking the landmark distances, we intend to take a better estimation of distances required for the k-medoids algorithm.\nConsider some data points which are partitioned to m groups where the distances within each group are known or estimated. However, the distances between data points from different groups are unknown. Our goal is to estimate these unknown distances instead of asking them. In such situation, we can choose t finite points from each group and ask the distances between these mt points between different groups and estimate the other distances using these asked distances. The number of these distances is ( m 2 ) t2. Figure 1 gives an intuition about this distance estimation method. We want to estimate the distance between a and b and the distances between those points that are connected by solid lines and dotted lines are known.\nSuch estimation algorithm can be done in O ( n2t2 ) , when n is the number of data points. If we have t n, the time complexity of the algorithm will be O ( n2 ) .\nBased on the estimation method used for unknown distances, we present an active k-medoids algorithm. The approach is based on partitioning the data points into some groups by a hierarchical manner. In the other words, we partition data points into b groups and partition each of these groups to b groups and so on until we get to a threshold like th for the number of data points in each partition. In this level, we ask all the distances within each group among all its data points and choose t points from each group (to ask their distances) and using the explained algorithm for estimating distances in a bottom-up approach until we get to the highest level. After that, we cluster the data using the kmedoids algorithm on the estimated distances. According to these explanation, it seems that choosing t points in each group is a critical step and choosing a bad point can lead to unfavourable estimations. For this purpose, consider a group of data points like G which its inner distances are known or estimated. In order to choosing t points, we perform k-medoids algorithm on G and find clusters and medoids for this group. Then, we choose medoids and s random points from each cluster as the points whose distances are needed to be asked. Therefore, the number of the chosen data points in G will be t = k(s + 1). It is obvious that a greater s will lead to more accurate estimations.\nAlgorithm 1 present the pseudo code of the proposed method. This function clusters n data points into k different categories and return clusters of data. Here, b shows the branching factor which is used to partition data points to b different groups with the same size. The partitioning algorithm will perform for each group recursively. There is also a threshold th which clarify the minimum size of a group of data points. Clearly, th \u2265 k(s + 1) since we need to choose at least k(s+1) points in each group. It is also noteworthy that if n \u2264 2th, we need to ask all distance pairs since these data points cannot be partitioned.\nFigure 2 shows an example workflow for Algorithm 1 for 1600 data points with branching factor 2 and threshold 400.\nNow we calculate the complexity of our Algorithm 1. It makes a tree with the branching factor b and the threshold th for the number of the data points in the leaves of the tree. Therefore, the height of this tree is dlogb (n/th)e. The number of nodes in the ith level of the tree is bi and each node of the ith level has n/bi data points. According to [20], the time complexity of the k-medoids algorithm is O ( kn2 ) for n data points and k clusters in each iteration. Consider p as the maximum number of iterations used in the k-medoids algorithm, then the time complexity in each node in the ith level of the tree is O ( (n2/b2i)kp ) . Therefore, the overall complexity of Algorithm 1 is\nO (blogb (n/th)c+1\u2211\ni=1\n( n2\nb2i kp)bi\n) = O ( n2kp ) . (3)\nA major factor that measures the quality of an active clustering algorithm, is the number of distances that the algorithm demands from the oracle. The\nAlgorithm 1 Active k-medoids\nINPUT: De, n, k, b, th . distance estimation, #data, #clusters, branch factor, threshold OUTPUT: C1, . . . , Ck 1: procedure ActiveKmedoids(De, n, k, b, th) 2: if n \u2264 2th then 3: Update De by querying all distances 4: C1, . . . , Ck \u2190 kmedoids(De, k) . kmedoids function is a regular k-medoids 5: return 6: end if 7: Partition data to b equal size groups, like G1, . . . , Gb 8: for i from 1 to b do 9: T1, . . . , Tk \u2190 ActiveKmedoids(De(Gi), |Gi|, k, b, th) . De(Gi) is the part\nof the estimated distance matrix corresponding to Gi 10: Gci \u2190 medoids of T1, . . . , Tk and s random points from each of them. 11: end for 12: Update De by querying distances between all those pairs that one of them is\nin Gci and the other is in G c j .\n13: Update De by the triangle inequality and new inquired distances. 14: C1, . . . , Ck \u2190 kmedoids(De, k) 15: end procedure\nnumber of the asked queries in the internal nodes of the tree is O ( b2(k(s+ 1))2 ) and in the leaves is O ( nth ) . Since, in the proposed method, there are n/th leaves and each of them has th data points, the ratio of the asked distances to all of the distances is almost equal to\n(b2(k(s + 1))2 bblogb (n/th)c \u2212 1\nb\u2212 1 + nth)/\n( n\n2\n) (4)\nwhere (bblogb (n/th)c \u2212 1)/(b \u2212 1) shows the approximate number of the internal nodes in the tree.\nIn order to improve the accuracy of the estimated distances, we can increase the s value. However, it may raise running time of calculating upper-bound estimations and the number of queries that should be asked.\nTo have a good baseline for comparison, we introduce an algorithm based on random selection of the distance queries, called Random-Rival and compare results of our algorithm with those of this algorithm. In Section 4, we show that asking queries using our method is much better than asking them randomly that is the aim of any active clustering algorithm.\nRandom-Rival(RR) algorithm, considers data points as the vertices of a weighted graph in which the weight of each edge shows the distance between its endpoints. Firstly, RR asks some distances randomly and then estimates all unknown distances using the triangle inequality and Floyd-Warshall [9, p. 693] algorithm. In the other words, we find shortest path distances of all pairs using the available distances in the aforementioned graph. Since our distance function\nis a distance metric, the length of these shortest paths is an upper-bound estimation of the true distances. Finally, RR clusters data runs the k-medoids algorithm on the estimated distances. Since Floyd-Warshall worst case time complexity is O ( n3 ) [9, p. 695], we can consider the runtime complexity of RR as O ( n3 )\nat worst. Algorithm 2 shows the pseudo code of the RR algorithm.\nAlgorithm 2 Random Rival\nINPUT: n, k,B . #data, #clusters, budget OUTPUT: C1, . . . , Ck 1: procedure RandomRival(n, k,B) 2: D \u2190 n\u00d7 n infinity matrix 3: (x1, y1), . . . , (xB , yB)\u2190 random pairs such that 1 6 xi < yi \u2264 n 4: \u2200i, 1 6 i 6 B : update D(xi, yi) by querying distances 5: De \u2190 FloydWarshall(D) 6: C1, . . . , Ck \u2190 kmedoids(De, k) 7: end procedure"}, {"heading": "4 Empirical results", "text": "In this section, we show the results of our algorithm on some synthesized and real world datasets. General information about these datasets are presented in Table 1. Most of them are real world datasets, but some are synthesized which are marked with letter s in Table 1. NORM-10 [4] contains 10000 data points having 20 features. This dataset has been generated by choosing 10 real centers uniformly at random from the hypercube of side length 50. Then, for each of the real centers, 1000 points from a Gaussian distribution of variance one centered at the corresponding point is generated. We converted samples in NEC animal [15] and ALOI200 [1] dataset into 32 \u00d7 32 grayscale images. ALOI [1] (Object Viewpoint version) has 1000 classes but we use only 200 classes of it.\nAlthough active version of some clustering algorithms like DBSCAN and spectral clustering have been introduced in [14,26], these clustering algorithms are substantially different from the k-medoids algorithm. For example, DBSCAN and spectral clustering methods can find clusters of different shapes while kmedoids cannot. Thus, we cannot compare results of our active k-medoid with those of active DBSCAN and active spectral clustering methods. One way to evaluate an active clustering method that asks distances is to compare it with a clustering method that asks a random subset of distances. , we compare our method with the Random-Rival algorithm introduced in section 3 that tries to use a random subset of distances to estimate the whole distance matrix (using shortest paths on the graph of data points and the triangle inequality). It must be mentioned that in both the proposed active k-medoid and the Random-Rival algorithm, the clustering algorithm that is run on the obtained distance matrix will be k-medoids. One of the most common measures for comparison of clustering algorithms is normalized mutual information (NMI) [17]. This measure\nshows the agreement of the two assignments, ignoring permutations. In the other words, NMI for the clustering obtained by an algorithm shows the agreement between the obtained grouping by this algorithm and the ground truth grouping of data.\nWe run our algorithm with s = 1, 3 and branching factor b = 2 over all the datasets. Threshold th is set to the minimum possible value which is equal to the number of classes for each dataset. Greater value of s, branching factor b, or threshold th can improve NMI score for some datasets. However, it would also increase the number of queries which is usually quite unsatisfactory. We also run Random-Rival over these datasets which requires a specified proportion of distances starting from 5% to 100% (with the step 5 percent).\nResults of our method with the parameters mentioned in the previous paragraph are shown in Table 2. For each algorithm and each dataset, NMI score and the ratio of the asked distances are presented in the table cells. For the Random-Rival algorithm, the results for the proportion of distances (between 5% and 100%) that is the first place where the number of inquired distances is greater than or equal to the inquired ones in our method are reported.\nMoreover, results of the Random-Rival algorithm which uses 0 percent of distances up to 100% of them is presented for some datasets in figure 3. According to figure 3, RR shows an ascending trend by asking extra distances and it gets close to the maximum value by asking about 20% of distances. Although it sounds to be a good algorithm, the proposed active k-medoids algorithm gets better NMI values with asking fewer number of distances. Moreover, the time complexity of our active k-medoid is also better. These results state the power of our algorithms which find accurate clusters by asking only a small subset of distances."}, {"heading": "5 Conclusion", "text": "In this paper, we introduce an innovative active distance-based clustering method. Its goal is to cluster n points from a metric dataset into k clusters by using lowest number of distances that is possible. We design a recursive model that makes a tree and split data with a branching factor b unless the number of objects is less than a threshold th. Then, it actively selects and ask some pairwise similarities from and oracle. After that it tries to make an upper-bound estimation for unknown distances utilizing triangular inequality. Eventually, it clusters data with a simple k-medoids algorithm. We run our algorithm over some synthesized and real world datasets. In order to show privilege of our method and to compare the results, we also introduce an algorithm which randomly selects pairwise distances and estimates unknown ones using Floyd-Warshall algorithm."}], "references": [{"title": "Parallelising the k-medoids clustering problem using space-partitioning", "author": ["A. Arbelaez", "L. Quesada"], "venue": "In Proceedings of the Sixth Annual Symposium on Combinatorial Search,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Uci machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "datasets. https: //archive.ics.uci.edu/ml/datasets.html,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Active semi-supervision for pairwise constrained clustering", "author": ["S. Basu", "A. Banerjee", "R.J. Mooney"], "venue": "In Proceedings of the Fourth SIAM International Conference on Data", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Active image clustering with pairwise constraints from humans", "author": ["A. Biswas", "D.W. Jacobs"], "venue": "International Journal of Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Active clustering: Robust and efficient hierarchical clustering using adaptively selected similarities", "author": ["B. Eriksson", "G. Dasarathy", "A. Singh", "R.D. Nowak"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Active semi-supervised fuzzy clustering", "author": ["N. Grira", "M. Crucianu", "N. Boujemaa"], "venue": "Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Finding Groups in Data: An Introduction to Cluster Analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": "John Wiley,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Efficient active algorithms for hierarchical clustering", "author": ["A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Active density-based clustering", "author": ["S.T. Mai", "X. He", "N. Hubig", "C. Plant", "C. B\u00f6hm"], "venue": "IEEE 13th International Conference on Data", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Columbia object image library (coil 100)", "author": ["S. Nayar", "S.A. Nene", "H. Murase"], "venue": "Department of Comp. Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Information theoretic measures for clusterings comparison: is a correction for chance necessary", "author": ["X.V. Nguyen", "J. Epps", "J. Bailey"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Spectral clustering on a budget", "author": ["O. Shamir", "N. Tishby"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "K-means v/s k-medoids: A comparative study", "author": ["S.S. Singh", "N. Chauhan"], "venue": "In National Conference on Recent Trends in Engineering & Technology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Fundamental clustering problems suite (fcps)", "author": ["A. Ultsch"], "venue": "Technical report, Technical report, University of Marburg,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Active clustering of biological sequences", "author": ["K. Voevodski", "M. Balcan", "H. R\u00f6glin", "S. Teng", "Y. Xia"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Improving constrained clustering with active query selection", "author": ["V. Vu", "N. Labroche", "B. Bouchon-Meunier"], "venue": "Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning (ICML", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Active spectral clustering", "author": ["X. Wang", "I. Davidson"], "venue": "ICDM", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Active spectral clustering via iterative uncertainty reduction", "author": ["F.L. Wauthier", "N. Jojic", "M.I. Jordan"], "venue": "In The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Active clustering with model-based uncertainty reduction", "author": ["C. Xiong", "D.M. Johnson", "J.J. Corso"], "venue": "CoRR, abs/1402.1783,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Ucr time series classification archive", "author": ["Eamonn Keogh", "G. Batista"], "venue": "http://www.cs.ucr.edu/~eamonn/time_series_ data/,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In [3], some usages and applications of k-medoids algorithm are discussed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Furthermore, as reported by [3], medoid queries also arise in the sensor networks and many other fields.", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "For example, the sequence similarity of proteins [23] or similarity of face images [7] which needs to be obtained from human as an oracle, may be difficult to be responded.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "For example, the sequence similarity of proteins [23] or similarity of face images [7] which needs to be obtained from human as an oracle, may be difficult to be responded.", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "The active version of some of the well known clustering algorithms have been recently presented in [14,26].", "startOffset": 99, "endOffset": 106}, {"referenceID": 22, "context": "The active version of some of the well known clustering algorithms have been recently presented in [14,26].", "startOffset": 99, "endOffset": 106}, {"referenceID": 14, "context": "most informative ones, most uncertain ones, or the ones that have a large effect in the results [18].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "Until now, the active version of some well known clustering methods has been proposed in [14,26].", "startOffset": 89, "endOffset": 96}, {"referenceID": 22, "context": "Until now, the active version of some well known clustering methods has been proposed in [14,26].", "startOffset": 89, "endOffset": 96}, {"referenceID": 17, "context": "The purpose of the active learning approach is reducing the number of required queries via active selection of them instead of random selection [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "The existing active clustering methods can be categorized into constraintbased and distance-based ones [24] .", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25].", "startOffset": 74, "endOffset": 94}, {"referenceID": 24, "context": "Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25].", "startOffset": 74, "endOffset": 94}, {"referenceID": 7, "context": "Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25].", "startOffset": 74, "endOffset": 94}, {"referenceID": 22, "context": "Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25].", "startOffset": 74, "endOffset": 94}, {"referenceID": 4, "context": "Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25].", "startOffset": 74, "endOffset": 94}, {"referenceID": 3, "context": "Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25].", "startOffset": 74, "endOffset": 94}, {"referenceID": 21, "context": "Some constraint-based methods for active clustering have been proposed in [24,28,11,26,7,6,25].", "startOffset": 74, "endOffset": 94}, {"referenceID": 10, "context": "Distance-based methods for active clustering have been recently attended in [14,10,13,19,27,23].", "startOffset": 76, "endOffset": 95}, {"referenceID": 6, "context": "Distance-based methods for active clustering have been recently attended in [14,10,13,19,27,23].", "startOffset": 76, "endOffset": 95}, {"referenceID": 9, "context": "Distance-based methods for active clustering have been recently attended in [14,10,13,19,27,23].", "startOffset": 76, "endOffset": 95}, {"referenceID": 15, "context": "Distance-based methods for active clustering have been recently attended in [14,10,13,19,27,23].", "startOffset": 76, "endOffset": 95}, {"referenceID": 23, "context": "Distance-based methods for active clustering have been recently attended in [14,10,13,19,27,23].", "startOffset": 76, "endOffset": 95}, {"referenceID": 19, "context": "Distance-based methods for active clustering have been recently attended in [14,10,13,19,27,23].", "startOffset": 76, "endOffset": 95}, {"referenceID": 10, "context": "In [14], an algorithm for active DBSCAN clustering is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Moreover, an updating technique is introduced in [14] that update clustering after a query.", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "In [19,27], distance-based algorithms are presented for active spectral clustering in which a perturbation theory approach is used to select queries.", "startOffset": 3, "endOffset": 10}, {"referenceID": 23, "context": "In [19,27], distance-based algorithms are presented for active spectral clustering in which a perturbation theory approach is used to select queries.", "startOffset": 3, "endOffset": 10}, {"referenceID": 22, "context": "A constraint-based algorithm has also been presented in [26] for active spectral clustering that uses an approach based on maximum expected error reduction to select queries.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "An active clustering method for k-median clustering has also been proposed in [23].", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "As mentioned above, many clustering algorithms such as K-medoids, PAM [12], and some other distance-based methods, calculate an n \u00d7 n distance matrix at first and perform the clustering algorithm on this distance matrix.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "According to [20], the time complexity of the k-medoids algorithm is O ( kn ) for n data points and k clusters in each iteration.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "NORM-10 [4] contains 10000 data points having 20 features.", "startOffset": 8, "endOffset": 11}, {"referenceID": 11, "context": "We converted samples in NEC animal [15] and ALOI200 [1] dataset into 32 \u00d7 32 grayscale images.", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "seeds 210 7 3 21945 [5]", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "fisheriris 150 4 3 11175 [5]", "startOffset": 25, "endOffset": 28}, {"referenceID": 25, "context": "Trace 200 275 4 19900 [29]", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "multi-features 2000 649 10 1999000 [5]", "startOffset": 35, "endOffset": 38}, {"referenceID": 18, "context": "TwoDiamonds(s) 800 2 2 319600 [22]", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "EngyTime(s) 4096 2 2 8386560 [22]", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "COIL100 7200 1024 100 25916400 [16]", "startOffset": 31, "endOffset": 35}, {"referenceID": 1, "context": "NORM10(s) 10000 20 10 49995000 [4]", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "NEC animal 4371 1024 60 9550635 [15]", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "Although active version of some clustering algorithms like DBSCAN and spectral clustering have been introduced in [14,26], these clustering algorithms are substantially different from the k-medoids algorithm.", "startOffset": 114, "endOffset": 121}, {"referenceID": 22, "context": "Although active version of some clustering algorithms like DBSCAN and spectral clustering have been introduced in [14,26], these clustering algorithms are substantially different from the k-medoids algorithm.", "startOffset": 114, "endOffset": 121}, {"referenceID": 13, "context": "One of the most common measures for comparison of clustering algorithms is normalized mutual information (NMI) [17].", "startOffset": 111, "endOffset": 115}], "year": 2015, "abstractText": "k-medoids algorithm is a partitional, centroid-based clustering algorithm which uses pairwise distances of data points and tries to directly decompose the dataset with n points into a set of k disjoint clusters. However, k-medoids itself requires all distances between data points that are not so easy to get in many applications. In this paper, we introduce a new method which requires only a small proportion of the whole set of distances and makes an effort to estimate an upperbound for unknown distances using the inquired ones. This algorithm makes use of the triangle inequality to calculate an upper-bound estimation of the unknown distances. Our method is built upon a recursive approach to cluster objects and to choose some points actively from each bunch of data and acquire the distances between these prominent points from oracle. Experimental results show that the proposed method using only a small subset of the distances can find proper clustering on many real-world and synthetic datasets.", "creator": "LaTeX with hyperref package"}}}