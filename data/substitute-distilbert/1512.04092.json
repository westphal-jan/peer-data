{"id": "1512.04092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "Stack Exchange Tagger", "abstract": "the goal of our project is to develop an accurate definition for questions posted on stack exchange. our problem is an instance of basically more general problem of developing accurate patterns for large scale text datasets. we are tackling binary multilabel classification problem where each item ( in good case, question ) can belong to multiple classes ( in this class, description ). we are predicting consistent tags ( or keywords ) for a complex stack exchange post given only the question text and the title of the post. in the process, we compare the performance of support vector classification ( svc ) for different kernel functions, loss function, otherwise. that found linear svc error kernel singer thus produces best results.", "histories": [["v1", "Sun, 13 Dec 2015 17:52:44 GMT  (455kb)", "http://arxiv.org/abs/1512.04092v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sanket mehta", "shagun sodhani"], "accepted": false, "id": "1512.04092"}, "pdf": {"name": "1512.04092.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shagun Sodhani"], "emails": ["sanketmehta.iitr@gmail.com", "sshagunsodhani@gmail.com."], "sections": [{"heading": null, "text": "Our problem is an instance of the more general problem of developing accurate classifiers for large scale text datasets. We are tackling the multilabel classification problem where each item (in this case, question) can belong to multiple classes (in this case, tags). We are predicting the tags (or keywords) for a particular Stack Exchange post given only the question text and the title of the post. In the process, we compare the performance of Support Vector Classification (SVC) for different kernel functions, loss function, etc. We found linear SVC with Crammer Singer technique produces best results."}, {"heading": "1. Main Objectives", "text": "- Use SVC with different kernel functions (rbf, linear, polynomial, sigmoid). - Compare performance with respect to the number of iterations, loss function,\nregularization term."}, {"heading": "2. Status and other details", "text": "- Fully completed and open sourced. (https://github.com/shagunsodhani/Stack\nExchange-tagger).\n- Total time spent on the project: 12 days"}, {"heading": "3. Major stumbling blocks", "text": "- Stack Exchange Dataset: It took us time to scrape the entire dataset. - Computational Power Limitation: The time complexity for finding Singular\nValue Decomposition (SVD) for an mxn\nmatrix is \ud835\udc42(\ud835\udc5a2\ud835\udc5b + \ud835\udc5b3). - Choice of Error Metric: Since multi-\nlabel classification is different from multi-class classification, we need to modify accuracy, precision and recall for multi-label classifiers."}, {"heading": "4. Introduction", "text": "Stay organized, get found and promote yourself \u2013 3 reasons why tags are important\n[1]. Tags are also used as a form of query based search for information retrieval [2]. Tagging of online content by humans is increasing everyday. Hashtags for tweets on Twitter and posts on Facebook and Google Plus are examples of hashtags in social networks. Some work has already been done around this problem to address tag prediction but it still remains a challenge [3]. Facebook also conducted a competiton for predicting tags for questions posted on \u201cStackoverflow Network\u201d. This contest, titled \"Facebook Recruiting III - Keyword Extraction\" [4], was conducted on Kaggle to recruit developers to Facebook. Our work is also inspired by this contest. There are many challenges involved in building a tag prediction system to solve this problem. First we need to get data in abundance for training our system. Secondly data should be constrained which means we should have limited number of possible tags. For e.g., in case of Twitter, there is no restriction on hashtags so Twitter dataset is unconstrained in nature. Third real data contains lot of noise so pre-processing of data (Singular Value Decomposition for dimensionality reduction) takes lot of time and is also computationally expensive. To solve the first two challenges, we used Stack Exchange dataset. Stack Exchange is a network of 130+ Q&A communities\nincluding the very popular Stack Overflow, the preeminent site for programmers to find, ask, and answer questions about software development [5]. The Stack Exchange Network covers topics as diverse as Mathematics, Home Improvement, Statistics, English Language and Usage. To overcome computational limitations, we used DELL PRECISION T5600 Sever. The problem which we are addressing in this paper is an instance of the more general problem of developing accurate classifiers for large scale text datasets (here the dataset comprises of posts made on the StackExchange network). We are tackling the multilabel classification problem where each item (in this case, question) can belong to multiple classes (in this case, tags). We are predicting the tags (or keywords) for a particular Stack Exchange post given only the question text and the title of the post. Given the text and the title, we first parse the data to get rid of stop-words. Next we perform stemming and lemmatiztion. This is followed by tf-idf based filtering and then we extract features using SVD. Once we have our training data in form of features and classes, we train various classifiers with linear, polynomial, sigmoid and rbf kernels. We vary the number of iterations and the error function as well and do a comprehensive comparison of the different approaches for different values of the parameters. The organization of the paper is as follows. Section 5 summarises related work in this field. Section 6 deals with the proposed approach. It also deals with the feature vector extraction mechanism and dimensionality reduction. Section 7 presents the results of our experiments. Section 8 concludes the paper and section 9 recommends directions for future extension of our work."}, {"heading": "5. Related Work", "text": "[3] focuses on mining user interest from their behavior on stackoverflow.com and leveraging that information for predicting tags. Also they focus only on stackoverflow.com and not other member\nsites of the StackExchange network. Our work is different from existing work as none of the existing work does a survey analysis. Also most of the related work focus on getting good results for a given member site of Stack Exchange Network while in our case, we keep all the methods to be very generalized thereby making them applicale in all the member sites. [10] uses a cooccurrence model that predicts tags based on the words in the post and their relation (cooccurrence) to tags. They built model for StackOverflow dataset by constraining the next word predicted to only tags. His cooccurrence model has a 47% classification accuracy predicting one tag per post. Our experimental results show that we beat his accuracy as mentioned in Section 7."}, {"heading": "6. Proposed Approach", "text": "Figure 6.1 shows proposed workflow of our system. We explain each step in detail in following subsection."}, {"heading": "6.1 Data Collection \u2013 Stack Exchange Data", "text": "StackExchange Network provides all community-contributed content under the Creative Commons BY-SA 3.0 license. A\nquarterly dump of all this data (after sanitization) is updated on the Internet Archive. Other than this method, all the data ia accessible via StackExchange API. We have used both the dumps as well as the API to get our data. This data included information about Posts, Users, Votes, Comments, Badges, PostHistory, and PostLinks. Of these, we kept the information related to the problem and tags and filtered out the remaining information. Figure 6.2 shows snapshot of a example from stackoverflow.com member site."}, {"heading": "6.2 Data Preprocessing", "text": ""}, {"heading": "6.2.1 Parsing and Removing Noise", "text": "The content obtained from Stack Exchange archives and by scraping is in html format. So we first parse out the text part by filtering HTML tags. Next we remove any code snippets that users might have added with their question and retain only the words used in the question itself."}, {"heading": "6.2.2 Removing Stop Words", "text": "Stop words refer to words like \u201cand\u201d, \u201cor\u201d, \u201cthe\u201d etc which do not add any specific information about the context of text. These words are normally removed as a part of preprocessing stage. There is no single universal list of stop-words which can be used in all contexts. In many cases, developers have to come up with their own list of stopwords. Also what is stopword in one context, may not be stopword in another context. Eg we may normally treat mathematical symbols as stopwords but they become relevant if our text contains words like C++."}, {"heading": "6.2.3 Stemming", "text": "Stemming [6], [7] refers to the process of reducing words to their word root, also called as word stem, and hence the name stemming. A program that can perform stemming is referred to as stemmer. E.g., words \u201cfishing\", \"fished\", and \"fisher\" would be stemmed to the word \"fish\". Most Information Retrival systems use stemming as a preprocessing step before storing data or before performing applying more sophisticated techniques on user data. A lot of algorithms are available for stemming. The prominent ones include the porter stemmer, the snowball stemmer and the lancaster stemmer. Porter stemmer is the most comman algorithm and consists of 5 phases of word reduction that are applied sequentially. We have used porter stemmer [8] in our implementation as well."}, {"heading": "6.2.4 Lemmatization", "text": "Lemmatization is the process of grouping together different forms of a word so as to treat them as a single word. This single word is called lemma and hence the name lemmatization. E.g., the verb \u2018to eat\u2019 may appear as \u2018eat\u2019, \u2018ate\u2019, \u2018eating\u2019, etc though all these words can be reduced to a common lemma i.e., \u2018eat\u2019. We have used the \u2018Wordnet lemmetizer\u2019 in our implementation."}, {"heading": "6.2.5 Tf-Idf based filtering", "text": "tf\u2013idf [9] (term frequency\u2013inverse document frequency) is defined for a word given a collection of documents (also called a corpus). It indicates how important the word is for the given corpus. We have used it as a weighing factor to remove some words that do not convey information about the context of the problem at hand. The importance varies proportionally with the number of times the word appears in the document and is inversely proportional to the frequency of the word in the corpus.\n\ud835\udc61\ud835\udc53(\ud835\udc61, \ud835\udc51) = 0.5 + 0.5 \u2217 \ud835\udc53(\ud835\udc61, \ud835\udc51)\nmax{\ud835\udc53(\ud835\udc64, \ud835\udc51): \ud835\udc64 \u2208 \ud835\udc51}\nWhere, \ud835\udc61 refers to term, \ud835\udc51 refers to document,\n\ud835\udc61\ud835\udc53(\ud835\udc61, \ud835\udc51) is term frequency, \ud835\udc53(\ud835\udc61, \ud835\udc51) is the raw frequency of a term in a document.\n\ud835\udc56\ud835\udc51\ud835\udc53(\ud835\udc61, \ud835\udc37) = \ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc41\n|{\ud835\udc51 \u2208 \ud835\udc37 \u2236 \ud835\udc61 \u2208 \ud835\udc51}|\nWhere, \ud835\udc41 is the total no. of documents in the corpus, |{\ud835\udc51 \u2208 \ud835\udc37 \u2236 \ud835\udc61 \u2208 \ud835\udc51}| is number of documents where the term \ud835\udc61 appears. Finally tf-idf is calculated as :\n\ud835\udc61\ud835\udc53\ud835\udc56\ud835\udc51\ud835\udc53(\ud835\udc61, \ud835\udc51, \ud835\udc37) = \ud835\udc61\ud835\udc53(\ud835\udc61, \ud835\udc51) \u2217 \ud835\udc56\ud835\udc51\ud835\udc53(\ud835\udc61, \ud835\udc37)"}, {"heading": "6.3 Feature Extraction", "text": "Feature extraction refers to the process of deriving features/values from the given dataset such that the derived features are more informative and less redundant than the parameters in the given dataset. This is closely related to dimensionality reduction where in we reduce the number of dimensions of the given dataset to make computations feasible. Some important techniques include SVD (Singular Value Decomposition) and PCA (Principle Component Analysis). We have used SVD and will be explaining it further. SVD [9] is a dimensionality reduction technique that produces a factorization of any matrix, real or complex. SVD connects the rows and columns of a matrix by defining a small number of \u201cconcepts\u201d.\nLet \ud835\udc40 be an m \u00d7 n matrix, and let the rank of \ud835\udc40 be r. Rank of a matrix is the largest number of rows (or equivalently columns) that we can choose for which no nonzero linear combination of the rows is the all-zero vector 0. Figure 6.3 shows the form of a Singular Value Decomposition. Then, given \ud835\udc40, we can find matrices \ud835\udc48, \ud835\udc46, and \ud835\udc49 such that :\n\ud835\udc40 = \ud835\udc48\u03a3\ud835\udc49\ud835\udc47 Where, \ud835\udc48, \u03a3 and \ud835\udc49\ud835\udc47 satisfies the following properties :\n1. \ud835\udc48 is an m x r column-orthonormal matrix. 2. \ud835\udc49 is an n x r column-orthonormal matrix.\n3. \u03a3 is a diagonal matrix. The diagonal entries \ud835\udf0e\ud835\udc56 of \u03a3 are known as the singular values of \ud835\udc40. If we list the singular values in descending order, the diagonal matrix \u03a3 is uniquely determined by \ud835\udc40."}, {"heading": "6.4 Building Tag Predictor", "text": ""}, {"heading": "6.4.1 Support Vector Classification (SVC)", "text": "We consider one-vs-all classifier. Given training vectors \ud835\udc65\ud835\udc56 \u2208 \u211d \ud835\udc5d, \ud835\udc56 = 1, \u2026 , \ud835\udc5b in 2 classes, and a vector \ud835\udc66\ud835\udc56 \u2208 {1, \u22121} \ud835\udc5b, our primal problem formulation is as follows:\nmin 1\n2 \ud835\udc64\ud835\udc47\ud835\udc64 + \ud835\udc36 \u2211 \ud835\udf09\ud835\udc56 \ud835\udc5b \ud835\udc56=1\nsubject to\n\ud835\udc66\ud835\udc56(\ud835\udc64 \ud835\udc47\ud835\udf19(\ud835\udc65\ud835\udc56) + \ud835\udc4f) \u2265 1 \u2212 \ud835\udf09\ud835\udc56, \ud835\udf09\ud835\udc56 \u2265 0, \ud835\udc56\n= 1, \u2026 , \ud835\udc5b Its dual is as follows:\nmin 1\n2 \ud835\udefc\ud835\udc47\ud835\udc44\ud835\udefc \u2212 \ud835\udc52\ud835\udc47\ud835\udefc\nsubject to\n\ud835\udc66\ud835\udc47\ud835\udefc = 0 and 0 \u2264 \ud835\udefc\ud835\udc56 \u2264 \ud835\udc36, \ud835\udc56 = 1, \u2026 , \ud835\udc5b where, \ud835\udc52 is the vector of all ones,\n\ud835\udc36 > 0 is the upper bound and \ud835\udc36 is regularization parameter,\n\ud835\udc44 is an \ud835\udc5b by \ud835\udc5b positive semidefinite matrix,\n\ud835\udc44\ud835\udc56\ud835\udc57 = \ud835\udc3e(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) = \u2205(\ud835\udc65\ud835\udc56) \ud835\udc47\u2205(\ud835\udc65\ud835\udc57) is the\nkernel. The decision function as defined in [11], [13] is:\n\ud835\udc60\ud835\udc54\ud835\udc5b(\u2211 \ud835\udc66\ud835\udc56\ud835\udefc\ud835\udc56\ud835\udc3e(\ud835\udc65\ud835\udc56 , \ud835\udc65 \ud835\udc5b \ud835\udc56=1 ) + \ud835\udf0c ),\nWhere, \ud835\udf0c is intercept. We have considered various kernel functions in our case \u2013 rbf, linear, polynomial with degree = 2 and 3 and also sigmoid. Comparative study of all these kernels is presented in the next section. We also varied \ud835\udc36. Large \ud835\udc36 means we are modeling hardmargin svc which leads to low training error but poor generalization. We also vary the\nnumber of iterations. Experimental results are covered in next section."}, {"heading": "6.4.2 Linear Support Vector Classification (Linear SVC)", "text": "Linear SVC is SVC with a Linear Kernel. We are performing further tweaking with linear SVC as our previous results indicated that Linear SVC peforms better than SVC with other kernels. When using Linear SVC, we experiment around with both the loss function and with the optimization technique - namely the traditional multi-class optimization technique or the crammer singer approach. We played around with \"hinge\" loss function and \"squared hinge\" loss function. Next we take up the traditional multi-class optimization technique vs crammer singer approach. The primary approach for solving multiclass problems using support vector machines has focused on reducing a single multiclass problems into multiple binary problems. For e.g., we may build a set of binary classifiers to distinguish between labels. This approach is more commonly known as the one-vs-rest approach. An alternate method was proposed by Crammer and Singer [12]. They have used the dual of the optimization problem to incorporate kernels with a compact set of constraints and decomposed the dual problem into multiple optimization problems, each of reduced size. They then use a fixedpoint algorithm to solve these reduced optimization problems. This way crammer singer approach optimizes a joint objective over all classes. Also in crammer singer approach, the results are not affected by the loss function used which we infer from the next section."}, {"heading": "6.5 Testing Tag Predictor", "text": "Multi-label classification is different from multi-class classification and hence requires different metrics than the ones we use for traditional multi-class classification. The error metrices that we have used are proposed in [14] for multi-label classification problems.\nLet \ud835\udc37 be a multi-label evaluation data set, consisting of |\ud835\udc37| multi-label examples (\ud835\udc65\ud835\udc56, \ud835\udc4c\ud835\udc56), \ud835\udc56 = 1. . |\ud835\udc37|, \ud835\udc4c\ud835\udc56 \u2286 \ud835\udc3f. Let \ud835\udc3b be a multi-label classifier and \ud835\udc4d\ud835\udc56 = \ud835\udc3b(\ud835\udc65\ud835\udc56) be the set of labels predicted by \ud835\udc3b for \ud835\udc65\ud835\udc56. The following metrics for the evaluation of \ud835\udc3b and \ud835\udc37 are used:\n\ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66(\ud835\udc3b, \ud835\udc37) = 1\n|\ud835\udc37| \u2211\n|\ud835\udc4c\ud835\udc56 \u2229 \ud835\udc4d\ud835\udc56 | |\ud835\udc4c\ud835\udc56 \u222a \ud835\udc4d\ud835\udc56 |\n|\ud835\udc37|\n\ud835\udc56=1\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc3b, \ud835\udc37) = 1\n|\ud835\udc37| \u2211\n|\ud835\udc4c\ud835\udc56 \u2229 \ud835\udc4d\ud835\udc56 |\n|\ud835\udc4d\ud835\udc56|\n|\ud835\udc37|\n\ud835\udc56=1\n\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59(\ud835\udc3b, \ud835\udc37) = 1\n|\ud835\udc37| \u2211\n|\ud835\udc4c\ud835\udc56 \u2229 \ud835\udc4d\ud835\udc56 |\n|\ud835\udc4c\ud835\udc56|\n|\ud835\udc37|\n\ud835\udc56=1\nWe define percentage error as follows:\n\ud835\udc52\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc5f = (1 \u2212 \ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66(\ud835\udc3b, \ud835\udc37)) \u2217 100."}, {"heading": "7. Experimental Results", "text": "We considered total 10,000 questions. We divided the preprocessed data into training set (80%) and testing set (20%). We applied k-fold cross validation to obtain an average accuracy. We retain 90% of variance when using SVD. The number of features after applying SVD are ~3,000. The number of classes we are dealing with are 10. All the results below are for exact matching (as opposed to atleast one match). All the code used in these experiment has been implemented from scratch and has been open sourced on github [15].\n1000 22.34 % 36.1 %\nTable 7.1 shows the performance of SVC with RBF kernel for the training dataset for the soft margin and the hard margin case as the number of iterations are varied. As the number of iterations increases, the training error decreases. Also soft-margin has more training error which means good generalization as expected.\nTable 7.2 shows the performance of SVC with RBF kernel for the testing dataset for the soft margin and the hard margin case as the number of iterations are varied. As the number of iterations increases, the testing error decreases. Also rbf kernel is able to beat the method in [10].\nTable 7.2 shows the performance of SVC with RBF kernel for the testing dataset for the soft margin and the hard margin case as the\nnumber of iterations are varied. As the number of iterations increases, the testing error decreases. Also rbf kernel is able to beat the method in [10]. Table 7.3 shows the performance of SVC with different kernel function for the training dataset for the soft margin and the hard margin case while the number of iterations fixed to 10,000. As we can infer that linear kernel performs best followed by rbf then polynomial with degree 2 and polynomial of degree 3. Sigmoid kernel gives the worst performance. Also soft-margin has more training error which means good generalization as expected.\nTable 7.4 shows the performance of SVC with different kernel function for the testing dataset for the soft margin and the hard margin case while the number of iterations fixed to 10,000. As we can infer that linear kernel performs best (soft-margin) followed by rbf then polynomial with degree 2 and polynomial of degree 3. Sigmoid kernel gives the worst performance. Also soft-margin has less testing error which means good generalization as expected.\nCrammer Singer 30.71 % 30.71 %\nTable 7.5 shows the performance of linear SVC with different error functions and techniques. C is set to 0.001 (soft-margin) and the number of iterations is fixed to 10,000. First we observe that training error remains same for Crammer Singer technique irrespective of the error function. Crammer Singer technique performs better than Onevs-rest approach. For One-vs-rest, Square Hinge Loss function gives more training error because outliers are penalized more.\nTable 7.6 shows the performance of linear SVC with different error functions and techniques. C is set to 0.001 (soft-margin) and the number of iterations is fixed to 10,000. First we observe that testing error remains same for Crammer Singer technique irrespective of the error function. Crammer Singer technique performs better than Onevs-rest approach. For One-vs-rest, Square Hinge Loss function gives more testing error because outliers are penalized more."}, {"heading": "8. Conclusion", "text": "We conclude that linear SVC performs better than all other kernel functions in case of both soft and hard margin problem. In case of linear SVC, linear SVC with Crammer Singer technique for soft-margin performs better than ome-vs-rest technique. The best accuracy obtained in our case is 54.75%."}, {"heading": "9. Future Scope", "text": "Feature selection (dimensionality reduction) is a computationally expensive step, so we need to deal with this step for large data size. Also for our analysis we considered only the\ntext part of the data and ignored any code segements or user information present in the system. Also many tags co-occur. E.g., a question tagged \u201candroid\u201d would likely be tagged \u201cjava\u201d as well. We did not try to learn these co-occurences. These considerations can help to further improve upon accuracy.\nReferences\n1. http://vizibility.net/blog/tag-youreit-3-reasons-why-tags-are-\nimportant/\n2. Diakopoulos, Nicholas A., and David A. Shamma. \"Characterizing\ndebate performance via aggregated twitter sentiment.\" Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2010. 3. Stanley, Clayton, and Michael D. Byrne. \"Predicting tags for\nstackoverflow posts.\" Proceedings of ICCM. Vol. 2013. 2013.\n4. http://www.kaggle.com/c/facebookrecruiting-iii-keyword-extraction 5. http://stackexchange.com/about 6. http://bit.ly/1Mb5wEe 7. http://bit.ly/1RLPkR4 8. http://tartarus.org/martin/PorterStem\nmer\n9. http://stanford.io/1TJtE6w 10. Kuo, Darren. On word prediction\nmethods. Technical report, EECS Department, University of California, Berkeley, 2011. 11. Cortes, Corinna, and Vladimir Vapnik. \"Support-vector\nnetworks.\" Machine learning 20.3 (1995): 273-297. 12. Crammer, Koby; and Singer, Yoram (2001). \"On the Algorithmic\nImplementation of Multiclass Kernel-based Vector Machines\" (PDF). J. of Machine Learning Research 2: 265\u2013292. 13. Burges, Christopher JC. \"A tutorial on support vector machines for\npattern recognition.\" Data mining and knowledge discovery 2.2 (1998): 121-167.\n14. Tsoumakas, Grigorios, and Ioannis Katakis. \"Multi-label classification:\nAn overview.\" Dept. of Informatics, Aristotle University of Thessaloniki, Greece(2006). 15. https://github.com/shagunsodhani/St ackExchange-tagger"}], "references": [{"title": "Characterizing debate performance via aggregated twitter sentiment.", "author": ["Diakopoulos", "Nicholas A", "David A. Shamma"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Predicting tags for stackoverflow posts.", "author": ["Stanley", "Clayton", "Michael D. Byrne"], "venue": "Proceedings of ICCM. Vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "On word prediction methods", "author": ["Kuo", "Darren"], "venue": "Technical report, EECS Department,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Support-vector networks.\" Machine learning", "author": ["Cortes", "Corinna", "Vladimir Vapnik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "On the Algorithmic Implementation of Multiclass Kernel-based  Vector Machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "(PDF). J. of Machine Learning Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "A tutorial on support vector machines for pattern recognition.\" Data mining and knowledge discovery", "author": ["Burges", "Christopher JC"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Multi-label classification: An overview.", "author": ["Tsoumakas", "Grigorios", "Ioannis Katakis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Tags are also used as a form of query based search for information retrieval [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Some work has already been done around this problem to address tag prediction but it still remains a challenge [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "[3] focuses on mining user interest from their behavior on stackoverflow.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[10] uses a cooccurrence model that predicts tags based on the words in the post and their relation (cooccurrence) to tags.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "The decision function as defined in [11], [13] is: sgn(\u2211 yi\u03b1iK(xi , x n i=1 ) + \u03c1 ), Where, \u03c1 is intercept.", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "The decision function as defined in [11], [13] is: sgn(\u2211 yi\u03b1iK(xi , x n i=1 ) + \u03c1 ), Where, \u03c1 is intercept.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "An alternate method was proposed by Crammer and Singer [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "The error metrices that we have used are proposed in [14] for multi-label classification problems.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Also rbf kernel is able to beat the method in [10].", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "Also rbf kernel is able to beat the method in [10].", "startOffset": 46, "endOffset": 50}], "year": 2015, "abstractText": "The goal of our project is to develop an accurate tagger for questions posted on Stack Exchange. Our problem is an instance of the more general problem of developing accurate classifiers for large scale text datasets. We are tackling the multilabel classification problem where each item (in this case, question) can belong to multiple classes (in this case, tags). We are predicting the tags (or keywords) for a particular Stack Exchange post given only the question text and the title of the post. In the process, we compare the performance of Support Vector Classification (SVC) for different kernel functions, loss function, etc. We found linear SVC with Crammer Singer technique produces best results.", "creator": "Microsoft\u00ae Word 2013"}}}