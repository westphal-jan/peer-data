{"id": "1512.05509", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments", "abstract": "this paper explores the performance of stretched frontal q iteration for reinforcement learning upon several partially supportive environments, using three recurrent neural network architectures : long short - term memory, gated recurrent unit and mut1, a recurrent knowledge architecture evolved from a pool of several thousands candidate architectures. a variant of fitted first iteration, based on advantage estimation instead of q values, is also explored. the show shown that gru performs significantly better than lstm and mut1 for most of the problems considered, requiring less training episodes and less cpu concentration before learning a very good behavior. advantage comparison technique tends to produce better results.", "histories": [["v1", "Thu, 17 Dec 2015 09:45:51 GMT  (120kb,D)", "http://arxiv.org/abs/1512.05509v1", "Presented at the 27th Benelux Conference on Artificial Intelligence"]], "COMMENTS": "Presented at the 27th Benelux Conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["denis steckelmacher", "peter vrancx"], "accepted": false, "id": "1512.05509"}, "pdf": {"name": "1512.05509.pdf", "metadata": {"source": "CRF", "title": "An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments", "authors": ["Denis Steckelmacher", "Peter Vrancx"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning was originally developed for Markov Decision Processes (MDPs). It allows an agent to learn a policy to maximize a possibly delayed reward signal in a stochastic environment and guarantees convergence to an optimal policy, provided that the agent can sufficiently experiment and the environment in which it is operating is Markovian.\nIn many real world problems, however, the agent cannot directly perceive the full state of its environment and must make decisions based on incomplete observations of the system state. This partial observability introduces uncertainty about the true environment state and renders the problem nonMarkovian from the agent\u2019s point of view. One way to deal with partially observable environments is to equip the agent with a memory of past observations and actions in order to help it discover what the current state of the environment is. This memory can be implemented in a variety of ways, including explicit history windows [9, 10], but this article only focuses on reinforcement learning using recurrent neural networks for function approximation. Unlike basic feed-forward networks, recurrent neural networks can contain cyclic connections between neurons. These cycles give rise to dynamic temporal behavior, which can function as an internal memory that allows these networks to model values associated with sequences of observations [7, 4, 1]. This paper aims at comparing different recurrent neural architectures when used to model value functions in a reinforcement learning context.\nThe next section provides necessary background on reinforcement learning and the recurrent network architectures compared in this paper. Section 3 describes the experimental setup and environments used for the comparison. The empirical results are provided in Section 4. Finally, we conclude in Section 5."}, {"heading": "2 Background", "text": "Discrete-time reinforcement learning consists of an agent that repeatedly senses observations of its environment and performs actions. After each action at \u2208 A, the environment changes state to st+1 \u2208 S and the agent receives a reward rt+1 = R(st, at, st+1) \u2208 R and an observation ot+1 \u2208 O = f(st+1).\nar X\niv :1\n51 2.\n05 50\n9v 1\n[ cs\n.N E\n] 1\n7 D\nec 2\nThe agent has no knowledge of R(s, a, s\u2032) and f(s) and has to interact with its environment in order to learn a policy \u03c0(ot) \u2208 R|A|, that gives the probability distribution of taking each of the actions for any given observation. The optimal policy \u03c0\u2217 is the one that, when followed by the agent, maximizes the cumulative discounted reward r = \u2211 t \u03b3\ntrt, with 0 \u2264 \u03b3 \u2264 1. When the reward received by the agent depends solely on its current observation and action, the problem is reduced to a Markov decision process and is said to be completely observable (the agent can assume that ot = st without losing learning abilities). Partially observable Markov decision problems occur when the reward does not depend only on ot, but on state st, whose dynamics still obey some underlying MDP, but that the agent cannot observe directly. In this case, ot = f(st), with f an unknown one-way function part of the environment."}, {"heading": "2.1 Q-Learning and Advantage Learning", "text": "Q-Learning [13] and Advantage Learning [6] allow an agent to learn a policy that converges to the optimal policy given an infinite amount of time and in discrete domains.\nQ-Learning estimates the Q(o, a) function, that maps each state-action pair to the expected, optimal cumulative discounted reward reachable by taking action a given observation o. At each time step, the agent observes ot, takes action at and observes rt+1 and ot+1. Equation 1 is used to update the Q function after each time step, with 0 \u2264 \u03b1 \u2264 1 the learning factor.\n\u03b4t = rt+1 + \u03b3max a\nQk(ot+1, a)\u2212Qk(ot, at) (1)\nQk+1(ot, at) = Qk(ot, at) + \u03b1\u03b4t (2)\nAdvantage Learning [6] is related to Q-Learning, but artificially decreases the value of non-optimal actions. This widens the difference between the value of the optimal action and the other ones, which allows learning to converge more easily even if the values are approximated (using function approximation). Equation 3 is used to update the Advantage values at each time step [1]. The smaller \u03ba is, the widest the gap between the optimal and non-optimal actions becomes.\n\u03b4t = max a\nAk(ot, a) + rt+1 + \u03b3maxaAk(ot+1, a)\u2212maxaAk(ot, a)\n\u03ba \u2212Ak(ot, at) (3)\nAk+1(ot, at) = Ak(ot, at) + \u03b1\u03b4t (4)\nIn very large or even continuous environments, exact representation of the Q-function (or Advantage function) is no longer possible. In these cases a function approximation architecture is needed to represent the target function. It has been shown, however, that on-line Q-Learning can diverge, or converge very slowly, when used in combination with function approximation [11]. One solution to this problem is to learn the Q-values off-line. The method used in this paper is the neural fitted Q iteration described in [11], an adaptation of fitted Q iteration [5] using neural networks. The agent interacts with its environment using a fixed policy until reaching the goal or a maximum number time steps have elapsed, and collects samples of the form (ot, at, rt+1, ot+1) . After a number of episodes have been run, the model is trained in batch on the collected data. The model maps sequences of observations to action values: M : O\u221e \u2192 R|A|.\nThe next subsections describe the different recurrent network architectures that we consider in this paper to represent the target functions."}, {"heading": "2.2 Long Short Term Memory", "text": "An LSTM [7] cell stores a value. An output gate allows the cell to modulate its output strength, while an input gate controls the intensity of the input signal that is continuously added to the cell\u2019s content. A forget gate, when set to zero, clears the content of the cell. Equations 5 to 7 show how the values of the gates are computed. Equations 8 and 9 show how to compute the value of the memory cell, and Equation 10 shows the output of an LSTM cell.\nijt = \u03c3(Wixt + Uiht\u22121) j (5)\nf jt = \u03c3(Wfxt + Ufht\u22121) j (6) ojt = \u03c3(Woxt + Uoht\u22121) j (7) c\u0303jt = tanh(Wcxt + Ucht\u22121) j (8) cjt = f j t c j t\u22121 + i j t c\u0303 j t (9) hjt = o j t tanh(c j t ) (10)\nIn [1], Bakker proposes a neural network architecture tailored for reinforcement learning. The network has one input neuron per observation variable, and one output neuron per action. A softmax layer transforms the output of the neural network to a probability distribution over the actions. The neural network itself consists of an LSTM layer and a simple tanh layer working in parallel: the input of the network is fed to the LSTM and tanh layer, both these layers are connected to the output, the output of the tanh layer is connected to the input of the LSTM layer and the output of the LSTM layer is connected to the input of the tanh layer (see Figure 1). This article uses a simpler version of the network: the input is connected to a tanh layer, that is in turn connected to an LSTM layer, that is connected to the output. Both the tanh layer and the LSTM layer contain 100 neurons (or LSTM cells). 1"}, {"heading": "2.3 Gated Recurrent Unit", "text": "GRU has been introduced recently and follows a design completely different from LSTM [3, 4]. Instead of storing a value in a memory cell and updating it using input and forget gates, a GRU unit computes a candidate activation h\u0303t based on its input, and then produces an output that is a blend of its past output and the candidate activation. Equations 11 and 12 show how the Z (modulation) and R (reset) gates are computed. Equations 13 and 14 show how the input is mixed with the last activation in order to produce the candidate activation, and Equation 15 shows how the last activation and the candidate activation are mixed to produce the new activation.\n1The models themselves are built on Keras http://keras.io/, a Python library providing neural network primitives based on Theano [2]. Keras provides LSTM, GRU, MUT and dense fully-connected weighted layers (among others). Layers can be assembled either in a stack or in a directed acyclic graph. The connection scheme in [1] makes the network layer graph cyclic, and hence impossible to build using the current version of Keras.\nzjt = \u03c3(Wzxt + Uzht\u22121) j (11) rjt = \u03c3(Wrxt + Urht\u22121) j (12) x\u0303jt = r j th j t\u22121 (13) h\u0303jt = tanh(Wxt + Ux\u0303t) j (14) hjt = (1\u2212 z j t )h j t\u22121 + z j t h\u0303 j t (15)"}, {"heading": "2.4 MUT1", "text": "Jo\u0301zefowicz et al. observed that GRU and LSTM are very different from each other, and wondered whether other recurrent neural architectures could be used. In order to discover them, they developed a genetic algorithm that evaluated thousands of recurrent neural architectures. Once the experiment was finished, they identified three architectures that performed as good as or better than LSTM and GRU on their test vectors: MUT1, MUT2 and MUT3 [3].\nThis paper only considers MUT1, that produced the best results on preliminary experiments. Equations 16 and 17 show to compute the value of the Z and R gates, Equations 18 and 19 show how to compute the candidate activation, and Equation 20 shows that the output of the MUT1 uses the same type of mixing as the one used by GRU.\nzjt = \u03c3(Wzxt) j (16) rjt = \u03c3(Wrxt +Whht\u22121) j (17) h\u0302jt = r j th j t\u22121 (18) h\u0303jt = tanh(Wh\u0302h\u0302t + tanh(xt)) j (19) ht = (1\u2212 zjt )h j t\u22121 + z j t h\u0303 j t (20)"}, {"heading": "3 Experimental Setup", "text": "In order to keep training time manageable, the neural networks are trained to associate values with the last 10 observations, instead of the complete history. LSTM, GRU and MUT1 are able to associate values to arbitrarily long sequences of inputs, but Keras requires all the sequences on which it is trained to have the same length (possibly with padding).\nTraining has to be done carefully, because one does not want the model to forget past experiences when a new batch of episodes is learned. The network has been configured to perform 2 training epochs on the data, using a batch size of 10 (batches of 10 O10 \u00d7R|A| samples are used to compute an average gradient when performing backpropagation). The small number of epochs prevents the model from overfitting specific episodes."}, {"heading": "3.1 Environments", "text": "Three environments are used to evaluate the neural network models. The first one is a simple fullyobservable 10\u00d7 5 grid world with the initial position at (0, 2), the goal at (9, 2) and an obstacle at (5, 2) (see Figure 2). The agent can observe its (x, y) coordinates. It receives a reward of \u22121 at each time step, \u22125 if it hits a wall or the obstacle, and 10 when it reaches the goal.\nThe second environment is based on the same grid world as the first one, but the agent can only observe its x coordinate. The y coordinate is masked to zero.\nThe last environment is also based on the grid world, but the agent can only observe its orientation (whether it is facing up, down, left or right, expressed as a 0 to 3 integer number) and the distance between it and the wall in front of it. This agent-centric environment is very close to what actual robots can experience.\nThe \u201cstochastic\u201d variant of the experiments uses a random initial position for every episode. The agent can sense its initial (x, y) coordinate at the first time step, even in otherwise partially observable environments2.\nThe observations of the agent, that consist of integer numbers, are encoded using a one-hot encoding so that they are more easily processed by neural networks. For instance, the y coordinate of the grid world can take values from 0 to 4, which are encoded as (1, 0, 0, 0, 0), (0, 1, 0, 0, 0), ..., (0, 0, 0, 0, 1). For the 10\u00d7 5 grid world, the neural networks therefore have 15 input neurons."}, {"heading": "3.2 Experiments", "text": "Each experiment consists of 5000 episodes of a maximum of 500 time steps. During the episodes, the neural network is not trained on any new data, but Qk+1(s, a) values are computed based on Qk(s, a) and stored in a list. After every batch of 10 episodes, the neural networks are trained on theQk+1 values, as described in [11] and shown in Figure 3.\nThe experiments themselves consist of trying to reach the goal in one of the environments described in Section 3.1. Each experiment is run 15 times for each combination of the following parameters:\n\u2022 Value iteration: Q-Learning and Advantage learning, \u03b1 = 0.2, \u03b3 = 0.9 and \u03ba = 0.3 \u2022 Neural network architecture: feed-forward perceptron with a single hidden layer (nnet), LSTM\n(lstm), GRU (gru) and MUT1 (mut1) \u2022 World: gridworld (gw), partially observable gridworld (po) and agent-centric gridworld (ac) \u2022 Fixed initial position and random initial position \u2022 Softmax action selection with a temperature of 0.5"}, {"heading": "4 Empirical Results", "text": "Each experiment (see Section 3.2) is run 15 times. The first time step at which the agent is able to maintain an average (over the 1000 next time steps) reward of more than \u221215 with a standard deviation\n2Some experiments have been re-run without this hint, with no change in the results. The agent learns to look left, then up, and uses those observations as initial position.\nless than 20 is called the learning time. The best average reward obtained during a 1000-time-steps window is called the learning performance.\nTable 1 shows the learning time of the different neural networks for all the experiment configurations. Best results are emphasized in bold. Results are displayed in a mean/stddev format.\nAdvantage Learning leads to smaller learning times and standard deviations than Q-Learning in all worlds except the partially observable grid world. Figure 4 shows the behavior of the Q and Advantage learning algorithms in the partially observable grid world. Q-Learning allows faster convergence with a smaller standard deviation.\nWhen using a fixed initial position, GRU learns faster than any other network. The difference of learning speed between GRU and LSTM is statistically significant for (gw, Advantage), (gw, QLearning), (po, Q-Learning) and (ac, Q-Learning) (p-values of 0.003, 0.008, 0.0003 and 0.0003, respectively), but not for (po, Advantage) and (ac, Advantage) (p-values of 0.118 and 0.140, respectively).\nWhen using a random initial position, GRU is the only model allowing learning in all the environments when Advantage Learning is used. LSTM and GRU give comparable results in the partially observable worlds, with no statistically significant difference between them.\nAgents using MUT1 as a function approximator nearly always manage to learn a good enough policy in partially observable worlds, but they need a large number of episodes to do so. However, plain perceptron-based agents don\u2019t manage at all to learn a policy in these worlds3, which shows that MUT1 allows better learning in partially observable worlds than a simple non-recurrent neural network.\nTable 2 shows the learning performance of the different neural networks, with the highest values highlighted in bold. Results are displayed in a mean/stddev format.\nThe feed-forward neural network always achieves the best scores in the grid world, followed by GRU, then LSTM, and finally MUT1. GRU always outperforms the other network architectures in the partially observable worlds. In these worlds, GRU is statistically significantly better than LSTM in all cases (p-value less than 0.0001) except in (random initial, ac, Q-Learning) (p-value of 0.682).\n3Except in the partially observable grid world using Q-Learning and random initial positions, where the agent learns to go left, then randomly go up and down until the goal is reached by chance."}, {"heading": "5 Conclusion", "text": "LSTM, GRU and MUT1 have been compared on simple reinforcement learning problems. It has been shown that agents using LSTM and GRU for approximating Q or Advantage values perform significantly better than the ones using MUT1, obtaining higher rewards and learning faster.\nGRU and LSTM provide comparable performance, with GRU often being significantly better than LSTM. LSTM is never significantly better than GRU. When considering the rewards received by the agents once they have learned, and not the time required for learning, GRU always achieves better results than LSTM.\nThis shows that using GRU instead of LSTM should be considered when tackling reinforcement problems. Moreover, on the machine used for the experiments, the simpler GRU cell (compared to LSTM) allowed the GRU-based agents to complete their 5000 episodes approximately two times faster than LSTM-based agents."}], "references": [{"title": "Reinforcement Learning with Long Short-Term Memory", "author": ["B. Bakker"], "venue": "Advances in Neural Information Processing Systems 14, pages 1475\u20131482", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Theano: a CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde- Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, \u00c7. G\u00fcl\u00e7ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1724\u20131734", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree-Based Batch Mode Reinforcement Learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research, 6:503\u2013556", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-player residual advantage learning with general function approximation", "author": ["M.E. Harmon", "L.C. Baird III"], "venue": "Wright Laboratory, WL/AACF, Wright-Patterson Air Force Base, OH", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u2013 1780", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["R. J\u00f3zefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, pages 2342\u20132350", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning with Hidden States", "author": ["L.-J. Lin", "T. Mitchell"], "venue": "From animals to animats 2: Proceedings of the second international conference on simulation of adaptive behavior, volume 2, page 271. MIT Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning to use selective attention and short-term memory in sequential tasks", "author": ["A.K. McCallum"], "venue": "From animals to animats 4: proceedings of the fourth international conference on simulation of adaptive behavior, volume 4, page 315. MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method", "author": ["M.A. Riedmiller"], "venue": "Machine Learning: ECML 2005, 16th European Conference on Machine Learning, pages 317\u2013328", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "GNU Parallel - The Command-Line Power Tool", "author": ["O. Tange"], "venue": ";login: The USENIX Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8(3-4):279\u2013292", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 6, "context": "Abstract This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8].", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "Abstract This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8].", "startOffset": 247, "endOffset": 250}, {"referenceID": 7, "context": "Abstract This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8].", "startOffset": 358, "endOffset": 361}, {"referenceID": 5, "context": "A variant of fitted Q iteration, based on Advantage values [6, 1] instead of Q values, is also explored.", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "A variant of fitted Q iteration, based on Advantage values [6, 1] instead of Q values, is also explored.", "startOffset": 59, "endOffset": 65}, {"referenceID": 8, "context": "This memory can be implemented in a variety of ways, including explicit history windows [9, 10], but this article only focuses on reinforcement learning using recurrent neural networks for function approximation.", "startOffset": 88, "endOffset": 95}, {"referenceID": 9, "context": "This memory can be implemented in a variety of ways, including explicit history windows [9, 10], but this article only focuses on reinforcement learning using recurrent neural networks for function approximation.", "startOffset": 88, "endOffset": 95}, {"referenceID": 6, "context": "These cycles give rise to dynamic temporal behavior, which can function as an internal memory that allows these networks to model values associated with sequences of observations [7, 4, 1].", "startOffset": 179, "endOffset": 188}, {"referenceID": 3, "context": "These cycles give rise to dynamic temporal behavior, which can function as an internal memory that allows these networks to model values associated with sequences of observations [7, 4, 1].", "startOffset": 179, "endOffset": 188}, {"referenceID": 0, "context": "These cycles give rise to dynamic temporal behavior, which can function as an internal memory that allows these networks to model values associated with sequences of observations [7, 4, 1].", "startOffset": 179, "endOffset": 188}, {"referenceID": 12, "context": "Q-Learning [13] and Advantage Learning [6] allow an agent to learn a policy that converges to the optimal policy given an infinite amount of time and in discrete domains.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "Q-Learning [13] and Advantage Learning [6] allow an agent to learn a policy that converges to the optimal policy given an infinite amount of time and in discrete domains.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Advantage Learning [6] is related to Q-Learning, but artificially decreases the value of non-optimal actions.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "Equation 3 is used to update the Advantage values at each time step [1].", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "It has been shown, however, that on-line Q-Learning can diverge, or converge very slowly, when used in combination with function approximation [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": "The method used in this paper is the neural fitted Q iteration described in [11], an adaptation of fitted Q iteration [5] using neural networks.", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "The method used in this paper is the neural fitted Q iteration described in [11], an adaptation of fitted Q iteration [5] using neural networks.", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "2 Long Short Term Memory An LSTM [7] cell stores a value.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "Figure 1: Cyclic architecture proposed by [1] (left), and architecture used in this paper (right).", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "In [1], Bakker proposes a neural network architecture tailored for reinforcement learning.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "GRU has been introduced recently and follows a design completely different from LSTM [3, 4].", "startOffset": 85, "endOffset": 91}, {"referenceID": 3, "context": "GRU has been introduced recently and follows a design completely different from LSTM [3, 4].", "startOffset": 85, "endOffset": 91}, {"referenceID": 1, "context": "io/, a Python library providing neural network primitives based on Theano [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "The connection scheme in [1] makes the network layer graph cyclic, and hence impossible to build using the current version of Keras.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Once the experiment was finished, they identified three architectures that performed as good as or better than LSTM and GRU on their test vectors: MUT1, MUT2 and MUT3 [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 10, "context": "After every batch of 10 episodes, the neural networks are trained on theQk+1 values, as described in [11] and shown in Figure 3.", "startOffset": 101, "endOffset": 105}], "year": 2015, "abstractText": "This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8]. A variant of fitted Q iteration, based on Advantage values [6, 1] instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results.", "creator": "LaTeX with hyperref package"}}}