{"id": "1512.05808", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Successive Ray Refinement and Its Application to Coordinate Descent for LASSO", "abstract": "axial descent is one of the most popular approaches for geometric ct and its extensions due to its shape and efficiency. when adapting coordinate descent to solving lasso, we update one coordinate for a time while using the remaining coordinates. such an update, which is usually easy to compute, directly decreases the objective function profile. in this paper, we aim to improve its computational precision by reducing each number unnecessary geometric descent iterations. to this end, we propose a novel work called successive projection refinement ( srr ). srr makes use of the following ray continuation property on the successive iterations : for a particular coordinate, the value obtained until the next iteration condition always lies on a ray that starts at its previous iteration and passes through the current iteration. motivated by this ray - continuation property, we propose that coordinate descent be concentrated not directly on the previous iteration but on a refined search point that treats the respective properties : on physical hand, it lies on a ray that starts at each history solution and passes through the previous iteration, and strictly the other hand, it achieves the minimum objective function value consider which the points on the ray. we propose two schemes for defining the search point and show that later refined search point can be locally obtained. empirical results for real and synthetic data sets show that the proposed srr can significantly reduce the number of coordinate descent iterations, especially for manipulating lasso regularization parameters.", "histories": [["v1", "Thu, 17 Dec 2015 21:47:02 GMT  (880kb,D)", "http://arxiv.org/abs/1512.05808v1", "26 pages, 6 figures, 6 tables"]], "COMMENTS": "26 pages, 6 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun liu", "zheng zhao", "ruiwen zhang"], "accepted": false, "id": "1512.05808"}, "pdf": {"name": "1512.05808.pdf", "metadata": {"source": "CRF", "title": "Successive Ray Refinement and Its Application to Coordinate Descent for LASSO", "authors": ["Jun Liu", "Zheng Zhao", "Ruiwen Zhang"], "emails": ["junliu.nt@gmail.com."], "sections": [{"heading": null, "text": "and its extensions due to its simplicity and efficiency. When applying coordinate descent to solving Lasso, we update one coordinate at a time while fixing the remaining coordinates. Such an update, which is usually easy to compute, greedily decreases the objective function value. In this paper, we aim to improve its computational efficiency by reducing the number of coordinate descent iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). SRR makes use of the following ray continuation property on the successive iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, it lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, it achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point and show that the refined search point can be efficiently obtained. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially for small Lasso regularization parameters."}, {"heading": "1 Introduction", "text": "Lasso [12] is an effective technique for analyzing high-dimensional data. It has been applied successfully in various areas, such as machine learning, signal processing, image processing, medical imaging, and so on. LetX = [x1,x2, . . . ,xp] \u2208 Rn\u00d7p denote the data matrix composed of n samples with p variables, and let y \u2208 Rn\u00d71 be the response vector. In Lasso, we compute the \u03b2 that optimizes\nmin \u03b2 f(\u03b2) =\n1 2 \u2016X\u03b2 \u2212 y\u201622 + \u03bb\u2016\u03b2\u20161, (1)\n\u2217Corresponding author. E-mail address: junliu.nt@gmail.com. \u2020This work was done when Zheng Zhao was with SAS.\nar X\niv :1\n51 2.\n05 80\n8v 1\n[ cs\n.L G\n] 1\n7 D\nwhere the first term measures the discrepancy between the prediction and the response and the second term controls the sparsity of \u03b2 with `1 regularization. The regularization parameter \u03bb is nonnegative, and a larger \u03bb usually leads to a sparser solution.\nResearchers have developed many approaches for solving Lasso in Equation (1). Least Angle Regression (LARS) [3] is one of the most well-known homotopy approaches for Lasso. LARS adds or drops one variable at a time, generating a piecewise linear solution path for Lasso. Unlike LARS, other approaches usually solve Equation (1) according to some prespecified regularization parameters. These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on. Among these approaches, coordinate descent is one of the most popular approaches due to its simplicity and efficiency. When applying coordinate descent to Lasso, we update one coordinate at a time while fixing the remaining coordinates. This type of update, which is easy to compute, can effectively decrease the objective function value in a greedy way.\nTo improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19]. Screening 1) identifies and removes the variables that have zero entries in the solution \u03b2 and 2) solves Equation (1) by using only the kept variables. When one is able to discard the variables that have zero entries in the final solution \u03b2 and identify the signs of the nonzero entries, the Lasso problem in Equation (1) becomes a standard quadratic programming problem. However, it is usually very hard to identify all the zero entries, especially when the regularization parameter is small. In addition, the computational cost of Lasso usually increases as the the regularization parameter decreases. The computational cost increase motivates us to come up with an approach that can accelerate the computation of Lasso for small regularization parameters.\nIn this paper, we aim to improve the computational efficiency of coordinate descent by reducing its iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). Our proposed SRR is motivated by an interesting ray-continuation property on the coordinate descent iterations: for a given coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Figure 1 illustrates the ray-continuation property by using the data specified in Section 2. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, the search point lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, the search point achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point, and we show that the refined search point can be efficiently computed. Experimental results on both synthetic and real data sets demonstrate that the proposed SRR can greatly accelerate the convergence of coordinate descent for Lasso, especially when the regularization parameter is small.\nOrganization The rest of this paper is organized as follows. We introduce the traditional coordinate descent for Lasso and present the ray-continuation property that motivates this paper in Section 2, propose the SRR technique in Section 3, discuss the efficient computation of the refinement factor that is used in SRR in Section 4, conduct\ni + (1 \u2212 \u03b1ki )\u03b2ki . Ray-continuation\nproperty: for a given coordinate i, the value obtained in the next iteration denoted by \u03b2k+1i almost always lies on a ray that starts at its previous iteration, \u03b2 k\u22121 i , and passes through the current iteration, \u03b1ki . For numerical details of plot (a) and plot (b), see Table 1 and Table 2.\nan eigenvalue analysis on the proposed SRR in Section 5, and compare SRR with related work in Section 6. We report experimental results on both synthetic and real data sets in Section 7, and we conclude this paper in Section 8.\nNotations Throughout this paper, scalars are denoted by italic letters and vectors by bold face letters. Let \u2016 \u00b7 \u20161 denote the `1 norm, let \u2016 \u00b7 \u20162 denote the Euclidean norm, and let \u2016 \u00b7 \u2016\u221e denote the infinity norm. Let \u3008x,y\u3009 denote the inner product between x and y. Let a superscript denote the iteration number, and let a subscript denote the index of the variable or coordinate. We assume that X does not contain a zero column; that is, \u2016xi\u20162 6= 0,\u2200i."}, {"heading": "2 Coordinate Descent For Lasso", "text": "In this section, we first review the coordinate descent method for solving Lasso, and then analyze the adjacent iterations to motivate the proposed SRR technique.\nLet \u03b2ki denote the ith element of \u03b2, which is obtained at the kth iteration of coordinate descent. In coordinate descent, we compute \u03b2ki while fixing \u03b2j = \u03b2 k j , 1 \u2264 j < i, and \u03b2j = \u03b2k\u22121j , i < j \u2264 p. Specifically, \u03b2ki is computed as the minimizer to the following univariate optimization problem:\n\u03b2ki = arg min \u03b2 f([\u03b2k1 , . . . , \u03b2 k i\u22121, \u03b2, \u03b2 k\u22121 i+1 , . . . , \u03b2 k\u22121 p ] T ).\nIt can be computed in a closed form as:\n\u03b2ki = S(xTi y \u2212\n\u2211 j<i x T i xj\u03b2 k j \u2212 \u2211 j>i x T i xj\u03b2 k\u22121 j , \u03bb)\n\u2016xi\u201622 , (2)\nwhere S(\u00b7, \u00b7) is the shrinkage function\nS(x, \u03bb) =  x\u2212 \u03bb x > \u03bbx+ \u03bb x < \u2212\u03bb 0 |x| \u2264 \u03bb.\n(3)\nLet rki = y \u2212X[\u03b2k1 , . . . , \u03b2ki\u22121, \u03b2ki , \u03b2k\u22121i+1 , . . . , \u03b2 k\u22121 p ] T (4)\ndenote the residual obtained after updating \u03b2k\u22121i to \u03b2 k i . With Equation (4), we can rewrite Equation (2) as\n\u03b2ki = S(\u03b2 k\u22121 i +\nxTi r k i\u22121\n\u2016xi\u201622 ,\n\u03bb\n\u2016xi\u201622 ). (5)\nIn addition, with the updated \u03b2ki , we can update the residual from r k i\u22121 to r k i as\nrki = r k i\u22121 + xi(\u03b2 k\u22121 i \u2212 \u03b2 k i ). (6)\nAlgorithm 1 illustrates solving Lasso via coordinate descent. Since the non-smooth `1 penalty in Equation (1) is separable, the algorithm is guaranteed to converge [14].\nAlgorithm 1 Coordinate Descent for Lasso Input: X , y, \u03bb Output: \u03b2k\n1: k = 0, \u03b20 = 0, r0 = y 2: repeat 3: Set k = k + 1, rk = rk\u22121 4: for i = 1 to p do 5: Compute \u03b2ki = S(\u03b2 k\u22121 i + xTi r k\n\u2016xi\u201622 , \u03bb\u2016xi\u201622 )\n6: Update residual rk = rk + xi(\u03b2k\u22121i \u2212 \u03b2ki ) 7: end for 8: until convergence criterion satisfied"}, {"heading": "30 -0.083806 -0.141752 0.468749 0.033883 0.218827 0.000082 ...", "text": ""}, {"heading": "29 -0.082490 -0.142044 0.468368 0.032406 0.218288 0.000093", "text": ""}, {"heading": "28 -0.081090 -0.142355 0.467964 0.030835 0.217715 0.000106", "text": ""}, {"heading": "10 -0.031899 -0.149927 0.454929 -0.020507 0.197895 0.000950 ...", "text": "We demonstrate Algorithm 1 using the following randomly generated X and y:\nX =  \u22120.204708 0.478943 \u22120.519439 \u22120.555730 1.965781 1.393406 0.092908 0.281746 0.769023 1.246435 1.007189 \u22121.296221 0.274992 0.228913 1.352917 0.886429 \u22122.001637 \u22120.371843 1.669025 \u22120.438570 \u22120.539741 0.476985 3.248944 \u22121.021228 \u22120.577087  , (7)\ny = [0.124121, 0.302614, 0.523772, 0.000940, 1.343810]T . (8)\nWe show the iterations of coordinate descent for Lasso with \u03bb = 0 in Table 1 and Figure 1 (a). We set \u03bb = 0 to facilitate the eigenvalue analysis in Section 5. Note that the results reported here also generalize to Lasso, because if we know the sign of the optimal solution \u03b2\u2217, the nonzero entries of \u03b2\u2217 can be solved by the following equivalent convex smooth problem:\nmin \u03b2\n1 2 \u2016 \u2211 i:si 6=0 xi\u03b2i \u2212 y\u201622 + \u03bb \u2211 i:si 6=0 \u03b2isi, (9)\nwhere si = 0 if \u03b2\u2217i = 0, si = 1 if \u03b2 \u2217 i > 0, and si = \u22121 if \u03b2\u2217i < 0.\nIt can be observed from the results in Table 1 and Figure 1 (a) that we can obtain an approximate solution with a small objective function value within a few iterations. However, achieving a solution with high precision takes quite a few iterations for this example. More interestingly, for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. To show this, we compute \u03b1ki that satisfies the following equation:\n\u03b2k+1i = \u03b1 k i \u03b2 k\u22121 i + (1\u2212 \u03b1 k i )\u03b2 k i . (10)\nTable 2 and Figure 1 (b) show the values of \u03b1ki for different iterations. It can be observed that the values of \u03b1ki are almost always positive except \u03b1 2 1 for this example. In addition, most of the values of \u03b1ki are larger than 1. We tried quite a few synthetic data and observed a similar phenomenon.\nFor a particular iteration number k, if \u03b1ki = \u03b1,\u2200i, we can easily achieve \u03b2k+1 = \u03b1\u03b2k\u22121 + (1\u2212\u03b1)\u03b2k without needing to perform any coordinate descent iteration. This motivated us to come up with the successive ray refinement technique to be discussed in the next section."}, {"heading": "3 Successive Ray Refinement", "text": "In the proposed SRR technique, we make use of the ray-continuation property shown in Figure 1, Table 1, and Table 2. Our idea is as follows: To obtain \u03b2k+1, we perform coordinate descent based on a refined search point sk rather than on its previous solution \u03b2k. We propose setting the refined search point as:\nsk = (1\u2212 \u03b1k)hk + \u03b1k\u03b2k, (11)"}, {"heading": "10 1.897021 1.240529 2.128716 2.068764 1.924043 ...", "text": ""}, {"heading": "28 1.939534 1.939949 1.939646 1.939628 1.939556", "text": ""}, {"heading": "29 1.939545 1.939821 1.939620 1.939608 1.939560", "text": ""}, {"heading": "30 1.939552 1.939736 1.939602 1.939594 1.939562", "text": "where hk is a properly chosen history solution, \u03b2k is the current solution, and \u03b1k is an optimal refinement factor that optimizes the following univariate optimization problem:\n\u03b1k = arg min \u03b1 {g(\u03b1) = f((1\u2212 \u03b1)hk + \u03b1\u03b2k)}. (12)\nThe setting of hk to one of the history solutions is based on the following two considerations. First, we aim to use the ray-continuation property to reduce the number of iterations. Second, we need to ensure that the univariate optimization problem in Equation (12) can be efficiently computed. We discuss the computation of Equation (12) in Section 4.\nFigure 2 illustrates the proposed SRR technique. When \u03b1k = 1, we have sk = \u03b2k; that is, the refined search point becomes the current solution \u03b2k. When \u03b1k = 0, we\nhave sk = hk; that is, the refined search point becomes the specified history solution hk. However, our next theorem shows that sk 6= hk because \u03b1k is always positive. In other words, the search point always lies on a ray that starts with the history point hk and passes through the current solution \u03b2k.\nTheorem 1 Assume that the history point hk satisfies\nf(hk) > f(\u03b2k). (13)\nThen, \u03b1k that minimizes Equation (12) is positive. In addition, if Xhk 6= X\u03b2k, \u03b1k is unique.\nProof It is easy to verify that g(\u03b1) is convex. Therefore, \u03b1k that minimizes Equation (12) has at least one solution. Equation (13) leads to\ng(1) < g(0). (14)\nTherefore, the global refinement factor \u03b1k 6= 0. Next, we show that \u03b1k cannot be negative.\nIf \u03b1k < 0, due to the convexity of g(\u03b1), we have\ng((1\u2212 \u03b8)\u03b1k + \u03b8) \u2264 (1\u2212 \u03b8)g(\u03b1k) + \u03b8g(1),\u2200\u03b8 \u2208 [0, 1]. (15)\nSetting \u03b8 = \u03b1 k\n\u03b1k\u22121 , we have\ng(0) \u2264 \u22121 \u03b1k \u2212 1 g(\u03b1k) + \u03b1k \u03b1k \u2212 1 g(1),\u2200\u03b8 \u2208 [0, 1]. (16)\nMaking use of Equation (14), we have g(1) < g(\u03b1k). This contradicts the fact that \u03b1k minimizes Equation (12). Therefore, \u03b1k is always positive. IfXhk 6= X\u03b2k, g(\u03b1) is strongly convex and thus \u03b1k is unique. This ends the proof of this theorem. For coordinate descent, the condition in Equation (13) always holds, because the objective function value keeps decreasing. The selection of an appropriate hk is key to the success of the proposed SRR, and the following theorem says that if hk is good enough, the refined search solution sk is an optimal solution to Equation (1).\nTheorem 2 Let \u03b2\u2217 be an optimal solution to Equation (1). If\n\u03b2\u2217 \u2212 hk = \u03b3(\u03b2k \u2212 hk), (17)\nfor some positive \u03b3, sk achieved by SRR in Equation (11) satisfies f(sk) = f(\u03b2\u2217).\nProof When setting \u03b1k = \u03b3, we have sk = \u03b2\u2217 under the assumption in Equation (17). Therefore, with the SRR technique, we can obtain a refined solution sk that is an optimal solution to Equation (1).\nIn the following subsections, we discuss two schemes for choosing the history solution hk."}, {"heading": "3.1 Successive Ray Refinement Chain", "text": "In the first scheme, we set\nhk = sk\u22121. (18)\nThat is, the history point is set to the most recent refined search point. Figure 3 demonstrates this scheme. Since the generated points follow a chain structure, we call this scheme the Successive Ray Refinement Chain (SRRC). In SRRC, sk\u22121, \u03b2k, and sk lie on the same line. In addition, coordinate descent (CD) controls the direction of the chain. In this illustration, it is assumed that the optimal refinement factor \u03b1k is larger than 1 in each step. According to Theorem 1, \u03b1k > 0. When \u03b1k \u2208 (0, 1), sk lies between sk\u22121 and \u03b2k. When \u03b1k = 1, sk coincides with \u03b2k.\nIn Algorithm 2, we apply the proposed SRRC to coordinate descent for Lasso. Compared with the traditional coordinate descent in Algorithm 1, the coordinate update is based on the search point sk\u22121 rather than on the previous solution \u03b2k\u22121. When \u03b1k in line 9 of Algorithm 2 is set to 1, Algorithm 2 becomes identical to Algorithm 1. Table 3 illustrates Algorithm 2 with the same input X and y that are used in Table 1. Comparing Table 3 with Table 1, we can see that the number of iterations can be significantly reduced with the usage of the SRRC technique. Specifically, to achieve a function value of below 10\u22123, the traditional coordinate descent takes 10 iterations, whereas the one with the SRRC technique takes 7 iterations; to achieve a function value below 10\u22124, the traditional coordinate descent takes 29 iterations, whereas the one with the SRRC technique 14 iterations; and to achieve a function value below 10\u22128, the traditional coordinate descent takes 103 iterations, whereas the one with the SRRC technique takes 16 iterations.\nAs can be seen from Figure 3, we generate two sequences: {sk} and {\u03b2k}. At iteration k, the SRRC technique is very greedy in that it constructs the search point sk by using the two existing points sk\u22121 and \u03b2k to achieve the lowest objective function\nAlgorithm 2 Coordinate Descent plus SRRC (CD+SRRC) for Lasso Input: X , y, \u03bb Output: \u03b2k\n1: Set k = 0, s0 = 0, r0s = y 2: repeat 3: Set k = k + 1, rk = rk\u22121s 4: for i = 1 to p do 5: Compute \u03b2ki = S(s k\u22121 i + xTi r k\n\u2016xi\u201622 , \u03bb\u2016xi\u201622 )\n6: Obtain rk = rk + xi(sk\u22121i \u2212 \u03b2ki ) 7: end for 8: if convergence criterion not satisfied then 9: Set \u03b1k = arg min\u03b1 f((1\u2212 \u03b1)sk\u22121 + \u03b1\u03b2k)\n10: Set sk = (1\u2212 \u03b1k)sk\u22121 + \u03b1k\u03b2k 11: Set rks = (1\u2212 \u03b1k)rk\u22121s + \u03b1krk 12: end if 13: until convergence criterion satisfied\nvalue. If the search point sk\u22121 is dense at some iteration number k and \u03b1k 6= 1, it can be shown that sk is also dense. This is not good for Lasso, which usually has a sparse solution. Interestingly, our empirical simulations show that Algorithm 2 can set \u03b1k = 1 in some iterations, leading to a sparse search point."}, {"heading": "3.2 Successive Ray Refinement Triangle", "text": "In the second scheme, we set\nhk = \u03b2k\u22121. (19)\nFigure 4 demonstrates this scheme. Since the generated points follow a triangle structure, we call this scheme the Successive Ray Refinement Triangle (SRRT). SRRT is less greedy compared to SRRC because \u03b2k\u22121 leads to a higher objective function value than sk\u22121 leads to. However, SRRT can sometimes outperform SRRC in solving Lasso.\nAlgorithm 3 shows the application of the proposed SRRT technique to coordinate descent for Lasso. Similar to Algorithm 2, if\u03b1k in line 9 is set to 1, Algorithm 3 reduces to the traditional coordinate descent in Algorithm 1. Table 4 illustrates Algorithm 3. Similar to SRRC, SRRT greatly reduces the number of iterations used in coordinate descent for Lasso."}, {"heading": "3.3 Convergence of CD plus SRR", "text": "In this subsection, we show that both the combination of CD and SRRC (CD+SRRC) and the combination of CD and SRRT (CD+SRRT) are guaranteed to converge.\nTheorem 3 For the sequence s0,\u03b21, s1,\u03b22, s2,\u03b23, . . . generated by CD+SRRC and CD+SRRT, the objective function value is monotonically decreasing until convergence; that is,\nf(sk\u22121) \u2265 f(\u03b2k) \u2265 f(sk) \u2265 f(\u03b2k+1). (20)\nAlgorithm 3 Coordinate Descent plus SRRT (CD+SRRT) for Lasso Input: X , y, \u03bb Output: \u03b2k\n1: Set k = 0, s0 = 0, r0s = y 2: repeat 3: Set k = k + 1, rk = rk\u22121s 4: for i = 1 to p do 5: Compute \u03b2ki = S(s k\u22121 i + xTi r k\n\u2016xi\u201622 , \u03bb\u2016xi\u201622 )\n6: Obtain rk = rk + xi(sk\u22121i \u2212 \u03b2ki ) 7: end for 8: if convergence criterion not satisfied then 9: Set \u03b1k = arg min\u03b1 f((1\u2212 \u03b1)\u03b2k\u22121 + \u03b1\u03b2k)\n10: Set sk = (1\u2212 \u03b1k)\u03b2k\u22121 + \u03b1k\u03b2k 11: Set rks = (1\u2212 \u03b1k)rk\u22121 + \u03b1krk 12: end if 13: until convergence criterion satisfied\nIn addition, if f(sk\u22121) = f(\u03b2k), we have sk\u22121 = \u03b2k and \u03b2k is an optimal solution; that is,\nf(\u03b2k) = min \u03b2 f(\u03b2). (21)\nTherefore, we have lim k\u2192\u221e f(\u03b2k) = min \u03b2 f(\u03b2). (22)\nProof \u03b2k is computed by applying coordinate descent based on sk\u22121; that is,\n\u03b2ki = arg min \u03b2 f([\u03b2k1 , . . . , \u03b2 k i\u22121, \u03b2, s k\u22121 i+1 , . . . , s k\u22121 p ] T ), (23)\nor equivalently\n\u03b2ki = S(xTi y \u2212\n\u2211 j<i x T i xj\u03b2 k j \u2212 \u2211 j>i x T i xjs k\u22121 j , \u03bb)\n\u2016xi\u201622 . (24)\nTherefore, we have\nf([\u03b2k1 , . . . , \u03b2 k i\u22121, \u03b2 k i , s k\u22121 i+1 , . . . , s k\u22121 p ] T )\n\u2264 f([\u03b2k1 , . . . , \u03b2ki\u22121, sk\u22121i , s k\u22121 i+1 , . . . , s k\u22121 p ]\nT ). (25)\nfor all i. Since \u2016xi\u20162 6= 0, f([\u03b2k1 , . . . , \u03b2ki\u22121, \u03b2, s k\u22121 i+1 , . . . , s k\u22121 p ]\nT ) is strongly convex in \u03b2. As a result, if the equality in Equation (25) holds, we have \u03b2ki = s k\u22121 i . Recursively applying Equation (25), we have the following two facts: f(\u03b2k) \u2264 f(sk\u22121) and if f(\u03b2k) = f(sk\u22121), then sk\u22121 = \u03b2k.\nIf sk\u22121 = \u03b2k, it follows from Equation (24) that \u2016xi\u201622\u03b2ki = S(xTi y \u2212 \u2211 j 6=i xTi xj\u03b2 k j , \u03bb)\n= S(xTi y \u2212 xTi X\u03b2k + \u2016xi\u201622\u03b2ki , \u03bb), (26)\nwhich leads to xTi y \u2212 xTi X\u03b2k \u2208 SGN(\u03b2ki ), (27)\nwhere\nSGN(t) =  {1}, t > 0{\u22121}, t < 0 [\u22121, 1] , t = 0.\n(28)\nSince \u03b2\u2217 is an optimal solution to Equation (1) if and only if\nxTi y \u2212 xTi X\u03b2\u2217 \u2208 SGN(\u03b2\u2217i ),\u2200i, (29)\nit follows from Equation (27) that \u03b2k is an optimal solution to Equation (1). The relationship f(\u03b2k) \u2265 f(sk) is guaranteed by the univariate optimization problem in Equation (12). Therefore, the sequence {f(\u03b2k)} is decreasing. Meanwhile, the squence {f(\u03b2k)} has a lower bound min\u03b2 f(\u03b2). According to the well-known monotone convergence theorem, we have Equation (22).\nThis completes the proof of this theorem."}, {"heading": "4 Efficient Refinement Factor Computation", "text": "In this section, we discuss how to efficiently compute the refinement factor \u03b1k in Equation (12). The function g(\u03b1) can be written as:\ng(\u03b1) = 1\n2\n\u2225\u2225X((1\u2212 \u03b1)hk + \u03b1\u03b2k)\u2212 y\u2225\u22252 2 + \u03bb\u2016(1\u2212 \u03b1)hk + \u03b1\u03b2k\u20161\n= 1\n2 \u2225\u2225rkh \u2212 \u03b1(rkh \u2212 rk)\u2225\u222522 + \u03bb\u2016hk \u2212 \u03b1(hk \u2212 \u03b2k)\u20161, (30) where rkh = y \u2212Xhk and rk = y \u2212X\u03b2k are the residuals that correspond to hk and \u03b2k, respectively. Note that 1) rkh = r k\u22121 s for SRRC and r k h = r\nk\u22121 for SRRT, and 2) both rkh and r\nk have been obtained before line 8 of Algorithm 2 and Algorithm 3. Before the convergence, we have rkh 6= rk. Therefore, g(\u03b1) is strongly convex in \u03b1, and \u03b1k, the minimizer to Equation (12), is unique.\nWhen \u03bb = 0, Equation (12) has a nice closed form solution,\n\u03b1k = \u3008rkh, rkh \u2212 rk\u3009 \u2016rkh \u2212 rk\u201622 . (31)\nNext, we discuss the case \u03bb > 0. The subgradient of g(\u03b1) with regard to \u03b1 can be computed as\n\u2202g(\u03b1) = \u03b1\u2016rkh \u2212 rk\u201622 \u2212 \u3008rkh, rkh \u2212 rk\u3009\n+ \u03bb p\u2211 i=1 (\u03b2ki \u2212 hi)SGN(hi \u2212 \u03b1(hi \u2212 \u03b2ki )). (32)\nCompute \u03b1k is a root-finding problem. According to Theorem 1, we have \u03b1k > 0. Next, we consider only \u03b1 > 0 for \u2202g(\u03b1). We consider the following three cases:\n1. If hi = 0, we have\n(\u03b2ki \u2212 hi)SGN(hi \u2212 \u03b1(hi \u2212 \u03b2ki )) = {|\u03b2ki |}.\n2. If hi(\u03b2ki \u2212 hi) > 0, we have\n(\u03b2ki \u2212 hi)SGN(hi \u2212 \u03b1(hi \u2212 \u03b2ki )) = {|\u03b2ki \u2212 hi|}.\n3. If hi(\u03b2ki \u2212 hi) < 0, we let\nwi = hi\nhi \u2212 \u03b2ki , (33)\nand we have (\u03b2ki \u2212 hi)SGN(hi \u2212 \u03b1(hi \u2212 \u03b2ki )) = {\u2212|\u03b2 k i \u2212 hi|} \u03b1 \u2208 (0, wi)\n{|\u03b2ki \u2212 hi|} \u03b1 \u2208 (wi,+\u221e) |\u03b2ki \u2212 hi|{[\u22121, 1]} \u03b1 = wi.\n(34)\nFor the first two cases, the set SGN(hi \u2212 \u03b1(hi \u2212 \u03b2ki )) is deterministic. For the third case, SGN(hi \u2212 \u03b1(hi \u2212 \u03b2ki )) is deterministic when \u03b1 6= wi. Define\n\u2126(hk,\u03b2k) = {i : hi(\u03b2ki \u2212 hi) < 0}. (35)\nFigure 5 illustrates the function \u2202g(\u03b1), \u03b1 > 0. It can be observed that \u2202g(\u03b1) is a piecewise linear function. If \u2126(hk,\u03b2k) is empty, \u2202g(\u03b1) is continuous; otherwise, \u2202g(\u03b1) is not continuous at \u03b1 = wi, i \u2208 \u2126(hk,\u03b2k)."}, {"heading": "4.1 An Algorithm Based on Sorting", "text": "To compute the refinement factor, one approach is to sort wi as follows:\nFirst, we sort wi, i \u2208 \u2126(hk,\u03b2k), and assume wi0 \u2264 wi1 \u2264 . . . \u2264 wi|\u2126(hk,\u03b2k)| . Second, for j = 1, 2, . . . , |\u2126(hk,\u03b2k)|, we evaluate \u2202g(\u03b1) at \u03b1 = wij with the\nfollowing three cases:\n1. If 0 \u2208 \u2202g(wij ), we have \u03b1k = wij and terminate the search.\n2. If an element in \u2202g(wij ) is positive, \u03b1 k lies in the piecewise line starting \u03b1 =\nwij\u22121 and ending \u03b1 = wij , and it can be analytically computed.\n3. If all elements in \u2202g(wij ) are negative, we set j = j+1 and continue the search.\nFinally, if all elements in \u2202g(wij ) are negative when j = |\u2126(hk,\u03b2k)|, \u03b1k lies on the piecewise line that starts at \u03b1 = wij . Thus, \u03b1\u0303\nk can be analytically computed. With a careful implementation, the naive approach can be completed in O(p + m log(m)), where m = |\u2126(hk,\u03b2k)|. In Lasso, the solution is usually sparse, and thus m is much smaller than p, the number of variables."}, {"heading": "4.2 An Algorithm Based on Bisection", "text": "A second approach is to make use of the improved bisection proposed in [7]. The idea is to 1) determine an initial guess of the interval [\u03b11, \u03b12] to which the root belongs, where all elements in \u2202g(\u03b11) are negative and all elements in \u2202g(\u03b12) are positive, 2) evaluate \u2202g(\u03b1) at \u03b1 = \u03b11+\u03b122 and update the interval to [\u03b11, \u03b1) if all the elements in \u2202g(\u03b1) are positive or to [\u03b1, \u03b12) if all the elements in \u2202g(\u03b1) are negative, 3) set the value of \u03b1 to the largest value of wi that satisfy wi < \u03b1 if all the elements in \u2202g(\u03b1) are positive or to the smallest value of wi that satisfy wi > \u03b1 if all the elements in \u2202g(\u03b1) are negative, and 4) repeat 2) and 3) until finding the root of \u2202g(\u03b1). With a similar implementation as in [7], the improved bisection approach has a time complexity of O(p)."}, {"heading": "5 An Eigenvalue Analysis on the Proposed SRR", "text": "Let A = XXT = L+D + U, (36)\nwhere D is A\u2019s diagonal part, L is A\u2019s strictly lower triangular part, and U is A\u2019s strictly upper triangular part. It is easy to see that\nLij =\n{ xTi xj i < j\n0 i \u2265 j, (37)\nDij =\n{ xTi xi i = j\n0 i 6= j, (38)\nUij =\n{ xTi xj i > j\n0 i \u2264 j. (39)\nWe can rewrite Equation (2) as\nDii\u03b2 k i = S(x T i y \u2212 Li:\u03b2k \u2212 Ui:\u03b2k\u22121, \u03bb), (40)\nwhere Li: and Ui: denote the ith row of L and U , respectively. Therefore, we can write coordinate descent iteration as:\nD\u03b2k = S(XTy \u2212 L\u03b2k \u2212 U\u03b2k\u22121, \u03bb). (41)\nWhen \u03bb = 0, Equation (41) becomes\n(L+D)\u03b2k+1 = XTy \u2212 U\u03b2k, (42)\nwhich is the Gauss-Seidel method for solving\nXTX\u03b2 = (L+D + U)\u03b2 = XTy. (43)\nEquation (43) is also the optimality condition for Equation (1) when \u03bb = 0. Our next discussion is for the case \u03bb = 0 because it is easy to write the linear systems for the iterations.\nDenote G = \u2212(L+D)\u22121U. (44)\nLet G have the following eigendecomposition:\nG = P\u2206P\u22121, (45)\nwhere \u2206 = diag(\u03b41, \u03b42, . . . , \u03b4p) is a diagonal matrix consisting of its eigenvalues.\nLemma 1 The magnitudes of the eigenvalues of G are all less than or equal to 1; that is,\n|\u03b4i| \u2264 1,\u2200i. (46)\nProof Let Gz = \u03c3z, (47)\nwhere \u03c3 is an eigenvalue of G with the corresponding eigenvector being z. Note that \u03c3 and the entries in z can be complex. Using Equation (36) and Equation (44), we have\n(L+D \u2212XTX)z = \u2212Uz = (L+D)\u03c3z. (48)\nwhich leads to (L+D)(1\u2212 \u03c3)z = XTXz. (49)\nIf \u03c3 = 1, the corresponding eigenvector z is in the null space of XTX . If \u03c3 6= 1, we have\n(L+D)z = 1\n(1\u2212 \u03c3) XTXz. (50)\nPremultiplying Equation (50) by zH , the conjugate transpose of z, we have\nzH(L+D)z = 1\n(1\u2212 \u03c3) zHXTXz. (51)\nTaking the conjugate transpose of Equation (51), we have\nzH(U +D)z = 1\n(1\u2212 \u03c3\u0304) zHXTXz, (52)\nwhere \u03c3\u0304 denotes the conjugate of \u03c3. Adding Equation (51) and Equation (52) and subtracting zHXTXz, we have\nzHDz =\n( 1\n(1\u2212 \u03c3\u0304) +\n1 (1\u2212 \u03c3) \u2212 1 ) zHXTXz. (53)\nSince zHDz > 0 and zHXTXz \u2265 0, we have\n0 < 1\n(1\u2212 \u03c3\u0304) +\n1 (1\u2212 \u03c3) \u2212 1 = 1\u2212 |\u03c3| 2 |1\u2212 \u03c3|2 . (54)\nTherefore, we have 1\u2212 |\u03c3|2 > 0 or equivalently |\u03c3| < 1. This ends the proof of this lemma."}, {"heading": "5.1 An Eigenvalue Analysis on CD+SRRC", "text": "For CD+SRRC in Algorithm 2, when \u03bb = 0 we have\n\u03b2k = (L+D)\u22121[XTy \u2212 Usk\u22121] (55)\nsk = (1\u2212 \u03b1k)sk\u22121 + \u03b1k\u03b2k. (56)\nIt can be shown that sk \u2212 sk\u22121 = \u03b1k [ \u2212(L+D)\u22121U + 1\u2212 \u03b1 k\u22121\n\u03b1k\u22121 I\n] (sk\u22121 \u2212 sk\u22122), (57)\n\u03b2k \u2212 \u03b2k\u22121 = \u2212(L+D)\u22121U(sk\u22121 \u2212 sk\u22122). (58)\nWhen k \u2265 2, we denote Ak = \u03b1k [ \u2212(L+D)\u22121U + 1\u2212 \u03b1 k\u22121\n\u03b1k\u22121 I\n] . (59)\nIt can be shown that Ak = P\u03a3kP\u22121, (60)\nwhere \u03a3k = diag(\u03c3k1 , \u03c3 k 2 , . . . , \u03c3 k p) is a diagonal matrix and\n\u03c3ki = \u03b1 k(\u03b4i +\n1\u2212 \u03b1k\u22121\n\u03b1k\u22121 ). (61)\nTherefore, we have sk \u2212 sk\u22121 = ( \u03a0ki=2A k ) (s1 \u2212 s0) = P (\u03a0ki=2\u03a3k)P\u22121(s1 \u2212 s0), (62)\nFor discussion convenience, we let \u03a31 = \u2206 and\nT k = diag(tk1 , t k 2 , . . . , t k p) = \u03a0 k i=1\u03a3 k, (63)\nwhere tki = \u03a0 k i=1\u03c3 k i . (64)\nWe have \u03b2k \u2212 \u03b2k\u22121 = PT k\u22121P\u22121(s1 \u2212 s0). (65)\nFor the traditional coordinate descent in Algorithm 1, \u03b1k = 1,\u2200k. For the proposed CD+SRRC in Algorithm 2, \u03b1k optimizes Equation (12).\nFor the data used in Table 1, the eigenvalues of \u2212(L+D)\u22121U are\n\u03b41 = 0, \u03b42 = 0.00219338, \u03b43 = 0.12412229,\n\u03b44 = 0.62606165, \u03b45 = 0.93956707.\nFor the traditional coordinate descent in Algorithm 1, when k = 30, we have\nt291 = 0, t 29 2 < 0.000001, t 29 3 < 0.000001,\nt294 = 0.000002, t 29 5 = 0.164023.\nFor the coordinate descent with SRRC in Algorithm 2, we have\nt291 = 0, t 29 2 < 0.000001, t 29 3 < 0.000001,\nt294 < 0.000001, t 29 5 = 0.000008.\nThis explains why the proposed SRRC can greatly accelerate the convergence of the coordinate descent method."}, {"heading": "5.2 An Eigenvalue Analysis on CD+SRRT", "text": "For coordinate descent with SRRT in Algorithm 3, when \u03bb = 0 we have\n\u03b2k = (L+D)\u22121[XTy \u2212 Usk\u22121] (66)\nsk = (1\u2212 \u03b1k)\u03b2k\u22121 + \u03b1k\u03b2k. (67)\nWhen k \u2265 3, it can be shown that\n\u03b2k \u2212 \u03b2k\u22121 = G[(1\u2212 \u03b1k\u22122)(\u03b2k\u22122 \u2212 \u03b2k\u22123) + \u03b1k\u22121(\u03b2k\u22121 \u2212 \u03b2k\u22122)]. (68)\nWhen k = 2, we have \u03b22 \u2212 \u03b21 = \u03b11G(\u03b21 \u2212 \u03b20). (69)\nUsing the recursion in Equation (68), we can get\n\u03b23 \u2212 \u03b22 = [(1\u2212 \u03b11)G+ \u03b11G\u03b12G](\u03b21 \u2212 \u03b20). (70)\n\u03b24 \u2212 \u03b23 =[\u03b11G(1\u2212 \u03b12)G+ (1\u2212 \u03b11)G\u03b13G + \u03b11G\u03b12G\u03b13G](\u03b21 \u2212 \u03b20).\n(71)\nGenerally speaking, we can write\n\u03b2k \u2212 \u03b2k\u22121 = PT k\u22121P\u22121(\u03b21 \u2212 \u03b20), (72)\nwhere T k = diag(tk1 , t k 2 , . . . , t k p) is a diagonal matrix. For t k i , it is a polynomial function of \u03b4i; that is, tki = \u03c6k(\u03b4i), where\n\u03c6k(t) = t\u00d7 . . .\u00d7 t\ufe38 \ufe37\ufe37 \ufe38 dk/2e k\u2212dk/2e\u2211 i=0 ci \u00d7 t\u00d7 . . .\u00d7 t\ufe38 \ufe37\ufe37 \ufe38 i ), (73)\nand c0, c1, . . . , ck\u2212dk/2e are dependent on \u03b11, \u03b12, . . . , \u03b1k\u22121. When k = 2, we have\nc0 = \u03b1 1. (74)\nWhen k = 3, we have c0 = 1\u2212 \u03b11, c1 = \u03b1 1\u03b12. (75)\nWhen k = 4, we have\nc0 = \u03b1 1(1\u2212 \u03b12) + \u03b13(1\u2212 \u03b11), c1 = \u03b1 1\u03b12\u03b13.\n(76)\nWhen k = 5, we have\nc0 = (1\u2212 \u03b12)(1\u2212 \u03b13), c1 = \u03b1\n1(1\u2212 \u03b12)\u03b14 + \u03b13(1\u2212 \u03b11)\u03b14 + \u03b11\u03b12(1\u2212 \u03b13), c2 = \u03b1 1\u03b12\u03b13\u03b14. (77)\nFor the coordinate descent with SRRT in Algorithm 3, we have\nt291 = 0, t 29 2 < 0.000001, t 29 3 < 0.000001,\nt294 = 0.000002, t 29 5 = 0.000393,\nwhich are smaller than the ones in the traditional coordinate descent shown in Section 5.1."}, {"heading": "6 Related Work", "text": "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9]."}, {"heading": "6.1 Relationship between SRRC and Successive Over-Relaxation", "text": "Successive over-relaxation (SOR) is a classical approach for accelerating the GaussSeidel approach. Our discussion in this section considers only \u03bb = 0, because SOR targets the acceleration of the Gauss-Seidel approach.\nFrom Equation (43), we have\n(wL+D)\u03b2 = wXTy \u2212 wU\u03b2 \u2212 (w \u2212 1)D\u03b2,\u2200w > 0. (78)\nThe iteration used in successive over-relaxation is:\n\u03b2k = (wL+D)\u22121[wXT y \u2212 [wU + (w \u2212 1)D]\u03b2k\u22121, (79)\nwhich can be obtained by plugging \u03b2k and \u03b2k\u22121 into Equation (78). Equation (79) can be rewritten as:\n\u03b2k = \u03b2k\u22121 \u2212 w(wL+D)\u22121[XTX\u03b2k\u22121 \u2212XTy]. (80)\nFor the proposed CD+SRRC in Algorithm 2, when \u03bb = 0 we have\nsk = (1\u2212 \u03b1k)sk\u22121 + \u03b1k(L+D)\u22121 [ XTy \u2212 Usk\u22121 ] = sk\u22121 \u2212 \u03b1k(L+D)\u22121 [ XTXsk\u22121 \u2212XTy ] .\n(81)\nWhen w = 1 and \u03b1k = 1, both SOR and CD+SRRC reduce to the traditional coordinate descent. Equation (79) and Equation (81) share the following two similaries: 1) both make use of the gradient in the recursive iterations in that XTX\u03b2k\u22121 \u2212XTy is the gradient of 12\u2016X\u03b2 \u2212 y\u2016 2 2 at \u03b2\nk\u22121 and XTXsk\u22121 \u2212 XTy is the gradient of 1 2\u2016Xs\nk\u22121 \u2212 y\u201622 at sk\u22121, and 2) both use a precondition matrix in that SOR uses (wL + D) whereas SRRC uses (L + D). A key difference is that the precondition matrix used in SRRC is parameter-free whereas the one used in SOR has a parameter. As a result, we can perform an inexpensive univariate search to find the optimal \u03b1k used in SRRC whereas it is usually expensive for SOR to search for an optimal w in the same way as Equation (12).\nWhen the design matrix has some special structures, it has been shown in [17] that the optimal value of w can be found for SOR. However, for the general design matrix X , it is hard to obtain the optimal w used for SOR. This might be a major reason that SOR is not widely used in solving Lasso with coordinate descent. For our proposed SRRC, the criterion in Equation (12) enables us to adaptively set the refinement factor \u03b1k."}, {"heading": "6.2 Relationship between SRRT and the Nesterov\u2019s Method", "text": "The SRRT scheme presented in Figure 4 is similar to the Nesterov\u2019s method in that both make use of a search point in the iterations. In addition, both set the search point using\nsk = (1\u2212 \u03b1k)\u03b2k\u22121 + \u03b1k\u03b2k. (82)\nHowever, the key difference is that the \u03b1k used in the Nesterov\u2019s method is predefined according to a specified formula, whereas the \u03b1k used in SRRT is set to optimize the objective function as shown in Equation (12). Note that if the Nesterov\u2019s method sets the \u03b1k to optimize the objective function, it reduces to the traditional steepest descent method thus the good acceleration property of the Nesterov\u2019s method is gone."}, {"heading": "7 Experiments", "text": "In this section, we report experimental results for synthetic and real data sets, studying the number of iterations of CD, CD+SRRC and CD+SRRT for solving Lasso. The consumed computational time is proportional to the number of iterations.\nSynthetic Data Sets We generate the synthetic data as follows. The entries in the n\u00d7p design matrix X and the n\u00d7 1 response y are drawn from a Gaussian distribution. We try the following three settings of n and p: 1) n = 500, p = 1000, 2) n = 1000, p = 1000, and 3) n = 1000, p = 500.\nReal Data Sets We make use of the following three real data sets provided in [2]: leukemia, colon, and gisette. The leukemia data set has n = 38 samples and p = 7129 variables. The colon data set has n = 62 samples and p = 2000 variables. The gisette data set has n = 6000 samples and p = 5000 variables.\nExperimental Settings For the value of the regularization parameter, we try \u03bb = r\u2016XTy\u2016\u221e, where r = 0.5, 0.1, 0.05, 0.01. For the synthetic data sets, the reported results are averaged over 10 runs. For a particular regularization parameter, we first run CD in Algorithm 1 until \u2016\u03b2k \u2212 \u03b2k\u22121\u20162 \u2264 10\u22126, and then run CD+SRRC and CD+SRRT until the obtained objective function value is less than or equal to the one obtained by CD.\nResults Table 5 and Table 6 show the results for the synthetic and real data sets, respectively. The last column of each table shows the sparsity of the obtained Lasso solution, which is defined as the number of zero entries in the solution divided by the number of variables p. Figure 6 visualizes the results in these two tables. We can see that when the solution is very sparse (for example, \u03bb = 0.5\u2016XTy\u2016\u221e), the proposed CD+SRRC and CD+SRRT consume comparable number of iterations to the traditional CD. The reason is that the optimal refinement factor computed by SRR in Equation (12) is equal to or close to 1, and thus CD+SRRC and CD+SRRT is very close to the traditional CD. Note that a regularization parameter \u03bb = 0.5\u2016XTy\u2016\u221e is usually too large for practical applications because it selects too few variables, and we usually need to try a smaller \u03bb = r\u2016XTy\u2016\u221e for example, r = 0.01. It can be observed that the proposed CD+SRRC and CD+SRRT requires much fewer iterations, especially for smaller regularization parameters."}, {"heading": "8 Conclusion", "text": "In this paper, we propose a novel technique called successive ray refinement. Our proposed SRR is motivated by an interesting ray-continuation property on the coordinate descent iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. We propose two schemes for SRR and apply them to solving Lasso with coordinate descent. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially when the regularization parameter is small.\nWe have established the convergence of CD+SRR, and it is interesting to study the convergence rate. We focus on a least squares loss function in (1), and we plan to apply the SRR technique to solving the generalized linear models. We compute the refinement factor as an optimal solution to Equation (12), and we plan to obtain the refinement factor as an approximate solution, especially in the case of generalized linear models."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J.H. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "An interior-point method for large-scale l1regularized logistic regression", "author": ["K. Koh", "S. Kim", "S. Boyd"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Efficient euclidean projections in linear time", "author": ["J. Liu", "J. Ye"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Safe screening with variational inequalities and its application to lasso", "author": ["J. Liu", "Z. Zhao", "J. Wang", "J. Ye"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Introductory lectures on convex optimization : a basic course", "author": ["Y. Nesterov"], "venue": "Applied optimization. Kluwer Academic Publ.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Safe screening of non-support vectors in pathwise SVM computation", "author": ["K. Ogawa", "Y. Suzuki", "I. Takeuchi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Stochastic methods for `1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J.H. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng", "Communicated O.L. Mangasarian"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "B. Lin", "P. Gong", "P. Wonka", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Iterative methods for solving partial difference equations of elliptical type", "author": ["D.M. Yong"], "venue": "PhD thesis,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1950}, {"title": "An improved glmnet for l1-regularized logistic regression", "author": ["G.X. Yuan", "C.H. Ho", "C.J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Learning sparse representations of high dimensional data on large scale dictionaries", "author": ["J.X. Zhen", "X. Hao", "J.R. Peter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "Lasso [12] is an effective technique for analyzing high-dimensional data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "Least Angle Regression (LARS) [3] is one of the most well-known homotopy approaches for Lasso.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 52, "endOffset": 59}, {"referenceID": 0, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 89, "endOffset": 96}, {"referenceID": 15, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 89, "endOffset": 96}, {"referenceID": 5, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 151, "endOffset": 155}, {"referenceID": 4, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 7, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 9, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 12, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 14, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 18, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 13, "context": "Since the non-smooth `1 penalty in Equation (1) is separable, the algorithm is guaranteed to converge [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "If \u03b1 < 0, due to the convexity of g(\u03b1), we have g((1\u2212 \u03b8)\u03b1 + \u03b8) \u2264 (1\u2212 \u03b8)g(\u03b1) + \u03b8g(1),\u2200\u03b8 \u2208 [0, 1].", "startOffset": 89, "endOffset": 95}, {"referenceID": 0, "context": "Setting \u03b8 = \u03b1 k \u03b1k\u22121 , we have g(0) \u2264 \u22121 \u03b1k \u2212 1 g(\u03b1) + \u03b1 \u03b1k \u2212 1 g(1),\u2200\u03b8 \u2208 [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 6, "context": "2 An Algorithm Based on Bisection A second approach is to make use of the improved bisection proposed in [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "With a similar implementation as in [7], the improved bisection approach has a time complexity of O(p).", "startOffset": 36, "endOffset": 39}, {"referenceID": 16, "context": "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 16, "context": "When the design matrix has some special structures, it has been shown in [17] that the optimal value of w can be found for SOR.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Real Data Sets We make use of the following three real data sets provided in [2]: leukemia, colon, and gisette.", "startOffset": 77, "endOffset": 80}], "year": 2015, "abstractText": "Coordinate descent is one of the most popular approaches for solving Lasso and its extensions due to its simplicity and efficiency. When applying coordinate descent to solving Lasso, we update one coordinate at a time while fixing the remaining coordinates. Such an update, which is usually easy to compute, greedily decreases the objective function value. In this paper, we aim to improve its computational efficiency by reducing the number of coordinate descent iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). SRR makes use of the following ray continuation property on the successive iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, it lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, it achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point and show that the refined search point can be efficiently obtained. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially for small Lasso regularization parameters.", "creator": "LaTeX with hyperref package"}}}