{"id": "1512.06612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "Backward and Forward Language Modeling for Constrained Sentence Generation", "abstract": "recent language models, especially those based on recurrent neural networks ( rnns ), make it possible to comprehend natural language from a learned probability. text generation has wide advantages including machine translation, summarization, question answering, conversation systems, etc. existing methods typically predicted a joint probability of words counting on additional information, which is ( either statically or dynamically ) fed to linguistics's interaction layer. in many cases, we are likely to impose hard constraints on the generated code, i. e., a particular word must appear in the sentence. unfortunately, existing methods could not solve this problem. in this paper, we propose a backbone language hierarchy ( backbone lm ) for constrained language generation. provided a specific word, our model derives previous forms and redundant words successively. in this view, the given word could appear at any position in the dictionary. experimental results show that previously generated texts are coherent physically fluent.", "histories": [["v1", "Mon, 21 Dec 2015 13:07:31 GMT  (222kb,D)", "https://arxiv.org/abs/1512.06612v1", null], ["v2", "Sun, 3 Jan 2016 20:15:44 GMT  (445kb,D)", "http://arxiv.org/abs/1512.06612v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["lili mou", "rui yan", "ge li", "lu zhang", "zhi jin"], "accepted": false, "id": "1512.06612"}, "pdf": {"name": "1512.06612.pdf", "metadata": {"source": "CRF", "title": "Backward and Forward Language Modeling for Constrained Sentence Generation", "authors": ["Lili Mou", "Rui Yan", "Ge Li", "Lu zhang", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com", "rui.yan.peking@gmail.com", "lige@sei.pku.edu.cn", "zhanglu@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Language modeling is aimed at minimizing the joint probability of a corpus. It has long been the core of natural language processing (NLP) [8], and has inspired a variety of other models, e.g., the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17]. In particular, the renewed prosperity of neural models has made groundbreaking improvement in many tasks, including language modeling per se [2], part-of-speech tagging, named entity recognition, semantic role labeling [6], etc.\nThe recurrent neural network (RNN) is a prevailing class of language models; it is suitable for modeling time-series data (e.g., a sequence of words) by its iterative nature. An RNN usually keeps one or a few hidden layers; at each time slot, it reads a word, and changes its state accordingly. Compared with traditional n-gram models, RNNs are more capable of learning long range features\u2014especially with long short term memory (LSTM) units [7] or gated recurrent units (GRU) [5]\u2014and hence are better at capturing the nature of sentences. On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18]. The sentence generation process\nar X\niv :1\n51 2.\n06 61\n2v 2\n[ cs\n.C L\n] 3\nis typically accomplished by choosing the most likely word at a time, conditioned on previous words as well as additional information depending on the task (e.g., the vector representation of the source sentence in a machine translation system [15]).\nIn many scenarios, however, we are likely to impose constraints on the generated sentences. For example, a question answering system may involve analyzing the question and querying an existing knowledge base, to the point of which, a candidate answer is at hand. A natural language generator is then supposed to generate a sentence, coherent in semantics, containing the candidate answer. Unfortunately, using existing language models to generate a sentence with a given word is non-trivial: adding additional information [16, 19] about a word does not guarantee that the wanted word will appear; generic probabilistic samplers (e.g., Markov chain Monte Carlo methods) hardly applies to RNN language models1; setting an arbitrary word to be the wanted word damages the fluency of a sentence; imposing the constraint on the first word restricts the form of generated sentences.\nIn this paper, we propose a novel backward and forward (B/F) language model to tackle the problem of constrained natural language generation. Rather than generate a sentence from the first word to the last in sequence as in traditional models, we use RNNs to generate previous and subsequent words conditioned on the given word. The forward and backward generation can be accomplished either simultaneously or asynchronously, resulting in two variants, syn-B/F and asyn-B/F. In this way, our model is complete in theory for generating a sentence with a wanted word, which can appear at an arbitrary position in the sentence.\nThe rest of this paper is organized as follows. Section 2 reviews existing language models and natural language generators. Section 3 describes the proposed B/F language models in detail. Section 4 presents experimental results. Finally, we have conclusion in Section 5."}, {"heading": "2 Background and Related Work", "text": ""}, {"heading": "2.1 Language Modeling", "text": "Given a corpus w = w1, \u00b7 \u00b7 \u00b7 , wm, language modeling aims to minimize the joint distribution of w, i.e. p(w). Inspired by the observation that people always say a sentence from the beginning to the end, we would like to decompose the joint probability as2\np(w) = m\u220f t=1 p(wt|wt\u221211 ) (1)\nParameterizing by multinomial distributions, we need to further simplify the above equation in order to estimate the parameters. Imposing a Markov assumption\u2014a word is only dependent on previous n\u2212 1 words and independent of its position\u2014results in the classic n-gram model, where the joint probability is given by\np(w) \u2248 m\u220f t=1 p ( wt \u2223\u2223wt\u22121t\u2212n+1) (2)\nTo mitigate the data sparsity problem, a variety of smoothing methods have been proposed. We refer interested readers to textbooks like [8] for n-gram models and their variants.\n1With recent efforts in [3]. 2 w1, w2, \u00b7 \u00b7 \u00b7 , wt is denoted as wt1 for short.\nBengio et al. [2] propose to use feed-forward neural networks to estimate the probability in Equation 2. In their model, a word is first mapped to a small dimensional vector, known as an embedding ; then a feed-forward neural network propagates information to a softmax output layer, which estimates the probability of the next word.\nA recurrent neural network (RNN) can also be used in language modeling. It keeps a hidden state vector (ht at time t), dependent on the its previous state (ht\u22121) and the current input vector x, the word embedding of the current word. An output layer estimates the probability that each word occurs at this time slot. Following are listed the formulas for a vanilla RNN.3\nht = RNN(xt,ht\u22121) (3)\n= f (Winxt +Whidht\u22121) (4)\np(wt|wt\u221210 ) \u2248 softmax (Woutht) (5)\nAs is indicated from the equations, an RNN provides a means of direct parametrization of Equation 1, and hence has the ability to capture long term dependency, compared with ngram models. In practice, the vanilla RNN is difficult to train due to the gradient vanishing or exploding problem; long short term (LSTM) units [7] and gated recurrent units (GRU) [5] are proposed to better balance between the previous state and the current input."}, {"heading": "2.2 Language Generation", "text": "Using RNNs to model the joint probability of language makes it feasible to generate new sentences. An early attempt generates texts by a character-level RNN language model [14]; recently, RNN-based language generation has made breakthroughs in several real applications.\nThe sequence to sequence machine translation model [15] uses an RNN to encode a source sentence (in foreign language) into one or a few fixed-size vectors; another RNN then decodes the vector(s) to the target sentence. Such network can be viewed as a language model, conditioned on the source sentence. At each time step, the RNN predicts the most likely word as the output; the embedding of the word is fed to the input layer at next step. The process continues until the RNN generates a special symbol <eos> indicating the end of the sequence. Beam search [15] or sampling methods [16] can be used to improve the quality and diversity of generated texts.\nIf the source sentence is too long to fit into one or a few fixed-size vectors, an attention mechanism [1] can be used to dynamically focus on different parts of the source sentence during target generation. In other studies, Wen et al. use an RNN to generate a sentence based on some abstract representations of semantics; they feed a one-hot vector, as additional information, to the RNN\u2019s hidden layer [16]. In a question answering system, Yin et al. leverage a soft logistic switcher to either generate a word from the vocabulary or copy the candidate answer [19]."}, {"heading": "3 The Proposed B/F Language Model", "text": "In this part, we introduce our B/F language model in detail. Our intuition is to seek a new approach to decompose the joint probability of a sentence (Equation 1). If we know a priori\n3W \u2019s refer to weights; biases are omitted.\nthat a word ws should appear in the sentence (w = w1, \u00b7 \u00b7 \u00b7 , wm, ws \u2208 w), it is natural to design a Bayesian network where ws is the root node, and other words are conditioned on ws. Following the spirit of \u201csequence\u201d generation, ws split the sentence into two subsequences:\n\u2022 Backward sequence: ws, ws\u22121, ws\u22122, \u00b7 \u00b7 \u00b7 , w1 (s words)\n\u2022 Forward sequence: ws, ws+1, ws+2, \u00b7 \u00b7 \u00b7 , wn (m\u2212 s+ 1 words)\nThe probability that the sentence w with the split word at position s decomposes as follows.4\np ( w1s wns ) = p(ws) s\u220f i=0 p(bw)(ws\u2212i|\u00b7) m\u2212s+1\u220f i=0 p(fw)(ws+1|\u00b7) (6)\nTo parametrize the equation, we propose two model variants. The first approach is to generate previous and backward models simultaneously, and we call this syn-B/F language model (Figure 1).5 Concretely, Equation 6 takes the form\np ( w1s wns ) = max{s,m\u2212s+1}\u22121\u220f t=0 p ( ws\u2212t ws+t \u2223\u2223\u2223\u2223\u2223 ws\u2212t+1sws+t\u22121s )\n(7)\nwhere the factor p(=|=) refers to the conditional probability that current time step t generates ws\u2212t, ws+t in the forward and backward sequences, respectively, given the middle part of the sentence, that is, ws\u2212t+1 \u00b7 \u00b7 \u00b7ws \u00b7 \u00b7 \u00b7ws+t\u22121. If one part has generated <eos>, we pad the special symbol <eos> for this sequence until the other part also terminates.\n4p( \u00b7 = \u00b7 ) denotes the probability of a particular backward/forward sequence. 5Previously called backbone LM.\nFollowing the studies introduced in Section 2, we also leverage a recurrent neural network (RNN) . The factors in Equation 7 are computed by\np\n( ws\u2212t\nws+t \u2223\u2223\u2223\u2223\u2223 ws\u2212t+1sws+t\u22121s )\n(8)\n=p(bw) (ws\u2212t|ht) \u00b7 p(fw) (ws+t|ht) (9) = softmax ( W\n(bw) out ht\n) \u00b7 softmax ( W\n(fw) out ht\n) (10)\nHere, ht is the hidden layer, which is dependent on the previous state ht\u22121 and current input word embeddings x\u0303 = [ x (fw) t ;x (bw) t ] . We use GRU [5] in our model, given by\nr = \u03c3(Wrx\u0303 + Urht\u22121) (11)\nz = \u03c3(Wzx\u0303 + Uzht\u22121) (12) h\u0303 = tanh ( Wxx\u0303 + Ux(r \u25e6 ht\u22121) ) (13)\nht = (1\u2212 z) \u25e6 ht\u22121 + z \u25e6 h\u0303 (14)\nwhere \u03c3(\u00b7) = 1 1+e(\u2212\u00b7) ; \u25e6 denotes element-wise product. r and z are known as gates, h\u0303 the candidate hidden state at the current step.\nIn the syn-B/F model, we design a single RNN to generate both chains in hope that each is aware of the other\u2019s state. Besides, we also propose an asynchronous version, denoted as asyn-B/F (Figure 2). The idea is to first generate the backward sequence, and then feed the obtained result to another forward RNN to generate future words. The detailed formulas are not repeated.\nIt is important to notice that asyn-B/F\u2019s RNN for backward sequence generation is different from a generic backward LM. The latter is presumed to model a sentence from the last word to the first one, whereas our backward RNN is, in fact, a \u201chalf\u201d LM, starting from ws.\nTraining Criteria. If we assume ws is always given, the training criterion shall be the crossentropy loss of all words in both chains except ws. We can alternatively penalize the split word ws in addition, which will make it possible to generate an entire sentence without ws being provided. We do not deem the two criteria differ significantly, and adopt the latter one in our experiments.\nBoth labeled and unlabeled datasets suffice to train the B/F language model. If a sentence is annotated with a specially interesting word ws, it is natural to use it as the split word. For an unlabeled dataset, we can randomly choose a word as ws.\nNotice that Equation 6 gives the joint probability of a sentence w with a particular split word ws. To compute the probability of the sentence, we shall marginalize out different split words, i.e.,\np(w) = m\u2211 s=1 p ( w1s wns ) (15)\nIn our scenarios, however, we always assume that ws is given in practice. Hence, different from language modeling in general, the joint probability of a sentence is not the number one concern in our model."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 The Dataset and Settings", "text": "To evaluate our B/F LMs, we prefer a vertical domain corpus with interesting application nuggets instead of using generic texts like Wikipedia. In particular, we chose to build a language model upon scientific paper titles on arXiv.6 Building a language model on paper titles may help researchers when they are preparing their drafts. Provided a topic (designated by a given word), constrained natural language generation could also acts as a way of brainstorming.7\nWe crawled computer science-related paper titles from January 2014 to November 2015.8 Each word was decapitalized, but no stemming was performed. Rare words (\u2264 10 occurrences) were grouped as a single token, <unk>, (referring to unknown). We removed non-English titles, and those with more than three <unk>\u2019s. We notice that <unk>\u2019s may appear frequently, but a large number of them refer to acronyms, and thus are mostly consistent in semantics.\nCurrently, we have 25,000 samples for training, 1455 for validation and another 1455 for testing; our vocabulary size is 3380. The asyn-B/F has one hidden layer with 100 units; synB/F has 200; This makes a fair comparison because syn-B/F should simultaneously learn implicit forward and backward LMs, which are completely different. In our models, embeddings are 50 dimensional, initialized randomly. To train the model, we used standard backpropagation (batch size 50) with element-wise gradient clipping. Following [9], we applied rmsprop for optimization (embeddings excluded), which is more suitable for training RNNs than na\u0308\u0131ve stochastic gradient descent, and less sensitive to hyperparameters compared with momentum methods. Initial weights were uniformly sampled from [\u22120.08, 0.08]. Initial learning rate was\n6http://arxiv.org 7The title of this paper is NOT generated by our model. 8Crawled from http://http://dblp.uni-trier.de/db/journals/corr/\n0.002, with a multiplicative learning rate decay of 0.97, moving average decay 0.99, and a damping term = 10\u22128. As word embeddings are sparse in use [12], they were optimized asynchronously by pure stochastic gradient descent with learning rate being divided by \u221a .9"}, {"heading": "4.2 Results", "text": "We first use the perplexity measure to evaluate the learned language models. Perplexity is defined as 2\u2212`, where ` is the log-likelihood (with base 2), averaged over each word.\n` = 1\nm m\u2211 i=1 log p(wi)\nNote that <eos> is not considered when we compute the perplexity. We compare our models with several baselines:\n\u2022 Sequential LM: A pure LM, which is not applicable to constrained sentence generation.\n\u2022 Info-all: Built upon sequential LM, Info-all takes the wanted word\u2019s embedding as additional input at each time step during sequence generation, similar to [16].\n\u2022 Info-init: The wanted word\u2019s embedding is only added at the first step (sequence to sequence model [15]).\n\u2022 Sep-B/F: We train two separate forward and backward LMs (both starting from the split word).\nTable 1 summarizes the perplexity of our models and baselines. We further plot the perplexity of a word with respect to its position when generation (Figure 3).\nFrom the experimental results, we have the following observations.\n\u2022 All B/F variants yield a larger perplexity than a sequantial LM. This makes much sense because randomly choosing a split word increases uncertainly. It should be also noticed that, in our model, the perplexity reflects the probability of a sentence with a specific split word, whereas the perplexity of the sequential LM assesses the probability of a sentence itself.\n9The implementation was based on [10, 11].\nSheet1\nPage\u00a01\n\u2022 Randomly choosing a split word cannot make use of position information in sentences. The titles of scientific papers, for example, oftentimes follow templates, which may begin with \u201c<unk> : an approach\u201d or \u201c<unk> - based approach.\u201d Therefore, sequential LM yields low perplexity when generating the word at a particular position (t = 2), but such information is smoothed out in our B/F LMs because the split word is chosen randomly.\n\u2022 When t is large (e.g., t \u2265 4), B/F models yield almost the same perplexity as sequential LM. The long term behavior is similar to sequential LM, if we rule out the impact of choosing random words. For syn-B/F, in particular, the result indicates that feeding two words\u2019 embeddings to the hidden layer does not add to confusion.\n\u2022 In our applications, ws is always given, which indicates p(ws) = 1 (denoted as ws oracle in Table 1). This reduces the perplexity to less than 100, showing that our B/F LMs can well make use of such information that some word should appear in the generated text. Further, our syn-B/F is better than na\u0308\u0131ve sep-B/F; asyn-B/F is further capable of integrating information in backward and forward sequences.\nWe then generate new paper titles from the learned language model with a specific word being given, which can be thought of, in the application, as a particular interest of research topics. Table 2 illustrates examples generated from B/F models and baselines. As we see, for words that are common at the beginning of a paper title\u2014like the adjective convolutional and gerund tracking\u2014sequential LM can generate reasonable results. For plural nouns like systems and models, the titles generated by sequential LM are somewhat influent, but they basically comply with grammar rules. For words that are unlikely to be the initial word, sequential LM fails to generate grammatically correct sentences.\nAdding additional information does guide the network to generate sentences relevant to the topic, but the wanted word may not appear. The problem is also addressed in [16].\nBy contrast, B/F LMs have the ability to generate correct sentences. But the sep-B/F model is too greedy in its each chain. As generating short and general texts is a known issue with neural network-based LMs, sep-B/F can hardly generate a sentence containing much substance. syn-B/F is better, and asyn-B/F is able to generate sentences whose quality is comparable with sequential LMs."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a backward and forward language model (B/F LM) for constrained natural language generation. Given a particular word, our model can generate previous words and future words either synchronously or asynchronously. Experiments show a similar perplexity to sequential LM, if we disregard the perplexity introduced by random splitting. Our case study demonstrates that the asynchronous B/F LM can generate sentences that contain the given word and are comparable to sequential LM in quality."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Bidirectional recurrent neural networks as generative models", "author": ["M. Berglund", "T. Raiko", "M. Honkala", "L. K\u00e4rkk\u00e4inen", "A. Vetek", "J.T. Karhunen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Speech and Language Processing", "author": ["D. Jurafsky", "J.H. Martin"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["L. Mou", "G. Li", "L. Zhang", "T. Wang", "Z. Jin"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["L. Mou", "H. Peng", "G. Li", "Y. Xu", "L. Zhang", "Z. Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A comparative study on regularization strategies for embedding-based neural networks", "author": ["H. Peng", "L. Mou", "G. Li", "Y. Chen", "Y. Lu", "Z. Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Y. Xu", "L. Mou", "G. Li", "Y. Chen", "H. Peng", "Z. Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Attention with intention for a neural network conversation model", "author": ["K. Yao", "G. Zweig", "B. Peng"], "venue": "arXiv preprint arXiv:1510.08565 (NIPS Workshop),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Neural generative question answering", "author": ["J. Yin", "X. Jiang", "Z. Lu", "L. Shang", "H. Li", "X. Li"], "venue": "arXiv preprint arXiv:1512.01337,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "It has long been the core of natural language processing (NLP) [8], and has inspired a variety of other models, e.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 4, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 15, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 1, "context": "In particular, the renewed prosperity of neural models has made groundbreaking improvement in many tasks, including language modeling per se [2], part-of-speech tagging, named entity recognition, semantic role labeling [6], etc.", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "In particular, the renewed prosperity of neural models has made groundbreaking improvement in many tasks, including language modeling per se [2], part-of-speech tagging, named entity recognition, semantic role labeling [6], etc.", "startOffset": 219, "endOffset": 222}, {"referenceID": 5, "context": "Compared with traditional n-gram models, RNNs are more capable of learning long range features\u2014especially with long short term memory (LSTM) units [7] or gated recurrent units (GRU) [5]\u2014and hence are better at capturing the nature of sentences.", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": ", the vector representation of the source sentence in a machine translation system [15]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "Unfortunately, using existing language models to generate a sentence with a given word is non-trivial: adding additional information [16, 19] about a word does not guarantee that the wanted word will appear; generic probabilistic samplers (e.", "startOffset": 133, "endOffset": 141}, {"referenceID": 17, "context": "Unfortunately, using existing language models to generate a sentence with a given word is non-trivial: adding additional information [16, 19] about a word does not guarantee that the wanted word will appear; generic probabilistic samplers (e.", "startOffset": 133, "endOffset": 141}, {"referenceID": 6, "context": "We refer interested readers to textbooks like [8] for n-gram models and their variants.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "With recent efforts in [3].", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "[2] propose to use feed-forward neural networks to estimate the probability in Equation 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In practice, the vanilla RNN is difficult to train due to the gradient vanishing or exploding problem; long short term (LSTM) units [7] and gated recurrent units (GRU) [5] are proposed to better balance between the previous state and the current input.", "startOffset": 132, "endOffset": 135}, {"referenceID": 12, "context": "An early attempt generates texts by a character-level RNN language model [14]; recently, RNN-based language generation has made breakthroughs in several real applications.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "The sequence to sequence machine translation model [15] uses an RNN to encode a source sentence (in foreign language) into one or a few fixed-size vectors; another RNN then decodes the vector(s) to the target sentence.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "Beam search [15] or sampling methods [16] can be used to improve the quality and diversity of generated texts.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "Beam search [15] or sampling methods [16] can be used to improve the quality and diversity of generated texts.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "If the source sentence is too long to fit into one or a few fixed-size vectors, an attention mechanism [1] can be used to dynamically focus on different parts of the source sentence during target generation.", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "use an RNN to generate a sentence based on some abstract representations of semantics; they feed a one-hot vector, as additional information, to the RNN\u2019s hidden layer [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "leverage a soft logistic switcher to either generate a word from the vocabulary or copy the candidate answer [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "Following [9], we applied rmsprop for optimization (embeddings excluded), which is more suitable for training RNNs than n\u00e4\u0131ve stochastic gradient descent, and less sensitive to hyperparameters compared with momentum methods.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "As word embeddings are sparse in use [12], they were optimized asynchronously by pure stochastic gradient descent with learning rate being divided by \u221a .", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "\u2022 Info-all: Built upon sequential LM, Info-all takes the wanted word\u2019s embedding as additional input at each time step during sequence generation, similar to [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "\u2022 Info-init: The wanted word\u2019s embedding is only added at the first step (sequence to sequence model [15]).", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "The implementation was based on [10, 11].", "startOffset": 32, "endOffset": 40}, {"referenceID": 9, "context": "The implementation was based on [10, 11].", "startOffset": 32, "endOffset": 40}, {"referenceID": 14, "context": "The problem is also addressed in [16].", "startOffset": 33, "endOffset": 37}], "year": 2016, "abstractText": "Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN\u2019s hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing approaches could not solve this problem. In this paper, we propose a novel backward and forward language model. Provided a specific word, we use RNNs to generate previous words and future words, either simultaneously or asynchronously, resulting in two model variants. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are comparable to sequential LMs in quality.", "creator": "LaTeX with hyperref package"}}}