{"id": "1512.06633", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "Constrained Sampling and Counting: Universal Hashing Meets SAT Solving", "abstract": "constrained sampling and counting are two fundamental assumptions in artificial intelligence of a diverse range of applications, spanning probabilistic reasoning and broadly to constrained - random verification. ; the theory of these problems was thoroughly investigated in the 1980s, prior work either did not scale to industrial size estimate or gave up correctness guarantees to achieve scalability. recently, we proposed a novel approach that combines universal hashing and sat oracle and scales to formulas with hundreds of thousands of variables without allowing up performance guarantees. this paper provides broad overview of the key ingredients onto the approach and discusses challenges that need to be pursued to resolve larger real - world instances.", "histories": [["v1", "Mon, 21 Dec 2015 14:10:10 GMT  (117kb,D)", "http://arxiv.org/abs/1512.06633v1", "Appears in proceedings of AAAI-16 Workshop on Beyond NP"]], "COMMENTS": "Appears in proceedings of AAAI-16 Workshop on Beyond NP", "reviews": [], "SUBJECTS": "cs.AI cs.LO", "authors": ["kuldeep s meel", "moshe vardi", "supratik chakraborty", "daniel j fremont", "sanjit a seshia", "dror fried", "alexander ivrii", "sharad malik"], "accepted": false, "id": "1512.06633"}, "pdf": {"name": "1512.06633.pdf", "metadata": {"source": "CRF", "title": "Constrained Sampling and Counting: Universal Hashing Meets SAT Solving\u2217", "authors": ["Kuldeep S. Meel", "Moshe Y. Vardi", "Supratik Chakraborty", "Daniel J. Fremont", "Sanjit A. Seshia", "Dror Fried", "Alexander Ivrii", "Sharad Malik"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Constrained sampling and counting are two fundamental problems in artificial intelligence. In constrained sampling, the task is to sample randomly from the set of solutions of input constraints while the problem of constrained counting is to count the number of solutions. Both problems have numerous applications, including in probabilistic reasoning, machine learning, planning, statistical physics, inexact computing, and constrained-random verification (Bacchus, Dalmao, and Pitassi 2003; Jerrum and Sinclair 1996; Naveh et al. 2006; Roth 1996). For example, probabilistic inference over graphical models can be reduced to constrained counting for propositional formulas (Cooper 1990; Roth 1996). In addition, approximate probabilistic reasoning relies heavily on sampling from high-dimensional probabilistic spaces encoded as sets of constraints (Ermon et al. 2013a; Jerrum and Sinclair 1996). Both constrained sampling and counting can be viewed as aspects of one of the most fundamental problems in artificial intelligence: exploring the structure of the solution space of a set of constraints (Russell and Norvig 2009).\nConstrained sampling and counting are known to be computationally hard (Valiant 1979; Jerrum, Valiant, and Vazirani 1986; Toda 1989). To bypass these hardness results, ap-\n\u2217The order of authors is based on the number of published works contributed to the project and ties have been broken alphabetically. Copyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nproximate versions of the problems have been investigated. Despite strong theoretical and practical interest in approximation techniques over the years, there is still an immense gap between theory and practice in this area. Theoretical algorithms offer guarantees on the quality of approximation, but do not scale in practice, whereas practical tools achieve scalability at the cost of offering weaker or no guarantees.\nOur recent work in constrained sampling and counting (Chakraborty et al. 2014; Chakraborty, Meel, and Vardi 2013a; Chakraborty, Meel, and Vardi 2013b; Chakraborty, Meel, and Vardi 2014; Chakraborty et al. 2014; Chakraborty et al. 2015a; Chakraborty et al. 2015b; Ivrii et al. 2015), has yielded significant progress in this area. By combining the ideas of using SAT solving as an oracle and the reduction of the solution space via universal hashing, we have developed highly scalable algorithms that offer rigorous approximation guarantees. Thus, we were able to take the first step in bridging the gap between theory and practice in approximate constrained sampling and counting. A key enabling factor has been the tremendous progress over the past two decades in propositional satisfiability (SAT) solving, which makes SAT solving usable as an algorithmic building block in practical algorithms.\nIn this paper, we provide an overview of hashing-based sampling and counting techniques and put them in the context of related work. We then highlight key insights that have allowed us to further push the scalability envelope of these algorithms. Finally, we discuss challenges that still need to be overcome before hashing-based sampling and counting algorithms can be applied to large scale real-world instances.\nThe rest of the paper is organized as follows. We introduce notation and preliminaries in Section 2 and discuss related work in Section 3. We discuss key enabling algorithmic techniques for the hashing-based approach in Section 4, followed by an overview of the sampling and counting algorithms themselves in Section 5. We describe recent advances in pushing forward the scalability of these algorithms in Section 6, and finally conclude in Section 7."}, {"heading": "2 Preliminaries", "text": "Let F denote a Boolean formula in conjunctive normal form (CNF), and let X be the set of variables appearing in F . The set X is called the support of F . We also use Vars(F ) to denote the support of F . Given a set of variables S \u2286 X\nar X\niv :1\n51 2.\n06 63\n3v 1\n[ cs\n.A I]\n2 1\nD ec\n2 01\n5\nand an assignment \u03c3 of truth values to the variables inX , we write \u03c3|S for the projection of \u03c3 onto S. A satisfying assignment or witness of F is an assignment that makes F evaluate to true. We denote the set of all witnesses of F by RF and the projection of RF onto S by RF |S . For notational convenience, whenever the formula F is clear from the context, we omit mentioning it.\nLet I \u2286 X be a subset of the support such that if two satisfying assignments \u03c31 and \u03c32 agree on I, then \u03c31 = \u03c32. In other words, in every satisfying assignment, the truth values of variables in I uniquely determine the truth value of every variable in X \\ I. The set I is called an independent support of F , and D = X \\ I is a dependent support. Note that there is a one-to-one correspondence between RF and RF |I . There may be more than one independent support: (a \u2228 \u00acb) \u2227 (\u00aca \u2228 b) has three, namely {a}, {b} and {a, b}. Clearly, if I is an independent support of F , so is every superset of I.\nThe constrained-sampling problem is to sample randomly fromRF , given F . A probabilistic generator is a probabilistic algorithm that generates from F a random solution in RF . Let Pr [E] denote the probability of an event E. A uniform generator Gu(\u00b7) is a probabilistic generator that, given F , guarantees Pr [Gu(F ) = y] = 1/|RF |, for every y \u2208 RF . An almost-uniform generator Gau(\u00b7, \u00b7) guarantees that for every y \u2208 RF , we have 1(1+\u03b5)|RF | \u2264 Pr [G\nau(F, \u03b5) = y] \u2264 1+\u03b5 |RF | , where \u03b5 > 0 is a specified tolerance. Probabilistic generators are allowed to occasionally \u201cfail\u201d in the sense that no solution may be returned even if RF is non-empty. The failure probability for such generators must be bounded by a constant strictly less than 1.\nThe constrained-counting problem is to compute the size of the set RF for a given CNF formula F . An approximate counter is a probabilistic algorithm ApproxCount(\u00b7, \u00b7, \u00b7) that, given a formula F , a tolerance \u03b5 > 0, and a confidence 1 \u2212 \u03b4 \u2208 (0, 1], guarantees that Pr [ |RF |/(1 + \u03b5) \u2264\nApproxCount(F, \u03b5, 1\u2212 \u03b4) \u2264 (1 + \u03b5)|RF | ] \u2265 1\u2212 \u03b4.\nThe type of hash functions used in the hashing-based approach to sampling and counting are r-universal hash functions. For positive integers n,m, and r, we writeH(n,m, r) to denote a family of r-universal hash functions mapping {0, 1}n to {0, 1}m. We use h R\u2190\u2212 H(n,m, r) to denote the probability space obtained by choosing a hash function h uniformly at random from H(n,m, r). The property of runiversality guarantees that for all \u03b11, . . . , \u03b1r \u2208 {0, 1}m and all distinct y1, . . . , yr \u2208 {0, 1}n, Pr [ \u2227r i=1 h(yi) = \u03b1i : h R\u2190\u2212 H(n,m, r) ] = 2\u2212mr. We use a particular class of\nsuch hash functions, denoted by Hxor(n,m), which is defined as follows. Let h(y)[i] denote the ith component of the vector h(y). This family of hash functions is then defined as {h | h(y)[i] = ai,0 \u2295 ( \u2295n k=1 ai,k \u00b7 y[k]), ai,k \u2208 {0, 1}, 1 \u2264 i \u2264 m, 0 \u2264 k \u2264 n}, where \u2295 denotes the XOR operation. By choosing values of ai,k randomly and independently, we can effectively choose a random hash function from Hxor(n,m). It was shown in (Gomes, Sabharwal, and Selman 2007) that this family is 3-universal."}, {"heading": "3 Related Work", "text": "Constrained counting, the problem of counting the number of solutions of a propositional formula, is known as #SAT. It is #P-complete (Valiant 1979), where #P is the set of counting problems associated with NP decision problems. Theoretical investigations of #P have led to the discovery of deep connections in complexity theory (Toda 1989; Valiant 1979), and there is strong evidence for its hardness (Arora and Barak 2009). It is also known that an efficient algorithm for constrained sampling would yield a fully polynomial randomized approximation scheme (FPRAS) for #P-complete inference problems (Jerrum and Sinclair 1996) \u2013 a possibility that lacks any evidence so far and is widely disbelieved.\nIn many applications of constrained counting, such as in probabilistic reasoning, exact counting may not be critically important, and approximate counts suffice. Even when exact counts are important, the inherent complexity of the problem may force one to work with approximate counters. Jerrum, Valiant, and Vazirani (Jerrum, Valiant, and Vazirani 1986) showed that approximate counting of solutions of CNF formulas, to within a given tolerance factor, can be done with high confidence in randomized polynomial time using an NP oracle. A key result of (Jerrum, Valiant, and Vazirani 1986) states that for many problems, generating solutions almost uniformly is inter-reducible with approximate counting; hence, they have similar complexity. Building on the Sipser\u2019s and Stockmeyer\u2019s early work (Sipser 1983; Stockmeyer 1983), Bellare, Goldreich, and Petrank (Bellare, Goldreich, and Petrank 2000) later showed that in fact, an NP-oracle suffices for generating solutions of CNF formulas exactly uniformly in randomized polynomial time. Unfortunately, these deep theoretical results have not been successfully reduced to practice. Our experience in implementing these techniques indicates that they do not scale in practice even to small problem instances involving few tens of variables (Meel 2014).\nIndustrial approaches to constrained sampling in the context of constrained-random verification (Naveh et al. 2006) either rely on Binary Decision Diagram (BDD)-based techniques (Yuan et al. 2004), which scale rather poorly, or use heuristics that offer no guarantee of performance or uniformity when applied to large problem instances (Kitchen and Kuehlmann 2007). In prior academic works (Ermon, Gomes, and Selman 2012b; Kirkpatrick, Gelatt, and Vecchi 1983; Gomes, Sabharwal, and Selman 2007; Wei, Erenrich, and Selman 2004), the focus is on heuristic techniques including Markov chain Monte Carlo (MCMC) methods and techniques based on random seeding of SAT solvers. These methods scale to large problem instances, but either offer very weak or no guarantees on the uniformity of sampling, or require the user to provide hard-to-estimate problemspecific parameters that crucially affect the performance and uniformity of sampling (Ermon et al. 2013b; Ermon et al. 2013c; Gogate and Dechter 2008; Kitchen and Kuehlmann 2007).\nThe earliest approaches to #SAT were based on DPLLstyle SAT solvers and computed exact counts. These approaches, e.g. CDP (Birnbaum and Lozinskii 1999), in-\ncrementally counted the number of solutions by introducing appropriate multiplication factors for each partial solution found, eventually covering the entire solution space. Subsequent counters such as Relsat (Jr. and Schrag 1997), Cachet (Sang et al. 2004), and sharpSAT (Thurley 2006) improved upon this by using several optimizations such as component caching, clause learning, and the like. Techniques based on BDDs and their variants (Minato 1993), or d-DNNF formulas (Darwiche 2004), have also been used to compute exact counts. Although exact counters have been successfully used in small- to medium-sized problems, scaling to larger problem instances has posed significant challenges. Consequently, a large class of practical applications has remained beyond the reach of exact counters (Meel 2014).\nTo overcome the scalability challenge, more efficient techniques for approximate counting have been proposed. The large majority of approximate counters used in practice are bounding counters, which provide lower or upper bounds but do not offer guarantees on the tightness of these bounds. Examples include SampleCount (Gomes et al. 2007a), BPCount (Kroc, Sabharwal, and Selman 2008), MBound and Hybrid-MBound (Gomes, Sabharwal, and Selman 2006), and MiniCount (Kroc, Sabharwal, and Selman 2008). Another category of counters is called guaranteeless counters. While these counters may be efficient, they provide no guarantees and the computed estimates may differ from the exact counts by several orders of magnitude (Gomes et al. 2007b). Examples of guarantee-less counters include ApproxCount (Wei and Selman 2005), SearchTreeSampler (Ermon, Gomes, and Selman 2012a), SE (Rubinstein 2012), and SampleSearch (Gogate and Dechter 2011)."}, {"heading": "4 Enabling Algorithmic Techniques", "text": "Hashing-based approaches to sampling and counting rely on three key algorithmic techniques \u2013 one classical, and two more recent: universal hashing, satisfiability (SAT) solving, and satisfiability modulo theories (SMT) solving.\nUniversal Hashing Universal hashing is an algorithmic technique that selects a hash function at random from a family of functions with a certain mathematical property (Carter and Wegman 1977). This technique guarantees a low expected number of collisions, even for an arbitrary distribution of the data being hashed. We have shown that universal hashing enables us to partition the set RF of satisfying assignments of a formula F into roughly equallysized \u201csmall\u201d cells (Chakraborty, Meel, and Vardi 2013a). By choosing the definition of \u201csmall\u201d carefully, we can sample almost uniformly by first choosing a random cell and then sampling uniformly inside the cell (Chakraborty, Meel, and Vardi 2014). Measuring the size of sufficiently many randomly chosen cells also gives us an approximation of the size of RF (Chakraborty, Meel, and Vardi 2013b). To get a good approximation of uniformity and count, we employ a 3-universal family of hash functions.\nSAT Solving The paradigmatic NP-complete problem of boolean satisfiability (SAT) solving (Cook 1971), is a cen-\ntral problem in computer science. Efforts to develop practically successful SAT solvers go back to the 1950s. The past 20 years have witnessed a \u201cSAT revolution\u201d with the development of conflict-driven clause-learning (CDCL) solvers (Biere et al. 2009). Such solvers combine a classical backtracking search with a rich set of effective heuristics. While 20 years ago SAT solvers were able to solve instances with at most a few hundred variables, modern SAT solvers solve instances with up to millions of variables in a reasonable time (Malik and Zhang 2009). Furthermore, SAT solving is continuing to demonstrate impressive progress (Vardi 2014). Propositional sampling and counting are both extensions of SAT; thus, practically effective SAT solving is a key enabler in our work.\nSMT Solving The Satisfiability Modulo Theories (SMT) problem is a decision problem for logical formulas in combinations of background theories. Examples of theories typically used are the theory of real numbers, the theory of integers, and the theories of various data structures such as lists, arrays, bit vectors, and others. SMT solvers have shown dramatic progress over the past couple of decades and are now routinely used in industrial software development (de Moura and Bj\u00f8rner 2011). Even though we focus on sampling and counting for propositional formulas, we have to be able to combine propositional reasoning with reasoning about hash values, which requires the power of SMT solvers. In our approach, we express hashing by means of XOR constraints. These constraints can be reduced to CNF, but such a reduction typically leads to computational inefficiencies during SAT solving. Instead, we use CryptoMiniSAT, an SMT solver which combines the power of CDCL SAT solving with algebraic treatment of XOR constraints, to yield a highly effective solver for a combination of propositional CNF and XOR constraints (Soos, Nohl, and Castelluccia 2009)."}, {"heading": "5 Hashing-Based Sampling and Counting", "text": "In recent years, we have shown that the combination of universal hashing and SAT/SMT solving can yield a dramatic breakthrough in our ability to perform almost-uniform sampling and approximate counting for industrial-scale formulas with hundreds of thousands of variables (Chakraborty et al. 2014; Chakraborty, Meel, and Vardi 2013a; Chakraborty, Meel, and Vardi 2013b; Chakraborty, Meel, and Vardi 2014; Chakraborty et al. 2014; Chakraborty et al. 2015a; Chakraborty et al. 2015b; Ivrii et al. 2015). The algorithms and tools we developed provide the first scalable implementation with provable approximation guarantees of these fundamental algorithmic building blocks.\nApproximate Counting In (Chakraborty, Meel, and Vardi 2013b), we introduced an approximate constrained counter, called ApproxMC. The algorithm employs XOR constraints to partition the solution space into \u201csmall\u201d cells, where finding the right parameters to get the desired sizes of cells requires an iterative search. The algorithm then repeatedly invokes CryptoMiniSAT to exactly measure the size of sufficiently many random cells (to achieve the desired confidence) and returns an estimate given by multiplying the me-\ndian of the measured cell sizes by the number of cells covering the solution space.\nWe compared ApproxMC with Cachet, a well known exact counter (Sang et al. 2004), using a timeout of 20 hours. While Cachet can solve instances with up to 13K variables, ApproxMC can solve very large instances with over 400K variables (see Fig 1 for performance comparison over a subset of benchmarks). On instances where the exact count was available, the approximate count computed by ApproxMC was within 4% of the exact count, even when the tolerance requested was only 75% (See Figure 2).\n1.0e+02\n1.0e+03\n1.0e+04\n1.0e+05\n1.0e+06\n1.0e+07\n1.0e+08\n1.0e+09\n1.0e+10\n0 5 10 15 20 25 30 35 40\n# o\nf S o lu\nti o n s\nBenchmarks\nApproxMC ExactCount/1.75 ExactCount*1.75\nFigure 2: Quality of counts computed by ApproxMC. The benchmarks are arranged in increasing order of model counts.\nAlmost-uniform Sampling In (Chakraborty, Meel, and Vardi 2013a; Chakraborty, Meel, and Vardi 2014), we described an almost-uniform propositional sampler called UniGen. The algorithm first calls ApproxMC to get an approximate count of the size of the solution space. Using this count enables us to fine-tune the parameters for using XOR constraints to partition the solution space in such a way that a randomly chosen cell is expected to be small enough so the solutions in it can be enumerated and sampled. UniGen was\nable to handle formulas with approximately 0.5M variables. Its performance is several orders of magnitude better than that of a competing algorithm called XORSample\u2019 (Gomes, Sabharwal, and Selman 2007), which does not offer an approximation guarantee of almost uniformity (see Fig. 3). To evaluate the uniformity of sampling, we compared UniGen with a uniform sampler (called US, implemented by enumerating all solutions) on a benchmark with about 16K solutions. We generated 4M samples, each from US and UniGen; the resulting distributions were statistically indistinguishable. (see Fig. 4).\n0.1$\n1$\n10$\n100$\n1000$\n10000$\n100000$\nca se 47 ' ca se _3 _b 14 _3 ' ca se 10 5' ca se 8' ca se 20 3' ca se 14 5' ca se 61 ' ca se 9' ca se 15 ' ca se 14 0' ca se _2 _b 14 _1 ' ca se _3 _b 14 _1 ' sq ua rin g1 4' sq ua rin g7 ' ca se _2 _p tb _1 ' ca se _1 _p tb _1 ' ca se _2 _b 14 _2 ' ca se _3 _b 14 _2 '\nTime(s)(\nBenchmarks(\nUniGen$\nXORSample'$\nFigure 3: Performance comparison between UniGen and XORSample\u2019.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n160 180 200 220 240 260 280 300 320\n# o\nf S o lu\nti o n s\nCount\nUS UniGen\nFigure 4: Histogram of solution frequencies comparing the distribution of UniGen to that of an ideal uniform sampler (US).\nWeighted Counting and Sampling In (Chakraborty et al. 2014), we showed how to extend the above algorithms for approximate counting and almost-uniform sampling from the unweighted case, in which all assignments are given equal weight, to the weighted case, in which a weight function associates a weight with each assignment, and counting and sampling must be done with respect to these weights. Under some mild assumptions on the distribution of weights, we showed that the unweighted algorithms for approximate counting and almost-uniform sampling can be adapted to\nwork in the weighted setting, using only a SAT solver (NPoracle) and a black-box weight function w(\u00b7). For the algorithm to work well in practice, we require that the tilt of the weight function, which is the ratio of the maximum weight of a satisfying assignment to the minimum weight of a satisfying assignment, be small. This is a reasonable assumption for several important classes of problems (Chakraborty et al. 2014). We were able to handle formulas with over 60K variables, and our tools significantly outperformed SDD, a state-of-the-art exact weighted constrained counter (Darwiche 2011).\nRelated recent work on hashing-based approaches The above work generated interest in the research community focused on sampling and counting problems. We introduced the approach of combining universal hashing with SAT solving for almost-uniform sampling and approximate counting in (Chakraborty, Meel, and Vardi 2013a; Chakraborty, Meel, and Vardi 2013b). Recently, (Belle, Van den Broeck, and Passerini 2015) proposed a probabilistic inference algorithm that is a \u201csimple reworking\u201d (quoting authors) of WeightMC.\nBuilding on our underlying approach, Ermon et al. suggested further improvements and proposed an algorithm, called WISH, for constrained weighted counting (Ermon et al. 2013b; Ermon et al. 2013c). The algorithm WISH falls short of WeightMC in terms of approximation guarantees and performance. Unlike WeightMC, WISH does not provide (\u03b5, \u03b4) guarantees and only provides constant-factor approximation guarantees. Furthermore, WISH requires access to a most-probable-explanation (MPE) oracle, which is an optimization oracle and more expensive in practice than the NP oracle (i.e. SAT solver) required by our approach (Park and Darwiche 2006). Recently, Zhu and Erom (Zhu and Ermon 2015) proposed an approximate algorithm, named RPInfAlg, for approximate probabilistic inference. This algorithm provides very weak approximation guarantees, and requires the use of hard-to-estimate parameters. Furthermore, their experiments were done with specific values of parameters, which are not easy to guess or compute efficiently. The computational effort required in identifying the right values of the parameters is not addressed in their work. On the constrained sampling front, Ermon et al (Ermon et al. 2013a) proposed the PAWS algorithm, which provides weaker guarantees than UniGen. Like UniGen, PAWS requires a parameter estimation step. However, given a propositional formula with n variables, UniGen uses O(n) calls to an NP oracle to estimate the parameters, while PAWS requires O(n. log n) calls."}, {"heading": "6 Towards Scalable Sampling and Counting Algorithms", "text": "We now discuss four recent promising directions towards further improving the scalability of hashing-based sampling and counting algorithms.\nParallelism There has been a strong recent revival of interest in parallelizing a wide variety of algorithms to achieve improved performance (Larus 2009). One of the main goals\nin parallel-algorithm design is to achieve a speedup nearly linear in the number of processors, which requires the avoidance of dependencies among different parts of the algorithm (Eager, Zahorjan, and Lazowska 1989). Most of the sampling algorithms employed for sampling fail to meet this criterion, and are hence not easily parallelizable. For example, by using constraint solvers with randomized branching heuristics, samples with sequential dependencies are generated. Similarly, MCMC samplers often require a long sequential walk before converging to the stationary distribution. The lack of techniques for sampling solutions of constraints in parallel while preserving guarantees of effectiveness in finding bugs has been a major impediment to highperformance constrained-random verification (CRV).\nThe algorithm UniGen2 presented in (Chakraborty et al. 2015a) takes a step forward in addressing this problem. It has an initial preprocessing step that is sequential but low-overhead, followed by inherently parallelizable sampling steps. It generates samples (stimuli) that are provably nearly as effective as those generated by an almostuniform sampler for purposes of detecting a bug. Furthermore, experimental evaluation over a diverse set of benchmarks demonstrates that the performance improvement for UniGen2 scales linearly with the number of processors: low communication overhead allows UniGen2 to achieve efficiency levels close to those of ideal distributed algorithms. Given that current practitioners are forced to trade guarantees of effectiveness in bug hunting for scalability, the above properties of UniGen2 are significant. Specifically, they enable a new paradigm for CRV wherein both stimulus generation and simulation are done in parallel, providing the required runtime performance without sacrificing theoretical guarantees.\nIndependence Another way to boost the performance of UniGen is by reducing \u201cwaste\u201d of hashed solutions. Currently, after UniGen chooses a random cell of solutions, it selects a single sample from that cell and throws away all the other solutions. The reason for this \u201cwasting\u201d of solutions is to ensure that the samples obtained are not only almost-uniformly distributed, but also independently distributed. Independence is an important feature of sampling, which is required to ensure that probabilities are multiplicative. (For example, in a sequence of Bernoulli coin tosses, independence is required to ensure that an outcome of heads is almost-certain in the limit.) In MCMC sampling, ensuring independence requires very long random walks between samples. An important feature of UniGen is that its almostuniform samples are independent. Such independence, however, comes at the cost of throwing away many solutions.\nIn some applications, such as constrained-random verification, full independence between successive samples is not needed: it is only required that the samples provide good coverage over the space of all solutions. In (Chakraborty et al. 2015a), our proposed algorithm, UniGen2, gains efficiency by relaxing independence, while still maintaining provable coverage guarantees. Specifically, a cell is \u201csmall\u201d in the sense discussed earlier when the number of solutions it contains is between parameters loThresh and hiThresh,\nboth computed from the tolerance \u03b5. Instead of picking only one solution from a small cell, UniGen2 randomly picks loThresh distinct solutions. This leads to a theoretical guarantee that UniGen2 requires significantly fewer SAT calls than UniGen to obtain a given level of bug-finding effectiveness in constrained-random verification.\nIndependent Support These hashing-based techniques crucially rely on the ability of combinatorial solvers to solve propositional formulas represented as a conjunction of the input formula and 3-universal hash functions. Due to our use of Hxor, this translates to hybrid formulas which are conjunctions of CNF and XOR clauses. Therefore, a key challenge is to further speed up search for the solutions of such hybrid formulas. The XOR clauses invoked here are highdensity clauses, consisting typically of n/2 variables, where n is the number of variables in the input formula. Experience has shown that the high density of the XOR clauses plays a major role in runtime performance of solving hybrid formulas.\nRecently, Ermon et al. introduced a technique to reduce the density of XOR constraints. Their technique gives hash functions with properties that are weaker than 2\u2013 universality, but sufficient for approximate counting (Ermon et al. 2014). In practice, however, this approach does not appear to achieve significant reduction in the density of XOR constraints and, therefore, the practical impact on runtime is not significant. Furthermore, It is not yet clear whether the constraints resulting from Ermon et al\u2019s techniques (Ermon et al. 2014) can be used for constrained sampling, while providing the same theoretical guarantees that our work gives.\nMotivated by the problem of high-density XOR constraints, we proposed the idea of restricting the hash functions to only the independent support I of a formula, and showed that even with this restriction, we obtain the required universality properties of hash functions needed for constrained sampling and counting techniques (Chakraborty, Meel, and Vardi 2014). Since many practical instances admit independent supports much smaller than the total number of variables (e.g. we may drop all variables introduced by Tseitin encoding), this often allows the use of substantially less dense XOR constraints. While it is possible in many cases to obtain an over-approximation of I by examining the domain from which the instance is derived, the work in (Chakraborty, Meel, and Vardi 2014) does not provide an algorithmic approach for determining I, and experience has shown that the manual approach is error prone.\nIn (Ivrii et al. 2015), we proposed the first algorithm, called MIS, to find minimal independent supports. The key idea of this algorithmic procedure is the reduction of the problem of minimizing an independent support of a Boolean formula to Group Minimal Unsatisfiable Subset (GMUS). To illustrate the practical value of this approach, we used MIS to compute a minimal independent support for each of our UniGen2 and ApproxMC benchmarks, and ran both algorithms using hashing only over the computed supports. Over a wide suite of benchmarks, experiments demonstrated that this hashing scheme improves the runtime performance of UniGen2 and ApproxMC by two to three orders of mag-\nnitude. It is worth noting that these runtime improvements come at no cost of theoretical guarantees: both UniGen2 and ApproxMC still provide the same strong theoretical guarantees.\nFrom Weighted to Unweighted Model Counting Recent approaches to weighted model counting (WMC) have focused on adapting unweighted model counting (UMC) techniques to work in the weighted setting (Sang, Bearne, and Kautz 2005; Xue, Choi, and Darwiche 2012; Chakraborty et al. 2014). Such adaptation requires intimate understanding of the implementation details of the UMC techniques, and on-going maintenance, since some of these techniques evolve over time. In (Chakraborty et al. 2015b), we flip this approach and present an efficient reduction of literalweighted WMC to UMC. The reduction preserves the normal form of the input formula, i.e. it provides the UMC formula in the same normal form as the input WMC formula. Therefore, an important contribution of our reduction is to provide a WMC-to-UMC module that allows any UMC solver, viewed as a black box, to be converted to a WMC solver. This enables the automatic leveraging of progress in UMC solving to make progress in WMC solving.\nWe have implemented our WMC-to-UMC module on top of state-of-the-art exact unweighted model counters to obtain exact weighted model counters for CNF formulas with literal-weighted representation. Experiments on a suite of benchmarks indicate that the resulting counters scale to significantly larger problem instances than what can be handled by a state-of-the-art exact weighted model counter (Choi and Darwiche 2013). Our results suggest that we can leverage powerful techniques developed for SAT and related domains in recent years to handle probabilistic inference queries for graphical models encoded as WMC instances. Furthermore, we demonstrate that our techniques can be extended to more general representations where weights are associated with constraints instead of individual literals."}, {"heading": "7 Conclusion", "text": "Constrained sampling and counting problems have a wide range of applications in artificial intelligence, verification, machine learning and the like. In contrast to prior work that failed to provide scalable algorithms with strong theoretical guarantees, hashing-based techniques offer scalable sampling and counting algorithms with rigorous guarantees on quality of approximation. Yet, many challenges remain in making this approach more viable for real-world problem instances. Promising directions of future research include designing efficient hash functions and developing SAT/SMT solvers specialized to handle constraints arising from these techniques."}], "references": [{"title": "and Barak", "author": ["S. Arora"], "venue": "B.", "citeRegEx": "Arora and Barak 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms and complexity results for #SAT and Bayesian inference", "author": ["Dalmao Bacchus", "F. Pitassi 2003] Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "In Proc. of FOCS,", "citeRegEx": "Bacchus et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2003}, {"title": "Uniform generation of NP-witnesses using an NP-oracle", "author": ["Goldreich Bellare", "M. Petrank 2000] Bellare", "O. Goldreich", "E. Petrank"], "venue": "Information and Computation", "citeRegEx": "Bellare et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bellare et al\\.", "year": 2000}, {"title": "Hashing-based approximate probabilistic inference in hybrid domains", "author": ["Van den Broeck Belle", "V. Passerini 2015] Belle", "G. Van den Broeck", "A. Passerini"], "venue": "In Proc. of UAI", "citeRegEx": "Belle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Belle et al\\.", "year": 2015}, {"title": "Handbook of Satisfiability", "author": ["Biere"], "venue": null, "citeRegEx": "Biere,? \\Q2009\\E", "shortCiteRegEx": "Biere", "year": 2009}, {"title": "E", "author": ["E. Birnbaum", "Lozinskii"], "venue": "L.", "citeRegEx": "Birnbaum and Lozinskii 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "M", "author": ["J.L. Carter", "Wegman"], "venue": "N.", "citeRegEx": "Carter and Wegman 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "M", "author": ["S. Chakraborty", "D.J. Fremont", "K.S. Meel", "S.A. Seshia", "Vardi"], "venue": "Y.", "citeRegEx": "Chakraborty et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "On parallel scalable uniform sat witness generation. In Tools and Algorithms for the Construction and Analysis of Systems, 304\u2013319", "author": ["Chakraborty"], "venue": null, "citeRegEx": "Chakraborty,? \\Q2015\\E", "shortCiteRegEx": "Chakraborty", "year": 2015}, {"title": "2015b. From weighted to unweighted model counting", "author": ["Chakraborty"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Chakraborty,? \\Q2015\\E", "shortCiteRegEx": "Chakraborty", "year": 2015}, {"title": "A scalable and nearly uniform generator of SAT witnesses", "author": ["Meel Chakraborty", "S. Vardi 2013a] Chakraborty", "K.S. Meel", "M.Y. Vardi"], "venue": "In Proc. of CAV,", "citeRegEx": "Chakraborty et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chakraborty et al\\.", "year": 2013}, {"title": "A scalable approximate model counter", "author": ["Meel Chakraborty", "S. Vardi 2013b] Chakraborty", "K.S. Meel", "M.Y. Vardi"], "venue": "In Proc. of CP,", "citeRegEx": "Chakraborty et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chakraborty et al\\.", "year": 2013}, {"title": "M", "author": ["S. Chakraborty", "K.S. Meel", "Vardi"], "venue": "Y.", "citeRegEx": "Chakraborty. Meel. and Vardi 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Darwiche", "author": ["A. Choi"], "venue": "A.", "citeRegEx": "Choi and Darwiche 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "S", "author": ["Cook"], "venue": "A.", "citeRegEx": "Cook 1971", "shortCiteRegEx": null, "year": 1971}, {"title": "G", "author": ["Cooper"], "venue": "F.", "citeRegEx": "Cooper 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "E", "author": ["D.L. Eager", "J. Zahorjan", "Lazowska"], "venue": "D.", "citeRegEx": "Eager. Zahorjan. and Lazowska 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Embed and project: Discrete sampling with universal hashing", "author": ["Ermon"], "venue": "In Proc. of NIPS,", "citeRegEx": "Ermon,? \\Q2013\\E", "shortCiteRegEx": "Ermon", "year": 2013}, {"title": "Optimization with parity constraints: From binary codes to discrete integration", "author": ["Ermon"], "venue": "In Proc. of UAI", "citeRegEx": "Ermon,? \\Q2013\\E", "shortCiteRegEx": "Ermon", "year": 2013}, {"title": "Taming the curse of dimensionality: Discrete integration by hashing and optimization", "author": ["Ermon"], "venue": "In Proc. of ICML,", "citeRegEx": "Ermon,? \\Q2013\\E", "shortCiteRegEx": "Ermon", "year": 2013}, {"title": "C", "author": ["Ermon, S.", "Gomes"], "venue": "P.; Sabharwal, A.; and Selman, B.", "citeRegEx": "Ermon et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Uniform solution sampling using a constraint solver as an oracle", "author": ["Gomes Ermon", "S. Selman 2012a] Ermon", "C.P. Gomes", "B. Selman"], "venue": "In Proc. of UAI", "citeRegEx": "Ermon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ermon et al\\.", "year": 2012}, {"title": "Uniform solution sampling using a constraint solver as an oracle", "author": ["Gomes Ermon", "S. Selman 2012b] Ermon", "C.P. Gomes", "B. Selman"], "venue": "In Proc. of UAI,", "citeRegEx": "Ermon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ermon et al\\.", "year": 2012}, {"title": "and Dechter", "author": ["V. Gogate"], "venue": "R.", "citeRegEx": "Gogate and Dechter 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Dechter", "author": ["V. Gogate"], "venue": "R.", "citeRegEx": "Gogate and Dechter 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "From sampling to model counting", "author": ["Gomes"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Gomes,? \\Q2007\\E", "shortCiteRegEx": "Gomes", "year": 2007}, {"title": "From sampling to model counting", "author": ["Gomes"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Gomes,? \\Q2007\\E", "shortCiteRegEx": "Gomes", "year": 2007}, {"title": "C", "author": ["Gomes"], "venue": "P.; Sabharwal, A.; and Selman, B.", "citeRegEx": "Gomes. Sabharwal. and Selman 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Near-uniform sampling of combinatorial spaces using XOR constraints", "author": ["Sabharwal Gomes", "C. Selman 2007] Gomes", "A. Sabharwal", "B. Selman"], "venue": "In Proc. of NIPS,", "citeRegEx": "Gomes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2007}, {"title": "On computing minimal independent support and its applications to sampling and counting", "author": ["Ivrii"], "venue": null, "citeRegEx": "Ivrii,? \\Q2015\\E", "shortCiteRegEx": "Ivrii", "year": 2015}, {"title": "and Sinclair", "author": ["M. Jerrum"], "venue": "A.", "citeRegEx": "Jerrum and Sinclair 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Random generation of combinatorial structures from a uniform distribution", "author": ["Valiant Jerrum", "M. Vazirani 1986] Jerrum", "L. Valiant", "V. Vazirani"], "venue": "Theoretical Computer Science", "citeRegEx": "Jerrum et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Jerrum et al\\.", "year": 1986}, {"title": "and Schrag", "author": ["R.J.B. Jr."], "venue": "R.", "citeRegEx": "Jr. and Schrag 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Optimization by simmulated annealing", "author": ["Gelatt Kirkpatrick", "S. Vecchi 1983] Kirkpatrick", "C. Gelatt", "M. Vecchi"], "venue": "Science", "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "and Kuehlmann", "author": ["N. Kitchen"], "venue": "A.", "citeRegEx": "Kitchen and Kuehlmann 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Leveraging belief propagation, backtrack search, and statistics for model counting", "author": ["Sabharwal Kroc", "L. Selman 2008] Kroc", "A. Sabharwal", "B. Selman"], "venue": "In Proc. of CPAIOR,", "citeRegEx": "Kroc et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kroc et al\\.", "year": 2008}, {"title": "and Zhang", "author": ["S. Malik"], "venue": "L.", "citeRegEx": "Malik and Zhang 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "K", "author": ["Meel"], "venue": "S.", "citeRegEx": "Meel 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Constraint-based random stimuli generation for hardware verification", "author": ["Naveh"], "venue": "In Proc of IAAI,", "citeRegEx": "Naveh,? \\Q2006\\E", "shortCiteRegEx": "Naveh", "year": 2006}, {"title": "and Darwiche", "author": ["J.D. Park"], "venue": "A.", "citeRegEx": "Park and Darwiche 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Norvig", "author": ["S. Russell"], "venue": "P.", "citeRegEx": "Russell and Norvig 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Performing bayesian inference by weighted model counting", "author": ["Bearne Sang", "T. Kautz 2005] Sang", "P. Bearne", "H. Kautz"], "venue": "In Prof. of AAAI,", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Combining component caching and clause learning for effective model counting", "author": ["Sang"], "venue": "In Proc. of SAT", "citeRegEx": "Sang,? \\Q2004\\E", "shortCiteRegEx": "Sang", "year": 2004}, {"title": "Extending SAT Solvers to Cryptographic Problems", "author": ["Nohl Soos", "M. Castelluccia 2009] Soos", "K. Nohl", "C. Castelluccia"], "venue": "In Proc. of SAT. Springer-Verlag", "citeRegEx": "Soos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Soos et al\\.", "year": 2009}, {"title": "and Selman", "author": ["W. Wei"], "venue": "B.", "citeRegEx": "Wei and Selman 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards efficient sampling: Exploiting random walk strategies", "author": ["Erenrich Wei", "W. Selman 2004] Wei", "J. Erenrich", "B. Selman"], "venue": "In Proc. of AAAI,", "citeRegEx": "Wei et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2004}, {"title": "Basing decisions on sentences in decision diagrams", "author": ["Choi Xue", "Y. Darwiche 2012] Xue", "A. Choi", "A. Darwiche"], "venue": "In Proc. of AAAI", "citeRegEx": "Xue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2012}, {"title": "Simplifying boolean constraint solving for random simulation-vector generation", "author": ["Yuan"], "venue": "IEEE Trans. on CAD of Integrated Circuits and Systems", "citeRegEx": "Yuan,? \\Q2004\\E", "shortCiteRegEx": "Yuan", "year": 2004}, {"title": "and Ermon", "author": ["M. Zhu"], "venue": "S.", "citeRegEx": "Zhu and Ermon 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2015, "abstractText": "Constrained sampling and counting are two fundamental problems in artificial intelligence with a diverse range of applications, spanning probabilistic reasoning and planning to constrained-random verification. While the theory of these problems was thoroughly investigated in the 1980s, prior work either did not scale to industrial size instances or gave up correctness guarantees to achieve scalability. Recently, we proposed a novel approach that combines universal hashing and SAT solving and scales to formulas with hundreds of thousands of variables without giving up correctness guarantees. This paper provides an overview of the key ingredients of the approach and discusses challenges that need to be overcome to handle larger real-world instances.", "creator": "LaTeX with hyperref package"}}}