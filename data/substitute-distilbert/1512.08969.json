{"id": "1512.08969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2015", "title": "Evaluating Go Game Records for Prediction of Player Attributes", "abstract": "we propose a way of recording and aggregating per - move evaluations from sets of go career records. the evaluations concern different aspects of the games such as played preference or statistic of sente / ot wins. using machine adaptive algorithms, the evaluations can be utilized including predict different relevant performance variables. we apply this methodology to predict the strength and playing style of competitive player ( e. g. emotion or aggressivity ) with good accuracy. we propose a number of game applications including aiding in go study, seeding real - work ranks of internet players or tuning of go - playing programs.", "histories": [["v1", "Wed, 30 Dec 2015 15:09:51 GMT  (89kb)", "http://arxiv.org/abs/1512.08969v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["josef moud\\v{r}\\'ik", "petr baudi\\v{s}", "roman neruda"], "accepted": false, "id": "1512.08969"}, "pdf": {"name": "1512.08969.pdf", "metadata": {"source": "CRF", "title": "Evaluating Go Game Records for Prediction of Player Attributes", "authors": ["Josef Moud\u0159\u0131\u0301k", "Roman Neruda"], "emails": ["j.moudrik@gmail.com", "pasky@ucw.cz", "roman@cs.cas.cz"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n08 96\n9v 1\n[ cs\n.A I]\n3 0\nD ec\nIndex Terms\u2014Computer Go, Machine Learning, Feature Extraction, Board Games, Skill Assessment\nI. INTRODUCTION\nThe field of computer Go is primarily focused on the problem of creating a program to play the game by finding the best move from a given board position [1]. We focus on analyzing existing game records with the aim of helping humans to play and understand the game better instead.\nGo is a two-player full-information board game played on a square grid (usually 19 \u00d7 19 lines) with black and white stones; the goal of the game is to surround territory and capture enemy stones. In the following text we assume basic familiarity with the game rules, a small glossary can be found at the end of the paper.\nFollowing up on our initial research [2], we present a method for extracting information from game records. We extract different pieces of domain-specific information from the records to create a complex evaluation of the game sample. The evaluation is a vector composed of independent features \u2013 each of the features captures different aspect of the sample. For example, a statistic of most frequent local patterns played, or a statistic of high and low plays in different game stages are used.\nUsing machine learning methods, the evaluation of the sample can be used to predict relevant variables. In this work in particular, the data sample consists of games of a player, and it is used to predict the player\u2019s strength and playing style.\nThis paper is organized as follows. Section II summarizes related work in the area of machine learning applications in the\ngame of Go. Section III presents the features comprising the evaluation. Section IV gives details about the machine learning method we have used. In Section V we give details about our datasets \u2013 for prediction of strength and style \u2013 and show how precisely can the prediction be conducted. Section VI discusses applications and future work."}, {"heading": "II. RELATED WORK", "text": "Since the game of Go has a worldwide popularity, large collections of Go game records have been compiled, covering both amateur and professional games, e.g. [3], [4].\nCurrently, the ways to utilize these records could be divided into two directions. Firstly, there is the field of computer Go, where the records have been used to rank certain patterns which serve as a heuristic to speed up the tree-search [5], or to generate databases of standard openings [6]. They have also been used as a source of training data by various neuralnetwork based move-predictors. Until very recently, these did not perform convincingly [7]. The recent improvements [8], [9] based on deep convolutional neural networks seem to be changing the situation and promising big changes in the field.\nSecondly, the records of professional games have traditionally served as a study material for human players. There exist software tools [10], [11] designed to enable the user to search the games. These tools also give statistics of next moves and appropriate win rate among professional games.\nOur approach seems to reside on the boundary between the two above mentioned directions, with possible applications in both computer Go and tools aiding in human study. To our knowledge, the only work somewhat resembling ours is [12], where the authors claim to be able to classify player\u2019s strength into 3 predefined classes (casual, intermediate, advanced player). In their work, the domain-specific features were extracted by using GnuGo\u2019s [13] positional assessment and learned using random forests [14]. It is hard to say how precise their method is, since neither precision, nor recall was given. The only account given were two examples of development of skill of two players (picked in an unspecified manner) in time.978-1-4799-8622-4/15/$31 c\u00a92015 IEEE\nOne of the applications of hereby proposed methodology is a utilization of predicted styles of a player to recommend relevant professional players to review. The playing style is traditionally of great importance to human players, but so far, the methodology for deciding player\u2019s style has been limited to expert judgement and hand-constructed questionnaires [15], [16]. See the Discussion (Section VI) for details."}, {"heading": "III. FEATURE EXTRACTION", "text": "This section presents the methods for extracting the evaluation vector (denoted ev) from a set of games. Because we aggregate data by player, each game in the set is accompanied by the color which specifies our player of interest. The sample is therefore regarded as a set of colored games, GC = {(game1, color1), . . .}.\nThe evaluation vector ev is composed by concatenating several sub-vectors of features \u2013 examples include the aforementioned local patterns or statistic of sente and gote sequences. These will be described in detail in the rest of this section. Some of the details are omitted, see [17] for an extended description."}, {"heading": "A. Raw Game Processing", "text": "The games are processed by the Pachi Go Engine [18] which exports a variety of analytical data about each move in the game. For each move, Pachi outputs a list of key-value pairs regarding the current move:\n\u2022 atari flag \u2014 whether the move put enemy stones in atari, \u2022 atari escape flag \u2014 whether the move saved own stones\nfrom atari, \u2022 capture \u2014 number of enemy stones the move captured, \u2022 contiguity to last move \u2014 the gridcular distance (cf.\nequation 1) from the last move, \u2022 board edge distance \u2014 the distance from the nearest edge\nof the board, \u2022 spatial pattern \u2014 configuration of stones around the\nplayed move.\nWe use this information to compute the higher level features given below. The spatial pattern is comprised of positions of stones around the current move up to a certain distance, given by the gridcular metric\nd(x, y) = |\u03b4x|+ |\u03b4y|+max(|\u03b4x|, |\u03b4y|). (1)\nThis metric produces a circle-like structure on the Go board square grid [19]. Spatial patterns of sizes 2 to 6 are taken into account."}, {"heading": "B. Patterns", "text": "The pattern feature family is essentially a statistic of N the most frequently occurring spatial patterns (together with both atari flags). The list of the N most frequently played patterns is computed beforehand from the whole database of games. The patterns are normalized so that it is black\u2019s turn, and they are invariant under rotation and mirroring. We used N = 1000 for the domain of strength and N = 600 for the domain of style (which has a smaller dataset, see Section V-B for details).\nGiven a set of colored games GC we then count how many times was each of the N patterns played \u2013 thus obtaining a vector ~c of counts (|~c| = N ). With simple occurrence count however, particular counts ci increase proportionally to number of games in GC. To maintain invariance under the number of games in the sample, a normalization is needed. We do this by dividing the ~c by |GC|, though other normalization schemes are possible, see [17]."}, {"heading": "C. \u03c9-local Sente and Gote Sequences", "text": "The concept of sente and gote is traditionally very important for human players, which means it could bear some interesting information. Based on this intuition, we have devised a statistic which tries to capture distribution of sente and gote plays in the games from the sample. In general, deciding what moves are sente or gote is hard. Therefore, we restrict ourselves to what we call \u03c9-local (sente and gote) sequences.\nWe say that a move is \u03c9-local (with respect to the previous move) if its gridcular distance from previous move is smaller than a fixed number \u03c9; in this paper, we used \u03c9 = 10 for the strength dataset and \u03c9 = 5 for the style dataset. The simplifying assumption we make is that responses to sente moves are always local. Although this does not hold in general, the feature proves useful.\nThe assumption allows to partition each game into disjunct \u03c9-local sequences (that is, each move in the sequence is \u03c9local with respect to its directly previous move) and observe whether the player who started the sequence is different from the player who ended it. If it is so, the \u03c9-local sequence is said to be sente for the player who started it because he gets to play somewhere else first (tenuki). Similarly if the player who started the sequence had to respond last we say that the sequence is gote for him. Based on this partitioning, we can count the average number of sente and gote sequences per game from the sample GC and these two numbers form the feature."}, {"heading": "D. Border Distance", "text": "The border distance feature is a two dimensional histogram counting the average number of moves in the sample played low or high in different game stages. The original inspiration was to help distinguishing between territorial and influence based moves in the opening, though it turns out that the feature is useful in other phases of the game as well.\nThe first dimension is specified by the move\u2019s border distance, the second one by the number of the current move from the beginning of the game. The granularity of each dimension is given by intervals dividing the domains. We use the\nByDist = {\u30081, 2\u3009, \u30083\u3009, \u30084\u3009, \u30085,\u221e)}\ndivision for the border distance dimension (distinguishing between the first 2 lines, 3rd line of territory, 4th line of influence and higher plays for the rest). The move number division is given by\nByMovesSTR = {\u30081, 10\u3009, \u300811, 64\u3009, \u300865, 200\u3009, \u3008201,\u221e)}\nfor the strength dataset and\nByMovesSTY LE = {\u30081, 16\u3009, \u300817, 64\u3009, \u300865, 160\u3009, \u3008161,\u221e)}\nfor the style dataset. The motivation is to (very roughly) distinguish between the opening, early middle game, middle game and endgame. Differences in the interval sizes were found empirically and our interpretation is that in the case of style, we want to put bigger stress on opening and endgame (both of which can be said to follow standard patterns) on behalf of the middle game (where the situation is usually very complex).\nIf we use the ByMoves and ByDist intervals to divide the domains, we obtain a histogram of total |ByMoves| \u00d7 |ByDist| = 16 fields. For each move in the games GC, we increase the count in the appropriate histogram field. In the end, the whole histogram is normalized to establish invariancy under the number of games scanned by dividing the histogram elements by |GC|. The resulting 16 numbers form the border distance feature."}, {"heading": "E. Captured Stones", "text": "Apart from the border distance feature, we also maintain a two-dimensional histogram which counts the numbers of captured stones in different game stages. The motivation is simple \u2013 especially beginners tend to capture stones because \u201cthey could\u201d instead of because it is the \u201cbest move\u201d. Such capture could be a grave mistake in the opening and it would not be played by skilled players.\nAs before, one of the dimensions is given by the intervals\nByMoves = {\u30081, 60\u3009, \u300861, 240\u3009, \u3008241,\u221e)}\nwhich try to specify the game stages (roughly: opening, middle game, endgame). The division into game stages is coarser than for the previous feature because captures occur relatively infrequently. Finer graining would require more data.\nThe second dimension has a fixed size of three bins. Along the number of captives of the player of interest (the first bin), we also count the number of his opponent\u2019s captives (the second bin) and a difference between the two numbers (the third bin). Together, we obtain a histogram of |ByMoves| \u00d7 3 = 9 elements.\nAgain, the colored games GC are processed move by move by increasing the counts of captivated stones (or 0) in the appropriate field. The 9 numbers (again normalized by dividing by |GC|) together comprise the feature."}, {"heading": "F. Win/Loss Statistic", "text": "The next feature is a statistic of wins and losses and whether they were by points or by resignation. The motivation is that many weak players continue playing games that are already lost until the end, either because their counting is not very good (they do not know there is no way to win), or because they hope the opponent will make a blunder. On the other hand, professionals do not hesitate to resign if they think that nothing can be done, continuing with a lost game could even be considered rude.\nWe disregard forfeited, unfinished or tie games in this feature because the frequency of these events is so small it would require a very large dataset to utilize them reliably.\nIn the colored games of GC, we count how many times did the player of interest:\n\u2022 win by counting, \u2022 win by resignation, \u2022 lost by counting, \u2022 and lost by resignation.\nAgain, we divide these four numbers by |GC| to maintain the invariance under the number of games in GC. Furthermore, for the games won or lost in counting we count the average size of the win or loss in points. The six numbers form the feature."}, {"heading": "IV. PREDICTION", "text": "So far, we have considered how we can turn a set of coloured games GC into an evaluation vector. Now, we are going to show how to utilize the evaluation. To predict various player attributes, we start with a given input dataset D consisting of pairs D = {(GCi, yi), . . .}, where GCi corresponds to a set of colored games of i-th player and yi is the target attribute. The yi might be fairly arbitrary, as long as it has some relation to the GCi. For example, yi might be i\u2019s strength.\nNow, let us denote our evaluation process presented before as eval and let evi be evaluation of i-th player, evi = eval(GCi). Then, we can transform D into Dev = {(evi, yi), . . .}, which forms our training dataset.\nAs usual, the task of subsequent machine learning algorithm is to generalize the knowledge from the dataset Dev to predict correct yX even for previously unseen GCX . In the case of strength, we might therefore be able to predict strength yX of an unknown player X given a set of his games GCX (from which we can compute the evaluation evX )."}, {"heading": "A. Prediction Model", "text": "Choosing the best performing predictor is often a tedious task, which depends on the nature of the dataset at hand, requires expert judgement and repeated trial and error. In [17], we experimented with various methods, out of which stacked ensembles [20] with different base learners turned out to have supreme performance. Since this paper focuses on the evaluation rather than finding the very best prediction model, we decided to use a bagged artificial neural network, because of its simplicity and the fact that it performs very well in practice.\nThe network is composed of simple computational units which are organized in a layered topology, as described e.g. in monograph by [21]. We have used a simple feedforward neural network with 20 hidden units in one hidden layer. The neurons have standard sigmoidal activation function and the network is trained using the RPROP algorithm [22] for at most 100 iterations (or until the error is smaller than 0.001). In both datasets used, the domain of the particular target variable (strength, style) was linearly rescaled to \u3008\u22121, 1\u3009 prior\nto learning. Similarly, predicted outputs were rescaled back by the inverse mapping.\nThe bagging [23] is a method that combines an ensemble of N models (trained on differently sampled data) to improve their performance and robustness. In this work, we used a bag of N = 20 above specified neural networks."}, {"heading": "B. Reference Model and Performance Measures", "text": "In our experiments, mean regression was used as a reference model. The mean regression is a simple method which constantly predicts the average of the target attributes y\u0304 in the dataset regardless of the particular evaluation evi. Although mean regression is a very trivial model, it gives some useful insights about the distribution of target variables y. For instance, low error of the mean regression model raises suspicion that the target attribute y is ill-defined, as discussed in the results section of the style prediction, Section V-B.\nTo assess the efficiency of our method and give estimates of its precision for unseen inputs, we measure the performance of our algorithm given a dataset Dev . A standard way to do this is to divide the Dev into training and testing parts and compute the error of the method on the testing part. For this, we have used a standard method of 10-fold cross-validation [24], which randomly divides the dataset into 10 disjunct partitions of (almost) the same size. Repeatedly, each partition is then taken as testing data, while the remaining 9/10 partitions are used to train the model. Cross-validation is known to provide error estimates which are close to the true error value of the given prediction model.\nTo estimate the variance of the errors, the whole 10-fold cross-validation process was repeated 5 times, as the results in Tables I, III and IV show.\nA commonly used performance measure is the mean square error (MSE) which estimates variance of the error distribution. We use its square root (RMSE) which is an estimate of standard deviation of the predictions,\nRMSE =\n\u221a \u221a \u221a \u221a 1\n|Ts|\n\u2211\n(ev,y)\u2208Ts\n(predict(ev)\u2212 y)2\nwhere the machine learning model predict is trained on the training data Tr and Ts denotes the testing data."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "A. Strength", "text": "One of the two major domains we have tested our framework on is the prediction of player strength.\nDataset: We have collected a large sample of games from the public archives of the Kiseido Go server [25]. The sample consists of over 100 000 records of games in the .sgf format [26].\nFor each rank r in the range of 6-dan to 20-kyu, we gathered a list of players Pr of the particular rank. To avoid biases caused by different strategies, the sample only consists of games played on 19\u00d719 board between players of comparable strength (excluding games with handicap stones). The set of\ncolored games GCp for a player p \u2208 Pr consists of the games player p played when he had the rank r. We only use the GCp if the number of games is not smaller than 10 games; if the sample is larger than 50 games, we randomly choose a subset of the sample (the size of subset is uniformly randomly chosen from interval \u300810, 50\u3009). Note that by cutting the number of games to a fixed number (say 50) for large samples, we would create an artificial disproportion in sizes of GCp, which could introduce bias into the process. The distribution of sample sizes is shown in Figure 1.\nFor each of the 26 ranks, we gathered 120 such GCp\u2019s. The target variable y to learn from directly corresponds to the ranks: y = 20 for rank of 20-kyu, y = 1 for 1-kyu, y = 0 for 1- dan, y = \u22125 for 6-dan, other values similarly. (With increasing strength, the y decreases.) Since the prediction model used (bagged neural network) rescales the input data to \u3008\u22121, 1\u3009, the direction of the ordering or its scale can be chosen fairly arbitrarily.\nResults: The performance of the prediction of strength is given in Table I. The table compares performances of different features (predicted by the bagged neural network, Section IV-A) with the reference model of mean regression.\nThe results show that the prediction of strength has standard deviation \u03c3 (estimated by the RMSE error) of approximately 2.66 rank. Comparing different features reveals that for the prediction of strength, the Pattern feature works by far the best, while other features bring smaller, yet nontrivial contribution."}, {"heading": "B. Style", "text": "The second domain is the prediction of different aspects of player styles.\nDataset: The collection of games in this dataset comes from the Games of Go on Disk database by [4]. This database contains more than 70 000 professional games, spanning from the ancient times to the present.\nWe chose 25 popular professional players (mainly from the 20th century) and asked several experts (professional and strong amateur players) to evaluate these players using a questionnaire. The experts (Alexander Dinerchtein 3-pro, Motoki Noguchi 7-dan, Vladim\u0131\u0301r Dane\u030ck 5-dan, Luka\u0301s\u030c Podpe\u030cra\n5-dan and V\u0131\u0301t Brunner 4-dan) were asked to assess the players on four scales, each ranging from 1 to 10.\nThe scales (cf. Table II) try to reflect some of the traditionally perceived playing styles. For example, the first scale (territoriality) stresses whether a player prefers safe, yet inherently smaller territory (number 10 on the scale), or roughly sketched large territory (moyo, 1 on the scale), which is however insecure. For detailed analysis of playing styles, please refer to [27], or [28].\nFor each of the selected professionals, we took 192 of his games from the GoGoD database at random. We divided these games (at random) into 12 colored sets GC of 16 games. The target variable (for each of the four styles) y is given by average of the answers of the experts. Results of the questionnaire are published online in [29]. Please observe, that the style dataset has both much smaller domain size and data size (only 4800 games).\nResults: Table III compares performances of different features (as predicted by the bagged neural network, Section IV-A) with the mean regression learner. Results in the table have been averaged over different styles. The table shows that the two features with biggest contribution are the pattern feature and the border distance feature. Other features perform either weakly, or even slightly worse than the mean regression learner.\nThe prediction performance per style is shown Table IV (computed on the full feature set). Given that the style scales have range of 1 to 10, we consider the average standard deviation from correct answers of around 1.6 to be a good precision.\nWe should note that the mean regression has very small RMSE for the scale of thickness. This stems from the fact that the experts\u2019 answers from the questionnaire have themselves very little variance. Our conclusion is that the scale of thickness is not well defined. Refer to [17] for further discussion."}, {"heading": "VI. DISCUSSION", "text": "In this paper, we have chosen the target variables to be the strength and four different aspects of style. This has several motivations. Firstly, the strength is arguably the most important player attribute and the online Go servers allow to obtain reasonably precise data easily. The playing styles have been chosen for their strong intuitive appeal to players, and because they are understood pretty well in traditional Go theory. Unlike the strength, the data for the style target variables are however hard to obtain, since the concepts have not been traditionally treated with numerical rigour. To overcome this obstacle, we used the questionnaire, as discussed in Section V-B.\nThe choice of target variable can be quite arbitrary, as long as some dependencies between the target variable and evaluations exist (and can be learned). Some other possible choices might be the era of a player (e.g. standard opening patterns have been evolving rapidly during the last 100 years), or player nationality.\nThe possibility to predict player\u2019s attributes demonstrated in this paper shows that the evaluations are a very useful representation. Both the predictive power and the representation can have a number of possible applications.\nSo far, we have utilized some of the findings in an online web application1. It evaluates games submitted by players and predicts their playing strength and style. The predicted strength is then used to recommend relevant literature and the playing style is utilized by recommending relevant professional players to review. So far, the web application has served thousands of players and it was generally very well received. We are aware of only two tools, that do something alike, both of them are however based on a predefined questionnaire. The first one is the tool of [15] \u2014 the user answers 15 questions and based on the answers he gets one of predefined recommendations. The second tool is not available at the time of writing, but the discussion at [16] suggests, that it computed distances to some pros based on user\u2019s answers to 20 questions regarding the style. We believe that our approach is more precise, because the evaluation takes into account many different aspects of the games.\nOf course, our methods for style estimation are trained on very strong players and thus they might not be fully generalizable to ordinary players. Weak players might not have a consistent style, or the whole concept of style might not be even applicable for them. Estimating this effect is however not easily possible, since we do not have data about weak players\u2019 styles. Our web application allows the users to submit their own opinion about their style, therefore we should be able to consider this effect in the future research.\nIt is also possible to study dependencies between single elements of the evaluation vector and the target variable y directly. By pinpointing e.g. the patterns of the strongest correlation with bad strength (players who play them are weak), we can warn the users not to play the moves associated with the pattern. We have also realised this feature in the online\n1http://gostyle.j2m.cz\nweb application [30]. However, this method seems to be usable only for the few most strongly correlated attributes, the weakly correlated attributes are prone to larger errors.\nOther possible applications include helping the ranking algorithms to converge faster \u2014 usually, the ranking of a player is determined from his opponents\u2019 ranking by looking at the numbers of wins and losses (e.g. by computing an Elo rating [31]). Our methods might improve this by including the domain knowledge. Similarly, a computer Go program can quickly classify the level of its human opponent based on the evaluation from their previous games and auto-adjust its difficulty settings accordingly to provide more even games for beginners. We will research these options in the future."}, {"heading": "VII. CONCLUSION", "text": "This paper presents a method for evaluating players based on a sample of their games. From the sample, we extract a number of different domain-specific features, trying to capture different pieces of information. Resulting summary evaluations turn out to be very useful for prediction of different player attributes (such as strength or playing style) with reasonable accuracy.\nThe ability to predict such player attributes has some very interesting applications in both computer Go and in development of teaching tools for human players, some of which we realized in an on-line web application. The paper also discusses other potential extensions and applications which we will be exploring in the future.\nWe believe that the applications of our findings can help to improve both human and computer understanding of the game of Go.\nVIII. IMPLEMENTATION\nThe code used in this work is released online as a part of GoStyle project [30]. The majority of the source code is implemented in the Python programming language [32].\nThe machine learning models were implemented and evaluated using the Orange Datamining suite [33] and the Fast Artificial Neural Network library FANN [34]. We used the Pachi Go engine [18] for the raw game processing."}, {"heading": "Acknowledgment", "text": "This research has been partially supported by the Czech Science Foundation project no. P103-15-19877S. J. Moudr\u030c\u0131\u0301k has been supported by the Charles University Grant Agency project no. 364015 and by SVV project no. 260 224.\nGLOSSARY\n\u2022 atari \u2014 a situation where a stone (or group of stones) can be captured by the next opponent move, \u2022 sente \u2014 a move that requires immediate enemy response, and thus keeps the initiative, \u2022 gote \u2014 a move that does not require immediate enemy response, and thus loses the initiative, \u2022 tenuki \u2014 a move gaining initiative \u2013 ignoring last (gote) enemy move,\n\u2022 handicap \u2014 a situation where a weaker player gets some stones placed on predefined positions on the board as an advantage to start the game with (their number is set to compensate for the difference in skill)."}], "references": [{"title": "Achieving master level play in 9x9 computer go,", "author": ["S. Gelly", "D. Silver"], "venue": "Proceedings of the 23rd national conference on Artificial intelligence. AAAI Press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Moud\u0159\u0131\u0301k, \u201cOn move pattern trends in a large go games corpus,", "author": ["J.P. Baudi\u0161"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "winter 2011) Games of Go on Disk \u2014 GoGoD Encyclopaedia and Database", "author": ["T.M. Hall", "J. Fairbairn"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Computing Elo Ratings of Move Patterns in the Game of Go,", "author": ["R. Coulom"], "venue": "Computer Games Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "and O", "author": ["P. Audouard", "G. Chaslot", "J.-B. Hoock", "J. Perez", "A. Rimmel"], "venue": "Teytaud, \u201cGrid coevolution for adaptive simulations: Application to the building of opening books in the game of go,\u201d in Applications of Evolutionary Computing. Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "The integration of a priori knowledge into a go playing neural network,", "author": ["M. Enzenberger"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Mimicking go experts with convolutional neural networks,\u201d in Artificial Neural Networks-ICANN", "author": ["I. Sutskever", "V. Nair"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Teaching deep convolutional neural networks to play go,", "author": ["C. Clark", "A. Storkey"], "venue": "arXiv preprint arXiv:1412.3409,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Kombilo \u2014 a Go database program (version 0.7)", "author": ["U. G\u00f6rtz"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Competency awareness in strategic decision making,", "author": ["A. Ghoneim", "D. Essam", "H. Abbass"], "venue": "IEEE First International Multi- Disciplinary Conference on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A", "author": ["D. Bump", "G. Farneback"], "venue": "Bayer et al. ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Random forests,", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "What is your playing style? [Online]. Available: http://style.baduk.com", "author": ["A. Dinerchtein"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Meta-learning methods for analyzing go playing trends,", "author": ["J. Moud\u0159\u0131\u0301k"], "venue": "Master\u2019s thesis,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "and T", "author": ["D. Stern", "R. Herbrich"], "venue": "Graepel, \u201cBayesian pattern ranking for move prediction in the game of go,\u201d in ICML \u201906: Proceedings of the 23rd international conference on Machine learning. New York, NY, USA: ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Stacked regressions,", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Neural Networks: A Comprehensive Foundation (2nd Edition), 2nd ed", "author": ["S. Haykin"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm,", "author": ["M. Riedmiller", "H. Braun"], "venue": "IEEE International Conference on Neural Networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1993}, {"title": "Bagging predictors,", "author": ["L. Breiman"], "venue": "Mach. Learn.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1996}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection.", "author": ["R. Kohavi"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "KGS archives \u2014 kiseido go", "author": ["W. Shubert"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "winter 2011) Games of Go on Disk \u2014 GoGoD Encyclopaedia and Database, Go players", "author": ["J. Fairbairn"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Baudi\u0161, \u201cStyle consensus: Style of professional players, judged by strong players,", "author": ["P.J. Moud\u0159\u0131\u0301k"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "GoStyle \u2014 Determine playing style in the game of Go", "author": ["J. Moud\u0159\u0131\u0301k", "P. Baudi\u0161"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "The rating of chessplayers", "author": ["A.E. Elo"], "venue": "past and present. Arco, New York", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1978}, {"title": "Orange: Data mining toolbox in python,", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Implementation of a fast artificial neural network library (fann),", "author": ["S. Nissen"], "venue": "Department of Computer Science University of Copenhagen (DIKU), Tech. Rep.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "The field of computer Go is primarily focused on the problem of creating a program to play the game by finding the best move from a given board position [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Following up on our initial research [2], we present a method for extracting information from game records.", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "[3], [4].", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "Firstly, there is the field of computer Go, where the records have been used to rank certain patterns which serve as a heuristic to speed up the tree-search [5], or to generate databases of standard openings [6].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Firstly, there is the field of computer Go, where the records have been used to rank certain patterns which serve as a heuristic to speed up the tree-search [5], or to generate databases of standard openings [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "Until very recently, these did not perform convincingly [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "The recent improvements [8], [9] based on deep convolutional neural networks seem to be changing the situation and promising big changes in the field.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "The recent improvements [8], [9] based on deep convolutional neural networks seem to be changing the situation and promising big changes in the field.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "There exist software tools [10], [11] designed to enable the user to search the games.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "To our knowledge, the only work somewhat resembling ours is [12], where the authors claim to be able to classify player\u2019s strength into 3 predefined classes (casual, intermediate, advanced player).", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "In their work, the domain-specific features were extracted by using GnuGo\u2019s [13] positional assessment and learned using random forests [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "In their work, the domain-specific features were extracted by using GnuGo\u2019s [13] positional assessment and learned using random forests [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "The playing style is traditionally of great importance to human players, but so far, the methodology for deciding player\u2019s style has been limited to expert judgement and hand-constructed questionnaires [15], [16].", "startOffset": 202, "endOffset": 206}, {"referenceID": 13, "context": "Some of the details are omitted, see [17] for an extended description.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "This metric produces a circle-like structure on the Go board square grid [19].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "We do this by dividing the ~c by |GC|, though other normalization schemes are possible, see [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "In [17], we experimented with various methods, out of which stacked ensembles [20] with different base learners turned out to have supreme performance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [17], we experimented with various methods, out of which stacked ensembles [20] with different base learners turned out to have supreme performance.", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "in monograph by [21].", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "The neurons have standard sigmoidal activation function and the network is trained using the RPROP algorithm [22] for at most 100 iterations (or until the error is smaller than 0.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "The bagging [23] is a method that combines an ensemble of N models (trained on differently sampled data) to improve their performance and robustness.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "For this, we have used a standard method of 10-fold cross-validation [24], which randomly divides the dataset into 10 disjunct partitions of (almost) the same size.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "Dataset: We have collected a large sample of games from the public archives of the Kiseido Go server [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "Dataset: The collection of games in this dataset comes from the Games of Go on Disk database by [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 21, "context": "For detailed analysis of playing styles, please refer to [27], or [28].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "Results of the questionnaire are published online in [29].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "Refer to [17] for further discussion.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "The first one is the tool of [15] \u2014 the user answers 15 questions and based on the answers he gets one of predefined recommendations.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "cz web application [30].", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "by computing an Elo rating [31]).", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "The code used in this work is released online as a part of GoStyle project [30].", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "The machine learning models were implemented and evaluated using the Orange Datamining suite [33] and the Fast Artificial Neural Network library FANN [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "The machine learning models were implemented and evaluated using the Orange Datamining suite [33] and the Fast Artificial Neural Network library FANN [34].", "startOffset": 150, "endOffset": 154}], "year": 2013, "abstractText": "We propose a way of extracting and aggregating permove evaluations from sets of Go game records. The evaluations capture different aspects of the games such as played patterns or statistic of sente/gote sequences. Using machine learning algorithms, the evaluations can be utilized to predict different relevant target variables. We apply this methodology to predict the strength and playing style of the player (e.g. territoriality or aggressivity) with good accuracy. We propose a number of possible applications including aiding in Go study, seeding realwork ranks of internet players or tuning of Go-playing programs.", "creator": "gnuplot 4.6 patchlevel 1"}}}