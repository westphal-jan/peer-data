{"id": "1601.01799", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jan-2016", "title": "Dense Bag-of-Temporal-SIFT-Words for Time Series Classification", "abstract": "time region assessment is an application of particular interest for the exchange of data to monitor. classical techniques for time series classification rely on point - to - point distances. recently, bag - between - words approaches have been used in this context. words are quantized versions of simple features extracted from sliding intervals. the sift framework has potential essential for image classification. in this paper, we design a time region classification scheme that builds on every sift framework fitted to time series to feed a bag - of - words. we then refine our method by studying the impact of removing bag - square - words, ( well as densely extract point descriptors. proposed adjustements achieve better performance. the evaluation shows that our method outperforms classical techniques in terms of classification.", "histories": [["v1", "Fri, 8 Jan 2016 09:06:44 GMT  (922kb,D)", "http://arxiv.org/abs/1601.01799v1", null], ["v2", "Wed, 13 Jan 2016 08:12:54 GMT  (1157kb,D)", "http://arxiv.org/abs/1601.01799v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["adeline bailly", "simon malinowski", "romain tavenard", "thomas guyet", "laetitia chapel"], "accepted": false, "id": "1601.01799"}, "pdf": {"name": "1601.01799.pdf", "metadata": {"source": "CRF", "title": "Dense Bag-of-Temporal-SIFT-Words for Time Series Classification", "authors": ["Adeline Bailly", "Simon Malinowski", "Romain Tavenard", "Laetitia Chapel", "Thomas Guyet"], "emails": [], "sections": [{"heading": null, "text": "Keywords: time series classification, Bag-of-Words, SIFT, dense features, BoTSW, D-BoTSW"}, {"heading": "1 Introduction", "text": "Classification of time series has received an important amount of interest over the past years due to many real-life applications, such as medicine [24], environmental modeling [7], speech recognition [12]. A wide range of algorithms have been proposed to solve this problem. One simple classifier is the k-nearestneighbor (kNN), which is usually combined with Euclidean Distance (ED) or Dynamic Time Warping (DTW) metric. The combination of the kNN classifier with DTW is one of the most popular method since it achieves high classification accuracy [20]. However, this method has a high computation cost which makes its use difficult for large-scale real-life applications.\nAbove-mentioned techniques compute similarity between time series based on point-to-point comparisons. Classification techniques based on higher level structures (e.g. feature vectors) are most of the time faster, while being at least as accurate as DTW-based classifiers. Hence, various works have investigated the extraction of local and global features in time series. Among these works, the Bag-of-Words (BoW) approach (also called Bag-of-Features) consists in representing documents using a histogram of word occurrences. It is a very common technique in text mining, information retrieval and content-based image retrieval\nar X\niv :1\n60 1.\n01 79\n9v 1\n[ cs\n.L G\n] 8\nJ an\nbecause of its simplicity and performance. For these reasons, it has been adapted to time series data in some recent works [2,3,14,21,24]. Different kinds of features based on simple statistics, computed at a local scale, are used to create the words.\nIn the context of image retrieval and classification, scale-invariant descriptors have proved their accuracy. Particularly, the Scale-Invariant Feature Transform (SIFT) framework has led to widely used descriptors [17]. These descriptors are scale and rotation invariant while being robust to noise. In [1], we built on this framework to design a BoW approach for time series classification where words correspond to quantized versions of local features. Features are built using the SIFT framework for both detection and description of the keypoints. This approach can be seen as an adaptation of [22], which uses SIFT features associated with visual words, to time series. In this paper, we improve our previous work by applying enhancement techniques for BoW approaches, such as dense extraction and BoW normalization. To validate this, we conduct extensive experiments on a wide range of datasets.\nThis paper is organized as follows. Section 2 summarizes related work, Section 3 describes the proposed Bag-of-Temporal-SIFT-Words (BoTSW) method and its improved version (dense extraction and BoW normalization, D-BoTSW), and Section 4 reports experimental results. Finally, Section 5 concludes and discusses future work."}, {"heading": "2 Related work", "text": "Our approach for time series classification builds on two well-known methods in computer vision: local features are extracted from time series using a SIFT-based approach and a global representation of time series is produced using Bag-ofWords. This section first introduces state-of-the-art distance-based methods in time series classification and then presents previous works that make use of Bag-of-Words approaches for time series classification."}, {"heading": "2.1 Distance-based time series classification", "text": "Data mining community has, for long, investigated the field of time series classification. Early works focus on the use of dedicated metrics to assess similarity between time series. In [20], Ratanamahatana and Keogh compare Dynamic Time Warping to Euclidean Distance when used with a simple kNN classifier. While the former benefits from its robustness to temporal distortions to achieve high accuracy, ED is known to have much lower computational cost. Cuturi [5] shows that, although DTW is well-suited to retrieval tasks since it focuses on the best possible alignment between time series, it fails at precisely quantifying dissimilarity between non-matching sequences (which is backed by the fact that DTW-derived kernel is not positive definite). Hence, he introduces the Global Alignment Kernel that takes into account all possible alignments in order to produce a reliable dissimilarity metric to be used at the core of standard kernel\nmethods such as Support Vector Machines (SVM). Lines and Bagnall [15] propose an ensemble classifier based on elastic distance measures (including DTW), named Proportional Elastic Ensemble (PROP). Instead of building classification decision on similarities between time series, Ye and Keogh [26] use a decision tree in which the partitioning of time series is performed with respect to the presence (or absence) of discriminant sub-sequences (named shapelets) in the series. Though accurate, the method is very computational demanding as building the decision tree requires one to check for all candidate shapelets. Douzal and Amblard [6] define a dedicated metric for time series which is then used in a classification tree."}, {"heading": "2.2 Bag-of-Words for time series classification", "text": "Inspired by text mining, information retrieval and computer vision communities, recent works have investigated the use of Bag-of-Words for time series classification [2,3,14,21,24]. These works are based on two main operations: converting time series into Bag-of-Words, and building a classifier upon this BoW representation. Usually, standard techniques such as random forests, SVM, neural networks or kNN are used for the classification step. Yet, many different ways of converting time series into Bag-of-Words have been introduced. Among them, Baydogan et al. [3] propose a framework to classify time series denoted TSBF where local features such as mean, variance and extremum values are computed on sliding windows. These features are then quantized into words using a codebook learned by a class probability estimate distribution. In [24], discrete wavelet coefficients are extracted on sliding windows and then quantized into words using k-means. In [14,21], words are constructed using the Symbolic Aggregate approXimation (SAX) representation [13] of time series. SAX symbols are extracted from time series and histograms of n-grams of these symbols are computed to form a Bag-of-Patterns (BoP). In [21], Senin and Malinchik combine SAX with Vector Space Model to form the SAX-VSM method. In [2], Baydogan and Runger design a symbolic representation of multivariate time series (MTS), called SMTS, where MTS are transformed into a feature matrix, whose rows are feature vectors containing a time index, the values and the gradient of time series at this time index (on all dimensions). Random samples of this matrix are given to decision trees whose leaves are seen as words. A histogram of words is output when the different trees are learned.\nLocal feature extraction has been investigated for long in the computer vision community. One of the most powerful local feature for image is SIFT [17]. It consists in detecting keypoints as extremum values of the the Difference-ofGaussians (DoG) function and describing their neighborhoods using histograms of gradients. Xie and Beigi [25] use similar keypoint detection for time series. Keypoints are then described by scale-invariant features that characterize the shapes surrounding the extremum. In [4], extraction and description of time series keypoints in a SIFT-like framework is used to reduce the complexity of DTW: features are used to match anchor points from two different time series and prune the search space when searching for the optimal path for DTW.\nIn this paper, we build upon BoW of SIFT-based descriptors. We propose an adaptation of SIFT to mono-dimensional signals that preserves their robustness to noise and their scale invariance. We then use BoW to gather information from many local features into a single global one."}, {"heading": "3 Bag-of-Temporal-SIFT-Words (BoTSW) method", "text": "The proposed method is based on three main steps: (i) extraction of keypoints in time series, (ii) description of these keypoints by gradient magnitude at a specific scale and (iii) representation of time series by a BoW, where words correspond to quantized version of the description of keypoints. These steps are depicted in Fig. 1 and detailed below."}, {"heading": "3.1 Keypoints extraction in time series", "text": "The first step of our method consists in extracting keypoints in time series. Two approaches are described here: the first one is based on scale-space extrema detection (as in [1]) and the second one proposes a dense extraction scheme.\nScale-space extrema detection. Following the SIFT framework, keypoints in time series are detected as local extrema in terms of both scale and (temporal)\nlocation. These scale-space extrema are identified using a DoG function, and form a list of scale-invariant keypoints. Let L(t, \u03c3) be the convolution (\u2217) of a Gaussian function G(t, \u03c3) of width \u03c3 with a time series S(t):\nL(t, \u03c3) = G(t, \u03c3) \u2217 S(t) (1) where G(t, \u03c3) is defined as\nG(t, \u03c3) = 1\u221a\n2\u03c0 \u03c3 e\u2212t 2/2\u03c32 . (2)\nLowe [16] proposes the Difference-of-Gaussians (DoG) function to detect scalespace extrema in images. Adapted to time series, a DoG function is obtained by subtracting two time series filtered at consecutive scales:\nD(t, \u03c3) = L(t, ksc\u03c3)\u2212 L(t, \u03c3), (3) where ksc is a parameter of the method that controls the scale ratio between two consecutive scales.\nKeypoints are then detected at time index t in scale j if they correspond to extrema of D(t, kjsc\u03c30) in both time and scale, where \u03c30 is the width of the Gaussian corresponding to the reference scale. At a given scale, each point has two neighbors: one at the previous and one at the following time instant. Points also have neighbors one scale up and one scale down at the previous, same and next time instants, leading to a total of eight neighbors. If a point is higher (or lower) than all of its neighbors, it is considered as an extremum in the scale-space domain and hence a keypoint of S.\nDense extraction. Previous researches have shown that accurate classification could be achieved by using densely extracted local features [10,23]. In this section, we present the adaptation of this setup to our BoTSW scheme. Keypoints selected with dense extraction no longer correspond to extrema but are rather systematically extracted at all scales every \u03c4step time steps on Gaussian-filtered time series L(\u00b7, kjsc\u03c30).\nUnlike scale-space extrema detection, regular sampling guarantees a minimal amount of keypoints per time series. This is especially crucial for smooth time series from which very few keypoints are detected when using scale-space extrema detection. In addition, even if the densely extracted keypoints are not scale-space extrema, description of these keypoints (cf. Section 3.2) covers the description of scale-space extrema if \u03c4step is not too large. This usually leads to more robust global descriptors.\nA dense extraction scheme is represented in Fig. 1, where we consider a step of \u03c4step = 15 for the sake of readability. In the following, when dense extraction is performed, we will refer to our method as D-BoTSW (for dense BoTSW)."}, {"heading": "3.2 Description of the extracted keypoints", "text": "Next step in our process is the description of keypoints. A keypoint at time index t and scale j is described by gradient magnitudes of L(\u00b7, kjsc\u03c30) around t. To do\nso, nb blocks of size a are selected around the keypoint. Gradients are computed at each point of each block and weighted using a Gaussian window of standard deviation a\u00d7nb2 so that points that are farther in time from the detected keypoint have lower influence. Then, each block is described by two values: the sum of positive gradients and the sum of negative gradients. Resulting feature vector is hence of dimension 2\u00d7 nb."}, {"heading": "3.3 Bag-of-Temporal-SIFT-Words for time series classification", "text": "The set of all training features is used to learn a codebook of k words using kmeans clustering. Words represent different local behaviors in time series. Then, for a given time series, each feature vector is assigned the closest word in the codebook. The number of occurrences of each word in a time series is computed. (D-)BoTSW representation of a time series is the `2-normalized histogram (i.e. frequency vector) of word occurrences.\nBag-of-Words normalization. Dense sampling on multiple Gaussian-filtered time series provides considerable information to process. It also tends to generate words with little informative power, as stop words do in text mining applications. In order to reduce the impact of those words, we compare two normalization schemes for BoW: Signed Square Root normalization (SSR) and Inverse Document Frequency normalization (IDF). These normalizations are commonly used in image retrieval and classification based on histograms [8,9,19,22].\nJe\u0301gou et al. [9] and Perronin et al. [19] show that reducing the influence of frequent codewords before `2 normalization could be profitable. They apply a power \u03b1 \u2208 [0, 1] on their global representation. SSR normalization corresponds to the case where \u03b1 = 0.5, which leads to near-optimal results [9,19].\nIDF normalization also tends to lower the influence of frequent codewords. To do so, document frequency of words is computed as the number of training time series in which the word occurs. BoW are then updated by diving each component by its associated document frequency.\nSSR and IDF normalizations both reduce the influence of frequent codewords in the codebook, and are applied before `2 normalization. We show in the experimental part of this paper that using BoW normalization improves the accuracy of our method.\nNormalized histograms are finally given to a classifier that learns how to discriminate classes from this D-BoTSW representation."}, {"heading": "4 Experiments and results", "text": "In this section, we investigate the impact of both dense extraction of the keypoints and normalization of the Bag-of-Words on classification performance. We then compare our results to the ones obtained with standard time series classification techniques.\nFor the sake of reproducibility, C++ source code used for (D-)BoTSW in these experiments is made available for download1. To provide illustrative timings for our methods, we ran it on a personal computer, for a given set of parameters, using dataset Cricket X [11] that is made of 390 training time series and 390 test ones. Each time series in the dataset is of length 300. Extraction and description of dense keypoints takes around 1 second for all time series in the dataset. Then, 35 seconds are necessary to learn a k-means and fit a linear SVM classifier using training data only. Finally, classification of all D-BoTSW corresponding to test time series takes less than 1 second."}, {"heading": "4.1 Experimental setup", "text": "Experiments are conducted on the 86 currently available datasets from the UCR repository [11], the largest online database for time series classification. It includes a wide variety of problems, such as sensor reading (ECG), image outline (ArrowHead), human motion (GunPoint), as well as simulated problems (TwoPatterns). All datasets are split into a training and a test set, whose size varies between less than 20 and more than 8000 time series. For a given dataset, all time series have the same length, ranging from 24 to more than 2500 points.\nParameters a, nb, k and CSVM of (D-)BoTSW are learned, while we set \u03c30 = 1.6 and ksc = 2\n1/3, as these values have shown to produce stable results [17]. Parameters a, nb, k and CSVM vary inside the following sets: {4, 8}, {4, 8, 12, 16, 20}, { 2i,\u2200i \u2208 {5..10} } and {1, 10, 100} respectively. Codebooks are obtained via k-means quantization and a linear SVM is used to classify time series represented as (D-)BoTSW. For our approach, the best sets (in terms of accuracy) of (a, nb, k, CSVM ) parameters are selected by performing cross-validation on the training set. Due to the heterogeneity of the datasets, leave-one-out crossvalidation is performed on datasets where the training set contains less than 300 time series, and 10-fold cross-validation is used otherwise. These best sets of parameters are then used to build the classifier on the training set and evaluate it on the test set. For datasets with little training data, it is likely that several sets of parameters yield best performance during the cross-validation process. For example, when using DiatomSizeReduction dataset, BoTSW has 150 out of 180 parameter sets yielding best performance, while there are 42 such sets for D-BoTSW with SSR normalization. In both cases, the number of best parameter sets is too high to allow a fair parameter selection. When this happens, we keep all parameter sets with best performance at training and perform a majority voting between their outputs at test time.\nParameters a and nb both influence the descriptions of the keypoints; their optimal values vary between sets so that the description of keypoints can fit the shape of the data. If the data contains sharp peaks, the size of the neighborhood on which features are computed (equal to a \u00d7 nb) should be small. On the contrary, if it contains smooth peaks, descriptions should take more points into account. Parameter k of the k-means needs to be large enough to precisely\n1 http://people.irisa.fr/Adeline.Bailly/code.html\nrepresent the different features. However, it needs to be small enough in order to avoid overfitting. We consequently allow a large range of values for k.\nIn the following, BoTSW denotes the approach where keypoints are selected as scale-space extrema and BoW histograms are `2-normalized. For all experiments with dense extraction, we set \u03c4step = 10, and we extract keypoints at all scales. Using such a value for \u03c4step enables one to have a sufficient number of keypoints even for small time series, and guarantees that keypoint neighborhoods overlap so that all subparts of the time series are described."}, {"heading": "4.2 Experiments on dense extraction", "text": "Fig. 2 shows a pairwise comparison of error rates between BoTSW and its dense counterpart D-BoTSW for all datasets in the UCR repository. A point on the diagonal means that obtained error rates are equals. A point above the diagonal illustrates a case where D-BoTSW has a smaller error rate than BoTSW. Wilcoxon signed rank test\u2019s p-value and Win/Tie/Lose scores are given in the bottom-right corner of the figure. Win/Tie/Lose scores indicate that D-BoTSW reaches better performance than BoTSW on 61 datasets, equivalent performance on 4 datasets and worse on 21 datasets. Wilcoxon test shows that this difference is significant (in the following, we will use a significance level of 10% for all statistical tests).\nD-BoTSW improves classification on a large majority of the datasets. However, most points are close to the diagonal, which means that the improvement is of little magnitude. In the following, we show how to further improve these results thanks to D-BoTSW normalization."}, {"heading": "4.3 Experiments on BoW normalization", "text": "In image retrieval and classification, Bag-of-Words normalizations have been shown to improve classification rates with dense extracted keypoints. We investigate here the impact of SSR and IDF normalizations on D-BoTSW for time series classification.\nAs it can be seen in Fig. 3, both SSR and IDF normalizations improve classification performance (though the improvement of using IDF is not statistically significant). Lowering the influence of largely-represented codewords hence leads to more accurate classification with D-BoTSW.\nIDF normalization only leads to a small improvement in classification accuracy: Win/Tie/Lose score against non-normalized D-BoSTW is 38/14/34. On the contrary, SSR normalization significantly improves the classification accuracy, with a Win/Tie/Lose score of 61/10/15 over non-normalized D-BoSTW.\nThis is backed by Fig. 4, in which one can see that when using SSR normalization, variance (i.e. energy) is spread across all dimensions of the BoW, leading to a more balanced representation than with other two normalization schemes."}, {"heading": "4.4 Comparison with state-of-the-art methods", "text": "In the following, we will refer to dense SSR-normalized BoTSW as D-BoTSW, since this setup is the one providing the best classification performance. We now compare D-BoTSW to the most popular state-of-the-art methods for time series classification. The UCR repository provides error rates for the 86 datasets with Euclidean distance 1NN (EDNN) and Dynamic Time Warping 1NN (DTWNN) [20]. We use published error rates for TSBF (45 datasets) [3], SAX-VSM (51 datasets) [21], SMTS (45 datasets) [2], PROP (46 datasets) [15] and BoP (20 datasets).\nAs BoP [14] only provides classification performance for 20 datasets, we decided not to plot pairwise comparison of error rates between D-BoTSW and BoP.\nNote however that the Win/Tie/Lose score is 17/1/2 in favor of D-BoTSW and this difference is statistically significant (p < 0.001). BoP has smaller error rate than D-BoTSW on wafer (0.003 vs. 0.004) and Olive Oil (0.133 vs. 0.167) data sets.\nFig. 5 shows that D-BoTSW performs better than 1NN combined with ED (EDNN) or DTW (DTWNN), TSBF, SAX-VSM and SMTS. Though relying on a single similarity measure that has linear time complexity in the length of time series, D-BoTSW slightly outperforms PROP, which relies on outputs from several metrics with quadratic time complexity. In Fig. 5, it is striking to realize that D-BoTSW not only improves the classification, but might improve it considerably. Error rate on Shapelet Sim dataset drops from 0.461 (EDNN) and 0.35 (DTWNN) to 0 (D-BoTSW), for example. Pairwise comparisons of methods show that all observed differences between D-BoTSW and state-ofthe-art methods are statistically significant, except for PROP. Error rates (ER) obtained with D-BoTSW are reported in Table 1, together with baseline scores publicly available at [11].\nThis set of experiments, conducted on a wide variety of time series datasets, shows that D-BoTSW significantly outperforms most state-of-the-art methods."}, {"heading": "5 Conclusion", "text": "In this paper, we presented the D-BoTSW technique, which transforms time series into histograms of quantized local features. The association of SIFT keypoints and Bag-of-Words has been widely used and is considered as a standard\ntechnique in image domain, however it has never been investigated for time series classification. We carried out extensive experiments and showed that dense keypoint extraction and SSR normalization of Bag-of-Words lead to the best performance for our method. We compared the results with standard techniques for time series classification: D-BoTSW has comparable performance to PROP with lower time complexity and significantly outperforms all other techniques.\nWe believe that classification performance could be further improved by taking more time information into account, as well as reducing the impact of quantization losses in our representation. Indeed, only local temporal information is embedded in our model and the global structure of time series is ignored. Moreover, more detailed global representations for sets of features than the standard BoW have been proposed in the computer vision community [9,18], and such global features could be used in our framework."}, {"heading": "Acknowledgments", "text": "This work has been partly funded by ANR project ASTERIX (ANR-13-JS020005-01), Re\u0301gion Bretagne and CNES-TOSCA project VEGIDAR."}], "references": [{"title": "Bag-of-Temporal-SIFT-Words for Time Series Classification", "author": ["Adeline Bailly", "Simon Malinowski", "Romain Tavenard", "Thomas Guyet", "Laetitia Chapel"], "venue": "ECMLPKDD Workshop on Advanced Analytics and Learning on Temporal Data,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Learning a symbolic representation for multivariate time series classification", "author": ["Mustafa G. Baydogan", "George Runger"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A Bag-of-Features Framework to Classify Time Series", "author": ["Mustafa G. Baydogan", "George Runger", "Eugene Tuv"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "sDTW: Computing DTW Distances using Locally Relevant Constraints based on Salient Feature Alignments", "author": ["Kasim S. Candan", "Rosaria Rossini", "Maria L. Sapino"], "venue": "Proceedings of the International Conference on Very Large DataBases,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Fast global alignment kernels", "author": ["Marco Cuturi"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Classification trees for time series", "author": ["Ahlame Douzal-Chouakria", "C\u00e9cile Amblard"], "venue": "Elsevier Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Temporal kernels for the identification of grassland management using time series of high spatial resolution satellite images", "author": ["Pauline Dusseux", "Thomas Corpetti", "Laurence Hubert-Moy"], "venue": "In Geoscience and Remote Sensing Symposium (IGARSS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Negative evidences and co-occurrences in image retrieval: the benefit of PCA and whitening", "author": ["Herv\u00e9 J\u00e9gou", "Ondrej Chum"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid", "Patrick P\u00e9rez"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Creating Efficient Codebooks for Visual Recognition", "author": ["Frederic Jurie", "Bill Triggs"], "venue": "International Conference on Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "The UCR Time Series Classification/Clustering", "author": ["Eamonn Keogh", "Qiang Zhu", "Bing Hu", "Yuan Hao", "Xiaopeng Xi", "Li Wei", "Choti- rat A. Ratanamahatana"], "venue": "Home- page,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann Le Cun", "Yoshua Bengio"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "A symbolic represen- tation of time series, with implications for streaming algorithms", "author": ["Jessica Lin", "Eamonn Keogh", "Stefano Lonardi", "Bill Chiu"], "venue": "In Proceedings of the ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Rotation-invariant similarity in time series using bag-of-patterns representation", "author": ["Jessica Lin", "Rohan Khade", "Yuan Li"], "venue": "International Journal of Information Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Time series classification with ensembles of elastic distance measures", "author": ["Jason Lines", "Anthony Bagnall"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Object Recognition from Local Scale-Invariant Features", "author": ["David G. Lowe"], "venue": "In Pro- ceedings of the International Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["David G. Lowe"], "venue": "Inter- national Journal of Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["Florent Perronnin", "Christopher Dance"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Large-Scale Image Retrieval with Compressed Fisher Vectors", "author": ["Florent Perronnin", "Yan Liu", "Jorge Sanchez", "Herv\u00e9 Poirier"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Everything you know about dy- namic time warping is wrong", "author": ["Chotirat A. Ratanamahatana", "Eamonn Keogh"], "venue": "In Proceedings of the Workshop on Mining Temporal and Sequential Data,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "SAX-VSM: Interpretable Time Series Classifi- cation Using SAX and Vector Space Model", "author": ["Pavel Senin", "Sergey Malinchik"], "venue": "Proceedings of the IEEE International Conference on Data Mining,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["Josef Sivic", "Andrew Zisserman"], "venue": "In Proceedings of the International Conference on Com- puter Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Evaluation of local spatio-temporal features for action recognition", "author": ["Heng Wang", "Muhammad M. Ullah", "Alexander Klaser", "Ivan Laptev", "Cordelia Schmid"], "venue": "In Proceedings of the British Machine Vision Conference,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Bag- of-Words Representation for Biomedical Time Series Classification", "author": ["Jim Wang", "Ping Liu", "Mary F.H. She", "Saeid Nahavandi", "Addas Kouzani"], "venue": "Biomedical Signal Processing and Control,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "A Scale-Invariant Local Descriptor for Event Recog- nition in 1D Sensor Signals", "author": ["Jierui Xie", "Mandis Beigi"], "venue": "In Proceedings of the IEEE International Conference on Multimedia and Expo,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["Lexiang Ye", "Eamonn Keogh"], "venue": "In Proceedings of the ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In [1], we designed a Bag-of-Words approach based on an adaptation of this framework to time series classification.", "startOffset": 3, "endOffset": 6}, {"referenceID": 23, "context": "1 Introduction Classification of time series has received an important amount of interest over the past years due to many real-life applications, such as medicine [24], environmental modeling [7], speech recognition [12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 6, "context": "1 Introduction Classification of time series has received an important amount of interest over the past years due to many real-life applications, such as medicine [24], environmental modeling [7], speech recognition [12].", "startOffset": 192, "endOffset": 195}, {"referenceID": 11, "context": "1 Introduction Classification of time series has received an important amount of interest over the past years due to many real-life applications, such as medicine [24], environmental modeling [7], speech recognition [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 19, "context": "The combination of the kNN classifier with DTW is one of the most popular method since it achieves high classification accuracy [20].", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "For these reasons, it has been adapted to time series data in some recent works [2,3,14,21,24].", "startOffset": 80, "endOffset": 94}, {"referenceID": 2, "context": "For these reasons, it has been adapted to time series data in some recent works [2,3,14,21,24].", "startOffset": 80, "endOffset": 94}, {"referenceID": 13, "context": "For these reasons, it has been adapted to time series data in some recent works [2,3,14,21,24].", "startOffset": 80, "endOffset": 94}, {"referenceID": 20, "context": "For these reasons, it has been adapted to time series data in some recent works [2,3,14,21,24].", "startOffset": 80, "endOffset": 94}, {"referenceID": 23, "context": "For these reasons, it has been adapted to time series data in some recent works [2,3,14,21,24].", "startOffset": 80, "endOffset": 94}, {"referenceID": 16, "context": "Particularly, the Scale-Invariant Feature Transform (SIFT) framework has led to widely used descriptors [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "In [1], we built on this framework to design a BoW approach for time series classification where words correspond to quantized versions of local features.", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "This approach can be seen as an adaptation of [22], which uses SIFT features associated with visual words, to time series.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "In [20], Ratanamahatana and Keogh compare Dynamic Time Warping to Euclidean Distance when used with a simple kNN classifier.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Cuturi [5] shows that, although DTW is well-suited to retrieval tasks since it focuses on the best possible alignment between time series, it fails at precisely quantifying dissimilarity between non-matching sequences (which is backed by the fact that DTW-derived kernel is not positive definite).", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "Lines and Bagnall [15] propose an ensemble classifier based on elastic distance measures (including DTW), named Proportional Elastic Ensemble (PROP).", "startOffset": 18, "endOffset": 22}, {"referenceID": 25, "context": "Instead of building classification decision on similarities between time series, Ye and Keogh [26] use a decision tree in which the partitioning of time series is performed with respect to the presence (or absence) of discriminant sub-sequences (named shapelets) in the series.", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "Douzal and Amblard [6] define a dedicated metric for time series which is then used in a classification tree.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "2 Bag-of-Words for time series classification Inspired by text mining, information retrieval and computer vision communities, recent works have investigated the use of Bag-of-Words for time series classification [2,3,14,21,24].", "startOffset": 212, "endOffset": 226}, {"referenceID": 2, "context": "2 Bag-of-Words for time series classification Inspired by text mining, information retrieval and computer vision communities, recent works have investigated the use of Bag-of-Words for time series classification [2,3,14,21,24].", "startOffset": 212, "endOffset": 226}, {"referenceID": 13, "context": "2 Bag-of-Words for time series classification Inspired by text mining, information retrieval and computer vision communities, recent works have investigated the use of Bag-of-Words for time series classification [2,3,14,21,24].", "startOffset": 212, "endOffset": 226}, {"referenceID": 20, "context": "2 Bag-of-Words for time series classification Inspired by text mining, information retrieval and computer vision communities, recent works have investigated the use of Bag-of-Words for time series classification [2,3,14,21,24].", "startOffset": 212, "endOffset": 226}, {"referenceID": 23, "context": "2 Bag-of-Words for time series classification Inspired by text mining, information retrieval and computer vision communities, recent works have investigated the use of Bag-of-Words for time series classification [2,3,14,21,24].", "startOffset": 212, "endOffset": 226}, {"referenceID": 2, "context": "[3] propose a framework to classify time series denoted TSBF where local features such as mean, variance and extremum values are computed on sliding windows.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "In [24], discrete wavelet coefficients are extracted on sliding windows and then quantized into words using k-means.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [14,21], words are constructed using the Symbolic Aggregate approXimation (SAX) representation [13] of time series.", "startOffset": 3, "endOffset": 10}, {"referenceID": 20, "context": "In [14,21], words are constructed using the Symbolic Aggregate approXimation (SAX) representation [13] of time series.", "startOffset": 3, "endOffset": 10}, {"referenceID": 12, "context": "In [14,21], words are constructed using the Symbolic Aggregate approXimation (SAX) representation [13] of time series.", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "In [21], Senin and Malinchik combine SAX with Vector Space Model to form the SAX-VSM method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "In [2], Baydogan and Runger design a symbolic representation of multivariate time series (MTS), called SMTS, where MTS are transformed into a feature matrix, whose rows are feature vectors containing a time index, the values and the gradient of time series at this time index (on all dimensions).", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "One of the most powerful local feature for image is SIFT [17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "Xie and Beigi [25] use similar keypoint detection for time series.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "In [4], extraction and description of time series keypoints in a SIFT-like framework is used to reduce the complexity of DTW: features are used to match anchor points from two different time series and prune the search space when searching for the optimal path for DTW.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Two approaches are described here: the first one is based on scale-space extrema detection (as in [1]) and the second one proposes a dense extraction scheme.", "startOffset": 98, "endOffset": 101}, {"referenceID": 15, "context": "Lowe [16] proposes the Difference-of-Gaussians (DoG) function to detect scalespace extrema in images.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Previous researches have shown that accurate classification could be achieved by using densely extracted local features [10,23].", "startOffset": 120, "endOffset": 127}, {"referenceID": 22, "context": "Previous researches have shown that accurate classification could be achieved by using densely extracted local features [10,23].", "startOffset": 120, "endOffset": 127}, {"referenceID": 7, "context": "These normalizations are commonly used in image retrieval and classification based on histograms [8,9,19,22].", "startOffset": 97, "endOffset": 108}, {"referenceID": 8, "context": "These normalizations are commonly used in image retrieval and classification based on histograms [8,9,19,22].", "startOffset": 97, "endOffset": 108}, {"referenceID": 18, "context": "These normalizations are commonly used in image retrieval and classification based on histograms [8,9,19,22].", "startOffset": 97, "endOffset": 108}, {"referenceID": 21, "context": "These normalizations are commonly used in image retrieval and classification based on histograms [8,9,19,22].", "startOffset": 97, "endOffset": 108}, {"referenceID": 8, "context": "[9] and Perronin et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] show that reducing the influence of frequent codewords before `2 normalization could be profitable.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "They apply a power \u03b1 \u2208 [0, 1] on their global representation.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "5, which leads to near-optimal results [9,19].", "startOffset": 39, "endOffset": 45}, {"referenceID": 18, "context": "5, which leads to near-optimal results [9,19].", "startOffset": 39, "endOffset": 45}, {"referenceID": 10, "context": "To provide illustrative timings for our methods, we ran it on a personal computer, for a given set of parameters, using dataset Cricket X [11] that is made of 390 training time series and 390 test ones.", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "1 Experimental setup Experiments are conducted on the 86 currently available datasets from the UCR repository [11], the largest online database for time series classification.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "6 and ksc = 2 , as these values have shown to produce stable results [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "The UCR repository provides error rates for the 86 datasets with Euclidean distance 1NN (EDNN) and Dynamic Time Warping 1NN (DTWNN) [20].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "We use published error rates for TSBF (45 datasets) [3], SAX-VSM (51 datasets) [21], SMTS (45 datasets) [2], PROP (46 datasets) [15] and BoP (20 datasets).", "startOffset": 52, "endOffset": 55}, {"referenceID": 20, "context": "We use published error rates for TSBF (45 datasets) [3], SAX-VSM (51 datasets) [21], SMTS (45 datasets) [2], PROP (46 datasets) [15] and BoP (20 datasets).", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "We use published error rates for TSBF (45 datasets) [3], SAX-VSM (51 datasets) [21], SMTS (45 datasets) [2], PROP (46 datasets) [15] and BoP (20 datasets).", "startOffset": 104, "endOffset": 107}, {"referenceID": 14, "context": "We use published error rates for TSBF (45 datasets) [3], SAX-VSM (51 datasets) [21], SMTS (45 datasets) [2], PROP (46 datasets) [15] and BoP (20 datasets).", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "As BoP [14] only provides classification performance for 20 datasets, we decided not to plot pairwise comparison of error rates between D-BoTSW and BoP.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Error rates (ER) obtained with D-BoTSW are reported in Table 1, together with baseline scores publicly available at [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "Moreover, more detailed global representations for sets of features than the standard BoW have been proposed in the computer vision community [9,18], and such global features could be used in our framework.", "startOffset": 142, "endOffset": 148}, {"referenceID": 17, "context": "Moreover, more detailed global representations for sets of features than the standard BoW have been proposed in the computer vision community [9,18], and such global features could be used in our framework.", "startOffset": 142, "endOffset": 148}], "year": 2017, "abstractText": "The SIFT framework has shown to be accurate in the image classification context. In [1], we designed a Bag-of-Words approach based on an adaptation of this framework to time series classification. It relies on two steps: SIFT-based features are first extracted and quantized into words; histograms of occurrences of each word are then fed into a classifier. In this paper, we investigate techniques to improve the performance of Bag-of-Temporal-SIFT-Words: dense extraction of keypoints and normalization of Bag-of-Words histograms. Extensive experiments show that our method significantly outperforms most state-of-the-art techniques for time series classification.", "creator": "LaTeX with hyperref package"}}}